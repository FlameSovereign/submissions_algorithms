python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=341695162 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-16-23-27.log
2025-03-07 16:23:32.527073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741364612.549064       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741364612.555781       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:23:39.090692 139683440854208 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax.
I0307 16:23:39.973474 139683440854208 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:23:39.976557 139683440854208 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:23:39.978415 139683440854208 submission_runner.py:606] Using RNG seed 341695162
I0307 16:23:40.573133 139683440854208 submission_runner.py:615] --- Tuning run 4/5 ---
I0307 16:23:40.573368 139683440854208 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_4.
I0307 16:23:40.573589 139683440854208 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_4/hparams.json.
I0307 16:23:40.817687 139683440854208 submission_runner.py:218] Initializing dataset.
I0307 16:23:45.817424 139683440854208 submission_runner.py:229] Initializing model.
I0307 16:23:55.885611 139683440854208 submission_runner.py:272] Initializing optimizer.
I0307 16:23:56.353784 139683440854208 submission_runner.py:279] Initializing metrics bundle.
I0307 16:23:56.354000 139683440854208 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:23:56.354764 139683440854208 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_4 with prefix checkpoint_
I0307 16:23:56.354867 139683440854208 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_4/meta_data_0.json.
I0307 16:23:56.355028 139683440854208 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:23:56.355073 139683440854208 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:23:56.512254 139683440854208 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_4/flags_0.json.
I0307 16:23:56.543292 139683440854208 submission_runner.py:337] Starting training loop.
E0307 16:24:22.783563       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:22.991967       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:23.401747       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:23.610916       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:25.450462       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:25.659423       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 16:24:37.212162 139546339555072 logging_writer.py:48] [0] global_step=0, grad_norm=4.96845006942749, loss=0.9736577868461609
I0307 16:24:37.268380 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:25:29.779608 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:26:20.228929 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:26:40.563388 139683440854208 submission_runner.py:469] Time since start: 164.02s, 	Step: 1, 	{'train/ssim': 0.2215797049658639, 'train/loss': 0.9878394263131278, 'validation/ssim': 0.211564235225714, 'validation/loss': 1.0006924416150815, 'validation/num_examples': 3554, 'test/ssim': 0.23443838725107338, 'test/loss': 0.997806347738062, 'test/num_examples': 3581, 'score': 40.72488880157471, 'total_duration': 164.02004098892212, 'accumulated_submission_time': 40.72488880157471, 'accumulated_eval_time': 123.29496097564697, 'accumulated_logging_time': 0}
I0307 16:26:40.570879 139522968884992 logging_writer.py:48] [1] accumulated_eval_time=123.295, accumulated_logging_time=0, accumulated_submission_time=40.7249, global_step=1, preemption_count=0, score=40.7249, test/loss=0.997806, test/num_examples=3581, test/ssim=0.234438, total_duration=164.02, train/loss=0.987839, train/ssim=0.22158, validation/loss=1.00069, validation/num_examples=3554, validation/ssim=0.211564
I0307 16:26:52.639986 139522885023488 logging_writer.py:48] [100] global_step=100, grad_norm=0.8400844931602478, loss=0.27136290073394775
I0307 16:27:09.258999 139522968884992 logging_writer.py:48] [200] global_step=200, grad_norm=0.5134999752044678, loss=0.37507927417755127
I0307 16:27:25.199449 139522885023488 logging_writer.py:48] [300] global_step=300, grad_norm=0.16264864802360535, loss=0.3535114526748657
I0307 16:27:41.657646 139522968884992 logging_writer.py:48] [400] global_step=400, grad_norm=0.33344051241874695, loss=0.3465437889099121
I0307 16:27:57.915354 139522885023488 logging_writer.py:48] [500] global_step=500, grad_norm=0.07708799093961716, loss=0.2800450325012207
I0307 16:28:00.594484 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:28:02.185415 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:28:03.491618 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:28:04.803174 139683440854208 submission_runner.py:469] Time since start: 248.26s, 	Step: 514, 	{'train/ssim': 0.7288086754935128, 'train/loss': 0.2848827838897705, 'validation/ssim': 0.7104518978395822, 'validation/loss': 0.30286932555087576, 'validation/num_examples': 3554, 'test/ssim': 0.727477498974623, 'test/loss': 0.30499351094535393, 'test/num_examples': 3581, 'score': 120.65378880500793, 'total_duration': 248.25979471206665, 'accumulated_submission_time': 120.65378880500793, 'accumulated_eval_time': 127.50357937812805, 'accumulated_logging_time': 0.01652979850769043}
I0307 16:28:04.813263 139522968884992 logging_writer.py:48] [514] accumulated_eval_time=127.504, accumulated_logging_time=0.0165298, accumulated_submission_time=120.654, global_step=514, preemption_count=0, score=120.654, test/loss=0.304994, test/num_examples=3581, test/ssim=0.727477, total_duration=248.26, train/loss=0.284883, train/ssim=0.728809, validation/loss=0.302869, validation/num_examples=3554, validation/ssim=0.710452
I0307 16:28:16.241988 139522885023488 logging_writer.py:48] [600] global_step=600, grad_norm=0.1268302947282791, loss=0.28008848428726196
I0307 16:28:33.987614 139522968884992 logging_writer.py:48] [700] global_step=700, grad_norm=0.13464468717575073, loss=0.30434703826904297
I0307 16:28:50.456917 139522885023488 logging_writer.py:48] [800] global_step=800, grad_norm=0.260824590921402, loss=0.2139522284269333
I0307 16:29:07.559576 139522968884992 logging_writer.py:48] [900] global_step=900, grad_norm=0.20628808438777924, loss=0.33309853076934814
I0307 16:29:22.065785 139522885023488 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.19489602744579315, loss=0.29231977462768555
I0307 16:29:24.871406 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:29:26.173828 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:29:27.480400 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:29:28.791987 139683440854208 submission_runner.py:469] Time since start: 332.25s, 	Step: 1035, 	{'train/ssim': 0.7304443631853376, 'train/loss': 0.2783775329589844, 'validation/ssim': 0.7109470485500492, 'validation/loss': 0.2967352751740996, 'validation/num_examples': 3554, 'test/ssim': 0.7283911344160151, 'test/loss': 0.29839530546416854, 'test/num_examples': 3581, 'score': 200.59399437904358, 'total_duration': 332.2486312389374, 'accumulated_submission_time': 200.59399437904358, 'accumulated_eval_time': 131.42411375045776, 'accumulated_logging_time': 0.040845394134521484}
I0307 16:29:28.800970 139522968884992 logging_writer.py:48] [1035] accumulated_eval_time=131.424, accumulated_logging_time=0.0408454, accumulated_submission_time=200.594, global_step=1035, preemption_count=0, score=200.594, test/loss=0.298395, test/num_examples=3581, test/ssim=0.728391, total_duration=332.249, train/loss=0.278378, train/ssim=0.730444, validation/loss=0.296735, validation/num_examples=3554, validation/ssim=0.710947
I0307 16:29:34.244340 139522885023488 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.12323354184627533, loss=0.2670404314994812
I0307 16:29:42.489362 139522968884992 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.12496528029441833, loss=0.3235277533531189
I0307 16:29:50.720164 139522885023488 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.22093477845191956, loss=0.2260434776544571
I0307 16:29:58.976693 139522968884992 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.18308612704277039, loss=0.2633897364139557
I0307 16:30:07.215435 139522885023488 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.35185301303863525, loss=0.27224287390708923
I0307 16:30:15.488030 139522968884992 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.2689378261566162, loss=0.24000251293182373
I0307 16:30:23.761420 139522885023488 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.11816468089818954, loss=0.3242418169975281
I0307 16:30:31.996737 139522968884992 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.1670479029417038, loss=0.2536596655845642
I0307 16:30:40.252959 139522885023488 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.1796598732471466, loss=0.2792864441871643
I0307 16:30:48.513451 139522968884992 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.07672096788883209, loss=0.30723878741264343
I0307 16:30:48.844900 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:30:50.149914 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:30:51.460351 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:30:52.775850 139683440854208 submission_runner.py:469] Time since start: 416.23s, 	Step: 2005, 	{'train/ssim': 0.7385732105800084, 'train/loss': 0.2740830523627145, 'validation/ssim': 0.7191288507447594, 'validation/loss': 0.293035212030019, 'validation/num_examples': 3554, 'test/ssim': 0.736226678127618, 'test/loss': 0.29469583527864074, 'test/num_examples': 3581, 'score': 280.58061170578003, 'total_duration': 416.2325031757355, 'accumulated_submission_time': 280.58061170578003, 'accumulated_eval_time': 135.3550262451172, 'accumulated_logging_time': 0.05792498588562012}
I0307 16:30:52.784326 139522885023488 logging_writer.py:48] [2005] accumulated_eval_time=135.355, accumulated_logging_time=0.057925, accumulated_submission_time=280.581, global_step=2005, preemption_count=0, score=280.581, test/loss=0.294696, test/num_examples=3581, test/ssim=0.736227, total_duration=416.233, train/loss=0.274083, train/ssim=0.738573, validation/loss=0.293035, validation/num_examples=3554, validation/ssim=0.719129
I0307 16:31:00.738807 139522968884992 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.20560520887374878, loss=0.2805093228816986
I0307 16:31:08.996640 139522885023488 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.19891586899757385, loss=0.2544068396091461
I0307 16:31:17.244272 139522968884992 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.12919287383556366, loss=0.30893781781196594
I0307 16:31:25.493885 139522885023488 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.32920601963996887, loss=0.24120929837226868
I0307 16:31:33.741264 139522968884992 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.14296212792396545, loss=0.3231595456600189
I0307 16:31:41.994779 139522885023488 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.24113713204860687, loss=0.25999581813812256
I0307 16:31:50.229027 139522968884992 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.1382206529378891, loss=0.2506011426448822
I0307 16:31:58.481355 139522885023488 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.11605033278465271, loss=0.27422794699668884
I0307 16:32:06.705919 139522968884992 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.3674866259098053, loss=0.2573041319847107
I0307 16:32:12.799189 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:32:14.108340 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:32:15.418313 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:32:16.732965 139683440854208 submission_runner.py:469] Time since start: 500.19s, 	Step: 2975, 	{'train/ssim': 0.7397689819335938, 'train/loss': 0.27164842401232037, 'validation/ssim': 0.7196033243792206, 'validation/loss': 0.29060345737197524, 'validation/num_examples': 3554, 'test/ssim': 0.7369792121177744, 'test/loss': 0.29216821964011447, 'test/num_examples': 3581, 'score': 360.53930377960205, 'total_duration': 500.18961906433105, 'accumulated_submission_time': 360.53930377960205, 'accumulated_eval_time': 139.28876042366028, 'accumulated_logging_time': 0.07444190979003906}
I0307 16:32:16.741657 139522885023488 logging_writer.py:48] [2975] accumulated_eval_time=139.289, accumulated_logging_time=0.0744419, accumulated_submission_time=360.539, global_step=2975, preemption_count=0, score=360.539, test/loss=0.292168, test/num_examples=3581, test/ssim=0.736979, total_duration=500.19, train/loss=0.271648, train/ssim=0.739769, validation/loss=0.290603, validation/num_examples=3554, validation/ssim=0.719603
I0307 16:32:18.909579 139522968884992 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.28684335947036743, loss=0.2044503390789032
I0307 16:32:27.167299 139522885023488 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2181980460882187, loss=0.3200288414955139
I0307 16:32:35.416245 139522968884992 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.12968431413173676, loss=0.2335713803768158
I0307 16:32:43.652239 139522885023488 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.11829642206430435, loss=0.2499937117099762
I0307 16:32:51.943474 139522968884992 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.2343747615814209, loss=0.2758808434009552
I0307 16:33:00.169356 139522885023488 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2902485430240631, loss=0.23671264946460724
I0307 16:33:08.426793 139522968884992 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.09371493011713028, loss=0.3005269169807434
I0307 16:33:16.678937 139522885023488 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.23363789916038513, loss=0.2532529830932617
I0307 16:33:24.942980 139522968884992 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.16920465230941772, loss=0.24411430954933167
I0307 16:33:33.175735 139522885023488 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.23579992353916168, loss=0.3252190947532654
I0307 16:33:36.811666 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:33:38.121330 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:33:39.431658 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:33:40.744040 139683440854208 submission_runner.py:469] Time since start: 584.20s, 	Step: 3945, 	{'train/ssim': 0.740063054221017, 'train/loss': 0.2727797542299543, 'validation/ssim': 0.7196340995621131, 'validation/loss': 0.2920518487641566, 'validation/num_examples': 3554, 'test/ssim': 0.7368745609422996, 'test/loss': 0.2934836542363341, 'test/num_examples': 3581, 'score': 440.55210971832275, 'total_duration': 584.200689792633, 'accumulated_submission_time': 440.55210971832275, 'accumulated_eval_time': 143.22108936309814, 'accumulated_logging_time': 0.09151864051818848}
I0307 16:33:40.753006 139522968884992 logging_writer.py:48] [3945] accumulated_eval_time=143.221, accumulated_logging_time=0.0915186, accumulated_submission_time=440.552, global_step=3945, preemption_count=0, score=440.552, test/loss=0.293484, test/num_examples=3581, test/ssim=0.736875, total_duration=584.201, train/loss=0.27278, train/ssim=0.740063, validation/loss=0.292052, validation/num_examples=3554, validation/ssim=0.719634
I0307 16:33:45.400359 139522885023488 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.12502272427082062, loss=0.3233254551887512
I0307 16:33:53.636958 139522968884992 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0986337661743164, loss=0.3439788818359375
I0307 16:34:01.880273 139522885023488 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.13028912246227264, loss=0.22245144844055176
I0307 16:34:10.111338 139522968884992 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.09934087097644806, loss=0.23702333867549896
I0307 16:34:18.344216 139522885023488 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.33294054865837097, loss=0.1766960769891739
I0307 16:34:26.582717 139522968884992 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.06640885025262833, loss=0.2679511606693268
I0307 16:34:34.813387 139522885023488 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.2338908165693283, loss=0.2855924367904663
I0307 16:34:43.067309 139522968884992 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.08346672356128693, loss=0.2589920163154602
I0307 16:34:51.324837 139522885023488 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.27878057956695557, loss=0.35726678371429443
I0307 16:34:59.572595 139522968884992 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.15070997178554535, loss=0.2461320161819458
I0307 16:35:00.808768 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:35:02.116465 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:35:03.424342 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:35:04.736804 139683440854208 submission_runner.py:469] Time since start: 668.19s, 	Step: 4916, 	{'train/ssim': 0.7335399900163923, 'train/loss': 0.27308130264282227, 'validation/ssim': 0.7132919388804868, 'validation/loss': 0.2918663389842431, 'validation/num_examples': 3554, 'test/ssim': 0.7309693030490785, 'test/loss': 0.29338275277724446, 'test/num_examples': 3581, 'score': 520.5496509075165, 'total_duration': 668.1934564113617, 'accumulated_submission_time': 520.5496509075165, 'accumulated_eval_time': 147.14908170700073, 'accumulated_logging_time': 0.10814404487609863}
I0307 16:35:04.746263 139522885023488 logging_writer.py:48] [4916] accumulated_eval_time=147.149, accumulated_logging_time=0.108144, accumulated_submission_time=520.55, global_step=4916, preemption_count=0, score=520.55, test/loss=0.293383, test/num_examples=3581, test/ssim=0.730969, total_duration=668.193, train/loss=0.273081, train/ssim=0.73354, validation/loss=0.291866, validation/num_examples=3554, validation/ssim=0.713292
I0307 16:35:11.761841 139522968884992 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1988140493631363, loss=0.24747851490974426
I0307 16:35:20.008960 139522885023488 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0861220732331276, loss=0.30976173281669617
I0307 16:35:28.241463 139522968884992 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.07693564146757126, loss=0.2899722456932068
I0307 16:35:36.495059 139522885023488 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.09289699792861938, loss=0.26382747292518616
I0307 16:35:44.753084 139522968884992 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.23888346552848816, loss=0.24455121159553528
I0307 16:35:53.001421 139522885023488 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0882965698838234, loss=0.22624415159225464
I0307 16:36:01.246285 139522968884992 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.16861014068126678, loss=0.23993922770023346
I0307 16:36:09.490886 139522885023488 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.28898072242736816, loss=0.2949606776237488
I0307 16:36:17.740356 139522968884992 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.10471288859844208, loss=0.284616619348526
I0307 16:36:24.750977 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:36:26.058595 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:36:27.369094 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:36:28.681965 139683440854208 submission_runner.py:469] Time since start: 752.14s, 	Step: 5886, 	{'train/ssim': 0.7429137229919434, 'train/loss': 0.270447952406747, 'validation/ssim': 0.7230239722188028, 'validation/loss': 0.2891694231521701, 'validation/num_examples': 3554, 'test/ssim': 0.7401742431845155, 'test/loss': 0.29080772027061225, 'test/num_examples': 3581, 'score': 600.4965524673462, 'total_duration': 752.1386196613312, 'accumulated_submission_time': 600.4965524673462, 'accumulated_eval_time': 151.08002352714539, 'accumulated_logging_time': 0.12511444091796875}
I0307 16:36:28.691004 139522885023488 logging_writer.py:48] [5886] accumulated_eval_time=151.08, accumulated_logging_time=0.125114, accumulated_submission_time=600.497, global_step=5886, preemption_count=0, score=600.497, test/loss=0.290808, test/num_examples=3581, test/ssim=0.740174, total_duration=752.139, train/loss=0.270448, train/ssim=0.742914, validation/loss=0.289169, validation/num_examples=3554, validation/ssim=0.723024
I0307 16:36:29.931648 139522968884992 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.32367146015167236, loss=0.24387361109256744
I0307 16:36:38.206214 139522885023488 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.09082470089197159, loss=0.2638050615787506
I0307 16:36:46.438211 139522968884992 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.1266072392463684, loss=0.41257214546203613
I0307 16:36:54.674588 139522885023488 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.24710145592689514, loss=0.23933053016662598
I0307 16:37:02.939511 139522968884992 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.11717086285352707, loss=0.2551534175872803
I0307 16:37:11.180511 139522885023488 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.26522088050842285, loss=0.3684610426425934
I0307 16:37:19.429974 139522968884992 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.19992896914482117, loss=0.2118675410747528
I0307 16:37:27.686733 139522885023488 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.09887327253818512, loss=0.21702975034713745
I0307 16:37:35.956040 139522968884992 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.12417810410261154, loss=0.27954256534576416
I0307 16:37:44.199674 139522885023488 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.12518726289272308, loss=0.33255133032798767
I0307 16:37:48.751885 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:37:50.060140 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:37:51.369457 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:37:52.680390 139683440854208 submission_runner.py:469] Time since start: 836.14s, 	Step: 6856, 	{'train/ssim': 0.7354585783822196, 'train/loss': 0.2712125778198242, 'validation/ssim': 0.714933052986072, 'validation/loss': 0.2902903817111881, 'validation/num_examples': 3554, 'test/ssim': 0.7324399418153099, 'test/loss': 0.2917314458579133, 'test/num_examples': 3581, 'score': 680.498996257782, 'total_duration': 836.1370406150818, 'accumulated_submission_time': 680.498996257782, 'accumulated_eval_time': 155.00847625732422, 'accumulated_logging_time': 0.14178729057312012}
I0307 16:37:52.689446 139522968884992 logging_writer.py:48] [6856] accumulated_eval_time=155.008, accumulated_logging_time=0.141787, accumulated_submission_time=680.499, global_step=6856, preemption_count=0, score=680.499, test/loss=0.291731, test/num_examples=3581, test/ssim=0.73244, total_duration=836.137, train/loss=0.271213, train/ssim=0.735459, validation/loss=0.29029, validation/num_examples=3554, validation/ssim=0.714933
I0307 16:37:56.414974 139522885023488 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.10000021010637283, loss=0.3018512725830078
I0307 16:38:04.679284 139522968884992 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.10693781077861786, loss=0.27533280849456787
I0307 16:38:12.946918 139522885023488 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.14223748445510864, loss=0.2979465425014496
I0307 16:38:21.199892 139522968884992 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.07778637856245041, loss=0.30395469069480896
I0307 16:38:29.444578 139522885023488 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.19948633015155792, loss=0.2718288004398346
I0307 16:38:37.694973 139522968884992 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.20160500705242157, loss=0.2855066955089569
I0307 16:38:45.926517 139522885023488 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.07126904278993607, loss=0.30233338475227356
I0307 16:38:54.179170 139522968884992 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.22044126689434052, loss=0.23984086513519287
I0307 16:39:02.434185 139522885023488 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.10360576957464218, loss=0.2538400888442993
I0307 16:39:10.697371 139522968884992 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.2611488997936249, loss=0.23629485070705414
I0307 16:39:12.688314 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:39:13.995904 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:39:15.306118 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:39:16.620504 139683440854208 submission_runner.py:469] Time since start: 920.08s, 	Step: 7825, 	{'train/ssim': 0.7433005741664341, 'train/loss': 0.2701167379106794, 'validation/ssim': 0.7223796168269907, 'validation/loss': 0.2897191173787458, 'validation/num_examples': 3554, 'test/ssim': 0.7395611304672228, 'test/loss': 0.2913283854348471, 'test/num_examples': 3581, 'score': 760.4393343925476, 'total_duration': 920.0771481990814, 'accumulated_submission_time': 760.4393343925476, 'accumulated_eval_time': 158.94061517715454, 'accumulated_logging_time': 0.15912485122680664}
I0307 16:39:16.629813 139522885023488 logging_writer.py:48] [7825] accumulated_eval_time=158.941, accumulated_logging_time=0.159125, accumulated_submission_time=760.439, global_step=7825, preemption_count=0, score=760.439, test/loss=0.291328, test/num_examples=3581, test/ssim=0.739561, total_duration=920.077, train/loss=0.270117, train/ssim=0.743301, validation/loss=0.289719, validation/num_examples=3554, validation/ssim=0.72238
I0307 16:39:22.894679 139522968884992 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.39197954535484314, loss=0.24198269844055176
I0307 16:39:31.162936 139522885023488 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.19073763489723206, loss=0.357344388961792
I0307 16:39:39.423486 139522968884992 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.14175742864608765, loss=0.2961493134498596
I0307 16:39:47.669639 139522885023488 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.11884775012731552, loss=0.28416141867637634
I0307 16:39:55.910908 139522968884992 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.1396208554506302, loss=0.34922122955322266
I0307 16:40:04.168569 139522885023488 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.208616703748703, loss=0.24654574692249298
I0307 16:40:12.405176 139522968884992 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.13818468153476715, loss=0.35695475339889526
I0307 16:40:20.654524 139522885023488 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.1761600822210312, loss=0.22466307878494263
I0307 16:40:28.886141 139522968884992 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.16308528184890747, loss=0.24851420521736145
I0307 16:40:36.653007 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:40:37.959922 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:40:39.272732 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:40:40.581664 139683440854208 submission_runner.py:469] Time since start: 1004.04s, 	Step: 8795, 	{'train/ssim': 0.7394962991986956, 'train/loss': 0.2686404841286795, 'validation/ssim': 0.7182505902240434, 'validation/loss': 0.2881865407485404, 'validation/num_examples': 3554, 'test/ssim': 0.7358922034260681, 'test/loss': 0.28961957150150097, 'test/num_examples': 3581, 'score': 840.4042134284973, 'total_duration': 1004.0383207798004, 'accumulated_submission_time': 840.4042134284973, 'accumulated_eval_time': 162.86923050880432, 'accumulated_logging_time': 0.1769697666168213}
I0307 16:40:40.591656 139522885023488 logging_writer.py:48] [8795] accumulated_eval_time=162.869, accumulated_logging_time=0.17697, accumulated_submission_time=840.404, global_step=8795, preemption_count=0, score=840.404, test/loss=0.28962, test/num_examples=3581, test/ssim=0.735892, total_duration=1004.04, train/loss=0.26864, train/ssim=0.739496, validation/loss=0.288187, validation/num_examples=3554, validation/ssim=0.718251
I0307 16:40:41.099776 139522968884992 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.17762690782546997, loss=0.2205587774515152
I0307 16:40:49.353917 139522885023488 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.08525139838457108, loss=0.26459580659866333
I0307 16:40:57.608722 139522968884992 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.08454981446266174, loss=0.21591605246067047
I0307 16:41:05.841903 139522885023488 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.06263204663991928, loss=0.28590917587280273
I0307 16:41:14.065821 139522968884992 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1284245252609253, loss=0.3069406747817993
I0307 16:41:22.320100 139522885023488 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.08216384053230286, loss=0.27222883701324463
I0307 16:41:30.562916 139522968884992 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.22375799715518951, loss=0.33509159088134766
I0307 16:41:38.813797 139522885023488 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.06000799313187599, loss=0.2325468361377716
I0307 16:41:47.039863 139522968884992 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.07647106796503067, loss=0.25165313482284546
I0307 16:41:55.268114 139522885023488 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.06429420411586761, loss=0.32199209928512573
I0307 16:42:00.623749 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:42:01.930117 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:42:03.238874 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:42:04.551514 139683440854208 submission_runner.py:469] Time since start: 1088.01s, 	Step: 9766, 	{'train/ssim': 0.7425753729684013, 'train/loss': 0.2686694179262434, 'validation/ssim': 0.7215607084447102, 'validation/loss': 0.288002473555325, 'validation/num_examples': 3554, 'test/ssim': 0.7389045892165247, 'test/loss': 0.28952872609998953, 'test/num_examples': 3581, 'score': 920.3752770423889, 'total_duration': 1088.0081460475922, 'accumulated_submission_time': 920.3752770423889, 'accumulated_eval_time': 166.79692602157593, 'accumulated_logging_time': 0.19477009773254395}
I0307 16:42:04.562911 139522968884992 logging_writer.py:48] [9766] accumulated_eval_time=166.797, accumulated_logging_time=0.19477, accumulated_submission_time=920.375, global_step=9766, preemption_count=0, score=920.375, test/loss=0.289529, test/num_examples=3581, test/ssim=0.738905, total_duration=1088.01, train/loss=0.268669, train/ssim=0.742575, validation/loss=0.288002, validation/num_examples=3554, validation/ssim=0.721561
I0307 16:42:10.088377 139522885023488 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.08568175882101059, loss=0.28746145963668823
I0307 16:42:18.330040 139522968884992 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.16224415600299835, loss=0.23811273276805878
I0307 16:42:26.625392 139522885023488 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.17294174432754517, loss=0.23302388191223145
I0307 16:42:35.176047 139522968884992 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.17175725102424622, loss=0.26619160175323486
I0307 16:42:43.417446 139522885023488 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.20038136839866638, loss=0.23906579613685608
I0307 16:42:51.665880 139522968884992 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.10204023122787476, loss=0.2644202709197998
I0307 16:42:59.894795 139522885023488 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18071144819259644, loss=0.2791578769683838
I0307 16:43:08.141083 139522968884992 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.30572372674942017, loss=0.2046857327222824
I0307 16:43:16.377466 139522885023488 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.07665526121854782, loss=0.26980286836624146
I0307 16:43:24.621882 139522968884992 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.08185417950153351, loss=0.3544120788574219
I0307 16:43:24.627250 139683440854208 spec.py:321] Evaluating on the training split.
I0307 16:43:25.936974 139683440854208 spec.py:333] Evaluating on the validation split.
I0307 16:43:27.247395 139683440854208 spec.py:349] Evaluating on the test split.
I0307 16:43:28.556148 139683440854208 submission_runner.py:469] Time since start: 1172.01s, 	Step: 10701, 	{'train/ssim': 0.744645254952567, 'train/loss': 0.26771460260663715, 'validation/ssim': 0.7238757853167206, 'validation/loss': 0.28719998318356077, 'validation/num_examples': 3554, 'test/ssim': 0.7410993323050474, 'test/loss': 0.2886523492041329, 'test/num_examples': 3581, 'score': 997.777331829071, 'total_duration': 1172.0128042697906, 'accumulated_submission_time': 997.777331829071, 'accumulated_eval_time': 170.72576236724854, 'accumulated_logging_time': 2.816911458969116}
I0307 16:43:28.565957 139522885023488 logging_writer.py:48] [10701] accumulated_eval_time=170.726, accumulated_logging_time=2.81691, accumulated_submission_time=997.777, global_step=10701, preemption_count=0, score=997.777, test/loss=0.288652, test/num_examples=3581, test/ssim=0.741099, total_duration=1172.01, train/loss=0.267715, train/ssim=0.744645, validation/loss=0.2872, validation/num_examples=3554, validation/ssim=0.723876
I0307 16:43:28.579075 139522968884992 logging_writer.py:48] [10701] global_step=10701, preemption_count=0, score=997.777
I0307 16:43:29.788448 139683440854208 submission_runner.py:646] Tuning trial 4/5
I0307 16:43:29.788649 139683440854208 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0307 16:43:29.789320 139683440854208 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.2215797049658639, 'train/loss': 0.9878394263131278, 'validation/ssim': 0.211564235225714, 'validation/loss': 1.0006924416150815, 'validation/num_examples': 3554, 'test/ssim': 0.23443838725107338, 'test/loss': 0.997806347738062, 'test/num_examples': 3581, 'score': 40.72488880157471, 'total_duration': 164.02004098892212, 'accumulated_submission_time': 40.72488880157471, 'accumulated_eval_time': 123.29496097564697, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (514, {'train/ssim': 0.7288086754935128, 'train/loss': 0.2848827838897705, 'validation/ssim': 0.7104518978395822, 'validation/loss': 0.30286932555087576, 'validation/num_examples': 3554, 'test/ssim': 0.727477498974623, 'test/loss': 0.30499351094535393, 'test/num_examples': 3581, 'score': 120.65378880500793, 'total_duration': 248.25979471206665, 'accumulated_submission_time': 120.65378880500793, 'accumulated_eval_time': 127.50357937812805, 'accumulated_logging_time': 0.01652979850769043, 'global_step': 514, 'preemption_count': 0}), (1035, {'train/ssim': 0.7304443631853376, 'train/loss': 0.2783775329589844, 'validation/ssim': 0.7109470485500492, 'validation/loss': 0.2967352751740996, 'validation/num_examples': 3554, 'test/ssim': 0.7283911344160151, 'test/loss': 0.29839530546416854, 'test/num_examples': 3581, 'score': 200.59399437904358, 'total_duration': 332.2486312389374, 'accumulated_submission_time': 200.59399437904358, 'accumulated_eval_time': 131.42411375045776, 'accumulated_logging_time': 0.040845394134521484, 'global_step': 1035, 'preemption_count': 0}), (2005, {'train/ssim': 0.7385732105800084, 'train/loss': 0.2740830523627145, 'validation/ssim': 0.7191288507447594, 'validation/loss': 0.293035212030019, 'validation/num_examples': 3554, 'test/ssim': 0.736226678127618, 'test/loss': 0.29469583527864074, 'test/num_examples': 3581, 'score': 280.58061170578003, 'total_duration': 416.2325031757355, 'accumulated_submission_time': 280.58061170578003, 'accumulated_eval_time': 135.3550262451172, 'accumulated_logging_time': 0.05792498588562012, 'global_step': 2005, 'preemption_count': 0}), (2975, {'train/ssim': 0.7397689819335938, 'train/loss': 0.27164842401232037, 'validation/ssim': 0.7196033243792206, 'validation/loss': 0.29060345737197524, 'validation/num_examples': 3554, 'test/ssim': 0.7369792121177744, 'test/loss': 0.29216821964011447, 'test/num_examples': 3581, 'score': 360.53930377960205, 'total_duration': 500.18961906433105, 'accumulated_submission_time': 360.53930377960205, 'accumulated_eval_time': 139.28876042366028, 'accumulated_logging_time': 0.07444190979003906, 'global_step': 2975, 'preemption_count': 0}), (3945, {'train/ssim': 0.740063054221017, 'train/loss': 0.2727797542299543, 'validation/ssim': 0.7196340995621131, 'validation/loss': 0.2920518487641566, 'validation/num_examples': 3554, 'test/ssim': 0.7368745609422996, 'test/loss': 0.2934836542363341, 'test/num_examples': 3581, 'score': 440.55210971832275, 'total_duration': 584.200689792633, 'accumulated_submission_time': 440.55210971832275, 'accumulated_eval_time': 143.22108936309814, 'accumulated_logging_time': 0.09151864051818848, 'global_step': 3945, 'preemption_count': 0}), (4916, {'train/ssim': 0.7335399900163923, 'train/loss': 0.27308130264282227, 'validation/ssim': 0.7132919388804868, 'validation/loss': 0.2918663389842431, 'validation/num_examples': 3554, 'test/ssim': 0.7309693030490785, 'test/loss': 0.29338275277724446, 'test/num_examples': 3581, 'score': 520.5496509075165, 'total_duration': 668.1934564113617, 'accumulated_submission_time': 520.5496509075165, 'accumulated_eval_time': 147.14908170700073, 'accumulated_logging_time': 0.10814404487609863, 'global_step': 4916, 'preemption_count': 0}), (5886, {'train/ssim': 0.7429137229919434, 'train/loss': 0.270447952406747, 'validation/ssim': 0.7230239722188028, 'validation/loss': 0.2891694231521701, 'validation/num_examples': 3554, 'test/ssim': 0.7401742431845155, 'test/loss': 0.29080772027061225, 'test/num_examples': 3581, 'score': 600.4965524673462, 'total_duration': 752.1386196613312, 'accumulated_submission_time': 600.4965524673462, 'accumulated_eval_time': 151.08002352714539, 'accumulated_logging_time': 0.12511444091796875, 'global_step': 5886, 'preemption_count': 0}), (6856, {'train/ssim': 0.7354585783822196, 'train/loss': 0.2712125778198242, 'validation/ssim': 0.714933052986072, 'validation/loss': 0.2902903817111881, 'validation/num_examples': 3554, 'test/ssim': 0.7324399418153099, 'test/loss': 0.2917314458579133, 'test/num_examples': 3581, 'score': 680.498996257782, 'total_duration': 836.1370406150818, 'accumulated_submission_time': 680.498996257782, 'accumulated_eval_time': 155.00847625732422, 'accumulated_logging_time': 0.14178729057312012, 'global_step': 6856, 'preemption_count': 0}), (7825, {'train/ssim': 0.7433005741664341, 'train/loss': 0.2701167379106794, 'validation/ssim': 0.7223796168269907, 'validation/loss': 0.2897191173787458, 'validation/num_examples': 3554, 'test/ssim': 0.7395611304672228, 'test/loss': 0.2913283854348471, 'test/num_examples': 3581, 'score': 760.4393343925476, 'total_duration': 920.0771481990814, 'accumulated_submission_time': 760.4393343925476, 'accumulated_eval_time': 158.94061517715454, 'accumulated_logging_time': 0.15912485122680664, 'global_step': 7825, 'preemption_count': 0}), (8795, {'train/ssim': 0.7394962991986956, 'train/loss': 0.2686404841286795, 'validation/ssim': 0.7182505902240434, 'validation/loss': 0.2881865407485404, 'validation/num_examples': 3554, 'test/ssim': 0.7358922034260681, 'test/loss': 0.28961957150150097, 'test/num_examples': 3581, 'score': 840.4042134284973, 'total_duration': 1004.0383207798004, 'accumulated_submission_time': 840.4042134284973, 'accumulated_eval_time': 162.86923050880432, 'accumulated_logging_time': 0.1769697666168213, 'global_step': 8795, 'preemption_count': 0}), (9766, {'train/ssim': 0.7425753729684013, 'train/loss': 0.2686694179262434, 'validation/ssim': 0.7215607084447102, 'validation/loss': 0.288002473555325, 'validation/num_examples': 3554, 'test/ssim': 0.7389045892165247, 'test/loss': 0.28952872609998953, 'test/num_examples': 3581, 'score': 920.3752770423889, 'total_duration': 1088.0081460475922, 'accumulated_submission_time': 920.3752770423889, 'accumulated_eval_time': 166.79692602157593, 'accumulated_logging_time': 0.19477009773254395, 'global_step': 9766, 'preemption_count': 0}), (10701, {'train/ssim': 0.744645254952567, 'train/loss': 0.26771460260663715, 'validation/ssim': 0.7238757853167206, 'validation/loss': 0.28719998318356077, 'validation/num_examples': 3554, 'test/ssim': 0.7410993323050474, 'test/loss': 0.2886523492041329, 'test/num_examples': 3581, 'score': 997.777331829071, 'total_duration': 1172.0128042697906, 'accumulated_submission_time': 997.777331829071, 'accumulated_eval_time': 170.72576236724854, 'accumulated_logging_time': 2.816911458969116, 'global_step': 10701, 'preemption_count': 0})], 'global_step': 10701}
I0307 16:43:29.789398 139683440854208 submission_runner.py:649] Timing: 997.777331829071
I0307 16:43:29.789435 139683440854208 submission_runner.py:651] Total number of evals: 13
I0307 16:43:29.789463 139683440854208 submission_runner.py:652] ====================
I0307 16:43:29.789561 139683440854208 submission_runner.py:750] Final fastmri score: 3
