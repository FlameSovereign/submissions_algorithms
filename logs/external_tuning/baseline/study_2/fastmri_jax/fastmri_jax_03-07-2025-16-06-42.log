python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=702162548 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-16-06-42.log
2025-03-07 16:06:54.219580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741363614.285211       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741363614.324647       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:07:14.312241 140656718828736 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax.
I0307 16:07:15.356371 140656718828736 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:07:15.359733 140656718828736 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:07:15.361594 140656718828736 submission_runner.py:606] Using RNG seed 702162548
I0307 16:07:16.090822 140656718828736 submission_runner.py:615] --- Tuning run 3/5 ---
I0307 16:07:16.091036 140656718828736 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_3.
I0307 16:07:16.091238 140656718828736 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_3/hparams.json.
I0307 16:07:16.327151 140656718828736 submission_runner.py:218] Initializing dataset.
I0307 16:07:21.560295 140656718828736 submission_runner.py:229] Initializing model.
I0307 16:07:31.902056 140656718828736 submission_runner.py:272] Initializing optimizer.
I0307 16:07:32.373756 140656718828736 submission_runner.py:279] Initializing metrics bundle.
I0307 16:07:32.373953 140656718828736 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:07:32.374677 140656718828736 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_3 with prefix checkpoint_
I0307 16:07:32.374776 140656718828736 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_3/meta_data_0.json.
I0307 16:07:32.374970 140656718828736 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:07:32.375019 140656718828736 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:07:32.786138 140656718828736 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_3/flags_0.json.
I0307 16:07:32.817723 140656718828736 submission_runner.py:337] Starting training loop.
E0307 16:07:56.215965       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:56.421735       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:56.826388       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:57.032340       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:58.898916       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:59.105214       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 16:08:10.235898 140519434868480 logging_writer.py:48] [0] global_step=0, grad_norm=3.7019336223602295, loss=0.943912148475647
I0307 16:08:10.282941 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:09:47.242413 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:10:09.555282 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:10:29.808495 140656718828736 submission_runner.py:469] Time since start: 176.99s, 	Step: 1, 	{'train/ssim': 0.24988419669015066, 'train/loss': 0.9285712242126465, 'validation/ssim': 0.24356833024848762, 'validation/loss': 0.936524055751442, 'validation/num_examples': 3554, 'test/ssim': 0.26563994773304417, 'test/loss': 0.9333185208173346, 'test/num_examples': 3581, 'score': 37.46510195732117, 'total_duration': 176.9906849861145, 'accumulated_submission_time': 37.46510195732117, 'accumulated_eval_time': 139.5254521369934, 'accumulated_logging_time': 0}
I0307 16:10:29.818290 140498983450368 logging_writer.py:48] [1] accumulated_eval_time=139.525, accumulated_logging_time=0, accumulated_submission_time=37.4651, global_step=1, preemption_count=0, score=37.4651, test/loss=0.933319, test/num_examples=3581, test/ssim=0.26564, total_duration=176.991, train/loss=0.928571, train/ssim=0.249884, validation/loss=0.936524, validation/num_examples=3554, validation/ssim=0.243568
I0307 16:10:42.482618 140498975057664 logging_writer.py:48] [100] global_step=100, grad_norm=0.7776477932929993, loss=0.363639771938324
I0307 16:10:59.054986 140498983450368 logging_writer.py:48] [200] global_step=200, grad_norm=0.18261218070983887, loss=0.43384575843811035
I0307 16:11:15.314880 140498975057664 logging_writer.py:48] [300] global_step=300, grad_norm=0.17722158133983612, loss=0.32816189527511597
I0307 16:11:32.135087 140498983450368 logging_writer.py:48] [400] global_step=400, grad_norm=0.09415928274393082, loss=0.364035427570343
I0307 16:11:48.572721 140498975057664 logging_writer.py:48] [500] global_step=500, grad_norm=0.1633855104446411, loss=0.29935041069984436
I0307 16:11:50.036634 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:11:51.886152 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:11:53.190039 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:11:54.499212 140656718828736 submission_runner.py:469] Time since start: 261.68s, 	Step: 509, 	{'train/ssim': 0.7103781700134277, 'train/loss': 0.29674080439976286, 'validation/ssim': 0.6924728024321187, 'validation/loss': 0.3143727523125352, 'validation/num_examples': 3554, 'test/ssim': 0.7102639827605767, 'test/loss': 0.31639643256073724, 'test/num_examples': 3581, 'score': 117.57442593574524, 'total_duration': 261.68143463134766, 'accumulated_submission_time': 117.57442593574524, 'accumulated_eval_time': 143.98797464370728, 'accumulated_logging_time': 0.01915574073791504}
I0307 16:11:54.511368 140498983450368 logging_writer.py:48] [509] accumulated_eval_time=143.988, accumulated_logging_time=0.0191557, accumulated_submission_time=117.574, global_step=509, preemption_count=0, score=117.574, test/loss=0.316396, test/num_examples=3581, test/ssim=0.710264, total_duration=261.681, train/loss=0.296741, train/ssim=0.710378, validation/loss=0.314373, validation/num_examples=3554, validation/ssim=0.692473
I0307 16:12:07.995546 140498975057664 logging_writer.py:48] [600] global_step=600, grad_norm=0.125046044588089, loss=0.3278650939464569
I0307 16:12:24.876858 140498983450368 logging_writer.py:48] [700] global_step=700, grad_norm=0.11855720728635788, loss=0.3027929365634918
I0307 16:12:42.294051 140498975057664 logging_writer.py:48] [800] global_step=800, grad_norm=0.1150769516825676, loss=0.2664892077445984
I0307 16:12:59.876455 140498983450368 logging_writer.py:48] [900] global_step=900, grad_norm=0.06517651677131653, loss=0.3065360486507416
I0307 16:13:14.542158 140498975057664 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.07568470388650894, loss=0.38519760966300964
I0307 16:13:14.546793 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:13:15.846551 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:13:17.149208 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:13:18.461320 140656718828736 submission_runner.py:469] Time since start: 345.64s, 	Step: 1001, 	{'train/ssim': 0.7267413820539202, 'train/loss': 0.28188778672899517, 'validation/ssim': 0.7089707734682752, 'validation/loss': 0.29891268805834975, 'validation/num_examples': 3554, 'test/ssim': 0.7259610454918319, 'test/loss': 0.300941703772951, 'test/num_examples': 3581, 'score': 197.500150680542, 'total_duration': 345.64355516433716, 'accumulated_submission_time': 197.500150680542, 'accumulated_eval_time': 147.9024486541748, 'accumulated_logging_time': 0.04244589805603027}
I0307 16:13:18.497453 140498983450368 logging_writer.py:48] [1001] accumulated_eval_time=147.902, accumulated_logging_time=0.0424459, accumulated_submission_time=197.5, global_step=1001, preemption_count=0, score=197.5, test/loss=0.300942, test/num_examples=3581, test/ssim=0.725961, total_duration=345.644, train/loss=0.281888, train/ssim=0.726741, validation/loss=0.298913, validation/num_examples=3554, validation/ssim=0.708971
I0307 16:13:26.747891 140498975057664 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.09039425104856491, loss=0.27025964856147766
I0307 16:13:34.990727 140498983450368 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.12780873477458954, loss=0.2809388041496277
I0307 16:13:43.215999 140498975057664 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.09436856955289841, loss=0.2200160026550293
I0307 16:13:51.455666 140498983450368 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.11514080315828323, loss=0.33891576528549194
I0307 16:13:59.701195 140498975057664 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.3133295178413391, loss=0.2654946744441986
I0307 16:14:07.933372 140498983450368 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.1403685361146927, loss=0.23723983764648438
I0307 16:14:16.185386 140498975057664 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.06910647451877594, loss=0.33501771092414856
I0307 16:14:24.407839 140498983450368 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.19215421378612518, loss=0.34843289852142334
I0307 16:14:32.650226 140498975057664 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.1249975711107254, loss=0.31576037406921387
I0307 16:14:38.505572 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:14:39.811155 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:14:41.120743 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:14:42.438519 140656718828736 submission_runner.py:469] Time since start: 429.62s, 	Step: 1972, 	{'train/ssim': 0.7345756803240094, 'train/loss': 0.27490200315202984, 'validation/ssim': 0.7165438727709975, 'validation/loss': 0.29196436618510835, 'validation/num_examples': 3554, 'test/ssim': 0.7336703942029112, 'test/loss': 0.2936552208051173, 'test/num_examples': 3581, 'score': 277.45259618759155, 'total_duration': 429.6207549571991, 'accumulated_submission_time': 277.45259618759155, 'accumulated_eval_time': 151.83535885810852, 'accumulated_logging_time': 0.08849930763244629}
I0307 16:14:42.447159 140498983450368 logging_writer.py:48] [1972] accumulated_eval_time=151.835, accumulated_logging_time=0.0884993, accumulated_submission_time=277.453, global_step=1972, preemption_count=0, score=277.453, test/loss=0.293655, test/num_examples=3581, test/ssim=0.73367, total_duration=429.621, train/loss=0.274902, train/ssim=0.734576, validation/loss=0.291964, validation/num_examples=3554, validation/ssim=0.716544
I0307 16:14:44.860282 140498975057664 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.22432394325733185, loss=0.2483818531036377
I0307 16:14:53.100922 140498983450368 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.03335876017808914, loss=0.3190493881702423
I0307 16:15:01.317603 140498975057664 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.12124064564704895, loss=0.29600855708122253
I0307 16:15:09.538898 140498983450368 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.10442353785037994, loss=0.32073211669921875
I0307 16:15:17.794867 140498975057664 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7489483952522278, loss=0.20782722532749176
I0307 16:15:26.029189 140498983450368 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.029676171019673347, loss=0.3127349615097046
I0307 16:15:34.270670 140498975057664 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.08513226360082626, loss=0.23973160982131958
I0307 16:15:42.505378 140498983450368 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.32442915439605713, loss=0.2323891669511795
I0307 16:15:50.745223 140498975057664 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.1618749499320984, loss=0.2605859339237213
I0307 16:15:58.979417 140498983450368 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.18940739333629608, loss=0.21887347102165222
I0307 16:16:02.439299 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:16:03.743510 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:16:05.051741 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:16:06.368450 140656718828736 submission_runner.py:469] Time since start: 513.55s, 	Step: 2943, 	{'train/ssim': 0.7375583648681641, 'train/loss': 0.2732396977288382, 'validation/ssim': 0.7189204313141883, 'validation/loss': 0.2905422161319112, 'validation/num_examples': 3554, 'test/ssim': 0.7360904611578469, 'test/loss': 0.2920998384485828, 'test/num_examples': 3581, 'score': 357.3906636238098, 'total_duration': 513.550684928894, 'accumulated_submission_time': 357.3906636238098, 'accumulated_eval_time': 155.76447868347168, 'accumulated_logging_time': 0.10526657104492188}
I0307 16:16:06.378454 140498975057664 logging_writer.py:48] [2943] accumulated_eval_time=155.764, accumulated_logging_time=0.105267, accumulated_submission_time=357.391, global_step=2943, preemption_count=0, score=357.391, test/loss=0.2921, test/num_examples=3581, test/ssim=0.73609, total_duration=513.551, train/loss=0.27324, train/ssim=0.737558, validation/loss=0.290542, validation/num_examples=3554, validation/ssim=0.71892
I0307 16:16:11.159606 140498983450368 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.18925821781158447, loss=0.2910192906856537
I0307 16:16:19.411031 140498975057664 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.05177424103021622, loss=0.2765122950077057
I0307 16:16:27.652147 140498983450368 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.12412775307893753, loss=0.24787844717502594
I0307 16:16:35.881559 140498975057664 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.17164471745491028, loss=0.183949276804924
I0307 16:16:44.105370 140498983450368 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.05849689245223999, loss=0.4038069248199463
I0307 16:16:52.338418 140498975057664 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.14716483652591705, loss=0.24890300631523132
I0307 16:17:00.586778 140498983450368 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.15017126500606537, loss=0.28900259733200073
I0307 16:17:08.850594 140498975057664 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.19641061127185822, loss=0.32672053575515747
I0307 16:17:17.088730 140498983450368 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.1088458001613617, loss=0.20729486644268036
I0307 16:17:25.326023 140498975057664 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.2890823781490326, loss=0.2305961549282074
I0307 16:17:26.403118 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:17:27.712599 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:17:29.023285 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:17:30.334374 140656718828736 submission_runner.py:469] Time since start: 597.52s, 	Step: 3914, 	{'train/ssim': 0.7399422781808036, 'train/loss': 0.27173398222242084, 'validation/ssim': 0.7213518768465109, 'validation/loss': 0.2891579855004924, 'validation/num_examples': 3554, 'test/ssim': 0.7383873328853672, 'test/loss': 0.29076037157916784, 'test/num_examples': 3581, 'score': 437.3619804382324, 'total_duration': 597.5166091918945, 'accumulated_submission_time': 437.3619804382324, 'accumulated_eval_time': 159.6956934928894, 'accumulated_logging_time': 0.12380409240722656}
I0307 16:17:30.343927 140498983450368 logging_writer.py:48] [3914] accumulated_eval_time=159.696, accumulated_logging_time=0.123804, accumulated_submission_time=437.362, global_step=3914, preemption_count=0, score=437.362, test/loss=0.29076, test/num_examples=3581, test/ssim=0.738387, total_duration=597.517, train/loss=0.271734, train/ssim=0.739942, validation/loss=0.289158, validation/num_examples=3554, validation/ssim=0.721352
I0307 16:17:37.509319 140498975057664 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.23994268476963043, loss=0.2601209580898285
I0307 16:17:45.761701 140498983450368 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.14422348141670227, loss=0.3014644980430603
I0307 16:17:53.998037 140498975057664 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.16773603856563568, loss=0.2945536673069
I0307 16:18:02.229156 140498983450368 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.1621517837047577, loss=0.23358622193336487
I0307 16:18:10.466272 140498975057664 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.13451005518436432, loss=0.28516989946365356
I0307 16:18:18.719824 140498983450368 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.08167920261621475, loss=0.2564169764518738
I0307 16:18:26.957600 140498975057664 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.06474551558494568, loss=0.24260127544403076
I0307 16:18:35.187384 140498983450368 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0794672816991806, loss=0.21590647101402283
I0307 16:18:43.456424 140498975057664 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.08790727704763412, loss=0.37542033195495605
I0307 16:18:50.367817 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:18:51.676726 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:18:52.989140 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:18:54.302615 140656718828736 submission_runner.py:469] Time since start: 681.48s, 	Step: 4885, 	{'train/ssim': 0.7407423428126744, 'train/loss': 0.2703198024204799, 'validation/ssim': 0.7218363798932541, 'validation/loss': 0.28791217449748524, 'validation/num_examples': 3554, 'test/ssim': 0.7391321629127687, 'test/loss': 0.2893578412978218, 'test/num_examples': 3581, 'score': 517.3318326473236, 'total_duration': 681.4848523139954, 'accumulated_submission_time': 517.3318326473236, 'accumulated_eval_time': 163.6304521560669, 'accumulated_logging_time': 0.141524076461792}
I0307 16:18:54.311941 140498983450368 logging_writer.py:48] [4885] accumulated_eval_time=163.63, accumulated_logging_time=0.141524, accumulated_submission_time=517.332, global_step=4885, preemption_count=0, score=517.332, test/loss=0.289358, test/num_examples=3581, test/ssim=0.739132, total_duration=681.485, train/loss=0.27032, train/ssim=0.740742, validation/loss=0.287912, validation/num_examples=3554, validation/ssim=0.721836
I0307 16:18:55.652975 140498975057664 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0987858846783638, loss=0.24389541149139404
I0307 16:19:03.894550 140498983450368 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.15568692982196808, loss=0.2666237950325012
I0307 16:19:12.119655 140498975057664 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.15687133371829987, loss=0.30013731122016907
I0307 16:19:20.348733 140498983450368 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.09245316684246063, loss=0.18285265564918518
I0307 16:19:28.603083 140498975057664 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.13185034692287445, loss=0.36078107357025146
I0307 16:19:36.835217 140498983450368 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.23305842280387878, loss=0.2525123953819275
I0307 16:19:45.056724 140498975057664 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.21980921924114227, loss=0.30407628417015076
I0307 16:19:53.299160 140498983450368 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.03516266867518425, loss=0.26986145973205566
I0307 16:20:01.543289 140498975057664 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.06342693418264389, loss=0.35428884625434875
I0307 16:20:09.784620 140498983450368 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.07597994059324265, loss=0.36526596546173096
I0307 16:20:14.303975 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:20:15.607726 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:20:16.919757 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:20:18.234556 140656718828736 submission_runner.py:469] Time since start: 765.42s, 	Step: 5856, 	{'train/ssim': 0.742095811026437, 'train/loss': 0.26965202604021343, 'validation/ssim': 0.7232473670731921, 'validation/loss': 0.28719148222623275, 'validation/num_examples': 3554, 'test/ssim': 0.7404645394093828, 'test/loss': 0.28862166970643677, 'test/num_examples': 3581, 'score': 597.2696733474731, 'total_duration': 765.41677069664, 'accumulated_submission_time': 597.2696733474731, 'accumulated_eval_time': 167.56097149848938, 'accumulated_logging_time': 0.15858817100524902}
I0307 16:20:18.246397 140498975057664 logging_writer.py:48] [5856] accumulated_eval_time=167.561, accumulated_logging_time=0.158588, accumulated_submission_time=597.27, global_step=5856, preemption_count=0, score=597.27, test/loss=0.288622, test/num_examples=3581, test/ssim=0.740465, total_duration=765.417, train/loss=0.269652, train/ssim=0.742096, validation/loss=0.287191, validation/num_examples=3554, validation/ssim=0.723247
I0307 16:20:21.983638 140498983450368 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.09991810470819473, loss=0.32640647888183594
I0307 16:20:30.209881 140498975057664 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.06596022099256516, loss=0.22986310720443726
I0307 16:20:38.449669 140498983450368 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.06721092760562897, loss=0.3143029808998108
I0307 16:20:46.687498 140498975057664 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.09867799282073975, loss=0.3551436960697174
I0307 16:20:54.909665 140498983450368 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.13744597136974335, loss=0.24920105934143066
I0307 16:21:03.144652 140498975057664 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0675157979130745, loss=0.29371005296707153
I0307 16:21:11.391067 140498983450368 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.10984130203723907, loss=0.20298327505588531
I0307 16:21:19.615644 140498975057664 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.06002551317214966, loss=0.25765615701675415
I0307 16:21:27.831115 140498983450368 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.06343527883291245, loss=0.2486502081155777
I0307 16:21:36.073726 140498975057664 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.14639697968959808, loss=0.2635234594345093
I0307 16:21:38.298763 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:21:39.604470 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:21:40.915134 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:21:42.227232 140656718828736 submission_runner.py:469] Time since start: 849.41s, 	Step: 6828, 	{'train/ssim': 0.7422446523393903, 'train/loss': 0.2690481628690447, 'validation/ssim': 0.7228011956158554, 'validation/loss': 0.28700375704531866, 'validation/num_examples': 3554, 'test/ssim': 0.7400646832894093, 'test/loss': 0.2883737111883901, 'test/num_examples': 3581, 'score': 677.2659695148468, 'total_duration': 849.4094648361206, 'accumulated_submission_time': 677.2659695148468, 'accumulated_eval_time': 171.48939514160156, 'accumulated_logging_time': 0.1792898178100586}
I0307 16:21:42.237291 140498983450368 logging_writer.py:48] [6828] accumulated_eval_time=171.489, accumulated_logging_time=0.17929, accumulated_submission_time=677.266, global_step=6828, preemption_count=0, score=677.266, test/loss=0.288374, test/num_examples=3581, test/ssim=0.740065, total_duration=849.409, train/loss=0.269048, train/ssim=0.742245, validation/loss=0.287004, validation/num_examples=3554, validation/ssim=0.722801
I0307 16:21:48.259879 140498975057664 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.2050963193178177, loss=0.2986001670360565
I0307 16:21:56.498881 140498983450368 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.08914290368556976, loss=0.2831588685512543
I0307 16:22:04.740608 140498975057664 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.09726346284151077, loss=0.30344104766845703
I0307 16:22:12.977917 140498983450368 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.13427656888961792, loss=0.27631619572639465
I0307 16:22:21.201438 140498975057664 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.15236639976501465, loss=0.2834717035293579
I0307 16:22:29.435234 140498983450368 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.06959218531847, loss=0.21265964210033417
I0307 16:22:37.689853 140498975057664 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.10889061540365219, loss=0.2704261243343353
I0307 16:22:45.916560 140498983450368 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.06000185385346413, loss=0.22898928821086884
I0307 16:22:54.140622 140498975057664 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.10707969963550568, loss=0.2739337682723999
I0307 16:23:02.278325 140656718828736 spec.py:321] Evaluating on the training split.
I0307 16:23:03.583562 140656718828736 spec.py:333] Evaluating on the validation split.
I0307 16:23:04.893190 140656718828736 spec.py:349] Evaluating on the test split.
I0307 16:23:06.206389 140656718828736 submission_runner.py:469] Time since start: 933.39s, 	Step: 7800, 	{'train/ssim': 0.7431694439479283, 'train/loss': 0.2685808965138027, 'validation/ssim': 0.7237640191949212, 'validation/loss': 0.28664482773591377, 'validation/num_examples': 3554, 'test/ssim': 0.7410341554166084, 'test/loss': 0.2880088978724518, 'test/num_examples': 3581, 'score': 757.2527964115143, 'total_duration': 933.3886137008667, 'accumulated_submission_time': 757.2527964115143, 'accumulated_eval_time': 175.4174075126648, 'accumulated_logging_time': 0.19724225997924805}
I0307 16:23:06.215881 140498983450368 logging_writer.py:48] [7800] accumulated_eval_time=175.417, accumulated_logging_time=0.197242, accumulated_submission_time=757.253, global_step=7800, preemption_count=0, score=757.253, test/loss=0.288009, test/num_examples=3581, test/ssim=0.741034, total_duration=933.389, train/loss=0.268581, train/ssim=0.743169, validation/loss=0.286645, validation/num_examples=3554, validation/ssim=0.723764
I0307 16:23:06.228345 140498975057664 logging_writer.py:48] [7800] global_step=7800, preemption_count=0, score=757.253
I0307 16:23:07.023351 140656718828736 submission_runner.py:646] Tuning trial 3/5
I0307 16:23:07.023530 140656718828736 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 16:23:07.024201 140656718828736 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.24988419669015066, 'train/loss': 0.9285712242126465, 'validation/ssim': 0.24356833024848762, 'validation/loss': 0.936524055751442, 'validation/num_examples': 3554, 'test/ssim': 0.26563994773304417, 'test/loss': 0.9333185208173346, 'test/num_examples': 3581, 'score': 37.46510195732117, 'total_duration': 176.9906849861145, 'accumulated_submission_time': 37.46510195732117, 'accumulated_eval_time': 139.5254521369934, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (509, {'train/ssim': 0.7103781700134277, 'train/loss': 0.29674080439976286, 'validation/ssim': 0.6924728024321187, 'validation/loss': 0.3143727523125352, 'validation/num_examples': 3554, 'test/ssim': 0.7102639827605767, 'test/loss': 0.31639643256073724, 'test/num_examples': 3581, 'score': 117.57442593574524, 'total_duration': 261.68143463134766, 'accumulated_submission_time': 117.57442593574524, 'accumulated_eval_time': 143.98797464370728, 'accumulated_logging_time': 0.01915574073791504, 'global_step': 509, 'preemption_count': 0}), (1001, {'train/ssim': 0.7267413820539202, 'train/loss': 0.28188778672899517, 'validation/ssim': 0.7089707734682752, 'validation/loss': 0.29891268805834975, 'validation/num_examples': 3554, 'test/ssim': 0.7259610454918319, 'test/loss': 0.300941703772951, 'test/num_examples': 3581, 'score': 197.500150680542, 'total_duration': 345.64355516433716, 'accumulated_submission_time': 197.500150680542, 'accumulated_eval_time': 147.9024486541748, 'accumulated_logging_time': 0.04244589805603027, 'global_step': 1001, 'preemption_count': 0}), (1972, {'train/ssim': 0.7345756803240094, 'train/loss': 0.27490200315202984, 'validation/ssim': 0.7165438727709975, 'validation/loss': 0.29196436618510835, 'validation/num_examples': 3554, 'test/ssim': 0.7336703942029112, 'test/loss': 0.2936552208051173, 'test/num_examples': 3581, 'score': 277.45259618759155, 'total_duration': 429.6207549571991, 'accumulated_submission_time': 277.45259618759155, 'accumulated_eval_time': 151.83535885810852, 'accumulated_logging_time': 0.08849930763244629, 'global_step': 1972, 'preemption_count': 0}), (2943, {'train/ssim': 0.7375583648681641, 'train/loss': 0.2732396977288382, 'validation/ssim': 0.7189204313141883, 'validation/loss': 0.2905422161319112, 'validation/num_examples': 3554, 'test/ssim': 0.7360904611578469, 'test/loss': 0.2920998384485828, 'test/num_examples': 3581, 'score': 357.3906636238098, 'total_duration': 513.550684928894, 'accumulated_submission_time': 357.3906636238098, 'accumulated_eval_time': 155.76447868347168, 'accumulated_logging_time': 0.10526657104492188, 'global_step': 2943, 'preemption_count': 0}), (3914, {'train/ssim': 0.7399422781808036, 'train/loss': 0.27173398222242084, 'validation/ssim': 0.7213518768465109, 'validation/loss': 0.2891579855004924, 'validation/num_examples': 3554, 'test/ssim': 0.7383873328853672, 'test/loss': 0.29076037157916784, 'test/num_examples': 3581, 'score': 437.3619804382324, 'total_duration': 597.5166091918945, 'accumulated_submission_time': 437.3619804382324, 'accumulated_eval_time': 159.6956934928894, 'accumulated_logging_time': 0.12380409240722656, 'global_step': 3914, 'preemption_count': 0}), (4885, {'train/ssim': 0.7407423428126744, 'train/loss': 0.2703198024204799, 'validation/ssim': 0.7218363798932541, 'validation/loss': 0.28791217449748524, 'validation/num_examples': 3554, 'test/ssim': 0.7391321629127687, 'test/loss': 0.2893578412978218, 'test/num_examples': 3581, 'score': 517.3318326473236, 'total_duration': 681.4848523139954, 'accumulated_submission_time': 517.3318326473236, 'accumulated_eval_time': 163.6304521560669, 'accumulated_logging_time': 0.141524076461792, 'global_step': 4885, 'preemption_count': 0}), (5856, {'train/ssim': 0.742095811026437, 'train/loss': 0.26965202604021343, 'validation/ssim': 0.7232473670731921, 'validation/loss': 0.28719148222623275, 'validation/num_examples': 3554, 'test/ssim': 0.7404645394093828, 'test/loss': 0.28862166970643677, 'test/num_examples': 3581, 'score': 597.2696733474731, 'total_duration': 765.41677069664, 'accumulated_submission_time': 597.2696733474731, 'accumulated_eval_time': 167.56097149848938, 'accumulated_logging_time': 0.15858817100524902, 'global_step': 5856, 'preemption_count': 0}), (6828, {'train/ssim': 0.7422446523393903, 'train/loss': 0.2690481628690447, 'validation/ssim': 0.7228011956158554, 'validation/loss': 0.28700375704531866, 'validation/num_examples': 3554, 'test/ssim': 0.7400646832894093, 'test/loss': 0.2883737111883901, 'test/num_examples': 3581, 'score': 677.2659695148468, 'total_duration': 849.4094648361206, 'accumulated_submission_time': 677.2659695148468, 'accumulated_eval_time': 171.48939514160156, 'accumulated_logging_time': 0.1792898178100586, 'global_step': 6828, 'preemption_count': 0}), (7800, {'train/ssim': 0.7431694439479283, 'train/loss': 0.2685808965138027, 'validation/ssim': 0.7237640191949212, 'validation/loss': 0.28664482773591377, 'validation/num_examples': 3554, 'test/ssim': 0.7410341554166084, 'test/loss': 0.2880088978724518, 'test/num_examples': 3581, 'score': 757.2527964115143, 'total_duration': 933.3886137008667, 'accumulated_submission_time': 757.2527964115143, 'accumulated_eval_time': 175.4174075126648, 'accumulated_logging_time': 0.19724225997924805, 'global_step': 7800, 'preemption_count': 0})], 'global_step': 7800}
I0307 16:23:07.024286 140656718828736 submission_runner.py:649] Timing: 757.2527964115143
I0307 16:23:07.024321 140656718828736 submission_runner.py:651] Total number of evals: 10
I0307 16:23:07.024353 140656718828736 submission_runner.py:652] ====================
I0307 16:23:07.024452 140656718828736 submission_runner.py:750] Final fastmri score: 2
