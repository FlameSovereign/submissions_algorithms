python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-971483621 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-16-06-28.log
2025-03-07 16:06:32.754354: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741363592.777307       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741363592.810087       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:07:02.654813 140000997733568 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax.
I0307 16:07:04.390232 140000997733568 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:07:04.393355 140000997733568 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:07:04.419809 140000997733568 submission_runner.py:606] Using RNG seed -971483621
I0307 16:07:05.246738 140000997733568 submission_runner.py:615] --- Tuning run 2/5 ---
I0307 16:07:05.246971 140000997733568 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_2.
I0307 16:07:05.247199 140000997733568 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_2/hparams.json.
I0307 16:07:05.488404 140000997733568 submission_runner.py:218] Initializing dataset.
I0307 16:07:10.566506 140000997733568 submission_runner.py:229] Initializing model.
I0307 16:07:21.695594 140000997733568 submission_runner.py:272] Initializing optimizer.
I0307 16:07:22.172778 140000997733568 submission_runner.py:279] Initializing metrics bundle.
I0307 16:07:22.172970 140000997733568 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:07:22.173761 140000997733568 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_2 with prefix checkpoint_
I0307 16:07:22.173861 140000997733568 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_2/meta_data_0.json.
I0307 16:07:22.174046 140000997733568 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:07:22.174093 140000997733568 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:07:22.374704 140000997733568 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_2/flags_0.json.
I0307 16:07:22.529080 140000997733568 submission_runner.py:337] Starting training loop.
E0307 16:07:51.613239       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:51.822743       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:52.223059       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:52.431622       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:54.301581       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:07:54.510336       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 16:08:05.717368 139864024524544 logging_writer.py:48] [0] global_step=0, grad_norm=4.476779460906982, loss=0.946336030960083
I0307 16:08:05.765685 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:09:58.291294 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:10:20.798393 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:10:41.725695 140000997733568 submission_runner.py:469] Time since start: 199.20s, 	Step: 1, 	{'train/ssim': 0.24835775579724992, 'train/loss': 0.9409921509878976, 'validation/ssim': 0.23820290380337297, 'validation/loss': 0.9470240260753728, 'validation/num_examples': 3554, 'test/ssim': 0.2611691779312692, 'test/loss': 0.9445872366744624, 'test/num_examples': 3581, 'score': 43.236422538757324, 'total_duration': 199.19654607772827, 'accumulated_submission_time': 43.236422538757324, 'accumulated_eval_time': 155.95994591712952, 'accumulated_logging_time': 0}
I0307 16:10:41.733442 139840905537280 logging_writer.py:48] [1] accumulated_eval_time=155.96, accumulated_logging_time=0, accumulated_submission_time=43.2364, global_step=1, preemption_count=0, score=43.2364, test/loss=0.944587, test/num_examples=3581, test/ssim=0.261169, total_duration=199.197, train/loss=0.940992, train/ssim=0.248358, validation/loss=0.947024, validation/num_examples=3554, validation/ssim=0.238203
I0307 16:10:54.083567 139840897144576 logging_writer.py:48] [100] global_step=100, grad_norm=0.27585941553115845, loss=0.4053471088409424
I0307 16:11:11.430151 139840905537280 logging_writer.py:48] [200] global_step=200, grad_norm=0.13757427036762238, loss=0.2789989709854126
I0307 16:11:27.269190 139840897144576 logging_writer.py:48] [300] global_step=300, grad_norm=0.14492455124855042, loss=0.3813788592815399
I0307 16:11:44.555251 139840905537280 logging_writer.py:48] [400] global_step=400, grad_norm=0.37785494327545166, loss=0.36446475982666016
I0307 16:12:01.618626 139840897144576 logging_writer.py:48] [500] global_step=500, grad_norm=0.21839167177677155, loss=0.31420236825942993
I0307 16:12:01.801259 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:12:03.606224 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:12:04.894422 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:12:06.186720 140000997733568 submission_runner.py:469] Time since start: 283.66s, 	Step: 502, 	{'train/ssim': 0.7162906101771763, 'train/loss': 0.2942422458103725, 'validation/ssim': 0.6970328876793753, 'validation/loss': 0.3100463317630663, 'validation/num_examples': 3554, 'test/ssim': 0.7143907160840198, 'test/loss': 0.31236385120689053, 'test/num_examples': 3581, 'score': 123.18770098686218, 'total_duration': 283.6575894355774, 'accumulated_submission_time': 123.18770098686218, 'accumulated_eval_time': 160.3453676700592, 'accumulated_logging_time': 0.01663517951965332}
I0307 16:12:06.197085 139840905537280 logging_writer.py:48] [502] accumulated_eval_time=160.345, accumulated_logging_time=0.0166352, accumulated_submission_time=123.188, global_step=502, preemption_count=0, score=123.188, test/loss=0.312364, test/num_examples=3581, test/ssim=0.714391, total_duration=283.658, train/loss=0.294242, train/ssim=0.716291, validation/loss=0.310046, validation/num_examples=3554, validation/ssim=0.697033
I0307 16:12:20.956841 139840897144576 logging_writer.py:48] [600] global_step=600, grad_norm=0.24765445291996002, loss=0.3222846984863281
I0307 16:12:38.226516 139840905537280 logging_writer.py:48] [700] global_step=700, grad_norm=0.40856724977493286, loss=0.27655357122421265
I0307 16:12:55.419891 139840897144576 logging_writer.py:48] [800] global_step=800, grad_norm=0.4632468819618225, loss=0.20065456628799438
I0307 16:13:13.330315 139840905537280 logging_writer.py:48] [900] global_step=900, grad_norm=0.37760770320892334, loss=0.2826118767261505
I0307 16:13:26.269214 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:13:27.557405 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:13:28.848217 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:13:30.149346 140000997733568 submission_runner.py:469] Time since start: 367.62s, 	Step: 987, 	{'train/ssim': 0.728595529283796, 'train/loss': 0.28365727833339144, 'validation/ssim': 0.7092496735632386, 'validation/loss': 0.2993952676561621, 'validation/num_examples': 3554, 'test/ssim': 0.7260969897549567, 'test/loss': 0.30160025623516473, 'test/num_examples': 3581, 'score': 203.15602111816406, 'total_duration': 367.6201820373535, 'accumulated_submission_time': 203.15602111816406, 'accumulated_eval_time': 164.22542524337769, 'accumulated_logging_time': 0.044245004653930664}
I0307 16:13:30.160044 139840897144576 logging_writer.py:48] [987] accumulated_eval_time=164.225, accumulated_logging_time=0.044245, accumulated_submission_time=203.156, global_step=987, preemption_count=0, score=203.156, test/loss=0.3016, test/num_examples=3581, test/ssim=0.726097, total_duration=367.62, train/loss=0.283657, train/ssim=0.728596, validation/loss=0.299395, validation/num_examples=3554, validation/ssim=0.70925
I0307 16:13:31.325776 139840905537280 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.08860460668802261, loss=0.2605999708175659
I0307 16:13:39.581133 139840897144576 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.07487501204013824, loss=0.2517619729042053
I0307 16:13:47.825798 139840905537280 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.1736392080783844, loss=0.27691134810447693
I0307 16:13:56.072233 139840897144576 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.2619822919368744, loss=0.1873851865530014
I0307 16:14:04.325384 139840905537280 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.23423372209072113, loss=0.3096885085105896
I0307 16:14:12.592038 139840897144576 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2864411771297455, loss=0.35039204359054565
I0307 16:14:20.820544 139840905537280 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.2348056584596634, loss=0.2576214075088501
I0307 16:14:29.057763 139840897144576 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.15997813642024994, loss=0.310183048248291
I0307 16:14:37.301085 139840905537280 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.08151841163635254, loss=0.29321303963661194
I0307 16:14:45.581194 139840897144576 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.16447654366493225, loss=0.2604691982269287
I0307 16:14:50.202225 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:14:51.493174 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:14:52.787240 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:14:54.092972 140000997733568 submission_runner.py:469] Time since start: 451.56s, 	Step: 1957, 	{'train/ssim': 0.7321905408586774, 'train/loss': 0.2792520523071289, 'validation/ssim': 0.7138773543014912, 'validation/loss': 0.2946233282481007, 'validation/num_examples': 3554, 'test/ssim': 0.7307899984292097, 'test/loss': 0.2963700836445825, 'test/num_examples': 3581, 'score': 283.13985300064087, 'total_duration': 451.56383752822876, 'accumulated_submission_time': 283.13985300064087, 'accumulated_eval_time': 168.1161229610443, 'accumulated_logging_time': 0.06322145462036133}
I0307 16:14:54.101619 139840905537280 logging_writer.py:48] [1957] accumulated_eval_time=168.116, accumulated_logging_time=0.0632215, accumulated_submission_time=283.14, global_step=1957, preemption_count=0, score=283.14, test/loss=0.29637, test/num_examples=3581, test/ssim=0.73079, total_duration=451.564, train/loss=0.279252, train/ssim=0.732191, validation/loss=0.294623, validation/num_examples=3554, validation/ssim=0.713877
I0307 16:14:57.743402 139840897144576 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.10949628055095673, loss=0.2745361328125
I0307 16:15:06.022604 139840905537280 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.10595402121543884, loss=0.3223695158958435
I0307 16:15:14.256386 139840897144576 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.13755187392234802, loss=0.3043912351131439
I0307 16:15:22.517913 139840905537280 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.12031980603933334, loss=0.28258317708969116
I0307 16:15:30.755636 139840897144576 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.36295637488365173, loss=0.283955842256546
I0307 16:15:39.016557 139840905537280 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.12945178151130676, loss=0.2629263401031494
I0307 16:15:47.270748 139840897144576 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.17736457288265228, loss=0.23776547610759735
I0307 16:15:55.519753 139840905537280 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.06793610006570816, loss=0.2948164641857147
I0307 16:16:03.789064 139840897144576 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.24656206369400024, loss=0.2414630502462387
I0307 16:16:12.047370 139840905537280 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.14834125339984894, loss=0.19328658282756805
I0307 16:16:14.114312 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:16:15.404846 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:16:16.698866 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:16:17.998933 140000997733568 submission_runner.py:469] Time since start: 535.47s, 	Step: 2926, 	{'train/ssim': 0.7375854764665876, 'train/loss': 0.2764324801308768, 'validation/ssim': 0.7177632020039041, 'validation/loss': 0.2922787813906865, 'validation/num_examples': 3554, 'test/ssim': 0.7350119745488342, 'test/loss': 0.2939001454617251, 'test/num_examples': 3581, 'score': 363.0950999259949, 'total_duration': 535.4698045253754, 'accumulated_submission_time': 363.0950999259949, 'accumulated_eval_time': 172.0007038116455, 'accumulated_logging_time': 0.0799708366394043}
I0307 16:16:18.007617 139840897144576 logging_writer.py:48] [2926] accumulated_eval_time=172.001, accumulated_logging_time=0.0799708, accumulated_submission_time=363.095, global_step=2926, preemption_count=0, score=363.095, test/loss=0.2939, test/num_examples=3581, test/ssim=0.735012, total_duration=535.47, train/loss=0.276432, train/ssim=0.737585, validation/loss=0.292279, validation/num_examples=3554, validation/ssim=0.717763
I0307 16:16:24.198619 139840905537280 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2216532826423645, loss=0.3515445590019226
I0307 16:16:32.418355 139840897144576 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.20847120881080627, loss=0.353783518075943
I0307 16:16:40.649264 139840905537280 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.21328409016132355, loss=0.3467746675014496
I0307 16:16:48.897233 139840897144576 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3446425199508667, loss=0.27646899223327637
I0307 16:16:57.169556 139840905537280 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.08248571306467056, loss=0.2604896128177643
I0307 16:17:05.418461 139840897144576 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.17436359822750092, loss=0.22846587002277374
I0307 16:17:13.658966 139840905537280 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.14005732536315918, loss=0.3049106299877167
I0307 16:17:21.919627 139840897144576 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.13080888986587524, loss=0.2255849540233612
I0307 16:17:30.156944 139840905537280 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.12718692421913147, loss=0.28216466307640076
I0307 16:17:38.000544 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:17:39.292923 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:17:40.591798 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:17:41.888665 140000997733568 submission_runner.py:469] Time since start: 619.36s, 	Step: 3896, 	{'train/ssim': 0.7402594430106026, 'train/loss': 0.2741525854383196, 'validation/ssim': 0.7203755204303249, 'validation/loss': 0.2901603771718486, 'validation/num_examples': 3554, 'test/ssim': 0.7375349882627059, 'test/loss': 0.29180719012889206, 'test/num_examples': 3581, 'score': 443.03084778785706, 'total_duration': 619.3595366477966, 'accumulated_submission_time': 443.03084778785706, 'accumulated_eval_time': 175.88878297805786, 'accumulated_logging_time': 0.09674382209777832}
I0307 16:17:41.897646 139840897144576 logging_writer.py:48] [3896] accumulated_eval_time=175.889, accumulated_logging_time=0.0967438, accumulated_submission_time=443.031, global_step=3896, preemption_count=0, score=443.031, test/loss=0.291807, test/num_examples=3581, test/ssim=0.737535, total_duration=619.36, train/loss=0.274153, train/ssim=0.740259, validation/loss=0.29016, validation/num_examples=3554, validation/ssim=0.720376
I0307 16:17:42.321485 139840905537280 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.11002100259065628, loss=0.23210249841213226
I0307 16:17:50.560787 139840897144576 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.1907733976840973, loss=0.3105970621109009
I0307 16:17:58.820033 139840905537280 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.1400570571422577, loss=0.2784540355205536
I0307 16:18:07.067014 139840897144576 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.11655537784099579, loss=0.31285667419433594
I0307 16:18:15.329288 139840905537280 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3602962791919708, loss=0.32060861587524414
I0307 16:18:23.569504 139840897144576 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.16718627512454987, loss=0.21365199983119965
I0307 16:18:31.822012 139840905537280 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.16739264130592346, loss=0.2272641956806183
I0307 16:18:40.074784 139840897144576 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.06691677123308182, loss=0.28039640188217163
I0307 16:18:48.334158 139840905537280 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.11390069127082825, loss=0.28947925567626953
I0307 16:18:56.573298 139840897144576 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.13951991498470306, loss=0.267660915851593
I0307 16:19:01.922314 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:19:03.215655 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:19:04.512403 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:19:05.811415 140000997733568 submission_runner.py:469] Time since start: 703.28s, 	Step: 4866, 	{'train/ssim': 0.7350491796221051, 'train/loss': 0.27774124486105783, 'validation/ssim': 0.7169271886650604, 'validation/loss': 0.2928503205015476, 'validation/num_examples': 3554, 'test/ssim': 0.7338051794627897, 'test/loss': 0.29451186055745604, 'test/num_examples': 3581, 'score': 522.9981009960175, 'total_duration': 703.2822904586792, 'accumulated_submission_time': 522.9981009960175, 'accumulated_eval_time': 179.7778444290161, 'accumulated_logging_time': 0.11357402801513672}
I0307 16:19:05.820484 139840905537280 logging_writer.py:48] [4866] accumulated_eval_time=179.778, accumulated_logging_time=0.113574, accumulated_submission_time=522.998, global_step=4866, preemption_count=0, score=522.998, test/loss=0.294512, test/num_examples=3581, test/ssim=0.733805, total_duration=703.282, train/loss=0.277741, train/ssim=0.735049, validation/loss=0.29285, validation/num_examples=3554, validation/ssim=0.716927
I0307 16:19:08.722758 139840897144576 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.041686031967401505, loss=0.30238816142082214
I0307 16:19:16.962085 139840905537280 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.06892126053571701, loss=0.2953629791736603
I0307 16:19:25.206308 139840897144576 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.2599882483482361, loss=0.26653599739074707
I0307 16:19:33.460076 139840905537280 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.23855797946453094, loss=0.22075501084327698
I0307 16:19:41.717400 139840897144576 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.08570680022239685, loss=0.23828715085983276
I0307 16:19:49.963736 139840905537280 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.11165870726108551, loss=0.21532121300697327
I0307 16:19:58.200913 139840897144576 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.1524927318096161, loss=0.22379806637763977
I0307 16:20:06.462762 139840905537280 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.08085986226797104, loss=0.28970035910606384
I0307 16:20:14.703452 139840897144576 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.18953637778759003, loss=0.2754156291484833
I0307 16:20:22.950509 139840905537280 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.050835903733968735, loss=0.3148515820503235
I0307 16:20:25.847392 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:20:27.141826 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:20:28.439079 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:20:29.737396 140000997733568 submission_runner.py:469] Time since start: 787.21s, 	Step: 5836, 	{'train/ssim': 0.742548942565918, 'train/loss': 0.2722194194793701, 'validation/ssim': 0.722578212929094, 'validation/loss': 0.288302325504713, 'validation/num_examples': 3554, 'test/ssim': 0.7396988473235478, 'test/loss': 0.2897705828068277, 'test/num_examples': 3581, 'score': 602.9667115211487, 'total_duration': 787.2082369327545, 'accumulated_submission_time': 602.9667115211487, 'accumulated_eval_time': 183.6677713394165, 'accumulated_logging_time': 0.13066792488098145}
I0307 16:20:29.748669 139840897144576 logging_writer.py:48] [5836] accumulated_eval_time=183.668, accumulated_logging_time=0.130668, accumulated_submission_time=602.967, global_step=5836, preemption_count=0, score=602.967, test/loss=0.289771, test/num_examples=3581, test/ssim=0.739699, total_duration=787.208, train/loss=0.272219, train/ssim=0.742549, validation/loss=0.288302, validation/num_examples=3554, validation/ssim=0.722578
I0307 16:20:35.135566 139840905537280 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.08985927700996399, loss=0.23506799340248108
I0307 16:20:43.396605 139840897144576 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.07322101294994354, loss=0.2833843529224396
I0307 16:20:51.638956 139840905537280 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.18948276340961456, loss=0.24080581963062286
I0307 16:20:59.876337 139840897144576 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.39912843704223633, loss=0.27501899003982544
I0307 16:21:08.143666 139840905537280 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.16184644401073456, loss=0.35494568943977356
I0307 16:21:16.395985 139840897144576 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.04530104622244835, loss=0.3069707751274109
I0307 16:21:24.622647 139840905537280 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.3131670355796814, loss=0.26258584856987
I0307 16:21:32.882883 139840897144576 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.06175569072365761, loss=0.28373289108276367
I0307 16:21:41.125133 139840905537280 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.1162925586104393, loss=0.32133087515830994
I0307 16:21:49.373071 139840897144576 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.21360215544700623, loss=0.2721661925315857
I0307 16:21:49.787210 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:21:51.077479 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:21:52.374367 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:21:53.676076 140000997733568 submission_runner.py:469] Time since start: 871.15s, 	Step: 6806, 	{'train/ssim': 0.7418174062456403, 'train/loss': 0.2721409116472517, 'validation/ssim': 0.7217322388725732, 'validation/loss': 0.288529876382685, 'validation/num_examples': 3554, 'test/ssim': 0.7389553126527157, 'test/loss': 0.28997981698111563, 'test/num_examples': 3581, 'score': 682.94770860672, 'total_duration': 871.1469399929047, 'accumulated_submission_time': 682.94770860672, 'accumulated_eval_time': 187.55658507347107, 'accumulated_logging_time': 0.1504805088043213}
I0307 16:21:53.685556 139840905537280 logging_writer.py:48] [6806] accumulated_eval_time=187.557, accumulated_logging_time=0.150481, accumulated_submission_time=682.948, global_step=6806, preemption_count=0, score=682.948, test/loss=0.28998, test/num_examples=3581, test/ssim=0.738955, total_duration=871.147, train/loss=0.272141, train/ssim=0.741817, validation/loss=0.28853, validation/num_examples=3554, validation/ssim=0.721732
I0307 16:22:01.531915 139840897144576 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.06903401762247086, loss=0.3457202613353729
I0307 16:22:09.784865 139840905537280 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.19094665348529816, loss=0.23600047826766968
I0307 16:22:18.032202 139840897144576 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.06939774006605148, loss=0.24416309595108032
I0307 16:22:26.267588 139840905537280 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.13791392743587494, loss=0.28370392322540283
I0307 16:22:34.517007 139840897144576 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.11837560683488846, loss=0.2090977281332016
I0307 16:22:42.760504 139840905537280 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.07476149499416351, loss=0.25562185049057007
I0307 16:22:50.972146 139840897144576 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.20418551564216614, loss=0.28363656997680664
I0307 16:22:59.207628 139840905537280 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5003054738044739, loss=0.2333083599805832
I0307 16:23:07.450523 139840897144576 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.057708874344825745, loss=0.2586841881275177
I0307 16:23:13.706353 140000997733568 spec.py:321] Evaluating on the training split.
I0307 16:23:14.999209 140000997733568 spec.py:333] Evaluating on the validation split.
I0307 16:23:16.296036 140000997733568 spec.py:349] Evaluating on the test split.
I0307 16:23:17.594291 140000997733568 submission_runner.py:469] Time since start: 955.07s, 	Step: 7777, 	{'train/ssim': 0.7442505019051688, 'train/loss': 0.2704741954803467, 'validation/ssim': 0.7236815856693163, 'validation/loss': 0.28719941645307223, 'validation/num_examples': 3554, 'test/ssim': 0.7408752356185423, 'test/loss': 0.2886526559991099, 'test/num_examples': 3581, 'score': 762.910413980484, 'total_duration': 955.065144777298, 'accumulated_submission_time': 762.910413980484, 'accumulated_eval_time': 191.44445872306824, 'accumulated_logging_time': 0.16780352592468262}
I0307 16:23:17.605585 139840905537280 logging_writer.py:48] [7777] accumulated_eval_time=191.444, accumulated_logging_time=0.167804, accumulated_submission_time=762.91, global_step=7777, preemption_count=0, score=762.91, test/loss=0.288653, test/num_examples=3581, test/ssim=0.740875, total_duration=955.065, train/loss=0.270474, train/ssim=0.744251, validation/loss=0.287199, validation/num_examples=3554, validation/ssim=0.723682
I0307 16:23:17.618406 139840897144576 logging_writer.py:48] [7777] global_step=7777, preemption_count=0, score=762.91
I0307 16:23:18.739300 140000997733568 submission_runner.py:646] Tuning trial 2/5
I0307 16:23:18.739495 140000997733568 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0307 16:23:18.740143 140000997733568 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.24835775579724992, 'train/loss': 0.9409921509878976, 'validation/ssim': 0.23820290380337297, 'validation/loss': 0.9470240260753728, 'validation/num_examples': 3554, 'test/ssim': 0.2611691779312692, 'test/loss': 0.9445872366744624, 'test/num_examples': 3581, 'score': 43.236422538757324, 'total_duration': 199.19654607772827, 'accumulated_submission_time': 43.236422538757324, 'accumulated_eval_time': 155.95994591712952, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (502, {'train/ssim': 0.7162906101771763, 'train/loss': 0.2942422458103725, 'validation/ssim': 0.6970328876793753, 'validation/loss': 0.3100463317630663, 'validation/num_examples': 3554, 'test/ssim': 0.7143907160840198, 'test/loss': 0.31236385120689053, 'test/num_examples': 3581, 'score': 123.18770098686218, 'total_duration': 283.6575894355774, 'accumulated_submission_time': 123.18770098686218, 'accumulated_eval_time': 160.3453676700592, 'accumulated_logging_time': 0.01663517951965332, 'global_step': 502, 'preemption_count': 0}), (987, {'train/ssim': 0.728595529283796, 'train/loss': 0.28365727833339144, 'validation/ssim': 0.7092496735632386, 'validation/loss': 0.2993952676561621, 'validation/num_examples': 3554, 'test/ssim': 0.7260969897549567, 'test/loss': 0.30160025623516473, 'test/num_examples': 3581, 'score': 203.15602111816406, 'total_duration': 367.6201820373535, 'accumulated_submission_time': 203.15602111816406, 'accumulated_eval_time': 164.22542524337769, 'accumulated_logging_time': 0.044245004653930664, 'global_step': 987, 'preemption_count': 0}), (1957, {'train/ssim': 0.7321905408586774, 'train/loss': 0.2792520523071289, 'validation/ssim': 0.7138773543014912, 'validation/loss': 0.2946233282481007, 'validation/num_examples': 3554, 'test/ssim': 0.7307899984292097, 'test/loss': 0.2963700836445825, 'test/num_examples': 3581, 'score': 283.13985300064087, 'total_duration': 451.56383752822876, 'accumulated_submission_time': 283.13985300064087, 'accumulated_eval_time': 168.1161229610443, 'accumulated_logging_time': 0.06322145462036133, 'global_step': 1957, 'preemption_count': 0}), (2926, {'train/ssim': 0.7375854764665876, 'train/loss': 0.2764324801308768, 'validation/ssim': 0.7177632020039041, 'validation/loss': 0.2922787813906865, 'validation/num_examples': 3554, 'test/ssim': 0.7350119745488342, 'test/loss': 0.2939001454617251, 'test/num_examples': 3581, 'score': 363.0950999259949, 'total_duration': 535.4698045253754, 'accumulated_submission_time': 363.0950999259949, 'accumulated_eval_time': 172.0007038116455, 'accumulated_logging_time': 0.0799708366394043, 'global_step': 2926, 'preemption_count': 0}), (3896, {'train/ssim': 0.7402594430106026, 'train/loss': 0.2741525854383196, 'validation/ssim': 0.7203755204303249, 'validation/loss': 0.2901603771718486, 'validation/num_examples': 3554, 'test/ssim': 0.7375349882627059, 'test/loss': 0.29180719012889206, 'test/num_examples': 3581, 'score': 443.03084778785706, 'total_duration': 619.3595366477966, 'accumulated_submission_time': 443.03084778785706, 'accumulated_eval_time': 175.88878297805786, 'accumulated_logging_time': 0.09674382209777832, 'global_step': 3896, 'preemption_count': 0}), (4866, {'train/ssim': 0.7350491796221051, 'train/loss': 0.27774124486105783, 'validation/ssim': 0.7169271886650604, 'validation/loss': 0.2928503205015476, 'validation/num_examples': 3554, 'test/ssim': 0.7338051794627897, 'test/loss': 0.29451186055745604, 'test/num_examples': 3581, 'score': 522.9981009960175, 'total_duration': 703.2822904586792, 'accumulated_submission_time': 522.9981009960175, 'accumulated_eval_time': 179.7778444290161, 'accumulated_logging_time': 0.11357402801513672, 'global_step': 4866, 'preemption_count': 0}), (5836, {'train/ssim': 0.742548942565918, 'train/loss': 0.2722194194793701, 'validation/ssim': 0.722578212929094, 'validation/loss': 0.288302325504713, 'validation/num_examples': 3554, 'test/ssim': 0.7396988473235478, 'test/loss': 0.2897705828068277, 'test/num_examples': 3581, 'score': 602.9667115211487, 'total_duration': 787.2082369327545, 'accumulated_submission_time': 602.9667115211487, 'accumulated_eval_time': 183.6677713394165, 'accumulated_logging_time': 0.13066792488098145, 'global_step': 5836, 'preemption_count': 0}), (6806, {'train/ssim': 0.7418174062456403, 'train/loss': 0.2721409116472517, 'validation/ssim': 0.7217322388725732, 'validation/loss': 0.288529876382685, 'validation/num_examples': 3554, 'test/ssim': 0.7389553126527157, 'test/loss': 0.28997981698111563, 'test/num_examples': 3581, 'score': 682.94770860672, 'total_duration': 871.1469399929047, 'accumulated_submission_time': 682.94770860672, 'accumulated_eval_time': 187.55658507347107, 'accumulated_logging_time': 0.1504805088043213, 'global_step': 6806, 'preemption_count': 0}), (7777, {'train/ssim': 0.7442505019051688, 'train/loss': 0.2704741954803467, 'validation/ssim': 0.7236815856693163, 'validation/loss': 0.28719941645307223, 'validation/num_examples': 3554, 'test/ssim': 0.7408752356185423, 'test/loss': 0.2886526559991099, 'test/num_examples': 3581, 'score': 762.910413980484, 'total_duration': 955.065144777298, 'accumulated_submission_time': 762.910413980484, 'accumulated_eval_time': 191.44445872306824, 'accumulated_logging_time': 0.16780352592468262, 'global_step': 7777, 'preemption_count': 0})], 'global_step': 7777}
I0307 16:23:18.740228 140000997733568 submission_runner.py:649] Timing: 762.910413980484
I0307 16:23:18.740262 140000997733568 submission_runner.py:651] Total number of evals: 10
I0307 16:23:18.740290 140000997733568 submission_runner.py:652] ====================
I0307 16:23:18.740414 140000997733568 submission_runner.py:750] Final fastmri score: 1
