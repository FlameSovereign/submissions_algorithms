python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-881520971 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-16-23-48.log
2025-03-07 16:23:51.094657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741364631.116914       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741364631.123752       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:23:57.284636 139874142557376 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax.
I0307 16:23:58.147379 139874142557376 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:23:58.150252 139874142557376 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:23:58.152078 139874142557376 submission_runner.py:606] Using RNG seed -881520971
I0307 16:23:58.732695 139874142557376 submission_runner.py:615] --- Tuning run 5/5 ---
I0307 16:23:58.732906 139874142557376 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_5.
I0307 16:23:58.733118 139874142557376 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_5/hparams.json.
I0307 16:23:58.974155 139874142557376 submission_runner.py:218] Initializing dataset.
I0307 16:24:03.883271 139874142557376 submission_runner.py:229] Initializing model.
I0307 16:24:14.109801 139874142557376 submission_runner.py:272] Initializing optimizer.
I0307 16:24:14.592136 139874142557376 submission_runner.py:279] Initializing metrics bundle.
I0307 16:24:14.592338 139874142557376 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:24:14.593187 139874142557376 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_5 with prefix checkpoint_
I0307 16:24:14.593296 139874142557376 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_5/meta_data_0.json.
I0307 16:24:14.593485 139874142557376 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:24:14.593539 139874142557376 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:24:14.751253 139874142557376 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_5/flags_0.json.
I0307 16:24:14.782480 139874142557376 submission_runner.py:337] Starting training loop.
E0307 16:24:42.266182       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:42.472514       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:42.879435       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:43.085658       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:44.920895       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:24:45.126643       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 16:25:01.075087 139737314596608 logging_writer.py:48] [0] global_step=0, grad_norm=3.397475242614746, loss=1.0164908170700073
I0307 16:25:01.127554 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:25:54.558994 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:26:34.534539 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:26:55.415622 139874142557376 submission_runner.py:469] Time since start: 160.63s, 	Step: 1, 	{'train/ssim': 0.22352738039834158, 'train/loss': 1.022589819771903, 'validation/ssim': 0.21093519873074354, 'validation/loss': 1.0275126700328854, 'validation/num_examples': 3554, 'test/ssim': 0.23437303992098052, 'test/loss': 1.024500305976857, 'test/num_examples': 3581, 'score': 46.344897508621216, 'total_duration': 160.63308477401733, 'accumulated_submission_time': 46.344897508621216, 'accumulated_eval_time': 114.28802156448364, 'accumulated_logging_time': 0}
I0307 16:26:55.423133 139723137873664 logging_writer.py:48] [1] accumulated_eval_time=114.288, accumulated_logging_time=0, accumulated_submission_time=46.3449, global_step=1, preemption_count=0, score=46.3449, test/loss=1.0245, test/num_examples=3581, test/ssim=0.234373, total_duration=160.633, train/loss=1.02259, train/ssim=0.223527, validation/loss=1.02751, validation/num_examples=3554, validation/ssim=0.210935
I0307 16:27:08.302638 139723129480960 logging_writer.py:48] [100] global_step=100, grad_norm=0.15484857559204102, loss=0.47578415274620056
I0307 16:27:25.850772 139723137873664 logging_writer.py:48] [200] global_step=200, grad_norm=0.31232479214668274, loss=0.39057159423828125
I0307 16:27:41.770175 139723129480960 logging_writer.py:48] [300] global_step=300, grad_norm=0.13019192218780518, loss=0.36945390701293945
I0307 16:28:00.075285 139723137873664 logging_writer.py:48] [400] global_step=400, grad_norm=0.17396187782287598, loss=0.42617058753967285
I0307 16:28:15.528534 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:28:17.323472 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:28:18.614113 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:28:19.907639 139874142557376 submission_runner.py:469] Time since start: 245.13s, 	Step: 489, 	{'train/ssim': 0.7115646089826312, 'train/loss': 0.30297371319362093, 'validation/ssim': 0.6929954996790588, 'validation/loss': 0.3161163244209869, 'validation/num_examples': 3554, 'test/ssim': 0.7104821480775272, 'test/loss': 0.318564382221272, 'test/num_examples': 3581, 'score': 126.343585729599, 'total_duration': 245.12510919570923, 'accumulated_submission_time': 126.343585729599, 'accumulated_eval_time': 118.66709995269775, 'accumulated_logging_time': 0.01603102684020996}
I0307 16:28:19.918409 139723129480960 logging_writer.py:48] [489] accumulated_eval_time=118.667, accumulated_logging_time=0.016031, accumulated_submission_time=126.344, global_step=489, preemption_count=0, score=126.344, test/loss=0.318564, test/num_examples=3581, test/ssim=0.710482, total_duration=245.125, train/loss=0.302974, train/ssim=0.711565, validation/loss=0.316116, validation/num_examples=3554, validation/ssim=0.692995
I0307 16:28:20.955632 139723137873664 logging_writer.py:48] [500] global_step=500, grad_norm=0.37323665618896484, loss=0.375489205121994
I0307 16:28:36.924689 139723129480960 logging_writer.py:48] [600] global_step=600, grad_norm=0.1584901362657547, loss=0.34786248207092285
I0307 16:28:55.237377 139723137873664 logging_writer.py:48] [700] global_step=700, grad_norm=0.23441986739635468, loss=0.3794906437397003
I0307 16:29:13.406424 139723129480960 logging_writer.py:48] [800] global_step=800, grad_norm=0.2987823486328125, loss=0.25166869163513184
I0307 16:29:31.255393 139723137873664 logging_writer.py:48] [900] global_step=900, grad_norm=0.10449197143316269, loss=0.3142026364803314
I0307 16:29:40.010054 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:29:41.298904 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:29:42.592918 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:29:43.888924 139874142557376 submission_runner.py:469] Time since start: 329.11s, 	Step: 948, 	{'train/ssim': 0.7228605406624931, 'train/loss': 0.293184723172869, 'validation/ssim': 0.7045563894775253, 'validation/loss': 0.30598345806441685, 'validation/num_examples': 3554, 'test/ssim': 0.721692777419017, 'test/loss': 0.30844226145804243, 'test/num_examples': 3581, 'score': 206.34170126914978, 'total_duration': 329.1063702106476, 'accumulated_submission_time': 206.34170126914978, 'accumulated_eval_time': 122.54590368270874, 'accumulated_logging_time': 0.04238629341125488}
I0307 16:29:43.901073 139723129480960 logging_writer.py:48] [948] accumulated_eval_time=122.546, accumulated_logging_time=0.0423863, accumulated_submission_time=206.342, global_step=948, preemption_count=0, score=206.342, test/loss=0.308442, test/num_examples=3581, test/ssim=0.721693, total_duration=329.106, train/loss=0.293185, train/ssim=0.722861, validation/loss=0.305983, validation/num_examples=3554, validation/ssim=0.704556
I0307 16:29:48.490697 139723137873664 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.05968593806028366, loss=0.3173108696937561
I0307 16:29:56.833982 139723129480960 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.19657543301582336, loss=0.3240032196044922
I0307 16:30:05.133991 139723137873664 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.061955638229846954, loss=0.304635226726532
I0307 16:30:13.453300 139723129480960 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.10713982582092285, loss=0.31869977712631226
I0307 16:30:21.818270 139723137873664 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.2296726405620575, loss=0.2370990812778473
I0307 16:30:30.143575 139723129480960 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.1436173915863037, loss=0.4042682349681854
I0307 16:30:38.466128 139723137873664 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.19863088428974152, loss=0.26645714044570923
I0307 16:30:46.768868 139723129480960 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.2751786708831787, loss=0.2763863205909729
I0307 16:30:55.084773 139723137873664 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.23212411999702454, loss=0.33495020866394043
I0307 16:31:03.421396 139723129480960 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.2567351460456848, loss=0.2553943395614624
I0307 16:31:03.929523 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:31:05.219627 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:31:06.515268 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:31:07.810512 139874142557376 submission_runner.py:469] Time since start: 413.03s, 	Step: 1907, 	{'train/ssim': 0.7280037743704659, 'train/loss': 0.2855214391435896, 'validation/ssim': 0.7089410973990574, 'validation/loss': 0.29881744298897367, 'validation/num_examples': 3554, 'test/ssim': 0.7263000198530438, 'test/loss': 0.30102583377330006, 'test/num_examples': 3581, 'score': 286.3103656768799, 'total_duration': 413.0279688835144, 'accumulated_submission_time': 286.3103656768799, 'accumulated_eval_time': 126.42684245109558, 'accumulated_logging_time': 0.06871747970581055}
I0307 16:31:07.819991 139723137873664 logging_writer.py:48] [1907] accumulated_eval_time=126.427, accumulated_logging_time=0.0687175, accumulated_submission_time=286.31, global_step=1907, preemption_count=0, score=286.31, test/loss=0.301026, test/num_examples=3581, test/ssim=0.7263, total_duration=413.028, train/loss=0.285521, train/ssim=0.728004, validation/loss=0.298817, validation/num_examples=3554, validation/ssim=0.708941
I0307 16:31:15.652391 139723129480960 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.1688949018716812, loss=0.2826005816459656
I0307 16:31:23.955979 139723137873664 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.4942825436592102, loss=0.295084148645401
I0307 16:31:32.263735 139723129480960 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.2724575698375702, loss=0.3014300465583801
I0307 16:31:40.574537 139723137873664 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.22144469618797302, loss=0.25785043835639954
I0307 16:31:48.901213 139723129480960 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.07156868278980255, loss=0.2585541605949402
I0307 16:31:57.212736 139723137873664 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.21430926024913788, loss=0.4167324900627136
I0307 16:32:05.538861 139723129480960 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.15012812614440918, loss=0.33707496523857117
I0307 16:32:13.858080 139723137873664 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.22950363159179688, loss=0.37973207235336304
I0307 16:32:22.175869 139723129480960 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.39901003241539, loss=0.235856831073761
I0307 16:32:27.821219 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:32:29.115361 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:32:30.411660 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:32:31.708173 139874142557376 submission_runner.py:469] Time since start: 496.93s, 	Step: 2869, 	{'train/ssim': 0.7314768518720355, 'train/loss': 0.2824376310620989, 'validation/ssim': 0.712147143293648, 'validation/loss': 0.2957527018961909, 'validation/num_examples': 3554, 'test/ssim': 0.7293492892173974, 'test/loss': 0.29767921189960905, 'test/num_examples': 3581, 'score': 366.26108622550964, 'total_duration': 496.9256341457367, 'accumulated_submission_time': 366.26108622550964, 'accumulated_eval_time': 130.31374716758728, 'accumulated_logging_time': 0.08663320541381836}
I0307 16:32:31.717610 139723137873664 logging_writer.py:48] [2869] accumulated_eval_time=130.314, accumulated_logging_time=0.0866332, accumulated_submission_time=366.261, global_step=2869, preemption_count=0, score=366.261, test/loss=0.297679, test/num_examples=3581, test/ssim=0.729349, total_duration=496.926, train/loss=0.282438, train/ssim=0.731477, validation/loss=0.295753, validation/num_examples=3554, validation/ssim=0.712147
I0307 16:32:34.390322 139723129480960 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.11546243727207184, loss=0.31037187576293945
I0307 16:32:42.722059 139723137873664 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.09313919395208359, loss=0.32194003462791443
I0307 16:32:51.043928 139723129480960 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.09357280284166336, loss=0.3801056742668152
I0307 16:32:59.373111 139723137873664 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.26460060477256775, loss=0.26179563999176025
I0307 16:33:07.694370 139723129480960 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.10371513664722443, loss=0.25872913002967834
I0307 16:33:15.994824 139723137873664 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.29682737588882446, loss=0.31826668977737427
I0307 16:33:24.304287 139723129480960 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.20510892570018768, loss=0.2787427306175232
I0307 16:33:32.627363 139723137873664 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.15532265603542328, loss=0.29364684224128723
I0307 16:33:40.930377 139723129480960 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.2622898817062378, loss=0.35463467240333557
I0307 16:33:49.228647 139723137873664 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.31459933519363403, loss=0.2851964831352234
I0307 16:33:51.731362 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:33:53.025649 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:33:54.320466 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:33:55.616945 139874142557376 submission_runner.py:469] Time since start: 580.83s, 	Step: 3831, 	{'train/ssim': 0.7349011557442802, 'train/loss': 0.28076822417122976, 'validation/ssim': 0.7156549645865574, 'validation/loss': 0.2940992914563696, 'validation/num_examples': 3554, 'test/ssim': 0.7329428810475426, 'test/loss': 0.29594224100504396, 'test/num_examples': 3581, 'score': 446.22419810295105, 'total_duration': 580.8344089984894, 'accumulated_submission_time': 446.22419810295105, 'accumulated_eval_time': 134.19928288459778, 'accumulated_logging_time': 0.10394978523254395}
I0307 16:33:55.627670 139723129480960 logging_writer.py:48] [3831] accumulated_eval_time=134.199, accumulated_logging_time=0.10395, accumulated_submission_time=446.224, global_step=3831, preemption_count=0, score=446.224, test/loss=0.295942, test/num_examples=3581, test/ssim=0.732943, total_duration=580.834, train/loss=0.280768, train/ssim=0.734901, validation/loss=0.294099, validation/num_examples=3554, validation/ssim=0.715655
I0307 16:34:01.457192 139723137873664 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.18432272970676422, loss=0.3602377474308014
I0307 16:34:09.785571 139723129480960 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.19395259022712708, loss=0.26479271054267883
I0307 16:34:18.126945 139723137873664 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.09333186596632004, loss=0.35253795981407166
I0307 16:34:26.454821 139723129480960 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.25459927320480347, loss=0.3841584026813507
I0307 16:34:34.791042 139723137873664 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.15326379239559174, loss=0.30702510476112366
I0307 16:34:43.084011 139723129480960 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.19550171494483948, loss=0.22232681512832642
I0307 16:34:51.401685 139723137873664 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.1783382147550583, loss=0.2949395775794983
I0307 16:34:59.739502 139723129480960 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.17730295658111572, loss=0.3128238618373871
I0307 16:35:08.068389 139723137873664 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.1227613165974617, loss=0.2767621874809265
I0307 16:35:15.640924 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:35:16.936175 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:35:18.233673 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:35:19.529164 139874142557376 submission_runner.py:469] Time since start: 664.75s, 	Step: 4792, 	{'train/ssim': 0.7363511494227818, 'train/loss': 0.27842869077410015, 'validation/ssim': 0.7165936763593838, 'validation/loss': 0.29226435552370567, 'validation/num_examples': 3554, 'test/ssim': 0.7339420781991762, 'test/loss': 0.2938926801172857, 'test/num_examples': 3581, 'score': 526.1864306926727, 'total_duration': 664.7466311454773, 'accumulated_submission_time': 526.1864306926727, 'accumulated_eval_time': 138.08747601509094, 'accumulated_logging_time': 0.12322020530700684}
I0307 16:35:19.539120 139723129480960 logging_writer.py:48] [4792] accumulated_eval_time=138.087, accumulated_logging_time=0.12322, accumulated_submission_time=526.186, global_step=4792, preemption_count=0, score=526.186, test/loss=0.293893, test/num_examples=3581, test/ssim=0.733942, total_duration=664.747, train/loss=0.278429, train/ssim=0.736351, validation/loss=0.292264, validation/num_examples=3554, validation/ssim=0.716594
I0307 16:35:20.299190 139723137873664 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1974070519208908, loss=0.31402474641799927
I0307 16:35:28.635586 139723129480960 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.21475006639957428, loss=0.24073179066181183
I0307 16:35:36.966343 139723137873664 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.14577455818653107, loss=0.3124459385871887
I0307 16:35:45.287262 139723129480960 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.16035068035125732, loss=0.27267271280288696
I0307 16:35:53.615967 139723137873664 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.10954935103654861, loss=0.39996451139450073
I0307 16:36:01.925248 139723129480960 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.23147359490394592, loss=0.29848966002464294
I0307 16:36:10.224827 139723137873664 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.44552290439605713, loss=0.2561047673225403
I0307 16:36:18.521523 139723129480960 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.380066841840744, loss=0.2826249301433563
I0307 16:36:26.822754 139723137873664 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.11191506683826447, loss=0.28321027755737305
I0307 16:36:35.150582 139723129480960 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.13030995428562164, loss=0.3592551350593567
I0307 16:36:39.546226 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:36:40.837873 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:36:42.135947 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:36:43.433790 139874142557376 submission_runner.py:469] Time since start: 748.65s, 	Step: 5754, 	{'train/ssim': 0.7373614992414202, 'train/loss': 0.2778820310320173, 'validation/ssim': 0.717423919351435, 'validation/loss': 0.2917593814847707, 'validation/num_examples': 3554, 'test/ssim': 0.7344892640847529, 'test/loss': 0.29348000678494135, 'test/num_examples': 3581, 'score': 606.1383287906647, 'total_duration': 748.6512529850006, 'accumulated_submission_time': 606.1383287906647, 'accumulated_eval_time': 141.9749927520752, 'accumulated_logging_time': 0.14121794700622559}
I0307 16:36:43.444226 139723137873664 logging_writer.py:48] [5754] accumulated_eval_time=141.975, accumulated_logging_time=0.141218, accumulated_submission_time=606.138, global_step=5754, preemption_count=0, score=606.138, test/loss=0.29348, test/num_examples=3581, test/ssim=0.734489, total_duration=748.651, train/loss=0.277882, train/ssim=0.737361, validation/loss=0.291759, validation/num_examples=3554, validation/ssim=0.717424
I0307 16:36:47.362174 139723129480960 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.09135124832391739, loss=0.31013795733451843
I0307 16:36:55.681772 139723137873664 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.08647948503494263, loss=0.3436388671398163
I0307 16:37:04.007643 139723129480960 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.14764192700386047, loss=0.2806438207626343
I0307 16:37:12.366137 139723137873664 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.16605742275714874, loss=0.33460038900375366
I0307 16:37:20.686073 139723129480960 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.14297053217887878, loss=0.23864439129829407
I0307 16:37:29.000962 139723137873664 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.08339575678110123, loss=0.318611204624176
I0307 16:37:37.330507 139723129480960 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.086433045566082, loss=0.3272780776023865
I0307 16:37:45.665775 139723137873664 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0904899314045906, loss=0.25245800614356995
I0307 16:37:53.961528 139723129480960 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2032768279314041, loss=0.25917428731918335
I0307 16:38:02.284257 139723137873664 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.09769004583358765, loss=0.2927224636077881
I0307 16:38:03.456814 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:38:04.752225 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:38:06.047884 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:38:07.345703 139874142557376 submission_runner.py:469] Time since start: 832.56s, 	Step: 6715, 	{'train/ssim': 0.7367210388183594, 'train/loss': 0.2767845903124128, 'validation/ssim': 0.7165620081466305, 'validation/loss': 0.29060627385076676, 'validation/num_examples': 3554, 'test/ssim': 0.7342842568634809, 'test/loss': 0.29201345861840267, 'test/num_examples': 3581, 'score': 686.0969247817993, 'total_duration': 832.5631713867188, 'accumulated_submission_time': 686.0969247817993, 'accumulated_eval_time': 145.86383700370789, 'accumulated_logging_time': 0.1596517562866211}
I0307 16:38:07.356658 139723129480960 logging_writer.py:48] [6715] accumulated_eval_time=145.864, accumulated_logging_time=0.159652, accumulated_submission_time=686.097, global_step=6715, preemption_count=0, score=686.097, test/loss=0.292013, test/num_examples=3581, test/ssim=0.734284, total_duration=832.563, train/loss=0.276785, train/ssim=0.736721, validation/loss=0.290606, validation/num_examples=3554, validation/ssim=0.716562
I0307 16:38:14.521421 139723137873664 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.11028549820184708, loss=0.33870917558670044
I0307 16:38:22.821321 139723129480960 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.22360637784004211, loss=0.25078684091567993
I0307 16:38:31.137575 139723137873664 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.09605603665113449, loss=0.2942594289779663
I0307 16:38:39.445068 139723129480960 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.18056613206863403, loss=0.25554805994033813
I0307 16:38:47.743007 139723137873664 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.13804762065410614, loss=0.32651287317276
I0307 16:38:56.044742 139723129480960 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.09288471192121506, loss=0.24026361107826233
I0307 16:39:04.374676 139723137873664 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.3593335449695587, loss=0.2684870660305023
I0307 16:39:12.667356 139723129480960 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.07442473620176315, loss=0.3199504613876343
I0307 16:39:20.994417 139723137873664 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.20478574931621552, loss=0.22133557498455048
I0307 16:39:27.363099 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:39:28.654864 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:39:29.950054 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:39:31.247355 139874142557376 submission_runner.py:469] Time since start: 916.46s, 	Step: 7678, 	{'train/ssim': 0.7392879894801548, 'train/loss': 0.2757589987346104, 'validation/ssim': 0.7192405481719542, 'validation/loss': 0.28964272897835186, 'validation/num_examples': 3554, 'test/ssim': 0.7366069675457274, 'test/loss': 0.2912133032301557, 'test/num_examples': 3581, 'score': 766.0476813316345, 'total_duration': 916.4647932052612, 'accumulated_submission_time': 766.0476813316345, 'accumulated_eval_time': 149.74801683425903, 'accumulated_logging_time': 0.17925143241882324}
I0307 16:39:31.260342 139723129480960 logging_writer.py:48] [7678] accumulated_eval_time=149.748, accumulated_logging_time=0.179251, accumulated_submission_time=766.048, global_step=7678, preemption_count=0, score=766.048, test/loss=0.291213, test/num_examples=3581, test/ssim=0.736607, total_duration=916.465, train/loss=0.275759, train/ssim=0.739288, validation/loss=0.289643, validation/num_examples=3554, validation/ssim=0.719241
I0307 16:39:33.195000 139723137873664 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.1362290382385254, loss=0.3717159032821655
I0307 16:39:41.527996 139723129480960 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.16495057940483093, loss=0.24787741899490356
I0307 16:39:49.863873 139723137873664 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.07057443261146545, loss=0.3148956894874573
I0307 16:39:58.206900 139723129480960 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.07190416008234024, loss=0.3599996566772461
I0307 16:40:06.532300 139723137873664 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.18509186804294586, loss=0.27597907185554504
I0307 16:40:14.865097 139723129480960 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.19810068607330322, loss=0.32095617055892944
I0307 16:40:23.208114 139723137873664 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.11830838024616241, loss=0.2526136636734009
I0307 16:40:31.562698 139723129480960 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.43333351612091064, loss=0.34467753767967224
I0307 16:40:39.867480 139723137873664 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.14158622920513153, loss=0.3330124616622925
I0307 16:40:48.172676 139723129480960 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.2780778110027313, loss=0.26431402564048767
I0307 16:40:51.257311 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:40:52.551350 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:40:53.847666 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:40:55.144683 139874142557376 submission_runner.py:469] Time since start: 1000.36s, 	Step: 8638, 	{'train/ssim': 0.7391907146998814, 'train/loss': 0.27576025894709993, 'validation/ssim': 0.7192065443426421, 'validation/loss': 0.28964049640370004, 'validation/num_examples': 3554, 'test/ssim': 0.7364210497896886, 'test/loss': 0.2911922025534069, 'test/num_examples': 3581, 'score': 845.9909300804138, 'total_duration': 1000.3621490001678, 'accumulated_submission_time': 845.9909300804138, 'accumulated_eval_time': 153.63534140586853, 'accumulated_logging_time': 0.20130515098571777}
I0307 16:40:55.155833 139723137873664 logging_writer.py:48] [8638] accumulated_eval_time=153.635, accumulated_logging_time=0.201305, accumulated_submission_time=845.991, global_step=8638, preemption_count=0, score=845.991, test/loss=0.291192, test/num_examples=3581, test/ssim=0.736421, total_duration=1000.36, train/loss=0.27576, train/ssim=0.739191, validation/loss=0.28964, validation/num_examples=3554, validation/ssim=0.719207
I0307 16:41:00.400936 139723129480960 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.1415187120437622, loss=0.2613888680934906
I0307 16:41:08.675622 139723137873664 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.18725208938121796, loss=0.297432005405426
I0307 16:41:16.986686 139723129480960 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.13208872079849243, loss=0.23642456531524658
I0307 16:41:25.321421 139723137873664 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.13579192757606506, loss=0.24328775703907013
I0307 16:41:33.636590 139723129480960 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.2251097559928894, loss=0.3048609793186188
I0307 16:41:41.932016 139723137873664 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.17272423207759857, loss=0.26403704285621643
I0307 16:41:50.234976 139723129480960 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0930347815155983, loss=0.2842146158218384
I0307 16:41:58.546322 139723137873664 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.14539821445941925, loss=0.25176554918289185
I0307 16:42:06.899811 139723129480960 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.1342940628528595, loss=0.21898597478866577
I0307 16:42:15.212990 139723137873664 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.06823842972517014, loss=0.2957654893398285
I0307 16:42:15.218106 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:42:16.511777 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:42:17.809167 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:42:19.107030 139874142557376 submission_runner.py:469] Time since start: 1084.32s, 	Step: 9601, 	{'train/ssim': 0.7402725219726562, 'train/loss': 0.2750066007886614, 'validation/ssim': 0.7203701622511607, 'validation/loss': 0.2888273927155142, 'validation/num_examples': 3554, 'test/ssim': 0.7376605696732756, 'test/loss': 0.2903655946204796, 'test/num_examples': 3581, 'score': 926.0004110336304, 'total_duration': 1084.324478149414, 'accumulated_submission_time': 926.0004110336304, 'accumulated_eval_time': 157.52419233322144, 'accumulated_logging_time': 0.22099041938781738}
I0307 16:42:19.120996 139723129480960 logging_writer.py:48] [9601] accumulated_eval_time=157.524, accumulated_logging_time=0.22099, accumulated_submission_time=926, global_step=9601, preemption_count=0, score=926, test/loss=0.290366, test/num_examples=3581, test/ssim=0.737661, total_duration=1084.32, train/loss=0.275007, train/ssim=0.740273, validation/loss=0.288827, validation/num_examples=3554, validation/ssim=0.72037
I0307 16:42:31.766181 139723137873664 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.061073899269104004, loss=0.33637428283691406
I0307 16:42:40.081832 139723129480960 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.21148023009300232, loss=0.24860039353370667
I0307 16:42:48.385886 139723137873664 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.12401807308197021, loss=0.2907779812812805
I0307 16:42:56.669972 139723129480960 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3150531053543091, loss=0.2734350264072418
I0307 16:43:04.975537 139723137873664 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.22605855762958527, loss=0.316427618265152
I0307 16:43:13.261097 139723129480960 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.28903961181640625, loss=0.2587994635105133
I0307 16:43:21.561881 139723137873664 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.13829074800014496, loss=0.23615393042564392
I0307 16:43:29.870450 139723129480960 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.12368428707122803, loss=0.3296920359134674
I0307 16:43:38.176161 139723137873664 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.13292261958122253, loss=0.2553336024284363
I0307 16:43:39.180729 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:43:40.470894 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:43:41.768811 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:43:43.068054 139874142557376 submission_runner.py:469] Time since start: 1168.29s, 	Step: 10513, 	{'train/ssim': 0.7398304258074079, 'train/loss': 0.2749426705496652, 'validation/ssim': 0.7199834803214687, 'validation/loss': 0.28870274635533905, 'validation/num_examples': 3554, 'test/ssim': 0.7373252768517872, 'test/loss': 0.29027955567360725, 'test/num_examples': 3581, 'score': 1001.8691554069519, 'total_duration': 1168.2855155467987, 'accumulated_submission_time': 1001.8691554069519, 'accumulated_eval_time': 161.4114646911621, 'accumulated_logging_time': 4.383329391479492}
I0307 16:43:43.079340 139723129480960 logging_writer.py:48] [10513] accumulated_eval_time=161.411, accumulated_logging_time=4.38333, accumulated_submission_time=1001.87, global_step=10513, preemption_count=0, score=1001.87, test/loss=0.29028, test/num_examples=3581, test/ssim=0.737325, total_duration=1168.29, train/loss=0.274943, train/ssim=0.73983, validation/loss=0.288703, validation/num_examples=3554, validation/ssim=0.719983
I0307 16:43:50.406801 139723137873664 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.2594195008277893, loss=0.2271760106086731
I0307 16:43:58.725710 139723129480960 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.12331227213144302, loss=0.3167128562927246
I0307 16:44:07.059824 139723137873664 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.10299170762300491, loss=0.2929876446723938
I0307 16:44:15.350803 139723129480960 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.09883589297533035, loss=0.298459529876709
I0307 16:44:23.655737 139723137873664 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.08850029855966568, loss=0.2749381363391876
I0307 16:44:31.941344 139723129480960 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.15999454259872437, loss=0.2112441509962082
I0307 16:44:40.263631 139723137873664 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.10261251032352448, loss=0.3308589458465576
I0307 16:44:48.553808 139723129480960 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.20085971057415009, loss=0.26301100850105286
I0307 16:44:56.858466 139723137873664 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.07784479856491089, loss=0.3688255548477173
I0307 16:45:03.098708 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:45:04.393263 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:45:05.687727 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:45:06.984036 139874142557376 submission_runner.py:469] Time since start: 1252.20s, 	Step: 11476, 	{'train/ssim': 0.7408531733921596, 'train/loss': 0.27451971599033903, 'validation/ssim': 0.720891348216798, 'validation/loss': 0.28844414551605585, 'validation/num_examples': 3554, 'test/ssim': 0.7382553428686122, 'test/loss': 0.289964102260629, 'test/num_examples': 3581, 'score': 1081.83860373497, 'total_duration': 1252.2015018463135, 'accumulated_submission_time': 1081.83860373497, 'accumulated_eval_time': 165.29674291610718, 'accumulated_logging_time': 4.402867794036865}
I0307 16:45:06.996121 139723129480960 logging_writer.py:48] [11476] accumulated_eval_time=165.297, accumulated_logging_time=4.40287, accumulated_submission_time=1081.84, global_step=11476, preemption_count=0, score=1081.84, test/loss=0.289964, test/num_examples=3581, test/ssim=0.738255, total_duration=1252.2, train/loss=0.27452, train/ssim=0.740853, validation/loss=0.288444, validation/num_examples=3554, validation/ssim=0.720891
I0307 16:45:09.086285 139723137873664 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.1604512333869934, loss=0.26915737986564636
I0307 16:45:17.407974 139723129480960 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.15871837735176086, loss=0.27303385734558105
I0307 16:45:25.721520 139723137873664 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.11885654926300049, loss=0.3342728912830353
I0307 16:45:34.036356 139723129480960 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.21364746987819672, loss=0.31401291489601135
I0307 16:45:42.324275 139723137873664 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.11740724742412567, loss=0.20074094831943512
I0307 16:45:50.629589 139723129480960 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.1554870903491974, loss=0.27627789974212646
I0307 16:45:58.939929 139723137873664 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0753723680973053, loss=0.2745729088783264
I0307 16:46:07.242069 139723129480960 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.15034489333629608, loss=0.3307899832725525
I0307 16:46:15.532289 139723137873664 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3217952251434326, loss=0.2341012954711914
I0307 16:46:23.859142 139723129480960 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.21428662538528442, loss=0.31079909205436707
I0307 16:46:27.013578 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:46:28.306075 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:46:29.604540 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:46:30.901440 139874142557376 submission_runner.py:469] Time since start: 1336.12s, 	Step: 12439, 	{'train/ssim': 0.7410992894853864, 'train/loss': 0.27413473810468403, 'validation/ssim': 0.721452033580121, 'validation/loss': 0.2880175520210502, 'validation/num_examples': 3554, 'test/ssim': 0.7387314886728568, 'test/loss': 0.2895541559947466, 'test/num_examples': 3581, 'score': 1161.8065090179443, 'total_duration': 1336.1189105510712, 'accumulated_submission_time': 1161.8065090179443, 'accumulated_eval_time': 169.18456363677979, 'accumulated_logging_time': 4.422452688217163}
I0307 16:46:30.913564 139723137873664 logging_writer.py:48] [12439] accumulated_eval_time=169.185, accumulated_logging_time=4.42245, accumulated_submission_time=1161.81, global_step=12439, preemption_count=0, score=1161.81, test/loss=0.289554, test/num_examples=3581, test/ssim=0.738731, total_duration=1336.12, train/loss=0.274135, train/ssim=0.741099, validation/loss=0.288018, validation/num_examples=3554, validation/ssim=0.721452
I0307 16:46:36.077766 139723129480960 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0973869189620018, loss=0.2743537724018097
I0307 16:46:44.391731 139723137873664 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.15944069623947144, loss=0.3352445960044861
I0307 16:46:52.697934 139723129480960 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.10211868584156036, loss=0.2659827768802643
I0307 16:47:00.992968 139723137873664 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1336553692817688, loss=0.25174498558044434
I0307 16:47:09.299256 139723129480960 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3094456195831299, loss=0.3015669286251068
I0307 16:47:17.600239 139723137873664 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.21525365114212036, loss=0.29316526651382446
I0307 16:47:25.884909 139723129480960 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.190946564078331, loss=0.2210986316204071
I0307 16:47:34.175543 139723137873664 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.21229888498783112, loss=0.25894594192504883
I0307 16:47:42.487880 139723129480960 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.18564946949481964, loss=0.3024144470691681
I0307 16:47:50.784820 139723137873664 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.21628250181674957, loss=0.3734109699726105
I0307 16:47:50.956663 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:47:52.248663 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:47:53.544128 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:47:54.841438 139874142557376 submission_runner.py:469] Time since start: 1420.06s, 	Step: 13403, 	{'train/ssim': 0.7433362688337054, 'train/loss': 0.2738545111247471, 'validation/ssim': 0.7231015284274761, 'validation/loss': 0.28842968530177265, 'validation/num_examples': 3554, 'test/ssim': 0.7403071876745323, 'test/loss': 0.28997794212292305, 'test/num_examples': 3581, 'score': 1241.8006756305695, 'total_duration': 1420.0589079856873, 'accumulated_submission_time': 1241.8006756305695, 'accumulated_eval_time': 173.0692937374115, 'accumulated_logging_time': 4.442254304885864}
I0307 16:47:54.853382 139723129480960 logging_writer.py:48] [13403] accumulated_eval_time=173.069, accumulated_logging_time=4.44225, accumulated_submission_time=1241.8, global_step=13403, preemption_count=0, score=1241.8, test/loss=0.289978, test/num_examples=3581, test/ssim=0.740307, total_duration=1420.06, train/loss=0.273855, train/ssim=0.743336, validation/loss=0.28843, validation/num_examples=3554, validation/ssim=0.723102
I0307 16:48:02.994588 139723137873664 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.17812304198741913, loss=0.2958289384841919
I0307 16:48:11.288988 139723129480960 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.13388091325759888, loss=0.23824375867843628
I0307 16:48:19.569716 139723137873664 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0890013799071312, loss=0.26961061358451843
I0307 16:48:27.906414 139723129480960 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.08877209573984146, loss=0.24010658264160156
I0307 16:48:36.234797 139723137873664 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.19010771811008453, loss=0.2863544821739197
I0307 16:48:44.573210 139723129480960 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.1633884608745575, loss=0.26521939039230347
I0307 16:48:52.914258 139723137873664 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.10752063989639282, loss=0.26520881056785583
I0307 16:49:01.224660 139723129480960 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.22637705504894257, loss=0.22511552274227142
I0307 16:49:09.530220 139723137873664 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.19898074865341187, loss=0.26108336448669434
I0307 16:49:14.845515 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:49:16.138938 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:49:17.439344 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:49:18.736156 139874142557376 submission_runner.py:469] Time since start: 1503.95s, 	Step: 14365, 	{'train/ssim': 0.7419112069266183, 'train/loss': 0.27333434990474154, 'validation/ssim': 0.7215948496632316, 'validation/loss': 0.28759360326832445, 'validation/num_examples': 3554, 'test/ssim': 0.7389639029120707, 'test/loss': 0.28904463771467465, 'test/num_examples': 3581, 'score': 1321.7436938285828, 'total_duration': 1503.953621149063, 'accumulated_submission_time': 1321.7436938285828, 'accumulated_eval_time': 176.95988535881042, 'accumulated_logging_time': 4.4619951248168945}
I0307 16:49:18.749387 139723129480960 logging_writer.py:48] [14365] accumulated_eval_time=176.96, accumulated_logging_time=4.462, accumulated_submission_time=1321.74, global_step=14365, preemption_count=0, score=1321.74, test/loss=0.289045, test/num_examples=3581, test/ssim=0.738964, total_duration=1503.95, train/loss=0.273334, train/ssim=0.741911, validation/loss=0.287594, validation/num_examples=3554, validation/ssim=0.721595
I0307 16:49:21.765572 139723137873664 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.216978058218956, loss=0.2823275923728943
I0307 16:49:30.097800 139723129480960 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.08169256150722504, loss=0.27737727761268616
I0307 16:49:38.377420 139723137873664 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0637621209025383, loss=0.32937806844711304
I0307 16:49:46.671529 139723129480960 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.09955596178770065, loss=0.29219257831573486
I0307 16:49:55.003870 139723137873664 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.12481790035963058, loss=0.3294989764690399
I0307 16:50:03.295577 139723129480960 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.11264923959970474, loss=0.24432584643363953
I0307 16:50:11.613633 139723137873664 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.21011343598365784, loss=0.22573642432689667
I0307 16:50:19.913125 139723129480960 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.07007069885730743, loss=0.33373138308525085
I0307 16:50:28.213987 139723137873664 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.19891877472400665, loss=0.24815905094146729
I0307 16:50:36.520054 139723129480960 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.20343156158924103, loss=0.2354537695646286
I0307 16:50:38.760912 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:50:40.053889 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:50:41.354145 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:50:42.655574 139874142557376 submission_runner.py:469] Time since start: 1587.87s, 	Step: 15328, 	{'train/ssim': 0.742173331124442, 'train/loss': 0.2732717820576259, 'validation/ssim': 0.7217690591806767, 'validation/loss': 0.28764110558745426, 'validation/num_examples': 3554, 'test/ssim': 0.7390887343793633, 'test/loss': 0.289144380170518, 'test/num_examples': 3581, 'score': 1401.7052421569824, 'total_duration': 1587.8730199337006, 'accumulated_submission_time': 1401.7052421569824, 'accumulated_eval_time': 180.85447812080383, 'accumulated_logging_time': 4.484252214431763}
I0307 16:50:42.670037 139723137873664 logging_writer.py:48] [15328] accumulated_eval_time=180.854, accumulated_logging_time=4.48425, accumulated_submission_time=1401.71, global_step=15328, preemption_count=0, score=1401.71, test/loss=0.289144, test/num_examples=3581, test/ssim=0.739089, total_duration=1587.87, train/loss=0.273272, train/ssim=0.742173, validation/loss=0.287641, validation/num_examples=3554, validation/ssim=0.721769
I0307 16:50:48.754959 139723129480960 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1907869130373001, loss=0.2612534463405609
I0307 16:50:57.099746 139723137873664 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.05181625112891197, loss=0.3013835549354553
I0307 16:51:05.418286 139723129480960 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.1303529441356659, loss=0.29790955781936646
I0307 16:51:13.718725 139723137873664 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.08529923111200333, loss=0.3108319342136383
I0307 16:51:22.038580 139723129480960 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.058782611042261124, loss=0.36500611901283264
I0307 16:51:30.355114 139723137873664 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.13295841217041016, loss=0.27950718998908997
I0307 16:51:38.646453 139723129480960 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.096836619079113, loss=0.29873496294021606
I0307 16:51:46.934270 139723137873664 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.13827265799045563, loss=0.330148309469223
I0307 16:51:55.216922 139723129480960 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.06916911900043488, loss=0.27738651633262634
I0307 16:52:02.687212 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:52:03.979752 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:52:05.275306 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:52:06.572675 139874142557376 submission_runner.py:469] Time since start: 1671.79s, 	Step: 16291, 	{'train/ssim': 0.7427377019609723, 'train/loss': 0.2730858496257237, 'validation/ssim': 0.7225748468934651, 'validation/loss': 0.2872575664359261, 'validation/num_examples': 3554, 'test/ssim': 0.7399480330215024, 'test/loss': 0.2887111515747173, 'test/num_examples': 3581, 'score': 1481.672688961029, 'total_duration': 1671.7901425361633, 'accumulated_submission_time': 1481.672688961029, 'accumulated_eval_time': 184.7398931980133, 'accumulated_logging_time': 4.507124423980713}
I0307 16:52:06.584432 139723137873664 logging_writer.py:48] [16291] accumulated_eval_time=184.74, accumulated_logging_time=4.50712, accumulated_submission_time=1481.67, global_step=16291, preemption_count=0, score=1481.67, test/loss=0.288711, test/num_examples=3581, test/ssim=0.739948, total_duration=1671.79, train/loss=0.273086, train/ssim=0.742738, validation/loss=0.287258, validation/num_examples=3554, validation/ssim=0.722575
I0307 16:52:07.430142 139723129480960 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1988803744316101, loss=0.2681548297405243
I0307 16:52:15.734025 139723137873664 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.07093380391597748, loss=0.34677186608314514
I0307 16:52:24.025881 139723129480960 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.2244161069393158, loss=0.2470165193080902
I0307 16:52:32.350049 139723137873664 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.0833701491355896, loss=0.3058353066444397
I0307 16:52:40.625720 139723129480960 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.19786430895328522, loss=0.2977464199066162
I0307 16:52:48.936191 139723137873664 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.13566596806049347, loss=0.25971975922584534
I0307 16:52:57.229376 139723129480960 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.0735270231962204, loss=0.33835041522979736
I0307 16:53:05.516099 139723137873664 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.06577160954475403, loss=0.19375644624233246
I0307 16:53:13.826963 139723129480960 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.06093408539891243, loss=0.22130447626113892
I0307 16:53:22.108016 139723137873664 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.10749070346355438, loss=0.2694263756275177
I0307 16:53:26.599543 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:53:27.894224 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:53:29.193159 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:53:30.491677 139874142557376 submission_runner.py:469] Time since start: 1755.71s, 	Step: 17255, 	{'train/ssim': 0.742950303213937, 'train/loss': 0.27289388860974995, 'validation/ssim': 0.7226799496386114, 'validation/loss': 0.2871784474249965, 'validation/num_examples': 3554, 'test/ssim': 0.7400392533946524, 'test/loss': 0.28864092961332377, 'test/num_examples': 3581, 'score': 1561.6386981010437, 'total_duration': 1755.7091271877289, 'accumulated_submission_time': 1561.6386981010437, 'accumulated_eval_time': 188.6319715976715, 'accumulated_logging_time': 4.526903390884399}
I0307 16:53:30.505968 139723129480960 logging_writer.py:48] [17255] accumulated_eval_time=188.632, accumulated_logging_time=4.5269, accumulated_submission_time=1561.64, global_step=17255, preemption_count=0, score=1561.64, test/loss=0.288641, test/num_examples=3581, test/ssim=0.740039, total_duration=1755.71, train/loss=0.272894, train/ssim=0.74295, validation/loss=0.287178, validation/num_examples=3554, validation/ssim=0.72268
I0307 16:53:34.332533 139723137873664 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.08075916767120361, loss=0.3299276828765869
I0307 16:53:42.622653 139723129480960 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.15014958381652832, loss=0.2597186267375946
I0307 16:53:50.913319 139723137873664 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.20310306549072266, loss=0.2944367527961731
I0307 16:53:59.216896 139723129480960 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2455224096775055, loss=0.2180311679840088
I0307 16:54:07.537293 139723137873664 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.08117012679576874, loss=0.35114791989326477
I0307 16:54:15.842532 139723129480960 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.15268714725971222, loss=0.35079726576805115
I0307 16:54:24.151899 139723137873664 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.13278117775917053, loss=0.2768639028072357
I0307 16:54:32.475352 139723129480960 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.1836046725511551, loss=0.2939421236515045
I0307 16:54:40.791659 139723137873664 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.1454310566186905, loss=0.23093320429325104
I0307 16:54:49.114907 139723129480960 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1053585559129715, loss=0.21999390423297882
I0307 16:54:50.538517 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:54:51.829939 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:54:53.126015 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:54:54.423187 139874142557376 submission_runner.py:469] Time since start: 1839.64s, 	Step: 18218, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1641.6214306354523, 'total_duration': 1839.6406576633453, 'accumulated_submission_time': 1641.6214306354523, 'accumulated_eval_time': 192.5165994167328, 'accumulated_logging_time': 4.5494184494018555}
I0307 16:54:54.435532 139723137873664 logging_writer.py:48] [18218] accumulated_eval_time=192.517, accumulated_logging_time=4.54942, accumulated_submission_time=1641.62, global_step=18218, preemption_count=0, score=1641.62, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=1839.64, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 16:55:01.358194 139723129480960 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.10590279847383499, loss=0.2751108407974243
I0307 16:55:09.645611 139723137873664 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.15190573036670685, loss=0.25752389430999756
I0307 16:55:17.952593 139723129480960 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.08410394936800003, loss=0.24397140741348267
I0307 16:55:26.267025 139723137873664 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.15344050526618958, loss=0.2924453914165497
I0307 16:55:34.595590 139723129480960 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.26949605345726013, loss=0.23307086527347565
I0307 16:55:42.900111 139723137873664 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.11125416308641434, loss=0.3205697536468506
I0307 16:55:51.199609 139723129480960 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.10003890097141266, loss=0.24140149354934692
I0307 16:55:59.515181 139723137873664 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.07960161566734314, loss=0.2763971984386444
I0307 16:56:07.825439 139723129480960 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.09234833717346191, loss=0.34807389974594116
I0307 16:56:14.481254 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:56:15.775639 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:56:17.075021 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:56:18.373951 139874142557376 submission_runner.py:469] Time since start: 1923.59s, 	Step: 19181, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1721.6177320480347, 'total_duration': 1923.5913984775543, 'accumulated_submission_time': 1721.6177320480347, 'accumulated_eval_time': 196.40924048423767, 'accumulated_logging_time': 4.569700241088867}
I0307 16:56:18.389698 139723137873664 logging_writer.py:48] [19181] accumulated_eval_time=196.409, accumulated_logging_time=4.5697, accumulated_submission_time=1721.62, global_step=19181, preemption_count=0, score=1721.62, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=1923.59, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 16:56:20.055953 139723129480960 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.21860894560813904, loss=0.2192075103521347
I0307 16:56:28.363165 139723137873664 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.09145769476890564, loss=0.24175074696540833
I0307 16:56:36.667909 139723129480960 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.2392456829547882, loss=0.38600677251815796
I0307 16:56:44.948475 139723137873664 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.39237186312675476, loss=0.3165737986564636
I0307 16:56:53.208938 139723129480960 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1726759821176529, loss=0.2476634830236435
I0307 16:57:01.521970 139723137873664 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.11180901527404785, loss=0.24383430182933807
I0307 16:57:09.804471 139723129480960 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.10817334055900574, loss=0.30667582154273987
I0307 16:57:18.094023 139723137873664 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.13433806598186493, loss=0.296840637922287
I0307 16:57:26.395683 139723129480960 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.23043830692768097, loss=0.3009195625782013
I0307 16:57:34.712505 139723137873664 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.09138590842485428, loss=0.28156065940856934
I0307 16:57:38.450500 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:57:39.743788 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:57:41.038835 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:57:42.335494 139874142557376 submission_runner.py:469] Time since start: 2007.55s, 	Step: 20146, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1801.6282346248627, 'total_duration': 2007.5529630184174, 'accumulated_submission_time': 1801.6282346248627, 'accumulated_eval_time': 200.29419326782227, 'accumulated_logging_time': 4.592954874038696}
I0307 16:57:42.347795 139723129480960 logging_writer.py:48] [20146] accumulated_eval_time=200.294, accumulated_logging_time=4.59295, accumulated_submission_time=1801.63, global_step=20146, preemption_count=0, score=1801.63, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2007.55, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 16:57:46.930631 139723137873664 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.09859514236450195, loss=0.34319812059402466
I0307 16:57:55.239382 139723129480960 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.11927276849746704, loss=0.26874032616615295
I0307 16:58:03.517264 139723137873664 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.12545748054981232, loss=0.2877972424030304
I0307 16:58:11.827120 139723129480960 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.06393355876207352, loss=0.25943875312805176
I0307 16:58:20.111834 139723137873664 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3570011258125305, loss=0.24210256338119507
I0307 16:58:28.396907 139723129480960 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1700698733329773, loss=0.27524489164352417
I0307 16:58:36.693428 139723137873664 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.07165335863828659, loss=0.23535111546516418
I0307 16:58:45.011198 139723129480960 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.19384898245334625, loss=0.2850951552391052
I0307 16:58:53.303568 139723137873664 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.12912219762802124, loss=0.39067935943603516
I0307 16:59:01.603038 139723129480960 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.14319884777069092, loss=0.28199315071105957
I0307 16:59:02.349504 139874142557376 spec.py:321] Evaluating on the training split.
I0307 16:59:03.643907 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 16:59:04.939191 139874142557376 spec.py:349] Evaluating on the test split.
I0307 16:59:06.236825 139874142557376 submission_runner.py:469] Time since start: 2091.45s, 	Step: 21110, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1881.581708431244, 'total_duration': 2091.4542939662933, 'accumulated_submission_time': 1881.581708431244, 'accumulated_eval_time': 204.18146800994873, 'accumulated_logging_time': 4.612586259841919}
I0307 16:59:06.249861 139723137873664 logging_writer.py:48] [21110] accumulated_eval_time=204.181, accumulated_logging_time=4.61259, accumulated_submission_time=1881.58, global_step=21110, preemption_count=0, score=1881.58, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2091.45, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 16:59:13.836007 139723129480960 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.09447511285543442, loss=0.3526478707790375
I0307 16:59:22.141754 139723137873664 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.1872178167104721, loss=0.2908715605735779
I0307 16:59:30.458593 139723129480960 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.11510869860649109, loss=0.2714858949184418
I0307 16:59:38.764184 139723137873664 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.08558877557516098, loss=0.320869505405426
I0307 16:59:47.074531 139723129480960 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.09778957068920135, loss=0.3396297097206116
I0307 16:59:55.391760 139723137873664 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.09149619191884995, loss=0.24546919763088226
I0307 17:00:03.693496 139723129480960 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.387645959854126, loss=0.268693745136261
I0307 17:00:11.984477 139723137873664 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.1411113739013672, loss=0.25083282589912415
I0307 17:00:20.296360 139723129480960 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.11775361001491547, loss=0.28506627678871155
I0307 17:00:26.276024 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:00:27.568958 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:00:28.866392 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:00:30.166400 139874142557376 submission_runner.py:469] Time since start: 2175.38s, 	Step: 22073, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1961.558806180954, 'total_duration': 2175.3838670253754, 'accumulated_submission_time': 1961.558806180954, 'accumulated_eval_time': 208.07179713249207, 'accumulated_logging_time': 4.633031606674194}
I0307 17:00:30.179138 139723137873664 logging_writer.py:48] [22073] accumulated_eval_time=208.072, accumulated_logging_time=4.63303, accumulated_submission_time=1961.56, global_step=22073, preemption_count=0, score=1961.56, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2175.38, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:00:32.517440 139723129480960 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.24710297584533691, loss=0.3405851125717163
I0307 17:00:40.807755 139723137873664 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.16911368072032928, loss=0.23841047286987305
I0307 17:00:49.137974 139723129480960 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.06579434871673584, loss=0.2714900076389313
I0307 17:00:57.464357 139723137873664 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.07031320035457611, loss=0.36887359619140625
I0307 17:01:05.773573 139723129480960 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.14809438586235046, loss=0.307333379983902
I0307 17:01:14.086903 139723137873664 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.2243027240037918, loss=0.29060453176498413
I0307 17:01:22.410956 139723129480960 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.13537414371967316, loss=0.32340022921562195
I0307 17:01:30.704357 139723137873664 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.2396167665719986, loss=0.28879496455192566
I0307 17:01:39.006876 139723129480960 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.07761400192975998, loss=0.26089975237846375
I0307 17:01:47.319571 139723137873664 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.12555161118507385, loss=0.31716662645339966
I0307 17:01:50.224090 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:01:51.516374 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:01:52.811968 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:01:54.108959 139874142557376 submission_runner.py:469] Time since start: 2259.33s, 	Step: 23036, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2041.5548367500305, 'total_duration': 2259.3264276981354, 'accumulated_submission_time': 2041.5548367500305, 'accumulated_eval_time': 211.9566216468811, 'accumulated_logging_time': 4.653255462646484}
I0307 17:01:54.121424 139723129480960 logging_writer.py:48] [23036] accumulated_eval_time=211.957, accumulated_logging_time=4.65326, accumulated_submission_time=2041.55, global_step=23036, preemption_count=0, score=2041.55, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2259.33, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:01:59.535344 139723137873664 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.1804502159357071, loss=0.3038889467716217
I0307 17:02:07.846704 139723129480960 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.08440214395523071, loss=0.31654390692710876
I0307 17:02:16.161432 139723137873664 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.07821998000144958, loss=0.3609154224395752
I0307 17:02:24.484460 139723129480960 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.18408465385437012, loss=0.30647188425064087
I0307 17:02:32.806612 139723137873664 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.11373788863420486, loss=0.2641807794570923
I0307 17:02:41.131244 139723129480960 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.12559612095355988, loss=0.22958645224571228
I0307 17:02:49.434706 139723137873664 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.1554604172706604, loss=0.30687397718429565
I0307 17:02:57.745526 139723129480960 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.21904802322387695, loss=0.3071722388267517
I0307 17:03:06.055319 139723137873664 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.18831445276737213, loss=0.2921549379825592
I0307 17:03:14.157156 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:03:15.451070 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:03:16.746672 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:03:18.047343 139874142557376 submission_runner.py:469] Time since start: 2343.26s, 	Step: 23999, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2121.5423896312714, 'total_duration': 2343.264815092087, 'accumulated_submission_time': 2121.5423896312714, 'accumulated_eval_time': 215.84676694869995, 'accumulated_logging_time': 4.672826528549194}
I0307 17:03:18.059926 139723129480960 logging_writer.py:48] [23999] accumulated_eval_time=215.847, accumulated_logging_time=4.67283, accumulated_submission_time=2121.54, global_step=23999, preemption_count=0, score=2121.54, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2343.26, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:03:18.237853 139723137873664 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2542422115802765, loss=0.24960947036743164
I0307 17:03:26.527964 139723129480960 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.07986941188573837, loss=0.22027148306369781
I0307 17:03:34.833877 139723137873664 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.17520897090435028, loss=0.32792308926582336
I0307 17:03:43.137396 139723129480960 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.10178323835134506, loss=0.31369563937187195
I0307 17:03:51.430140 139723137873664 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.12592221796512604, loss=0.3368026912212372
I0307 17:03:59.706063 139723129480960 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.13822896778583527, loss=0.3181585669517517
I0307 17:04:08.007540 139723137873664 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.06222173199057579, loss=0.23497894406318665
I0307 17:04:16.311481 139723129480960 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.22787579894065857, loss=0.2626626491546631
I0307 17:04:24.593802 139723137873664 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.09833087772130966, loss=0.24230965971946716
I0307 17:04:32.906251 139723129480960 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.13674025237560272, loss=0.3193100690841675
I0307 17:04:38.048702 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:04:39.342164 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:04:40.638366 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:04:41.937059 139874142557376 submission_runner.py:469] Time since start: 2427.15s, 	Step: 24963, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2201.4822473526, 'total_duration': 2427.154510498047, 'accumulated_submission_time': 2201.4822473526, 'accumulated_eval_time': 219.73506712913513, 'accumulated_logging_time': 4.6926939487457275}
I0307 17:04:41.950765 139723137873664 logging_writer.py:48] [24963] accumulated_eval_time=219.735, accumulated_logging_time=4.69269, accumulated_submission_time=2201.48, global_step=24963, preemption_count=0, score=2201.48, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2427.15, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:04:45.126690 139723129480960 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.10040909051895142, loss=0.2754499614238739
I0307 17:04:53.430794 139723137873664 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.05830542370676994, loss=0.2860617935657501
I0307 17:05:01.738757 139723129480960 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.1224641427397728, loss=0.2493140995502472
I0307 17:05:10.045482 139723137873664 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.1527528613805771, loss=0.4000691771507263
I0307 17:05:18.349550 139723129480960 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.12642505764961243, loss=0.26687702536582947
I0307 17:05:26.622692 139723137873664 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.10179190337657928, loss=0.27361977100372314
I0307 17:05:34.903066 139723129480960 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.12135664373636246, loss=0.2561590075492859
I0307 17:05:43.157969 139723137873664 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.19860686361789703, loss=0.23313726484775543
I0307 17:05:51.438807 139723129480960 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.08731231838464737, loss=0.290905624628067
I0307 17:05:59.730157 139723137873664 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.09753644466400146, loss=0.346134752035141
I0307 17:06:01.984161 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:06:03.280972 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:06:04.578423 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:06:05.875194 139874142557376 submission_runner.py:469] Time since start: 2511.09s, 	Step: 25928, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2281.467113018036, 'total_duration': 2511.092609643936, 'accumulated_submission_time': 2281.467113018036, 'accumulated_eval_time': 223.62600779533386, 'accumulated_logging_time': 4.713877201080322}
I0307 17:06:05.891059 139723129480960 logging_writer.py:48] [25928] accumulated_eval_time=223.626, accumulated_logging_time=4.71388, accumulated_submission_time=2281.47, global_step=25928, preemption_count=0, score=2281.47, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2511.09, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:06:11.966408 139723137873664 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0706474781036377, loss=0.2775002121925354
I0307 17:06:20.266879 139723129480960 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.13577212393283844, loss=0.3786081075668335
I0307 17:06:28.599548 139723137873664 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.22655291855335236, loss=0.2639978229999542
I0307 17:06:36.884104 139723129480960 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.09401363879442215, loss=0.2228546291589737
I0307 17:06:45.156290 139723137873664 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.11557140946388245, loss=0.367349773645401
I0307 17:06:53.456248 139723129480960 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.13363876938819885, loss=0.2416956126689911
I0307 17:07:01.739815 139723137873664 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.23949319124221802, loss=0.23797428607940674
I0307 17:07:10.029160 139723129480960 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.10890614241361618, loss=0.2906814515590668
I0307 17:07:18.300930 139723137873664 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.11017470061779022, loss=0.34560853242874146
I0307 17:07:25.940529 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:07:27.233643 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:07:28.529637 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:07:29.825489 139874142557376 submission_runner.py:469] Time since start: 2595.04s, 	Step: 26893, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2361.4665875434875, 'total_duration': 2595.0429589748383, 'accumulated_submission_time': 2361.4665875434875, 'accumulated_eval_time': 227.5109236240387, 'accumulated_logging_time': 4.737409353256226}
I0307 17:07:29.838862 139723129480960 logging_writer.py:48] [26893] accumulated_eval_time=227.511, accumulated_logging_time=4.73741, accumulated_submission_time=2361.47, global_step=26893, preemption_count=0, score=2361.47, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2595.04, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:07:30.515055 139723137873664 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.08224737644195557, loss=0.3341204822063446
I0307 17:07:38.820055 139723129480960 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.05617927759885788, loss=0.2738736569881439
I0307 17:07:47.110087 139723137873664 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.16390803456306458, loss=0.24276891350746155
I0307 17:07:55.381177 139723129480960 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.08985964953899384, loss=0.23054078221321106
I0307 17:08:03.674453 139723137873664 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.18111851811408997, loss=0.2510344684123993
I0307 17:08:11.958017 139723129480960 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.09752257913351059, loss=0.24126997590065002
I0307 17:08:20.248420 139723137873664 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.16389086842536926, loss=0.34654420614242554
I0307 17:08:28.539643 139723129480960 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.11050111800432205, loss=0.30192282795906067
I0307 17:08:36.825483 139723137873664 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.16203773021697998, loss=0.27445751428604126
I0307 17:08:45.100376 139723129480960 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.0937909260392189, loss=0.29375341534614563
I0307 17:08:49.837498 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:08:51.133068 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:08:52.430441 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:08:53.732015 139874142557376 submission_runner.py:469] Time since start: 2678.95s, 	Step: 27858, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2441.4167125225067, 'total_duration': 2678.9494805336, 'accumulated_submission_time': 2441.4167125225067, 'accumulated_eval_time': 231.4053921699524, 'accumulated_logging_time': 4.7578887939453125}
I0307 17:08:53.745611 139723137873664 logging_writer.py:48] [27858] accumulated_eval_time=231.405, accumulated_logging_time=4.75789, accumulated_submission_time=2441.42, global_step=27858, preemption_count=0, score=2441.42, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2678.95, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:08:57.324225 139723129480960 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.1231035441160202, loss=0.26350319385528564
I0307 17:09:05.609894 139723137873664 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.2393103837966919, loss=0.22063784301280975
I0307 17:09:13.914571 139723129480960 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.10647381097078323, loss=0.2688344717025757
I0307 17:09:22.212141 139723137873664 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.12104545533657074, loss=0.24166660010814667
I0307 17:09:30.493604 139723129480960 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.07621091604232788, loss=0.2550574243068695
I0307 17:09:38.800043 139723137873664 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.09791243821382523, loss=0.29435625672340393
I0307 17:09:47.092521 139723129480960 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.08485531806945801, loss=0.27280816435813904
I0307 17:09:55.389936 139723137873664 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.0878131240606308, loss=0.310371071100235
I0307 17:10:03.680396 139723129480960 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.11315693706274033, loss=0.27819669246673584
I0307 17:10:11.963111 139723137873664 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1387326568365097, loss=0.30174827575683594
I0307 17:10:13.786910 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:10:15.083209 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:10:16.381080 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:10:17.677972 139874142557376 submission_runner.py:469] Time since start: 2762.90s, 	Step: 28823, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2521.4087913036346, 'total_duration': 2762.8954396247864, 'accumulated_submission_time': 2521.4087913036346, 'accumulated_eval_time': 235.2964050769806, 'accumulated_logging_time': 4.7788660526275635}
I0307 17:10:17.691964 139723129480960 logging_writer.py:48] [28823] accumulated_eval_time=235.296, accumulated_logging_time=4.77887, accumulated_submission_time=2521.41, global_step=28823, preemption_count=0, score=2521.41, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2762.9, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:10:24.186020 139723137873664 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.05910501256585121, loss=0.3235165476799011
I0307 17:10:32.496235 139723129480960 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.08226441591978073, loss=0.25531575083732605
I0307 17:10:40.781625 139723137873664 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.0730087161064148, loss=0.3402518630027771
I0307 17:10:49.082881 139723129480960 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.10441523790359497, loss=0.386601060628891
I0307 17:10:57.401195 139723137873664 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1350013017654419, loss=0.27795663475990295
I0307 17:11:05.691305 139723129480960 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.11441154032945633, loss=0.23073694109916687
I0307 17:11:13.987833 139723137873664 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.09510426223278046, loss=0.37108781933784485
I0307 17:11:22.294386 139723129480960 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.12813939154148102, loss=0.4056013226509094
I0307 17:11:30.582067 139723137873664 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.13877956569194794, loss=0.30163395404815674
I0307 17:11:37.721822 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:11:39.015418 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:11:40.312239 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:11:41.610805 139874142557376 submission_runner.py:469] Time since start: 2846.83s, 	Step: 29787, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2601.389988899231, 'total_duration': 2846.82825422287, 'accumulated_submission_time': 2601.389988899231, 'accumulated_eval_time': 239.18532419204712, 'accumulated_logging_time': 4.800422191619873}
I0307 17:11:41.627121 139723129480960 logging_writer.py:48] [29787] accumulated_eval_time=239.185, accumulated_logging_time=4.80042, accumulated_submission_time=2601.39, global_step=29787, preemption_count=0, score=2601.39, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2846.83, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:11:42.794869 139723137873664 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.07822416722774506, loss=0.29417693614959717
I0307 17:11:51.107426 139723129480960 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.10452853888273239, loss=0.35071051120758057
I0307 17:11:59.383531 139723137873664 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.08271442353725433, loss=0.2975822985172272
I0307 17:12:07.703843 139723129480960 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19765104353427887, loss=0.2575724124908447
I0307 17:12:15.989615 139723137873664 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.06876817345619202, loss=0.30073705315589905
I0307 17:12:24.285813 139723129480960 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.08587371557950974, loss=0.3505282700061798
I0307 17:12:32.580071 139723137873664 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.17081859707832336, loss=0.26578131318092346
I0307 17:12:40.887266 139723129480960 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.192287415266037, loss=0.19891059398651123
I0307 17:12:49.192682 139723137873664 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.11720103770494461, loss=0.2399633377790451
I0307 17:12:57.485408 139723129480960 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.06718950718641281, loss=0.2648436725139618
I0307 17:13:01.629288 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:13:02.922834 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:13:04.218911 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:13:05.516362 139874142557376 submission_runner.py:469] Time since start: 2930.73s, 	Step: 30751, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2681.3422050476074, 'total_duration': 2930.7338349819183, 'accumulated_submission_time': 2681.3422050476074, 'accumulated_eval_time': 243.07236647605896, 'accumulated_logging_time': 4.824802875518799}
I0307 17:13:05.529809 139723137873664 logging_writer.py:48] [30751] accumulated_eval_time=243.072, accumulated_logging_time=4.8248, accumulated_submission_time=2681.34, global_step=30751, preemption_count=0, score=2681.34, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=2930.73, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:13:09.690416 139723129480960 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.09677055478096008, loss=0.3205735683441162
I0307 17:13:17.991967 139723137873664 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.07496193051338196, loss=0.33259907364845276
I0307 17:13:26.285113 139723129480960 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.0755641981959343, loss=0.32745102047920227
I0307 17:13:34.586088 139723137873664 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.1064724326133728, loss=0.24124443531036377
I0307 17:13:42.894324 139723129480960 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.1533685177564621, loss=0.21320150792598724
I0307 17:13:51.182287 139723137873664 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.24184316396713257, loss=0.26084256172180176
I0307 17:13:59.459087 139723129480960 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.21570773422718048, loss=0.3006470203399658
I0307 17:14:07.761252 139723137873664 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.071928970515728, loss=0.37004637718200684
I0307 17:14:16.063088 139723129480960 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.05899978056550026, loss=0.28468987345695496
I0307 17:14:24.366340 139723137873664 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.11491423845291138, loss=0.2909380793571472
I0307 17:14:25.536111 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:14:26.829858 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:14:28.127018 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:14:29.426906 139874142557376 submission_runner.py:469] Time since start: 3014.64s, 	Step: 31715, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2761.299434900284, 'total_duration': 3014.6443660259247, 'accumulated_submission_time': 2761.299434900284, 'accumulated_eval_time': 246.9631052017212, 'accumulated_logging_time': 4.845642328262329}
I0307 17:14:29.440849 139723129480960 logging_writer.py:48] [31715] accumulated_eval_time=246.963, accumulated_logging_time=4.84564, accumulated_submission_time=2761.3, global_step=31715, preemption_count=0, score=2761.3, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3014.64, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:14:36.596213 139723137873664 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.13990509510040283, loss=0.35079699754714966
I0307 17:14:44.874019 139723129480960 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.11280118674039841, loss=0.3077227473258972
I0307 17:14:53.168291 139723137873664 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.08603251725435257, loss=0.23597019910812378
I0307 17:15:01.466633 139723129480960 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.07892060279846191, loss=0.26317259669303894
I0307 17:15:09.761292 139723137873664 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.07410930842161179, loss=0.3242027759552002
I0307 17:15:18.077328 139723129480960 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.12160734087228775, loss=0.2521175742149353
I0307 17:15:26.395929 139723137873664 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.09640452265739441, loss=0.27785900235176086
I0307 17:15:34.683152 139723129480960 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.11334656924009323, loss=0.292182058095932
I0307 17:15:42.975270 139723137873664 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.10834222286939621, loss=0.2499271184206009
I0307 17:15:49.440147 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:15:50.733660 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:15:52.029929 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:15:53.328866 139874142557376 submission_runner.py:469] Time since start: 3098.55s, 	Step: 32679, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2841.2503139972687, 'total_duration': 3098.5463168621063, 'accumulated_submission_time': 2841.2503139972687, 'accumulated_eval_time': 250.8517611026764, 'accumulated_logging_time': 4.867064714431763}
I0307 17:15:53.345861 139723129480960 logging_writer.py:48] [32679] accumulated_eval_time=250.852, accumulated_logging_time=4.86706, accumulated_submission_time=2841.25, global_step=32679, preemption_count=0, score=2841.25, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3098.55, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:15:55.180422 139723137873664 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.08275672048330307, loss=0.31689518690109253
I0307 17:16:03.486067 139723129480960 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.12004830688238144, loss=0.2179178148508072
I0307 17:16:11.781002 139723137873664 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05875063315033913, loss=0.35067862272262573
I0307 17:16:20.090188 139723129480960 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.2106064409017563, loss=0.29297369718551636
I0307 17:16:28.368006 139723137873664 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.15509502589702606, loss=0.27112847566604614
I0307 17:16:36.656466 139723129480960 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.13007612526416779, loss=0.3216736912727356
I0307 17:16:44.949584 139723137873664 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.1906442493200302, loss=0.2444039136171341
I0307 17:16:53.256435 139723129480960 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.12033060193061829, loss=0.3513469696044922
I0307 17:17:01.572521 139723137873664 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.11716096848249435, loss=0.31427082419395447
I0307 17:17:09.876025 139723129480960 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.23653282225131989, loss=0.23639699816703796
I0307 17:17:13.361575 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:17:14.656044 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:17:15.952193 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:17:17.253008 139874142557376 submission_runner.py:469] Time since start: 3182.47s, 	Step: 33643, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2921.216874361038, 'total_duration': 3182.4704785346985, 'accumulated_submission_time': 2921.216874361038, 'accumulated_eval_time': 254.74315071105957, 'accumulated_logging_time': 4.892060995101929}
I0307 17:17:17.268009 139723137873664 logging_writer.py:48] [33643] accumulated_eval_time=254.743, accumulated_logging_time=4.89206, accumulated_submission_time=2921.22, global_step=33643, preemption_count=0, score=2921.22, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3182.47, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:17:22.101474 139723129480960 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.15502578020095825, loss=0.29989752173423767
I0307 17:17:30.395093 139723137873664 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.20016181468963623, loss=0.3370499014854431
I0307 17:17:38.706970 139723129480960 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.21614962816238403, loss=0.18893693387508392
I0307 17:17:47.026808 139723137873664 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1371295303106308, loss=0.2985861301422119
I0307 17:17:55.308531 139723129480960 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.10161805152893066, loss=0.33226341009140015
I0307 17:18:03.592486 139723137873664 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.07616359740495682, loss=0.25531405210494995
I0307 17:18:11.868306 139723129480960 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.28018516302108765, loss=0.30201998353004456
I0307 17:18:20.163823 139723137873664 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.05587922781705856, loss=0.26178714632987976
I0307 17:18:28.465716 139723129480960 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.11954127997159958, loss=0.28041526675224304
I0307 17:18:36.755114 139723137873664 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.16175445914268494, loss=0.25182539224624634
I0307 17:18:37.259173 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:18:38.552976 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:18:39.849781 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:18:41.150142 139874142557376 submission_runner.py:469] Time since start: 3266.37s, 	Step: 34607, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3001.159170627594, 'total_duration': 3266.367614507675, 'accumulated_submission_time': 3001.159170627594, 'accumulated_eval_time': 258.6340937614441, 'accumulated_logging_time': 4.9145026206970215}
I0307 17:18:41.164008 139723129480960 logging_writer.py:48] [34607] accumulated_eval_time=258.634, accumulated_logging_time=4.9145, accumulated_submission_time=3001.16, global_step=34607, preemption_count=0, score=3001.16, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3266.37, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:18:48.962418 139723137873664 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.12514352798461914, loss=0.3114813268184662
I0307 17:18:57.265894 139723129480960 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.07468428462743759, loss=0.21042092144489288
I0307 17:19:05.546489 139723137873664 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1844530552625656, loss=0.26433587074279785
I0307 17:19:13.859084 139723129480960 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.14157406985759735, loss=0.2983491122722626
I0307 17:19:22.161253 139723137873664 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.10134494304656982, loss=0.30980536341667175
I0307 17:19:30.460916 139723129480960 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.10107321292161942, loss=0.30765190720558167
I0307 17:19:38.763227 139723137873664 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.11001484841108322, loss=0.3314613699913025
I0307 17:19:47.056803 139723129480960 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.35589057207107544, loss=0.3260959982872009
I0307 17:19:55.357609 139723137873664 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.18625633418560028, loss=0.3044569790363312
I0307 17:20:01.166770 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:20:02.459778 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:20:03.757622 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:20:05.058383 139874142557376 submission_runner.py:469] Time since start: 3350.28s, 	Step: 35571, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3081.1128964424133, 'total_duration': 3350.2758524417877, 'accumulated_submission_time': 3081.1128964424133, 'accumulated_eval_time': 262.5256586074829, 'accumulated_logging_time': 4.93588924407959}
I0307 17:20:05.072835 139723129480960 logging_writer.py:48] [35571] accumulated_eval_time=262.526, accumulated_logging_time=4.93589, accumulated_submission_time=3081.11, global_step=35571, preemption_count=0, score=3081.11, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3350.28, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:20:07.587245 139723137873664 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.16956323385238647, loss=0.2703750729560852
I0307 17:20:15.861012 139723129480960 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.1297159641981125, loss=0.26931077241897583
I0307 17:20:24.152674 139723137873664 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.10396336019039154, loss=0.26449054479599
I0307 17:20:32.445851 139723129480960 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1403694897890091, loss=0.3260200619697571
I0307 17:20:40.741778 139723137873664 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.10420823842287064, loss=0.28087857365608215
I0307 17:20:49.042929 139723129480960 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.15262258052825928, loss=0.2554945647716522
I0307 17:20:57.345383 139723137873664 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.08400101959705353, loss=0.2457902431488037
I0307 17:21:05.621905 139723129480960 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.09427500516176224, loss=0.2801501452922821
I0307 17:21:13.923555 139723137873664 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.08773928135633469, loss=0.3250967860221863
I0307 17:21:22.207579 139723129480960 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.1242687851190567, loss=0.3279959559440613
I0307 17:21:25.108746 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:21:26.403495 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:21:27.701726 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:21:28.999211 139874142557376 submission_runner.py:469] Time since start: 3434.22s, 	Step: 36536, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3161.099362373352, 'total_duration': 3434.2166855335236, 'accumulated_submission_time': 3161.099362373352, 'accumulated_eval_time': 266.4160840511322, 'accumulated_logging_time': 4.959021806716919}
I0307 17:21:29.013234 139723137873664 logging_writer.py:48] [36536] accumulated_eval_time=266.416, accumulated_logging_time=4.95902, accumulated_submission_time=3161.1, global_step=36536, preemption_count=0, score=3161.1, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3434.22, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:21:34.419140 139723129480960 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.18420179188251495, loss=0.26467782258987427
I0307 17:21:42.729820 139723137873664 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.12378643453121185, loss=0.27534329891204834
I0307 17:21:51.026769 139723129480960 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.09826638549566269, loss=0.34681227803230286
I0307 17:21:59.351668 139723137873664 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.10156592726707458, loss=0.20520566403865814
I0307 17:22:07.638076 139723129480960 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.1850588023662567, loss=0.31178420782089233
I0307 17:22:15.934157 139723137873664 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.11144295334815979, loss=0.21952340006828308
I0307 17:22:24.253213 139723129480960 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.10496398061513901, loss=0.2749917507171631
I0307 17:22:32.545648 139723137873664 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.08362717181444168, loss=0.3587912917137146
I0307 17:22:40.824429 139723129480960 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.19127312302589417, loss=0.31773242354393005
I0307 17:22:49.043114 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:22:50.335782 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:22:51.632618 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:22:52.930513 139874142557376 submission_runner.py:469] Time since start: 3518.15s, 	Step: 37500, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3241.08052778244, 'total_duration': 3518.147956609726, 'accumulated_submission_time': 3241.08052778244, 'accumulated_eval_time': 270.30341243743896, 'accumulated_logging_time': 4.980231046676636}
I0307 17:22:52.945560 139723137873664 logging_writer.py:48] [37500] accumulated_eval_time=270.303, accumulated_logging_time=4.98023, accumulated_submission_time=3241.08, global_step=37500, preemption_count=0, score=3241.08, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3518.15, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:22:53.038439 139723129480960 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.09587068110704422, loss=0.359747976064682
I0307 17:23:01.346840 139723137873664 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2150265872478485, loss=0.3507671654224396
I0307 17:23:09.667227 139723129480960 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.1437315046787262, loss=0.3231557309627533
I0307 17:23:17.980721 139723137873664 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.09942863881587982, loss=0.25438109040260315
I0307 17:23:26.283616 139723129480960 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.09805048257112503, loss=0.3366599380970001
I0307 17:23:34.592029 139723137873664 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.14403656125068665, loss=0.24813267588615417
I0307 17:23:42.882218 139723129480960 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3016684353351593, loss=0.24397525191307068
I0307 17:23:51.196767 139723137873664 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1389155089855194, loss=0.2427680790424347
I0307 17:23:59.492499 139723129480960 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07500215619802475, loss=0.3049759566783905
I0307 17:24:07.806247 139723137873664 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.13033033907413483, loss=0.28962886333465576
I0307 17:24:12.960183 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:24:14.254754 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:24:15.554413 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:24:16.854811 139874142557376 submission_runner.py:469] Time since start: 3602.07s, 	Step: 38463, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3321.0460953712463, 'total_duration': 3602.0722699165344, 'accumulated_submission_time': 3321.0460953712463, 'accumulated_eval_time': 274.1979868412018, 'accumulated_logging_time': 5.00262188911438}
I0307 17:24:16.869179 139723129480960 logging_writer.py:48] [38463] accumulated_eval_time=274.198, accumulated_logging_time=5.00262, accumulated_submission_time=3321.05, global_step=38463, preemption_count=0, score=3321.05, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3602.07, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:24:20.035183 139723137873664 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.11869556456804276, loss=0.35564738512039185
I0307 17:24:28.345569 139723129480960 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.12537451088428497, loss=0.29784244298934937
I0307 17:24:36.649072 139723137873664 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2600851058959961, loss=0.25535401701927185
I0307 17:24:44.968174 139723129480960 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.09631425887346268, loss=0.3133729100227356
I0307 17:24:53.292702 139723137873664 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.12169066071510315, loss=0.2956835627555847
I0307 17:25:01.585191 139723129480960 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.2525917887687683, loss=0.30524682998657227
I0307 17:25:09.856444 139723137873664 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.09012821316719055, loss=0.32146376371383667
I0307 17:25:18.166884 139723129480960 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.1288646161556244, loss=0.3562682867050171
I0307 17:25:26.452475 139723137873664 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.15527655184268951, loss=0.27848365902900696
I0307 17:25:34.740220 139723129480960 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.2090805023908615, loss=0.24177944660186768
I0307 17:25:36.896504 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:25:38.188848 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:25:39.482599 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:25:40.780858 139874142557376 submission_runner.py:469] Time since start: 3686.00s, 	Step: 39427, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3401.0251636505127, 'total_duration': 3685.998311281204, 'accumulated_submission_time': 3401.0251636505127, 'accumulated_eval_time': 278.0822787284851, 'accumulated_logging_time': 5.024211168289185}
I0307 17:25:40.798402 139723137873664 logging_writer.py:48] [39427] accumulated_eval_time=278.082, accumulated_logging_time=5.02421, accumulated_submission_time=3401.03, global_step=39427, preemption_count=0, score=3401.03, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3686, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:25:46.942810 139723129480960 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2506273686885834, loss=0.2571038007736206
I0307 17:25:55.256949 139723137873664 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.23996639251708984, loss=0.2731417417526245
I0307 17:26:03.557459 139723129480960 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.05635060742497444, loss=0.2943858504295349
I0307 17:26:11.861479 139723137873664 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.12927232682704926, loss=0.23498627543449402
I0307 17:26:20.183047 139723129480960 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.15803402662277222, loss=0.2658707797527313
I0307 17:26:28.495138 139723137873664 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.14546968042850494, loss=0.3215481638908386
I0307 17:26:36.807380 139723129480960 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07623136043548584, loss=0.3512542247772217
I0307 17:26:45.110903 139723137873664 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.0596049539744854, loss=0.24199610948562622
I0307 17:26:53.417606 139723129480960 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.13879764080047607, loss=0.33801689743995667
I0307 17:27:00.801558 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:27:02.093815 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:27:03.389445 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:27:04.687015 139874142557376 submission_runner.py:469] Time since start: 3769.90s, 	Step: 40390, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3480.9792766571045, 'total_duration': 3769.904486656189, 'accumulated_submission_time': 3480.9792766571045, 'accumulated_eval_time': 281.9676904678345, 'accumulated_logging_time': 5.049941062927246}
I0307 17:27:04.701822 139723137873664 logging_writer.py:48] [40390] accumulated_eval_time=281.968, accumulated_logging_time=5.04994, accumulated_submission_time=3480.98, global_step=40390, preemption_count=0, score=3480.98, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3769.9, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:27:05.625350 139723129480960 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.12624827027320862, loss=0.27036184072494507
I0307 17:27:13.925987 139723137873664 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0762009546160698, loss=0.34889885783195496
I0307 17:27:22.249668 139723129480960 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.09662703424692154, loss=0.2919187843799591
I0307 17:27:30.543959 139723137873664 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.15170958638191223, loss=0.24413815140724182
I0307 17:27:38.841369 139723129480960 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.15012717247009277, loss=0.35751888155937195
I0307 17:27:47.148693 139723137873664 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.1321450173854828, loss=0.3019373416900635
I0307 17:27:55.461492 139723129480960 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.24262462556362152, loss=0.2867772877216339
I0307 17:28:03.767830 139723137873664 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.1114620491862297, loss=0.2758522033691406
I0307 17:28:12.065629 139723129480960 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.06645210832357407, loss=0.2934064567089081
I0307 17:28:20.322932 139723137873664 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.2727040946483612, loss=0.29221296310424805
I0307 17:28:24.725288 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:28:26.018218 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:28:27.313993 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:28:28.611587 139874142557376 submission_runner.py:469] Time since start: 3853.83s, 	Step: 41354, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3560.952558040619, 'total_duration': 3853.8290553092957, 'accumulated_submission_time': 3560.952558040619, 'accumulated_eval_time': 285.8539435863495, 'accumulated_logging_time': 5.072075128555298}
I0307 17:28:28.627606 139723129480960 logging_writer.py:48] [41354] accumulated_eval_time=285.854, accumulated_logging_time=5.07208, accumulated_submission_time=3560.95, global_step=41354, preemption_count=0, score=3560.95, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3853.83, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:28:32.529975 139723137873664 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.17880137264728546, loss=0.2838357985019684
I0307 17:28:40.826937 139723129480960 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1446000188589096, loss=0.2899126708507538
I0307 17:28:49.123981 139723137873664 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.09086538851261139, loss=0.4331892728805542
I0307 17:28:57.431480 139723129480960 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.20247741043567657, loss=0.2181086242198944
I0307 17:29:05.718393 139723137873664 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.1527785062789917, loss=0.2601505517959595
I0307 17:29:14.015908 139723129480960 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.17470917105674744, loss=0.4810369908809662
I0307 17:29:22.344955 139723137873664 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.08164726942777634, loss=0.2936972975730896
I0307 17:29:30.638070 139723129480960 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.14897669851779938, loss=0.2770339846611023
I0307 17:29:38.984378 139723137873664 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.13404980301856995, loss=0.32707515358924866
I0307 17:29:47.267602 139723129480960 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.09389788657426834, loss=0.27739670872688293
I0307 17:29:48.672295 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:29:49.965307 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:29:51.260546 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:29:52.557927 139874142557376 submission_runner.py:469] Time since start: 3937.78s, 	Step: 42318, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3640.947919845581, 'total_duration': 3937.7753982543945, 'accumulated_submission_time': 3640.947919845581, 'accumulated_eval_time': 289.7395317554474, 'accumulated_logging_time': 5.095789432525635}
I0307 17:29:52.573829 139723137873664 logging_writer.py:48] [42318] accumulated_eval_time=289.74, accumulated_logging_time=5.09579, accumulated_submission_time=3640.95, global_step=42318, preemption_count=0, score=3640.95, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=3937.78, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:29:59.466392 139723129480960 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.09002046287059784, loss=0.294743150472641
I0307 17:30:07.779701 139723137873664 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.1248781681060791, loss=0.2506282925605774
I0307 17:30:16.104752 139723129480960 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.12437422573566437, loss=0.27449870109558105
I0307 17:30:24.414276 139723137873664 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.11682812124490738, loss=0.31960517168045044
I0307 17:30:32.707491 139723129480960 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.06399176269769669, loss=0.3208966851234436
I0307 17:30:40.991124 139723137873664 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.1014891043305397, loss=0.25652968883514404
I0307 17:30:49.291841 139723129480960 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.131319060921669, loss=0.2944292724132538
I0307 17:30:57.570943 139723137873664 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.05743676796555519, loss=0.29536172747612
I0307 17:31:05.839679 139723129480960 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1030416488647461, loss=0.2404853254556656
I0307 17:31:12.558133 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:31:13.853573 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:31:15.152487 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:31:16.448293 139874142557376 submission_runner.py:469] Time since start: 4021.67s, 	Step: 43282, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3720.88298535347, 'total_duration': 4021.6657605171204, 'accumulated_submission_time': 3720.88298535347, 'accumulated_eval_time': 293.62964630126953, 'accumulated_logging_time': 5.119950294494629}
I0307 17:31:16.464404 139723137873664 logging_writer.py:48] [43282] accumulated_eval_time=293.63, accumulated_logging_time=5.11995, accumulated_submission_time=3720.88, global_step=43282, preemption_count=0, score=3720.88, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4021.67, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:31:18.052064 139723129480960 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.14545825123786926, loss=0.3286725878715515
I0307 17:31:26.346599 139723137873664 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.14222699403762817, loss=0.27152761816978455
I0307 17:31:34.651497 139723129480960 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.10632061213254929, loss=0.28643208742141724
I0307 17:31:42.937222 139723137873664 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.06677128374576569, loss=0.27567121386528015
I0307 17:31:51.234927 139723129480960 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.06476466357707977, loss=0.27017712593078613
I0307 17:31:59.519348 139723137873664 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.13809113204479218, loss=0.29156404733657837
I0307 17:32:07.805555 139723129480960 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.05597103014588356, loss=0.30862510204315186
I0307 17:32:16.085267 139723137873664 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.09718228876590729, loss=0.32835137844085693
I0307 17:32:24.376887 139723129480960 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.10618437826633453, loss=0.2820616364479065
I0307 17:32:32.672979 139723137873664 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.28817278146743774, loss=0.24157778918743134
I0307 17:32:36.490139 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:32:37.784940 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:32:39.079423 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:32:40.375358 139874142557376 submission_runner.py:469] Time since start: 4105.59s, 	Step: 44247, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3800.8591706752777, 'total_duration': 4105.592796802521, 'accumulated_submission_time': 3800.8591706752777, 'accumulated_eval_time': 297.5147876739502, 'accumulated_logging_time': 5.14327597618103}
I0307 17:32:40.394371 139723129480960 logging_writer.py:48] [44247] accumulated_eval_time=297.515, accumulated_logging_time=5.14328, accumulated_submission_time=3800.86, global_step=44247, preemption_count=0, score=3800.86, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4105.59, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:32:44.891877 139723137873664 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.09198208153247833, loss=0.3049480617046356
I0307 17:32:53.174139 139723129480960 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08941274136304855, loss=0.3742733895778656
I0307 17:33:01.462167 139723137873664 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.10859853029251099, loss=0.25538375973701477
I0307 17:33:09.742593 139723129480960 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.22014376521110535, loss=0.2908441424369812
I0307 17:33:18.041340 139723137873664 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.13099414110183716, loss=0.29838404059410095
I0307 17:33:26.331927 139723129480960 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08074472844600677, loss=0.30051320791244507
I0307 17:33:34.642202 139723137873664 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.1913793683052063, loss=0.27610376477241516
I0307 17:33:42.948455 139723129480960 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07175154238939285, loss=0.31092169880867004
I0307 17:33:51.230311 139723137873664 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.1182013601064682, loss=0.28899967670440674
I0307 17:33:59.521202 139723129480960 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.13946591317653656, loss=0.32159584760665894
I0307 17:34:00.436422 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:34:01.727820 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:34:03.023425 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:34:04.321689 139874142557376 submission_runner.py:469] Time since start: 4189.54s, 	Step: 45212, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3880.8518149852753, 'total_duration': 4189.53914141655, 'accumulated_submission_time': 3880.8518149852753, 'accumulated_eval_time': 301.3999900817871, 'accumulated_logging_time': 5.169995069503784}
I0307 17:34:04.341187 139723137873664 logging_writer.py:48] [45212] accumulated_eval_time=301.4, accumulated_logging_time=5.17, accumulated_submission_time=3880.85, global_step=45212, preemption_count=0, score=3880.85, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4189.54, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:34:11.738483 139723129480960 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.16242164373397827, loss=0.22235190868377686
I0307 17:34:20.025366 139723137873664 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.17664770781993866, loss=0.3245396316051483
I0307 17:34:28.335350 139723129480960 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.10094082355499268, loss=0.3126935362815857
I0307 17:34:36.649412 139723137873664 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.11119598150253296, loss=0.30583706498146057
I0307 17:34:44.928042 139723129480960 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.12123002111911774, loss=0.22134028375148773
I0307 17:34:53.202089 139723137873664 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.27903398871421814, loss=0.2652491629123688
I0307 17:35:01.507314 139723129480960 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.16758020222187042, loss=0.2838209867477417
I0307 17:35:09.788092 139723137873664 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.06774850934743881, loss=0.2892286777496338
I0307 17:35:18.075516 139723129480960 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.05825817957520485, loss=0.30188897252082825
I0307 17:35:24.363010 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:35:25.654437 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:35:26.952099 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:35:28.250011 139874142557376 submission_runner.py:469] Time since start: 4273.47s, 	Step: 46177, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3960.8236577510834, 'total_duration': 4273.467481851578, 'accumulated_submission_time': 3960.8236577510834, 'accumulated_eval_time': 305.28694677352905, 'accumulated_logging_time': 5.196955442428589}
I0307 17:35:28.265572 139723137873664 logging_writer.py:48] [46177] accumulated_eval_time=305.287, accumulated_logging_time=5.19696, accumulated_submission_time=3960.82, global_step=46177, preemption_count=0, score=3960.82, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4273.47, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:35:30.274500 139723129480960 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.13758400082588196, loss=0.3608522415161133
I0307 17:35:38.570365 139723137873664 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.0960041955113411, loss=0.35372859239578247
I0307 17:35:46.851933 139723129480960 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.19895979762077332, loss=0.2853880524635315
I0307 17:35:55.145744 139723137873664 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.07971522212028503, loss=0.26656562089920044
I0307 17:36:03.443512 139723129480960 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.056786637753248215, loss=0.31166815757751465
I0307 17:36:11.716571 139723137873664 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.13079418241977692, loss=0.2426740825176239
I0307 17:36:19.975037 139723129480960 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.08078932762145996, loss=0.29017573595046997
I0307 17:36:28.253460 139723137873664 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.053304098546504974, loss=0.352334588766098
I0307 17:36:36.537390 139723129480960 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.13741278648376465, loss=0.2515302002429962
I0307 17:36:44.823276 139723137873664 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.0839381217956543, loss=0.27880537509918213
I0307 17:36:48.304726 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:36:49.599905 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:36:50.894521 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:36:52.192954 139874142557376 submission_runner.py:469] Time since start: 4357.41s, 	Step: 47143, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4040.81325173378, 'total_duration': 4357.410423994064, 'accumulated_submission_time': 4040.81325173378, 'accumulated_eval_time': 309.1751365661621, 'accumulated_logging_time': 5.219738721847534}
I0307 17:36:52.208293 139723129480960 logging_writer.py:48] [47143] accumulated_eval_time=309.175, accumulated_logging_time=5.21974, accumulated_submission_time=4040.81, global_step=47143, preemption_count=0, score=4040.81, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4357.41, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:36:57.010803 139723137873664 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.07083866000175476, loss=0.209188774228096
I0307 17:37:05.307511 139723129480960 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.13776473701000214, loss=0.2893662452697754
I0307 17:37:13.621775 139723137873664 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.06917037069797516, loss=0.2752326726913452
I0307 17:37:21.911156 139723129480960 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.09541278332471848, loss=0.24709102511405945
I0307 17:37:30.224962 139723137873664 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.0817832201719284, loss=0.29101309180259705
I0307 17:37:38.538347 139723129480960 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.15210823714733124, loss=0.3263242244720459
I0307 17:37:46.810337 139723137873664 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.11356721073389053, loss=0.27244606614112854
I0307 17:37:55.097244 139723129480960 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09708793461322784, loss=0.304980993270874
I0307 17:38:03.391581 139723137873664 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.11441057175397873, loss=0.2375641018152237
I0307 17:38:11.682133 139723129480960 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.1489821821451187, loss=0.2877974510192871
I0307 17:38:12.269955 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:38:13.566014 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:38:14.866030 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:38:16.163399 139874142557376 submission_runner.py:469] Time since start: 4441.38s, 	Step: 48108, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4120.82520198822, 'total_duration': 4441.3808715343475, 'accumulated_submission_time': 4120.82520198822, 'accumulated_eval_time': 313.06853675842285, 'accumulated_logging_time': 5.242339849472046}
I0307 17:38:16.179910 139723137873664 logging_writer.py:48] [48108] accumulated_eval_time=313.069, accumulated_logging_time=5.24234, accumulated_submission_time=4120.83, global_step=48108, preemption_count=0, score=4120.83, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4441.38, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:38:23.907323 139723129480960 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.06563663482666016, loss=0.33605536818504333
I0307 17:38:32.201741 139723137873664 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.08020655065774918, loss=0.2716202437877655
I0307 17:38:40.506753 139723129480960 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.08006591349840164, loss=0.3277309238910675
I0307 17:38:48.835880 139723137873664 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.20768766105175018, loss=0.2302573025226593
I0307 17:38:57.139332 139723129480960 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.24948371946811676, loss=0.2193586379289627
I0307 17:39:05.436036 139723137873664 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.11278747767210007, loss=0.25086453557014465
I0307 17:39:13.739225 139723129480960 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.08480837941169739, loss=0.331112265586853
I0307 17:39:22.049098 139723137873664 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.20789819955825806, loss=0.2842559516429901
I0307 17:39:30.334607 139723129480960 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.12679488956928253, loss=0.28752055764198303
I0307 17:39:36.175360 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:39:37.467954 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:39:38.765295 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:39:40.064129 139874142557376 submission_runner.py:469] Time since start: 4525.28s, 	Step: 49071, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4200.770702600479, 'total_duration': 4525.281599521637, 'accumulated_submission_time': 4200.770702600479, 'accumulated_eval_time': 316.9572687149048, 'accumulated_logging_time': 5.266968727111816}
I0307 17:39:40.080571 139723137873664 logging_writer.py:48] [49071] accumulated_eval_time=316.957, accumulated_logging_time=5.26697, accumulated_submission_time=4200.77, global_step=49071, preemption_count=0, score=4200.77, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4525.28, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:39:42.612460 139723129480960 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.12148125469684601, loss=0.2691177427768707
I0307 17:39:50.905179 139723137873664 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.10218222439289093, loss=0.34931403398513794
I0307 17:39:59.210249 139723129480960 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.05635557696223259, loss=0.30488157272338867
I0307 17:40:07.516683 139723137873664 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1542973667383194, loss=0.31134045124053955
I0307 17:40:15.810088 139723129480960 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.16257597506046295, loss=0.2689248323440552
I0307 17:40:24.134499 139723137873664 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.17568111419677734, loss=0.2659216523170471
I0307 17:40:32.430113 139723129480960 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.07685922831296921, loss=0.2924281656742096
I0307 17:40:40.746546 139723137873664 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.21773552894592285, loss=0.24830567836761475
I0307 17:40:49.067716 139723129480960 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.17841598391532898, loss=0.26232993602752686
I0307 17:40:57.349351 139723137873664 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.11662278324365616, loss=0.2046402394771576
I0307 17:41:00.078089 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:41:01.374825 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:41:02.672240 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:41:03.968665 139874142557376 submission_runner.py:469] Time since start: 4609.19s, 	Step: 50034, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4280.718178033829, 'total_duration': 4609.186134815216, 'accumulated_submission_time': 4280.718178033829, 'accumulated_eval_time': 320.84779596328735, 'accumulated_logging_time': 5.290837049484253}
I0307 17:41:03.984645 139723129480960 logging_writer.py:48] [50034] accumulated_eval_time=320.848, accumulated_logging_time=5.29084, accumulated_submission_time=4280.72, global_step=50034, preemption_count=0, score=4280.72, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4609.19, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:41:09.557479 139723137873664 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1486998200416565, loss=0.2622731626033783
I0307 17:41:17.856947 139723129480960 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.12007006257772446, loss=0.33261236548423767
I0307 17:41:26.161095 139723137873664 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.0663660541176796, loss=0.27299806475639343
I0307 17:41:34.453570 139723129480960 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.17084743082523346, loss=0.30783066153526306
I0307 17:41:42.745256 139723137873664 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.2164350301027298, loss=0.24247702956199646
I0307 17:41:51.026790 139723129480960 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.14680707454681396, loss=0.34176015853881836
I0307 17:41:59.327547 139723137873664 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.12968873977661133, loss=0.29057106375694275
I0307 17:42:07.640471 139723129480960 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.24813319742679596, loss=0.2700602412223816
I0307 17:42:15.917323 139723137873664 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.08125760406255722, loss=0.2816338837146759
I0307 17:42:24.018750 139874142557376 spec.py:321] Evaluating on the training split.
I0307 17:42:25.312905 139874142557376 spec.py:333] Evaluating on the validation split.
I0307 17:42:26.608352 139874142557376 spec.py:349] Evaluating on the test split.
I0307 17:42:27.905780 139874142557376 submission_runner.py:469] Time since start: 4693.12s, 	Step: 50999, 	{'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4360.701196193695, 'total_duration': 4693.123251914978, 'accumulated_submission_time': 4360.701196193695, 'accumulated_eval_time': 324.73479080200195, 'accumulated_logging_time': 5.314228773117065}
I0307 17:42:27.922218 139723129480960 logging_writer.py:48] [50999] accumulated_eval_time=324.735, accumulated_logging_time=5.31423, accumulated_submission_time=4360.7, global_step=50999, preemption_count=0, score=4360.7, test/loss=0.288706, test/num_examples=3581, test/ssim=0.739916, total_duration=4693.12, train/loss=0.272934, train/ssim=0.742869, validation/loss=0.287249, validation/num_examples=3554, validation/ssim=0.722545
I0307 17:42:28.098954 139723137873664 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2820395827293396, loss=0.2481330782175064
I0307 17:42:36.363839 139723129480960 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.11235910654067993, loss=0.2863931953907013
I0307 17:42:44.646200 139723137873664 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.0951336920261383, loss=0.2861848473548889
I0307 17:42:52.925496 139723129480960 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.06901136785745621, loss=0.2727561295032501
I0307 17:43:01.206097 139723137873664 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.09683020412921906, loss=0.3157053589820862
I0307 17:43:09.504075 139723129480960 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.061627648770809174, loss=0.3229958415031433
I0307 17:43:17.784766 139723137873664 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.07281061261892319, loss=0.31541067361831665
I0307 17:43:26.078338 139723129480960 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.08159929513931274, loss=0.31614696979522705
I0307 17:43:34.365118 139723137873664 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.20299744606018066, loss=0.21547111868858337
I0307 17:43:42.659729 139723129480960 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.05802467092871666, loss=0.3459073305130005
I0307 17:43:47.976526 139723137873664 logging_writer.py:48] [51965] global_step=51965, preemption_count=0, score=4440.69
I0307 17:43:48.502744 139874142557376 submission_runner.py:646] Tuning trial 5/5
I0307 17:43:48.502912 139874142557376 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0307 17:43:48.504480 139874142557376 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.22352738039834158, 'train/loss': 1.022589819771903, 'validation/ssim': 0.21093519873074354, 'validation/loss': 1.0275126700328854, 'validation/num_examples': 3554, 'test/ssim': 0.23437303992098052, 'test/loss': 1.024500305976857, 'test/num_examples': 3581, 'score': 46.344897508621216, 'total_duration': 160.63308477401733, 'accumulated_submission_time': 46.344897508621216, 'accumulated_eval_time': 114.28802156448364, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (489, {'train/ssim': 0.7115646089826312, 'train/loss': 0.30297371319362093, 'validation/ssim': 0.6929954996790588, 'validation/loss': 0.3161163244209869, 'validation/num_examples': 3554, 'test/ssim': 0.7104821480775272, 'test/loss': 0.318564382221272, 'test/num_examples': 3581, 'score': 126.343585729599, 'total_duration': 245.12510919570923, 'accumulated_submission_time': 126.343585729599, 'accumulated_eval_time': 118.66709995269775, 'accumulated_logging_time': 0.01603102684020996, 'global_step': 489, 'preemption_count': 0}), (948, {'train/ssim': 0.7228605406624931, 'train/loss': 0.293184723172869, 'validation/ssim': 0.7045563894775253, 'validation/loss': 0.30598345806441685, 'validation/num_examples': 3554, 'test/ssim': 0.721692777419017, 'test/loss': 0.30844226145804243, 'test/num_examples': 3581, 'score': 206.34170126914978, 'total_duration': 329.1063702106476, 'accumulated_submission_time': 206.34170126914978, 'accumulated_eval_time': 122.54590368270874, 'accumulated_logging_time': 0.04238629341125488, 'global_step': 948, 'preemption_count': 0}), (1907, {'train/ssim': 0.7280037743704659, 'train/loss': 0.2855214391435896, 'validation/ssim': 0.7089410973990574, 'validation/loss': 0.29881744298897367, 'validation/num_examples': 3554, 'test/ssim': 0.7263000198530438, 'test/loss': 0.30102583377330006, 'test/num_examples': 3581, 'score': 286.3103656768799, 'total_duration': 413.0279688835144, 'accumulated_submission_time': 286.3103656768799, 'accumulated_eval_time': 126.42684245109558, 'accumulated_logging_time': 0.06871747970581055, 'global_step': 1907, 'preemption_count': 0}), (2869, {'train/ssim': 0.7314768518720355, 'train/loss': 0.2824376310620989, 'validation/ssim': 0.712147143293648, 'validation/loss': 0.2957527018961909, 'validation/num_examples': 3554, 'test/ssim': 0.7293492892173974, 'test/loss': 0.29767921189960905, 'test/num_examples': 3581, 'score': 366.26108622550964, 'total_duration': 496.9256341457367, 'accumulated_submission_time': 366.26108622550964, 'accumulated_eval_time': 130.31374716758728, 'accumulated_logging_time': 0.08663320541381836, 'global_step': 2869, 'preemption_count': 0}), (3831, {'train/ssim': 0.7349011557442802, 'train/loss': 0.28076822417122976, 'validation/ssim': 0.7156549645865574, 'validation/loss': 0.2940992914563696, 'validation/num_examples': 3554, 'test/ssim': 0.7329428810475426, 'test/loss': 0.29594224100504396, 'test/num_examples': 3581, 'score': 446.22419810295105, 'total_duration': 580.8344089984894, 'accumulated_submission_time': 446.22419810295105, 'accumulated_eval_time': 134.19928288459778, 'accumulated_logging_time': 0.10394978523254395, 'global_step': 3831, 'preemption_count': 0}), (4792, {'train/ssim': 0.7363511494227818, 'train/loss': 0.27842869077410015, 'validation/ssim': 0.7165936763593838, 'validation/loss': 0.29226435552370567, 'validation/num_examples': 3554, 'test/ssim': 0.7339420781991762, 'test/loss': 0.2938926801172857, 'test/num_examples': 3581, 'score': 526.1864306926727, 'total_duration': 664.7466311454773, 'accumulated_submission_time': 526.1864306926727, 'accumulated_eval_time': 138.08747601509094, 'accumulated_logging_time': 0.12322020530700684, 'global_step': 4792, 'preemption_count': 0}), (5754, {'train/ssim': 0.7373614992414202, 'train/loss': 0.2778820310320173, 'validation/ssim': 0.717423919351435, 'validation/loss': 0.2917593814847707, 'validation/num_examples': 3554, 'test/ssim': 0.7344892640847529, 'test/loss': 0.29348000678494135, 'test/num_examples': 3581, 'score': 606.1383287906647, 'total_duration': 748.6512529850006, 'accumulated_submission_time': 606.1383287906647, 'accumulated_eval_time': 141.9749927520752, 'accumulated_logging_time': 0.14121794700622559, 'global_step': 5754, 'preemption_count': 0}), (6715, {'train/ssim': 0.7367210388183594, 'train/loss': 0.2767845903124128, 'validation/ssim': 0.7165620081466305, 'validation/loss': 0.29060627385076676, 'validation/num_examples': 3554, 'test/ssim': 0.7342842568634809, 'test/loss': 0.29201345861840267, 'test/num_examples': 3581, 'score': 686.0969247817993, 'total_duration': 832.5631713867188, 'accumulated_submission_time': 686.0969247817993, 'accumulated_eval_time': 145.86383700370789, 'accumulated_logging_time': 0.1596517562866211, 'global_step': 6715, 'preemption_count': 0}), (7678, {'train/ssim': 0.7392879894801548, 'train/loss': 0.2757589987346104, 'validation/ssim': 0.7192405481719542, 'validation/loss': 0.28964272897835186, 'validation/num_examples': 3554, 'test/ssim': 0.7366069675457274, 'test/loss': 0.2912133032301557, 'test/num_examples': 3581, 'score': 766.0476813316345, 'total_duration': 916.4647932052612, 'accumulated_submission_time': 766.0476813316345, 'accumulated_eval_time': 149.74801683425903, 'accumulated_logging_time': 0.17925143241882324, 'global_step': 7678, 'preemption_count': 0}), (8638, {'train/ssim': 0.7391907146998814, 'train/loss': 0.27576025894709993, 'validation/ssim': 0.7192065443426421, 'validation/loss': 0.28964049640370004, 'validation/num_examples': 3554, 'test/ssim': 0.7364210497896886, 'test/loss': 0.2911922025534069, 'test/num_examples': 3581, 'score': 845.9909300804138, 'total_duration': 1000.3621490001678, 'accumulated_submission_time': 845.9909300804138, 'accumulated_eval_time': 153.63534140586853, 'accumulated_logging_time': 0.20130515098571777, 'global_step': 8638, 'preemption_count': 0}), (9601, {'train/ssim': 0.7402725219726562, 'train/loss': 0.2750066007886614, 'validation/ssim': 0.7203701622511607, 'validation/loss': 0.2888273927155142, 'validation/num_examples': 3554, 'test/ssim': 0.7376605696732756, 'test/loss': 0.2903655946204796, 'test/num_examples': 3581, 'score': 926.0004110336304, 'total_duration': 1084.324478149414, 'accumulated_submission_time': 926.0004110336304, 'accumulated_eval_time': 157.52419233322144, 'accumulated_logging_time': 0.22099041938781738, 'global_step': 9601, 'preemption_count': 0}), (10513, {'train/ssim': 0.7398304258074079, 'train/loss': 0.2749426705496652, 'validation/ssim': 0.7199834803214687, 'validation/loss': 0.28870274635533905, 'validation/num_examples': 3554, 'test/ssim': 0.7373252768517872, 'test/loss': 0.29027955567360725, 'test/num_examples': 3581, 'score': 1001.8691554069519, 'total_duration': 1168.2855155467987, 'accumulated_submission_time': 1001.8691554069519, 'accumulated_eval_time': 161.4114646911621, 'accumulated_logging_time': 4.383329391479492, 'global_step': 10513, 'preemption_count': 0}), (11476, {'train/ssim': 0.7408531733921596, 'train/loss': 0.27451971599033903, 'validation/ssim': 0.720891348216798, 'validation/loss': 0.28844414551605585, 'validation/num_examples': 3554, 'test/ssim': 0.7382553428686122, 'test/loss': 0.289964102260629, 'test/num_examples': 3581, 'score': 1081.83860373497, 'total_duration': 1252.2015018463135, 'accumulated_submission_time': 1081.83860373497, 'accumulated_eval_time': 165.29674291610718, 'accumulated_logging_time': 4.402867794036865, 'global_step': 11476, 'preemption_count': 0}), (12439, {'train/ssim': 0.7410992894853864, 'train/loss': 0.27413473810468403, 'validation/ssim': 0.721452033580121, 'validation/loss': 0.2880175520210502, 'validation/num_examples': 3554, 'test/ssim': 0.7387314886728568, 'test/loss': 0.2895541559947466, 'test/num_examples': 3581, 'score': 1161.8065090179443, 'total_duration': 1336.1189105510712, 'accumulated_submission_time': 1161.8065090179443, 'accumulated_eval_time': 169.18456363677979, 'accumulated_logging_time': 4.422452688217163, 'global_step': 12439, 'preemption_count': 0}), (13403, {'train/ssim': 0.7433362688337054, 'train/loss': 0.2738545111247471, 'validation/ssim': 0.7231015284274761, 'validation/loss': 0.28842968530177265, 'validation/num_examples': 3554, 'test/ssim': 0.7403071876745323, 'test/loss': 0.28997794212292305, 'test/num_examples': 3581, 'score': 1241.8006756305695, 'total_duration': 1420.0589079856873, 'accumulated_submission_time': 1241.8006756305695, 'accumulated_eval_time': 173.0692937374115, 'accumulated_logging_time': 4.442254304885864, 'global_step': 13403, 'preemption_count': 0}), (14365, {'train/ssim': 0.7419112069266183, 'train/loss': 0.27333434990474154, 'validation/ssim': 0.7215948496632316, 'validation/loss': 0.28759360326832445, 'validation/num_examples': 3554, 'test/ssim': 0.7389639029120707, 'test/loss': 0.28904463771467465, 'test/num_examples': 3581, 'score': 1321.7436938285828, 'total_duration': 1503.953621149063, 'accumulated_submission_time': 1321.7436938285828, 'accumulated_eval_time': 176.95988535881042, 'accumulated_logging_time': 4.4619951248168945, 'global_step': 14365, 'preemption_count': 0}), (15328, {'train/ssim': 0.742173331124442, 'train/loss': 0.2732717820576259, 'validation/ssim': 0.7217690591806767, 'validation/loss': 0.28764110558745426, 'validation/num_examples': 3554, 'test/ssim': 0.7390887343793633, 'test/loss': 0.289144380170518, 'test/num_examples': 3581, 'score': 1401.7052421569824, 'total_duration': 1587.8730199337006, 'accumulated_submission_time': 1401.7052421569824, 'accumulated_eval_time': 180.85447812080383, 'accumulated_logging_time': 4.484252214431763, 'global_step': 15328, 'preemption_count': 0}), (16291, {'train/ssim': 0.7427377019609723, 'train/loss': 0.2730858496257237, 'validation/ssim': 0.7225748468934651, 'validation/loss': 0.2872575664359261, 'validation/num_examples': 3554, 'test/ssim': 0.7399480330215024, 'test/loss': 0.2887111515747173, 'test/num_examples': 3581, 'score': 1481.672688961029, 'total_duration': 1671.7901425361633, 'accumulated_submission_time': 1481.672688961029, 'accumulated_eval_time': 184.7398931980133, 'accumulated_logging_time': 4.507124423980713, 'global_step': 16291, 'preemption_count': 0}), (17255, {'train/ssim': 0.742950303213937, 'train/loss': 0.27289388860974995, 'validation/ssim': 0.7226799496386114, 'validation/loss': 0.2871784474249965, 'validation/num_examples': 3554, 'test/ssim': 0.7400392533946524, 'test/loss': 0.28864092961332377, 'test/num_examples': 3581, 'score': 1561.6386981010437, 'total_duration': 1755.7091271877289, 'accumulated_submission_time': 1561.6386981010437, 'accumulated_eval_time': 188.6319715976715, 'accumulated_logging_time': 4.526903390884399, 'global_step': 17255, 'preemption_count': 0}), (18218, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1641.6214306354523, 'total_duration': 1839.6406576633453, 'accumulated_submission_time': 1641.6214306354523, 'accumulated_eval_time': 192.5165994167328, 'accumulated_logging_time': 4.5494184494018555, 'global_step': 18218, 'preemption_count': 0}), (19181, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1721.6177320480347, 'total_duration': 1923.5913984775543, 'accumulated_submission_time': 1721.6177320480347, 'accumulated_eval_time': 196.40924048423767, 'accumulated_logging_time': 4.569700241088867, 'global_step': 19181, 'preemption_count': 0}), (20146, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1801.6282346248627, 'total_duration': 2007.5529630184174, 'accumulated_submission_time': 1801.6282346248627, 'accumulated_eval_time': 200.29419326782227, 'accumulated_logging_time': 4.592954874038696, 'global_step': 20146, 'preemption_count': 0}), (21110, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1881.581708431244, 'total_duration': 2091.4542939662933, 'accumulated_submission_time': 1881.581708431244, 'accumulated_eval_time': 204.18146800994873, 'accumulated_logging_time': 4.612586259841919, 'global_step': 21110, 'preemption_count': 0}), (22073, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 1961.558806180954, 'total_duration': 2175.3838670253754, 'accumulated_submission_time': 1961.558806180954, 'accumulated_eval_time': 208.07179713249207, 'accumulated_logging_time': 4.633031606674194, 'global_step': 22073, 'preemption_count': 0}), (23036, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2041.5548367500305, 'total_duration': 2259.3264276981354, 'accumulated_submission_time': 2041.5548367500305, 'accumulated_eval_time': 211.9566216468811, 'accumulated_logging_time': 4.653255462646484, 'global_step': 23036, 'preemption_count': 0}), (23999, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2121.5423896312714, 'total_duration': 2343.264815092087, 'accumulated_submission_time': 2121.5423896312714, 'accumulated_eval_time': 215.84676694869995, 'accumulated_logging_time': 4.672826528549194, 'global_step': 23999, 'preemption_count': 0}), (24963, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2201.4822473526, 'total_duration': 2427.154510498047, 'accumulated_submission_time': 2201.4822473526, 'accumulated_eval_time': 219.73506712913513, 'accumulated_logging_time': 4.6926939487457275, 'global_step': 24963, 'preemption_count': 0}), (25928, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2281.467113018036, 'total_duration': 2511.092609643936, 'accumulated_submission_time': 2281.467113018036, 'accumulated_eval_time': 223.62600779533386, 'accumulated_logging_time': 4.713877201080322, 'global_step': 25928, 'preemption_count': 0}), (26893, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2361.4665875434875, 'total_duration': 2595.0429589748383, 'accumulated_submission_time': 2361.4665875434875, 'accumulated_eval_time': 227.5109236240387, 'accumulated_logging_time': 4.737409353256226, 'global_step': 26893, 'preemption_count': 0}), (27858, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2441.4167125225067, 'total_duration': 2678.9494805336, 'accumulated_submission_time': 2441.4167125225067, 'accumulated_eval_time': 231.4053921699524, 'accumulated_logging_time': 4.7578887939453125, 'global_step': 27858, 'preemption_count': 0}), (28823, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2521.4087913036346, 'total_duration': 2762.8954396247864, 'accumulated_submission_time': 2521.4087913036346, 'accumulated_eval_time': 235.2964050769806, 'accumulated_logging_time': 4.7788660526275635, 'global_step': 28823, 'preemption_count': 0}), (29787, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2601.389988899231, 'total_duration': 2846.82825422287, 'accumulated_submission_time': 2601.389988899231, 'accumulated_eval_time': 239.18532419204712, 'accumulated_logging_time': 4.800422191619873, 'global_step': 29787, 'preemption_count': 0}), (30751, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2681.3422050476074, 'total_duration': 2930.7338349819183, 'accumulated_submission_time': 2681.3422050476074, 'accumulated_eval_time': 243.07236647605896, 'accumulated_logging_time': 4.824802875518799, 'global_step': 30751, 'preemption_count': 0}), (31715, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2761.299434900284, 'total_duration': 3014.6443660259247, 'accumulated_submission_time': 2761.299434900284, 'accumulated_eval_time': 246.9631052017212, 'accumulated_logging_time': 4.845642328262329, 'global_step': 31715, 'preemption_count': 0}), (32679, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2841.2503139972687, 'total_duration': 3098.5463168621063, 'accumulated_submission_time': 2841.2503139972687, 'accumulated_eval_time': 250.8517611026764, 'accumulated_logging_time': 4.867064714431763, 'global_step': 32679, 'preemption_count': 0}), (33643, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 2921.216874361038, 'total_duration': 3182.4704785346985, 'accumulated_submission_time': 2921.216874361038, 'accumulated_eval_time': 254.74315071105957, 'accumulated_logging_time': 4.892060995101929, 'global_step': 33643, 'preemption_count': 0}), (34607, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3001.159170627594, 'total_duration': 3266.367614507675, 'accumulated_submission_time': 3001.159170627594, 'accumulated_eval_time': 258.6340937614441, 'accumulated_logging_time': 4.9145026206970215, 'global_step': 34607, 'preemption_count': 0}), (35571, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3081.1128964424133, 'total_duration': 3350.2758524417877, 'accumulated_submission_time': 3081.1128964424133, 'accumulated_eval_time': 262.5256586074829, 'accumulated_logging_time': 4.93588924407959, 'global_step': 35571, 'preemption_count': 0}), (36536, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3161.099362373352, 'total_duration': 3434.2166855335236, 'accumulated_submission_time': 3161.099362373352, 'accumulated_eval_time': 266.4160840511322, 'accumulated_logging_time': 4.959021806716919, 'global_step': 36536, 'preemption_count': 0}), (37500, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3241.08052778244, 'total_duration': 3518.147956609726, 'accumulated_submission_time': 3241.08052778244, 'accumulated_eval_time': 270.30341243743896, 'accumulated_logging_time': 4.980231046676636, 'global_step': 37500, 'preemption_count': 0}), (38463, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3321.0460953712463, 'total_duration': 3602.0722699165344, 'accumulated_submission_time': 3321.0460953712463, 'accumulated_eval_time': 274.1979868412018, 'accumulated_logging_time': 5.00262188911438, 'global_step': 38463, 'preemption_count': 0}), (39427, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3401.0251636505127, 'total_duration': 3685.998311281204, 'accumulated_submission_time': 3401.0251636505127, 'accumulated_eval_time': 278.0822787284851, 'accumulated_logging_time': 5.024211168289185, 'global_step': 39427, 'preemption_count': 0}), (40390, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3480.9792766571045, 'total_duration': 3769.904486656189, 'accumulated_submission_time': 3480.9792766571045, 'accumulated_eval_time': 281.9676904678345, 'accumulated_logging_time': 5.049941062927246, 'global_step': 40390, 'preemption_count': 0}), (41354, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3560.952558040619, 'total_duration': 3853.8290553092957, 'accumulated_submission_time': 3560.952558040619, 'accumulated_eval_time': 285.8539435863495, 'accumulated_logging_time': 5.072075128555298, 'global_step': 41354, 'preemption_count': 0}), (42318, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3640.947919845581, 'total_duration': 3937.7753982543945, 'accumulated_submission_time': 3640.947919845581, 'accumulated_eval_time': 289.7395317554474, 'accumulated_logging_time': 5.095789432525635, 'global_step': 42318, 'preemption_count': 0}), (43282, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3720.88298535347, 'total_duration': 4021.6657605171204, 'accumulated_submission_time': 3720.88298535347, 'accumulated_eval_time': 293.62964630126953, 'accumulated_logging_time': 5.119950294494629, 'global_step': 43282, 'preemption_count': 0}), (44247, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3800.8591706752777, 'total_duration': 4105.592796802521, 'accumulated_submission_time': 3800.8591706752777, 'accumulated_eval_time': 297.5147876739502, 'accumulated_logging_time': 5.14327597618103, 'global_step': 44247, 'preemption_count': 0}), (45212, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3880.8518149852753, 'total_duration': 4189.53914141655, 'accumulated_submission_time': 3880.8518149852753, 'accumulated_eval_time': 301.3999900817871, 'accumulated_logging_time': 5.169995069503784, 'global_step': 45212, 'preemption_count': 0}), (46177, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 3960.8236577510834, 'total_duration': 4273.467481851578, 'accumulated_submission_time': 3960.8236577510834, 'accumulated_eval_time': 305.28694677352905, 'accumulated_logging_time': 5.196955442428589, 'global_step': 46177, 'preemption_count': 0}), (47143, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4040.81325173378, 'total_duration': 4357.410423994064, 'accumulated_submission_time': 4040.81325173378, 'accumulated_eval_time': 309.1751365661621, 'accumulated_logging_time': 5.219738721847534, 'global_step': 47143, 'preemption_count': 0}), (48108, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4120.82520198822, 'total_duration': 4441.3808715343475, 'accumulated_submission_time': 4120.82520198822, 'accumulated_eval_time': 313.06853675842285, 'accumulated_logging_time': 5.242339849472046, 'global_step': 48108, 'preemption_count': 0}), (49071, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4200.770702600479, 'total_duration': 4525.281599521637, 'accumulated_submission_time': 4200.770702600479, 'accumulated_eval_time': 316.9572687149048, 'accumulated_logging_time': 5.266968727111816, 'global_step': 49071, 'preemption_count': 0}), (50034, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4280.718178033829, 'total_duration': 4609.186134815216, 'accumulated_submission_time': 4280.718178033829, 'accumulated_eval_time': 320.84779596328735, 'accumulated_logging_time': 5.290837049484253, 'global_step': 50034, 'preemption_count': 0}), (50999, {'train/ssim': 0.7428689684186663, 'train/loss': 0.2729338918413435, 'validation/ssim': 0.7225451021296426, 'validation/loss': 0.2872489967839934, 'validation/num_examples': 3554, 'test/ssim': 0.7399156491072675, 'test/loss': 0.2887058678834474, 'test/num_examples': 3581, 'score': 4360.701196193695, 'total_duration': 4693.123251914978, 'accumulated_submission_time': 4360.701196193695, 'accumulated_eval_time': 324.73479080200195, 'accumulated_logging_time': 5.314228773117065, 'global_step': 50999, 'preemption_count': 0})], 'global_step': 51965}
I0307 17:43:48.504654 139874142557376 submission_runner.py:649] Timing: 4440.69345831871
I0307 17:43:48.504690 139874142557376 submission_runner.py:651] Total number of evals: 55
I0307 17:43:48.504716 139874142557376 submission_runner.py:652] ====================
I0307 17:43:48.504884 139874142557376 submission_runner.py:750] Final fastmri score: 4
