python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=230927579 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-16-05-35.log
2025-03-07 16:05:42.890397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741363542.975919       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741363543.026102       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:06:01.035700 140503400502464 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax.
I0307 16:06:02.215570 140503400502464 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:06:02.219597 140503400502464 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:06:02.221448 140503400502464 submission_runner.py:606] Using RNG seed 230927579
I0307 16:06:02.876783 140503400502464 submission_runner.py:615] --- Tuning run 1/5 ---
I0307 16:06:02.876987 140503400502464 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_1.
I0307 16:06:02.877221 140503400502464 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_1/hparams.json.
I0307 16:06:03.115685 140503400502464 submission_runner.py:218] Initializing dataset.
I0307 16:06:08.445206 140503400502464 submission_runner.py:229] Initializing model.
I0307 16:06:19.123692 140503400502464 submission_runner.py:272] Initializing optimizer.
I0307 16:06:19.613411 140503400502464 submission_runner.py:279] Initializing metrics bundle.
I0307 16:06:19.613630 140503400502464 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:06:19.614469 140503400502464 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_1 with prefix checkpoint_
I0307 16:06:19.614581 140503400502464 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_1/meta_data_0.json.
I0307 16:06:19.614757 140503400502464 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:06:19.614807 140503400502464 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:06:19.844033 140503400502464 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/fastmri_jax/trial_1/flags_0.json.
I0307 16:06:19.877210 140503400502464 submission_runner.py:337] Starting training loop.
E0307 16:06:53.273118       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:06:53.487877       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:06:53.900473       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:06:54.115188       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:06:55.991890       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 16:06:56.208262       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 16:07:07.391187 140366669915904 logging_writer.py:48] [0] global_step=0, grad_norm=4.606741428375244, loss=1.2695949077606201
I0307 16:07:07.444337 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:09:21.390398 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:09:45.049749 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:10:06.667254 140503400502464 submission_runner.py:469] Time since start: 226.79s, 	Step: 1, 	{'train/ssim': 0.18552499158041819, 'train/loss': 1.2679830278669084, 'validation/ssim': 0.1769749389717132, 'validation/loss': 1.2802880007561903, 'validation/num_examples': 3554, 'test/ssim': 0.1960801862531852, 'test/loss': 1.2780091542166991, 'test/num_examples': 3581, 'score': 47.56694793701172, 'total_duration': 226.78998112678528, 'accumulated_submission_time': 47.56694793701172, 'accumulated_eval_time': 179.22285509109497, 'accumulated_logging_time': 0}
I0307 16:10:06.674997 140345924884224 logging_writer.py:48] [1] accumulated_eval_time=179.223, accumulated_logging_time=0, accumulated_submission_time=47.5669, global_step=1, preemption_count=0, score=47.5669, test/loss=1.27801, test/num_examples=3581, test/ssim=0.19608, total_duration=226.79, train/loss=1.26798, train/ssim=0.185525, validation/loss=1.28029, validation/num_examples=3554, validation/ssim=0.176975
I0307 16:10:19.656523 140345438369536 logging_writer.py:48] [100] global_step=100, grad_norm=0.9544115662574768, loss=0.5248252153396606
I0307 16:10:37.730034 140345924884224 logging_writer.py:48] [200] global_step=200, grad_norm=0.19897128641605377, loss=0.26903557777404785
I0307 16:10:54.270413 140345438369536 logging_writer.py:48] [300] global_step=300, grad_norm=0.11269279569387436, loss=0.35632002353668213
I0307 16:11:12.454045 140345924884224 logging_writer.py:48] [400] global_step=400, grad_norm=0.09430112689733505, loss=0.37159743905067444
I0307 16:11:26.875859 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:11:28.675343 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:11:29.974885 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:11:31.275483 140503400502464 submission_runner.py:469] Time since start: 311.40s, 	Step: 481, 	{'train/ssim': 0.7003184727260044, 'train/loss': 0.31068267141069683, 'validation/ssim': 0.6808968685152293, 'validation/loss': 0.3272965087203679, 'validation/num_examples': 3554, 'test/ssim': 0.6994568910787838, 'test/loss': 0.3290439873093235, 'test/num_examples': 3581, 'score': 127.66476726531982, 'total_duration': 311.3981935977936, 'accumulated_submission_time': 127.66476726531982, 'accumulated_eval_time': 183.62240862846375, 'accumulated_logging_time': 0.017316818237304688}
I0307 16:11:31.289954 140345438369536 logging_writer.py:48] [481] accumulated_eval_time=183.622, accumulated_logging_time=0.0173168, accumulated_submission_time=127.665, global_step=481, preemption_count=0, score=127.665, test/loss=0.329044, test/num_examples=3581, test/ssim=0.699457, total_duration=311.398, train/loss=0.310683, train/ssim=0.700318, validation/loss=0.327297, validation/num_examples=3554, validation/ssim=0.680897
I0307 16:11:32.965737 140345924884224 logging_writer.py:48] [500] global_step=500, grad_norm=0.09142910689115524, loss=0.35169339179992676
I0307 16:11:49.934530 140345438369536 logging_writer.py:48] [600] global_step=600, grad_norm=0.17051903903484344, loss=0.3822956085205078
I0307 16:12:09.287327 140345924884224 logging_writer.py:48] [700] global_step=700, grad_norm=0.33805620670318604, loss=0.23751378059387207
I0307 16:12:27.660298 140345438369536 logging_writer.py:48] [800] global_step=800, grad_norm=0.07324746996164322, loss=0.19585084915161133
I0307 16:12:47.113062 140345924884224 logging_writer.py:48] [900] global_step=900, grad_norm=0.36114513874053955, loss=0.288268119096756
I0307 16:12:51.360782 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:12:52.656943 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:12:53.954261 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:12:55.251173 140503400502464 submission_runner.py:469] Time since start: 395.37s, 	Step: 923, 	{'train/ssim': 0.7199142319815499, 'train/loss': 0.29130356652396067, 'validation/ssim': 0.7021796248505205, 'validation/loss': 0.3067781172512486, 'validation/num_examples': 3554, 'test/ssim': 0.7193804295893256, 'test/loss': 0.3090006283161128, 'test/num_examples': 3581, 'score': 207.65257263183594, 'total_duration': 395.373877286911, 'accumulated_submission_time': 207.65257263183594, 'accumulated_eval_time': 187.5127284526825, 'accumulated_logging_time': 0.0491185188293457}
I0307 16:12:55.267799 140345438369536 logging_writer.py:48] [923] accumulated_eval_time=187.513, accumulated_logging_time=0.0491185, accumulated_submission_time=207.653, global_step=923, preemption_count=0, score=207.653, test/loss=0.309001, test/num_examples=3581, test/ssim=0.71938, total_duration=395.374, train/loss=0.291304, train/ssim=0.719914, validation/loss=0.306778, validation/num_examples=3554, validation/ssim=0.70218
I0307 16:13:02.533612 140345924884224 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.23002929985523224, loss=0.38261541724205017
I0307 16:13:10.802480 140345438369536 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4066533148288727, loss=0.281965434551239
I0307 16:13:19.019117 140345924884224 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.11885534226894379, loss=0.30816274881362915
I0307 16:13:27.245947 140345438369536 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5495354533195496, loss=0.19861182570457458
I0307 16:13:35.472101 140345924884224 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.1890646517276764, loss=0.26474258303642273
I0307 16:13:43.692147 140345438369536 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2111690789461136, loss=0.2837234437465668
I0307 16:13:51.912330 140345924884224 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.39748048782348633, loss=0.31262972950935364
I0307 16:14:00.122163 140345438369536 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.10357919335365295, loss=0.3347262144088745
I0307 16:14:08.334451 140345924884224 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6654087901115417, loss=0.2758370041847229
I0307 16:14:15.257596 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:14:16.554961 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:14:18.022069 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:14:19.336679 140503400502464 submission_runner.py:469] Time since start: 479.46s, 	Step: 1885, 	{'train/ssim': 0.733295372554234, 'train/loss': 0.28001182419913156, 'validation/ssim': 0.7142226133845667, 'validation/loss': 0.29560497414884634, 'validation/num_examples': 3554, 'test/ssim': 0.7313767949551452, 'test/loss': 0.29755543717057037, 'test/num_examples': 3581, 'score': 287.5713081359863, 'total_duration': 479.4594051837921, 'accumulated_submission_time': 287.5713081359863, 'accumulated_eval_time': 191.59176206588745, 'accumulated_logging_time': 0.07665657997131348}
I0307 16:14:19.380210 140345438369536 logging_writer.py:48] [1885] accumulated_eval_time=191.592, accumulated_logging_time=0.0766566, accumulated_submission_time=287.571, global_step=1885, preemption_count=0, score=287.571, test/loss=0.297555, test/num_examples=3581, test/ssim=0.731377, total_duration=479.459, train/loss=0.280012, train/ssim=0.733295, validation/loss=0.295605, validation/num_examples=3554, validation/ssim=0.714223
I0307 16:14:20.726975 140345924884224 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.11692570894956589, loss=0.3031408190727234
I0307 16:14:28.951453 140345438369536 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.04358455911278725, loss=0.27892372012138367
I0307 16:14:37.177225 140345924884224 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.06993264704942703, loss=0.3202565014362335
I0307 16:14:45.423809 140345438369536 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3076823949813843, loss=0.3001575767993927
I0307 16:14:53.663330 140345924884224 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.12332713603973389, loss=0.2887132167816162
I0307 16:15:01.896031 140345438369536 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.39037999510765076, loss=0.2115035504102707
I0307 16:15:10.134515 140345924884224 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.11096339672803879, loss=0.32458722591400146
I0307 16:15:18.383464 140345438369536 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2969686686992645, loss=0.37436264753341675
I0307 16:15:26.628883 140345924884224 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.12233836203813553, loss=0.25839322805404663
I0307 16:15:34.855707 140345438369536 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.07132753729820251, loss=0.2501348853111267
I0307 16:15:39.378874 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:15:40.678321 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:15:41.981584 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:15:43.292891 140503400502464 submission_runner.py:469] Time since start: 563.42s, 	Step: 2856, 	{'train/ssim': 0.737513814653669, 'train/loss': 0.2761204072407314, 'validation/ssim': 0.718266527372327, 'validation/loss': 0.291941490881753, 'validation/num_examples': 3554, 'test/ssim': 0.7354618723383831, 'test/loss': 0.29363967652628453, 'test/num_examples': 3581, 'score': 367.50900411605835, 'total_duration': 563.4156262874603, 'accumulated_submission_time': 367.50900411605835, 'accumulated_eval_time': 195.50573205947876, 'accumulated_logging_time': 0.12910866737365723}
I0307 16:15:43.302415 140345924884224 logging_writer.py:48] [2856] accumulated_eval_time=195.506, accumulated_logging_time=0.129109, accumulated_submission_time=367.509, global_step=2856, preemption_count=0, score=367.509, test/loss=0.29364, test/num_examples=3581, test/ssim=0.735462, total_duration=563.416, train/loss=0.27612, train/ssim=0.737514, validation/loss=0.291941, validation/num_examples=3554, validation/ssim=0.718267
I0307 16:15:47.029440 140345438369536 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.13240478932857513, loss=0.25308987498283386
I0307 16:15:55.231046 140345924884224 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.20240825414657593, loss=0.29499873518943787
I0307 16:16:03.440423 140345438369536 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2187422513961792, loss=0.39110296964645386
I0307 16:16:11.667110 140345924884224 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.21422462165355682, loss=0.23413339257240295
I0307 16:16:19.883092 140345438369536 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3462066054344177, loss=0.28322508931159973
I0307 16:16:28.123635 140345924884224 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.22748593986034393, loss=0.3113977313041687
I0307 16:16:36.331842 140345438369536 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.15357892215251923, loss=0.22591249644756317
I0307 16:16:44.559446 140345924884224 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.11152969300746918, loss=0.23817738890647888
I0307 16:16:52.800634 140345438369536 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.1082751527428627, loss=0.26915717124938965
I0307 16:17:01.010396 140345924884224 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.28189611434936523, loss=0.31068605184555054
I0307 16:17:03.304453 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:17:04.604761 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:17:05.909064 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:17:07.215162 140503400502464 submission_runner.py:469] Time since start: 647.34s, 	Step: 3829, 	{'train/ssim': 0.7398936407906669, 'train/loss': 0.27339982986450195, 'validation/ssim': 0.7203334793322664, 'validation/loss': 0.2894187158725204, 'validation/num_examples': 3554, 'test/ssim': 0.7375842118123429, 'test/loss': 0.29093544924602066, 'test/num_examples': 3581, 'score': 447.4507918357849, 'total_duration': 647.3378970623016, 'accumulated_submission_time': 447.4507918357849, 'accumulated_eval_time': 199.41640186309814, 'accumulated_logging_time': 0.14669537544250488}
I0307 16:17:07.224767 140345438369536 logging_writer.py:48] [3829] accumulated_eval_time=199.416, accumulated_logging_time=0.146695, accumulated_submission_time=447.451, global_step=3829, preemption_count=0, score=447.451, test/loss=0.290935, test/num_examples=3581, test/ssim=0.737584, total_duration=647.338, train/loss=0.2734, train/ssim=0.739894, validation/loss=0.289419, validation/num_examples=3554, validation/ssim=0.720333
I0307 16:17:13.168771 140345924884224 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.12242423743009567, loss=0.27947741746902466
I0307 16:17:21.373472 140345438369536 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.09111104160547256, loss=0.23346379399299622
I0307 16:17:29.589910 140345924884224 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.07509134709835052, loss=0.2737566828727722
I0307 16:17:37.826377 140345438369536 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.19851693511009216, loss=0.28875088691711426
I0307 16:17:46.050379 140345924884224 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.39050862193107605, loss=0.27626973390579224
I0307 16:17:54.258964 140345438369536 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.17985612154006958, loss=0.1999860405921936
I0307 16:18:02.469804 140345924884224 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.19844810664653778, loss=0.2936713695526123
I0307 16:18:10.719266 140345438369536 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.10513173788785934, loss=0.3093511462211609
I0307 16:18:18.952799 140345924884224 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.15386536717414856, loss=0.21835467219352722
I0307 16:18:27.196928 140345438369536 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.20711536705493927, loss=0.3387923240661621
I0307 16:18:27.283720 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:18:28.582376 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:18:29.884072 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:18:31.192843 140503400502464 submission_runner.py:469] Time since start: 731.32s, 	Step: 4802, 	{'train/ssim': 0.7406514031546456, 'train/loss': 0.2726429189954485, 'validation/ssim': 0.720959355875422, 'validation/loss': 0.2887256903532991, 'validation/num_examples': 3554, 'test/ssim': 0.7381828710773876, 'test/loss': 0.29023022985897795, 'test/num_examples': 3581, 'score': 527.4489407539368, 'total_duration': 731.3155679702759, 'accumulated_submission_time': 527.4489407539368, 'accumulated_eval_time': 203.32546520233154, 'accumulated_logging_time': 0.16431093215942383}
I0307 16:18:31.202676 140345924884224 logging_writer.py:48] [4802] accumulated_eval_time=203.325, accumulated_logging_time=0.164311, accumulated_submission_time=527.449, global_step=4802, preemption_count=0, score=527.449, test/loss=0.29023, test/num_examples=3581, test/ssim=0.738183, total_duration=731.316, train/loss=0.272643, train/ssim=0.740651, validation/loss=0.288726, validation/num_examples=3554, validation/ssim=0.720959
I0307 16:18:39.354961 140345438369536 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.06777601689100266, loss=0.29636484384536743
I0307 16:18:47.555840 140345924884224 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.10832741856575012, loss=0.31975850462913513
I0307 16:18:55.778378 140345438369536 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.15416377782821655, loss=0.25850117206573486
I0307 16:19:03.972783 140345924884224 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.07405835390090942, loss=0.27707940340042114
I0307 16:19:12.186752 140345438369536 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.23508210480213165, loss=0.25924915075302124
I0307 16:19:20.392617 140345924884224 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.11724776774644852, loss=0.3500435948371887
I0307 16:19:28.612942 140345438369536 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.08995619416236877, loss=0.26454997062683105
I0307 16:19:36.829649 140345924884224 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.11108789592981339, loss=0.23558661341667175
I0307 16:19:45.050710 140345438369536 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.08707023411989212, loss=0.27355796098709106
I0307 16:19:51.202190 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:19:52.502439 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:19:53.803617 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:19:55.111529 140503400502464 submission_runner.py:469] Time since start: 815.23s, 	Step: 5776, 	{'train/ssim': 0.7413891383579799, 'train/loss': 0.2716599702835083, 'validation/ssim': 0.7219010215162492, 'validation/loss': 0.2876522684607133, 'validation/num_examples': 3554, 'test/ssim': 0.7390823257731779, 'test/loss': 0.2891841612525307, 'test/num_examples': 3581, 'score': 607.3876566886902, 'total_duration': 815.2342691421509, 'accumulated_submission_time': 607.3876566886902, 'accumulated_eval_time': 207.23476457595825, 'accumulated_logging_time': 0.18202877044677734}
I0307 16:19:55.121530 140345924884224 logging_writer.py:48] [5776] accumulated_eval_time=207.235, accumulated_logging_time=0.182029, accumulated_submission_time=607.388, global_step=5776, preemption_count=0, score=607.388, test/loss=0.289184, test/num_examples=3581, test/ssim=0.739082, total_duration=815.234, train/loss=0.27166, train/ssim=0.741389, validation/loss=0.287652, validation/num_examples=3554, validation/ssim=0.721901
I0307 16:19:57.196868 140345438369536 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.09502346813678741, loss=0.3512720763683319
I0307 16:20:05.418253 140345924884224 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.10641977190971375, loss=0.23442065715789795
I0307 16:20:13.631078 140345438369536 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.058913808315992355, loss=0.316028356552124
I0307 16:20:21.846936 140345924884224 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.13912351429462433, loss=0.3205787241458893
I0307 16:20:30.072027 140345438369536 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.14162853360176086, loss=0.24164435267448425
I0307 16:20:38.288052 140345924884224 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.1713310331106186, loss=0.29963499307632446
I0307 16:20:46.520292 140345438369536 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.10999294370412827, loss=0.26590031385421753
I0307 16:20:54.737361 140345924884224 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1637977957725525, loss=0.2629828155040741
I0307 16:21:02.965663 140345438369536 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.1340915411710739, loss=0.2194860279560089
I0307 16:21:11.180288 140345924884224 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.1256057471036911, loss=0.2587338984012604
I0307 16:21:15.113942 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:21:16.415208 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:21:17.717911 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:21:19.022707 140503400502464 submission_runner.py:469] Time since start: 899.15s, 	Step: 6749, 	{'train/ssim': 0.7416739463806152, 'train/loss': 0.2714054584503174, 'validation/ssim': 0.7217593045468135, 'validation/loss': 0.2876997020852385, 'validation/num_examples': 3554, 'test/ssim': 0.7390866890795169, 'test/loss': 0.2891263815318696, 'test/num_examples': 3581, 'score': 687.3203332424164, 'total_duration': 899.1454448699951, 'accumulated_submission_time': 687.3203332424164, 'accumulated_eval_time': 211.14348554611206, 'accumulated_logging_time': 0.20027565956115723}
I0307 16:21:19.032172 140345438369536 logging_writer.py:48] [6749] accumulated_eval_time=211.143, accumulated_logging_time=0.200276, accumulated_submission_time=687.32, global_step=6749, preemption_count=0, score=687.32, test/loss=0.289126, test/num_examples=3581, test/ssim=0.739087, total_duration=899.145, train/loss=0.271405, train/ssim=0.741674, validation/loss=0.2877, validation/num_examples=3554, validation/ssim=0.721759
I0307 16:21:23.318729 140345924884224 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.10589249432086945, loss=0.25038841366767883
I0307 16:21:31.539309 140345438369536 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.13733866810798645, loss=0.3343967795372009
I0307 16:21:39.763703 140345924884224 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.46753260493278503, loss=0.20678532123565674
I0307 16:21:47.985683 140345438369536 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.1399899572134018, loss=0.2892678380012512
I0307 16:21:56.193348 140345924884224 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.137831911444664, loss=0.3681638836860657
I0307 16:22:04.416483 140345438369536 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.2188720554113388, loss=0.28363966941833496
I0307 16:22:12.642399 140345924884224 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.16462941467761993, loss=0.18105973303318024
I0307 16:22:20.869312 140345438369536 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.1179049015045166, loss=0.3363681137561798
I0307 16:22:29.084941 140345924884224 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.21514616906642914, loss=0.27082952857017517
I0307 16:22:37.300086 140345438369536 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.19653242826461792, loss=0.2686883211135864
I0307 16:22:39.027291 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:22:40.327191 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:22:41.630017 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:22:42.933002 140503400502464 submission_runner.py:469] Time since start: 983.06s, 	Step: 7722, 	{'train/ssim': 0.7436167853219169, 'train/loss': 0.2704251493726458, 'validation/ssim': 0.7232916063986001, 'validation/loss': 0.2870169120621131, 'validation/num_examples': 3554, 'test/ssim': 0.74046617564926, 'test/loss': 0.2884939748193591, 'test/num_examples': 3581, 'score': 767.2559506893158, 'total_duration': 983.055739402771, 'accumulated_submission_time': 767.2559506893158, 'accumulated_eval_time': 215.04915618896484, 'accumulated_logging_time': 0.2176833152770996}
I0307 16:22:42.943407 140345924884224 logging_writer.py:48] [7722] accumulated_eval_time=215.049, accumulated_logging_time=0.217683, accumulated_submission_time=767.256, global_step=7722, preemption_count=0, score=767.256, test/loss=0.288494, test/num_examples=3581, test/ssim=0.740466, total_duration=983.056, train/loss=0.270425, train/ssim=0.743617, validation/loss=0.287017, validation/num_examples=3554, validation/ssim=0.723292
I0307 16:22:49.445670 140345438369536 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.5028936862945557, loss=0.19670535624027252
I0307 16:22:57.670156 140345924884224 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.046897608786821365, loss=0.2893895208835602
I0307 16:23:05.878794 140345438369536 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.20210763812065125, loss=0.34833091497421265
I0307 16:23:14.093979 140345924884224 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.08349871635437012, loss=0.27815762162208557
I0307 16:23:22.309469 140345438369536 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.14928875863552094, loss=0.40089190006256104
I0307 16:23:30.518857 140345924884224 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3884192109107971, loss=0.2666773200035095
I0307 16:23:38.768513 140345438369536 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.2118225246667862, loss=0.258651465177536
I0307 16:23:46.982949 140345924884224 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.08746197074651718, loss=0.2492738962173462
I0307 16:23:55.200592 140345438369536 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.12293948978185654, loss=0.3342765271663666
I0307 16:24:02.999221 140503400502464 spec.py:321] Evaluating on the training split.
I0307 16:24:04.300121 140503400502464 spec.py:333] Evaluating on the validation split.
I0307 16:24:05.602919 140503400502464 spec.py:349] Evaluating on the test split.
I0307 16:24:06.906487 140503400502464 submission_runner.py:469] Time since start: 1067.03s, 	Step: 8696, 	{'train/ssim': 0.7441639219011579, 'train/loss': 0.2704944269997733, 'validation/ssim': 0.724166363494478, 'validation/loss': 0.28669720737197524, 'validation/num_examples': 3554, 'test/ssim': 0.7414528964718305, 'test/loss': 0.28809861835904776, 'test/num_examples': 3581, 'score': 847.2528138160706, 'total_duration': 1067.029224395752, 'accumulated_submission_time': 847.2528138160706, 'accumulated_eval_time': 218.95638418197632, 'accumulated_logging_time': 0.23604798316955566}
I0307 16:24:06.917022 140345924884224 logging_writer.py:48] [8696] accumulated_eval_time=218.956, accumulated_logging_time=0.236048, accumulated_submission_time=847.253, global_step=8696, preemption_count=0, score=847.253, test/loss=0.288099, test/num_examples=3581, test/ssim=0.741453, total_duration=1067.03, train/loss=0.270494, train/ssim=0.744164, validation/loss=0.286697, validation/num_examples=3554, validation/ssim=0.724166
I0307 16:24:06.929231 140345438369536 logging_writer.py:48] [8696] global_step=8696, preemption_count=0, score=847.253
I0307 16:24:07.918159 140503400502464 submission_runner.py:646] Tuning trial 1/5
I0307 16:24:07.918382 140503400502464 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 16:24:07.919132 140503400502464 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.18552499158041819, 'train/loss': 1.2679830278669084, 'validation/ssim': 0.1769749389717132, 'validation/loss': 1.2802880007561903, 'validation/num_examples': 3554, 'test/ssim': 0.1960801862531852, 'test/loss': 1.2780091542166991, 'test/num_examples': 3581, 'score': 47.56694793701172, 'total_duration': 226.78998112678528, 'accumulated_submission_time': 47.56694793701172, 'accumulated_eval_time': 179.22285509109497, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (481, {'train/ssim': 0.7003184727260044, 'train/loss': 0.31068267141069683, 'validation/ssim': 0.6808968685152293, 'validation/loss': 0.3272965087203679, 'validation/num_examples': 3554, 'test/ssim': 0.6994568910787838, 'test/loss': 0.3290439873093235, 'test/num_examples': 3581, 'score': 127.66476726531982, 'total_duration': 311.3981935977936, 'accumulated_submission_time': 127.66476726531982, 'accumulated_eval_time': 183.62240862846375, 'accumulated_logging_time': 0.017316818237304688, 'global_step': 481, 'preemption_count': 0}), (923, {'train/ssim': 0.7199142319815499, 'train/loss': 0.29130356652396067, 'validation/ssim': 0.7021796248505205, 'validation/loss': 0.3067781172512486, 'validation/num_examples': 3554, 'test/ssim': 0.7193804295893256, 'test/loss': 0.3090006283161128, 'test/num_examples': 3581, 'score': 207.65257263183594, 'total_duration': 395.373877286911, 'accumulated_submission_time': 207.65257263183594, 'accumulated_eval_time': 187.5127284526825, 'accumulated_logging_time': 0.0491185188293457, 'global_step': 923, 'preemption_count': 0}), (1885, {'train/ssim': 0.733295372554234, 'train/loss': 0.28001182419913156, 'validation/ssim': 0.7142226133845667, 'validation/loss': 0.29560497414884634, 'validation/num_examples': 3554, 'test/ssim': 0.7313767949551452, 'test/loss': 0.29755543717057037, 'test/num_examples': 3581, 'score': 287.5713081359863, 'total_duration': 479.4594051837921, 'accumulated_submission_time': 287.5713081359863, 'accumulated_eval_time': 191.59176206588745, 'accumulated_logging_time': 0.07665657997131348, 'global_step': 1885, 'preemption_count': 0}), (2856, {'train/ssim': 0.737513814653669, 'train/loss': 0.2761204072407314, 'validation/ssim': 0.718266527372327, 'validation/loss': 0.291941490881753, 'validation/num_examples': 3554, 'test/ssim': 0.7354618723383831, 'test/loss': 0.29363967652628453, 'test/num_examples': 3581, 'score': 367.50900411605835, 'total_duration': 563.4156262874603, 'accumulated_submission_time': 367.50900411605835, 'accumulated_eval_time': 195.50573205947876, 'accumulated_logging_time': 0.12910866737365723, 'global_step': 2856, 'preemption_count': 0}), (3829, {'train/ssim': 0.7398936407906669, 'train/loss': 0.27339982986450195, 'validation/ssim': 0.7203334793322664, 'validation/loss': 0.2894187158725204, 'validation/num_examples': 3554, 'test/ssim': 0.7375842118123429, 'test/loss': 0.29093544924602066, 'test/num_examples': 3581, 'score': 447.4507918357849, 'total_duration': 647.3378970623016, 'accumulated_submission_time': 447.4507918357849, 'accumulated_eval_time': 199.41640186309814, 'accumulated_logging_time': 0.14669537544250488, 'global_step': 3829, 'preemption_count': 0}), (4802, {'train/ssim': 0.7406514031546456, 'train/loss': 0.2726429189954485, 'validation/ssim': 0.720959355875422, 'validation/loss': 0.2887256903532991, 'validation/num_examples': 3554, 'test/ssim': 0.7381828710773876, 'test/loss': 0.29023022985897795, 'test/num_examples': 3581, 'score': 527.4489407539368, 'total_duration': 731.3155679702759, 'accumulated_submission_time': 527.4489407539368, 'accumulated_eval_time': 203.32546520233154, 'accumulated_logging_time': 0.16431093215942383, 'global_step': 4802, 'preemption_count': 0}), (5776, {'train/ssim': 0.7413891383579799, 'train/loss': 0.2716599702835083, 'validation/ssim': 0.7219010215162492, 'validation/loss': 0.2876522684607133, 'validation/num_examples': 3554, 'test/ssim': 0.7390823257731779, 'test/loss': 0.2891841612525307, 'test/num_examples': 3581, 'score': 607.3876566886902, 'total_duration': 815.2342691421509, 'accumulated_submission_time': 607.3876566886902, 'accumulated_eval_time': 207.23476457595825, 'accumulated_logging_time': 0.18202877044677734, 'global_step': 5776, 'preemption_count': 0}), (6749, {'train/ssim': 0.7416739463806152, 'train/loss': 0.2714054584503174, 'validation/ssim': 0.7217593045468135, 'validation/loss': 0.2876997020852385, 'validation/num_examples': 3554, 'test/ssim': 0.7390866890795169, 'test/loss': 0.2891263815318696, 'test/num_examples': 3581, 'score': 687.3203332424164, 'total_duration': 899.1454448699951, 'accumulated_submission_time': 687.3203332424164, 'accumulated_eval_time': 211.14348554611206, 'accumulated_logging_time': 0.20027565956115723, 'global_step': 6749, 'preemption_count': 0}), (7722, {'train/ssim': 0.7436167853219169, 'train/loss': 0.2704251493726458, 'validation/ssim': 0.7232916063986001, 'validation/loss': 0.2870169120621131, 'validation/num_examples': 3554, 'test/ssim': 0.74046617564926, 'test/loss': 0.2884939748193591, 'test/num_examples': 3581, 'score': 767.2559506893158, 'total_duration': 983.055739402771, 'accumulated_submission_time': 767.2559506893158, 'accumulated_eval_time': 215.04915618896484, 'accumulated_logging_time': 0.2176833152770996, 'global_step': 7722, 'preemption_count': 0}), (8696, {'train/ssim': 0.7441639219011579, 'train/loss': 0.2704944269997733, 'validation/ssim': 0.724166363494478, 'validation/loss': 0.28669720737197524, 'validation/num_examples': 3554, 'test/ssim': 0.7414528964718305, 'test/loss': 0.28809861835904776, 'test/num_examples': 3581, 'score': 847.2528138160706, 'total_duration': 1067.029224395752, 'accumulated_submission_time': 847.2528138160706, 'accumulated_eval_time': 218.95638418197632, 'accumulated_logging_time': 0.23604798316955566, 'global_step': 8696, 'preemption_count': 0})], 'global_step': 8696}
I0307 16:24:07.919227 140503400502464 submission_runner.py:649] Timing: 847.2528138160706
I0307 16:24:07.919264 140503400502464 submission_runner.py:651] Total number of evals: 11
I0307 16:24:07.919302 140503400502464 submission_runner.py:652] ====================
I0307 16:24:07.919432 140503400502464 submission_runner.py:750] Final fastmri score: 0
