python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-735892277 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-09-09-13.log
2025-03-07 09:09:34.425693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741338575.118967       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741338575.479516       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 09:10:31.110062 140352918893760 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax.
I0307 09:10:33.709940 140352918893760 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 09:10:33.712940 140352918893760 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 09:10:33.736138 140352918893760 submission_runner.py:606] Using RNG seed -735892277
I0307 09:10:37.186131 140352918893760 submission_runner.py:615] --- Tuning run 3/5 ---
I0307 09:10:37.186356 140352918893760 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_3.
I0307 09:10:37.186579 140352918893760 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_3/hparams.json.
I0307 09:10:37.466849 140352918893760 submission_runner.py:218] Initializing dataset.
I0307 09:10:39.049972 140352918893760 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:10:39.388128 140352918893760 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:10:39.711397 140352918893760 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:10:41.690697 140352918893760 submission_runner.py:229] Initializing model.
I0307 09:11:06.582287 140352918893760 submission_runner.py:272] Initializing optimizer.
I0307 09:11:07.868452 140352918893760 submission_runner.py:279] Initializing metrics bundle.
I0307 09:11:07.868693 140352918893760 submission_runner.py:301] Initializing checkpoint and logger.
I0307 09:11:07.869789 140352918893760 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0307 09:11:07.869891 140352918893760 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_3/meta_data_0.json.
I0307 09:11:08.473219 140352918893760 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_3/flags_0.json.
I0307 09:11:08.836394 140352918893760 submission_runner.py:337] Starting training loop.
I0307 09:12:07.689082 140211384174336 logging_writer.py:48] [0] global_step=0, grad_norm=0.6882613301277161, loss=6.933265209197998
I0307 09:12:08.050182 140352918893760 spec.py:321] Evaluating on the training split.
I0307 09:12:08.512898 140352918893760 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:12:08.536929 140352918893760 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:12:08.579439 140352918893760 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:12:28.673015 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 09:12:29.225791 140352918893760 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:12:29.272696 140352918893760 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:12:29.456960 140352918893760 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:13:05.796202 140352918893760 spec.py:349] Evaluating on the test split.
I0307 09:13:06.380559 140352918893760 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:13:06.414069 140352918893760 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 09:13:06.470823 140352918893760 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:13:29.098638 140352918893760 submission_runner.py:469] Time since start: 140.26s, 	Step: 1, 	{'train/accuracy': 0.0008769132546149194, 'train/loss': 6.91168737411499, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.911850929260254, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.9116034507751465, 'test/num_examples': 10000, 'score': 59.21358060836792, 'total_duration': 140.26218605041504, 'accumulated_submission_time': 59.21358060836792, 'accumulated_eval_time': 81.04838967323303, 'accumulated_logging_time': 0}
I0307 09:13:29.108770 140197555578624 logging_writer.py:48] [1] accumulated_eval_time=81.0484, accumulated_logging_time=0, accumulated_submission_time=59.2136, global_step=1, preemption_count=0, score=59.2136, test/accuracy=0.0009, test/loss=6.9116, test/num_examples=10000, total_duration=140.262, train/accuracy=0.000876913, train/loss=6.91169, validation/accuracy=0.00096, validation/loss=6.91185, validation/num_examples=50000
I0307 09:14:05.313518 140197547185920 logging_writer.py:48] [100] global_step=100, grad_norm=0.660532534122467, loss=6.903403282165527
I0307 09:14:41.590063 140197555578624 logging_writer.py:48] [200] global_step=200, grad_norm=0.669024646282196, loss=6.857669830322266
I0307 09:15:18.974032 140197547185920 logging_writer.py:48] [300] global_step=300, grad_norm=0.709337592124939, loss=6.782736301422119
I0307 09:15:56.242947 140197555578624 logging_writer.py:48] [400] global_step=400, grad_norm=0.7419551014900208, loss=6.677526473999023
I0307 09:16:34.091596 140197547185920 logging_writer.py:48] [500] global_step=500, grad_norm=0.7866564393043518, loss=6.574863910675049
I0307 09:17:11.688721 140197555578624 logging_writer.py:48] [600] global_step=600, grad_norm=0.8195648193359375, loss=6.4852166175842285
I0307 09:17:49.710568 140197547185920 logging_writer.py:48] [700] global_step=700, grad_norm=0.942270040512085, loss=6.316039562225342
I0307 09:18:28.247640 140197555578624 logging_writer.py:48] [800] global_step=800, grad_norm=1.0323411226272583, loss=6.2673234939575195
I0307 09:19:06.047160 140197547185920 logging_writer.py:48] [900] global_step=900, grad_norm=1.660710334777832, loss=6.083968162536621
I0307 09:19:43.870234 140197555578624 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.5672533512115479, loss=5.990311145782471
I0307 09:20:22.210355 140197547185920 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.1749162673950195, loss=5.940195083618164
I0307 09:21:00.595132 140197555578624 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.134967565536499, loss=5.823344707489014
I0307 09:21:39.074546 140197547185920 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.6573340892791748, loss=5.702861309051514
I0307 09:21:59.462718 140352918893760 spec.py:321] Evaluating on the training split.
I0307 09:22:11.475219 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 09:22:45.662337 140352918893760 spec.py:349] Evaluating on the test split.
I0307 09:22:47.580765 140352918893760 submission_runner.py:469] Time since start: 698.74s, 	Step: 1354, 	{'train/accuracy': 0.059091996401548386, 'train/loss': 5.493776798248291, 'validation/accuracy': 0.05179999768733978, 'validation/loss': 5.573646068572998, 'validation/num_examples': 50000, 'test/accuracy': 0.03630000352859497, 'test/loss': 5.78989315032959, 'test/num_examples': 10000, 'score': 569.3945550918579, 'total_duration': 698.7443125247955, 'accumulated_submission_time': 569.3945550918579, 'accumulated_eval_time': 129.16638731956482, 'accumulated_logging_time': 0.020788908004760742}
I0307 09:22:47.609737 140197563971328 logging_writer.py:48] [1354] accumulated_eval_time=129.166, accumulated_logging_time=0.0207889, accumulated_submission_time=569.395, global_step=1354, preemption_count=0, score=569.395, test/accuracy=0.0363, test/loss=5.78989, test/num_examples=10000, total_duration=698.744, train/accuracy=0.059092, train/loss=5.49378, validation/accuracy=0.0518, validation/loss=5.57365, validation/num_examples=50000
I0307 09:23:05.467369 140197572364032 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.8556017875671387, loss=5.666993141174316
I0307 09:23:43.600731 140197563971328 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.214613199234009, loss=5.634566307067871
I0307 09:24:21.458394 140197572364032 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.5515739917755127, loss=5.503779411315918
I0307 09:24:59.954351 140197563971328 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.031263828277588, loss=5.445249557495117
I0307 09:25:38.421521 140197572364032 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.242783308029175, loss=5.41818904876709
I0307 09:26:16.716578 140197563971328 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.639207363128662, loss=5.289968013763428
I0307 09:26:54.918887 140197572364032 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.510135173797607, loss=5.199000835418701
I0307 09:27:33.543572 140197563971328 logging_writer.py:48] [2100] global_step=2100, grad_norm=7.628291130065918, loss=5.07876443862915
I0307 09:28:11.793852 140197572364032 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.093658447265625, loss=5.045807361602783
I0307 09:28:50.139032 140197563971328 logging_writer.py:48] [2300] global_step=2300, grad_norm=9.604702949523926, loss=5.04536247253418
I0307 09:29:28.150718 140197572364032 logging_writer.py:48] [2400] global_step=2400, grad_norm=7.533233165740967, loss=4.984541893005371
I0307 09:30:06.683456 140197563971328 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.422938346862793, loss=4.885105133056641
I0307 09:30:45.110681 140197572364032 logging_writer.py:48] [2600] global_step=2600, grad_norm=6.8712382316589355, loss=4.853758811950684
I0307 09:31:17.811591 140352918893760 spec.py:321] Evaluating on the training split.
I0307 09:31:29.711960 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 09:31:48.513894 140352918893760 spec.py:349] Evaluating on the test split.
I0307 09:31:50.333268 140352918893760 submission_runner.py:469] Time since start: 1241.50s, 	Step: 2687, 	{'train/accuracy': 0.14706233143806458, 'train/loss': 4.467044830322266, 'validation/accuracy': 0.1281999945640564, 'validation/loss': 4.618517875671387, 'validation/num_examples': 50000, 'test/accuracy': 0.09490000456571579, 'test/loss': 5.007242202758789, 'test/num_examples': 10000, 'score': 1079.442931652069, 'total_duration': 1241.496834039688, 'accumulated_submission_time': 1079.442931652069, 'accumulated_eval_time': 161.68802952766418, 'accumulated_logging_time': 0.05840349197387695}
I0307 09:31:50.357123 140197563971328 logging_writer.py:48] [2687] accumulated_eval_time=161.688, accumulated_logging_time=0.0584035, accumulated_submission_time=1079.44, global_step=2687, preemption_count=0, score=1079.44, test/accuracy=0.0949, test/loss=5.00724, test/num_examples=10000, total_duration=1241.5, train/accuracy=0.147062, train/loss=4.46704, validation/accuracy=0.1282, validation/loss=4.61852, validation/num_examples=50000
I0307 09:31:55.822078 140197572364032 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.304862022399902, loss=4.79079008102417
I0307 09:32:34.396837 140197563971328 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.821913719177246, loss=4.686905860900879
I0307 09:33:12.382703 140197572364032 logging_writer.py:48] [2900] global_step=2900, grad_norm=6.64280891418457, loss=4.836772918701172
I0307 09:33:50.303827 140197563971328 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.7769253253936768, loss=4.622505187988281
I0307 09:34:28.881542 140197572364032 logging_writer.py:48] [3100] global_step=3100, grad_norm=8.279519081115723, loss=4.541625022888184
I0307 09:35:06.590147 140197563971328 logging_writer.py:48] [3200] global_step=3200, grad_norm=5.906195640563965, loss=4.610058784484863
I0307 09:35:44.997231 140197572364032 logging_writer.py:48] [3300] global_step=3300, grad_norm=8.673589706420898, loss=4.511228561401367
I0307 09:36:22.993714 140197563971328 logging_writer.py:48] [3400] global_step=3400, grad_norm=8.167490005493164, loss=4.4648919105529785
I0307 09:37:00.895061 140197572364032 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.626564979553223, loss=4.345259666442871
I0307 09:37:39.350692 140197563971328 logging_writer.py:48] [3600] global_step=3600, grad_norm=6.923275947570801, loss=4.45728874206543
I0307 09:38:17.726333 140197572364032 logging_writer.py:48] [3700] global_step=3700, grad_norm=7.968555927276611, loss=4.341304302215576
I0307 09:38:55.649999 140197563971328 logging_writer.py:48] [3800] global_step=3800, grad_norm=6.386529445648193, loss=4.210487365722656
I0307 09:39:34.316913 140197572364032 logging_writer.py:48] [3900] global_step=3900, grad_norm=6.598545551300049, loss=4.2074384689331055
I0307 09:40:12.928440 140197563971328 logging_writer.py:48] [4000] global_step=4000, grad_norm=8.124680519104004, loss=4.262001991271973
I0307 09:40:20.573663 140352918893760 spec.py:321] Evaluating on the training split.
I0307 09:40:31.803422 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 09:40:50.789110 140352918893760 spec.py:349] Evaluating on the test split.
I0307 09:40:52.670037 140352918893760 submission_runner.py:469] Time since start: 1783.83s, 	Step: 4021, 	{'train/accuracy': 0.23419561982154846, 'train/loss': 3.7651970386505127, 'validation/accuracy': 0.20155999064445496, 'validation/loss': 4.000546455383301, 'validation/num_examples': 50000, 'test/accuracy': 0.1558000147342682, 'test/loss': 4.543863296508789, 'test/num_examples': 10000, 'score': 1589.5220942497253, 'total_duration': 1783.8335990905762, 'accumulated_submission_time': 1589.5220942497253, 'accumulated_eval_time': 193.78435516357422, 'accumulated_logging_time': 0.09580397605895996}
I0307 09:40:52.679324 140197572364032 logging_writer.py:48] [4021] accumulated_eval_time=193.784, accumulated_logging_time=0.095804, accumulated_submission_time=1589.52, global_step=4021, preemption_count=0, score=1589.52, test/accuracy=0.1558, test/loss=4.54386, test/num_examples=10000, total_duration=1783.83, train/accuracy=0.234196, train/loss=3.7652, validation/accuracy=0.20156, validation/loss=4.00055, validation/num_examples=50000
I0307 09:41:23.499802 140197563971328 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.856635570526123, loss=4.046872138977051
I0307 09:42:01.938414 140197572364032 logging_writer.py:48] [4200] global_step=4200, grad_norm=12.196403503417969, loss=4.1641435623168945
I0307 09:42:40.438774 140197563971328 logging_writer.py:48] [4300] global_step=4300, grad_norm=7.380473613739014, loss=4.013304233551025
I0307 09:43:18.691334 140197572364032 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.8304615020751953, loss=3.959407329559326
I0307 09:43:57.140940 140197563971328 logging_writer.py:48] [4500] global_step=4500, grad_norm=8.293968200683594, loss=3.9671995639801025
I0307 09:44:35.874686 140197572364032 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.3276567459106445, loss=3.8874502182006836
I0307 09:45:14.221055 140197563971328 logging_writer.py:48] [4700] global_step=4700, grad_norm=9.284388542175293, loss=3.9154059886932373
I0307 09:45:52.712720 140197572364032 logging_writer.py:48] [4800] global_step=4800, grad_norm=9.794356346130371, loss=3.768815517425537
I0307 09:46:30.962152 140197563971328 logging_writer.py:48] [4900] global_step=4900, grad_norm=15.461101531982422, loss=3.717820405960083
I0307 09:47:09.298666 140197572364032 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.648195743560791, loss=3.649705648422241
I0307 09:47:47.987502 140197563971328 logging_writer.py:48] [5100] global_step=5100, grad_norm=6.256418228149414, loss=3.7950527667999268
I0307 09:48:26.496162 140197572364032 logging_writer.py:48] [5200] global_step=5200, grad_norm=8.832375526428223, loss=3.6468851566314697
I0307 09:49:04.892503 140197563971328 logging_writer.py:48] [5300] global_step=5300, grad_norm=7.630593776702881, loss=3.665360450744629
I0307 09:49:22.706086 140352918893760 spec.py:321] Evaluating on the training split.
I0307 09:49:33.819165 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 09:49:54.355692 140352918893760 spec.py:349] Evaluating on the test split.
I0307 09:49:56.097506 140352918893760 submission_runner.py:469] Time since start: 2327.26s, 	Step: 5348, 	{'train/accuracy': 0.3355787694454193, 'train/loss': 3.090468645095825, 'validation/accuracy': 0.29679998755455017, 'validation/loss': 3.333317756652832, 'validation/num_examples': 50000, 'test/accuracy': 0.22510001063346863, 'test/loss': 3.9231512546539307, 'test/num_examples': 10000, 'score': 2099.42506980896, 'total_duration': 2327.261080265045, 'accumulated_submission_time': 2099.42506980896, 'accumulated_eval_time': 227.1757457256317, 'accumulated_logging_time': 0.11387968063354492}
I0307 09:49:56.118153 140197572364032 logging_writer.py:48] [5348] accumulated_eval_time=227.176, accumulated_logging_time=0.11388, accumulated_submission_time=2099.43, global_step=5348, preemption_count=0, score=2099.43, test/accuracy=0.2251, test/loss=3.92315, test/num_examples=10000, total_duration=2327.26, train/accuracy=0.335579, train/loss=3.09047, validation/accuracy=0.2968, validation/loss=3.33332, validation/num_examples=50000
I0307 09:50:16.349178 140197563971328 logging_writer.py:48] [5400] global_step=5400, grad_norm=6.963141918182373, loss=3.55232572555542
I0307 09:50:54.751321 140197572364032 logging_writer.py:48] [5500] global_step=5500, grad_norm=10.405638694763184, loss=3.6701505184173584
I0307 09:51:32.790126 140197563971328 logging_writer.py:48] [5600] global_step=5600, grad_norm=6.961201190948486, loss=3.5343196392059326
I0307 09:52:10.779074 140197572364032 logging_writer.py:48] [5700] global_step=5700, grad_norm=5.307099342346191, loss=3.464395046234131
I0307 09:52:49.381333 140197563971328 logging_writer.py:48] [5800] global_step=5800, grad_norm=6.605583190917969, loss=3.539360284805298
I0307 09:53:27.563038 140197572364032 logging_writer.py:48] [5900] global_step=5900, grad_norm=6.315997123718262, loss=3.4470903873443604
I0307 09:54:06.118535 140197563971328 logging_writer.py:48] [6000] global_step=6000, grad_norm=13.179420471191406, loss=3.49902606010437
I0307 09:54:44.358412 140197572364032 logging_writer.py:48] [6100] global_step=6100, grad_norm=7.404033660888672, loss=3.3398489952087402
I0307 09:55:22.868320 140197563971328 logging_writer.py:48] [6200] global_step=6200, grad_norm=7.11270809173584, loss=3.4263803958892822
I0307 09:56:00.956642 140197572364032 logging_writer.py:48] [6300] global_step=6300, grad_norm=9.70183277130127, loss=3.3634183406829834
I0307 09:56:39.130320 140197563971328 logging_writer.py:48] [6400] global_step=6400, grad_norm=8.802194595336914, loss=3.2394094467163086
I0307 09:57:17.379634 140197572364032 logging_writer.py:48] [6500] global_step=6500, grad_norm=5.631226539611816, loss=3.306457757949829
I0307 09:57:55.480218 140197563971328 logging_writer.py:48] [6600] global_step=6600, grad_norm=8.33063793182373, loss=3.176884651184082
I0307 09:58:26.436210 140352918893760 spec.py:321] Evaluating on the training split.
I0307 09:58:38.221544 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 09:58:59.070976 140352918893760 spec.py:349] Evaluating on the test split.
I0307 09:59:00.868552 140352918893760 submission_runner.py:469] Time since start: 2872.03s, 	Step: 6682, 	{'train/accuracy': 0.39568716287612915, 'train/loss': 2.6923294067382812, 'validation/accuracy': 0.354559987783432, 'validation/loss': 2.964087963104248, 'validation/num_examples': 50000, 'test/accuracy': 0.2713000178337097, 'test/loss': 3.6099040508270264, 'test/num_examples': 10000, 'score': 2609.6209440231323, 'total_duration': 2872.0320682525635, 'accumulated_submission_time': 2609.6209440231323, 'accumulated_eval_time': 261.6079971790314, 'accumulated_logging_time': 0.1436445713043213}
I0307 09:59:00.923783 140197572364032 logging_writer.py:48] [6682] accumulated_eval_time=261.608, accumulated_logging_time=0.143645, accumulated_submission_time=2609.62, global_step=6682, preemption_count=0, score=2609.62, test/accuracy=0.2713, test/loss=3.6099, test/num_examples=10000, total_duration=2872.03, train/accuracy=0.395687, train/loss=2.69233, validation/accuracy=0.35456, validation/loss=2.96409, validation/num_examples=50000
I0307 09:59:08.276756 140197563971328 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.437033653259277, loss=3.047908306121826
I0307 09:59:46.512461 140197572364032 logging_writer.py:48] [6800] global_step=6800, grad_norm=11.33709716796875, loss=3.1820032596588135
I0307 10:00:25.055912 140197563971328 logging_writer.py:48] [6900] global_step=6900, grad_norm=8.392318725585938, loss=3.079096794128418
I0307 10:01:03.074472 140197572364032 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.1371870040893555, loss=3.21806001663208
I0307 10:01:41.516799 140197563971328 logging_writer.py:48] [7100] global_step=7100, grad_norm=9.92280101776123, loss=3.0853447914123535
I0307 10:02:19.645647 140197572364032 logging_writer.py:48] [7200] global_step=7200, grad_norm=6.797918796539307, loss=3.107725143432617
I0307 10:02:57.886677 140197563971328 logging_writer.py:48] [7300] global_step=7300, grad_norm=9.02743148803711, loss=3.147383213043213
I0307 10:03:36.036628 140197572364032 logging_writer.py:48] [7400] global_step=7400, grad_norm=7.074491024017334, loss=2.898648977279663
I0307 10:04:14.994680 140197563971328 logging_writer.py:48] [7500] global_step=7500, grad_norm=7.4186577796936035, loss=3.0471441745758057
I0307 10:04:53.189378 140197572364032 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.327775001525879, loss=2.8974146842956543
I0307 10:05:31.474834 140197563971328 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.9654717445373535, loss=3.0525338649749756
I0307 10:06:10.122055 140197572364032 logging_writer.py:48] [7800] global_step=7800, grad_norm=8.149975776672363, loss=2.994703531265259
I0307 10:06:48.681240 140197563971328 logging_writer.py:48] [7900] global_step=7900, grad_norm=7.481416702270508, loss=2.9569778442382812
I0307 10:07:26.845929 140197572364032 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.114480495452881, loss=3.053318500518799
I0307 10:07:31.068970 140352918893760 spec.py:321] Evaluating on the training split.
I0307 10:07:42.576231 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 10:08:03.222926 140352918893760 spec.py:349] Evaluating on the test split.
I0307 10:08:05.036132 140352918893760 submission_runner.py:469] Time since start: 3416.20s, 	Step: 8012, 	{'train/accuracy': 0.4654017686843872, 'train/loss': 2.3428409099578857, 'validation/accuracy': 0.4157799780368805, 'validation/loss': 2.6067278385162354, 'validation/num_examples': 50000, 'test/accuracy': 0.31690001487731934, 'test/loss': 3.3188130855560303, 'test/num_examples': 10000, 'score': 3119.636923313141, 'total_duration': 3416.199696779251, 'accumulated_submission_time': 3119.636923313141, 'accumulated_eval_time': 295.5751292705536, 'accumulated_logging_time': 0.21811509132385254}
I0307 10:08:05.105762 140197563971328 logging_writer.py:48] [8012] accumulated_eval_time=295.575, accumulated_logging_time=0.218115, accumulated_submission_time=3119.64, global_step=8012, preemption_count=0, score=3119.64, test/accuracy=0.3169, test/loss=3.31881, test/num_examples=10000, total_duration=3416.2, train/accuracy=0.465402, train/loss=2.34284, validation/accuracy=0.41578, validation/loss=2.60673, validation/num_examples=50000
I0307 10:08:38.946625 140197572364032 logging_writer.py:48] [8100] global_step=8100, grad_norm=5.386093616485596, loss=3.0241787433624268
I0307 10:09:17.418172 140197563971328 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.9393179416656494, loss=2.811508893966675
I0307 10:09:55.392349 140197572364032 logging_writer.py:48] [8300] global_step=8300, grad_norm=7.371183395385742, loss=2.8826122283935547
I0307 10:10:33.674081 140197563971328 logging_writer.py:48] [8400] global_step=8400, grad_norm=9.051780700683594, loss=2.977949619293213
I0307 10:11:12.087395 140197572364032 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.4607415199279785, loss=2.6920571327209473
I0307 10:11:50.394864 140197563971328 logging_writer.py:48] [8600] global_step=8600, grad_norm=8.86303424835205, loss=2.8859236240386963
I0307 10:12:29.312749 140197572364032 logging_writer.py:48] [8700] global_step=8700, grad_norm=8.648202896118164, loss=2.777778148651123
I0307 10:13:07.549344 140197563971328 logging_writer.py:48] [8800] global_step=8800, grad_norm=7.8249030113220215, loss=2.837052345275879
I0307 10:13:46.171168 140197572364032 logging_writer.py:48] [8900] global_step=8900, grad_norm=8.060517311096191, loss=2.778022289276123
I0307 10:14:24.427547 140197563971328 logging_writer.py:48] [9000] global_step=9000, grad_norm=5.004417419433594, loss=2.5564849376678467
I0307 10:15:02.791689 140197572364032 logging_writer.py:48] [9100] global_step=9100, grad_norm=5.988569736480713, loss=2.649399757385254
I0307 10:15:41.289658 140197563971328 logging_writer.py:48] [9200] global_step=9200, grad_norm=8.804109573364258, loss=2.6994590759277344
I0307 10:16:19.563858 140197572364032 logging_writer.py:48] [9300] global_step=9300, grad_norm=7.688720226287842, loss=2.661423921585083
I0307 10:16:35.294433 140352918893760 spec.py:321] Evaluating on the training split.
I0307 10:16:47.483027 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 10:17:08.878074 140352918893760 spec.py:349] Evaluating on the test split.
I0307 10:17:10.647185 140352918893760 submission_runner.py:469] Time since start: 3961.81s, 	Step: 9342, 	{'train/accuracy': 0.5025709271430969, 'train/loss': 2.1312415599823, 'validation/accuracy': 0.4534199833869934, 'validation/loss': 2.3976571559906006, 'validation/num_examples': 50000, 'test/accuracy': 0.34700000286102295, 'test/loss': 3.103010892868042, 'test/num_examples': 10000, 'score': 3629.697919845581, 'total_duration': 3961.8107562065125, 'accumulated_submission_time': 3629.697919845581, 'accumulated_eval_time': 330.9278519153595, 'accumulated_logging_time': 0.3063318729400635}
I0307 10:17:10.676378 140197563971328 logging_writer.py:48] [9342] accumulated_eval_time=330.928, accumulated_logging_time=0.306332, accumulated_submission_time=3629.7, global_step=9342, preemption_count=0, score=3629.7, test/accuracy=0.347, test/loss=3.10301, test/num_examples=10000, total_duration=3961.81, train/accuracy=0.502571, train/loss=2.13124, validation/accuracy=0.45342, validation/loss=2.39766, validation/num_examples=50000
I0307 10:17:33.295661 140197572364032 logging_writer.py:48] [9400] global_step=9400, grad_norm=6.682357311248779, loss=2.587278127670288
I0307 10:18:11.718791 140197563971328 logging_writer.py:48] [9500] global_step=9500, grad_norm=7.655394077301025, loss=2.737818956375122
I0307 10:18:50.195422 140197572364032 logging_writer.py:48] [9600] global_step=9600, grad_norm=5.137876510620117, loss=2.786735773086548
I0307 10:19:28.865888 140197563971328 logging_writer.py:48] [9700] global_step=9700, grad_norm=5.826737403869629, loss=2.8074402809143066
I0307 10:20:07.418332 140197572364032 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.609833717346191, loss=2.5847558975219727
I0307 10:20:45.999357 140197563971328 logging_writer.py:48] [9900] global_step=9900, grad_norm=5.505930423736572, loss=2.5659756660461426
I0307 10:21:24.219702 140197572364032 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.947890758514404, loss=2.623988628387451
I0307 10:22:02.663590 140197563971328 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.491329193115234, loss=2.6380205154418945
I0307 10:22:41.375457 140197572364032 logging_writer.py:48] [10200] global_step=10200, grad_norm=5.198843002319336, loss=2.5286827087402344
I0307 10:23:19.642402 140197563971328 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.233313083648682, loss=2.6492085456848145
I0307 10:23:57.983285 140197572364032 logging_writer.py:48] [10400] global_step=10400, grad_norm=5.583529949188232, loss=2.6332075595855713
I0307 10:24:36.490768 140197563971328 logging_writer.py:48] [10500] global_step=10500, grad_norm=8.464863777160645, loss=2.5062575340270996
I0307 10:25:14.780086 140197572364032 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.900915622711182, loss=2.628037691116333
I0307 10:25:40.672037 140352918893760 spec.py:321] Evaluating on the training split.
I0307 10:25:53.722080 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 10:26:15.346116 140352918893760 spec.py:349] Evaluating on the test split.
I0307 10:26:17.196606 140352918893760 submission_runner.py:469] Time since start: 4508.32s, 	Step: 10668, 	{'train/accuracy': 0.5410953164100647, 'train/loss': 1.9475765228271484, 'validation/accuracy': 0.49091997742652893, 'validation/loss': 2.207526683807373, 'validation/num_examples': 50000, 'test/accuracy': 0.3695000112056732, 'test/loss': 2.921912670135498, 'test/num_examples': 10000, 'score': 4139.552673101425, 'total_duration': 4508.319432735443, 'accumulated_submission_time': 4139.552673101425, 'accumulated_eval_time': 367.41164231300354, 'accumulated_logging_time': 0.3439664840698242}
I0307 10:26:17.262136 140197563971328 logging_writer.py:48] [10668] accumulated_eval_time=367.412, accumulated_logging_time=0.343966, accumulated_submission_time=4139.55, global_step=10668, preemption_count=0, score=4139.55, test/accuracy=0.3695, test/loss=2.92191, test/num_examples=10000, total_duration=4508.32, train/accuracy=0.541095, train/loss=1.94758, validation/accuracy=0.49092, validation/loss=2.20753, validation/num_examples=50000
I0307 10:26:29.945883 140197572364032 logging_writer.py:48] [10700] global_step=10700, grad_norm=8.46496295928955, loss=2.432070732116699
I0307 10:27:09.007077 140197563971328 logging_writer.py:48] [10800] global_step=10800, grad_norm=7.271470069885254, loss=2.5326900482177734
I0307 10:27:47.378472 140197572364032 logging_writer.py:48] [10900] global_step=10900, grad_norm=6.957669734954834, loss=2.4142398834228516
I0307 10:28:26.513622 140197563971328 logging_writer.py:48] [11000] global_step=11000, grad_norm=5.89660120010376, loss=2.6100380420684814
I0307 10:29:05.194630 140197572364032 logging_writer.py:48] [11100] global_step=11100, grad_norm=6.259066581726074, loss=2.600003719329834
I0307 10:29:43.806384 140197563971328 logging_writer.py:48] [11200] global_step=11200, grad_norm=6.97894287109375, loss=2.5288257598876953
I0307 10:30:22.719110 140197572364032 logging_writer.py:48] [11300] global_step=11300, grad_norm=8.096973419189453, loss=2.325019121170044
I0307 10:31:00.910588 140197563971328 logging_writer.py:48] [11400] global_step=11400, grad_norm=8.82150936126709, loss=2.51324725151062
I0307 10:31:39.078927 140197572364032 logging_writer.py:48] [11500] global_step=11500, grad_norm=8.246939659118652, loss=2.489914894104004
I0307 10:32:17.494298 140197563971328 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.477855205535889, loss=2.512162923812866
I0307 10:32:56.014086 140197572364032 logging_writer.py:48] [11700] global_step=11700, grad_norm=7.057440757751465, loss=2.5200998783111572
I0307 10:33:34.300427 140197563971328 logging_writer.py:48] [11800] global_step=11800, grad_norm=6.1607232093811035, loss=2.349717617034912
I0307 10:34:12.654646 140197572364032 logging_writer.py:48] [11900] global_step=11900, grad_norm=6.349006652832031, loss=2.4161224365234375
I0307 10:34:47.495115 140352918893760 spec.py:321] Evaluating on the training split.
I0307 10:35:05.062854 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 10:35:27.733925 140352918893760 spec.py:349] Evaluating on the test split.
I0307 10:35:29.543436 140352918893760 submission_runner.py:469] Time since start: 5060.71s, 	Step: 11991, 	{'train/accuracy': 0.5500836968421936, 'train/loss': 1.8846064805984497, 'validation/accuracy': 0.500819981098175, 'validation/loss': 2.170717716217041, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 2.9513235092163086, 'test/num_examples': 10000, 'score': 4649.585605144501, 'total_duration': 5060.706884860992, 'accumulated_submission_time': 4649.585605144501, 'accumulated_eval_time': 409.4598104953766, 'accumulated_logging_time': 0.47458720207214355}
I0307 10:35:29.623036 140197563971328 logging_writer.py:48] [11991] accumulated_eval_time=409.46, accumulated_logging_time=0.474587, accumulated_submission_time=4649.59, global_step=11991, preemption_count=0, score=4649.59, test/accuracy=0.3887, test/loss=2.95132, test/num_examples=10000, total_duration=5060.71, train/accuracy=0.550084, train/loss=1.88461, validation/accuracy=0.50082, validation/loss=2.17072, validation/num_examples=50000
I0307 10:35:33.512400 140197572364032 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.583080291748047, loss=2.5299782752990723
I0307 10:36:11.832420 140197563971328 logging_writer.py:48] [12100] global_step=12100, grad_norm=4.721802711486816, loss=2.358027458190918
I0307 10:36:49.906244 140197572364032 logging_writer.py:48] [12200] global_step=12200, grad_norm=5.695642948150635, loss=2.3141863346099854
I0307 10:37:28.550646 140197563971328 logging_writer.py:48] [12300] global_step=12300, grad_norm=9.200907707214355, loss=2.3784987926483154
I0307 10:38:07.068809 140197572364032 logging_writer.py:48] [12400] global_step=12400, grad_norm=6.2231926918029785, loss=2.340413808822632
I0307 10:38:45.915221 140197563971328 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.87019681930542, loss=2.355086326599121
I0307 10:39:24.893326 140197572364032 logging_writer.py:48] [12600] global_step=12600, grad_norm=6.912930011749268, loss=2.425044298171997
I0307 10:40:03.290974 140197563971328 logging_writer.py:48] [12700] global_step=12700, grad_norm=6.533393859863281, loss=2.430866003036499
I0307 10:40:42.164352 140197572364032 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.663695335388184, loss=2.308927536010742
I0307 10:41:20.911645 140197563971328 logging_writer.py:48] [12900] global_step=12900, grad_norm=8.838713645935059, loss=2.3981356620788574
I0307 10:41:59.308597 140197572364032 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.538148880004883, loss=2.2402820587158203
I0307 10:42:37.886450 140197563971328 logging_writer.py:48] [13100] global_step=13100, grad_norm=7.864748001098633, loss=2.287449598312378
I0307 10:43:16.355695 140197572364032 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.912257671356201, loss=2.3410346508026123
I0307 10:43:55.129147 140197563971328 logging_writer.py:48] [13300] global_step=13300, grad_norm=8.036498069763184, loss=2.311943531036377
I0307 10:43:59.776915 140352918893760 spec.py:321] Evaluating on the training split.
I0307 10:44:17.064035 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 10:44:39.278332 140352918893760 spec.py:349] Evaluating on the test split.
I0307 10:44:41.090455 140352918893760 submission_runner.py:469] Time since start: 5612.25s, 	Step: 13313, 	{'train/accuracy': 0.5772281289100647, 'train/loss': 1.7610937356948853, 'validation/accuracy': 0.523099958896637, 'validation/loss': 2.0334198474884033, 'validation/num_examples': 50000, 'test/accuracy': 0.4066000282764435, 'test/loss': 2.750124216079712, 'test/num_examples': 10000, 'score': 5159.578532218933, 'total_duration': 5612.253901004791, 'accumulated_submission_time': 5159.578532218933, 'accumulated_eval_time': 450.77318930625916, 'accumulated_logging_time': 0.5811779499053955}
I0307 10:44:41.224733 140197572364032 logging_writer.py:48] [13313] accumulated_eval_time=450.773, accumulated_logging_time=0.581178, accumulated_submission_time=5159.58, global_step=13313, preemption_count=0, score=5159.58, test/accuracy=0.4066, test/loss=2.75012, test/num_examples=10000, total_duration=5612.25, train/accuracy=0.577228, train/loss=1.76109, validation/accuracy=0.5231, validation/loss=2.03342, validation/num_examples=50000
I0307 10:45:15.501122 140197563971328 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.6527910232543945, loss=2.2757408618927
I0307 10:45:53.789789 140197572364032 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.892958164215088, loss=2.34883975982666
I0307 10:46:32.838408 140197563971328 logging_writer.py:48] [13600] global_step=13600, grad_norm=6.397205352783203, loss=2.2933497428894043
I0307 10:47:11.292485 140197572364032 logging_writer.py:48] [13700] global_step=13700, grad_norm=5.1164350509643555, loss=2.3483517169952393
I0307 10:47:51.073214 140197563971328 logging_writer.py:48] [13800] global_step=13800, grad_norm=8.369650840759277, loss=2.436649799346924
I0307 10:48:28.845695 140197572364032 logging_writer.py:48] [13900] global_step=13900, grad_norm=5.17536735534668, loss=2.3654069900512695
I0307 10:49:07.375767 140197563971328 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.348730564117432, loss=2.323068857192993
I0307 10:49:46.043669 140197572364032 logging_writer.py:48] [14100] global_step=14100, grad_norm=7.821998596191406, loss=2.3874878883361816
I0307 10:50:24.631082 140197563971328 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.855429172515869, loss=2.2147326469421387
I0307 10:51:03.676380 140197572364032 logging_writer.py:48] [14300] global_step=14300, grad_norm=6.5889387130737305, loss=2.351391077041626
I0307 10:51:42.163556 140197563971328 logging_writer.py:48] [14400] global_step=14400, grad_norm=6.56697940826416, loss=2.369441509246826
I0307 10:52:20.547343 140197572364032 logging_writer.py:48] [14500] global_step=14500, grad_norm=4.963481426239014, loss=2.270646572113037
I0307 10:52:59.370832 140197563971328 logging_writer.py:48] [14600] global_step=14600, grad_norm=6.892375946044922, loss=2.3887176513671875
I0307 10:53:11.378532 140352918893760 spec.py:321] Evaluating on the training split.
I0307 10:53:30.241726 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 10:53:53.975950 140352918893760 spec.py:349] Evaluating on the test split.
I0307 10:53:55.720089 140352918893760 submission_runner.py:469] Time since start: 6166.88s, 	Step: 14632, 	{'train/accuracy': 0.5929926633834839, 'train/loss': 1.6760523319244385, 'validation/accuracy': 0.539680004119873, 'validation/loss': 1.9616038799285889, 'validation/num_examples': 50000, 'test/accuracy': 0.4223000109195709, 'test/loss': 2.6981382369995117, 'test/num_examples': 10000, 'score': 5669.556174516678, 'total_duration': 6166.8834710121155, 'accumulated_submission_time': 5669.556174516678, 'accumulated_eval_time': 495.11451840400696, 'accumulated_logging_time': 0.7391164302825928}
I0307 10:53:55.955556 140197572364032 logging_writer.py:48] [14632] accumulated_eval_time=495.115, accumulated_logging_time=0.739116, accumulated_submission_time=5669.56, global_step=14632, preemption_count=0, score=5669.56, test/accuracy=0.4223, test/loss=2.69814, test/num_examples=10000, total_duration=6166.88, train/accuracy=0.592993, train/loss=1.67605, validation/accuracy=0.53968, validation/loss=1.9616, validation/num_examples=50000
I0307 10:54:22.601808 140197563971328 logging_writer.py:48] [14700] global_step=14700, grad_norm=6.270485877990723, loss=2.15559720993042
I0307 10:55:01.260673 140197572364032 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.582919120788574, loss=2.1862244606018066
I0307 10:55:39.743716 140197563971328 logging_writer.py:48] [14900] global_step=14900, grad_norm=7.065171718597412, loss=2.234689712524414
I0307 10:56:18.124523 140197572364032 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.122755527496338, loss=2.05875301361084
I0307 10:56:57.278727 140197563971328 logging_writer.py:48] [15100] global_step=15100, grad_norm=8.359167098999023, loss=2.2725729942321777
I0307 10:57:35.677720 140197572364032 logging_writer.py:48] [15200] global_step=15200, grad_norm=8.449411392211914, loss=2.289069890975952
I0307 10:58:14.152948 140197563971328 logging_writer.py:48] [15300] global_step=15300, grad_norm=5.00075626373291, loss=2.0696334838867188
I0307 10:58:52.907099 140197572364032 logging_writer.py:48] [15400] global_step=15400, grad_norm=6.144460678100586, loss=2.1591339111328125
I0307 10:59:31.492835 140197563971328 logging_writer.py:48] [15500] global_step=15500, grad_norm=7.072298049926758, loss=2.3458075523376465
I0307 11:00:10.214706 140197572364032 logging_writer.py:48] [15600] global_step=15600, grad_norm=5.510639667510986, loss=2.172941207885742
I0307 11:00:49.182265 140197563971328 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.682753086090088, loss=2.130078077316284
I0307 11:01:27.759703 140197572364032 logging_writer.py:48] [15800] global_step=15800, grad_norm=6.544619083404541, loss=2.1979918479919434
I0307 11:02:06.300471 140197563971328 logging_writer.py:48] [15900] global_step=15900, grad_norm=7.434877872467041, loss=2.3707401752471924
I0307 11:02:25.816807 140352918893760 spec.py:321] Evaluating on the training split.
I0307 11:02:45.633295 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 11:03:06.535898 140352918893760 spec.py:349] Evaluating on the test split.
I0307 11:03:08.350913 140352918893760 submission_runner.py:469] Time since start: 6719.51s, 	Step: 15951, 	{'train/accuracy': 0.5925143361091614, 'train/loss': 1.6730601787567139, 'validation/accuracy': 0.5413599610328674, 'validation/loss': 1.9574065208435059, 'validation/num_examples': 50000, 'test/accuracy': 0.4173000156879425, 'test/loss': 2.7239809036254883, 'test/num_examples': 10000, 'score': 6179.263436555862, 'total_duration': 6719.514345169067, 'accumulated_submission_time': 6179.263436555862, 'accumulated_eval_time': 537.6484472751617, 'accumulated_logging_time': 0.996953010559082}
I0307 11:03:08.428514 140197572364032 logging_writer.py:48] [15951] accumulated_eval_time=537.648, accumulated_logging_time=0.996953, accumulated_submission_time=6179.26, global_step=15951, preemption_count=0, score=6179.26, test/accuracy=0.4173, test/loss=2.72398, test/num_examples=10000, total_duration=6719.51, train/accuracy=0.592514, train/loss=1.67306, validation/accuracy=0.54136, validation/loss=1.95741, validation/num_examples=50000
I0307 11:03:27.744771 140197563971328 logging_writer.py:48] [16000] global_step=16000, grad_norm=5.946274757385254, loss=2.219888210296631
I0307 11:04:06.265188 140197572364032 logging_writer.py:48] [16100] global_step=16100, grad_norm=4.9398884773254395, loss=2.2116236686706543
I0307 11:04:44.890067 140197563971328 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.6398797035217285, loss=2.1837377548217773
I0307 11:05:39.558855 140197572364032 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.471859455108643, loss=2.1921610832214355
I0307 11:06:17.960889 140197563971328 logging_writer.py:48] [16400] global_step=16400, grad_norm=7.007518768310547, loss=2.266481876373291
I0307 11:06:56.958546 140197572364032 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.8628764152526855, loss=2.0991530418395996
I0307 11:07:35.844785 140197563971328 logging_writer.py:48] [16600] global_step=16600, grad_norm=5.6777567863464355, loss=2.1877846717834473
I0307 11:08:14.461151 140197572364032 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.577767372131348, loss=2.3108530044555664
I0307 11:08:52.686198 140197563971328 logging_writer.py:48] [16800] global_step=16800, grad_norm=7.19038200378418, loss=2.161989212036133
I0307 11:09:31.434408 140197572364032 logging_writer.py:48] [16900] global_step=16900, grad_norm=5.3146586418151855, loss=2.181931257247925
I0307 11:10:09.908346 140197563971328 logging_writer.py:48] [17000] global_step=17000, grad_norm=6.5927958488464355, loss=2.1507246494293213
I0307 11:10:48.283821 140197572364032 logging_writer.py:48] [17100] global_step=17100, grad_norm=6.116661071777344, loss=2.1070823669433594
I0307 11:11:26.737531 140197563971328 logging_writer.py:48] [17200] global_step=17200, grad_norm=4.987441539764404, loss=2.0907952785491943
I0307 11:11:38.374273 140352918893760 spec.py:321] Evaluating on the training split.
I0307 11:11:57.370021 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 11:12:20.211330 140352918893760 spec.py:349] Evaluating on the test split.
I0307 11:12:22.033779 140352918893760 submission_runner.py:469] Time since start: 7273.20s, 	Step: 17231, 	{'train/accuracy': 0.6097137928009033, 'train/loss': 1.5923843383789062, 'validation/accuracy': 0.5523200035095215, 'validation/loss': 1.9010858535766602, 'validation/num_examples': 50000, 'test/accuracy': 0.4349000155925751, 'test/loss': 2.673339605331421, 'test/num_examples': 10000, 'score': 6689.04709815979, 'total_duration': 7273.19722032547, 'accumulated_submission_time': 6689.04709815979, 'accumulated_eval_time': 581.3077845573425, 'accumulated_logging_time': 1.106475591659546}
I0307 11:12:22.133719 140197572364032 logging_writer.py:48] [17231] accumulated_eval_time=581.308, accumulated_logging_time=1.10648, accumulated_submission_time=6689.05, global_step=17231, preemption_count=0, score=6689.05, test/accuracy=0.4349, test/loss=2.67334, test/num_examples=10000, total_duration=7273.2, train/accuracy=0.609714, train/loss=1.59238, validation/accuracy=0.55232, validation/loss=1.90109, validation/num_examples=50000
I0307 11:12:49.093453 140197563971328 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.9964561462402344, loss=2.176697254180908
I0307 11:13:27.645074 140197572364032 logging_writer.py:48] [17400] global_step=17400, grad_norm=4.801512718200684, loss=2.1807608604431152
I0307 11:14:06.073686 140197563971328 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.575422286987305, loss=2.2289085388183594
I0307 11:14:44.650068 140197572364032 logging_writer.py:48] [17600] global_step=17600, grad_norm=5.000674724578857, loss=2.151850938796997
I0307 11:15:23.188289 140197563971328 logging_writer.py:48] [17700] global_step=17700, grad_norm=5.668015956878662, loss=2.196373224258423
I0307 11:16:01.738310 140197572364032 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.4455108642578125, loss=2.2374210357666016
I0307 11:16:40.410670 140197563971328 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.687990188598633, loss=2.136536121368408
I0307 11:17:18.621445 140197572364032 logging_writer.py:48] [18000] global_step=18000, grad_norm=5.298276424407959, loss=2.199974536895752
I0307 11:17:56.969302 140197563971328 logging_writer.py:48] [18100] global_step=18100, grad_norm=4.985885143280029, loss=2.0996036529541016
I0307 11:18:35.228887 140197572364032 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.832675933837891, loss=2.1797025203704834
I0307 11:19:14.010417 140197563971328 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.832923412322998, loss=2.1837451457977295
I0307 11:19:52.293067 140197572364032 logging_writer.py:48] [18400] global_step=18400, grad_norm=6.065246105194092, loss=2.129984140396118
I0307 11:20:30.502032 140197563971328 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.603659152984619, loss=2.2978129386901855
I0307 11:20:52.145172 140352918893760 spec.py:321] Evaluating on the training split.
I0307 11:21:06.780508 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 11:21:29.624281 140352918893760 spec.py:349] Evaluating on the test split.
I0307 11:21:31.420035 140352918893760 submission_runner.py:469] Time since start: 7822.58s, 	Step: 18557, 	{'train/accuracy': 0.6106305718421936, 'train/loss': 1.590878963470459, 'validation/accuracy': 0.5554599761962891, 'validation/loss': 1.8810646533966064, 'validation/num_examples': 50000, 'test/accuracy': 0.4351000189781189, 'test/loss': 2.635240316390991, 'test/num_examples': 10000, 'score': 7198.9117159843445, 'total_duration': 7822.58359003067, 'accumulated_submission_time': 7198.9117159843445, 'accumulated_eval_time': 620.582596540451, 'accumulated_logging_time': 1.2362072467803955}
I0307 11:21:31.522686 140197572364032 logging_writer.py:48] [18557] accumulated_eval_time=620.583, accumulated_logging_time=1.23621, accumulated_submission_time=7198.91, global_step=18557, preemption_count=0, score=7198.91, test/accuracy=0.4351, test/loss=2.63524, test/num_examples=10000, total_duration=7822.58, train/accuracy=0.610631, train/loss=1.59088, validation/accuracy=0.55546, validation/loss=1.88106, validation/num_examples=50000
I0307 11:21:48.497812 140197563971328 logging_writer.py:48] [18600] global_step=18600, grad_norm=6.589621543884277, loss=2.3697640895843506
I0307 11:22:26.719958 140197572364032 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.5674285888671875, loss=2.1424643993377686
I0307 11:23:05.308696 140197563971328 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.53738260269165, loss=2.0354793071746826
I0307 11:23:44.110007 140197572364032 logging_writer.py:48] [18900] global_step=18900, grad_norm=5.1294846534729, loss=2.2219247817993164
I0307 11:24:22.610810 140197563971328 logging_writer.py:48] [19000] global_step=19000, grad_norm=5.680319786071777, loss=2.121096134185791
I0307 11:25:01.032554 140197572364032 logging_writer.py:48] [19100] global_step=19100, grad_norm=4.987520694732666, loss=2.0371060371398926
I0307 11:25:39.688043 140197563971328 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.933964729309082, loss=2.1086199283599854
I0307 11:26:18.212530 140197572364032 logging_writer.py:48] [19300] global_step=19300, grad_norm=4.749815464019775, loss=2.281531810760498
I0307 11:26:56.713714 140197563971328 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.9026947021484375, loss=2.187741279602051
I0307 11:27:35.159900 140197572364032 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.925654888153076, loss=2.049490451812744
I0307 11:28:13.611883 140197563971328 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.6778345108032227, loss=2.0424604415893555
I0307 11:28:52.197385 140197572364032 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.540246963500977, loss=2.167060613632202
I0307 11:29:30.765937 140197563971328 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.668403625488281, loss=2.197176933288574
I0307 11:30:01.568062 140352918893760 spec.py:321] Evaluating on the training split.
I0307 11:30:20.509072 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 11:30:42.698114 140352918893760 spec.py:349] Evaluating on the test split.
I0307 11:30:44.511547 140352918893760 submission_runner.py:469] Time since start: 8375.68s, 	Step: 19881, 	{'train/accuracy': 0.6021006107330322, 'train/loss': 1.6173579692840576, 'validation/accuracy': 0.5558199882507324, 'validation/loss': 1.877522349357605, 'validation/num_examples': 50000, 'test/accuracy': 0.4326000213623047, 'test/loss': 2.6301968097686768, 'test/num_examples': 10000, 'score': 7708.774195432663, 'total_duration': 8375.675012111664, 'accumulated_submission_time': 7708.774195432663, 'accumulated_eval_time': 663.525942325592, 'accumulated_logging_time': 1.4039108753204346}
I0307 11:30:44.537843 140197572364032 logging_writer.py:48] [19881] accumulated_eval_time=663.526, accumulated_logging_time=1.40391, accumulated_submission_time=7708.77, global_step=19881, preemption_count=0, score=7708.77, test/accuracy=0.4326, test/loss=2.6302, test/num_examples=10000, total_duration=8375.68, train/accuracy=0.602101, train/loss=1.61736, validation/accuracy=0.55582, validation/loss=1.87752, validation/num_examples=50000
I0307 11:30:52.281677 140197563971328 logging_writer.py:48] [19900] global_step=19900, grad_norm=8.381281852722168, loss=2.1779258251190186
I0307 11:31:30.946975 140197572364032 logging_writer.py:48] [20000] global_step=20000, grad_norm=4.181033611297607, loss=2.256368398666382
I0307 11:32:09.249402 140197563971328 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.745862007141113, loss=2.146575450897217
I0307 11:32:47.680615 140197572364032 logging_writer.py:48] [20200] global_step=20200, grad_norm=5.29055118560791, loss=2.100846529006958
I0307 11:33:26.029203 140197563971328 logging_writer.py:48] [20300] global_step=20300, grad_norm=4.578990459442139, loss=2.1666617393493652
I0307 11:34:04.338963 140197572364032 logging_writer.py:48] [20400] global_step=20400, grad_norm=5.459272861480713, loss=2.135859489440918
I0307 11:34:42.969963 140197563971328 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.47405481338501, loss=2.1762399673461914
I0307 11:35:21.797559 140197572364032 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.264189720153809, loss=2.167634963989258
I0307 11:36:00.690490 140197563971328 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.834845066070557, loss=2.1346211433410645
I0307 11:36:39.207458 140197572364032 logging_writer.py:48] [20800] global_step=20800, grad_norm=4.538871765136719, loss=2.164475679397583
I0307 11:37:17.587219 140197563971328 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.850397825241089, loss=2.1429049968719482
I0307 11:37:56.438438 140197572364032 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.8537795543670654, loss=2.151355504989624
I0307 11:38:35.017457 140197563971328 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.7266554832458496, loss=2.217953681945801
I0307 11:39:13.530330 140197572364032 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.53824782371521, loss=2.0971741676330566
I0307 11:39:14.735851 140352918893760 spec.py:321] Evaluating on the training split.
I0307 11:39:35.015074 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 11:39:58.509158 140352918893760 spec.py:349] Evaluating on the test split.
I0307 11:40:00.243542 140352918893760 submission_runner.py:469] Time since start: 8931.41s, 	Step: 21204, 	{'train/accuracy': 0.6167888641357422, 'train/loss': 1.5513757467269897, 'validation/accuracy': 0.5619400143623352, 'validation/loss': 1.8469746112823486, 'validation/num_examples': 50000, 'test/accuracy': 0.4458000063896179, 'test/loss': 2.568434238433838, 'test/num_examples': 10000, 'score': 8218.84060716629, 'total_duration': 8931.406993150711, 'accumulated_submission_time': 8218.84060716629, 'accumulated_eval_time': 709.0334758758545, 'accumulated_logging_time': 1.437941074371338}
I0307 11:40:00.276974 140197563971328 logging_writer.py:48] [21204] accumulated_eval_time=709.033, accumulated_logging_time=1.43794, accumulated_submission_time=8218.84, global_step=21204, preemption_count=0, score=8218.84, test/accuracy=0.4458, test/loss=2.56843, test/num_examples=10000, total_duration=8931.41, train/accuracy=0.616789, train/loss=1.55138, validation/accuracy=0.56194, validation/loss=1.84697, validation/num_examples=50000
I0307 11:40:38.312329 140197572364032 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.355380535125732, loss=2.1421706676483154
I0307 11:41:16.962138 140197563971328 logging_writer.py:48] [21400] global_step=21400, grad_norm=4.443060398101807, loss=2.194631338119507
I0307 11:41:55.866150 140197572364032 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.1697893142700195, loss=1.9880783557891846
I0307 11:42:34.642493 140197563971328 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.66951847076416, loss=2.1789000034332275
I0307 11:43:13.206550 140197572364032 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.828225135803223, loss=2.117738962173462
I0307 11:43:52.020686 140197563971328 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.7548553943634033, loss=2.1899666786193848
I0307 11:44:30.631388 140197572364032 logging_writer.py:48] [21900] global_step=21900, grad_norm=4.33140754699707, loss=2.1024019718170166
I0307 11:45:09.100444 140197563971328 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.30979585647583, loss=2.137613296508789
I0307 11:45:47.553700 140197572364032 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.420113563537598, loss=2.0395705699920654
I0307 11:46:26.125375 140197563971328 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.48168683052063, loss=2.0586416721343994
I0307 11:47:04.598155 140197572364032 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.399073362350464, loss=1.9550929069519043
I0307 11:47:43.312727 140197563971328 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.7561163902282715, loss=1.9251811504364014
I0307 11:48:21.824045 140197572364032 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.949707269668579, loss=2.1024022102355957
I0307 11:48:30.383744 140352918893760 spec.py:321] Evaluating on the training split.
I0307 11:48:44.920824 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 11:49:07.213140 140352918893760 spec.py:349] Evaluating on the test split.
I0307 11:49:09.019293 140352918893760 submission_runner.py:469] Time since start: 9480.18s, 	Step: 22522, 	{'train/accuracy': 0.6152742505073547, 'train/loss': 1.5636672973632812, 'validation/accuracy': 0.5654799938201904, 'validation/loss': 1.8313333988189697, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.5066232681274414, 'test/num_examples': 10000, 'score': 8728.814687252045, 'total_duration': 9480.182833194733, 'accumulated_submission_time': 8728.814687252045, 'accumulated_eval_time': 747.6689591407776, 'accumulated_logging_time': 1.4795784950256348}
I0307 11:49:09.085897 140197563971328 logging_writer.py:48] [22522] accumulated_eval_time=747.669, accumulated_logging_time=1.47958, accumulated_submission_time=8728.81, global_step=22522, preemption_count=0, score=8728.81, test/accuracy=0.4479, test/loss=2.50662, test/num_examples=10000, total_duration=9480.18, train/accuracy=0.615274, train/loss=1.56367, validation/accuracy=0.56548, validation/loss=1.83133, validation/num_examples=50000
I0307 11:49:39.722383 140197572364032 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.795934677124023, loss=2.0628581047058105
I0307 11:50:18.576253 140197563971328 logging_writer.py:48] [22700] global_step=22700, grad_norm=5.261863708496094, loss=2.18581485748291
I0307 11:50:57.386366 140197572364032 logging_writer.py:48] [22800] global_step=22800, grad_norm=4.717069149017334, loss=2.2330856323242188
I0307 11:51:36.349864 140197563971328 logging_writer.py:48] [22900] global_step=22900, grad_norm=5.269597053527832, loss=2.122502565383911
I0307 11:52:14.888380 140197572364032 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.5097479820251465, loss=2.198080539703369
I0307 11:52:54.741291 140197563971328 logging_writer.py:48] [23100] global_step=23100, grad_norm=5.65605354309082, loss=2.0379061698913574
I0307 11:53:33.952378 140197572364032 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.7326200008392334, loss=2.1158225536346436
I0307 11:54:12.526200 140197563971328 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.4595422744750977, loss=2.2134814262390137
I0307 11:54:51.096302 140197572364032 logging_writer.py:48] [23400] global_step=23400, grad_norm=4.627342700958252, loss=2.1100077629089355
I0307 11:55:29.484491 140197563971328 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.2049691677093506, loss=1.9616048336029053
I0307 11:56:07.938814 140197572364032 logging_writer.py:48] [23600] global_step=23600, grad_norm=4.811172008514404, loss=2.027729034423828
I0307 11:56:46.562946 140197563971328 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.9869229793548584, loss=2.095996618270874
I0307 11:57:25.495600 140197572364032 logging_writer.py:48] [23800] global_step=23800, grad_norm=4.209205627441406, loss=2.057140827178955
I0307 11:57:39.045496 140352918893760 spec.py:321] Evaluating on the training split.
I0307 11:57:56.386052 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 11:58:16.043182 140352918893760 spec.py:349] Evaluating on the test split.
I0307 11:58:17.840040 140352918893760 submission_runner.py:469] Time since start: 10029.00s, 	Step: 23835, 	{'train/accuracy': 0.6325932741165161, 'train/loss': 1.482499599456787, 'validation/accuracy': 0.5803200006484985, 'validation/loss': 1.760344386100769, 'validation/num_examples': 50000, 'test/accuracy': 0.46410003304481506, 'test/loss': 2.4846160411834717, 'test/num_examples': 10000, 'score': 9238.612963438034, 'total_duration': 10029.003471374512, 'accumulated_submission_time': 9238.612963438034, 'accumulated_eval_time': 786.4633274078369, 'accumulated_logging_time': 1.5778069496154785}
I0307 11:58:17.932123 140197563971328 logging_writer.py:48] [23835] accumulated_eval_time=786.463, accumulated_logging_time=1.57781, accumulated_submission_time=9238.61, global_step=23835, preemption_count=0, score=9238.61, test/accuracy=0.4641, test/loss=2.48462, test/num_examples=10000, total_duration=10029, train/accuracy=0.632593, train/loss=1.4825, validation/accuracy=0.58032, validation/loss=1.76034, validation/num_examples=50000
I0307 11:58:43.623047 140197572364032 logging_writer.py:48] [23900] global_step=23900, grad_norm=5.222579002380371, loss=2.101726531982422
I0307 11:59:22.069191 140197563971328 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.4902563095092773, loss=2.052412271499634
I0307 12:00:00.596425 140197572364032 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.581258773803711, loss=2.0267558097839355
I0307 12:00:39.174230 140197563971328 logging_writer.py:48] [24200] global_step=24200, grad_norm=4.283072471618652, loss=2.105354070663452
I0307 12:01:17.459965 140197572364032 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.6405017375946045, loss=2.0582878589630127
I0307 12:01:57.349848 140197563971328 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.1654136180877686, loss=1.8628031015396118
I0307 12:02:36.117652 140197572364032 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.1541483402252197, loss=2.034872055053711
I0307 12:03:14.649804 140197563971328 logging_writer.py:48] [24600] global_step=24600, grad_norm=4.360584735870361, loss=2.023465871810913
I0307 12:03:53.214933 140197572364032 logging_writer.py:48] [24700] global_step=24700, grad_norm=4.03009557723999, loss=2.1604251861572266
I0307 12:04:31.489378 140197563971328 logging_writer.py:48] [24800] global_step=24800, grad_norm=4.399166584014893, loss=2.037484884262085
I0307 12:05:10.511681 140197572364032 logging_writer.py:48] [24900] global_step=24900, grad_norm=4.1756181716918945, loss=2.1763293743133545
I0307 12:05:49.071618 140197563971328 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.3732292652130127, loss=2.0986320972442627
I0307 12:06:29.058524 140197572364032 logging_writer.py:48] [25100] global_step=25100, grad_norm=4.939474105834961, loss=2.04583740234375
I0307 12:06:48.112363 140352918893760 spec.py:321] Evaluating on the training split.
I0307 12:07:07.218685 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 12:07:27.731881 140352918893760 spec.py:349] Evaluating on the test split.
I0307 12:07:29.516805 140352918893760 submission_runner.py:469] Time since start: 10580.68s, 	Step: 25150, 	{'train/accuracy': 0.6347058415412903, 'train/loss': 1.473044991493225, 'validation/accuracy': 0.58406001329422, 'validation/loss': 1.7415127754211426, 'validation/num_examples': 50000, 'test/accuracy': 0.4538000226020813, 'test/loss': 2.4774622917175293, 'test/num_examples': 10000, 'score': 9748.636342048645, 'total_duration': 10580.68021440506, 'accumulated_submission_time': 9748.636342048645, 'accumulated_eval_time': 827.8675727844238, 'accumulated_logging_time': 1.6966907978057861}
I0307 12:07:29.585399 140197563971328 logging_writer.py:48] [25150] accumulated_eval_time=827.868, accumulated_logging_time=1.69669, accumulated_submission_time=9748.64, global_step=25150, preemption_count=0, score=9748.64, test/accuracy=0.4538, test/loss=2.47746, test/num_examples=10000, total_duration=10580.7, train/accuracy=0.634706, train/loss=1.47304, validation/accuracy=0.58406, validation/loss=1.74151, validation/num_examples=50000
I0307 12:07:49.325791 140197572364032 logging_writer.py:48] [25200] global_step=25200, grad_norm=4.709313869476318, loss=2.1539840698242188
I0307 12:08:27.847836 140197563971328 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.897932529449463, loss=2.056943416595459
I0307 12:09:06.538035 140197572364032 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.42037296295166, loss=1.9002372026443481
I0307 12:09:45.049267 140197563971328 logging_writer.py:48] [25500] global_step=25500, grad_norm=4.007245063781738, loss=2.122627019882202
I0307 12:10:23.766717 140197572364032 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.4619555473327637, loss=2.0277156829833984
I0307 12:11:02.702586 140197563971328 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.727085590362549, loss=2.0054190158843994
I0307 12:11:41.291296 140197572364032 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.5209038257598877, loss=2.0223374366760254
I0307 12:12:19.970189 140197563971328 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.774000406265259, loss=2.2218751907348633
I0307 12:12:58.493244 140197572364032 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.5864243507385254, loss=1.9240907430648804
I0307 12:13:37.004172 140197563971328 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.639878988265991, loss=1.952690601348877
I0307 12:14:15.873066 140197572364032 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.638028383255005, loss=1.9774249792099
I0307 12:14:55.408482 140197563971328 logging_writer.py:48] [26300] global_step=26300, grad_norm=4.986538887023926, loss=2.016880512237549
I0307 12:15:33.776204 140197572364032 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.541309118270874, loss=1.966291069984436
I0307 12:15:59.741534 140352918893760 spec.py:321] Evaluating on the training split.
I0307 12:16:19.779183 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 12:16:40.826990 140352918893760 spec.py:349] Evaluating on the test split.
I0307 12:16:42.610249 140352918893760 submission_runner.py:469] Time since start: 11133.77s, 	Step: 26468, 	{'train/accuracy': 0.6271523833274841, 'train/loss': 1.4970682859420776, 'validation/accuracy': 0.5855399966239929, 'validation/loss': 1.7396633625030518, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.4662978649139404, 'test/num_examples': 10000, 'score': 10258.644327402115, 'total_duration': 11133.77368092537, 'accumulated_submission_time': 10258.644327402115, 'accumulated_eval_time': 870.7361159324646, 'accumulated_logging_time': 1.7886905670166016}
I0307 12:16:42.757565 140197563971328 logging_writer.py:48] [26468] accumulated_eval_time=870.736, accumulated_logging_time=1.78869, accumulated_submission_time=10258.6, global_step=26468, preemption_count=0, score=10258.6, test/accuracy=0.4589, test/loss=2.4663, test/num_examples=10000, total_duration=11133.8, train/accuracy=0.627152, train/loss=1.49707, validation/accuracy=0.58554, validation/loss=1.73966, validation/num_examples=50000
I0307 12:16:56.222800 140197572364032 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.439927816390991, loss=1.9764207601547241
I0307 12:17:34.793624 140197563971328 logging_writer.py:48] [26600] global_step=26600, grad_norm=4.9763383865356445, loss=2.03104567527771
I0307 12:18:13.516114 140197572364032 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.611201763153076, loss=2.050682544708252
I0307 12:18:52.128212 140197563971328 logging_writer.py:48] [26800] global_step=26800, grad_norm=4.328134059906006, loss=2.0249838829040527
I0307 12:19:31.167721 140197572364032 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.5875742435455322, loss=2.032480239868164
I0307 12:20:09.417383 140197563971328 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.5020389556884766, loss=1.9801461696624756
I0307 12:20:47.985199 140197572364032 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.966845750808716, loss=1.9907606840133667
I0307 12:21:26.294111 140197563971328 logging_writer.py:48] [27200] global_step=27200, grad_norm=4.106600284576416, loss=2.066235303878784
I0307 12:22:04.701767 140197572364032 logging_writer.py:48] [27300] global_step=27300, grad_norm=4.25487756729126, loss=2.118011951446533
I0307 12:22:43.234079 140197563971328 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.8419036865234375, loss=1.963155746459961
I0307 12:23:21.789598 140197572364032 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.9271419048309326, loss=2.0533676147460938
I0307 12:24:01.366887 140197563971328 logging_writer.py:48] [27600] global_step=27600, grad_norm=4.782137393951416, loss=2.0975494384765625
I0307 12:24:40.970185 140197572364032 logging_writer.py:48] [27700] global_step=27700, grad_norm=4.252116680145264, loss=2.090460777282715
I0307 12:25:12.673870 140352918893760 spec.py:321] Evaluating on the training split.
I0307 12:25:30.920237 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 12:25:52.175911 140352918893760 spec.py:349] Evaluating on the test split.
I0307 12:25:54.245559 140352918893760 submission_runner.py:469] Time since start: 11685.41s, 	Step: 27783, 	{'train/accuracy': 0.6384725570678711, 'train/loss': 1.4756149053573608, 'validation/accuracy': 0.5844599604606628, 'validation/loss': 1.7352402210235596, 'validation/num_examples': 50000, 'test/accuracy': 0.46320003271102905, 'test/loss': 2.448728322982788, 'test/num_examples': 10000, 'score': 10767.809199094772, 'total_duration': 11685.408977270126, 'accumulated_submission_time': 10767.809199094772, 'accumulated_eval_time': 912.3076181411743, 'accumulated_logging_time': 2.556812286376953}
I0307 12:25:54.315304 140197563971328 logging_writer.py:48] [27783] accumulated_eval_time=912.308, accumulated_logging_time=2.55681, accumulated_submission_time=10767.8, global_step=27783, preemption_count=0, score=10767.8, test/accuracy=0.4632, test/loss=2.44873, test/num_examples=10000, total_duration=11685.4, train/accuracy=0.638473, train/loss=1.47561, validation/accuracy=0.58446, validation/loss=1.73524, validation/num_examples=50000
I0307 12:26:01.265726 140197572364032 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.5902652740478516, loss=1.9931130409240723
I0307 12:26:39.448113 140197563971328 logging_writer.py:48] [27900] global_step=27900, grad_norm=4.08190393447876, loss=1.997389316558838
I0307 12:27:17.776401 140197572364032 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.835228204727173, loss=1.954884648323059
I0307 12:27:56.210214 140197563971328 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.730339288711548, loss=2.012892246246338
I0307 12:28:34.541104 140197572364032 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.927640199661255, loss=2.0355918407440186
I0307 12:29:13.098505 140197563971328 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.4275355339050293, loss=1.9500162601470947
I0307 12:29:51.396759 140197572364032 logging_writer.py:48] [28400] global_step=28400, grad_norm=4.326691627502441, loss=1.9177870750427246
I0307 12:30:29.942287 140197563971328 logging_writer.py:48] [28500] global_step=28500, grad_norm=4.169864654541016, loss=2.0422024726867676
I0307 12:31:08.733445 140197572364032 logging_writer.py:48] [28600] global_step=28600, grad_norm=5.14976167678833, loss=2.066375732421875
I0307 12:31:47.096108 140197563971328 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.905799150466919, loss=1.9762279987335205
I0307 12:32:26.496733 140197572364032 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.9170279502868652, loss=1.9654059410095215
I0307 12:33:05.345900 140197563971328 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.3965649604797363, loss=2.050302028656006
I0307 12:33:44.305128 140197572364032 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.2591748237609863, loss=1.923325538635254
I0307 12:34:23.073426 140197563971328 logging_writer.py:48] [29100] global_step=29100, grad_norm=4.119398593902588, loss=2.0920279026031494
I0307 12:34:24.248208 140352918893760 spec.py:321] Evaluating on the training split.
I0307 12:34:41.805787 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 12:35:03.830172 140352918893760 spec.py:349] Evaluating on the test split.
I0307 12:35:05.623024 140352918893760 submission_runner.py:469] Time since start: 12236.79s, 	Step: 29104, 	{'train/accuracy': 0.6354830861091614, 'train/loss': 1.4651622772216797, 'validation/accuracy': 0.5838599801063538, 'validation/loss': 1.7416489124298096, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4830355644226074, 'test/num_examples': 10000, 'score': 11277.531613349915, 'total_duration': 12236.78643321991, 'accumulated_submission_time': 11277.531613349915, 'accumulated_eval_time': 953.6822338104248, 'accumulated_logging_time': 2.7083470821380615}
I0307 12:35:05.725069 140197572364032 logging_writer.py:48] [29104] accumulated_eval_time=953.682, accumulated_logging_time=2.70835, accumulated_submission_time=11277.5, global_step=29104, preemption_count=0, score=11277.5, test/accuracy=0.4611, test/loss=2.48304, test/num_examples=10000, total_duration=12236.8, train/accuracy=0.635483, train/loss=1.46516, validation/accuracy=0.58386, validation/loss=1.74165, validation/num_examples=50000
I0307 12:35:42.992874 140197563971328 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.155320405960083, loss=1.9955142736434937
I0307 12:36:21.622189 140197572364032 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.344414234161377, loss=2.1483314037323
I0307 12:36:59.959517 140197563971328 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.7781124114990234, loss=1.9528884887695312
I0307 12:37:38.486397 140197572364032 logging_writer.py:48] [29500] global_step=29500, grad_norm=4.113337516784668, loss=1.8996354341506958
I0307 12:38:17.370459 140197563971328 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.825373888015747, loss=1.9815634489059448
I0307 12:38:56.293084 140197572364032 logging_writer.py:48] [29700] global_step=29700, grad_norm=4.303916931152344, loss=2.0338752269744873
I0307 12:39:34.941076 140197563971328 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.481881618499756, loss=1.9002575874328613
I0307 12:40:13.587139 140197572364032 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.846674919128418, loss=2.020460844039917
I0307 12:40:52.453743 140197563971328 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.4631316661834717, loss=1.831716775894165
I0307 12:41:32.508834 140197572364032 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.7336418628692627, loss=1.903182864189148
I0307 12:42:11.588366 140197563971328 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.8460347652435303, loss=2.022043228149414
I0307 12:42:50.486755 140197572364032 logging_writer.py:48] [30300] global_step=30300, grad_norm=4.024806022644043, loss=2.0217676162719727
I0307 12:43:29.240986 140197563971328 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.439311981201172, loss=1.987489938735962
I0307 12:43:35.737845 140352918893760 spec.py:321] Evaluating on the training split.
I0307 12:43:54.039996 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 12:44:14.563652 140352918893760 spec.py:349] Evaluating on the test split.
I0307 12:44:16.335142 140352918893760 submission_runner.py:469] Time since start: 12787.50s, 	Step: 30418, 	{'train/accuracy': 0.6411232352256775, 'train/loss': 1.4582377672195435, 'validation/accuracy': 0.5916000008583069, 'validation/loss': 1.7062532901763916, 'validation/num_examples': 50000, 'test/accuracy': 0.4677000343799591, 'test/loss': 2.437067985534668, 'test/num_examples': 10000, 'score': 11787.374530792236, 'total_duration': 12787.498551368713, 'accumulated_submission_time': 11787.374530792236, 'accumulated_eval_time': 994.2793338298798, 'accumulated_logging_time': 2.8469278812408447}
I0307 12:44:16.484017 140197572364032 logging_writer.py:48] [30418] accumulated_eval_time=994.279, accumulated_logging_time=2.84693, accumulated_submission_time=11787.4, global_step=30418, preemption_count=0, score=11787.4, test/accuracy=0.4677, test/loss=2.43707, test/num_examples=10000, total_duration=12787.5, train/accuracy=0.641123, train/loss=1.45824, validation/accuracy=0.5916, validation/loss=1.70625, validation/num_examples=50000
I0307 12:44:48.443414 140197563971328 logging_writer.py:48] [30500] global_step=30500, grad_norm=4.257371425628662, loss=2.075392723083496
I0307 12:45:26.841886 140197572364032 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.624666213989258, loss=1.935363531112671
I0307 12:46:05.361593 140197563971328 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.3293251991271973, loss=1.9810189008712769
I0307 12:46:43.751061 140197572364032 logging_writer.py:48] [30800] global_step=30800, grad_norm=4.299080848693848, loss=2.0454065799713135
I0307 12:47:22.222033 140197563971328 logging_writer.py:48] [30900] global_step=30900, grad_norm=4.487479209899902, loss=2.050349712371826
I0307 12:48:00.636835 140197572364032 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.7655060291290283, loss=2.00343656539917
I0307 12:48:39.356893 140197563971328 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.5442426204681396, loss=2.0494441986083984
I0307 12:49:18.001217 140197572364032 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.6220145225524902, loss=1.93965482711792
I0307 12:49:56.939953 140197563971328 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.836306095123291, loss=1.950284481048584
I0307 12:50:35.837127 140197572364032 logging_writer.py:48] [31400] global_step=31400, grad_norm=4.58432674407959, loss=1.9265637397766113
I0307 12:51:14.352870 140197563971328 logging_writer.py:48] [31500] global_step=31500, grad_norm=4.371440887451172, loss=1.902063250541687
I0307 12:51:53.197016 140197572364032 logging_writer.py:48] [31600] global_step=31600, grad_norm=4.17072868347168, loss=2.0888521671295166
I0307 12:52:32.021615 140197563971328 logging_writer.py:48] [31700] global_step=31700, grad_norm=4.296306610107422, loss=2.0672178268432617
I0307 12:52:46.367837 140352918893760 spec.py:321] Evaluating on the training split.
I0307 12:53:02.697664 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 12:53:23.867867 140352918893760 spec.py:349] Evaluating on the test split.
I0307 12:53:25.626688 140352918893760 submission_runner.py:469] Time since start: 13336.79s, 	Step: 31738, 	{'train/accuracy': 0.638113796710968, 'train/loss': 1.456536889076233, 'validation/accuracy': 0.5877999663352966, 'validation/loss': 1.7231141328811646, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.4735653400421143, 'test/num_examples': 10000, 'score': 12297.110484361649, 'total_duration': 13336.790102481842, 'accumulated_submission_time': 12297.110484361649, 'accumulated_eval_time': 1033.5379920005798, 'accumulated_logging_time': 3.0118300914764404}
I0307 12:53:25.715792 140197572364032 logging_writer.py:48] [31738] accumulated_eval_time=1033.54, accumulated_logging_time=3.01183, accumulated_submission_time=12297.1, global_step=31738, preemption_count=0, score=12297.1, test/accuracy=0.4638, test/loss=2.47357, test/num_examples=10000, total_duration=13336.8, train/accuracy=0.638114, train/loss=1.45654, validation/accuracy=0.5878, validation/loss=1.72311, validation/num_examples=50000
I0307 12:53:50.113528 140197563971328 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.478269577026367, loss=2.102931022644043
I0307 12:54:28.456265 140197572364032 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.389561176300049, loss=2.0097038745880127
I0307 12:55:07.033726 140197563971328 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.1023221015930176, loss=1.9125763177871704
I0307 12:55:45.366415 140197572364032 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.4181182384490967, loss=2.0026168823242188
I0307 12:56:23.975754 140197563971328 logging_writer.py:48] [32200] global_step=32200, grad_norm=4.441127777099609, loss=1.9931674003601074
I0307 12:57:02.527456 140197572364032 logging_writer.py:48] [32300] global_step=32300, grad_norm=4.4424967765808105, loss=2.030132293701172
I0307 12:57:40.849030 140197563971328 logging_writer.py:48] [32400] global_step=32400, grad_norm=4.0955424308776855, loss=1.9920356273651123
I0307 12:58:19.675602 140197572364032 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.708878517150879, loss=1.9820021390914917
I0307 12:58:58.633162 140197563971328 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.4701433181762695, loss=2.018038034439087
I0307 12:59:37.143904 140197572364032 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.942786455154419, loss=1.862421989440918
I0307 13:00:15.764699 140197563971328 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.6879148483276367, loss=1.9994356632232666
I0307 13:00:54.022902 140197572364032 logging_writer.py:48] [32900] global_step=32900, grad_norm=4.644589424133301, loss=1.9424325227737427
I0307 13:01:32.601479 140197563971328 logging_writer.py:48] [33000] global_step=33000, grad_norm=4.1060709953308105, loss=1.9282958507537842
I0307 13:01:55.868242 140352918893760 spec.py:321] Evaluating on the training split.
I0307 13:02:08.438560 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 13:02:36.163945 140352918893760 spec.py:349] Evaluating on the test split.
I0307 13:02:37.918515 140352918893760 submission_runner.py:469] Time since start: 13889.08s, 	Step: 33061, 	{'train/accuracy': 0.6460060477256775, 'train/loss': 1.4213043451309204, 'validation/accuracy': 0.5931999683380127, 'validation/loss': 1.6828004121780396, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.418006658554077, 'test/num_examples': 10000, 'score': 12807.092128276825, 'total_duration': 13889.081949234009, 'accumulated_submission_time': 12807.092128276825, 'accumulated_eval_time': 1075.588097333908, 'accumulated_logging_time': 3.139647960662842}
I0307 13:02:38.007523 140197572364032 logging_writer.py:48] [33061] accumulated_eval_time=1075.59, accumulated_logging_time=3.13965, accumulated_submission_time=12807.1, global_step=33061, preemption_count=0, score=12807.1, test/accuracy=0.4674, test/loss=2.41801, test/num_examples=10000, total_duration=13889.1, train/accuracy=0.646006, train/loss=1.4213, validation/accuracy=0.5932, validation/loss=1.6828, validation/num_examples=50000
I0307 13:02:53.444901 140197563971328 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.982813835144043, loss=1.8118090629577637
I0307 13:03:32.261613 140197572364032 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.6079978942871094, loss=1.9821393489837646
I0307 13:04:10.798879 140197563971328 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.8060028553009033, loss=1.9058303833007812
I0307 13:04:49.856481 140197572364032 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.9354708194732666, loss=2.0489656925201416
I0307 13:05:28.477451 140197563971328 logging_writer.py:48] [33500] global_step=33500, grad_norm=4.786448001861572, loss=1.9708940982818604
I0307 13:06:07.281284 140197572364032 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.289090871810913, loss=2.0248146057128906
I0307 13:06:45.744103 140197563971328 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.433643102645874, loss=1.9224575757980347
I0307 13:07:24.843918 140197572364032 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.942760944366455, loss=1.857210397720337
I0307 13:08:03.813673 140197563971328 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.2385950088500977, loss=1.9301304817199707
I0307 13:08:42.360994 140197572364032 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.4368669986724854, loss=1.9641096591949463
I0307 13:09:21.155248 140197563971328 logging_writer.py:48] [34100] global_step=34100, grad_norm=4.112111568450928, loss=2.021672248840332
I0307 13:10:00.028817 140197572364032 logging_writer.py:48] [34200] global_step=34200, grad_norm=4.4553022384643555, loss=1.9983787536621094
I0307 13:10:38.361408 140197563971328 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.8157544136047363, loss=1.9979634284973145
I0307 13:11:08.071907 140352918893760 spec.py:321] Evaluating on the training split.
I0307 13:11:20.625098 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 13:11:44.428631 140352918893760 spec.py:349] Evaluating on the test split.
I0307 13:11:46.170169 140352918893760 submission_runner.py:469] Time since start: 14437.33s, 	Step: 34377, 	{'train/accuracy': 0.6480388641357422, 'train/loss': 1.4073448181152344, 'validation/accuracy': 0.598800003528595, 'validation/loss': 1.6664913892745972, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.385242462158203, 'test/num_examples': 10000, 'score': 13316.98905658722, 'total_duration': 14437.333611011505, 'accumulated_submission_time': 13316.98905658722, 'accumulated_eval_time': 1113.6861951351166, 'accumulated_logging_time': 3.261392116546631}
I0307 13:11:46.236601 140197572364032 logging_writer.py:48] [34377] accumulated_eval_time=1113.69, accumulated_logging_time=3.26139, accumulated_submission_time=13317, global_step=34377, preemption_count=0, score=13317, test/accuracy=0.4779, test/loss=2.38524, test/num_examples=10000, total_duration=14437.3, train/accuracy=0.648039, train/loss=1.40734, validation/accuracy=0.5988, validation/loss=1.66649, validation/num_examples=50000
I0307 13:11:55.660622 140197563971328 logging_writer.py:48] [34400] global_step=34400, grad_norm=4.130619049072266, loss=1.910488247871399
I0307 13:12:34.077247 140197572364032 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.827754497528076, loss=1.9895952939987183
I0307 13:13:12.393137 140197563971328 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.2090864181518555, loss=1.9664990901947021
I0307 13:13:51.069873 140197572364032 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.7830865383148193, loss=1.9607822895050049
I0307 13:14:29.388864 140197563971328 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.69480562210083, loss=1.9737932682037354
I0307 13:15:07.833406 140197572364032 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.613008499145508, loss=1.9367241859436035
I0307 13:15:46.595297 140197563971328 logging_writer.py:48] [35000] global_step=35000, grad_norm=4.359628200531006, loss=2.0274717807769775
I0307 13:16:26.048460 140197572364032 logging_writer.py:48] [35100] global_step=35100, grad_norm=4.164279460906982, loss=1.993355631828308
I0307 13:17:04.757011 140197563971328 logging_writer.py:48] [35200] global_step=35200, grad_norm=4.101440906524658, loss=1.9189026355743408
I0307 13:17:43.347115 140197572364032 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.538991928100586, loss=1.8949604034423828
I0307 13:18:22.088261 140197563971328 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.848261833190918, loss=1.9834684133529663
I0307 13:19:01.017471 140197572364032 logging_writer.py:48] [35500] global_step=35500, grad_norm=5.480741500854492, loss=1.8722796440124512
I0307 13:19:40.304090 140197563971328 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.7151453495025635, loss=1.9841114282608032
I0307 13:20:16.247500 140352918893760 spec.py:321] Evaluating on the training split.
I0307 13:20:28.383944 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 13:20:49.759058 140352918893760 spec.py:349] Evaluating on the test split.
I0307 13:20:51.509521 140352918893760 submission_runner.py:469] Time since start: 14982.67s, 	Step: 35695, 	{'train/accuracy': 0.6469228267669678, 'train/loss': 1.4248980283737183, 'validation/accuracy': 0.5960999727249146, 'validation/loss': 1.6695075035095215, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.380704402923584, 'test/num_examples': 10000, 'score': 13826.821261644363, 'total_duration': 14982.672957897186, 'accumulated_submission_time': 13826.821261644363, 'accumulated_eval_time': 1148.9480550289154, 'accumulated_logging_time': 3.375527858734131}
I0307 13:20:51.581973 140197572364032 logging_writer.py:48] [35695] accumulated_eval_time=1148.95, accumulated_logging_time=3.37553, accumulated_submission_time=13826.8, global_step=35695, preemption_count=0, score=13826.8, test/accuracy=0.4757, test/loss=2.3807, test/num_examples=10000, total_duration=14982.7, train/accuracy=0.646923, train/loss=1.4249, validation/accuracy=0.5961, validation/loss=1.66951, validation/num_examples=50000
I0307 13:20:54.005443 140197563971328 logging_writer.py:48] [35700] global_step=35700, grad_norm=4.491967678070068, loss=1.8282403945922852
I0307 13:21:32.465343 140197572364032 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.8558905124664307, loss=1.966442346572876
I0307 13:22:10.933465 140197563971328 logging_writer.py:48] [35900] global_step=35900, grad_norm=4.55265474319458, loss=1.9286140203475952
I0307 13:22:48.952516 140197572364032 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.511413097381592, loss=1.8726258277893066
I0307 13:23:26.899022 140197563971328 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.3583500385284424, loss=1.9718862771987915
I0307 13:24:05.879959 140197572364032 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.8885865211486816, loss=1.8882977962493896
I0307 13:24:45.055594 140197563971328 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.513659715652466, loss=1.9781345129013062
I0307 13:25:23.981273 140197572364032 logging_writer.py:48] [36400] global_step=36400, grad_norm=4.0166239738464355, loss=1.9510235786437988
I0307 13:26:02.810492 140197563971328 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.6928467750549316, loss=2.096733570098877
I0307 13:26:41.193573 140197572364032 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.9270644187927246, loss=2.0515875816345215
I0307 13:27:19.883514 140197563971328 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.908437490463257, loss=1.8964365720748901
I0307 13:27:58.753770 140197572364032 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.9481217861175537, loss=1.957721471786499
I0307 13:28:37.264339 140197563971328 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.31626033782959, loss=1.9280469417572021
I0307 13:29:15.912162 140197572364032 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.5428273677825928, loss=1.9303820133209229
I0307 13:29:21.770869 140352918893760 spec.py:321] Evaluating on the training split.
I0307 13:29:34.032665 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 13:29:57.674497 140352918893760 spec.py:349] Evaluating on the test split.
I0307 13:29:59.400769 140352918893760 submission_runner.py:469] Time since start: 15530.56s, 	Step: 37016, 	{'train/accuracy': 0.6443917155265808, 'train/loss': 1.4418485164642334, 'validation/accuracy': 0.597599983215332, 'validation/loss': 1.6946007013320923, 'validation/num_examples': 50000, 'test/accuracy': 0.4725000262260437, 'test/loss': 2.4381091594696045, 'test/num_examples': 10000, 'score': 14336.823198318481, 'total_duration': 15530.5641913414, 'accumulated_submission_time': 14336.823198318481, 'accumulated_eval_time': 1186.57777094841, 'accumulated_logging_time': 3.503100872039795}
I0307 13:29:59.514686 140197563971328 logging_writer.py:48] [37016] accumulated_eval_time=1186.58, accumulated_logging_time=3.5031, accumulated_submission_time=14336.8, global_step=37016, preemption_count=0, score=14336.8, test/accuracy=0.4725, test/loss=2.43811, test/num_examples=10000, total_duration=15530.6, train/accuracy=0.644392, train/loss=1.44185, validation/accuracy=0.5976, validation/loss=1.6946, validation/num_examples=50000
I0307 13:30:32.484835 140197572364032 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.577329397201538, loss=1.8997591733932495
I0307 13:31:10.722033 140197563971328 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.949432611465454, loss=1.8989003896713257
I0307 13:31:49.233292 140197572364032 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.739316463470459, loss=2.047896385192871
I0307 13:32:27.795871 140197563971328 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.58011531829834, loss=1.943646788597107
I0307 13:33:06.736024 140197572364032 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.2830917835235596, loss=1.9711976051330566
I0307 13:33:45.778896 140197563971328 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.4099347591400146, loss=1.7861177921295166
I0307 13:34:24.401046 140197572364032 logging_writer.py:48] [37700] global_step=37700, grad_norm=4.5225701332092285, loss=1.9482686519622803
I0307 13:35:03.101812 140197563971328 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.5389883518218994, loss=1.875615119934082
I0307 13:35:42.192520 140197572364032 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.2382595539093018, loss=1.9432189464569092
I0307 13:36:20.916157 140197563971328 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.6521670818328857, loss=2.054291248321533
I0307 13:36:59.679853 140197572364032 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.9630324840545654, loss=1.8568823337554932
I0307 13:37:38.516607 140197563971328 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.236335515975952, loss=1.7884804010391235
I0307 13:38:17.023095 140197572364032 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.457475185394287, loss=1.8978641033172607
I0307 13:38:29.809695 140352918893760 spec.py:321] Evaluating on the training split.
I0307 13:38:42.728111 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 13:39:01.596437 140352918893760 spec.py:349] Evaluating on the test split.
I0307 13:39:03.386851 140352918893760 submission_runner.py:469] Time since start: 16074.55s, 	Step: 38334, 	{'train/accuracy': 0.6358617544174194, 'train/loss': 1.4668822288513184, 'validation/accuracy': 0.587660014629364, 'validation/loss': 1.7146292924880981, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4165847301483154, 'test/num_examples': 10000, 'score': 14846.957060098648, 'total_duration': 16074.550265073776, 'accumulated_submission_time': 14846.957060098648, 'accumulated_eval_time': 1220.1547420024872, 'accumulated_logging_time': 3.6423532962799072}
I0307 13:39:03.527122 140197563971328 logging_writer.py:48] [38334] accumulated_eval_time=1220.15, accumulated_logging_time=3.64235, accumulated_submission_time=14847, global_step=38334, preemption_count=0, score=14847, test/accuracy=0.4735, test/loss=2.41658, test/num_examples=10000, total_duration=16074.6, train/accuracy=0.635862, train/loss=1.46688, validation/accuracy=0.58766, validation/loss=1.71463, validation/num_examples=50000
I0307 13:39:29.479147 140197572364032 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.4636285305023193, loss=1.9724527597427368
I0307 13:40:08.190246 140197563971328 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.669631004333496, loss=1.9771144390106201
I0307 13:40:46.349988 140197572364032 logging_writer.py:48] [38600] global_step=38600, grad_norm=4.192232131958008, loss=1.902467966079712
I0307 13:41:24.871070 140197563971328 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.7181501388549805, loss=1.8791321516036987
I0307 13:42:03.756837 140197572364032 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.9584734439849854, loss=1.9612627029418945
I0307 13:42:42.353712 140197563971328 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.9218175411224365, loss=1.8772926330566406
I0307 13:43:20.451336 140197572364032 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.4675662517547607, loss=1.9081202745437622
I0307 13:43:58.706439 140197563971328 logging_writer.py:48] [39100] global_step=39100, grad_norm=4.241086006164551, loss=1.8555290699005127
I0307 13:44:37.264882 140197572364032 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.397104263305664, loss=1.9161312580108643
I0307 13:45:15.481881 140197563971328 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.8084452152252197, loss=1.8700000047683716
I0307 13:45:53.994574 140197572364032 logging_writer.py:48] [39400] global_step=39400, grad_norm=4.445559501647949, loss=1.916385531425476
I0307 13:46:32.624644 140197563971328 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.4014158248901367, loss=2.057976245880127
I0307 13:47:10.836908 140197572364032 logging_writer.py:48] [39600] global_step=39600, grad_norm=4.105184555053711, loss=1.9287197589874268
I0307 13:47:33.627974 140352918893760 spec.py:321] Evaluating on the training split.
I0307 13:47:45.934161 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 13:48:10.486330 140352918893760 spec.py:349] Evaluating on the test split.
I0307 13:48:12.246083 140352918893760 submission_runner.py:469] Time since start: 16623.41s, 	Step: 39660, 	{'train/accuracy': 0.6470822691917419, 'train/loss': 1.414178490638733, 'validation/accuracy': 0.5989599823951721, 'validation/loss': 1.6651763916015625, 'validation/num_examples': 50000, 'test/accuracy': 0.47690001130104065, 'test/loss': 2.404301404953003, 'test/num_examples': 10000, 'score': 15356.887708425522, 'total_duration': 16623.40948367119, 'accumulated_submission_time': 15356.887708425522, 'accumulated_eval_time': 1258.7726547718048, 'accumulated_logging_time': 3.8216123580932617}
I0307 13:48:12.360735 140197563971328 logging_writer.py:48] [39660] accumulated_eval_time=1258.77, accumulated_logging_time=3.82161, accumulated_submission_time=15356.9, global_step=39660, preemption_count=0, score=15356.9, test/accuracy=0.4769, test/loss=2.4043, test/num_examples=10000, total_duration=16623.4, train/accuracy=0.647082, train/loss=1.41418, validation/accuracy=0.59896, validation/loss=1.66518, validation/num_examples=50000
I0307 13:48:28.349250 140197572364032 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.8392858505249023, loss=2.005436897277832
I0307 13:49:06.763611 140197563971328 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.740612506866455, loss=2.0443785190582275
I0307 13:49:45.500499 140197572364032 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.4258365631103516, loss=2.008286476135254
I0307 13:50:24.227290 140197563971328 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.713467836380005, loss=1.9056923389434814
I0307 13:51:03.782736 140197572364032 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.3790619373321533, loss=1.9315495491027832
I0307 13:51:42.407234 140197563971328 logging_writer.py:48] [40200] global_step=40200, grad_norm=4.2407755851745605, loss=2.0255300998687744
I0307 13:52:21.226396 140197572364032 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.7048511505126953, loss=1.9614710807800293
I0307 13:52:59.322675 140197563971328 logging_writer.py:48] [40400] global_step=40400, grad_norm=4.986721992492676, loss=2.0056607723236084
I0307 13:53:37.513294 140197572364032 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.588101625442505, loss=1.9767956733703613
I0307 13:54:15.655150 140197563971328 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.220684289932251, loss=1.9612376689910889
I0307 13:54:54.047709 140197572364032 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.911278009414673, loss=2.0284276008605957
I0307 13:55:32.959119 140197563971328 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.487220048904419, loss=1.9220647811889648
I0307 13:56:11.734725 140197572364032 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.9434146881103516, loss=2.043261766433716
I0307 13:56:42.417492 140352918893760 spec.py:321] Evaluating on the training split.
I0307 13:56:54.785974 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 13:57:17.665448 140352918893760 spec.py:349] Evaluating on the test split.
I0307 13:57:19.415219 140352918893760 submission_runner.py:469] Time since start: 17170.58s, 	Step: 40981, 	{'train/accuracy': 0.6447504758834839, 'train/loss': 1.428864598274231, 'validation/accuracy': 0.5973799824714661, 'validation/loss': 1.664876937866211, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.373089551925659, 'test/num_examples': 10000, 'score': 15866.78595328331, 'total_duration': 17170.578610658646, 'accumulated_submission_time': 15866.78595328331, 'accumulated_eval_time': 1295.770164489746, 'accumulated_logging_time': 3.9590885639190674}
I0307 13:57:19.512373 140197563971328 logging_writer.py:48] [40981] accumulated_eval_time=1295.77, accumulated_logging_time=3.95909, accumulated_submission_time=15866.8, global_step=40981, preemption_count=0, score=15866.8, test/accuracy=0.4794, test/loss=2.37309, test/num_examples=10000, total_duration=17170.6, train/accuracy=0.64475, train/loss=1.42886, validation/accuracy=0.59738, validation/loss=1.66488, validation/num_examples=50000
I0307 13:57:27.365008 140197572364032 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.8495965003967285, loss=2.001451015472412
I0307 13:58:05.805940 140197563971328 logging_writer.py:48] [41100] global_step=41100, grad_norm=4.54224967956543, loss=1.7821729183197021
I0307 13:58:44.570222 140197572364032 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.6713366508483887, loss=1.9199914932250977
I0307 13:59:23.461325 140197563971328 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.3950788974761963, loss=1.8102115392684937
I0307 14:00:03.187076 140197572364032 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.7473461627960205, loss=1.8559740781784058
I0307 14:00:41.724541 140197563971328 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.6907968521118164, loss=1.9652279615402222
I0307 14:01:20.126036 140197572364032 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.079387664794922, loss=1.853710651397705
I0307 14:01:58.498730 140197563971328 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.0259480476379395, loss=1.9928712844848633
I0307 14:02:36.993438 140197572364032 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.8014914989471436, loss=1.9023686647415161
I0307 14:03:15.433658 140197563971328 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.5006258487701416, loss=1.838977336883545
I0307 14:03:54.153864 140197572364032 logging_writer.py:48] [42000] global_step=42000, grad_norm=4.000809669494629, loss=1.8305481672286987
I0307 14:04:32.478940 140197563971328 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.815164089202881, loss=1.8528485298156738
I0307 14:05:10.715051 140197572364032 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.7920546531677246, loss=2.0492310523986816
I0307 14:05:49.152057 140197563971328 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.717748165130615, loss=2.0616610050201416
I0307 14:05:49.629433 140352918893760 spec.py:321] Evaluating on the training split.
I0307 14:06:02.036564 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 14:06:25.200709 140352918893760 spec.py:349] Evaluating on the test split.
I0307 14:06:26.942166 140352918893760 submission_runner.py:469] Time since start: 17718.11s, 	Step: 42302, 	{'train/accuracy': 0.6488759517669678, 'train/loss': 1.4041895866394043, 'validation/accuracy': 0.5999999642372131, 'validation/loss': 1.6582494974136353, 'validation/num_examples': 50000, 'test/accuracy': 0.4732000231742859, 'test/loss': 2.4052813053131104, 'test/num_examples': 10000, 'score': 16376.734061479568, 'total_duration': 17718.105592489243, 'accumulated_submission_time': 16376.734061479568, 'accumulated_eval_time': 1333.0827143192291, 'accumulated_logging_time': 4.0905561447143555}
I0307 14:06:27.023872 140197572364032 logging_writer.py:48] [42302] accumulated_eval_time=1333.08, accumulated_logging_time=4.09056, accumulated_submission_time=16376.7, global_step=42302, preemption_count=0, score=16376.7, test/accuracy=0.4732, test/loss=2.40528, test/num_examples=10000, total_duration=17718.1, train/accuracy=0.648876, train/loss=1.40419, validation/accuracy=0.6, validation/loss=1.65825, validation/num_examples=50000
I0307 14:07:04.922072 140197563971328 logging_writer.py:48] [42400] global_step=42400, grad_norm=4.3473968505859375, loss=1.8942756652832031
I0307 14:07:44.220855 140197572364032 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.401989698410034, loss=1.923470139503479
I0307 14:08:23.788386 140197563971328 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.7389142513275146, loss=2.0292422771453857
I0307 14:09:02.491361 140197572364032 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.9918580055236816, loss=1.883053183555603
I0307 14:09:41.537113 140197563971328 logging_writer.py:48] [42800] global_step=42800, grad_norm=4.026939392089844, loss=1.9419028759002686
I0307 14:10:20.435317 140197572364032 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.590228796005249, loss=1.9335463047027588
I0307 14:10:58.958751 140197563971328 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.600153923034668, loss=1.8987605571746826
I0307 14:11:37.690721 140197572364032 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.7275452613830566, loss=1.9645557403564453
I0307 14:12:16.239191 140197563971328 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.2324209213256836, loss=1.8992960453033447
I0307 14:12:54.850174 140197572364032 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.090200901031494, loss=1.8063850402832031
I0307 14:13:33.511855 140197563971328 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.9071507453918457, loss=1.983219861984253
I0307 14:14:12.014257 140197572364032 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.6527788639068604, loss=1.8475106954574585
I0307 14:14:50.785359 140197563971328 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.4445431232452393, loss=1.9552874565124512
I0307 14:14:57.245050 140352918893760 spec.py:321] Evaluating on the training split.
I0307 14:15:09.983959 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 14:15:30.313809 140352918893760 spec.py:349] Evaluating on the test split.
I0307 14:15:32.043510 140352918893760 submission_runner.py:469] Time since start: 18263.21s, 	Step: 43618, 	{'train/accuracy': 0.6426379084587097, 'train/loss': 1.4269685745239258, 'validation/accuracy': 0.5949400067329407, 'validation/loss': 1.6832212209701538, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.40596604347229, 'test/num_examples': 10000, 'score': 16886.791135787964, 'total_duration': 18263.206958532333, 'accumulated_submission_time': 16886.791135787964, 'accumulated_eval_time': 1367.8810138702393, 'accumulated_logging_time': 4.205563068389893}
I0307 14:15:32.114234 140197572364032 logging_writer.py:48] [43618] accumulated_eval_time=1367.88, accumulated_logging_time=4.20556, accumulated_submission_time=16886.8, global_step=43618, preemption_count=0, score=16886.8, test/accuracy=0.48, test/loss=2.40597, test/num_examples=10000, total_duration=18263.2, train/accuracy=0.642638, train/loss=1.42697, validation/accuracy=0.59494, validation/loss=1.68322, validation/num_examples=50000
I0307 14:16:04.045436 140197563971328 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.8768253326416016, loss=1.8549623489379883
I0307 14:16:43.271934 140197572364032 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.659257650375366, loss=1.8926928043365479
I0307 14:17:21.796788 140197563971328 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.356189012527466, loss=1.9321893453598022
I0307 14:18:00.130379 140197572364032 logging_writer.py:48] [44000] global_step=44000, grad_norm=4.222614765167236, loss=1.997030258178711
I0307 14:18:38.694003 140197563971328 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.4154019355773926, loss=1.978029727935791
I0307 14:19:17.340139 140197572364032 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.501162052154541, loss=1.8978641033172607
I0307 14:19:56.002779 140197563971328 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.7841780185699463, loss=1.8426837921142578
I0307 14:20:34.910706 140197572364032 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.3984627723693848, loss=1.9269803762435913
I0307 14:21:13.345141 140197563971328 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.3770854473114014, loss=1.9153687953948975
I0307 14:21:51.656896 140197572364032 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.4767556190490723, loss=1.811354160308838
I0307 14:22:30.401842 140197563971328 logging_writer.py:48] [44700] global_step=44700, grad_norm=4.813483715057373, loss=1.9053552150726318
I0307 14:23:08.964716 140197572364032 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.0994296073913574, loss=1.8076908588409424
I0307 14:23:47.529165 140197563971328 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.7733750343322754, loss=1.8218402862548828
I0307 14:24:02.196648 140352918893760 spec.py:321] Evaluating on the training split.
I0307 14:24:15.270274 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 14:24:34.769375 140352918893760 spec.py:349] Evaluating on the test split.
I0307 14:24:36.515000 140352918893760 submission_runner.py:469] Time since start: 18807.68s, 	Step: 44937, 	{'train/accuracy': 0.6607541441917419, 'train/loss': 1.3578873872756958, 'validation/accuracy': 0.6111800074577332, 'validation/loss': 1.6031805276870728, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.3247499465942383, 'test/num_examples': 10000, 'score': 17396.713400125504, 'total_duration': 18807.67843437195, 'accumulated_submission_time': 17396.713400125504, 'accumulated_eval_time': 1402.1991946697235, 'accumulated_logging_time': 4.3023681640625}
I0307 14:24:36.602765 140197572364032 logging_writer.py:48] [44937] accumulated_eval_time=1402.2, accumulated_logging_time=4.30237, accumulated_submission_time=17396.7, global_step=44937, preemption_count=0, score=17396.7, test/accuracy=0.4865, test/loss=2.32475, test/num_examples=10000, total_duration=18807.7, train/accuracy=0.660754, train/loss=1.35789, validation/accuracy=0.61118, validation/loss=1.60318, validation/num_examples=50000
I0307 14:25:01.457087 140197563971328 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.219102382659912, loss=1.8987126350402832
I0307 14:25:40.789235 140197572364032 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.7632880210876465, loss=1.9562383890151978
I0307 14:26:19.381745 140197563971328 logging_writer.py:48] [45200] global_step=45200, grad_norm=4.291871547698975, loss=1.9970557689666748
I0307 14:26:57.986381 140197572364032 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.4960591793060303, loss=2.051473617553711
I0307 14:27:36.638591 140197563971328 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.8427822589874268, loss=2.063652992248535
I0307 14:28:15.148239 140197572364032 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.460019588470459, loss=1.944048285484314
I0307 14:28:53.657684 140197563971328 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.1966636180877686, loss=1.7608528137207031
I0307 14:29:32.266216 140197572364032 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.3123955726623535, loss=1.9573522806167603
I0307 14:30:10.927652 140197563971328 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.748267650604248, loss=1.8980412483215332
I0307 14:30:49.344620 140197572364032 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.135139465332031, loss=1.9720866680145264
I0307 14:31:27.909068 140197563971328 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.560255527496338, loss=1.904882550239563
I0307 14:32:06.657779 140197572364032 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.323772430419922, loss=1.8826048374176025
I0307 14:32:44.784957 140197563971328 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.7490358352661133, loss=1.8999426364898682
I0307 14:33:06.537701 140352918893760 spec.py:321] Evaluating on the training split.
I0307 14:33:18.824965 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 14:33:45.280400 140352918893760 spec.py:349] Evaluating on the test split.
I0307 14:33:47.020503 140352918893760 submission_runner.py:469] Time since start: 19358.18s, 	Step: 46257, 	{'train/accuracy': 0.6428172588348389, 'train/loss': 1.4479413032531738, 'validation/accuracy': 0.5979399681091309, 'validation/loss': 1.6964576244354248, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.3945939540863037, 'test/num_examples': 10000, 'score': 17906.49174261093, 'total_duration': 19358.183930158615, 'accumulated_submission_time': 17906.49174261093, 'accumulated_eval_time': 1442.6818182468414, 'accumulated_logging_time': 4.415114402770996}
I0307 14:33:47.119955 140197572364032 logging_writer.py:48] [46257] accumulated_eval_time=1442.68, accumulated_logging_time=4.41511, accumulated_submission_time=17906.5, global_step=46257, preemption_count=0, score=17906.5, test/accuracy=0.4745, test/loss=2.39459, test/num_examples=10000, total_duration=19358.2, train/accuracy=0.642817, train/loss=1.44794, validation/accuracy=0.59794, validation/loss=1.69646, validation/num_examples=50000
I0307 14:34:04.588921 140197563971328 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.43247389793396, loss=1.8643858432769775
I0307 14:34:43.158148 140197572364032 logging_writer.py:48] [46400] global_step=46400, grad_norm=4.238635540008545, loss=1.9749901294708252
I0307 14:35:21.624852 140197563971328 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.5620830059051514, loss=1.7958486080169678
I0307 14:36:00.560790 140197572364032 logging_writer.py:48] [46600] global_step=46600, grad_norm=4.165076732635498, loss=1.8210210800170898
I0307 14:36:39.038365 140197563971328 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.5680630207061768, loss=1.9791258573532104
I0307 14:37:17.457456 140197572364032 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.716522693634033, loss=1.886906623840332
I0307 14:37:55.955504 140197563971328 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.7810769081115723, loss=1.8067941665649414
I0307 14:38:34.600719 140197572364032 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.2473130226135254, loss=1.9103519916534424
I0307 14:39:13.082857 140197563971328 logging_writer.py:48] [47100] global_step=47100, grad_norm=4.024412155151367, loss=1.774010181427002
I0307 14:39:51.645819 140197572364032 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.265500783920288, loss=1.8528779745101929
I0307 14:40:30.644916 140197563971328 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.6687004566192627, loss=1.9394930601119995
I0307 14:41:09.165498 140197572364032 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.6820287704467773, loss=1.7859203815460205
I0307 14:41:48.202680 140197563971328 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.5037765502929688, loss=1.883780837059021
I0307 14:42:17.382514 140352918893760 spec.py:321] Evaluating on the training split.
I0307 14:42:30.073859 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 14:42:50.226606 140352918893760 spec.py:349] Evaluating on the test split.
I0307 14:42:51.960848 140352918893760 submission_runner.py:469] Time since start: 19903.12s, 	Step: 47574, 	{'train/accuracy': 0.6429567933082581, 'train/loss': 1.4278398752212524, 'validation/accuracy': 0.6006399989128113, 'validation/loss': 1.6661896705627441, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.3839430809020996, 'test/num_examples': 10000, 'score': 18416.60352039337, 'total_duration': 19903.124287366867, 'accumulated_submission_time': 18416.60352039337, 'accumulated_eval_time': 1477.2599959373474, 'accumulated_logging_time': 4.535336494445801}
I0307 14:42:52.058995 140197572364032 logging_writer.py:48] [47574] accumulated_eval_time=1477.26, accumulated_logging_time=4.53534, accumulated_submission_time=18416.6, global_step=47574, preemption_count=0, score=18416.6, test/accuracy=0.473, test/loss=2.38394, test/num_examples=10000, total_duration=19903.1, train/accuracy=0.642957, train/loss=1.42784, validation/accuracy=0.60064, validation/loss=1.66619, validation/num_examples=50000
I0307 14:43:02.388826 140197563971328 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.550389528274536, loss=1.7967289686203003
I0307 14:43:40.449458 140197572364032 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.3220319747924805, loss=1.9643863439559937
I0307 14:44:18.683358 140197563971328 logging_writer.py:48] [47800] global_step=47800, grad_norm=4.140371322631836, loss=2.00765323638916
I0307 14:44:57.483955 140197572364032 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.826136827468872, loss=1.8429189920425415
I0307 14:45:35.909792 140197563971328 logging_writer.py:48] [48000] global_step=48000, grad_norm=4.474875450134277, loss=1.8395408391952515
I0307 14:46:14.529224 140197572364032 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.4645042419433594, loss=1.9321038722991943
I0307 14:46:53.016308 140197563971328 logging_writer.py:48] [48200] global_step=48200, grad_norm=4.425059795379639, loss=1.8531558513641357
I0307 14:47:31.702279 140197572364032 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.4919817447662354, loss=1.8706872463226318
I0307 14:48:10.261380 140197563971328 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.6234612464904785, loss=1.8692270517349243
I0307 14:48:48.984065 140197572364032 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.565115213394165, loss=1.892679214477539
I0307 14:49:27.965697 140197563971328 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.3264429569244385, loss=1.9313154220581055
I0307 14:50:07.532168 140197572364032 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.7694456577301025, loss=1.9179328680038452
I0307 14:50:46.744313 140197563971328 logging_writer.py:48] [48800] global_step=48800, grad_norm=4.019393444061279, loss=1.79532790184021
I0307 14:51:22.336941 140352918893760 spec.py:321] Evaluating on the training split.
I0307 14:51:35.204986 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 14:52:00.144337 140352918893760 spec.py:349] Evaluating on the test split.
I0307 14:52:02.473534 140352918893760 submission_runner.py:469] Time since start: 20453.64s, 	Step: 48890, 	{'train/accuracy': 0.6544961333274841, 'train/loss': 1.3837900161743164, 'validation/accuracy': 0.6061199903488159, 'validation/loss': 1.633638858795166, 'validation/num_examples': 50000, 'test/accuracy': 0.47760000824928284, 'test/loss': 2.377366065979004, 'test/num_examples': 10000, 'score': 18926.72492337227, 'total_duration': 20453.636912345886, 'accumulated_submission_time': 18926.72492337227, 'accumulated_eval_time': 1517.3963639736176, 'accumulated_logging_time': 4.659128189086914}
I0307 14:52:02.585849 140197572364032 logging_writer.py:48] [48890] accumulated_eval_time=1517.4, accumulated_logging_time=4.65913, accumulated_submission_time=18926.7, global_step=48890, preemption_count=0, score=18926.7, test/accuracy=0.4776, test/loss=2.37737, test/num_examples=10000, total_duration=20453.6, train/accuracy=0.654496, train/loss=1.38379, validation/accuracy=0.60612, validation/loss=1.63364, validation/num_examples=50000
I0307 14:52:07.027935 140197563971328 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.5705926418304443, loss=1.8892475366592407
I0307 14:52:45.722090 140197572364032 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.5831449031829834, loss=1.810799479484558
I0307 14:53:24.332653 140197563971328 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.7464897632598877, loss=1.934544563293457
I0307 14:54:02.795720 140197572364032 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.280524969100952, loss=1.8840714693069458
I0307 14:54:41.129353 140197563971328 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.2669224739074707, loss=1.8549628257751465
I0307 14:55:19.846402 140197572364032 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.7056779861450195, loss=1.7611020803451538
I0307 14:55:58.829598 140197563971328 logging_writer.py:48] [49500] global_step=49500, grad_norm=4.144995212554932, loss=1.9703404903411865
I0307 14:56:37.132322 140197572364032 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.733302593231201, loss=1.8043386936187744
I0307 14:57:16.072373 140197563971328 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.8133082389831543, loss=1.9019458293914795
I0307 14:57:54.597356 140197572364032 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.971755027770996, loss=1.882498025894165
I0307 14:58:33.628388 140197563971328 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.725517749786377, loss=1.8724766969680786
I0307 14:59:12.428178 140197572364032 logging_writer.py:48] [50000] global_step=50000, grad_norm=4.1684465408325195, loss=1.8160560131072998
I0307 14:59:51.261538 140197563971328 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.696873664855957, loss=1.7506909370422363
I0307 15:00:29.515442 140197572364032 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.635913848876953, loss=1.8497521877288818
I0307 15:00:32.634916 140352918893760 spec.py:321] Evaluating on the training split.
I0307 15:00:44.877756 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 15:01:04.862384 140352918893760 spec.py:349] Evaluating on the test split.
I0307 15:01:06.588943 140352918893760 submission_runner.py:469] Time since start: 20997.75s, 	Step: 50209, 	{'train/accuracy': 0.6552136540412903, 'train/loss': 1.374187707901001, 'validation/accuracy': 0.6062799692153931, 'validation/loss': 1.6164963245391846, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.331477165222168, 'test/num_examples': 10000, 'score': 19436.585611343384, 'total_duration': 20997.752379179, 'accumulated_submission_time': 19436.585611343384, 'accumulated_eval_time': 1551.3502197265625, 'accumulated_logging_time': 4.833312749862671}
I0307 15:01:06.748654 140197563971328 logging_writer.py:48] [50209] accumulated_eval_time=1551.35, accumulated_logging_time=4.83331, accumulated_submission_time=19436.6, global_step=50209, preemption_count=0, score=19436.6, test/accuracy=0.4872, test/loss=2.33148, test/num_examples=10000, total_duration=20997.8, train/accuracy=0.655214, train/loss=1.37419, validation/accuracy=0.60628, validation/loss=1.6165, validation/num_examples=50000
I0307 15:01:42.217663 140197572364032 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.2838547229766846, loss=1.7973302602767944
I0307 15:02:20.501743 140197563971328 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.705645799636841, loss=2.0767228603363037
I0307 15:02:59.440770 140197572364032 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.859226703643799, loss=1.8326773643493652
I0307 15:03:37.715908 140197563971328 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.4654252529144287, loss=1.8394203186035156
I0307 15:04:15.999576 140197572364032 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.246664524078369, loss=1.8524489402770996
I0307 15:04:54.770590 140197563971328 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.6947739124298096, loss=1.8295801877975464
I0307 15:05:33.013974 140197572364032 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.8267667293548584, loss=2.021977186203003
I0307 15:06:11.596715 140197563971328 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.72210955619812, loss=1.9276167154312134
I0307 15:06:50.152890 140197572364032 logging_writer.py:48] [51100] global_step=51100, grad_norm=4.078231334686279, loss=1.9513843059539795
I0307 15:07:29.549203 140197563971328 logging_writer.py:48] [51200] global_step=51200, grad_norm=4.2170538902282715, loss=1.8453892469406128
I0307 15:08:08.475519 140197572364032 logging_writer.py:48] [51300] global_step=51300, grad_norm=4.083808422088623, loss=1.8516138792037964
I0307 15:08:47.933089 140197563971328 logging_writer.py:48] [51400] global_step=51400, grad_norm=4.4444475173950195, loss=1.9076485633850098
I0307 15:09:26.726755 140197572364032 logging_writer.py:48] [51500] global_step=51500, grad_norm=4.380975246429443, loss=1.9040430784225464
I0307 15:09:36.754154 140352918893760 spec.py:321] Evaluating on the training split.
I0307 15:09:49.724196 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 15:10:09.930150 140352918893760 spec.py:349] Evaluating on the test split.
I0307 15:10:11.659952 140352918893760 submission_runner.py:469] Time since start: 21542.82s, 	Step: 51527, 	{'train/accuracy': 0.654316782951355, 'train/loss': 1.392316460609436, 'validation/accuracy': 0.6056599617004395, 'validation/loss': 1.6421709060668945, 'validation/num_examples': 50000, 'test/accuracy': 0.48490002751350403, 'test/loss': 2.364668607711792, 'test/num_examples': 10000, 'score': 19946.409878253937, 'total_duration': 21542.82337975502, 'accumulated_submission_time': 19946.409878253937, 'accumulated_eval_time': 1586.2558369636536, 'accumulated_logging_time': 5.044023513793945}
I0307 15:10:11.744977 140197563971328 logging_writer.py:48] [51527] accumulated_eval_time=1586.26, accumulated_logging_time=5.04402, accumulated_submission_time=19946.4, global_step=51527, preemption_count=0, score=19946.4, test/accuracy=0.4849, test/loss=2.36467, test/num_examples=10000, total_duration=21542.8, train/accuracy=0.654317, train/loss=1.39232, validation/accuracy=0.60566, validation/loss=1.64217, validation/num_examples=50000
I0307 15:10:40.267339 140197572364032 logging_writer.py:48] [51600] global_step=51600, grad_norm=4.147921562194824, loss=2.0044362545013428
I0307 15:11:18.599681 140197563971328 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.783494234085083, loss=1.8374111652374268
I0307 15:11:57.186784 140197572364032 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.3412435054779053, loss=1.9376447200775146
I0307 15:12:35.739151 140197563971328 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.987940788269043, loss=1.89967942237854
I0307 15:13:14.915609 140197572364032 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.4872617721557617, loss=1.9009151458740234
I0307 15:13:53.680281 140197563971328 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.7769830226898193, loss=1.8034343719482422
I0307 15:14:32.266837 140197572364032 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.438169002532959, loss=1.9694714546203613
I0307 15:15:11.125209 140197563971328 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.844337224960327, loss=1.9282951354980469
I0307 15:15:49.479055 140197572364032 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.830625295639038, loss=1.8251417875289917
I0307 15:16:28.231866 140197563971328 logging_writer.py:48] [52500] global_step=52500, grad_norm=4.129021167755127, loss=1.813812255859375
I0307 15:17:07.148577 140197572364032 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.8301541805267334, loss=1.8976969718933105
I0307 15:17:45.937187 140197563971328 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.3823001384735107, loss=1.9337519407272339
I0307 15:18:24.285614 140197572364032 logging_writer.py:48] [52800] global_step=52800, grad_norm=4.221856117248535, loss=1.8092361688613892
I0307 15:18:41.692863 140352918893760 spec.py:321] Evaluating on the training split.
I0307 15:18:54.425435 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 15:19:18.014368 140352918893760 spec.py:349] Evaluating on the test split.
I0307 15:19:19.760254 140352918893760 submission_runner.py:469] Time since start: 22090.92s, 	Step: 52846, 	{'train/accuracy': 0.6613919138908386, 'train/loss': 1.3567700386047363, 'validation/accuracy': 0.6155200004577637, 'validation/loss': 1.5953295230865479, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.300241470336914, 'test/num_examples': 10000, 'score': 20456.194468975067, 'total_duration': 22090.923674345016, 'accumulated_submission_time': 20456.194468975067, 'accumulated_eval_time': 1624.3230538368225, 'accumulated_logging_time': 5.1561198234558105}
I0307 15:19:19.846263 140197563971328 logging_writer.py:48] [52846] accumulated_eval_time=1624.32, accumulated_logging_time=5.15612, accumulated_submission_time=20456.2, global_step=52846, preemption_count=0, score=20456.2, test/accuracy=0.4864, test/loss=2.30024, test/num_examples=10000, total_duration=22090.9, train/accuracy=0.661392, train/loss=1.35677, validation/accuracy=0.61552, validation/loss=1.59533, validation/num_examples=50000
I0307 15:19:41.096006 140197572364032 logging_writer.py:48] [52900] global_step=52900, grad_norm=4.2863311767578125, loss=1.8662328720092773
I0307 15:20:19.565510 140197563971328 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.4803004264831543, loss=1.900522232055664
I0307 15:20:58.103510 140197572364032 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.4503250122070312, loss=1.7531582117080688
I0307 15:21:36.681141 140197563971328 logging_writer.py:48] [53200] global_step=53200, grad_norm=4.186587333679199, loss=1.9491878747940063
I0307 15:22:15.026789 140197572364032 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.654966115951538, loss=1.9183919429779053
I0307 15:22:53.573249 140197563971328 logging_writer.py:48] [53400] global_step=53400, grad_norm=4.046082496643066, loss=1.8711395263671875
I0307 15:23:31.892411 140197572364032 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.675244092941284, loss=1.880537748336792
I0307 15:24:10.401219 140197563971328 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.920459032058716, loss=1.9373304843902588
I0307 15:24:48.956193 140197572364032 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.7718567848205566, loss=1.8349862098693848
I0307 15:25:28.301904 140197563971328 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.046642303466797, loss=1.9068851470947266
I0307 15:26:06.821996 140197572364032 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.825355052947998, loss=1.910529613494873
I0307 15:26:45.377745 140197563971328 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.5444440841674805, loss=1.8593308925628662
I0307 15:27:23.955356 140197572364032 logging_writer.py:48] [54100] global_step=54100, grad_norm=4.363542556762695, loss=1.8935956954956055
I0307 15:27:49.929576 140352918893760 spec.py:321] Evaluating on the training split.
I0307 15:28:02.911136 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 15:28:25.353429 140352918893760 spec.py:349] Evaluating on the test split.
I0307 15:28:27.082587 140352918893760 submission_runner.py:469] Time since start: 22638.25s, 	Step: 54168, 	{'train/accuracy': 0.6633848547935486, 'train/loss': 1.3555982112884521, 'validation/accuracy': 0.6178399920463562, 'validation/loss': 1.5910332202911377, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.305330276489258, 'test/num_examples': 10000, 'score': 20966.108649492264, 'total_duration': 22638.246022224426, 'accumulated_submission_time': 20966.108649492264, 'accumulated_eval_time': 1661.4758970737457, 'accumulated_logging_time': 5.275086164474487}
I0307 15:28:27.166610 140197563971328 logging_writer.py:48] [54168] accumulated_eval_time=1661.48, accumulated_logging_time=5.27509, accumulated_submission_time=20966.1, global_step=54168, preemption_count=0, score=20966.1, test/accuracy=0.4956, test/loss=2.30533, test/num_examples=10000, total_duration=22638.2, train/accuracy=0.663385, train/loss=1.3556, validation/accuracy=0.61784, validation/loss=1.59103, validation/num_examples=50000
I0307 15:28:39.720846 140197572364032 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.650104284286499, loss=2.035324811935425
I0307 15:29:18.728322 140197563971328 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.530839443206787, loss=1.8890135288238525
I0307 15:29:57.030314 140197572364032 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.611630916595459, loss=1.7927687168121338
I0307 15:30:35.214178 140197563971328 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.258819341659546, loss=1.7774609327316284
I0307 15:31:13.633036 140197572364032 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.3622446060180664, loss=1.849301815032959
I0307 15:31:52.080435 140197563971328 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.8361918926239014, loss=1.8331706523895264
I0307 15:32:30.601089 140197572364032 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.5332279205322266, loss=1.7998132705688477
I0307 15:33:09.428255 140197563971328 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.629763126373291, loss=1.8220820426940918
I0307 15:33:48.304256 140197572364032 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.6516318321228027, loss=1.932713270187378
I0307 15:34:27.407923 140197563971328 logging_writer.py:48] [55100] global_step=55100, grad_norm=4.319724082946777, loss=1.8756766319274902
I0307 15:35:05.980000 140197572364032 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.7332613468170166, loss=1.9329853057861328
I0307 15:35:44.528833 140197563971328 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.5801596641540527, loss=1.8350112438201904
I0307 15:36:22.786346 140197572364032 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.452944278717041, loss=1.8033816814422607
I0307 15:36:57.247680 140352918893760 spec.py:321] Evaluating on the training split.
I0307 15:37:09.943486 140352918893760 spec.py:333] Evaluating on the validation split.
2025-03-07 15:37:13.303193: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 1073741824 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:37:13.682010: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 966367744 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:37:13.689347: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 869731072 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:37:13.717117: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 782758144 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:37:13.729047: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 704482304 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:37:13.735388: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 634034176 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:37:13.739949: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 570630912 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
I0307 15:37:51.874175 140352918893760 spec.py:349] Evaluating on the test split.
I0307 15:37:53.598067 140352918893760 submission_runner.py:469] Time since start: 23204.76s, 	Step: 55489, 	{'train/accuracy': 0.661531388759613, 'train/loss': 1.3388372659683228, 'validation/accuracy': 0.6109799742698669, 'validation/loss': 1.6009527444839478, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.3151907920837402, 'test/num_examples': 10000, 'score': 21476.029215574265, 'total_duration': 23204.76149201393, 'accumulated_submission_time': 21476.029215574265, 'accumulated_eval_time': 1717.8261032104492, 'accumulated_logging_time': 5.385563611984253}
I0307 15:37:53.714080 140197563971328 logging_writer.py:48] [55489] accumulated_eval_time=1717.83, accumulated_logging_time=5.38556, accumulated_submission_time=21476, global_step=55489, preemption_count=0, score=21476, test/accuracy=0.4937, test/loss=2.31519, test/num_examples=10000, total_duration=23204.8, train/accuracy=0.661531, train/loss=1.33884, validation/accuracy=0.61098, validation/loss=1.60095, validation/num_examples=50000
I0307 15:37:58.541270 140197572364032 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.3440775871276855, loss=1.85135018825531
I0307 15:38:37.036939 140197563971328 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.961031913757324, loss=1.915122151374817
I0307 15:39:15.139306 140197572364032 logging_writer.py:48] [55700] global_step=55700, grad_norm=4.06837272644043, loss=1.8517985343933105
I0307 15:39:53.488082 140197563971328 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.9951047897338867, loss=1.8631930351257324
I0307 15:40:32.394934 140197572364032 logging_writer.py:48] [55900] global_step=55900, grad_norm=4.276836395263672, loss=1.8754487037658691
I0307 15:41:10.920592 140197563971328 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.5996310710906982, loss=1.7903690338134766
I0307 15:41:49.799109 140197572364032 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.8477284908294678, loss=1.9308403730392456
I0307 15:42:29.014566 140197563971328 logging_writer.py:48] [56200] global_step=56200, grad_norm=4.201566696166992, loss=1.9054465293884277
I0307 15:43:08.086034 140197572364032 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.7239785194396973, loss=1.7478322982788086
I0307 15:43:47.088093 140197563971328 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.471561908721924, loss=1.7845548391342163
I0307 15:44:25.667799 140197572364032 logging_writer.py:48] [56500] global_step=56500, grad_norm=4.177104949951172, loss=1.8605939149856567
I0307 15:45:04.330982 140197563971328 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.8980393409729004, loss=1.8022137880325317
I0307 15:45:42.923586 140197572364032 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.660151958465576, loss=1.8918261528015137
I0307 15:46:21.553615 140197563971328 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.9490482807159424, loss=1.9229514598846436
I0307 15:46:23.869555 140352918893760 spec.py:321] Evaluating on the training split.
I0307 15:46:35.725808 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 15:47:18.708150 140352918893760 spec.py:349] Evaluating on the test split.
I0307 15:47:20.436146 140352918893760 submission_runner.py:469] Time since start: 23771.60s, 	Step: 56807, 	{'train/accuracy': 0.6571269035339355, 'train/loss': 1.3812739849090576, 'validation/accuracy': 0.6097800135612488, 'validation/loss': 1.6252782344818115, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3605759143829346, 'test/num_examples': 10000, 'score': 21986.00971031189, 'total_duration': 23771.59955573082, 'accumulated_submission_time': 21986.00971031189, 'accumulated_eval_time': 1774.39249253273, 'accumulated_logging_time': 5.536667108535767}
I0307 15:47:20.516497 140197572364032 logging_writer.py:48] [56807] accumulated_eval_time=1774.39, accumulated_logging_time=5.53667, accumulated_submission_time=21986, global_step=56807, preemption_count=0, score=21986, test/accuracy=0.4917, test/loss=2.36058, test/num_examples=10000, total_duration=23771.6, train/accuracy=0.657127, train/loss=1.38127, validation/accuracy=0.60978, validation/loss=1.62528, validation/num_examples=50000
I0307 15:47:56.886976 140197563971328 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.634690284729004, loss=1.7758665084838867
I0307 15:48:35.373568 140197572364032 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.245823621749878, loss=1.73537015914917
I0307 15:49:13.765305 140197563971328 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.4411072731018066, loss=1.7884901762008667
I0307 15:49:52.645900 140197572364032 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.4193003177642822, loss=1.899261236190796
I0307 15:50:31.136497 140197563971328 logging_writer.py:48] [57300] global_step=57300, grad_norm=4.444727897644043, loss=1.989042043685913
I0307 15:51:09.884655 140197572364032 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.776355028152466, loss=1.886833667755127
I0307 15:51:48.144445 140197563971328 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.5497114658355713, loss=1.7562446594238281
I0307 15:52:27.305325 140197572364032 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.607764959335327, loss=1.788224220275879
I0307 15:53:05.625592 140197563971328 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.852309226989746, loss=1.8080308437347412
I0307 15:53:44.705456 140197572364032 logging_writer.py:48] [57800] global_step=57800, grad_norm=4.125341892242432, loss=1.908024549484253
I0307 15:54:57.158868 140197563971328 logging_writer.py:48] [57900] global_step=57900, grad_norm=4.136922836303711, loss=2.0229978561401367
I0307 15:55:36.716243 140197572364032 logging_writer.py:48] [58000] global_step=58000, grad_norm=4.008355617523193, loss=1.842519760131836
I0307 15:55:50.648909 140352918893760 spec.py:321] Evaluating on the training split.
I0307 15:56:02.950751 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 15:56:53.659940 140352918893760 spec.py:349] Evaluating on the test split.
I0307 15:56:55.381042 140352918893760 submission_runner.py:469] Time since start: 24346.54s, 	Step: 58037, 	{'train/accuracy': 0.6680285334587097, 'train/loss': 1.3184261322021484, 'validation/accuracy': 0.6144799590110779, 'validation/loss': 1.5974048376083374, 'validation/num_examples': 50000, 'test/accuracy': 0.4896000325679779, 'test/loss': 2.3437349796295166, 'test/num_examples': 10000, 'score': 22495.949635267258, 'total_duration': 24346.544462919235, 'accumulated_submission_time': 22495.949635267258, 'accumulated_eval_time': 1839.1244399547577, 'accumulated_logging_time': 5.676953554153442}
I0307 15:56:55.485175 140197563971328 logging_writer.py:48] [58037] accumulated_eval_time=1839.12, accumulated_logging_time=5.67695, accumulated_submission_time=22495.9, global_step=58037, preemption_count=0, score=22495.9, test/accuracy=0.4896, test/loss=2.34373, test/num_examples=10000, total_duration=24346.5, train/accuracy=0.668029, train/loss=1.31843, validation/accuracy=0.61448, validation/loss=1.5974, validation/num_examples=50000
I0307 15:57:20.329151 140197572364032 logging_writer.py:48] [58100] global_step=58100, grad_norm=4.541924953460693, loss=1.9278895854949951
I0307 15:57:58.768380 140197563971328 logging_writer.py:48] [58200] global_step=58200, grad_norm=4.367109298706055, loss=1.9184246063232422
I0307 15:58:37.880730 140197572364032 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.534444808959961, loss=1.8357598781585693
I0307 15:59:16.937184 140197563971328 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.5604805946350098, loss=1.8195213079452515
I0307 15:59:55.668969 140197572364032 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.8301141262054443, loss=1.8391138315200806
I0307 16:00:34.998915 140197563971328 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.378955602645874, loss=1.79118812084198
I0307 16:01:14.774609 140197572364032 logging_writer.py:48] [58700] global_step=58700, grad_norm=4.6384382247924805, loss=1.921126127243042
I0307 16:01:53.825680 140197563971328 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.4737443923950195, loss=1.8461132049560547
I0307 16:02:32.684137 140197572364032 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.7226994037628174, loss=1.7815786600112915
I0307 16:03:11.674658 140197563971328 logging_writer.py:48] [59000] global_step=59000, grad_norm=4.2262115478515625, loss=1.7824853658676147
I0307 16:03:50.697580 140197572364032 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.4548420906066895, loss=1.8252544403076172
I0307 16:04:29.152392 140197563971328 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.4133095741271973, loss=1.8178138732910156
I0307 16:05:07.975208 140197572364032 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.7321951389312744, loss=1.8942861557006836
I0307 16:05:25.478545 140352918893760 spec.py:321] Evaluating on the training split.
I0307 16:05:38.182214 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 16:06:25.640113 140352918893760 spec.py:349] Evaluating on the test split.
I0307 16:06:27.346725 140352918893760 submission_runner.py:469] Time since start: 24918.51s, 	Step: 59346, 	{'train/accuracy': 0.64453125, 'train/loss': 1.429885983467102, 'validation/accuracy': 0.5986199975013733, 'validation/loss': 1.6816864013671875, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.440908432006836, 'test/num_examples': 10000, 'score': 23005.76326084137, 'total_duration': 24918.510147094727, 'accumulated_submission_time': 23005.76326084137, 'accumulated_eval_time': 1900.992432832718, 'accumulated_logging_time': 5.813225507736206}
I0307 16:06:27.407397 140197563971328 logging_writer.py:48] [59346] accumulated_eval_time=1900.99, accumulated_logging_time=5.81323, accumulated_submission_time=23005.8, global_step=59346, preemption_count=0, score=23005.8, test/accuracy=0.4731, test/loss=2.44091, test/num_examples=10000, total_duration=24918.5, train/accuracy=0.644531, train/loss=1.42989, validation/accuracy=0.59862, validation/loss=1.68169, validation/num_examples=50000
I0307 16:06:48.601140 140197572364032 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.2973828315734863, loss=1.870117425918579
I0307 16:07:27.533989 140197563971328 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.575291872024536, loss=1.8194079399108887
I0307 16:08:06.572902 140197572364032 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.9516141414642334, loss=1.8677181005477905
I0307 16:08:45.557460 140197563971328 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.9433114528656006, loss=1.8910939693450928
I0307 16:09:24.403050 140197572364032 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.8742785453796387, loss=1.9674019813537598
I0307 16:10:04.073164 140197563971328 logging_writer.py:48] [59900] global_step=59900, grad_norm=4.028130054473877, loss=1.7642300128936768
I0307 16:10:44.080964 140197572364032 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.3332595825195312, loss=1.7517399787902832
2025-03-07 16:10:54.881723: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:11:23.437716 140197563971328 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.209426164627075, loss=1.8348335027694702
I0307 16:12:02.926240 140197572364032 logging_writer.py:48] [60200] global_step=60200, grad_norm=4.128173351287842, loss=1.7496964931488037
I0307 16:12:41.353682 140197563971328 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.824726104736328, loss=1.7237801551818848
I0307 16:13:20.417696 140197572364032 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.6075589656829834, loss=1.916633129119873
I0307 16:13:59.778993 140197563971328 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.556324005126953, loss=1.8634061813354492
I0307 16:14:39.324676 140197572364032 logging_writer.py:48] [60600] global_step=60600, grad_norm=4.631348609924316, loss=1.8100706338882446
I0307 16:14:57.462922 140352918893760 spec.py:321] Evaluating on the training split.
I0307 16:15:09.316890 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 16:15:56.294156 140352918893760 spec.py:349] Evaluating on the test split.
I0307 16:15:59.493290 140352918893760 submission_runner.py:469] Time since start: 25490.66s, 	Step: 60647, 	{'train/accuracy': 0.6511678695678711, 'train/loss': 1.397667407989502, 'validation/accuracy': 0.602840006351471, 'validation/loss': 1.6418415307998657, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.380911350250244, 'test/num_examples': 10000, 'score': 23515.648574113846, 'total_duration': 25490.656725645065, 'accumulated_submission_time': 23515.648574113846, 'accumulated_eval_time': 1963.022631406784, 'accumulated_logging_time': 5.893031597137451}
I0307 16:15:59.624489 140197563971328 logging_writer.py:48] [60647] accumulated_eval_time=1963.02, accumulated_logging_time=5.89303, accumulated_submission_time=23515.6, global_step=60647, preemption_count=0, score=23515.6, test/accuracy=0.4794, test/loss=2.38091, test/num_examples=10000, total_duration=25490.7, train/accuracy=0.651168, train/loss=1.39767, validation/accuracy=0.60284, validation/loss=1.64184, validation/num_examples=50000
I0307 16:16:20.933476 140197572364032 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.4655680656433105, loss=1.691852331161499
I0307 16:17:00.647445 140197563971328 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.902740001678467, loss=1.702347755432129
I0307 16:17:40.776971 140197572364032 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.443716526031494, loss=1.8579998016357422
I0307 16:18:19.760359 140197563971328 logging_writer.py:48] [61000] global_step=61000, grad_norm=4.003902912139893, loss=1.8105939626693726
I0307 16:18:59.221140 140197572364032 logging_writer.py:48] [61100] global_step=61100, grad_norm=4.395778179168701, loss=1.8653502464294434
I0307 16:19:39.449795 140197563971328 logging_writer.py:48] [61200] global_step=61200, grad_norm=4.145920276641846, loss=1.9433168172836304
2025-03-07 16:20:10.442412: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:20:19.119095 140197572364032 logging_writer.py:48] [61300] global_step=61300, grad_norm=4.199350833892822, loss=1.9084182977676392
I0307 16:20:57.947317 140197563971328 logging_writer.py:48] [61400] global_step=61400, grad_norm=4.054062366485596, loss=1.9860724210739136
I0307 16:21:36.852899 140197572364032 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.8354814052581787, loss=1.8379173278808594
I0307 16:22:16.291194 140197563971328 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.6184418201446533, loss=1.7796118259429932
I0307 16:22:55.596129 140197572364032 logging_writer.py:48] [61700] global_step=61700, grad_norm=4.2674360275268555, loss=1.8686580657958984
I0307 16:23:35.324373 140197563971328 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.558422803878784, loss=1.7429592609405518
I0307 16:24:14.332305 140197572364032 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.7732505798339844, loss=1.8554538488388062
I0307 16:24:29.832725 140352918893760 spec.py:321] Evaluating on the training split.
I0307 16:24:41.770897 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 16:25:18.493126 140352918893760 spec.py:349] Evaluating on the test split.
I0307 16:25:20.234729 140352918893760 submission_runner.py:469] Time since start: 26051.40s, 	Step: 61941, 	{'train/accuracy': 0.6648397445678711, 'train/loss': 1.3401981592178345, 'validation/accuracy': 0.6149199604988098, 'validation/loss': 1.5920711755752563, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.3203177452087402, 'test/num_examples': 10000, 'score': 24025.66006588936, 'total_duration': 26051.398130893707, 'accumulated_submission_time': 24025.66006588936, 'accumulated_eval_time': 2013.4244344234467, 'accumulated_logging_time': 6.06721305847168}
I0307 16:25:20.324786 140197563971328 logging_writer.py:48] [61941] accumulated_eval_time=2013.42, accumulated_logging_time=6.06721, accumulated_submission_time=24025.7, global_step=61941, preemption_count=0, score=24025.7, test/accuracy=0.4933, test/loss=2.32032, test/num_examples=10000, total_duration=26051.4, train/accuracy=0.66484, train/loss=1.3402, validation/accuracy=0.61492, validation/loss=1.59207, validation/num_examples=50000
I0307 16:25:43.698915 140197572364032 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.6109235286712646, loss=1.8002557754516602
I0307 16:26:23.057566 140197563971328 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.6953279972076416, loss=1.832653522491455
I0307 16:27:02.733300 140197572364032 logging_writer.py:48] [62200] global_step=62200, grad_norm=4.135374546051025, loss=1.8858650922775269
I0307 16:27:41.596635 140197563971328 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.8407559394836426, loss=1.8612210750579834
I0307 16:28:21.262699 140197572364032 logging_writer.py:48] [62400] global_step=62400, grad_norm=4.0504255294799805, loss=1.9048465490341187
I0307 16:29:01.550538 140197563971328 logging_writer.py:48] [62500] global_step=62500, grad_norm=4.0101213455200195, loss=1.8329997062683105
I0307 16:29:41.237604 140197572364032 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.8771417140960693, loss=1.869112491607666
I0307 16:30:19.917951 140197563971328 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.9880290031433105, loss=1.7883116006851196
I0307 16:30:58.975774 140197572364032 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.5128164291381836, loss=1.9619405269622803
I0307 16:31:38.517140 140197563971328 logging_writer.py:48] [62900] global_step=62900, grad_norm=4.132991313934326, loss=1.8555643558502197
I0307 16:32:18.186710 140197572364032 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.8717002868652344, loss=1.861226201057434
I0307 16:32:57.850721 140197563971328 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.6594676971435547, loss=1.9351370334625244
I0307 16:33:37.726448 140197572364032 logging_writer.py:48] [63200] global_step=63200, grad_norm=4.091361045837402, loss=1.8142098188400269
I0307 16:33:50.278912 140352918893760 spec.py:321] Evaluating on the training split.
I0307 16:34:02.885959 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 16:34:47.910482 140352918893760 spec.py:349] Evaluating on the test split.
I0307 16:34:49.616895 140352918893760 submission_runner.py:469] Time since start: 26620.78s, 	Step: 63232, 	{'train/accuracy': 0.6753228306770325, 'train/loss': 1.284428358078003, 'validation/accuracy': 0.6259799599647522, 'validation/loss': 1.5430666208267212, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.2611074447631836, 'test/num_examples': 10000, 'score': 24535.41486644745, 'total_duration': 26620.780292749405, 'accumulated_submission_time': 24535.41486644745, 'accumulated_eval_time': 2072.762209415436, 'accumulated_logging_time': 6.205001354217529}
I0307 16:34:49.713339 140197563971328 logging_writer.py:48] [63232] accumulated_eval_time=2072.76, accumulated_logging_time=6.205, accumulated_submission_time=24535.4, global_step=63232, preemption_count=0, score=24535.4, test/accuracy=0.4986, test/loss=2.26111, test/num_examples=10000, total_duration=26620.8, train/accuracy=0.675323, train/loss=1.28443, validation/accuracy=0.62598, validation/loss=1.54307, validation/num_examples=50000
I0307 16:35:17.013847 140197572364032 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.8930232524871826, loss=1.9270817041397095
I0307 16:35:56.401177 140197563971328 logging_writer.py:48] [63400] global_step=63400, grad_norm=4.206237316131592, loss=1.8442785739898682
I0307 16:36:35.028722 140197572364032 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.668100357055664, loss=1.7957775592803955
I0307 16:37:15.020501 140197563971328 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.809828996658325, loss=1.9768818616867065
I0307 16:37:55.431430 140197572364032 logging_writer.py:48] [63700] global_step=63700, grad_norm=4.3154144287109375, loss=1.9755933284759521
I0307 16:38:35.223665 140197563971328 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.957125186920166, loss=1.8608161211013794
I0307 16:39:14.605593 140197572364032 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.7502830028533936, loss=1.851361870765686
I0307 16:39:53.915661 140197563971328 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.552509069442749, loss=1.8244248628616333
I0307 16:40:33.451656 140197572364032 logging_writer.py:48] [64100] global_step=64100, grad_norm=4.545726776123047, loss=1.8366161584854126
I0307 16:41:12.932248 140197563971328 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.81835675239563, loss=1.8688846826553345
I0307 16:41:52.675155 140197572364032 logging_writer.py:48] [64300] global_step=64300, grad_norm=4.117501735687256, loss=1.8482524156570435
I0307 16:42:32.480045 140197563971328 logging_writer.py:48] [64400] global_step=64400, grad_norm=4.633176803588867, loss=1.8409303426742554
I0307 16:43:12.366238 140197572364032 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.734962224960327, loss=1.772240400314331
I0307 16:43:19.681779 140352918893760 spec.py:321] Evaluating on the training split.
I0307 16:43:31.734561 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 16:44:18.011605 140352918893760 spec.py:349] Evaluating on the test split.
I0307 16:44:19.697182 140352918893760 submission_runner.py:469] Time since start: 27190.86s, 	Step: 64519, 	{'train/accuracy': 0.6681281924247742, 'train/loss': 1.325324535369873, 'validation/accuracy': 0.6156399846076965, 'validation/loss': 1.583498239517212, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.3166275024414062, 'test/num_examples': 10000, 'score': 25045.1871778965, 'total_duration': 27190.860607624054, 'accumulated_submission_time': 25045.1871778965, 'accumulated_eval_time': 2132.777435541153, 'accumulated_logging_time': 6.342515230178833}
I0307 16:44:19.769078 140197563971328 logging_writer.py:48] [64519] accumulated_eval_time=2132.78, accumulated_logging_time=6.34252, accumulated_submission_time=25045.2, global_step=64519, preemption_count=0, score=25045.2, test/accuracy=0.4862, test/loss=2.31663, test/num_examples=10000, total_duration=27190.9, train/accuracy=0.668128, train/loss=1.32532, validation/accuracy=0.61564, validation/loss=1.5835, validation/num_examples=50000
I0307 16:44:52.992587 140197572364032 logging_writer.py:48] [64600] global_step=64600, grad_norm=4.024388790130615, loss=1.8141812086105347
I0307 16:45:32.733143 140197563971328 logging_writer.py:48] [64700] global_step=64700, grad_norm=4.151470184326172, loss=1.9213696718215942
I0307 16:46:12.714894 140197572364032 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.9740469455718994, loss=1.869633674621582
I0307 16:46:52.648884 140197563971328 logging_writer.py:48] [64900] global_step=64900, grad_norm=5.0904622077941895, loss=1.8824779987335205
I0307 16:47:31.925187 140197572364032 logging_writer.py:48] [65000] global_step=65000, grad_norm=4.204732418060303, loss=1.841796875
I0307 16:48:11.032885 140197563971328 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.7151005268096924, loss=1.8046369552612305
I0307 16:48:49.704249 140197572364032 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.9537367820739746, loss=1.8517200946807861
I0307 16:49:27.989697 140197563971328 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.7411482334136963, loss=1.7968308925628662
I0307 16:50:06.879005 140197572364032 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.6294217109680176, loss=1.8083341121673584
I0307 16:50:45.725660 140197563971328 logging_writer.py:48] [65500] global_step=65500, grad_norm=4.063958168029785, loss=1.8457103967666626
I0307 16:51:24.049197 140197572364032 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.5567424297332764, loss=1.7464289665222168
I0307 16:52:02.610046 140197563971328 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.631988763809204, loss=1.843621015548706
I0307 16:52:41.818996 140197572364032 logging_writer.py:48] [65800] global_step=65800, grad_norm=4.63422155380249, loss=1.780552625656128
I0307 16:52:50.053547 140352918893760 spec.py:321] Evaluating on the training split.
I0307 16:53:01.916861 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 16:53:54.192145 140352918893760 spec.py:349] Evaluating on the test split.
I0307 16:53:57.289031 140352918893760 submission_runner.py:469] Time since start: 27768.45s, 	Step: 65822, 	{'train/accuracy': 0.6639030575752258, 'train/loss': 1.3512518405914307, 'validation/accuracy': 0.6155799627304077, 'validation/loss': 1.5866771936416626, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.323444366455078, 'test/num_examples': 10000, 'score': 25555.278134822845, 'total_duration': 27768.452460050583, 'accumulated_submission_time': 25555.278134822845, 'accumulated_eval_time': 2200.0127398967743, 'accumulated_logging_time': 6.463808536529541}
I0307 16:53:57.371938 140197563971328 logging_writer.py:48] [65822] accumulated_eval_time=2200.01, accumulated_logging_time=6.46381, accumulated_submission_time=25555.3, global_step=65822, preemption_count=0, score=25555.3, test/accuracy=0.4861, test/loss=2.32344, test/num_examples=10000, total_duration=27768.5, train/accuracy=0.663903, train/loss=1.35125, validation/accuracy=0.61558, validation/loss=1.58668, validation/num_examples=50000
I0307 16:54:27.871658 140197572364032 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.6760520935058594, loss=1.8200948238372803
I0307 16:55:06.520048 140197563971328 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.837705373764038, loss=1.7716253995895386
I0307 16:55:45.452153 140197572364032 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.6528091430664062, loss=1.7465505599975586
I0307 16:56:25.349340 140197563971328 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.433072328567505, loss=1.7210736274719238
I0307 16:57:04.394613 140197572364032 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.671359062194824, loss=1.8668218851089478
I0307 16:57:43.265476 140197563971328 logging_writer.py:48] [66400] global_step=66400, grad_norm=4.390320301055908, loss=1.9071133136749268
I0307 16:58:22.060317 140197572364032 logging_writer.py:48] [66500] global_step=66500, grad_norm=4.172622203826904, loss=1.7532823085784912
I0307 16:59:00.584508 140197563971328 logging_writer.py:48] [66600] global_step=66600, grad_norm=4.465733051300049, loss=1.8152720928192139
I0307 16:59:39.277289 140197572364032 logging_writer.py:48] [66700] global_step=66700, grad_norm=4.465835094451904, loss=1.8399672508239746
I0307 17:00:18.160639 140197563971328 logging_writer.py:48] [66800] global_step=66800, grad_norm=4.009918689727783, loss=1.7811338901519775
I0307 17:00:56.811077 140197572364032 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.538174629211426, loss=1.7095108032226562
I0307 17:01:35.282698 140197563971328 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.3738322257995605, loss=1.7814544439315796
I0307 17:02:13.900738 140197572364032 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.623253107070923, loss=1.8374093770980835
I0307 17:02:27.471057 140352918893760 spec.py:321] Evaluating on the training split.
I0307 17:02:39.827115 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 17:03:22.733786 140352918893760 spec.py:349] Evaluating on the test split.
I0307 17:03:24.427927 140352918893760 submission_runner.py:469] Time since start: 28335.59s, 	Step: 67136, 	{'train/accuracy': 0.6723533272743225, 'train/loss': 1.2799941301345825, 'validation/accuracy': 0.6279999613761902, 'validation/loss': 1.5221290588378906, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.2592999935150146, 'test/num_examples': 10000, 'score': 26065.21945309639, 'total_duration': 28335.591351270676, 'accumulated_submission_time': 26065.21945309639, 'accumulated_eval_time': 2256.9694254398346, 'accumulated_logging_time': 6.569955587387085}
I0307 17:03:24.569791 140197563971328 logging_writer.py:48] [67136] accumulated_eval_time=2256.97, accumulated_logging_time=6.56996, accumulated_submission_time=26065.2, global_step=67136, preemption_count=0, score=26065.2, test/accuracy=0.5012, test/loss=2.2593, test/num_examples=10000, total_duration=28335.6, train/accuracy=0.672353, train/loss=1.27999, validation/accuracy=0.628, validation/loss=1.52213, validation/num_examples=50000
I0307 17:03:50.378079 140197572364032 logging_writer.py:48] [67200] global_step=67200, grad_norm=4.076510906219482, loss=1.9320497512817383
I0307 17:04:29.136810 140197563971328 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.9060466289520264, loss=1.8244513273239136
I0307 17:05:08.230754 140197572364032 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.5538318157196045, loss=1.7608517408370972
I0307 17:05:47.348721 140197563971328 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.675717353820801, loss=1.7452392578125
I0307 17:06:26.615822 140197572364032 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.66866135597229, loss=1.8448971509933472
I0307 17:07:14.630069 140197563971328 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.8244340419769287, loss=1.8742594718933105
I0307 17:07:53.808098 140197572364032 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.45399808883667, loss=1.7655601501464844
I0307 17:08:32.858537 140197563971328 logging_writer.py:48] [67900] global_step=67900, grad_norm=4.717792987823486, loss=1.9061475992202759
I0307 17:09:11.846623 140197572364032 logging_writer.py:48] [68000] global_step=68000, grad_norm=4.915699481964111, loss=1.6619720458984375
I0307 17:09:50.612642 140197563971328 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.8664557933807373, loss=1.9067611694335938
I0307 17:10:29.589880 140197572364032 logging_writer.py:48] [68200] global_step=68200, grad_norm=4.353416442871094, loss=1.8137623071670532
I0307 17:11:08.250741 140197563971328 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.9278199672698975, loss=1.762906789779663
I0307 17:11:47.464649 140197572364032 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.4370017051696777, loss=1.6578727960586548
I0307 17:11:54.668665 140352918893760 spec.py:321] Evaluating on the training split.
I0307 17:12:07.232397 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 17:12:56.277764 140352918893760 spec.py:349] Evaluating on the test split.
I0307 17:12:58.728731 140352918893760 submission_runner.py:469] Time since start: 28909.89s, 	Step: 68420, 	{'train/accuracy': 0.671894907951355, 'train/loss': 1.2970863580703735, 'validation/accuracy': 0.6215199828147888, 'validation/loss': 1.5568580627441406, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.2685935497283936, 'test/num_examples': 10000, 'score': 26575.14043903351, 'total_duration': 28909.892155647278, 'accumulated_submission_time': 26575.14043903351, 'accumulated_eval_time': 2321.029307603836, 'accumulated_logging_time': 6.747398614883423}
I0307 17:12:58.828592 140197563971328 logging_writer.py:48] [68420] accumulated_eval_time=2321.03, accumulated_logging_time=6.7474, accumulated_submission_time=26575.1, global_step=68420, preemption_count=0, score=26575.1, test/accuracy=0.4958, test/loss=2.26859, test/num_examples=10000, total_duration=28909.9, train/accuracy=0.671895, train/loss=1.29709, validation/accuracy=0.62152, validation/loss=1.55686, validation/num_examples=50000
I0307 17:13:30.385641 140197572364032 logging_writer.py:48] [68500] global_step=68500, grad_norm=4.215150833129883, loss=1.793897032737732
I0307 17:14:09.767427 140197563971328 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.6272900104522705, loss=1.859545111656189
I0307 17:14:50.167980 140197572364032 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.9250564575195312, loss=1.7545199394226074
I0307 17:15:29.955773 140197563971328 logging_writer.py:48] [68800] global_step=68800, grad_norm=4.129769802093506, loss=1.8321685791015625
I0307 17:16:09.403151 140197572364032 logging_writer.py:48] [68900] global_step=68900, grad_norm=4.296208381652832, loss=1.7861971855163574
I0307 17:16:48.510888 140197563971328 logging_writer.py:48] [69000] global_step=69000, grad_norm=4.199113368988037, loss=1.85768723487854
I0307 17:17:28.389008 140197572364032 logging_writer.py:48] [69100] global_step=69100, grad_norm=4.789078712463379, loss=1.8484103679656982
I0307 17:18:07.373966 140197563971328 logging_writer.py:48] [69200] global_step=69200, grad_norm=4.114993572235107, loss=1.8134595155715942
I0307 17:18:46.732508 140197572364032 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.8572287559509277, loss=1.869615077972412
I0307 17:19:25.974212 140197563971328 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.9268505573272705, loss=1.7845230102539062
I0307 17:20:05.553932 140197572364032 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.7709877490997314, loss=1.7943344116210938
I0307 17:20:44.546558 140197563971328 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.84359073638916, loss=1.7903279066085815
I0307 17:21:23.557482 140197572364032 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.8762409687042236, loss=1.8449134826660156
I0307 17:21:29.068341 140352918893760 spec.py:321] Evaluating on the training split.
I0307 17:21:40.823528 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 17:22:30.223193 140352918893760 spec.py:349] Evaluating on the test split.
I0307 17:22:31.913026 140352918893760 submission_runner.py:469] Time since start: 29483.08s, 	Step: 69715, 	{'train/accuracy': 0.6678292155265808, 'train/loss': 1.3177603483200073, 'validation/accuracy': 0.6180799603462219, 'validation/loss': 1.5713753700256348, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3242578506469727, 'test/num_examples': 10000, 'score': 27085.180109977722, 'total_duration': 29483.076415777206, 'accumulated_submission_time': 27085.180109977722, 'accumulated_eval_time': 2383.873781442642, 'accumulated_logging_time': 6.89722204208374}
I0307 17:22:32.032806 140197563971328 logging_writer.py:48] [69715] accumulated_eval_time=2383.87, accumulated_logging_time=6.89722, accumulated_submission_time=27085.2, global_step=69715, preemption_count=0, score=27085.2, test/accuracy=0.4901, test/loss=2.32426, test/num_examples=10000, total_duration=29483.1, train/accuracy=0.667829, train/loss=1.31776, validation/accuracy=0.61808, validation/loss=1.57138, validation/num_examples=50000
I0307 17:23:05.859496 140197572364032 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.7491841316223145, loss=1.837336778640747
I0307 17:23:45.757320 140197563971328 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.7942616939544678, loss=1.73574697971344
I0307 17:24:25.520522 140197572364032 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.612905740737915, loss=1.8245545625686646
I0307 17:25:05.533126 140197563971328 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.9398040771484375, loss=1.942686676979065
I0307 17:25:44.911428 140197572364032 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.4654977321624756, loss=1.9183837175369263
I0307 17:26:24.274747 140197563971328 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.8140151500701904, loss=1.7966935634613037
I0307 17:27:03.858039 140197572364032 logging_writer.py:48] [70400] global_step=70400, grad_norm=4.213277816772461, loss=1.7882061004638672
I0307 17:27:43.197294 140197563971328 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.6224873065948486, loss=1.710652232170105
I0307 17:28:22.434250 140197572364032 logging_writer.py:48] [70600] global_step=70600, grad_norm=4.044372081756592, loss=1.8107807636260986
I0307 17:29:01.681297 140197563971328 logging_writer.py:48] [70700] global_step=70700, grad_norm=4.141202449798584, loss=1.7667864561080933
I0307 17:29:41.013926 140197572364032 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.4915010929107666, loss=1.680818796157837
I0307 17:30:20.093258 140197563971328 logging_writer.py:48] [70900] global_step=70900, grad_norm=4.2811408042907715, loss=1.8889776468276978
I0307 17:30:59.592134 140197572364032 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.9973902702331543, loss=1.8901050090789795
I0307 17:31:02.031483 140352918893760 spec.py:321] Evaluating on the training split.
I0307 17:31:14.800584 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 17:31:59.237522 140352918893760 spec.py:349] Evaluating on the test split.
I0307 17:32:00.945156 140352918893760 submission_runner.py:469] Time since start: 30052.11s, 	Step: 71007, 	{'train/accuracy': 0.6826171875, 'train/loss': 1.2628735303878784, 'validation/accuracy': 0.63264000415802, 'validation/loss': 1.5083205699920654, 'validation/num_examples': 50000, 'test/accuracy': 0.5042000412940979, 'test/loss': 2.244354009628296, 'test/num_examples': 10000, 'score': 27594.98939538002, 'total_duration': 30052.108595609665, 'accumulated_submission_time': 27594.98939538002, 'accumulated_eval_time': 2442.787286758423, 'accumulated_logging_time': 7.0539679527282715}
I0307 17:32:01.074970 140197563971328 logging_writer.py:48] [71007] accumulated_eval_time=2442.79, accumulated_logging_time=7.05397, accumulated_submission_time=27595, global_step=71007, preemption_count=0, score=27595, test/accuracy=0.5042, test/loss=2.24435, test/num_examples=10000, total_duration=30052.1, train/accuracy=0.682617, train/loss=1.26287, validation/accuracy=0.63264, validation/loss=1.50832, validation/num_examples=50000
I0307 17:32:37.922548 140197572364032 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.7634027004241943, loss=1.8391867876052856
I0307 17:33:18.003638 140197563971328 logging_writer.py:48] [71200] global_step=71200, grad_norm=4.082867622375488, loss=1.797427773475647
I0307 17:33:57.654520 140197572364032 logging_writer.py:48] [71300] global_step=71300, grad_norm=4.2150044441223145, loss=1.8307732343673706
I0307 17:34:36.714511 140197563971328 logging_writer.py:48] [71400] global_step=71400, grad_norm=4.318863868713379, loss=1.904725432395935
I0307 17:35:16.276906 140197572364032 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.4290168285369873, loss=1.8440732955932617
I0307 17:35:55.608400 140197563971328 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.562608242034912, loss=1.7799599170684814
I0307 17:36:35.027897 140197572364032 logging_writer.py:48] [71700] global_step=71700, grad_norm=4.040592193603516, loss=1.8408631086349487
I0307 17:37:14.514654 140197563971328 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.6495368480682373, loss=1.7685688734054565
I0307 17:37:53.930197 140197572364032 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.660109519958496, loss=1.7584564685821533
I0307 17:38:33.095094 140197563971328 logging_writer.py:48] [72000] global_step=72000, grad_norm=4.382194995880127, loss=1.8158670663833618
I0307 17:39:12.070116 140197572364032 logging_writer.py:48] [72100] global_step=72100, grad_norm=4.164381504058838, loss=1.8404361009597778
I0307 17:39:51.594407 140197563971328 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.650012493133545, loss=1.7409157752990723
I0307 17:40:31.117401 140197572364032 logging_writer.py:48] [72300] global_step=72300, grad_norm=4.173266887664795, loss=1.6889166831970215
I0307 17:40:31.128931 140352918893760 spec.py:321] Evaluating on the training split.
I0307 17:40:43.461469 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 17:41:29.495491 140352918893760 spec.py:349] Evaluating on the test split.
I0307 17:41:31.225389 140352918893760 submission_runner.py:469] Time since start: 30622.39s, 	Step: 72301, 	{'train/accuracy': 0.6799266338348389, 'train/loss': 1.2744773626327515, 'validation/accuracy': 0.6286399960517883, 'validation/loss': 1.5363175868988037, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2561392784118652, 'test/num_examples': 10000, 'score': 28104.866969823837, 'total_duration': 30622.38880801201, 'accumulated_submission_time': 28104.866969823837, 'accumulated_eval_time': 2502.883547306061, 'accumulated_logging_time': 7.203964948654175}
I0307 17:41:31.306751 140197563971328 logging_writer.py:48] [72301] accumulated_eval_time=2502.88, accumulated_logging_time=7.20396, accumulated_submission_time=28104.9, global_step=72301, preemption_count=0, score=28104.9, test/accuracy=0.5049, test/loss=2.25614, test/num_examples=10000, total_duration=30622.4, train/accuracy=0.679927, train/loss=1.27448, validation/accuracy=0.62864, validation/loss=1.53632, validation/num_examples=50000
I0307 17:42:11.071913 140197572364032 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.8047046661376953, loss=1.7731133699417114
I0307 17:42:51.251393 140197563971328 logging_writer.py:48] [72500] global_step=72500, grad_norm=4.774210453033447, loss=1.7121517658233643
I0307 17:43:31.118086 140197572364032 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.805445671081543, loss=1.7378836870193481
I0307 17:44:10.453161 140197563971328 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.256483316421509, loss=1.7112058401107788
I0307 17:44:49.811855 140197572364032 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.727742910385132, loss=1.6791739463806152
I0307 17:45:29.261542 140197563971328 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.8535268306732178, loss=1.6592509746551514
I0307 17:46:09.361668 140197572364032 logging_writer.py:48] [73000] global_step=73000, grad_norm=4.506389617919922, loss=1.7634060382843018
I0307 17:46:49.181490 140197563971328 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.92307448387146, loss=1.7304948568344116
I0307 17:47:29.109038 140197572364032 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.861751079559326, loss=1.6342427730560303
I0307 17:48:08.493757 140197563971328 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.540432929992676, loss=1.8159635066986084
I0307 17:48:47.594687 140197572364032 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.654418706893921, loss=1.8344192504882812
I0307 17:49:27.049545 140197563971328 logging_writer.py:48] [73500] global_step=73500, grad_norm=4.6142258644104, loss=1.8787014484405518
I0307 17:50:01.465716 140352918893760 spec.py:321] Evaluating on the training split.
I0307 17:50:13.948343 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 17:51:03.146399 140352918893760 spec.py:349] Evaluating on the test split.
I0307 17:51:04.837930 140352918893760 submission_runner.py:469] Time since start: 31196.00s, 	Step: 73588, 	{'train/accuracy': 0.6809430718421936, 'train/loss': 1.259148120880127, 'validation/accuracy': 0.627519965171814, 'validation/loss': 1.5180516242980957, 'validation/num_examples': 50000, 'test/accuracy': 0.5052000284194946, 'test/loss': 2.226288080215454, 'test/num_examples': 10000, 'score': 28614.846009731293, 'total_duration': 31196.001319169998, 'accumulated_submission_time': 28614.846009731293, 'accumulated_eval_time': 2566.2555482387543, 'accumulated_logging_time': 7.3076252937316895}
I0307 17:51:04.968507 140197572364032 logging_writer.py:48] [73588] accumulated_eval_time=2566.26, accumulated_logging_time=7.30763, accumulated_submission_time=28614.8, global_step=73588, preemption_count=0, score=28614.8, test/accuracy=0.5052, test/loss=2.22629, test/num_examples=10000, total_duration=31196, train/accuracy=0.680943, train/loss=1.25915, validation/accuracy=0.62752, validation/loss=1.51805, validation/num_examples=50000
I0307 17:51:10.122498 140197563971328 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.8394110202789307, loss=1.7415857315063477
I0307 17:51:49.174247 140197572364032 logging_writer.py:48] [73700] global_step=73700, grad_norm=4.193845272064209, loss=1.754181146621704
2025-03-07 17:52:25.327219: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:52:29.097299 140197563971328 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.994189500808716, loss=1.7097649574279785
I0307 17:53:08.840492 140197572364032 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.6457762718200684, loss=1.840130090713501
I0307 17:53:48.240003 140197563971328 logging_writer.py:48] [74000] global_step=74000, grad_norm=4.130156517028809, loss=1.8072872161865234
I0307 17:54:27.412500 140197572364032 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.920959234237671, loss=1.8127379417419434
I0307 17:55:07.201930 140197563971328 logging_writer.py:48] [74200] global_step=74200, grad_norm=4.0293660163879395, loss=1.7971844673156738
I0307 17:55:46.652457 140197572364032 logging_writer.py:48] [74300] global_step=74300, grad_norm=4.090718746185303, loss=1.7513372898101807
I0307 17:56:26.177884 140197563971328 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.9588236808776855, loss=1.7088316679000854
I0307 17:57:05.186741 140197572364032 logging_writer.py:48] [74500] global_step=74500, grad_norm=4.495129585266113, loss=1.8136287927627563
I0307 17:57:44.551594 140197563971328 logging_writer.py:48] [74600] global_step=74600, grad_norm=4.093227386474609, loss=1.753202199935913
I0307 17:58:23.881138 140197572364032 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.8457531929016113, loss=1.7461479902267456
I0307 17:59:03.776103 140197563971328 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.921215295791626, loss=1.9004765748977661
I0307 17:59:35.155755 140352918893760 spec.py:321] Evaluating on the training split.
I0307 17:59:47.650233 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 18:00:31.282284 140352918893760 spec.py:349] Evaluating on the test split.
I0307 18:00:33.852004 140352918893760 submission_runner.py:469] Time since start: 31765.02s, 	Step: 74880, 	{'train/accuracy': 0.666035532951355, 'train/loss': 1.3196016550064087, 'validation/accuracy': 0.6209999918937683, 'validation/loss': 1.5700033903121948, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.256957530975342, 'test/num_examples': 10000, 'score': 29124.857882499695, 'total_duration': 31765.015270233154, 'accumulated_submission_time': 29124.857882499695, 'accumulated_eval_time': 2624.9514589309692, 'accumulated_logging_time': 7.458817005157471}
I0307 18:00:33.988622 140197572364032 logging_writer.py:48] [74880] accumulated_eval_time=2624.95, accumulated_logging_time=7.45882, accumulated_submission_time=29124.9, global_step=74880, preemption_count=0, score=29124.9, test/accuracy=0.505, test/loss=2.25696, test/num_examples=10000, total_duration=31765, train/accuracy=0.666036, train/loss=1.3196, validation/accuracy=0.621, validation/loss=1.57, validation/num_examples=50000
I0307 18:00:42.385616 140197563971328 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.8940744400024414, loss=1.7620221376419067
I0307 18:01:22.147549 140197572364032 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.6878457069396973, loss=1.7641172409057617
2025-03-07 18:01:38.558656: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:02:01.873085 140197563971328 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.5671045780181885, loss=1.6594663858413696
I0307 18:02:41.368888 140197572364032 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.6362507343292236, loss=1.738365650177002
I0307 18:03:20.207524 140197563971328 logging_writer.py:48] [75300] global_step=75300, grad_norm=4.208885669708252, loss=1.9066540002822876
I0307 18:04:00.022127 140197572364032 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.968261480331421, loss=1.6480987071990967
I0307 18:04:38.752470 140197563971328 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.278726100921631, loss=1.7754074335098267
I0307 18:05:17.965296 140197572364032 logging_writer.py:48] [75600] global_step=75600, grad_norm=4.876770496368408, loss=1.7289530038833618
I0307 18:05:56.719944 140197563971328 logging_writer.py:48] [75700] global_step=75700, grad_norm=4.046629428863525, loss=1.6900253295898438
I0307 18:06:35.966915 140197572364032 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.7353785037994385, loss=1.789018988609314
I0307 18:07:15.326053 140197563971328 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.9055802822113037, loss=1.7954639196395874
I0307 18:07:54.630991 140197572364032 logging_writer.py:48] [76000] global_step=76000, grad_norm=5.262313365936279, loss=1.8954284191131592
I0307 18:08:33.900750 140197563971328 logging_writer.py:48] [76100] global_step=76100, grad_norm=4.2891526222229, loss=1.7619373798370361
I0307 18:09:04.143676 140352918893760 spec.py:321] Evaluating on the training split.
I0307 18:09:16.486823 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 18:09:54.335028 140352918893760 spec.py:349] Evaluating on the test split.
I0307 18:09:56.056642 140352918893760 submission_runner.py:469] Time since start: 32327.22s, 	Step: 76177, 	{'train/accuracy': 0.6743662357330322, 'train/loss': 1.2831982374191284, 'validation/accuracy': 0.6276800036430359, 'validation/loss': 1.5420352220535278, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.305387258529663, 'test/num_examples': 10000, 'score': 29634.795800209045, 'total_duration': 32327.220051765442, 'accumulated_submission_time': 29634.795800209045, 'accumulated_eval_time': 2676.864230155945, 'accumulated_logging_time': 7.655877113342285}
I0307 18:09:56.163189 140197572364032 logging_writer.py:48] [76177] accumulated_eval_time=2676.86, accumulated_logging_time=7.65588, accumulated_submission_time=29634.8, global_step=76177, preemption_count=0, score=29634.8, test/accuracy=0.4995, test/loss=2.30539, test/num_examples=10000, total_duration=32327.2, train/accuracy=0.674366, train/loss=1.2832, validation/accuracy=0.62768, validation/loss=1.54204, validation/num_examples=50000
I0307 18:10:05.665828 140197563971328 logging_writer.py:48] [76200] global_step=76200, grad_norm=4.183656215667725, loss=1.7303130626678467
2025-03-07 18:10:42.127284: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:10:45.076694 140197572364032 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.731013298034668, loss=1.7824382781982422
I0307 18:11:24.325941 140197563971328 logging_writer.py:48] [76400] global_step=76400, grad_norm=5.122213840484619, loss=1.7674503326416016
I0307 18:12:03.015865 140197572364032 logging_writer.py:48] [76500] global_step=76500, grad_norm=4.103092670440674, loss=1.675358772277832
I0307 18:12:41.880282 140197563971328 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.8734145164489746, loss=1.8064684867858887
I0307 18:13:20.526237 140197572364032 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.8978612422943115, loss=1.7563607692718506
I0307 18:13:59.825504 140197563971328 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.733595848083496, loss=1.9004281759262085
I0307 18:14:39.015976 140197572364032 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.8018524646759033, loss=1.8824779987335205
I0307 18:15:18.289266 140197563971328 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.851027488708496, loss=1.7567403316497803
I0307 18:15:57.695319 140197572364032 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.690274715423584, loss=1.783292531967163
I0307 18:16:36.447797 140197563971328 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.868746280670166, loss=1.706572413444519
I0307 18:17:15.746036 140197572364032 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.7474772930145264, loss=1.6689791679382324
I0307 18:17:55.394654 140197563971328 logging_writer.py:48] [77400] global_step=77400, grad_norm=4.615525722503662, loss=1.8404614925384521
I0307 18:18:26.343176 140352918893760 spec.py:321] Evaluating on the training split.
I0307 18:18:38.725317 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 18:19:18.525641 140352918893760 spec.py:349] Evaluating on the test split.
I0307 18:19:21.008896 140352918893760 submission_runner.py:469] Time since start: 32892.17s, 	Step: 77479, 	{'train/accuracy': 0.6734494566917419, 'train/loss': 1.2976173162460327, 'validation/accuracy': 0.6261799931526184, 'validation/loss': 1.5456987619400024, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.305140256881714, 'test/num_examples': 10000, 'score': 30144.764360666275, 'total_duration': 32892.17233514786, 'accumulated_submission_time': 30144.764360666275, 'accumulated_eval_time': 2731.5297803878784, 'accumulated_logging_time': 7.819450855255127}
I0307 18:19:21.114492 140197572364032 logging_writer.py:48] [77479] accumulated_eval_time=2731.53, accumulated_logging_time=7.81945, accumulated_submission_time=30144.8, global_step=77479, preemption_count=0, score=30144.8, test/accuracy=0.5001, test/loss=2.30514, test/num_examples=10000, total_duration=32892.2, train/accuracy=0.673449, train/loss=1.29762, validation/accuracy=0.62618, validation/loss=1.5457, validation/num_examples=50000
I0307 18:19:29.699888 140197563971328 logging_writer.py:48] [77500] global_step=77500, grad_norm=5.54573917388916, loss=1.7324788570404053
2025-03-07 18:19:46.352733: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:20:08.453561 140197572364032 logging_writer.py:48] [77600] global_step=77600, grad_norm=4.830408573150635, loss=1.7578201293945312
I0307 18:20:47.644877 140197563971328 logging_writer.py:48] [77700] global_step=77700, grad_norm=4.080976486206055, loss=1.8647710084915161
I0307 18:21:26.529932 140197572364032 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.5223388671875, loss=1.7247488498687744
I0307 18:22:04.835254 140197563971328 logging_writer.py:48] [77900] global_step=77900, grad_norm=4.022802352905273, loss=1.7104356288909912
I0307 18:22:43.653006 140197572364032 logging_writer.py:48] [78000] global_step=78000, grad_norm=4.473493576049805, loss=1.8013169765472412
I0307 18:23:22.779904 140197563971328 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.9763824939727783, loss=1.7380520105361938
I0307 18:24:01.368693 140197572364032 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.798572301864624, loss=1.8243762254714966
I0307 18:24:39.814246 140197563971328 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.8607840538024902, loss=1.811766266822815
I0307 18:25:19.058988 140197572364032 logging_writer.py:48] [78400] global_step=78400, grad_norm=4.1496357917785645, loss=1.642916202545166
I0307 18:25:57.611397 140197563971328 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.9494526386260986, loss=1.7315794229507446
I0307 18:26:36.681015 140197572364032 logging_writer.py:48] [78600] global_step=78600, grad_norm=4.642373085021973, loss=1.756173849105835
I0307 18:27:16.390073 140197563971328 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.6043052673339844, loss=1.7110157012939453
I0307 18:27:51.100898 140352918893760 spec.py:321] Evaluating on the training split.
I0307 18:28:03.335118 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 18:28:47.677690 140352918893760 spec.py:349] Evaluating on the test split.
I0307 18:28:49.425748 140352918893760 submission_runner.py:469] Time since start: 33460.59s, 	Step: 78789, 	{'train/accuracy': 0.6911272406578064, 'train/loss': 1.2141087055206299, 'validation/accuracy': 0.6365799903869629, 'validation/loss': 1.4830577373504639, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.1785385608673096, 'test/num_examples': 10000, 'score': 30654.563216924667, 'total_duration': 33460.589171886444, 'accumulated_submission_time': 30654.563216924667, 'accumulated_eval_time': 2789.8544533252716, 'accumulated_logging_time': 7.959044694900513}
I0307 18:28:49.526405 140197572364032 logging_writer.py:48] [78789] accumulated_eval_time=2789.85, accumulated_logging_time=7.95904, accumulated_submission_time=30654.6, global_step=78789, preemption_count=0, score=30654.6, test/accuracy=0.5158, test/loss=2.17854, test/num_examples=10000, total_duration=33460.6, train/accuracy=0.691127, train/loss=1.21411, validation/accuracy=0.63658, validation/loss=1.48306, validation/num_examples=50000
2025-03-07 18:28:52.230892: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:28:54.544928 140197563971328 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.878077983856201, loss=1.7356247901916504
I0307 18:29:33.940692 140197572364032 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.916860580444336, loss=1.7320066690444946
I0307 18:30:13.049081 140197563971328 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.804807424545288, loss=1.8300230503082275
I0307 18:30:52.078105 140197572364032 logging_writer.py:48] [79100] global_step=79100, grad_norm=4.381404876708984, loss=1.8136391639709473
I0307 18:31:31.162919 140197563971328 logging_writer.py:48] [79200] global_step=79200, grad_norm=4.141546249389648, loss=1.7826995849609375
I0307 18:32:09.656939 140197572364032 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.5774731636047363, loss=1.6652686595916748
I0307 18:32:49.256878 140197563971328 logging_writer.py:48] [79400] global_step=79400, grad_norm=4.164102554321289, loss=1.8123114109039307
I0307 18:33:28.224939 140197572364032 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.7487192153930664, loss=1.8463819026947021
I0307 18:34:07.149127 140197563971328 logging_writer.py:48] [79600] global_step=79600, grad_norm=4.238691806793213, loss=1.8148646354675293
I0307 18:34:45.888341 140197572364032 logging_writer.py:48] [79700] global_step=79700, grad_norm=4.6625189781188965, loss=1.7566165924072266
I0307 18:35:25.285588 140197563971328 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.670386791229248, loss=1.8801825046539307
I0307 18:36:05.296241 140197572364032 logging_writer.py:48] [79900] global_step=79900, grad_norm=4.330811977386475, loss=1.7142304182052612
I0307 18:36:45.604046 140197563971328 logging_writer.py:48] [80000] global_step=80000, grad_norm=4.090562343597412, loss=1.9579188823699951
2025-03-07 18:37:03.245081: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:37:19.596137 140352918893760 spec.py:321] Evaluating on the training split.
I0307 18:37:32.123140 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 18:38:19.322434 140352918893760 spec.py:349] Evaluating on the test split.
I0307 18:38:21.027085 140352918893760 submission_runner.py:469] Time since start: 34032.19s, 	Step: 80087, 	{'train/accuracy': 0.6937180757522583, 'train/loss': 1.2197816371917725, 'validation/accuracy': 0.6360999941825867, 'validation/loss': 1.4961830377578735, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.233011245727539, 'test/num_examples': 10000, 'score': 31164.439919233322, 'total_duration': 34032.19050502777, 'accumulated_submission_time': 31164.439919233322, 'accumulated_eval_time': 2851.285222053528, 'accumulated_logging_time': 8.09725046157837}
I0307 18:38:21.167413 140197572364032 logging_writer.py:48] [80087] accumulated_eval_time=2851.29, accumulated_logging_time=8.09725, accumulated_submission_time=31164.4, global_step=80087, preemption_count=0, score=31164.4, test/accuracy=0.5051, test/loss=2.23301, test/num_examples=10000, total_duration=34032.2, train/accuracy=0.693718, train/loss=1.21978, validation/accuracy=0.6361, validation/loss=1.49618, validation/num_examples=50000
I0307 18:38:26.676721 140197563971328 logging_writer.py:48] [80100] global_step=80100, grad_norm=5.343461513519287, loss=1.7757837772369385
I0307 18:39:05.918439 140197572364032 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.9450721740722656, loss=1.8199232816696167
I0307 18:39:45.109690 140197563971328 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.322685718536377, loss=1.7904942035675049
I0307 18:40:23.990643 140197572364032 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.946357011795044, loss=1.7120076417922974
I0307 18:41:02.994873 140197563971328 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.228260517120361, loss=1.7484592199325562
I0307 18:41:41.712917 140197572364032 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.451815605163574, loss=1.7146623134613037
I0307 18:42:20.398814 140197563971328 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.917201519012451, loss=1.807344913482666
I0307 18:42:59.236900 140197572364032 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.885209560394287, loss=1.6914739608764648
I0307 18:43:37.724564 140197563971328 logging_writer.py:48] [80900] global_step=80900, grad_norm=5.3532280921936035, loss=1.833702564239502
I0307 18:44:16.604174 140197572364032 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.951997756958008, loss=1.621572732925415
I0307 18:44:55.751047 140197563971328 logging_writer.py:48] [81100] global_step=81100, grad_norm=4.2628960609436035, loss=1.6504648923873901
I0307 18:45:35.879840 140197572364032 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.033447265625, loss=1.8481488227844238
2025-03-07 18:46:14.203491: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:46:15.255191 140197563971328 logging_writer.py:48] [81300] global_step=81300, grad_norm=4.124022960662842, loss=1.7358019351959229
I0307 18:46:51.305894 140352918893760 spec.py:321] Evaluating on the training split.
I0307 18:47:03.705243 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 18:47:45.705065 140352918893760 spec.py:349] Evaluating on the test split.
I0307 18:47:47.441229 140352918893760 submission_runner.py:469] Time since start: 34598.60s, 	Step: 81393, 	{'train/accuracy': 0.6854671239852905, 'train/loss': 1.2439196109771729, 'validation/accuracy': 0.6332399845123291, 'validation/loss': 1.5081596374511719, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2297580242156982, 'test/num_examples': 10000, 'score': 31674.395730495453, 'total_duration': 34598.60466694832, 'accumulated_submission_time': 31674.395730495453, 'accumulated_eval_time': 2907.4203991889954, 'accumulated_logging_time': 8.265035629272461}
I0307 18:47:47.586102 140197572364032 logging_writer.py:48] [81393] accumulated_eval_time=2907.42, accumulated_logging_time=8.26504, accumulated_submission_time=31674.4, global_step=81393, preemption_count=0, score=31674.4, test/accuracy=0.5059, test/loss=2.22976, test/num_examples=10000, total_duration=34598.6, train/accuracy=0.685467, train/loss=1.24392, validation/accuracy=0.63324, validation/loss=1.50816, validation/num_examples=50000
I0307 18:47:50.789638 140197563971328 logging_writer.py:48] [81400] global_step=81400, grad_norm=4.386787414550781, loss=1.7174348831176758
I0307 18:48:29.249131 140197572364032 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.779109001159668, loss=1.7030507326126099
I0307 18:49:08.157231 140197563971328 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.819173574447632, loss=1.805963397026062
I0307 18:49:46.986842 140197572364032 logging_writer.py:48] [81700] global_step=81700, grad_norm=4.608156681060791, loss=1.7662181854248047
I0307 18:50:26.043754 140197563971328 logging_writer.py:48] [81800] global_step=81800, grad_norm=5.757883548736572, loss=1.8150629997253418
I0307 18:51:05.129152 140197572364032 logging_writer.py:48] [81900] global_step=81900, grad_norm=4.431591033935547, loss=1.7430418729782104
I0307 18:51:44.366531 140197563971328 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.8580210208892822, loss=1.772391676902771
I0307 18:52:23.750407 140197572364032 logging_writer.py:48] [82100] global_step=82100, grad_norm=4.165401458740234, loss=1.660486102104187
I0307 18:53:02.275996 140197563971328 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.484786033630371, loss=1.7774256467819214
I0307 18:53:41.220125 140197572364032 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.9934310913085938, loss=1.6802070140838623
I0307 18:54:21.179971 140197563971328 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.8595035076141357, loss=1.8180651664733887
I0307 18:55:01.008045 140197572364032 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.915003776550293, loss=1.684522271156311
2025-03-07 18:55:20.064027: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:55:40.269191 140197563971328 logging_writer.py:48] [82600] global_step=82600, grad_norm=4.003538608551025, loss=1.7252053022384644
I0307 18:56:17.509996 140352918893760 spec.py:321] Evaluating on the training split.
I0307 18:56:31.300174 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 18:57:18.760741 140352918893760 spec.py:349] Evaluating on the test split.
I0307 18:57:20.462308 140352918893760 submission_runner.py:469] Time since start: 35171.63s, 	Step: 82696, 	{'train/accuracy': 0.6929408311843872, 'train/loss': 1.2060176134109497, 'validation/accuracy': 0.6411799788475037, 'validation/loss': 1.4821573495864868, 'validation/num_examples': 50000, 'test/accuracy': 0.5143000483512878, 'test/loss': 2.2411344051361084, 'test/num_examples': 10000, 'score': 32184.08911204338, 'total_duration': 35171.62567663193, 'accumulated_submission_time': 32184.08911204338, 'accumulated_eval_time': 2970.3724806308746, 'accumulated_logging_time': 8.484735488891602}
I0307 18:57:20.584057 140197572364032 logging_writer.py:48] [82696] accumulated_eval_time=2970.37, accumulated_logging_time=8.48474, accumulated_submission_time=32184.1, global_step=82696, preemption_count=0, score=32184.1, test/accuracy=0.5143, test/loss=2.24113, test/num_examples=10000, total_duration=35171.6, train/accuracy=0.692941, train/loss=1.20602, validation/accuracy=0.64118, validation/loss=1.48216, validation/num_examples=50000
I0307 18:57:22.585499 140197563971328 logging_writer.py:48] [82700] global_step=82700, grad_norm=4.555375576019287, loss=1.853151559829712
I0307 18:58:01.478611 140197572364032 logging_writer.py:48] [82800] global_step=82800, grad_norm=4.489297389984131, loss=1.839299201965332
I0307 18:58:40.851615 140197563971328 logging_writer.py:48] [82900] global_step=82900, grad_norm=4.019767761230469, loss=1.7260701656341553
I0307 18:59:20.251216 140197572364032 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.033457279205322, loss=1.6982598304748535
I0307 18:59:58.937598 140197563971328 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.9472382068634033, loss=1.801370620727539
I0307 19:00:38.254615 140197572364032 logging_writer.py:48] [83200] global_step=83200, grad_norm=4.307894229888916, loss=1.7637196779251099
I0307 19:01:17.446764 140197563971328 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.649348497390747, loss=1.7722477912902832
I0307 19:01:56.471239 140197572364032 logging_writer.py:48] [83400] global_step=83400, grad_norm=4.225775241851807, loss=1.6308850049972534
I0307 19:02:35.246453 140197563971328 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.9660913944244385, loss=1.6725963354110718
I0307 19:03:14.731926 140197572364032 logging_writer.py:48] [83600] global_step=83600, grad_norm=4.1102705001831055, loss=1.6492770910263062
I0307 19:03:54.609408 140197563971328 logging_writer.py:48] [83700] global_step=83700, grad_norm=4.23773193359375, loss=1.6885027885437012
2025-03-07 19:04:33.651403: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:04:33.933766 140197572364032 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.429071426391602, loss=1.892340898513794
I0307 19:05:13.189232 140197563971328 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.335093975067139, loss=1.8204320669174194
I0307 19:05:50.659480 140352918893760 spec.py:321] Evaluating on the training split.
I0307 19:06:02.528040 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 19:06:45.158685 140352918893760 spec.py:349] Evaluating on the test split.
I0307 19:06:47.456603 140352918893760 submission_runner.py:469] Time since start: 35738.62s, 	Step: 83997, 	{'train/accuracy': 0.6819395422935486, 'train/loss': 1.2571889162063599, 'validation/accuracy': 0.6292999982833862, 'validation/loss': 1.531537652015686, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.2787511348724365, 'test/num_examples': 10000, 'score': 32693.98401594162, 'total_duration': 35738.61975502968, 'accumulated_submission_time': 32693.98401594162, 'accumulated_eval_time': 3027.1691522598267, 'accumulated_logging_time': 8.632344722747803}
I0307 19:06:47.570223 140197572364032 logging_writer.py:48] [83997] accumulated_eval_time=3027.17, accumulated_logging_time=8.63234, accumulated_submission_time=32694, global_step=83997, preemption_count=0, score=32694, test/accuracy=0.499, test/loss=2.27875, test/num_examples=10000, total_duration=35738.6, train/accuracy=0.68194, train/loss=1.25719, validation/accuracy=0.6293, validation/loss=1.53154, validation/num_examples=50000
I0307 19:06:49.141978 140197563971328 logging_writer.py:48] [84000] global_step=84000, grad_norm=4.24022912979126, loss=1.6722019910812378
I0307 19:07:28.233143 140197572364032 logging_writer.py:48] [84100] global_step=84100, grad_norm=4.97655725479126, loss=1.83902108669281
I0307 19:08:07.123697 140197563971328 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.846839427947998, loss=1.7748050689697266
I0307 19:08:46.197542 140197572364032 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.7966761589050293, loss=1.6085529327392578
I0307 19:09:25.029852 140197563971328 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.863452434539795, loss=1.7020002603530884
I0307 19:10:03.877682 140197572364032 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.8297808170318604, loss=1.6493865251541138
I0307 19:10:43.156249 140197563971328 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.310329437255859, loss=1.7478358745574951
I0307 19:11:22.514579 140197572364032 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.955461025238037, loss=1.8387641906738281
I0307 19:12:01.556748 140197563971328 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.9510273933410645, loss=1.761479377746582
I0307 19:12:40.359890 140197572364032 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.470067024230957, loss=1.6276090145111084
I0307 19:13:19.377631 140197563971328 logging_writer.py:48] [85000] global_step=85000, grad_norm=4.133724212646484, loss=1.77333402633667
2025-03-07 19:13:39.054962: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:13:58.382156 140197572364032 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.993438720703125, loss=1.781148076057434
I0307 19:14:37.034187 140197563971328 logging_writer.py:48] [85200] global_step=85200, grad_norm=4.032042026519775, loss=1.8059782981872559
I0307 19:15:15.290421 140197572364032 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.8696913719177246, loss=1.7174632549285889
I0307 19:15:17.698850 140352918893760 spec.py:321] Evaluating on the training split.
I0307 19:15:30.475536 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 19:16:16.149591 140352918893760 spec.py:349] Evaluating on the test split.
I0307 19:16:17.897584 140352918893760 submission_runner.py:469] Time since start: 36309.06s, 	Step: 85307, 	{'train/accuracy': 0.69242262840271, 'train/loss': 1.201663613319397, 'validation/accuracy': 0.6385799646377563, 'validation/loss': 1.482727289199829, 'validation/num_examples': 50000, 'test/accuracy': 0.5124000310897827, 'test/loss': 2.189887285232544, 'test/num_examples': 10000, 'score': 33203.931676626205, 'total_duration': 36309.06070280075, 'accumulated_submission_time': 33203.931676626205, 'accumulated_eval_time': 3087.367395401001, 'accumulated_logging_time': 8.771151542663574}
I0307 19:16:18.140177 140197563971328 logging_writer.py:48] [85307] accumulated_eval_time=3087.37, accumulated_logging_time=8.77115, accumulated_submission_time=33203.9, global_step=85307, preemption_count=0, score=33203.9, test/accuracy=0.5124, test/loss=2.18989, test/num_examples=10000, total_duration=36309.1, train/accuracy=0.692423, train/loss=1.20166, validation/accuracy=0.63858, validation/loss=1.48273, validation/num_examples=50000
I0307 19:16:55.114700 140197572364032 logging_writer.py:48] [85400] global_step=85400, grad_norm=4.2797417640686035, loss=1.7972729206085205
I0307 19:17:34.682007 140197563971328 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.4581804275512695, loss=1.727890133857727
I0307 19:18:14.194360 140197572364032 logging_writer.py:48] [85600] global_step=85600, grad_norm=4.21133279800415, loss=1.6499099731445312
I0307 19:18:53.269470 140197563971328 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.145851135253906, loss=1.6934500932693481
I0307 19:19:32.512087 140197572364032 logging_writer.py:48] [85800] global_step=85800, grad_norm=5.289883613586426, loss=1.6711413860321045
I0307 19:20:12.086308 140197563971328 logging_writer.py:48] [85900] global_step=85900, grad_norm=4.641598224639893, loss=1.6869707107543945
I0307 19:20:51.737895 140197572364032 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.7897841930389404, loss=1.690659523010254
I0307 19:21:31.379522 140197563971328 logging_writer.py:48] [86100] global_step=86100, grad_norm=4.115823268890381, loss=1.7908995151519775
I0307 19:22:10.722949 140197572364032 logging_writer.py:48] [86200] global_step=86200, grad_norm=4.0453667640686035, loss=1.8064991235733032
I0307 19:22:49.670945 140197563971328 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.501380443572998, loss=1.6759248971939087
2025-03-07 19:22:51.794677: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:23:28.878169 140197572364032 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.972283124923706, loss=1.766984462738037
I0307 19:24:07.700602 140197563971328 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.359378814697266, loss=1.7624614238739014
I0307 19:24:46.935494 140197572364032 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.8645853996276855, loss=1.7433730363845825
I0307 19:24:48.191327 140352918893760 spec.py:321] Evaluating on the training split.
I0307 19:25:01.497632 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 19:25:42.692201 140352918893760 spec.py:349] Evaluating on the test split.
I0307 19:25:44.419843 140352918893760 submission_runner.py:469] Time since start: 36875.58s, 	Step: 86604, 	{'train/accuracy': 0.6988002061843872, 'train/loss': 1.1646647453308105, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.4550102949142456, 'validation/num_examples': 50000, 'test/accuracy': 0.516800045967102, 'test/loss': 2.1701242923736572, 'test/num_examples': 10000, 'score': 33713.75606608391, 'total_duration': 36875.58327436447, 'accumulated_submission_time': 33713.75606608391, 'accumulated_eval_time': 3143.595734357834, 'accumulated_logging_time': 9.082584381103516}
I0307 19:25:44.509874 140197563971328 logging_writer.py:48] [86604] accumulated_eval_time=3143.6, accumulated_logging_time=9.08258, accumulated_submission_time=33713.8, global_step=86604, preemption_count=0, score=33713.8, test/accuracy=0.5168, test/loss=2.17012, test/num_examples=10000, total_duration=36875.6, train/accuracy=0.6988, train/loss=1.16466, validation/accuracy=0.64166, validation/loss=1.45501, validation/num_examples=50000
I0307 19:26:22.297567 140197572364032 logging_writer.py:48] [86700] global_step=86700, grad_norm=5.072408676147461, loss=1.741449236869812
I0307 19:27:01.797068 140197563971328 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.907573699951172, loss=1.7046377658843994
I0307 19:27:41.152606 140197572364032 logging_writer.py:48] [86900] global_step=86900, grad_norm=4.002504348754883, loss=1.6388062238693237
I0307 19:28:20.417797 140197563971328 logging_writer.py:48] [87000] global_step=87000, grad_norm=4.047293663024902, loss=1.7376998662948608
I0307 19:28:59.550393 140197572364032 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.8279507160186768, loss=1.7663856744766235
I0307 19:29:39.017339 140197563971328 logging_writer.py:48] [87200] global_step=87200, grad_norm=4.247657775878906, loss=1.7114460468292236
I0307 19:30:18.260193 140197572364032 logging_writer.py:48] [87300] global_step=87300, grad_norm=4.1358323097229, loss=1.7346575260162354
I0307 19:30:57.882972 140197563971328 logging_writer.py:48] [87400] global_step=87400, grad_norm=4.944819450378418, loss=1.7814455032348633
I0307 19:31:37.306254 140197572364032 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.9668667316436768, loss=1.727160096168518
2025-03-07 19:31:59.580279: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:32:16.821327 140197563971328 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.8949785232543945, loss=1.7682623863220215
I0307 19:32:56.498394 140197572364032 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.084968566894531, loss=1.6935187578201294
I0307 19:33:36.290219 140197563971328 logging_writer.py:48] [87800] global_step=87800, grad_norm=4.187071323394775, loss=1.7106322050094604
I0307 19:34:14.428314 140352918893760 spec.py:321] Evaluating on the training split.
I0307 19:34:26.749444 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 19:35:11.189082 140352918893760 spec.py:349] Evaluating on the test split.
I0307 19:35:12.929464 140352918893760 submission_runner.py:469] Time since start: 37444.09s, 	Step: 87898, 	{'train/accuracy': 0.6991190910339355, 'train/loss': 1.1791150569915771, 'validation/accuracy': 0.6412000060081482, 'validation/loss': 1.477604627609253, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.1600022315979004, 'test/num_examples': 10000, 'score': 34223.408479213715, 'total_duration': 37444.092901706696, 'accumulated_submission_time': 34223.408479213715, 'accumulated_eval_time': 3202.0967190265656, 'accumulated_logging_time': 9.284285306930542}
I0307 19:35:13.004445 140197572364032 logging_writer.py:48] [87898] accumulated_eval_time=3202.1, accumulated_logging_time=9.28429, accumulated_submission_time=34223.4, global_step=87898, preemption_count=0, score=34223.4, test/accuracy=0.5172, test/loss=2.16, test/num_examples=10000, total_duration=37444.1, train/accuracy=0.699119, train/loss=1.17912, validation/accuracy=0.6412, validation/loss=1.4776, validation/num_examples=50000
I0307 19:35:14.218156 140197563971328 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.531747817993164, loss=1.698606014251709
I0307 19:35:53.676739 140197572364032 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.19152307510376, loss=1.6714543104171753
I0307 19:36:32.944107 140197563971328 logging_writer.py:48] [88100] global_step=88100, grad_norm=4.510367393493652, loss=1.7022321224212646
I0307 19:37:12.188891 140197572364032 logging_writer.py:48] [88200] global_step=88200, grad_norm=4.252439022064209, loss=1.6048996448516846
I0307 19:37:51.650755 140197563971328 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.8989596366882324, loss=1.7822399139404297
I0307 19:38:31.124747 140197572364032 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.114748954772949, loss=1.7260996103286743
I0307 19:39:10.674649 140197563971328 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.155559539794922, loss=1.7520915269851685
I0307 19:39:50.168115 140197572364032 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.536046028137207, loss=1.670919418334961
I0307 19:40:29.844285 140197563971328 logging_writer.py:48] [88700] global_step=88700, grad_norm=4.406853675842285, loss=1.7067475318908691
I0307 19:41:09.419805 140197572364032 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.759406328201294, loss=1.7655326128005981
2025-03-07 19:41:10.991323: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:41:48.843537 140197563971328 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.909050464630127, loss=1.738936185836792
I0307 19:42:27.727773 140197572364032 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.8174080848693848, loss=1.716110110282898
I0307 19:43:06.905228 140197563971328 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.592889308929443, loss=1.6205036640167236
I0307 19:43:42.933916 140352918893760 spec.py:321] Evaluating on the training split.
I0307 19:43:54.972225 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 19:44:44.440814 140352918893760 spec.py:349] Evaluating on the test split.
I0307 19:44:46.183323 140352918893760 submission_runner.py:469] Time since start: 38017.35s, 	Step: 89192, 	{'train/accuracy': 0.697265625, 'train/loss': 1.1688207387924194, 'validation/accuracy': 0.6422799825668335, 'validation/loss': 1.4667043685913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.195091724395752, 'test/num_examples': 10000, 'score': 34733.114661216736, 'total_duration': 38017.346732616425, 'accumulated_submission_time': 34733.114661216736, 'accumulated_eval_time': 3265.3459293842316, 'accumulated_logging_time': 9.423890590667725}
I0307 19:44:46.375768 140197572364032 logging_writer.py:48] [89192] accumulated_eval_time=3265.35, accumulated_logging_time=9.42389, accumulated_submission_time=34733.1, global_step=89192, preemption_count=0, score=34733.1, test/accuracy=0.5166, test/loss=2.19509, test/num_examples=10000, total_duration=38017.3, train/accuracy=0.697266, train/loss=1.16882, validation/accuracy=0.64228, validation/loss=1.4667, validation/num_examples=50000
I0307 19:44:50.109244 140197563971328 logging_writer.py:48] [89200] global_step=89200, grad_norm=4.124807834625244, loss=1.7255302667617798
I0307 19:45:29.006644 140197572364032 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.058370590209961, loss=1.7235486507415771
I0307 19:46:08.111091 140197563971328 logging_writer.py:48] [89400] global_step=89400, grad_norm=4.527365207672119, loss=1.612335443496704
I0307 19:46:47.262571 140197572364032 logging_writer.py:48] [89500] global_step=89500, grad_norm=4.583991050720215, loss=1.761967658996582
I0307 19:47:26.209371 140197563971328 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.843987464904785, loss=1.6356483697891235
I0307 19:48:05.033910 140197572364032 logging_writer.py:48] [89700] global_step=89700, grad_norm=4.57134485244751, loss=1.7308166027069092
I0307 19:48:44.282301 140197563971328 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.108652114868164, loss=1.7007083892822266
I0307 19:49:23.913058 140197572364032 logging_writer.py:48] [89900] global_step=89900, grad_norm=4.6463799476623535, loss=1.712369680404663
I0307 19:50:03.931988 140197563971328 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.903573513031006, loss=1.7296198606491089
2025-03-07 19:50:25.849968: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:50:43.590420 140197572364032 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.2422943115234375, loss=1.8354541063308716
I0307 19:51:22.278796 140197563971328 logging_writer.py:48] [90200] global_step=90200, grad_norm=4.058863639831543, loss=1.8556095361709595
I0307 19:52:01.483405 140197572364032 logging_writer.py:48] [90300] global_step=90300, grad_norm=4.097548484802246, loss=1.7483396530151367
I0307 19:52:40.620538 140197563971328 logging_writer.py:48] [90400] global_step=90400, grad_norm=4.424006462097168, loss=1.662609577178955
I0307 19:53:16.457288 140352918893760 spec.py:321] Evaluating on the training split.
I0307 19:53:29.101028 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 19:54:20.587128 140352918893760 spec.py:349] Evaluating on the test split.
I0307 19:54:22.329415 140352918893760 submission_runner.py:469] Time since start: 38593.49s, 	Step: 90492, 	{'train/accuracy': 0.6983617544174194, 'train/loss': 1.1918472051620483, 'validation/accuracy': 0.6398000121116638, 'validation/loss': 1.4658702611923218, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.1696877479553223, 'test/num_examples': 10000, 'score': 35242.96688723564, 'total_duration': 38593.49281334877, 'accumulated_submission_time': 35242.96688723564, 'accumulated_eval_time': 3331.2178497314453, 'accumulated_logging_time': 9.688908100128174}
I0307 19:54:22.415160 140197572364032 logging_writer.py:48] [90492] accumulated_eval_time=3331.22, accumulated_logging_time=9.68891, accumulated_submission_time=35243, global_step=90492, preemption_count=0, score=35243, test/accuracy=0.5154, test/loss=2.16969, test/num_examples=10000, total_duration=38593.5, train/accuracy=0.698362, train/loss=1.19185, validation/accuracy=0.6398, validation/loss=1.46587, validation/num_examples=50000
I0307 19:54:25.871971 140197563971328 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.125951290130615, loss=1.65094792842865
I0307 19:55:05.469233 140197572364032 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.383557319641113, loss=1.6719470024108887
I0307 19:55:44.750963 140197563971328 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.906831979751587, loss=1.5177338123321533
I0307 19:56:24.047538 140197572364032 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.904963970184326, loss=1.7156888246536255
I0307 19:57:03.024219 140197563971328 logging_writer.py:48] [90900] global_step=90900, grad_norm=4.619035243988037, loss=1.6406079530715942
I0307 19:57:42.242931 140197572364032 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.070532321929932, loss=1.6486215591430664
I0307 19:58:21.402046 140197563971328 logging_writer.py:48] [91100] global_step=91100, grad_norm=4.167987823486328, loss=1.682291865348816
I0307 19:59:01.273126 140197572364032 logging_writer.py:48] [91200] global_step=91200, grad_norm=4.349292278289795, loss=1.674617052078247
I0307 19:59:41.322082 140197563971328 logging_writer.py:48] [91300] global_step=91300, grad_norm=4.482802867889404, loss=1.6407939195632935
2025-03-07 19:59:43.951597: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:00:21.009281 140197572364032 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.13910436630249, loss=1.7245858907699585
I0307 20:01:00.248448 140197563971328 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.048062324523926, loss=1.7151120901107788
I0307 20:01:39.520695 140197572364032 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.465020179748535, loss=1.8614912033081055
I0307 20:02:19.080167 140197563971328 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.2502546310424805, loss=1.6365501880645752
I0307 20:02:52.582751 140352918893760 spec.py:321] Evaluating on the training split.
I0307 20:03:04.850014 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 20:03:52.620739 140352918893760 spec.py:349] Evaluating on the test split.
I0307 20:03:54.395596 140352918893760 submission_runner.py:469] Time since start: 39165.56s, 	Step: 91785, 	{'train/accuracy': 0.7052175998687744, 'train/loss': 1.1566941738128662, 'validation/accuracy': 0.6466000080108643, 'validation/loss': 1.4528599977493286, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.150705099105835, 'test/num_examples': 10000, 'score': 35752.9436879158, 'total_duration': 39165.55902886391, 'accumulated_submission_time': 35752.9436879158, 'accumulated_eval_time': 3393.0305194854736, 'accumulated_logging_time': 9.80940294265747}
I0307 20:03:54.479367 140197572364032 logging_writer.py:48] [91785] accumulated_eval_time=3393.03, accumulated_logging_time=9.8094, accumulated_submission_time=35752.9, global_step=91785, preemption_count=0, score=35752.9, test/accuracy=0.5255, test/loss=2.15071, test/num_examples=10000, total_duration=39165.6, train/accuracy=0.705218, train/loss=1.15669, validation/accuracy=0.6466, validation/loss=1.45286, validation/num_examples=50000
I0307 20:04:00.830781 140197563971328 logging_writer.py:48] [91800] global_step=91800, grad_norm=4.598717212677002, loss=1.7389147281646729
I0307 20:04:40.273189 140197572364032 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.3120436668396, loss=1.6667951345443726
I0307 20:05:20.066103 140197563971328 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.016107082366943, loss=1.7311854362487793
I0307 20:05:59.362663 140197572364032 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.500349044799805, loss=1.7202990055084229
I0307 20:06:38.764706 140197563971328 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.738095760345459, loss=1.646856427192688
I0307 20:07:17.902622 140197572364032 logging_writer.py:48] [92300] global_step=92300, grad_norm=4.803797245025635, loss=1.5868606567382812
I0307 20:07:58.007721 140197563971328 logging_writer.py:48] [92400] global_step=92400, grad_norm=4.380610942840576, loss=1.736464023590088
I0307 20:08:37.796687 140197572364032 logging_writer.py:48] [92500] global_step=92500, grad_norm=4.567505836486816, loss=1.6392375230789185
2025-03-07 20:09:00.997539: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:09:17.808442 140197563971328 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.966688871383667, loss=1.7098184823989868
I0307 20:09:56.681266 140197572364032 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.035892009735107, loss=1.5446043014526367
I0307 20:10:35.917509 140197563971328 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.313307762145996, loss=1.7049379348754883
I0307 20:11:14.847507 140197572364032 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.9199790954589844, loss=1.5772485733032227
I0307 20:11:54.073269 140197563971328 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.3054304122924805, loss=1.6624772548675537
I0307 20:12:24.788493 140352918893760 spec.py:321] Evaluating on the training split.
I0307 20:12:37.495487 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 20:13:24.557282 140352918893760 spec.py:349] Evaluating on the test split.
I0307 20:13:26.287100 140352918893760 submission_runner.py:469] Time since start: 39737.45s, 	Step: 93081, 	{'train/accuracy': 0.7030253410339355, 'train/loss': 1.1503082513809204, 'validation/accuracy': 0.6487199664115906, 'validation/loss': 1.4397367238998413, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1711199283599854, 'test/num_examples': 10000, 'score': 36263.06934309006, 'total_duration': 39737.45051789284, 'accumulated_submission_time': 36263.06934309006, 'accumulated_eval_time': 3454.528933286667, 'accumulated_logging_time': 9.922516822814941}
I0307 20:13:26.363243 140197572364032 logging_writer.py:48] [93081] accumulated_eval_time=3454.53, accumulated_logging_time=9.92252, accumulated_submission_time=36263.1, global_step=93081, preemption_count=0, score=36263.1, test/accuracy=0.5205, test/loss=2.17112, test/num_examples=10000, total_duration=39737.5, train/accuracy=0.703025, train/loss=1.15031, validation/accuracy=0.64872, validation/loss=1.43974, validation/num_examples=50000
I0307 20:13:34.171749 140197563971328 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.368307590484619, loss=1.772182583808899
I0307 20:14:13.501282 140197572364032 logging_writer.py:48] [93200] global_step=93200, grad_norm=4.0965576171875, loss=1.6337592601776123
I0307 20:14:52.715548 140197563971328 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.185434341430664, loss=1.756978154182434
I0307 20:15:31.908498 140197572364032 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.950613260269165, loss=1.6593612432479858
I0307 20:16:11.042766 140197563971328 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.275392532348633, loss=1.645646572113037
I0307 20:16:50.426508 140197572364032 logging_writer.py:48] [93600] global_step=93600, grad_norm=4.4205241203308105, loss=1.6769435405731201
I0307 20:17:30.377898 140197563971328 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.6386334896087646, loss=1.6701250076293945
I0307 20:18:10.292824 140197572364032 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.311188697814941, loss=1.6589196920394897
2025-03-07 20:18:15.089667: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:18:50.148326 140197563971328 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.547611713409424, loss=1.6049312353134155
I0307 20:19:30.240273 140197572364032 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.267204761505127, loss=1.7614989280700684
I0307 20:20:09.168339 140197563971328 logging_writer.py:48] [94100] global_step=94100, grad_norm=4.39124059677124, loss=1.615120530128479
I0307 20:20:48.768393 140197572364032 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.858775615692139, loss=1.7378590106964111
I0307 20:21:28.183578 140197563971328 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.199185848236084, loss=1.6006940603256226
I0307 20:21:56.291580 140352918893760 spec.py:321] Evaluating on the training split.
I0307 20:22:09.136107 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 20:22:54.245281 140352918893760 spec.py:349] Evaluating on the test split.
I0307 20:22:55.950109 140352918893760 submission_runner.py:469] Time since start: 40307.11s, 	Step: 94372, 	{'train/accuracy': 0.7121531963348389, 'train/loss': 1.1211084127426147, 'validation/accuracy': 0.6522200107574463, 'validation/loss': 1.4152565002441406, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.1081056594848633, 'test/num_examples': 10000, 'score': 36772.82030248642, 'total_duration': 40307.11348390579, 'accumulated_submission_time': 36772.82030248642, 'accumulated_eval_time': 3514.187231063843, 'accumulated_logging_time': 10.021862030029297}
I0307 20:22:56.030309 140197572364032 logging_writer.py:48] [94372] accumulated_eval_time=3514.19, accumulated_logging_time=10.0219, accumulated_submission_time=36772.8, global_step=94372, preemption_count=0, score=36772.8, test/accuracy=0.5314, test/loss=2.10811, test/num_examples=10000, total_duration=40307.1, train/accuracy=0.712153, train/loss=1.12111, validation/accuracy=0.65222, validation/loss=1.41526, validation/num_examples=50000
I0307 20:23:07.524905 140197563971328 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.371571063995361, loss=1.7440264225006104
I0307 20:23:46.780237 140197572364032 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.075634002685547, loss=1.6362782716751099
I0307 20:24:25.991364 140197563971328 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.118455410003662, loss=1.6493767499923706
I0307 20:25:05.423624 140197572364032 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.455093860626221, loss=1.5868847370147705
I0307 20:25:44.591624 140197563971328 logging_writer.py:48] [94800] global_step=94800, grad_norm=5.104820728302002, loss=1.6382286548614502
I0307 20:26:24.039573 140197572364032 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.801735877990723, loss=1.6739516258239746
I0307 20:27:03.315484 140197563971328 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.8043220043182373, loss=1.7069334983825684
2025-03-07 20:27:29.018280: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:27:42.627571 140197572364032 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.133126735687256, loss=1.716579794883728
I0307 20:28:22.558584 140197563971328 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.575334548950195, loss=1.5866163969039917
I0307 20:29:01.866379 140197572364032 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.9096839427948, loss=1.6001759767532349
I0307 20:29:41.147470 140197563971328 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.1913228034973145, loss=1.5998873710632324
I0307 20:30:19.961920 140197572364032 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.0127716064453125, loss=1.6395223140716553
I0307 20:30:59.326781 140197563971328 logging_writer.py:48] [95600] global_step=95600, grad_norm=5.007410049438477, loss=1.614100694656372
I0307 20:31:26.024006 140352918893760 spec.py:321] Evaluating on the training split.
I0307 20:31:38.782717 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 20:32:20.409853 140352918893760 spec.py:349] Evaluating on the test split.
I0307 20:32:22.122664 140352918893760 submission_runner.py:469] Time since start: 40873.29s, 	Step: 95668, 	{'train/accuracy': 0.7100805044174194, 'train/loss': 1.1191871166229248, 'validation/accuracy': 0.6510399580001831, 'validation/loss': 1.4259061813354492, 'validation/num_examples': 50000, 'test/accuracy': 0.5286000370979309, 'test/loss': 2.1263363361358643, 'test/num_examples': 10000, 'score': 37282.60960936546, 'total_duration': 40873.28606629372, 'accumulated_submission_time': 37282.60960936546, 'accumulated_eval_time': 3570.2856843471527, 'accumulated_logging_time': 10.138883113861084}
I0307 20:32:22.213362 140197572364032 logging_writer.py:48] [95668] accumulated_eval_time=3570.29, accumulated_logging_time=10.1389, accumulated_submission_time=37282.6, global_step=95668, preemption_count=0, score=37282.6, test/accuracy=0.5286, test/loss=2.12634, test/num_examples=10000, total_duration=40873.3, train/accuracy=0.710081, train/loss=1.11919, validation/accuracy=0.65104, validation/loss=1.42591, validation/num_examples=50000
I0307 20:32:35.429648 140197563971328 logging_writer.py:48] [95700] global_step=95700, grad_norm=3.740906238555908, loss=1.6739610433578491
I0307 20:33:14.605051 140197572364032 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.546665191650391, loss=1.6465721130371094
I0307 20:33:53.775830 140197563971328 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.9247758388519287, loss=1.7128503322601318
I0307 20:34:33.370460 140197572364032 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.281214714050293, loss=1.6639316082000732
I0307 20:35:12.857043 140197563971328 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.060243129730225, loss=1.792725920677185
I0307 20:35:53.332086 140197572364032 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.7893571853637695, loss=1.797249436378479
I0307 20:36:33.529123 140197563971328 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.49270486831665, loss=1.67911696434021
2025-03-07 20:36:38.156360: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:37:13.360542 140197572364032 logging_writer.py:48] [96400] global_step=96400, grad_norm=5.229318618774414, loss=1.7743151187896729
I0307 20:37:53.394311 140197563971328 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.176093101501465, loss=1.6992042064666748
I0307 20:38:32.603277 140197572364032 logging_writer.py:48] [96600] global_step=96600, grad_norm=4.29482364654541, loss=1.676632046699524
I0307 20:39:12.151821 140197563971328 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.70803689956665, loss=1.7522398233413696
I0307 20:39:51.449664 140197572364032 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.467565059661865, loss=1.6840543746948242
I0307 20:40:30.683270 140197563971328 logging_writer.py:48] [96900] global_step=96900, grad_norm=4.181124687194824, loss=1.6213091611862183
I0307 20:40:52.147845 140352918893760 spec.py:321] Evaluating on the training split.
I0307 20:41:05.954833 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 20:41:56.400873 140352918893760 spec.py:349] Evaluating on the test split.
I0307 20:41:58.101148 140352918893760 submission_runner.py:469] Time since start: 41449.26s, 	Step: 96956, 	{'train/accuracy': 0.7156209945678711, 'train/loss': 1.1100255250930786, 'validation/accuracy': 0.6535999774932861, 'validation/loss': 1.4162813425064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5283000469207764, 'test/loss': 2.1131956577301025, 'test/num_examples': 10000, 'score': 37792.304157972336, 'total_duration': 41449.264556884766, 'accumulated_submission_time': 37792.304157972336, 'accumulated_eval_time': 3636.2387998104095, 'accumulated_logging_time': 10.314347982406616}
I0307 20:41:58.177787 140197572364032 logging_writer.py:48] [96956] accumulated_eval_time=3636.24, accumulated_logging_time=10.3143, accumulated_submission_time=37792.3, global_step=96956, preemption_count=0, score=37792.3, test/accuracy=0.5283, test/loss=2.1132, test/num_examples=10000, total_duration=41449.3, train/accuracy=0.715621, train/loss=1.11003, validation/accuracy=0.6536, validation/loss=1.41628, validation/num_examples=50000
I0307 20:42:15.722947 140197563971328 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.9742977619171143, loss=1.615018606185913
I0307 20:42:55.034604 140197572364032 logging_writer.py:48] [97100] global_step=97100, grad_norm=4.529067516326904, loss=1.5717341899871826
I0307 20:43:34.436235 140197563971328 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.160750389099121, loss=1.6877533197402954
I0307 20:44:13.933939 140197572364032 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.3955793380737305, loss=1.659109115600586
I0307 20:44:53.774739 140197563971328 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.863153457641602, loss=1.6145058870315552
I0307 20:45:33.080643 140197572364032 logging_writer.py:48] [97500] global_step=97500, grad_norm=4.249100208282471, loss=1.674148440361023
2025-03-07 20:46:00.055639: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:46:12.411607 140197563971328 logging_writer.py:48] [97600] global_step=97600, grad_norm=5.158007621765137, loss=1.6464996337890625
I0307 20:46:51.083678 140197572364032 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.152069091796875, loss=1.642819881439209
I0307 20:47:29.549065 140197563971328 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.1740241050720215, loss=1.7151710987091064
I0307 20:48:08.815144 140197572364032 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.7224860191345215, loss=1.5616848468780518
I0307 20:48:48.733322 140197563971328 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.25855827331543, loss=1.6578350067138672
I0307 20:49:28.249529 140197572364032 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.1725592613220215, loss=1.6183496713638306
I0307 20:50:08.237660 140197563971328 logging_writer.py:48] [98200] global_step=98200, grad_norm=4.667760372161865, loss=1.6295439004898071
I0307 20:50:28.395825 140352918893760 spec.py:321] Evaluating on the training split.
I0307 20:50:40.664878 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 20:51:19.022003 140352918893760 spec.py:349] Evaluating on the test split.
I0307 20:51:20.723308 140352918893760 submission_runner.py:469] Time since start: 42011.89s, 	Step: 98252, 	{'train/accuracy': 0.713289201259613, 'train/loss': 1.117990493774414, 'validation/accuracy': 0.6503399610519409, 'validation/loss': 1.4127005338668823, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.1397383213043213, 'test/num_examples': 10000, 'score': 38302.33400964737, 'total_duration': 42011.88674235344, 'accumulated_submission_time': 38302.33400964737, 'accumulated_eval_time': 3688.5661101341248, 'accumulated_logging_time': 10.423690557479858}
I0307 20:51:20.817464 140197572364032 logging_writer.py:48] [98252] accumulated_eval_time=3688.57, accumulated_logging_time=10.4237, accumulated_submission_time=38302.3, global_step=98252, preemption_count=0, score=38302.3, test/accuracy=0.5262, test/loss=2.13974, test/num_examples=10000, total_duration=42011.9, train/accuracy=0.713289, train/loss=1.11799, validation/accuracy=0.65034, validation/loss=1.4127, validation/num_examples=50000
I0307 20:51:40.121251 140197563971328 logging_writer.py:48] [98300] global_step=98300, grad_norm=3.9093899726867676, loss=1.641669750213623
I0307 20:52:19.881309 140197572364032 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.23085880279541, loss=1.719617486000061
I0307 20:52:59.872342 140197563971328 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.550622463226318, loss=1.5434772968292236
I0307 20:53:39.760744 140197572364032 logging_writer.py:48] [98600] global_step=98600, grad_norm=4.014923572540283, loss=1.620256781578064
I0307 20:54:19.538910 140197563971328 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.218212127685547, loss=1.6045596599578857
I0307 20:54:58.143986 140197572364032 logging_writer.py:48] [98800] global_step=98800, grad_norm=4.520090579986572, loss=1.6464602947235107
2025-03-07 20:55:06.603806: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:55:37.243662 140197563971328 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.870236396789551, loss=1.6204547882080078
I0307 20:56:16.911827 140197572364032 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.450949668884277, loss=1.7016222476959229
I0307 20:56:56.712140 140197563971328 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.217528820037842, loss=1.6207326650619507
I0307 20:57:36.093641 140197572364032 logging_writer.py:48] [99200] global_step=99200, grad_norm=5.036678791046143, loss=1.6293184757232666
I0307 20:58:16.065679 140197563971328 logging_writer.py:48] [99300] global_step=99300, grad_norm=5.982213020324707, loss=1.7423121929168701
I0307 20:58:55.899325 140197572364032 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.371458053588867, loss=1.6676645278930664
I0307 20:59:35.415890 140197563971328 logging_writer.py:48] [99500] global_step=99500, grad_norm=5.300907135009766, loss=1.820417046546936
I0307 20:59:50.741208 140352918893760 spec.py:321] Evaluating on the training split.
I0307 21:00:03.986969 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 21:00:52.338660 140352918893760 spec.py:349] Evaluating on the test split.
I0307 21:00:54.131871 140352918893760 submission_runner.py:469] Time since start: 42585.30s, 	Step: 99540, 	{'train/accuracy': 0.7054368257522583, 'train/loss': 1.1409116983413696, 'validation/accuracy': 0.6470800042152405, 'validation/loss': 1.444724678993225, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.1654844284057617, 'test/num_examples': 10000, 'score': 38812.04327368736, 'total_duration': 42585.295284986496, 'accumulated_submission_time': 38812.04327368736, 'accumulated_eval_time': 3751.95658993721, 'accumulated_logging_time': 10.577909231185913}
I0307 21:00:54.246506 140197572364032 logging_writer.py:48] [99540] accumulated_eval_time=3751.96, accumulated_logging_time=10.5779, accumulated_submission_time=38812, global_step=99540, preemption_count=0, score=38812, test/accuracy=0.5206, test/loss=2.16548, test/num_examples=10000, total_duration=42585.3, train/accuracy=0.705437, train/loss=1.14091, validation/accuracy=0.64708, validation/loss=1.44472, validation/num_examples=50000
I0307 21:01:18.134778 140197563971328 logging_writer.py:48] [99600] global_step=99600, grad_norm=4.7180094718933105, loss=1.6697959899902344
I0307 21:01:56.913957 140197572364032 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.262358665466309, loss=1.6405564546585083
I0307 21:02:35.576319 140197563971328 logging_writer.py:48] [99800] global_step=99800, grad_norm=4.3672051429748535, loss=1.5270800590515137
I0307 21:03:14.414476 140197572364032 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.921826362609863, loss=1.6755335330963135
I0307 21:03:52.765773 140197563971328 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.287302017211914, loss=1.6143219470977783
2025-03-07 21:04:17.399378: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:04:30.985419 140197572364032 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.23830509185791, loss=1.6162539720535278
I0307 21:05:09.462977 140197563971328 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.328400135040283, loss=1.6568454504013062
I0307 21:05:48.369493 140197572364032 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.51584005355835, loss=1.6381675004959106
I0307 21:06:26.725285 140197563971328 logging_writer.py:48] [100400] global_step=100400, grad_norm=3.9555153846740723, loss=1.6329286098480225
I0307 21:07:05.283902 140197572364032 logging_writer.py:48] [100500] global_step=100500, grad_norm=3.8433637619018555, loss=1.5339807271957397
I0307 21:07:44.309090 140197563971328 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.204970359802246, loss=1.571181058883667
I0307 21:08:23.565751 140197572364032 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.361799240112305, loss=1.6215535402297974
I0307 21:09:02.910484 140197563971328 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.919704437255859, loss=1.5981358289718628
I0307 21:09:24.193815 140352918893760 spec.py:321] Evaluating on the training split.
I0307 21:09:36.550852 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 21:10:24.950091 140352918893760 spec.py:349] Evaluating on the test split.
I0307 21:10:27.537631 140352918893760 submission_runner.py:469] Time since start: 43158.70s, 	Step: 100854, 	{'train/accuracy': 0.724629282951355, 'train/loss': 1.0570652484893799, 'validation/accuracy': 0.6636399626731873, 'validation/loss': 1.3612900972366333, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.066345691680908, 'test/num_examples': 10000, 'score': 39321.80171942711, 'total_duration': 43158.70108675957, 'accumulated_submission_time': 39321.80171942711, 'accumulated_eval_time': 3815.3002536296844, 'accumulated_logging_time': 10.727151870727539}
I0307 21:10:27.644449 140197572364032 logging_writer.py:48] [100854] accumulated_eval_time=3815.3, accumulated_logging_time=10.7272, accumulated_submission_time=39321.8, global_step=100854, preemption_count=0, score=39321.8, test/accuracy=0.5407, test/loss=2.06635, test/num_examples=10000, total_duration=43158.7, train/accuracy=0.724629, train/loss=1.05707, validation/accuracy=0.66364, validation/loss=1.36129, validation/num_examples=50000
I0307 21:10:45.997000 140197563971328 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.309802532196045, loss=1.6668689250946045
I0307 21:11:25.079794 140197572364032 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.326269626617432, loss=1.6693434715270996
I0307 21:12:04.475439 140197563971328 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.333302974700928, loss=1.5208067893981934
I0307 21:12:44.820432 140197572364032 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.53981351852417, loss=1.5045093297958374
I0307 21:13:24.181632 140197563971328 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.352219581604004, loss=1.6043903827667236
2025-03-07 21:13:30.341695: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:14:03.880942 140197572364032 logging_writer.py:48] [101400] global_step=101400, grad_norm=4.667069911956787, loss=1.6409250497817993
I0307 21:14:43.165078 140197563971328 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.242320537567139, loss=1.5944156646728516
I0307 21:15:22.925930 140197572364032 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.601208686828613, loss=1.5538194179534912
I0307 21:16:02.457366 140197563971328 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.827049255371094, loss=1.7242680788040161
I0307 21:16:41.777910 140197572364032 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.405270099639893, loss=1.6347728967666626
I0307 21:17:21.671349 140197563971328 logging_writer.py:48] [101900] global_step=101900, grad_norm=6.667525291442871, loss=1.6057157516479492
I0307 21:18:01.387947 140197572364032 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.746495723724365, loss=1.6002089977264404
I0307 21:18:40.775900 140197563971328 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.938608169555664, loss=1.697310447692871
I0307 21:18:57.613669 140352918893760 spec.py:321] Evaluating on the training split.
I0307 21:19:10.172342 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 21:19:46.535813 140352918893760 spec.py:349] Evaluating on the test split.
I0307 21:19:48.274088 140352918893760 submission_runner.py:469] Time since start: 43719.44s, 	Step: 102143, 	{'train/accuracy': 0.7178930044174194, 'train/loss': 1.09544038772583, 'validation/accuracy': 0.6579200029373169, 'validation/loss': 1.3941709995269775, 'validation/num_examples': 50000, 'test/accuracy': 0.5315999984741211, 'test/loss': 2.109208822250366, 'test/num_examples': 10000, 'score': 39831.58630847931, 'total_duration': 43719.43748879433, 'accumulated_submission_time': 39831.58630847931, 'accumulated_eval_time': 3865.9604680538177, 'accumulated_logging_time': 10.864547491073608}
I0307 21:19:48.429764 140197572364032 logging_writer.py:48] [102143] accumulated_eval_time=3865.96, accumulated_logging_time=10.8645, accumulated_submission_time=39831.6, global_step=102143, preemption_count=0, score=39831.6, test/accuracy=0.5316, test/loss=2.10921, test/num_examples=10000, total_duration=43719.4, train/accuracy=0.717893, train/loss=1.09544, validation/accuracy=0.65792, validation/loss=1.39417, validation/num_examples=50000
I0307 21:20:11.354680 140197563971328 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.267198085784912, loss=1.5753411054611206
I0307 21:20:50.969093 140197572364032 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.140221118927002, loss=1.58689284324646
I0307 21:21:30.867881 140197563971328 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.934802532196045, loss=1.6443299055099487
I0307 21:22:11.001172 140197572364032 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.098378658294678, loss=1.492200255393982
2025-03-07 21:22:37.404409: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:22:50.505296 140197563971328 logging_writer.py:48] [102600] global_step=102600, grad_norm=3.8091204166412354, loss=1.615635871887207
I0307 21:23:30.047842 140197572364032 logging_writer.py:48] [102700] global_step=102700, grad_norm=3.9437358379364014, loss=1.4696546792984009
I0307 21:24:09.651854 140197563971328 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.680293083190918, loss=1.6505340337753296
I0307 21:24:49.715578 140197572364032 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.734238147735596, loss=1.6671303510665894
I0307 21:25:28.978866 140197563971328 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.311347961425781, loss=1.6676355600357056
I0307 21:26:08.579647 140197572364032 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.861067771911621, loss=1.65370774269104
I0307 21:26:48.021442 140197563971328 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.286838054656982, loss=1.5601557493209839
I0307 21:27:27.644801 140197572364032 logging_writer.py:48] [103300] global_step=103300, grad_norm=4.615150451660156, loss=1.6843243837356567
I0307 21:28:06.717650 140197563971328 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.365374565124512, loss=1.5675866603851318
I0307 21:28:18.645090 140352918893760 spec.py:321] Evaluating on the training split.
I0307 21:28:31.236527 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 21:29:19.537498 140352918893760 spec.py:349] Evaluating on the test split.
I0307 21:29:21.237076 140352918893760 submission_runner.py:469] Time since start: 44292.40s, 	Step: 103431, 	{'train/accuracy': 0.7240114808082581, 'train/loss': 1.0633416175842285, 'validation/accuracy': 0.6632399559020996, 'validation/loss': 1.3685472011566162, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.0787832736968994, 'test/num_examples': 10000, 'score': 40341.621921777725, 'total_duration': 44292.40053629875, 'accumulated_submission_time': 40341.621921777725, 'accumulated_eval_time': 3928.552306652069, 'accumulated_logging_time': 11.045519351959229}
I0307 21:29:21.316248 140197572364032 logging_writer.py:48] [103431] accumulated_eval_time=3928.55, accumulated_logging_time=11.0455, accumulated_submission_time=40341.6, global_step=103431, preemption_count=0, score=40341.6, test/accuracy=0.5307, test/loss=2.07878, test/num_examples=10000, total_duration=44292.4, train/accuracy=0.724011, train/loss=1.06334, validation/accuracy=0.66324, validation/loss=1.36855, validation/num_examples=50000
I0307 21:29:48.970570 140197563971328 logging_writer.py:48] [103500] global_step=103500, grad_norm=5.098579406738281, loss=1.677495002746582
I0307 21:30:28.765664 140197572364032 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.320088863372803, loss=1.5441187620162964
I0307 21:31:08.917504 140197563971328 logging_writer.py:48] [103700] global_step=103700, grad_norm=5.3964738845825195, loss=1.65152907371521
I0307 21:31:48.759448 140197572364032 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.547147750854492, loss=1.4659913778305054
2025-03-07 21:31:57.108839: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:32:28.588205 140197563971328 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.627871036529541, loss=1.6520284414291382
I0307 21:33:08.005692 140197572364032 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.146077632904053, loss=1.635556936264038
I0307 21:33:47.686255 140197563971328 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.741401672363281, loss=1.7111668586730957
I0307 21:34:26.797722 140197572364032 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.373150825500488, loss=1.612644076347351
I0307 21:35:06.169954 140197563971328 logging_writer.py:48] [104300] global_step=104300, grad_norm=5.35361385345459, loss=1.708993911743164
I0307 21:35:45.684261 140197572364032 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.731664657592773, loss=1.651010513305664
I0307 21:36:25.014002 140197563971328 logging_writer.py:48] [104500] global_step=104500, grad_norm=4.446551322937012, loss=1.6628446578979492
I0307 21:37:04.309523 140197572364032 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.922523021697998, loss=1.6251428127288818
I0307 21:37:44.159486 140197563971328 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.3633952140808105, loss=1.6585315465927124
I0307 21:37:51.622155 140352918893760 spec.py:321] Evaluating on the training split.
I0307 21:38:04.277832 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 21:38:31.187999 140352918893760 spec.py:349] Evaluating on the test split.
I0307 21:38:33.828363 140352918893760 submission_runner.py:469] Time since start: 44844.99s, 	Step: 104720, 	{'train/accuracy': 0.7229551672935486, 'train/loss': 1.070923089981079, 'validation/accuracy': 0.6643999814987183, 'validation/loss': 1.3667947053909302, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0891666412353516, 'test/num_examples': 10000, 'score': 40851.740795850754, 'total_duration': 44844.991621255875, 'accumulated_submission_time': 40851.740795850754, 'accumulated_eval_time': 3970.7581713199615, 'accumulated_logging_time': 11.154846668243408}
I0307 21:38:33.971457 140197572364032 logging_writer.py:48] [104720] accumulated_eval_time=3970.76, accumulated_logging_time=11.1548, accumulated_submission_time=40851.7, global_step=104720, preemption_count=0, score=40851.7, test/accuracy=0.5377, test/loss=2.08917, test/num_examples=10000, total_duration=44845, train/accuracy=0.722955, train/loss=1.07092, validation/accuracy=0.6644, validation/loss=1.36679, validation/num_examples=50000
I0307 21:39:05.401753 140197563971328 logging_writer.py:48] [104800] global_step=104800, grad_norm=5.068922519683838, loss=1.6948983669281006
I0307 21:39:44.967772 140197572364032 logging_writer.py:48] [104900] global_step=104900, grad_norm=5.158730983734131, loss=1.7437517642974854
I0307 21:40:24.840525 140197563971328 logging_writer.py:48] [105000] global_step=105000, grad_norm=5.072014808654785, loss=1.4990112781524658
2025-03-07 21:40:52.857343: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:41:04.699870 140197572364032 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.536813735961914, loss=1.6167994737625122
I0307 21:41:43.884962 140197563971328 logging_writer.py:48] [105200] global_step=105200, grad_norm=3.9744021892547607, loss=1.6260368824005127
I0307 21:42:23.383357 140197572364032 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.9527692794799805, loss=1.6486790180206299
I0307 21:43:02.748585 140197563971328 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.294943332672119, loss=1.5758330821990967
I0307 21:43:42.527837 140197572364032 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.271313667297363, loss=1.6199592351913452
I0307 21:44:21.777575 140197563971328 logging_writer.py:48] [105600] global_step=105600, grad_norm=6.118321895599365, loss=1.5122960805892944
I0307 21:45:01.152489 140197572364032 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.601696968078613, loss=1.5109076499938965
I0307 21:45:40.816602 140197563971328 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.01881742477417, loss=1.4345612525939941
I0307 21:46:21.005260 140197572364032 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.29693603515625, loss=1.514911413192749
I0307 21:47:00.084469 140197563971328 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.683441638946533, loss=1.499112606048584
I0307 21:47:03.973888 140352918893760 spec.py:321] Evaluating on the training split.
I0307 21:47:16.302352 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 21:47:59.290428 140352918893760 spec.py:349] Evaluating on the test split.
I0307 21:48:00.992118 140352918893760 submission_runner.py:469] Time since start: 45412.16s, 	Step: 106011, 	{'train/accuracy': 0.7268216013908386, 'train/loss': 1.0451542139053345, 'validation/accuracy': 0.6627399921417236, 'validation/loss': 1.3652856349945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5373000502586365, 'test/loss': 2.0615460872650146, 'test/num_examples': 10000, 'score': 41361.53125357628, 'total_duration': 45412.15554738045, 'accumulated_submission_time': 41361.53125357628, 'accumulated_eval_time': 4027.776224374771, 'accumulated_logging_time': 11.355185985565186}
I0307 21:48:01.112959 140197572364032 logging_writer.py:48] [106011] accumulated_eval_time=4027.78, accumulated_logging_time=11.3552, accumulated_submission_time=41361.5, global_step=106011, preemption_count=0, score=41361.5, test/accuracy=0.5373, test/loss=2.06155, test/num_examples=10000, total_duration=45412.2, train/accuracy=0.726822, train/loss=1.04515, validation/accuracy=0.66274, validation/loss=1.36529, validation/num_examples=50000
I0307 21:48:36.473357 140197563971328 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.320802688598633, loss=1.5827465057373047
I0307 21:49:16.379276 140197572364032 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.731614589691162, loss=1.6165308952331543
I0307 21:49:56.851469 140197563971328 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.825958251953125, loss=1.5731046199798584
2025-03-07 21:50:04.410605: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:50:36.364921 140197572364032 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.300836086273193, loss=1.5226649045944214
I0307 21:51:15.397575 140197563971328 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.824582099914551, loss=1.5526560544967651
I0307 21:51:54.613008 140197572364032 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.511388301849365, loss=1.533101201057434
I0307 21:52:33.611638 140197563971328 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.480863094329834, loss=1.611097812652588
I0307 21:53:12.823303 140197572364032 logging_writer.py:48] [106800] global_step=106800, grad_norm=4.869187831878662, loss=1.5219018459320068
I0307 21:53:51.851572 140197563971328 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.404838562011719, loss=1.6905673742294312
I0307 21:54:30.740511 140197572364032 logging_writer.py:48] [107000] global_step=107000, grad_norm=5.035008907318115, loss=1.7029616832733154
I0307 21:55:09.977514 140197563971328 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.82455587387085, loss=1.5784189701080322
I0307 21:55:49.344436 140197572364032 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.791388511657715, loss=1.6054644584655762
I0307 21:56:28.912926 140197563971328 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.290744304656982, loss=1.5365235805511475
I0307 21:56:31.405034 140352918893760 spec.py:321] Evaluating on the training split.
I0307 21:56:43.753554 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 21:57:24.109305 140352918893760 spec.py:349] Evaluating on the test split.
I0307 21:57:25.854203 140352918893760 submission_runner.py:469] Time since start: 45977.02s, 	Step: 107307, 	{'train/accuracy': 0.7290736436843872, 'train/loss': 1.043601155281067, 'validation/accuracy': 0.6650199890136719, 'validation/loss': 1.3559585809707642, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.0329790115356445, 'test/num_examples': 10000, 'score': 41871.644416093826, 'total_duration': 45977.01761698723, 'accumulated_submission_time': 41871.644416093826, 'accumulated_eval_time': 4082.2251999378204, 'accumulated_logging_time': 11.502586364746094}
I0307 21:57:26.001624 140197572364032 logging_writer.py:48] [107307] accumulated_eval_time=4082.23, accumulated_logging_time=11.5026, accumulated_submission_time=41871.6, global_step=107307, preemption_count=0, score=41871.6, test/accuracy=0.5392, test/loss=2.03298, test/num_examples=10000, total_duration=45977, train/accuracy=0.729074, train/loss=1.0436, validation/accuracy=0.66502, validation/loss=1.35596, validation/num_examples=50000
I0307 21:58:03.555084 140197563971328 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.268477439880371, loss=1.497491717338562
I0307 21:58:43.671053 140197572364032 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.461565971374512, loss=1.510750412940979
2025-03-07 21:59:10.849451: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:59:22.358023 140197563971328 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.6064300537109375, loss=1.542864203453064
I0307 22:00:01.461383 140197572364032 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.6618828773498535, loss=1.5768166780471802
I0307 22:00:40.696185 140197563971328 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.680325984954834, loss=1.5758131742477417
I0307 22:01:20.134710 140197572364032 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.359126091003418, loss=1.57817542552948
I0307 22:01:59.151546 140197563971328 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.776622295379639, loss=1.477279543876648
I0307 22:02:38.294130 140197572364032 logging_writer.py:48] [108100] global_step=108100, grad_norm=5.3404388427734375, loss=1.7045984268188477
I0307 22:03:17.768484 140197563971328 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.5065460205078125, loss=1.6097211837768555
I0307 22:03:56.958837 140197572364032 logging_writer.py:48] [108300] global_step=108300, grad_norm=5.861187934875488, loss=1.59862220287323
I0307 22:04:35.490604 140197563971328 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.815282821655273, loss=1.5644460916519165
I0307 22:05:14.563127 140197572364032 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.572396755218506, loss=1.5791903734207153
I0307 22:05:54.118039 140197563971328 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.8748860359191895, loss=1.6357907056808472
I0307 22:05:56.056006 140352918893760 spec.py:321] Evaluating on the training split.
I0307 22:06:08.572774 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 22:06:48.032430 140352918893760 spec.py:349] Evaluating on the test split.
I0307 22:06:49.751891 140352918893760 submission_runner.py:469] Time since start: 46540.92s, 	Step: 108606, 	{'train/accuracy': 0.7270208597183228, 'train/loss': 1.0440120697021484, 'validation/accuracy': 0.6652199625968933, 'validation/loss': 1.3684667348861694, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.07810115814209, 'test/num_examples': 10000, 'score': 42381.51034498215, 'total_duration': 46540.91531252861, 'accumulated_submission_time': 42381.51034498215, 'accumulated_eval_time': 4135.920903921127, 'accumulated_logging_time': 11.686487674713135}
I0307 22:06:49.845339 140197572364032 logging_writer.py:48] [108606] accumulated_eval_time=4135.92, accumulated_logging_time=11.6865, accumulated_submission_time=42381.5, global_step=108606, preemption_count=0, score=42381.5, test/accuracy=0.5356, test/loss=2.0781, test/num_examples=10000, total_duration=46540.9, train/accuracy=0.727021, train/loss=1.04401, validation/accuracy=0.66522, validation/loss=1.36847, validation/num_examples=50000
I0307 22:07:26.969853 140197563971328 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.407925605773926, loss=1.568749189376831
I0307 22:08:05.927772 140197572364032 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.635871887207031, loss=1.5406699180603027
2025-03-07 22:08:14.513941: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:08:45.055361 140197563971328 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.958815574645996, loss=1.4128639698028564
I0307 22:09:23.549013 140197572364032 logging_writer.py:48] [109000] global_step=109000, grad_norm=4.805150985717773, loss=1.4957435131072998
I0307 22:10:01.900494 140197563971328 logging_writer.py:48] [109100] global_step=109100, grad_norm=5.092280864715576, loss=1.4816150665283203
I0307 22:10:41.138416 140197572364032 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.34273099899292, loss=1.4428634643554688
I0307 22:11:20.399920 140197563971328 logging_writer.py:48] [109300] global_step=109300, grad_norm=5.202560901641846, loss=1.612326979637146
I0307 22:11:59.154187 140197572364032 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.401869773864746, loss=1.6532354354858398
I0307 22:12:37.998664 140197563971328 logging_writer.py:48] [109500] global_step=109500, grad_norm=5.055309295654297, loss=1.5934892892837524
I0307 22:13:16.580417 140197572364032 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.757171630859375, loss=1.4442118406295776
I0307 22:13:55.504904 140197563971328 logging_writer.py:48] [109700] global_step=109700, grad_norm=5.026294708251953, loss=1.517861008644104
I0307 22:14:34.483987 140197572364032 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.9441375732421875, loss=1.595689296722412
I0307 22:15:13.646513 140197563971328 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.841418743133545, loss=1.4849684238433838
I0307 22:15:19.894126 140352918893760 spec.py:321] Evaluating on the training split.
I0307 22:15:31.944005 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 22:16:19.279457 140352918893760 spec.py:349] Evaluating on the test split.
I0307 22:16:20.980350 140352918893760 submission_runner.py:469] Time since start: 47112.14s, 	Step: 109917, 	{'train/accuracy': 0.7233936190605164, 'train/loss': 1.063387155532837, 'validation/accuracy': 0.6618399620056152, 'validation/loss': 1.3791370391845703, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.1093966960906982, 'test/num_examples': 10000, 'score': 42891.381705999374, 'total_duration': 47112.14376115799, 'accumulated_submission_time': 42891.381705999374, 'accumulated_eval_time': 4197.006940364838, 'accumulated_logging_time': 11.8048677444458}
I0307 22:16:21.126497 140197572364032 logging_writer.py:48] [109917] accumulated_eval_time=4197.01, accumulated_logging_time=11.8049, accumulated_submission_time=42891.4, global_step=109917, preemption_count=0, score=42891.4, test/accuracy=0.5317, test/loss=2.1094, test/num_examples=10000, total_duration=47112.1, train/accuracy=0.723394, train/loss=1.06339, validation/accuracy=0.66184, validation/loss=1.37914, validation/num_examples=50000
I0307 22:16:53.846599 140197563971328 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.744542121887207, loss=1.453015923500061
2025-03-07 22:17:22.826149: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:17:33.605935 140197572364032 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.981367111206055, loss=1.5192899703979492
I0307 22:18:12.622434 140197563971328 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.136429309844971, loss=1.4984450340270996
I0307 22:18:52.091109 140197572364032 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.3891682624816895, loss=1.593449354171753
I0307 22:19:31.231758 140197563971328 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.615610599517822, loss=1.6460623741149902
I0307 22:20:10.709398 140197572364032 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.943236827850342, loss=1.6534878015518188
I0307 22:20:50.063905 140197563971328 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.684348106384277, loss=1.4364455938339233
I0307 22:21:29.645507 140197572364032 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.546908855438232, loss=1.602898120880127
I0307 22:22:09.023995 140197563971328 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.5247802734375, loss=1.5161371231079102
I0307 22:22:48.376781 140197572364032 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.541497707366943, loss=1.5962082147598267
I0307 22:23:27.932069 140197563971328 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.499089241027832, loss=1.5191770792007446
I0307 22:24:07.621460 140197572364032 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.9659342765808105, loss=1.6111549139022827
I0307 22:24:47.840279 140197563971328 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.188461780548096, loss=1.6102747917175293
I0307 22:24:51.333953 140352918893760 spec.py:321] Evaluating on the training split.
I0307 22:25:03.598875 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 22:25:42.438616 140352918893760 spec.py:349] Evaluating on the test split.
I0307 22:25:44.149539 140352918893760 submission_runner.py:469] Time since start: 47675.31s, 	Step: 111210, 	{'train/accuracy': 0.7387595772743225, 'train/loss': 1.0036503076553345, 'validation/accuracy': 0.671239972114563, 'validation/loss': 1.3343429565429688, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.0269627571105957, 'test/num_examples': 10000, 'score': 43401.40015435219, 'total_duration': 47675.31294465065, 'accumulated_submission_time': 43401.40015435219, 'accumulated_eval_time': 4249.822323799133, 'accumulated_logging_time': 11.984727144241333}
I0307 22:25:44.312838 140197572364032 logging_writer.py:48] [111210] accumulated_eval_time=4249.82, accumulated_logging_time=11.9847, accumulated_submission_time=43401.4, global_step=111210, preemption_count=0, score=43401.4, test/accuracy=0.5434, test/loss=2.02696, test/num_examples=10000, total_duration=47675.3, train/accuracy=0.73876, train/loss=1.00365, validation/accuracy=0.67124, validation/loss=1.33434, validation/num_examples=50000
I0307 22:26:20.084270 140197563971328 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.682016849517822, loss=1.5700623989105225
2025-03-07 22:26:29.694868: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:26:59.262513 140197572364032 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.68013334274292, loss=1.4964598417282104
I0307 22:27:38.388761 140197563971328 logging_writer.py:48] [111500] global_step=111500, grad_norm=5.032381534576416, loss=1.5261330604553223
I0307 22:28:17.827380 140197572364032 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.402586936950684, loss=1.550582766532898
I0307 22:28:57.071821 140197563971328 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.3023529052734375, loss=1.5350327491760254
I0307 22:29:36.303539 140197572364032 logging_writer.py:48] [111800] global_step=111800, grad_norm=5.479933738708496, loss=1.5259802341461182
I0307 22:30:15.849170 140197563971328 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.630810737609863, loss=1.4825695753097534
I0307 22:30:55.439907 140197572364032 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.539009094238281, loss=1.5353630781173706
I0307 22:31:34.798434 140197563971328 logging_writer.py:48] [112100] global_step=112100, grad_norm=5.0847578048706055, loss=1.6337677240371704
I0307 22:32:14.169013 140197572364032 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.547602653503418, loss=1.4630941152572632
I0307 22:32:53.612082 140197563971328 logging_writer.py:48] [112300] global_step=112300, grad_norm=5.008879661560059, loss=1.6731024980545044
I0307 22:33:33.022583 140197572364032 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.848603248596191, loss=1.5315197706222534
I0307 22:34:13.332979 140197563971328 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.997172832489014, loss=1.5037841796875
I0307 22:34:14.165446 140352918893760 spec.py:321] Evaluating on the training split.
I0307 22:34:26.461707 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 22:35:06.632112 140352918893760 spec.py:349] Evaluating on the test split.
I0307 22:35:08.356211 140352918893760 submission_runner.py:469] Time since start: 48239.52s, 	Step: 112503, 	{'train/accuracy': 0.7416493892669678, 'train/loss': 0.9858709573745728, 'validation/accuracy': 0.670960009098053, 'validation/loss': 1.3273751735687256, 'validation/num_examples': 50000, 'test/accuracy': 0.5464000105857849, 'test/loss': 2.031764030456543, 'test/num_examples': 10000, 'score': 43911.0682926178, 'total_duration': 48239.51965045929, 'accumulated_submission_time': 43911.0682926178, 'accumulated_eval_time': 4304.01292848587, 'accumulated_logging_time': 12.177767276763916}
I0307 22:35:08.457529 140197572364032 logging_writer.py:48] [112503] accumulated_eval_time=4304.01, accumulated_logging_time=12.1778, accumulated_submission_time=43911.1, global_step=112503, preemption_count=0, score=43911.1, test/accuracy=0.5464, test/loss=2.03176, test/num_examples=10000, total_duration=48239.5, train/accuracy=0.741649, train/loss=0.985871, validation/accuracy=0.67096, validation/loss=1.32738, validation/num_examples=50000
2025-03-07 22:35:37.345565: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:35:46.898516 140197563971328 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.275594711303711, loss=1.4158755540847778
I0307 22:36:26.762634 140197572364032 logging_writer.py:48] [112700] global_step=112700, grad_norm=5.084573268890381, loss=1.6107089519500732
I0307 22:37:06.639176 140197563971328 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.776036739349365, loss=1.4831420183181763
I0307 22:37:46.425211 140197572364032 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.150848388671875, loss=1.4355409145355225
I0307 22:38:25.957293 140197563971328 logging_writer.py:48] [113000] global_step=113000, grad_norm=5.009341239929199, loss=1.5892077684402466
I0307 22:39:05.484931 140197572364032 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.636970520019531, loss=1.4367202520370483
I0307 22:39:44.589139 140197563971328 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.875414848327637, loss=1.449412226676941
I0307 22:40:23.913930 140197572364032 logging_writer.py:48] [113300] global_step=113300, grad_norm=5.137204170227051, loss=1.5066841840744019
I0307 22:41:03.515251 140197563971328 logging_writer.py:48] [113400] global_step=113400, grad_norm=5.27291202545166, loss=1.5463252067565918
I0307 22:41:43.197219 140197572364032 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.61383581161499, loss=1.6105880737304688
I0307 22:42:22.618913 140197563971328 logging_writer.py:48] [113600] global_step=113600, grad_norm=5.0268120765686035, loss=1.4918620586395264
I0307 22:43:02.564004 140197572364032 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.472089767456055, loss=1.489747166633606
I0307 22:43:38.421439 140352918893760 spec.py:321] Evaluating on the training split.
I0307 22:43:50.604860 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 22:44:33.721340 140352918893760 spec.py:349] Evaluating on the test split.
I0307 22:44:35.453538 140352918893760 submission_runner.py:469] Time since start: 48806.62s, 	Step: 113791, 	{'train/accuracy': 0.7413504123687744, 'train/loss': 0.9853124022483826, 'validation/accuracy': 0.6724199652671814, 'validation/loss': 1.3267568349838257, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.032102346420288, 'test/num_examples': 10000, 'score': 44420.78960490227, 'total_duration': 48806.616983413696, 'accumulated_submission_time': 44420.78960490227, 'accumulated_eval_time': 4361.044865608215, 'accumulated_logging_time': 12.369447469711304}
I0307 22:44:35.549578 140197563971328 logging_writer.py:48] [113791] accumulated_eval_time=4361.04, accumulated_logging_time=12.3694, accumulated_submission_time=44420.8, global_step=113791, preemption_count=0, score=44420.8, test/accuracy=0.5495, test/loss=2.0321, test/num_examples=10000, total_duration=48806.6, train/accuracy=0.74135, train/loss=0.985312, validation/accuracy=0.67242, validation/loss=1.32676, validation/num_examples=50000
I0307 22:44:39.464176 140197572364032 logging_writer.py:48] [113800] global_step=113800, grad_norm=5.113447189331055, loss=1.5929315090179443
2025-03-07 22:44:50.171466: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:45:19.099749 140197563971328 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.4392805099487305, loss=1.4875584840774536
I0307 22:45:59.391284 140197572364032 logging_writer.py:48] [114000] global_step=114000, grad_norm=5.424975872039795, loss=1.6234800815582275
I0307 22:46:38.949062 140197563971328 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.482529640197754, loss=1.4980534315109253
I0307 22:47:18.193735 140197572364032 logging_writer.py:48] [114200] global_step=114200, grad_norm=5.350147247314453, loss=1.5372647047042847
I0307 22:47:57.968600 140197563971328 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.849386215209961, loss=1.5538554191589355
I0307 22:48:37.278356 140197572364032 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.65956974029541, loss=1.6042944192886353
I0307 22:49:16.919668 140197563971328 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.485930919647217, loss=1.5456867218017578
I0307 22:49:56.536843 140197572364032 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.815981388092041, loss=1.5447344779968262
I0307 22:50:36.024184 140197563971328 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.645123481750488, loss=1.504854679107666
I0307 22:51:15.873875 140197572364032 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.7004780769348145, loss=1.5026577711105347
I0307 22:51:55.792658 140197563971328 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.959394454956055, loss=1.4453253746032715
I0307 22:52:35.180162 140197572364032 logging_writer.py:48] [115000] global_step=115000, grad_norm=4.817796230316162, loss=1.5891038179397583
I0307 22:53:05.792307 140352918893760 spec.py:321] Evaluating on the training split.
2025-03-07 22:53:09.085777: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:53:20.107730 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 22:54:02.147813 140352918893760 spec.py:349] Evaluating on the test split.
I0307 22:54:03.871103 140352918893760 submission_runner.py:469] Time since start: 49375.03s, 	Step: 115079, 	{'train/accuracy': 0.7469706535339355, 'train/loss': 0.9639348387718201, 'validation/accuracy': 0.6804599761962891, 'validation/loss': 1.2937437295913696, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 1.9953489303588867, 'test/num_examples': 10000, 'score': 44930.846220731735, 'total_duration': 49375.03449702263, 'accumulated_submission_time': 44930.846220731735, 'accumulated_eval_time': 4419.123460292816, 'accumulated_logging_time': 12.49951982498169}
I0307 22:54:04.046109 140197563971328 logging_writer.py:48] [115079] accumulated_eval_time=4419.12, accumulated_logging_time=12.4995, accumulated_submission_time=44930.8, global_step=115079, preemption_count=0, score=44930.8, test/accuracy=0.5505, test/loss=1.99535, test/num_examples=10000, total_duration=49375, train/accuracy=0.746971, train/loss=0.963935, validation/accuracy=0.68046, validation/loss=1.29374, validation/num_examples=50000
I0307 22:54:12.800753 140197572364032 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.8877058029174805, loss=1.4411537647247314
I0307 22:54:52.231041 140197563971328 logging_writer.py:48] [115200] global_step=115200, grad_norm=5.659228324890137, loss=1.4617317914962769
I0307 22:55:32.107809 140197572364032 logging_writer.py:48] [115300] global_step=115300, grad_norm=5.102512836456299, loss=1.5514469146728516
I0307 22:56:11.419786 140197563971328 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.684344291687012, loss=1.6064703464508057
I0307 22:56:50.742006 140197572364032 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.655701637268066, loss=1.5687484741210938
I0307 22:57:30.457869 140197563971328 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.503485202789307, loss=1.4771887063980103
I0307 22:58:09.729197 140197572364032 logging_writer.py:48] [115700] global_step=115700, grad_norm=5.218381404876709, loss=1.4226969480514526
I0307 22:58:49.321387 140197563971328 logging_writer.py:48] [115800] global_step=115800, grad_norm=5.0302300453186035, loss=1.503283143043518
I0307 22:59:28.552192 140197572364032 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.884820461273193, loss=1.5240768194198608
I0307 23:00:08.031321 140197563971328 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.622831344604492, loss=1.4398304224014282
I0307 23:00:47.976171 140197572364032 logging_writer.py:48] [116100] global_step=116100, grad_norm=5.054311752319336, loss=1.4935133457183838
I0307 23:01:27.897494 140197563971328 logging_writer.py:48] [116200] global_step=116200, grad_norm=5.022232532501221, loss=1.481408953666687
I0307 23:02:06.928580 140197572364032 logging_writer.py:48] [116300] global_step=116300, grad_norm=5.212212085723877, loss=1.523759126663208
2025-03-07 23:02:21.515715: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:02:34.010416 140352918893760 spec.py:321] Evaluating on the training split.
I0307 23:02:46.421975 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 23:03:39.623410 140352918893760 spec.py:349] Evaluating on the test split.
I0307 23:03:41.591395 140352918893760 submission_runner.py:469] Time since start: 49952.75s, 	Step: 116370, 	{'train/accuracy': 0.752351701259613, 'train/loss': 0.9424148797988892, 'validation/accuracy': 0.6810399889945984, 'validation/loss': 1.2949098348617554, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 2.0026252269744873, 'test/num_examples': 10000, 'score': 45440.590339422226, 'total_duration': 49952.754564762115, 'accumulated_submission_time': 45440.590339422226, 'accumulated_eval_time': 4486.704003810883, 'accumulated_logging_time': 12.73878288269043}
I0307 23:03:41.773046 140197563971328 logging_writer.py:48] [116370] accumulated_eval_time=4486.7, accumulated_logging_time=12.7388, accumulated_submission_time=45440.6, global_step=116370, preemption_count=0, score=45440.6, test/accuracy=0.553, test/loss=2.00263, test/num_examples=10000, total_duration=49952.8, train/accuracy=0.752352, train/loss=0.942415, validation/accuracy=0.68104, validation/loss=1.29491, validation/num_examples=50000
I0307 23:03:53.965832 140197572364032 logging_writer.py:48] [116400] global_step=116400, grad_norm=5.3480448722839355, loss=1.56950044631958
I0307 23:04:33.041739 140197563971328 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.914296627044678, loss=1.3861668109893799
I0307 23:05:12.176796 140197572364032 logging_writer.py:48] [116600] global_step=116600, grad_norm=5.543606758117676, loss=1.5778958797454834
I0307 23:05:51.891108 140197563971328 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.73140811920166, loss=1.4463367462158203
I0307 23:06:31.197008 140197572364032 logging_writer.py:48] [116800] global_step=116800, grad_norm=5.187051773071289, loss=1.5228008031845093
I0307 23:07:10.365557 140197563971328 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.746855735778809, loss=1.5080461502075195
I0307 23:07:49.807812 140197572364032 logging_writer.py:48] [117000] global_step=117000, grad_norm=5.139358043670654, loss=1.476635456085205
I0307 23:08:28.917182 140197563971328 logging_writer.py:48] [117100] global_step=117100, grad_norm=4.779387950897217, loss=1.5363595485687256
I0307 23:09:08.064925 140197572364032 logging_writer.py:48] [117200] global_step=117200, grad_norm=5.054074764251709, loss=1.4634920358657837
I0307 23:09:47.458484 140197563971328 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.67836856842041, loss=1.4002132415771484
I0307 23:10:26.827577 140197572364032 logging_writer.py:48] [117400] global_step=117400, grad_norm=5.077397346496582, loss=1.4161525964736938
I0307 23:11:06.998645 140197563971328 logging_writer.py:48] [117500] global_step=117500, grad_norm=5.412546634674072, loss=1.5232770442962646
2025-03-07 23:11:39.369253: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:11:46.953515 140197572364032 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.984197616577148, loss=1.5958466529846191
I0307 23:12:12.003163 140352918893760 spec.py:321] Evaluating on the training split.
I0307 23:12:25.134048 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 23:13:13.808855 140352918893760 spec.py:349] Evaluating on the test split.
I0307 23:13:15.514977 140352918893760 submission_runner.py:469] Time since start: 50526.68s, 	Step: 117665, 	{'train/accuracy': 0.7501992583274841, 'train/loss': 0.9420855045318604, 'validation/accuracy': 0.6777999997138977, 'validation/loss': 1.2976300716400146, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.0283420085906982, 'test/num_examples': 10000, 'score': 45950.57492518425, 'total_duration': 50526.678396224976, 'accumulated_submission_time': 45950.57492518425, 'accumulated_eval_time': 4550.2156291008, 'accumulated_logging_time': 13.008969783782959}
I0307 23:13:15.681586 140197563971328 logging_writer.py:48] [117665] accumulated_eval_time=4550.22, accumulated_logging_time=13.009, accumulated_submission_time=45950.6, global_step=117665, preemption_count=0, score=45950.6, test/accuracy=0.5512, test/loss=2.02834, test/num_examples=10000, total_duration=50526.7, train/accuracy=0.750199, train/loss=0.942086, validation/accuracy=0.6778, validation/loss=1.29763, validation/num_examples=50000
I0307 23:13:29.490548 140197572364032 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.9577531814575195, loss=1.4661892652511597
I0307 23:14:08.819741 140197563971328 logging_writer.py:48] [117800] global_step=117800, grad_norm=5.324298858642578, loss=1.5484449863433838
I0307 23:14:48.468590 140197572364032 logging_writer.py:48] [117900] global_step=117900, grad_norm=5.38557767868042, loss=1.499650239944458
I0307 23:15:27.687370 140197563971328 logging_writer.py:48] [118000] global_step=118000, grad_norm=5.266849994659424, loss=1.5274548530578613
I0307 23:16:06.782740 140197572364032 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.501947402954102, loss=1.480968713760376
I0307 23:16:45.949990 140197563971328 logging_writer.py:48] [118200] global_step=118200, grad_norm=5.08671760559082, loss=1.5071297883987427
I0307 23:17:24.962889 140197572364032 logging_writer.py:48] [118300] global_step=118300, grad_norm=5.1075239181518555, loss=1.4486268758773804
I0307 23:18:04.365425 140197563971328 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.486217498779297, loss=1.4129602909088135
I0307 23:18:44.199609 140197572364032 logging_writer.py:48] [118500] global_step=118500, grad_norm=5.136904239654541, loss=1.531112551689148
I0307 23:19:23.903561 140197563971328 logging_writer.py:48] [118600] global_step=118600, grad_norm=5.24462890625, loss=1.3807191848754883
I0307 23:20:03.000367 140197572364032 logging_writer.py:48] [118700] global_step=118700, grad_norm=5.6332879066467285, loss=1.5205647945404053
I0307 23:20:41.485913 140197563971328 logging_writer.py:48] [118800] global_step=118800, grad_norm=5.323457717895508, loss=1.4948137998580933
2025-03-07 23:20:56.820937: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:21:20.054487 140197572364032 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.885050296783447, loss=1.4431164264678955
I0307 23:21:45.548679 140352918893760 spec.py:321] Evaluating on the training split.
I0307 23:21:58.756764 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 23:22:39.901794 140352918893760 spec.py:349] Evaluating on the test split.
I0307 23:22:41.612778 140352918893760 submission_runner.py:469] Time since start: 51092.78s, 	Step: 118966, 	{'train/accuracy': 0.74125075340271, 'train/loss': 0.979218065738678, 'validation/accuracy': 0.6746799945831299, 'validation/loss': 1.3188567161560059, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.018056869506836, 'test/num_examples': 10000, 'score': 46460.261176109314, 'total_duration': 51092.77618813515, 'accumulated_submission_time': 46460.261176109314, 'accumulated_eval_time': 4606.27952837944, 'accumulated_logging_time': 13.200393438339233}
I0307 23:22:41.734045 140197563971328 logging_writer.py:48] [118966] accumulated_eval_time=4606.28, accumulated_logging_time=13.2004, accumulated_submission_time=46460.3, global_step=118966, preemption_count=0, score=46460.3, test/accuracy=0.5482, test/loss=2.01806, test/num_examples=10000, total_duration=51092.8, train/accuracy=0.741251, train/loss=0.979218, validation/accuracy=0.67468, validation/loss=1.31886, validation/num_examples=50000
I0307 23:22:55.430025 140197572364032 logging_writer.py:48] [119000] global_step=119000, grad_norm=5.02342414855957, loss=1.505749225616455
I0307 23:23:34.922223 140197563971328 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.083811283111572, loss=1.4892244338989258
I0307 23:24:14.497807 140197572364032 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.84079122543335, loss=1.416419506072998
I0307 23:24:53.779848 140197563971328 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.4073100090026855, loss=1.537208080291748
I0307 23:25:33.191017 140197572364032 logging_writer.py:48] [119400] global_step=119400, grad_norm=5.2345123291015625, loss=1.5408343076705933
I0307 23:26:12.358474 140197563971328 logging_writer.py:48] [119500] global_step=119500, grad_norm=5.751801013946533, loss=1.5628421306610107
I0307 23:26:51.730016 140197572364032 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.770509719848633, loss=1.4214893579483032
I0307 23:27:30.998599 140197563971328 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.60249137878418, loss=1.431679606437683
I0307 23:28:10.435153 140197572364032 logging_writer.py:48] [119800] global_step=119800, grad_norm=5.061939239501953, loss=1.427575707435608
I0307 23:28:49.610104 140197563971328 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.716914176940918, loss=1.4167706966400146
I0307 23:29:29.547411 140197572364032 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.320659160614014, loss=1.4416496753692627
2025-03-07 23:30:03.261682: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:30:09.337152 140197563971328 logging_writer.py:48] [120100] global_step=120100, grad_norm=5.617049694061279, loss=1.4665571451187134
I0307 23:30:48.929045 140197572364032 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.838629245758057, loss=1.4051417112350464
I0307 23:31:11.804848 140352918893760 spec.py:321] Evaluating on the training split.
I0307 23:31:24.226371 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 23:32:02.607862 140352918893760 spec.py:349] Evaluating on the test split.
I0307 23:32:04.315063 140352918893760 submission_runner.py:469] Time since start: 51655.48s, 	Step: 120258, 	{'train/accuracy': 0.7507971525192261, 'train/loss': 0.9507015347480774, 'validation/accuracy': 0.6778199672698975, 'validation/loss': 1.3064419031143188, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.0099639892578125, 'test/num_examples': 10000, 'score': 46970.10282039642, 'total_duration': 51655.478496313095, 'accumulated_submission_time': 46970.10282039642, 'accumulated_eval_time': 4658.789568901062, 'accumulated_logging_time': 13.395655155181885}
I0307 23:32:04.424907 140197563971328 logging_writer.py:48] [120258] accumulated_eval_time=4658.79, accumulated_logging_time=13.3957, accumulated_submission_time=46970.1, global_step=120258, preemption_count=0, score=46970.1, test/accuracy=0.5487, test/loss=2.00996, test/num_examples=10000, total_duration=51655.5, train/accuracy=0.750797, train/loss=0.950702, validation/accuracy=0.67782, validation/loss=1.30644, validation/num_examples=50000
I0307 23:32:21.194797 140197572364032 logging_writer.py:48] [120300] global_step=120300, grad_norm=4.77028751373291, loss=1.4708346128463745
I0307 23:33:00.088111 140197563971328 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.574665546417236, loss=1.434984803199768
I0307 23:33:39.434773 140197572364032 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.434487819671631, loss=1.5205105543136597
I0307 23:34:18.786150 140197563971328 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.962950229644775, loss=1.4240087270736694
I0307 23:34:57.604367 140197572364032 logging_writer.py:48] [120700] global_step=120700, grad_norm=5.115294933319092, loss=1.4267892837524414
I0307 23:35:36.825847 140197563971328 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.778198719024658, loss=1.4106309413909912
I0307 23:36:16.147256 140197572364032 logging_writer.py:48] [120900] global_step=120900, grad_norm=5.016523838043213, loss=1.3608602285385132
I0307 23:36:56.022156 140197563971328 logging_writer.py:48] [121000] global_step=121000, grad_norm=5.420664310455322, loss=1.331916093826294
I0307 23:37:35.712011 140197572364032 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.609866619110107, loss=1.5680980682373047
I0307 23:38:15.421725 140197563971328 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.981783390045166, loss=1.4048182964324951
I0307 23:38:55.557069 140197572364032 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.157931327819824, loss=1.5252550840377808
2025-03-07 23:39:09.751930: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:39:35.385664 140197563971328 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.0091023445129395, loss=1.4388675689697266
I0307 23:40:14.888709 140197572364032 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.0045294761657715, loss=1.579358458518982
I0307 23:40:34.418853 140352918893760 spec.py:321] Evaluating on the training split.
I0307 23:40:46.624128 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 23:41:28.141018 140352918893760 spec.py:349] Evaluating on the test split.
I0307 23:41:29.867882 140352918893760 submission_runner.py:469] Time since start: 52221.03s, 	Step: 121551, 	{'train/accuracy': 0.7558394074440002, 'train/loss': 0.9138184785842896, 'validation/accuracy': 0.6812799572944641, 'validation/loss': 1.2765713930130005, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 1.9922151565551758, 'test/num_examples': 10000, 'score': 47479.90102767944, 'total_duration': 52221.03129673004, 'accumulated_submission_time': 47479.90102767944, 'accumulated_eval_time': 4714.238405942917, 'accumulated_logging_time': 13.545711755752563}
I0307 23:41:29.978976 140197563971328 logging_writer.py:48] [121551] accumulated_eval_time=4714.24, accumulated_logging_time=13.5457, accumulated_submission_time=47479.9, global_step=121551, preemption_count=0, score=47479.9, test/accuracy=0.5535, test/loss=1.99222, test/num_examples=10000, total_duration=52221, train/accuracy=0.755839, train/loss=0.913818, validation/accuracy=0.68128, validation/loss=1.27657, validation/num_examples=50000
I0307 23:41:49.894719 140197572364032 logging_writer.py:48] [121600] global_step=121600, grad_norm=5.610227108001709, loss=1.4013094902038574
I0307 23:42:28.790402 140197563971328 logging_writer.py:48] [121700] global_step=121700, grad_norm=6.12393045425415, loss=1.5745060443878174
I0307 23:43:07.891333 140197572364032 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.436978340148926, loss=1.5437266826629639
I0307 23:43:46.670366 140197563971328 logging_writer.py:48] [121900] global_step=121900, grad_norm=5.245317459106445, loss=1.5808773040771484
I0307 23:44:25.756311 140197572364032 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.969018936157227, loss=1.5250219106674194
I0307 23:45:05.091399 140197563971328 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.779489040374756, loss=1.3293170928955078
I0307 23:45:44.845345 140197572364032 logging_writer.py:48] [122200] global_step=122200, grad_norm=5.298003196716309, loss=1.41787850856781
I0307 23:46:24.016265 140197563971328 logging_writer.py:48] [122300] global_step=122300, grad_norm=5.385561943054199, loss=1.525750756263733
I0307 23:47:03.498481 140197572364032 logging_writer.py:48] [122400] global_step=122400, grad_norm=5.564084053039551, loss=1.435746669769287
I0307 23:47:43.412768 140197563971328 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.34987211227417, loss=1.428877830505371
2025-03-07 23:48:16.815523: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:48:22.837067 140197572364032 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.738375186920166, loss=1.4294519424438477
I0307 23:49:02.709403 140197563971328 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.266502857208252, loss=1.4062886238098145
I0307 23:49:42.056672 140197572364032 logging_writer.py:48] [122800] global_step=122800, grad_norm=5.0258378982543945, loss=1.406518578529358
I0307 23:50:00.068348 140352918893760 spec.py:321] Evaluating on the training split.
I0307 23:50:11.979525 140352918893760 spec.py:333] Evaluating on the validation split.
I0307 23:51:02.055706 140352918893760 spec.py:349] Evaluating on the test split.
I0307 23:51:03.765301 140352918893760 submission_runner.py:469] Time since start: 52794.93s, 	Step: 122846, 	{'train/accuracy': 0.7626155614852905, 'train/loss': 0.8847436308860779, 'validation/accuracy': 0.6919199824333191, 'validation/loss': 1.2373853921890259, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 1.9326844215393066, 'test/num_examples': 10000, 'score': 47989.78750681877, 'total_duration': 52794.92872285843, 'accumulated_submission_time': 47989.78750681877, 'accumulated_eval_time': 4777.935172319412, 'accumulated_logging_time': 13.705866813659668}
I0307 23:51:04.025954 140197563971328 logging_writer.py:48] [122846] accumulated_eval_time=4777.94, accumulated_logging_time=13.7059, accumulated_submission_time=47989.8, global_step=122846, preemption_count=0, score=47989.8, test/accuracy=0.5612, test/loss=1.93268, test/num_examples=10000, total_duration=52794.9, train/accuracy=0.762616, train/loss=0.884744, validation/accuracy=0.69192, validation/loss=1.23739, validation/num_examples=50000
I0307 23:51:25.276636 140197572364032 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.9910454750061035, loss=1.4080018997192383
I0307 23:52:04.604153 140197563971328 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.89457893371582, loss=1.4405757188796997
I0307 23:52:43.711720 140197572364032 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.8173828125, loss=1.4019604921340942
I0307 23:53:23.257683 140197563971328 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.367553234100342, loss=1.5140304565429688
I0307 23:54:02.487401 140197572364032 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.1683669090271, loss=1.4639469385147095
I0307 23:54:41.945174 140197563971328 logging_writer.py:48] [123400] global_step=123400, grad_norm=5.311310291290283, loss=1.4061843156814575
I0307 23:55:21.006927 140197572364032 logging_writer.py:48] [123500] global_step=123500, grad_norm=5.063728332519531, loss=1.382042646408081
I0307 23:55:59.958427 140197563971328 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.070561408996582, loss=1.4858019351959229
I0307 23:56:39.470071 140197572364032 logging_writer.py:48] [123700] global_step=123700, grad_norm=5.331174850463867, loss=1.538008689880371
I0307 23:57:19.164539 140197563971328 logging_writer.py:48] [123800] global_step=123800, grad_norm=4.875361442565918, loss=1.3860241174697876
2025-03-07 23:57:34.050152: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:57:58.882190 140197572364032 logging_writer.py:48] [123900] global_step=123900, grad_norm=5.013205051422119, loss=1.4374141693115234
I0307 23:58:37.913977 140197563971328 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.19835901260376, loss=1.4645665884017944
I0307 23:59:16.849869 140197572364032 logging_writer.py:48] [124100] global_step=124100, grad_norm=4.898314476013184, loss=1.4870598316192627
I0307 23:59:34.133652 140352918893760 spec.py:321] Evaluating on the training split.
I0307 23:59:46.945834 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 00:00:33.645908 140352918893760 spec.py:349] Evaluating on the test split.
I0308 00:00:35.393855 140352918893760 submission_runner.py:469] Time since start: 53366.56s, 	Step: 124145, 	{'train/accuracy': 0.7470105290412903, 'train/loss': 0.9557898044586182, 'validation/accuracy': 0.6754599809646606, 'validation/loss': 1.3239905834197998, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.0508055686950684, 'test/num_examples': 10000, 'score': 48499.69542980194, 'total_duration': 53366.55727934837, 'accumulated_submission_time': 48499.69542980194, 'accumulated_eval_time': 4839.195190668106, 'accumulated_logging_time': 14.011986255645752}
I0308 00:00:35.568181 140197563971328 logging_writer.py:48] [124145] accumulated_eval_time=4839.2, accumulated_logging_time=14.012, accumulated_submission_time=48499.7, global_step=124145, preemption_count=0, score=48499.7, test/accuracy=0.5459, test/loss=2.05081, test/num_examples=10000, total_duration=53366.6, train/accuracy=0.747011, train/loss=0.95579, validation/accuracy=0.67546, validation/loss=1.32399, validation/num_examples=50000
I0308 00:00:57.638125 140197572364032 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.686579704284668, loss=1.490408182144165
I0308 00:01:36.638950 140197563971328 logging_writer.py:48] [124300] global_step=124300, grad_norm=5.669740676879883, loss=1.319345474243164
I0308 00:02:15.593127 140197572364032 logging_writer.py:48] [124400] global_step=124400, grad_norm=5.168915748596191, loss=1.5610430240631104
I0308 00:02:54.491216 140197563971328 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.3260016441345215, loss=1.3992037773132324
I0308 00:03:33.281814 140197572364032 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.614140510559082, loss=1.348653793334961
I0308 00:04:12.097836 140197563971328 logging_writer.py:48] [124700] global_step=124700, grad_norm=5.729813098907471, loss=1.525162696838379
I0308 00:04:51.179472 140197572364032 logging_writer.py:48] [124800] global_step=124800, grad_norm=5.342367172241211, loss=1.4283374547958374
I0308 00:05:30.365697 140197563971328 logging_writer.py:48] [124900] global_step=124900, grad_norm=6.081456661224365, loss=1.4241628646850586
I0308 00:06:09.749635 140197572364032 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.769631862640381, loss=1.577404499053955
2025-03-08 00:06:43.733074: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:06:49.280726 140197563971328 logging_writer.py:48] [125100] global_step=125100, grad_norm=4.995106220245361, loss=1.367156982421875
I0308 00:07:28.377017 140197572364032 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.2139434814453125, loss=1.487389087677002
I0308 00:08:07.284490 140197563971328 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.322447776794434, loss=1.486998200416565
I0308 00:08:46.292172 140197572364032 logging_writer.py:48] [125400] global_step=125400, grad_norm=5.306240558624268, loss=1.4801839590072632
I0308 00:09:05.394967 140352918893760 spec.py:321] Evaluating on the training split.
I0308 00:09:17.191452 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 00:09:56.347726 140352918893760 spec.py:349] Evaluating on the test split.
I0308 00:09:58.078836 140352918893760 submission_runner.py:469] Time since start: 53929.24s, 	Step: 125451, 	{'train/accuracy': 0.7651067972183228, 'train/loss': 0.8890909552574158, 'validation/accuracy': 0.6888799667358398, 'validation/loss': 1.2617881298065186, 'validation/num_examples': 50000, 'test/accuracy': 0.5600000023841858, 'test/loss': 1.9780079126358032, 'test/num_examples': 10000, 'score': 49009.34534573555, 'total_duration': 53929.24225854874, 'accumulated_submission_time': 49009.34534573555, 'accumulated_eval_time': 4891.87887430191, 'accumulated_logging_time': 14.207602500915527}
I0308 00:09:58.242846 140197563971328 logging_writer.py:48] [125451] accumulated_eval_time=4891.88, accumulated_logging_time=14.2076, accumulated_submission_time=49009.3, global_step=125451, preemption_count=0, score=49009.3, test/accuracy=0.56, test/loss=1.97801, test/num_examples=10000, total_duration=53929.2, train/accuracy=0.765107, train/loss=0.889091, validation/accuracy=0.68888, validation/loss=1.26179, validation/num_examples=50000
I0308 00:10:17.845447 140197572364032 logging_writer.py:48] [125500] global_step=125500, grad_norm=4.759823322296143, loss=1.4342104196548462
I0308 00:10:57.182952 140197563971328 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.537834167480469, loss=1.4687997102737427
I0308 00:11:36.179977 140197572364032 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.398481845855713, loss=1.4098329544067383
I0308 00:12:15.323900 140197563971328 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.171131610870361, loss=1.6120450496673584
I0308 00:12:54.365768 140197572364032 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.291572570800781, loss=1.485477328300476
I0308 00:13:33.430531 140197563971328 logging_writer.py:48] [126000] global_step=126000, grad_norm=6.071686267852783, loss=1.3964118957519531
I0308 00:14:12.566994 140197572364032 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.189353942871094, loss=1.5519235134124756
I0308 00:14:52.321482 140197563971328 logging_writer.py:48] [126200] global_step=126200, grad_norm=5.880537986755371, loss=1.3872274160385132
I0308 00:15:32.071313 140197572364032 logging_writer.py:48] [126300] global_step=126300, grad_norm=5.609498023986816, loss=1.5234246253967285
2025-03-08 00:15:46.552833: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:16:11.374098 140197563971328 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.412103176116943, loss=1.4824249744415283
I0308 00:16:50.712087 140197572364032 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.347924709320068, loss=1.473036527633667
I0308 00:17:29.839851 140197563971328 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.132074356079102, loss=1.477212905883789
I0308 00:18:08.886305 140197572364032 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.507303714752197, loss=1.3459999561309814
I0308 00:18:28.316841 140352918893760 spec.py:321] Evaluating on the training split.
I0308 00:18:40.725436 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 00:19:23.156180 140352918893760 spec.py:349] Evaluating on the test split.
I0308 00:19:24.920682 140352918893760 submission_runner.py:469] Time since start: 54496.08s, 	Step: 126750, 	{'train/accuracy': 0.7654854655265808, 'train/loss': 0.8835881352424622, 'validation/accuracy': 0.6863999962806702, 'validation/loss': 1.2579063177108765, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 1.9555834531784058, 'test/num_examples': 10000, 'score': 49519.23848557472, 'total_duration': 54496.08412384987, 'accumulated_submission_time': 49519.23848557472, 'accumulated_eval_time': 4948.482552051544, 'accumulated_logging_time': 14.394435405731201}
I0308 00:19:25.034787 140197563971328 logging_writer.py:48] [126750] accumulated_eval_time=4948.48, accumulated_logging_time=14.3944, accumulated_submission_time=49519.2, global_step=126750, preemption_count=0, score=49519.2, test/accuracy=0.5607, test/loss=1.95558, test/num_examples=10000, total_duration=54496.1, train/accuracy=0.765485, train/loss=0.883588, validation/accuracy=0.6864, validation/loss=1.25791, validation/num_examples=50000
I0308 00:19:44.689898 140197572364032 logging_writer.py:48] [126800] global_step=126800, grad_norm=4.8947858810424805, loss=1.4194165468215942
I0308 00:20:24.012935 140197563971328 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.502325534820557, loss=1.3496102094650269
I0308 00:21:03.150978 140197572364032 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.078199863433838, loss=1.3785263299942017
I0308 00:21:41.919299 140197563971328 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.918947696685791, loss=1.4097636938095093
I0308 00:22:20.790062 140197572364032 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.3937764167785645, loss=1.5257587432861328
I0308 00:22:59.492423 140197563971328 logging_writer.py:48] [127300] global_step=127300, grad_norm=5.285641670227051, loss=1.341434121131897
I0308 00:23:39.235262 140197572364032 logging_writer.py:48] [127400] global_step=127400, grad_norm=5.332777976989746, loss=1.4677003622055054
I0308 00:24:18.686949 140197563971328 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.675073623657227, loss=1.4453468322753906
2025-03-08 00:24:53.691208: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:24:58.313393 140197572364032 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.089442253112793, loss=1.3685855865478516
I0308 00:25:37.569474 140197563971328 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.246029853820801, loss=1.3816524744033813
I0308 00:26:16.249258 140197572364032 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.407638072967529, loss=1.3810083866119385
I0308 00:26:55.547685 140197563971328 logging_writer.py:48] [127900] global_step=127900, grad_norm=6.485782623291016, loss=1.4532856941223145
I0308 00:27:34.585403 140197572364032 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.252985954284668, loss=1.3804163932800293
I0308 00:27:55.196799 140352918893760 spec.py:321] Evaluating on the training split.
I0308 00:28:08.078657 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 00:28:56.005717 140352918893760 spec.py:349] Evaluating on the test split.
I0308 00:28:57.733458 140352918893760 submission_runner.py:469] Time since start: 55068.90s, 	Step: 128054, 	{'train/accuracy': 0.7738759517669678, 'train/loss': 0.8408008813858032, 'validation/accuracy': 0.6960999965667725, 'validation/loss': 1.2316733598709106, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 1.9281764030456543, 'test/num_examples': 10000, 'score': 50029.22094273567, 'total_duration': 55068.89689993858, 'accumulated_submission_time': 50029.22094273567, 'accumulated_eval_time': 5011.019055843353, 'accumulated_logging_time': 14.529919147491455}
I0308 00:28:57.824765 140197563971328 logging_writer.py:48] [128054] accumulated_eval_time=5011.02, accumulated_logging_time=14.5299, accumulated_submission_time=50029.2, global_step=128054, preemption_count=0, score=50029.2, test/accuracy=0.5695, test/loss=1.92818, test/num_examples=10000, total_duration=55068.9, train/accuracy=0.773876, train/loss=0.840801, validation/accuracy=0.6961, validation/loss=1.23167, validation/num_examples=50000
I0308 00:29:16.345205 140197572364032 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.03032922744751, loss=1.3012707233428955
I0308 00:29:55.578522 140197563971328 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.298133850097656, loss=1.4568347930908203
I0308 00:30:34.424530 140197572364032 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.709423065185547, loss=1.4465744495391846
I0308 00:31:13.472091 140197563971328 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.164055347442627, loss=1.3764023780822754
I0308 00:31:52.324165 140197572364032 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.288112640380859, loss=1.3972957134246826
I0308 00:32:31.000069 140197563971328 logging_writer.py:48] [128600] global_step=128600, grad_norm=5.32354211807251, loss=1.3987911939620972
I0308 00:33:10.837170 140197572364032 logging_writer.py:48] [128700] global_step=128700, grad_norm=5.608670711517334, loss=1.4096800088882446
I0308 00:33:50.518988 140197563971328 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.522537708282471, loss=1.394497036933899
2025-03-08 00:34:05.819671: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:34:29.916020 140197572364032 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.327018737792969, loss=1.318296194076538
I0308 00:35:08.956857 140197563971328 logging_writer.py:48] [129000] global_step=129000, grad_norm=5.456127166748047, loss=1.3890223503112793
I0308 00:35:48.098704 140197572364032 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.650144100189209, loss=1.4432182312011719
I0308 00:36:26.545279 140197563971328 logging_writer.py:48] [129200] global_step=129200, grad_norm=5.4863080978393555, loss=1.374987006187439
I0308 00:37:05.646746 140197572364032 logging_writer.py:48] [129300] global_step=129300, grad_norm=6.266258716583252, loss=1.4347965717315674
I0308 00:37:27.778474 140352918893760 spec.py:321] Evaluating on the training split.
I0308 00:37:40.279603 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 00:38:24.948241 140352918893760 spec.py:349] Evaluating on the test split.
I0308 00:38:26.704663 140352918893760 submission_runner.py:469] Time since start: 55637.87s, 	Step: 129357, 	{'train/accuracy': 0.7737165093421936, 'train/loss': 0.8375166654586792, 'validation/accuracy': 0.695580005645752, 'validation/loss': 1.2336235046386719, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 1.94514799118042, 'test/num_examples': 10000, 'score': 50538.982206106186, 'total_duration': 55637.86808180809, 'accumulated_submission_time': 50538.982206106186, 'accumulated_eval_time': 5069.94505405426, 'accumulated_logging_time': 14.655816316604614}
I0308 00:38:26.848176 140197563971328 logging_writer.py:48] [129357] accumulated_eval_time=5069.95, accumulated_logging_time=14.6558, accumulated_submission_time=50539, global_step=129357, preemption_count=0, score=50539, test/accuracy=0.5659, test/loss=1.94515, test/num_examples=10000, total_duration=55637.9, train/accuracy=0.773717, train/loss=0.837517, validation/accuracy=0.69558, validation/loss=1.23362, validation/num_examples=50000
I0308 00:38:44.185556 140197572364032 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.49783182144165, loss=1.3785499334335327
I0308 00:39:22.931794 140197563971328 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.593453884124756, loss=1.3401386737823486
I0308 00:40:02.067681 140197572364032 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.599920272827148, loss=1.4631717205047607
I0308 00:40:40.849630 140197563971328 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.484930038452148, loss=1.444967269897461
I0308 00:41:20.138193 140197572364032 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.236137866973877, loss=1.3672620058059692
I0308 00:41:59.072753 140197563971328 logging_writer.py:48] [129900] global_step=129900, grad_norm=5.448596000671387, loss=1.3510432243347168
I0308 00:42:38.978098 140197572364032 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.566078186035156, loss=1.4232333898544312
2025-03-08 00:43:15.072988: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:43:18.668373 140197563971328 logging_writer.py:48] [130100] global_step=130100, grad_norm=6.1516008377075195, loss=1.3791035413742065
I0308 00:43:58.284468 140197572364032 logging_writer.py:48] [130200] global_step=130200, grad_norm=5.248196125030518, loss=1.350071668624878
I0308 00:44:37.004335 140197563971328 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.8842597007751465, loss=1.3700355291366577
I0308 00:45:16.375700 140197572364032 logging_writer.py:48] [130400] global_step=130400, grad_norm=5.17646598815918, loss=1.3777741193771362
I0308 00:45:55.657226 140197563971328 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.835306644439697, loss=1.4261690378189087
I0308 00:46:35.127066 140197572364032 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.4839301109313965, loss=1.302582025527954
I0308 00:46:56.971579 140352918893760 spec.py:321] Evaluating on the training split.
I0308 00:47:09.649854 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 00:47:42.635801 140352918893760 spec.py:349] Evaluating on the test split.
I0308 00:47:44.367053 140352918893760 submission_runner.py:469] Time since start: 56195.53s, 	Step: 130657, 	{'train/accuracy': 0.7756297588348389, 'train/loss': 0.8288185596466064, 'validation/accuracy': 0.6969000101089478, 'validation/loss': 1.2263697385787964, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.9130133390426636, 'test/num_examples': 10000, 'score': 51048.91535425186, 'total_duration': 56195.530470371246, 'accumulated_submission_time': 51048.91535425186, 'accumulated_eval_time': 5117.340341091156, 'accumulated_logging_time': 14.831950902938843}
I0308 00:47:44.543670 140197563971328 logging_writer.py:48] [130657] accumulated_eval_time=5117.34, accumulated_logging_time=14.832, accumulated_submission_time=51048.9, global_step=130657, preemption_count=0, score=51048.9, test/accuracy=0.5681, test/loss=1.91301, test/num_examples=10000, total_duration=56195.5, train/accuracy=0.77563, train/loss=0.828819, validation/accuracy=0.6969, validation/loss=1.22637, validation/num_examples=50000
I0308 00:48:01.914439 140197572364032 logging_writer.py:48] [130700] global_step=130700, grad_norm=5.365302085876465, loss=1.3689781427383423
I0308 00:48:40.738506 140197563971328 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.715183258056641, loss=1.3098877668380737
I0308 00:49:19.587909 140197572364032 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.506217002868652, loss=1.3542404174804688
I0308 00:49:58.632215 140197563971328 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.768738269805908, loss=1.376787543296814
I0308 00:50:37.555516 140197572364032 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.3288750648498535, loss=1.307418704032898
I0308 00:51:17.459510 140197563971328 logging_writer.py:48] [131200] global_step=131200, grad_norm=5.514629364013672, loss=1.3868067264556885
I0308 00:51:57.104770 140197572364032 logging_writer.py:48] [131300] global_step=131300, grad_norm=6.1435394287109375, loss=1.38932466506958
2025-03-08 00:52:13.194590: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:52:36.443823 140197563971328 logging_writer.py:48] [131400] global_step=131400, grad_norm=5.649799823760986, loss=1.393625020980835
I0308 00:53:15.668683 140197572364032 logging_writer.py:48] [131500] global_step=131500, grad_norm=5.5299201011657715, loss=1.4049667119979858
I0308 00:53:54.953905 140197563971328 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.411604404449463, loss=1.2876923084259033
I0308 00:54:33.807684 140197572364032 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.376522541046143, loss=1.2718051671981812
I0308 00:55:13.093657 140197563971328 logging_writer.py:48] [131800] global_step=131800, grad_norm=6.203114032745361, loss=1.305762529373169
I0308 00:55:51.764257 140197572364032 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.426743507385254, loss=1.3683708906173706
I0308 00:56:14.637847 140352918893760 spec.py:321] Evaluating on the training split.
I0308 00:56:26.607160 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 00:57:15.256093 140352918893760 spec.py:349] Evaluating on the test split.
I0308 00:57:16.959872 140352918893760 submission_runner.py:469] Time since start: 56768.12s, 	Step: 131960, 	{'train/accuracy': 0.7819275856018066, 'train/loss': 0.8150589466094971, 'validation/accuracy': 0.698639988899231, 'validation/loss': 1.2118993997573853, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9055372476577759, 'test/num_examples': 10000, 'score': 51558.7761015892, 'total_duration': 56768.12331032753, 'accumulated_submission_time': 51558.7761015892, 'accumulated_eval_time': 5179.6621997356415, 'accumulated_logging_time': 15.08397388458252}
I0308 00:57:17.125191 140197563971328 logging_writer.py:48] [131960] accumulated_eval_time=5179.66, accumulated_logging_time=15.084, accumulated_submission_time=51558.8, global_step=131960, preemption_count=0, score=51558.8, test/accuracy=0.5707, test/loss=1.90554, test/num_examples=10000, total_duration=56768.1, train/accuracy=0.781928, train/loss=0.815059, validation/accuracy=0.69864, validation/loss=1.2119, validation/num_examples=50000
I0308 00:57:32.993653 140197572364032 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.633513450622559, loss=1.3716174364089966
I0308 00:58:11.710043 140197563971328 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.202090263366699, loss=1.4114683866500854
I0308 00:58:50.207544 140197572364032 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.340498447418213, loss=1.2984850406646729
I0308 00:59:29.368288 140197563971328 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.53329610824585, loss=1.3167214393615723
I0308 01:00:08.282106 140197572364032 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.683018684387207, loss=1.4002177715301514
I0308 01:00:47.889925 140197563971328 logging_writer.py:48] [132500] global_step=132500, grad_norm=6.240456581115723, loss=1.4121272563934326
2025-03-08 01:01:24.507272: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:01:27.354093 140197572364032 logging_writer.py:48] [132600] global_step=132600, grad_norm=6.378056526184082, loss=1.4290082454681396
I0308 01:02:06.313794 140197563971328 logging_writer.py:48] [132700] global_step=132700, grad_norm=5.584873199462891, loss=1.4474445581436157
I0308 01:02:45.516436 140197572364032 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.9017157554626465, loss=1.3749852180480957
I0308 01:03:24.701794 140197563971328 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.4977264404296875, loss=1.4122101068496704
I0308 01:04:03.511089 140197572364032 logging_writer.py:48] [133000] global_step=133000, grad_norm=5.496682167053223, loss=1.3361879587173462
I0308 01:04:42.851000 140197563971328 logging_writer.py:48] [133100] global_step=133100, grad_norm=5.5662384033203125, loss=1.3494396209716797
I0308 01:05:21.867143 140197572364032 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.587473392486572, loss=1.451092004776001
I0308 01:05:47.236464 140352918893760 spec.py:321] Evaluating on the training split.
I0308 01:05:59.298600 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 01:06:46.845258 140352918893760 spec.py:349] Evaluating on the test split.
I0308 01:06:48.592918 140352918893760 submission_runner.py:469] Time since start: 57339.76s, 	Step: 133266, 	{'train/accuracy': 0.7813695669174194, 'train/loss': 0.8108658194541931, 'validation/accuracy': 0.7012400031089783, 'validation/loss': 1.2071820497512817, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.9240167140960693, 'test/num_examples': 10000, 'score': 52068.70831656456, 'total_duration': 57339.75633382797, 'accumulated_submission_time': 52068.70831656456, 'accumulated_eval_time': 5241.018486261368, 'accumulated_logging_time': 15.271233558654785}
I0308 01:06:48.709681 140197563971328 logging_writer.py:48] [133266] accumulated_eval_time=5241.02, accumulated_logging_time=15.2712, accumulated_submission_time=52068.7, global_step=133266, preemption_count=0, score=52068.7, test/accuracy=0.5764, test/loss=1.92402, test/num_examples=10000, total_duration=57339.8, train/accuracy=0.78137, train/loss=0.810866, validation/accuracy=0.70124, validation/loss=1.20718, validation/num_examples=50000
I0308 01:07:02.301311 140197572364032 logging_writer.py:48] [133300] global_step=133300, grad_norm=6.3480353355407715, loss=1.3309190273284912
I0308 01:07:41.316270 140197563971328 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.465007781982422, loss=1.3160275220870972
I0308 01:08:20.291654 140197572364032 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.744776725769043, loss=1.4224586486816406
I0308 01:08:59.100391 140197563971328 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.793722629547119, loss=1.3124339580535889
I0308 01:09:38.355580 140197572364032 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.192165374755859, loss=1.2970346212387085
I0308 01:10:17.845003 140197563971328 logging_writer.py:48] [133800] global_step=133800, grad_norm=5.515666484832764, loss=1.2719547748565674
2025-03-08 01:10:34.821490: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:10:56.822450 140197572364032 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.216433525085449, loss=1.3575392961502075
I0308 01:11:35.721629 140197563971328 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.476069927215576, loss=1.4606268405914307
I0308 01:12:14.279126 140197572364032 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.800975799560547, loss=1.2755279541015625
I0308 01:12:53.406742 140197563971328 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.837969779968262, loss=1.3606992959976196
I0308 01:13:32.308494 140197572364032 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.5126423835754395, loss=1.2643828392028809
I0308 01:14:10.790266 140197563971328 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.622984409332275, loss=1.2552138566970825
I0308 01:14:49.704073 140197572364032 logging_writer.py:48] [134500] global_step=134500, grad_norm=6.646966457366943, loss=1.3993417024612427
I0308 01:15:18.885941 140352918893760 spec.py:321] Evaluating on the training split.
I0308 01:15:31.077108 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 01:16:13.010135 140352918893760 spec.py:349] Evaluating on the test split.
I0308 01:16:14.716531 140352918893760 submission_runner.py:469] Time since start: 57905.88s, 	Step: 134576, 	{'train/accuracy': 0.7850366830825806, 'train/loss': 0.7943534851074219, 'validation/accuracy': 0.7020999789237976, 'validation/loss': 1.2008260488510132, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.8859989643096924, 'test/num_examples': 10000, 'score': 52578.6908454895, 'total_duration': 57905.87995100021, 'accumulated_submission_time': 52578.6908454895, 'accumulated_eval_time': 5296.84888958931, 'accumulated_logging_time': 15.424875497817993}
I0308 01:16:14.875931 140197563971328 logging_writer.py:48] [134576] accumulated_eval_time=5296.85, accumulated_logging_time=15.4249, accumulated_submission_time=52578.7, global_step=134576, preemption_count=0, score=52578.7, test/accuracy=0.5789, test/loss=1.886, test/num_examples=10000, total_duration=57905.9, train/accuracy=0.785037, train/loss=0.794353, validation/accuracy=0.7021, validation/loss=1.20083, validation/num_examples=50000
I0308 01:16:24.712648 140197572364032 logging_writer.py:48] [134600] global_step=134600, grad_norm=6.163879871368408, loss=1.3449639081954956
I0308 01:17:03.105442 140197563971328 logging_writer.py:48] [134700] global_step=134700, grad_norm=6.050121784210205, loss=1.326683759689331
I0308 01:17:42.675698 140197572364032 logging_writer.py:48] [134800] global_step=134800, grad_norm=5.521261692047119, loss=1.352749228477478
I0308 01:18:21.779265 140197563971328 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.269231796264648, loss=1.2990052700042725
I0308 01:19:01.262547 140197572364032 logging_writer.py:48] [135000] global_step=135000, grad_norm=6.367578983306885, loss=1.3084129095077515
2025-03-08 01:19:38.552495: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:19:40.905955 140197563971328 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.34402322769165, loss=1.3646421432495117
I0308 01:20:19.590106 140197572364032 logging_writer.py:48] [135200] global_step=135200, grad_norm=5.776732444763184, loss=1.3095561265945435
I0308 01:20:59.005367 140197563971328 logging_writer.py:48] [135300] global_step=135300, grad_norm=6.483758449554443, loss=1.3670796155929565
I0308 01:21:37.749165 140197572364032 logging_writer.py:48] [135400] global_step=135400, grad_norm=5.771261215209961, loss=1.3022319078445435
I0308 01:22:16.395913 140197563971328 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.625762939453125, loss=1.3743910789489746
I0308 01:22:55.291479 140197572364032 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.784189701080322, loss=1.347327709197998
I0308 01:23:34.168409 140197563971328 logging_writer.py:48] [135700] global_step=135700, grad_norm=6.258826732635498, loss=1.3326140642166138
I0308 01:24:13.690683 140197572364032 logging_writer.py:48] [135800] global_step=135800, grad_norm=6.464626312255859, loss=1.3256324529647827
I0308 01:24:44.805470 140352918893760 spec.py:321] Evaluating on the training split.
I0308 01:24:57.425676 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 01:25:46.323179 140352918893760 spec.py:349] Evaluating on the test split.
I0308 01:25:48.053246 140352918893760 submission_runner.py:469] Time since start: 58479.22s, 	Step: 135881, 	{'train/accuracy': 0.7904974222183228, 'train/loss': 0.7781922817230225, 'validation/accuracy': 0.7061399817466736, 'validation/loss': 1.1804100275039673, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.861924171447754, 'test/num_examples': 10000, 'score': 53088.44025182724, 'total_duration': 58479.216676712036, 'accumulated_submission_time': 53088.44025182724, 'accumulated_eval_time': 5360.096490859985, 'accumulated_logging_time': 15.60699462890625}
I0308 01:25:48.218607 140197563971328 logging_writer.py:48] [135881] accumulated_eval_time=5360.1, accumulated_logging_time=15.607, accumulated_submission_time=53088.4, global_step=135881, preemption_count=0, score=53088.4, test/accuracy=0.5805, test/loss=1.86192, test/num_examples=10000, total_duration=58479.2, train/accuracy=0.790497, train/loss=0.778192, validation/accuracy=0.70614, validation/loss=1.18041, validation/num_examples=50000
I0308 01:25:56.317230 140197572364032 logging_writer.py:48] [135900] global_step=135900, grad_norm=6.653109073638916, loss=1.337926983833313
I0308 01:26:35.156724 140197563971328 logging_writer.py:48] [136000] global_step=136000, grad_norm=6.296703338623047, loss=1.3408656120300293
I0308 01:27:14.122917 140197572364032 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.478250980377197, loss=1.2888325452804565
I0308 01:27:53.865001 140197563971328 logging_writer.py:48] [136200] global_step=136200, grad_norm=6.195115089416504, loss=1.4741169214248657
I0308 01:28:33.760310 140197572364032 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.957392692565918, loss=1.403493046760559
2025-03-08 01:28:51.773605: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:29:13.037562 140197563971328 logging_writer.py:48] [136400] global_step=136400, grad_norm=5.604378700256348, loss=1.3804320096969604
I0308 01:29:51.828511 140197572364032 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.263391494750977, loss=1.2834606170654297
I0308 01:30:31.125236 140197563971328 logging_writer.py:48] [136600] global_step=136600, grad_norm=6.2777180671691895, loss=1.4159231185913086
I0308 01:31:10.456934 140197572364032 logging_writer.py:48] [136700] global_step=136700, grad_norm=6.723665237426758, loss=1.3024107217788696
I0308 01:31:49.641957 140197563971328 logging_writer.py:48] [136800] global_step=136800, grad_norm=6.214902400970459, loss=1.2469947338104248
I0308 01:32:28.638609 140197572364032 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.7289652824401855, loss=1.27988600730896
I0308 01:33:07.997378 140197563971328 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.897470951080322, loss=1.2215094566345215
I0308 01:33:47.383055 140197572364032 logging_writer.py:48] [137100] global_step=137100, grad_norm=6.405747890472412, loss=1.3977211713790894
I0308 01:34:18.235316 140352918893760 spec.py:321] Evaluating on the training split.
I0308 01:34:30.495415 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 01:35:09.013006 140352918893760 spec.py:349] Evaluating on the test split.
I0308 01:35:10.728201 140352918893760 submission_runner.py:469] Time since start: 59041.89s, 	Step: 137179, 	{'train/accuracy': 0.794941782951355, 'train/loss': 0.7663384675979614, 'validation/accuracy': 0.7063599824905396, 'validation/loss': 1.1768110990524292, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 1.863598108291626, 'test/num_examples': 10000, 'score': 53598.2692899704, 'total_duration': 59041.89164376259, 'accumulated_submission_time': 53598.2692899704, 'accumulated_eval_time': 5412.58920955658, 'accumulated_logging_time': 15.803770542144775}
I0308 01:35:10.836071 140197563971328 logging_writer.py:48] [137179] accumulated_eval_time=5412.59, accumulated_logging_time=15.8038, accumulated_submission_time=53598.3, global_step=137179, preemption_count=0, score=53598.3, test/accuracy=0.5795, test/loss=1.8636, test/num_examples=10000, total_duration=59041.9, train/accuracy=0.794942, train/loss=0.766338, validation/accuracy=0.70636, validation/loss=1.17681, validation/num_examples=50000
I0308 01:35:19.485300 140197572364032 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.678609848022461, loss=1.3224077224731445
I0308 01:35:58.373013 140197563971328 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.72931432723999, loss=1.4208873510360718
I0308 01:36:37.943937 140197572364032 logging_writer.py:48] [137400] global_step=137400, grad_norm=5.815760612487793, loss=1.3570301532745361
I0308 01:37:18.146714 140197563971328 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.237856388092041, loss=1.29330575466156
2025-03-08 01:37:56.981114: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:37:57.996779 140197572364032 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.942555904388428, loss=1.2732129096984863
I0308 01:38:37.449116 140197563971328 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.368039608001709, loss=1.2423661947250366
I0308 01:39:17.179148 140197572364032 logging_writer.py:48] [137800] global_step=137800, grad_norm=8.093972206115723, loss=1.2789803743362427
I0308 01:39:56.225600 140197563971328 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.7133588790893555, loss=1.3049800395965576
I0308 01:40:35.568680 140197572364032 logging_writer.py:48] [138000] global_step=138000, grad_norm=6.745529651641846, loss=1.3194777965545654
I0308 01:41:14.549304 140197563971328 logging_writer.py:48] [138100] global_step=138100, grad_norm=5.941474914550781, loss=1.3410570621490479
I0308 01:41:53.912053 140197572364032 logging_writer.py:48] [138200] global_step=138200, grad_norm=6.072674751281738, loss=1.2737478017807007
I0308 01:42:33.048023 140197563971328 logging_writer.py:48] [138300] global_step=138300, grad_norm=5.975054740905762, loss=1.2377560138702393
I0308 01:43:12.520864 140197572364032 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.493621349334717, loss=1.42445969581604
I0308 01:43:40.969031 140352918893760 spec.py:321] Evaluating on the training split.
I0308 01:43:53.760337 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 01:44:44.639833 140352918893760 spec.py:349] Evaluating on the test split.
I0308 01:44:46.669259 140352918893760 submission_runner.py:469] Time since start: 59617.83s, 	Step: 138474, 	{'train/accuracy': 0.793367326259613, 'train/loss': 0.765191912651062, 'validation/accuracy': 0.706059992313385, 'validation/loss': 1.1773537397384644, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.866062045097351, 'test/num_examples': 10000, 'score': 54108.17970252037, 'total_duration': 59617.83230113983, 'accumulated_submission_time': 54108.17970252037, 'accumulated_eval_time': 5478.288891077042, 'accumulated_logging_time': 15.97880244255066}
I0308 01:44:46.798644 140197563971328 logging_writer.py:48] [138474] accumulated_eval_time=5478.29, accumulated_logging_time=15.9788, accumulated_submission_time=54108.2, global_step=138474, preemption_count=0, score=54108.2, test/accuracy=0.5804, test/loss=1.86606, test/num_examples=10000, total_duration=59617.8, train/accuracy=0.793367, train/loss=0.765192, validation/accuracy=0.70606, validation/loss=1.17735, validation/num_examples=50000
I0308 01:44:57.472238 140197572364032 logging_writer.py:48] [138500] global_step=138500, grad_norm=5.74667501449585, loss=1.3606864213943481
I0308 01:45:36.025848 140197563971328 logging_writer.py:48] [138600] global_step=138600, grad_norm=5.739255905151367, loss=1.1578866243362427
I0308 01:46:15.716798 140197572364032 logging_writer.py:48] [138700] global_step=138700, grad_norm=5.84682559967041, loss=1.2967216968536377
I0308 01:46:55.648817 140197563971328 logging_writer.py:48] [138800] global_step=138800, grad_norm=6.2996015548706055, loss=1.3589413166046143
2025-03-08 01:47:14.595207: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:47:34.927066 140197572364032 logging_writer.py:48] [138900] global_step=138900, grad_norm=6.184712886810303, loss=1.255454182624817
I0308 01:48:13.972039 140197563971328 logging_writer.py:48] [139000] global_step=139000, grad_norm=5.664449691772461, loss=1.2338787317276
I0308 01:48:53.111882 140197572364032 logging_writer.py:48] [139100] global_step=139100, grad_norm=6.040166854858398, loss=1.3021773099899292
I0308 01:49:32.035721 140197563971328 logging_writer.py:48] [139200] global_step=139200, grad_norm=5.8597917556762695, loss=1.3130810260772705
I0308 01:50:10.713359 140197572364032 logging_writer.py:48] [139300] global_step=139300, grad_norm=5.76480770111084, loss=1.3007919788360596
I0308 01:50:49.774249 140197563971328 logging_writer.py:48] [139400] global_step=139400, grad_norm=6.163630485534668, loss=1.3399171829223633
I0308 01:51:28.459711 140197572364032 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.1409382820129395, loss=1.3008465766906738
I0308 01:52:07.212653 140197563971328 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.795895099639893, loss=1.2445918321609497
I0308 01:52:46.604958 140197572364032 logging_writer.py:48] [139700] global_step=139700, grad_norm=6.3416056632995605, loss=1.2713135480880737
I0308 01:53:16.990513 140352918893760 spec.py:321] Evaluating on the training split.
I0308 01:53:29.656855 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 01:54:16.132203 140352918893760 spec.py:349] Evaluating on the test split.
I0308 01:54:17.840972 140352918893760 submission_runner.py:469] Time since start: 60189.00s, 	Step: 139778, 	{'train/accuracy': 0.7893813848495483, 'train/loss': 0.7836411595344543, 'validation/accuracy': 0.6978200078010559, 'validation/loss': 1.213788628578186, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.9142110347747803, 'test/num_examples': 10000, 'score': 54618.18823552132, 'total_duration': 60189.00439476967, 'accumulated_submission_time': 54618.18823552132, 'accumulated_eval_time': 5539.139169454575, 'accumulated_logging_time': 16.136438131332397}
I0308 01:54:17.951830 140197563971328 logging_writer.py:48] [139778] accumulated_eval_time=5539.14, accumulated_logging_time=16.1364, accumulated_submission_time=54618.2, global_step=139778, preemption_count=0, score=54618.2, test/accuracy=0.5748, test/loss=1.91421, test/num_examples=10000, total_duration=60189, train/accuracy=0.789381, train/loss=0.783641, validation/accuracy=0.69782, validation/loss=1.21379, validation/num_examples=50000
I0308 01:54:27.132831 140197572364032 logging_writer.py:48] [139800] global_step=139800, grad_norm=5.7008442878723145, loss=1.3049161434173584
I0308 01:55:06.629855 140197563971328 logging_writer.py:48] [139900] global_step=139900, grad_norm=5.988309860229492, loss=1.3252384662628174
I0308 01:55:46.696770 140197572364032 logging_writer.py:48] [140000] global_step=140000, grad_norm=5.870763778686523, loss=1.2875064611434937
2025-03-08 01:56:26.222380: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:56:26.477320 140197563971328 logging_writer.py:48] [140100] global_step=140100, grad_norm=6.537710666656494, loss=1.351598858833313
I0308 01:57:06.054273 140197572364032 logging_writer.py:48] [140200] global_step=140200, grad_norm=5.956225872039795, loss=1.309617042541504
I0308 01:57:45.335152 140197563971328 logging_writer.py:48] [140300] global_step=140300, grad_norm=5.828293800354004, loss=1.3166903257369995
I0308 01:58:24.340985 140197572364032 logging_writer.py:48] [140400] global_step=140400, grad_norm=6.7644782066345215, loss=1.2790859937667847
I0308 01:59:03.539566 140197563971328 logging_writer.py:48] [140500] global_step=140500, grad_norm=5.902341365814209, loss=1.2880202531814575
I0308 01:59:42.679083 140197572364032 logging_writer.py:48] [140600] global_step=140600, grad_norm=6.1275410652160645, loss=1.332193374633789
I0308 02:00:21.558226 140197563971328 logging_writer.py:48] [140700] global_step=140700, grad_norm=5.762098789215088, loss=1.2563813924789429
I0308 02:01:00.360579 140197572364032 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.078165054321289, loss=1.3213976621627808
I0308 02:01:39.620584 140197563971328 logging_writer.py:48] [140900] global_step=140900, grad_norm=6.091877460479736, loss=1.3038381338119507
I0308 02:02:18.888565 140197572364032 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.079626083374023, loss=1.3637113571166992
I0308 02:02:47.867254 140352918893760 spec.py:321] Evaluating on the training split.
I0308 02:02:59.587952 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 02:03:41.578421 140352918893760 spec.py:349] Evaluating on the test split.
I0308 02:03:43.318770 140352918893760 submission_runner.py:469] Time since start: 60754.48s, 	Step: 141075, 	{'train/accuracy': 0.8024752736091614, 'train/loss': 0.7169877290725708, 'validation/accuracy': 0.7141000032424927, 'validation/loss': 1.1460405588150024, 'validation/num_examples': 50000, 'test/accuracy': 0.5905000567436218, 'test/loss': 1.8393865823745728, 'test/num_examples': 10000, 'score': 55127.92451548576, 'total_duration': 60754.48213672638, 'accumulated_submission_time': 55127.92451548576, 'accumulated_eval_time': 5594.590469121933, 'accumulated_logging_time': 16.269219160079956}
I0308 02:03:43.487056 140197563971328 logging_writer.py:48] [141075] accumulated_eval_time=5594.59, accumulated_logging_time=16.2692, accumulated_submission_time=55127.9, global_step=141075, preemption_count=0, score=55127.9, test/accuracy=0.5905, test/loss=1.83939, test/num_examples=10000, total_duration=60754.5, train/accuracy=0.802475, train/loss=0.716988, validation/accuracy=0.7141, validation/loss=1.14604, validation/num_examples=50000
I0308 02:03:53.711866 140197572364032 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.40368127822876, loss=1.3101403713226318
I0308 02:04:32.350804 140197563971328 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.851773262023926, loss=1.2522987127304077
I0308 02:05:11.049991 140197572364032 logging_writer.py:48] [141300] global_step=141300, grad_norm=5.5156378746032715, loss=1.2341939210891724
2025-03-08 02:05:30.626370: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:05:49.992253 140197563971328 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.1884541511535645, loss=1.3348783254623413
I0308 02:06:28.823668 140197572364032 logging_writer.py:48] [141500] global_step=141500, grad_norm=5.666928768157959, loss=1.1450412273406982
I0308 02:07:07.364226 140197563971328 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.727209091186523, loss=1.3152269124984741
I0308 02:07:46.526084 140197572364032 logging_writer.py:48] [141700] global_step=141700, grad_norm=5.88622522354126, loss=1.3016431331634521
I0308 02:08:25.141568 140197563971328 logging_writer.py:48] [141800] global_step=141800, grad_norm=7.172259330749512, loss=1.2497994899749756
I0308 02:09:03.564938 140197572364032 logging_writer.py:48] [141900] global_step=141900, grad_norm=5.7314653396606445, loss=1.2262154817581177
I0308 02:09:42.499503 140197563971328 logging_writer.py:48] [142000] global_step=142000, grad_norm=6.41986608505249, loss=1.3322322368621826
I0308 02:10:21.519306 140197572364032 logging_writer.py:48] [142100] global_step=142100, grad_norm=6.311981678009033, loss=1.207390546798706
I0308 02:11:00.812571 140197563971328 logging_writer.py:48] [142200] global_step=142200, grad_norm=7.206005573272705, loss=1.240889072418213
I0308 02:11:39.948446 140197572364032 logging_writer.py:48] [142300] global_step=142300, grad_norm=5.680251598358154, loss=1.1911615133285522
I0308 02:12:13.510351 140352918893760 spec.py:321] Evaluating on the training split.
I0308 02:12:25.854957 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 02:13:09.195859 140352918893760 spec.py:349] Evaluating on the test split.
I0308 02:13:10.915758 140352918893760 submission_runner.py:469] Time since start: 61322.08s, 	Step: 142387, 	{'train/accuracy': 0.8033920526504517, 'train/loss': 0.714832603931427, 'validation/accuracy': 0.7083199620246887, 'validation/loss': 1.1697689294815063, 'validation/num_examples': 50000, 'test/accuracy': 0.5829000473022461, 'test/loss': 1.8625290393829346, 'test/num_examples': 10000, 'score': 55637.77014827728, 'total_duration': 61322.079132556915, 'accumulated_submission_time': 55637.77014827728, 'accumulated_eval_time': 5651.995651006699, 'accumulated_logging_time': 16.458147525787354}
I0308 02:13:11.045624 140197563971328 logging_writer.py:48] [142387] accumulated_eval_time=5652, accumulated_logging_time=16.4581, accumulated_submission_time=55637.8, global_step=142387, preemption_count=0, score=55637.8, test/accuracy=0.5829, test/loss=1.86253, test/num_examples=10000, total_duration=61322.1, train/accuracy=0.803392, train/loss=0.714833, validation/accuracy=0.70832, validation/loss=1.16977, validation/num_examples=50000
I0308 02:13:16.580294 140197572364032 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.348297119140625, loss=1.1829807758331299
I0308 02:13:55.988606 140197563971328 logging_writer.py:48] [142500] global_step=142500, grad_norm=5.9802775382995605, loss=1.2439647912979126
I0308 02:14:35.206389 140197572364032 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.081864356994629, loss=1.1850637197494507
2025-03-08 02:14:35.914846: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:15:14.816835 140197563971328 logging_writer.py:48] [142700] global_step=142700, grad_norm=7.121518611907959, loss=1.2650052309036255
I0308 02:15:53.538486 140197572364032 logging_writer.py:48] [142800] global_step=142800, grad_norm=6.468404293060303, loss=1.2729969024658203
I0308 02:16:32.743811 140197563971328 logging_writer.py:48] [142900] global_step=142900, grad_norm=6.777644634246826, loss=1.2214789390563965
I0308 02:17:11.882618 140197572364032 logging_writer.py:48] [143000] global_step=143000, grad_norm=5.8180012702941895, loss=1.1793757677078247
I0308 02:17:51.767682 140197563971328 logging_writer.py:48] [143100] global_step=143100, grad_norm=6.141955375671387, loss=1.1843727827072144
I0308 02:18:30.802273 140197572364032 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.092694282531738, loss=1.172400951385498
I0308 02:19:09.679837 140197563971328 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.3644304275512695, loss=1.1332948207855225
I0308 02:19:49.348705 140197572364032 logging_writer.py:48] [143400] global_step=143400, grad_norm=6.2109551429748535, loss=1.2154154777526855
I0308 02:20:28.848026 140197563971328 logging_writer.py:48] [143500] global_step=143500, grad_norm=5.948698997497559, loss=1.2671451568603516
I0308 02:21:08.404087 140197572364032 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.649930953979492, loss=1.2314780950546265
I0308 02:21:41.163522 140352918893760 spec.py:321] Evaluating on the training split.
I0308 02:21:53.751175 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 02:22:32.164481 140352918893760 spec.py:349] Evaluating on the test split.
I0308 02:22:33.899851 140352918893760 submission_runner.py:469] Time since start: 61885.06s, 	Step: 143684, 	{'train/accuracy': 0.8064213991165161, 'train/loss': 0.7029650807380676, 'validation/accuracy': 0.7127199769020081, 'validation/loss': 1.1480932235717773, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.8467190265655518, 'test/num_examples': 10000, 'score': 56147.70947575569, 'total_duration': 61885.06326842308, 'accumulated_submission_time': 56147.70947575569, 'accumulated_eval_time': 5704.731792449951, 'accumulated_logging_time': 16.610419988632202}
I0308 02:22:33.981934 140197563971328 logging_writer.py:48] [143684] accumulated_eval_time=5704.73, accumulated_logging_time=16.6104, accumulated_submission_time=56147.7, global_step=143684, preemption_count=0, score=56147.7, test/accuracy=0.5906, test/loss=1.84672, test/num_examples=10000, total_duration=61885.1, train/accuracy=0.806421, train/loss=0.702965, validation/accuracy=0.71272, validation/loss=1.14809, validation/num_examples=50000
I0308 02:22:40.534500 140197572364032 logging_writer.py:48] [143700] global_step=143700, grad_norm=6.5951457023620605, loss=1.2219974994659424
I0308 02:23:20.716755 140197563971328 logging_writer.py:48] [143800] global_step=143800, grad_norm=5.986695766448975, loss=1.2074617147445679
2025-03-08 02:23:42.166386: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:24:00.855716 140197572364032 logging_writer.py:48] [143900] global_step=143900, grad_norm=6.922872066497803, loss=1.2714289426803589
I0308 02:24:41.299442 140197563971328 logging_writer.py:48] [144000] global_step=144000, grad_norm=6.418247699737549, loss=1.2359421253204346
I0308 02:25:21.552257 140197572364032 logging_writer.py:48] [144100] global_step=144100, grad_norm=6.529234409332275, loss=1.2336268424987793
I0308 02:26:01.462628 140197563971328 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.615776062011719, loss=1.2846605777740479
I0308 02:26:41.491453 140197572364032 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.489253044128418, loss=1.2208337783813477
I0308 02:27:21.644744 140197563971328 logging_writer.py:48] [144400] global_step=144400, grad_norm=5.919039249420166, loss=1.263727068901062
I0308 02:28:01.974994 140197572364032 logging_writer.py:48] [144500] global_step=144500, grad_norm=7.0634565353393555, loss=1.2606170177459717
I0308 02:28:41.991561 140197563971328 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.7347917556762695, loss=1.2133796215057373
I0308 02:29:21.786684 140197572364032 logging_writer.py:48] [144700] global_step=144700, grad_norm=6.050116539001465, loss=1.1092442274093628
I0308 02:30:02.229117 140197563971328 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.401064395904541, loss=1.2122172117233276
I0308 02:30:41.333919 140197572364032 logging_writer.py:48] [144900] global_step=144900, grad_norm=6.612861633300781, loss=1.2324650287628174
I0308 02:31:04.088114 140352918893760 spec.py:321] Evaluating on the training split.
I0308 02:31:17.450271 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 02:31:59.175788 140352918893760 spec.py:349] Evaluating on the test split.
I0308 02:32:00.920982 140352918893760 submission_runner.py:469] Time since start: 62452.08s, 	Step: 144959, 	{'train/accuracy': 0.8153499364852905, 'train/loss': 0.6741594672203064, 'validation/accuracy': 0.7151599526405334, 'validation/loss': 1.1357368230819702, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.8387424945831299, 'test/num_examples': 10000, 'score': 56657.6377427578, 'total_duration': 62452.08435153961, 'accumulated_submission_time': 56657.6377427578, 'accumulated_eval_time': 5761.564425230026, 'accumulated_logging_time': 16.714773416519165}
I0308 02:32:01.015514 140197563971328 logging_writer.py:48] [144959] accumulated_eval_time=5761.56, accumulated_logging_time=16.7148, accumulated_submission_time=56657.6, global_step=144959, preemption_count=0, score=56657.6, test/accuracy=0.5962, test/loss=1.83874, test/num_examples=10000, total_duration=62452.1, train/accuracy=0.81535, train/loss=0.674159, validation/accuracy=0.71516, validation/loss=1.13574, validation/num_examples=50000
I0308 02:32:17.799441 140197572364032 logging_writer.py:48] [145000] global_step=145000, grad_norm=6.818168640136719, loss=1.2389967441558838
I0308 02:32:57.349262 140197563971328 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.660318851470947, loss=1.251356601715088
2025-03-08 02:33:02.498010: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:33:37.317888 140197572364032 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.119565010070801, loss=1.1947580575942993
I0308 02:34:17.777523 140197563971328 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.099955081939697, loss=1.1545188426971436
I0308 02:34:56.584382 140197572364032 logging_writer.py:48] [145400] global_step=145400, grad_norm=5.831629753112793, loss=1.1652603149414062
I0308 02:35:36.308894 140197563971328 logging_writer.py:48] [145500] global_step=145500, grad_norm=7.688735008239746, loss=1.2041643857955933
I0308 02:36:15.492640 140197572364032 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.4827680587768555, loss=1.2099521160125732
I0308 02:36:55.446900 140197563971328 logging_writer.py:48] [145700] global_step=145700, grad_norm=7.2398362159729, loss=1.3874415159225464
I0308 02:37:35.391886 140197572364032 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.0344109535217285, loss=1.1482089757919312
I0308 02:38:15.045887 140197563971328 logging_writer.py:48] [145900] global_step=145900, grad_norm=6.970905303955078, loss=1.2292044162750244
I0308 02:38:54.265106 140197572364032 logging_writer.py:48] [146000] global_step=146000, grad_norm=6.569101810455322, loss=1.1203025579452515
I0308 02:39:33.605717 140197563971328 logging_writer.py:48] [146100] global_step=146100, grad_norm=7.289911270141602, loss=1.2686152458190918
I0308 02:40:13.364633 140197572364032 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.957695960998535, loss=1.2896946668624878
I0308 02:40:31.097930 140352918893760 spec.py:321] Evaluating on the training split.
I0308 02:40:44.386444 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 02:41:29.467636 140352918893760 spec.py:349] Evaluating on the test split.
I0308 02:41:31.217158 140352918893760 submission_runner.py:469] Time since start: 63022.38s, 	Step: 146245, 	{'train/accuracy': 0.8169642686843872, 'train/loss': 0.6747756600379944, 'validation/accuracy': 0.7185399532318115, 'validation/loss': 1.1395584344863892, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.8311971426010132, 'test/num_examples': 10000, 'score': 57167.52866220474, 'total_duration': 63022.38056015968, 'accumulated_submission_time': 57167.52866220474, 'accumulated_eval_time': 5821.683450937271, 'accumulated_logging_time': 16.843200206756592}
I0308 02:41:31.320780 140197563971328 logging_writer.py:48] [146245] accumulated_eval_time=5821.68, accumulated_logging_time=16.8432, accumulated_submission_time=57167.5, global_step=146245, preemption_count=0, score=57167.5, test/accuracy=0.5932, test/loss=1.8312, test/num_examples=10000, total_duration=63022.4, train/accuracy=0.816964, train/loss=0.674776, validation/accuracy=0.71854, validation/loss=1.13956, validation/num_examples=50000
I0308 02:41:53.775584 140197572364032 logging_writer.py:48] [146300] global_step=146300, grad_norm=5.666698932647705, loss=1.2449283599853516
2025-03-08 02:42:15.271105: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:42:32.957148 140197563971328 logging_writer.py:48] [146400] global_step=146400, grad_norm=6.326333045959473, loss=1.1631361246109009
I0308 02:43:12.511344 140197572364032 logging_writer.py:48] [146500] global_step=146500, grad_norm=7.228416442871094, loss=1.2320891618728638
I0308 02:43:51.999467 140197563971328 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.814597129821777, loss=1.1534397602081299
I0308 02:44:31.229965 140197572364032 logging_writer.py:48] [146700] global_step=146700, grad_norm=6.859755039215088, loss=1.2092214822769165
I0308 02:45:10.869238 140197563971328 logging_writer.py:48] [146800] global_step=146800, grad_norm=7.842198848724365, loss=1.1740312576293945
I0308 02:45:50.710690 140197572364032 logging_writer.py:48] [146900] global_step=146900, grad_norm=6.312804698944092, loss=1.3149350881576538
I0308 02:46:29.780218 140197563971328 logging_writer.py:48] [147000] global_step=147000, grad_norm=6.706868648529053, loss=1.1868832111358643
I0308 02:47:09.583455 140197572364032 logging_writer.py:48] [147100] global_step=147100, grad_norm=6.23102331161499, loss=1.1338088512420654
I0308 02:47:48.544420 140197563971328 logging_writer.py:48] [147200] global_step=147200, grad_norm=5.926445007324219, loss=1.1371252536773682
I0308 02:48:28.228857 140197572364032 logging_writer.py:48] [147300] global_step=147300, grad_norm=7.550498008728027, loss=1.232835054397583
I0308 02:49:07.713960 140197563971328 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.301970958709717, loss=1.2980600595474243
I0308 02:49:47.781763 140197572364032 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.167247295379639, loss=1.1422817707061768
I0308 02:50:01.504162 140352918893760 spec.py:321] Evaluating on the training split.
I0308 02:50:13.747403 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 02:51:01.593655 140352918893760 spec.py:349] Evaluating on the test split.
I0308 02:51:03.330857 140352918893760 submission_runner.py:469] Time since start: 63594.49s, 	Step: 147535, 	{'train/accuracy': 0.8150908350944519, 'train/loss': 0.6641317009925842, 'validation/accuracy': 0.7188599705696106, 'validation/loss': 1.1230790615081787, 'validation/num_examples': 50000, 'test/accuracy': 0.5939000248908997, 'test/loss': 1.8428027629852295, 'test/num_examples': 10000, 'score': 57677.50322961807, 'total_duration': 63594.49426460266, 'accumulated_submission_time': 57677.50322961807, 'accumulated_eval_time': 5883.509951353073, 'accumulated_logging_time': 17.001859426498413}
I0308 02:51:03.427023 140197563971328 logging_writer.py:48] [147535] accumulated_eval_time=5883.51, accumulated_logging_time=17.0019, accumulated_submission_time=57677.5, global_step=147535, preemption_count=0, score=57677.5, test/accuracy=0.5939, test/loss=1.8428, test/num_examples=10000, total_duration=63594.5, train/accuracy=0.815091, train/loss=0.664132, validation/accuracy=0.71886, validation/loss=1.12308, validation/num_examples=50000
I0308 02:51:29.052809 140197572364032 logging_writer.py:48] [147600] global_step=147600, grad_norm=6.595093250274658, loss=1.2233537435531616
2025-03-08 02:51:31.607189: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:52:08.743242 140197563971328 logging_writer.py:48] [147700] global_step=147700, grad_norm=7.187502861022949, loss=1.1704521179199219
I0308 02:52:47.519741 140197572364032 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.473677158355713, loss=1.1902636289596558
I0308 02:53:27.299096 140197563971328 logging_writer.py:48] [147900] global_step=147900, grad_norm=6.468729496002197, loss=1.142543077468872
I0308 02:54:06.218365 140197572364032 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.124935150146484, loss=1.215638279914856
I0308 02:54:45.875102 140197563971328 logging_writer.py:48] [148100] global_step=148100, grad_norm=6.961032867431641, loss=1.3057397603988647
I0308 02:55:25.355312 140197572364032 logging_writer.py:48] [148200] global_step=148200, grad_norm=7.0378737449646, loss=1.0906391143798828
I0308 02:56:05.164021 140197563971328 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.964753150939941, loss=1.2627910375595093
I0308 02:56:44.784466 140197572364032 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.575274467468262, loss=1.1760199069976807
I0308 02:57:24.541593 140197563971328 logging_writer.py:48] [148500] global_step=148500, grad_norm=7.017128944396973, loss=1.2650113105773926
I0308 02:58:04.451181 140197572364032 logging_writer.py:48] [148600] global_step=148600, grad_norm=7.722098350524902, loss=1.1111280918121338
I0308 02:58:44.676310 140197563971328 logging_writer.py:48] [148700] global_step=148700, grad_norm=6.754982948303223, loss=1.3026950359344482
I0308 02:59:23.788962 140197572364032 logging_writer.py:48] [148800] global_step=148800, grad_norm=7.241127967834473, loss=1.1571590900421143
I0308 02:59:33.347259 140352918893760 spec.py:321] Evaluating on the training split.
I0308 02:59:46.929764 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 03:00:31.377417 140352918893760 spec.py:349] Evaluating on the test split.
I0308 03:00:33.109272 140352918893760 submission_runner.py:469] Time since start: 64164.27s, 	Step: 148826, 	{'train/accuracy': 0.8249362111091614, 'train/loss': 0.6391320824623108, 'validation/accuracy': 0.7222999930381775, 'validation/loss': 1.1168736219406128, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8205305337905884, 'test/num_examples': 10000, 'score': 58187.21367764473, 'total_duration': 64164.27270078659, 'accumulated_submission_time': 58187.21367764473, 'accumulated_eval_time': 5943.271821022034, 'accumulated_logging_time': 17.15196418762207}
I0308 03:00:33.175661 140197563971328 logging_writer.py:48] [148826] accumulated_eval_time=5943.27, accumulated_logging_time=17.152, accumulated_submission_time=58187.2, global_step=148826, preemption_count=0, score=58187.2, test/accuracy=0.5966, test/loss=1.82053, test/num_examples=10000, total_duration=64164.3, train/accuracy=0.824936, train/loss=0.639132, validation/accuracy=0.7223, validation/loss=1.11687, validation/num_examples=50000
2025-03-08 03:00:46.193566: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:01:02.451931 140197572364032 logging_writer.py:48] [148900] global_step=148900, grad_norm=7.194001197814941, loss=1.2466808557510376
I0308 03:01:42.003955 140197563971328 logging_writer.py:48] [149000] global_step=149000, grad_norm=6.984590530395508, loss=1.1263712644577026
I0308 03:02:21.450216 140197572364032 logging_writer.py:48] [149100] global_step=149100, grad_norm=5.9577131271362305, loss=1.1468210220336914
I0308 03:03:00.757354 140197563971328 logging_writer.py:48] [149200] global_step=149200, grad_norm=6.428553581237793, loss=1.1162534952163696
I0308 03:03:39.881449 140197572364032 logging_writer.py:48] [149300] global_step=149300, grad_norm=7.245835304260254, loss=1.1487884521484375
I0308 03:04:19.511009 140197563971328 logging_writer.py:48] [149400] global_step=149400, grad_norm=7.059567451477051, loss=1.3194842338562012
I0308 03:04:58.708480 140197572364032 logging_writer.py:48] [149500] global_step=149500, grad_norm=7.040041923522949, loss=1.234657883644104
I0308 03:05:37.986335 140197563971328 logging_writer.py:48] [149600] global_step=149600, grad_norm=7.155938625335693, loss=1.2596641778945923
I0308 03:06:17.125346 140197572364032 logging_writer.py:48] [149700] global_step=149700, grad_norm=6.485531806945801, loss=1.0927926301956177
I0308 03:06:56.422839 140197563971328 logging_writer.py:48] [149800] global_step=149800, grad_norm=7.09161901473999, loss=1.2449685335159302
I0308 03:07:35.731240 140197572364032 logging_writer.py:48] [149900] global_step=149900, grad_norm=7.239561557769775, loss=1.1613487005233765
I0308 03:08:15.758900 140197563971328 logging_writer.py:48] [150000] global_step=150000, grad_norm=6.584179401397705, loss=1.153215765953064
I0308 03:08:54.204739 140197572364032 logging_writer.py:48] [150100] global_step=150100, grad_norm=6.44391393661499, loss=1.1033601760864258
2025-03-08 03:09:01.337001: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:09:03.293195 140352918893760 spec.py:321] Evaluating on the training split.
I0308 03:09:16.841325 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 03:10:08.377572 140352918893760 spec.py:349] Evaluating on the test split.
I0308 03:10:10.087627 140352918893760 submission_runner.py:469] Time since start: 64741.25s, 	Step: 150124, 	{'train/accuracy': 0.8336853981018066, 'train/loss': 0.6037167310714722, 'validation/accuracy': 0.7246800065040588, 'validation/loss': 1.0998338460922241, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.7922621965408325, 'test/num_examples': 10000, 'score': 58697.15329504013, 'total_duration': 64741.25106525421, 'accumulated_submission_time': 58697.15329504013, 'accumulated_eval_time': 6010.0660881996155, 'accumulated_logging_time': 17.239588499069214}
I0308 03:10:10.207752 140197563971328 logging_writer.py:48] [150124] accumulated_eval_time=6010.07, accumulated_logging_time=17.2396, accumulated_submission_time=58697.2, global_step=150124, preemption_count=0, score=58697.2, test/accuracy=0.6033, test/loss=1.79226, test/num_examples=10000, total_duration=64741.3, train/accuracy=0.833685, train/loss=0.603717, validation/accuracy=0.72468, validation/loss=1.09983, validation/num_examples=50000
I0308 03:10:40.364857 140197572364032 logging_writer.py:48] [150200] global_step=150200, grad_norm=7.055968284606934, loss=1.1543450355529785
I0308 03:11:19.612028 140197563971328 logging_writer.py:48] [150300] global_step=150300, grad_norm=6.645527362823486, loss=1.1542599201202393
I0308 03:11:59.637485 140197572364032 logging_writer.py:48] [150400] global_step=150400, grad_norm=6.315632343292236, loss=1.1760938167572021
I0308 03:12:38.889489 140197563971328 logging_writer.py:48] [150500] global_step=150500, grad_norm=6.893373489379883, loss=1.1874921321868896
I0308 03:13:18.448729 140197572364032 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.6775593757629395, loss=1.1282541751861572
I0308 03:13:57.958586 140197563971328 logging_writer.py:48] [150700] global_step=150700, grad_norm=6.8957319259643555, loss=1.0261777639389038
I0308 03:14:37.434494 140197572364032 logging_writer.py:48] [150800] global_step=150800, grad_norm=7.540192127227783, loss=1.1430954933166504
I0308 03:15:16.715643 140197563971328 logging_writer.py:48] [150900] global_step=150900, grad_norm=6.721006393432617, loss=1.3395274877548218
I0308 03:15:56.038321 140197572364032 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.553910732269287, loss=1.128665566444397
I0308 03:16:35.366246 140197563971328 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.4629106521606445, loss=1.191737413406372
I0308 03:17:14.844100 140197572364032 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.968652248382568, loss=1.1263465881347656
I0308 03:17:54.073262 140197563971328 logging_writer.py:48] [151300] global_step=151300, grad_norm=6.771787643432617, loss=1.2409669160842896
2025-03-08 03:18:22.443079: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:18:34.416759 140197572364032 logging_writer.py:48] [151400] global_step=151400, grad_norm=6.968520641326904, loss=1.2060027122497559
I0308 03:18:40.494358 140352918893760 spec.py:321] Evaluating on the training split.
I0308 03:18:53.679775 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 03:19:38.733083 140352918893760 spec.py:349] Evaluating on the test split.
I0308 03:19:41.596389 140352918893760 submission_runner.py:469] Time since start: 65312.76s, 	Step: 151416, 	{'train/accuracy': 0.8305364847183228, 'train/loss': 0.614683210849762, 'validation/accuracy': 0.7263399958610535, 'validation/loss': 1.1018314361572266, 'validation/num_examples': 50000, 'test/accuracy': 0.5980000495910645, 'test/loss': 1.8055157661437988, 'test/num_examples': 10000, 'score': 59207.241606235504, 'total_duration': 65312.759472608566, 'accumulated_submission_time': 59207.241606235504, 'accumulated_eval_time': 6071.16760468483, 'accumulated_logging_time': 17.401784896850586}
I0308 03:19:41.756597 140197563971328 logging_writer.py:48] [151416] accumulated_eval_time=6071.17, accumulated_logging_time=17.4018, accumulated_submission_time=59207.2, global_step=151416, preemption_count=0, score=59207.2, test/accuracy=0.598, test/loss=1.80552, test/num_examples=10000, total_duration=65312.8, train/accuracy=0.830536, train/loss=0.614683, validation/accuracy=0.72634, validation/loss=1.10183, validation/num_examples=50000
I0308 03:20:15.437197 140197572364032 logging_writer.py:48] [151500] global_step=151500, grad_norm=6.752191543579102, loss=1.120831847190857
I0308 03:20:55.003654 140197563971328 logging_writer.py:48] [151600] global_step=151600, grad_norm=8.461752891540527, loss=1.1384811401367188
I0308 03:21:34.522664 140197572364032 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.768459796905518, loss=1.149852991104126
I0308 03:22:14.020856 140197563971328 logging_writer.py:48] [151800] global_step=151800, grad_norm=6.705819606781006, loss=1.1242473125457764
I0308 03:22:53.639547 140197572364032 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.0171589851379395, loss=1.1829315423965454
I0308 03:23:33.225818 140197563971328 logging_writer.py:48] [152000] global_step=152000, grad_norm=7.197752475738525, loss=1.1116485595703125
I0308 03:24:12.920744 140197572364032 logging_writer.py:48] [152100] global_step=152100, grad_norm=6.990189552307129, loss=1.2080827951431274
I0308 03:24:52.416820 140197563971328 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.597057819366455, loss=1.0944010019302368
I0308 03:25:32.202173 140197572364032 logging_writer.py:48] [152300] global_step=152300, grad_norm=6.356038570404053, loss=1.0721334218978882
I0308 03:26:11.694556 140197563971328 logging_writer.py:48] [152400] global_step=152400, grad_norm=7.078995704650879, loss=1.0383254289627075
I0308 03:26:50.248053 140197572364032 logging_writer.py:48] [152500] global_step=152500, grad_norm=6.319705009460449, loss=1.019860863685608
I0308 03:27:30.500551 140197563971328 logging_writer.py:48] [152600] global_step=152600, grad_norm=8.102415084838867, loss=1.2300283908843994
2025-03-08 03:27:38.909654: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:28:09.574806 140197572364032 logging_writer.py:48] [152700] global_step=152700, grad_norm=7.703999996185303, loss=1.2021312713623047
I0308 03:28:11.844528 140352918893760 spec.py:321] Evaluating on the training split.
I0308 03:28:25.215231 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 03:29:21.959879 140352918893760 spec.py:349] Evaluating on the test split.
I0308 03:29:23.667297 140352918893760 submission_runner.py:469] Time since start: 65894.83s, 	Step: 152706, 	{'train/accuracy': 0.8395248651504517, 'train/loss': 0.5832342505455017, 'validation/accuracy': 0.727400004863739, 'validation/loss': 1.0825368165969849, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.7959704399108887, 'test/num_examples': 10000, 'score': 59717.1159825325, 'total_duration': 65894.83074331284, 'accumulated_submission_time': 59717.1159825325, 'accumulated_eval_time': 6142.990214109421, 'accumulated_logging_time': 17.617989540100098}
I0308 03:29:23.748167 140197563971328 logging_writer.py:48] [152706] accumulated_eval_time=6142.99, accumulated_logging_time=17.618, accumulated_submission_time=59717.1, global_step=152706, preemption_count=0, score=59717.1, test/accuracy=0.604, test/loss=1.79597, test/num_examples=10000, total_duration=65894.8, train/accuracy=0.839525, train/loss=0.583234, validation/accuracy=0.7274, validation/loss=1.08254, validation/num_examples=50000
I0308 03:30:01.136305 140197572364032 logging_writer.py:48] [152800] global_step=152800, grad_norm=6.493847370147705, loss=1.0069286823272705
I0308 03:30:40.494592 140197563971328 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.511396884918213, loss=1.1659468412399292
I0308 03:31:20.131611 140197572364032 logging_writer.py:48] [153000] global_step=153000, grad_norm=8.336888313293457, loss=1.1258749961853027
I0308 03:31:59.599933 140197563971328 logging_writer.py:48] [153100] global_step=153100, grad_norm=6.698300838470459, loss=1.1101758480072021
I0308 03:32:39.001702 140197572364032 logging_writer.py:48] [153200] global_step=153200, grad_norm=7.1953277587890625, loss=1.149970293045044
I0308 03:33:18.354746 140197563971328 logging_writer.py:48] [153300] global_step=153300, grad_norm=7.264284133911133, loss=1.1100707054138184
I0308 03:33:57.627746 140197572364032 logging_writer.py:48] [153400] global_step=153400, grad_norm=7.393024444580078, loss=1.2163811922073364
I0308 03:34:36.763158 140197563971328 logging_writer.py:48] [153500] global_step=153500, grad_norm=7.443456172943115, loss=1.1535923480987549
I0308 03:35:16.301337 140197572364032 logging_writer.py:48] [153600] global_step=153600, grad_norm=7.577328205108643, loss=1.016487956047058
I0308 03:35:56.158282 140197563971328 logging_writer.py:48] [153700] global_step=153700, grad_norm=8.07984733581543, loss=1.2445347309112549
I0308 03:36:36.031814 140197572364032 logging_writer.py:48] [153800] global_step=153800, grad_norm=6.891097068786621, loss=1.0601869821548462
2025-03-08 03:37:02.428076: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:37:15.596457 140197563971328 logging_writer.py:48] [153900] global_step=153900, grad_norm=7.004268646240234, loss=1.0627728700637817
I0308 03:37:53.840657 140352918893760 spec.py:321] Evaluating on the training split.
I0308 03:38:06.476553 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 03:38:49.420280 140352918893760 spec.py:349] Evaluating on the test split.
I0308 03:38:51.167742 140352918893760 submission_runner.py:469] Time since start: 66462.33s, 	Step: 153997, 	{'train/accuracy': 0.8410793542861938, 'train/loss': 0.5604580640792847, 'validation/accuracy': 0.7313999533653259, 'validation/loss': 1.0849483013153076, 'validation/num_examples': 50000, 'test/accuracy': 0.6071000099182129, 'test/loss': 1.7804244756698608, 'test/num_examples': 10000, 'score': 60227.02983236313, 'total_duration': 66462.33111572266, 'accumulated_submission_time': 60227.02983236313, 'accumulated_eval_time': 6200.317070484161, 'accumulated_logging_time': 17.721428155899048}
I0308 03:38:51.362169 140197572364032 logging_writer.py:48] [153997] accumulated_eval_time=6200.32, accumulated_logging_time=17.7214, accumulated_submission_time=60227, global_step=153997, preemption_count=0, score=60227, test/accuracy=0.6071, test/loss=1.78042, test/num_examples=10000, total_duration=66462.3, train/accuracy=0.841079, train/loss=0.560458, validation/accuracy=0.7314, validation/loss=1.08495, validation/num_examples=50000
I0308 03:38:52.934770 140197563971328 logging_writer.py:48] [154000] global_step=154000, grad_norm=7.49490213394165, loss=1.1925647258758545
I0308 03:39:31.993342 140197572364032 logging_writer.py:48] [154100] global_step=154100, grad_norm=6.8267412185668945, loss=1.1848695278167725
I0308 03:40:11.269318 140197563971328 logging_writer.py:48] [154200] global_step=154200, grad_norm=7.249356269836426, loss=1.1735373735427856
I0308 03:40:50.885734 140197572364032 logging_writer.py:48] [154300] global_step=154300, grad_norm=7.893817901611328, loss=1.1531318426132202
I0308 03:41:30.597073 140197563971328 logging_writer.py:48] [154400] global_step=154400, grad_norm=7.50056791305542, loss=1.0901854038238525
I0308 03:42:10.051261 140197572364032 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.085625648498535, loss=1.2072737216949463
I0308 03:42:49.095310 140197563971328 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.8573174476623535, loss=1.0651330947875977
I0308 03:43:28.171538 140197572364032 logging_writer.py:48] [154700] global_step=154700, grad_norm=8.986955642700195, loss=1.1755787134170532
I0308 03:44:07.398912 140197563971328 logging_writer.py:48] [154800] global_step=154800, grad_norm=7.925933361053467, loss=1.0650584697723389
I0308 03:44:46.624299 140197572364032 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.550634860992432, loss=1.1080608367919922
I0308 03:45:26.160555 140197563971328 logging_writer.py:48] [155000] global_step=155000, grad_norm=7.066453456878662, loss=1.1349472999572754
I0308 03:46:06.109986 140197572364032 logging_writer.py:48] [155100] global_step=155100, grad_norm=7.356713771820068, loss=1.1299443244934082
2025-03-08 03:46:11.399596: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:46:45.711688 140197563971328 logging_writer.py:48] [155200] global_step=155200, grad_norm=7.2950592041015625, loss=1.1273826360702515
I0308 03:47:21.507590 140352918893760 spec.py:321] Evaluating on the training split.
I0308 03:47:34.205370 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 03:48:18.812881 140352918893760 spec.py:349] Evaluating on the test split.
I0308 03:48:20.523438 140352918893760 submission_runner.py:469] Time since start: 67031.69s, 	Step: 155292, 	{'train/accuracy': 0.8443877100944519, 'train/loss': 0.5534412860870361, 'validation/accuracy': 0.7317999601364136, 'validation/loss': 1.081277847290039, 'validation/num_examples': 50000, 'test/accuracy': 0.6061000227928162, 'test/loss': 1.7662510871887207, 'test/num_examples': 10000, 'score': 60736.97341442108, 'total_duration': 67031.68683695793, 'accumulated_submission_time': 60736.97341442108, 'accumulated_eval_time': 6259.332718610764, 'accumulated_logging_time': 17.9585862159729}
I0308 03:48:20.615134 140197572364032 logging_writer.py:48] [155292] accumulated_eval_time=6259.33, accumulated_logging_time=17.9586, accumulated_submission_time=60737, global_step=155292, preemption_count=0, score=60737, test/accuracy=0.6061, test/loss=1.76625, test/num_examples=10000, total_duration=67031.7, train/accuracy=0.844388, train/loss=0.553441, validation/accuracy=0.7318, validation/loss=1.08128, validation/num_examples=50000
I0308 03:48:24.128747 140197563971328 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.209170341491699, loss=1.0541801452636719
I0308 03:49:03.421663 140197572364032 logging_writer.py:48] [155400] global_step=155400, grad_norm=6.862101078033447, loss=1.0492359399795532
I0308 03:49:43.128053 140197563971328 logging_writer.py:48] [155500] global_step=155500, grad_norm=6.626190185546875, loss=1.1615711450576782
I0308 03:50:22.244117 140197572364032 logging_writer.py:48] [155600] global_step=155600, grad_norm=8.001956939697266, loss=1.1472398042678833
I0308 03:51:01.426284 140197563971328 logging_writer.py:48] [155700] global_step=155700, grad_norm=7.039376735687256, loss=1.11368727684021
I0308 03:51:40.569206 140197572364032 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.354160308837891, loss=1.0430595874786377
I0308 03:52:19.735496 140197563971328 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.274521350860596, loss=1.1057538986206055
I0308 03:52:59.187094 140197572364032 logging_writer.py:48] [156000] global_step=156000, grad_norm=7.37031888961792, loss=1.098367691040039
I0308 03:53:38.474565 140197563971328 logging_writer.py:48] [156100] global_step=156100, grad_norm=7.064510822296143, loss=1.0554825067520142
I0308 03:54:18.084654 140197572364032 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.391185760498047, loss=1.0182654857635498
I0308 03:54:57.203008 140197563971328 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.534295082092285, loss=1.1324197053909302
2025-03-08 03:55:26.562396: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:55:36.914592 140197572364032 logging_writer.py:48] [156400] global_step=156400, grad_norm=7.050161838531494, loss=1.0190885066986084
I0308 03:56:15.498598 140197563971328 logging_writer.py:48] [156500] global_step=156500, grad_norm=7.236902713775635, loss=1.0295753479003906
I0308 03:56:50.880508 140352918893760 spec.py:321] Evaluating on the training split.
I0308 03:57:03.605225 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 03:57:54.429257 140352918893760 spec.py:349] Evaluating on the test split.
I0308 03:57:56.154353 140352918893760 submission_runner.py:469] Time since start: 67607.32s, 	Step: 156591, 	{'train/accuracy': 0.8515027165412903, 'train/loss': 0.5283141732215881, 'validation/accuracy': 0.7360399961471558, 'validation/loss': 1.063529133796692, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.7761375904083252, 'test/num_examples': 10000, 'score': 61247.05866241455, 'total_duration': 67607.31779408455, 'accumulated_submission_time': 61247.05866241455, 'accumulated_eval_time': 6324.606401205063, 'accumulated_logging_time': 18.073328256607056}
I0308 03:57:56.249762 140197572364032 logging_writer.py:48] [156591] accumulated_eval_time=6324.61, accumulated_logging_time=18.0733, accumulated_submission_time=61247.1, global_step=156591, preemption_count=0, score=61247.1, test/accuracy=0.6103, test/loss=1.77614, test/num_examples=10000, total_duration=67607.3, train/accuracy=0.851503, train/loss=0.528314, validation/accuracy=0.73604, validation/loss=1.06353, validation/num_examples=50000
I0308 03:58:00.271555 140197563971328 logging_writer.py:48] [156600] global_step=156600, grad_norm=6.851110458374023, loss=1.0197913646697998
I0308 03:58:39.643281 140197572364032 logging_writer.py:48] [156700] global_step=156700, grad_norm=7.247749328613281, loss=1.1180051565170288
I0308 03:59:18.581974 140197563971328 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.052387714385986, loss=1.0829541683197021
I0308 03:59:57.545478 140197572364032 logging_writer.py:48] [156900] global_step=156900, grad_norm=7.6423163414001465, loss=1.111225962638855
I0308 04:00:37.025130 140197563971328 logging_writer.py:48] [157000] global_step=157000, grad_norm=6.925257205963135, loss=1.0532907247543335
I0308 04:01:16.035766 140197572364032 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.398841381072998, loss=1.2042772769927979
I0308 04:01:55.378600 140197563971328 logging_writer.py:48] [157200] global_step=157200, grad_norm=7.417226314544678, loss=1.0853452682495117
I0308 04:02:34.561540 140197572364032 logging_writer.py:48] [157300] global_step=157300, grad_norm=7.227130889892578, loss=1.0576833486557007
I0308 04:03:13.874678 140197563971328 logging_writer.py:48] [157400] global_step=157400, grad_norm=7.507035732269287, loss=1.0445926189422607
I0308 04:03:53.422595 140197572364032 logging_writer.py:48] [157500] global_step=157500, grad_norm=7.720262050628662, loss=1.1128634214401245
I0308 04:04:33.051312 140197563971328 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.357131481170654, loss=1.0247515439987183
2025-03-08 04:04:39.233895: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:05:12.188377 140197572364032 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.49533224105835, loss=1.0880765914916992
I0308 04:05:51.472403 140197563971328 logging_writer.py:48] [157800] global_step=157800, grad_norm=7.27608060836792, loss=1.0239070653915405
I0308 04:06:26.167346 140352918893760 spec.py:321] Evaluating on the training split.
I0308 04:06:38.340595 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 04:07:31.825657 140352918893760 spec.py:349] Evaluating on the test split.
I0308 04:07:33.552044 140352918893760 submission_runner.py:469] Time since start: 68184.72s, 	Step: 157889, 	{'train/accuracy': 0.8520607352256775, 'train/loss': 0.5210450291633606, 'validation/accuracy': 0.736299991607666, 'validation/loss': 1.0596239566802979, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.7377647161483765, 'test/num_examples': 10000, 'score': 61756.77537345886, 'total_duration': 68184.71546888351, 'accumulated_submission_time': 61756.77537345886, 'accumulated_eval_time': 6391.990918874741, 'accumulated_logging_time': 18.215017080307007}
I0308 04:07:33.692678 140197572364032 logging_writer.py:48] [157889] accumulated_eval_time=6391.99, accumulated_logging_time=18.215, accumulated_submission_time=61756.8, global_step=157889, preemption_count=0, score=61756.8, test/accuracy=0.6135, test/loss=1.73776, test/num_examples=10000, total_duration=68184.7, train/accuracy=0.852061, train/loss=0.521045, validation/accuracy=0.7363, validation/loss=1.05962, validation/num_examples=50000
I0308 04:07:38.506851 140197563971328 logging_writer.py:48] [157900] global_step=157900, grad_norm=7.662930011749268, loss=1.086421012878418
I0308 04:08:17.675755 140197572364032 logging_writer.py:48] [158000] global_step=158000, grad_norm=7.3466668128967285, loss=1.0489482879638672
I0308 04:08:57.555190 140197563971328 logging_writer.py:48] [158100] global_step=158100, grad_norm=8.480669021606445, loss=1.130437970161438
I0308 04:09:37.401345 140197572364032 logging_writer.py:48] [158200] global_step=158200, grad_norm=7.055469512939453, loss=1.0611317157745361
I0308 04:10:16.867588 140197563971328 logging_writer.py:48] [158300] global_step=158300, grad_norm=8.327963829040527, loss=1.0948448181152344
I0308 04:10:56.429561 140197572364032 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.102485656738281, loss=1.0497220754623413
I0308 04:11:35.809649 140197563971328 logging_writer.py:48] [158500] global_step=158500, grad_norm=7.624433517456055, loss=1.0788238048553467
I0308 04:12:15.488152 140197572364032 logging_writer.py:48] [158600] global_step=158600, grad_norm=8.076756477355957, loss=1.1069525480270386
I0308 04:12:54.792740 140197563971328 logging_writer.py:48] [158700] global_step=158700, grad_norm=7.633864879608154, loss=1.02567458152771
I0308 04:13:34.343606 140197572364032 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.24822473526001, loss=0.9887381196022034
2025-03-08 04:14:02.538683: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:14:13.470036 140197563971328 logging_writer.py:48] [158900] global_step=158900, grad_norm=7.858801364898682, loss=1.0323952436447144
I0308 04:14:52.807628 140197572364032 logging_writer.py:48] [159000] global_step=159000, grad_norm=7.7028937339782715, loss=1.0577905178070068
I0308 04:15:32.281186 140197563971328 logging_writer.py:48] [159100] global_step=159100, grad_norm=7.548018932342529, loss=1.0341180562973022
I0308 04:16:03.641529 140352918893760 spec.py:321] Evaluating on the training split.
I0308 04:16:17.002027 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 04:16:50.589219 140352918893760 spec.py:349] Evaluating on the test split.
I0308 04:16:52.366810 140352918893760 submission_runner.py:469] Time since start: 68743.53s, 	Step: 159181, 	{'train/accuracy': 0.8557277917861938, 'train/loss': 0.5128253102302551, 'validation/accuracy': 0.7378799915313721, 'validation/loss': 1.0601978302001953, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.7477099895477295, 'test/num_examples': 10000, 'score': 62266.53978395462, 'total_duration': 68743.53025627136, 'accumulated_submission_time': 62266.53978395462, 'accumulated_eval_time': 6440.716040611267, 'accumulated_logging_time': 18.381216049194336}
I0308 04:16:52.442886 140197572364032 logging_writer.py:48] [159181] accumulated_eval_time=6440.72, accumulated_logging_time=18.3812, accumulated_submission_time=62266.5, global_step=159181, preemption_count=0, score=62266.5, test/accuracy=0.6138, test/loss=1.74771, test/num_examples=10000, total_duration=68743.5, train/accuracy=0.855728, train/loss=0.512825, validation/accuracy=0.73788, validation/loss=1.0602, validation/num_examples=50000
I0308 04:17:00.315037 140197563971328 logging_writer.py:48] [159200] global_step=159200, grad_norm=7.543331146240234, loss=1.05558443069458
I0308 04:17:39.627521 140197572364032 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.188111782073975, loss=1.061292290687561
I0308 04:18:19.071062 140197563971328 logging_writer.py:48] [159400] global_step=159400, grad_norm=7.121068477630615, loss=1.030508041381836
I0308 04:18:58.422275 140197572364032 logging_writer.py:48] [159500] global_step=159500, grad_norm=7.2625732421875, loss=1.0218497514724731
I0308 04:19:37.534205 140197563971328 logging_writer.py:48] [159600] global_step=159600, grad_norm=8.122576713562012, loss=1.011366367340088
I0308 04:20:16.649642 140197572364032 logging_writer.py:48] [159700] global_step=159700, grad_norm=7.33864688873291, loss=1.0530965328216553
I0308 04:20:56.202080 140197563971328 logging_writer.py:48] [159800] global_step=159800, grad_norm=7.6285014152526855, loss=1.00749933719635
I0308 04:21:35.375502 140197572364032 logging_writer.py:48] [159900] global_step=159900, grad_norm=7.783895492553711, loss=1.0526374578475952
I0308 04:22:15.009399 140197563971328 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.028287887573242, loss=0.9017144441604614
I0308 04:22:54.298782 140197572364032 logging_writer.py:48] [160100] global_step=160100, grad_norm=8.368974685668945, loss=1.0453104972839355
2025-03-08 04:23:01.627424: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:23:33.882768 140197563971328 logging_writer.py:48] [160200] global_step=160200, grad_norm=7.737958908081055, loss=1.065442442893982
I0308 04:24:13.257620 140197572364032 logging_writer.py:48] [160300] global_step=160300, grad_norm=7.640453338623047, loss=1.067099690437317
I0308 04:24:52.474397 140197563971328 logging_writer.py:48] [160400] global_step=160400, grad_norm=7.2672438621521, loss=1.0147294998168945
I0308 04:25:22.745808 140352918893760 spec.py:321] Evaluating on the training split.
I0308 04:25:35.424683 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 04:26:07.093931 140352918893760 spec.py:349] Evaluating on the test split.
I0308 04:26:08.812169 140352918893760 submission_runner.py:469] Time since start: 69299.98s, 	Step: 160479, 	{'train/accuracy': 0.8646165132522583, 'train/loss': 0.4872450530529022, 'validation/accuracy': 0.7372199892997742, 'validation/loss': 1.0514651536941528, 'validation/num_examples': 50000, 'test/accuracy': 0.6137000322341919, 'test/loss': 1.7524752616882324, 'test/num_examples': 10000, 'score': 62776.62667822838, 'total_duration': 69299.97558164597, 'accumulated_submission_time': 62776.62667822838, 'accumulated_eval_time': 6486.782221317291, 'accumulated_logging_time': 18.515195846557617}
I0308 04:26:08.951119 140197572364032 logging_writer.py:48] [160479] accumulated_eval_time=6486.78, accumulated_logging_time=18.5152, accumulated_submission_time=62776.6, global_step=160479, preemption_count=0, score=62776.6, test/accuracy=0.6137, test/loss=1.75248, test/num_examples=10000, total_duration=69300, train/accuracy=0.864617, train/loss=0.487245, validation/accuracy=0.73722, validation/loss=1.05147, validation/num_examples=50000
I0308 04:26:17.575403 140197563971328 logging_writer.py:48] [160500] global_step=160500, grad_norm=7.634428977966309, loss=1.0178782939910889
I0308 04:26:56.918387 140197572364032 logging_writer.py:48] [160600] global_step=160600, grad_norm=7.899394512176514, loss=1.2156317234039307
I0308 04:27:36.182784 140197563971328 logging_writer.py:48] [160700] global_step=160700, grad_norm=8.104610443115234, loss=0.9548605680465698
I0308 04:28:14.909337 140197572364032 logging_writer.py:48] [160800] global_step=160800, grad_norm=7.606413841247559, loss=1.061946153640747
I0308 04:28:54.129035 140197563971328 logging_writer.py:48] [160900] global_step=160900, grad_norm=8.041685104370117, loss=1.083425521850586
I0308 04:29:33.753689 140197572364032 logging_writer.py:48] [161000] global_step=161000, grad_norm=8.103569984436035, loss=1.0285149812698364
I0308 04:30:13.006152 140197563971328 logging_writer.py:48] [161100] global_step=161100, grad_norm=7.917301177978516, loss=1.0245959758758545
I0308 04:30:52.858894 140197572364032 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.849353790283203, loss=1.0411536693572998
I0308 04:31:32.543312 140197563971328 logging_writer.py:48] [161300] global_step=161300, grad_norm=7.292871952056885, loss=1.0537374019622803
2025-03-08 04:32:00.856572: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:32:11.779606 140197572364032 logging_writer.py:48] [161400] global_step=161400, grad_norm=7.698399066925049, loss=0.9775559902191162
I0308 04:32:51.474782 140197563971328 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.9207024574279785, loss=1.0517996549606323
I0308 04:33:31.544915 140197572364032 logging_writer.py:48] [161600] global_step=161600, grad_norm=8.49370288848877, loss=1.09274423122406
I0308 04:34:10.998648 140197563971328 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.948744773864746, loss=0.9687286019325256
I0308 04:34:39.161632 140352918893760 spec.py:321] Evaluating on the training split.
I0308 04:34:52.001736 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 04:35:34.481733 140352918893760 spec.py:349] Evaluating on the test split.
I0308 04:35:36.228589 140352918893760 submission_runner.py:469] Time since start: 69867.39s, 	Step: 161772, 	{'train/accuracy': 0.8703164458274841, 'train/loss': 0.45442503690719604, 'validation/accuracy': 0.7425999641418457, 'validation/loss': 1.0323022603988647, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.738883137702942, 'test/num_examples': 10000, 'score': 63286.635056734085, 'total_duration': 69867.39201664925, 'accumulated_submission_time': 63286.635056734085, 'accumulated_eval_time': 6543.849009275436, 'accumulated_logging_time': 18.700711488723755}
I0308 04:35:36.324506 140197572364032 logging_writer.py:48] [161772] accumulated_eval_time=6543.85, accumulated_logging_time=18.7007, accumulated_submission_time=63286.6, global_step=161772, preemption_count=0, score=63286.6, test/accuracy=0.6173, test/loss=1.73888, test/num_examples=10000, total_duration=69867.4, train/accuracy=0.870316, train/loss=0.454425, validation/accuracy=0.7426, validation/loss=1.0323, validation/num_examples=50000
I0308 04:35:47.903900 140197563971328 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.143248558044434, loss=1.0190184116363525
I0308 04:36:26.875215 140197572364032 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.424731254577637, loss=0.9947032332420349
I0308 04:37:05.673101 140197563971328 logging_writer.py:48] [162000] global_step=162000, grad_norm=8.358332633972168, loss=1.075555682182312
I0308 04:37:44.866497 140197572364032 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.395142078399658, loss=1.0488615036010742
I0308 04:38:23.891571 140197563971328 logging_writer.py:48] [162200] global_step=162200, grad_norm=7.5728583335876465, loss=0.9773048162460327
I0308 04:39:03.061439 140197572364032 logging_writer.py:48] [162300] global_step=162300, grad_norm=7.002911567687988, loss=0.9451233148574829
I0308 04:39:42.818458 140197563971328 logging_writer.py:48] [162400] global_step=162400, grad_norm=7.8989577293396, loss=1.0156152248382568
I0308 04:40:22.723726 140197572364032 logging_writer.py:48] [162500] global_step=162500, grad_norm=7.974916458129883, loss=1.0427839756011963
I0308 04:41:02.149497 140197563971328 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.315483570098877, loss=0.9183349609375
2025-03-08 04:41:10.285872: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:41:41.734187 140197572364032 logging_writer.py:48] [162700] global_step=162700, grad_norm=7.9314188957214355, loss=0.9708156585693359
I0308 04:42:20.819525 140197563971328 logging_writer.py:48] [162800] global_step=162800, grad_norm=7.416916847229004, loss=0.9652286767959595
I0308 04:43:00.258177 140197572364032 logging_writer.py:48] [162900] global_step=162900, grad_norm=8.101967811584473, loss=0.9592853784561157
I0308 04:43:39.729926 140197563971328 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.507455825805664, loss=0.9279771447181702
I0308 04:44:06.349706 140352918893760 spec.py:321] Evaluating on the training split.
I0308 04:44:19.550956 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 04:45:04.177536 140352918893760 spec.py:349] Evaluating on the test split.
I0308 04:45:05.881330 140352918893760 submission_runner.py:469] Time since start: 70437.04s, 	Step: 163068, 	{'train/accuracy': 0.8712930083274841, 'train/loss': 0.4574160575866699, 'validation/accuracy': 0.7414799928665161, 'validation/loss': 1.0349931716918945, 'validation/num_examples': 50000, 'test/accuracy': 0.6151000261306763, 'test/loss': 1.7563148736953735, 'test/num_examples': 10000, 'score': 63796.46826982498, 'total_duration': 70437.044765234, 'accumulated_submission_time': 63796.46826982498, 'accumulated_eval_time': 6603.380485057831, 'accumulated_logging_time': 18.830911874771118}
I0308 04:45:05.998324 140197572364032 logging_writer.py:48] [163068] accumulated_eval_time=6603.38, accumulated_logging_time=18.8309, accumulated_submission_time=63796.5, global_step=163068, preemption_count=0, score=63796.5, test/accuracy=0.6151, test/loss=1.75631, test/num_examples=10000, total_duration=70437, train/accuracy=0.871293, train/loss=0.457416, validation/accuracy=0.74148, validation/loss=1.03499, validation/num_examples=50000
I0308 04:45:19.120155 140197563971328 logging_writer.py:48] [163100] global_step=163100, grad_norm=7.555572986602783, loss=0.9647319912910461
I0308 04:45:58.130393 140197572364032 logging_writer.py:48] [163200] global_step=163200, grad_norm=7.650913715362549, loss=1.0087509155273438
I0308 04:46:37.446933 140197563971328 logging_writer.py:48] [163300] global_step=163300, grad_norm=7.216230392456055, loss=0.8930187225341797
I0308 04:47:16.649064 140197572364032 logging_writer.py:48] [163400] global_step=163400, grad_norm=7.351579666137695, loss=0.9762856960296631
I0308 04:47:55.451588 140197563971328 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.700356960296631, loss=0.9923557639122009
I0308 04:48:34.974074 140197572364032 logging_writer.py:48] [163600] global_step=163600, grad_norm=8.612672805786133, loss=1.062167763710022
I0308 04:49:14.795170 140197563971328 logging_writer.py:48] [163700] global_step=163700, grad_norm=7.844045162200928, loss=1.0252959728240967
I0308 04:49:54.865797 140197572364032 logging_writer.py:48] [163800] global_step=163800, grad_norm=8.118809700012207, loss=1.0344598293304443
2025-03-08 04:50:24.121562: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:50:34.069087 140197563971328 logging_writer.py:48] [163900] global_step=163900, grad_norm=7.745989799499512, loss=0.8459854125976562
I0308 04:51:13.082577 140197572364032 logging_writer.py:48] [164000] global_step=164000, grad_norm=8.142583847045898, loss=1.000500202178955
I0308 04:51:52.522815 140197563971328 logging_writer.py:48] [164100] global_step=164100, grad_norm=7.270358085632324, loss=0.9548423290252686
I0308 04:52:31.551670 140197572364032 logging_writer.py:48] [164200] global_step=164200, grad_norm=7.456835746765137, loss=0.9185823798179626
I0308 04:53:10.812433 140197563971328 logging_writer.py:48] [164300] global_step=164300, grad_norm=8.377081871032715, loss=1.0514159202575684
I0308 04:53:36.025967 140352918893760 spec.py:321] Evaluating on the training split.
I0308 04:53:48.673200 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 04:54:42.230267 140352918893760 spec.py:349] Evaluating on the test split.
I0308 04:54:43.960281 140352918893760 submission_runner.py:469] Time since start: 71015.12s, 	Step: 164365, 	{'train/accuracy': 0.8731664419174194, 'train/loss': 0.45019686222076416, 'validation/accuracy': 0.7411800026893616, 'validation/loss': 1.0385890007019043, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.750708818435669, 'test/num_examples': 10000, 'score': 64306.3120701313, 'total_duration': 71015.12366104126, 'accumulated_submission_time': 64306.3120701313, 'accumulated_eval_time': 6671.314571380615, 'accumulated_logging_time': 18.975023984909058}
I0308 04:54:44.103815 140197572364032 logging_writer.py:48] [164365] accumulated_eval_time=6671.31, accumulated_logging_time=18.975, accumulated_submission_time=64306.3, global_step=164365, preemption_count=0, score=64306.3, test/accuracy=0.6191, test/loss=1.75071, test/num_examples=10000, total_duration=71015.1, train/accuracy=0.873166, train/loss=0.450197, validation/accuracy=0.74118, validation/loss=1.03859, validation/num_examples=50000
I0308 04:54:58.165629 140197563971328 logging_writer.py:48] [164400] global_step=164400, grad_norm=8.179841041564941, loss=0.9978189468383789
I0308 04:55:37.536463 140197572364032 logging_writer.py:48] [164500] global_step=164500, grad_norm=8.560288429260254, loss=1.0009779930114746
I0308 04:56:17.070137 140197563971328 logging_writer.py:48] [164600] global_step=164600, grad_norm=7.415096759796143, loss=0.898330569267273
I0308 04:56:56.626661 140197572364032 logging_writer.py:48] [164700] global_step=164700, grad_norm=7.657355308532715, loss=0.9449741840362549
I0308 04:57:36.263710 140197563971328 logging_writer.py:48] [164800] global_step=164800, grad_norm=7.906224727630615, loss=0.9996695518493652
I0308 04:58:15.863753 140197572364032 logging_writer.py:48] [164900] global_step=164900, grad_norm=7.126079082489014, loss=0.913922131061554
I0308 04:58:55.504360 140197563971328 logging_writer.py:48] [165000] global_step=165000, grad_norm=7.863971710205078, loss=1.0122348070144653
I0308 04:59:34.663476 140197572364032 logging_writer.py:48] [165100] global_step=165100, grad_norm=9.136627197265625, loss=1.014434576034546
2025-03-08 04:59:43.734119: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:00:14.236817 140197563971328 logging_writer.py:48] [165200] global_step=165200, grad_norm=7.727510452270508, loss=1.0430198907852173
I0308 05:00:52.887616 140197572364032 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.980557441711426, loss=0.9840016961097717
I0308 05:01:32.162359 140197563971328 logging_writer.py:48] [165400] global_step=165400, grad_norm=8.778806686401367, loss=0.9838399291038513
I0308 05:02:11.492689 140197572364032 logging_writer.py:48] [165500] global_step=165500, grad_norm=8.858386993408203, loss=0.9278956651687622
I0308 05:02:50.708926 140197563971328 logging_writer.py:48] [165600] global_step=165600, grad_norm=8.057543754577637, loss=1.0414425134658813
I0308 05:03:14.087628 140352918893760 spec.py:321] Evaluating on the training split.
I0308 05:03:26.448569 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 05:04:09.279677 140352918893760 spec.py:349] Evaluating on the test split.
I0308 05:04:10.989441 140352918893760 submission_runner.py:469] Time since start: 71582.15s, 	Step: 165660, 	{'train/accuracy': 0.8804607391357422, 'train/loss': 0.4232844412326813, 'validation/accuracy': 0.7456199526786804, 'validation/loss': 1.0242973566055298, 'validation/num_examples': 50000, 'test/accuracy': 0.6225000023841858, 'test/loss': 1.7256665229797363, 'test/num_examples': 10000, 'score': 64816.105321884155, 'total_duration': 71582.1528480053, 'accumulated_submission_time': 64816.105321884155, 'accumulated_eval_time': 6728.2161860466, 'accumulated_logging_time': 19.14898133277893}
I0308 05:04:11.170129 140197572364032 logging_writer.py:48] [165660] accumulated_eval_time=6728.22, accumulated_logging_time=19.149, accumulated_submission_time=64816.1, global_step=165660, preemption_count=0, score=64816.1, test/accuracy=0.6225, test/loss=1.72567, test/num_examples=10000, total_duration=71582.2, train/accuracy=0.880461, train/loss=0.423284, validation/accuracy=0.74562, validation/loss=1.0243, validation/num_examples=50000
I0308 05:04:27.419066 140197563971328 logging_writer.py:48] [165700] global_step=165700, grad_norm=8.590760231018066, loss=0.9786578416824341
I0308 05:05:06.721373 140197572364032 logging_writer.py:48] [165800] global_step=165800, grad_norm=8.46191120147705, loss=1.0106838941574097
I0308 05:05:45.807320 140197563971328 logging_writer.py:48] [165900] global_step=165900, grad_norm=7.800049304962158, loss=0.9110902547836304
I0308 05:06:25.130726 140197572364032 logging_writer.py:48] [166000] global_step=166000, grad_norm=7.71674919128418, loss=0.9352618455886841
I0308 05:07:04.452268 140197563971328 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.996277332305908, loss=0.9707565307617188
I0308 05:07:44.027624 140197572364032 logging_writer.py:48] [166200] global_step=166200, grad_norm=8.920510292053223, loss=1.0038374662399292
I0308 05:08:24.254500 140197563971328 logging_writer.py:48] [166300] global_step=166300, grad_norm=7.665589809417725, loss=0.9890244603157043
2025-03-08 05:08:53.723839: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:09:04.109001 140197572364032 logging_writer.py:48] [166400] global_step=166400, grad_norm=7.977351665496826, loss=0.9384681582450867
I0308 05:09:43.717188 140197563971328 logging_writer.py:48] [166500] global_step=166500, grad_norm=7.479398727416992, loss=0.8912038207054138
I0308 05:10:23.336513 140197572364032 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.811657905578613, loss=0.8847242593765259
I0308 05:11:02.561827 140197563971328 logging_writer.py:48] [166700] global_step=166700, grad_norm=8.028341293334961, loss=1.0085920095443726
I0308 05:11:41.811513 140197572364032 logging_writer.py:48] [166800] global_step=166800, grad_norm=8.490671157836914, loss=0.9598779678344727
I0308 05:12:21.451532 140197563971328 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.903099536895752, loss=0.9370478391647339
I0308 05:12:41.305083 140352918893760 spec.py:321] Evaluating on the training split.
I0308 05:12:54.321429 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 05:13:33.058055 140352918893760 spec.py:349] Evaluating on the test split.
I0308 05:13:34.786056 140352918893760 submission_runner.py:469] Time since start: 72145.95s, 	Step: 166952, 	{'train/accuracy': 0.88285231590271, 'train/loss': 0.4053460359573364, 'validation/accuracy': 0.7495999932289124, 'validation/loss': 1.0118756294250488, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.7067680358886719, 'test/num_examples': 10000, 'score': 65326.05572628975, 'total_duration': 72145.94945240021, 'accumulated_submission_time': 65326.05572628975, 'accumulated_eval_time': 6781.696949958801, 'accumulated_logging_time': 19.354141235351562}
I0308 05:13:34.880516 140197572364032 logging_writer.py:48] [166952] accumulated_eval_time=6781.7, accumulated_logging_time=19.3541, accumulated_submission_time=65326.1, global_step=166952, preemption_count=0, score=65326.1, test/accuracy=0.6313, test/loss=1.70677, test/num_examples=10000, total_duration=72145.9, train/accuracy=0.882852, train/loss=0.405346, validation/accuracy=0.7496, validation/loss=1.01188, validation/num_examples=50000
I0308 05:13:54.344892 140197563971328 logging_writer.py:48] [167000] global_step=167000, grad_norm=8.255573272705078, loss=0.9941465854644775
I0308 05:14:33.611146 140197572364032 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.518228530883789, loss=0.9122039675712585
I0308 05:15:12.989117 140197563971328 logging_writer.py:48] [167200] global_step=167200, grad_norm=8.340490341186523, loss=1.00338876247406
I0308 05:15:52.701917 140197572364032 logging_writer.py:48] [167300] global_step=167300, grad_norm=7.744598388671875, loss=0.8649647831916809
I0308 05:16:32.244416 140197563971328 logging_writer.py:48] [167400] global_step=167400, grad_norm=8.940013885498047, loss=0.9020416736602783
I0308 05:17:11.992963 140197572364032 logging_writer.py:48] [167500] global_step=167500, grad_norm=7.186197280883789, loss=0.8280935287475586
I0308 05:17:51.280936 140197563971328 logging_writer.py:48] [167600] global_step=167600, grad_norm=8.425317764282227, loss=0.9461957216262817
2025-03-08 05:18:04.706685: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:18:30.500884 140197572364032 logging_writer.py:48] [167700] global_step=167700, grad_norm=7.608827590942383, loss=0.8875945806503296
I0308 05:19:08.912538 140197563971328 logging_writer.py:48] [167800] global_step=167800, grad_norm=8.276387214660645, loss=0.9540436863899231
I0308 05:19:47.893872 140197572364032 logging_writer.py:48] [167900] global_step=167900, grad_norm=7.899402141571045, loss=0.9154924154281616
I0308 05:20:26.939397 140197563971328 logging_writer.py:48] [168000] global_step=168000, grad_norm=7.723135948181152, loss=0.9179400205612183
I0308 05:21:06.524545 140197572364032 logging_writer.py:48] [168100] global_step=168100, grad_norm=7.989102840423584, loss=1.0114320516586304
I0308 05:21:46.110200 140197563971328 logging_writer.py:48] [168200] global_step=168200, grad_norm=8.719627380371094, loss=0.9821360111236572
I0308 05:22:05.193533 140352918893760 spec.py:321] Evaluating on the training split.
I0308 05:22:17.951707 140352918893760 spec.py:333] Evaluating on the validation split.
I0308 05:22:48.303777 140352918893760 spec.py:349] Evaluating on the test split.
I0308 05:22:50.054535 140352918893760 submission_runner.py:469] Time since start: 72701.22s, 	Step: 168249, 	{'train/accuracy': 0.8884526491165161, 'train/loss': 0.38864967226982117, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.0168981552124023, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.7082061767578125, 'test/num_examples': 10000, 'score': 65836.15519762039, 'total_duration': 72701.2179467678, 'accumulated_submission_time': 65836.15519762039, 'accumulated_eval_time': 6826.55775809288, 'accumulated_logging_time': 19.50336265563965}
I0308 05:22:50.192848 140197572364032 logging_writer.py:48] [168249] accumulated_eval_time=6826.56, accumulated_logging_time=19.5034, accumulated_submission_time=65836.2, global_step=168249, preemption_count=0, score=65836.2, test/accuracy=0.6262, test/loss=1.70821, test/num_examples=10000, total_duration=72701.2, train/accuracy=0.888453, train/loss=0.38865, validation/accuracy=0.74724, validation/loss=1.0169, validation/num_examples=50000
I0308 05:23:10.808665 140197563971328 logging_writer.py:48] [168300] global_step=168300, grad_norm=8.685286521911621, loss=1.0022335052490234
I0308 05:23:50.699297 140197572364032 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.4797258377075195, loss=0.926141619682312
I0308 05:24:30.323068 140197563971328 logging_writer.py:48] [168500] global_step=168500, grad_norm=7.844207286834717, loss=0.9415425062179565
I0308 05:25:10.120138 140197572364032 logging_writer.py:48] [168600] global_step=168600, grad_norm=8.483550071716309, loss=0.930734395980835
I0308 05:25:49.795347 140197563971328 logging_writer.py:48] [168700] global_step=168700, grad_norm=8.993939399719238, loss=0.9661390781402588
I0308 05:26:29.680360 140197572364032 logging_writer.py:48] [168800] global_step=168800, grad_norm=7.8131208419799805, loss=0.8326976299285889
2025-03-08 05:26:59.981262: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:27:09.532027 140197563971328 logging_writer.py:48] [168900] global_step=168900, grad_norm=8.802497863769531, loss=0.8988128900527954
I0308 05:27:49.186499 140197572364032 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.990689754486084, loss=0.8706282377243042
I0308 05:28:28.563443 140197563971328 logging_writer.py:48] [169100] global_step=169100, grad_norm=8.282340049743652, loss=0.8200547099113464
I0308 05:29:08.209304 140197572364032 logging_writer.py:48] [169200] global_step=169200, grad_norm=9.573853492736816, loss=0.947995662689209
I0308 05:29:47.751531 140197563971328 logging_writer.py:48] [169300] global_step=169300, grad_norm=9.646568298339844, loss=0.9996063709259033
I0308 05:30:27.116635 140197572364032 logging_writer.py:48] [169400] global_step=169400, grad_norm=8.70141887664795, loss=0.9412550926208496
I0308 05:31:06.562761 140197563971328 logging_writer.py:48] [169500] global_step=169500, grad_norm=9.233122825622559, loss=0.9856462478637695
I0308 05:31:20.329763 140197572364032 logging_writer.py:48] [169536] global_step=169536, preemption_count=0, score=66346
I0308 05:31:21.903600 140352918893760 submission_runner.py:646] Tuning trial 3/5
I0308 05:31:21.903784 140352918893760 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0308 05:31:21.907958 140352918893760 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008769132546149194, 'train/loss': 6.91168737411499, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.911850929260254, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.9116034507751465, 'test/num_examples': 10000, 'score': 59.21358060836792, 'total_duration': 140.26218605041504, 'accumulated_submission_time': 59.21358060836792, 'accumulated_eval_time': 81.04838967323303, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1354, {'train/accuracy': 0.059091996401548386, 'train/loss': 5.493776798248291, 'validation/accuracy': 0.05179999768733978, 'validation/loss': 5.573646068572998, 'validation/num_examples': 50000, 'test/accuracy': 0.03630000352859497, 'test/loss': 5.78989315032959, 'test/num_examples': 10000, 'score': 569.3945550918579, 'total_duration': 698.7443125247955, 'accumulated_submission_time': 569.3945550918579, 'accumulated_eval_time': 129.16638731956482, 'accumulated_logging_time': 0.020788908004760742, 'global_step': 1354, 'preemption_count': 0}), (2687, {'train/accuracy': 0.14706233143806458, 'train/loss': 4.467044830322266, 'validation/accuracy': 0.1281999945640564, 'validation/loss': 4.618517875671387, 'validation/num_examples': 50000, 'test/accuracy': 0.09490000456571579, 'test/loss': 5.007242202758789, 'test/num_examples': 10000, 'score': 1079.442931652069, 'total_duration': 1241.496834039688, 'accumulated_submission_time': 1079.442931652069, 'accumulated_eval_time': 161.68802952766418, 'accumulated_logging_time': 0.05840349197387695, 'global_step': 2687, 'preemption_count': 0}), (4021, {'train/accuracy': 0.23419561982154846, 'train/loss': 3.7651970386505127, 'validation/accuracy': 0.20155999064445496, 'validation/loss': 4.000546455383301, 'validation/num_examples': 50000, 'test/accuracy': 0.1558000147342682, 'test/loss': 4.543863296508789, 'test/num_examples': 10000, 'score': 1589.5220942497253, 'total_duration': 1783.8335990905762, 'accumulated_submission_time': 1589.5220942497253, 'accumulated_eval_time': 193.78435516357422, 'accumulated_logging_time': 0.09580397605895996, 'global_step': 4021, 'preemption_count': 0}), (5348, {'train/accuracy': 0.3355787694454193, 'train/loss': 3.090468645095825, 'validation/accuracy': 0.29679998755455017, 'validation/loss': 3.333317756652832, 'validation/num_examples': 50000, 'test/accuracy': 0.22510001063346863, 'test/loss': 3.9231512546539307, 'test/num_examples': 10000, 'score': 2099.42506980896, 'total_duration': 2327.261080265045, 'accumulated_submission_time': 2099.42506980896, 'accumulated_eval_time': 227.1757457256317, 'accumulated_logging_time': 0.11387968063354492, 'global_step': 5348, 'preemption_count': 0}), (6682, {'train/accuracy': 0.39568716287612915, 'train/loss': 2.6923294067382812, 'validation/accuracy': 0.354559987783432, 'validation/loss': 2.964087963104248, 'validation/num_examples': 50000, 'test/accuracy': 0.2713000178337097, 'test/loss': 3.6099040508270264, 'test/num_examples': 10000, 'score': 2609.6209440231323, 'total_duration': 2872.0320682525635, 'accumulated_submission_time': 2609.6209440231323, 'accumulated_eval_time': 261.6079971790314, 'accumulated_logging_time': 0.1436445713043213, 'global_step': 6682, 'preemption_count': 0}), (8012, {'train/accuracy': 0.4654017686843872, 'train/loss': 2.3428409099578857, 'validation/accuracy': 0.4157799780368805, 'validation/loss': 2.6067278385162354, 'validation/num_examples': 50000, 'test/accuracy': 0.31690001487731934, 'test/loss': 3.3188130855560303, 'test/num_examples': 10000, 'score': 3119.636923313141, 'total_duration': 3416.199696779251, 'accumulated_submission_time': 3119.636923313141, 'accumulated_eval_time': 295.5751292705536, 'accumulated_logging_time': 0.21811509132385254, 'global_step': 8012, 'preemption_count': 0}), (9342, {'train/accuracy': 0.5025709271430969, 'train/loss': 2.1312415599823, 'validation/accuracy': 0.4534199833869934, 'validation/loss': 2.3976571559906006, 'validation/num_examples': 50000, 'test/accuracy': 0.34700000286102295, 'test/loss': 3.103010892868042, 'test/num_examples': 10000, 'score': 3629.697919845581, 'total_duration': 3961.8107562065125, 'accumulated_submission_time': 3629.697919845581, 'accumulated_eval_time': 330.9278519153595, 'accumulated_logging_time': 0.3063318729400635, 'global_step': 9342, 'preemption_count': 0}), (10668, {'train/accuracy': 0.5410953164100647, 'train/loss': 1.9475765228271484, 'validation/accuracy': 0.49091997742652893, 'validation/loss': 2.207526683807373, 'validation/num_examples': 50000, 'test/accuracy': 0.3695000112056732, 'test/loss': 2.921912670135498, 'test/num_examples': 10000, 'score': 4139.552673101425, 'total_duration': 4508.319432735443, 'accumulated_submission_time': 4139.552673101425, 'accumulated_eval_time': 367.41164231300354, 'accumulated_logging_time': 0.3439664840698242, 'global_step': 10668, 'preemption_count': 0}), (11991, {'train/accuracy': 0.5500836968421936, 'train/loss': 1.8846064805984497, 'validation/accuracy': 0.500819981098175, 'validation/loss': 2.170717716217041, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 2.9513235092163086, 'test/num_examples': 10000, 'score': 4649.585605144501, 'total_duration': 5060.706884860992, 'accumulated_submission_time': 4649.585605144501, 'accumulated_eval_time': 409.4598104953766, 'accumulated_logging_time': 0.47458720207214355, 'global_step': 11991, 'preemption_count': 0}), (13313, {'train/accuracy': 0.5772281289100647, 'train/loss': 1.7610937356948853, 'validation/accuracy': 0.523099958896637, 'validation/loss': 2.0334198474884033, 'validation/num_examples': 50000, 'test/accuracy': 0.4066000282764435, 'test/loss': 2.750124216079712, 'test/num_examples': 10000, 'score': 5159.578532218933, 'total_duration': 5612.253901004791, 'accumulated_submission_time': 5159.578532218933, 'accumulated_eval_time': 450.77318930625916, 'accumulated_logging_time': 0.5811779499053955, 'global_step': 13313, 'preemption_count': 0}), (14632, {'train/accuracy': 0.5929926633834839, 'train/loss': 1.6760523319244385, 'validation/accuracy': 0.539680004119873, 'validation/loss': 1.9616038799285889, 'validation/num_examples': 50000, 'test/accuracy': 0.4223000109195709, 'test/loss': 2.6981382369995117, 'test/num_examples': 10000, 'score': 5669.556174516678, 'total_duration': 6166.8834710121155, 'accumulated_submission_time': 5669.556174516678, 'accumulated_eval_time': 495.11451840400696, 'accumulated_logging_time': 0.7391164302825928, 'global_step': 14632, 'preemption_count': 0}), (15951, {'train/accuracy': 0.5925143361091614, 'train/loss': 1.6730601787567139, 'validation/accuracy': 0.5413599610328674, 'validation/loss': 1.9574065208435059, 'validation/num_examples': 50000, 'test/accuracy': 0.4173000156879425, 'test/loss': 2.7239809036254883, 'test/num_examples': 10000, 'score': 6179.263436555862, 'total_duration': 6719.514345169067, 'accumulated_submission_time': 6179.263436555862, 'accumulated_eval_time': 537.6484472751617, 'accumulated_logging_time': 0.996953010559082, 'global_step': 15951, 'preemption_count': 0}), (17231, {'train/accuracy': 0.6097137928009033, 'train/loss': 1.5923843383789062, 'validation/accuracy': 0.5523200035095215, 'validation/loss': 1.9010858535766602, 'validation/num_examples': 50000, 'test/accuracy': 0.4349000155925751, 'test/loss': 2.673339605331421, 'test/num_examples': 10000, 'score': 6689.04709815979, 'total_duration': 7273.19722032547, 'accumulated_submission_time': 6689.04709815979, 'accumulated_eval_time': 581.3077845573425, 'accumulated_logging_time': 1.106475591659546, 'global_step': 17231, 'preemption_count': 0}), (18557, {'train/accuracy': 0.6106305718421936, 'train/loss': 1.590878963470459, 'validation/accuracy': 0.5554599761962891, 'validation/loss': 1.8810646533966064, 'validation/num_examples': 50000, 'test/accuracy': 0.4351000189781189, 'test/loss': 2.635240316390991, 'test/num_examples': 10000, 'score': 7198.9117159843445, 'total_duration': 7822.58359003067, 'accumulated_submission_time': 7198.9117159843445, 'accumulated_eval_time': 620.582596540451, 'accumulated_logging_time': 1.2362072467803955, 'global_step': 18557, 'preemption_count': 0}), (19881, {'train/accuracy': 0.6021006107330322, 'train/loss': 1.6173579692840576, 'validation/accuracy': 0.5558199882507324, 'validation/loss': 1.877522349357605, 'validation/num_examples': 50000, 'test/accuracy': 0.4326000213623047, 'test/loss': 2.6301968097686768, 'test/num_examples': 10000, 'score': 7708.774195432663, 'total_duration': 8375.675012111664, 'accumulated_submission_time': 7708.774195432663, 'accumulated_eval_time': 663.525942325592, 'accumulated_logging_time': 1.4039108753204346, 'global_step': 19881, 'preemption_count': 0}), (21204, {'train/accuracy': 0.6167888641357422, 'train/loss': 1.5513757467269897, 'validation/accuracy': 0.5619400143623352, 'validation/loss': 1.8469746112823486, 'validation/num_examples': 50000, 'test/accuracy': 0.4458000063896179, 'test/loss': 2.568434238433838, 'test/num_examples': 10000, 'score': 8218.84060716629, 'total_duration': 8931.406993150711, 'accumulated_submission_time': 8218.84060716629, 'accumulated_eval_time': 709.0334758758545, 'accumulated_logging_time': 1.437941074371338, 'global_step': 21204, 'preemption_count': 0}), (22522, {'train/accuracy': 0.6152742505073547, 'train/loss': 1.5636672973632812, 'validation/accuracy': 0.5654799938201904, 'validation/loss': 1.8313333988189697, 'validation/num_examples': 50000, 'test/accuracy': 0.44790002703666687, 'test/loss': 2.5066232681274414, 'test/num_examples': 10000, 'score': 8728.814687252045, 'total_duration': 9480.182833194733, 'accumulated_submission_time': 8728.814687252045, 'accumulated_eval_time': 747.6689591407776, 'accumulated_logging_time': 1.4795784950256348, 'global_step': 22522, 'preemption_count': 0}), (23835, {'train/accuracy': 0.6325932741165161, 'train/loss': 1.482499599456787, 'validation/accuracy': 0.5803200006484985, 'validation/loss': 1.760344386100769, 'validation/num_examples': 50000, 'test/accuracy': 0.46410003304481506, 'test/loss': 2.4846160411834717, 'test/num_examples': 10000, 'score': 9238.612963438034, 'total_duration': 10029.003471374512, 'accumulated_submission_time': 9238.612963438034, 'accumulated_eval_time': 786.4633274078369, 'accumulated_logging_time': 1.5778069496154785, 'global_step': 23835, 'preemption_count': 0}), (25150, {'train/accuracy': 0.6347058415412903, 'train/loss': 1.473044991493225, 'validation/accuracy': 0.58406001329422, 'validation/loss': 1.7415127754211426, 'validation/num_examples': 50000, 'test/accuracy': 0.4538000226020813, 'test/loss': 2.4774622917175293, 'test/num_examples': 10000, 'score': 9748.636342048645, 'total_duration': 10580.68021440506, 'accumulated_submission_time': 9748.636342048645, 'accumulated_eval_time': 827.8675727844238, 'accumulated_logging_time': 1.6966907978057861, 'global_step': 25150, 'preemption_count': 0}), (26468, {'train/accuracy': 0.6271523833274841, 'train/loss': 1.4970682859420776, 'validation/accuracy': 0.5855399966239929, 'validation/loss': 1.7396633625030518, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.4662978649139404, 'test/num_examples': 10000, 'score': 10258.644327402115, 'total_duration': 11133.77368092537, 'accumulated_submission_time': 10258.644327402115, 'accumulated_eval_time': 870.7361159324646, 'accumulated_logging_time': 1.7886905670166016, 'global_step': 26468, 'preemption_count': 0}), (27783, {'train/accuracy': 0.6384725570678711, 'train/loss': 1.4756149053573608, 'validation/accuracy': 0.5844599604606628, 'validation/loss': 1.7352402210235596, 'validation/num_examples': 50000, 'test/accuracy': 0.46320003271102905, 'test/loss': 2.448728322982788, 'test/num_examples': 10000, 'score': 10767.809199094772, 'total_duration': 11685.408977270126, 'accumulated_submission_time': 10767.809199094772, 'accumulated_eval_time': 912.3076181411743, 'accumulated_logging_time': 2.556812286376953, 'global_step': 27783, 'preemption_count': 0}), (29104, {'train/accuracy': 0.6354830861091614, 'train/loss': 1.4651622772216797, 'validation/accuracy': 0.5838599801063538, 'validation/loss': 1.7416489124298096, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4830355644226074, 'test/num_examples': 10000, 'score': 11277.531613349915, 'total_duration': 12236.78643321991, 'accumulated_submission_time': 11277.531613349915, 'accumulated_eval_time': 953.6822338104248, 'accumulated_logging_time': 2.7083470821380615, 'global_step': 29104, 'preemption_count': 0}), (30418, {'train/accuracy': 0.6411232352256775, 'train/loss': 1.4582377672195435, 'validation/accuracy': 0.5916000008583069, 'validation/loss': 1.7062532901763916, 'validation/num_examples': 50000, 'test/accuracy': 0.4677000343799591, 'test/loss': 2.437067985534668, 'test/num_examples': 10000, 'score': 11787.374530792236, 'total_duration': 12787.498551368713, 'accumulated_submission_time': 11787.374530792236, 'accumulated_eval_time': 994.2793338298798, 'accumulated_logging_time': 2.8469278812408447, 'global_step': 30418, 'preemption_count': 0}), (31738, {'train/accuracy': 0.638113796710968, 'train/loss': 1.456536889076233, 'validation/accuracy': 0.5877999663352966, 'validation/loss': 1.7231141328811646, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.4735653400421143, 'test/num_examples': 10000, 'score': 12297.110484361649, 'total_duration': 13336.790102481842, 'accumulated_submission_time': 12297.110484361649, 'accumulated_eval_time': 1033.5379920005798, 'accumulated_logging_time': 3.0118300914764404, 'global_step': 31738, 'preemption_count': 0}), (33061, {'train/accuracy': 0.6460060477256775, 'train/loss': 1.4213043451309204, 'validation/accuracy': 0.5931999683380127, 'validation/loss': 1.6828004121780396, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.418006658554077, 'test/num_examples': 10000, 'score': 12807.092128276825, 'total_duration': 13889.081949234009, 'accumulated_submission_time': 12807.092128276825, 'accumulated_eval_time': 1075.588097333908, 'accumulated_logging_time': 3.139647960662842, 'global_step': 33061, 'preemption_count': 0}), (34377, {'train/accuracy': 0.6480388641357422, 'train/loss': 1.4073448181152344, 'validation/accuracy': 0.598800003528595, 'validation/loss': 1.6664913892745972, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.385242462158203, 'test/num_examples': 10000, 'score': 13316.98905658722, 'total_duration': 14437.333611011505, 'accumulated_submission_time': 13316.98905658722, 'accumulated_eval_time': 1113.6861951351166, 'accumulated_logging_time': 3.261392116546631, 'global_step': 34377, 'preemption_count': 0}), (35695, {'train/accuracy': 0.6469228267669678, 'train/loss': 1.4248980283737183, 'validation/accuracy': 0.5960999727249146, 'validation/loss': 1.6695075035095215, 'validation/num_examples': 50000, 'test/accuracy': 0.4757000207901001, 'test/loss': 2.380704402923584, 'test/num_examples': 10000, 'score': 13826.821261644363, 'total_duration': 14982.672957897186, 'accumulated_submission_time': 13826.821261644363, 'accumulated_eval_time': 1148.9480550289154, 'accumulated_logging_time': 3.375527858734131, 'global_step': 35695, 'preemption_count': 0}), (37016, {'train/accuracy': 0.6443917155265808, 'train/loss': 1.4418485164642334, 'validation/accuracy': 0.597599983215332, 'validation/loss': 1.6946007013320923, 'validation/num_examples': 50000, 'test/accuracy': 0.4725000262260437, 'test/loss': 2.4381091594696045, 'test/num_examples': 10000, 'score': 14336.823198318481, 'total_duration': 15530.5641913414, 'accumulated_submission_time': 14336.823198318481, 'accumulated_eval_time': 1186.57777094841, 'accumulated_logging_time': 3.503100872039795, 'global_step': 37016, 'preemption_count': 0}), (38334, {'train/accuracy': 0.6358617544174194, 'train/loss': 1.4668822288513184, 'validation/accuracy': 0.587660014629364, 'validation/loss': 1.7146292924880981, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4165847301483154, 'test/num_examples': 10000, 'score': 14846.957060098648, 'total_duration': 16074.550265073776, 'accumulated_submission_time': 14846.957060098648, 'accumulated_eval_time': 1220.1547420024872, 'accumulated_logging_time': 3.6423532962799072, 'global_step': 38334, 'preemption_count': 0}), (39660, {'train/accuracy': 0.6470822691917419, 'train/loss': 1.414178490638733, 'validation/accuracy': 0.5989599823951721, 'validation/loss': 1.6651763916015625, 'validation/num_examples': 50000, 'test/accuracy': 0.47690001130104065, 'test/loss': 2.404301404953003, 'test/num_examples': 10000, 'score': 15356.887708425522, 'total_duration': 16623.40948367119, 'accumulated_submission_time': 15356.887708425522, 'accumulated_eval_time': 1258.7726547718048, 'accumulated_logging_time': 3.8216123580932617, 'global_step': 39660, 'preemption_count': 0}), (40981, {'train/accuracy': 0.6447504758834839, 'train/loss': 1.428864598274231, 'validation/accuracy': 0.5973799824714661, 'validation/loss': 1.664876937866211, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.373089551925659, 'test/num_examples': 10000, 'score': 15866.78595328331, 'total_duration': 17170.578610658646, 'accumulated_submission_time': 15866.78595328331, 'accumulated_eval_time': 1295.770164489746, 'accumulated_logging_time': 3.9590885639190674, 'global_step': 40981, 'preemption_count': 0}), (42302, {'train/accuracy': 0.6488759517669678, 'train/loss': 1.4041895866394043, 'validation/accuracy': 0.5999999642372131, 'validation/loss': 1.6582494974136353, 'validation/num_examples': 50000, 'test/accuracy': 0.4732000231742859, 'test/loss': 2.4052813053131104, 'test/num_examples': 10000, 'score': 16376.734061479568, 'total_duration': 17718.105592489243, 'accumulated_submission_time': 16376.734061479568, 'accumulated_eval_time': 1333.0827143192291, 'accumulated_logging_time': 4.0905561447143555, 'global_step': 42302, 'preemption_count': 0}), (43618, {'train/accuracy': 0.6426379084587097, 'train/loss': 1.4269685745239258, 'validation/accuracy': 0.5949400067329407, 'validation/loss': 1.6832212209701538, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.40596604347229, 'test/num_examples': 10000, 'score': 16886.791135787964, 'total_duration': 18263.206958532333, 'accumulated_submission_time': 16886.791135787964, 'accumulated_eval_time': 1367.8810138702393, 'accumulated_logging_time': 4.205563068389893, 'global_step': 43618, 'preemption_count': 0}), (44937, {'train/accuracy': 0.6607541441917419, 'train/loss': 1.3578873872756958, 'validation/accuracy': 0.6111800074577332, 'validation/loss': 1.6031805276870728, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.3247499465942383, 'test/num_examples': 10000, 'score': 17396.713400125504, 'total_duration': 18807.67843437195, 'accumulated_submission_time': 17396.713400125504, 'accumulated_eval_time': 1402.1991946697235, 'accumulated_logging_time': 4.3023681640625, 'global_step': 44937, 'preemption_count': 0}), (46257, {'train/accuracy': 0.6428172588348389, 'train/loss': 1.4479413032531738, 'validation/accuracy': 0.5979399681091309, 'validation/loss': 1.6964576244354248, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.3945939540863037, 'test/num_examples': 10000, 'score': 17906.49174261093, 'total_duration': 19358.183930158615, 'accumulated_submission_time': 17906.49174261093, 'accumulated_eval_time': 1442.6818182468414, 'accumulated_logging_time': 4.415114402770996, 'global_step': 46257, 'preemption_count': 0}), (47574, {'train/accuracy': 0.6429567933082581, 'train/loss': 1.4278398752212524, 'validation/accuracy': 0.6006399989128113, 'validation/loss': 1.6661896705627441, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.3839430809020996, 'test/num_examples': 10000, 'score': 18416.60352039337, 'total_duration': 19903.124287366867, 'accumulated_submission_time': 18416.60352039337, 'accumulated_eval_time': 1477.2599959373474, 'accumulated_logging_time': 4.535336494445801, 'global_step': 47574, 'preemption_count': 0}), (48890, {'train/accuracy': 0.6544961333274841, 'train/loss': 1.3837900161743164, 'validation/accuracy': 0.6061199903488159, 'validation/loss': 1.633638858795166, 'validation/num_examples': 50000, 'test/accuracy': 0.47760000824928284, 'test/loss': 2.377366065979004, 'test/num_examples': 10000, 'score': 18926.72492337227, 'total_duration': 20453.636912345886, 'accumulated_submission_time': 18926.72492337227, 'accumulated_eval_time': 1517.3963639736176, 'accumulated_logging_time': 4.659128189086914, 'global_step': 48890, 'preemption_count': 0}), (50209, {'train/accuracy': 0.6552136540412903, 'train/loss': 1.374187707901001, 'validation/accuracy': 0.6062799692153931, 'validation/loss': 1.6164963245391846, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.331477165222168, 'test/num_examples': 10000, 'score': 19436.585611343384, 'total_duration': 20997.752379179, 'accumulated_submission_time': 19436.585611343384, 'accumulated_eval_time': 1551.3502197265625, 'accumulated_logging_time': 4.833312749862671, 'global_step': 50209, 'preemption_count': 0}), (51527, {'train/accuracy': 0.654316782951355, 'train/loss': 1.392316460609436, 'validation/accuracy': 0.6056599617004395, 'validation/loss': 1.6421709060668945, 'validation/num_examples': 50000, 'test/accuracy': 0.48490002751350403, 'test/loss': 2.364668607711792, 'test/num_examples': 10000, 'score': 19946.409878253937, 'total_duration': 21542.82337975502, 'accumulated_submission_time': 19946.409878253937, 'accumulated_eval_time': 1586.2558369636536, 'accumulated_logging_time': 5.044023513793945, 'global_step': 51527, 'preemption_count': 0}), (52846, {'train/accuracy': 0.6613919138908386, 'train/loss': 1.3567700386047363, 'validation/accuracy': 0.6155200004577637, 'validation/loss': 1.5953295230865479, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.300241470336914, 'test/num_examples': 10000, 'score': 20456.194468975067, 'total_duration': 22090.923674345016, 'accumulated_submission_time': 20456.194468975067, 'accumulated_eval_time': 1624.3230538368225, 'accumulated_logging_time': 5.1561198234558105, 'global_step': 52846, 'preemption_count': 0}), (54168, {'train/accuracy': 0.6633848547935486, 'train/loss': 1.3555982112884521, 'validation/accuracy': 0.6178399920463562, 'validation/loss': 1.5910332202911377, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.305330276489258, 'test/num_examples': 10000, 'score': 20966.108649492264, 'total_duration': 22638.246022224426, 'accumulated_submission_time': 20966.108649492264, 'accumulated_eval_time': 1661.4758970737457, 'accumulated_logging_time': 5.275086164474487, 'global_step': 54168, 'preemption_count': 0}), (55489, {'train/accuracy': 0.661531388759613, 'train/loss': 1.3388372659683228, 'validation/accuracy': 0.6109799742698669, 'validation/loss': 1.6009527444839478, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.3151907920837402, 'test/num_examples': 10000, 'score': 21476.029215574265, 'total_duration': 23204.76149201393, 'accumulated_submission_time': 21476.029215574265, 'accumulated_eval_time': 1717.8261032104492, 'accumulated_logging_time': 5.385563611984253, 'global_step': 55489, 'preemption_count': 0}), (56807, {'train/accuracy': 0.6571269035339355, 'train/loss': 1.3812739849090576, 'validation/accuracy': 0.6097800135612488, 'validation/loss': 1.6252782344818115, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3605759143829346, 'test/num_examples': 10000, 'score': 21986.00971031189, 'total_duration': 23771.59955573082, 'accumulated_submission_time': 21986.00971031189, 'accumulated_eval_time': 1774.39249253273, 'accumulated_logging_time': 5.536667108535767, 'global_step': 56807, 'preemption_count': 0}), (58037, {'train/accuracy': 0.6680285334587097, 'train/loss': 1.3184261322021484, 'validation/accuracy': 0.6144799590110779, 'validation/loss': 1.5974048376083374, 'validation/num_examples': 50000, 'test/accuracy': 0.4896000325679779, 'test/loss': 2.3437349796295166, 'test/num_examples': 10000, 'score': 22495.949635267258, 'total_duration': 24346.544462919235, 'accumulated_submission_time': 22495.949635267258, 'accumulated_eval_time': 1839.1244399547577, 'accumulated_logging_time': 5.676953554153442, 'global_step': 58037, 'preemption_count': 0}), (59346, {'train/accuracy': 0.64453125, 'train/loss': 1.429885983467102, 'validation/accuracy': 0.5986199975013733, 'validation/loss': 1.6816864013671875, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.440908432006836, 'test/num_examples': 10000, 'score': 23005.76326084137, 'total_duration': 24918.510147094727, 'accumulated_submission_time': 23005.76326084137, 'accumulated_eval_time': 1900.992432832718, 'accumulated_logging_time': 5.813225507736206, 'global_step': 59346, 'preemption_count': 0}), (60647, {'train/accuracy': 0.6511678695678711, 'train/loss': 1.397667407989502, 'validation/accuracy': 0.602840006351471, 'validation/loss': 1.6418415307998657, 'validation/num_examples': 50000, 'test/accuracy': 0.47940000891685486, 'test/loss': 2.380911350250244, 'test/num_examples': 10000, 'score': 23515.648574113846, 'total_duration': 25490.656725645065, 'accumulated_submission_time': 23515.648574113846, 'accumulated_eval_time': 1963.022631406784, 'accumulated_logging_time': 5.893031597137451, 'global_step': 60647, 'preemption_count': 0}), (61941, {'train/accuracy': 0.6648397445678711, 'train/loss': 1.3401981592178345, 'validation/accuracy': 0.6149199604988098, 'validation/loss': 1.5920711755752563, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.3203177452087402, 'test/num_examples': 10000, 'score': 24025.66006588936, 'total_duration': 26051.398130893707, 'accumulated_submission_time': 24025.66006588936, 'accumulated_eval_time': 2013.4244344234467, 'accumulated_logging_time': 6.06721305847168, 'global_step': 61941, 'preemption_count': 0}), (63232, {'train/accuracy': 0.6753228306770325, 'train/loss': 1.284428358078003, 'validation/accuracy': 0.6259799599647522, 'validation/loss': 1.5430666208267212, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.2611074447631836, 'test/num_examples': 10000, 'score': 24535.41486644745, 'total_duration': 26620.780292749405, 'accumulated_submission_time': 24535.41486644745, 'accumulated_eval_time': 2072.762209415436, 'accumulated_logging_time': 6.205001354217529, 'global_step': 63232, 'preemption_count': 0}), (64519, {'train/accuracy': 0.6681281924247742, 'train/loss': 1.325324535369873, 'validation/accuracy': 0.6156399846076965, 'validation/loss': 1.583498239517212, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.3166275024414062, 'test/num_examples': 10000, 'score': 25045.1871778965, 'total_duration': 27190.860607624054, 'accumulated_submission_time': 25045.1871778965, 'accumulated_eval_time': 2132.777435541153, 'accumulated_logging_time': 6.342515230178833, 'global_step': 64519, 'preemption_count': 0}), (65822, {'train/accuracy': 0.6639030575752258, 'train/loss': 1.3512518405914307, 'validation/accuracy': 0.6155799627304077, 'validation/loss': 1.5866771936416626, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.323444366455078, 'test/num_examples': 10000, 'score': 25555.278134822845, 'total_duration': 27768.452460050583, 'accumulated_submission_time': 25555.278134822845, 'accumulated_eval_time': 2200.0127398967743, 'accumulated_logging_time': 6.463808536529541, 'global_step': 65822, 'preemption_count': 0}), (67136, {'train/accuracy': 0.6723533272743225, 'train/loss': 1.2799941301345825, 'validation/accuracy': 0.6279999613761902, 'validation/loss': 1.5221290588378906, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.2592999935150146, 'test/num_examples': 10000, 'score': 26065.21945309639, 'total_duration': 28335.591351270676, 'accumulated_submission_time': 26065.21945309639, 'accumulated_eval_time': 2256.9694254398346, 'accumulated_logging_time': 6.569955587387085, 'global_step': 67136, 'preemption_count': 0}), (68420, {'train/accuracy': 0.671894907951355, 'train/loss': 1.2970863580703735, 'validation/accuracy': 0.6215199828147888, 'validation/loss': 1.5568580627441406, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.2685935497283936, 'test/num_examples': 10000, 'score': 26575.14043903351, 'total_duration': 28909.892155647278, 'accumulated_submission_time': 26575.14043903351, 'accumulated_eval_time': 2321.029307603836, 'accumulated_logging_time': 6.747398614883423, 'global_step': 68420, 'preemption_count': 0}), (69715, {'train/accuracy': 0.6678292155265808, 'train/loss': 1.3177603483200073, 'validation/accuracy': 0.6180799603462219, 'validation/loss': 1.5713753700256348, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3242578506469727, 'test/num_examples': 10000, 'score': 27085.180109977722, 'total_duration': 29483.076415777206, 'accumulated_submission_time': 27085.180109977722, 'accumulated_eval_time': 2383.873781442642, 'accumulated_logging_time': 6.89722204208374, 'global_step': 69715, 'preemption_count': 0}), (71007, {'train/accuracy': 0.6826171875, 'train/loss': 1.2628735303878784, 'validation/accuracy': 0.63264000415802, 'validation/loss': 1.5083205699920654, 'validation/num_examples': 50000, 'test/accuracy': 0.5042000412940979, 'test/loss': 2.244354009628296, 'test/num_examples': 10000, 'score': 27594.98939538002, 'total_duration': 30052.108595609665, 'accumulated_submission_time': 27594.98939538002, 'accumulated_eval_time': 2442.787286758423, 'accumulated_logging_time': 7.0539679527282715, 'global_step': 71007, 'preemption_count': 0}), (72301, {'train/accuracy': 0.6799266338348389, 'train/loss': 1.2744773626327515, 'validation/accuracy': 0.6286399960517883, 'validation/loss': 1.5363175868988037, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2561392784118652, 'test/num_examples': 10000, 'score': 28104.866969823837, 'total_duration': 30622.38880801201, 'accumulated_submission_time': 28104.866969823837, 'accumulated_eval_time': 2502.883547306061, 'accumulated_logging_time': 7.203964948654175, 'global_step': 72301, 'preemption_count': 0}), (73588, {'train/accuracy': 0.6809430718421936, 'train/loss': 1.259148120880127, 'validation/accuracy': 0.627519965171814, 'validation/loss': 1.5180516242980957, 'validation/num_examples': 50000, 'test/accuracy': 0.5052000284194946, 'test/loss': 2.226288080215454, 'test/num_examples': 10000, 'score': 28614.846009731293, 'total_duration': 31196.001319169998, 'accumulated_submission_time': 28614.846009731293, 'accumulated_eval_time': 2566.2555482387543, 'accumulated_logging_time': 7.3076252937316895, 'global_step': 73588, 'preemption_count': 0}), (74880, {'train/accuracy': 0.666035532951355, 'train/loss': 1.3196016550064087, 'validation/accuracy': 0.6209999918937683, 'validation/loss': 1.5700033903121948, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.256957530975342, 'test/num_examples': 10000, 'score': 29124.857882499695, 'total_duration': 31765.015270233154, 'accumulated_submission_time': 29124.857882499695, 'accumulated_eval_time': 2624.9514589309692, 'accumulated_logging_time': 7.458817005157471, 'global_step': 74880, 'preemption_count': 0}), (76177, {'train/accuracy': 0.6743662357330322, 'train/loss': 1.2831982374191284, 'validation/accuracy': 0.6276800036430359, 'validation/loss': 1.5420352220535278, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.305387258529663, 'test/num_examples': 10000, 'score': 29634.795800209045, 'total_duration': 32327.220051765442, 'accumulated_submission_time': 29634.795800209045, 'accumulated_eval_time': 2676.864230155945, 'accumulated_logging_time': 7.655877113342285, 'global_step': 76177, 'preemption_count': 0}), (77479, {'train/accuracy': 0.6734494566917419, 'train/loss': 1.2976173162460327, 'validation/accuracy': 0.6261799931526184, 'validation/loss': 1.5456987619400024, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.305140256881714, 'test/num_examples': 10000, 'score': 30144.764360666275, 'total_duration': 32892.17233514786, 'accumulated_submission_time': 30144.764360666275, 'accumulated_eval_time': 2731.5297803878784, 'accumulated_logging_time': 7.819450855255127, 'global_step': 77479, 'preemption_count': 0}), (78789, {'train/accuracy': 0.6911272406578064, 'train/loss': 1.2141087055206299, 'validation/accuracy': 0.6365799903869629, 'validation/loss': 1.4830577373504639, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.1785385608673096, 'test/num_examples': 10000, 'score': 30654.563216924667, 'total_duration': 33460.589171886444, 'accumulated_submission_time': 30654.563216924667, 'accumulated_eval_time': 2789.8544533252716, 'accumulated_logging_time': 7.959044694900513, 'global_step': 78789, 'preemption_count': 0}), (80087, {'train/accuracy': 0.6937180757522583, 'train/loss': 1.2197816371917725, 'validation/accuracy': 0.6360999941825867, 'validation/loss': 1.4961830377578735, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.233011245727539, 'test/num_examples': 10000, 'score': 31164.439919233322, 'total_duration': 34032.19050502777, 'accumulated_submission_time': 31164.439919233322, 'accumulated_eval_time': 2851.285222053528, 'accumulated_logging_time': 8.09725046157837, 'global_step': 80087, 'preemption_count': 0}), (81393, {'train/accuracy': 0.6854671239852905, 'train/loss': 1.2439196109771729, 'validation/accuracy': 0.6332399845123291, 'validation/loss': 1.5081596374511719, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2297580242156982, 'test/num_examples': 10000, 'score': 31674.395730495453, 'total_duration': 34598.60466694832, 'accumulated_submission_time': 31674.395730495453, 'accumulated_eval_time': 2907.4203991889954, 'accumulated_logging_time': 8.265035629272461, 'global_step': 81393, 'preemption_count': 0}), (82696, {'train/accuracy': 0.6929408311843872, 'train/loss': 1.2060176134109497, 'validation/accuracy': 0.6411799788475037, 'validation/loss': 1.4821573495864868, 'validation/num_examples': 50000, 'test/accuracy': 0.5143000483512878, 'test/loss': 2.2411344051361084, 'test/num_examples': 10000, 'score': 32184.08911204338, 'total_duration': 35171.62567663193, 'accumulated_submission_time': 32184.08911204338, 'accumulated_eval_time': 2970.3724806308746, 'accumulated_logging_time': 8.484735488891602, 'global_step': 82696, 'preemption_count': 0}), (83997, {'train/accuracy': 0.6819395422935486, 'train/loss': 1.2571889162063599, 'validation/accuracy': 0.6292999982833862, 'validation/loss': 1.531537652015686, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.2787511348724365, 'test/num_examples': 10000, 'score': 32693.98401594162, 'total_duration': 35738.61975502968, 'accumulated_submission_time': 32693.98401594162, 'accumulated_eval_time': 3027.1691522598267, 'accumulated_logging_time': 8.632344722747803, 'global_step': 83997, 'preemption_count': 0}), (85307, {'train/accuracy': 0.69242262840271, 'train/loss': 1.201663613319397, 'validation/accuracy': 0.6385799646377563, 'validation/loss': 1.482727289199829, 'validation/num_examples': 50000, 'test/accuracy': 0.5124000310897827, 'test/loss': 2.189887285232544, 'test/num_examples': 10000, 'score': 33203.931676626205, 'total_duration': 36309.06070280075, 'accumulated_submission_time': 33203.931676626205, 'accumulated_eval_time': 3087.367395401001, 'accumulated_logging_time': 8.771151542663574, 'global_step': 85307, 'preemption_count': 0}), (86604, {'train/accuracy': 0.6988002061843872, 'train/loss': 1.1646647453308105, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.4550102949142456, 'validation/num_examples': 50000, 'test/accuracy': 0.516800045967102, 'test/loss': 2.1701242923736572, 'test/num_examples': 10000, 'score': 33713.75606608391, 'total_duration': 36875.58327436447, 'accumulated_submission_time': 33713.75606608391, 'accumulated_eval_time': 3143.595734357834, 'accumulated_logging_time': 9.082584381103516, 'global_step': 86604, 'preemption_count': 0}), (87898, {'train/accuracy': 0.6991190910339355, 'train/loss': 1.1791150569915771, 'validation/accuracy': 0.6412000060081482, 'validation/loss': 1.477604627609253, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.1600022315979004, 'test/num_examples': 10000, 'score': 34223.408479213715, 'total_duration': 37444.092901706696, 'accumulated_submission_time': 34223.408479213715, 'accumulated_eval_time': 3202.0967190265656, 'accumulated_logging_time': 9.284285306930542, 'global_step': 87898, 'preemption_count': 0}), (89192, {'train/accuracy': 0.697265625, 'train/loss': 1.1688207387924194, 'validation/accuracy': 0.6422799825668335, 'validation/loss': 1.4667043685913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.195091724395752, 'test/num_examples': 10000, 'score': 34733.114661216736, 'total_duration': 38017.346732616425, 'accumulated_submission_time': 34733.114661216736, 'accumulated_eval_time': 3265.3459293842316, 'accumulated_logging_time': 9.423890590667725, 'global_step': 89192, 'preemption_count': 0}), (90492, {'train/accuracy': 0.6983617544174194, 'train/loss': 1.1918472051620483, 'validation/accuracy': 0.6398000121116638, 'validation/loss': 1.4658702611923218, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.1696877479553223, 'test/num_examples': 10000, 'score': 35242.96688723564, 'total_duration': 38593.49281334877, 'accumulated_submission_time': 35242.96688723564, 'accumulated_eval_time': 3331.2178497314453, 'accumulated_logging_time': 9.688908100128174, 'global_step': 90492, 'preemption_count': 0}), (91785, {'train/accuracy': 0.7052175998687744, 'train/loss': 1.1566941738128662, 'validation/accuracy': 0.6466000080108643, 'validation/loss': 1.4528599977493286, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.150705099105835, 'test/num_examples': 10000, 'score': 35752.9436879158, 'total_duration': 39165.55902886391, 'accumulated_submission_time': 35752.9436879158, 'accumulated_eval_time': 3393.0305194854736, 'accumulated_logging_time': 9.80940294265747, 'global_step': 91785, 'preemption_count': 0}), (93081, {'train/accuracy': 0.7030253410339355, 'train/loss': 1.1503082513809204, 'validation/accuracy': 0.6487199664115906, 'validation/loss': 1.4397367238998413, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1711199283599854, 'test/num_examples': 10000, 'score': 36263.06934309006, 'total_duration': 39737.45051789284, 'accumulated_submission_time': 36263.06934309006, 'accumulated_eval_time': 3454.528933286667, 'accumulated_logging_time': 9.922516822814941, 'global_step': 93081, 'preemption_count': 0}), (94372, {'train/accuracy': 0.7121531963348389, 'train/loss': 1.1211084127426147, 'validation/accuracy': 0.6522200107574463, 'validation/loss': 1.4152565002441406, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.1081056594848633, 'test/num_examples': 10000, 'score': 36772.82030248642, 'total_duration': 40307.11348390579, 'accumulated_submission_time': 36772.82030248642, 'accumulated_eval_time': 3514.187231063843, 'accumulated_logging_time': 10.021862030029297, 'global_step': 94372, 'preemption_count': 0}), (95668, {'train/accuracy': 0.7100805044174194, 'train/loss': 1.1191871166229248, 'validation/accuracy': 0.6510399580001831, 'validation/loss': 1.4259061813354492, 'validation/num_examples': 50000, 'test/accuracy': 0.5286000370979309, 'test/loss': 2.1263363361358643, 'test/num_examples': 10000, 'score': 37282.60960936546, 'total_duration': 40873.28606629372, 'accumulated_submission_time': 37282.60960936546, 'accumulated_eval_time': 3570.2856843471527, 'accumulated_logging_time': 10.138883113861084, 'global_step': 95668, 'preemption_count': 0}), (96956, {'train/accuracy': 0.7156209945678711, 'train/loss': 1.1100255250930786, 'validation/accuracy': 0.6535999774932861, 'validation/loss': 1.4162813425064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5283000469207764, 'test/loss': 2.1131956577301025, 'test/num_examples': 10000, 'score': 37792.304157972336, 'total_duration': 41449.264556884766, 'accumulated_submission_time': 37792.304157972336, 'accumulated_eval_time': 3636.2387998104095, 'accumulated_logging_time': 10.314347982406616, 'global_step': 96956, 'preemption_count': 0}), (98252, {'train/accuracy': 0.713289201259613, 'train/loss': 1.117990493774414, 'validation/accuracy': 0.6503399610519409, 'validation/loss': 1.4127005338668823, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.1397383213043213, 'test/num_examples': 10000, 'score': 38302.33400964737, 'total_duration': 42011.88674235344, 'accumulated_submission_time': 38302.33400964737, 'accumulated_eval_time': 3688.5661101341248, 'accumulated_logging_time': 10.423690557479858, 'global_step': 98252, 'preemption_count': 0}), (99540, {'train/accuracy': 0.7054368257522583, 'train/loss': 1.1409116983413696, 'validation/accuracy': 0.6470800042152405, 'validation/loss': 1.444724678993225, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.1654844284057617, 'test/num_examples': 10000, 'score': 38812.04327368736, 'total_duration': 42585.295284986496, 'accumulated_submission_time': 38812.04327368736, 'accumulated_eval_time': 3751.95658993721, 'accumulated_logging_time': 10.577909231185913, 'global_step': 99540, 'preemption_count': 0}), (100854, {'train/accuracy': 0.724629282951355, 'train/loss': 1.0570652484893799, 'validation/accuracy': 0.6636399626731873, 'validation/loss': 1.3612900972366333, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.066345691680908, 'test/num_examples': 10000, 'score': 39321.80171942711, 'total_duration': 43158.70108675957, 'accumulated_submission_time': 39321.80171942711, 'accumulated_eval_time': 3815.3002536296844, 'accumulated_logging_time': 10.727151870727539, 'global_step': 100854, 'preemption_count': 0}), (102143, {'train/accuracy': 0.7178930044174194, 'train/loss': 1.09544038772583, 'validation/accuracy': 0.6579200029373169, 'validation/loss': 1.3941709995269775, 'validation/num_examples': 50000, 'test/accuracy': 0.5315999984741211, 'test/loss': 2.109208822250366, 'test/num_examples': 10000, 'score': 39831.58630847931, 'total_duration': 43719.43748879433, 'accumulated_submission_time': 39831.58630847931, 'accumulated_eval_time': 3865.9604680538177, 'accumulated_logging_time': 10.864547491073608, 'global_step': 102143, 'preemption_count': 0}), (103431, {'train/accuracy': 0.7240114808082581, 'train/loss': 1.0633416175842285, 'validation/accuracy': 0.6632399559020996, 'validation/loss': 1.3685472011566162, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.0787832736968994, 'test/num_examples': 10000, 'score': 40341.621921777725, 'total_duration': 44292.40053629875, 'accumulated_submission_time': 40341.621921777725, 'accumulated_eval_time': 3928.552306652069, 'accumulated_logging_time': 11.045519351959229, 'global_step': 103431, 'preemption_count': 0}), (104720, {'train/accuracy': 0.7229551672935486, 'train/loss': 1.070923089981079, 'validation/accuracy': 0.6643999814987183, 'validation/loss': 1.3667947053909302, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0891666412353516, 'test/num_examples': 10000, 'score': 40851.740795850754, 'total_duration': 44844.991621255875, 'accumulated_submission_time': 40851.740795850754, 'accumulated_eval_time': 3970.7581713199615, 'accumulated_logging_time': 11.154846668243408, 'global_step': 104720, 'preemption_count': 0}), (106011, {'train/accuracy': 0.7268216013908386, 'train/loss': 1.0451542139053345, 'validation/accuracy': 0.6627399921417236, 'validation/loss': 1.3652856349945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5373000502586365, 'test/loss': 2.0615460872650146, 'test/num_examples': 10000, 'score': 41361.53125357628, 'total_duration': 45412.15554738045, 'accumulated_submission_time': 41361.53125357628, 'accumulated_eval_time': 4027.776224374771, 'accumulated_logging_time': 11.355185985565186, 'global_step': 106011, 'preemption_count': 0}), (107307, {'train/accuracy': 0.7290736436843872, 'train/loss': 1.043601155281067, 'validation/accuracy': 0.6650199890136719, 'validation/loss': 1.3559585809707642, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.0329790115356445, 'test/num_examples': 10000, 'score': 41871.644416093826, 'total_duration': 45977.01761698723, 'accumulated_submission_time': 41871.644416093826, 'accumulated_eval_time': 4082.2251999378204, 'accumulated_logging_time': 11.502586364746094, 'global_step': 107307, 'preemption_count': 0}), (108606, {'train/accuracy': 0.7270208597183228, 'train/loss': 1.0440120697021484, 'validation/accuracy': 0.6652199625968933, 'validation/loss': 1.3684667348861694, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.07810115814209, 'test/num_examples': 10000, 'score': 42381.51034498215, 'total_duration': 46540.91531252861, 'accumulated_submission_time': 42381.51034498215, 'accumulated_eval_time': 4135.920903921127, 'accumulated_logging_time': 11.686487674713135, 'global_step': 108606, 'preemption_count': 0}), (109917, {'train/accuracy': 0.7233936190605164, 'train/loss': 1.063387155532837, 'validation/accuracy': 0.6618399620056152, 'validation/loss': 1.3791370391845703, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.1093966960906982, 'test/num_examples': 10000, 'score': 42891.381705999374, 'total_duration': 47112.14376115799, 'accumulated_submission_time': 42891.381705999374, 'accumulated_eval_time': 4197.006940364838, 'accumulated_logging_time': 11.8048677444458, 'global_step': 109917, 'preemption_count': 0}), (111210, {'train/accuracy': 0.7387595772743225, 'train/loss': 1.0036503076553345, 'validation/accuracy': 0.671239972114563, 'validation/loss': 1.3343429565429688, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.0269627571105957, 'test/num_examples': 10000, 'score': 43401.40015435219, 'total_duration': 47675.31294465065, 'accumulated_submission_time': 43401.40015435219, 'accumulated_eval_time': 4249.822323799133, 'accumulated_logging_time': 11.984727144241333, 'global_step': 111210, 'preemption_count': 0}), (112503, {'train/accuracy': 0.7416493892669678, 'train/loss': 0.9858709573745728, 'validation/accuracy': 0.670960009098053, 'validation/loss': 1.3273751735687256, 'validation/num_examples': 50000, 'test/accuracy': 0.5464000105857849, 'test/loss': 2.031764030456543, 'test/num_examples': 10000, 'score': 43911.0682926178, 'total_duration': 48239.51965045929, 'accumulated_submission_time': 43911.0682926178, 'accumulated_eval_time': 4304.01292848587, 'accumulated_logging_time': 12.177767276763916, 'global_step': 112503, 'preemption_count': 0}), (113791, {'train/accuracy': 0.7413504123687744, 'train/loss': 0.9853124022483826, 'validation/accuracy': 0.6724199652671814, 'validation/loss': 1.3267568349838257, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.032102346420288, 'test/num_examples': 10000, 'score': 44420.78960490227, 'total_duration': 48806.616983413696, 'accumulated_submission_time': 44420.78960490227, 'accumulated_eval_time': 4361.044865608215, 'accumulated_logging_time': 12.369447469711304, 'global_step': 113791, 'preemption_count': 0}), (115079, {'train/accuracy': 0.7469706535339355, 'train/loss': 0.9639348387718201, 'validation/accuracy': 0.6804599761962891, 'validation/loss': 1.2937437295913696, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 1.9953489303588867, 'test/num_examples': 10000, 'score': 44930.846220731735, 'total_duration': 49375.03449702263, 'accumulated_submission_time': 44930.846220731735, 'accumulated_eval_time': 4419.123460292816, 'accumulated_logging_time': 12.49951982498169, 'global_step': 115079, 'preemption_count': 0}), (116370, {'train/accuracy': 0.752351701259613, 'train/loss': 0.9424148797988892, 'validation/accuracy': 0.6810399889945984, 'validation/loss': 1.2949098348617554, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 2.0026252269744873, 'test/num_examples': 10000, 'score': 45440.590339422226, 'total_duration': 49952.754564762115, 'accumulated_submission_time': 45440.590339422226, 'accumulated_eval_time': 4486.704003810883, 'accumulated_logging_time': 12.73878288269043, 'global_step': 116370, 'preemption_count': 0}), (117665, {'train/accuracy': 0.7501992583274841, 'train/loss': 0.9420855045318604, 'validation/accuracy': 0.6777999997138977, 'validation/loss': 1.2976300716400146, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.0283420085906982, 'test/num_examples': 10000, 'score': 45950.57492518425, 'total_duration': 50526.678396224976, 'accumulated_submission_time': 45950.57492518425, 'accumulated_eval_time': 4550.2156291008, 'accumulated_logging_time': 13.008969783782959, 'global_step': 117665, 'preemption_count': 0}), (118966, {'train/accuracy': 0.74125075340271, 'train/loss': 0.979218065738678, 'validation/accuracy': 0.6746799945831299, 'validation/loss': 1.3188567161560059, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.018056869506836, 'test/num_examples': 10000, 'score': 46460.261176109314, 'total_duration': 51092.77618813515, 'accumulated_submission_time': 46460.261176109314, 'accumulated_eval_time': 4606.27952837944, 'accumulated_logging_time': 13.200393438339233, 'global_step': 118966, 'preemption_count': 0}), (120258, {'train/accuracy': 0.7507971525192261, 'train/loss': 0.9507015347480774, 'validation/accuracy': 0.6778199672698975, 'validation/loss': 1.3064419031143188, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.0099639892578125, 'test/num_examples': 10000, 'score': 46970.10282039642, 'total_duration': 51655.478496313095, 'accumulated_submission_time': 46970.10282039642, 'accumulated_eval_time': 4658.789568901062, 'accumulated_logging_time': 13.395655155181885, 'global_step': 120258, 'preemption_count': 0}), (121551, {'train/accuracy': 0.7558394074440002, 'train/loss': 0.9138184785842896, 'validation/accuracy': 0.6812799572944641, 'validation/loss': 1.2765713930130005, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 1.9922151565551758, 'test/num_examples': 10000, 'score': 47479.90102767944, 'total_duration': 52221.03129673004, 'accumulated_submission_time': 47479.90102767944, 'accumulated_eval_time': 4714.238405942917, 'accumulated_logging_time': 13.545711755752563, 'global_step': 121551, 'preemption_count': 0}), (122846, {'train/accuracy': 0.7626155614852905, 'train/loss': 0.8847436308860779, 'validation/accuracy': 0.6919199824333191, 'validation/loss': 1.2373853921890259, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 1.9326844215393066, 'test/num_examples': 10000, 'score': 47989.78750681877, 'total_duration': 52794.92872285843, 'accumulated_submission_time': 47989.78750681877, 'accumulated_eval_time': 4777.935172319412, 'accumulated_logging_time': 13.705866813659668, 'global_step': 122846, 'preemption_count': 0}), (124145, {'train/accuracy': 0.7470105290412903, 'train/loss': 0.9557898044586182, 'validation/accuracy': 0.6754599809646606, 'validation/loss': 1.3239905834197998, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.0508055686950684, 'test/num_examples': 10000, 'score': 48499.69542980194, 'total_duration': 53366.55727934837, 'accumulated_submission_time': 48499.69542980194, 'accumulated_eval_time': 4839.195190668106, 'accumulated_logging_time': 14.011986255645752, 'global_step': 124145, 'preemption_count': 0}), (125451, {'train/accuracy': 0.7651067972183228, 'train/loss': 0.8890909552574158, 'validation/accuracy': 0.6888799667358398, 'validation/loss': 1.2617881298065186, 'validation/num_examples': 50000, 'test/accuracy': 0.5600000023841858, 'test/loss': 1.9780079126358032, 'test/num_examples': 10000, 'score': 49009.34534573555, 'total_duration': 53929.24225854874, 'accumulated_submission_time': 49009.34534573555, 'accumulated_eval_time': 4891.87887430191, 'accumulated_logging_time': 14.207602500915527, 'global_step': 125451, 'preemption_count': 0}), (126750, {'train/accuracy': 0.7654854655265808, 'train/loss': 0.8835881352424622, 'validation/accuracy': 0.6863999962806702, 'validation/loss': 1.2579063177108765, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 1.9555834531784058, 'test/num_examples': 10000, 'score': 49519.23848557472, 'total_duration': 54496.08412384987, 'accumulated_submission_time': 49519.23848557472, 'accumulated_eval_time': 4948.482552051544, 'accumulated_logging_time': 14.394435405731201, 'global_step': 126750, 'preemption_count': 0}), (128054, {'train/accuracy': 0.7738759517669678, 'train/loss': 0.8408008813858032, 'validation/accuracy': 0.6960999965667725, 'validation/loss': 1.2316733598709106, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 1.9281764030456543, 'test/num_examples': 10000, 'score': 50029.22094273567, 'total_duration': 55068.89689993858, 'accumulated_submission_time': 50029.22094273567, 'accumulated_eval_time': 5011.019055843353, 'accumulated_logging_time': 14.529919147491455, 'global_step': 128054, 'preemption_count': 0}), (129357, {'train/accuracy': 0.7737165093421936, 'train/loss': 0.8375166654586792, 'validation/accuracy': 0.695580005645752, 'validation/loss': 1.2336235046386719, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 1.94514799118042, 'test/num_examples': 10000, 'score': 50538.982206106186, 'total_duration': 55637.86808180809, 'accumulated_submission_time': 50538.982206106186, 'accumulated_eval_time': 5069.94505405426, 'accumulated_logging_time': 14.655816316604614, 'global_step': 129357, 'preemption_count': 0}), (130657, {'train/accuracy': 0.7756297588348389, 'train/loss': 0.8288185596466064, 'validation/accuracy': 0.6969000101089478, 'validation/loss': 1.2263697385787964, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.9130133390426636, 'test/num_examples': 10000, 'score': 51048.91535425186, 'total_duration': 56195.530470371246, 'accumulated_submission_time': 51048.91535425186, 'accumulated_eval_time': 5117.340341091156, 'accumulated_logging_time': 14.831950902938843, 'global_step': 130657, 'preemption_count': 0}), (131960, {'train/accuracy': 0.7819275856018066, 'train/loss': 0.8150589466094971, 'validation/accuracy': 0.698639988899231, 'validation/loss': 1.2118993997573853, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9055372476577759, 'test/num_examples': 10000, 'score': 51558.7761015892, 'total_duration': 56768.12331032753, 'accumulated_submission_time': 51558.7761015892, 'accumulated_eval_time': 5179.6621997356415, 'accumulated_logging_time': 15.08397388458252, 'global_step': 131960, 'preemption_count': 0}), (133266, {'train/accuracy': 0.7813695669174194, 'train/loss': 0.8108658194541931, 'validation/accuracy': 0.7012400031089783, 'validation/loss': 1.2071820497512817, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.9240167140960693, 'test/num_examples': 10000, 'score': 52068.70831656456, 'total_duration': 57339.75633382797, 'accumulated_submission_time': 52068.70831656456, 'accumulated_eval_time': 5241.018486261368, 'accumulated_logging_time': 15.271233558654785, 'global_step': 133266, 'preemption_count': 0}), (134576, {'train/accuracy': 0.7850366830825806, 'train/loss': 0.7943534851074219, 'validation/accuracy': 0.7020999789237976, 'validation/loss': 1.2008260488510132, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.8859989643096924, 'test/num_examples': 10000, 'score': 52578.6908454895, 'total_duration': 57905.87995100021, 'accumulated_submission_time': 52578.6908454895, 'accumulated_eval_time': 5296.84888958931, 'accumulated_logging_time': 15.424875497817993, 'global_step': 134576, 'preemption_count': 0}), (135881, {'train/accuracy': 0.7904974222183228, 'train/loss': 0.7781922817230225, 'validation/accuracy': 0.7061399817466736, 'validation/loss': 1.1804100275039673, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.861924171447754, 'test/num_examples': 10000, 'score': 53088.44025182724, 'total_duration': 58479.216676712036, 'accumulated_submission_time': 53088.44025182724, 'accumulated_eval_time': 5360.096490859985, 'accumulated_logging_time': 15.60699462890625, 'global_step': 135881, 'preemption_count': 0}), (137179, {'train/accuracy': 0.794941782951355, 'train/loss': 0.7663384675979614, 'validation/accuracy': 0.7063599824905396, 'validation/loss': 1.1768110990524292, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 1.863598108291626, 'test/num_examples': 10000, 'score': 53598.2692899704, 'total_duration': 59041.89164376259, 'accumulated_submission_time': 53598.2692899704, 'accumulated_eval_time': 5412.58920955658, 'accumulated_logging_time': 15.803770542144775, 'global_step': 137179, 'preemption_count': 0}), (138474, {'train/accuracy': 0.793367326259613, 'train/loss': 0.765191912651062, 'validation/accuracy': 0.706059992313385, 'validation/loss': 1.1773537397384644, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.866062045097351, 'test/num_examples': 10000, 'score': 54108.17970252037, 'total_duration': 59617.83230113983, 'accumulated_submission_time': 54108.17970252037, 'accumulated_eval_time': 5478.288891077042, 'accumulated_logging_time': 15.97880244255066, 'global_step': 138474, 'preemption_count': 0}), (139778, {'train/accuracy': 0.7893813848495483, 'train/loss': 0.7836411595344543, 'validation/accuracy': 0.6978200078010559, 'validation/loss': 1.213788628578186, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 1.9142110347747803, 'test/num_examples': 10000, 'score': 54618.18823552132, 'total_duration': 60189.00439476967, 'accumulated_submission_time': 54618.18823552132, 'accumulated_eval_time': 5539.139169454575, 'accumulated_logging_time': 16.136438131332397, 'global_step': 139778, 'preemption_count': 0}), (141075, {'train/accuracy': 0.8024752736091614, 'train/loss': 0.7169877290725708, 'validation/accuracy': 0.7141000032424927, 'validation/loss': 1.1460405588150024, 'validation/num_examples': 50000, 'test/accuracy': 0.5905000567436218, 'test/loss': 1.8393865823745728, 'test/num_examples': 10000, 'score': 55127.92451548576, 'total_duration': 60754.48213672638, 'accumulated_submission_time': 55127.92451548576, 'accumulated_eval_time': 5594.590469121933, 'accumulated_logging_time': 16.269219160079956, 'global_step': 141075, 'preemption_count': 0}), (142387, {'train/accuracy': 0.8033920526504517, 'train/loss': 0.714832603931427, 'validation/accuracy': 0.7083199620246887, 'validation/loss': 1.1697689294815063, 'validation/num_examples': 50000, 'test/accuracy': 0.5829000473022461, 'test/loss': 1.8625290393829346, 'test/num_examples': 10000, 'score': 55637.77014827728, 'total_duration': 61322.079132556915, 'accumulated_submission_time': 55637.77014827728, 'accumulated_eval_time': 5651.995651006699, 'accumulated_logging_time': 16.458147525787354, 'global_step': 142387, 'preemption_count': 0}), (143684, {'train/accuracy': 0.8064213991165161, 'train/loss': 0.7029650807380676, 'validation/accuracy': 0.7127199769020081, 'validation/loss': 1.1480932235717773, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.8467190265655518, 'test/num_examples': 10000, 'score': 56147.70947575569, 'total_duration': 61885.06326842308, 'accumulated_submission_time': 56147.70947575569, 'accumulated_eval_time': 5704.731792449951, 'accumulated_logging_time': 16.610419988632202, 'global_step': 143684, 'preemption_count': 0}), (144959, {'train/accuracy': 0.8153499364852905, 'train/loss': 0.6741594672203064, 'validation/accuracy': 0.7151599526405334, 'validation/loss': 1.1357368230819702, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.8387424945831299, 'test/num_examples': 10000, 'score': 56657.6377427578, 'total_duration': 62452.08435153961, 'accumulated_submission_time': 56657.6377427578, 'accumulated_eval_time': 5761.564425230026, 'accumulated_logging_time': 16.714773416519165, 'global_step': 144959, 'preemption_count': 0}), (146245, {'train/accuracy': 0.8169642686843872, 'train/loss': 0.6747756600379944, 'validation/accuracy': 0.7185399532318115, 'validation/loss': 1.1395584344863892, 'validation/num_examples': 50000, 'test/accuracy': 0.5932000279426575, 'test/loss': 1.8311971426010132, 'test/num_examples': 10000, 'score': 57167.52866220474, 'total_duration': 63022.38056015968, 'accumulated_submission_time': 57167.52866220474, 'accumulated_eval_time': 5821.683450937271, 'accumulated_logging_time': 16.843200206756592, 'global_step': 146245, 'preemption_count': 0}), (147535, {'train/accuracy': 0.8150908350944519, 'train/loss': 0.6641317009925842, 'validation/accuracy': 0.7188599705696106, 'validation/loss': 1.1230790615081787, 'validation/num_examples': 50000, 'test/accuracy': 0.5939000248908997, 'test/loss': 1.8428027629852295, 'test/num_examples': 10000, 'score': 57677.50322961807, 'total_duration': 63594.49426460266, 'accumulated_submission_time': 57677.50322961807, 'accumulated_eval_time': 5883.509951353073, 'accumulated_logging_time': 17.001859426498413, 'global_step': 147535, 'preemption_count': 0}), (148826, {'train/accuracy': 0.8249362111091614, 'train/loss': 0.6391320824623108, 'validation/accuracy': 0.7222999930381775, 'validation/loss': 1.1168736219406128, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8205305337905884, 'test/num_examples': 10000, 'score': 58187.21367764473, 'total_duration': 64164.27270078659, 'accumulated_submission_time': 58187.21367764473, 'accumulated_eval_time': 5943.271821022034, 'accumulated_logging_time': 17.15196418762207, 'global_step': 148826, 'preemption_count': 0}), (150124, {'train/accuracy': 0.8336853981018066, 'train/loss': 0.6037167310714722, 'validation/accuracy': 0.7246800065040588, 'validation/loss': 1.0998338460922241, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.7922621965408325, 'test/num_examples': 10000, 'score': 58697.15329504013, 'total_duration': 64741.25106525421, 'accumulated_submission_time': 58697.15329504013, 'accumulated_eval_time': 6010.0660881996155, 'accumulated_logging_time': 17.239588499069214, 'global_step': 150124, 'preemption_count': 0}), (151416, {'train/accuracy': 0.8305364847183228, 'train/loss': 0.614683210849762, 'validation/accuracy': 0.7263399958610535, 'validation/loss': 1.1018314361572266, 'validation/num_examples': 50000, 'test/accuracy': 0.5980000495910645, 'test/loss': 1.8055157661437988, 'test/num_examples': 10000, 'score': 59207.241606235504, 'total_duration': 65312.759472608566, 'accumulated_submission_time': 59207.241606235504, 'accumulated_eval_time': 6071.16760468483, 'accumulated_logging_time': 17.401784896850586, 'global_step': 151416, 'preemption_count': 0}), (152706, {'train/accuracy': 0.8395248651504517, 'train/loss': 0.5832342505455017, 'validation/accuracy': 0.727400004863739, 'validation/loss': 1.0825368165969849, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.7959704399108887, 'test/num_examples': 10000, 'score': 59717.1159825325, 'total_duration': 65894.83074331284, 'accumulated_submission_time': 59717.1159825325, 'accumulated_eval_time': 6142.990214109421, 'accumulated_logging_time': 17.617989540100098, 'global_step': 152706, 'preemption_count': 0}), (153997, {'train/accuracy': 0.8410793542861938, 'train/loss': 0.5604580640792847, 'validation/accuracy': 0.7313999533653259, 'validation/loss': 1.0849483013153076, 'validation/num_examples': 50000, 'test/accuracy': 0.6071000099182129, 'test/loss': 1.7804244756698608, 'test/num_examples': 10000, 'score': 60227.02983236313, 'total_duration': 66462.33111572266, 'accumulated_submission_time': 60227.02983236313, 'accumulated_eval_time': 6200.317070484161, 'accumulated_logging_time': 17.721428155899048, 'global_step': 153997, 'preemption_count': 0}), (155292, {'train/accuracy': 0.8443877100944519, 'train/loss': 0.5534412860870361, 'validation/accuracy': 0.7317999601364136, 'validation/loss': 1.081277847290039, 'validation/num_examples': 50000, 'test/accuracy': 0.6061000227928162, 'test/loss': 1.7662510871887207, 'test/num_examples': 10000, 'score': 60736.97341442108, 'total_duration': 67031.68683695793, 'accumulated_submission_time': 60736.97341442108, 'accumulated_eval_time': 6259.332718610764, 'accumulated_logging_time': 17.9585862159729, 'global_step': 155292, 'preemption_count': 0}), (156591, {'train/accuracy': 0.8515027165412903, 'train/loss': 0.5283141732215881, 'validation/accuracy': 0.7360399961471558, 'validation/loss': 1.063529133796692, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.7761375904083252, 'test/num_examples': 10000, 'score': 61247.05866241455, 'total_duration': 67607.31779408455, 'accumulated_submission_time': 61247.05866241455, 'accumulated_eval_time': 6324.606401205063, 'accumulated_logging_time': 18.073328256607056, 'global_step': 156591, 'preemption_count': 0}), (157889, {'train/accuracy': 0.8520607352256775, 'train/loss': 0.5210450291633606, 'validation/accuracy': 0.736299991607666, 'validation/loss': 1.0596239566802979, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.7377647161483765, 'test/num_examples': 10000, 'score': 61756.77537345886, 'total_duration': 68184.71546888351, 'accumulated_submission_time': 61756.77537345886, 'accumulated_eval_time': 6391.990918874741, 'accumulated_logging_time': 18.215017080307007, 'global_step': 157889, 'preemption_count': 0}), (159181, {'train/accuracy': 0.8557277917861938, 'train/loss': 0.5128253102302551, 'validation/accuracy': 0.7378799915313721, 'validation/loss': 1.0601978302001953, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.7477099895477295, 'test/num_examples': 10000, 'score': 62266.53978395462, 'total_duration': 68743.53025627136, 'accumulated_submission_time': 62266.53978395462, 'accumulated_eval_time': 6440.716040611267, 'accumulated_logging_time': 18.381216049194336, 'global_step': 159181, 'preemption_count': 0}), (160479, {'train/accuracy': 0.8646165132522583, 'train/loss': 0.4872450530529022, 'validation/accuracy': 0.7372199892997742, 'validation/loss': 1.0514651536941528, 'validation/num_examples': 50000, 'test/accuracy': 0.6137000322341919, 'test/loss': 1.7524752616882324, 'test/num_examples': 10000, 'score': 62776.62667822838, 'total_duration': 69299.97558164597, 'accumulated_submission_time': 62776.62667822838, 'accumulated_eval_time': 6486.782221317291, 'accumulated_logging_time': 18.515195846557617, 'global_step': 160479, 'preemption_count': 0}), (161772, {'train/accuracy': 0.8703164458274841, 'train/loss': 0.45442503690719604, 'validation/accuracy': 0.7425999641418457, 'validation/loss': 1.0323022603988647, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.738883137702942, 'test/num_examples': 10000, 'score': 63286.635056734085, 'total_duration': 69867.39201664925, 'accumulated_submission_time': 63286.635056734085, 'accumulated_eval_time': 6543.849009275436, 'accumulated_logging_time': 18.700711488723755, 'global_step': 161772, 'preemption_count': 0}), (163068, {'train/accuracy': 0.8712930083274841, 'train/loss': 0.4574160575866699, 'validation/accuracy': 0.7414799928665161, 'validation/loss': 1.0349931716918945, 'validation/num_examples': 50000, 'test/accuracy': 0.6151000261306763, 'test/loss': 1.7563148736953735, 'test/num_examples': 10000, 'score': 63796.46826982498, 'total_duration': 70437.044765234, 'accumulated_submission_time': 63796.46826982498, 'accumulated_eval_time': 6603.380485057831, 'accumulated_logging_time': 18.830911874771118, 'global_step': 163068, 'preemption_count': 0}), (164365, {'train/accuracy': 0.8731664419174194, 'train/loss': 0.45019686222076416, 'validation/accuracy': 0.7411800026893616, 'validation/loss': 1.0385890007019043, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.750708818435669, 'test/num_examples': 10000, 'score': 64306.3120701313, 'total_duration': 71015.12366104126, 'accumulated_submission_time': 64306.3120701313, 'accumulated_eval_time': 6671.314571380615, 'accumulated_logging_time': 18.975023984909058, 'global_step': 164365, 'preemption_count': 0}), (165660, {'train/accuracy': 0.8804607391357422, 'train/loss': 0.4232844412326813, 'validation/accuracy': 0.7456199526786804, 'validation/loss': 1.0242973566055298, 'validation/num_examples': 50000, 'test/accuracy': 0.6225000023841858, 'test/loss': 1.7256665229797363, 'test/num_examples': 10000, 'score': 64816.105321884155, 'total_duration': 71582.1528480053, 'accumulated_submission_time': 64816.105321884155, 'accumulated_eval_time': 6728.2161860466, 'accumulated_logging_time': 19.14898133277893, 'global_step': 165660, 'preemption_count': 0}), (166952, {'train/accuracy': 0.88285231590271, 'train/loss': 0.4053460359573364, 'validation/accuracy': 0.7495999932289124, 'validation/loss': 1.0118756294250488, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.7067680358886719, 'test/num_examples': 10000, 'score': 65326.05572628975, 'total_duration': 72145.94945240021, 'accumulated_submission_time': 65326.05572628975, 'accumulated_eval_time': 6781.696949958801, 'accumulated_logging_time': 19.354141235351562, 'global_step': 166952, 'preemption_count': 0}), (168249, {'train/accuracy': 0.8884526491165161, 'train/loss': 0.38864967226982117, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.0168981552124023, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.7082061767578125, 'test/num_examples': 10000, 'score': 65836.15519762039, 'total_duration': 72701.2179467678, 'accumulated_submission_time': 65836.15519762039, 'accumulated_eval_time': 6826.55775809288, 'accumulated_logging_time': 19.50336265563965, 'global_step': 168249, 'preemption_count': 0})], 'global_step': 169536}
I0308 05:31:21.908300 140352918893760 submission_runner.py:649] Timing: 66346.04735946655
I0308 05:31:21.908347 140352918893760 submission_runner.py:651] Total number of evals: 130
I0308 05:31:21.908376 140352918893760 submission_runner.py:652] ====================
I0308 05:31:21.908600 140352918893760 submission_runner.py:750] Final imagenet_resnet score: 2
