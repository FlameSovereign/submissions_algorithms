python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=1913974348 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-09-08-57.log
2025-03-07 09:09:16.991062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741338557.529740       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741338557.803568       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 09:10:11.715808 140341280416960 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax.
I0307 09:10:14.533249 140341280416960 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 09:10:14.535896 140341280416960 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 09:10:14.547962 140341280416960 submission_runner.py:606] Using RNG seed 1913974348
I0307 09:10:18.230722 140341280416960 submission_runner.py:615] --- Tuning run 2/5 ---
I0307 09:10:18.230933 140341280416960 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_2.
I0307 09:10:18.231137 140341280416960 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_2/hparams.json.
I0307 09:10:18.494422 140341280416960 submission_runner.py:218] Initializing dataset.
I0307 09:10:20.244759 140341280416960 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:10:20.587433 140341280416960 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:10:21.029396 140341280416960 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:10:23.127061 140341280416960 submission_runner.py:229] Initializing model.
I0307 09:10:48.427298 140341280416960 submission_runner.py:272] Initializing optimizer.
I0307 09:10:49.673656 140341280416960 submission_runner.py:279] Initializing metrics bundle.
I0307 09:10:49.673914 140341280416960 submission_runner.py:301] Initializing checkpoint and logger.
I0307 09:10:49.675014 140341280416960 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0307 09:10:49.675117 140341280416960 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_2/meta_data_0.json.
I0307 09:10:50.148058 140341280416960 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_2/flags_0.json.
I0307 09:10:50.545468 140341280416960 submission_runner.py:337] Starting training loop.
I0307 09:11:49.407627 140203717007104 logging_writer.py:48] [0] global_step=0, grad_norm=0.5600300431251526, loss=6.929930210113525
I0307 09:11:49.758421 140341280416960 spec.py:321] Evaluating on the training split.
I0307 09:11:50.229048 140341280416960 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:11:50.252980 140341280416960 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:11:50.295510 140341280416960 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:12:10.211842 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 09:12:10.766417 140341280416960 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:12:10.776494 140341280416960 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:12:10.818217 140341280416960 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:13:07.104239 140341280416960 spec.py:349] Evaluating on the test split.
I0307 09:13:07.568081 140341280416960 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:13:07.583574 140341280416960 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 09:13:07.621815 140341280416960 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:13:38.232288 140341280416960 submission_runner.py:469] Time since start: 167.69s, 	Step: 1, 	{'train/accuracy': 0.0012954400153830647, 'train/loss': 6.912797927856445, 'validation/accuracy': 0.0011399999493733048, 'validation/loss': 6.913561820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9131364822387695, 'test/num_examples': 10000, 'score': 59.212727785110474, 'total_duration': 167.68675565719604, 'accumulated_submission_time': 59.212727785110474, 'accumulated_eval_time': 108.47379875183105, 'accumulated_logging_time': 0}
I0307 09:13:38.241264 140185106900736 logging_writer.py:48] [1] accumulated_eval_time=108.474, accumulated_logging_time=0, accumulated_submission_time=59.2127, global_step=1, preemption_count=0, score=59.2127, test/accuracy=0.0013, test/loss=6.91314, test/num_examples=10000, total_duration=167.687, train/accuracy=0.00129544, train/loss=6.9128, validation/accuracy=0.00114, validation/loss=6.91356, validation/num_examples=50000
I0307 09:14:14.673571 140185098508032 logging_writer.py:48] [100] global_step=100, grad_norm=0.5658280849456787, loss=6.906854629516602
I0307 09:14:50.865248 140185106900736 logging_writer.py:48] [200] global_step=200, grad_norm=0.5409868955612183, loss=6.854202747344971
I0307 09:15:28.028105 140185098508032 logging_writer.py:48] [300] global_step=300, grad_norm=0.578217089176178, loss=6.773313522338867
I0307 09:16:05.473165 140185106900736 logging_writer.py:48] [400] global_step=400, grad_norm=0.624371349811554, loss=6.704167366027832
I0307 09:16:43.143657 140185098508032 logging_writer.py:48] [500] global_step=500, grad_norm=0.6532341241836548, loss=6.6011881828308105
I0307 09:17:20.981941 140185106900736 logging_writer.py:48] [600] global_step=600, grad_norm=0.6771021485328674, loss=6.535096168518066
I0307 09:17:58.427652 140185098508032 logging_writer.py:48] [700] global_step=700, grad_norm=0.8336561322212219, loss=6.469560623168945
I0307 09:18:35.726277 140185106900736 logging_writer.py:48] [800] global_step=800, grad_norm=0.8668092489242554, loss=6.326044082641602
I0307 09:19:13.645770 140185098508032 logging_writer.py:48] [900] global_step=900, grad_norm=1.2594208717346191, loss=6.293898582458496
I0307 09:19:51.418997 140185106900736 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.3774828910827637, loss=6.263062953948975
I0307 09:20:29.075915 140185098508032 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.6340484619140625, loss=6.214530944824219
I0307 09:21:07.090457 140185106900736 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.3109312057495117, loss=6.160755157470703
I0307 09:21:45.225550 140185098508032 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.1046693325042725, loss=6.05873441696167
I0307 09:22:08.581895 140341280416960 spec.py:321] Evaluating on the training split.
I0307 09:22:20.416714 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 09:22:45.361738 140341280416960 spec.py:349] Evaluating on the test split.
I0307 09:22:47.406558 140341280416960 submission_runner.py:469] Time since start: 716.86s, 	Step: 1362, 	{'train/accuracy': 0.07712850719690323, 'train/loss': 5.349559307098389, 'validation/accuracy': 0.06757999956607819, 'validation/loss': 5.4484663009643555, 'validation/num_examples': 50000, 'test/accuracy': 0.049800001084804535, 'test/loss': 5.662440299987793, 'test/num_examples': 10000, 'score': 569.3144462108612, 'total_duration': 716.8610239028931, 'accumulated_submission_time': 569.3144462108612, 'accumulated_eval_time': 147.29842066764832, 'accumulated_logging_time': 0.0688316822052002}
I0307 09:22:47.445557 140185115293440 logging_writer.py:48] [1362] accumulated_eval_time=147.298, accumulated_logging_time=0.0688317, accumulated_submission_time=569.314, global_step=1362, preemption_count=0, score=569.314, test/accuracy=0.0498, test/loss=5.66244, test/num_examples=10000, total_duration=716.861, train/accuracy=0.0771285, train/loss=5.34956, validation/accuracy=0.06758, validation/loss=5.44847, validation/num_examples=50000
I0307 09:23:02.394142 140185199154944 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7150647640228271, loss=6.0218119621276855
I0307 09:23:39.975986 140185115293440 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.68191397190094, loss=5.90260648727417
I0307 09:24:17.726043 140185199154944 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.3123133182525635, loss=5.794893741607666
I0307 09:24:55.675587 140185115293440 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.3853230476379395, loss=5.83419132232666
I0307 09:25:32.557730 140185199154944 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.8039158582687378, loss=5.7961883544921875
I0307 09:26:09.608758 140185115293440 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.519040584564209, loss=5.671475887298584
I0307 09:26:45.553888 140185199154944 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.93233585357666, loss=5.653414249420166
I0307 09:27:23.658737 140185115293440 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.1841533184051514, loss=5.684288501739502
I0307 09:28:02.651669 140185199154944 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.70353364944458, loss=5.557809352874756
I0307 09:28:41.198748 140185115293440 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.7651126384735107, loss=5.557110786437988
I0307 09:29:19.796590 140185199154944 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.079007625579834, loss=5.533473014831543
I0307 09:29:58.858682 140185115293440 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.678150177001953, loss=5.453505516052246
I0307 09:30:36.964147 140185199154944 logging_writer.py:48] [2600] global_step=2600, grad_norm=6.647884368896484, loss=5.406007766723633
I0307 09:31:14.820187 140185115293440 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.4702212810516357, loss=5.391486167907715
I0307 09:31:17.532889 140341280416960 spec.py:321] Evaluating on the training split.
I0307 09:31:29.421383 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 09:31:58.535661 140341280416960 spec.py:349] Evaluating on the test split.
I0307 09:32:00.340044 140341280416960 submission_runner.py:469] Time since start: 1269.79s, 	Step: 2708, 	{'train/accuracy': 0.17384804785251617, 'train/loss': 4.372182846069336, 'validation/accuracy': 0.15455999970436096, 'validation/loss': 4.523075103759766, 'validation/num_examples': 50000, 'test/accuracy': 0.10610000789165497, 'test/loss': 4.935251712799072, 'test/num_examples': 10000, 'score': 1079.2011749744415, 'total_duration': 1269.7945284843445, 'accumulated_submission_time': 1079.2011749744415, 'accumulated_eval_time': 190.10553526878357, 'accumulated_logging_time': 0.13519978523254395}
I0307 09:32:00.376116 140185199154944 logging_writer.py:48] [2708] accumulated_eval_time=190.106, accumulated_logging_time=0.1352, accumulated_submission_time=1079.2, global_step=2708, preemption_count=0, score=1079.2, test/accuracy=0.1061, test/loss=4.93525, test/num_examples=10000, total_duration=1269.79, train/accuracy=0.173848, train/loss=4.37218, validation/accuracy=0.15456, validation/loss=4.52308, validation/num_examples=50000
I0307 09:32:35.457562 140185115293440 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.92131233215332, loss=5.369656085968018
I0307 09:33:13.024976 140185199154944 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.3722195625305176, loss=5.376864910125732
I0307 09:33:50.620581 140185115293440 logging_writer.py:48] [3000] global_step=3000, grad_norm=6.241259574890137, loss=5.355695724487305
I0307 09:34:28.695029 140185199154944 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.613929033279419, loss=5.237725257873535
I0307 09:35:06.614858 140185115293440 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.1598992347717285, loss=5.207148551940918
I0307 09:35:44.629620 140185199154944 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.218851327896118, loss=5.196260929107666
I0307 09:36:22.146424 140185115293440 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.011895656585693, loss=5.135623455047607
I0307 09:37:00.274760 140185199154944 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.987651348114014, loss=5.120098114013672
I0307 09:37:38.168514 140185115293440 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.9357380867004395, loss=5.0919508934021
I0307 09:38:16.273304 140185199154944 logging_writer.py:48] [3700] global_step=3700, grad_norm=5.151635646820068, loss=5.064823150634766
I0307 09:38:54.218441 140185115293440 logging_writer.py:48] [3800] global_step=3800, grad_norm=6.753452777862549, loss=4.955253601074219
I0307 09:39:32.916743 140185199154944 logging_writer.py:48] [3900] global_step=3900, grad_norm=7.0828022956848145, loss=4.898430824279785
I0307 09:40:11.108723 140185115293440 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.9803993701934814, loss=4.910226821899414
I0307 09:40:30.387976 140341280416960 spec.py:321] Evaluating on the training split.
I0307 09:40:41.689589 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 09:41:06.643229 140341280416960 spec.py:349] Evaluating on the test split.
I0307 09:41:08.458469 140341280416960 submission_runner.py:469] Time since start: 1817.91s, 	Step: 4051, 	{'train/accuracy': 0.2845184803009033, 'train/loss': 3.656397819519043, 'validation/accuracy': 0.24833999574184418, 'validation/loss': 3.841278314590454, 'validation/num_examples': 50000, 'test/accuracy': 0.18820001184940338, 'test/loss': 4.302127838134766, 'test/num_examples': 10000, 'score': 1588.9874997138977, 'total_duration': 1817.9129548072815, 'accumulated_submission_time': 1588.9874997138977, 'accumulated_eval_time': 228.17598390579224, 'accumulated_logging_time': 0.25188350677490234}
I0307 09:41:08.468127 140185199154944 logging_writer.py:48] [4051] accumulated_eval_time=228.176, accumulated_logging_time=0.251884, accumulated_submission_time=1588.99, global_step=4051, preemption_count=0, score=1588.99, test/accuracy=0.1882, test/loss=4.30213, test/num_examples=10000, total_duration=1817.91, train/accuracy=0.284518, train/loss=3.6564, validation/accuracy=0.24834, validation/loss=3.84128, validation/num_examples=50000
I0307 09:41:27.309099 140185115293440 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.8897759914398193, loss=4.884088516235352
I0307 09:42:05.461556 140185199154944 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.931957721710205, loss=4.82418155670166
I0307 09:42:43.790040 140185115293440 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.2365760803222656, loss=4.871560096740723
I0307 09:43:21.772936 140185199154944 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.452584981918335, loss=4.820710182189941
I0307 09:43:59.368741 140185115293440 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.012513637542725, loss=4.754664421081543
I0307 09:44:37.523078 140185199154944 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.605938911437988, loss=4.719996929168701
I0307 09:45:15.907669 140185115293440 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.8326215744018555, loss=4.7343878746032715
I0307 09:45:54.072274 140185199154944 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.4671010971069336, loss=4.767484664916992
I0307 09:46:32.359563 140185115293440 logging_writer.py:48] [4900] global_step=4900, grad_norm=5.362948894500732, loss=4.642853260040283
I0307 09:47:10.196598 140185199154944 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.0226101875305176, loss=4.593481063842773
I0307 09:47:48.578944 140185115293440 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.2251975536346436, loss=4.599917411804199
I0307 09:48:27.225218 140185199154944 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.2373178005218506, loss=4.541524887084961
I0307 09:49:05.211926 140185115293440 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.831369638442993, loss=4.527207374572754
I0307 09:49:38.618655 140341280416960 spec.py:321] Evaluating on the training split.
I0307 09:49:49.576697 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 09:50:12.986879 140341280416960 spec.py:349] Evaluating on the test split.
I0307 09:50:14.745579 140341280416960 submission_runner.py:469] Time since start: 2364.20s, 	Step: 5389, 	{'train/accuracy': 0.3719308078289032, 'train/loss': 3.036378860473633, 'validation/accuracy': 0.3323200047016144, 'validation/loss': 3.238503932952881, 'validation/num_examples': 50000, 'test/accuracy': 0.2510000169277191, 'test/loss': 3.8246071338653564, 'test/num_examples': 10000, 'score': 2098.994063138962, 'total_duration': 2364.200074195862, 'accumulated_submission_time': 2098.994063138962, 'accumulated_eval_time': 264.302875995636, 'accumulated_logging_time': 0.2702193260192871}
I0307 09:50:14.807504 140185199154944 logging_writer.py:48] [5389] accumulated_eval_time=264.303, accumulated_logging_time=0.270219, accumulated_submission_time=2098.99, global_step=5389, preemption_count=0, score=2098.99, test/accuracy=0.251, test/loss=3.82461, test/num_examples=10000, total_duration=2364.2, train/accuracy=0.371931, train/loss=3.03638, validation/accuracy=0.33232, validation/loss=3.2385, validation/num_examples=50000
I0307 09:50:19.518803 140185115293440 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.7520151138305664, loss=4.544654369354248
I0307 09:50:57.218364 140185199154944 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.342186689376831, loss=4.480306625366211
I0307 09:51:35.266237 140185115293440 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.658933162689209, loss=4.49686861038208
I0307 09:52:13.586133 140185199154944 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.551638603210449, loss=4.383743762969971
I0307 09:52:52.016871 140185115293440 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.928983211517334, loss=4.372416019439697
I0307 09:53:29.994864 140185199154944 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.703558921813965, loss=4.4711503982543945
I0307 09:54:08.396966 140185115293440 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.6421473026275635, loss=4.493278503417969
I0307 09:54:46.369506 140185199154944 logging_writer.py:48] [6100] global_step=6100, grad_norm=4.787374496459961, loss=4.386558532714844
I0307 09:55:24.830857 140185115293440 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.745093584060669, loss=4.372386455535889
I0307 09:56:03.023695 140185199154944 logging_writer.py:48] [6300] global_step=6300, grad_norm=4.4990129470825195, loss=4.384113311767578
I0307 09:56:41.359018 140185115293440 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.1249139308929443, loss=4.32399320602417
I0307 09:57:19.542505 140185199154944 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.8357584476470947, loss=4.308239936828613
I0307 09:57:58.028227 140185115293440 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.1733739376068115, loss=4.339642524719238
I0307 09:58:36.278837 140185199154944 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.490013122558594, loss=4.1960225105285645
I0307 09:58:45.077305 140341280416960 spec.py:321] Evaluating on the training split.
I0307 09:58:56.480315 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 09:59:18.731874 140341280416960 spec.py:349] Evaluating on the test split.
I0307 09:59:20.564363 140341280416960 submission_runner.py:469] Time since start: 2910.02s, 	Step: 6724, 	{'train/accuracy': 0.4240274131298065, 'train/loss': 2.73190975189209, 'validation/accuracy': 0.38415998220443726, 'validation/loss': 2.949188470840454, 'validation/num_examples': 50000, 'test/accuracy': 0.28290000557899475, 'test/loss': 3.5845649242401123, 'test/num_examples': 10000, 'score': 2609.108383655548, 'total_duration': 2910.018846511841, 'accumulated_submission_time': 2609.108383655548, 'accumulated_eval_time': 299.7898905277252, 'accumulated_logging_time': 0.360884428024292}
I0307 09:59:20.610221 140185115293440 logging_writer.py:48] [6724] accumulated_eval_time=299.79, accumulated_logging_time=0.360884, accumulated_submission_time=2609.11, global_step=6724, preemption_count=0, score=2609.11, test/accuracy=0.2829, test/loss=3.58456, test/num_examples=10000, total_duration=2910.02, train/accuracy=0.424027, train/loss=2.73191, validation/accuracy=0.38416, validation/loss=2.94919, validation/num_examples=50000
I0307 09:59:50.275857 140185199154944 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.2744221687316895, loss=4.360838890075684
I0307 10:00:28.420722 140185115293440 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.727496385574341, loss=4.319457054138184
I0307 10:01:06.839840 140185199154944 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.37588357925415, loss=4.185068607330322
I0307 10:01:45.186701 140185115293440 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.9397634267807007, loss=4.265445232391357
I0307 10:02:23.709700 140185199154944 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.2990126609802246, loss=4.142401695251465
I0307 10:03:01.816448 140185115293440 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.5793349742889404, loss=4.16113805770874
I0307 10:03:39.874452 140185199154944 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.538295269012451, loss=4.224664688110352
I0307 10:04:17.916331 140185115293440 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.0826919078826904, loss=4.157361030578613
I0307 10:04:56.066501 140185199154944 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.9327917098999023, loss=4.157741546630859
I0307 10:05:34.359607 140185115293440 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.3368964195251465, loss=4.119132041931152
I0307 10:06:12.540611 140185199154944 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.5270538330078125, loss=4.044083118438721
I0307 10:06:50.357146 140185115293440 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.0205373764038086, loss=4.121147155761719
I0307 10:07:28.672076 140185199154944 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.1549432277679443, loss=4.060245037078857
I0307 10:07:50.592934 140341280416960 spec.py:321] Evaluating on the training split.
I0307 10:08:02.095415 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 10:08:25.211242 140341280416960 spec.py:349] Evaluating on the test split.
I0307 10:08:27.016484 140341280416960 submission_runner.py:469] Time since start: 3456.47s, 	Step: 8058, 	{'train/accuracy': 0.4624720811843872, 'train/loss': 2.4840662479400635, 'validation/accuracy': 0.4181399941444397, 'validation/loss': 2.70945143699646, 'validation/num_examples': 50000, 'test/accuracy': 0.3295000195503235, 'test/loss': 3.3195393085479736, 'test/num_examples': 10000, 'score': 3118.952538728714, 'total_duration': 3456.4709782600403, 'accumulated_submission_time': 3118.952538728714, 'accumulated_eval_time': 336.2134144306183, 'accumulated_logging_time': 0.4149470329284668}
I0307 10:08:27.085824 140185115293440 logging_writer.py:48] [8058] accumulated_eval_time=336.213, accumulated_logging_time=0.414947, accumulated_submission_time=3118.95, global_step=8058, preemption_count=0, score=3118.95, test/accuracy=0.3295, test/loss=3.31954, test/num_examples=10000, total_duration=3456.47, train/accuracy=0.462472, train/loss=2.48407, validation/accuracy=0.41814, validation/loss=2.70945, validation/num_examples=50000
I0307 10:08:43.501222 140185199154944 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.9217424392700195, loss=4.112202167510986
I0307 10:09:21.831275 140185115293440 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.9495234489440918, loss=4.116217613220215
I0307 10:09:59.873240 140185199154944 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5613856315612793, loss=4.064100742340088
I0307 10:10:38.283350 140185115293440 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.16141939163208, loss=4.036425590515137
I0307 10:11:16.517027 140185199154944 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.5218088626861572, loss=4.098161697387695
I0307 10:11:54.815849 140185115293440 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.3148603439331055, loss=3.969238758087158
I0307 10:12:33.114121 140185199154944 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.4009346961975098, loss=4.034906387329102
I0307 10:13:10.909695 140185115293440 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.6952327489852905, loss=3.999276638031006
I0307 10:13:49.198069 140185199154944 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.9038245677948, loss=4.032948970794678
I0307 10:14:27.035055 140185115293440 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.8535425662994385, loss=3.887965679168701
I0307 10:15:05.482961 140185199154944 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.506151795387268, loss=3.8731095790863037
I0307 10:15:43.480389 140185115293440 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.6204094886779785, loss=3.903226613998413
I0307 10:16:21.539690 140185199154944 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.3302595615386963, loss=3.9661781787872314
I0307 10:16:57.133668 140341280416960 spec.py:321] Evaluating on the training split.
I0307 10:17:09.728720 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 10:17:33.005970 140341280416960 spec.py:349] Evaluating on the test split.
I0307 10:17:34.785706 140341280416960 submission_runner.py:469] Time since start: 4004.24s, 	Step: 9394, 	{'train/accuracy': 0.5198899507522583, 'train/loss': 2.29365873336792, 'validation/accuracy': 0.47005999088287354, 'validation/loss': 2.531599521636963, 'validation/num_examples': 50000, 'test/accuracy': 0.3564000129699707, 'test/loss': 3.1833980083465576, 'test/num_examples': 10000, 'score': 3628.852597951889, 'total_duration': 4004.2401163578033, 'accumulated_submission_time': 3628.852597951889, 'accumulated_eval_time': 373.8653357028961, 'accumulated_logging_time': 0.5019807815551758}
I0307 10:17:34.890261 140185115293440 logging_writer.py:48] [9394] accumulated_eval_time=373.865, accumulated_logging_time=0.501981, accumulated_submission_time=3628.85, global_step=9394, preemption_count=0, score=3628.85, test/accuracy=0.3564, test/loss=3.1834, test/num_examples=10000, total_duration=4004.24, train/accuracy=0.51989, train/loss=2.29366, validation/accuracy=0.47006, validation/loss=2.5316, validation/num_examples=50000
I0307 10:17:37.530698 140185199154944 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.576643943786621, loss=4.022942066192627
I0307 10:18:15.405111 140185115293440 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.7459321022033691, loss=3.8836917877197266
I0307 10:18:53.493073 140185199154944 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.0119404792785645, loss=4.002490043640137
I0307 10:19:31.474885 140185115293440 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.1789512634277344, loss=3.9254868030548096
I0307 10:20:09.757750 140185199154944 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.2570536136627197, loss=3.9211347103118896
I0307 10:20:47.579658 140185115293440 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.2863659858703613, loss=3.8952767848968506
I0307 10:21:25.634113 140185199154944 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.276132106781006, loss=3.8297271728515625
I0307 10:22:03.577873 140185115293440 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.188828945159912, loss=3.771965980529785
I0307 10:22:41.710542 140185199154944 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.3498146533966064, loss=3.822722911834717
I0307 10:23:20.161626 140185115293440 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.295912981033325, loss=3.794424533843994
I0307 10:23:58.135656 140185199154944 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.2242326736450195, loss=3.8712782859802246
I0307 10:24:35.813308 140185115293440 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.0791995525360107, loss=3.7344393730163574
I0307 10:25:13.934782 140185199154944 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.157651901245117, loss=3.769650936126709
I0307 10:25:51.737488 140185115293440 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.5572168827056885, loss=3.7676336765289307
I0307 10:26:04.941869 140341280416960 spec.py:321] Evaluating on the training split.
I0307 10:26:19.459943 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 10:26:46.245684 140341280416960 spec.py:349] Evaluating on the test split.
I0307 10:26:48.003888 140341280416960 submission_runner.py:469] Time since start: 4557.46s, 	Step: 10736, 	{'train/accuracy': 0.5587930083274841, 'train/loss': 2.0877444744110107, 'validation/accuracy': 0.5124199986457825, 'validation/loss': 2.3099277019500732, 'validation/num_examples': 50000, 'test/accuracy': 0.3995000123977661, 'test/loss': 2.9412453174591064, 'test/num_examples': 10000, 'score': 4138.687603473663, 'total_duration': 4557.458254098892, 'accumulated_submission_time': 4138.687603473663, 'accumulated_eval_time': 416.9271948337555, 'accumulated_logging_time': 0.6678164005279541}
I0307 10:26:48.100792 140185199154944 logging_writer.py:48] [10736] accumulated_eval_time=416.927, accumulated_logging_time=0.667816, accumulated_submission_time=4138.69, global_step=10736, preemption_count=0, score=4138.69, test/accuracy=0.3995, test/loss=2.94125, test/num_examples=10000, total_duration=4557.46, train/accuracy=0.558793, train/loss=2.08774, validation/accuracy=0.51242, validation/loss=2.30993, validation/num_examples=50000
I0307 10:27:13.062383 140185115293440 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.335761785507202, loss=3.817298412322998
I0307 10:27:50.812994 140185199154944 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.285402536392212, loss=3.817225217819214
I0307 10:28:28.766877 140185115293440 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.3015246391296387, loss=3.791830539703369
I0307 10:29:07.405521 140185199154944 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.658193349838257, loss=3.7853760719299316
I0307 10:29:46.074947 140185115293440 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.6819113492965698, loss=3.763026475906372
I0307 10:30:24.583769 140185199154944 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.609300971031189, loss=3.7106733322143555
I0307 10:31:03.314268 140185115293440 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.7179616689682007, loss=3.685215473175049
I0307 10:31:41.653405 140185199154944 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.6001640558242798, loss=3.698232889175415
I0307 10:32:19.917029 140185115293440 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.9665544033050537, loss=3.75272536277771
I0307 10:32:58.325174 140185199154944 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.034473180770874, loss=3.666743516921997
I0307 10:33:36.915285 140185115293440 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.9473683834075928, loss=3.7015128135681152
I0307 10:34:14.952106 140185199154944 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.8148273229599, loss=3.7750496864318848
I0307 10:34:53.130035 140185115293440 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.7807114124298096, loss=3.660862684249878
I0307 10:35:18.061843 140341280416960 spec.py:321] Evaluating on the training split.
I0307 10:35:35.268348 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 10:36:02.531185 140341280416960 spec.py:349] Evaluating on the test split.
I0307 10:36:04.340463 140341280416960 submission_runner.py:469] Time since start: 5113.79s, 	Step: 12066, 	{'train/accuracy': 0.5911591053009033, 'train/loss': 1.9238556623458862, 'validation/accuracy': 0.5330600142478943, 'validation/loss': 2.187696933746338, 'validation/num_examples': 50000, 'test/accuracy': 0.4195000231266022, 'test/loss': 2.83099102973938, 'test/num_examples': 10000, 'score': 4648.455675840378, 'total_duration': 5113.794836759567, 'accumulated_submission_time': 4648.455675840378, 'accumulated_eval_time': 463.2056620121002, 'accumulated_logging_time': 0.8076694011688232}
I0307 10:36:04.523135 140185199154944 logging_writer.py:48] [12066] accumulated_eval_time=463.206, accumulated_logging_time=0.807669, accumulated_submission_time=4648.46, global_step=12066, preemption_count=0, score=4648.46, test/accuracy=0.4195, test/loss=2.83099, test/num_examples=10000, total_duration=5113.79, train/accuracy=0.591159, train/loss=1.92386, validation/accuracy=0.53306, validation/loss=2.1877, validation/num_examples=50000
I0307 10:36:18.099675 140185115293440 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.912188172340393, loss=3.792506217956543
I0307 10:36:56.801264 140185199154944 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.9633804559707642, loss=3.6729869842529297
I0307 10:37:35.353479 140185115293440 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.6921566724777222, loss=3.639362335205078
I0307 10:38:13.635146 140185199154944 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.177929162979126, loss=3.7222542762756348
I0307 10:38:51.950543 140185115293440 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.7144317626953125, loss=3.6740567684173584
I0307 10:39:30.473420 140185199154944 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.6755058765411377, loss=3.637895107269287
I0307 10:40:09.019883 140185115293440 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.5216846466064453, loss=3.666194200515747
I0307 10:40:46.807754 140185199154944 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.2722878456115723, loss=3.7085766792297363
I0307 10:41:25.465098 140185115293440 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.5540337562561035, loss=3.6249947547912598
I0307 10:42:03.826344 140185199154944 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.7859569787979126, loss=3.6518807411193848
I0307 10:42:42.250146 140185115293440 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.276513695716858, loss=3.6280677318573
I0307 10:43:20.275836 140185199154944 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.3473643064498901, loss=3.533487319946289
I0307 10:43:58.618515 140185115293440 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.8166980743408203, loss=3.6292855739593506
I0307 10:44:34.586340 140341280416960 spec.py:321] Evaluating on the training split.
I0307 10:44:53.920652 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 10:45:20.225177 140341280416960 spec.py:349] Evaluating on the test split.
I0307 10:45:21.999840 140341280416960 submission_runner.py:469] Time since start: 5671.45s, 	Step: 13395, 	{'train/accuracy': 0.62109375, 'train/loss': 1.7880302667617798, 'validation/accuracy': 0.5651599764823914, 'validation/loss': 2.0439517498016357, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.731745481491089, 'test/num_examples': 10000, 'score': 5158.338755130768, 'total_duration': 5671.454211473465, 'accumulated_submission_time': 5158.338755130768, 'accumulated_eval_time': 510.6190092563629, 'accumulated_logging_time': 1.0177834033966064}
I0307 10:45:22.242151 140185199154944 logging_writer.py:48] [13395] accumulated_eval_time=510.619, accumulated_logging_time=1.01778, accumulated_submission_time=5158.34, global_step=13395, preemption_count=0, score=5158.34, test/accuracy=0.4312, test/loss=2.73175, test/num_examples=10000, total_duration=5671.45, train/accuracy=0.621094, train/loss=1.78803, validation/accuracy=0.56516, validation/loss=2.04395, validation/num_examples=50000
I0307 10:45:24.487560 140185115293440 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.7146564722061157, loss=3.678119421005249
I0307 10:46:03.123265 140185199154944 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.0890393257141113, loss=3.6700830459594727
I0307 10:46:41.563642 140185115293440 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.2792423963546753, loss=3.6199495792388916
I0307 10:47:20.030403 140185199154944 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.8007023334503174, loss=3.6018826961517334
I0307 10:47:58.627255 140185115293440 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.478184700012207, loss=3.614298105239868
I0307 10:48:37.075089 140185199154944 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.238675832748413, loss=3.597423553466797
I0307 10:49:15.417881 140185115293440 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.4542193412780762, loss=3.5402839183807373
I0307 10:49:53.502822 140185199154944 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.8834463357925415, loss=3.6406779289245605
I0307 10:50:31.844367 140185115293440 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.917951226234436, loss=3.5926175117492676
I0307 10:51:10.458734 140185199154944 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.7318966388702393, loss=3.5669195652008057
I0307 10:51:49.311052 140185115293440 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.869600772857666, loss=3.5143938064575195
I0307 10:52:27.944773 140185199154944 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.9681891202926636, loss=3.555776834487915
I0307 10:53:06.842563 140185115293440 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.0594229698181152, loss=3.5389554500579834
I0307 10:53:45.580334 140185199154944 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.4466183185577393, loss=3.4887619018554688
I0307 10:53:52.137903 140341280416960 spec.py:321] Evaluating on the training split.
I0307 10:54:09.856248 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 10:54:34.384615 140341280416960 spec.py:349] Evaluating on the test split.
I0307 10:54:36.187452 140341280416960 submission_runner.py:469] Time since start: 6225.64s, 	Step: 14718, 	{'train/accuracy': 0.6342673897743225, 'train/loss': 1.7238037586212158, 'validation/accuracy': 0.5753799676895142, 'validation/loss': 1.981651782989502, 'validation/num_examples': 50000, 'test/accuracy': 0.4442000091075897, 'test/loss': 2.676860809326172, 'test/num_examples': 10000, 'score': 5668.027373313904, 'total_duration': 6225.641801357269, 'accumulated_submission_time': 5668.027373313904, 'accumulated_eval_time': 554.6683828830719, 'accumulated_logging_time': 1.322136640548706}
I0307 10:54:36.426893 140185115293440 logging_writer.py:48] [14718] accumulated_eval_time=554.668, accumulated_logging_time=1.32214, accumulated_submission_time=5668.03, global_step=14718, preemption_count=0, score=5668.03, test/accuracy=0.4442, test/loss=2.67686, test/num_examples=10000, total_duration=6225.64, train/accuracy=0.634267, train/loss=1.7238, validation/accuracy=0.57538, validation/loss=1.98165, validation/num_examples=50000
I0307 10:55:08.140623 140185199154944 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.2703956365585327, loss=3.528748035430908
I0307 10:55:46.372321 140185115293440 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.5777647495269775, loss=3.5330607891082764
I0307 10:56:24.980403 140185199154944 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.9254871606826782, loss=3.5892746448516846
I0307 10:57:03.385404 140185115293440 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5960876941680908, loss=3.626201629638672
I0307 10:57:41.616004 140185199154944 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.856544852256775, loss=3.4529671669006348
I0307 10:58:19.685133 140185115293440 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.7162303924560547, loss=3.4931156635284424
I0307 10:58:58.003018 140185199154944 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.679969072341919, loss=3.474050998687744
I0307 10:59:36.437402 140185115293440 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.4091994762420654, loss=3.4795379638671875
I0307 11:00:14.575892 140185199154944 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.537841558456421, loss=3.4525856971740723
I0307 11:00:53.558379 140185115293440 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.4641317129135132, loss=3.502760887145996
I0307 11:01:31.793060 140185199154944 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.3757445812225342, loss=3.5204358100891113
I0307 11:02:10.580778 140185115293440 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.381635069847107, loss=3.473949909210205
I0307 11:02:48.625236 140185199154944 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.4604785442352295, loss=3.5094075202941895
I0307 11:03:06.512345 140341280416960 spec.py:321] Evaluating on the training split.
I0307 11:03:26.600979 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 11:03:53.505751 140341280416960 spec.py:349] Evaluating on the test split.
I0307 11:03:55.309831 140341280416960 submission_runner.py:469] Time since start: 6784.76s, 	Step: 16048, 	{'train/accuracy': 0.6418606638908386, 'train/loss': 1.6886143684387207, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.9759666919708252, 'validation/num_examples': 50000, 'test/accuracy': 0.46300002932548523, 'test/loss': 2.591609477996826, 'test/num_examples': 10000, 'score': 6177.934530735016, 'total_duration': 6784.764118671417, 'accumulated_submission_time': 6177.934530735016, 'accumulated_eval_time': 603.4656329154968, 'accumulated_logging_time': 1.5904459953308105}
I0307 11:03:55.497407 140185115293440 logging_writer.py:48] [16048] accumulated_eval_time=603.466, accumulated_logging_time=1.59045, accumulated_submission_time=6177.93, global_step=16048, preemption_count=0, score=6177.93, test/accuracy=0.463, test/loss=2.59161, test/num_examples=10000, total_duration=6784.76, train/accuracy=0.641861, train/loss=1.68861, validation/accuracy=0.58068, validation/loss=1.97597, validation/num_examples=50000
I0307 11:04:15.759738 140185199154944 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.7286913394927979, loss=3.464750289916992
I0307 11:04:53.531700 140185115293440 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.810808539390564, loss=3.5477025508880615
I0307 11:05:31.777803 140185199154944 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.552696704864502, loss=3.4964730739593506
I0307 11:06:09.022947 140185115293440 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.5840420722961426, loss=3.4464519023895264
I0307 11:06:47.138560 140185199154944 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.5064579248428345, loss=3.389526605606079
I0307 11:07:24.999104 140185115293440 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.0724120140075684, loss=3.5430288314819336
I0307 11:08:03.335333 140185199154944 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.7001105546951294, loss=3.4854674339294434
I0307 11:08:41.489680 140185115293440 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.4590954780578613, loss=3.5058913230895996
I0307 11:09:20.135413 140185199154944 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.9908552169799805, loss=3.556257724761963
I0307 11:09:58.731717 140185115293440 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.6475695371627808, loss=3.4732744693756104
I0307 11:10:37.325252 140185199154944 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.2058849334716797, loss=3.4041056632995605
I0307 11:11:15.635607 140185115293440 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.046592950820923, loss=3.6193320751190186
I0307 11:11:54.133925 140185199154944 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.474164366722107, loss=3.482039213180542
I0307 11:12:25.726167 140341280416960 spec.py:321] Evaluating on the training split.
I0307 11:12:41.591430 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 11:13:14.439714 140341280416960 spec.py:349] Evaluating on the test split.
I0307 11:13:16.254579 140341280416960 submission_runner.py:469] Time since start: 7345.71s, 	Step: 17384, 	{'train/accuracy': 0.6472217440605164, 'train/loss': 1.6516441106796265, 'validation/accuracy': 0.5870800018310547, 'validation/loss': 1.9247711896896362, 'validation/num_examples': 50000, 'test/accuracy': 0.46150001883506775, 'test/loss': 2.6072232723236084, 'test/num_examples': 10000, 'score': 6687.970390558243, 'total_duration': 7345.708931446075, 'accumulated_submission_time': 6687.970390558243, 'accumulated_eval_time': 653.9938857555389, 'accumulated_logging_time': 1.8182072639465332}
I0307 11:13:16.344563 140185115293440 logging_writer.py:48] [17384] accumulated_eval_time=653.994, accumulated_logging_time=1.81821, accumulated_submission_time=6687.97, global_step=17384, preemption_count=0, score=6687.97, test/accuracy=0.4615, test/loss=2.60722, test/num_examples=10000, total_duration=7345.71, train/accuracy=0.647222, train/loss=1.65164, validation/accuracy=0.58708, validation/loss=1.92477, validation/num_examples=50000
I0307 11:13:23.047597 140185199154944 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.4038523435592651, loss=3.4154701232910156
I0307 11:14:01.491965 140185115293440 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.698790431022644, loss=3.4736568927764893
I0307 11:14:39.936668 140185199154944 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.4468481540679932, loss=3.4533796310424805
I0307 11:15:18.316907 140185115293440 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.139681100845337, loss=3.4748101234436035
I0307 11:15:57.140560 140185199154944 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.444466233253479, loss=3.4119527339935303
I0307 11:16:35.399863 140185115293440 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.398484468460083, loss=3.401850461959839
I0307 11:17:13.611600 140185199154944 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.327763319015503, loss=3.3549270629882812
I0307 11:17:52.710756 140185115293440 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.4187564849853516, loss=3.4627671241760254
I0307 11:18:31.239728 140185199154944 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.2957279682159424, loss=3.4137673377990723
I0307 11:19:09.624592 140185115293440 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.608243703842163, loss=3.5233025550842285
I0307 11:19:47.946477 140185199154944 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.4691197872161865, loss=3.332656145095825
I0307 11:20:26.260515 140185115293440 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.214512586593628, loss=3.429978370666504
I0307 11:21:04.616953 140185199154944 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.2550995349884033, loss=3.4166946411132812
I0307 11:21:43.030066 140185115293440 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.5283037424087524, loss=3.411588668823242
I0307 11:21:46.545462 140341280416960 spec.py:321] Evaluating on the training split.
I0307 11:22:02.740538 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 11:22:33.454284 140341280416960 spec.py:349] Evaluating on the test split.
I0307 11:22:35.278289 140341280416960 submission_runner.py:469] Time since start: 7904.73s, 	Step: 18710, 	{'train/accuracy': 0.6691844463348389, 'train/loss': 1.5429637432098389, 'validation/accuracy': 0.6073200106620789, 'validation/loss': 1.8258731365203857, 'validation/num_examples': 50000, 'test/accuracy': 0.48100003600120544, 'test/loss': 2.503962993621826, 'test/num_examples': 10000, 'score': 7198.005915403366, 'total_duration': 7904.732781648636, 'accumulated_submission_time': 7198.005915403366, 'accumulated_eval_time': 702.7266757488251, 'accumulated_logging_time': 1.94059419631958}
I0307 11:22:35.298557 140185199154944 logging_writer.py:48] [18710] accumulated_eval_time=702.727, accumulated_logging_time=1.94059, accumulated_submission_time=7198.01, global_step=18710, preemption_count=0, score=7198.01, test/accuracy=0.481, test/loss=2.50396, test/num_examples=10000, total_duration=7904.73, train/accuracy=0.669184, train/loss=1.54296, validation/accuracy=0.60732, validation/loss=1.82587, validation/num_examples=50000
I0307 11:23:10.268819 140185115293440 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.4991192817687988, loss=3.485619068145752
I0307 11:23:48.167234 140185199154944 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.3535406589508057, loss=3.3789877891540527
I0307 11:24:26.450503 140185115293440 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.3116955757141113, loss=3.496790885925293
I0307 11:25:04.289550 140185199154944 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.7699416875839233, loss=3.403963327407837
I0307 11:25:42.969836 140185115293440 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7279154062271118, loss=3.428276300430298
I0307 11:26:21.301663 140185199154944 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.4754137992858887, loss=3.4154720306396484
I0307 11:26:59.505852 140185115293440 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.387524962425232, loss=3.370574951171875
I0307 11:27:37.865126 140185199154944 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.317347764968872, loss=3.392660140991211
I0307 11:28:16.028422 140185115293440 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.4329280853271484, loss=3.439211845397949
I0307 11:28:54.645955 140185199154944 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.6318086385726929, loss=3.3899266719818115
I0307 11:29:33.148229 140185115293440 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.2759498357772827, loss=3.384416341781616
I0307 11:30:11.932388 140185199154944 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.5180662870407104, loss=3.4566049575805664
I0307 11:30:50.366353 140185115293440 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1512531042099, loss=3.3333282470703125
I0307 11:31:05.456476 140341280416960 spec.py:321] Evaluating on the training split.
I0307 11:31:21.514352 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 11:31:57.521579 140341280416960 spec.py:349] Evaluating on the test split.
I0307 11:31:59.339687 140341280416960 submission_runner.py:469] Time since start: 8468.79s, 	Step: 20040, 	{'train/accuracy': 0.6738879084587097, 'train/loss': 1.554980754852295, 'validation/accuracy': 0.6146399974822998, 'validation/loss': 1.823943853378296, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.489971876144409, 'test/num_examples': 10000, 'score': 7708.015518426895, 'total_duration': 8468.794037103653, 'accumulated_submission_time': 7708.015518426895, 'accumulated_eval_time': 756.6097090244293, 'accumulated_logging_time': 1.9686574935913086}
I0307 11:31:59.453538 140185199154944 logging_writer.py:48] [20040] accumulated_eval_time=756.61, accumulated_logging_time=1.96866, accumulated_submission_time=7708.02, global_step=20040, preemption_count=0, score=7708.02, test/accuracy=0.486, test/loss=2.48997, test/num_examples=10000, total_duration=8468.79, train/accuracy=0.673888, train/loss=1.55498, validation/accuracy=0.61464, validation/loss=1.82394, validation/num_examples=50000
I0307 11:32:23.040990 140185115293440 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.3648840188980103, loss=3.4188735485076904
I0307 11:33:01.328430 140185199154944 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.943739652633667, loss=3.319080114364624
I0307 11:33:39.994442 140185115293440 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.4466599225997925, loss=3.30564022064209
I0307 11:34:19.145035 140185199154944 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4342795610427856, loss=3.474627733230591
I0307 11:34:57.801242 140185115293440 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.2522921562194824, loss=3.4021480083465576
I0307 11:35:36.514240 140185199154944 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.2924951314926147, loss=3.366929531097412
I0307 11:36:15.292318 140185115293440 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.305397391319275, loss=3.3315141201019287
I0307 11:36:53.649863 140185199154944 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7644398212432861, loss=3.3430099487304688
I0307 11:37:31.851999 140185115293440 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.2754476070404053, loss=3.3315744400024414
I0307 11:38:10.579604 140185199154944 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.1673033237457275, loss=3.3713467121124268
I0307 11:38:48.841506 140185115293440 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.4882681369781494, loss=3.444711685180664
I0307 11:39:27.232491 140185199154944 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.4109009504318237, loss=3.328315496444702
I0307 11:40:05.883670 140185115293440 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.361249327659607, loss=3.3964083194732666
I0307 11:40:29.351645 140341280416960 spec.py:321] Evaluating on the training split.
I0307 11:40:43.342347 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 11:41:18.332242 140341280416960 spec.py:349] Evaluating on the test split.
I0307 11:41:20.166660 140341280416960 submission_runner.py:469] Time since start: 9029.62s, 	Step: 21362, 	{'train/accuracy': 0.6915059089660645, 'train/loss': 1.4980823993682861, 'validation/accuracy': 0.6279599666595459, 'validation/loss': 1.7822824716567993, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.4577853679656982, 'test/num_examples': 10000, 'score': 8217.745678424835, 'total_duration': 9029.62114405632, 'accumulated_submission_time': 8217.745678424835, 'accumulated_eval_time': 807.4246821403503, 'accumulated_logging_time': 2.103929281234741}
I0307 11:41:20.195530 140185199154944 logging_writer.py:48] [21362] accumulated_eval_time=807.425, accumulated_logging_time=2.10393, accumulated_submission_time=8217.75, global_step=21362, preemption_count=0, score=8217.75, test/accuracy=0.4934, test/loss=2.45779, test/num_examples=10000, total_duration=9029.62, train/accuracy=0.691506, train/loss=1.49808, validation/accuracy=0.62796, validation/loss=1.78228, validation/num_examples=50000
I0307 11:41:35.380762 140185115293440 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.3746002912521362, loss=3.456772804260254
I0307 11:42:13.839316 140185199154944 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.59735906124115, loss=3.3805418014526367
I0307 11:42:52.409264 140185115293440 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.1799204349517822, loss=3.3410868644714355
I0307 11:43:30.954027 140185199154944 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.399604320526123, loss=3.361368417739868
I0307 11:44:09.737206 140185115293440 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.568227767944336, loss=3.3398613929748535
I0307 11:44:48.574641 140185199154944 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.4391995668411255, loss=3.3350436687469482
I0307 11:45:27.275068 140185115293440 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.6055182218551636, loss=3.359370470046997
I0307 11:46:05.619252 140185199154944 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.477715253829956, loss=3.3589067459106445
I0307 11:46:44.695171 140185115293440 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.4217501878738403, loss=3.28787899017334
I0307 11:47:24.106840 140185199154944 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.3653500080108643, loss=3.385636329650879
I0307 11:48:03.045795 140185115293440 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.0918519496917725, loss=3.2492172718048096
I0307 11:48:41.597804 140185199154944 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.3325961828231812, loss=3.3686718940734863
I0307 11:49:20.819017 140185115293440 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.2002519369125366, loss=3.3298184871673584
I0307 11:49:50.507085 140341280416960 spec.py:321] Evaluating on the training split.
I0307 11:50:04.368724 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 11:50:35.642811 140341280416960 spec.py:349] Evaluating on the test split.
I0307 11:50:37.464490 140341280416960 submission_runner.py:469] Time since start: 9586.92s, 	Step: 22677, 	{'train/accuracy': 0.6871811151504517, 'train/loss': 1.497862458229065, 'validation/accuracy': 0.6218799948692322, 'validation/loss': 1.7898929119110107, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.4349539279937744, 'test/num_examples': 10000, 'score': 8727.906654834747, 'total_duration': 9586.918984413147, 'accumulated_submission_time': 8727.906654834747, 'accumulated_eval_time': 854.382071018219, 'accumulated_logging_time': 2.1417856216430664}
I0307 11:50:37.531552 140185199154944 logging_writer.py:48] [22677] accumulated_eval_time=854.382, accumulated_logging_time=2.14179, accumulated_submission_time=8727.91, global_step=22677, preemption_count=0, score=8727.91, test/accuracy=0.499, test/loss=2.43495, test/num_examples=10000, total_duration=9586.92, train/accuracy=0.687181, train/loss=1.49786, validation/accuracy=0.62188, validation/loss=1.78989, validation/num_examples=50000
I0307 11:50:46.795736 140185115293440 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.9281266927719116, loss=3.409524440765381
I0307 11:51:25.161833 140185199154944 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.2067888975143433, loss=3.375699520111084
I0307 11:52:03.588853 140185115293440 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.2420858144760132, loss=3.274502754211426
I0307 11:52:41.845747 140185199154944 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.5389987230300903, loss=3.3751578330993652
I0307 11:53:20.413246 140185115293440 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.4170669317245483, loss=3.3491790294647217
I0307 11:53:58.989624 140185199154944 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.4695392847061157, loss=3.3393146991729736
I0307 11:54:37.377339 140185115293440 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.3759669065475464, loss=3.3295416831970215
I0307 11:55:15.635518 140185199154944 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.2267423868179321, loss=3.321498394012451
I0307 11:55:53.919609 140185115293440 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.2016888856887817, loss=3.315006971359253
I0307 11:56:32.513480 140185199154944 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.3154855966567993, loss=3.3274242877960205
I0307 11:57:10.775116 140185115293440 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3145248889923096, loss=3.3066375255584717
I0307 11:57:49.673532 140185199154944 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.2516571283340454, loss=3.3164010047912598
I0307 11:58:28.320185 140185115293440 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.3127726316452026, loss=3.4922690391540527
I0307 11:59:07.037531 140185199154944 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2741488218307495, loss=3.340264320373535
I0307 11:59:07.803487 140341280416960 spec.py:321] Evaluating on the training split.
I0307 11:59:25.653537 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 11:59:49.327754 140341280416960 spec.py:349] Evaluating on the test split.
I0307 11:59:51.120601 140341280416960 submission_runner.py:469] Time since start: 10140.57s, 	Step: 24003, 	{'train/accuracy': 0.6844905614852905, 'train/loss': 1.4853253364562988, 'validation/accuracy': 0.6228799819946289, 'validation/loss': 1.776080846786499, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.441228151321411, 'test/num_examples': 10000, 'score': 9238.029967308044, 'total_duration': 10140.574912309647, 'accumulated_submission_time': 9238.029967308044, 'accumulated_eval_time': 897.6989679336548, 'accumulated_logging_time': 2.217595338821411}
I0307 11:59:51.153303 140185115293440 logging_writer.py:48] [24003] accumulated_eval_time=897.699, accumulated_logging_time=2.2176, accumulated_submission_time=9238.03, global_step=24003, preemption_count=0, score=9238.03, test/accuracy=0.4944, test/loss=2.44123, test/num_examples=10000, total_duration=10140.6, train/accuracy=0.684491, train/loss=1.48533, validation/accuracy=0.62288, validation/loss=1.77608, validation/num_examples=50000
I0307 12:00:28.947004 140185199154944 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.5930979251861572, loss=3.405608654022217
I0307 12:01:07.619754 140185115293440 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.3819725513458252, loss=3.310971975326538
I0307 12:01:46.257605 140185199154944 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.3895965814590454, loss=3.370846748352051
I0307 12:02:24.799707 140185115293440 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.3470836877822876, loss=3.3287863731384277
I0307 12:03:04.921300 140185199154944 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3008005619049072, loss=3.406714916229248
I0307 12:03:46.384491 140185115293440 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.2532391548156738, loss=3.346653461456299
I0307 12:04:24.657328 140185199154944 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.35336172580719, loss=3.274282455444336
I0307 12:05:03.421552 140185115293440 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.1832804679870605, loss=3.3644001483917236
I0307 12:05:40.614877 140185199154944 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.1527432203292847, loss=3.2134625911712646
I0307 12:06:20.306684 140185115293440 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.5565316677093506, loss=3.2646243572235107
I0307 12:06:59.597265 140185199154944 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.3368793725967407, loss=3.39160418510437
I0307 12:07:38.092618 140185115293440 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.3629734516143799, loss=3.2853400707244873
I0307 12:08:16.989365 140185199154944 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.250027060508728, loss=3.3332126140594482
I0307 12:08:21.301880 140341280416960 spec.py:321] Evaluating on the training split.
I0307 12:08:39.394004 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 12:09:06.773189 140341280416960 spec.py:349] Evaluating on the test split.
I0307 12:09:08.592639 140341280416960 submission_runner.py:469] Time since start: 10698.05s, 	Step: 25312, 	{'train/accuracy': 0.6946946382522583, 'train/loss': 1.4503073692321777, 'validation/accuracy': 0.6246599555015564, 'validation/loss': 1.7585413455963135, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.4092037677764893, 'test/num_examples': 10000, 'score': 9748.021029233932, 'total_duration': 10698.047112941742, 'accumulated_submission_time': 9748.021029233932, 'accumulated_eval_time': 944.9897358417511, 'accumulated_logging_time': 2.2582345008850098}
I0307 12:09:08.646843 140185115293440 logging_writer.py:48] [25312] accumulated_eval_time=944.99, accumulated_logging_time=2.25823, accumulated_submission_time=9748.02, global_step=25312, preemption_count=0, score=9748.02, test/accuracy=0.4954, test/loss=2.4092, test/num_examples=10000, total_duration=10698, train/accuracy=0.694695, train/loss=1.45031, validation/accuracy=0.62466, validation/loss=1.75854, validation/num_examples=50000
I0307 12:09:43.655596 140185199154944 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.3990998268127441, loss=3.3646693229675293
I0307 12:10:22.580581 140185115293440 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.2541046142578125, loss=3.2632343769073486
I0307 12:11:01.556829 140185199154944 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.2667138576507568, loss=3.2806951999664307
I0307 12:11:43.863851 140185115293440 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1643551588058472, loss=3.2276268005371094
I0307 12:12:25.478388 140185199154944 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.515202283859253, loss=3.309093952178955
I0307 12:13:05.221484 140185115293440 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.3246804475784302, loss=3.273775577545166
I0307 12:13:44.401752 140185199154944 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.23816978931427, loss=3.312084674835205
I0307 12:14:23.181608 140185115293440 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.5847604274749756, loss=3.2835171222686768
I0307 12:15:01.901654 140185199154944 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.4252663850784302, loss=3.22573184967041
I0307 12:15:41.065275 140185115293440 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.2894508838653564, loss=3.3036727905273438
I0307 12:16:19.549102 140185199154944 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.4038512706756592, loss=3.309788942337036
I0307 12:16:58.301851 140185115293440 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.4236479997634888, loss=3.338099479675293
I0307 12:17:37.085227 140185199154944 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.4612607955932617, loss=3.306795835494995
I0307 12:17:38.611555 140341280416960 spec.py:321] Evaluating on the training split.
I0307 12:17:54.921545 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 12:18:21.693746 140341280416960 spec.py:349] Evaluating on the test split.
I0307 12:18:23.475680 140341280416960 submission_runner.py:469] Time since start: 11252.92s, 	Step: 26605, 	{'train/accuracy': 0.6996970772743225, 'train/loss': 1.4392324686050415, 'validation/accuracy': 0.6319599747657776, 'validation/loss': 1.730715274810791, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.427701950073242, 'test/num_examples': 10000, 'score': 10257.83259153366, 'total_duration': 11252.918318986893, 'accumulated_submission_time': 10257.83259153366, 'accumulated_eval_time': 989.8419723510742, 'accumulated_logging_time': 2.321309804916382}
I0307 12:18:23.501892 140185115293440 logging_writer.py:48] [26605] accumulated_eval_time=989.842, accumulated_logging_time=2.32131, accumulated_submission_time=10257.8, global_step=26605, preemption_count=0, score=10257.8, test/accuracy=0.4981, test/loss=2.4277, test/num_examples=10000, total_duration=11252.9, train/accuracy=0.699697, train/loss=1.43923, validation/accuracy=0.63196, validation/loss=1.73072, validation/num_examples=50000
I0307 12:19:00.474640 140185199154944 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.1686452627182007, loss=3.2657487392425537
I0307 12:19:39.492977 140185115293440 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.4472107887268066, loss=3.2674739360809326
I0307 12:20:20.087932 140185199154944 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.2138712406158447, loss=3.29473614692688
I0307 12:21:00.336551 140185115293440 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.2324614524841309, loss=3.301891326904297
I0307 12:21:39.739228 140185199154944 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2413479089736938, loss=3.211838960647583
I0307 12:22:18.067069 140185115293440 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.5741095542907715, loss=3.371971845626831
I0307 12:22:56.684031 140185199154944 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.3791269063949585, loss=3.2773704528808594
I0307 12:23:35.470371 140185115293440 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.3052377700805664, loss=3.2782254219055176
I0307 12:24:14.056274 140185199154944 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1194673776626587, loss=3.252096176147461
I0307 12:24:53.215355 140185115293440 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.337152123451233, loss=3.408989429473877
I0307 12:25:31.963125 140185199154944 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.3546907901763916, loss=3.2841176986694336
I0307 12:26:10.670369 140185115293440 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.8715592622756958, loss=3.236105442047119
I0307 12:26:48.881586 140185199154944 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.6064808368682861, loss=3.3240060806274414
I0307 12:26:53.540303 140341280416960 spec.py:321] Evaluating on the training split.
I0307 12:27:08.923173 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 12:27:34.242264 140341280416960 spec.py:349] Evaluating on the test split.
I0307 12:27:36.053144 140341280416960 submission_runner.py:469] Time since start: 11805.51s, 	Step: 27913, 	{'train/accuracy': 0.7160794138908386, 'train/loss': 1.3407766819000244, 'validation/accuracy': 0.644760012626648, 'validation/loss': 1.651583194732666, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.297161340713501, 'test/num_examples': 10000, 'score': 10767.716799736023, 'total_duration': 11805.50762295723, 'accumulated_submission_time': 10767.716799736023, 'accumulated_eval_time': 1032.3547706604004, 'accumulated_logging_time': 2.355203866958618}
I0307 12:27:36.093798 140185115293440 logging_writer.py:48] [27913] accumulated_eval_time=1032.35, accumulated_logging_time=2.3552, accumulated_submission_time=10767.7, global_step=27913, preemption_count=0, score=10767.7, test/accuracy=0.5175, test/loss=2.29716, test/num_examples=10000, total_duration=11805.5, train/accuracy=0.716079, train/loss=1.34078, validation/accuracy=0.64476, validation/loss=1.65158, validation/num_examples=50000
I0307 12:28:10.208687 140185199154944 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.3057928085327148, loss=3.2461798191070557
I0307 12:28:49.037344 140185115293440 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.2692158222198486, loss=3.3190605640411377
I0307 12:29:28.462586 140185199154944 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.1916658878326416, loss=3.246791124343872
I0307 12:30:09.675449 140185115293440 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.3111929893493652, loss=3.3487768173217773
I0307 12:30:48.225172 140185199154944 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.4335198402404785, loss=3.2162959575653076
I0307 12:31:29.636466 140185115293440 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.2964823246002197, loss=3.2777585983276367
I0307 12:32:08.814395 140185199154944 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.4391614198684692, loss=3.275294303894043
I0307 12:32:47.252352 140185115293440 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.2659252882003784, loss=3.335236072540283
I0307 12:33:26.531690 140185199154944 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.2518932819366455, loss=3.3073272705078125
I0307 12:34:05.543395 140185115293440 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.329756736755371, loss=3.253896713256836
I0307 12:34:43.852316 140185199154944 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6188445091247559, loss=3.291127920150757
I0307 12:35:22.501202 140185115293440 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.4388173818588257, loss=3.3205389976501465
I0307 12:36:01.064949 140185199154944 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.3909618854522705, loss=3.2548344135284424
I0307 12:36:06.143394 140341280416960 spec.py:321] Evaluating on the training split.
I0307 12:36:22.386987 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 12:36:46.540586 140341280416960 spec.py:349] Evaluating on the test split.
I0307 12:36:48.351073 140341280416960 submission_runner.py:469] Time since start: 12357.81s, 	Step: 29214, 	{'train/accuracy': 0.7053172588348389, 'train/loss': 1.3841984272003174, 'validation/accuracy': 0.6370799541473389, 'validation/loss': 1.687349796295166, 'validation/num_examples': 50000, 'test/accuracy': 0.5116000175476074, 'test/loss': 2.3664023876190186, 'test/num_examples': 10000, 'score': 11277.586159706116, 'total_duration': 12357.805555343628, 'accumulated_submission_time': 11277.586159706116, 'accumulated_eval_time': 1074.5624024868011, 'accumulated_logging_time': 2.431577682495117}
I0307 12:36:48.420877 140185115293440 logging_writer.py:48] [29214] accumulated_eval_time=1074.56, accumulated_logging_time=2.43158, accumulated_submission_time=11277.6, global_step=29214, preemption_count=0, score=11277.6, test/accuracy=0.5116, test/loss=2.3664, test/num_examples=10000, total_duration=12357.8, train/accuracy=0.705317, train/loss=1.3842, validation/accuracy=0.63708, validation/loss=1.68735, validation/num_examples=50000
I0307 12:37:22.620276 140185199154944 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.417011022567749, loss=3.2587084770202637
I0307 12:38:00.945124 140185115293440 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.3331643342971802, loss=3.314452648162842
I0307 12:38:39.821515 140185199154944 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.2662993669509888, loss=3.2117366790771484
I0307 12:39:18.270898 140185115293440 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.3025908470153809, loss=3.279024600982666
I0307 12:39:57.374401 140185199154944 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.633585810661316, loss=3.2760188579559326
I0307 12:40:36.907076 140185115293440 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.425729513168335, loss=3.290510416030884
I0307 12:41:15.265869 140185199154944 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.5147594213485718, loss=3.3312697410583496
I0307 12:41:53.918325 140185115293440 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.2773940563201904, loss=3.219144344329834
I0307 12:42:32.788268 140185199154944 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.3583160638809204, loss=3.315521717071533
I0307 12:43:11.170748 140185115293440 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.29033362865448, loss=3.2318732738494873
I0307 12:43:49.798955 140185199154944 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.3832741975784302, loss=3.3437232971191406
I0307 12:44:28.113124 140185115293440 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.4462387561798096, loss=3.293491840362549
I0307 12:45:06.470393 140185199154944 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.5790766477584839, loss=3.2934231758117676
I0307 12:45:18.645447 140341280416960 spec.py:321] Evaluating on the training split.
I0307 12:45:34.305918 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 12:45:58.351378 140341280416960 spec.py:349] Evaluating on the test split.
I0307 12:46:00.146287 140341280416960 submission_runner.py:469] Time since start: 12909.60s, 	Step: 30533, 	{'train/accuracy': 0.7027662396430969, 'train/loss': 1.4169032573699951, 'validation/accuracy': 0.6337199807167053, 'validation/loss': 1.72417151927948, 'validation/num_examples': 50000, 'test/accuracy': 0.5106000304222107, 'test/loss': 2.3677446842193604, 'test/num_examples': 10000, 'score': 11787.666238307953, 'total_duration': 12909.600771903992, 'accumulated_submission_time': 11787.666238307953, 'accumulated_eval_time': 1116.0632033348083, 'accumulated_logging_time': 2.5101253986358643}
I0307 12:46:00.179228 140185115293440 logging_writer.py:48] [30533] accumulated_eval_time=1116.06, accumulated_logging_time=2.51013, accumulated_submission_time=11787.7, global_step=30533, preemption_count=0, score=11787.7, test/accuracy=0.5106, test/loss=2.36774, test/num_examples=10000, total_duration=12909.6, train/accuracy=0.702766, train/loss=1.4169, validation/accuracy=0.63372, validation/loss=1.72417, validation/num_examples=50000
I0307 12:46:26.504626 140185199154944 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.4594558477401733, loss=3.2907533645629883
I0307 12:47:05.154872 140185115293440 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.2996351718902588, loss=3.257340431213379
I0307 12:47:43.651427 140185199154944 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.3829565048217773, loss=3.3321802616119385
I0307 12:48:22.703294 140185115293440 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.546517014503479, loss=3.304461717605591
I0307 12:49:01.388509 140185199154944 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.4022634029388428, loss=3.2223782539367676
I0307 12:49:39.819803 140185115293440 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.3895018100738525, loss=3.261948823928833
I0307 12:50:18.073320 140185199154944 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.403505563735962, loss=3.2716987133026123
I0307 12:50:56.765745 140185115293440 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.4111249446868896, loss=3.249330997467041
I0307 12:51:35.120342 140185199154944 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.58162522315979, loss=3.31925892829895
I0307 12:52:13.348069 140185115293440 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.4273858070373535, loss=3.2088639736175537
I0307 12:52:51.681168 140185199154944 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.344753623008728, loss=3.236100435256958
I0307 12:53:30.572510 140185115293440 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.6325708627700806, loss=3.267359972000122
I0307 12:54:09.209830 140185199154944 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.3787318468093872, loss=3.291330337524414
I0307 12:54:30.432083 140341280416960 spec.py:321] Evaluating on the training split.
I0307 12:54:48.741252 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 12:55:12.312071 140341280416960 spec.py:349] Evaluating on the test split.
I0307 12:55:14.117585 140341280416960 submission_runner.py:469] Time since start: 13463.57s, 	Step: 31856, 	{'train/accuracy': 0.7217394709587097, 'train/loss': 1.3666224479675293, 'validation/accuracy': 0.6528199911117554, 'validation/loss': 1.6680662631988525, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.3135781288146973, 'test/num_examples': 10000, 'score': 12297.766247272491, 'total_duration': 13463.571939229965, 'accumulated_submission_time': 12297.766247272491, 'accumulated_eval_time': 1159.7485346794128, 'accumulated_logging_time': 2.5615243911743164}
I0307 12:55:14.158607 140185115293440 logging_writer.py:48] [31856] accumulated_eval_time=1159.75, accumulated_logging_time=2.56152, accumulated_submission_time=12297.8, global_step=31856, preemption_count=0, score=12297.8, test/accuracy=0.5218, test/loss=2.31358, test/num_examples=10000, total_duration=13463.6, train/accuracy=0.721739, train/loss=1.36662, validation/accuracy=0.65282, validation/loss=1.66807, validation/num_examples=50000
I0307 12:55:31.667162 140185199154944 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.5486286878585815, loss=3.2017197608947754
I0307 12:56:09.988428 140185115293440 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.540596604347229, loss=3.208585262298584
I0307 12:56:48.399822 140185199154944 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.4423961639404297, loss=3.250063180923462
I0307 12:57:26.760374 140185115293440 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3148963451385498, loss=3.2934727668762207
I0307 12:58:06.136572 140185199154944 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.3152753114700317, loss=3.342519521713257
I0307 12:58:44.420909 140185115293440 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.393853783607483, loss=3.1910178661346436
I0307 12:59:23.132268 140185199154944 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.576141357421875, loss=3.2434091567993164
I0307 13:00:02.126612 140185115293440 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.5421998500823975, loss=3.257723331451416
I0307 13:00:40.681833 140185199154944 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.2652900218963623, loss=3.220695972442627
I0307 13:01:19.366821 140185115293440 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.245361328125, loss=3.182426929473877
I0307 13:01:58.184055 140185199154944 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.3242721557617188, loss=3.2306716442108154
I0307 13:02:37.029059 140185115293440 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.388798713684082, loss=3.2753562927246094
I0307 13:03:15.199900 140185199154944 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.4628688097000122, loss=3.28358793258667
I0307 13:03:44.322658 140341280416960 spec.py:321] Evaluating on the training split.
I0307 13:03:56.273237 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 13:04:41.864040 140341280416960 spec.py:349] Evaluating on the test split.
I0307 13:04:43.665831 140341280416960 submission_runner.py:469] Time since start: 14033.12s, 	Step: 33177, 	{'train/accuracy': 0.7101203799247742, 'train/loss': 1.36191725730896, 'validation/accuracy': 0.6447799801826477, 'validation/loss': 1.6609554290771484, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.3287246227264404, 'test/num_examples': 10000, 'score': 12807.7827064991, 'total_duration': 14033.12017083168, 'accumulated_submission_time': 12807.7827064991, 'accumulated_eval_time': 1219.0915217399597, 'accumulated_logging_time': 2.6112585067749023}
I0307 13:04:43.700843 140185115293440 logging_writer.py:48] [33177] accumulated_eval_time=1219.09, accumulated_logging_time=2.61126, accumulated_submission_time=12807.8, global_step=33177, preemption_count=0, score=12807.8, test/accuracy=0.5164, test/loss=2.32872, test/num_examples=10000, total_duration=14033.1, train/accuracy=0.71012, train/loss=1.36192, validation/accuracy=0.64478, validation/loss=1.66096, validation/num_examples=50000
I0307 13:04:53.011361 140185199154944 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.3784360885620117, loss=3.183067560195923
I0307 13:05:31.774044 140185115293440 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.6876673698425293, loss=3.2489330768585205
I0307 13:06:10.285246 140185199154944 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.6828984022140503, loss=3.1618635654449463
I0307 13:06:48.923878 140185115293440 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.3394287824630737, loss=3.2915337085723877
I0307 13:07:29.412459 140185199154944 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.2852658033370972, loss=3.273256778717041
I0307 13:08:07.875398 140185115293440 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.1219875812530518, loss=3.301919937133789
I0307 13:08:46.874313 140185199154944 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.4496150016784668, loss=3.18295955657959
I0307 13:09:26.080935 140185115293440 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.4051446914672852, loss=3.291110038757324
I0307 13:10:04.780686 140185199154944 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.639055848121643, loss=3.1801345348358154
I0307 13:10:43.167650 140185115293440 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.2078415155410767, loss=3.3199782371520996
I0307 13:11:21.880101 140185199154944 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.7514581680297852, loss=3.241797924041748
I0307 13:12:00.456584 140185115293440 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.6355036497116089, loss=3.2458889484405518
I0307 13:12:39.127333 140185199154944 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.533244013786316, loss=3.2140636444091797
I0307 13:13:13.847970 140341280416960 spec.py:321] Evaluating on the training split.
I0307 13:13:25.728067 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 13:14:07.046907 140341280416960 spec.py:349] Evaluating on the test split.
I0307 13:14:08.825710 140341280416960 submission_runner.py:469] Time since start: 14598.28s, 	Step: 34491, 	{'train/accuracy': 0.7182915806770325, 'train/loss': 1.3755179643630981, 'validation/accuracy': 0.6513800024986267, 'validation/loss': 1.6660535335540771, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.324406862258911, 'test/num_examples': 10000, 'score': 13317.775473594666, 'total_duration': 14598.280057668686, 'accumulated_submission_time': 13317.775473594666, 'accumulated_eval_time': 1274.069085597992, 'accumulated_logging_time': 2.6554343700408936}
I0307 13:14:08.863065 140185115293440 logging_writer.py:48] [34491] accumulated_eval_time=1274.07, accumulated_logging_time=2.65543, accumulated_submission_time=13317.8, global_step=34491, preemption_count=0, score=13317.8, test/accuracy=0.5176, test/loss=2.32441, test/num_examples=10000, total_duration=14598.3, train/accuracy=0.718292, train/loss=1.37552, validation/accuracy=0.65138, validation/loss=1.66605, validation/num_examples=50000
I0307 13:14:12.741300 140185199154944 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.2870144844055176, loss=3.225836753845215
I0307 13:14:51.216381 140185115293440 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.5920758247375488, loss=3.222985029220581
I0307 13:15:30.044908 140185199154944 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.6666350364685059, loss=3.2727487087249756
I0307 13:16:09.520904 140185115293440 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.459175944328308, loss=3.2333505153656006
I0307 13:16:48.656332 140185199154944 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.347360372543335, loss=3.212892532348633
I0307 13:17:28.165864 140185115293440 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.4223365783691406, loss=3.158217430114746
I0307 13:18:07.922812 140185199154944 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.915403962135315, loss=3.2675626277923584
I0307 13:18:46.621089 140185115293440 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.3986725807189941, loss=3.1496593952178955
I0307 13:19:24.970279 140185199154944 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.3930120468139648, loss=3.215930938720703
I0307 13:20:03.837359 140185115293440 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.594307780265808, loss=3.227360486984253
I0307 13:20:42.257077 140185199154944 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.5110270977020264, loss=3.2615394592285156
I0307 13:21:21.106549 140185115293440 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.5122673511505127, loss=3.1967735290527344
I0307 13:22:00.054816 140185199154944 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.5184431076049805, loss=3.198129177093506
I0307 13:22:38.670650 140185115293440 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.338531255722046, loss=3.2489123344421387
I0307 13:22:39.029618 140341280416960 spec.py:321] Evaluating on the training split.
I0307 13:22:51.373137 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 13:23:26.534190 140341280416960 spec.py:349] Evaluating on the test split.
I0307 13:23:28.338111 140341280416960 submission_runner.py:469] Time since start: 15157.79s, 	Step: 35802, 	{'train/accuracy': 0.7181521058082581, 'train/loss': 1.3607012033462524, 'validation/accuracy': 0.6498599648475647, 'validation/loss': 1.6598118543624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.316274642944336, 'test/num_examples': 10000, 'score': 13827.749715566635, 'total_duration': 15157.79241991043, 'accumulated_submission_time': 13827.749715566635, 'accumulated_eval_time': 1323.3773593902588, 'accumulated_logging_time': 2.7342426776885986}
I0307 13:23:28.390267 140185199154944 logging_writer.py:48] [35802] accumulated_eval_time=1323.38, accumulated_logging_time=2.73424, accumulated_submission_time=13827.7, global_step=35802, preemption_count=0, score=13827.7, test/accuracy=0.5208, test/loss=2.31627, test/num_examples=10000, total_duration=15157.8, train/accuracy=0.718152, train/loss=1.3607, validation/accuracy=0.64986, validation/loss=1.65981, validation/num_examples=50000
I0307 13:24:06.274684 140185115293440 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.9514166116714478, loss=3.1842215061187744
I0307 13:24:44.673455 140185199154944 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.5366405248641968, loss=3.221345901489258
I0307 13:25:23.743783 140185115293440 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7897566556930542, loss=3.219703197479248
I0307 13:26:02.216930 140185199154944 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.4605093002319336, loss=3.2667551040649414
I0307 13:26:40.917823 140185115293440 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.5643320083618164, loss=3.2821154594421387
I0307 13:27:20.335991 140185199154944 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.4358278512954712, loss=3.197868824005127
I0307 13:27:59.060248 140185115293440 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.4169721603393555, loss=3.265129566192627
I0307 13:28:37.552593 140185199154944 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.4479305744171143, loss=3.2264697551727295
I0307 13:29:15.752813 140185115293440 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.5283901691436768, loss=3.1562740802764893
I0307 13:29:54.367368 140185199154944 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.5421717166900635, loss=3.1969194412231445
I0307 13:30:33.177300 140185115293440 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.2818655967712402, loss=3.2216856479644775
I0307 13:31:11.779812 140185199154944 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.5918396711349487, loss=3.2538859844207764
I0307 13:31:50.390360 140185115293440 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.3695793151855469, loss=3.2170395851135254
I0307 13:31:58.580425 140341280416960 spec.py:321] Evaluating on the training split.
I0307 13:32:09.952149 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 13:32:43.333924 140341280416960 spec.py:349] Evaluating on the test split.
I0307 13:32:45.138749 140341280416960 submission_runner.py:469] Time since start: 15714.59s, 	Step: 37122, 	{'train/accuracy': 0.7237722873687744, 'train/loss': 1.3473910093307495, 'validation/accuracy': 0.6503199934959412, 'validation/loss': 1.652050256729126, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.296616554260254, 'test/num_examples': 10000, 'score': 14337.757759094238, 'total_duration': 15714.593129873276, 'accumulated_submission_time': 14337.757759094238, 'accumulated_eval_time': 1369.93554520607, 'accumulated_logging_time': 2.8205060958862305}
I0307 13:32:45.158205 140185199154944 logging_writer.py:48] [37122] accumulated_eval_time=1369.94, accumulated_logging_time=2.82051, accumulated_submission_time=14337.8, global_step=37122, preemption_count=0, score=14337.8, test/accuracy=0.5223, test/loss=2.29662, test/num_examples=10000, total_duration=15714.6, train/accuracy=0.723772, train/loss=1.34739, validation/accuracy=0.65032, validation/loss=1.65205, validation/num_examples=50000
I0307 13:33:15.534496 140185115293440 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.545981764793396, loss=3.2009174823760986
I0307 13:33:54.844456 140185199154944 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.670026421546936, loss=3.190399646759033
I0307 13:34:33.992855 140185115293440 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.476460576057434, loss=3.123910427093506
I0307 13:35:12.427501 140185199154944 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.615088939666748, loss=3.1989479064941406
I0307 13:35:51.583873 140185115293440 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.3626796007156372, loss=3.1683363914489746
I0307 13:36:30.213751 140185199154944 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.6677762269973755, loss=3.328857898712158
I0307 13:37:08.564040 140185115293440 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.6298184394836426, loss=3.267272710800171
I0307 13:37:47.610937 140185199154944 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.545277714729309, loss=3.2386059761047363
I0307 13:38:26.183023 140185115293440 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.5049339532852173, loss=3.1404969692230225
I0307 13:39:04.639638 140185199154944 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.585894227027893, loss=3.2612509727478027
I0307 13:39:43.184823 140185115293440 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.4211883544921875, loss=3.237173080444336
I0307 13:40:21.809178 140185199154944 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.4752418994903564, loss=3.2500991821289062
I0307 13:41:00.073606 140185115293440 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.5503169298171997, loss=3.2672619819641113
I0307 13:41:15.287532 140341280416960 spec.py:321] Evaluating on the training split.
I0307 13:41:26.426728 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 13:42:06.211110 140341280416960 spec.py:349] Evaluating on the test split.
I0307 13:42:08.007603 140341280416960 submission_runner.py:469] Time since start: 16277.46s, 	Step: 38440, 	{'train/accuracy': 0.7148237824440002, 'train/loss': 1.3588889837265015, 'validation/accuracy': 0.6502799987792969, 'validation/loss': 1.6584783792495728, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.3372254371643066, 'test/num_examples': 10000, 'score': 14847.710516929626, 'total_duration': 16277.461966514587, 'accumulated_submission_time': 14847.710516929626, 'accumulated_eval_time': 1422.655452489853, 'accumulated_logging_time': 2.874232292175293}
I0307 13:42:08.061774 140185199154944 logging_writer.py:48] [38440] accumulated_eval_time=1422.66, accumulated_logging_time=2.87423, accumulated_submission_time=14847.7, global_step=38440, preemption_count=0, score=14847.7, test/accuracy=0.519, test/loss=2.33723, test/num_examples=10000, total_duration=16277.5, train/accuracy=0.714824, train/loss=1.35889, validation/accuracy=0.65028, validation/loss=1.65848, validation/num_examples=50000
I0307 13:42:31.808337 140185115293440 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.3635517358779907, loss=3.2003116607666016
I0307 13:43:10.839510 140185199154944 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.6801352500915527, loss=3.219512701034546
I0307 13:43:49.213471 140185115293440 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.461201786994934, loss=3.220945119857788
I0307 13:44:27.841188 140185199154944 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.5714062452316284, loss=3.2028284072875977
I0307 13:45:06.020790 140185115293440 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.5150136947631836, loss=3.2345190048217773
I0307 13:45:44.508630 140185199154944 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.593576192855835, loss=3.2427804470062256
I0307 13:46:23.310795 140185115293440 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.5112184286117554, loss=3.176967144012451
I0307 13:47:01.727437 140185199154944 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.51820707321167, loss=3.1982319355010986
I0307 13:47:40.161999 140185115293440 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.4774200916290283, loss=3.116598606109619
I0307 13:48:18.178988 140185199154944 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.8107998371124268, loss=3.2205424308776855
I0307 13:48:56.239542 140185115293440 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.5159202814102173, loss=3.1946918964385986
I0307 13:49:34.709307 140185199154944 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.9027854204177856, loss=3.2284305095672607
I0307 13:50:13.415416 140185115293440 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.6436518430709839, loss=3.249021053314209
I0307 13:50:38.299542 140341280416960 spec.py:321] Evaluating on the training split.
I0307 13:50:49.795191 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 13:51:24.040603 140341280416960 spec.py:349] Evaluating on the test split.
I0307 13:51:25.849223 140341280416960 submission_runner.py:469] Time since start: 16835.30s, 	Step: 39765, 	{'train/accuracy': 0.7251474857330322, 'train/loss': 1.3125813007354736, 'validation/accuracy': 0.6564599871635437, 'validation/loss': 1.6125577688217163, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.249238967895508, 'test/num_examples': 10000, 'score': 15357.758349895477, 'total_duration': 16835.303562402725, 'accumulated_submission_time': 15357.758349895477, 'accumulated_eval_time': 1470.2049469947815, 'accumulated_logging_time': 2.9794087409973145}
I0307 13:51:25.904070 140185199154944 logging_writer.py:48] [39765] accumulated_eval_time=1470.2, accumulated_logging_time=2.97941, accumulated_submission_time=15357.8, global_step=39765, preemption_count=0, score=15357.8, test/accuracy=0.5337, test/loss=2.24924, test/num_examples=10000, total_duration=16835.3, train/accuracy=0.725147, train/loss=1.31258, validation/accuracy=0.65646, validation/loss=1.61256, validation/num_examples=50000
I0307 13:51:39.811938 140185115293440 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.575650691986084, loss=3.196258306503296
I0307 13:52:17.888909 140185199154944 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.4848287105560303, loss=3.2420592308044434
I0307 13:52:56.557421 140185115293440 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.6212702989578247, loss=3.188933849334717
I0307 13:53:35.646794 140185199154944 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.5491503477096558, loss=3.2639079093933105
I0307 13:54:14.152418 140185115293440 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.5336416959762573, loss=3.186145782470703
I0307 13:54:52.928166 140185199154944 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.5097684860229492, loss=3.209284782409668
I0307 13:55:31.433701 140185115293440 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.5677361488342285, loss=3.14764404296875
I0307 13:56:09.875324 140185199154944 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.590958833694458, loss=3.2494516372680664
I0307 13:56:48.662955 140185115293440 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.9345749616622925, loss=3.1873278617858887
I0307 13:57:27.514720 140185199154944 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.505205750465393, loss=3.1922597885131836
I0307 13:58:06.033174 140185115293440 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.4208650588989258, loss=3.163358449935913
I0307 13:58:44.520304 140185199154944 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.5929429531097412, loss=3.2309303283691406
I0307 13:59:23.115642 140185115293440 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.5939797163009644, loss=3.163346767425537
I0307 13:59:56.137822 140341280416960 spec.py:321] Evaluating on the training split.
I0307 14:00:07.550390 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 14:00:41.642758 140341280416960 spec.py:349] Evaluating on the test split.
I0307 14:00:43.440113 140341280416960 submission_runner.py:469] Time since start: 17392.89s, 	Step: 41084, 	{'train/accuracy': 0.7266222834587097, 'train/loss': 1.3028889894485474, 'validation/accuracy': 0.6598199605941772, 'validation/loss': 1.5991718769073486, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.284785032272339, 'test/num_examples': 10000, 'score': 15867.81780910492, 'total_duration': 17392.89449262619, 'accumulated_submission_time': 15867.81780910492, 'accumulated_eval_time': 1517.5070989131927, 'accumulated_logging_time': 3.0613551139831543}
I0307 14:00:43.479942 140185199154944 logging_writer.py:48] [41084] accumulated_eval_time=1517.51, accumulated_logging_time=3.06136, accumulated_submission_time=15867.8, global_step=41084, preemption_count=0, score=15867.8, test/accuracy=0.5218, test/loss=2.28479, test/num_examples=10000, total_duration=17392.9, train/accuracy=0.726622, train/loss=1.30289, validation/accuracy=0.65982, validation/loss=1.59917, validation/num_examples=50000
I0307 14:00:50.132669 140185115293440 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.4687379598617554, loss=3.17486572265625
I0307 14:01:28.585390 140185199154944 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.6196205615997314, loss=3.279860019683838
I0307 14:02:07.521229 140185115293440 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.647010326385498, loss=3.2837905883789062
I0307 14:02:46.162801 140185199154944 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.510694146156311, loss=3.215954065322876
I0307 14:03:24.872816 140185115293440 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.5907646417617798, loss=3.1965675354003906
I0307 14:04:02.762564 140185199154944 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.4756602048873901, loss=3.201756000518799
I0307 14:04:41.217208 140185115293440 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.5061081647872925, loss=3.2014997005462646
I0307 14:05:19.668148 140185199154944 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.5238401889801025, loss=3.185389280319214
I0307 14:05:58.061197 140185115293440 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.6092263460159302, loss=3.154374122619629
I0307 14:06:36.503086 140185199154944 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.4238343238830566, loss=3.1437489986419678
I0307 14:07:15.231966 140185115293440 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.453307867050171, loss=3.1268820762634277
I0307 14:07:54.264504 140185199154944 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.8431096076965332, loss=3.2104694843292236
I0307 14:08:33.894681 140185115293440 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6875001192092896, loss=3.163311004638672
I0307 14:09:12.977623 140185199154944 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.4842617511749268, loss=3.1392812728881836
I0307 14:09:13.728410 140341280416960 spec.py:321] Evaluating on the training split.
I0307 14:09:25.837763 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 14:09:58.976713 140341280416960 spec.py:349] Evaluating on the test split.
I0307 14:10:00.775609 140341280416960 submission_runner.py:469] Time since start: 17950.23s, 	Step: 42403, 	{'train/accuracy': 0.7292131781578064, 'train/loss': 1.2720746994018555, 'validation/accuracy': 0.6576600074768066, 'validation/loss': 1.592840313911438, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.24770450592041, 'test/num_examples': 10000, 'score': 16377.884187936783, 'total_duration': 17950.229959487915, 'accumulated_submission_time': 16377.884187936783, 'accumulated_eval_time': 1564.5541186332703, 'accumulated_logging_time': 3.1414103507995605}
I0307 14:10:00.808353 140185115293440 logging_writer.py:48] [42403] accumulated_eval_time=1564.55, accumulated_logging_time=3.14141, accumulated_submission_time=16377.9, global_step=42403, preemption_count=0, score=16377.9, test/accuracy=0.5323, test/loss=2.2477, test/num_examples=10000, total_duration=17950.2, train/accuracy=0.729213, train/loss=1.27207, validation/accuracy=0.65766, validation/loss=1.59284, validation/num_examples=50000
I0307 14:10:38.690984 140185199154944 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.8501594066619873, loss=3.1630280017852783
I0307 14:11:17.926254 140185115293440 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6821191310882568, loss=3.184255361557007
I0307 14:11:56.645736 140185199154944 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.7602938413619995, loss=3.2588698863983154
I0307 14:12:35.441458 140185115293440 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.4836556911468506, loss=3.2162764072418213
I0307 14:13:13.728362 140185199154944 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.5983222723007202, loss=3.2546772956848145
I0307 14:13:51.933303 140185115293440 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.7030203342437744, loss=3.2385001182556152
I0307 14:14:30.730393 140185199154944 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.5781365633010864, loss=3.170548915863037
I0307 14:15:09.324345 140185115293440 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7467297315597534, loss=3.1905484199523926
I0307 14:15:47.844818 140185199154944 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.5635639429092407, loss=3.1945576667785645
I0307 14:16:26.394286 140185115293440 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.6576273441314697, loss=3.3435282707214355
I0307 14:17:05.158391 140185199154944 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.6623135805130005, loss=3.2143561840057373
I0307 14:17:44.679180 140185115293440 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.5455658435821533, loss=3.1549935340881348
I0307 14:18:23.477336 140185199154944 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.5502744913101196, loss=3.1878104209899902
I0307 14:18:31.161289 140341280416960 spec.py:321] Evaluating on the training split.
I0307 14:18:42.447123 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 14:19:14.613793 140341280416960 spec.py:349] Evaluating on the test split.
I0307 14:19:16.396401 140341280416960 submission_runner.py:469] Time since start: 18505.85s, 	Step: 43721, 	{'train/accuracy': 0.725605845451355, 'train/loss': 1.3395122289657593, 'validation/accuracy': 0.6609999537467957, 'validation/loss': 1.6278122663497925, 'validation/num_examples': 50000, 'test/accuracy': 0.5358999967575073, 'test/loss': 2.2774667739868164, 'test/num_examples': 10000, 'score': 16888.06930756569, 'total_duration': 18505.850697040558, 'accumulated_submission_time': 16888.06930756569, 'accumulated_eval_time': 1609.7890067100525, 'accumulated_logging_time': 3.200737953186035}
I0307 14:19:16.440845 140185115293440 logging_writer.py:48] [43721] accumulated_eval_time=1609.79, accumulated_logging_time=3.20074, accumulated_submission_time=16888.1, global_step=43721, preemption_count=0, score=16888.1, test/accuracy=0.5359, test/loss=2.27747, test/num_examples=10000, total_duration=18505.9, train/accuracy=0.725606, train/loss=1.33951, validation/accuracy=0.661, validation/loss=1.62781, validation/num_examples=50000
I0307 14:19:48.119354 140185199154944 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.6796356439590454, loss=3.188915491104126
I0307 14:20:26.814912 140185115293440 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7440392971038818, loss=3.1588573455810547
I0307 14:21:05.455896 140185199154944 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.6568524837493896, loss=3.2067065238952637
I0307 14:21:43.992369 140185115293440 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.7083336114883423, loss=3.2015693187713623
I0307 14:22:22.596466 140185199154944 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.6098062992095947, loss=3.2313249111175537
I0307 14:23:01.572431 140185115293440 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.680763602256775, loss=3.102951765060425
I0307 14:23:39.988641 140185199154944 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.6799898147583008, loss=3.1966745853424072
I0307 14:24:18.463186 140185115293440 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.6774550676345825, loss=3.2646212577819824
I0307 14:24:57.164060 140185199154944 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6023935079574585, loss=3.1468255519866943
I0307 14:25:36.031607 140185115293440 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.7177764177322388, loss=3.1601672172546387
I0307 14:26:14.931340 140185199154944 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.5804812908172607, loss=3.2589218616485596
I0307 14:26:53.620997 140185115293440 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.8142093420028687, loss=3.2003326416015625
I0307 14:27:32.530141 140185199154944 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.6777606010437012, loss=3.2474570274353027
I0307 14:27:46.764639 140341280416960 spec.py:321] Evaluating on the training split.
I0307 14:27:57.744621 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 14:28:38.026608 140341280416960 spec.py:349] Evaluating on the test split.
I0307 14:28:39.837555 140341280416960 submission_runner.py:469] Time since start: 19069.29s, 	Step: 45037, 	{'train/accuracy': 0.72171950340271, 'train/loss': 1.3635785579681396, 'validation/accuracy': 0.6539799571037292, 'validation/loss': 1.658382773399353, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.3228673934936523, 'test/num_examples': 10000, 'score': 17398.229001522064, 'total_duration': 19069.29190993309, 'accumulated_submission_time': 17398.229001522064, 'accumulated_eval_time': 1662.861756324768, 'accumulated_logging_time': 3.2768237590789795}
I0307 14:28:39.869317 140185115293440 logging_writer.py:48] [45037] accumulated_eval_time=1662.86, accumulated_logging_time=3.27682, accumulated_submission_time=17398.2, global_step=45037, preemption_count=0, score=17398.2, test/accuracy=0.5245, test/loss=2.32287, test/num_examples=10000, total_duration=19069.3, train/accuracy=0.72172, train/loss=1.36358, validation/accuracy=0.65398, validation/loss=1.65838, validation/num_examples=50000
I0307 14:29:04.544569 140185199154944 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.5895380973815918, loss=3.2581467628479004
I0307 14:29:42.954120 140185115293440 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.5419167280197144, loss=3.1617238521575928
I0307 14:30:21.236240 140185199154944 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.5778146982192993, loss=3.1799073219299316
I0307 14:30:59.815212 140185115293440 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.6528366804122925, loss=3.175610303878784
I0307 14:31:38.435583 140185199154944 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.6218252182006836, loss=3.1540937423706055
I0307 14:32:17.053440 140185115293440 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.7251635789871216, loss=3.2237231731414795
I0307 14:32:55.315116 140185199154944 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.7498981952667236, loss=3.2040233612060547
I0307 14:33:33.680538 140185115293440 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.1019632816314697, loss=3.17425537109375
I0307 14:34:12.228434 140185199154944 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.053467273712158, loss=3.1726293563842773
I0307 14:34:50.392992 140185115293440 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7041014432907104, loss=3.1403932571411133
I0307 14:35:30.333351 140185199154944 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.6310534477233887, loss=3.2133114337921143
I0307 14:36:09.095446 140185115293440 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.7066881656646729, loss=3.1892781257629395
I0307 14:36:47.965066 140185199154944 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.5810412168502808, loss=3.2344846725463867
I0307 14:37:10.006772 140341280416960 spec.py:321] Evaluating on the training split.
I0307 14:37:21.085918 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 14:38:07.368939 140341280416960 spec.py:349] Evaluating on the test split.
I0307 14:38:09.174160 140341280416960 submission_runner.py:469] Time since start: 19638.63s, 	Step: 46358, 	{'train/accuracy': 0.7330596446990967, 'train/loss': 1.289806842803955, 'validation/accuracy': 0.6603599786758423, 'validation/loss': 1.6059191226959229, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.2774500846862793, 'test/num_examples': 10000, 'score': 17908.21753168106, 'total_duration': 19638.628463745117, 'accumulated_submission_time': 17908.21753168106, 'accumulated_eval_time': 1722.0289239883423, 'accumulated_logging_time': 3.325507164001465}
I0307 14:38:09.234744 140185115293440 logging_writer.py:48] [46358] accumulated_eval_time=1722.03, accumulated_logging_time=3.32551, accumulated_submission_time=17908.2, global_step=46358, preemption_count=0, score=17908.2, test/accuracy=0.5294, test/loss=2.27745, test/num_examples=10000, total_duration=19638.6, train/accuracy=0.73306, train/loss=1.28981, validation/accuracy=0.66036, validation/loss=1.60592, validation/num_examples=50000
I0307 14:38:26.153762 140185199154944 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.716735601425171, loss=3.1168456077575684
I0307 14:39:04.411717 140185115293440 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.6849068403244019, loss=3.2309083938598633
I0307 14:39:42.853913 140185199154944 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.7641465663909912, loss=3.215345859527588
I0307 14:40:21.826747 140185115293440 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.8081644773483276, loss=3.171815872192383
I0307 14:41:00.351848 140185199154944 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.6955431699752808, loss=3.265859603881836
I0307 14:41:38.968237 140185115293440 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.6857562065124512, loss=3.128173828125
I0307 14:42:17.560644 140185199154944 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.1817877292633057, loss=3.187504768371582
I0307 14:42:56.197055 140185115293440 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.627432107925415, loss=3.1299281120300293
I0307 14:43:34.428890 140185199154944 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.7908520698547363, loss=3.168341636657715
I0307 14:44:13.970265 140185115293440 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.9126452207565308, loss=3.2045416831970215
I0307 14:44:53.083337 140185199154944 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.6332650184631348, loss=3.079219102859497
I0307 14:45:32.199183 140185115293440 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.8326152563095093, loss=3.20473051071167
I0307 14:46:11.173660 140185199154944 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.716063380241394, loss=3.1207098960876465
I0307 14:46:39.364064 140341280416960 spec.py:321] Evaluating on the training split.
I0307 14:46:50.412212 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 14:47:25.815726 140341280416960 spec.py:349] Evaluating on the test split.
I0307 14:47:27.614401 140341280416960 submission_runner.py:469] Time since start: 20197.07s, 	Step: 47673, 	{'train/accuracy': 0.7172552347183228, 'train/loss': 1.310454249382019, 'validation/accuracy': 0.6531400084495544, 'validation/loss': 1.6099224090576172, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.278560161590576, 'test/num_examples': 10000, 'score': 18418.190346956253, 'total_duration': 20197.068747520447, 'accumulated_submission_time': 18418.190346956253, 'accumulated_eval_time': 1770.2790768146515, 'accumulated_logging_time': 3.405736207962036}
I0307 14:47:27.661132 140185115293440 logging_writer.py:48] [47673] accumulated_eval_time=1770.28, accumulated_logging_time=3.40574, accumulated_submission_time=18418.2, global_step=47673, preemption_count=0, score=18418.2, test/accuracy=0.5234, test/loss=2.27856, test/num_examples=10000, total_duration=20197.1, train/accuracy=0.717255, train/loss=1.31045, validation/accuracy=0.65314, validation/loss=1.60992, validation/num_examples=50000
I0307 14:47:38.711495 140185199154944 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7095532417297363, loss=3.1666011810302734
I0307 14:48:17.232536 140185115293440 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.6508351564407349, loss=3.117168664932251
I0307 14:48:56.141100 140185199154944 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.8286094665527344, loss=3.1912484169006348
I0307 14:49:34.728749 140185115293440 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.678004264831543, loss=3.1409554481506348
I0307 14:50:13.144083 140185199154944 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.7899963855743408, loss=3.1421358585357666
I0307 14:50:51.652135 140185115293440 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.0366787910461426, loss=3.089674472808838
I0307 14:51:30.185128 140185199154944 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.742149829864502, loss=3.157414674758911
I0307 14:52:08.731134 140185115293440 logging_writer.py:48] [48400] global_step=48400, grad_norm=2.333043336868286, loss=3.1785495281219482
I0307 14:52:48.090378 140185199154944 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.6916316747665405, loss=3.1945436000823975
I0307 14:53:26.955282 140185115293440 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.82280695438385, loss=3.14294695854187
I0307 14:54:06.797825 140185199154944 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.6928335428237915, loss=3.1203503608703613
I0307 14:54:46.055942 140185115293440 logging_writer.py:48] [48800] global_step=48800, grad_norm=2.31178879737854, loss=3.232938289642334
I0307 14:55:24.891657 140185199154944 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.679653525352478, loss=3.123260021209717
I0307 14:55:58.015027 140341280416960 spec.py:321] Evaluating on the training split.
I0307 14:56:09.326023 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 14:56:54.900499 140341280416960 spec.py:349] Evaluating on the test split.
I0307 14:56:56.685485 140341280416960 submission_runner.py:469] Time since start: 20766.14s, 	Step: 48987, 	{'train/accuracy': 0.7267019748687744, 'train/loss': 1.3229351043701172, 'validation/accuracy': 0.6597999930381775, 'validation/loss': 1.6181124448776245, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.287149429321289, 'test/num_examples': 10000, 'score': 18928.36222743988, 'total_duration': 20766.139822006226, 'accumulated_submission_time': 18928.36222743988, 'accumulated_eval_time': 1828.9493579864502, 'accumulated_logging_time': 3.4868218898773193}
I0307 14:56:56.739584 140185115293440 logging_writer.py:48] [48987] accumulated_eval_time=1828.95, accumulated_logging_time=3.48682, accumulated_submission_time=18928.4, global_step=48987, preemption_count=0, score=18928.4, test/accuracy=0.525, test/loss=2.28715, test/num_examples=10000, total_duration=20766.1, train/accuracy=0.726702, train/loss=1.32294, validation/accuracy=0.6598, validation/loss=1.61811, validation/num_examples=50000
I0307 14:57:02.364521 140185199154944 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.7970041036605835, loss=3.142035961151123
I0307 14:57:40.618304 140185115293440 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.6776655912399292, loss=3.143306016921997
I0307 14:58:19.139280 140185199154944 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.861141324043274, loss=3.22859525680542
I0307 14:58:57.448926 140185115293440 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.9554343223571777, loss=3.2221486568450928
I0307 14:59:36.003221 140185199154944 logging_writer.py:48] [49400] global_step=49400, grad_norm=2.020683765411377, loss=3.1144142150878906
I0307 15:00:14.582952 140185115293440 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8573637008666992, loss=3.1959102153778076
I0307 15:00:53.527909 140185199154944 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.8063368797302246, loss=3.1682162284851074
I0307 15:01:32.016147 140185115293440 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.737823724746704, loss=3.211179733276367
I0307 15:02:11.484890 140185199154944 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8670907020568848, loss=3.1711647510528564
I0307 15:02:51.294006 140185115293440 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.7640968561172485, loss=3.1112775802612305
I0307 15:03:29.454855 140185199154944 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7017890214920044, loss=3.1112494468688965
I0307 15:04:08.499954 140185115293440 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8131479024887085, loss=3.194037914276123
I0307 15:04:46.835568 140185199154944 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.7244330644607544, loss=3.201145648956299
I0307 15:05:25.173260 140185115293440 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7779688835144043, loss=3.269277334213257
I0307 15:05:26.791344 140341280416960 spec.py:321] Evaluating on the training split.
I0307 15:05:38.180871 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 15:06:18.754131 140341280416960 spec.py:349] Evaluating on the test split.
I0307 15:06:20.559011 140341280416960 submission_runner.py:469] Time since start: 21330.01s, 	Step: 50305, 	{'train/accuracy': 0.7238121628761292, 'train/loss': 1.357945203781128, 'validation/accuracy': 0.6561599969863892, 'validation/loss': 1.6645610332489014, 'validation/num_examples': 50000, 'test/accuracy': 0.5194000005722046, 'test/loss': 2.347830295562744, 'test/num_examples': 10000, 'score': 19438.232702970505, 'total_duration': 21330.013384103775, 'accumulated_submission_time': 19438.232702970505, 'accumulated_eval_time': 1882.7168672084808, 'accumulated_logging_time': 3.579538345336914}
I0307 15:06:20.627953 140185199154944 logging_writer.py:48] [50305] accumulated_eval_time=1882.72, accumulated_logging_time=3.57954, accumulated_submission_time=19438.2, global_step=50305, preemption_count=0, score=19438.2, test/accuracy=0.5194, test/loss=2.34783, test/num_examples=10000, total_duration=21330, train/accuracy=0.723812, train/loss=1.35795, validation/accuracy=0.65616, validation/loss=1.66456, validation/num_examples=50000
I0307 15:06:57.871783 140185115293440 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.8650346994400024, loss=3.1533658504486084
I0307 15:07:36.325644 140185199154944 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.8414771556854248, loss=3.142277956008911
I0307 15:08:14.862421 140185115293440 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.8124008178710938, loss=3.0479278564453125
I0307 15:08:53.437537 140185199154944 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.606141209602356, loss=3.125251293182373
I0307 15:09:32.181847 140185115293440 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.810781717300415, loss=3.147319793701172
I0307 15:10:12.369206 140185199154944 logging_writer.py:48] [50900] global_step=50900, grad_norm=2.110372304916382, loss=3.259507894515991
I0307 15:10:51.221352 140185115293440 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.7675927877426147, loss=3.100989818572998
I0307 15:11:31.911637 140185199154944 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.8630881309509277, loss=3.1136178970336914
I0307 15:12:10.984892 140185115293440 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.8650181293487549, loss=3.1099047660827637
I0307 15:12:49.786214 140185199154944 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9238860607147217, loss=3.2212774753570557
I0307 15:13:28.538914 140185115293440 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7057170867919922, loss=3.154660224914551
I0307 15:14:07.276799 140185199154944 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.677049994468689, loss=3.1350061893463135
I0307 15:14:45.824500 140185115293440 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.7253179550170898, loss=3.0819091796875
I0307 15:14:50.820521 140341280416960 spec.py:321] Evaluating on the training split.
I0307 15:15:01.885666 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 15:15:43.021674 140341280416960 spec.py:349] Evaluating on the test split.
I0307 15:15:44.820576 140341280416960 submission_runner.py:469] Time since start: 21894.27s, 	Step: 51614, 	{'train/accuracy': 0.7358298897743225, 'train/loss': 1.290172815322876, 'validation/accuracy': 0.667140007019043, 'validation/loss': 1.5935722589492798, 'validation/num_examples': 50000, 'test/accuracy': 0.5333999991416931, 'test/loss': 2.267338752746582, 'test/num_examples': 10000, 'score': 19948.236372470856, 'total_duration': 21894.27494931221, 'accumulated_submission_time': 19948.236372470856, 'accumulated_eval_time': 1936.7167658805847, 'accumulated_logging_time': 3.6928820610046387}
I0307 15:15:44.857582 140185199154944 logging_writer.py:48] [51614] accumulated_eval_time=1936.72, accumulated_logging_time=3.69288, accumulated_submission_time=19948.2, global_step=51614, preemption_count=0, score=19948.2, test/accuracy=0.5334, test/loss=2.26734, test/num_examples=10000, total_duration=21894.3, train/accuracy=0.73583, train/loss=1.29017, validation/accuracy=0.66714, validation/loss=1.59357, validation/num_examples=50000
I0307 15:16:18.342226 140185115293440 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.104698419570923, loss=3.165127992630005
I0307 15:16:56.582643 140185199154944 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.7468311786651611, loss=3.2474451065063477
I0307 15:17:35.204345 140185115293440 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.807194471359253, loss=3.160365343093872
I0307 15:18:14.017493 140185199154944 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.0941104888916016, loss=3.1711783409118652
I0307 15:18:52.826381 140185115293440 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.7697480916976929, loss=3.0870821475982666
I0307 15:19:31.551103 140185199154944 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.6383509635925293, loss=3.084404706954956
I0307 15:20:11.188544 140185115293440 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.9082286357879639, loss=3.1486682891845703
I0307 15:20:50.218919 140185199154944 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.6717699766159058, loss=3.056098461151123
I0307 15:21:28.618480 140185115293440 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.7873666286468506, loss=3.0717244148254395
I0307 15:22:07.620489 140185199154944 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.8022050857543945, loss=3.204935312271118
I0307 15:22:46.475699 140185115293440 logging_writer.py:48] [52700] global_step=52700, grad_norm=2.0525004863739014, loss=3.142813205718994
I0307 15:23:25.522222 140185199154944 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.6890653371810913, loss=3.1439802646636963
I0307 15:24:04.351987 140185115293440 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.8554013967514038, loss=3.166881799697876
I0307 15:24:15.156803 140341280416960 spec.py:321] Evaluating on the training split.
I0307 15:24:26.722071 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 15:25:03.028337 140341280416960 spec.py:349] Evaluating on the test split.
I0307 15:25:04.833837 140341280416960 submission_runner.py:469] Time since start: 22454.29s, 	Step: 52929, 	{'train/accuracy': 0.734394907951355, 'train/loss': 1.2459553480148315, 'validation/accuracy': 0.6674599647521973, 'validation/loss': 1.5557643175125122, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2205395698547363, 'test/num_examples': 10000, 'score': 20458.37513923645, 'total_duration': 22454.288200378418, 'accumulated_submission_time': 20458.37513923645, 'accumulated_eval_time': 1986.3936340808868, 'accumulated_logging_time': 3.7527828216552734}
I0307 15:25:04.932768 140185199154944 logging_writer.py:48] [52929] accumulated_eval_time=1986.39, accumulated_logging_time=3.75278, accumulated_submission_time=20458.4, global_step=52929, preemption_count=0, score=20458.4, test/accuracy=0.5386, test/loss=2.22054, test/num_examples=10000, total_duration=22454.3, train/accuracy=0.734395, train/loss=1.24596, validation/accuracy=0.66746, validation/loss=1.55576, validation/num_examples=50000
I0307 15:25:32.732769 140185115293440 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8102612495422363, loss=3.1745667457580566
I0307 15:26:11.206411 140185199154944 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.9048866033554077, loss=3.121028184890747
I0307 15:26:50.198322 140185115293440 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.014859199523926, loss=3.1446783542633057
I0307 15:27:28.896682 140185199154944 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.812433123588562, loss=3.1653892993927
I0307 15:28:07.487700 140185115293440 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8377339839935303, loss=3.1192870140075684
I0307 15:28:46.140012 140185199154944 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.93980073928833, loss=3.1109275817871094
I0307 15:29:25.759830 140185115293440 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.764704942703247, loss=3.128807544708252
I0307 15:30:04.994491 140185199154944 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.8422847986221313, loss=3.196822166442871
I0307 15:30:43.875257 140185115293440 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.857878565788269, loss=3.222785234451294
I0307 15:31:22.841395 140185199154944 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.9129974842071533, loss=3.175915002822876
I0307 15:32:01.720803 140185115293440 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.884436845779419, loss=3.1992592811584473
I0307 15:32:40.748728 140185199154944 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.7979472875595093, loss=3.2164483070373535
I0307 15:33:19.536743 140185115293440 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8451164960861206, loss=3.169674873352051
I0307 15:33:34.976908 140341280416960 spec.py:321] Evaluating on the training split.
I0307 15:33:46.874628 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 15:34:27.953093 140341280416960 spec.py:349] Evaluating on the test split.
I0307 15:34:29.733585 140341280416960 submission_runner.py:469] Time since start: 23019.19s, 	Step: 54241, 	{'train/accuracy': 0.7156807780265808, 'train/loss': 1.3730186223983765, 'validation/accuracy': 0.6506999731063843, 'validation/loss': 1.677892804145813, 'validation/num_examples': 50000, 'test/accuracy': 0.5204000473022461, 'test/loss': 2.332780361175537, 'test/num_examples': 10000, 'score': 20968.234202861786, 'total_duration': 23019.18790245056, 'accumulated_submission_time': 20968.234202861786, 'accumulated_eval_time': 2041.1501019001007, 'accumulated_logging_time': 3.890122175216675}
I0307 15:34:29.812834 140185199154944 logging_writer.py:48] [54241] accumulated_eval_time=2041.15, accumulated_logging_time=3.89012, accumulated_submission_time=20968.2, global_step=54241, preemption_count=0, score=20968.2, test/accuracy=0.5204, test/loss=2.33278, test/num_examples=10000, total_duration=23019.2, train/accuracy=0.715681, train/loss=1.37302, validation/accuracy=0.6507, validation/loss=1.67789, validation/num_examples=50000
I0307 15:34:52.947827 140185115293440 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.9362159967422485, loss=3.190491199493408
I0307 15:35:31.484424 140185199154944 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.788122296333313, loss=3.116652488708496
I0307 15:36:10.556630 140185115293440 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.8007307052612305, loss=3.165652275085449
I0307 15:36:49.484580 140185199154944 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.9599655866622925, loss=3.2244975566864014
I0307 15:37:27.974738 140185115293440 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.9022798538208008, loss=3.185073137283325
I0307 15:38:08.328240 140185199154944 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.7314708232879639, loss=3.098928451538086
I0307 15:38:48.775966 140185115293440 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.062635898590088, loss=3.142911195755005
I0307 15:39:29.123605 140185199154944 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.7916865348815918, loss=3.149925470352173
2025-03-07 15:39:38.342606: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:40:08.739802 140185115293440 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.7960052490234375, loss=3.1417887210845947
I0307 15:40:47.866562 140185199154944 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.9434269666671753, loss=3.1729252338409424
I0307 15:41:26.252833 140185115293440 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.7063004970550537, loss=3.1005349159240723
I0307 15:42:05.787376 140185199154944 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.986987829208374, loss=3.1075706481933594
I0307 15:42:45.172336 140185115293440 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.7535074949264526, loss=3.0323190689086914
I0307 15:42:59.970457 140341280416960 spec.py:321] Evaluating on the training split.
I0307 15:43:11.571385 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 15:43:38.587907 140341280416960 spec.py:349] Evaluating on the test split.
I0307 15:43:40.375862 140341280416960 submission_runner.py:469] Time since start: 23569.83s, 	Step: 55538, 	{'train/accuracy': 0.7281369566917419, 'train/loss': 1.3163373470306396, 'validation/accuracy': 0.6577000021934509, 'validation/loss': 1.632766842842102, 'validation/num_examples': 50000, 'test/accuracy': 0.5351999998092651, 'test/loss': 2.261859655380249, 'test/num_examples': 10000, 'score': 21478.206674575806, 'total_duration': 23569.830231666565, 'accumulated_submission_time': 21478.206674575806, 'accumulated_eval_time': 2081.555350780487, 'accumulated_logging_time': 4.000834703445435}
I0307 15:43:40.535205 140185199154944 logging_writer.py:48] [55538] accumulated_eval_time=2081.56, accumulated_logging_time=4.00083, accumulated_submission_time=21478.2, global_step=55538, preemption_count=0, score=21478.2, test/accuracy=0.5352, test/loss=2.26186, test/num_examples=10000, total_duration=23569.8, train/accuracy=0.728137, train/loss=1.31634, validation/accuracy=0.6577, validation/loss=1.63277, validation/num_examples=50000
I0307 15:44:05.315299 140185115293440 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.8796868324279785, loss=3.1353743076324463
I0307 15:44:44.357876 140185199154944 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.8179713487625122, loss=3.1315646171569824
I0307 15:45:23.719264 140185115293440 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.050654411315918, loss=3.155744791030884
I0307 15:46:03.294991 140185199154944 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.7111263275146484, loss=3.0946385860443115
I0307 15:46:42.570808 140185115293440 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.920462727546692, loss=3.0911026000976562
I0307 15:47:25.014031 140185199154944 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.9306453466415405, loss=3.17219614982605
I0307 15:48:05.122211 140185115293440 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8534842729568481, loss=3.0711421966552734
I0307 15:48:45.193811 140185199154944 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.7949295043945312, loss=3.1044304370880127
I0307 15:49:24.372208 140185115293440 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.9022802114486694, loss=3.0947022438049316
I0307 15:50:04.250664 140185199154944 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.9226206541061401, loss=3.143463373184204
I0307 15:50:44.126005 140185115293440 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.7294917106628418, loss=3.1310079097747803
I0307 15:51:23.432446 140185199154944 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.9387465715408325, loss=3.1453399658203125
I0307 15:52:02.759427 140185115293440 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.878296136856079, loss=3.1469597816467285
I0307 15:52:10.709922 140341280416960 spec.py:321] Evaluating on the training split.
I0307 15:52:21.945362 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 15:53:00.755047 140341280416960 spec.py:349] Evaluating on the test split.
I0307 15:53:02.535094 140341280416960 submission_runner.py:469] Time since start: 24131.99s, 	Step: 56821, 	{'train/accuracy': 0.7429647445678711, 'train/loss': 1.2034153938293457, 'validation/accuracy': 0.6712599992752075, 'validation/loss': 1.5273125171661377, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.1589515209198, 'test/num_examples': 10000, 'score': 21988.14218521118, 'total_duration': 24131.989455223083, 'accumulated_submission_time': 21988.14218521118, 'accumulated_eval_time': 2133.380360364914, 'accumulated_logging_time': 4.242295026779175}
I0307 15:53:02.604900 140185199154944 logging_writer.py:48] [56821] accumulated_eval_time=2133.38, accumulated_logging_time=4.2423, accumulated_submission_time=21988.1, global_step=56821, preemption_count=0, score=21988.1, test/accuracy=0.5452, test/loss=2.15895, test/num_examples=10000, total_duration=24132, train/accuracy=0.742965, train/loss=1.20342, validation/accuracy=0.67126, validation/loss=1.52731, validation/num_examples=50000
I0307 15:53:33.837128 140185115293440 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.9461073875427246, loss=3.125854969024658
I0307 15:54:13.571565 140185199154944 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.0360636711120605, loss=3.1108200550079346
I0307 15:54:53.629577 140185115293440 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8685303926467896, loss=3.170941114425659
I0307 15:55:32.876046 140185199154944 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.8451664447784424, loss=3.1158297061920166
I0307 15:56:12.490153 140185115293440 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.1622164249420166, loss=3.1792683601379395
I0307 15:56:51.637590 140185199154944 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.7617453336715698, loss=3.171450614929199
I0307 15:57:31.405983 140185115293440 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8881808519363403, loss=3.067601203918457
I0307 15:58:10.688069 140185199154944 logging_writer.py:48] [57600] global_step=57600, grad_norm=2.004271984100342, loss=3.1216509342193604
I0307 15:58:50.135850 140185115293440 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.8943612575531006, loss=3.1657698154449463
I0307 15:59:30.049537 140185199154944 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.9939422607421875, loss=3.1395833492279053
I0307 16:00:10.364597 140185115293440 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8909693956375122, loss=3.0922274589538574
I0307 16:00:50.437928 140185199154944 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.0308632850646973, loss=3.212498426437378
I0307 16:01:30.080458 140185115293440 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.8758291006088257, loss=3.1921260356903076
I0307 16:01:32.811708 140341280416960 spec.py:321] Evaluating on the training split.
I0307 16:01:43.883206 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 16:02:24.421850 140341280416960 spec.py:349] Evaluating on the test split.
I0307 16:02:26.216228 140341280416960 submission_runner.py:469] Time since start: 24695.67s, 	Step: 58108, 	{'train/accuracy': 0.7422871589660645, 'train/loss': 1.2285799980163574, 'validation/accuracy': 0.6701200008392334, 'validation/loss': 1.5347797870635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5401000380516052, 'test/loss': 2.2144806385040283, 'test/num_examples': 10000, 'score': 22498.170780658722, 'total_duration': 24695.670558214188, 'accumulated_submission_time': 22498.170780658722, 'accumulated_eval_time': 2186.784686565399, 'accumulated_logging_time': 4.331315994262695}
I0307 16:02:26.302211 140185199154944 logging_writer.py:48] [58108] accumulated_eval_time=2186.78, accumulated_logging_time=4.33132, accumulated_submission_time=22498.2, global_step=58108, preemption_count=0, score=22498.2, test/accuracy=0.5401, test/loss=2.21448, test/num_examples=10000, total_duration=24695.7, train/accuracy=0.742287, train/loss=1.22858, validation/accuracy=0.67012, validation/loss=1.53478, validation/num_examples=50000
I0307 16:03:03.011087 140185115293440 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.0221052169799805, loss=3.171121597290039
I0307 16:03:42.678226 140185199154944 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.0208585262298584, loss=3.1272478103637695
I0307 16:04:23.035962 140185115293440 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9774267673492432, loss=3.1012160778045654
I0307 16:05:01.800700 140185199154944 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.9664281606674194, loss=3.04417085647583
I0307 16:05:42.064868 140185115293440 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.9478893280029297, loss=3.122252941131592
I0307 16:06:21.829774 140185199154944 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.8038146495819092, loss=3.104004144668579
I0307 16:07:01.670190 140185115293440 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.89012610912323, loss=3.1387219429016113
I0307 16:07:40.793947 140185199154944 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8452610969543457, loss=3.1279337406158447
I0307 16:08:20.314368 140185115293440 logging_writer.py:48] [59000] global_step=59000, grad_norm=2.0359020233154297, loss=3.093494415283203
I0307 16:08:59.755796 140185199154944 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.8871126174926758, loss=3.1852617263793945
I0307 16:09:39.726286 140185115293440 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.1375648975372314, loss=3.1841650009155273
I0307 16:10:19.698381 140185199154944 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.7908830642700195, loss=3.0591683387756348
I0307 16:10:56.292735 140341280416960 spec.py:321] Evaluating on the training split.
I0307 16:11:07.542033 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 16:11:40.032530 140341280416960 spec.py:349] Evaluating on the test split.
I0307 16:11:41.826597 140341280416960 submission_runner.py:469] Time since start: 25251.28s, 	Step: 59393, 	{'train/accuracy': 0.7423867583274841, 'train/loss': 1.2358310222625732, 'validation/accuracy': 0.6685799956321716, 'validation/loss': 1.5634585618972778, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.2128000259399414, 'test/num_examples': 10000, 'score': 23007.977536439896, 'total_duration': 25251.28094100952, 'accumulated_submission_time': 23007.977536439896, 'accumulated_eval_time': 2232.3183736801147, 'accumulated_logging_time': 4.4424896240234375}
I0307 16:11:41.939048 140185115293440 logging_writer.py:48] [59393] accumulated_eval_time=2232.32, accumulated_logging_time=4.44249, accumulated_submission_time=23008, global_step=59393, preemption_count=0, score=23008, test/accuracy=0.5407, test/loss=2.2128, test/num_examples=10000, total_duration=25251.3, train/accuracy=0.742387, train/loss=1.23583, validation/accuracy=0.66858, validation/loss=1.56346, validation/num_examples=50000
I0307 16:11:45.208715 140185199154944 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.8830385208129883, loss=3.1264538764953613
I0307 16:12:24.939536 140185115293440 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.7785207033157349, loss=3.1523375511169434
I0307 16:13:04.666563 140185199154944 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.8888245820999146, loss=3.2131009101867676
I0307 16:13:44.404211 140185115293440 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9567437171936035, loss=3.144235134124756
I0307 16:14:23.966354 140185199154944 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.0017433166503906, loss=3.126753330230713
I0307 16:15:04.519783 140185115293440 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.0329627990722656, loss=3.1751768589019775
I0307 16:15:44.412722 140185199154944 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.9522804021835327, loss=3.137232780456543
I0307 16:16:24.484580 140185115293440 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.093668222427368, loss=3.10949444770813
I0307 16:17:04.203525 140185199154944 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.9373292922973633, loss=3.0907769203186035
I0307 16:17:44.131970 140185115293440 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.9498112201690674, loss=3.134993553161621
I0307 16:18:24.047630 140185199154944 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9520840644836426, loss=3.0927295684814453
I0307 16:19:03.836646 140185115293440 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.1182243824005127, loss=3.159046173095703
I0307 16:19:43.726544 140185199154944 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.0329384803771973, loss=3.0856263637542725
I0307 16:20:11.896793 140341280416960 spec.py:321] Evaluating on the training split.
I0307 16:20:22.944370 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 16:20:55.030716 140341280416960 spec.py:349] Evaluating on the test split.
I0307 16:20:56.796565 140341280416960 submission_runner.py:469] Time since start: 25806.25s, 	Step: 60671, 	{'train/accuracy': 0.7311065196990967, 'train/loss': 1.2819805145263672, 'validation/accuracy': 0.6538800001144409, 'validation/loss': 1.6223808526992798, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.279731035232544, 'test/num_examples': 10000, 'score': 23517.7104511261, 'total_duration': 25806.25092124939, 'accumulated_submission_time': 23517.7104511261, 'accumulated_eval_time': 2277.2179844379425, 'accumulated_logging_time': 4.621394872665405}
I0307 16:20:56.912105 140185115293440 logging_writer.py:48] [60671] accumulated_eval_time=2277.22, accumulated_logging_time=4.62139, accumulated_submission_time=23517.7, global_step=60671, preemption_count=0, score=23517.7, test/accuracy=0.5269, test/loss=2.27973, test/num_examples=10000, total_duration=25806.3, train/accuracy=0.731107, train/loss=1.28198, validation/accuracy=0.65388, validation/loss=1.62238, validation/num_examples=50000
I0307 16:21:08.794199 140185199154944 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.7991760969161987, loss=3.0494797229766846
I0307 16:21:48.234106 140185115293440 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.063438892364502, loss=3.172781467437744
I0307 16:22:27.412625 140185199154944 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9397867918014526, loss=3.1126210689544678
I0307 16:23:06.102949 140185115293440 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.786594033241272, loss=3.0752766132354736
I0307 16:23:46.288552 140185199154944 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.9198545217514038, loss=3.1238365173339844
I0307 16:24:26.032871 140185115293440 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.284557580947876, loss=3.16457462310791
I0307 16:25:06.146400 140185199154944 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.0778884887695312, loss=3.094770908355713
I0307 16:25:45.659044 140185115293440 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.2016243934631348, loss=3.1389002799987793
I0307 16:26:25.470613 140185199154944 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8441829681396484, loss=3.066474676132202
I0307 16:27:04.767892 140185115293440 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.9868561029434204, loss=3.13220477104187
I0307 16:27:44.573649 140185199154944 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9891307353973389, loss=3.090579032897949
I0307 16:28:24.142626 140185115293440 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.0800375938415527, loss=3.1553683280944824
I0307 16:29:03.927695 140185199154944 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.8595479726791382, loss=3.1099793910980225
I0307 16:29:27.176296 140341280416960 spec.py:321] Evaluating on the training split.
I0307 16:29:38.512195 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 16:30:18.565408 140341280416960 spec.py:349] Evaluating on the test split.
I0307 16:30:20.363349 140341280416960 submission_runner.py:469] Time since start: 26369.82s, 	Step: 61960, 	{'train/accuracy': 0.7463727593421936, 'train/loss': 1.210816740989685, 'validation/accuracy': 0.6752200126647949, 'validation/loss': 1.5320411920547485, 'validation/num_examples': 50000, 'test/accuracy': 0.5464000105857849, 'test/loss': 2.1709399223327637, 'test/num_examples': 10000, 'score': 24027.76775407791, 'total_duration': 26369.81768345833, 'accumulated_submission_time': 24027.76775407791, 'accumulated_eval_time': 2330.4048466682434, 'accumulated_logging_time': 4.786064386367798}
I0307 16:30:20.446915 140185115293440 logging_writer.py:48] [61960] accumulated_eval_time=2330.4, accumulated_logging_time=4.78606, accumulated_submission_time=24027.8, global_step=61960, preemption_count=0, score=24027.8, test/accuracy=0.5464, test/loss=2.17094, test/num_examples=10000, total_duration=26369.8, train/accuracy=0.746373, train/loss=1.21082, validation/accuracy=0.67522, validation/loss=1.53204, validation/num_examples=50000
I0307 16:30:37.054405 140185199154944 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.0267984867095947, loss=3.157376527786255
I0307 16:31:16.567954 140185115293440 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.9832847118377686, loss=3.04738712310791
I0307 16:31:56.267755 140185199154944 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.0307555198669434, loss=3.102139472961426
I0307 16:32:36.505785 140185115293440 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.9201756715774536, loss=3.0904760360717773
I0307 16:33:15.856767 140185199154944 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.8906322717666626, loss=3.088850975036621
I0307 16:33:55.626721 140185115293440 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.1131343841552734, loss=3.0912585258483887
I0307 16:34:35.324102 140185199154944 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.1037967205047607, loss=3.158193588256836
I0307 16:35:14.874280 140185115293440 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.016242265701294, loss=3.1148757934570312
I0307 16:35:54.476438 140185199154944 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.9785255193710327, loss=3.107097625732422
I0307 16:36:34.348913 140185115293440 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.9288185834884644, loss=3.102597713470459
I0307 16:37:14.323505 140185199154944 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.0671470165252686, loss=3.134589672088623
I0307 16:37:54.519819 140185115293440 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.0630767345428467, loss=3.060983657836914
I0307 16:38:34.868138 140185199154944 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.9801619052886963, loss=3.0123751163482666
I0307 16:38:50.613154 140341280416960 spec.py:321] Evaluating on the training split.
I0307 16:39:01.365254 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 16:39:50.842665 140341280416960 spec.py:349] Evaluating on the test split.
I0307 16:39:52.632609 140341280416960 submission_runner.py:469] Time since start: 26942.09s, 	Step: 63241, 	{'train/accuracy': 0.7547233700752258, 'train/loss': 1.2259612083435059, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5537362098693848, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.22103214263916, 'test/num_examples': 10000, 'score': 24537.714914798737, 'total_duration': 26942.08695077896, 'accumulated_submission_time': 24537.714914798737, 'accumulated_eval_time': 2392.4241247177124, 'accumulated_logging_time': 4.930222272872925}
I0307 16:39:52.747282 140185115293440 logging_writer.py:48] [63241] accumulated_eval_time=2392.42, accumulated_logging_time=4.93022, accumulated_submission_time=24537.7, global_step=63241, preemption_count=0, score=24537.7, test/accuracy=0.5491, test/loss=2.22103, test/num_examples=10000, total_duration=26942.1, train/accuracy=0.754723, train/loss=1.22596, validation/accuracy=0.68016, validation/loss=1.55374, validation/num_examples=50000
I0307 16:40:16.674202 140185199154944 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.0978357791900635, loss=3.068758249282837
I0307 16:40:56.360233 140185115293440 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.8511346578598022, loss=3.0811264514923096
I0307 16:41:36.467721 140185199154944 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.9314240217208862, loss=3.131389856338501
I0307 16:42:16.846196 140185115293440 logging_writer.py:48] [63600] global_step=63600, grad_norm=2.1944057941436768, loss=3.1167426109313965
I0307 16:42:56.293684 140185199154944 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.030970811843872, loss=3.1487646102905273
I0307 16:43:35.947019 140185115293440 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.9330289363861084, loss=3.0729684829711914
I0307 16:44:15.943026 140185199154944 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.0635783672332764, loss=3.020796060562134
I0307 16:44:56.061691 140185115293440 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.9017219543457031, loss=3.146324396133423
I0307 16:45:35.946249 140185199154944 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.0534722805023193, loss=3.0761985778808594
I0307 16:46:16.071375 140185115293440 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.332106113433838, loss=3.158670425415039
I0307 16:46:56.580200 140185199154944 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9712259769439697, loss=3.078362464904785
I0307 16:47:37.039162 140185115293440 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.0854151248931885, loss=3.082327365875244
I0307 16:48:17.159867 140185199154944 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.8742326498031616, loss=3.015756607055664
I0307 16:48:22.691936 140341280416960 spec.py:321] Evaluating on the training split.
I0307 16:48:33.725477 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 16:49:18.562539 140341280416960 spec.py:349] Evaluating on the test split.
I0307 16:49:20.356523 140341280416960 submission_runner.py:469] Time since start: 27509.81s, 	Step: 64515, 	{'train/accuracy': 0.7471898794174194, 'train/loss': 1.169657588005066, 'validation/accuracy': 0.6731799840927124, 'validation/loss': 1.5018417835235596, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.1772921085357666, 'test/num_examples': 10000, 'score': 25047.46474480629, 'total_duration': 27509.810849905014, 'accumulated_submission_time': 25047.46474480629, 'accumulated_eval_time': 2450.088513612747, 'accumulated_logging_time': 5.0803258419036865}
I0307 16:49:20.429675 140185115293440 logging_writer.py:48] [64515] accumulated_eval_time=2450.09, accumulated_logging_time=5.08033, accumulated_submission_time=25047.5, global_step=64515, preemption_count=0, score=25047.5, test/accuracy=0.5433, test/loss=2.17729, test/num_examples=10000, total_duration=27509.8, train/accuracy=0.74719, train/loss=1.16966, validation/accuracy=0.67318, validation/loss=1.50184, validation/num_examples=50000
I0307 16:49:54.884538 140185199154944 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.982000470161438, loss=3.147754192352295
I0307 16:50:34.836513 140185115293440 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9402605295181274, loss=3.1704065799713135
I0307 16:51:15.177438 140185199154944 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.9947364330291748, loss=3.1390724182128906
I0307 16:51:56.007084 140185115293440 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.078580617904663, loss=3.1454570293426514
I0307 16:52:35.128478 140185199154944 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.921958327293396, loss=3.1004819869995117
2025-03-07 16:52:51.420196: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:53:15.006681 140185115293440 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.234186887741089, loss=3.1554102897644043
I0307 16:53:55.326171 140185199154944 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.920344352722168, loss=3.1268367767333984
I0307 16:54:35.516496 140185115293440 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9952843189239502, loss=3.193572998046875
I0307 16:55:15.461562 140185199154944 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.11480712890625, loss=3.1018331050872803
I0307 16:55:54.937817 140185115293440 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.086252450942993, loss=3.1705739498138428
I0307 16:56:34.453037 140185199154944 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.244694232940674, loss=3.077579975128174
I0307 16:57:14.386070 140185115293440 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.1937522888183594, loss=3.155463695526123
I0307 16:57:50.390383 140341280416960 spec.py:321] Evaluating on the training split.
I0307 16:58:01.479249 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 16:58:41.352061 140341280416960 spec.py:349] Evaluating on the test split.
I0307 16:58:43.113887 140341280416960 submission_runner.py:469] Time since start: 28072.57s, 	Step: 65791, 	{'train/accuracy': 0.7495216727256775, 'train/loss': 1.172637939453125, 'validation/accuracy': 0.6743599772453308, 'validation/loss': 1.5099639892578125, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.1694185733795166, 'test/num_examples': 10000, 'score': 25557.217126131058, 'total_duration': 28072.56822991371, 'accumulated_submission_time': 25557.217126131058, 'accumulated_eval_time': 2502.8118398189545, 'accumulated_logging_time': 5.20369029045105}
I0307 16:58:43.220046 140185199154944 logging_writer.py:48] [65791] accumulated_eval_time=2502.81, accumulated_logging_time=5.20369, accumulated_submission_time=25557.2, global_step=65791, preemption_count=0, score=25557.2, test/accuracy=0.5504, test/loss=2.16942, test/num_examples=10000, total_duration=28072.6, train/accuracy=0.749522, train/loss=1.17264, validation/accuracy=0.67436, validation/loss=1.50996, validation/num_examples=50000
I0307 16:58:56.258244 140185115293440 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.102992057800293, loss=3.142634391784668
I0307 16:59:35.752510 140185199154944 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.020419120788574, loss=3.112633228302002
I0307 17:00:15.682563 140185115293440 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.0070207118988037, loss=3.068164348602295
I0307 17:00:55.500271 140185199154944 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9966260194778442, loss=3.0176689624786377
I0307 17:01:35.714984 140185115293440 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.9101179838180542, loss=3.1382033824920654
I0307 17:02:15.865893 140185199154944 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.9922672510147095, loss=3.02455997467041
I0307 17:02:55.465012 140185115293440 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9500300884246826, loss=3.1744842529296875
I0307 17:03:35.550201 140185199154944 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.9141162633895874, loss=3.080918788909912
I0307 17:04:15.631822 140185115293440 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.1045351028442383, loss=3.0779619216918945
I0307 17:04:55.185688 140185199154944 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.8563647270202637, loss=2.996356964111328
I0307 17:05:34.867685 140185115293440 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.32194447517395, loss=3.136965751647949
I0307 17:06:15.403476 140185199154944 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.02687931060791, loss=3.118105411529541
I0307 17:06:55.264256 140185115293440 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.0476202964782715, loss=3.1396679878234863
I0307 17:07:13.289264 140341280416960 spec.py:321] Evaluating on the training split.
I0307 17:07:24.602595 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 17:07:58.448344 140341280416960 spec.py:349] Evaluating on the test split.
I0307 17:08:00.256109 140341280416960 submission_runner.py:469] Time since start: 28629.71s, 	Step: 67046, 	{'train/accuracy': 0.7508171200752258, 'train/loss': 1.1620087623596191, 'validation/accuracy': 0.677079975605011, 'validation/loss': 1.4924728870391846, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.165454864501953, 'test/num_examples': 10000, 'score': 26067.101620435715, 'total_duration': 28629.710269212723, 'accumulated_submission_time': 26067.101620435715, 'accumulated_eval_time': 2549.7783250808716, 'accumulated_logging_time': 5.338649034500122}
I0307 17:08:00.361968 140185199154944 logging_writer.py:48] [67046] accumulated_eval_time=2549.78, accumulated_logging_time=5.33865, accumulated_submission_time=26067.1, global_step=67046, preemption_count=0, score=26067.1, test/accuracy=0.5457, test/loss=2.16545, test/num_examples=10000, total_duration=28629.7, train/accuracy=0.750817, train/loss=1.16201, validation/accuracy=0.67708, validation/loss=1.49247, validation/num_examples=50000
I0307 17:08:22.068671 140185115293440 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.041843891143799, loss=3.0892796516418457
I0307 17:09:01.131829 140185199154944 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.184960126876831, loss=3.147509813308716
I0307 17:09:40.657072 140185115293440 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.051680088043213, loss=3.07735538482666
I0307 17:10:20.723986 140185199154944 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.0970518589019775, loss=3.0994174480438232
I0307 17:11:00.647474 140185115293440 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.050178289413452, loss=3.105318069458008
I0307 17:11:40.541529 140185199154944 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.1574952602386475, loss=3.139638900756836
I0307 17:12:20.107790 140185115293440 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.9788678884506226, loss=3.095034599304199
I0307 17:12:59.725283 140185199154944 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.1944849491119385, loss=3.121018171310425
I0307 17:13:39.398718 140185115293440 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.1759140491485596, loss=3.0463619232177734
I0307 17:14:18.948713 140185199154944 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.2048158645629883, loss=3.099714756011963
I0307 17:14:59.280514 140185115293440 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.001810312271118, loss=3.0523698329925537
I0307 17:15:39.462249 140185199154944 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.0757896900177, loss=3.1168136596679688
I0307 17:16:19.609966 140185115293440 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.9261997938156128, loss=3.003035068511963
I0307 17:16:30.359310 140341280416960 spec.py:321] Evaluating on the training split.
I0307 17:16:41.787675 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 17:17:20.414322 140341280416960 spec.py:349] Evaluating on the test split.
I0307 17:17:22.221827 140341280416960 submission_runner.py:469] Time since start: 29191.68s, 	Step: 68328, 	{'train/accuracy': 0.7555603981018066, 'train/loss': 1.1769049167633057, 'validation/accuracy': 0.6757000088691711, 'validation/loss': 1.5179646015167236, 'validation/num_examples': 50000, 'test/accuracy': 0.553600013256073, 'test/loss': 2.147552728652954, 'test/num_examples': 10000, 'score': 26576.909748077393, 'total_duration': 29191.676191568375, 'accumulated_submission_time': 26576.909748077393, 'accumulated_eval_time': 2601.6406919956207, 'accumulated_logging_time': 5.473933219909668}
I0307 17:17:22.321639 140185199154944 logging_writer.py:48] [68328] accumulated_eval_time=2601.64, accumulated_logging_time=5.47393, accumulated_submission_time=26576.9, global_step=68328, preemption_count=0, score=26576.9, test/accuracy=0.5536, test/loss=2.14755, test/num_examples=10000, total_duration=29191.7, train/accuracy=0.75556, train/loss=1.1769, validation/accuracy=0.6757, validation/loss=1.51796, validation/num_examples=50000
I0307 17:17:51.206966 140185115293440 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.0974526405334473, loss=3.056666612625122
I0307 17:18:30.931859 140185199154944 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.9614840745925903, loss=3.047729730606079
I0307 17:19:14.290446 140185115293440 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.1511926651000977, loss=3.0394701957702637
I0307 17:19:55.512929 140185199154944 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.0361368656158447, loss=3.1202237606048584
I0307 17:20:35.076483 140185115293440 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0890262126922607, loss=3.116501808166504
I0307 17:21:13.576995 140185199154944 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.066579818725586, loss=3.1290273666381836
I0307 17:21:53.063925 140185115293440 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.9674882888793945, loss=3.044102907180786
I0307 17:22:32.841562 140185199154944 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.9938485622406006, loss=3.0532233715057373
I0307 17:23:12.780724 140185115293440 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.1832408905029297, loss=3.0607008934020996
I0307 17:24:30.768523 140185199154944 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.125211000442505, loss=3.100210189819336
I0307 17:25:11.157665 140185115293440 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.2064950466156006, loss=3.143411874771118
I0307 17:25:51.052227 140185199154944 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.9833673238754272, loss=3.0074710845947266
I0307 17:25:52.296402 140341280416960 spec.py:321] Evaluating on the training split.
I0307 17:26:03.594864 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 17:26:36.669418 140341280416960 spec.py:349] Evaluating on the test split.
I0307 17:26:38.630496 140341280416960 submission_runner.py:469] Time since start: 29748.08s, 	Step: 69504, 	{'train/accuracy': 0.7591079473495483, 'train/loss': 1.165501356124878, 'validation/accuracy': 0.675059974193573, 'validation/loss': 1.5185964107513428, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.1634695529937744, 'test/num_examples': 10000, 'score': 27086.707017421722, 'total_duration': 29748.084569454193, 'accumulated_submission_time': 27086.707017421722, 'accumulated_eval_time': 2647.9743316173553, 'accumulated_logging_time': 5.604231595993042}
I0307 17:26:38.812871 140185115293440 logging_writer.py:48] [69504] accumulated_eval_time=2647.97, accumulated_logging_time=5.60423, accumulated_submission_time=27086.7, global_step=69504, preemption_count=0, score=27086.7, test/accuracy=0.5475, test/loss=2.16347, test/num_examples=10000, total_duration=29748.1, train/accuracy=0.759108, train/loss=1.1655, validation/accuracy=0.67506, validation/loss=1.5186, validation/num_examples=50000
I0307 17:27:16.989551 140185199154944 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.003971815109253, loss=3.13362717628479
I0307 17:27:56.981670 140185115293440 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.0592691898345947, loss=3.0718393325805664
I0307 17:28:37.321501 140185199154944 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.052680015563965, loss=3.1575021743774414
I0307 17:29:16.938951 140185115293440 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.115582227706909, loss=3.1556575298309326
I0307 17:29:56.548438 140185199154944 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.037139654159546, loss=3.067420721054077
2025-03-07 17:30:10.835451: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:30:35.913107 140185115293440 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0804927349090576, loss=3.071270227432251
I0307 17:31:15.955898 140185199154944 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.9526495933532715, loss=3.1279211044311523
I0307 17:31:55.851895 140185115293440 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.083158254623413, loss=3.1332719326019287
I0307 17:32:35.664017 140185199154944 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.15012264251709, loss=3.0585293769836426
I0307 17:33:14.886474 140185115293440 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.0289225578308105, loss=3.07426381111145
I0307 17:33:55.012885 140185199154944 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.04146409034729, loss=3.055225372314453
I0307 17:34:34.614864 140185115293440 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.3382174968719482, loss=3.1396713256835938
I0307 17:35:08.732901 140341280416960 spec.py:321] Evaluating on the training split.
I0307 17:35:20.028897 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 17:35:56.988278 140341280416960 spec.py:349] Evaluating on the test split.
I0307 17:35:58.779865 140341280416960 submission_runner.py:469] Time since start: 30308.23s, 	Step: 70788, 	{'train/accuracy': 0.7620774507522583, 'train/loss': 1.177040934562683, 'validation/accuracy': 0.6827600002288818, 'validation/loss': 1.529455304145813, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.1483676433563232, 'test/num_examples': 10000, 'score': 27596.426473617554, 'total_duration': 30308.234234809875, 'accumulated_submission_time': 27596.426473617554, 'accumulated_eval_time': 2698.021147251129, 'accumulated_logging_time': 5.833938360214233}
I0307 17:35:58.878256 140185199154944 logging_writer.py:48] [70788] accumulated_eval_time=2698.02, accumulated_logging_time=5.83394, accumulated_submission_time=27596.4, global_step=70788, preemption_count=0, score=27596.4, test/accuracy=0.5569, test/loss=2.14837, test/num_examples=10000, total_duration=30308.2, train/accuracy=0.762077, train/loss=1.17704, validation/accuracy=0.68276, validation/loss=1.52946, validation/num_examples=50000
I0307 17:36:04.173364 140185115293440 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.0114753246307373, loss=3.0656073093414307
I0307 17:36:44.204493 140185199154944 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.0305418968200684, loss=3.0951528549194336
I0307 17:37:24.432819 140185115293440 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.0443127155303955, loss=3.057299852371216
I0307 17:38:04.669584 140185199154944 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.178938865661621, loss=3.021040678024292
I0307 17:38:44.501188 140185115293440 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.9365124702453613, loss=3.070500135421753
I0307 17:39:23.268397 140185199154944 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.1157422065734863, loss=3.0742645263671875
I0307 17:40:03.451981 140185115293440 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.142918586730957, loss=3.105502128601074
I0307 17:40:43.841143 140185199154944 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.080397367477417, loss=3.1155827045440674
I0307 17:41:23.984192 140185115293440 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.1381046772003174, loss=3.1234447956085205
I0307 17:42:04.047773 140185199154944 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.0593912601470947, loss=3.0790209770202637
I0307 17:42:44.044816 140185115293440 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.052488327026367, loss=3.0233075618743896
I0307 17:43:25.139511 140185199154944 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.911284327507019, loss=2.972456932067871
I0307 17:44:09.780740 140185115293440 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.3085570335388184, loss=3.185945987701416
I0307 17:44:29.072497 140341280416960 spec.py:321] Evaluating on the training split.
I0307 17:44:41.568022 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 17:45:09.350326 140341280416960 spec.py:349] Evaluating on the test split.
I0307 17:45:11.142137 140341280416960 submission_runner.py:469] Time since start: 30860.60s, 	Step: 72045, 	{'train/accuracy': 0.7677175998687744, 'train/loss': 1.107324242591858, 'validation/accuracy': 0.6855999827384949, 'validation/loss': 1.4698505401611328, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.1142189502716064, 'test/num_examples': 10000, 'score': 28106.427806854248, 'total_duration': 30860.596434116364, 'accumulated_submission_time': 28106.427806854248, 'accumulated_eval_time': 2740.0905561447144, 'accumulated_logging_time': 5.975797176361084}
I0307 17:45:11.227025 140185199154944 logging_writer.py:48] [72045] accumulated_eval_time=2740.09, accumulated_logging_time=5.9758, accumulated_submission_time=28106.4, global_step=72045, preemption_count=0, score=28106.4, test/accuracy=0.5598, test/loss=2.11422, test/num_examples=10000, total_duration=30860.6, train/accuracy=0.767718, train/loss=1.10732, validation/accuracy=0.6856, validation/loss=1.46985, validation/num_examples=50000
I0307 17:45:34.055286 140185115293440 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.018714427947998, loss=3.07588267326355
I0307 17:46:18.486366 140185199154944 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.1426265239715576, loss=3.0666561126708984
I0307 17:47:02.085946 140185115293440 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.295703649520874, loss=3.062541961669922
I0307 17:47:42.694374 140185199154944 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.3865582942962646, loss=3.082402467727661
I0307 17:48:20.062374 140185115293440 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.0657691955566406, loss=3.067248582839966
I0307 17:48:58.539647 140185199154944 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.1765129566192627, loss=3.060619354248047
I0307 17:49:37.916953 140185115293440 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.1141226291656494, loss=3.0639727115631104
I0307 17:50:17.434467 140185199154944 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.2084403038024902, loss=3.113640785217285
I0307 17:50:56.881520 140185115293440 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.1219027042388916, loss=3.0405924320220947
I0307 17:51:36.643373 140185199154944 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.0120961666107178, loss=3.0032308101654053
I0307 17:52:16.001210 140185115293440 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.1561145782470703, loss=3.1141629219055176
I0307 17:52:56.462696 140185199154944 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.0492429733276367, loss=3.0342328548431396
I0307 17:53:41.511931 140341280416960 spec.py:321] Evaluating on the training split.
I0307 17:53:54.110785 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 17:54:20.017827 140341280416960 spec.py:349] Evaluating on the test split.
I0307 17:54:21.830270 140341280416960 submission_runner.py:469] Time since start: 31411.28s, 	Step: 73297, 	{'train/accuracy': 0.7648476958274841, 'train/loss': 1.1459081172943115, 'validation/accuracy': 0.6822400093078613, 'validation/loss': 1.502637267112732, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 2.1479387283325195, 'test/num_examples': 10000, 'score': 28616.536382198334, 'total_duration': 31411.28459239006, 'accumulated_submission_time': 28616.536382198334, 'accumulated_eval_time': 2780.4086968898773, 'accumulated_logging_time': 6.0878424644470215}
I0307 17:54:21.917769 140185115293440 logging_writer.py:48] [73297] accumulated_eval_time=2780.41, accumulated_logging_time=6.08784, accumulated_submission_time=28616.5, global_step=73297, preemption_count=0, score=28616.5, test/accuracy=0.5551, test/loss=2.14794, test/num_examples=10000, total_duration=31411.3, train/accuracy=0.764848, train/loss=1.14591, validation/accuracy=0.68224, validation/loss=1.50264, validation/num_examples=50000
I0307 17:54:23.477367 140185199154944 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.133852958679199, loss=3.1137452125549316
I0307 17:55:03.378453 140185115293440 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.0953736305236816, loss=3.0698390007019043
I0307 17:55:44.046868 140185199154944 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.0766193866729736, loss=3.085993528366089
I0307 17:56:36.280590 140185115293440 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.9451065063476562, loss=3.071478843688965
I0307 17:57:23.853366 140185199154944 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.19883131980896, loss=3.0746896266937256
2025-03-07 17:58:04.505396: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:58:05.166615 140185115293440 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.1931254863739014, loss=3.089447259902954
I0307 17:58:43.454736 140185199154944 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.1101608276367188, loss=3.0304179191589355
I0307 17:59:22.673817 140185115293440 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.2663323879241943, loss=3.168180227279663
I0307 18:00:02.991400 140185199154944 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.1682159900665283, loss=3.0408129692077637
I0307 18:00:44.452785 140185115293440 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.9718925952911377, loss=3.0400891304016113
I0307 18:01:25.437984 140185199154944 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.9478143453598022, loss=3.070474863052368
I0307 18:02:10.586042 140185115293440 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.1265957355499268, loss=3.1016035079956055
I0307 18:02:52.358571 140341280416960 spec.py:321] Evaluating on the training split.
I0307 18:03:05.127301 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 18:03:36.334281 140341280416960 spec.py:349] Evaluating on the test split.
I0307 18:03:38.140463 140341280416960 submission_runner.py:469] Time since start: 31967.59s, 	Step: 74494, 	{'train/accuracy': 0.7622767686843872, 'train/loss': 1.1620529890060425, 'validation/accuracy': 0.6791399717330933, 'validation/loss': 1.5248935222625732, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.1987879276275635, 'test/num_examples': 10000, 'score': 29126.793616771698, 'total_duration': 31967.594815969467, 'accumulated_submission_time': 29126.793616771698, 'accumulated_eval_time': 2826.190417289734, 'accumulated_logging_time': 6.215915679931641}
I0307 18:03:38.247054 140185199154944 logging_writer.py:48] [74494] accumulated_eval_time=2826.19, accumulated_logging_time=6.21592, accumulated_submission_time=29126.8, global_step=74494, preemption_count=0, score=29126.8, test/accuracy=0.5449, test/loss=2.19879, test/num_examples=10000, total_duration=31967.6, train/accuracy=0.762277, train/loss=1.16205, validation/accuracy=0.67914, validation/loss=1.52489, validation/num_examples=50000
I0307 18:03:41.721500 140185115293440 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.0825788974761963, loss=3.0108606815338135
I0307 18:04:21.821712 140185199154944 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.0649259090423584, loss=3.081879138946533
I0307 18:05:02.095645 140185115293440 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.035869598388672, loss=3.0118601322174072
I0307 18:05:42.087988 140185199154944 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.183971881866455, loss=3.092466354370117
I0307 18:06:27.926512 140185115293440 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.088548421859741, loss=3.0161287784576416
I0307 18:07:05.938441 140185199154944 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.2270662784576416, loss=3.126497268676758
I0307 18:08:29.259764 140185115293440 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.1777751445770264, loss=3.0819249153137207
I0307 18:09:07.409550 140185199154944 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.27468204498291, loss=3.126321792602539
I0307 18:09:45.829980 140185115293440 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.9513859748840332, loss=2.9518706798553467
I0307 18:10:23.417798 140185199154944 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.2094829082489014, loss=3.08756422996521
I0307 18:11:03.997590 140185115293440 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.2807908058166504, loss=3.088437557220459
I0307 18:11:52.065631 140185199154944 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.110031843185425, loss=3.1104302406311035
I0307 18:12:08.219751 140341280416960 spec.py:321] Evaluating on the training split.
I0307 18:12:21.024427 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 18:12:50.889526 140341280416960 spec.py:349] Evaluating on the test split.
I0307 18:12:52.698480 140341280416960 submission_runner.py:469] Time since start: 32522.15s, 	Step: 75631, 	{'train/accuracy': 0.7773038744926453, 'train/loss': 1.1048989295959473, 'validation/accuracy': 0.6800199747085571, 'validation/loss': 1.5323559045791626, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.1937952041625977, 'test/num_examples': 10000, 'score': 29635.829847812653, 'total_duration': 32522.152797698975, 'accumulated_submission_time': 29635.829847812653, 'accumulated_eval_time': 2870.66894197464, 'accumulated_logging_time': 7.124221563339233}
I0307 18:12:52.773291 140185115293440 logging_writer.py:48] [75631] accumulated_eval_time=2870.67, accumulated_logging_time=7.12422, accumulated_submission_time=29635.8, global_step=75631, preemption_count=0, score=29635.8, test/accuracy=0.5523, test/loss=2.1938, test/num_examples=10000, total_duration=32522.2, train/accuracy=0.777304, train/loss=1.1049, validation/accuracy=0.68002, validation/loss=1.53236, validation/num_examples=50000
I0307 18:13:20.349046 140185199154944 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.0070950984954834, loss=3.154149293899536
I0307 18:14:05.722527 140185115293440 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.0346484184265137, loss=3.0166521072387695
I0307 18:14:50.874641 140185199154944 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.2115306854248047, loss=3.1215877532958984
I0307 18:15:37.051083 140185115293440 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.2808640003204346, loss=3.061204195022583
I0307 18:16:30.411542 140185199154944 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.0968165397644043, loss=2.977295160293579
I0307 18:17:30.506183 140185115293440 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.02953839302063, loss=3.00100040435791
I0307 18:18:16.050665 140185199154944 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.125546932220459, loss=3.0536577701568604
2025-03-07 18:18:17.629171: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:18:56.915938 140185115293440 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.0309109687805176, loss=2.954707622528076
I0307 18:19:37.975779 140185199154944 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.124114990234375, loss=3.0112974643707275
I0307 18:20:23.410071 140185115293440 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.2025904655456543, loss=3.055203914642334
I0307 18:21:18.522793 140185199154944 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.2050468921661377, loss=3.066807985305786
I0307 18:21:22.873290 140341280416960 spec.py:321] Evaluating on the training split.
I0307 18:21:35.998350 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 18:22:17.283119 140341280416960 spec.py:349] Evaluating on the test split.
I0307 18:22:19.067685 140341280416960 submission_runner.py:469] Time since start: 33088.52s, 	Step: 76709, 	{'train/accuracy': 0.7604830861091614, 'train/loss': 1.1634560823440552, 'validation/accuracy': 0.6773799657821655, 'validation/loss': 1.5237208604812622, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 2.1779494285583496, 'test/num_examples': 10000, 'score': 30145.77255797386, 'total_duration': 33088.52205848694, 'accumulated_submission_time': 30145.77255797386, 'accumulated_eval_time': 2926.8631818294525, 'accumulated_logging_time': 7.230270862579346}
I0307 18:22:19.163917 140185115293440 logging_writer.py:48] [76709] accumulated_eval_time=2926.86, accumulated_logging_time=7.23027, accumulated_submission_time=30145.8, global_step=76709, preemption_count=0, score=30145.8, test/accuracy=0.5551, test/loss=2.17795, test/num_examples=10000, total_duration=33088.5, train/accuracy=0.760483, train/loss=1.16346, validation/accuracy=0.67738, validation/loss=1.52372, validation/num_examples=50000
I0307 18:22:55.873496 140185199154944 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.0955309867858887, loss=3.0375311374664307
I0307 18:23:38.311735 140185115293440 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.316288948059082, loss=3.0153863430023193
I0307 18:24:17.492251 140185199154944 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.911745309829712, loss=2.980252265930176
I0307 18:25:00.218044 140185115293440 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.1065642833709717, loss=3.0541770458221436
I0307 18:25:42.162189 140185199154944 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.172713041305542, loss=2.9911746978759766
I0307 18:26:22.045383 140185115293440 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.0978899002075195, loss=3.0108587741851807
I0307 18:27:09.835604 140185199154944 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.1406877040863037, loss=3.000919818878174
I0307 18:27:50.717198 140185115293440 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.001202344894409, loss=3.0084829330444336
2025-03-07 18:28:11.257924: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:28:30.118655 140185199154944 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.28373646736145, loss=3.0710508823394775
I0307 18:29:09.346241 140185115293440 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.3137314319610596, loss=3.0185186862945557
I0307 18:29:50.705943 140185199154944 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.1098968982696533, loss=3.072917938232422
I0307 18:30:32.329003 140185115293440 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.2307465076446533, loss=2.993173837661743
I0307 18:30:49.377541 140341280416960 spec.py:321] Evaluating on the training split.
I0307 18:31:01.276557 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 18:31:31.865656 140341280416960 spec.py:349] Evaluating on the test split.
I0307 18:31:33.612052 140341280416960 submission_runner.py:469] Time since start: 33643.07s, 	Step: 77943, 	{'train/accuracy': 0.7495415806770325, 'train/loss': 1.237155556678772, 'validation/accuracy': 0.6779400110244751, 'validation/loss': 1.560349702835083, 'validation/num_examples': 50000, 'test/accuracy': 0.5441000461578369, 'test/loss': 2.215446949005127, 'test/num_examples': 10000, 'score': 30655.801669836044, 'total_duration': 33643.0664191246, 'accumulated_submission_time': 30655.801669836044, 'accumulated_eval_time': 2971.097531557083, 'accumulated_logging_time': 7.3665220737457275}
I0307 18:31:33.668283 140185199154944 logging_writer.py:48] [77943] accumulated_eval_time=2971.1, accumulated_logging_time=7.36652, accumulated_submission_time=30655.8, global_step=77943, preemption_count=0, score=30655.8, test/accuracy=0.5441, test/loss=2.21545, test/num_examples=10000, total_duration=33643.1, train/accuracy=0.749542, train/loss=1.23716, validation/accuracy=0.67794, validation/loss=1.56035, validation/num_examples=50000
I0307 18:31:57.834228 140185115293440 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.0619704723358154, loss=3.042891502380371
I0307 18:32:51.676154 140185199154944 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.08097767829895, loss=3.0379552841186523
I0307 18:33:43.441859 140185115293440 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.15366530418396, loss=2.9937744140625
I0307 18:34:31.973218 140185199154944 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.2879981994628906, loss=3.050745964050293
I0307 18:35:44.945111 140185115293440 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.285784959793091, loss=3.0546436309814453
I0307 18:36:36.919896 140185199154944 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.221996307373047, loss=3.042065143585205
I0307 18:37:47.556922 140185115293440 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.3431315422058105, loss=3.0288166999816895
I0307 18:38:30.115857 140185199154944 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.27467679977417, loss=3.075927734375
I0307 18:39:10.787016 140185115293440 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.241486072540283, loss=3.00170636177063
I0307 18:39:50.063422 140185199154944 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.291706085205078, loss=3.132150411605835
I0307 18:40:03.631371 140341280416960 spec.py:321] Evaluating on the training split.
I0307 18:40:15.651671 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 18:40:46.262241 140341280416960 spec.py:349] Evaluating on the test split.
I0307 18:40:48.052226 140341280416960 submission_runner.py:469] Time since start: 34197.51s, 	Step: 78936, 	{'train/accuracy': 0.7708864808082581, 'train/loss': 1.1294012069702148, 'validation/accuracy': 0.6919400095939636, 'validation/loss': 1.4710290431976318, 'validation/num_examples': 50000, 'test/accuracy': 0.5579000115394592, 'test/loss': 2.1404590606689453, 'test/num_examples': 10000, 'score': 31165.594794988632, 'total_duration': 34197.5065677166, 'accumulated_submission_time': 31165.594794988632, 'accumulated_eval_time': 3015.51820397377, 'accumulated_logging_time': 7.475649833679199}
I0307 18:40:48.123344 140185115293440 logging_writer.py:48] [78936] accumulated_eval_time=3015.52, accumulated_logging_time=7.47565, accumulated_submission_time=31165.6, global_step=78936, preemption_count=0, score=31165.6, test/accuracy=0.5579, test/loss=2.14046, test/num_examples=10000, total_duration=34197.5, train/accuracy=0.770886, train/loss=1.1294, validation/accuracy=0.69194, validation/loss=1.47103, validation/num_examples=50000
I0307 18:41:13.842345 140185199154944 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.0247676372528076, loss=3.0001535415649414
I0307 18:41:55.963177 140185115293440 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.2348430156707764, loss=3.038377285003662
I0307 18:42:35.408667 140185199154944 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.21014142036438, loss=2.9817771911621094
I0307 18:43:14.850378 140185115293440 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.0263330936431885, loss=2.9641404151916504
I0307 18:43:54.867309 140185199154944 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.1778407096862793, loss=3.1016628742218018
I0307 18:44:34.963794 140185115293440 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.978210687637329, loss=3.0610883235931396
I0307 18:45:14.827042 140185199154944 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.2764408588409424, loss=3.0946249961853027
I0307 18:45:55.072558 140185115293440 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.1793265342712402, loss=3.147719383239746
I0307 18:46:34.924612 140185199154944 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.4242944717407227, loss=3.1460273265838623
I0307 18:47:20.373950 140185115293440 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.0924689769744873, loss=3.0950088500976562
I0307 18:48:03.540699 140185199154944 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.2726991176605225, loss=3.002742290496826
I0307 18:48:47.797753 140185115293440 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.119760513305664, loss=2.9512147903442383
I0307 18:49:18.136356 140341280416960 spec.py:321] Evaluating on the training split.
I0307 18:49:30.726220 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 18:50:01.893903 140341280416960 spec.py:349] Evaluating on the test split.
I0307 18:50:03.673853 140341280416960 submission_runner.py:469] Time since start: 34753.13s, 	Step: 80177, 	{'train/accuracy': 0.7633529901504517, 'train/loss': 1.1053038835525513, 'validation/accuracy': 0.6852799654006958, 'validation/loss': 1.4545778036117554, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.107264518737793, 'test/num_examples': 10000, 'score': 31675.427356481552, 'total_duration': 34753.1281747818, 'accumulated_submission_time': 31675.427356481552, 'accumulated_eval_time': 3061.055492401123, 'accumulated_logging_time': 7.58136510848999}
I0307 18:50:03.761838 140185199154944 logging_writer.py:48] [80177] accumulated_eval_time=3061.06, accumulated_logging_time=7.58137, accumulated_submission_time=31675.4, global_step=80177, preemption_count=0, score=31675.4, test/accuracy=0.5584, test/loss=2.10726, test/num_examples=10000, total_duration=34753.1, train/accuracy=0.763353, train/loss=1.1053, validation/accuracy=0.68528, validation/loss=1.45458, validation/num_examples=50000
I0307 18:50:13.425064 140185115293440 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.2253565788269043, loss=3.089791774749756
I0307 18:51:02.804796 140185199154944 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.1562728881835938, loss=3.0916528701782227
I0307 18:51:53.781754 140185115293440 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.2989108562469482, loss=3.119108200073242
I0307 18:52:50.678705 140185199154944 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.1504478454589844, loss=3.0818049907684326
I0307 18:53:49.002094 140185115293440 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.1753058433532715, loss=3.0348973274230957
I0307 18:54:29.727963 140185199154944 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.1490654945373535, loss=3.060105085372925
I0307 18:55:22.454998 140185115293440 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.3622984886169434, loss=3.031343460083008
I0307 18:56:17.700375 140185199154944 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.3189239501953125, loss=3.0395185947418213
I0307 18:56:59.145102 140185115293440 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.1356234550476074, loss=3.0140111446380615
I0307 18:57:40.896767 140185199154944 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.2474758625030518, loss=2.9736251831054688
I0307 18:58:21.918174 140185115293440 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.4352529048919678, loss=3.0921878814697266
I0307 18:58:33.996861 140341280416960 spec.py:321] Evaluating on the training split.
I0307 18:58:45.860831 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 18:59:28.431623 140341280416960 spec.py:349] Evaluating on the test split.
I0307 18:59:30.208536 140341280416960 submission_runner.py:469] Time since start: 35319.66s, 	Step: 81231, 	{'train/accuracy': 0.7667211294174194, 'train/loss': 1.1120444536209106, 'validation/accuracy': 0.6812399625778198, 'validation/loss': 1.4861243963241577, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.177626371383667, 'test/num_examples': 10000, 'score': 32185.51617050171, 'total_duration': 35319.66288971901, 'accumulated_submission_time': 32185.51617050171, 'accumulated_eval_time': 3117.266996860504, 'accumulated_logging_time': 7.694184064865112}
I0307 18:59:30.322057 140185199154944 logging_writer.py:48] [81231] accumulated_eval_time=3117.27, accumulated_logging_time=7.69418, accumulated_submission_time=32185.5, global_step=81231, preemption_count=0, score=32185.5, test/accuracy=0.5449, test/loss=2.17763, test/num_examples=10000, total_duration=35319.7, train/accuracy=0.766721, train/loss=1.11204, validation/accuracy=0.68124, validation/loss=1.48612, validation/num_examples=50000
I0307 19:00:03.673581 140185115293440 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.250257730484009, loss=3.0720014572143555
I0307 19:00:44.804301 140185199154944 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.286749839782715, loss=3.0674519538879395
I0307 19:01:24.305268 140185115293440 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.2670350074768066, loss=3.0396785736083984
I0307 19:02:03.578101 140185199154944 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.4904611110687256, loss=3.0500407218933105
I0307 19:02:42.510816 140185115293440 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.0888919830322266, loss=3.036513328552246
I0307 19:03:23.418291 140185199154944 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.305525302886963, loss=3.09928560256958
I0307 19:04:14.586560 140185115293440 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.1334316730499268, loss=2.979038715362549
I0307 19:05:24.554928 140185199154944 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.2617719173431396, loss=3.1126022338867188
I0307 19:06:13.586139 140185115293440 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.4532973766326904, loss=3.082151412963867
I0307 19:06:54.658417 140185199154944 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.5970664024353027, loss=3.123452663421631
I0307 19:07:34.901234 140185115293440 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.374391555786133, loss=3.062844753265381
I0307 19:08:00.350037 140341280416960 spec.py:321] Evaluating on the training split.
I0307 19:08:12.708206 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 19:08:55.997706 140341280416960 spec.py:349] Evaluating on the test split.
I0307 19:08:57.787931 140341280416960 submission_runner.py:469] Time since start: 35887.24s, 	Step: 82364, 	{'train/accuracy': 0.7679169178009033, 'train/loss': 1.10458505153656, 'validation/accuracy': 0.6771399974822998, 'validation/loss': 1.4980515241622925, 'validation/num_examples': 50000, 'test/accuracy': 0.5519000291824341, 'test/loss': 2.148900032043457, 'test/num_examples': 10000, 'score': 32695.381889104843, 'total_duration': 35887.24229502678, 'accumulated_submission_time': 32695.381889104843, 'accumulated_eval_time': 3174.7047324180603, 'accumulated_logging_time': 7.836381435394287}
I0307 19:08:57.919060 140185199154944 logging_writer.py:48] [82364] accumulated_eval_time=3174.7, accumulated_logging_time=7.83638, accumulated_submission_time=32695.4, global_step=82364, preemption_count=0, score=32695.4, test/accuracy=0.5519, test/loss=2.1489, test/num_examples=10000, total_duration=35887.2, train/accuracy=0.767917, train/loss=1.10459, validation/accuracy=0.67714, validation/loss=1.49805, validation/num_examples=50000
I0307 19:09:12.478135 140185115293440 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.293717384338379, loss=3.076378107070923
I0307 19:10:01.685501 140185199154944 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2764129638671875, loss=2.997297763824463
2025-03-07 19:10:29.048454: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:10:46.274534 140185115293440 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.1475961208343506, loss=3.128141164779663
I0307 19:11:25.803562 140185199154944 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.3931503295898438, loss=3.096652030944824
I0307 19:12:08.624030 140185115293440 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.242009162902832, loss=3.0867815017700195
I0307 19:12:53.477498 140185199154944 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.340442180633545, loss=2.966331958770752
I0307 19:13:57.003624 140185115293440 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.3100900650024414, loss=3.050122022628784
I0307 19:15:14.144003 140185199154944 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.31278657913208, loss=2.971254587173462
I0307 19:16:11.507528 140185115293440 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.3209362030029297, loss=3.0856380462646484
I0307 19:17:09.718056 140185199154944 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.4787757396698, loss=3.086108684539795
I0307 19:17:28.099889 140341280416960 spec.py:321] Evaluating on the training split.
I0307 19:17:40.598814 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 19:18:06.905420 140341280416960 spec.py:349] Evaluating on the test split.
I0307 19:18:08.684802 140341280416960 submission_runner.py:469] Time since start: 36438.14s, 	Step: 83334, 	{'train/accuracy': 0.8089126348495483, 'train/loss': 0.9515841603279114, 'validation/accuracy': 0.6881999969482422, 'validation/loss': 1.4534344673156738, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.1114156246185303, 'test/num_examples': 10000, 'score': 33205.43201088905, 'total_duration': 36438.139144182205, 'accumulated_submission_time': 33205.43201088905, 'accumulated_eval_time': 3215.289463043213, 'accumulated_logging_time': 7.983832836151123}
I0307 19:18:08.795176 140185115293440 logging_writer.py:48] [83334] accumulated_eval_time=3215.29, accumulated_logging_time=7.98383, accumulated_submission_time=33205.4, global_step=83334, preemption_count=0, score=33205.4, test/accuracy=0.5599, test/loss=2.11142, test/num_examples=10000, total_duration=36438.1, train/accuracy=0.808913, train/loss=0.951584, validation/accuracy=0.6882, validation/loss=1.45343, validation/num_examples=50000
I0307 19:18:35.149731 140185199154944 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1828231811523438, loss=3.0094540119171143
I0307 19:19:23.689194 140185115293440 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.217884063720703, loss=3.005216121673584
I0307 19:20:23.634846 140185199154944 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.076857566833496, loss=3.0222954750061035
I0307 19:21:42.924063 140185115293440 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.101553440093994, loss=2.989849805831909
I0307 19:22:37.918335 140185199154944 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.128845691680908, loss=3.0280494689941406
I0307 19:23:20.145958 140185115293440 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.238422393798828, loss=3.1280133724212646
I0307 19:24:04.888764 140185199154944 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.1494944095611572, loss=2.9151904582977295
I0307 19:25:02.145148 140185115293440 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.096012830734253, loss=2.9806034564971924
I0307 19:26:33.797531 140185199154944 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.344650983810425, loss=3.0055088996887207
I0307 19:26:38.806516 140341280416960 spec.py:321] Evaluating on the training split.
I0307 19:26:51.233621 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 19:27:20.131405 140341280416960 spec.py:349] Evaluating on the test split.
I0307 19:27:21.909584 140341280416960 submission_runner.py:469] Time since start: 36991.36s, 	Step: 84209, 	{'train/accuracy': 0.7755500674247742, 'train/loss': 1.0668928623199463, 'validation/accuracy': 0.69514000415802, 'validation/loss': 1.4248559474945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 2.0720713138580322, 'test/num_examples': 10000, 'score': 33715.3077750206, 'total_duration': 36991.36394572258, 'accumulated_submission_time': 33715.3077750206, 'accumulated_eval_time': 3258.392363548279, 'accumulated_logging_time': 8.128730058670044}
I0307 19:27:22.034593 140185115293440 logging_writer.py:48] [84209] accumulated_eval_time=3258.39, accumulated_logging_time=8.12873, accumulated_submission_time=33715.3, global_step=84209, preemption_count=0, score=33715.3, test/accuracy=0.5686, test/loss=2.07207, test/num_examples=10000, total_duration=36991.4, train/accuracy=0.77555, train/loss=1.06689, validation/accuracy=0.69514, validation/loss=1.42486, validation/num_examples=50000
I0307 19:28:47.842612 140185199154944 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.338239908218384, loss=3.006845235824585
I0307 19:29:40.784341 140185115293440 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.2531986236572266, loss=3.0565199851989746
I0307 19:30:41.612687 140185199154944 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.2004506587982178, loss=2.9984519481658936
I0307 19:31:49.710368 140185115293440 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.1524975299835205, loss=3.0248782634735107
I0307 19:32:37.870353 140185199154944 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.4977564811706543, loss=3.046614646911621
I0307 19:33:24.392793 140185115293440 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.0528624057769775, loss=3.015805959701538
I0307 19:35:09.917680 140185199154944 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.082749366760254, loss=2.9992356300354004
I0307 19:35:52.048978 140341280416960 spec.py:321] Evaluating on the training split.
I0307 19:36:04.611001 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 19:36:30.298835 140341280416960 spec.py:349] Evaluating on the test split.
I0307 19:36:32.083046 140341280416960 submission_runner.py:469] Time since start: 37541.51s, 	Step: 84975, 	{'train/accuracy': 0.7753507494926453, 'train/loss': 1.0685268640518188, 'validation/accuracy': 0.6874200105667114, 'validation/loss': 1.4645692110061646, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.135952949523926, 'test/num_examples': 10000, 'score': 34225.21005773544, 'total_duration': 37541.50943017006, 'accumulated_submission_time': 34225.21005773544, 'accumulated_eval_time': 3298.3982903957367, 'accumulated_logging_time': 8.280007362365723}
I0307 19:36:32.153018 140185115293440 logging_writer.py:48] [84975] accumulated_eval_time=3298.4, accumulated_logging_time=8.28001, accumulated_submission_time=34225.2, global_step=84975, preemption_count=0, score=34225.2, test/accuracy=0.5588, test/loss=2.13595, test/num_examples=10000, total_duration=37541.5, train/accuracy=0.775351, train/loss=1.06853, validation/accuracy=0.68742, validation/loss=1.46457, validation/num_examples=50000
I0307 19:36:42.560495 140185199154944 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.3252952098846436, loss=3.041255474090576
I0307 19:37:34.823175 140185115293440 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.5489823818206787, loss=2.961735725402832
I0307 19:38:19.611324 140185199154944 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.3098702430725098, loss=3.058483123779297
I0307 19:39:20.477129 140185115293440 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.2753703594207764, loss=3.027101516723633
I0307 19:40:52.313043 140185199154944 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.209418773651123, loss=3.015807628631592
I0307 19:42:00.281676 140185115293440 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.1853461265563965, loss=3.000075101852417
I0307 19:42:42.562329 140185199154944 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.2887043952941895, loss=3.091468334197998
I0307 19:43:40.802603 140185115293440 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.2604777812957764, loss=2.9986860752105713
I0307 19:44:50.736131 140185199154944 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.2854561805725098, loss=2.9980409145355225
I0307 19:45:02.533483 140341280416960 spec.py:321] Evaluating on the training split.
I0307 19:45:14.687688 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 19:45:38.339443 140341280416960 spec.py:349] Evaluating on the test split.
I0307 19:45:40.131704 140341280416960 submission_runner.py:469] Time since start: 38089.56s, 	Step: 85822, 	{'train/accuracy': 0.7726801633834839, 'train/loss': 1.0913530588150024, 'validation/accuracy': 0.6910799741744995, 'validation/loss': 1.4536737203598022, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 2.07099986076355, 'test/num_examples': 10000, 'score': 34735.47550153732, 'total_duration': 38089.56416344643, 'accumulated_submission_time': 34735.47550153732, 'accumulated_eval_time': 3335.974443912506, 'accumulated_logging_time': 8.370090961456299}
I0307 19:45:40.232954 140185115293440 logging_writer.py:48] [85822] accumulated_eval_time=3335.97, accumulated_logging_time=8.37009, accumulated_submission_time=34735.5, global_step=85822, preemption_count=0, score=34735.5, test/accuracy=0.5719, test/loss=2.071, test/num_examples=10000, total_duration=38089.6, train/accuracy=0.77268, train/loss=1.09135, validation/accuracy=0.69108, validation/loss=1.45367, validation/num_examples=50000
I0307 19:46:12.476341 140185199154944 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.321516275405884, loss=2.9801244735717773
I0307 19:46:58.582185 140185115293440 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.3761398792266846, loss=3.010695695877075
I0307 19:47:52.294450 140185199154944 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.1410458087921143, loss=3.0822577476501465
I0307 19:48:44.095901 140185115293440 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.2582645416259766, loss=2.962480306625366
I0307 19:49:43.573873 140185199154944 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.3037965297698975, loss=3.0457844734191895
I0307 19:50:43.065812 140185115293440 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.4018638134002686, loss=3.0451037883758545
I0307 19:51:53.077856 140185199154944 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.2315824031829834, loss=3.0195364952087402
I0307 19:53:04.261413 140185115293440 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.4059507846832275, loss=3.047606945037842
I0307 19:53:48.406170 140185199154944 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.364509105682373, loss=3.04142165184021
I0307 19:54:10.376650 140341280416960 spec.py:321] Evaluating on the training split.
I0307 19:54:22.358060 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 19:54:47.816440 140341280416960 spec.py:349] Evaluating on the test split.
I0307 19:54:49.625727 140341280416960 submission_runner.py:469] Time since start: 38639.04s, 	Step: 86741, 	{'train/accuracy': 0.77347731590271, 'train/loss': 1.1010082960128784, 'validation/accuracy': 0.6898199915885925, 'validation/loss': 1.457945466041565, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.113004207611084, 'test/num_examples': 10000, 'score': 35245.44828724861, 'total_duration': 38639.041773080826, 'accumulated_submission_time': 35245.44828724861, 'accumulated_eval_time': 3375.185043811798, 'accumulated_logging_time': 8.537782192230225}
I0307 19:54:49.686702 140185115293440 logging_writer.py:48] [86741] accumulated_eval_time=3375.19, accumulated_logging_time=8.53778, accumulated_submission_time=35245.4, global_step=86741, preemption_count=0, score=35245.4, test/accuracy=0.5653, test/loss=2.113, test/num_examples=10000, total_duration=38639, train/accuracy=0.773477, train/loss=1.10101, validation/accuracy=0.68982, validation/loss=1.45795, validation/num_examples=50000
I0307 19:55:34.980952 140185199154944 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.303054094314575, loss=3.0089054107666016
I0307 19:56:47.792799 140185115293440 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.191087007522583, loss=3.0753896236419678
I0307 19:57:45.449432 140185199154944 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.2112748622894287, loss=3.0841426849365234
I0307 19:58:38.259889 140185115293440 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.310547351837158, loss=2.9645087718963623
I0307 19:59:20.287013 140185199154944 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.2719271183013916, loss=2.9846279621124268
I0307 20:00:23.246096 140185115293440 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.3550257682800293, loss=3.05645489692688
I0307 20:01:20.761283 140185199154944 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.3394851684570312, loss=3.0353281497955322
I0307 20:02:17.929932 140185115293440 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.1749496459960938, loss=2.99292254447937
I0307 20:03:19.821834 140341280416960 spec.py:321] Evaluating on the training split.
I0307 20:03:33.478055 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 20:03:55.509068 140341280416960 spec.py:349] Evaluating on the test split.
I0307 20:03:57.288430 140341280416960 submission_runner.py:469] Time since start: 39186.72s, 	Step: 87564, 	{'train/accuracy': 0.7821866869926453, 'train/loss': 1.0664405822753906, 'validation/accuracy': 0.6843000054359436, 'validation/loss': 1.4900623559951782, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.1404166221618652, 'test/num_examples': 10000, 'score': 35755.461497068405, 'total_duration': 39186.72447896004, 'accumulated_submission_time': 35755.461497068405, 'accumulated_eval_time': 3412.6331615448, 'accumulated_logging_time': 8.624871492385864}
I0307 20:03:57.402186 140185199154944 logging_writer.py:48] [87564] accumulated_eval_time=3412.63, accumulated_logging_time=8.62487, accumulated_submission_time=35755.5, global_step=87564, preemption_count=0, score=35755.5, test/accuracy=0.5589, test/loss=2.14042, test/num_examples=10000, total_duration=39186.7, train/accuracy=0.782187, train/loss=1.06644, validation/accuracy=0.6843, validation/loss=1.49006, validation/num_examples=50000
I0307 20:04:31.405818 140185115293440 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.3305013179779053, loss=2.9982783794403076
I0307 20:05:32.075364 140185199154944 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.396678924560547, loss=2.991140604019165
I0307 20:06:21.248476 140185115293440 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.386950731277466, loss=3.0319793224334717
I0307 20:07:06.284588 140185199154944 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.4604289531707764, loss=3.1118335723876953
I0307 20:07:56.113238 140185115293440 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.245469570159912, loss=2.993495464324951
I0307 20:08:56.486117 140185199154944 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3098032474517822, loss=2.9975998401641846
I0307 20:10:02.923787 140185115293440 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3430662155151367, loss=3.0542538166046143
I0307 20:11:18.492024 140185199154944 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.2928452491760254, loss=3.0539040565490723
I0307 20:12:22.424338 140185115293440 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.292083740234375, loss=2.991978168487549
I0307 20:12:27.468149 140341280416960 spec.py:321] Evaluating on the training split.
I0307 20:12:39.673321 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 20:13:04.851449 140341280416960 spec.py:349] Evaluating on the test split.
I0307 20:13:06.645201 140341280416960 submission_runner.py:469] Time since start: 39736.08s, 	Step: 88409, 	{'train/accuracy': 0.7744738459587097, 'train/loss': 1.0919506549835205, 'validation/accuracy': 0.6951999664306641, 'validation/loss': 1.4406651258468628, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 2.0831708908081055, 'test/num_examples': 10000, 'score': 36265.38892579079, 'total_duration': 39736.07839012146, 'accumulated_submission_time': 36265.38892579079, 'accumulated_eval_time': 3451.788872241974, 'accumulated_logging_time': 8.782371520996094}
I0307 20:13:06.722784 140185199154944 logging_writer.py:48] [88409] accumulated_eval_time=3451.79, accumulated_logging_time=8.78237, accumulated_submission_time=36265.4, global_step=88409, preemption_count=0, score=36265.4, test/accuracy=0.5675, test/loss=2.08317, test/num_examples=10000, total_duration=39736.1, train/accuracy=0.774474, train/loss=1.09195, validation/accuracy=0.6952, validation/loss=1.44067, validation/num_examples=50000
I0307 20:13:48.321947 140185115293440 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.5184481143951416, loss=2.95032000541687
I0307 20:14:59.472457 140185199154944 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.2385759353637695, loss=3.009093999862671
I0307 20:16:07.335699 140185115293440 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.380571126937866, loss=3.0502893924713135
I0307 20:17:09.335161 140185199154944 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.261230230331421, loss=3.0028138160705566
I0307 20:18:16.724928 140185115293440 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.321887731552124, loss=3.037120819091797
I0307 20:19:13.294312 140185199154944 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.533092975616455, loss=3.017646312713623
I0307 20:20:08.546644 140185115293440 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.1566333770751953, loss=3.041692018508911
I0307 20:21:03.875115 140185199154944 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.2679712772369385, loss=2.9840922355651855
I0307 20:21:37.207650 140341280416960 spec.py:321] Evaluating on the training split.
I0307 20:21:49.662927 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 20:22:12.741402 140341280416960 spec.py:349] Evaluating on the test split.
I0307 20:22:14.524423 140341280416960 submission_runner.py:469] Time since start: 40283.95s, 	Step: 89258, 	{'train/accuracy': 0.7863121628761292, 'train/loss': 1.01169753074646, 'validation/accuracy': 0.6937999725341797, 'validation/loss': 1.4100854396820068, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 2.0428457260131836, 'test/num_examples': 10000, 'score': 36775.75869703293, 'total_duration': 40283.95372772217, 'accumulated_submission_time': 36775.75869703293, 'accumulated_eval_time': 3489.080422639847, 'accumulated_logging_time': 8.881547212600708}
I0307 20:22:14.605844 140185115293440 logging_writer.py:48] [89258] accumulated_eval_time=3489.08, accumulated_logging_time=8.88155, accumulated_submission_time=36775.8, global_step=89258, preemption_count=0, score=36775.8, test/accuracy=0.5732, test/loss=2.04285, test/num_examples=10000, total_duration=40284, train/accuracy=0.786312, train/loss=1.0117, validation/accuracy=0.6938, validation/loss=1.41009, validation/num_examples=50000
I0307 20:22:38.598766 140185199154944 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.530428886413574, loss=3.020124912261963
I0307 20:24:27.124984 140185115293440 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.373664140701294, loss=2.993743896484375
I0307 20:25:12.116990 140185199154944 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.254859685897827, loss=2.939790725708008
I0307 20:26:05.414566 140185115293440 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.2441670894622803, loss=3.0358598232269287
I0307 20:27:06.060773 140185199154944 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.1803598403930664, loss=2.984589099884033
I0307 20:28:21.890861 140185115293440 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.324281692504883, loss=2.9475629329681396
I0307 20:29:55.436758 140185199154944 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.3240132331848145, loss=3.023484468460083
I0307 20:30:44.697422 140341280416960 spec.py:321] Evaluating on the training split.
I0307 20:30:56.035270 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 20:31:18.653124 140341280416960 spec.py:349] Evaluating on the test split.
I0307 20:31:20.423798 140341280416960 submission_runner.py:469] Time since start: 40829.88s, 	Step: 89962, 	{'train/accuracy': 0.8044084906578064, 'train/loss': 0.9964030385017395, 'validation/accuracy': 0.6862199902534485, 'validation/loss': 1.499239206314087, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.1454038619995117, 'test/num_examples': 10000, 'score': 37285.75470209122, 'total_duration': 40829.878294467926, 'accumulated_submission_time': 37285.75470209122, 'accumulated_eval_time': 3524.806770801544, 'accumulated_logging_time': 8.980858325958252}
I0307 20:31:20.463051 140185115293440 logging_writer.py:48] [89962] accumulated_eval_time=3524.81, accumulated_logging_time=8.98086, accumulated_submission_time=37285.8, global_step=89962, preemption_count=0, score=37285.8, test/accuracy=0.5582, test/loss=2.1454, test/num_examples=10000, total_duration=40829.9, train/accuracy=0.804408, train/loss=0.996403, validation/accuracy=0.68622, validation/loss=1.49924, validation/num_examples=50000
I0307 20:31:38.955958 140185199154944 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.3586344718933105, loss=3.004396438598633
I0307 20:32:57.809760 140185115293440 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.5367698669433594, loss=3.0098876953125
I0307 20:34:15.102160 140185199154944 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.171640157699585, loss=2.9910569190979004
I0307 20:35:45.533938 140185115293440 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.3707072734832764, loss=2.9591259956359863
I0307 20:36:55.211888 140185199154944 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.270141839981079, loss=2.9812774658203125
I0307 20:37:59.995478 140185115293440 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.1547532081604004, loss=2.995110511779785
I0307 20:39:15.026009 140185199154944 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.3403573036193848, loss=2.9952728748321533
I0307 20:39:50.778174 140341280416960 spec.py:321] Evaluating on the training split.
I0307 20:40:01.735108 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 20:40:25.247128 140341280416960 spec.py:349] Evaluating on the test split.
I0307 20:40:27.001194 140341280416960 submission_runner.py:469] Time since start: 41376.46s, 	Step: 90644, 	{'train/accuracy': 0.7860331535339355, 'train/loss': 1.073421835899353, 'validation/accuracy': 0.6959799528121948, 'validation/loss': 1.4504474401474, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 2.0913236141204834, 'test/num_examples': 10000, 'score': 37795.98446011543, 'total_duration': 41376.45565891266, 'accumulated_submission_time': 37795.98446011543, 'accumulated_eval_time': 3561.029732942581, 'accumulated_logging_time': 9.029114484786987}
I0307 20:40:27.084930 140185115293440 logging_writer.py:48] [90644] accumulated_eval_time=3561.03, accumulated_logging_time=9.02911, accumulated_submission_time=37796, global_step=90644, preemption_count=0, score=37796, test/accuracy=0.5705, test/loss=2.09132, test/num_examples=10000, total_duration=41376.5, train/accuracy=0.786033, train/loss=1.07342, validation/accuracy=0.69598, validation/loss=1.45045, validation/num_examples=50000
I0307 20:41:02.271445 140185199154944 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.358140707015991, loss=3.0243422985076904
I0307 20:42:26.149609 140185115293440 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.3098347187042236, loss=3.0518455505371094
I0307 20:44:09.128405 140185199154944 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.332929849624634, loss=3.0071709156036377
I0307 20:45:09.670135 140185115293440 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.3945255279541016, loss=3.0428500175476074
I0307 20:46:49.208228 140185199154944 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.244356155395508, loss=3.0214056968688965
I0307 20:48:57.054391 140341280416960 spec.py:321] Evaluating on the training split.
I0307 20:49:07.304192 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 20:49:29.168595 140341280416960 spec.py:349] Evaluating on the test split.
I0307 20:49:30.888380 140341280416960 submission_runner.py:469] Time since start: 41920.34s, 	Step: 91146, 	{'train/accuracy': 0.7768654227256775, 'train/loss': 1.0826961994171143, 'validation/accuracy': 0.6928600072860718, 'validation/loss': 1.452547311782837, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 2.0919785499572754, 'test/num_examples': 10000, 'score': 38305.88724422455, 'total_duration': 41920.34286212921, 'accumulated_submission_time': 38305.88724422455, 'accumulated_eval_time': 3594.863690137863, 'accumulated_logging_time': 9.121721744537354}
I0307 20:49:30.930611 140185115293440 logging_writer.py:48] [91146] accumulated_eval_time=3594.86, accumulated_logging_time=9.12172, accumulated_submission_time=38305.9, global_step=91146, preemption_count=0, score=38305.9, test/accuracy=0.5736, test/loss=2.09198, test/num_examples=10000, total_duration=41920.3, train/accuracy=0.776865, train/loss=1.0827, validation/accuracy=0.69286, validation/loss=1.45255, validation/num_examples=50000
I0307 20:49:56.663980 140185199154944 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.5526154041290283, loss=2.912921905517578
I0307 20:51:22.307185 140185115293440 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.1807281970977783, loss=2.8794262409210205
2025-03-07 20:51:37.369771: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:52:35.320506 140185199154944 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.3358817100524902, loss=3.0640385150909424
I0307 20:54:02.577392 140185115293440 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.385157346725464, loss=2.9429931640625
I0307 20:55:08.797333 140185199154944 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.396991014480591, loss=2.9682040214538574
I0307 20:56:03.352833 140185115293440 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.3200442790985107, loss=2.9566664695739746
I0307 20:57:00.992701 140185199154944 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.2214035987854004, loss=2.9045536518096924
I0307 20:58:01.261885 140341280416960 spec.py:321] Evaluating on the training split.
I0307 20:58:12.154783 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 20:58:36.265934 140341280416960 spec.py:349] Evaluating on the test split.
I0307 20:58:38.022985 140341280416960 submission_runner.py:469] Time since start: 42467.48s, 	Step: 91898, 	{'train/accuracy': 0.7866509556770325, 'train/loss': 1.0733829736709595, 'validation/accuracy': 0.6958000063896179, 'validation/loss': 1.4727671146392822, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.099191427230835, 'test/num_examples': 10000, 'score': 38816.12714076042, 'total_duration': 42467.47746562958, 'accumulated_submission_time': 38816.12714076042, 'accumulated_eval_time': 3631.624750137329, 'accumulated_logging_time': 9.17157244682312}
I0307 20:58:38.117856 140185115293440 logging_writer.py:48] [91898] accumulated_eval_time=3631.62, accumulated_logging_time=9.17157, accumulated_submission_time=38816.1, global_step=91898, preemption_count=0, score=38816.1, test/accuracy=0.5727, test/loss=2.09919, test/num_examples=10000, total_duration=42467.5, train/accuracy=0.786651, train/loss=1.07338, validation/accuracy=0.6958, validation/loss=1.47277, validation/num_examples=50000
I0307 20:58:39.234049 140185199154944 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.289400577545166, loss=2.938591241836548
I0307 21:00:19.608065 140185115293440 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.197152614593506, loss=3.030266284942627
I0307 21:02:25.099628 140185199154944 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.2673704624176025, loss=3.0163094997406006
I0307 21:04:31.072652 140185115293440 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.2869319915771484, loss=2.9441866874694824
I0307 21:05:35.758110 140185199154944 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.4685018062591553, loss=2.968761682510376
I0307 21:07:11.633568 140341280416960 spec.py:321] Evaluating on the training split.
I0307 21:07:21.237934 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 21:07:43.987626 140341280416960 spec.py:349] Evaluating on the test split.
I0307 21:07:45.706681 140341280416960 submission_runner.py:469] Time since start: 43015.16s, 	Step: 92347, 	{'train/accuracy': 0.7689133882522583, 'train/loss': 1.0710959434509277, 'validation/accuracy': 0.6904199719429016, 'validation/loss': 1.4342259168624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0925800800323486, 'test/num_examples': 10000, 'score': 39329.58285307884, 'total_duration': 43015.16115760803, 'accumulated_submission_time': 39329.58285307884, 'accumulated_eval_time': 3665.6978204250336, 'accumulated_logging_time': 9.276031732559204}
I0307 21:07:45.771174 140185115293440 logging_writer.py:48] [92347] accumulated_eval_time=3665.7, accumulated_logging_time=9.27603, accumulated_submission_time=39329.6, global_step=92347, preemption_count=0, score=39329.6, test/accuracy=0.564, test/loss=2.09258, test/num_examples=10000, total_duration=43015.2, train/accuracy=0.768913, train/loss=1.0711, validation/accuracy=0.69042, validation/loss=1.43423, validation/num_examples=50000
I0307 21:08:29.456151 140185199154944 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.292588472366333, loss=3.0318028926849365
I0307 21:10:20.661551 140185115293440 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.2292873859405518, loss=2.928823947906494
I0307 21:12:26.798140 140185199154944 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.5922160148620605, loss=3.0752644538879395
I0307 21:13:34.058068 140185115293440 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.4063634872436523, loss=2.9689602851867676
I0307 21:14:35.425174 140185199154944 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.5836188793182373, loss=2.98453950881958
I0307 21:15:37.593092 140185115293440 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.473400115966797, loss=2.9792392253875732
I0307 21:16:15.919763 140341280416960 spec.py:321] Evaluating on the training split.
I0307 21:16:26.655445 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 21:16:50.162791 140341280416960 spec.py:349] Evaluating on the test split.
I0307 21:16:51.905195 140341280416960 submission_runner.py:469] Time since start: 43561.36s, 	Step: 92964, 	{'train/accuracy': 0.7914540767669678, 'train/loss': 1.0489636659622192, 'validation/accuracy': 0.6917399764060974, 'validation/loss': 1.4696197509765625, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 2.1137452125549316, 'test/num_examples': 10000, 'score': 39839.65118050575, 'total_duration': 43561.359634399414, 'accumulated_submission_time': 39839.65118050575, 'accumulated_eval_time': 3701.6831691265106, 'accumulated_logging_time': 9.348866939544678}
I0307 21:16:51.956386 140185199154944 logging_writer.py:48] [92964] accumulated_eval_time=3701.68, accumulated_logging_time=9.34887, accumulated_submission_time=39839.7, global_step=92964, preemption_count=0, score=39839.7, test/accuracy=0.5695, test/loss=2.11375, test/num_examples=10000, total_duration=43561.4, train/accuracy=0.791454, train/loss=1.04896, validation/accuracy=0.69174, validation/loss=1.46962, validation/num_examples=50000
I0307 21:17:15.024309 140185115293440 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.432952404022217, loss=3.0594518184661865
I0307 21:20:30.310665 140185199154944 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.35306453704834, loss=3.0101418495178223
I0307 21:22:07.461115 140185115293440 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.3219916820526123, loss=2.984741449356079
I0307 21:23:38.354725 140185199154944 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.4933059215545654, loss=3.046271800994873
I0307 21:25:23.214831 140341280416960 spec.py:321] Evaluating on the training split.
I0307 21:25:33.569862 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 21:25:55.624913 140341280416960 spec.py:349] Evaluating on the test split.
I0307 21:25:57.407119 140341280416960 submission_runner.py:469] Time since start: 44106.86s, 	Step: 93393, 	{'train/accuracy': 0.7860929369926453, 'train/loss': 1.0468393564224243, 'validation/accuracy': 0.69896000623703, 'validation/loss': 1.4283665418624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 2.05535888671875, 'test/num_examples': 10000, 'score': 40350.85218667984, 'total_duration': 44106.8616027832, 'accumulated_submission_time': 40350.85218667984, 'accumulated_eval_time': 3735.875420331955, 'accumulated_logging_time': 9.408864974975586}
I0307 21:25:57.441165 140185115293440 logging_writer.py:48] [93393] accumulated_eval_time=3735.88, accumulated_logging_time=9.40886, accumulated_submission_time=40350.9, global_step=93393, preemption_count=0, score=40350.9, test/accuracy=0.5758, test/loss=2.05536, test/num_examples=10000, total_duration=44106.9, train/accuracy=0.786093, train/loss=1.04684, validation/accuracy=0.69896, validation/loss=1.42837, validation/num_examples=50000
I0307 21:26:00.679980 140185199154944 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.3504433631896973, loss=3.026172637939453
I0307 21:27:04.504715 140185115293440 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.526945114135742, loss=3.018536329269409
I0307 21:28:33.944792 140185199154944 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.4346251487731934, loss=2.9913387298583984
I0307 21:31:12.444131 140185115293440 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.4620437622070312, loss=3.0101730823516846
I0307 21:32:40.782963 140185199154944 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.4024789333343506, loss=2.9859631061553955
I0307 21:34:28.270900 140341280416960 spec.py:321] Evaluating on the training split.
I0307 21:34:39.554827 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 21:35:00.591157 140341280416960 spec.py:349] Evaluating on the test split.
I0307 21:35:02.319728 140341280416960 submission_runner.py:469] Time since start: 44651.77s, 	Step: 93895, 	{'train/accuracy': 0.7711853981018066, 'train/loss': 1.1222543716430664, 'validation/accuracy': 0.6908400058746338, 'validation/loss': 1.479730248451233, 'validation/num_examples': 50000, 'test/accuracy': 0.565500020980835, 'test/loss': 2.1182796955108643, 'test/num_examples': 10000, 'score': 40861.59526729584, 'total_duration': 44651.774198532104, 'accumulated_submission_time': 40861.59526729584, 'accumulated_eval_time': 3769.9242022037506, 'accumulated_logging_time': 9.473358631134033}
I0307 21:35:02.356343 140185115293440 logging_writer.py:48] [93895] accumulated_eval_time=3769.92, accumulated_logging_time=9.47336, accumulated_submission_time=40861.6, global_step=93895, preemption_count=0, score=40861.6, test/accuracy=0.5655, test/loss=2.11828, test/num_examples=10000, total_duration=44651.8, train/accuracy=0.771185, train/loss=1.12225, validation/accuracy=0.69084, validation/loss=1.47973, validation/num_examples=50000
I0307 21:35:04.677342 140185199154944 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.458320379257202, loss=3.0372631549835205
I0307 21:36:15.340109 140185115293440 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.440769672393799, loss=2.975342273712158
I0307 21:38:26.525943 140185199154944 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.30810284614563, loss=2.9787938594818115
I0307 21:39:43.881502 140185115293440 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.368643283843994, loss=2.9720523357391357
I0307 21:41:28.639682 140185199154944 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.472576141357422, loss=2.9693245887756348
I0307 21:43:33.634287 140341280416960 spec.py:321] Evaluating on the training split.
I0307 21:43:44.073037 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 21:44:04.981338 140341280416960 spec.py:349] Evaluating on the test split.
I0307 21:44:06.737344 140341280416960 submission_runner.py:469] Time since start: 45196.19s, 	Step: 94347, 	{'train/accuracy': 0.8000637888908386, 'train/loss': 0.9999526739120483, 'validation/accuracy': 0.7022599577903748, 'validation/loss': 1.42261803150177, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 2.066884756088257, 'test/num_examples': 10000, 'score': 41372.81498336792, 'total_duration': 45196.191808223724, 'accumulated_submission_time': 41372.81498336792, 'accumulated_eval_time': 3803.0272052288055, 'accumulated_logging_time': 9.517786979675293}
I0307 21:44:06.823873 140185115293440 logging_writer.py:48] [94347] accumulated_eval_time=3803.03, accumulated_logging_time=9.51779, accumulated_submission_time=41372.8, global_step=94347, preemption_count=0, score=41372.8, test/accuracy=0.5751, test/loss=2.06688, test/num_examples=10000, total_duration=45196.2, train/accuracy=0.800064, train/loss=0.999953, validation/accuracy=0.70226, validation/loss=1.42262, validation/num_examples=50000
I0307 21:44:59.541740 140185199154944 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.488626003265381, loss=2.965122938156128
I0307 21:46:48.660939 140185115293440 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.2376060485839844, loss=2.887474298477173
I0307 21:48:25.534193 140185199154944 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.4567081928253174, loss=2.9472193717956543
I0307 21:51:29.792164 140185115293440 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.2969558238983154, loss=3.0457706451416016
I0307 21:52:28.711997 140185199154944 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.3516554832458496, loss=2.9753305912017822
I0307 21:52:37.340477 140341280416960 spec.py:321] Evaluating on the training split.
I0307 21:52:48.095088 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 21:53:14.684239 140341280416960 spec.py:349] Evaluating on the test split.
I0307 21:53:16.456210 140341280416960 submission_runner.py:469] Time since start: 45745.91s, 	Step: 94807, 	{'train/accuracy': 0.7833425998687744, 'train/loss': 1.0782657861709595, 'validation/accuracy': 0.6968399882316589, 'validation/loss': 1.4597746133804321, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 2.080094575881958, 'test/num_examples': 10000, 'score': 41883.268496990204, 'total_duration': 45745.91068840027, 'accumulated_submission_time': 41883.268496990204, 'accumulated_eval_time': 3842.1428899765015, 'accumulated_logging_time': 9.613422632217407}
I0307 21:53:16.537063 140185115293440 logging_writer.py:48] [94807] accumulated_eval_time=3842.14, accumulated_logging_time=9.61342, accumulated_submission_time=41883.3, global_step=94807, preemption_count=0, score=41883.3, test/accuracy=0.5769, test/loss=2.08009, test/num_examples=10000, total_duration=45745.9, train/accuracy=0.783343, train/loss=1.07827, validation/accuracy=0.69684, validation/loss=1.45977, validation/num_examples=50000
I0307 21:55:43.945611 140185199154944 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.4171741008758545, loss=2.99586820602417
I0307 21:57:43.046657 140185115293440 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.450040817260742, loss=2.911255359649658
I0307 21:58:55.632815 140185199154944 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.4100053310394287, loss=2.861253499984741
I0307 21:59:55.748115 140185115293440 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.2224690914154053, loss=2.913606882095337
I0307 22:01:13.497904 140185199154944 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.397028923034668, loss=2.918645143508911
I0307 22:01:46.651053 140341280416960 spec.py:321] Evaluating on the training split.
I0307 22:01:58.785341 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 22:02:21.246766 140341280416960 spec.py:349] Evaluating on the test split.
I0307 22:02:23.007695 140341280416960 submission_runner.py:469] Time since start: 46292.46s, 	Step: 95342, 	{'train/accuracy': 0.7805923223495483, 'train/loss': 1.0701628923416138, 'validation/accuracy': 0.697160005569458, 'validation/loss': 1.447644591331482, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.0923893451690674, 'test/num_examples': 10000, 'score': 42393.314445734024, 'total_duration': 46292.46217918396, 'accumulated_submission_time': 42393.314445734024, 'accumulated_eval_time': 3878.4994938373566, 'accumulated_logging_time': 9.70279836654663}
I0307 22:02:23.067117 140185115293440 logging_writer.py:48] [95342] accumulated_eval_time=3878.5, accumulated_logging_time=9.7028, accumulated_submission_time=42393.3, global_step=95342, preemption_count=0, score=42393.3, test/accuracy=0.5727, test/loss=2.09239, test/num_examples=10000, total_duration=46292.5, train/accuracy=0.780592, train/loss=1.07016, validation/accuracy=0.69716, validation/loss=1.44764, validation/num_examples=50000
I0307 22:03:01.111155 140185199154944 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.303424835205078, loss=3.018430471420288
I0307 22:04:18.242212 140185115293440 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.4275100231170654, loss=2.9468376636505127
I0307 22:05:36.243777 140185199154944 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.292008876800537, loss=2.9761581420898438
I0307 22:08:47.455235 140185115293440 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.4029040336608887, loss=2.9443233013153076
I0307 22:10:53.119229 140341280416960 spec.py:321] Evaluating on the training split.
I0307 22:11:03.712491 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 22:11:36.838480 140341280416960 spec.py:349] Evaluating on the test split.
I0307 22:11:38.602316 140341280416960 submission_runner.py:469] Time since start: 46848.06s, 	Step: 95731, 	{'train/accuracy': 0.7913345098495483, 'train/loss': 1.0269100666046143, 'validation/accuracy': 0.6927599906921387, 'validation/loss': 1.4490102529525757, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 2.124859571456909, 'test/num_examples': 10000, 'score': 42903.314015865326, 'total_duration': 46848.056789159775, 'accumulated_submission_time': 42903.314015865326, 'accumulated_eval_time': 3923.982547521591, 'accumulated_logging_time': 9.770930290222168}
I0307 22:11:38.680761 140185199154944 logging_writer.py:48] [95731] accumulated_eval_time=3923.98, accumulated_logging_time=9.77093, accumulated_submission_time=42903.3, global_step=95731, preemption_count=0, score=42903.3, test/accuracy=0.5622, test/loss=2.12486, test/num_examples=10000, total_duration=46848.1, train/accuracy=0.791335, train/loss=1.02691, validation/accuracy=0.69276, validation/loss=1.44901, validation/num_examples=50000
I0307 22:14:45.526643 140185115293440 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.290748119354248, loss=2.9470365047454834
I0307 22:18:28.277266 140185199154944 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.439363956451416, loss=2.960766553878784
I0307 22:19:29.534365 140185115293440 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.282430410385132, loss=2.9656026363372803
I0307 22:20:08.688871 140341280416960 spec.py:321] Evaluating on the training split.
I0307 22:20:20.299325 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 22:20:42.266882 140341280416960 spec.py:349] Evaluating on the test split.
I0307 22:20:43.995130 140341280416960 submission_runner.py:469] Time since start: 47393.45s, 	Step: 96062, 	{'train/accuracy': 0.7967354655265808, 'train/loss': 1.04228937625885, 'validation/accuracy': 0.7035599946975708, 'validation/loss': 1.4404469728469849, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 2.0683138370513916, 'test/num_examples': 10000, 'score': 43413.27730894089, 'total_duration': 47393.449577093124, 'accumulated_submission_time': 43413.27730894089, 'accumulated_eval_time': 3959.288734436035, 'accumulated_logging_time': 9.857762336730957}
I0307 22:20:44.037881 140185199154944 logging_writer.py:48] [96062] accumulated_eval_time=3959.29, accumulated_logging_time=9.85776, accumulated_submission_time=43413.3, global_step=96062, preemption_count=0, score=43413.3, test/accuracy=0.5795, test/loss=2.06831, test/num_examples=10000, total_duration=47393.4, train/accuracy=0.796735, train/loss=1.04229, validation/accuracy=0.70356, validation/loss=1.44045, validation/num_examples=50000
I0307 22:21:18.415408 140185115293440 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.375481128692627, loss=3.0019609928131104
I0307 22:23:04.368103 140185199154944 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.221745729446411, loss=2.9110989570617676
I0307 22:24:50.484729 140185115293440 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.3419413566589355, loss=2.9042577743530273
I0307 22:26:53.232330 140185199154944 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.3128199577331543, loss=2.9668822288513184
I0307 22:28:34.946734 140185115293440 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.2240235805511475, loss=2.9949684143066406
I0307 22:29:14.370787 140341280416960 spec.py:321] Evaluating on the training split.
I0307 22:29:26.093895 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 22:29:47.588135 140341280416960 spec.py:349] Evaluating on the test split.
I0307 22:29:49.337113 140341280416960 submission_runner.py:469] Time since start: 47938.79s, 	Step: 96537, 	{'train/accuracy': 0.7886240482330322, 'train/loss': 1.0366038084030151, 'validation/accuracy': 0.7020399570465088, 'validation/loss': 1.4125021696090698, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 2.0534791946411133, 'test/num_examples': 10000, 'score': 43923.54673600197, 'total_duration': 47938.79158616066, 'accumulated_submission_time': 43923.54673600197, 'accumulated_eval_time': 3994.255005121231, 'accumulated_logging_time': 9.910648584365845}
I0307 22:29:49.398882 140185199154944 logging_writer.py:48] [96537] accumulated_eval_time=3994.26, accumulated_logging_time=9.91065, accumulated_submission_time=43923.5, global_step=96537, preemption_count=0, score=43923.5, test/accuracy=0.5771, test/loss=2.05348, test/num_examples=10000, total_duration=47938.8, train/accuracy=0.788624, train/loss=1.0366, validation/accuracy=0.70204, validation/loss=1.4125, validation/num_examples=50000
I0307 22:30:49.281054 140185115293440 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.4592771530151367, loss=3.000842332839966
I0307 22:32:37.535869 140185199154944 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.331603765487671, loss=2.980046510696411
I0307 22:34:09.800993 140185115293440 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.498927116394043, loss=2.9910573959350586
I0307 22:35:40.687069 140185199154944 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.347801923751831, loss=2.980833053588867
I0307 22:37:12.348838 140185115293440 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.388875961303711, loss=2.991656541824341
I0307 22:38:22.429970 140341280416960 spec.py:321] Evaluating on the training split.
I0307 22:38:33.030456 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 22:39:11.471344 140341280416960 spec.py:349] Evaluating on the test split.
I0307 22:39:13.238690 140341280416960 submission_runner.py:469] Time since start: 48502.69s, 	Step: 97030, 	{'train/accuracy': 0.8112643361091614, 'train/loss': 0.9475439190864563, 'validation/accuracy': 0.7026199698448181, 'validation/loss': 1.4059715270996094, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 2.0581629276275635, 'test/num_examples': 10000, 'score': 44436.513724565506, 'total_duration': 48502.69314837456, 'accumulated_submission_time': 44436.513724565506, 'accumulated_eval_time': 4045.063665151596, 'accumulated_logging_time': 9.981146812438965}
I0307 22:39:13.289956 140185199154944 logging_writer.py:48] [97030] accumulated_eval_time=4045.06, accumulated_logging_time=9.98115, accumulated_submission_time=44436.5, global_step=97030, preemption_count=0, score=44436.5, test/accuracy=0.5761, test/loss=2.05816, test/num_examples=10000, total_duration=48502.7, train/accuracy=0.811264, train/loss=0.947544, validation/accuracy=0.70262, validation/loss=1.40597, validation/num_examples=50000
I0307 22:41:38.214997 140185115293440 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.4044904708862305, loss=2.9462132453918457
I0307 22:43:41.866340 140185199154944 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.3800113201141357, loss=2.9564359188079834
I0307 22:44:43.538473 140185115293440 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.3994507789611816, loss=3.004645347595215
I0307 22:45:54.748356 140185199154944 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.425452947616577, loss=3.0262420177459717
I0307 22:46:55.172382 140185115293440 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.44254207611084, loss=2.927685022354126
I0307 22:47:43.290865 140341280416960 spec.py:321] Evaluating on the training split.
I0307 22:47:57.305284 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 22:48:19.828137 140341280416960 spec.py:349] Evaluating on the test split.
I0307 22:48:21.561595 140341280416960 submission_runner.py:469] Time since start: 49051.02s, 	Step: 97579, 	{'train/accuracy': 0.7944634556770325, 'train/loss': 1.0361807346343994, 'validation/accuracy': 0.7009599804878235, 'validation/loss': 1.4246653318405151, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.088813066482544, 'test/num_examples': 10000, 'score': 44946.44351410866, 'total_duration': 49051.01607656479, 'accumulated_submission_time': 44946.44351410866, 'accumulated_eval_time': 4083.3343465328217, 'accumulated_logging_time': 10.042106866836548}
I0307 22:48:21.627538 140185199154944 logging_writer.py:48] [97579] accumulated_eval_time=4083.33, accumulated_logging_time=10.0421, accumulated_submission_time=44946.4, global_step=97579, preemption_count=0, score=44946.4, test/accuracy=0.5698, test/loss=2.08881, test/num_examples=10000, total_duration=49051, train/accuracy=0.794463, train/loss=1.03618, validation/accuracy=0.70096, validation/loss=1.42467, validation/num_examples=50000
I0307 22:48:30.398873 140185115293440 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.5065062046051025, loss=2.9433884620666504
I0307 22:49:16.948909 140185199154944 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.4471805095672607, loss=2.9791979789733887
I0307 22:50:06.522014 140185115293440 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.369316577911377, loss=2.942913055419922
I0307 22:50:56.241782 140185199154944 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.5933837890625, loss=2.9513916969299316
I0307 22:54:03.468944 140185115293440 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.4505083560943604, loss=2.9932210445404053
I0307 22:56:53.274101 140341280416960 spec.py:321] Evaluating on the training split.
I0307 22:57:03.028969 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 22:57:27.563865 140341280416960 spec.py:349] Evaluating on the test split.
I0307 22:57:29.323915 140341280416960 submission_runner.py:469] Time since start: 49598.78s, 	Step: 98041, 	{'train/accuracy': 0.7828842401504517, 'train/loss': 1.0549447536468506, 'validation/accuracy': 0.6923399567604065, 'validation/loss': 1.4419994354248047, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 2.082357406616211, 'test/num_examples': 10000, 'score': 45458.03125190735, 'total_duration': 49598.778399944305, 'accumulated_submission_time': 45458.03125190735, 'accumulated_eval_time': 4119.384139537811, 'accumulated_logging_time': 10.116724729537964}
I0307 22:57:29.362404 140185199154944 logging_writer.py:48] [98041] accumulated_eval_time=4119.38, accumulated_logging_time=10.1167, accumulated_submission_time=45458, global_step=98041, preemption_count=0, score=45458, test/accuracy=0.5705, test/loss=2.08236, test/num_examples=10000, total_duration=49598.8, train/accuracy=0.782884, train/loss=1.05494, validation/accuracy=0.69234, validation/loss=1.442, validation/num_examples=50000
I0307 23:01:24.423250 140185115293440 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.4993321895599365, loss=2.924992799758911
I0307 23:04:31.500974 140185199154944 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.369699001312256, loss=2.904153347015381
I0307 23:06:00.233324 140341280416960 spec.py:321] Evaluating on the training split.
I0307 23:06:10.976504 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 23:06:33.822468 140341280416960 spec.py:349] Evaluating on the test split.
I0307 23:06:35.565156 140341280416960 submission_runner.py:469] Time since start: 50145.02s, 	Step: 98262, 	{'train/accuracy': 0.8039500713348389, 'train/loss': 0.9573618769645691, 'validation/accuracy': 0.697160005569458, 'validation/loss': 1.4014830589294434, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 2.065692186355591, 'test/num_examples': 10000, 'score': 45968.87014555931, 'total_duration': 50145.01964735985, 'accumulated_submission_time': 45968.87014555931, 'accumulated_eval_time': 4154.715940475464, 'accumulated_logging_time': 10.163469791412354}
I0307 23:06:35.584408 140185115293440 logging_writer.py:48] [98262] accumulated_eval_time=4154.72, accumulated_logging_time=10.1635, accumulated_submission_time=45968.9, global_step=98262, preemption_count=0, score=45968.9, test/accuracy=0.5684, test/loss=2.06569, test/num_examples=10000, total_duration=50145, train/accuracy=0.80395, train/loss=0.957362, validation/accuracy=0.69716, validation/loss=1.40148, validation/num_examples=50000
I0307 23:07:16.384139 140185199154944 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.520362377166748, loss=2.9903712272644043
I0307 23:09:08.005410 140185115293440 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.4586994647979736, loss=2.967362403869629
I0307 23:10:14.286113 140185199154944 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.4880456924438477, loss=3.0044734477996826
I0307 23:11:24.275796 140185115293440 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.643947124481201, loss=3.0235655307769775
I0307 23:12:30.786775 140185199154944 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.3771865367889404, loss=2.953709602355957
I0307 23:15:07.393843 140341280416960 spec.py:321] Evaluating on the training split.
I0307 23:15:17.989808 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 23:15:48.213155 140341280416960 spec.py:349] Evaluating on the test split.
I0307 23:15:49.983891 140341280416960 submission_runner.py:469] Time since start: 50699.44s, 	Step: 98761, 	{'train/accuracy': 0.7974728941917419, 'train/loss': 1.007771372795105, 'validation/accuracy': 0.7041999697685242, 'validation/loss': 1.407898187637329, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 2.0709941387176514, 'test/num_examples': 10000, 'score': 46480.6161339283, 'total_duration': 50699.43836903572, 'accumulated_submission_time': 46480.6161339283, 'accumulated_eval_time': 4197.305947065353, 'accumulated_logging_time': 10.191502094268799}
I0307 23:15:50.026175 140185115293440 logging_writer.py:48] [98761] accumulated_eval_time=4197.31, accumulated_logging_time=10.1915, accumulated_submission_time=46480.6, global_step=98761, preemption_count=0, score=46480.6, test/accuracy=0.5753, test/loss=2.07099, test/num_examples=10000, total_duration=50699.4, train/accuracy=0.797473, train/loss=1.00777, validation/accuracy=0.7042, validation/loss=1.4079, validation/num_examples=50000
I0307 23:17:28.629864 140185199154944 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.7682249546051025, loss=3.029362916946411
2025-03-07 23:18:05.983893: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:19:31.886699 140185115293440 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.467263698577881, loss=2.9919323921203613
I0307 23:21:25.682742 140185199154944 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.5224392414093018, loss=2.9444124698638916
I0307 23:23:51.484773 140185115293440 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.4223828315734863, loss=2.9349417686462402
I0307 23:24:20.418815 140341280416960 spec.py:321] Evaluating on the training split.
I0307 23:24:32.722389 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 23:24:55.218580 140341280416960 spec.py:349] Evaluating on the test split.
I0307 23:24:56.962710 140341280416960 submission_runner.py:469] Time since start: 51246.42s, 	Step: 99121, 	{'train/accuracy': 0.7800741195678711, 'train/loss': 1.090338945388794, 'validation/accuracy': 0.694599986076355, 'validation/loss': 1.46392822265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 2.115920305252075, 'test/num_examples': 10000, 'score': 46990.95895695686, 'total_duration': 51246.417199611664, 'accumulated_submission_time': 46990.95895695686, 'accumulated_eval_time': 4233.849811077118, 'accumulated_logging_time': 10.243295907974243}
I0307 23:24:57.000511 140185199154944 logging_writer.py:48] [99121] accumulated_eval_time=4233.85, accumulated_logging_time=10.2433, accumulated_submission_time=46991, global_step=99121, preemption_count=0, score=46991, test/accuracy=0.5632, test/loss=2.11592, test/num_examples=10000, total_duration=51246.4, train/accuracy=0.780074, train/loss=1.09034, validation/accuracy=0.6946, validation/loss=1.46393, validation/num_examples=50000
I0307 23:26:43.160540 140185115293440 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.5194458961486816, loss=2.9359548091888428
I0307 23:30:20.114975 140185199154944 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.502312421798706, loss=2.943953514099121
I0307 23:33:27.870002 140341280416960 spec.py:321] Evaluating on the training split.
I0307 23:33:37.960913 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 23:34:09.793423 140341280416960 spec.py:349] Evaluating on the test split.
I0307 23:34:11.534491 140341280416960 submission_runner.py:469] Time since start: 51800.99s, 	Step: 99374, 	{'train/accuracy': 0.7768654227256775, 'train/loss': 1.0770403146743774, 'validation/accuracy': 0.6903600096702576, 'validation/loss': 1.4673442840576172, 'validation/num_examples': 50000, 'test/accuracy': 0.5656000375747681, 'test/loss': 2.115102529525757, 'test/num_examples': 10000, 'score': 47501.763241291046, 'total_duration': 51800.98893880844, 'accumulated_submission_time': 47501.763241291046, 'accumulated_eval_time': 4277.514222621918, 'accumulated_logging_time': 10.318831205368042}
I0307 23:34:11.559282 140185115293440 logging_writer.py:48] [99374] accumulated_eval_time=4277.51, accumulated_logging_time=10.3188, accumulated_submission_time=47501.8, global_step=99374, preemption_count=0, score=47501.8, test/accuracy=0.5656, test/loss=2.1151, test/num_examples=10000, total_duration=51801, train/accuracy=0.776865, train/loss=1.07704, validation/accuracy=0.69036, validation/loss=1.46734, validation/num_examples=50000
I0307 23:35:48.686171 140185199154944 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.6917269229888916, loss=2.9709107875823975
I0307 23:39:19.749487 140185115293440 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.665156364440918, loss=2.951974391937256
I0307 23:41:38.584668 140185199154944 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.4172682762145996, loss=2.96947979927063
I0307 23:42:41.554417 140341280416960 spec.py:321] Evaluating on the training split.
I0307 23:42:52.332489 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 23:43:14.893904 140341280416960 spec.py:349] Evaluating on the test split.
I0307 23:43:16.636581 140341280416960 submission_runner.py:469] Time since start: 52346.09s, 	Step: 99667, 	{'train/accuracy': 0.7882851958274841, 'train/loss': 1.040691614151001, 'validation/accuracy': 0.6994199752807617, 'validation/loss': 1.4258712530136108, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 2.086155414581299, 'test/num_examples': 10000, 'score': 48011.71732091904, 'total_duration': 52346.09106040001, 'accumulated_submission_time': 48011.71732091904, 'accumulated_eval_time': 4312.596350669861, 'accumulated_logging_time': 10.352495431900024}
I0307 23:43:16.660537 140185115293440 logging_writer.py:48] [99667] accumulated_eval_time=4312.6, accumulated_logging_time=10.3525, accumulated_submission_time=48011.7, global_step=99667, preemption_count=0, score=48011.7, test/accuracy=0.5719, test/loss=2.08616, test/num_examples=10000, total_duration=52346.1, train/accuracy=0.788285, train/loss=1.04069, validation/accuracy=0.69942, validation/loss=1.42587, validation/num_examples=50000
I0307 23:43:35.467404 140185199154944 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.528353214263916, loss=2.944149971008301
I0307 23:44:37.590307 140185115293440 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.42820405960083, loss=2.9903833866119385
I0307 23:45:50.702869 140185199154944 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.5130820274353027, loss=2.938413619995117
I0307 23:46:51.918201 140185115293440 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.3223509788513184, loss=2.8605329990386963
I0307 23:47:54.240461 140185199154944 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.446890115737915, loss=2.9082884788513184
I0307 23:48:53.186503 140185115293440 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.3852498531341553, loss=2.9607911109924316
I0307 23:49:51.791465 140185199154944 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.4121575355529785, loss=2.969712734222412
I0307 23:51:11.190226 140185115293440 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.4907140731811523, loss=2.927443742752075
I0307 23:51:47.124793 140341280416960 spec.py:321] Evaluating on the training split.
I0307 23:51:58.054454 140341280416960 spec.py:333] Evaluating on the validation split.
I0307 23:52:20.695852 140341280416960 spec.py:349] Evaluating on the test split.
I0307 23:52:22.456717 140341280416960 submission_runner.py:469] Time since start: 52891.91s, 	Step: 100434, 	{'train/accuracy': 0.7889827489852905, 'train/loss': 1.0474351644515991, 'validation/accuracy': 0.6964199542999268, 'validation/loss': 1.4476568698883057, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 2.072035789489746, 'test/num_examples': 10000, 'score': 48522.0870552063, 'total_duration': 52891.91120219231, 'accumulated_submission_time': 48522.0870552063, 'accumulated_eval_time': 4347.9282331466675, 'accumulated_logging_time': 10.385234832763672}
I0307 23:52:22.512756 140185199154944 logging_writer.py:48] [100434] accumulated_eval_time=4347.93, accumulated_logging_time=10.3852, accumulated_submission_time=48522.1, global_step=100434, preemption_count=0, score=48522.1, test/accuracy=0.5722, test/loss=2.07204, test/num_examples=10000, total_duration=52891.9, train/accuracy=0.788983, train/loss=1.04744, validation/accuracy=0.69642, validation/loss=1.44766, validation/num_examples=50000
I0307 23:53:25.159841 140185115293440 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.4790730476379395, loss=2.93170428276062
I0307 23:55:14.506284 140185199154944 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.5691511631011963, loss=2.952636241912842
I0307 23:57:04.156138 140185115293440 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.4624226093292236, loss=2.9576644897460938
I0307 23:58:54.214175 140185199154944 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.8053200244903564, loss=2.92771053314209
I0308 00:00:42.923065 140185115293440 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.455528497695923, loss=2.9453320503234863
I0308 00:00:52.637456 140341280416960 spec.py:321] Evaluating on the training split.
I0308 00:01:03.433718 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 00:01:25.670695 140341280416960 spec.py:349] Evaluating on the test split.
I0308 00:01:27.440598 140341280416960 submission_runner.py:469] Time since start: 53436.90s, 	Step: 100910, 	{'train/accuracy': 0.7965361475944519, 'train/loss': 0.994647204875946, 'validation/accuracy': 0.7077999711036682, 'validation/loss': 1.3743327856063843, 'validation/num_examples': 50000, 'test/accuracy': 0.5797000527381897, 'test/loss': 2.0325915813446045, 'test/num_examples': 10000, 'score': 49032.148569107056, 'total_duration': 53436.89508295059, 'accumulated_submission_time': 49032.148569107056, 'accumulated_eval_time': 4382.731330156326, 'accumulated_logging_time': 10.450151920318604}
I0308 00:01:27.484828 140185199154944 logging_writer.py:48] [100910] accumulated_eval_time=4382.73, accumulated_logging_time=10.4502, accumulated_submission_time=49032.1, global_step=100910, preemption_count=0, score=49032.1, test/accuracy=0.5797, test/loss=2.03259, test/num_examples=10000, total_duration=53436.9, train/accuracy=0.796536, train/loss=0.994647, validation/accuracy=0.7078, validation/loss=1.37433, validation/num_examples=50000
I0308 00:02:15.046700 140185115293440 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.685734510421753, loss=2.9747791290283203
I0308 00:03:20.222007 140185199154944 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.5352134704589844, loss=2.980466842651367
I0308 00:04:20.446443 140185115293440 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.461324691772461, loss=2.924440622329712
I0308 00:05:17.327094 140185199154944 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.5381717681884766, loss=2.9415698051452637
I0308 00:06:08.855481 140185115293440 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.4984447956085205, loss=2.969524383544922
I0308 00:06:58.846568 140185199154944 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.6156039237976074, loss=2.965059518814087
I0308 00:07:48.219625 140185115293440 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.362605571746826, loss=2.909222364425659
I0308 00:09:41.811396 140185199154944 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.7428674697875977, loss=2.98563289642334
I0308 00:09:58.814948 140341280416960 spec.py:321] Evaluating on the training split.
I0308 00:10:10.432526 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 00:10:34.281620 140341280416960 spec.py:349] Evaluating on the test split.
I0308 00:10:36.059897 140341280416960 submission_runner.py:469] Time since start: 53985.51s, 	Step: 101709, 	{'train/accuracy': 0.7998843789100647, 'train/loss': 0.9931908845901489, 'validation/accuracy': 0.7041400074958801, 'validation/loss': 1.399848222732544, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 2.049638509750366, 'test/num_examples': 10000, 'score': 49543.38068342209, 'total_duration': 53985.514385938644, 'accumulated_submission_time': 49543.38068342209, 'accumulated_eval_time': 4419.976241350174, 'accumulated_logging_time': 10.503279685974121}
I0308 00:10:36.094794 140185115293440 logging_writer.py:48] [101709] accumulated_eval_time=4419.98, accumulated_logging_time=10.5033, accumulated_submission_time=49543.4, global_step=101709, preemption_count=0, score=49543.4, test/accuracy=0.5751, test/loss=2.04964, test/num_examples=10000, total_duration=53985.5, train/accuracy=0.799884, train/loss=0.993191, validation/accuracy=0.70414, validation/loss=1.39985, validation/num_examples=50000
I0308 00:13:34.000901 140185199154944 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.449636220932007, loss=2.9427595138549805
I0308 00:17:06.976856 140185115293440 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.569577693939209, loss=2.968095064163208
I0308 00:19:07.673100 140341280416960 spec.py:321] Evaluating on the training split.
I0308 00:19:18.649178 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 00:19:44.528903 140341280416960 spec.py:349] Evaluating on the test split.
I0308 00:19:46.303927 140341280416960 submission_runner.py:469] Time since start: 54535.76s, 	Step: 101957, 	{'train/accuracy': 0.7979312539100647, 'train/loss': 0.9683020114898682, 'validation/accuracy': 0.7083199620246887, 'validation/loss': 1.3619030714035034, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 2.017000675201416, 'test/num_examples': 10000, 'score': 50054.9244158268, 'total_duration': 54535.75826764107, 'accumulated_submission_time': 50054.9244158268, 'accumulated_eval_time': 4458.606889486313, 'accumulated_logging_time': 10.546168327331543}
I0308 00:19:46.329005 140185199154944 logging_writer.py:48] [101957] accumulated_eval_time=4458.61, accumulated_logging_time=10.5462, accumulated_submission_time=50054.9, global_step=101957, preemption_count=0, score=50054.9, test/accuracy=0.5775, test/loss=2.017, test/num_examples=10000, total_duration=54535.8, train/accuracy=0.797931, train/loss=0.968302, validation/accuracy=0.70832, validation/loss=1.3619, validation/num_examples=50000
I0308 00:21:03.629558 140185115293440 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.713113307952881, loss=3.0066475868225098
I0308 00:26:44.532159 140185199154944 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.5286009311676025, loss=2.9630420207977295
I0308 00:28:18.162752 140341280416960 spec.py:321] Evaluating on the training split.
I0308 00:28:28.411300 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 00:29:03.125528 140341280416960 spec.py:349] Evaluating on the test split.
I0308 00:29:04.881359 140341280416960 submission_runner.py:469] Time since start: 55094.34s, 	Step: 102123, 	{'train/accuracy': 0.7993662357330322, 'train/loss': 0.9966561794281006, 'validation/accuracy': 0.7048400044441223, 'validation/loss': 1.4022161960601807, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 2.023531436920166, 'test/num_examples': 10000, 'score': 50566.73064374924, 'total_duration': 55094.33583045006, 'accumulated_submission_time': 50566.73064374924, 'accumulated_eval_time': 4505.325449228287, 'accumulated_logging_time': 10.580280065536499}
I0308 00:29:04.906757 140185115293440 logging_writer.py:48] [102123] accumulated_eval_time=4505.33, accumulated_logging_time=10.5803, accumulated_submission_time=50566.7, global_step=102123, preemption_count=0, score=50566.7, test/accuracy=0.5837, test/loss=2.02353, test/num_examples=10000, total_duration=55094.3, train/accuracy=0.799366, train/loss=0.996656, validation/accuracy=0.70484, validation/loss=1.40222, validation/num_examples=50000
I0308 00:32:49.649347 140185199154944 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.595588445663452, loss=2.9763827323913574
I0308 00:34:13.127667 140185115293440 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.5851166248321533, loss=2.954200029373169
I0308 00:35:48.754414 140185199154944 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.850954532623291, loss=2.9665820598602295
I0308 00:37:09.584801 140185115293440 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.5159084796905518, loss=2.9427719116210938
I0308 00:37:35.082275 140341280416960 spec.py:321] Evaluating on the training split.
I0308 00:37:46.473447 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 00:38:11.098741 140341280416960 spec.py:349] Evaluating on the test split.
I0308 00:38:12.846881 140341280416960 submission_runner.py:469] Time since start: 55642.30s, 	Step: 102533, 	{'train/accuracy': 0.8286033272743225, 'train/loss': 0.8490793704986572, 'validation/accuracy': 0.7035999894142151, 'validation/loss': 1.3774374723434448, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 2.0284581184387207, 'test/num_examples': 10000, 'score': 51076.85154104233, 'total_duration': 55642.3013548851, 'accumulated_submission_time': 51076.85154104233, 'accumulated_eval_time': 4543.090004444122, 'accumulated_logging_time': 10.614437580108643}
I0308 00:38:12.975330 140185199154944 logging_writer.py:48] [102533] accumulated_eval_time=4543.09, accumulated_logging_time=10.6144, accumulated_submission_time=51076.9, global_step=102533, preemption_count=0, score=51076.9, test/accuracy=0.5767, test/loss=2.02846, test/num_examples=10000, total_duration=55642.3, train/accuracy=0.828603, train/loss=0.849079, validation/accuracy=0.7036, validation/loss=1.37744, validation/num_examples=50000
I0308 00:40:24.782318 140185115293440 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.5543277263641357, loss=2.9288275241851807
I0308 00:43:59.259467 140185199154944 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.553800582885742, loss=2.997774124145508
I0308 00:46:44.187747 140341280416960 spec.py:321] Evaluating on the training split.
I0308 00:46:56.647510 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 00:47:22.167231 140341280416960 spec.py:349] Evaluating on the test split.
I0308 00:47:23.941431 140341280416960 submission_runner.py:469] Time since start: 56193.40s, 	Step: 102778, 	{'train/accuracy': 0.8122807741165161, 'train/loss': 0.9185537695884705, 'validation/accuracy': 0.7065399885177612, 'validation/loss': 1.3766194581985474, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 2.0210719108581543, 'test/num_examples': 10000, 'score': 51588.02870750427, 'total_duration': 56193.39591169357, 'accumulated_submission_time': 51588.02870750427, 'accumulated_eval_time': 4582.843652009964, 'accumulated_logging_time': 10.751986026763916}
I0308 00:47:23.966485 140185115293440 logging_writer.py:48] [102778] accumulated_eval_time=4582.84, accumulated_logging_time=10.752, accumulated_submission_time=51588, global_step=102778, preemption_count=0, score=51588, test/accuracy=0.5795, test/loss=2.02107, test/num_examples=10000, total_duration=56193.4, train/accuracy=0.812281, train/loss=0.918554, validation/accuracy=0.70654, validation/loss=1.37662, validation/num_examples=50000
I0308 00:48:19.366218 140185199154944 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.4714276790618896, loss=2.9597315788269043
I0308 00:55:27.875751 140185115293440 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.4933550357818604, loss=2.9356672763824463
I0308 00:55:58.232852 140341280416960 spec.py:321] Evaluating on the training split.
I0308 00:56:08.549948 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 00:56:43.778980 140341280416960 spec.py:349] Evaluating on the test split.
I0308 00:56:45.546394 140341280416960 submission_runner.py:469] Time since start: 56755.00s, 	Step: 102908, 	{'train/accuracy': 0.8042490482330322, 'train/loss': 0.9769500494003296, 'validation/accuracy': 0.7034800052642822, 'validation/loss': 1.4117834568023682, 'validation/num_examples': 50000, 'test/accuracy': 0.573900043964386, 'test/loss': 2.0533857345581055, 'test/num_examples': 10000, 'score': 52102.27171206474, 'total_duration': 56755.000868320465, 'accumulated_submission_time': 52102.27171206474, 'accumulated_eval_time': 4630.157147884369, 'accumulated_logging_time': 10.786733150482178}
I0308 00:56:45.571868 140185199154944 logging_writer.py:48] [102908] accumulated_eval_time=4630.16, accumulated_logging_time=10.7867, accumulated_submission_time=52102.3, global_step=102908, preemption_count=0, score=52102.3, test/accuracy=0.5739, test/loss=2.05339, test/num_examples=10000, total_duration=56755, train/accuracy=0.804249, train/loss=0.97695, validation/accuracy=0.70348, validation/loss=1.41178, validation/num_examples=50000
I0308 01:00:01.458938 140185115293440 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.5447022914886475, loss=2.966353416442871
I0308 01:02:26.481425 140185199154944 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.7554101943969727, loss=3.026458263397217
I0308 01:04:50.702289 140185115293440 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.467378616333008, loss=2.924255609512329
I0308 01:05:16.306612 140341280416960 spec.py:321] Evaluating on the training split.
I0308 01:05:27.235750 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 01:05:50.460789 140341280416960 spec.py:349] Evaluating on the test split.
I0308 01:05:52.161840 140341280416960 submission_runner.py:469] Time since start: 57301.62s, 	Step: 103219, 	{'train/accuracy': 0.8055444955825806, 'train/loss': 0.9517120122909546, 'validation/accuracy': 0.7067999839782715, 'validation/loss': 1.373279333114624, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 2.027841806411743, 'test/num_examples': 10000, 'score': 52612.96303200722, 'total_duration': 57301.61632394791, 'accumulated_submission_time': 52612.96303200722, 'accumulated_eval_time': 4666.012334823608, 'accumulated_logging_time': 10.821454524993896}
I0308 01:05:52.230190 140185199154944 logging_writer.py:48] [103219] accumulated_eval_time=4666.01, accumulated_logging_time=10.8215, accumulated_submission_time=52613, global_step=103219, preemption_count=0, score=52613, test/accuracy=0.5764, test/loss=2.02784, test/num_examples=10000, total_duration=57301.6, train/accuracy=0.805544, train/loss=0.951712, validation/accuracy=0.7068, validation/loss=1.37328, validation/num_examples=50000
I0308 01:07:34.359157 140185115293440 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.421672821044922, loss=2.875670909881592
I0308 01:09:57.427648 140185199154944 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.530121326446533, loss=2.952470302581787
I0308 01:12:23.042169 140185115293440 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.3434882164001465, loss=2.9331541061401367
I0308 01:14:14.499875 140185199154944 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.351269245147705, loss=2.893078088760376
I0308 01:14:22.506054 140341280416960 spec.py:321] Evaluating on the training split.
I0308 01:14:34.116669 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 01:14:56.185388 140341280416960 spec.py:349] Evaluating on the test split.
I0308 01:14:57.940010 140341280416960 submission_runner.py:469] Time since start: 57847.39s, 	Step: 103608, 	{'train/accuracy': 0.7960180044174194, 'train/loss': 0.9998266100883484, 'validation/accuracy': 0.7076599597930908, 'validation/loss': 1.3930076360702515, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 2.0357723236083984, 'test/num_examples': 10000, 'score': 53123.158005952835, 'total_duration': 57847.39448928833, 'accumulated_submission_time': 53123.158005952835, 'accumulated_eval_time': 4701.446239233017, 'accumulated_logging_time': 10.927058935165405}
I0308 01:14:57.993803 140185115293440 logging_writer.py:48] [103608] accumulated_eval_time=4701.45, accumulated_logging_time=10.9271, accumulated_submission_time=53123.2, global_step=103608, preemption_count=0, score=53123.2, test/accuracy=0.5811, test/loss=2.03577, test/num_examples=10000, total_duration=57847.4, train/accuracy=0.796018, train/loss=0.999827, validation/accuracy=0.70766, validation/loss=1.39301, validation/num_examples=50000
I0308 01:16:31.462241 140185199154944 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.5359342098236084, loss=2.9611124992370605
I0308 01:18:22.133357 140185115293440 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.486938714981079, loss=2.997126817703247
I0308 01:20:14.425661 140185199154944 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.4705793857574463, loss=2.9454140663146973
I0308 01:22:04.820696 140185115293440 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.749037027359009, loss=3.0269784927368164
I0308 01:23:28.899168 140341280416960 spec.py:321] Evaluating on the training split.
I0308 01:23:39.783880 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 01:24:01.718725 140341280416960 spec.py:349] Evaluating on the test split.
I0308 01:24:03.568032 140341280416960 submission_runner.py:469] Time since start: 58393.02s, 	Step: 104077, 	{'train/accuracy': 0.818379282951355, 'train/loss': 0.9330296516418457, 'validation/accuracy': 0.6978799700737, 'validation/loss': 1.4370510578155518, 'validation/num_examples': 50000, 'test/accuracy': 0.5703000426292419, 'test/loss': 2.093350410461426, 'test/num_examples': 10000, 'score': 53634.00277853012, 'total_duration': 58393.02249622345, 'accumulated_submission_time': 53634.00277853012, 'accumulated_eval_time': 4736.1150550842285, 'accumulated_logging_time': 10.989583253860474}
I0308 01:24:03.604903 140185199154944 logging_writer.py:48] [104077] accumulated_eval_time=4736.12, accumulated_logging_time=10.9896, accumulated_submission_time=53634, global_step=104077, preemption_count=0, score=53634, test/accuracy=0.5703, test/loss=2.09335, test/num_examples=10000, total_duration=58393, train/accuracy=0.818379, train/loss=0.93303, validation/accuracy=0.69788, validation/loss=1.43705, validation/num_examples=50000
I0308 01:24:17.282207 140185115293440 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.5623199939727783, loss=2.941129446029663
I0308 01:30:56.568032 140185199154944 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.5369088649749756, loss=2.867169141769409
I0308 01:32:36.202094 140341280416960 spec.py:321] Evaluating on the training split.
I0308 01:32:47.882607 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 01:33:14.542876 140341280416960 spec.py:349] Evaluating on the test split.
I0308 01:33:16.314881 140341280416960 submission_runner.py:469] Time since start: 58945.77s, 	Step: 104224, 	{'train/accuracy': 0.8160873651504517, 'train/loss': 0.9032125473022461, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.3705179691314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 2.0048046112060547, 'test/num_examples': 10000, 'score': 54146.575229644775, 'total_duration': 58945.76935505867, 'accumulated_submission_time': 54146.575229644775, 'accumulated_eval_time': 4776.227821826935, 'accumulated_logging_time': 11.03572392463684}
I0308 01:33:16.338279 140185115293440 logging_writer.py:48] [104224] accumulated_eval_time=4776.23, accumulated_logging_time=11.0357, accumulated_submission_time=54146.6, global_step=104224, preemption_count=0, score=54146.6, test/accuracy=0.5836, test/loss=2.0048, test/num_examples=10000, total_duration=58945.8, train/accuracy=0.816087, train/loss=0.903213, validation/accuracy=0.70792, validation/loss=1.37052, validation/num_examples=50000
I0308 01:38:25.379521 140185199154944 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.4932782649993896, loss=2.906926155090332
I0308 01:41:47.086283 140341280416960 spec.py:321] Evaluating on the training split.
I0308 01:41:57.283075 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 01:42:25.287654 140341280416960 spec.py:349] Evaluating on the test split.
I0308 01:42:27.036194 140341280416960 submission_runner.py:469] Time since start: 59496.49s, 	Step: 104348, 	{'train/accuracy': 0.8147321343421936, 'train/loss': 0.9399213194847107, 'validation/accuracy': 0.7075999975204468, 'validation/loss': 1.3960484266281128, 'validation/num_examples': 50000, 'test/accuracy': 0.5830000042915344, 'test/loss': 2.030853509902954, 'test/num_examples': 10000, 'score': 54657.30204105377, 'total_duration': 59496.49066925049, 'accumulated_submission_time': 54657.30204105377, 'accumulated_eval_time': 4816.177689790726, 'accumulated_logging_time': 11.067471981048584}
I0308 01:42:27.061547 140185115293440 logging_writer.py:48] [104348] accumulated_eval_time=4816.18, accumulated_logging_time=11.0675, accumulated_submission_time=54657.3, global_step=104348, preemption_count=0, score=54657.3, test/accuracy=0.583, test/loss=2.03085, test/num_examples=10000, total_duration=59496.5, train/accuracy=0.814732, train/loss=0.939921, validation/accuracy=0.7076, validation/loss=1.39605, validation/num_examples=50000
I0308 01:45:55.910305 140185199154944 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.5640299320220947, loss=2.9713594913482666
I0308 01:51:01.093812 140341280416960 spec.py:321] Evaluating on the training split.
I0308 01:51:11.300798 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 01:51:44.658316 140341280416960 spec.py:349] Evaluating on the test split.
I0308 01:51:46.419285 140341280416960 submission_runner.py:469] Time since start: 60055.87s, 	Step: 104472, 	{'train/accuracy': 0.81253981590271, 'train/loss': 0.9260028004646301, 'validation/accuracy': 0.706119954586029, 'validation/loss': 1.372018575668335, 'validation/num_examples': 50000, 'test/accuracy': 0.5831000208854675, 'test/loss': 1.9958629608154297, 'test/num_examples': 10000, 'score': 55171.312621831894, 'total_duration': 60055.873728990555, 'accumulated_submission_time': 55171.312621831894, 'accumulated_eval_time': 4861.5030863285065, 'accumulated_logging_time': 11.101954221725464}
I0308 01:51:46.444820 140185115293440 logging_writer.py:48] [104472] accumulated_eval_time=4861.5, accumulated_logging_time=11.102, accumulated_submission_time=55171.3, global_step=104472, preemption_count=0, score=55171.3, test/accuracy=0.5831, test/loss=1.99586, test/num_examples=10000, total_duration=60055.9, train/accuracy=0.81254, train/loss=0.926003, validation/accuracy=0.70612, validation/loss=1.37202, validation/num_examples=50000
I0308 01:53:30.952062 140185199154944 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.6956655979156494, loss=2.9447104930877686
I0308 02:00:16.474502 140341280416960 spec.py:321] Evaluating on the training split.
I0308 02:00:26.644389 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 02:00:57.015007 140341280416960 spec.py:349] Evaluating on the test split.
I0308 02:00:58.778873 140341280416960 submission_runner.py:469] Time since start: 60608.23s, 	Step: 104596, 	{'train/accuracy': 0.8046875, 'train/loss': 0.9457805156707764, 'validation/accuracy': 0.7028599977493286, 'validation/loss': 1.3919482231140137, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 2.0197806358337402, 'test/num_examples': 10000, 'score': 55681.32037258148, 'total_duration': 60608.233352184296, 'accumulated_submission_time': 55681.32037258148, 'accumulated_eval_time': 4903.807421445847, 'accumulated_logging_time': 11.136282444000244}
I0308 02:00:58.804808 140185115293440 logging_writer.py:48] [104596] accumulated_eval_time=4903.81, accumulated_logging_time=11.1363, accumulated_submission_time=55681.3, global_step=104596, preemption_count=0, score=55681.3, test/accuracy=0.5815, test/loss=2.01978, test/num_examples=10000, total_duration=60608.2, train/accuracy=0.804688, train/loss=0.945781, validation/accuracy=0.70286, validation/loss=1.39195, validation/num_examples=50000
I0308 02:01:00.725061 140185199154944 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.869549036026001, loss=2.965355396270752
I0308 02:08:06.460511 140185115293440 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.582534074783325, loss=2.9012579917907715
I0308 02:09:29.497362 140341280416960 spec.py:321] Evaluating on the training split.
I0308 02:09:40.564973 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 02:10:02.382812 140341280416960 spec.py:349] Evaluating on the test split.
I0308 02:10:04.129042 140341280416960 submission_runner.py:469] Time since start: 61153.58s, 	Step: 104773, 	{'train/accuracy': 0.8040298223495483, 'train/loss': 0.9433786869049072, 'validation/accuracy': 0.7065399885177612, 'validation/loss': 1.3692487478256226, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.9958858489990234, 'test/num_examples': 10000, 'score': 56191.984790086746, 'total_duration': 61153.583532333374, 'accumulated_submission_time': 56191.984790086746, 'accumulated_eval_time': 4938.43906712532, 'accumulated_logging_time': 11.170869588851929}
I0308 02:10:04.170376 140185199154944 logging_writer.py:48] [104773] accumulated_eval_time=4938.44, accumulated_logging_time=11.1709, accumulated_submission_time=56192, global_step=104773, preemption_count=0, score=56192, test/accuracy=0.5779, test/loss=1.99589, test/num_examples=10000, total_duration=61153.6, train/accuracy=0.80403, train/loss=0.943379, validation/accuracy=0.70654, validation/loss=1.36925, validation/num_examples=50000
I0308 02:10:19.161299 140185115293440 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.559370756149292, loss=2.938133716583252
I0308 02:11:59.963549 140185199154944 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.4228086471557617, loss=2.900179862976074
I0308 02:13:30.568011 140185115293440 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.6669387817382812, loss=2.915601968765259
2025-03-08 02:14:42.137280: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:15:02.513496 140185199154944 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.436378002166748, loss=2.923034191131592
I0308 02:16:32.308162 140185115293440 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.5909574031829834, loss=2.9854817390441895
I0308 02:18:01.711740 140185199154944 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.738245964050293, loss=2.9664363861083984
I0308 02:18:34.923392 140341280416960 spec.py:321] Evaluating on the training split.
I0308 02:18:45.899012 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 02:19:06.281472 140341280416960 spec.py:349] Evaluating on the test split.
I0308 02:19:08.032177 140341280416960 submission_runner.py:469] Time since start: 61697.49s, 	Step: 105338, 	{'train/accuracy': 0.7976921200752258, 'train/loss': 0.9928430914878845, 'validation/accuracy': 0.706779956817627, 'validation/loss': 1.3878968954086304, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 2.0457255840301514, 'test/num_examples': 10000, 'score': 56702.651848077774, 'total_duration': 61697.4866399765, 'accumulated_submission_time': 56702.651848077774, 'accumulated_eval_time': 4971.547793626785, 'accumulated_logging_time': 11.23337984085083}
I0308 02:19:08.068494 140185115293440 logging_writer.py:48] [105338] accumulated_eval_time=4971.55, accumulated_logging_time=11.2334, accumulated_submission_time=56702.7, global_step=105338, preemption_count=0, score=56702.7, test/accuracy=0.5726, test/loss=2.04573, test/num_examples=10000, total_duration=61697.5, train/accuracy=0.797692, train/loss=0.992843, validation/accuracy=0.70678, validation/loss=1.3879, validation/num_examples=50000
I0308 02:21:38.338618 140185199154944 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.5301690101623535, loss=2.935267210006714
I0308 02:27:38.847545 140341280416960 spec.py:321] Evaluating on the training split.
I0308 02:27:49.143597 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 02:28:16.238687 140341280416960 spec.py:349] Evaluating on the test split.
I0308 02:28:17.999621 140341280416960 submission_runner.py:469] Time since start: 62247.45s, 	Step: 105485, 	{'train/accuracy': 0.7901387214660645, 'train/loss': 1.0377357006072998, 'validation/accuracy': 0.6972399950027466, 'validation/loss': 1.4346237182617188, 'validation/num_examples': 50000, 'test/accuracy': 0.5764999985694885, 'test/loss': 2.07525634765625, 'test/num_examples': 10000, 'score': 57213.37969374657, 'total_duration': 62247.45408940315, 'accumulated_submission_time': 57213.37969374657, 'accumulated_eval_time': 5010.699824810028, 'accumulated_logging_time': 11.304818630218506}
I0308 02:28:18.024097 140185115293440 logging_writer.py:48] [105485] accumulated_eval_time=5010.7, accumulated_logging_time=11.3048, accumulated_submission_time=57213.4, global_step=105485, preemption_count=0, score=57213.4, test/accuracy=0.5765, test/loss=2.07526, test/num_examples=10000, total_duration=62247.5, train/accuracy=0.790139, train/loss=1.03774, validation/accuracy=0.69724, validation/loss=1.43462, validation/num_examples=50000
I0308 02:29:07.315291 140185199154944 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.7499399185180664, loss=2.936223030090332
I0308 02:36:16.148267 140185115293440 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.4361019134521484, loss=2.8932600021362305
I0308 02:36:50.287189 140341280416960 spec.py:321] Evaluating on the training split.
I0308 02:37:00.443580 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 02:37:28.263731 140341280416960 spec.py:349] Evaluating on the test split.
I0308 02:37:29.976048 140341280416960 submission_runner.py:469] Time since start: 62799.43s, 	Step: 105609, 	{'train/accuracy': 0.7908163070678711, 'train/loss': 1.0199248790740967, 'validation/accuracy': 0.7014999985694885, 'validation/loss': 1.4115504026412964, 'validation/num_examples': 50000, 'test/accuracy': 0.5657000541687012, 'test/loss': 2.0867257118225098, 'test/num_examples': 10000, 'score': 57725.62155151367, 'total_duration': 62799.43053650856, 'accumulated_submission_time': 57725.62155151367, 'accumulated_eval_time': 5050.3886568546295, 'accumulated_logging_time': 11.337771654129028}
I0308 02:37:29.997376 140185199154944 logging_writer.py:48] [105609] accumulated_eval_time=5050.39, accumulated_logging_time=11.3378, accumulated_submission_time=57725.6, global_step=105609, preemption_count=0, score=57725.6, test/accuracy=0.5657, test/loss=2.08673, test/num_examples=10000, total_duration=62799.4, train/accuracy=0.790816, train/loss=1.01992, validation/accuracy=0.7015, validation/loss=1.41155, validation/num_examples=50000
I0308 02:43:41.969261 140185115293440 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.6830050945281982, loss=2.8731534481048584
I0308 02:46:02.381392 140341280416960 spec.py:321] Evaluating on the training split.
I0308 02:46:12.653053 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 02:46:50.765661 140341280416960 spec.py:349] Evaluating on the test split.
I0308 02:46:52.535389 140341280416960 submission_runner.py:469] Time since start: 63361.99s, 	Step: 105734, 	{'train/accuracy': 0.8369738459587097, 'train/loss': 0.8548222780227661, 'validation/accuracy': 0.7094999551773071, 'validation/loss': 1.3812932968139648, 'validation/num_examples': 50000, 'test/accuracy': 0.5818000435829163, 'test/loss': 2.0417425632476807, 'test/num_examples': 10000, 'score': 58237.98412680626, 'total_duration': 63361.98986816406, 'accumulated_submission_time': 58237.98412680626, 'accumulated_eval_time': 5100.542615890503, 'accumulated_logging_time': 11.366876602172852}
I0308 02:46:52.574285 140185199154944 logging_writer.py:48] [105734] accumulated_eval_time=5100.54, accumulated_logging_time=11.3669, accumulated_submission_time=58238, global_step=105734, preemption_count=0, score=58238, test/accuracy=0.5818, test/loss=2.04174, test/num_examples=10000, total_duration=63362, train/accuracy=0.836974, train/loss=0.854822, validation/accuracy=0.7095, validation/loss=1.38129, validation/num_examples=50000
I0308 02:51:17.339690 140185115293440 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.5215489864349365, loss=2.858452796936035
I0308 02:55:23.239315 140341280416960 spec.py:321] Evaluating on the training split.
I0308 02:55:34.081806 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 02:55:57.052915 140341280416960 spec.py:349] Evaluating on the test split.
I0308 02:55:58.806198 140341280416960 submission_runner.py:469] Time since start: 63908.26s, 	Step: 105896, 	{'train/accuracy': 0.8235411047935486, 'train/loss': 0.8850187659263611, 'validation/accuracy': 0.7075799703598022, 'validation/loss': 1.367797613143921, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 2.022705078125, 'test/num_examples': 10000, 'score': 58748.622324466705, 'total_duration': 63908.26063513756, 'accumulated_submission_time': 58748.622324466705, 'accumulated_eval_time': 5136.109422683716, 'accumulated_logging_time': 11.414541721343994}
I0308 02:55:58.831395 140185199154944 logging_writer.py:48] [105896] accumulated_eval_time=5136.11, accumulated_logging_time=11.4145, accumulated_submission_time=58748.6, global_step=105896, preemption_count=0, score=58748.6, test/accuracy=0.5768, test/loss=2.02271, test/num_examples=10000, total_duration=63908.3, train/accuracy=0.823541, train/loss=0.885019, validation/accuracy=0.70758, validation/loss=1.3678, validation/num_examples=50000
I0308 02:56:00.803231 140185115293440 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.61666202545166, loss=2.90759539604187
I0308 02:58:18.380515 140185199154944 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.53056001663208, loss=2.9527547359466553
I0308 02:59:58.669068 140185115293440 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.567495584487915, loss=2.967639923095703
I0308 03:01:36.224802 140185199154944 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.6776037216186523, loss=2.934196949005127
I0308 03:03:08.773287 140185115293440 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.6071367263793945, loss=2.9238197803497314
I0308 03:04:29.688625 140341280416960 spec.py:321] Evaluating on the training split.
I0308 03:04:40.423324 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 03:05:04.998345 140341280416960 spec.py:349] Evaluating on the test split.
I0308 03:05:06.747508 140341280416960 submission_runner.py:469] Time since start: 64456.20s, 	Step: 106363, 	{'train/accuracy': 0.8068598508834839, 'train/loss': 0.9442111849784851, 'validation/accuracy': 0.7069999575614929, 'validation/loss': 1.3726935386657715, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 2.015824556350708, 'test/num_examples': 10000, 'score': 59259.398359298706, 'total_duration': 64456.201979637146, 'accumulated_submission_time': 59259.398359298706, 'accumulated_eval_time': 5173.168258190155, 'accumulated_logging_time': 11.469578266143799}
I0308 03:05:06.805868 140185199154944 logging_writer.py:48] [106363] accumulated_eval_time=5173.17, accumulated_logging_time=11.4696, accumulated_submission_time=59259.4, global_step=106363, preemption_count=0, score=59259.4, test/accuracy=0.585, test/loss=2.01582, test/num_examples=10000, total_duration=64456.2, train/accuracy=0.80686, train/loss=0.944211, validation/accuracy=0.707, validation/loss=1.37269, validation/num_examples=50000
I0308 03:05:46.189258 140185115293440 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.3716681003570557, loss=2.9011178016662598
I0308 03:08:10.242686 140185199154944 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.4194374084472656, loss=2.9216604232788086
I0308 03:10:35.775306 140185115293440 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.630222797393799, loss=2.947017192840576
I0308 03:13:00.896874 140185199154944 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.4969727993011475, loss=2.844677448272705
I0308 03:13:37.490377 140341280416960 spec.py:321] Evaluating on the training split.
I0308 03:13:48.972068 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 03:14:10.947756 140341280416960 spec.py:349] Evaluating on the test split.
I0308 03:14:12.672496 140341280416960 submission_runner.py:469] Time since start: 65002.13s, 	Step: 106726, 	{'train/accuracy': 0.78910231590271, 'train/loss': 1.0440272092819214, 'validation/accuracy': 0.6992999911308289, 'validation/loss': 1.4296166896820068, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 2.081758737564087, 'test/num_examples': 10000, 'score': 59770.035422086716, 'total_duration': 65002.12698054314, 'accumulated_submission_time': 59770.035422086716, 'accumulated_eval_time': 5208.350335121155, 'accumulated_logging_time': 11.537103652954102}
I0308 03:14:12.724647 140185115293440 logging_writer.py:48] [106726] accumulated_eval_time=5208.35, accumulated_logging_time=11.5371, accumulated_submission_time=59770, global_step=106726, preemption_count=0, score=59770, test/accuracy=0.5715, test/loss=2.08176, test/num_examples=10000, total_duration=65002.1, train/accuracy=0.789102, train/loss=1.04403, validation/accuracy=0.6993, validation/loss=1.42962, validation/num_examples=50000
I0308 03:15:45.331537 140185199154944 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.5150022506713867, loss=2.870922088623047
I0308 03:18:10.115774 140185115293440 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.548207998275757, loss=2.863251209259033
I0308 03:20:34.411814 140185199154944 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.62070894241333, loss=2.907118082046509
I0308 03:22:42.900352 140341280416960 spec.py:321] Evaluating on the training split.
I0308 03:22:53.486265 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 03:23:16.644668 140341280416960 spec.py:349] Evaluating on the test split.
I0308 03:23:18.403453 140341280416960 submission_runner.py:469] Time since start: 65547.86s, 	Step: 107090, 	{'train/accuracy': 0.7955994606018066, 'train/loss': 0.9991288185119629, 'validation/accuracy': 0.6988199949264526, 'validation/loss': 1.4133193492889404, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 2.055209159851074, 'test/num_examples': 10000, 'score': 60280.16394472122, 'total_duration': 65547.85793828964, 'accumulated_submission_time': 60280.16394472122, 'accumulated_eval_time': 5243.8534054756165, 'accumulated_logging_time': 11.596826314926147}
I0308 03:23:18.479265 140185115293440 logging_writer.py:48] [107090] accumulated_eval_time=5243.85, accumulated_logging_time=11.5968, accumulated_submission_time=60280.2, global_step=107090, preemption_count=0, score=60280.2, test/accuracy=0.5748, test/loss=2.05521, test/num_examples=10000, total_duration=65547.9, train/accuracy=0.795599, train/loss=0.999129, validation/accuracy=0.69882, validation/loss=1.41332, validation/num_examples=50000
I0308 03:23:22.712645 140185199154944 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.6004769802093506, loss=2.9885988235473633
I0308 03:25:44.631761 140185115293440 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.6979072093963623, loss=2.8899011611938477
I0308 03:28:08.953912 140185199154944 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.60615611076355, loss=2.894113302230835
I0308 03:30:34.378417 140185115293440 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.5339930057525635, loss=2.997661590576172
I0308 03:31:49.671967 140341280416960 spec.py:321] Evaluating on the training split.
I0308 03:32:00.373502 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 03:32:23.494858 140341280416960 spec.py:349] Evaluating on the test split.
I0308 03:32:25.255151 140341280416960 submission_runner.py:469] Time since start: 66094.71s, 	Step: 107453, 	{'train/accuracy': 0.8147720098495483, 'train/loss': 0.939460039138794, 'validation/accuracy': 0.70524001121521, 'validation/loss': 1.3963998556137085, 'validation/num_examples': 50000, 'test/accuracy': 0.5802000164985657, 'test/loss': 2.045531988143921, 'test/num_examples': 10000, 'score': 60791.30971312523, 'total_duration': 66094.709628582, 'accumulated_submission_time': 60791.30971312523, 'accumulated_eval_time': 5279.436554670334, 'accumulated_logging_time': 11.680552244186401}
I0308 03:32:25.337160 140185199154944 logging_writer.py:48] [107453] accumulated_eval_time=5279.44, accumulated_logging_time=11.6806, accumulated_submission_time=60791.3, global_step=107453, preemption_count=0, score=60791.3, test/accuracy=0.5802, test/loss=2.04553, test/num_examples=10000, total_duration=66094.7, train/accuracy=0.814772, train/loss=0.93946, validation/accuracy=0.70524, validation/loss=1.3964, validation/num_examples=50000
I0308 03:33:21.370882 140185115293440 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.5086374282836914, loss=2.90029239654541
2025-03-08 03:35:18.961804: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:35:48.346296 140185199154944 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.9510834217071533, loss=2.914170026779175
I0308 03:38:13.691064 140185115293440 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.7970213890075684, loss=2.9468743801116943
I0308 03:40:38.606554 140185199154944 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.6320090293884277, loss=2.883957624435425
I0308 03:40:55.992070 140341280416960 spec.py:321] Evaluating on the training split.
I0308 03:41:07.336405 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 03:41:29.524151 140341280416960 spec.py:349] Evaluating on the test split.
I0308 03:41:31.285001 140341280416960 submission_runner.py:469] Time since start: 66640.74s, 	Step: 107813, 	{'train/accuracy': 0.794343888759613, 'train/loss': 1.0124539136886597, 'validation/accuracy': 0.6936799883842468, 'validation/loss': 1.4419660568237305, 'validation/num_examples': 50000, 'test/accuracy': 0.5687000155448914, 'test/loss': 2.098330497741699, 'test/num_examples': 10000, 'score': 61301.904188632965, 'total_duration': 66640.73948550224, 'accumulated_submission_time': 61301.904188632965, 'accumulated_eval_time': 5314.729442358017, 'accumulated_logging_time': 11.784341096878052}
I0308 03:41:31.350338 140185115293440 logging_writer.py:48] [107813] accumulated_eval_time=5314.73, accumulated_logging_time=11.7843, accumulated_submission_time=61301.9, global_step=107813, preemption_count=0, score=61301.9, test/accuracy=0.5687, test/loss=2.09833, test/num_examples=10000, total_duration=66640.7, train/accuracy=0.794344, train/loss=1.01245, validation/accuracy=0.69368, validation/loss=1.44197, validation/num_examples=50000
I0308 03:45:12.728646 140185199154944 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.9622719287872314, loss=2.9928269386291504
I0308 03:50:04.460098 140341280416960 spec.py:321] Evaluating on the training split.
I0308 03:50:14.684609 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 03:50:50.722348 140341280416960 spec.py:349] Evaluating on the test split.
I0308 03:50:52.464213 140341280416960 submission_runner.py:469] Time since start: 67201.92s, 	Step: 107969, 	{'train/accuracy': 0.8130978941917419, 'train/loss': 0.9509666562080383, 'validation/accuracy': 0.7145999670028687, 'validation/loss': 1.3696093559265137, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 2.0138440132141113, 'test/num_examples': 10000, 'score': 61814.96117520332, 'total_duration': 67201.91868257523, 'accumulated_submission_time': 61814.96117520332, 'accumulated_eval_time': 5362.733507394791, 'accumulated_logging_time': 11.885761499404907}
I0308 03:50:52.490773 140185115293440 logging_writer.py:48] [107969] accumulated_eval_time=5362.73, accumulated_logging_time=11.8858, accumulated_submission_time=61815, global_step=107969, preemption_count=0, score=61815, test/accuracy=0.5866, test/loss=2.01384, test/num_examples=10000, total_duration=67201.9, train/accuracy=0.813098, train/loss=0.950967, validation/accuracy=0.7146, validation/loss=1.36961, validation/num_examples=50000
I0308 03:52:51.284209 140185199154944 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.718526601791382, loss=2.922624349594116
I0308 03:59:23.345264 140341280416960 spec.py:321] Evaluating on the training split.
I0308 03:59:33.522226 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 04:00:05.921214 140341280416960 spec.py:349] Evaluating on the test split.
I0308 04:00:07.689236 140341280416960 submission_runner.py:469] Time since start: 67757.14s, 	Step: 108092, 	{'train/accuracy': 0.8002431392669678, 'train/loss': 1.0036147832870483, 'validation/accuracy': 0.7033799886703491, 'validation/loss': 1.4123809337615967, 'validation/num_examples': 50000, 'test/accuracy': 0.5791000127792358, 'test/loss': 2.0430731773376465, 'test/num_examples': 10000, 'score': 62325.793167591095, 'total_duration': 67757.14371728897, 'accumulated_submission_time': 62325.793167591095, 'accumulated_eval_time': 5407.077453374863, 'accumulated_logging_time': 11.921654224395752}
I0308 04:00:07.716253 140185115293440 logging_writer.py:48] [108092] accumulated_eval_time=5407.08, accumulated_logging_time=11.9217, accumulated_submission_time=62325.8, global_step=108092, preemption_count=0, score=62325.8, test/accuracy=0.5791, test/loss=2.04307, test/num_examples=10000, total_duration=67757.1, train/accuracy=0.800243, train/loss=1.00361, validation/accuracy=0.70338, validation/loss=1.41238, validation/num_examples=50000
I0308 04:00:27.811568 140185199154944 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.4987263679504395, loss=2.8540425300598145
I0308 04:07:34.494501 140185115293440 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.755216598510742, loss=2.937199592590332
I0308 04:08:38.947789 140341280416960 spec.py:321] Evaluating on the training split.
I0308 04:08:49.209003 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 04:09:16.828183 140341280416960 spec.py:349] Evaluating on the test split.
I0308 04:09:18.611254 140341280416960 submission_runner.py:469] Time since start: 68308.07s, 	Step: 108216, 	{'train/accuracy': 0.8045280575752258, 'train/loss': 0.9853981733322144, 'validation/accuracy': 0.7089200019836426, 'validation/loss': 1.4000835418701172, 'validation/num_examples': 50000, 'test/accuracy': 0.5813000202178955, 'test/loss': 2.04768705368042, 'test/num_examples': 10000, 'score': 62837.002648830414, 'total_duration': 68308.06570458412, 'accumulated_submission_time': 62837.002648830414, 'accumulated_eval_time': 5446.74084186554, 'accumulated_logging_time': 11.957589387893677}
I0308 04:09:18.635391 140185199154944 logging_writer.py:48] [108216] accumulated_eval_time=5446.74, accumulated_logging_time=11.9576, accumulated_submission_time=62837, global_step=108216, preemption_count=0, score=62837, test/accuracy=0.5813, test/loss=2.04769, test/num_examples=10000, total_duration=68308.1, train/accuracy=0.804528, train/loss=0.985398, validation/accuracy=0.70892, validation/loss=1.40008, validation/num_examples=50000
I0308 04:14:59.344202 140185115293440 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.5585014820098877, loss=2.871919870376587
I0308 04:17:50.679696 140341280416960 spec.py:321] Evaluating on the training split.
I0308 04:18:00.458954 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 04:18:32.915262 140341280416960 spec.py:349] Evaluating on the test split.
I0308 04:18:34.687717 140341280416960 submission_runner.py:469] Time since start: 68864.14s, 	Step: 108343, 	{'train/accuracy': 0.8109055757522583, 'train/loss': 0.9281376004219055, 'validation/accuracy': 0.715399980545044, 'validation/loss': 1.3417048454284668, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.9914907217025757, 'test/num_examples': 10000, 'score': 63347.27316713333, 'total_duration': 68864.14220428467, 'accumulated_submission_time': 63347.27316713333, 'accumulated_eval_time': 5490.748834371567, 'accumulated_logging_time': 13.741579294204712}
I0308 04:18:34.711546 140185199154944 logging_writer.py:48] [108343] accumulated_eval_time=5490.75, accumulated_logging_time=13.7416, accumulated_submission_time=63347.3, global_step=108343, preemption_count=0, score=63347.3, test/accuracy=0.5779, test/loss=1.99149, test/num_examples=10000, total_duration=68864.1, train/accuracy=0.810906, train/loss=0.928138, validation/accuracy=0.7154, validation/loss=1.3417, validation/num_examples=50000
I0308 04:22:11.788981 140185115293440 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.5427520275115967, loss=2.8652985095977783
I0308 04:25:39.998119 140185199154944 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.675493001937866, loss=2.943424940109253
I0308 04:27:05.234827 140341280416960 spec.py:321] Evaluating on the training split.
I0308 04:27:15.017800 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 04:27:39.355290 140341280416960 spec.py:349] Evaluating on the test split.
I0308 04:27:41.155035 140341280416960 submission_runner.py:469] Time since start: 69410.61s, 	Step: 108578, 	{'train/accuracy': 0.8049266338348389, 'train/loss': 0.9976184368133545, 'validation/accuracy': 0.7079399824142456, 'validation/loss': 1.4058924913406372, 'validation/num_examples': 50000, 'test/accuracy': 0.582800030708313, 'test/loss': 2.03450345993042, 'test/num_examples': 10000, 'score': 63857.76262831688, 'total_duration': 69410.60951972008, 'accumulated_submission_time': 63857.76262831688, 'accumulated_eval_time': 5526.669007778168, 'accumulated_logging_time': 13.773183584213257}
I0308 04:27:41.180070 140185115293440 logging_writer.py:48] [108578] accumulated_eval_time=5526.67, accumulated_logging_time=13.7732, accumulated_submission_time=63857.8, global_step=108578, preemption_count=0, score=63857.8, test/accuracy=0.5828, test/loss=2.0345, test/num_examples=10000, total_duration=69410.6, train/accuracy=0.804927, train/loss=0.997618, validation/accuracy=0.70794, validation/loss=1.40589, validation/num_examples=50000
I0308 04:27:54.923597 140185199154944 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.7482595443725586, loss=2.854309558868408
I0308 04:29:44.135513 140185115293440 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.6203365325927734, loss=3.0093469619750977
I0308 04:31:30.399062 140185199154944 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.63533353805542, loss=2.9606058597564697
2025-03-08 04:32:04.725615: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:33:20.179615 140185115293440 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.7575459480285645, loss=2.982755661010742
I0308 04:35:08.344001 140185199154944 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.4380943775177, loss=2.8754303455352783
I0308 04:36:12.026517 140341280416960 spec.py:321] Evaluating on the training split.
I0308 04:36:22.632451 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 04:36:43.837216 140341280416960 spec.py:349] Evaluating on the test split.
I0308 04:36:45.629390 140341280416960 submission_runner.py:469] Time since start: 69955.08s, 	Step: 109060, 	{'train/accuracy': 0.8085339665412903, 'train/loss': 0.9669428467750549, 'validation/accuracy': 0.7022599577903748, 'validation/loss': 1.4240390062332153, 'validation/num_examples': 50000, 'test/accuracy': 0.5703000426292419, 'test/loss': 2.068380355834961, 'test/num_examples': 10000, 'score': 64368.493617773056, 'total_duration': 69955.08388447762, 'accumulated_submission_time': 64368.493617773056, 'accumulated_eval_time': 5560.271856546402, 'accumulated_logging_time': 13.859530687332153}
I0308 04:36:45.687617 140185115293440 logging_writer.py:48] [109060] accumulated_eval_time=5560.27, accumulated_logging_time=13.8595, accumulated_submission_time=64368.5, global_step=109060, preemption_count=0, score=64368.5, test/accuracy=0.5703, test/loss=2.06838, test/num_examples=10000, total_duration=69955.1, train/accuracy=0.808534, train/loss=0.966943, validation/accuracy=0.70226, validation/loss=1.42404, validation/num_examples=50000
I0308 04:37:17.120549 140185199154944 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.8097643852233887, loss=2.9037301540374756
I0308 04:39:04.182859 140185115293440 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.6537909507751465, loss=2.874922275543213
I0308 04:40:51.300376 140185199154944 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.7253832817077637, loss=2.945510149002075
I0308 04:42:38.975525 140185115293440 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.577754020690918, loss=2.993704319000244
I0308 04:44:25.895975 140185199154944 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.7121646404266357, loss=2.9047980308532715
I0308 04:45:16.335924 140341280416960 spec.py:321] Evaluating on the training split.
I0308 04:45:27.149789 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 04:45:48.305492 140341280416960 spec.py:349] Evaluating on the test split.
I0308 04:45:50.068956 140341280416960 submission_runner.py:469] Time since start: 70499.52s, 	Step: 109548, 	{'train/accuracy': 0.8052853941917419, 'train/loss': 0.9868594408035278, 'validation/accuracy': 0.711679995059967, 'validation/loss': 1.3989802598953247, 'validation/num_examples': 50000, 'test/accuracy': 0.5814000368118286, 'test/loss': 2.0523521900177, 'test/num_examples': 10000, 'score': 64879.0803527832, 'total_duration': 70499.52343559265, 'accumulated_submission_time': 64879.0803527832, 'accumulated_eval_time': 5594.0048451423645, 'accumulated_logging_time': 13.926214933395386}
I0308 04:45:50.128376 140185115293440 logging_writer.py:48] [109548] accumulated_eval_time=5594, accumulated_logging_time=13.9262, accumulated_submission_time=64879.1, global_step=109548, preemption_count=0, score=64879.1, test/accuracy=0.5814, test/loss=2.05235, test/num_examples=10000, total_duration=70499.5, train/accuracy=0.805285, train/loss=0.986859, validation/accuracy=0.71168, validation/loss=1.39898, validation/num_examples=50000
I0308 04:46:36.757850 140185199154944 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.9147825241088867, loss=2.9468069076538086
I0308 04:48:24.059500 140185115293440 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.648280143737793, loss=3.017228603363037
I0308 04:50:11.363888 140185199154944 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.637317419052124, loss=2.9576001167297363
I0308 04:51:59.341585 140185115293440 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.6860103607177734, loss=2.9358327388763428
I0308 04:53:47.107121 140185199154944 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.557852268218994, loss=2.8558266162872314
I0308 04:54:20.593385 140341280416960 spec.py:321] Evaluating on the training split.
I0308 04:54:31.079783 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 04:54:53.966701 140341280416960 spec.py:349] Evaluating on the test split.
I0308 04:54:55.747762 140341280416960 submission_runner.py:469] Time since start: 71045.20s, 	Step: 110032, 	{'train/accuracy': 0.8101881146430969, 'train/loss': 0.9567452669143677, 'validation/accuracy': 0.7170199751853943, 'validation/loss': 1.3539681434631348, 'validation/num_examples': 50000, 'test/accuracy': 0.5900000333786011, 'test/loss': 1.9913678169250488, 'test/num_examples': 10000, 'score': 65389.48230099678, 'total_duration': 71045.20224094391, 'accumulated_submission_time': 65389.48230099678, 'accumulated_eval_time': 5629.159185886383, 'accumulated_logging_time': 13.994304895401001}
I0308 04:54:55.794904 140185115293440 logging_writer.py:48] [110032] accumulated_eval_time=5629.16, accumulated_logging_time=13.9943, accumulated_submission_time=65389.5, global_step=110032, preemption_count=0, score=65389.5, test/accuracy=0.59, test/loss=1.99137, test/num_examples=10000, total_duration=71045.2, train/accuracy=0.810188, train/loss=0.956745, validation/accuracy=0.71702, validation/loss=1.35397, validation/num_examples=50000
2025-03-08 04:55:40.513024: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:56:00.600250 140185199154944 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.807854652404785, loss=2.880222797393799
I0308 04:57:48.192010 140185115293440 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.6863396167755127, loss=2.899559497833252
I0308 04:59:35.635680 140185199154944 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.603004217147827, loss=2.9318814277648926
I0308 05:01:23.707027 140185115293440 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.79398250579834, loss=2.9779601097106934
I0308 05:03:12.200147 140185199154944 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.5718090534210205, loss=2.8914403915405273
I0308 05:03:26.415244 140341280416960 spec.py:321] Evaluating on the training split.
I0308 05:03:37.046627 140341280416960 spec.py:333] Evaluating on the validation split.
I0308 05:04:02.530349 140341280416960 spec.py:349] Evaluating on the test split.
I0308 05:04:04.279542 140341280416960 submission_runner.py:469] Time since start: 71593.73s, 	Step: 110514, 	{'train/accuracy': 0.8205915093421936, 'train/loss': 0.9008785486221313, 'validation/accuracy': 0.7108799815177917, 'validation/loss': 1.3629835844039917, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 2.0012829303741455, 'test/num_examples': 10000, 'score': 65900.03963112831, 'total_duration': 71593.73402452469, 'accumulated_submission_time': 65900.03963112831, 'accumulated_eval_time': 5667.023441553116, 'accumulated_logging_time': 14.049530744552612}
I0308 05:04:04.338935 140185115293440 logging_writer.py:48] [110514] accumulated_eval_time=5667.02, accumulated_logging_time=14.0495, accumulated_submission_time=65900, global_step=110514, preemption_count=0, score=65900, test/accuracy=0.5811, test/loss=2.00128, test/num_examples=10000, total_duration=71593.7, train/accuracy=0.820592, train/loss=0.900879, validation/accuracy=0.71088, validation/loss=1.36298, validation/num_examples=50000
I0308 05:05:27.284065 140185199154944 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.8386292457580566, loss=2.8888440132141113
I0308 05:07:14.222844 140185115293440 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.614192485809326, loss=2.9472804069519043
I0308 05:09:01.431282 140185199154944 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.807745933532715, loss=2.86118221282959
I0308 05:10:48.591380 140185115293440 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.7156646251678467, loss=2.890474557876587
I0308 05:12:34.820793 140185199154944 logging_writer.py:48] [111000] global_step=111000, preemption_count=0, score=66410.4
I0308 05:12:36.274578 140341280416960 submission_runner.py:646] Tuning trial 2/5
I0308 05:12:36.300961 140341280416960 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0308 05:12:36.305794 140341280416960 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012954400153830647, 'train/loss': 6.912797927856445, 'validation/accuracy': 0.0011399999493733048, 'validation/loss': 6.913561820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.9131364822387695, 'test/num_examples': 10000, 'score': 59.212727785110474, 'total_duration': 167.68675565719604, 'accumulated_submission_time': 59.212727785110474, 'accumulated_eval_time': 108.47379875183105, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1362, {'train/accuracy': 0.07712850719690323, 'train/loss': 5.349559307098389, 'validation/accuracy': 0.06757999956607819, 'validation/loss': 5.4484663009643555, 'validation/num_examples': 50000, 'test/accuracy': 0.049800001084804535, 'test/loss': 5.662440299987793, 'test/num_examples': 10000, 'score': 569.3144462108612, 'total_duration': 716.8610239028931, 'accumulated_submission_time': 569.3144462108612, 'accumulated_eval_time': 147.29842066764832, 'accumulated_logging_time': 0.0688316822052002, 'global_step': 1362, 'preemption_count': 0}), (2708, {'train/accuracy': 0.17384804785251617, 'train/loss': 4.372182846069336, 'validation/accuracy': 0.15455999970436096, 'validation/loss': 4.523075103759766, 'validation/num_examples': 50000, 'test/accuracy': 0.10610000789165497, 'test/loss': 4.935251712799072, 'test/num_examples': 10000, 'score': 1079.2011749744415, 'total_duration': 1269.7945284843445, 'accumulated_submission_time': 1079.2011749744415, 'accumulated_eval_time': 190.10553526878357, 'accumulated_logging_time': 0.13519978523254395, 'global_step': 2708, 'preemption_count': 0}), (4051, {'train/accuracy': 0.2845184803009033, 'train/loss': 3.656397819519043, 'validation/accuracy': 0.24833999574184418, 'validation/loss': 3.841278314590454, 'validation/num_examples': 50000, 'test/accuracy': 0.18820001184940338, 'test/loss': 4.302127838134766, 'test/num_examples': 10000, 'score': 1588.9874997138977, 'total_duration': 1817.9129548072815, 'accumulated_submission_time': 1588.9874997138977, 'accumulated_eval_time': 228.17598390579224, 'accumulated_logging_time': 0.25188350677490234, 'global_step': 4051, 'preemption_count': 0}), (5389, {'train/accuracy': 0.3719308078289032, 'train/loss': 3.036378860473633, 'validation/accuracy': 0.3323200047016144, 'validation/loss': 3.238503932952881, 'validation/num_examples': 50000, 'test/accuracy': 0.2510000169277191, 'test/loss': 3.8246071338653564, 'test/num_examples': 10000, 'score': 2098.994063138962, 'total_duration': 2364.200074195862, 'accumulated_submission_time': 2098.994063138962, 'accumulated_eval_time': 264.302875995636, 'accumulated_logging_time': 0.2702193260192871, 'global_step': 5389, 'preemption_count': 0}), (6724, {'train/accuracy': 0.4240274131298065, 'train/loss': 2.73190975189209, 'validation/accuracy': 0.38415998220443726, 'validation/loss': 2.949188470840454, 'validation/num_examples': 50000, 'test/accuracy': 0.28290000557899475, 'test/loss': 3.5845649242401123, 'test/num_examples': 10000, 'score': 2609.108383655548, 'total_duration': 2910.018846511841, 'accumulated_submission_time': 2609.108383655548, 'accumulated_eval_time': 299.7898905277252, 'accumulated_logging_time': 0.360884428024292, 'global_step': 6724, 'preemption_count': 0}), (8058, {'train/accuracy': 0.4624720811843872, 'train/loss': 2.4840662479400635, 'validation/accuracy': 0.4181399941444397, 'validation/loss': 2.70945143699646, 'validation/num_examples': 50000, 'test/accuracy': 0.3295000195503235, 'test/loss': 3.3195393085479736, 'test/num_examples': 10000, 'score': 3118.952538728714, 'total_duration': 3456.4709782600403, 'accumulated_submission_time': 3118.952538728714, 'accumulated_eval_time': 336.2134144306183, 'accumulated_logging_time': 0.4149470329284668, 'global_step': 8058, 'preemption_count': 0}), (9394, {'train/accuracy': 0.5198899507522583, 'train/loss': 2.29365873336792, 'validation/accuracy': 0.47005999088287354, 'validation/loss': 2.531599521636963, 'validation/num_examples': 50000, 'test/accuracy': 0.3564000129699707, 'test/loss': 3.1833980083465576, 'test/num_examples': 10000, 'score': 3628.852597951889, 'total_duration': 4004.2401163578033, 'accumulated_submission_time': 3628.852597951889, 'accumulated_eval_time': 373.8653357028961, 'accumulated_logging_time': 0.5019807815551758, 'global_step': 9394, 'preemption_count': 0}), (10736, {'train/accuracy': 0.5587930083274841, 'train/loss': 2.0877444744110107, 'validation/accuracy': 0.5124199986457825, 'validation/loss': 2.3099277019500732, 'validation/num_examples': 50000, 'test/accuracy': 0.3995000123977661, 'test/loss': 2.9412453174591064, 'test/num_examples': 10000, 'score': 4138.687603473663, 'total_duration': 4557.458254098892, 'accumulated_submission_time': 4138.687603473663, 'accumulated_eval_time': 416.9271948337555, 'accumulated_logging_time': 0.6678164005279541, 'global_step': 10736, 'preemption_count': 0}), (12066, {'train/accuracy': 0.5911591053009033, 'train/loss': 1.9238556623458862, 'validation/accuracy': 0.5330600142478943, 'validation/loss': 2.187696933746338, 'validation/num_examples': 50000, 'test/accuracy': 0.4195000231266022, 'test/loss': 2.83099102973938, 'test/num_examples': 10000, 'score': 4648.455675840378, 'total_duration': 5113.794836759567, 'accumulated_submission_time': 4648.455675840378, 'accumulated_eval_time': 463.2056620121002, 'accumulated_logging_time': 0.8076694011688232, 'global_step': 12066, 'preemption_count': 0}), (13395, {'train/accuracy': 0.62109375, 'train/loss': 1.7880302667617798, 'validation/accuracy': 0.5651599764823914, 'validation/loss': 2.0439517498016357, 'validation/num_examples': 50000, 'test/accuracy': 0.4312000274658203, 'test/loss': 2.731745481491089, 'test/num_examples': 10000, 'score': 5158.338755130768, 'total_duration': 5671.454211473465, 'accumulated_submission_time': 5158.338755130768, 'accumulated_eval_time': 510.6190092563629, 'accumulated_logging_time': 1.0177834033966064, 'global_step': 13395, 'preemption_count': 0}), (14718, {'train/accuracy': 0.6342673897743225, 'train/loss': 1.7238037586212158, 'validation/accuracy': 0.5753799676895142, 'validation/loss': 1.981651782989502, 'validation/num_examples': 50000, 'test/accuracy': 0.4442000091075897, 'test/loss': 2.676860809326172, 'test/num_examples': 10000, 'score': 5668.027373313904, 'total_duration': 6225.641801357269, 'accumulated_submission_time': 5668.027373313904, 'accumulated_eval_time': 554.6683828830719, 'accumulated_logging_time': 1.322136640548706, 'global_step': 14718, 'preemption_count': 0}), (16048, {'train/accuracy': 0.6418606638908386, 'train/loss': 1.6886143684387207, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.9759666919708252, 'validation/num_examples': 50000, 'test/accuracy': 0.46300002932548523, 'test/loss': 2.591609477996826, 'test/num_examples': 10000, 'score': 6177.934530735016, 'total_duration': 6784.764118671417, 'accumulated_submission_time': 6177.934530735016, 'accumulated_eval_time': 603.4656329154968, 'accumulated_logging_time': 1.5904459953308105, 'global_step': 16048, 'preemption_count': 0}), (17384, {'train/accuracy': 0.6472217440605164, 'train/loss': 1.6516441106796265, 'validation/accuracy': 0.5870800018310547, 'validation/loss': 1.9247711896896362, 'validation/num_examples': 50000, 'test/accuracy': 0.46150001883506775, 'test/loss': 2.6072232723236084, 'test/num_examples': 10000, 'score': 6687.970390558243, 'total_duration': 7345.708931446075, 'accumulated_submission_time': 6687.970390558243, 'accumulated_eval_time': 653.9938857555389, 'accumulated_logging_time': 1.8182072639465332, 'global_step': 17384, 'preemption_count': 0}), (18710, {'train/accuracy': 0.6691844463348389, 'train/loss': 1.5429637432098389, 'validation/accuracy': 0.6073200106620789, 'validation/loss': 1.8258731365203857, 'validation/num_examples': 50000, 'test/accuracy': 0.48100003600120544, 'test/loss': 2.503962993621826, 'test/num_examples': 10000, 'score': 7198.005915403366, 'total_duration': 7904.732781648636, 'accumulated_submission_time': 7198.005915403366, 'accumulated_eval_time': 702.7266757488251, 'accumulated_logging_time': 1.94059419631958, 'global_step': 18710, 'preemption_count': 0}), (20040, {'train/accuracy': 0.6738879084587097, 'train/loss': 1.554980754852295, 'validation/accuracy': 0.6146399974822998, 'validation/loss': 1.823943853378296, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.489971876144409, 'test/num_examples': 10000, 'score': 7708.015518426895, 'total_duration': 8468.794037103653, 'accumulated_submission_time': 7708.015518426895, 'accumulated_eval_time': 756.6097090244293, 'accumulated_logging_time': 1.9686574935913086, 'global_step': 20040, 'preemption_count': 0}), (21362, {'train/accuracy': 0.6915059089660645, 'train/loss': 1.4980823993682861, 'validation/accuracy': 0.6279599666595459, 'validation/loss': 1.7822824716567993, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.4577853679656982, 'test/num_examples': 10000, 'score': 8217.745678424835, 'total_duration': 9029.62114405632, 'accumulated_submission_time': 8217.745678424835, 'accumulated_eval_time': 807.4246821403503, 'accumulated_logging_time': 2.103929281234741, 'global_step': 21362, 'preemption_count': 0}), (22677, {'train/accuracy': 0.6871811151504517, 'train/loss': 1.497862458229065, 'validation/accuracy': 0.6218799948692322, 'validation/loss': 1.7898929119110107, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.4349539279937744, 'test/num_examples': 10000, 'score': 8727.906654834747, 'total_duration': 9586.918984413147, 'accumulated_submission_time': 8727.906654834747, 'accumulated_eval_time': 854.382071018219, 'accumulated_logging_time': 2.1417856216430664, 'global_step': 22677, 'preemption_count': 0}), (24003, {'train/accuracy': 0.6844905614852905, 'train/loss': 1.4853253364562988, 'validation/accuracy': 0.6228799819946289, 'validation/loss': 1.776080846786499, 'validation/num_examples': 50000, 'test/accuracy': 0.4944000244140625, 'test/loss': 2.441228151321411, 'test/num_examples': 10000, 'score': 9238.029967308044, 'total_duration': 10140.574912309647, 'accumulated_submission_time': 9238.029967308044, 'accumulated_eval_time': 897.6989679336548, 'accumulated_logging_time': 2.217595338821411, 'global_step': 24003, 'preemption_count': 0}), (25312, {'train/accuracy': 0.6946946382522583, 'train/loss': 1.4503073692321777, 'validation/accuracy': 0.6246599555015564, 'validation/loss': 1.7585413455963135, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.4092037677764893, 'test/num_examples': 10000, 'score': 9748.021029233932, 'total_duration': 10698.047112941742, 'accumulated_submission_time': 9748.021029233932, 'accumulated_eval_time': 944.9897358417511, 'accumulated_logging_time': 2.2582345008850098, 'global_step': 25312, 'preemption_count': 0}), (26605, {'train/accuracy': 0.6996970772743225, 'train/loss': 1.4392324686050415, 'validation/accuracy': 0.6319599747657776, 'validation/loss': 1.730715274810791, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.427701950073242, 'test/num_examples': 10000, 'score': 10257.83259153366, 'total_duration': 11252.918318986893, 'accumulated_submission_time': 10257.83259153366, 'accumulated_eval_time': 989.8419723510742, 'accumulated_logging_time': 2.321309804916382, 'global_step': 26605, 'preemption_count': 0}), (27913, {'train/accuracy': 0.7160794138908386, 'train/loss': 1.3407766819000244, 'validation/accuracy': 0.644760012626648, 'validation/loss': 1.651583194732666, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.297161340713501, 'test/num_examples': 10000, 'score': 10767.716799736023, 'total_duration': 11805.50762295723, 'accumulated_submission_time': 10767.716799736023, 'accumulated_eval_time': 1032.3547706604004, 'accumulated_logging_time': 2.355203866958618, 'global_step': 27913, 'preemption_count': 0}), (29214, {'train/accuracy': 0.7053172588348389, 'train/loss': 1.3841984272003174, 'validation/accuracy': 0.6370799541473389, 'validation/loss': 1.687349796295166, 'validation/num_examples': 50000, 'test/accuracy': 0.5116000175476074, 'test/loss': 2.3664023876190186, 'test/num_examples': 10000, 'score': 11277.586159706116, 'total_duration': 12357.805555343628, 'accumulated_submission_time': 11277.586159706116, 'accumulated_eval_time': 1074.5624024868011, 'accumulated_logging_time': 2.431577682495117, 'global_step': 29214, 'preemption_count': 0}), (30533, {'train/accuracy': 0.7027662396430969, 'train/loss': 1.4169032573699951, 'validation/accuracy': 0.6337199807167053, 'validation/loss': 1.72417151927948, 'validation/num_examples': 50000, 'test/accuracy': 0.5106000304222107, 'test/loss': 2.3677446842193604, 'test/num_examples': 10000, 'score': 11787.666238307953, 'total_duration': 12909.600771903992, 'accumulated_submission_time': 11787.666238307953, 'accumulated_eval_time': 1116.0632033348083, 'accumulated_logging_time': 2.5101253986358643, 'global_step': 30533, 'preemption_count': 0}), (31856, {'train/accuracy': 0.7217394709587097, 'train/loss': 1.3666224479675293, 'validation/accuracy': 0.6528199911117554, 'validation/loss': 1.6680662631988525, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.3135781288146973, 'test/num_examples': 10000, 'score': 12297.766247272491, 'total_duration': 13463.571939229965, 'accumulated_submission_time': 12297.766247272491, 'accumulated_eval_time': 1159.7485346794128, 'accumulated_logging_time': 2.5615243911743164, 'global_step': 31856, 'preemption_count': 0}), (33177, {'train/accuracy': 0.7101203799247742, 'train/loss': 1.36191725730896, 'validation/accuracy': 0.6447799801826477, 'validation/loss': 1.6609554290771484, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.3287246227264404, 'test/num_examples': 10000, 'score': 12807.7827064991, 'total_duration': 14033.12017083168, 'accumulated_submission_time': 12807.7827064991, 'accumulated_eval_time': 1219.0915217399597, 'accumulated_logging_time': 2.6112585067749023, 'global_step': 33177, 'preemption_count': 0}), (34491, {'train/accuracy': 0.7182915806770325, 'train/loss': 1.3755179643630981, 'validation/accuracy': 0.6513800024986267, 'validation/loss': 1.6660535335540771, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.324406862258911, 'test/num_examples': 10000, 'score': 13317.775473594666, 'total_duration': 14598.280057668686, 'accumulated_submission_time': 13317.775473594666, 'accumulated_eval_time': 1274.069085597992, 'accumulated_logging_time': 2.6554343700408936, 'global_step': 34491, 'preemption_count': 0}), (35802, {'train/accuracy': 0.7181521058082581, 'train/loss': 1.3607012033462524, 'validation/accuracy': 0.6498599648475647, 'validation/loss': 1.6598118543624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.316274642944336, 'test/num_examples': 10000, 'score': 13827.749715566635, 'total_duration': 15157.79241991043, 'accumulated_submission_time': 13827.749715566635, 'accumulated_eval_time': 1323.3773593902588, 'accumulated_logging_time': 2.7342426776885986, 'global_step': 35802, 'preemption_count': 0}), (37122, {'train/accuracy': 0.7237722873687744, 'train/loss': 1.3473910093307495, 'validation/accuracy': 0.6503199934959412, 'validation/loss': 1.652050256729126, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.296616554260254, 'test/num_examples': 10000, 'score': 14337.757759094238, 'total_duration': 15714.593129873276, 'accumulated_submission_time': 14337.757759094238, 'accumulated_eval_time': 1369.93554520607, 'accumulated_logging_time': 2.8205060958862305, 'global_step': 37122, 'preemption_count': 0}), (38440, {'train/accuracy': 0.7148237824440002, 'train/loss': 1.3588889837265015, 'validation/accuracy': 0.6502799987792969, 'validation/loss': 1.6584783792495728, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.3372254371643066, 'test/num_examples': 10000, 'score': 14847.710516929626, 'total_duration': 16277.461966514587, 'accumulated_submission_time': 14847.710516929626, 'accumulated_eval_time': 1422.655452489853, 'accumulated_logging_time': 2.874232292175293, 'global_step': 38440, 'preemption_count': 0}), (39765, {'train/accuracy': 0.7251474857330322, 'train/loss': 1.3125813007354736, 'validation/accuracy': 0.6564599871635437, 'validation/loss': 1.6125577688217163, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.249238967895508, 'test/num_examples': 10000, 'score': 15357.758349895477, 'total_duration': 16835.303562402725, 'accumulated_submission_time': 15357.758349895477, 'accumulated_eval_time': 1470.2049469947815, 'accumulated_logging_time': 2.9794087409973145, 'global_step': 39765, 'preemption_count': 0}), (41084, {'train/accuracy': 0.7266222834587097, 'train/loss': 1.3028889894485474, 'validation/accuracy': 0.6598199605941772, 'validation/loss': 1.5991718769073486, 'validation/num_examples': 50000, 'test/accuracy': 0.5218000411987305, 'test/loss': 2.284785032272339, 'test/num_examples': 10000, 'score': 15867.81780910492, 'total_duration': 17392.89449262619, 'accumulated_submission_time': 15867.81780910492, 'accumulated_eval_time': 1517.5070989131927, 'accumulated_logging_time': 3.0613551139831543, 'global_step': 41084, 'preemption_count': 0}), (42403, {'train/accuracy': 0.7292131781578064, 'train/loss': 1.2720746994018555, 'validation/accuracy': 0.6576600074768066, 'validation/loss': 1.592840313911438, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.24770450592041, 'test/num_examples': 10000, 'score': 16377.884187936783, 'total_duration': 17950.229959487915, 'accumulated_submission_time': 16377.884187936783, 'accumulated_eval_time': 1564.5541186332703, 'accumulated_logging_time': 3.1414103507995605, 'global_step': 42403, 'preemption_count': 0}), (43721, {'train/accuracy': 0.725605845451355, 'train/loss': 1.3395122289657593, 'validation/accuracy': 0.6609999537467957, 'validation/loss': 1.6278122663497925, 'validation/num_examples': 50000, 'test/accuracy': 0.5358999967575073, 'test/loss': 2.2774667739868164, 'test/num_examples': 10000, 'score': 16888.06930756569, 'total_duration': 18505.850697040558, 'accumulated_submission_time': 16888.06930756569, 'accumulated_eval_time': 1609.7890067100525, 'accumulated_logging_time': 3.200737953186035, 'global_step': 43721, 'preemption_count': 0}), (45037, {'train/accuracy': 0.72171950340271, 'train/loss': 1.3635785579681396, 'validation/accuracy': 0.6539799571037292, 'validation/loss': 1.658382773399353, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.3228673934936523, 'test/num_examples': 10000, 'score': 17398.229001522064, 'total_duration': 19069.29190993309, 'accumulated_submission_time': 17398.229001522064, 'accumulated_eval_time': 1662.861756324768, 'accumulated_logging_time': 3.2768237590789795, 'global_step': 45037, 'preemption_count': 0}), (46358, {'train/accuracy': 0.7330596446990967, 'train/loss': 1.289806842803955, 'validation/accuracy': 0.6603599786758423, 'validation/loss': 1.6059191226959229, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.2774500846862793, 'test/num_examples': 10000, 'score': 17908.21753168106, 'total_duration': 19638.628463745117, 'accumulated_submission_time': 17908.21753168106, 'accumulated_eval_time': 1722.0289239883423, 'accumulated_logging_time': 3.325507164001465, 'global_step': 46358, 'preemption_count': 0}), (47673, {'train/accuracy': 0.7172552347183228, 'train/loss': 1.310454249382019, 'validation/accuracy': 0.6531400084495544, 'validation/loss': 1.6099224090576172, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.278560161590576, 'test/num_examples': 10000, 'score': 18418.190346956253, 'total_duration': 20197.068747520447, 'accumulated_submission_time': 18418.190346956253, 'accumulated_eval_time': 1770.2790768146515, 'accumulated_logging_time': 3.405736207962036, 'global_step': 47673, 'preemption_count': 0}), (48987, {'train/accuracy': 0.7267019748687744, 'train/loss': 1.3229351043701172, 'validation/accuracy': 0.6597999930381775, 'validation/loss': 1.6181124448776245, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.287149429321289, 'test/num_examples': 10000, 'score': 18928.36222743988, 'total_duration': 20766.139822006226, 'accumulated_submission_time': 18928.36222743988, 'accumulated_eval_time': 1828.9493579864502, 'accumulated_logging_time': 3.4868218898773193, 'global_step': 48987, 'preemption_count': 0}), (50305, {'train/accuracy': 0.7238121628761292, 'train/loss': 1.357945203781128, 'validation/accuracy': 0.6561599969863892, 'validation/loss': 1.6645610332489014, 'validation/num_examples': 50000, 'test/accuracy': 0.5194000005722046, 'test/loss': 2.347830295562744, 'test/num_examples': 10000, 'score': 19438.232702970505, 'total_duration': 21330.013384103775, 'accumulated_submission_time': 19438.232702970505, 'accumulated_eval_time': 1882.7168672084808, 'accumulated_logging_time': 3.579538345336914, 'global_step': 50305, 'preemption_count': 0}), (51614, {'train/accuracy': 0.7358298897743225, 'train/loss': 1.290172815322876, 'validation/accuracy': 0.667140007019043, 'validation/loss': 1.5935722589492798, 'validation/num_examples': 50000, 'test/accuracy': 0.5333999991416931, 'test/loss': 2.267338752746582, 'test/num_examples': 10000, 'score': 19948.236372470856, 'total_duration': 21894.27494931221, 'accumulated_submission_time': 19948.236372470856, 'accumulated_eval_time': 1936.7167658805847, 'accumulated_logging_time': 3.6928820610046387, 'global_step': 51614, 'preemption_count': 0}), (52929, {'train/accuracy': 0.734394907951355, 'train/loss': 1.2459553480148315, 'validation/accuracy': 0.6674599647521973, 'validation/loss': 1.5557643175125122, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2205395698547363, 'test/num_examples': 10000, 'score': 20458.37513923645, 'total_duration': 22454.288200378418, 'accumulated_submission_time': 20458.37513923645, 'accumulated_eval_time': 1986.3936340808868, 'accumulated_logging_time': 3.7527828216552734, 'global_step': 52929, 'preemption_count': 0}), (54241, {'train/accuracy': 0.7156807780265808, 'train/loss': 1.3730186223983765, 'validation/accuracy': 0.6506999731063843, 'validation/loss': 1.677892804145813, 'validation/num_examples': 50000, 'test/accuracy': 0.5204000473022461, 'test/loss': 2.332780361175537, 'test/num_examples': 10000, 'score': 20968.234202861786, 'total_duration': 23019.18790245056, 'accumulated_submission_time': 20968.234202861786, 'accumulated_eval_time': 2041.1501019001007, 'accumulated_logging_time': 3.890122175216675, 'global_step': 54241, 'preemption_count': 0}), (55538, {'train/accuracy': 0.7281369566917419, 'train/loss': 1.3163373470306396, 'validation/accuracy': 0.6577000021934509, 'validation/loss': 1.632766842842102, 'validation/num_examples': 50000, 'test/accuracy': 0.5351999998092651, 'test/loss': 2.261859655380249, 'test/num_examples': 10000, 'score': 21478.206674575806, 'total_duration': 23569.830231666565, 'accumulated_submission_time': 21478.206674575806, 'accumulated_eval_time': 2081.555350780487, 'accumulated_logging_time': 4.000834703445435, 'global_step': 55538, 'preemption_count': 0}), (56821, {'train/accuracy': 0.7429647445678711, 'train/loss': 1.2034153938293457, 'validation/accuracy': 0.6712599992752075, 'validation/loss': 1.5273125171661377, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.1589515209198, 'test/num_examples': 10000, 'score': 21988.14218521118, 'total_duration': 24131.989455223083, 'accumulated_submission_time': 21988.14218521118, 'accumulated_eval_time': 2133.380360364914, 'accumulated_logging_time': 4.242295026779175, 'global_step': 56821, 'preemption_count': 0}), (58108, {'train/accuracy': 0.7422871589660645, 'train/loss': 1.2285799980163574, 'validation/accuracy': 0.6701200008392334, 'validation/loss': 1.5347797870635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5401000380516052, 'test/loss': 2.2144806385040283, 'test/num_examples': 10000, 'score': 22498.170780658722, 'total_duration': 24695.670558214188, 'accumulated_submission_time': 22498.170780658722, 'accumulated_eval_time': 2186.784686565399, 'accumulated_logging_time': 4.331315994262695, 'global_step': 58108, 'preemption_count': 0}), (59393, {'train/accuracy': 0.7423867583274841, 'train/loss': 1.2358310222625732, 'validation/accuracy': 0.6685799956321716, 'validation/loss': 1.5634585618972778, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.2128000259399414, 'test/num_examples': 10000, 'score': 23007.977536439896, 'total_duration': 25251.28094100952, 'accumulated_submission_time': 23007.977536439896, 'accumulated_eval_time': 2232.3183736801147, 'accumulated_logging_time': 4.4424896240234375, 'global_step': 59393, 'preemption_count': 0}), (60671, {'train/accuracy': 0.7311065196990967, 'train/loss': 1.2819805145263672, 'validation/accuracy': 0.6538800001144409, 'validation/loss': 1.6223808526992798, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.279731035232544, 'test/num_examples': 10000, 'score': 23517.7104511261, 'total_duration': 25806.25092124939, 'accumulated_submission_time': 23517.7104511261, 'accumulated_eval_time': 2277.2179844379425, 'accumulated_logging_time': 4.621394872665405, 'global_step': 60671, 'preemption_count': 0}), (61960, {'train/accuracy': 0.7463727593421936, 'train/loss': 1.210816740989685, 'validation/accuracy': 0.6752200126647949, 'validation/loss': 1.5320411920547485, 'validation/num_examples': 50000, 'test/accuracy': 0.5464000105857849, 'test/loss': 2.1709399223327637, 'test/num_examples': 10000, 'score': 24027.76775407791, 'total_duration': 26369.81768345833, 'accumulated_submission_time': 24027.76775407791, 'accumulated_eval_time': 2330.4048466682434, 'accumulated_logging_time': 4.786064386367798, 'global_step': 61960, 'preemption_count': 0}), (63241, {'train/accuracy': 0.7547233700752258, 'train/loss': 1.2259612083435059, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5537362098693848, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.22103214263916, 'test/num_examples': 10000, 'score': 24537.714914798737, 'total_duration': 26942.08695077896, 'accumulated_submission_time': 24537.714914798737, 'accumulated_eval_time': 2392.4241247177124, 'accumulated_logging_time': 4.930222272872925, 'global_step': 63241, 'preemption_count': 0}), (64515, {'train/accuracy': 0.7471898794174194, 'train/loss': 1.169657588005066, 'validation/accuracy': 0.6731799840927124, 'validation/loss': 1.5018417835235596, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.1772921085357666, 'test/num_examples': 10000, 'score': 25047.46474480629, 'total_duration': 27509.810849905014, 'accumulated_submission_time': 25047.46474480629, 'accumulated_eval_time': 2450.088513612747, 'accumulated_logging_time': 5.0803258419036865, 'global_step': 64515, 'preemption_count': 0}), (65791, {'train/accuracy': 0.7495216727256775, 'train/loss': 1.172637939453125, 'validation/accuracy': 0.6743599772453308, 'validation/loss': 1.5099639892578125, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.1694185733795166, 'test/num_examples': 10000, 'score': 25557.217126131058, 'total_duration': 28072.56822991371, 'accumulated_submission_time': 25557.217126131058, 'accumulated_eval_time': 2502.8118398189545, 'accumulated_logging_time': 5.20369029045105, 'global_step': 65791, 'preemption_count': 0}), (67046, {'train/accuracy': 0.7508171200752258, 'train/loss': 1.1620087623596191, 'validation/accuracy': 0.677079975605011, 'validation/loss': 1.4924728870391846, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.165454864501953, 'test/num_examples': 10000, 'score': 26067.101620435715, 'total_duration': 28629.710269212723, 'accumulated_submission_time': 26067.101620435715, 'accumulated_eval_time': 2549.7783250808716, 'accumulated_logging_time': 5.338649034500122, 'global_step': 67046, 'preemption_count': 0}), (68328, {'train/accuracy': 0.7555603981018066, 'train/loss': 1.1769049167633057, 'validation/accuracy': 0.6757000088691711, 'validation/loss': 1.5179646015167236, 'validation/num_examples': 50000, 'test/accuracy': 0.553600013256073, 'test/loss': 2.147552728652954, 'test/num_examples': 10000, 'score': 26576.909748077393, 'total_duration': 29191.676191568375, 'accumulated_submission_time': 26576.909748077393, 'accumulated_eval_time': 2601.6406919956207, 'accumulated_logging_time': 5.473933219909668, 'global_step': 68328, 'preemption_count': 0}), (69504, {'train/accuracy': 0.7591079473495483, 'train/loss': 1.165501356124878, 'validation/accuracy': 0.675059974193573, 'validation/loss': 1.5185964107513428, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.1634695529937744, 'test/num_examples': 10000, 'score': 27086.707017421722, 'total_duration': 29748.084569454193, 'accumulated_submission_time': 27086.707017421722, 'accumulated_eval_time': 2647.9743316173553, 'accumulated_logging_time': 5.604231595993042, 'global_step': 69504, 'preemption_count': 0}), (70788, {'train/accuracy': 0.7620774507522583, 'train/loss': 1.177040934562683, 'validation/accuracy': 0.6827600002288818, 'validation/loss': 1.529455304145813, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.1483676433563232, 'test/num_examples': 10000, 'score': 27596.426473617554, 'total_duration': 30308.234234809875, 'accumulated_submission_time': 27596.426473617554, 'accumulated_eval_time': 2698.021147251129, 'accumulated_logging_time': 5.833938360214233, 'global_step': 70788, 'preemption_count': 0}), (72045, {'train/accuracy': 0.7677175998687744, 'train/loss': 1.107324242591858, 'validation/accuracy': 0.6855999827384949, 'validation/loss': 1.4698505401611328, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.1142189502716064, 'test/num_examples': 10000, 'score': 28106.427806854248, 'total_duration': 30860.596434116364, 'accumulated_submission_time': 28106.427806854248, 'accumulated_eval_time': 2740.0905561447144, 'accumulated_logging_time': 5.975797176361084, 'global_step': 72045, 'preemption_count': 0}), (73297, {'train/accuracy': 0.7648476958274841, 'train/loss': 1.1459081172943115, 'validation/accuracy': 0.6822400093078613, 'validation/loss': 1.502637267112732, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 2.1479387283325195, 'test/num_examples': 10000, 'score': 28616.536382198334, 'total_duration': 31411.28459239006, 'accumulated_submission_time': 28616.536382198334, 'accumulated_eval_time': 2780.4086968898773, 'accumulated_logging_time': 6.0878424644470215, 'global_step': 73297, 'preemption_count': 0}), (74494, {'train/accuracy': 0.7622767686843872, 'train/loss': 1.1620529890060425, 'validation/accuracy': 0.6791399717330933, 'validation/loss': 1.5248935222625732, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.1987879276275635, 'test/num_examples': 10000, 'score': 29126.793616771698, 'total_duration': 31967.594815969467, 'accumulated_submission_time': 29126.793616771698, 'accumulated_eval_time': 2826.190417289734, 'accumulated_logging_time': 6.215915679931641, 'global_step': 74494, 'preemption_count': 0}), (75631, {'train/accuracy': 0.7773038744926453, 'train/loss': 1.1048989295959473, 'validation/accuracy': 0.6800199747085571, 'validation/loss': 1.5323559045791626, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.1937952041625977, 'test/num_examples': 10000, 'score': 29635.829847812653, 'total_duration': 32522.152797698975, 'accumulated_submission_time': 29635.829847812653, 'accumulated_eval_time': 2870.66894197464, 'accumulated_logging_time': 7.124221563339233, 'global_step': 75631, 'preemption_count': 0}), (76709, {'train/accuracy': 0.7604830861091614, 'train/loss': 1.1634560823440552, 'validation/accuracy': 0.6773799657821655, 'validation/loss': 1.5237208604812622, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 2.1779494285583496, 'test/num_examples': 10000, 'score': 30145.77255797386, 'total_duration': 33088.52205848694, 'accumulated_submission_time': 30145.77255797386, 'accumulated_eval_time': 2926.8631818294525, 'accumulated_logging_time': 7.230270862579346, 'global_step': 76709, 'preemption_count': 0}), (77943, {'train/accuracy': 0.7495415806770325, 'train/loss': 1.237155556678772, 'validation/accuracy': 0.6779400110244751, 'validation/loss': 1.560349702835083, 'validation/num_examples': 50000, 'test/accuracy': 0.5441000461578369, 'test/loss': 2.215446949005127, 'test/num_examples': 10000, 'score': 30655.801669836044, 'total_duration': 33643.0664191246, 'accumulated_submission_time': 30655.801669836044, 'accumulated_eval_time': 2971.097531557083, 'accumulated_logging_time': 7.3665220737457275, 'global_step': 77943, 'preemption_count': 0}), (78936, {'train/accuracy': 0.7708864808082581, 'train/loss': 1.1294012069702148, 'validation/accuracy': 0.6919400095939636, 'validation/loss': 1.4710290431976318, 'validation/num_examples': 50000, 'test/accuracy': 0.5579000115394592, 'test/loss': 2.1404590606689453, 'test/num_examples': 10000, 'score': 31165.594794988632, 'total_duration': 34197.5065677166, 'accumulated_submission_time': 31165.594794988632, 'accumulated_eval_time': 3015.51820397377, 'accumulated_logging_time': 7.475649833679199, 'global_step': 78936, 'preemption_count': 0}), (80177, {'train/accuracy': 0.7633529901504517, 'train/loss': 1.1053038835525513, 'validation/accuracy': 0.6852799654006958, 'validation/loss': 1.4545778036117554, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.107264518737793, 'test/num_examples': 10000, 'score': 31675.427356481552, 'total_duration': 34753.1281747818, 'accumulated_submission_time': 31675.427356481552, 'accumulated_eval_time': 3061.055492401123, 'accumulated_logging_time': 7.58136510848999, 'global_step': 80177, 'preemption_count': 0}), (81231, {'train/accuracy': 0.7667211294174194, 'train/loss': 1.1120444536209106, 'validation/accuracy': 0.6812399625778198, 'validation/loss': 1.4861243963241577, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.177626371383667, 'test/num_examples': 10000, 'score': 32185.51617050171, 'total_duration': 35319.66288971901, 'accumulated_submission_time': 32185.51617050171, 'accumulated_eval_time': 3117.266996860504, 'accumulated_logging_time': 7.694184064865112, 'global_step': 81231, 'preemption_count': 0}), (82364, {'train/accuracy': 0.7679169178009033, 'train/loss': 1.10458505153656, 'validation/accuracy': 0.6771399974822998, 'validation/loss': 1.4980515241622925, 'validation/num_examples': 50000, 'test/accuracy': 0.5519000291824341, 'test/loss': 2.148900032043457, 'test/num_examples': 10000, 'score': 32695.381889104843, 'total_duration': 35887.24229502678, 'accumulated_submission_time': 32695.381889104843, 'accumulated_eval_time': 3174.7047324180603, 'accumulated_logging_time': 7.836381435394287, 'global_step': 82364, 'preemption_count': 0}), (83334, {'train/accuracy': 0.8089126348495483, 'train/loss': 0.9515841603279114, 'validation/accuracy': 0.6881999969482422, 'validation/loss': 1.4534344673156738, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.1114156246185303, 'test/num_examples': 10000, 'score': 33205.43201088905, 'total_duration': 36438.139144182205, 'accumulated_submission_time': 33205.43201088905, 'accumulated_eval_time': 3215.289463043213, 'accumulated_logging_time': 7.983832836151123, 'global_step': 83334, 'preemption_count': 0}), (84209, {'train/accuracy': 0.7755500674247742, 'train/loss': 1.0668928623199463, 'validation/accuracy': 0.69514000415802, 'validation/loss': 1.4248559474945068, 'validation/num_examples': 50000, 'test/accuracy': 0.5685999989509583, 'test/loss': 2.0720713138580322, 'test/num_examples': 10000, 'score': 33715.3077750206, 'total_duration': 36991.36394572258, 'accumulated_submission_time': 33715.3077750206, 'accumulated_eval_time': 3258.392363548279, 'accumulated_logging_time': 8.128730058670044, 'global_step': 84209, 'preemption_count': 0}), (84975, {'train/accuracy': 0.7753507494926453, 'train/loss': 1.0685268640518188, 'validation/accuracy': 0.6874200105667114, 'validation/loss': 1.4645692110061646, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.135952949523926, 'test/num_examples': 10000, 'score': 34225.21005773544, 'total_duration': 37541.50943017006, 'accumulated_submission_time': 34225.21005773544, 'accumulated_eval_time': 3298.3982903957367, 'accumulated_logging_time': 8.280007362365723, 'global_step': 84975, 'preemption_count': 0}), (85822, {'train/accuracy': 0.7726801633834839, 'train/loss': 1.0913530588150024, 'validation/accuracy': 0.6910799741744995, 'validation/loss': 1.4536737203598022, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 2.07099986076355, 'test/num_examples': 10000, 'score': 34735.47550153732, 'total_duration': 38089.56416344643, 'accumulated_submission_time': 34735.47550153732, 'accumulated_eval_time': 3335.974443912506, 'accumulated_logging_time': 8.370090961456299, 'global_step': 85822, 'preemption_count': 0}), (86741, {'train/accuracy': 0.77347731590271, 'train/loss': 1.1010082960128784, 'validation/accuracy': 0.6898199915885925, 'validation/loss': 1.457945466041565, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.113004207611084, 'test/num_examples': 10000, 'score': 35245.44828724861, 'total_duration': 38639.041773080826, 'accumulated_submission_time': 35245.44828724861, 'accumulated_eval_time': 3375.185043811798, 'accumulated_logging_time': 8.537782192230225, 'global_step': 86741, 'preemption_count': 0}), (87564, {'train/accuracy': 0.7821866869926453, 'train/loss': 1.0664405822753906, 'validation/accuracy': 0.6843000054359436, 'validation/loss': 1.4900623559951782, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.1404166221618652, 'test/num_examples': 10000, 'score': 35755.461497068405, 'total_duration': 39186.72447896004, 'accumulated_submission_time': 35755.461497068405, 'accumulated_eval_time': 3412.6331615448, 'accumulated_logging_time': 8.624871492385864, 'global_step': 87564, 'preemption_count': 0}), (88409, {'train/accuracy': 0.7744738459587097, 'train/loss': 1.0919506549835205, 'validation/accuracy': 0.6951999664306641, 'validation/loss': 1.4406651258468628, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 2.0831708908081055, 'test/num_examples': 10000, 'score': 36265.38892579079, 'total_duration': 39736.07839012146, 'accumulated_submission_time': 36265.38892579079, 'accumulated_eval_time': 3451.788872241974, 'accumulated_logging_time': 8.782371520996094, 'global_step': 88409, 'preemption_count': 0}), (89258, {'train/accuracy': 0.7863121628761292, 'train/loss': 1.01169753074646, 'validation/accuracy': 0.6937999725341797, 'validation/loss': 1.4100854396820068, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 2.0428457260131836, 'test/num_examples': 10000, 'score': 36775.75869703293, 'total_duration': 40283.95372772217, 'accumulated_submission_time': 36775.75869703293, 'accumulated_eval_time': 3489.080422639847, 'accumulated_logging_time': 8.881547212600708, 'global_step': 89258, 'preemption_count': 0}), (89962, {'train/accuracy': 0.8044084906578064, 'train/loss': 0.9964030385017395, 'validation/accuracy': 0.6862199902534485, 'validation/loss': 1.499239206314087, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.1454038619995117, 'test/num_examples': 10000, 'score': 37285.75470209122, 'total_duration': 40829.878294467926, 'accumulated_submission_time': 37285.75470209122, 'accumulated_eval_time': 3524.806770801544, 'accumulated_logging_time': 8.980858325958252, 'global_step': 89962, 'preemption_count': 0}), (90644, {'train/accuracy': 0.7860331535339355, 'train/loss': 1.073421835899353, 'validation/accuracy': 0.6959799528121948, 'validation/loss': 1.4504474401474, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 2.0913236141204834, 'test/num_examples': 10000, 'score': 37795.98446011543, 'total_duration': 41376.45565891266, 'accumulated_submission_time': 37795.98446011543, 'accumulated_eval_time': 3561.029732942581, 'accumulated_logging_time': 9.029114484786987, 'global_step': 90644, 'preemption_count': 0}), (91146, {'train/accuracy': 0.7768654227256775, 'train/loss': 1.0826961994171143, 'validation/accuracy': 0.6928600072860718, 'validation/loss': 1.452547311782837, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 2.0919785499572754, 'test/num_examples': 10000, 'score': 38305.88724422455, 'total_duration': 41920.34286212921, 'accumulated_submission_time': 38305.88724422455, 'accumulated_eval_time': 3594.863690137863, 'accumulated_logging_time': 9.121721744537354, 'global_step': 91146, 'preemption_count': 0}), (91898, {'train/accuracy': 0.7866509556770325, 'train/loss': 1.0733829736709595, 'validation/accuracy': 0.6958000063896179, 'validation/loss': 1.4727671146392822, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.099191427230835, 'test/num_examples': 10000, 'score': 38816.12714076042, 'total_duration': 42467.47746562958, 'accumulated_submission_time': 38816.12714076042, 'accumulated_eval_time': 3631.624750137329, 'accumulated_logging_time': 9.17157244682312, 'global_step': 91898, 'preemption_count': 0}), (92347, {'train/accuracy': 0.7689133882522583, 'train/loss': 1.0710959434509277, 'validation/accuracy': 0.6904199719429016, 'validation/loss': 1.4342259168624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0925800800323486, 'test/num_examples': 10000, 'score': 39329.58285307884, 'total_duration': 43015.16115760803, 'accumulated_submission_time': 39329.58285307884, 'accumulated_eval_time': 3665.6978204250336, 'accumulated_logging_time': 9.276031732559204, 'global_step': 92347, 'preemption_count': 0}), (92964, {'train/accuracy': 0.7914540767669678, 'train/loss': 1.0489636659622192, 'validation/accuracy': 0.6917399764060974, 'validation/loss': 1.4696197509765625, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 2.1137452125549316, 'test/num_examples': 10000, 'score': 39839.65118050575, 'total_duration': 43561.359634399414, 'accumulated_submission_time': 39839.65118050575, 'accumulated_eval_time': 3701.6831691265106, 'accumulated_logging_time': 9.348866939544678, 'global_step': 92964, 'preemption_count': 0}), (93393, {'train/accuracy': 0.7860929369926453, 'train/loss': 1.0468393564224243, 'validation/accuracy': 0.69896000623703, 'validation/loss': 1.4283665418624878, 'validation/num_examples': 50000, 'test/accuracy': 0.5758000016212463, 'test/loss': 2.05535888671875, 'test/num_examples': 10000, 'score': 40350.85218667984, 'total_duration': 44106.8616027832, 'accumulated_submission_time': 40350.85218667984, 'accumulated_eval_time': 3735.875420331955, 'accumulated_logging_time': 9.408864974975586, 'global_step': 93393, 'preemption_count': 0}), (93895, {'train/accuracy': 0.7711853981018066, 'train/loss': 1.1222543716430664, 'validation/accuracy': 0.6908400058746338, 'validation/loss': 1.479730248451233, 'validation/num_examples': 50000, 'test/accuracy': 0.565500020980835, 'test/loss': 2.1182796955108643, 'test/num_examples': 10000, 'score': 40861.59526729584, 'total_duration': 44651.774198532104, 'accumulated_submission_time': 40861.59526729584, 'accumulated_eval_time': 3769.9242022037506, 'accumulated_logging_time': 9.473358631134033, 'global_step': 93895, 'preemption_count': 0}), (94347, {'train/accuracy': 0.8000637888908386, 'train/loss': 0.9999526739120483, 'validation/accuracy': 0.7022599577903748, 'validation/loss': 1.42261803150177, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 2.066884756088257, 'test/num_examples': 10000, 'score': 41372.81498336792, 'total_duration': 45196.191808223724, 'accumulated_submission_time': 41372.81498336792, 'accumulated_eval_time': 3803.0272052288055, 'accumulated_logging_time': 9.517786979675293, 'global_step': 94347, 'preemption_count': 0}), (94807, {'train/accuracy': 0.7833425998687744, 'train/loss': 1.0782657861709595, 'validation/accuracy': 0.6968399882316589, 'validation/loss': 1.4597746133804321, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 2.080094575881958, 'test/num_examples': 10000, 'score': 41883.268496990204, 'total_duration': 45745.91068840027, 'accumulated_submission_time': 41883.268496990204, 'accumulated_eval_time': 3842.1428899765015, 'accumulated_logging_time': 9.613422632217407, 'global_step': 94807, 'preemption_count': 0}), (95342, {'train/accuracy': 0.7805923223495483, 'train/loss': 1.0701628923416138, 'validation/accuracy': 0.697160005569458, 'validation/loss': 1.447644591331482, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.0923893451690674, 'test/num_examples': 10000, 'score': 42393.314445734024, 'total_duration': 46292.46217918396, 'accumulated_submission_time': 42393.314445734024, 'accumulated_eval_time': 3878.4994938373566, 'accumulated_logging_time': 9.70279836654663, 'global_step': 95342, 'preemption_count': 0}), (95731, {'train/accuracy': 0.7913345098495483, 'train/loss': 1.0269100666046143, 'validation/accuracy': 0.6927599906921387, 'validation/loss': 1.4490102529525757, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 2.124859571456909, 'test/num_examples': 10000, 'score': 42903.314015865326, 'total_duration': 46848.056789159775, 'accumulated_submission_time': 42903.314015865326, 'accumulated_eval_time': 3923.982547521591, 'accumulated_logging_time': 9.770930290222168, 'global_step': 95731, 'preemption_count': 0}), (96062, {'train/accuracy': 0.7967354655265808, 'train/loss': 1.04228937625885, 'validation/accuracy': 0.7035599946975708, 'validation/loss': 1.4404469728469849, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 2.0683138370513916, 'test/num_examples': 10000, 'score': 43413.27730894089, 'total_duration': 47393.449577093124, 'accumulated_submission_time': 43413.27730894089, 'accumulated_eval_time': 3959.288734436035, 'accumulated_logging_time': 9.857762336730957, 'global_step': 96062, 'preemption_count': 0}), (96537, {'train/accuracy': 0.7886240482330322, 'train/loss': 1.0366038084030151, 'validation/accuracy': 0.7020399570465088, 'validation/loss': 1.4125021696090698, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 2.0534791946411133, 'test/num_examples': 10000, 'score': 43923.54673600197, 'total_duration': 47938.79158616066, 'accumulated_submission_time': 43923.54673600197, 'accumulated_eval_time': 3994.255005121231, 'accumulated_logging_time': 9.910648584365845, 'global_step': 96537, 'preemption_count': 0}), (97030, {'train/accuracy': 0.8112643361091614, 'train/loss': 0.9475439190864563, 'validation/accuracy': 0.7026199698448181, 'validation/loss': 1.4059715270996094, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 2.0581629276275635, 'test/num_examples': 10000, 'score': 44436.513724565506, 'total_duration': 48502.69314837456, 'accumulated_submission_time': 44436.513724565506, 'accumulated_eval_time': 4045.063665151596, 'accumulated_logging_time': 9.981146812438965, 'global_step': 97030, 'preemption_count': 0}), (97579, {'train/accuracy': 0.7944634556770325, 'train/loss': 1.0361807346343994, 'validation/accuracy': 0.7009599804878235, 'validation/loss': 1.4246653318405151, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.088813066482544, 'test/num_examples': 10000, 'score': 44946.44351410866, 'total_duration': 49051.01607656479, 'accumulated_submission_time': 44946.44351410866, 'accumulated_eval_time': 4083.3343465328217, 'accumulated_logging_time': 10.042106866836548, 'global_step': 97579, 'preemption_count': 0}), (98041, {'train/accuracy': 0.7828842401504517, 'train/loss': 1.0549447536468506, 'validation/accuracy': 0.6923399567604065, 'validation/loss': 1.4419994354248047, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 2.082357406616211, 'test/num_examples': 10000, 'score': 45458.03125190735, 'total_duration': 49598.778399944305, 'accumulated_submission_time': 45458.03125190735, 'accumulated_eval_time': 4119.384139537811, 'accumulated_logging_time': 10.116724729537964, 'global_step': 98041, 'preemption_count': 0}), (98262, {'train/accuracy': 0.8039500713348389, 'train/loss': 0.9573618769645691, 'validation/accuracy': 0.697160005569458, 'validation/loss': 1.4014830589294434, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 2.065692186355591, 'test/num_examples': 10000, 'score': 45968.87014555931, 'total_duration': 50145.01964735985, 'accumulated_submission_time': 45968.87014555931, 'accumulated_eval_time': 4154.715940475464, 'accumulated_logging_time': 10.163469791412354, 'global_step': 98262, 'preemption_count': 0}), (98761, {'train/accuracy': 0.7974728941917419, 'train/loss': 1.007771372795105, 'validation/accuracy': 0.7041999697685242, 'validation/loss': 1.407898187637329, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 2.0709941387176514, 'test/num_examples': 10000, 'score': 46480.6161339283, 'total_duration': 50699.43836903572, 'accumulated_submission_time': 46480.6161339283, 'accumulated_eval_time': 4197.305947065353, 'accumulated_logging_time': 10.191502094268799, 'global_step': 98761, 'preemption_count': 0}), (99121, {'train/accuracy': 0.7800741195678711, 'train/loss': 1.090338945388794, 'validation/accuracy': 0.694599986076355, 'validation/loss': 1.46392822265625, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 2.115920305252075, 'test/num_examples': 10000, 'score': 46990.95895695686, 'total_duration': 51246.417199611664, 'accumulated_submission_time': 46990.95895695686, 'accumulated_eval_time': 4233.849811077118, 'accumulated_logging_time': 10.243295907974243, 'global_step': 99121, 'preemption_count': 0}), (99374, {'train/accuracy': 0.7768654227256775, 'train/loss': 1.0770403146743774, 'validation/accuracy': 0.6903600096702576, 'validation/loss': 1.4673442840576172, 'validation/num_examples': 50000, 'test/accuracy': 0.5656000375747681, 'test/loss': 2.115102529525757, 'test/num_examples': 10000, 'score': 47501.763241291046, 'total_duration': 51800.98893880844, 'accumulated_submission_time': 47501.763241291046, 'accumulated_eval_time': 4277.514222621918, 'accumulated_logging_time': 10.318831205368042, 'global_step': 99374, 'preemption_count': 0}), (99667, {'train/accuracy': 0.7882851958274841, 'train/loss': 1.040691614151001, 'validation/accuracy': 0.6994199752807617, 'validation/loss': 1.4258712530136108, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 2.086155414581299, 'test/num_examples': 10000, 'score': 48011.71732091904, 'total_duration': 52346.09106040001, 'accumulated_submission_time': 48011.71732091904, 'accumulated_eval_time': 4312.596350669861, 'accumulated_logging_time': 10.352495431900024, 'global_step': 99667, 'preemption_count': 0}), (100434, {'train/accuracy': 0.7889827489852905, 'train/loss': 1.0474351644515991, 'validation/accuracy': 0.6964199542999268, 'validation/loss': 1.4476568698883057, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 2.072035789489746, 'test/num_examples': 10000, 'score': 48522.0870552063, 'total_duration': 52891.91120219231, 'accumulated_submission_time': 48522.0870552063, 'accumulated_eval_time': 4347.9282331466675, 'accumulated_logging_time': 10.385234832763672, 'global_step': 100434, 'preemption_count': 0}), (100910, {'train/accuracy': 0.7965361475944519, 'train/loss': 0.994647204875946, 'validation/accuracy': 0.7077999711036682, 'validation/loss': 1.3743327856063843, 'validation/num_examples': 50000, 'test/accuracy': 0.5797000527381897, 'test/loss': 2.0325915813446045, 'test/num_examples': 10000, 'score': 49032.148569107056, 'total_duration': 53436.89508295059, 'accumulated_submission_time': 49032.148569107056, 'accumulated_eval_time': 4382.731330156326, 'accumulated_logging_time': 10.450151920318604, 'global_step': 100910, 'preemption_count': 0}), (101709, {'train/accuracy': 0.7998843789100647, 'train/loss': 0.9931908845901489, 'validation/accuracy': 0.7041400074958801, 'validation/loss': 1.399848222732544, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 2.049638509750366, 'test/num_examples': 10000, 'score': 49543.38068342209, 'total_duration': 53985.514385938644, 'accumulated_submission_time': 49543.38068342209, 'accumulated_eval_time': 4419.976241350174, 'accumulated_logging_time': 10.503279685974121, 'global_step': 101709, 'preemption_count': 0}), (101957, {'train/accuracy': 0.7979312539100647, 'train/loss': 0.9683020114898682, 'validation/accuracy': 0.7083199620246887, 'validation/loss': 1.3619030714035034, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 2.017000675201416, 'test/num_examples': 10000, 'score': 50054.9244158268, 'total_duration': 54535.75826764107, 'accumulated_submission_time': 50054.9244158268, 'accumulated_eval_time': 4458.606889486313, 'accumulated_logging_time': 10.546168327331543, 'global_step': 101957, 'preemption_count': 0}), (102123, {'train/accuracy': 0.7993662357330322, 'train/loss': 0.9966561794281006, 'validation/accuracy': 0.7048400044441223, 'validation/loss': 1.4022161960601807, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 2.023531436920166, 'test/num_examples': 10000, 'score': 50566.73064374924, 'total_duration': 55094.33583045006, 'accumulated_submission_time': 50566.73064374924, 'accumulated_eval_time': 4505.325449228287, 'accumulated_logging_time': 10.580280065536499, 'global_step': 102123, 'preemption_count': 0}), (102533, {'train/accuracy': 0.8286033272743225, 'train/loss': 0.8490793704986572, 'validation/accuracy': 0.7035999894142151, 'validation/loss': 1.3774374723434448, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 2.0284581184387207, 'test/num_examples': 10000, 'score': 51076.85154104233, 'total_duration': 55642.3013548851, 'accumulated_submission_time': 51076.85154104233, 'accumulated_eval_time': 4543.090004444122, 'accumulated_logging_time': 10.614437580108643, 'global_step': 102533, 'preemption_count': 0}), (102778, {'train/accuracy': 0.8122807741165161, 'train/loss': 0.9185537695884705, 'validation/accuracy': 0.7065399885177612, 'validation/loss': 1.3766194581985474, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 2.0210719108581543, 'test/num_examples': 10000, 'score': 51588.02870750427, 'total_duration': 56193.39591169357, 'accumulated_submission_time': 51588.02870750427, 'accumulated_eval_time': 4582.843652009964, 'accumulated_logging_time': 10.751986026763916, 'global_step': 102778, 'preemption_count': 0}), (102908, {'train/accuracy': 0.8042490482330322, 'train/loss': 0.9769500494003296, 'validation/accuracy': 0.7034800052642822, 'validation/loss': 1.4117834568023682, 'validation/num_examples': 50000, 'test/accuracy': 0.573900043964386, 'test/loss': 2.0533857345581055, 'test/num_examples': 10000, 'score': 52102.27171206474, 'total_duration': 56755.000868320465, 'accumulated_submission_time': 52102.27171206474, 'accumulated_eval_time': 4630.157147884369, 'accumulated_logging_time': 10.786733150482178, 'global_step': 102908, 'preemption_count': 0}), (103219, {'train/accuracy': 0.8055444955825806, 'train/loss': 0.9517120122909546, 'validation/accuracy': 0.7067999839782715, 'validation/loss': 1.373279333114624, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 2.027841806411743, 'test/num_examples': 10000, 'score': 52612.96303200722, 'total_duration': 57301.61632394791, 'accumulated_submission_time': 52612.96303200722, 'accumulated_eval_time': 4666.012334823608, 'accumulated_logging_time': 10.821454524993896, 'global_step': 103219, 'preemption_count': 0}), (103608, {'train/accuracy': 0.7960180044174194, 'train/loss': 0.9998266100883484, 'validation/accuracy': 0.7076599597930908, 'validation/loss': 1.3930076360702515, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 2.0357723236083984, 'test/num_examples': 10000, 'score': 53123.158005952835, 'total_duration': 57847.39448928833, 'accumulated_submission_time': 53123.158005952835, 'accumulated_eval_time': 4701.446239233017, 'accumulated_logging_time': 10.927058935165405, 'global_step': 103608, 'preemption_count': 0}), (104077, {'train/accuracy': 0.818379282951355, 'train/loss': 0.9330296516418457, 'validation/accuracy': 0.6978799700737, 'validation/loss': 1.4370510578155518, 'validation/num_examples': 50000, 'test/accuracy': 0.5703000426292419, 'test/loss': 2.093350410461426, 'test/num_examples': 10000, 'score': 53634.00277853012, 'total_duration': 58393.02249622345, 'accumulated_submission_time': 53634.00277853012, 'accumulated_eval_time': 4736.1150550842285, 'accumulated_logging_time': 10.989583253860474, 'global_step': 104077, 'preemption_count': 0}), (104224, {'train/accuracy': 0.8160873651504517, 'train/loss': 0.9032125473022461, 'validation/accuracy': 0.7079199552536011, 'validation/loss': 1.3705179691314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 2.0048046112060547, 'test/num_examples': 10000, 'score': 54146.575229644775, 'total_duration': 58945.76935505867, 'accumulated_submission_time': 54146.575229644775, 'accumulated_eval_time': 4776.227821826935, 'accumulated_logging_time': 11.03572392463684, 'global_step': 104224, 'preemption_count': 0}), (104348, {'train/accuracy': 0.8147321343421936, 'train/loss': 0.9399213194847107, 'validation/accuracy': 0.7075999975204468, 'validation/loss': 1.3960484266281128, 'validation/num_examples': 50000, 'test/accuracy': 0.5830000042915344, 'test/loss': 2.030853509902954, 'test/num_examples': 10000, 'score': 54657.30204105377, 'total_duration': 59496.49066925049, 'accumulated_submission_time': 54657.30204105377, 'accumulated_eval_time': 4816.177689790726, 'accumulated_logging_time': 11.067471981048584, 'global_step': 104348, 'preemption_count': 0}), (104472, {'train/accuracy': 0.81253981590271, 'train/loss': 0.9260028004646301, 'validation/accuracy': 0.706119954586029, 'validation/loss': 1.372018575668335, 'validation/num_examples': 50000, 'test/accuracy': 0.5831000208854675, 'test/loss': 1.9958629608154297, 'test/num_examples': 10000, 'score': 55171.312621831894, 'total_duration': 60055.873728990555, 'accumulated_submission_time': 55171.312621831894, 'accumulated_eval_time': 4861.5030863285065, 'accumulated_logging_time': 11.101954221725464, 'global_step': 104472, 'preemption_count': 0}), (104596, {'train/accuracy': 0.8046875, 'train/loss': 0.9457805156707764, 'validation/accuracy': 0.7028599977493286, 'validation/loss': 1.3919482231140137, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 2.0197806358337402, 'test/num_examples': 10000, 'score': 55681.32037258148, 'total_duration': 60608.233352184296, 'accumulated_submission_time': 55681.32037258148, 'accumulated_eval_time': 4903.807421445847, 'accumulated_logging_time': 11.136282444000244, 'global_step': 104596, 'preemption_count': 0}), (104773, {'train/accuracy': 0.8040298223495483, 'train/loss': 0.9433786869049072, 'validation/accuracy': 0.7065399885177612, 'validation/loss': 1.3692487478256226, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.9958858489990234, 'test/num_examples': 10000, 'score': 56191.984790086746, 'total_duration': 61153.583532333374, 'accumulated_submission_time': 56191.984790086746, 'accumulated_eval_time': 4938.43906712532, 'accumulated_logging_time': 11.170869588851929, 'global_step': 104773, 'preemption_count': 0}), (105338, {'train/accuracy': 0.7976921200752258, 'train/loss': 0.9928430914878845, 'validation/accuracy': 0.706779956817627, 'validation/loss': 1.3878968954086304, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 2.0457255840301514, 'test/num_examples': 10000, 'score': 56702.651848077774, 'total_duration': 61697.4866399765, 'accumulated_submission_time': 56702.651848077774, 'accumulated_eval_time': 4971.547793626785, 'accumulated_logging_time': 11.23337984085083, 'global_step': 105338, 'preemption_count': 0}), (105485, {'train/accuracy': 0.7901387214660645, 'train/loss': 1.0377357006072998, 'validation/accuracy': 0.6972399950027466, 'validation/loss': 1.4346237182617188, 'validation/num_examples': 50000, 'test/accuracy': 0.5764999985694885, 'test/loss': 2.07525634765625, 'test/num_examples': 10000, 'score': 57213.37969374657, 'total_duration': 62247.45408940315, 'accumulated_submission_time': 57213.37969374657, 'accumulated_eval_time': 5010.699824810028, 'accumulated_logging_time': 11.304818630218506, 'global_step': 105485, 'preemption_count': 0}), (105609, {'train/accuracy': 0.7908163070678711, 'train/loss': 1.0199248790740967, 'validation/accuracy': 0.7014999985694885, 'validation/loss': 1.4115504026412964, 'validation/num_examples': 50000, 'test/accuracy': 0.5657000541687012, 'test/loss': 2.0867257118225098, 'test/num_examples': 10000, 'score': 57725.62155151367, 'total_duration': 62799.43053650856, 'accumulated_submission_time': 57725.62155151367, 'accumulated_eval_time': 5050.3886568546295, 'accumulated_logging_time': 11.337771654129028, 'global_step': 105609, 'preemption_count': 0}), (105734, {'train/accuracy': 0.8369738459587097, 'train/loss': 0.8548222780227661, 'validation/accuracy': 0.7094999551773071, 'validation/loss': 1.3812932968139648, 'validation/num_examples': 50000, 'test/accuracy': 0.5818000435829163, 'test/loss': 2.0417425632476807, 'test/num_examples': 10000, 'score': 58237.98412680626, 'total_duration': 63361.98986816406, 'accumulated_submission_time': 58237.98412680626, 'accumulated_eval_time': 5100.542615890503, 'accumulated_logging_time': 11.366876602172852, 'global_step': 105734, 'preemption_count': 0}), (105896, {'train/accuracy': 0.8235411047935486, 'train/loss': 0.8850187659263611, 'validation/accuracy': 0.7075799703598022, 'validation/loss': 1.367797613143921, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 2.022705078125, 'test/num_examples': 10000, 'score': 58748.622324466705, 'total_duration': 63908.26063513756, 'accumulated_submission_time': 58748.622324466705, 'accumulated_eval_time': 5136.109422683716, 'accumulated_logging_time': 11.414541721343994, 'global_step': 105896, 'preemption_count': 0}), (106363, {'train/accuracy': 0.8068598508834839, 'train/loss': 0.9442111849784851, 'validation/accuracy': 0.7069999575614929, 'validation/loss': 1.3726935386657715, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 2.015824556350708, 'test/num_examples': 10000, 'score': 59259.398359298706, 'total_duration': 64456.201979637146, 'accumulated_submission_time': 59259.398359298706, 'accumulated_eval_time': 5173.168258190155, 'accumulated_logging_time': 11.469578266143799, 'global_step': 106363, 'preemption_count': 0}), (106726, {'train/accuracy': 0.78910231590271, 'train/loss': 1.0440272092819214, 'validation/accuracy': 0.6992999911308289, 'validation/loss': 1.4296166896820068, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 2.081758737564087, 'test/num_examples': 10000, 'score': 59770.035422086716, 'total_duration': 65002.12698054314, 'accumulated_submission_time': 59770.035422086716, 'accumulated_eval_time': 5208.350335121155, 'accumulated_logging_time': 11.537103652954102, 'global_step': 106726, 'preemption_count': 0}), (107090, {'train/accuracy': 0.7955994606018066, 'train/loss': 0.9991288185119629, 'validation/accuracy': 0.6988199949264526, 'validation/loss': 1.4133193492889404, 'validation/num_examples': 50000, 'test/accuracy': 0.5748000144958496, 'test/loss': 2.055209159851074, 'test/num_examples': 10000, 'score': 60280.16394472122, 'total_duration': 65547.85793828964, 'accumulated_submission_time': 60280.16394472122, 'accumulated_eval_time': 5243.8534054756165, 'accumulated_logging_time': 11.596826314926147, 'global_step': 107090, 'preemption_count': 0}), (107453, {'train/accuracy': 0.8147720098495483, 'train/loss': 0.939460039138794, 'validation/accuracy': 0.70524001121521, 'validation/loss': 1.3963998556137085, 'validation/num_examples': 50000, 'test/accuracy': 0.5802000164985657, 'test/loss': 2.045531988143921, 'test/num_examples': 10000, 'score': 60791.30971312523, 'total_duration': 66094.709628582, 'accumulated_submission_time': 60791.30971312523, 'accumulated_eval_time': 5279.436554670334, 'accumulated_logging_time': 11.680552244186401, 'global_step': 107453, 'preemption_count': 0}), (107813, {'train/accuracy': 0.794343888759613, 'train/loss': 1.0124539136886597, 'validation/accuracy': 0.6936799883842468, 'validation/loss': 1.4419660568237305, 'validation/num_examples': 50000, 'test/accuracy': 0.5687000155448914, 'test/loss': 2.098330497741699, 'test/num_examples': 10000, 'score': 61301.904188632965, 'total_duration': 66640.73948550224, 'accumulated_submission_time': 61301.904188632965, 'accumulated_eval_time': 5314.729442358017, 'accumulated_logging_time': 11.784341096878052, 'global_step': 107813, 'preemption_count': 0}), (107969, {'train/accuracy': 0.8130978941917419, 'train/loss': 0.9509666562080383, 'validation/accuracy': 0.7145999670028687, 'validation/loss': 1.3696093559265137, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 2.0138440132141113, 'test/num_examples': 10000, 'score': 61814.96117520332, 'total_duration': 67201.91868257523, 'accumulated_submission_time': 61814.96117520332, 'accumulated_eval_time': 5362.733507394791, 'accumulated_logging_time': 11.885761499404907, 'global_step': 107969, 'preemption_count': 0}), (108092, {'train/accuracy': 0.8002431392669678, 'train/loss': 1.0036147832870483, 'validation/accuracy': 0.7033799886703491, 'validation/loss': 1.4123809337615967, 'validation/num_examples': 50000, 'test/accuracy': 0.5791000127792358, 'test/loss': 2.0430731773376465, 'test/num_examples': 10000, 'score': 62325.793167591095, 'total_duration': 67757.14371728897, 'accumulated_submission_time': 62325.793167591095, 'accumulated_eval_time': 5407.077453374863, 'accumulated_logging_time': 11.921654224395752, 'global_step': 108092, 'preemption_count': 0}), (108216, {'train/accuracy': 0.8045280575752258, 'train/loss': 0.9853981733322144, 'validation/accuracy': 0.7089200019836426, 'validation/loss': 1.4000835418701172, 'validation/num_examples': 50000, 'test/accuracy': 0.5813000202178955, 'test/loss': 2.04768705368042, 'test/num_examples': 10000, 'score': 62837.002648830414, 'total_duration': 68308.06570458412, 'accumulated_submission_time': 62837.002648830414, 'accumulated_eval_time': 5446.74084186554, 'accumulated_logging_time': 11.957589387893677, 'global_step': 108216, 'preemption_count': 0}), (108343, {'train/accuracy': 0.8109055757522583, 'train/loss': 0.9281376004219055, 'validation/accuracy': 0.715399980545044, 'validation/loss': 1.3417048454284668, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.9914907217025757, 'test/num_examples': 10000, 'score': 63347.27316713333, 'total_duration': 68864.14220428467, 'accumulated_submission_time': 63347.27316713333, 'accumulated_eval_time': 5490.748834371567, 'accumulated_logging_time': 13.741579294204712, 'global_step': 108343, 'preemption_count': 0}), (108578, {'train/accuracy': 0.8049266338348389, 'train/loss': 0.9976184368133545, 'validation/accuracy': 0.7079399824142456, 'validation/loss': 1.4058924913406372, 'validation/num_examples': 50000, 'test/accuracy': 0.582800030708313, 'test/loss': 2.03450345993042, 'test/num_examples': 10000, 'score': 63857.76262831688, 'total_duration': 69410.60951972008, 'accumulated_submission_time': 63857.76262831688, 'accumulated_eval_time': 5526.669007778168, 'accumulated_logging_time': 13.773183584213257, 'global_step': 108578, 'preemption_count': 0}), (109060, {'train/accuracy': 0.8085339665412903, 'train/loss': 0.9669428467750549, 'validation/accuracy': 0.7022599577903748, 'validation/loss': 1.4240390062332153, 'validation/num_examples': 50000, 'test/accuracy': 0.5703000426292419, 'test/loss': 2.068380355834961, 'test/num_examples': 10000, 'score': 64368.493617773056, 'total_duration': 69955.08388447762, 'accumulated_submission_time': 64368.493617773056, 'accumulated_eval_time': 5560.271856546402, 'accumulated_logging_time': 13.859530687332153, 'global_step': 109060, 'preemption_count': 0}), (109548, {'train/accuracy': 0.8052853941917419, 'train/loss': 0.9868594408035278, 'validation/accuracy': 0.711679995059967, 'validation/loss': 1.3989802598953247, 'validation/num_examples': 50000, 'test/accuracy': 0.5814000368118286, 'test/loss': 2.0523521900177, 'test/num_examples': 10000, 'score': 64879.0803527832, 'total_duration': 70499.52343559265, 'accumulated_submission_time': 64879.0803527832, 'accumulated_eval_time': 5594.0048451423645, 'accumulated_logging_time': 13.926214933395386, 'global_step': 109548, 'preemption_count': 0}), (110032, {'train/accuracy': 0.8101881146430969, 'train/loss': 0.9567452669143677, 'validation/accuracy': 0.7170199751853943, 'validation/loss': 1.3539681434631348, 'validation/num_examples': 50000, 'test/accuracy': 0.5900000333786011, 'test/loss': 1.9913678169250488, 'test/num_examples': 10000, 'score': 65389.48230099678, 'total_duration': 71045.20224094391, 'accumulated_submission_time': 65389.48230099678, 'accumulated_eval_time': 5629.159185886383, 'accumulated_logging_time': 13.994304895401001, 'global_step': 110032, 'preemption_count': 0}), (110514, {'train/accuracy': 0.8205915093421936, 'train/loss': 0.9008785486221313, 'validation/accuracy': 0.7108799815177917, 'validation/loss': 1.3629835844039917, 'validation/num_examples': 50000, 'test/accuracy': 0.5811000466346741, 'test/loss': 2.0012829303741455, 'test/num_examples': 10000, 'score': 65900.03963112831, 'total_duration': 71593.73402452469, 'accumulated_submission_time': 65900.03963112831, 'accumulated_eval_time': 5667.023441553116, 'accumulated_logging_time': 14.049530744552612, 'global_step': 110514, 'preemption_count': 0})], 'global_step': 111000}
I0308 05:12:36.306029 140341280416960 submission_runner.py:649] Timing: 66410.43762135506
I0308 05:12:36.306094 140341280416960 submission_runner.py:651] Total number of evals: 130
I0308 05:12:36.306127 140341280416960 submission_runner.py:652] ====================
I0308 05:12:36.306367 140341280416960 submission_runner.py:750] Final imagenet_resnet score: 1
