python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=693073869 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-09-40-16.log
2025-03-07 09:40:35.294217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741340435.943203       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741340436.110478       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 09:41:27.945074 140269360194752 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax.
I0307 09:41:30.396692 140269360194752 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 09:41:30.399326 140269360194752 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 09:41:30.432598 140269360194752 submission_runner.py:606] Using RNG seed 693073869
I0307 09:41:33.845730 140269360194752 submission_runner.py:615] --- Tuning run 5/5 ---
I0307 09:41:33.845924 140269360194752 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_5.
I0307 09:41:33.846104 140269360194752 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_5/hparams.json.
I0307 09:41:34.082225 140269360194752 submission_runner.py:218] Initializing dataset.
I0307 09:41:35.832822 140269360194752 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:41:36.203908 140269360194752 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:41:36.625178 140269360194752 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:41:38.788639 140269360194752 submission_runner.py:229] Initializing model.
I0307 09:42:04.066573 140269360194752 submission_runner.py:272] Initializing optimizer.
I0307 09:42:05.282492 140269360194752 submission_runner.py:279] Initializing metrics bundle.
I0307 09:42:05.282773 140269360194752 submission_runner.py:301] Initializing checkpoint and logger.
I0307 09:42:05.283909 140269360194752 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0307 09:42:05.284020 140269360194752 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_5/meta_data_0.json.
I0307 09:42:05.804260 140269360194752 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_5/flags_0.json.
I0307 09:42:06.142502 140269360194752 submission_runner.py:337] Starting training loop.
I0307 09:43:04.295576 140129721108224 logging_writer.py:48] [0] global_step=0, grad_norm=0.6830531358718872, loss=6.926575183868408
I0307 09:43:04.757829 140269360194752 spec.py:321] Evaluating on the training split.
I0307 09:43:05.220353 140269360194752 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:43:05.245158 140269360194752 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:43:05.287672 140269360194752 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:43:24.623477 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 09:43:25.195522 140269360194752 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:43:25.229462 140269360194752 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:43:25.488714 140269360194752 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:44:06.310711 140269360194752 spec.py:349] Evaluating on the test split.
I0307 09:44:06.783484 140269360194752 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:44:06.812406 140269360194752 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 09:44:06.852528 140269360194752 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:44:29.576589 140269360194752 submission_runner.py:469] Time since start: 143.43s, 	Step: 1, 	{'train/accuracy': 0.0009167729294858873, 'train/loss': 6.912870407104492, 'validation/accuracy': 0.0009399999980814755, 'validation/loss': 6.913017272949219, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.913171291351318, 'test/num_examples': 10000, 'score': 58.6150918006897, 'total_duration': 143.4340283870697, 'accumulated_submission_time': 58.6150918006897, 'accumulated_eval_time': 84.81869649887085, 'accumulated_logging_time': 0}
I0307 09:44:29.593466 140113493350144 logging_writer.py:48] [1] accumulated_eval_time=84.8187, accumulated_logging_time=0, accumulated_submission_time=58.6151, global_step=1, preemption_count=0, score=58.6151, test/accuracy=0.0011, test/loss=6.91317, test/num_examples=10000, total_duration=143.434, train/accuracy=0.000916773, train/loss=6.91287, validation/accuracy=0.00094, validation/loss=6.91302, validation/num_examples=50000
I0307 09:45:05.578701 140113484957440 logging_writer.py:48] [100] global_step=100, grad_norm=0.663215160369873, loss=6.820517063140869
I0307 09:45:41.426388 140113493350144 logging_writer.py:48] [200] global_step=200, grad_norm=0.8021809458732605, loss=6.518763542175293
I0307 09:46:18.103839 140113484957440 logging_writer.py:48] [300] global_step=300, grad_norm=0.910335898399353, loss=6.363659858703613
I0307 09:46:57.832573 140113493350144 logging_writer.py:48] [400] global_step=400, grad_norm=1.690433144569397, loss=5.9973344802856445
I0307 09:47:36.336384 140113484957440 logging_writer.py:48] [500] global_step=500, grad_norm=3.230761766433716, loss=5.913553714752197
I0307 09:48:14.859207 140113493350144 logging_writer.py:48] [600] global_step=600, grad_norm=4.4572038650512695, loss=5.684897422790527
I0307 09:48:53.316070 140113484957440 logging_writer.py:48] [700] global_step=700, grad_norm=3.9148809909820557, loss=5.452893257141113
I0307 09:49:32.250900 140113493350144 logging_writer.py:48] [800] global_step=800, grad_norm=2.681248426437378, loss=5.272940158843994
I0307 09:50:10.651373 140113484957440 logging_writer.py:48] [900] global_step=900, grad_norm=3.0304312705993652, loss=5.21433687210083
I0307 09:50:49.342757 140113493350144 logging_writer.py:48] [1000] global_step=1000, grad_norm=5.455102443695068, loss=5.076584815979004
I0307 09:51:27.932575 140113484957440 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.508203983306885, loss=5.009580135345459
I0307 09:52:06.403159 140113493350144 logging_writer.py:48] [1200] global_step=1200, grad_norm=4.796982288360596, loss=4.834217071533203
I0307 09:52:45.010694 140113484957440 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.535736560821533, loss=4.775853633880615
I0307 09:52:59.632722 140269360194752 spec.py:321] Evaluating on the training split.
I0307 09:53:11.374308 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 09:53:29.728194 140269360194752 spec.py:349] Evaluating on the test split.
I0307 09:53:31.681696 140269360194752 submission_runner.py:469] Time since start: 685.54s, 	Step: 1339, 	{'train/accuracy': 0.15929926931858063, 'train/loss': 4.339805603027344, 'validation/accuracy': 0.13293999433517456, 'validation/loss': 4.56612491607666, 'validation/num_examples': 50000, 'test/accuracy': 0.09910000115633011, 'test/loss': 5.026389122009277, 'test/num_examples': 10000, 'score': 568.4661321640015, 'total_duration': 685.5391473770142, 'accumulated_submission_time': 568.4661321640015, 'accumulated_eval_time': 116.86762952804565, 'accumulated_logging_time': 0.03965926170349121}
I0307 09:53:31.707851 140113501742848 logging_writer.py:48] [1339] accumulated_eval_time=116.868, accumulated_logging_time=0.0396593, accumulated_submission_time=568.466, global_step=1339, preemption_count=0, score=568.466, test/accuracy=0.0991, test/loss=5.02639, test/num_examples=10000, total_duration=685.539, train/accuracy=0.159299, train/loss=4.33981, validation/accuracy=0.13294, validation/loss=4.56612, validation/num_examples=50000
I0307 09:53:55.584006 140113510135552 logging_writer.py:48] [1400] global_step=1400, grad_norm=4.356955051422119, loss=4.582529067993164
I0307 09:54:33.864528 140113501742848 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.494131565093994, loss=4.620140552520752
I0307 09:55:12.569330 140113510135552 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.0473527908325195, loss=4.327848434448242
I0307 09:55:50.745071 140113501742848 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.501056671142578, loss=4.2329816818237305
I0307 09:56:29.324434 140113510135552 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.722610950469971, loss=4.212774753570557
I0307 09:57:07.815404 140113501742848 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.4604310989379883, loss=4.118921756744385
I0307 09:57:45.952387 140113510135552 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.120789527893066, loss=4.067012310028076
I0307 09:58:24.742531 140113501742848 logging_writer.py:48] [2100] global_step=2100, grad_norm=8.424397468566895, loss=4.098839282989502
I0307 09:59:03.135371 140113510135552 logging_writer.py:48] [2200] global_step=2200, grad_norm=7.069026470184326, loss=3.9518067836761475
I0307 09:59:41.876770 140113501742848 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.0526957511901855, loss=3.7798643112182617
I0307 10:00:20.402484 140113510135552 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.510568141937256, loss=3.8173441886901855
I0307 10:00:58.723264 140113501742848 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.8033318519592285, loss=3.853733539581299
I0307 10:01:36.746494 140113510135552 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.320291519165039, loss=3.6297414302825928
I0307 10:02:01.729107 140269360194752 spec.py:321] Evaluating on the training split.
I0307 10:02:13.398901 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 10:02:43.582166 140269360194752 spec.py:349] Evaluating on the test split.
I0307 10:02:45.421936 140269360194752 submission_runner.py:469] Time since start: 1239.28s, 	Step: 2666, 	{'train/accuracy': 0.3227439224720001, 'train/loss': 3.161668062210083, 'validation/accuracy': 0.26872000098228455, 'validation/loss': 3.495527982711792, 'validation/num_examples': 50000, 'test/accuracy': 0.19840000569820404, 'test/loss': 4.071988582611084, 'test/num_examples': 10000, 'score': 1078.3304996490479, 'total_duration': 1239.279393196106, 'accumulated_submission_time': 1078.3304996490479, 'accumulated_eval_time': 160.56041884422302, 'accumulated_logging_time': 0.07461929321289062}
I0307 10:02:45.462120 140113501742848 logging_writer.py:48] [2666] accumulated_eval_time=160.56, accumulated_logging_time=0.0746193, accumulated_submission_time=1078.33, global_step=2666, preemption_count=0, score=1078.33, test/accuracy=0.1984, test/loss=4.07199, test/num_examples=10000, total_duration=1239.28, train/accuracy=0.322744, train/loss=3.16167, validation/accuracy=0.26872, validation/loss=3.49553, validation/num_examples=50000
I0307 10:02:58.935106 140113510135552 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.221268653869629, loss=3.6775784492492676
I0307 10:03:37.103693 140113501742848 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.5901076793670654, loss=3.602325201034546
I0307 10:04:15.346223 140113510135552 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.508469343185425, loss=3.5297179222106934
I0307 10:04:53.672155 140113501742848 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.023029804229736, loss=3.4009652137756348
I0307 10:05:31.831944 140113510135552 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.4826505184173584, loss=3.3800015449523926
I0307 10:06:10.192187 140113501742848 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.9145562648773193, loss=3.3101859092712402
I0307 10:06:48.489883 140113510135552 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.5313780307769775, loss=3.276738405227661
I0307 10:07:26.736442 140113501742848 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.707274436950684, loss=3.3092896938323975
I0307 10:08:05.151461 140113510135552 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.432161569595337, loss=3.1023917198181152
I0307 10:08:43.491523 140113501742848 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.6732442378997803, loss=3.206785202026367
I0307 10:09:22.077778 140113510135552 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.3437774181365967, loss=3.135862112045288
I0307 10:10:00.327393 140113501742848 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.5083858966827393, loss=3.06058931350708
I0307 10:10:39.004535 140113510135552 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.4678103923797607, loss=3.178008794784546
I0307 10:11:15.562222 140269360194752 spec.py:321] Evaluating on the training split.
I0307 10:11:27.852575 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 10:11:53.589128 140269360194752 spec.py:349] Evaluating on the test split.
I0307 10:11:55.366770 140269360194752 submission_runner.py:469] Time since start: 1789.22s, 	Step: 3997, 	{'train/accuracy': 0.44738519191741943, 'train/loss': 2.45143723487854, 'validation/accuracy': 0.3901599943637848, 'validation/loss': 2.781484842300415, 'validation/num_examples': 50000, 'test/accuracy': 0.28690001368522644, 'test/loss': 3.502551794052124, 'test/num_examples': 10000, 'score': 1588.2982215881348, 'total_duration': 1789.2242217063904, 'accumulated_submission_time': 1588.2982215881348, 'accumulated_eval_time': 200.36492466926575, 'accumulated_logging_time': 0.12267780303955078}
I0307 10:11:55.375420 140113501742848 logging_writer.py:48] [3997] accumulated_eval_time=200.365, accumulated_logging_time=0.122678, accumulated_submission_time=1588.3, global_step=3997, preemption_count=0, score=1588.3, test/accuracy=0.2869, test/loss=3.50255, test/num_examples=10000, total_duration=1789.22, train/accuracy=0.447385, train/loss=2.45144, validation/accuracy=0.39016, validation/loss=2.78148, validation/num_examples=50000
I0307 10:11:56.973180 140113510135552 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.693354606628418, loss=2.906696081161499
I0307 10:12:35.674386 140113501742848 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.72468900680542, loss=2.912506580352783
I0307 10:13:13.983416 140113510135552 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.977430582046509, loss=2.997119188308716
I0307 10:13:51.949317 140113501742848 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.620981216430664, loss=2.984483480453491
I0307 10:14:30.455219 140113510135552 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.0128941535949707, loss=2.6900217533111572
I0307 10:15:08.643000 140113501742848 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.9130048751831055, loss=2.867155075073242
I0307 10:15:47.141093 140113510135552 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.9250824451446533, loss=2.8065643310546875
I0307 10:16:25.732305 140113501742848 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.000159978866577, loss=2.7241482734680176
I0307 10:17:03.968852 140113510135552 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.2885019779205322, loss=2.649359703063965
I0307 10:17:42.235953 140113501742848 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.6559250354766846, loss=2.639115810394287
I0307 10:18:20.599918 140113510135552 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.7453620433807373, loss=2.6547040939331055
I0307 10:18:58.761642 140113501742848 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.5766851902008057, loss=2.7399258613586426
I0307 10:19:37.485056 140113510135552 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.242777109146118, loss=2.6115920543670654
I0307 10:20:15.743388 140113501742848 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.8216148614883423, loss=2.5784077644348145
I0307 10:20:25.430803 140269360194752 spec.py:321] Evaluating on the training split.
I0307 10:20:36.455264 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 10:20:58.750937 140269360194752 spec.py:349] Evaluating on the test split.
I0307 10:21:00.587963 140269360194752 submission_runner.py:469] Time since start: 2334.45s, 	Step: 5326, 	{'train/accuracy': 0.5285993218421936, 'train/loss': 2.0123300552368164, 'validation/accuracy': 0.4635799825191498, 'validation/loss': 2.3488736152648926, 'validation/num_examples': 50000, 'test/accuracy': 0.3582000136375427, 'test/loss': 3.0545520782470703, 'test/num_examples': 10000, 'score': 2098.2309913635254, 'total_duration': 2334.445417404175, 'accumulated_submission_time': 2098.2309913635254, 'accumulated_eval_time': 235.52203559875488, 'accumulated_logging_time': 0.13881325721740723}
I0307 10:21:00.598844 140113510135552 logging_writer.py:48] [5326] accumulated_eval_time=235.522, accumulated_logging_time=0.138813, accumulated_submission_time=2098.23, global_step=5326, preemption_count=0, score=2098.23, test/accuracy=0.3582, test/loss=3.05455, test/num_examples=10000, total_duration=2334.45, train/accuracy=0.528599, train/loss=2.01233, validation/accuracy=0.46358, validation/loss=2.34887, validation/num_examples=50000
I0307 10:21:29.480749 140113501742848 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.8289562463760376, loss=2.446624755859375
I0307 10:22:07.795722 140113510135552 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.195208787918091, loss=2.595261335372925
I0307 10:22:45.996367 140113501742848 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.7635507583618164, loss=2.5264928340911865
I0307 10:23:24.607307 140113510135552 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.6158034801483154, loss=2.3838255405426025
I0307 10:24:02.860207 140113501742848 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.837444543838501, loss=2.5437357425689697
I0307 10:24:41.205165 140113510135552 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.9520378112792969, loss=2.5319881439208984
I0307 10:25:19.618751 140113501742848 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.8523952960968018, loss=2.2593984603881836
I0307 10:25:58.231981 140113510135552 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.6535468101501465, loss=2.519881248474121
I0307 10:26:36.447461 140113501742848 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.564018726348877, loss=2.4897022247314453
I0307 10:27:15.162730 140113510135552 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.2568953037261963, loss=2.3830952644348145
I0307 10:27:53.773367 140113501742848 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.2046072483062744, loss=2.327843189239502
I0307 10:28:31.589343 140113510135552 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.756917119026184, loss=2.380685806274414
I0307 10:29:10.050139 140113501742848 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.3469158411026, loss=2.361929178237915
I0307 10:29:30.806160 140269360194752 spec.py:321] Evaluating on the training split.
I0307 10:29:43.123934 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 10:30:03.471783 140269360194752 spec.py:349] Evaluating on the test split.
I0307 10:30:05.290632 140269360194752 submission_runner.py:469] Time since start: 2879.15s, 	Step: 6655, 	{'train/accuracy': 0.5837053656578064, 'train/loss': 1.7373634576797485, 'validation/accuracy': 0.5206400156021118, 'validation/loss': 2.0676352977752686, 'validation/num_examples': 50000, 'test/accuracy': 0.40390002727508545, 'test/loss': 2.8003625869750977, 'test/num_examples': 10000, 'score': 2608.3175547122955, 'total_duration': 2879.148097515106, 'accumulated_submission_time': 2608.3175547122955, 'accumulated_eval_time': 270.0064697265625, 'accumulated_logging_time': 0.15894794464111328}
I0307 10:30:05.300420 140113510135552 logging_writer.py:48] [6655] accumulated_eval_time=270.006, accumulated_logging_time=0.158948, accumulated_submission_time=2608.32, global_step=6655, preemption_count=0, score=2608.32, test/accuracy=0.4039, test/loss=2.80036, test/num_examples=10000, total_duration=2879.15, train/accuracy=0.583705, train/loss=1.73736, validation/accuracy=0.52064, validation/loss=2.06764, validation/num_examples=50000
I0307 10:30:22.811837 140113501742848 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.4017672538757324, loss=2.366241693496704
I0307 10:31:01.142249 140113510135552 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.814501166343689, loss=2.2836289405822754
I0307 10:31:39.475072 140113501742848 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.4843029975891113, loss=2.325871229171753
I0307 10:32:17.861720 140113510135552 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.7321351766586304, loss=2.3274898529052734
I0307 10:32:56.299780 140113501742848 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.1793482303619385, loss=2.388915538787842
I0307 10:33:34.761454 140113510135552 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.3897223472595215, loss=2.2528271675109863
I0307 10:34:13.093378 140113501742848 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.689428448677063, loss=2.2106735706329346
I0307 10:34:51.614878 140113510135552 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.458198070526123, loss=2.2372121810913086
I0307 10:35:30.395366 140113501742848 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.7488958835601807, loss=2.214707851409912
I0307 10:36:08.741877 140113510135552 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.8849444389343262, loss=2.295290946960449
I0307 10:36:47.131597 140113501742848 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.9495843648910522, loss=2.159383773803711
I0307 10:37:25.415663 140113510135552 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.837335467338562, loss=2.269416570663452
I0307 10:38:03.703809 140113501742848 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.1353635787963867, loss=2.2290375232696533
I0307 10:38:35.545899 140269360194752 spec.py:321] Evaluating on the training split.
I0307 10:38:47.867175 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 10:39:11.056960 140269360194752 spec.py:349] Evaluating on the test split.
I0307 10:39:12.852138 140269360194752 submission_runner.py:469] Time since start: 3426.71s, 	Step: 7984, 	{'train/accuracy': 0.5759526491165161, 'train/loss': 1.7723933458328247, 'validation/accuracy': 0.5143200159072876, 'validation/loss': 2.112335205078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4068000316619873, 'test/loss': 2.8483917713165283, 'test/num_examples': 10000, 'score': 3118.4493594169617, 'total_duration': 3426.7095811367035, 'accumulated_submission_time': 3118.4493594169617, 'accumulated_eval_time': 307.3126537799835, 'accumulated_logging_time': 0.1769120693206787}
I0307 10:39:12.887671 140113510135552 logging_writer.py:48] [7984] accumulated_eval_time=307.313, accumulated_logging_time=0.176912, accumulated_submission_time=3118.45, global_step=7984, preemption_count=0, score=3118.45, test/accuracy=0.4068, test/loss=2.84839, test/num_examples=10000, total_duration=3426.71, train/accuracy=0.575953, train/loss=1.77239, validation/accuracy=0.51432, validation/loss=2.11234, validation/num_examples=50000
I0307 10:39:19.432397 140113501742848 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.9713166952133179, loss=2.297680616378784
I0307 10:39:57.780664 140113510135552 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.4830079078674316, loss=2.233353614807129
I0307 10:40:35.791523 140113501742848 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.9213095903396606, loss=2.2533984184265137
I0307 10:41:14.093527 140113510135552 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.475339889526367, loss=2.1628475189208984
I0307 10:41:52.578997 140113501742848 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.6949623823165894, loss=2.1339004039764404
I0307 10:42:31.016814 140113510135552 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.89836847782135, loss=2.1317989826202393
I0307 10:43:09.257637 140113501742848 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.1867756843566895, loss=2.1412460803985596
I0307 10:43:47.535580 140113510135552 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.1546883583068848, loss=2.227102518081665
I0307 10:44:25.743383 140113501742848 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.6126893758773804, loss=2.2167558670043945
I0307 10:45:04.058339 140113510135552 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.5750048160552979, loss=2.2080748081207275
I0307 10:45:42.576358 140113501742848 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.4016127586364746, loss=2.140387773513794
I0307 10:46:20.765367 140113510135552 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.5848798751831055, loss=2.1799893379211426
I0307 10:46:59.245479 140113501742848 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.8268464803695679, loss=2.143129348754883
I0307 10:47:37.500868 140113510135552 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.7473498582839966, loss=2.122877359390259
I0307 10:47:42.951999 140269360194752 spec.py:321] Evaluating on the training split.
I0307 10:47:57.379104 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 10:48:19.008237 140269360194752 spec.py:349] Evaluating on the test split.
I0307 10:48:20.844331 140269360194752 submission_runner.py:469] Time since start: 3974.70s, 	Step: 9315, 	{'train/accuracy': 0.6282286047935486, 'train/loss': 1.5050336122512817, 'validation/accuracy': 0.5659399628639221, 'validation/loss': 1.846015453338623, 'validation/num_examples': 50000, 'test/accuracy': 0.43560001254081726, 'test/loss': 2.6101667881011963, 'test/num_examples': 10000, 'score': 3628.3923115730286, 'total_duration': 3974.701777458191, 'accumulated_submission_time': 3628.3923115730286, 'accumulated_eval_time': 345.2049353122711, 'accumulated_logging_time': 0.22958660125732422}
I0307 10:48:20.897357 140113501742848 logging_writer.py:48] [9315] accumulated_eval_time=345.205, accumulated_logging_time=0.229587, accumulated_submission_time=3628.39, global_step=9315, preemption_count=0, score=3628.39, test/accuracy=0.4356, test/loss=2.61017, test/num_examples=10000, total_duration=3974.7, train/accuracy=0.628229, train/loss=1.50503, validation/accuracy=0.56594, validation/loss=1.84602, validation/num_examples=50000
I0307 10:48:54.021864 140113510135552 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.8125693798065186, loss=2.1745333671569824
I0307 10:49:32.580025 140113501742848 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.7843691110610962, loss=2.086123466491699
I0307 10:50:10.834532 140113510135552 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.607786774635315, loss=2.001619815826416
I0307 10:50:49.854155 140113501742848 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8691864013671875, loss=2.246298313140869
I0307 10:51:27.447018 140113510135552 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.6718504428863525, loss=2.0924293994903564
I0307 10:52:04.895569 140113501742848 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.6597670316696167, loss=2.1400485038757324
I0307 10:52:42.370667 140113510135552 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.7007826566696167, loss=2.11286997795105
I0307 10:53:20.310193 140113501742848 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.3838926553726196, loss=1.9326856136322021
I0307 10:53:59.552800 140113510135552 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.1135754585266113, loss=2.1002490520477295
I0307 10:54:38.055062 140113501742848 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.2171406745910645, loss=2.0943102836608887
I0307 10:55:16.143958 140113510135552 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.5809142589569092, loss=1.9743685722351074
I0307 10:55:53.930890 140113501742848 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.6758077144622803, loss=2.186243772506714
I0307 10:56:31.918528 140113510135552 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.87233567237854, loss=1.9913532733917236
I0307 10:56:51.161594 140269360194752 spec.py:321] Evaluating on the training split.
I0307 10:57:10.466158 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 10:57:30.985671 140269360194752 spec.py:349] Evaluating on the test split.
I0307 10:57:32.791955 140269360194752 submission_runner.py:469] Time since start: 4526.65s, 	Step: 10652, 	{'train/accuracy': 0.6330716013908386, 'train/loss': 1.4845244884490967, 'validation/accuracy': 0.5704399943351746, 'validation/loss': 1.81955087184906, 'validation/num_examples': 50000, 'test/accuracy': 0.4433000087738037, 'test/loss': 2.587620735168457, 'test/num_examples': 10000, 'score': 4138.432383060455, 'total_duration': 4526.649419307709, 'accumulated_submission_time': 4138.432383060455, 'accumulated_eval_time': 386.835266828537, 'accumulated_logging_time': 0.3533949851989746}
I0307 10:57:32.842817 140113501742848 logging_writer.py:48] [10652] accumulated_eval_time=386.835, accumulated_logging_time=0.353395, accumulated_submission_time=4138.43, global_step=10652, preemption_count=0, score=4138.43, test/accuracy=0.4433, test/loss=2.58762, test/num_examples=10000, total_duration=4526.65, train/accuracy=0.633072, train/loss=1.48452, validation/accuracy=0.57044, validation/loss=1.81955, validation/num_examples=50000
I0307 10:57:51.665482 140113510135552 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.5723479986190796, loss=2.0078611373901367
I0307 10:58:30.328018 140113501742848 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.9787386655807495, loss=2.1050682067871094
I0307 10:59:08.770083 140113510135552 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.9295446872711182, loss=1.930757999420166
I0307 10:59:47.250238 140113501742848 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.8576140403747559, loss=1.9885883331298828
I0307 11:00:25.076431 140113510135552 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.0323855876922607, loss=2.0420424938201904
I0307 11:01:03.365010 140113501742848 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.8318570852279663, loss=2.071500539779663
I0307 11:01:42.249073 140113510135552 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.7511690855026245, loss=1.8775655031204224
I0307 11:02:21.225349 140113501742848 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.7819936275482178, loss=2.0871999263763428
I0307 11:02:59.834990 140113510135552 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.7542637586593628, loss=2.069477081298828
I0307 11:03:38.242435 140113501742848 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.5260775089263916, loss=1.9744329452514648
I0307 11:04:16.826261 140113510135552 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.924141526222229, loss=2.16121244430542
I0307 11:04:54.816973 140113501742848 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.6586512327194214, loss=1.9621647596359253
I0307 11:05:32.620424 140113510135552 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.699436068534851, loss=1.9104377031326294
I0307 11:06:02.954004 140269360194752 spec.py:321] Evaluating on the training split.
I0307 11:06:18.849689 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 11:06:45.101799 140269360194752 spec.py:349] Evaluating on the test split.
I0307 11:06:46.918140 140269360194752 submission_runner.py:469] Time since start: 5080.78s, 	Step: 11980, 	{'train/accuracy': 0.6530413031578064, 'train/loss': 1.4080978631973267, 'validation/accuracy': 0.5837599635124207, 'validation/loss': 1.747476577758789, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.503089427947998, 'test/num_examples': 10000, 'score': 4648.388277292252, 'total_duration': 5080.775400400162, 'accumulated_submission_time': 4648.388277292252, 'accumulated_eval_time': 430.79916405677795, 'accumulated_logging_time': 0.4122931957244873}
I0307 11:06:47.027956 140113501742848 logging_writer.py:48] [11980] accumulated_eval_time=430.799, accumulated_logging_time=0.412293, accumulated_submission_time=4648.39, global_step=11980, preemption_count=0, score=4648.39, test/accuracy=0.4571, test/loss=2.50309, test/num_examples=10000, total_duration=5080.78, train/accuracy=0.653041, train/loss=1.4081, validation/accuracy=0.58376, validation/loss=1.74748, validation/num_examples=50000
I0307 11:06:55.343070 140113510135552 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4174143075942993, loss=1.9411078691482544
I0307 11:07:34.097242 140113501742848 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.3946574926376343, loss=1.8697127103805542
I0307 11:08:12.506685 140113510135552 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6745115518569946, loss=1.988908290863037
I0307 11:08:51.006578 140113501742848 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.6229137182235718, loss=1.9463844299316406
I0307 11:09:29.844938 140113510135552 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.5618659257888794, loss=2.0154712200164795
I0307 11:10:08.393934 140113501742848 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.7253981828689575, loss=1.96806800365448
I0307 11:10:47.490186 140113510135552 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.6195638179779053, loss=2.038149356842041
I0307 11:11:25.912706 140113501742848 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.093858480453491, loss=1.8900645971298218
I0307 11:12:04.283917 140113510135552 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.0049705505371094, loss=1.8327397108078003
I0307 11:12:42.293372 140113501742848 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.4910296201705933, loss=2.0715041160583496
I0307 11:13:19.970302 140113510135552 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.9243282079696655, loss=1.9227344989776611
I0307 11:13:58.169037 140113501742848 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.7069118022918701, loss=1.999550700187683
I0307 11:14:36.746358 140113510135552 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.5522059202194214, loss=1.9086880683898926
I0307 11:15:15.436881 140113501742848 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.5470161437988281, loss=1.9295315742492676
I0307 11:15:17.052013 140269360194752 spec.py:321] Evaluating on the training split.
I0307 11:15:35.720291 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 11:15:59.946770 140269360194752 spec.py:349] Evaluating on the test split.
I0307 11:16:01.776948 140269360194752 submission_runner.py:469] Time since start: 5635.63s, 	Step: 13305, 	{'train/accuracy': 0.6422991156578064, 'train/loss': 1.4485238790512085, 'validation/accuracy': 0.5745999813079834, 'validation/loss': 1.811006784439087, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.5664117336273193, 'test/num_examples': 10000, 'score': 5158.23425412178, 'total_duration': 5635.634290218353, 'accumulated_submission_time': 5158.23425412178, 'accumulated_eval_time': 475.5239474773407, 'accumulated_logging_time': 0.5558822154998779}
I0307 11:16:01.904516 140113510135552 logging_writer.py:48] [13305] accumulated_eval_time=475.524, accumulated_logging_time=0.555882, accumulated_submission_time=5158.23, global_step=13305, preemption_count=0, score=5158.23, test/accuracy=0.4557, test/loss=2.56641, test/num_examples=10000, total_duration=5635.63, train/accuracy=0.642299, train/loss=1.44852, validation/accuracy=0.5746, validation/loss=1.81101, validation/num_examples=50000
I0307 11:16:38.948614 140113501742848 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.442712664604187, loss=2.039867877960205
I0307 11:17:17.361500 140113510135552 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.7169846296310425, loss=1.9472659826278687
I0307 11:17:55.701013 140113501742848 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.048778533935547, loss=1.8972198963165283
I0307 11:18:33.955759 140113510135552 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.784193992614746, loss=1.9830137491226196
I0307 11:19:12.752497 140113501742848 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.0215365886688232, loss=1.9565502405166626
I0307 11:19:51.640107 140113510135552 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.6089277267456055, loss=1.9581639766693115
I0307 11:20:30.462486 140113501742848 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.9624545574188232, loss=2.039368152618408
I0307 11:21:09.017021 140113510135552 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.1248788833618164, loss=1.8290386199951172
I0307 11:21:47.840934 140113501742848 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.7111314535140991, loss=2.001514434814453
I0307 11:22:26.314525 140113510135552 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.7615163326263428, loss=1.9657902717590332
I0307 11:23:05.387449 140113501742848 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.4957252740859985, loss=1.901557445526123
I0307 11:23:44.440990 140113510135552 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.9680595397949219, loss=1.8810025453567505
I0307 11:24:22.931217 140113501742848 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.3949921131134033, loss=1.897658109664917
I0307 11:24:31.945409 140269360194752 spec.py:321] Evaluating on the training split.
I0307 11:24:49.534463 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 11:25:15.548472 140269360194752 spec.py:349] Evaluating on the test split.
I0307 11:25:17.374550 140269360194752 submission_runner.py:469] Time since start: 6191.23s, 	Step: 14624, 	{'train/accuracy': 0.6447703838348389, 'train/loss': 1.4218053817749023, 'validation/accuracy': 0.5844399929046631, 'validation/loss': 1.7558026313781738, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.5262722969055176, 'test/num_examples': 10000, 'score': 5668.10534119606, 'total_duration': 6191.2318522930145, 'accumulated_submission_time': 5668.10534119606, 'accumulated_eval_time': 520.9528930187225, 'accumulated_logging_time': 0.7134666442871094}
I0307 11:25:17.602625 140113510135552 logging_writer.py:48] [14624] accumulated_eval_time=520.953, accumulated_logging_time=0.713467, accumulated_submission_time=5668.11, global_step=14624, preemption_count=0, score=5668.11, test/accuracy=0.4578, test/loss=2.52627, test/num_examples=10000, total_duration=6191.23, train/accuracy=0.64477, train/loss=1.42181, validation/accuracy=0.58444, validation/loss=1.7558, validation/num_examples=50000
I0307 11:25:47.455156 140113501742848 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.5197323560714722, loss=1.8379815816879272
I0307 11:26:26.245136 140113510135552 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.718106985092163, loss=2.004469633102417
I0307 11:27:05.194625 140113501742848 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.6301428079605103, loss=1.9194886684417725
I0307 11:27:43.804765 140113510135552 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.7758264541625977, loss=1.9721732139587402
I0307 11:28:22.887666 140113501742848 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5363324880599976, loss=1.946427583694458
I0307 11:29:01.487810 140113510135552 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.5562353134155273, loss=1.8368223905563354
I0307 11:29:39.885241 140113501742848 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.8004597425460815, loss=1.9650994539260864
I0307 11:30:18.637257 140113510135552 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.5399928092956543, loss=1.8244366645812988
I0307 11:30:57.479857 140113501742848 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.4560915231704712, loss=1.9445966482162476
I0307 11:31:36.468815 140113510135552 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.553924322128296, loss=1.9055421352386475
I0307 11:32:15.000278 140113501742848 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.388205885887146, loss=1.9271793365478516
I0307 11:32:53.615182 140113510135552 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.4651098251342773, loss=1.8731358051300049
I0307 11:33:32.410062 140113501742848 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.6017707586288452, loss=1.9930628538131714
I0307 11:33:47.426223 140269360194752 spec.py:321] Evaluating on the training split.
I0307 11:34:05.739765 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 11:34:27.133446 140269360194752 spec.py:349] Evaluating on the test split.
I0307 11:34:28.966145 140269360194752 submission_runner.py:469] Time since start: 6742.82s, 	Step: 15940, 	{'train/accuracy': 0.6671914458274841, 'train/loss': 1.3173516988754272, 'validation/accuracy': 0.5956999659538269, 'validation/loss': 1.6901887655258179, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4337799549102783, 'test/num_examples': 10000, 'score': 6177.7375519275665, 'total_duration': 6742.823451757431, 'accumulated_submission_time': 6177.7375519275665, 'accumulated_eval_time': 562.4926252365112, 'accumulated_logging_time': 0.9937787055969238}
I0307 11:34:29.078784 140113510135552 logging_writer.py:48] [15940] accumulated_eval_time=562.493, accumulated_logging_time=0.993779, accumulated_submission_time=6177.74, global_step=15940, preemption_count=0, score=6177.74, test/accuracy=0.4735, test/loss=2.43378, test/num_examples=10000, total_duration=6742.82, train/accuracy=0.667191, train/loss=1.31735, validation/accuracy=0.5957, validation/loss=1.69019, validation/num_examples=50000
I0307 11:34:52.818861 140113501742848 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.6936208009719849, loss=1.9138424396514893
I0307 11:35:30.963744 140113510135552 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.784651517868042, loss=1.8866721391677856
I0307 11:36:09.608474 140113501742848 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.4729427099227905, loss=1.85649836063385
I0307 11:36:48.876317 140113510135552 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.4765976667404175, loss=1.9959142208099365
I0307 11:37:27.448184 140113501742848 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.8632155656814575, loss=1.918017864227295
I0307 11:38:05.230446 140113510135552 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.6219477653503418, loss=1.8145099878311157
I0307 11:38:43.300370 140113501742848 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.5869840383529663, loss=1.7225971221923828
I0307 11:39:22.127983 140113510135552 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.6078596115112305, loss=1.8994194269180298
I0307 11:40:01.055849 140113501742848 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.874774694442749, loss=1.9243379831314087
I0307 11:40:39.803929 140113510135552 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.8421598672866821, loss=1.8707292079925537
I0307 11:41:18.726642 140113501742848 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.9654121398925781, loss=1.7779587507247925
I0307 11:41:58.383366 140113510135552 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.639663577079773, loss=1.7555557489395142
I0307 11:42:37.988259 140113501742848 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.6848517656326294, loss=1.7483806610107422
I0307 11:42:59.182842 140269360194752 spec.py:321] Evaluating on the training split.
I0307 11:43:15.742362 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 11:43:43.465777 140269360194752 spec.py:349] Evaluating on the test split.
I0307 11:43:45.277578 140269360194752 submission_runner.py:469] Time since start: 7299.13s, 	Step: 17256, 	{'train/accuracy': 0.6512874364852905, 'train/loss': 1.3997085094451904, 'validation/accuracy': 0.5823999643325806, 'validation/loss': 1.7467749118804932, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.470266580581665, 'test/num_examples': 10000, 'score': 6687.620101690292, 'total_duration': 7299.134912014008, 'accumulated_submission_time': 6687.620101690292, 'accumulated_eval_time': 608.5871965885162, 'accumulated_logging_time': 1.1844618320465088}
I0307 11:43:45.399837 140113510135552 logging_writer.py:48] [17256] accumulated_eval_time=608.587, accumulated_logging_time=1.18446, accumulated_submission_time=6687.62, global_step=17256, preemption_count=0, score=6687.62, test/accuracy=0.4626, test/loss=2.47027, test/num_examples=10000, total_duration=7299.13, train/accuracy=0.651287, train/loss=1.39971, validation/accuracy=0.5824, validation/loss=1.74677, validation/num_examples=50000
I0307 11:44:02.753366 140113501742848 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.6938543319702148, loss=1.79339599609375
I0307 11:44:41.436980 140113510135552 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.1973283290863037, loss=1.8789533376693726
I0307 11:45:20.039654 140113501742848 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.7755664587020874, loss=1.9151368141174316
I0307 11:45:58.499193 140113510135552 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.3378287553787231, loss=1.8217582702636719
I0307 11:46:37.246506 140113501742848 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.5905890464782715, loss=1.7689038515090942
I0307 11:47:15.994706 140113510135552 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.209287643432617, loss=1.9244205951690674
I0307 11:47:54.574873 140113501742848 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.4994434118270874, loss=1.8329542875289917
I0307 11:48:33.302317 140113510135552 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.4877067804336548, loss=1.809689998626709
I0307 11:49:11.728576 140113501742848 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.9222400188446045, loss=1.9268174171447754
I0307 11:49:50.174148 140113510135552 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.509545922279358, loss=1.7310361862182617
I0307 11:50:29.003522 140113501742848 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.639892339706421, loss=1.821257472038269
I0307 11:51:07.396600 140113510135552 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.8420919179916382, loss=1.7463603019714355
I0307 11:51:45.761573 140113501742848 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.7309046983718872, loss=1.8622162342071533
I0307 11:52:15.552068 140269360194752 spec.py:321] Evaluating on the training split.
I0307 11:52:30.237575 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 11:52:53.700980 140269360194752 spec.py:349] Evaluating on the test split.
I0307 11:52:55.469336 140269360194752 submission_runner.py:469] Time since start: 7849.33s, 	Step: 18578, 	{'train/accuracy': 0.6806241869926453, 'train/loss': 1.2696762084960938, 'validation/accuracy': 0.6096199750900269, 'validation/loss': 1.6212488412857056, 'validation/num_examples': 50000, 'test/accuracy': 0.4790000319480896, 'test/loss': 2.3717098236083984, 'test/num_examples': 10000, 'score': 7197.6196620464325, 'total_duration': 7849.326790332794, 'accumulated_submission_time': 7197.6196620464325, 'accumulated_eval_time': 648.504426240921, 'accumulated_logging_time': 1.3333828449249268}
I0307 11:52:55.590394 140113510135552 logging_writer.py:48] [18578] accumulated_eval_time=648.504, accumulated_logging_time=1.33338, accumulated_submission_time=7197.62, global_step=18578, preemption_count=0, score=7197.62, test/accuracy=0.479, test/loss=2.37171, test/num_examples=10000, total_duration=7849.33, train/accuracy=0.680624, train/loss=1.26968, validation/accuracy=0.60962, validation/loss=1.62125, validation/num_examples=50000
I0307 11:53:04.306741 140113501742848 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.7644480466842651, loss=1.8158011436462402
I0307 11:53:42.611872 140113510135552 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.9371240139007568, loss=1.839127779006958
I0307 11:54:21.407185 140113501742848 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.506886601448059, loss=1.7676714658737183
I0307 11:54:59.796037 140113510135552 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.8099874258041382, loss=1.9101574420928955
I0307 11:55:38.652887 140113501742848 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.8066761493682861, loss=1.9168146848678589
I0307 11:56:17.556368 140113510135552 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.4501748085021973, loss=1.7750115394592285
I0307 11:56:56.240363 140113501742848 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.720577597618103, loss=1.7442892789840698
I0307 11:57:35.479045 140113510135552 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.137521743774414, loss=1.96526300907135
I0307 11:58:13.914604 140113501742848 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.048248291015625, loss=1.8226064443588257
I0307 11:58:52.681348 140113510135552 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.8134127855300903, loss=1.73676335811615
I0307 11:59:31.010818 140113501742848 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.864997148513794, loss=1.7825496196746826
I0307 12:00:09.722135 140113510135552 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.7383061647415161, loss=1.798057198524475
I0307 12:00:48.102830 140113501742848 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6525495052337646, loss=1.8107552528381348
I0307 12:01:25.734178 140269360194752 spec.py:321] Evaluating on the training split.
I0307 12:01:42.429168 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 12:02:11.608153 140269360194752 spec.py:349] Evaluating on the test split.
I0307 12:02:13.416162 140269360194752 submission_runner.py:469] Time since start: 8407.27s, 	Step: 19898, 	{'train/accuracy': 0.6729910373687744, 'train/loss': 1.3033865690231323, 'validation/accuracy': 0.6078199744224548, 'validation/loss': 1.64429771900177, 'validation/num_examples': 50000, 'test/accuracy': 0.48990002274513245, 'test/loss': 2.365725517272949, 'test/num_examples': 10000, 'score': 7707.5985741615295, 'total_duration': 8407.273492097855, 'accumulated_submission_time': 7707.5985741615295, 'accumulated_eval_time': 696.186240196228, 'accumulated_logging_time': 1.4901890754699707}
I0307 12:02:13.444081 140113510135552 logging_writer.py:48] [19898] accumulated_eval_time=696.186, accumulated_logging_time=1.49019, accumulated_submission_time=7707.6, global_step=19898, preemption_count=0, score=7707.6, test/accuracy=0.4899, test/loss=2.36573, test/num_examples=10000, total_duration=8407.27, train/accuracy=0.672991, train/loss=1.30339, validation/accuracy=0.60782, validation/loss=1.6443, validation/num_examples=50000
I0307 12:02:14.640542 140113501742848 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.5263254642486572, loss=1.7638216018676758
I0307 12:02:53.155561 140113510135552 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.4868460893630981, loss=1.7554832696914673
I0307 12:03:33.251111 140113501742848 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.8428280353546143, loss=1.917481780052185
I0307 12:04:13.031611 140113510135552 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.6412514448165894, loss=1.950250267982483
I0307 12:04:52.271075 140113501742848 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.707627296447754, loss=1.8087544441223145
I0307 12:05:31.795444 140113510135552 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.654435634613037, loss=1.8425743579864502
I0307 12:06:10.907871 140113501742848 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.7574950456619263, loss=1.8010553121566772
I0307 12:06:50.021734 140113510135552 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.7497309446334839, loss=1.8786649703979492
I0307 12:07:29.088993 140113501742848 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.5167958736419678, loss=1.8314132690429688
I0307 12:08:07.807802 140113510135552 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.6798738241195679, loss=1.786425232887268
I0307 12:08:46.126306 140113501742848 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.9945931434631348, loss=1.8635259866714478
I0307 12:09:24.926135 140113510135552 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.5157899856567383, loss=1.703930139541626
I0307 12:10:03.678051 140113501742848 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.7490968704223633, loss=1.7826391458511353
I0307 12:10:42.570961 140113510135552 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.7952239513397217, loss=1.817757248878479
I0307 12:10:43.758627 140269360194752 spec.py:321] Evaluating on the training split.
I0307 12:11:04.055634 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 12:11:29.528248 140269360194752 spec.py:349] Evaluating on the test split.
I0307 12:11:31.344846 140269360194752 submission_runner.py:469] Time since start: 8965.20s, 	Step: 21204, 	{'train/accuracy': 0.6843510866165161, 'train/loss': 1.237943172454834, 'validation/accuracy': 0.6148599982261658, 'validation/loss': 1.613425612449646, 'validation/num_examples': 50000, 'test/accuracy': 0.4823000133037567, 'test/loss': 2.364466428756714, 'test/num_examples': 10000, 'score': 8217.763018369675, 'total_duration': 8965.202175617218, 'accumulated_submission_time': 8217.763018369675, 'accumulated_eval_time': 743.7722883224487, 'accumulated_logging_time': 1.527381181716919}
I0307 12:11:31.385967 140113501742848 logging_writer.py:48] [21204] accumulated_eval_time=743.772, accumulated_logging_time=1.52738, accumulated_submission_time=8217.76, global_step=21204, preemption_count=0, score=8217.76, test/accuracy=0.4823, test/loss=2.36447, test/num_examples=10000, total_duration=8965.2, train/accuracy=0.684351, train/loss=1.23794, validation/accuracy=0.61486, validation/loss=1.61343, validation/num_examples=50000
I0307 12:12:09.149833 140113510135552 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.6719201803207397, loss=1.8127940893173218
I0307 12:12:48.277211 140113501742848 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.928492546081543, loss=1.8231858015060425
I0307 12:13:27.580346 140113510135552 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.7300364971160889, loss=1.8133418560028076
I0307 12:14:06.999495 140113501742848 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.892845630645752, loss=1.7478344440460205
I0307 12:14:46.057354 140113501742848 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.6540000438690186, loss=1.7758444547653198
I0307 12:15:24.677414 140113510135552 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.786858081817627, loss=1.7308094501495361
I0307 12:16:05.366083 140113501742848 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.7835649251937866, loss=1.917880892753601
I0307 12:16:43.868542 140113510135552 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5348925590515137, loss=1.6815435886383057
I0307 12:17:22.877385 140113501742848 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.4384037256240845, loss=1.7739003896713257
I0307 12:18:01.547790 140113510135552 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.943170428276062, loss=1.8223028182983398
I0307 12:18:39.969151 140113501742848 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.7122812271118164, loss=1.7259670495986938
I0307 12:19:18.371666 140113510135552 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.5441749095916748, loss=1.681252360343933
I0307 12:19:56.602602 140113501742848 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.794333815574646, loss=1.798828363418579
I0307 12:20:01.414645 140269360194752 spec.py:321] Evaluating on the training split.
I0307 12:20:17.029000 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 12:20:38.529485 140269360194752 spec.py:349] Evaluating on the test split.
I0307 12:20:40.341545 140269360194752 submission_runner.py:469] Time since start: 9514.20s, 	Step: 22513, 	{'train/accuracy': 0.6660953164100647, 'train/loss': 1.3299880027770996, 'validation/accuracy': 0.5976399779319763, 'validation/loss': 1.6900964975357056, 'validation/num_examples': 50000, 'test/accuracy': 0.47440001368522644, 'test/loss': 2.4104833602905273, 'test/num_examples': 10000, 'score': 8727.635385751724, 'total_duration': 9514.198991298676, 'accumulated_submission_time': 8727.635385751724, 'accumulated_eval_time': 782.6991384029388, 'accumulated_logging_time': 1.5777478218078613}
I0307 12:20:40.471742 140113510135552 logging_writer.py:48] [22513] accumulated_eval_time=782.699, accumulated_logging_time=1.57775, accumulated_submission_time=8727.64, global_step=22513, preemption_count=0, score=8727.64, test/accuracy=0.4744, test/loss=2.41048, test/num_examples=10000, total_duration=9514.2, train/accuracy=0.666095, train/loss=1.32999, validation/accuracy=0.59764, validation/loss=1.6901, validation/num_examples=50000
I0307 12:21:14.218736 140113501742848 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.6921963691711426, loss=1.8371864557266235
I0307 12:21:52.606504 140113510135552 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.7690982818603516, loss=1.8482651710510254
I0307 12:22:31.261421 140113501742848 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.9157259464263916, loss=1.8052886724472046
I0307 12:23:09.782106 140113510135552 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.6963645219802856, loss=1.7951531410217285
I0307 12:23:48.818942 140113501742848 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.8112273216247559, loss=1.7327237129211426
I0307 12:24:27.185417 140113510135552 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.137618064880371, loss=1.7927526235580444
I0307 12:25:06.109681 140113501742848 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.5379884243011475, loss=1.7199488878250122
I0307 12:25:45.039991 140113510135552 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5106902122497559, loss=1.7371656894683838
I0307 12:26:23.880740 140113501742848 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.8014214038848877, loss=1.767110824584961
I0307 12:27:02.107722 140113510135552 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.7392420768737793, loss=1.701355218887329
I0307 12:27:40.789449 140113501742848 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.7732840776443481, loss=1.7577793598175049
I0307 12:28:43.681259 140113510135552 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.6002789735794067, loss=1.7144826650619507
I0307 12:29:10.444587 140269360194752 spec.py:321] Evaluating on the training split.
I0307 12:29:26.266173 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 12:29:53.192386 140269360194752 spec.py:349] Evaluating on the test split.
I0307 12:29:54.946224 140269360194752 submission_runner.py:469] Time since start: 10068.80s, 	Step: 23768, 	{'train/accuracy': 0.66015625, 'train/loss': 1.3520725965499878, 'validation/accuracy': 0.5965200066566467, 'validation/loss': 1.7051085233688354, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.4384641647338867, 'test/num_examples': 10000, 'score': 9237.437793016434, 'total_duration': 10068.803575515747, 'accumulated_submission_time': 9237.437793016434, 'accumulated_eval_time': 827.2006297111511, 'accumulated_logging_time': 1.7460052967071533}
I0307 12:29:55.039472 140113501742848 logging_writer.py:48] [23768] accumulated_eval_time=827.201, accumulated_logging_time=1.74601, accumulated_submission_time=9237.44, global_step=23768, preemption_count=0, score=9237.44, test/accuracy=0.48, test/loss=2.43846, test/num_examples=10000, total_duration=10068.8, train/accuracy=0.660156, train/loss=1.35207, validation/accuracy=0.59652, validation/loss=1.70511, validation/num_examples=50000
I0307 12:30:07.845695 140113510135552 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.452252984046936, loss=1.75144624710083
I0307 12:30:46.605377 140113501742848 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.6422743797302246, loss=1.8450045585632324
I0307 12:31:25.141255 140113510135552 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.9194504022598267, loss=1.7666497230529785
I0307 12:32:03.643038 140113501742848 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.6036853790283203, loss=1.6901066303253174
I0307 12:32:41.887047 140113510135552 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.5844345092773438, loss=1.7859610319137573
I0307 12:33:21.655565 140113501742848 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.8494017124176025, loss=1.9110726118087769
I0307 12:34:01.767949 140113510135552 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.5592434406280518, loss=1.701646089553833
I0307 12:34:41.493675 140113501742848 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.5231939554214478, loss=1.7341670989990234
I0307 12:35:20.485113 140113510135552 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.648673176765442, loss=1.6697969436645508
I0307 12:35:58.924636 140113501742848 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.8916656970977783, loss=1.7128705978393555
I0307 12:36:37.450865 140113510135552 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.691182017326355, loss=1.8065650463104248
I0307 12:37:16.199621 140113501742848 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.6868832111358643, loss=1.8201788663864136
I0307 12:37:54.905181 140113510135552 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.7151315212249756, loss=1.7427574396133423
I0307 12:38:25.145771 140269360194752 spec.py:321] Evaluating on the training split.
I0307 12:38:40.451812 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 12:39:08.532119 140269360194752 spec.py:349] Evaluating on the test split.
I0307 12:39:10.360317 140269360194752 submission_runner.py:469] Time since start: 10624.22s, 	Step: 25077, 	{'train/accuracy': 0.6676897406578064, 'train/loss': 1.3109707832336426, 'validation/accuracy': 0.6008999943733215, 'validation/loss': 1.6706515550613403, 'validation/num_examples': 50000, 'test/accuracy': 0.47510001063346863, 'test/loss': 2.416644334793091, 'test/num_examples': 10000, 'score': 9747.373111486435, 'total_duration': 10624.217602729797, 'accumulated_submission_time': 9747.373111486435, 'accumulated_eval_time': 872.4149825572968, 'accumulated_logging_time': 1.874401330947876}
I0307 12:39:10.439951 140113501742848 logging_writer.py:48] [25077] accumulated_eval_time=872.415, accumulated_logging_time=1.8744, accumulated_submission_time=9747.37, global_step=25077, preemption_count=0, score=9747.37, test/accuracy=0.4751, test/loss=2.41664, test/num_examples=10000, total_duration=10624.2, train/accuracy=0.66769, train/loss=1.31097, validation/accuracy=0.6009, validation/loss=1.67065, validation/num_examples=50000
I0307 12:39:19.772357 140113510135552 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.1999120712280273, loss=1.72055184841156
I0307 12:39:58.501028 140113501742848 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.772821307182312, loss=1.781038522720337
I0307 12:40:36.649935 140113510135552 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.7551366090774536, loss=1.7793785333633423
I0307 12:41:15.463462 140113501742848 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.563614845275879, loss=1.8847591876983643
I0307 12:41:54.765752 140113510135552 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.7513996362686157, loss=1.674871802330017
I0307 12:42:34.654834 140113501742848 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.5876091718673706, loss=1.7741329669952393
I0307 12:43:15.001513 140113510135552 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.65964937210083, loss=1.7837895154953003
I0307 12:43:54.372080 140113501742848 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.00553822517395, loss=1.771049976348877
I0307 12:44:33.094102 140113510135552 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.570135235786438, loss=1.7747092247009277
I0307 12:45:11.571188 140113501742848 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.647446870803833, loss=1.8098931312561035
I0307 12:45:50.412996 140113510135552 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.610988736152649, loss=1.8696279525756836
I0307 12:46:29.136778 140113501742848 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.777241826057434, loss=1.908037781715393
I0307 12:47:08.054455 140113510135552 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.6704761981964111, loss=1.6476109027862549
I0307 12:47:40.457832 140269360194752 spec.py:321] Evaluating on the training split.
I0307 12:47:59.956513 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 12:48:22.747358 140269360194752 spec.py:349] Evaluating on the test split.
I0307 12:48:24.550733 140269360194752 submission_runner.py:469] Time since start: 11178.41s, 	Step: 26384, 	{'train/accuracy': 0.6811822056770325, 'train/loss': 1.259221076965332, 'validation/accuracy': 0.6119399666786194, 'validation/loss': 1.6104940176010132, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.3759024143218994, 'test/num_examples': 10000, 'score': 10257.216561079025, 'total_duration': 11178.408034563065, 'accumulated_submission_time': 10257.216561079025, 'accumulated_eval_time': 916.5076866149902, 'accumulated_logging_time': 1.9939005374908447}
I0307 12:48:24.683704 140113501742848 logging_writer.py:48] [26384] accumulated_eval_time=916.508, accumulated_logging_time=1.9939, accumulated_submission_time=10257.2, global_step=26384, preemption_count=0, score=10257.2, test/accuracy=0.4841, test/loss=2.3759, test/num_examples=10000, total_duration=11178.4, train/accuracy=0.681182, train/loss=1.25922, validation/accuracy=0.61194, validation/loss=1.61049, validation/num_examples=50000
I0307 12:48:31.907363 140113510135552 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.8958221673965454, loss=1.6468571424484253
I0307 12:49:10.459606 140113501742848 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.9936639070510864, loss=1.7413285970687866
I0307 12:49:48.966480 140113510135552 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.8730130195617676, loss=1.7928619384765625
I0307 12:50:27.572992 140113501742848 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.8455960750579834, loss=1.7172787189483643
I0307 12:51:06.183039 140113510135552 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.7786178588867188, loss=1.811236023902893
I0307 12:51:44.758040 140113501742848 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.6165696382522583, loss=1.7505022287368774
I0307 12:52:23.447813 140113510135552 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.7536715269088745, loss=1.663339614868164
I0307 12:53:02.071737 140113501742848 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.7807577848434448, loss=1.749572992324829
I0307 12:53:40.448006 140113510135552 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.784172534942627, loss=1.7501057386398315
I0307 12:54:18.854280 140113501742848 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.5576121807098389, loss=1.6998417377471924
I0307 12:54:57.242162 140113510135552 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.80898118019104, loss=1.6787952184677124
I0307 12:55:35.919677 140113501742848 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.7853617668151855, loss=1.799882411956787
I0307 12:56:14.978365 140113510135552 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.7727242708206177, loss=1.7466647624969482
I0307 12:56:53.531748 140113501742848 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.8399608135223389, loss=1.7604798078536987
I0307 12:56:54.707510 140269360194752 spec.py:321] Evaluating on the training split.
I0307 12:57:14.895432 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 12:57:36.210213 140269360194752 spec.py:349] Evaluating on the test split.
I0307 12:57:38.039914 140269360194752 submission_runner.py:469] Time since start: 11731.90s, 	Step: 27704, 	{'train/accuracy': 0.6849290132522583, 'train/loss': 1.2387564182281494, 'validation/accuracy': 0.616320013999939, 'validation/loss': 1.5950056314468384, 'validation/num_examples': 50000, 'test/accuracy': 0.4926000237464905, 'test/loss': 2.350342273712158, 'test/num_examples': 10000, 'score': 10766.4341006279, 'total_duration': 11731.897221326828, 'accumulated_submission_time': 10766.4341006279, 'accumulated_eval_time': 959.8398950099945, 'accumulated_logging_time': 2.8018834590911865}
I0307 12:57:38.127240 140113510135552 logging_writer.py:48] [27704] accumulated_eval_time=959.84, accumulated_logging_time=2.80188, accumulated_submission_time=10766.4, global_step=27704, preemption_count=0, score=10766.4, test/accuracy=0.4926, test/loss=2.35034, test/num_examples=10000, total_duration=11731.9, train/accuracy=0.684929, train/loss=1.23876, validation/accuracy=0.61632, validation/loss=1.59501, validation/num_examples=50000
I0307 12:58:16.137397 140113501742848 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.5889418125152588, loss=1.7739524841308594
I0307 12:58:54.443229 140113510135552 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.7812541723251343, loss=1.7466230392456055
I0307 12:59:33.027034 140113501742848 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.7499487400054932, loss=1.719006061553955
I0307 13:00:10.426607 140113510135552 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.9402650594711304, loss=1.6399580240249634
I0307 13:00:48.445909 140113501742848 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.7597861289978027, loss=1.7282191514968872
I0307 13:01:26.923320 140113510135552 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.6529853343963623, loss=1.6935577392578125
I0307 13:02:05.301017 140113501742848 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.8343809843063354, loss=1.778326153755188
I0307 13:02:43.797423 140113510135552 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.7263684272766113, loss=1.7081491947174072
I0307 13:03:22.239324 140113501742848 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.62826406955719, loss=1.7315516471862793
I0307 13:04:00.826418 140113510135552 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.8610577583312988, loss=1.7466593980789185
I0307 13:04:40.101130 140113501742848 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.8289011716842651, loss=1.7408318519592285
I0307 13:05:19.896090 140113510135552 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.6367213726043701, loss=1.7109441757202148
I0307 13:05:58.850962 140113501742848 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.5655584335327148, loss=1.7574812173843384
I0307 13:06:08.059515 140269360194752 spec.py:321] Evaluating on the training split.
I0307 13:06:25.663801 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 13:06:49.063598 140269360194752 spec.py:349] Evaluating on the test split.
I0307 13:06:50.811109 140269360194752 submission_runner.py:469] Time since start: 12284.67s, 	Step: 29025, 	{'train/accuracy': 0.6882174611091614, 'train/loss': 1.2280173301696777, 'validation/accuracy': 0.6187799572944641, 'validation/loss': 1.5952154397964478, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.3284976482391357, 'test/num_examples': 10000, 'score': 11276.176182508469, 'total_duration': 12284.668390989304, 'accumulated_submission_time': 11276.176182508469, 'accumulated_eval_time': 1002.5912687778473, 'accumulated_logging_time': 2.948348045349121}
I0307 13:06:50.880007 140113510135552 logging_writer.py:48] [29025] accumulated_eval_time=1002.59, accumulated_logging_time=2.94835, accumulated_submission_time=11276.2, global_step=29025, preemption_count=0, score=11276.2, test/accuracy=0.4902, test/loss=2.3285, test/num_examples=10000, total_duration=12284.7, train/accuracy=0.688217, train/loss=1.22802, validation/accuracy=0.61878, validation/loss=1.59522, validation/num_examples=50000
I0307 13:07:20.575885 140113501742848 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.8949745893478394, loss=1.7876689434051514
I0307 13:07:58.984924 140113510135552 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.863058090209961, loss=1.782960295677185
I0307 13:08:37.959334 140113501742848 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.8261890411376953, loss=1.7105218172073364
I0307 13:09:16.649599 140113510135552 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.9121335744857788, loss=1.7210075855255127
I0307 13:09:55.484118 140113501742848 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.7058299779891968, loss=1.8198715448379517
I0307 13:10:34.058221 140113510135552 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.8841546773910522, loss=1.8360092639923096
I0307 13:11:12.711133 140113501742848 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.6155381202697754, loss=1.8496628999710083
I0307 13:11:51.736642 140113510135552 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7525795698165894, loss=1.7950748205184937
I0307 13:12:30.732746 140113501742848 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.6684242486953735, loss=1.7309634685516357
I0307 13:13:09.826481 140113510135552 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.7711902856826782, loss=1.6038615703582764
I0307 13:13:49.044425 140113501742848 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.9224764108657837, loss=1.6440818309783936
I0307 13:14:27.728851 140113510135552 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.7184653282165527, loss=1.7913999557495117
I0307 13:15:06.309676 140113501742848 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.867350697517395, loss=1.7365949153900146
I0307 13:15:20.879423 140269360194752 spec.py:321] Evaluating on the training split.
I0307 13:15:36.265231 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 13:16:00.318729 140269360194752 spec.py:349] Evaluating on the test split.
I0307 13:16:02.133032 140269360194752 submission_runner.py:469] Time since start: 12835.99s, 	Step: 30339, 	{'train/accuracy': 0.6851682066917419, 'train/loss': 1.2412374019622803, 'validation/accuracy': 0.6202999949455261, 'validation/loss': 1.5862261056900024, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.3271164894104004, 'test/num_examples': 10000, 'score': 11785.983068227768, 'total_duration': 12835.990358352661, 'accumulated_submission_time': 11785.983068227768, 'accumulated_eval_time': 1043.8447053432465, 'accumulated_logging_time': 3.077840566635132}
I0307 13:16:02.247781 140113510135552 logging_writer.py:48] [30339] accumulated_eval_time=1043.84, accumulated_logging_time=3.07784, accumulated_submission_time=11786, global_step=30339, preemption_count=0, score=11786, test/accuracy=0.492, test/loss=2.32712, test/num_examples=10000, total_duration=12836, train/accuracy=0.685168, train/loss=1.24124, validation/accuracy=0.6203, validation/loss=1.58623, validation/num_examples=50000
I0307 13:16:26.350958 140113501742848 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.767792820930481, loss=1.7367750406265259
I0307 13:17:05.262753 140113510135552 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.9315180778503418, loss=1.5979304313659668
I0307 13:17:43.720475 140113501742848 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.821716070175171, loss=1.6949007511138916
I0307 13:18:22.685827 140113510135552 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.7635376453399658, loss=1.6643567085266113
I0307 13:19:01.515075 140113501742848 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.7315770387649536, loss=1.703829050064087
I0307 13:19:39.754184 140113510135552 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.8254432678222656, loss=1.7276430130004883
I0307 13:20:18.351840 140113501742848 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.7556895017623901, loss=1.6493816375732422
I0307 13:20:56.978324 140113510135552 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.8136515617370605, loss=1.7829258441925049
I0307 13:21:35.608807 140113501742848 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.826231837272644, loss=1.8353395462036133
I0307 13:22:14.600919 140113510135552 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.854528784751892, loss=1.8859899044036865
I0307 13:22:53.554615 140113501742848 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.6871198415756226, loss=1.676796555519104
I0307 13:23:32.339903 140113510135552 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.6914339065551758, loss=1.7107696533203125
I0307 13:24:11.099372 140113501742848 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.8358672857284546, loss=1.6905066967010498
I0307 13:24:32.284058 140269360194752 spec.py:321] Evaluating on the training split.
I0307 13:24:48.968556 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 13:25:07.315103 140269360194752 spec.py:349] Evaluating on the test split.
I0307 13:25:09.134170 140269360194752 submission_runner.py:469] Time since start: 13382.99s, 	Step: 31656, 	{'train/accuracy': 0.689871609210968, 'train/loss': 1.216988444328308, 'validation/accuracy': 0.619879961013794, 'validation/loss': 1.5646140575408936, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.253394842147827, 'test/num_examples': 10000, 'score': 12295.836554527283, 'total_duration': 13382.991485595703, 'accumulated_submission_time': 12295.836554527283, 'accumulated_eval_time': 1080.6946363449097, 'accumulated_logging_time': 3.245150327682495}
I0307 13:25:09.266448 140113510135552 logging_writer.py:48] [31656] accumulated_eval_time=1080.69, accumulated_logging_time=3.24515, accumulated_submission_time=12295.8, global_step=31656, preemption_count=0, score=12295.8, test/accuracy=0.5009, test/loss=2.25339, test/num_examples=10000, total_duration=13383, train/accuracy=0.689872, train/loss=1.21699, validation/accuracy=0.61988, validation/loss=1.56461, validation/num_examples=50000
I0307 13:25:26.306077 140113501742848 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.687180519104004, loss=1.7142741680145264
I0307 13:26:05.078770 140113510135552 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7565367221832275, loss=1.7081871032714844
I0307 13:26:43.586523 140113501742848 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.8773889541625977, loss=1.7499065399169922
I0307 13:27:22.897480 140113510135552 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.7776753902435303, loss=1.7507133483886719
I0307 13:28:01.806511 140113501742848 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.6590725183486938, loss=1.7251780033111572
I0307 13:28:40.357721 140113510135552 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.7252349853515625, loss=1.735767126083374
I0307 13:29:18.820945 140113501742848 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.6759437322616577, loss=1.6476622819900513
I0307 13:29:57.185836 140113510135552 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.1444103717803955, loss=1.7336797714233398
I0307 13:30:35.657871 140113501742848 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.7048745155334473, loss=1.6682145595550537
I0307 13:31:14.901447 140113510135552 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.6258474588394165, loss=1.7102625370025635
I0307 13:31:53.478244 140113501742848 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.6268017292022705, loss=1.6847105026245117
I0307 13:32:32.002136 140113510135552 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.894840121269226, loss=1.6615550518035889
I0307 13:33:10.724584 140113501742848 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.9703733921051025, loss=1.620249629020691
I0307 13:33:39.368841 140269360194752 spec.py:321] Evaluating on the training split.
I0307 13:33:51.119786 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 13:34:33.673079 140269360194752 spec.py:349] Evaluating on the test split.
I0307 13:34:35.486394 140269360194752 submission_runner.py:469] Time since start: 13949.34s, 	Step: 32974, 	{'train/accuracy': 0.6886360049247742, 'train/loss': 1.223679542541504, 'validation/accuracy': 0.621399998664856, 'validation/loss': 1.5662318468093872, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.338261842727661, 'test/num_examples': 10000, 'score': 12805.7665848732, 'total_duration': 13949.34369301796, 'accumulated_submission_time': 12805.7665848732, 'accumulated_eval_time': 1136.8119888305664, 'accumulated_logging_time': 3.419074296951294}
I0307 13:34:35.541871 140113510135552 logging_writer.py:48] [32974] accumulated_eval_time=1136.81, accumulated_logging_time=3.41907, accumulated_submission_time=12805.8, global_step=32974, preemption_count=0, score=12805.8, test/accuracy=0.4864, test/loss=2.33826, test/num_examples=10000, total_duration=13949.3, train/accuracy=0.688636, train/loss=1.22368, validation/accuracy=0.6214, validation/loss=1.56623, validation/num_examples=50000
I0307 13:34:46.194267 140113501742848 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.9405056238174438, loss=1.7604588270187378
I0307 13:35:24.932033 140113510135552 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.003469944000244, loss=1.6779470443725586
I0307 13:36:03.263072 140113501742848 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.7518049478530884, loss=1.720623254776001
I0307 13:36:41.782457 140113510135552 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8696939945220947, loss=1.7825709581375122
I0307 13:37:20.406757 140113501742848 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.8967214822769165, loss=1.8065400123596191
I0307 13:37:58.990720 140113510135552 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.8315361738204956, loss=1.7361340522766113
I0307 13:38:38.193885 140113501742848 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.6980443000793457, loss=1.8504853248596191
I0307 13:39:17.056633 140113510135552 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.776448130607605, loss=1.710254192352295
I0307 13:39:55.644296 140113501742848 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.8512852191925049, loss=1.7686108350753784
I0307 13:40:34.679330 140113510135552 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.004826068878174, loss=1.7191728353500366
I0307 13:41:13.127081 140113501742848 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7330482006072998, loss=1.7289561033248901
I0307 13:41:51.782324 140113510135552 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.886336326599121, loss=1.73643159866333
I0307 13:42:30.517617 140113501742848 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.8838295936584473, loss=1.6767878532409668
I0307 13:43:05.589212 140269360194752 spec.py:321] Evaluating on the training split.
I0307 13:43:17.212921 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 13:43:58.356603 140269360194752 spec.py:349] Evaluating on the test split.
I0307 13:44:00.163917 140269360194752 submission_runner.py:469] Time since start: 14514.02s, 	Step: 34292, 	{'train/accuracy': 0.6939373016357422, 'train/loss': 1.2007282972335815, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.5255661010742188, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.266035556793213, 'test/num_examples': 10000, 'score': 13315.649727582932, 'total_duration': 14514.021225452423, 'accumulated_submission_time': 13315.649727582932, 'accumulated_eval_time': 1191.3865106105804, 'accumulated_logging_time': 3.5079076290130615}
I0307 13:44:00.244204 140113510135552 logging_writer.py:48] [34292] accumulated_eval_time=1191.39, accumulated_logging_time=3.50791, accumulated_submission_time=13315.6, global_step=34292, preemption_count=0, score=13315.6, test/accuracy=0.499, test/loss=2.26604, test/num_examples=10000, total_duration=14514, train/accuracy=0.693937, train/loss=1.20073, validation/accuracy=0.63204, validation/loss=1.52557, validation/num_examples=50000
I0307 13:44:03.843940 140113501742848 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.6563010215759277, loss=1.707026720046997
I0307 13:44:42.286379 140113510135552 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7028166055679321, loss=1.6523642539978027
I0307 13:45:20.374322 140113501742848 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.7524276971817017, loss=1.663329839706421
I0307 13:45:59.078839 140113510135552 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.796695351600647, loss=1.6519525051116943
I0307 13:46:37.746223 140113501742848 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.9310846328735352, loss=1.7389352321624756
I0307 13:47:16.171392 140113510135552 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.6962641477584839, loss=1.6498724222183228
I0307 13:47:55.038834 140113501742848 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.7023674249649048, loss=1.7306506633758545
I0307 13:48:33.465139 140113510135552 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.7788381576538086, loss=1.7580907344818115
I0307 13:49:12.375634 140113501742848 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.7787630558013916, loss=1.715189814567566
I0307 13:49:51.034351 140113510135552 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.9267572164535522, loss=1.7824413776397705
I0307 13:50:29.456334 140113501742848 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.8958524465560913, loss=1.6468652486801147
I0307 13:51:07.827087 140113510135552 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.6981136798858643, loss=1.7152080535888672
I0307 13:51:46.511651 140113501742848 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.7911423444747925, loss=1.7419764995574951
I0307 13:52:25.128335 140113510135552 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.9951114654541016, loss=1.690697431564331
I0307 13:52:30.229586 140269360194752 spec.py:321] Evaluating on the training split.
I0307 13:52:41.682884 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 13:53:08.828371 140269360194752 spec.py:349] Evaluating on the test split.
I0307 13:53:10.618152 140269360194752 submission_runner.py:469] Time since start: 15064.48s, 	Step: 35614, 	{'train/accuracy': 0.6938974857330322, 'train/loss': 1.19839346408844, 'validation/accuracy': 0.625220000743866, 'validation/loss': 1.543534278869629, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2703428268432617, 'test/num_examples': 10000, 'score': 13825.470371246338, 'total_duration': 15064.475488901138, 'accumulated_submission_time': 13825.470371246338, 'accumulated_eval_time': 1231.7749123573303, 'accumulated_logging_time': 3.619274854660034}
I0307 13:53:10.735505 140113501742848 logging_writer.py:48] [35614] accumulated_eval_time=1231.77, accumulated_logging_time=3.61927, accumulated_submission_time=13825.5, global_step=35614, preemption_count=0, score=13825.5, test/accuracy=0.5055, test/loss=2.27034, test/num_examples=10000, total_duration=15064.5, train/accuracy=0.693897, train/loss=1.19839, validation/accuracy=0.62522, validation/loss=1.54353, validation/num_examples=50000
I0307 13:53:44.179608 140113510135552 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.6956379413604736, loss=1.7277673482894897
I0307 13:54:22.894780 140113501742848 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.342407703399658, loss=1.644035816192627
I0307 13:55:01.922777 140113510135552 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.6263495683670044, loss=1.6616407632827759
I0307 13:55:40.227658 140113501742848 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.9703750610351562, loss=1.7405586242675781
I0307 13:56:18.523528 140113510135552 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7930667400360107, loss=1.6402071714401245
I0307 13:56:56.915077 140113501742848 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.7267976999282837, loss=1.7047313451766968
I0307 13:57:36.122535 140113510135552 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.0529990196228027, loss=1.640249252319336
I0307 13:58:14.549258 140113501742848 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.6811152696609497, loss=1.6954352855682373
I0307 13:58:52.977227 140113510135552 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.6831676959991455, loss=1.6717216968536377
I0307 13:59:31.730447 140113501742848 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.0004937648773193, loss=1.773042917251587
I0307 14:00:10.569353 140113510135552 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.7255237102508545, loss=1.725879192352295
I0307 14:00:49.335036 140113501742848 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.918789267539978, loss=1.824396014213562
I0307 14:01:27.863753 140113510135552 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.6918394565582275, loss=1.72397780418396
I0307 14:01:40.949471 140269360194752 spec.py:321] Evaluating on the training split.
I0307 14:01:53.128039 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 14:02:30.998436 140269360194752 spec.py:349] Evaluating on the test split.
I0307 14:02:32.811975 140269360194752 submission_runner.py:469] Time since start: 15626.67s, 	Step: 36934, 	{'train/accuracy': 0.6949537396430969, 'train/loss': 1.1953582763671875, 'validation/accuracy': 0.6283199787139893, 'validation/loss': 1.5368438959121704, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2652556896209717, 'test/num_examples': 10000, 'score': 14335.530648946762, 'total_duration': 15626.66927909851, 'accumulated_submission_time': 14335.530648946762, 'accumulated_eval_time': 1283.6372187137604, 'accumulated_logging_time': 3.7568199634552}
I0307 14:02:32.940599 140113501742848 logging_writer.py:48] [36934] accumulated_eval_time=1283.64, accumulated_logging_time=3.75682, accumulated_submission_time=14335.5, global_step=36934, preemption_count=0, score=14335.5, test/accuracy=0.5001, test/loss=2.26526, test/num_examples=10000, total_duration=15626.7, train/accuracy=0.694954, train/loss=1.19536, validation/accuracy=0.62832, validation/loss=1.53684, validation/num_examples=50000
I0307 14:02:59.265461 140113510135552 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.7696024179458618, loss=1.6291873455047607
I0307 14:03:37.425286 140113501742848 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.7504328489303589, loss=1.6408779621124268
I0307 14:04:15.890543 140113510135552 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.026038408279419, loss=1.6451977491378784
I0307 14:04:54.769975 140113501742848 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.72743821144104, loss=1.6726338863372803
I0307 14:05:33.279400 140113510135552 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.797850489616394, loss=1.673639178276062
I0307 14:06:42.136861 140113501742848 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.9473645687103271, loss=1.7588742971420288
I0307 14:07:21.902533 140113510135552 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.649369478225708, loss=1.6177667379379272
I0307 14:08:01.803404 140113501742848 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.9459410905838013, loss=1.6871538162231445
I0307 14:08:40.370074 140113510135552 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.5762914419174194, loss=1.6557031869888306
I0307 14:09:18.782053 140113501742848 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.7954262495040894, loss=1.712985873222351
I0307 14:09:57.539264 140113510135552 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.96304452419281, loss=1.5694618225097656
I0307 14:10:35.986220 140113501742848 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.8902281522750854, loss=1.637940526008606
I0307 14:11:02.863653 140269360194752 spec.py:321] Evaluating on the training split.
I0307 14:11:15.096904 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 14:11:58.602711 140269360194752 spec.py:349] Evaluating on the test split.
I0307 14:12:00.339823 140269360194752 submission_runner.py:469] Time since start: 16194.20s, 	Step: 38171, 	{'train/accuracy': 0.6978435516357422, 'train/loss': 1.1887882947921753, 'validation/accuracy': 0.6301199793815613, 'validation/loss': 1.5258427858352661, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.2854816913604736, 'test/num_examples': 10000, 'score': 14845.296821832657, 'total_duration': 16194.19715476036, 'accumulated_submission_time': 14845.296821832657, 'accumulated_eval_time': 1341.1132242679596, 'accumulated_logging_time': 3.909550189971924}
I0307 14:12:00.405907 140113510135552 logging_writer.py:48] [38171] accumulated_eval_time=1341.11, accumulated_logging_time=3.90955, accumulated_submission_time=14845.3, global_step=38171, preemption_count=0, score=14845.3, test/accuracy=0.5036, test/loss=2.28548, test/num_examples=10000, total_duration=16194.2, train/accuracy=0.697844, train/loss=1.18879, validation/accuracy=0.63012, validation/loss=1.52584, validation/num_examples=50000
I0307 14:12:12.414241 140113501742848 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.7025378942489624, loss=1.507845163345337
I0307 14:12:51.030470 140113510135552 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.072622537612915, loss=1.685200572013855
I0307 14:13:29.955291 140113501742848 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7204750776290894, loss=1.6521952152252197
I0307 14:14:08.366383 140113510135552 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.9711323976516724, loss=1.7091476917266846
I0307 14:14:47.596979 140113501742848 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.0675101280212402, loss=1.6709132194519043
I0307 14:15:26.310506 140113510135552 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.7547162771224976, loss=1.5963889360427856
I0307 14:16:05.463365 140113501742848 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.119884729385376, loss=1.694348931312561
I0307 14:16:44.349776 140113510135552 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.8259979486465454, loss=1.748029351234436
I0307 14:17:23.278007 140113501742848 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.8197559118270874, loss=1.6889842748641968
I0307 14:18:01.681078 140113510135552 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.7355873584747314, loss=1.6908204555511475
I0307 14:18:40.399102 140113501742848 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.1529269218444824, loss=1.635593056678772
I0307 14:19:19.014831 140113510135552 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.118072271347046, loss=1.6965861320495605
I0307 14:19:57.771406 140113501742848 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.6965430974960327, loss=1.5769531726837158
I0307 14:20:30.459089 140269360194752 spec.py:321] Evaluating on the training split.
I0307 14:20:42.990776 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 14:21:18.163970 140269360194752 spec.py:349] Evaluating on the test split.
I0307 14:21:19.961537 140269360194752 submission_runner.py:469] Time since start: 16753.82s, 	Step: 39485, 	{'train/accuracy': 0.6980627775192261, 'train/loss': 1.1838842630386353, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.5314708948135376, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2674360275268555, 'test/num_examples': 10000, 'score': 15355.186982154846, 'total_duration': 16753.81883621216, 'accumulated_submission_time': 15355.186982154846, 'accumulated_eval_time': 1390.6154713630676, 'accumulated_logging_time': 4.0049965381622314}
I0307 14:21:20.037997 140113510135552 logging_writer.py:48] [39485] accumulated_eval_time=1390.62, accumulated_logging_time=4.005, accumulated_submission_time=15355.2, global_step=39485, preemption_count=0, score=15355.2, test/accuracy=0.501, test/loss=2.26744, test/num_examples=10000, total_duration=16753.8, train/accuracy=0.698063, train/loss=1.18388, validation/accuracy=0.63068, validation/loss=1.53147, validation/num_examples=50000
I0307 14:21:26.349455 140113501742848 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.009488582611084, loss=1.7448465824127197
I0307 14:22:04.683363 140113510135552 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.8370387554168701, loss=1.7279243469238281
I0307 14:22:43.384570 140113501742848 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.738347053527832, loss=1.7193435430526733
I0307 14:23:22.173205 140113510135552 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.873093843460083, loss=1.687578558921814
I0307 14:24:00.676642 140113501742848 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.7075070142745972, loss=1.6200807094573975
I0307 14:24:39.590215 140113510135552 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.8036859035491943, loss=1.6703107357025146
I0307 14:25:19.027471 140113501742848 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.7532902956008911, loss=1.6454126834869385
I0307 14:25:57.699041 140113510135552 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.7165770530700684, loss=1.6869332790374756
I0307 14:26:36.208275 140113501742848 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.8781492710113525, loss=1.6836169958114624
I0307 14:27:14.936226 140113510135552 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.7765394449234009, loss=1.678494930267334
I0307 14:27:53.353062 140113501742848 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.7471154928207397, loss=1.5850590467453003
I0307 14:28:32.125450 140113510135552 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.8164106607437134, loss=1.525772213935852
I0307 14:29:11.002668 140113501742848 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.7330763339996338, loss=1.624672532081604
I0307 14:29:49.809727 140113510135552 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7805263996124268, loss=1.7283908128738403
I0307 14:29:50.270516 140269360194752 spec.py:321] Evaluating on the training split.
I0307 14:30:03.115000 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 14:30:38.089679 140269360194752 spec.py:349] Evaluating on the test split.
I0307 14:30:39.907493 140269360194752 submission_runner.py:469] Time since start: 17313.76s, 	Step: 40802, 	{'train/accuracy': 0.7097018361091614, 'train/loss': 1.1230547428131104, 'validation/accuracy': 0.6408799886703491, 'validation/loss': 1.482406497001648, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.2179179191589355, 'test/num_examples': 10000, 'score': 15865.208394289017, 'total_duration': 17313.76481819153, 'accumulated_submission_time': 15865.208394289017, 'accumulated_eval_time': 1440.2522711753845, 'accumulated_logging_time': 4.158671140670776}
I0307 14:30:39.987772 140113501742848 logging_writer.py:48] [40802] accumulated_eval_time=1440.25, accumulated_logging_time=4.15867, accumulated_submission_time=15865.2, global_step=40802, preemption_count=0, score=15865.2, test/accuracy=0.5129, test/loss=2.21792, test/num_examples=10000, total_duration=17313.8, train/accuracy=0.709702, train/loss=1.12305, validation/accuracy=0.64088, validation/loss=1.48241, validation/num_examples=50000
I0307 14:31:18.377471 140113510135552 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.0574700832366943, loss=1.6940975189208984
I0307 14:31:56.881970 140113501742848 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.8695921897888184, loss=1.7122855186462402
I0307 14:32:35.529066 140113510135552 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.731277346611023, loss=1.7144056558609009
I0307 14:33:14.228769 140113501742848 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.8918719291687012, loss=1.725465178489685
I0307 14:33:53.676044 140113510135552 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.163252592086792, loss=1.6363542079925537
I0307 14:34:32.562385 140113501742848 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.7066599130630493, loss=1.5443484783172607
I0307 14:35:11.274617 140113510135552 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.957406997680664, loss=1.6101195812225342
I0307 14:35:50.171356 140113501742848 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.8128312826156616, loss=1.5010652542114258
I0307 14:36:28.875044 140113510135552 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.02091121673584, loss=1.7213724851608276
I0307 14:37:07.297889 140113501742848 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.6787707805633545, loss=1.5454210042953491
I0307 14:37:45.980121 140113510135552 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.6724631786346436, loss=1.7098801136016846
I0307 14:38:24.810227 140113501742848 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7438715696334839, loss=1.690003514289856
I0307 14:39:03.350115 140113510135552 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.656518578529358, loss=1.791683554649353
I0307 14:39:09.959178 140269360194752 spec.py:321] Evaluating on the training split.
I0307 14:39:22.323485 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 14:39:52.700019 140269360194752 spec.py:349] Evaluating on the test split.
I0307 14:39:54.580232 140269360194752 submission_runner.py:469] Time since start: 17868.44s, 	Step: 42118, 	{'train/accuracy': 0.6983816623687744, 'train/loss': 1.167810320854187, 'validation/accuracy': 0.6308000087738037, 'validation/loss': 1.523690938949585, 'validation/num_examples': 50000, 'test/accuracy': 0.5017000436782837, 'test/loss': 2.2804453372955322, 'test/num_examples': 10000, 'score': 16375.017258882523, 'total_duration': 17868.437598466873, 'accumulated_submission_time': 16375.017258882523, 'accumulated_eval_time': 1484.8731899261475, 'accumulated_logging_time': 4.269007444381714}
I0307 14:39:54.676177 140113501742848 logging_writer.py:48] [42118] accumulated_eval_time=1484.87, accumulated_logging_time=4.26901, accumulated_submission_time=16375, global_step=42118, preemption_count=0, score=16375, test/accuracy=0.5017, test/loss=2.28045, test/num_examples=10000, total_duration=17868.4, train/accuracy=0.698382, train/loss=1.16781, validation/accuracy=0.6308, validation/loss=1.52369, validation/num_examples=50000
I0307 14:40:26.609711 140113510135552 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.7394331693649292, loss=1.6021037101745605
I0307 14:41:05.260177 140113501742848 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.7720834016799927, loss=1.6504778861999512
I0307 14:41:44.301356 140113510135552 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.7108805179595947, loss=1.5742685794830322
I0307 14:42:23.006420 140113501742848 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.045165777206421, loss=1.5635013580322266
I0307 14:43:02.001760 140113510135552 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.7717348337173462, loss=1.7002638578414917
I0307 14:43:40.848427 140113501742848 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.7379640340805054, loss=1.6432771682739258
I0307 14:44:19.605845 140113510135552 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.7750285863876343, loss=1.6562408208847046
I0307 14:44:58.184360 140113501742848 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.9031896591186523, loss=1.6678783893585205
I0307 14:45:36.943410 140113510135552 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.9555178880691528, loss=1.6013250350952148
I0307 14:46:15.691478 140113501742848 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.8814960718154907, loss=1.6037495136260986
I0307 14:46:54.196183 140113510135552 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.705140233039856, loss=1.686702847480774
I0307 14:47:32.793796 140113501742848 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.6679306030273438, loss=1.6627589464187622
I0307 14:48:11.714009 140113510135552 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.8407480716705322, loss=1.704115629196167
I0307 14:48:24.763230 140269360194752 spec.py:321] Evaluating on the training split.
I0307 14:48:37.150493 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 14:48:56.607147 140269360194752 spec.py:349] Evaluating on the test split.
I0307 14:48:58.448671 140269360194752 submission_runner.py:469] Time since start: 18412.31s, 	Step: 43435, 	{'train/accuracy': 0.7061343789100647, 'train/loss': 1.1475305557250977, 'validation/accuracy': 0.6379199624061584, 'validation/loss': 1.4962568283081055, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.2154202461242676, 'test/num_examples': 10000, 'score': 16884.944253206253, 'total_duration': 18412.305977106094, 'accumulated_submission_time': 16884.944253206253, 'accumulated_eval_time': 1518.5584359169006, 'accumulated_logging_time': 4.390644311904907}
I0307 14:48:58.501076 140113501742848 logging_writer.py:48] [43435] accumulated_eval_time=1518.56, accumulated_logging_time=4.39064, accumulated_submission_time=16884.9, global_step=43435, preemption_count=0, score=16884.9, test/accuracy=0.5126, test/loss=2.21542, test/num_examples=10000, total_duration=18412.3, train/accuracy=0.706134, train/loss=1.14753, validation/accuracy=0.63792, validation/loss=1.49626, validation/num_examples=50000
I0307 14:49:24.043388 140113510135552 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.9539116621017456, loss=1.6736905574798584
I0307 14:50:03.254860 140113501742848 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.9256746768951416, loss=1.6088498830795288
I0307 14:50:42.748115 140113510135552 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.757126808166504, loss=1.6155943870544434
I0307 14:51:22.045417 140113501742848 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.8608747720718384, loss=1.6632899045944214
I0307 14:52:00.710859 140113510135552 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.8483062982559204, loss=1.668726921081543
I0307 14:52:39.097201 140113501742848 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.8980001211166382, loss=1.726847767829895
I0307 14:53:17.882436 140113510135552 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.8940976858139038, loss=1.7110340595245361
I0307 14:53:56.636964 140113501742848 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.7501095533370972, loss=1.6362067461013794
I0307 14:54:35.370205 140113510135552 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.724738597869873, loss=1.5342769622802734
I0307 14:55:13.813788 140113501742848 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.932429313659668, loss=1.573805809020996
I0307 14:55:52.575808 140113510135552 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.7685514688491821, loss=1.6316698789596558
I0307 14:56:30.790420 140113501742848 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8290249109268188, loss=1.6381049156188965
I0307 14:57:09.197429 140113510135552 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.9731868505477905, loss=1.7104008197784424
I0307 14:57:28.539873 140269360194752 spec.py:321] Evaluating on the training split.
I0307 14:57:40.969239 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 14:58:13.131108 140269360194752 spec.py:349] Evaluating on the test split.
I0307 14:58:14.926403 140269360194752 submission_runner.py:469] Time since start: 18968.78s, 	Step: 44751, 	{'train/accuracy': 0.7056361436843872, 'train/loss': 1.1494941711425781, 'validation/accuracy': 0.6333799958229065, 'validation/loss': 1.5034329891204834, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.183959484100342, 'test/num_examples': 10000, 'score': 17394.775994062424, 'total_duration': 18968.783734560013, 'accumulated_submission_time': 17394.775994062424, 'accumulated_eval_time': 1564.9447996616364, 'accumulated_logging_time': 4.51846170425415}
I0307 14:58:14.977445 140113501742848 logging_writer.py:48] [44751] accumulated_eval_time=1564.94, accumulated_logging_time=4.51846, accumulated_submission_time=17394.8, global_step=44751, preemption_count=0, score=17394.8, test/accuracy=0.5137, test/loss=2.18396, test/num_examples=10000, total_duration=18968.8, train/accuracy=0.705636, train/loss=1.14949, validation/accuracy=0.63338, validation/loss=1.50343, validation/num_examples=50000
I0307 14:58:34.669574 140113510135552 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.7117986679077148, loss=1.6598185300827026
I0307 14:59:13.955734 140113501742848 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.786356806755066, loss=1.6207284927368164
I0307 14:59:52.756525 140113510135552 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.7766937017440796, loss=1.5551289319992065
I0307 15:00:32.329372 140113501742848 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.09606671333313, loss=1.6619300842285156
I0307 15:01:10.900092 140113510135552 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.7917472124099731, loss=1.6084599494934082
I0307 15:01:49.323502 140113501742848 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.130093574523926, loss=1.7172913551330566
I0307 15:02:28.107062 140113510135552 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.7840203046798706, loss=1.5983684062957764
I0307 15:03:06.943717 140113501742848 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.8423303365707397, loss=1.7285051345825195
I0307 15:03:45.578848 140113510135552 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.7588114738464355, loss=1.535088062286377
I0307 15:04:23.889670 140113501742848 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.8944292068481445, loss=1.5020084381103516
I0307 15:05:02.336217 140113510135552 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.9516843557357788, loss=1.6196684837341309
I0307 15:05:40.975970 140113501742848 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.6982136964797974, loss=1.546526312828064
I0307 15:06:19.956458 140113510135552 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.664831280708313, loss=1.5999360084533691
I0307 15:06:45.268900 140269360194752 spec.py:321] Evaluating on the training split.
I0307 15:06:58.021737 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 15:07:29.341305 140269360194752 spec.py:349] Evaluating on the test split.
I0307 15:07:31.158102 140269360194752 submission_runner.py:469] Time since start: 19525.02s, 	Step: 46066, 	{'train/accuracy': 0.7078284025192261, 'train/loss': 1.1308703422546387, 'validation/accuracy': 0.6412799954414368, 'validation/loss': 1.4750566482543945, 'validation/num_examples': 50000, 'test/accuracy': 0.5138000249862671, 'test/loss': 2.216357469558716, 'test/num_examples': 10000, 'score': 17904.899745464325, 'total_duration': 19525.015421628952, 'accumulated_submission_time': 17904.899745464325, 'accumulated_eval_time': 1610.8338239192963, 'accumulated_logging_time': 4.603740692138672}
I0307 15:07:31.230743 140113501742848 logging_writer.py:48] [46066] accumulated_eval_time=1610.83, accumulated_logging_time=4.60374, accumulated_submission_time=17904.9, global_step=46066, preemption_count=0, score=17904.9, test/accuracy=0.5138, test/loss=2.21636, test/num_examples=10000, total_duration=19525, train/accuracy=0.707828, train/loss=1.13087, validation/accuracy=0.64128, validation/loss=1.47506, validation/num_examples=50000
I0307 15:07:44.757296 140113510135552 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.7888152599334717, loss=1.605014681816101
I0307 15:08:23.962987 140113501742848 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.8442471027374268, loss=1.6437410116195679
I0307 15:09:03.182114 140113510135552 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.833257794380188, loss=1.7042186260223389
I0307 15:09:42.390267 140113501742848 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.8720366954803467, loss=1.687635898590088
I0307 15:10:21.116220 140113510135552 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.699426531791687, loss=1.6880502700805664
I0307 15:10:59.811185 140113501742848 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.689994215965271, loss=1.500174641609192
I0307 15:11:38.583005 140113510135552 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.820416808128357, loss=1.7255854606628418
I0307 15:12:17.186036 140113501742848 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.0798885822296143, loss=1.6606411933898926
I0307 15:12:55.899509 140113510135552 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.8888938426971436, loss=1.6546666622161865
I0307 15:13:34.696202 140113501742848 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.9772357940673828, loss=1.7267662286758423
I0307 15:14:13.561170 140113510135552 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7087328433990479, loss=1.5610768795013428
I0307 15:14:52.336992 140113501742848 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.7900128364562988, loss=1.6306124925613403
I0307 15:15:30.395033 140113510135552 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.980459213256836, loss=1.5765141248703003
I0307 15:16:01.312013 140269360194752 spec.py:321] Evaluating on the training split.
I0307 15:16:13.665048 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 15:16:44.626639 140269360194752 spec.py:349] Evaluating on the test split.
I0307 15:16:46.422202 140269360194752 submission_runner.py:469] Time since start: 20080.28s, 	Step: 47380, 	{'train/accuracy': 0.7111766338348389, 'train/loss': 1.115090250968933, 'validation/accuracy': 0.6467399597167969, 'validation/loss': 1.455227255821228, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.187701463699341, 'test/num_examples': 10000, 'score': 18414.812789201736, 'total_duration': 20080.279507637024, 'accumulated_submission_time': 18414.812789201736, 'accumulated_eval_time': 1655.9438173770905, 'accumulated_logging_time': 4.706700086593628}
I0307 15:16:46.542578 140113501742848 logging_writer.py:48] [47380] accumulated_eval_time=1655.94, accumulated_logging_time=4.7067, accumulated_submission_time=18414.8, global_step=47380, preemption_count=0, score=18414.8, test/accuracy=0.52, test/loss=2.1877, test/num_examples=10000, total_duration=20080.3, train/accuracy=0.711177, train/loss=1.11509, validation/accuracy=0.64674, validation/loss=1.45523, validation/num_examples=50000
I0307 15:16:55.077742 140113510135552 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.8575702905654907, loss=1.586970329284668
I0307 15:17:34.037855 140113501742848 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.892014980316162, loss=1.73862886428833
I0307 15:18:13.133146 140113510135552 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.9089558124542236, loss=1.6338555812835693
I0307 15:18:51.838165 140113501742848 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7837190628051758, loss=1.5138661861419678
I0307 15:19:30.641509 140113510135552 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.172276496887207, loss=1.6300809383392334
I0307 15:20:09.577222 140113501742848 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.886715054512024, loss=1.595359206199646
I0307 15:20:48.117351 140113510135552 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.8523567914962769, loss=1.5391018390655518
I0307 15:21:26.728446 140113501742848 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.9766771793365479, loss=1.7257883548736572
I0307 15:22:05.224921 140113510135552 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7047080993652344, loss=1.5685962438583374
I0307 15:22:44.059019 140113501742848 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.8935158252716064, loss=1.639240026473999
I0307 15:23:22.919924 140113510135552 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.8281499147415161, loss=1.6413114070892334
I0307 15:24:01.194875 140113501742848 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.6725618839263916, loss=1.5263557434082031
I0307 15:24:39.814909 140113510135552 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8126294612884521, loss=1.5763928890228271
I0307 15:25:16.756317 140269360194752 spec.py:321] Evaluating on the training split.
I0307 15:25:29.078797 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 15:25:49.521333 140269360194752 spec.py:349] Evaluating on the test split.
I0307 15:25:51.420830 140269360194752 submission_runner.py:469] Time since start: 20625.28s, 	Step: 48696, 	{'train/accuracy': 0.7036033272743225, 'train/loss': 1.1529393196105957, 'validation/accuracy': 0.6352399587631226, 'validation/loss': 1.5055887699127197, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.2442681789398193, 'test/num_examples': 10000, 'score': 18924.855459690094, 'total_duration': 20625.278146266937, 'accumulated_submission_time': 18924.855459690094, 'accumulated_eval_time': 1690.6081550121307, 'accumulated_logging_time': 4.8652544021606445}
I0307 15:25:51.574613 140113501742848 logging_writer.py:48] [48696] accumulated_eval_time=1690.61, accumulated_logging_time=4.86525, accumulated_submission_time=18924.9, global_step=48696, preemption_count=0, score=18924.9, test/accuracy=0.5099, test/loss=2.24427, test/num_examples=10000, total_duration=20625.3, train/accuracy=0.703603, train/loss=1.15294, validation/accuracy=0.63524, validation/loss=1.50559, validation/num_examples=50000
I0307 15:25:53.523879 140113510135552 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.8877841234207153, loss=1.5774524211883545
I0307 15:26:32.939259 140113501742848 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.9205785989761353, loss=1.6921135187149048
I0307 15:27:11.498926 140113510135552 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.9073432683944702, loss=1.7222349643707275
I0307 15:27:50.176027 140113501742848 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.847251057624817, loss=1.5752875804901123
I0307 15:28:28.628704 140113510135552 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.934457778930664, loss=1.572217345237732
I0307 15:29:07.115423 140113501742848 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.9648562669754028, loss=1.61403226852417
I0307 15:29:45.897409 140113510135552 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.7446008920669556, loss=1.642899513244629
I0307 15:30:24.656153 140113501742848 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.9617033004760742, loss=1.6287566423416138
I0307 15:31:03.478136 140113510135552 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.8102082014083862, loss=1.6247531175613403
I0307 15:31:41.667171 140113501742848 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.9330408573150635, loss=1.6450146436691284
I0307 15:32:20.693968 140113510135552 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.7866638898849487, loss=1.6251286268234253
I0307 15:32:59.121677 140113501742848 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.038142681121826, loss=1.7469347715377808
I0307 15:33:37.158841 140113510135552 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.8920698165893555, loss=1.7439934015274048
I0307 15:34:15.403560 140113501742848 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.8056498765945435, loss=1.6358041763305664
I0307 15:34:21.559713 140269360194752 spec.py:321] Evaluating on the training split.
I0307 15:34:34.594258 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 15:35:06.637644 140269360194752 spec.py:349] Evaluating on the test split.
I0307 15:35:08.559094 140269360194752 submission_runner.py:469] Time since start: 21182.42s, 	Step: 50017, 	{'train/accuracy': 0.7104990482330322, 'train/loss': 1.1222102642059326, 'validation/accuracy': 0.6417999863624573, 'validation/loss': 1.4830840826034546, 'validation/num_examples': 50000, 'test/accuracy': 0.5064000487327576, 'test/loss': 2.255596876144409, 'test/num_examples': 10000, 'score': 19434.673849105835, 'total_duration': 21182.41639828682, 'accumulated_submission_time': 19434.673849105835, 'accumulated_eval_time': 1737.6073410511017, 'accumulated_logging_time': 5.051330804824829}
I0307 15:35:08.686632 140113510135552 logging_writer.py:48] [50017] accumulated_eval_time=1737.61, accumulated_logging_time=5.05133, accumulated_submission_time=19434.7, global_step=50017, preemption_count=0, score=19434.7, test/accuracy=0.5064, test/loss=2.2556, test/num_examples=10000, total_duration=21182.4, train/accuracy=0.710499, train/loss=1.12221, validation/accuracy=0.6418, validation/loss=1.48308, validation/num_examples=50000
I0307 15:35:41.177521 140113501742848 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.9794870615005493, loss=1.6667640209197998
I0307 15:36:19.488251 140113510135552 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.8610138893127441, loss=1.6541725397109985
I0307 15:36:58.093987 140113501742848 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7781354188919067, loss=1.7067580223083496
I0307 15:37:36.632233 140113510135552 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.897958517074585, loss=1.5703251361846924
I0307 15:38:14.933109 140113501742848 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.8792520761489868, loss=1.648139476776123
I0307 15:38:53.589545 140113510135552 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.6895445585250854, loss=1.5333619117736816
I0307 15:39:32.779528 140113501742848 logging_writer.py:48] [50700] global_step=50700, grad_norm=2.1356446743011475, loss=1.6895675659179688
I0307 15:40:12.498676 140113510135552 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8277699947357178, loss=1.596459150314331
I0307 15:40:52.154374 140113501742848 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.9164605140686035, loss=1.5737733840942383
I0307 15:41:30.485830 140113510135552 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.8494484424591064, loss=1.6631782054901123
I0307 15:42:08.734470 140113501742848 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.6792408227920532, loss=1.6086902618408203
I0307 15:42:47.496209 140113510135552 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.130462408065796, loss=1.660611867904663
I0307 15:43:26.581534 140113501742848 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.865875005722046, loss=1.6162827014923096
I0307 15:43:38.586466 140269360194752 spec.py:321] Evaluating on the training split.
I0307 15:43:51.108097 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 15:44:16.579453 140269360194752 spec.py:349] Evaluating on the test split.
I0307 15:44:18.389314 140269360194752 submission_runner.py:469] Time since start: 21732.25s, 	Step: 51332, 	{'train/accuracy': 0.7073301672935486, 'train/loss': 1.131919503211975, 'validation/accuracy': 0.6402199864387512, 'validation/loss': 1.47739839553833, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2346036434173584, 'test/num_examples': 10000, 'score': 19944.40364098549, 'total_duration': 21732.246586322784, 'accumulated_submission_time': 19944.40364098549, 'accumulated_eval_time': 1777.4099655151367, 'accumulated_logging_time': 5.213693857192993}
I0307 15:44:18.497904 140113510135552 logging_writer.py:48] [51332] accumulated_eval_time=1777.41, accumulated_logging_time=5.21369, accumulated_submission_time=19944.4, global_step=51332, preemption_count=0, score=19944.4, test/accuracy=0.5055, test/loss=2.2346, test/num_examples=10000, total_duration=21732.2, train/accuracy=0.70733, train/loss=1.13192, validation/accuracy=0.64022, validation/loss=1.4774, validation/num_examples=50000
I0307 15:44:45.268045 140113501742848 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.8184653520584106, loss=1.5038353204727173
I0307 15:45:23.967296 140113510135552 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.766975998878479, loss=1.6012967824935913
I0307 15:46:02.473464 140113501742848 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.7763603925704956, loss=1.5451899766921997
I0307 15:46:41.389873 140113510135552 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8111281394958496, loss=1.7030529975891113
I0307 15:47:20.028815 140113501742848 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.7623473405838013, loss=1.5962855815887451
I0307 15:47:58.668328 140113510135552 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.8543262481689453, loss=1.5737396478652954
I0307 15:48:37.410429 140113501742848 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.8582712411880493, loss=1.6156935691833496
I0307 15:49:16.138508 140113510135552 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.847453236579895, loss=1.7081384658813477
I0307 15:49:54.934793 140113501742848 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.8833256959915161, loss=1.662336826324463
I0307 15:50:33.558056 140113510135552 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.9460045099258423, loss=1.6114842891693115
I0307 15:51:11.933432 140113501742848 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.9955440759658813, loss=1.584551215171814
I0307 15:51:50.541001 140113510135552 logging_writer.py:48] [52500] global_step=52500, grad_norm=2.0137016773223877, loss=1.5455381870269775
I0307 15:52:29.374979 140113501742848 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.7613682746887207, loss=1.5633211135864258
I0307 15:52:48.679439 140269360194752 spec.py:321] Evaluating on the training split.
I0307 15:53:01.319823 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 15:53:27.858950 140269360194752 spec.py:349] Evaluating on the test split.
I0307 15:53:29.658711 140269360194752 submission_runner.py:469] Time since start: 22283.52s, 	Step: 52651, 	{'train/accuracy': 0.6966677308082581, 'train/loss': 1.1785019636154175, 'validation/accuracy': 0.6322799921035767, 'validation/loss': 1.5149649381637573, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.2282726764678955, 'test/num_examples': 10000, 'score': 20454.419619083405, 'total_duration': 22283.516021966934, 'accumulated_submission_time': 20454.419619083405, 'accumulated_eval_time': 1818.3890566825867, 'accumulated_logging_time': 5.356108903884888}
I0307 15:53:29.747270 140113510135552 logging_writer.py:48] [52651] accumulated_eval_time=1818.39, accumulated_logging_time=5.35611, accumulated_submission_time=20454.4, global_step=52651, preemption_count=0, score=20454.4, test/accuracy=0.5134, test/loss=2.22827, test/num_examples=10000, total_duration=22283.5, train/accuracy=0.696668, train/loss=1.1785, validation/accuracy=0.63228, validation/loss=1.51496, validation/num_examples=50000
I0307 15:53:49.330855 140113501742848 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.9184945821762085, loss=1.586568832397461
I0307 15:54:27.938973 140113510135552 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.9799940586090088, loss=1.6822035312652588
I0307 15:55:06.628295 140113501742848 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.7532110214233398, loss=1.6382434368133545
I0307 15:55:45.468712 140113510135552 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.9059867858886719, loss=1.6652628183364868
I0307 15:56:23.984135 140113501742848 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.794049859046936, loss=1.6328768730163574
I0307 15:57:02.571660 140113510135552 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.754578948020935, loss=1.6193772554397583
I0307 15:57:42.400393 140113501742848 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.9486716985702515, loss=1.5775095224380493
I0307 15:58:21.064032 140113510135552 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8423335552215576, loss=1.6557841300964355
I0307 15:58:59.861997 140113501742848 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.1611409187316895, loss=1.7102926969528198
I0307 15:59:38.606497 140113510135552 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.8123376369476318, loss=1.4985647201538086
I0307 16:00:17.596105 140113501742848 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.864603877067566, loss=1.6200772523880005
I0307 16:00:56.421598 140113510135552 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.7296454906463623, loss=1.5343645811080933
I0307 16:01:35.501544 140113501742848 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.8224095106124878, loss=1.6257420778274536
I0307 16:02:00.064075 140269360194752 spec.py:321] Evaluating on the training split.
I0307 16:02:12.846154 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 16:02:34.964304 140269360194752 spec.py:349] Evaluating on the test split.
I0307 16:02:36.746717 140269360194752 submission_runner.py:469] Time since start: 22830.60s, 	Step: 53965, 	{'train/accuracy': 0.7091039419174194, 'train/loss': 1.1363623142242432, 'validation/accuracy': 0.6449199914932251, 'validation/loss': 1.4615753889083862, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.2126657962799072, 'test/num_examples': 10000, 'score': 20964.578180789948, 'total_duration': 22830.6040391922, 'accumulated_submission_time': 20964.578180789948, 'accumulated_eval_time': 1855.0715210437775, 'accumulated_logging_time': 5.467150449752808}
I0307 16:02:36.831953 140113510135552 logging_writer.py:48] [53965] accumulated_eval_time=1855.07, accumulated_logging_time=5.46715, accumulated_submission_time=20964.6, global_step=53965, preemption_count=0, score=20964.6, test/accuracy=0.5099, test/loss=2.21267, test/num_examples=10000, total_duration=22830.6, train/accuracy=0.709104, train/loss=1.13636, validation/accuracy=0.64492, validation/loss=1.46158, validation/num_examples=50000
I0307 16:02:50.864553 140113501742848 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8616001605987549, loss=1.6498363018035889
I0307 16:03:29.454271 140113510135552 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8787927627563477, loss=1.6575068235397339
I0307 16:04:08.086980 140113501742848 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.2891154289245605, loss=1.6308318376541138
I0307 16:04:46.717125 140113510135552 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.8157014846801758, loss=1.5060720443725586
I0307 16:05:25.317151 140113501742848 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8712918758392334, loss=1.5364866256713867
I0307 16:06:04.146667 140113510135552 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.8404021263122559, loss=1.5500209331512451
I0307 16:06:43.016515 140113501742848 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.7451720237731934, loss=1.6006686687469482
I0307 16:07:22.075026 140113510135552 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.830895185470581, loss=1.586099624633789
I0307 16:08:00.781500 140113501742848 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.8692346811294556, loss=1.575662612915039
I0307 16:08:39.472014 140113510135552 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.9481607675552368, loss=1.6477291584014893
I0307 16:09:18.319404 140113501742848 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.887081265449524, loss=1.5881743431091309
I0307 16:09:57.669506 140113510135552 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8351526260375977, loss=1.583466649055481
I0307 16:10:36.207412 140113501742848 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.9472888708114624, loss=1.491920828819275
I0307 16:11:06.802867 140269360194752 spec.py:321] Evaluating on the training split.
I0307 16:11:19.373688 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 16:11:47.053638 140269360194752 spec.py:349] Evaluating on the test split.
I0307 16:11:48.874558 140269360194752 submission_runner.py:469] Time since start: 23382.73s, 	Step: 55280, 	{'train/accuracy': 0.7094627022743225, 'train/loss': 1.1254081726074219, 'validation/accuracy': 0.6401399970054626, 'validation/loss': 1.4917874336242676, 'validation/num_examples': 50000, 'test/accuracy': 0.517300009727478, 'test/loss': 2.218022584915161, 'test/num_examples': 10000, 'score': 21474.393956899643, 'total_duration': 23382.731903076172, 'accumulated_submission_time': 21474.393956899643, 'accumulated_eval_time': 1897.1430563926697, 'accumulated_logging_time': 5.57506251335144}
I0307 16:11:48.948521 140113510135552 logging_writer.py:48] [55280] accumulated_eval_time=1897.14, accumulated_logging_time=5.57506, accumulated_submission_time=21474.4, global_step=55280, preemption_count=0, score=21474.4, test/accuracy=0.5173, test/loss=2.21802, test/num_examples=10000, total_duration=23382.7, train/accuracy=0.709463, train/loss=1.12541, validation/accuracy=0.64014, validation/loss=1.49179, validation/num_examples=50000
I0307 16:11:57.294462 140113501742848 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.8895282745361328, loss=1.5909310579299927
I0307 16:12:35.876456 140113510135552 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8182207345962524, loss=1.6410815715789795
I0307 16:13:14.739443 140113501742848 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.8673781156539917, loss=1.645680546760559
I0307 16:13:58.054415 140113510135552 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.9498834609985352, loss=1.6185078620910645
I0307 16:14:58.454236 140113501742848 logging_writer.py:48] [55700] global_step=55700, grad_norm=2.1220757961273193, loss=1.6780260801315308
I0307 16:15:37.768342 140113510135552 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.8722763061523438, loss=1.5978598594665527
I0307 16:16:16.522722 140113501742848 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.8120492696762085, loss=1.5629831552505493
I0307 16:16:54.865673 140113510135552 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.8399375677108765, loss=1.5962941646575928
I0307 16:17:33.880573 140113501742848 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.6958696842193604, loss=1.5069183111190796
I0307 16:18:12.654659 140113510135552 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.825425386428833, loss=1.4933019876480103
I0307 16:18:52.079152 140113501742848 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.9831995964050293, loss=1.5842835903167725
I0307 16:19:31.226132 140113510135552 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.7284544706344604, loss=1.5853880643844604
I0307 16:20:09.973693 140113501742848 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.7819757461547852, loss=1.4948029518127441
I0307 16:20:19.213280 140269360194752 spec.py:321] Evaluating on the training split.
I0307 16:20:31.818956 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 16:20:57.565192 140269360194752 spec.py:349] Evaluating on the test split.
I0307 16:20:59.301504 140269360194752 submission_runner.py:469] Time since start: 23933.16s, 	Step: 56525, 	{'train/accuracy': 0.7144650816917419, 'train/loss': 1.1070650815963745, 'validation/accuracy': 0.6505599617958069, 'validation/loss': 1.439046859741211, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.1772494316101074, 'test/num_examples': 10000, 'score': 21984.491243124008, 'total_duration': 23933.158789396286, 'accumulated_submission_time': 21984.491243124008, 'accumulated_eval_time': 1937.2310678958893, 'accumulated_logging_time': 5.685719013214111}
I0307 16:20:59.376412 140113510135552 logging_writer.py:48] [56525] accumulated_eval_time=1937.23, accumulated_logging_time=5.68572, accumulated_submission_time=21984.5, global_step=56525, preemption_count=0, score=21984.5, test/accuracy=0.517, test/loss=2.17725, test/num_examples=10000, total_duration=23933.2, train/accuracy=0.714465, train/loss=1.10707, validation/accuracy=0.65056, validation/loss=1.43905, validation/num_examples=50000
I0307 16:21:28.869172 140113501742848 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.0683083534240723, loss=1.580536961555481
I0307 16:22:07.141894 140113510135552 logging_writer.py:48] [56700] global_step=56700, grad_norm=2.0254502296447754, loss=1.6320724487304688
I0307 16:22:46.304869 140113501742848 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.696553111076355, loss=1.5467581748962402
I0307 16:23:25.229369 140113510135552 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.9877207279205322, loss=1.6381148099899292
I0307 16:24:03.876809 140113501742848 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.8607120513916016, loss=1.6004949808120728
I0307 16:24:42.631603 140113510135552 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.7559705972671509, loss=1.5372532606124878
I0307 16:25:21.542577 140113501742848 logging_writer.py:48] [57200] global_step=57200, grad_norm=2.162437677383423, loss=1.7175006866455078
I0307 16:26:00.598255 140113510135552 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.9636050462722778, loss=1.6081774234771729
I0307 16:26:39.909173 140113501742848 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9099171161651611, loss=1.609649419784546
I0307 16:27:19.608418 140113510135552 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.885042428970337, loss=1.6654947996139526
I0307 16:27:58.698659 140113501742848 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.991568922996521, loss=1.591984748840332
I0307 16:28:37.437684 140113510135552 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.8343052864074707, loss=1.5294245481491089
I0307 16:29:15.890238 140113501742848 logging_writer.py:48] [57800] global_step=57800, grad_norm=2.0210249423980713, loss=1.5640857219696045
I0307 16:29:29.543327 140269360194752 spec.py:321] Evaluating on the training split.
I0307 16:29:41.879470 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 16:30:06.360840 140269360194752 spec.py:349] Evaluating on the test split.
I0307 16:30:08.171548 140269360194752 submission_runner.py:469] Time since start: 24482.03s, 	Step: 57836, 	{'train/accuracy': 0.717793345451355, 'train/loss': 1.070703387260437, 'validation/accuracy': 0.652459979057312, 'validation/loss': 1.4229189157485962, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1448540687561035, 'test/num_examples': 10000, 'score': 22494.473898887634, 'total_duration': 24482.028872966766, 'accumulated_submission_time': 22494.473898887634, 'accumulated_eval_time': 1975.8591182231903, 'accumulated_logging_time': 5.807201862335205}
I0307 16:30:08.246943 140113510135552 logging_writer.py:48] [57836] accumulated_eval_time=1975.86, accumulated_logging_time=5.8072, accumulated_submission_time=22494.5, global_step=57836, preemption_count=0, score=22494.5, test/accuracy=0.5225, test/loss=2.14485, test/num_examples=10000, total_duration=24482, train/accuracy=0.717793, train/loss=1.0707, validation/accuracy=0.65246, validation/loss=1.42292, validation/num_examples=50000
I0307 16:30:33.382442 140113501742848 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8484671115875244, loss=1.6214783191680908
I0307 16:31:12.010618 140113510135552 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.078883647918701, loss=1.5932334661483765
I0307 16:31:50.428080 140113501742848 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.8840522766113281, loss=1.4686853885650635
I0307 16:32:29.584967 140113510135552 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.852072834968567, loss=1.5880322456359863
I0307 16:33:08.604394 140113501742848 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8385930061340332, loss=1.5600435733795166
I0307 16:33:48.172356 140113510135552 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9924428462982178, loss=1.6463011503219604
I0307 16:34:27.372556 140113501742848 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.9079331159591675, loss=1.6232863664627075
I0307 16:35:06.836123 140113510135552 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.00056529045105, loss=1.542228102684021
I0307 16:35:46.906865 140113501742848 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.6900830268859863, loss=1.4823623895645142
I0307 16:36:26.851188 140113510135552 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.927490234375, loss=1.4984703063964844
I0307 16:37:06.221745 140113501742848 logging_writer.py:48] [58900] global_step=58900, grad_norm=2.0503976345062256, loss=1.5422197580337524
I0307 16:37:45.595906 140113510135552 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9006022214889526, loss=1.5219625234603882
I0307 16:38:24.985904 140113501742848 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.048001289367676, loss=1.598524808883667
I0307 16:38:38.225337 140269360194752 spec.py:321] Evaluating on the training split.
I0307 16:38:50.498076 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 16:39:16.393988 140269360194752 spec.py:349] Evaluating on the test split.
I0307 16:39:18.210736 140269360194752 submission_runner.py:469] Time since start: 25032.07s, 	Step: 59135, 	{'train/accuracy': 0.7215202450752258, 'train/loss': 1.073554515838623, 'validation/accuracy': 0.6504600048065186, 'validation/loss': 1.4403984546661377, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.1634931564331055, 'test/num_examples': 10000, 'score': 23004.26758122444, 'total_duration': 25032.068063259125, 'accumulated_submission_time': 23004.26758122444, 'accumulated_eval_time': 2015.8443415164948, 'accumulated_logging_time': 5.917670249938965}
I0307 16:39:18.325981 140113510135552 logging_writer.py:48] [59135] accumulated_eval_time=2015.84, accumulated_logging_time=5.91767, accumulated_submission_time=23004.3, global_step=59135, preemption_count=0, score=23004.3, test/accuracy=0.5201, test/loss=2.16349, test/num_examples=10000, total_duration=25032.1, train/accuracy=0.72152, train/loss=1.07355, validation/accuracy=0.65046, validation/loss=1.4404, validation/num_examples=50000
I0307 16:39:43.968177 140113501742848 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.0915427207946777, loss=1.6564409732818604
I0307 16:40:23.670649 140113510135552 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.760046124458313, loss=1.536290168762207
I0307 16:41:03.266516 140113501742848 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.95266854763031, loss=1.6733297109603882
I0307 16:41:42.550107 140113510135552 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.092853546142578, loss=1.5198581218719482
I0307 16:42:21.475863 140113501742848 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.996587872505188, loss=1.641713261604309
I0307 16:43:00.456431 140113510135552 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9107270240783691, loss=1.569556474685669
I0307 16:43:39.186185 140113501742848 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.9992841482162476, loss=1.5956337451934814
I0307 16:44:19.376414 140113510135552 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.0314931869506836, loss=1.5644705295562744
I0307 16:45:00.136331 140113501742848 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.9188783168792725, loss=1.6085706949234009
2025-03-07 16:45:11.434151: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:45:40.323719 140113510135552 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.850881576538086, loss=1.5170321464538574
I0307 16:46:19.830102 140113501742848 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.1475131511688232, loss=1.692925214767456
I0307 16:46:59.480260 140113510135552 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.074363946914673, loss=1.7030750513076782
I0307 16:47:38.155177 140113501742848 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9045672416687012, loss=1.4662156105041504
I0307 16:47:48.609548 140269360194752 spec.py:321] Evaluating on the training split.
I0307 16:48:00.995165 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 16:48:23.332596 140269360194752 spec.py:349] Evaluating on the test split.
I0307 16:48:25.080469 140269360194752 submission_runner.py:469] Time since start: 25578.94s, 	Step: 60428, 	{'train/accuracy': 0.7120535373687744, 'train/loss': 1.1217913627624512, 'validation/accuracy': 0.643779993057251, 'validation/loss': 1.468058705329895, 'validation/num_examples': 50000, 'test/accuracy': 0.5224000215530396, 'test/loss': 2.160226821899414, 'test/num_examples': 10000, 'score': 23514.374061346054, 'total_duration': 25578.937777996063, 'accumulated_submission_time': 23514.374061346054, 'accumulated_eval_time': 2052.315071105957, 'accumulated_logging_time': 6.05895209312439}
I0307 16:48:25.198381 140113510135552 logging_writer.py:48] [60428] accumulated_eval_time=2052.32, accumulated_logging_time=6.05895, accumulated_submission_time=23514.4, global_step=60428, preemption_count=0, score=23514.4, test/accuracy=0.5224, test/loss=2.16023, test/num_examples=10000, total_duration=25578.9, train/accuracy=0.712054, train/loss=1.12179, validation/accuracy=0.64378, validation/loss=1.46806, validation/num_examples=50000
I0307 16:48:53.975072 140113501742848 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.7803703546524048, loss=1.6191574335098267
I0307 16:49:33.808053 140113510135552 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.9484729766845703, loss=1.5534650087356567
I0307 16:50:13.405286 140113501742848 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.910452127456665, loss=1.4853382110595703
I0307 16:50:52.831975 140113510135552 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.9834173917770386, loss=1.4953272342681885
I0307 16:51:32.594727 140113501742848 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.8860398530960083, loss=1.5153206586837769
I0307 16:52:11.390178 140113510135552 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.1006627082824707, loss=1.68491530418396
I0307 16:52:50.756698 140113501742848 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.892606496810913, loss=1.6263829469680786
I0307 16:53:31.234429 140113510135552 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.0868310928344727, loss=1.6747931241989136
I0307 16:54:11.848570 140113501742848 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.8841779232025146, loss=1.5990468263626099
I0307 16:54:51.035191 140113510135552 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.1021599769592285, loss=1.772209882736206
I0307 16:55:30.756819 140113501742848 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8000975847244263, loss=1.5995604991912842
I0307 16:56:10.211588 140113510135552 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.1715569496154785, loss=1.5755995512008667
I0307 16:56:50.048834 140113501742848 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.309809446334839, loss=1.6299564838409424
I0307 16:56:55.123222 140269360194752 spec.py:321] Evaluating on the training split.
I0307 16:57:07.573781 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 16:57:31.765975 140269360194752 spec.py:349] Evaluating on the test split.
I0307 16:57:33.564471 140269360194752 submission_runner.py:469] Time since start: 26127.42s, 	Step: 61714, 	{'train/accuracy': 0.7234534025192261, 'train/loss': 1.0633902549743652, 'validation/accuracy': 0.6550799608230591, 'validation/loss': 1.4097949266433716, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.108109474182129, 'test/num_examples': 10000, 'score': 24024.109657764435, 'total_duration': 26127.42178440094, 'accumulated_submission_time': 24024.109657764435, 'accumulated_eval_time': 2090.7561309337616, 'accumulated_logging_time': 6.214584827423096}
I0307 16:57:33.644324 140113510135552 logging_writer.py:48] [61714] accumulated_eval_time=2090.76, accumulated_logging_time=6.21458, accumulated_submission_time=24024.1, global_step=61714, preemption_count=0, score=24024.1, test/accuracy=0.5345, test/loss=2.10811, test/num_examples=10000, total_duration=26127.4, train/accuracy=0.723453, train/loss=1.06339, validation/accuracy=0.65508, validation/loss=1.40979, validation/num_examples=50000
I0307 16:58:08.421500 140113501742848 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.859740138053894, loss=1.5632948875427246
I0307 16:58:48.477669 140113510135552 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.1121745109558105, loss=1.610562801361084
I0307 16:59:27.568505 140113501742848 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.7884502410888672, loss=1.6776396036148071
I0307 17:00:07.408006 140113510135552 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.9166516065597534, loss=1.583113193511963
I0307 17:00:47.067819 140113501742848 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.9248770475387573, loss=1.6700717210769653
I0307 17:01:26.823766 140113510135552 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.967564582824707, loss=1.5530221462249756
I0307 17:02:06.688652 140113501742848 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.9584475755691528, loss=1.572403073310852
I0307 17:02:46.716381 140113510135552 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.0297462940216064, loss=1.4843425750732422
I0307 17:03:26.150003 140113501742848 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.8639682531356812, loss=1.6792826652526855
I0307 17:04:05.185615 140113510135552 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.030308961868286, loss=1.5737369060516357
I0307 17:04:44.697663 140113501742848 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.8337689638137817, loss=1.5886253118515015
I0307 17:05:24.750782 140113510135552 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.8920193910598755, loss=1.485413908958435
I0307 17:06:03.865291 140269360194752 spec.py:321] Evaluating on the training split.
I0307 17:06:16.295724 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 17:06:40.252279 140269360194752 spec.py:349] Evaluating on the test split.
I0307 17:06:42.054881 140269360194752 submission_runner.py:469] Time since start: 26675.91s, 	Step: 62999, 	{'train/accuracy': 0.7166174650192261, 'train/loss': 1.0886846780776978, 'validation/accuracy': 0.6485599875450134, 'validation/loss': 1.4411242008209229, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.166180372238159, 'test/num_examples': 10000, 'score': 24534.139717817307, 'total_duration': 26675.91220641136, 'accumulated_submission_time': 24534.139717817307, 'accumulated_eval_time': 2128.9455440044403, 'accumulated_logging_time': 6.331876516342163}
I0307 17:06:42.163812 140113501742848 logging_writer.py:48] [62999] accumulated_eval_time=2128.95, accumulated_logging_time=6.33188, accumulated_submission_time=24534.1, global_step=62999, preemption_count=0, score=24534.1, test/accuracy=0.5232, test/loss=2.16618, test/num_examples=10000, total_duration=26675.9, train/accuracy=0.716617, train/loss=1.08868, validation/accuracy=0.64856, validation/loss=1.44112, validation/num_examples=50000
I0307 17:06:42.970388 140113510135552 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.9915778636932373, loss=1.4915961027145386
I0307 17:07:22.580501 140113501742848 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.0647130012512207, loss=1.654842495918274
I0307 17:08:02.834664 140113510135552 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.0806193351745605, loss=1.5382176637649536
I0307 17:08:43.066934 140113501742848 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.820043921470642, loss=1.543727159500122
I0307 17:09:22.197049 140113510135552 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.0818536281585693, loss=1.5958679914474487
I0307 17:10:01.056283 140113501742848 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.8247019052505493, loss=1.4973866939544678
I0307 17:10:40.937742 140113510135552 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8945826292037964, loss=1.496717929840088
I0307 17:11:21.004743 140113501742848 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.068603515625, loss=1.5933153629302979
I0307 17:12:01.218042 140113510135552 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.8900107145309448, loss=1.494457721710205
I0307 17:12:40.359431 140113501742848 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.366710901260376, loss=1.6002123355865479
I0307 17:13:19.084050 140113510135552 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.9224519729614258, loss=1.6381477117538452
I0307 17:13:58.410997 140113501742848 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.903691291809082, loss=1.4818986654281616
I0307 17:14:37.082546 140113510135552 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.012610673904419, loss=1.5773866176605225
I0307 17:15:12.483904 140269360194752 spec.py:321] Evaluating on the training split.
I0307 17:15:25.114083 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 17:15:48.015959 140269360194752 spec.py:349] Evaluating on the test split.
I0307 17:15:49.913487 140269360194752 submission_runner.py:469] Time since start: 27223.77s, 	Step: 64290, 	{'train/accuracy': 0.7254264950752258, 'train/loss': 1.0430907011032104, 'validation/accuracy': 0.6535199880599976, 'validation/loss': 1.4232927560806274, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.1406497955322266, 'test/num_examples': 10000, 'score': 25044.27539539337, 'total_duration': 27223.770844459534, 'accumulated_submission_time': 25044.27539539337, 'accumulated_eval_time': 2166.374985933304, 'accumulated_logging_time': 6.471105575561523}
I0307 17:15:49.986344 140113501742848 logging_writer.py:48] [64290] accumulated_eval_time=2166.37, accumulated_logging_time=6.47111, accumulated_submission_time=25044.3, global_step=64290, preemption_count=0, score=25044.3, test/accuracy=0.5268, test/loss=2.14065, test/num_examples=10000, total_duration=27223.8, train/accuracy=0.725426, train/loss=1.04309, validation/accuracy=0.65352, validation/loss=1.42329, validation/num_examples=50000
I0307 17:15:54.267279 140113510135552 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9559632539749146, loss=1.4822841882705688
I0307 17:16:33.791727 140113501742848 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.881082534790039, loss=1.5357086658477783
I0307 17:17:14.595643 140113510135552 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.8611881732940674, loss=1.6194050312042236
I0307 17:17:55.012350 140113501742848 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8163421154022217, loss=1.437225341796875
I0307 17:18:35.122873 140113510135552 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.893047571182251, loss=1.5925465822219849
I0307 17:19:15.174827 140113501742848 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.9140022993087769, loss=1.4875335693359375
I0307 17:19:54.756351 140113510135552 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.8471660614013672, loss=1.419974684715271
I0307 17:20:34.832934 140113501742848 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.000670909881592, loss=1.5997028350830078
I0307 17:21:14.051393 140113510135552 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.772571325302124, loss=1.4952976703643799
I0307 17:21:53.455259 140113501742848 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.9910624027252197, loss=1.5969229936599731
I0307 17:22:32.802203 140113510135552 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.04563570022583, loss=1.6651792526245117
I0307 17:23:11.968353 140113501742848 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.9819170236587524, loss=1.527331829071045
I0307 17:23:51.093126 140113510135552 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.8099254369735718, loss=1.5163058042526245
I0307 17:24:19.979445 140269360194752 spec.py:321] Evaluating on the training split.
I0307 17:24:32.806021 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 17:24:52.056007 140269360194752 spec.py:349] Evaluating on the test split.
I0307 17:24:53.898943 140269360194752 submission_runner.py:469] Time since start: 27767.76s, 	Step: 65574, 	{'train/accuracy': 0.7141461968421936, 'train/loss': 1.1031270027160645, 'validation/accuracy': 0.6446599960327148, 'validation/loss': 1.4697213172912598, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.213465452194214, 'test/num_examples': 10000, 'score': 25554.087022304535, 'total_duration': 27767.756018161774, 'accumulated_submission_time': 25554.087022304535, 'accumulated_eval_time': 2200.294060945511, 'accumulated_logging_time': 6.5701744556427}
I0307 17:24:54.009416 140113501742848 logging_writer.py:48] [65574] accumulated_eval_time=2200.29, accumulated_logging_time=6.57017, accumulated_submission_time=25554.1, global_step=65574, preemption_count=0, score=25554.1, test/accuracy=0.5164, test/loss=2.21347, test/num_examples=10000, total_duration=27767.8, train/accuracy=0.714146, train/loss=1.10313, validation/accuracy=0.64466, validation/loss=1.46972, validation/num_examples=50000
I0307 17:25:04.738332 140113510135552 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.802993893623352, loss=1.474763035774231
I0307 17:25:44.209972 140113501742848 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.9687045812606812, loss=1.5341633558273315
I0307 17:26:24.194560 140113510135552 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.201782464981079, loss=1.6426407098770142
I0307 17:27:04.565744 140113501742848 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.9792919158935547, loss=1.550718069076538
I0307 17:27:44.461261 140113510135552 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.0661814212799072, loss=1.521941900253296
I0307 17:28:23.822412 140113501742848 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9059919118881226, loss=1.446789026260376
I0307 17:29:03.426372 140113510135552 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.9821909666061401, loss=1.4946259260177612
2025-03-07 17:29:36.827341: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:29:43.351008 140113501742848 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.0925750732421875, loss=1.615114688873291
I0307 17:31:01.622097 140113510135552 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.945298671722412, loss=1.5399935245513916
I0307 17:31:41.670614 140113501742848 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.081991195678711, loss=1.5494661331176758
I0307 17:32:21.025253 140113510135552 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.1396236419677734, loss=1.6054346561431885
I0307 17:33:00.854278 140113501742848 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.898391604423523, loss=1.5289427042007446
I0307 17:33:24.268470 140269360194752 spec.py:321] Evaluating on the training split.
I0307 17:33:36.655689 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 17:34:04.283513 140269360194752 spec.py:349] Evaluating on the test split.
I0307 17:34:06.091856 140269360194752 submission_runner.py:469] Time since start: 28319.95s, 	Step: 66761, 	{'train/accuracy': 0.7280372977256775, 'train/loss': 1.04458487033844, 'validation/accuracy': 0.652899980545044, 'validation/loss': 1.4218896627426147, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.1242177486419678, 'test/num_examples': 10000, 'score': 26064.145768642426, 'total_duration': 28319.94919037819, 'accumulated_submission_time': 26064.145768642426, 'accumulated_eval_time': 2242.11728143692, 'accumulated_logging_time': 6.736119270324707}
I0307 17:34:06.263145 140113510135552 logging_writer.py:48] [66761] accumulated_eval_time=2242.12, accumulated_logging_time=6.73612, accumulated_submission_time=26064.1, global_step=66761, preemption_count=0, score=26064.1, test/accuracy=0.5312, test/loss=2.12422, test/num_examples=10000, total_duration=28319.9, train/accuracy=0.728037, train/loss=1.04458, validation/accuracy=0.6529, validation/loss=1.42189, validation/num_examples=50000
I0307 17:34:22.367831 140113501742848 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.9314717054367065, loss=1.64998197555542
I0307 17:35:01.910334 140113510135552 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.9999809265136719, loss=1.5639671087265015
I0307 17:35:42.123074 140113501742848 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.9558436870574951, loss=1.5570950508117676
I0307 17:36:21.778246 140113510135552 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.9542779922485352, loss=1.6507304906845093
I0307 17:37:01.338732 140113501742848 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.1670987606048584, loss=1.505378007888794
I0307 17:37:40.807313 140113510135552 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9086675643920898, loss=1.4855583906173706
I0307 17:38:20.793049 140113501742848 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.9520180225372314, loss=1.5652414560317993
I0307 17:39:01.318068 140113510135552 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.1005077362060547, loss=1.6423648595809937
I0307 17:39:40.954502 140113501742848 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.9343252182006836, loss=1.3524243831634521
I0307 17:40:20.895362 140113510135552 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.192039966583252, loss=1.516229510307312
I0307 17:41:00.723088 140113501742848 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.9417786598205566, loss=1.605222463607788
I0307 17:41:40.347696 140113510135552 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.9941380023956299, loss=1.5874178409576416
I0307 17:42:19.591008 140113501742848 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.099916934967041, loss=1.5012133121490479
I0307 17:42:36.432303 140269360194752 spec.py:321] Evaluating on the training split.
I0307 17:42:48.868535 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 17:43:11.394257 140269360194752 spec.py:349] Evaluating on the test split.
I0307 17:43:13.139477 140269360194752 submission_runner.py:469] Time since start: 28867.00s, 	Step: 68044, 	{'train/accuracy': 0.7293526530265808, 'train/loss': 1.0390655994415283, 'validation/accuracy': 0.6547799706459045, 'validation/loss': 1.4202443361282349, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.1513993740081787, 'test/num_examples': 10000, 'score': 26574.129656791687, 'total_duration': 28866.996738910675, 'accumulated_submission_time': 26574.129656791687, 'accumulated_eval_time': 2278.824225664139, 'accumulated_logging_time': 6.938032627105713}
I0307 17:43:13.297689 140113510135552 logging_writer.py:48] [68044] accumulated_eval_time=2278.82, accumulated_logging_time=6.93803, accumulated_submission_time=26574.1, global_step=68044, preemption_count=0, score=26574.1, test/accuracy=0.5258, test/loss=2.1514, test/num_examples=10000, total_duration=28867, train/accuracy=0.729353, train/loss=1.03907, validation/accuracy=0.65478, validation/loss=1.42024, validation/num_examples=50000
I0307 17:43:35.695436 140113501742848 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9712257385253906, loss=1.5563957691192627
I0307 17:44:15.441130 140113510135552 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.8665460348129272, loss=1.5201607942581177
I0307 17:45:03.926662 140113501742848 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.2557640075683594, loss=1.6329433917999268
I0307 17:45:44.551583 140113510135552 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.2840116024017334, loss=1.6127041578292847
I0307 17:46:24.545145 140113501742848 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.959887981414795, loss=1.6233083009719849
I0307 17:47:04.798340 140113510135552 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.784593105316162, loss=1.3984664678573608
I0307 17:47:45.041421 140113501742848 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.9229621887207031, loss=1.5048019886016846
I0307 17:48:25.732931 140113510135552 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.2371721267700195, loss=1.5915309190750122
I0307 17:49:05.674850 140113501742848 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.193070888519287, loss=1.5974494218826294
I0307 17:49:45.391338 140113510135552 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.8988869190216064, loss=1.5117720365524292
I0307 17:50:25.367400 140113501742848 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.063190221786499, loss=1.5254255533218384
I0307 17:51:05.504322 140113510135552 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.998039722442627, loss=1.548248052597046
I0307 17:51:43.329061 140269360194752 spec.py:321] Evaluating on the training split.
I0307 17:51:55.763797 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 17:52:18.055672 140269360194752 spec.py:349] Evaluating on the test split.
I0307 17:52:19.793169 140269360194752 submission_runner.py:469] Time since start: 29413.65s, 	Step: 69296, 	{'train/accuracy': 0.7253667116165161, 'train/loss': 1.0567028522491455, 'validation/accuracy': 0.6482399702072144, 'validation/loss': 1.449615240097046, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.192610502243042, 'test/num_examples': 10000, 'score': 27083.97568178177, 'total_duration': 29413.650495052338, 'accumulated_submission_time': 27083.97568178177, 'accumulated_eval_time': 2315.288168668747, 'accumulated_logging_time': 7.128679275512695}
I0307 17:52:19.901689 140113501742848 logging_writer.py:48] [69296] accumulated_eval_time=2315.29, accumulated_logging_time=7.12868, accumulated_submission_time=27084, global_step=69296, preemption_count=0, score=27084, test/accuracy=0.521, test/loss=2.19261, test/num_examples=10000, total_duration=29413.7, train/accuracy=0.725367, train/loss=1.0567, validation/accuracy=0.64824, validation/loss=1.44962, validation/num_examples=50000
I0307 17:52:21.878630 140113510135552 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.916010856628418, loss=1.4854321479797363
I0307 17:53:01.625247 140113501742848 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.143065929412842, loss=1.6223094463348389
I0307 17:53:41.466689 140113510135552 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.052751064300537, loss=1.5211517810821533
I0307 17:54:21.387114 140113501742848 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.841008186340332, loss=1.4936972856521606
I0307 17:55:01.790780 140113510135552 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.502843141555786, loss=1.6102572679519653
I0307 17:55:41.311890 140113501742848 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.170128107070923, loss=1.6234805583953857
I0307 17:56:21.640012 140113510135552 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.073421001434326, loss=1.6089669466018677
I0307 17:57:02.139377 140113501742848 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.8440349102020264, loss=1.456169605255127
I0307 17:57:42.118697 140113510135552 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0814225673675537, loss=1.524942398071289
I0307 17:58:21.589737 140113501742848 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.3057758808135986, loss=1.5434595346450806
I0307 17:59:01.568040 140113510135552 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.1953654289245605, loss=1.4482191801071167
I0307 17:59:41.998142 140113501742848 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.9315557479858398, loss=1.5453295707702637
I0307 18:00:21.822897 140113510135552 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.0102603435516357, loss=1.5881493091583252
I0307 18:00:50.096097 140269360194752 spec.py:321] Evaluating on the training split.
I0307 18:01:02.707671 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 18:01:23.714340 140269360194752 spec.py:349] Evaluating on the test split.
I0307 18:01:25.449876 140269360194752 submission_runner.py:469] Time since start: 29959.31s, 	Step: 70572, 	{'train/accuracy': 0.7393574714660645, 'train/loss': 1.0069780349731445, 'validation/accuracy': 0.6589999794960022, 'validation/loss': 1.3940340280532837, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.0888493061065674, 'test/num_examples': 10000, 'score': 27593.986520051956, 'total_duration': 29959.30716943741, 'accumulated_submission_time': 27593.986520051956, 'accumulated_eval_time': 2350.641744375229, 'accumulated_logging_time': 7.267857789993286}
I0307 18:01:25.587671 140113501742848 logging_writer.py:48] [70572] accumulated_eval_time=2350.64, accumulated_logging_time=7.26786, accumulated_submission_time=27594, global_step=70572, preemption_count=0, score=27594, test/accuracy=0.5402, test/loss=2.08885, test/num_examples=10000, total_duration=29959.3, train/accuracy=0.739357, train/loss=1.00698, validation/accuracy=0.659, validation/loss=1.39403, validation/num_examples=50000
I0307 18:01:37.373146 140113510135552 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0659804344177246, loss=1.5107368230819702
I0307 18:02:17.325260 140113501742848 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.170811176300049, loss=1.5600723028182983
I0307 18:02:58.009635 140113510135552 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.9670466184616089, loss=1.5134022235870361
I0307 18:03:38.842838 140113501742848 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.028418779373169, loss=1.5420019626617432
I0307 18:04:19.037634 140113510135552 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.873473048210144, loss=1.5786828994750977
I0307 18:04:58.654243 140113501742848 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.7414058446884155, loss=1.439957857131958
I0307 18:05:39.279551 140113510135552 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.0754733085632324, loss=1.4908373355865479
I0307 18:06:18.901643 140113501742848 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.934521198272705, loss=1.5340385437011719
I0307 18:06:58.487981 140113510135552 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.035813570022583, loss=1.5933547019958496
I0307 18:07:38.652842 140113501742848 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.887901782989502, loss=1.5177339315414429
I0307 18:08:18.566346 140113510135552 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.9563239812850952, loss=1.6387706995010376
I0307 18:08:58.774824 140113501742848 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.1429712772369385, loss=1.523229956626892
I0307 18:09:38.554336 140113510135552 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.8450331687927246, loss=1.5378822088241577
I0307 18:09:55.797148 140269360194752 spec.py:321] Evaluating on the training split.
I0307 18:10:08.337340 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 18:10:25.935610 140269360194752 spec.py:349] Evaluating on the test split.
I0307 18:10:27.725545 140269360194752 submission_runner.py:469] Time since start: 30501.58s, 	Step: 71844, 	{'train/accuracy': 0.7378826141357422, 'train/loss': 1.0114717483520508, 'validation/accuracy': 0.6562199592590332, 'validation/loss': 1.416731834411621, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.1171469688415527, 'test/num_examples': 10000, 'score': 28104.007316589355, 'total_duration': 30501.58286857605, 'accumulated_submission_time': 28104.007316589355, 'accumulated_eval_time': 2382.5699660778046, 'accumulated_logging_time': 7.438830137252808}
I0307 18:10:27.794575 140113501742848 logging_writer.py:48] [71844] accumulated_eval_time=2382.57, accumulated_logging_time=7.43883, accumulated_submission_time=28104, global_step=71844, preemption_count=0, score=28104, test/accuracy=0.5372, test/loss=2.11715, test/num_examples=10000, total_duration=30501.6, train/accuracy=0.737883, train/loss=1.01147, validation/accuracy=0.65622, validation/loss=1.41673, validation/num_examples=50000
I0307 18:10:50.775394 140113510135552 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.8930312395095825, loss=1.4833385944366455
I0307 18:11:30.465411 140113501742848 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.9582329988479614, loss=1.4385287761688232
I0307 18:12:10.062733 140113510135552 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.0079259872436523, loss=1.5344855785369873
I0307 18:12:50.247244 140113501742848 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.182803153991699, loss=1.5593113899230957
I0307 18:13:29.538433 140113510135552 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.957188606262207, loss=1.4561089277267456
I0307 18:14:09.355533 140113501742848 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.9790610074996948, loss=1.5157601833343506
I0307 18:14:55.104423 140113510135552 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1122562885284424, loss=1.4330620765686035
I0307 18:15:35.899657 140113501742848 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.4001026153564453, loss=1.5190761089324951
I0307 18:16:15.246351 140113510135552 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.201423406600952, loss=1.5884525775909424
I0307 18:16:55.241135 140113501742848 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.856170892715454, loss=1.5115538835525513
I0307 18:17:35.376977 140113510135552 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.032179832458496, loss=1.3944202661514282
I0307 18:18:15.387017 140113501742848 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.398334264755249, loss=1.588679313659668
I0307 18:18:55.302059 140113510135552 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.0710222721099854, loss=1.5149716138839722
I0307 18:18:58.117275 140269360194752 spec.py:321] Evaluating on the training split.
I0307 18:19:10.539303 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 18:19:31.108447 140269360194752 spec.py:349] Evaluating on the test split.
I0307 18:19:32.849626 140269360194752 submission_runner.py:469] Time since start: 31046.71s, 	Step: 73108, 	{'train/accuracy': 0.7170957922935486, 'train/loss': 1.0847793817520142, 'validation/accuracy': 0.6380400061607361, 'validation/loss': 1.5005325078964233, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.293741464614868, 'test/num_examples': 10000, 'score': 28614.14536833763, 'total_duration': 31046.706951379776, 'accumulated_submission_time': 28614.14536833763, 'accumulated_eval_time': 2417.302140235901, 'accumulated_logging_time': 7.5397725105285645}
I0307 18:19:32.979569 140113501742848 logging_writer.py:48] [73108] accumulated_eval_time=2417.3, accumulated_logging_time=7.53977, accumulated_submission_time=28614.1, global_step=73108, preemption_count=0, score=28614.1, test/accuracy=0.5085, test/loss=2.29374, test/num_examples=10000, total_duration=31046.7, train/accuracy=0.717096, train/loss=1.08478, validation/accuracy=0.63804, validation/loss=1.50053, validation/num_examples=50000
I0307 18:20:09.820737 140113510135552 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.194042921066284, loss=1.6004098653793335
I0307 18:20:50.360854 140113501742848 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.9350284337997437, loss=1.5512876510620117
I0307 18:21:30.414179 140113510135552 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.1098341941833496, loss=1.5596741437911987
I0307 18:22:10.667965 140113501742848 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.0761337280273438, loss=1.5638704299926758
I0307 18:22:50.713098 140113510135552 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.884893536567688, loss=1.3609436750411987
I0307 18:23:32.102934 140113501742848 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.9454962015151978, loss=1.4882878065109253
I0307 18:24:13.481975 140113510135552 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.230600118637085, loss=1.5189090967178345
I0307 18:24:53.932637 140113501742848 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.059793710708618, loss=1.4526774883270264
I0307 18:25:34.420793 140113510135552 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.195467948913574, loss=1.4825727939605713
I0307 18:26:14.802130 140113501742848 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.014878511428833, loss=1.5208508968353271
I0307 18:26:55.071396 140113510135552 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.045591354370117, loss=1.4994614124298096
I0307 18:27:35.203733 140113501742848 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.088336229324341, loss=1.4402483701705933
I0307 18:28:03.029075 140269360194752 spec.py:321] Evaluating on the training split.
I0307 18:28:15.541458 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 18:28:33.211507 140269360194752 spec.py:349] Evaluating on the test split.
I0307 18:28:34.955200 140269360194752 submission_runner.py:469] Time since start: 31588.81s, 	Step: 74371, 	{'train/accuracy': 0.7361088991165161, 'train/loss': 1.0086668729782104, 'validation/accuracy': 0.6509199738502502, 'validation/loss': 1.4272668361663818, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.1644039154052734, 'test/num_examples': 10000, 'score': 29124.007921934128, 'total_duration': 31588.812475442886, 'accumulated_submission_time': 29124.007921934128, 'accumulated_eval_time': 2449.228042125702, 'accumulated_logging_time': 7.704905986785889}
I0307 18:28:35.062927 140113510135552 logging_writer.py:48] [74371] accumulated_eval_time=2449.23, accumulated_logging_time=7.70491, accumulated_submission_time=29124, global_step=74371, preemption_count=0, score=29124, test/accuracy=0.5258, test/loss=2.1644, test/num_examples=10000, total_duration=31588.8, train/accuracy=0.736109, train/loss=1.00867, validation/accuracy=0.65092, validation/loss=1.42727, validation/num_examples=50000
I0307 18:28:47.536442 140113501742848 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.1638989448547363, loss=1.482084035873413
I0307 18:29:26.909072 140113510135552 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.0215582847595215, loss=1.5512893199920654
I0307 18:30:06.842333 140113501742848 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.0619728565216064, loss=1.545478343963623
I0307 18:30:47.327587 140113510135552 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.1036717891693115, loss=1.550973653793335
I0307 18:31:27.525568 140113501742848 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.9253926277160645, loss=1.4717847108840942
I0307 18:32:10.157479 140113510135552 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.9264947175979614, loss=1.465551733970642
I0307 18:32:57.056650 140113501742848 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.373950719833374, loss=1.5156927108764648
I0307 18:33:38.714523 140113510135552 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.9747239351272583, loss=1.5355777740478516
I0307 18:34:18.582350 140113501742848 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.030104875564575, loss=1.5807782411575317
I0307 18:34:58.571844 140113510135552 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.1975595951080322, loss=1.425187110900879
I0307 18:35:38.250816 140113501742848 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.980210304260254, loss=1.488801121711731
I0307 18:36:17.853645 140113510135552 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.1275370121002197, loss=1.4481959342956543
I0307 18:36:57.983250 140113501742848 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.0290679931640625, loss=1.4184378385543823
I0307 18:37:05.324281 140269360194752 spec.py:321] Evaluating on the training split.
I0307 18:37:17.873965 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 18:37:41.750251 140269360194752 spec.py:349] Evaluating on the test split.
I0307 18:37:43.498140 140269360194752 submission_runner.py:469] Time since start: 32137.36s, 	Step: 75619, 	{'train/accuracy': 0.7685546875, 'train/loss': 0.8727958798408508, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.329728603363037, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.065916061401367, 'test/num_examples': 10000, 'score': 29633.580072164536, 'total_duration': 32137.35546541214, 'accumulated_submission_time': 29633.580072164536, 'accumulated_eval_time': 2487.4017350673676, 'accumulated_logging_time': 8.353960514068604}
I0307 18:37:43.583081 140113510135552 logging_writer.py:48] [75619] accumulated_eval_time=2487.4, accumulated_logging_time=8.35396, accumulated_submission_time=29633.6, global_step=75619, preemption_count=0, score=29633.6, test/accuracy=0.5484, test/loss=2.06592, test/num_examples=10000, total_duration=32137.4, train/accuracy=0.768555, train/loss=0.872796, validation/accuracy=0.67656, validation/loss=1.32973, validation/num_examples=50000
I0307 18:38:16.251779 140113501742848 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.178799867630005, loss=1.5563141107559204
I0307 18:38:56.332709 140113510135552 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.9793915748596191, loss=1.500973105430603
I0307 18:39:36.620685 140113501742848 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.2060251235961914, loss=1.499769687652588
I0307 18:40:16.666506 140113510135552 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.0612688064575195, loss=1.5165231227874756
I0307 18:40:56.390830 140113501742848 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.0079686641693115, loss=1.405563473701477
I0307 18:41:37.140120 140113510135552 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.008674383163452, loss=1.3898663520812988
I0307 18:42:17.868171 140113501742848 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.0240163803100586, loss=1.3889579772949219
I0307 18:42:58.223791 140113510135552 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.1654446125030518, loss=1.5041083097457886
I0307 18:43:37.913779 140113501742848 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.14778995513916, loss=1.5025547742843628
I0307 18:44:17.732975 140113510135552 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.2628631591796875, loss=1.4734777212142944
I0307 18:44:57.741041 140113501742848 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.9118732213974, loss=1.355504035949707
I0307 18:45:37.807761 140113510135552 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.2672579288482666, loss=1.4647798538208008
I0307 18:46:13.846455 140269360194752 spec.py:321] Evaluating on the training split.
I0307 18:46:26.929028 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 18:46:45.506609 140269360194752 spec.py:349] Evaluating on the test split.
I0307 18:46:47.265567 140269360194752 submission_runner.py:469] Time since start: 32681.12s, 	Step: 76891, 	{'train/accuracy': 0.7476482391357422, 'train/loss': 0.9540586471557617, 'validation/accuracy': 0.6596399545669556, 'validation/loss': 1.3977417945861816, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.172443389892578, 'test/num_examples': 10000, 'score': 30143.65061402321, 'total_duration': 32681.122876167297, 'accumulated_submission_time': 30143.65061402321, 'accumulated_eval_time': 2520.8206589221954, 'accumulated_logging_time': 8.47841191291809}
I0307 18:46:47.322948 140113501742848 logging_writer.py:48] [76891] accumulated_eval_time=2520.82, accumulated_logging_time=8.47841, accumulated_submission_time=30143.7, global_step=76891, preemption_count=0, score=30143.7, test/accuracy=0.5253, test/loss=2.17244, test/num_examples=10000, total_duration=32681.1, train/accuracy=0.747648, train/loss=0.954059, validation/accuracy=0.65964, validation/loss=1.39774, validation/num_examples=50000
I0307 18:46:51.278734 140113510135552 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.043409824371338, loss=1.40470290184021
I0307 18:47:31.036844 140113501742848 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.9623385667800903, loss=1.4588038921356201
I0307 18:48:11.117111 140113510135552 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.2044384479522705, loss=1.5258772373199463
I0307 18:48:56.841939 140113501742848 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.0225327014923096, loss=1.465720772743225
I0307 18:49:36.592169 140113510135552 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.1009910106658936, loss=1.5280511379241943
I0307 18:50:17.401210 140113501742848 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.1814522743225098, loss=1.5792779922485352
I0307 18:50:57.743603 140113510135552 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.176856756210327, loss=1.5515351295471191
I0307 18:51:38.652261 140113501742848 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.0818707942962646, loss=1.4683012962341309
I0307 18:52:19.192848 140113510135552 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.9522390365600586, loss=1.5131354331970215
I0307 18:52:59.104949 140113501742848 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.9425357580184937, loss=1.505185604095459
I0307 18:53:38.756192 140113510135552 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.990368366241455, loss=1.5378005504608154
I0307 18:54:19.029201 140113501742848 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.8920938968658447, loss=1.3144701719284058
I0307 18:54:59.280826 140113510135552 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.8523075580596924, loss=1.310964822769165
I0307 18:55:17.466437 140269360194752 spec.py:321] Evaluating on the training split.
I0307 18:55:30.223189 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 18:55:51.986268 140269360194752 spec.py:349] Evaluating on the test split.
I0307 18:55:53.714073 140269360194752 submission_runner.py:469] Time since start: 33227.57s, 	Step: 78146, 	{'train/accuracy': 0.7475486397743225, 'train/loss': 0.959528923034668, 'validation/accuracy': 0.6498799920082092, 'validation/loss': 1.437365174293518, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.175877332687378, 'test/num_examples': 10000, 'score': 30653.61021065712, 'total_duration': 33227.571417331696, 'accumulated_submission_time': 30653.61021065712, 'accumulated_eval_time': 2557.068140029907, 'accumulated_logging_time': 8.571596622467041}
I0307 18:55:53.832424 140113501742848 logging_writer.py:48] [78146] accumulated_eval_time=2557.07, accumulated_logging_time=8.5716, accumulated_submission_time=30653.6, global_step=78146, preemption_count=0, score=30653.6, test/accuracy=0.5215, test/loss=2.17588, test/num_examples=10000, total_duration=33227.6, train/accuracy=0.747549, train/loss=0.959529, validation/accuracy=0.64988, validation/loss=1.43737, validation/num_examples=50000
I0307 18:56:15.664351 140113510135552 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.902958631515503, loss=1.5114989280700684
I0307 18:56:55.805057 140113501742848 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.0045204162597656, loss=1.4867494106292725
I0307 18:57:36.451595 140113510135552 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.9695954322814941, loss=1.4006261825561523
I0307 18:58:16.455507 140113501742848 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.4057931900024414, loss=1.5594148635864258
I0307 18:58:56.759097 140113510135552 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.059049367904663, loss=1.3575246334075928
I0307 18:59:36.523955 140113501742848 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.9993011951446533, loss=1.463294267654419
I0307 19:00:18.031224 140113510135552 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.314967632293701, loss=1.5583288669586182
I0307 19:00:58.424039 140113501742848 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.133840799331665, loss=1.5189261436462402
I0307 19:01:37.963776 140113510135552 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.2907261848449707, loss=1.5383110046386719
I0307 19:02:17.973426 140113501742848 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.259159803390503, loss=1.5698809623718262
I0307 19:02:58.487075 140113510135552 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.476252317428589, loss=1.4994781017303467
I0307 19:03:38.444413 140113501742848 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.0205488204956055, loss=1.4015759229660034
I0307 19:04:18.454322 140113510135552 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.9447461366653442, loss=1.4999022483825684
I0307 19:04:23.748190 140269360194752 spec.py:321] Evaluating on the training split.
I0307 19:04:36.185965 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 19:04:55.348709 140269360194752 spec.py:349] Evaluating on the test split.
I0307 19:04:57.067900 140269360194752 submission_runner.py:469] Time since start: 33770.93s, 	Step: 79414, 	{'train/accuracy': 0.767996609210968, 'train/loss': 0.8771113157272339, 'validation/accuracy': 0.659339964389801, 'validation/loss': 1.4158713817596436, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.182405948638916, 'test/num_examples': 10000, 'score': 31163.34557890892, 'total_duration': 33770.92522621155, 'accumulated_submission_time': 31163.34557890892, 'accumulated_eval_time': 2590.387677192688, 'accumulated_logging_time': 8.721405744552612}
I0307 19:04:57.184186 140113501742848 logging_writer.py:48] [79414] accumulated_eval_time=2590.39, accumulated_logging_time=8.72141, accumulated_submission_time=31163.3, global_step=79414, preemption_count=0, score=31163.3, test/accuracy=0.5247, test/loss=2.18241, test/num_examples=10000, total_duration=33770.9, train/accuracy=0.767997, train/loss=0.877111, validation/accuracy=0.65934, validation/loss=1.41587, validation/num_examples=50000
I0307 19:05:32.198965 140113510135552 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.1097207069396973, loss=1.501476526260376
I0307 19:06:12.827345 140113501742848 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.0600008964538574, loss=1.5386466979980469
I0307 19:06:56.445589 140113510135552 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.0264360904693604, loss=1.4155954122543335
I0307 19:07:36.598399 140113501742848 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.316279888153076, loss=1.5407023429870605
I0307 19:08:16.655752 140113510135552 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.1164636611938477, loss=1.46854567527771
I0307 19:08:56.694648 140113501742848 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.0730931758880615, loss=1.542234182357788
I0307 19:09:38.682012 140113510135552 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.13564133644104, loss=1.5368030071258545
I0307 19:10:18.640710 140113501742848 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.93504798412323, loss=1.4756314754486084
I0307 19:10:58.736292 140113510135552 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.179178237915039, loss=1.5720326900482178
I0307 19:11:38.596387 140113501742848 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.396390676498413, loss=1.4821553230285645
I0307 19:12:18.470154 140113510135552 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.1745731830596924, loss=1.446000337600708
I0307 19:13:00.183331 140113501742848 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.093407154083252, loss=1.4871904850006104
I0307 19:13:27.298808 140269360194752 spec.py:321] Evaluating on the training split.
I0307 19:13:41.114506 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 19:14:01.923572 140269360194752 spec.py:349] Evaluating on the test split.
I0307 19:14:03.670432 140269360194752 submission_runner.py:469] Time since start: 34317.53s, 	Step: 80660, 	{'train/accuracy': 0.7736766338348389, 'train/loss': 0.8552953004837036, 'validation/accuracy': 0.6590999960899353, 'validation/loss': 1.4029873609542847, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.1389479637145996, 'test/num_examples': 10000, 'score': 31673.28662133217, 'total_duration': 34317.52774596214, 'accumulated_submission_time': 31673.28662133217, 'accumulated_eval_time': 2626.759115457535, 'accumulated_logging_time': 8.864092111587524}
I0307 19:14:03.751221 140113510135552 logging_writer.py:48] [80660] accumulated_eval_time=2626.76, accumulated_logging_time=8.86409, accumulated_submission_time=31673.3, global_step=80660, preemption_count=0, score=31673.3, test/accuracy=0.5347, test/loss=2.13895, test/num_examples=10000, total_duration=34317.5, train/accuracy=0.773677, train/loss=0.855295, validation/accuracy=0.6591, validation/loss=1.40299, validation/num_examples=50000
I0307 19:14:20.510152 140113501742848 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.2070634365081787, loss=1.5383373498916626
I0307 19:15:00.412439 140113510135552 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.134993076324463, loss=1.4047226905822754
I0307 19:15:40.353178 140113501742848 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.1122255325317383, loss=1.440140724182129
I0307 19:16:20.481468 140113510135552 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.119932174682617, loss=1.504873514175415
I0307 19:17:00.969563 140113501742848 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.961159586906433, loss=1.4679088592529297
I0307 19:17:44.034533 140113510135552 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.3505449295043945, loss=1.5299547910690308
I0307 19:18:28.577769 140113501742848 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.441432476043701, loss=1.3984522819519043
I0307 19:19:09.714306 140113510135552 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.0127546787261963, loss=1.370179295539856
I0307 19:19:49.428914 140113501742848 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.171398162841797, loss=1.5591516494750977
I0307 19:20:28.701145 140113510135552 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.310088634490967, loss=1.4151833057403564
I0307 19:21:13.058243 140113501742848 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.055772066116333, loss=1.3939521312713623
I0307 19:21:58.064295 140113510135552 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.2932283878326416, loss=1.5438201427459717
I0307 19:22:33.916284 140269360194752 spec.py:321] Evaluating on the training split.
I0307 19:22:47.058236 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 19:23:08.623685 140269360194752 spec.py:349] Evaluating on the test split.
I0307 19:23:10.369587 140269360194752 submission_runner.py:469] Time since start: 34864.23s, 	Step: 81888, 	{'train/accuracy': 0.7417091727256775, 'train/loss': 0.9829739928245544, 'validation/accuracy': 0.6683599948883057, 'validation/loss': 1.3575916290283203, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.074643850326538, 'test/num_examples': 10000, 'score': 32183.258754968643, 'total_duration': 34864.22689676285, 'accumulated_submission_time': 32183.258754968643, 'accumulated_eval_time': 2663.212234735489, 'accumulated_logging_time': 8.993061304092407}
I0307 19:23:10.441832 140113501742848 logging_writer.py:48] [81888] accumulated_eval_time=2663.21, accumulated_logging_time=8.99306, accumulated_submission_time=32183.3, global_step=81888, preemption_count=0, score=32183.3, test/accuracy=0.5443, test/loss=2.07464, test/num_examples=10000, total_duration=34864.2, train/accuracy=0.741709, train/loss=0.982974, validation/accuracy=0.66836, validation/loss=1.35759, validation/num_examples=50000
I0307 19:23:15.774206 140113510135552 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.4244983196258545, loss=1.4774664640426636
I0307 19:23:55.456357 140113501742848 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.0834991931915283, loss=1.6003926992416382
I0307 19:24:35.110268 140113510135552 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.0341036319732666, loss=1.4765543937683105
I0307 19:25:22.328174 140113501742848 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.0659806728363037, loss=1.4100812673568726
I0307 19:26:04.522687 140113510135552 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.102017641067505, loss=1.438781499862671
I0307 19:26:46.692819 140113501742848 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.207324743270874, loss=1.5401352643966675
I0307 19:27:29.178524 140113510135552 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.1587753295898438, loss=1.512977123260498
I0307 19:28:12.234422 140113501742848 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.02252459526062, loss=1.462528944015503
I0307 19:28:51.723351 140113510135552 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.9946261644363403, loss=1.4854896068572998
I0307 19:29:30.717064 140113501742848 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.3796277046203613, loss=1.535159707069397
I0307 19:30:09.995594 140113510135552 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.090374231338501, loss=1.4543122053146362
I0307 19:31:03.081631 140113501742848 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.1951091289520264, loss=1.4757153987884521
I0307 19:31:42.949569 140269360194752 spec.py:321] Evaluating on the training split.
I0307 19:31:56.722159 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 19:32:15.531733 140269360194752 spec.py:349] Evaluating on the test split.
I0307 19:32:17.263779 140269360194752 submission_runner.py:469] Time since start: 35411.12s, 	Step: 83025, 	{'train/accuracy': 0.7388990521430969, 'train/loss': 0.988299548625946, 'validation/accuracy': 0.6671599745750427, 'validation/loss': 1.3549338579177856, 'validation/num_examples': 50000, 'test/accuracy': 0.5501000285148621, 'test/loss': 2.047982931137085, 'test/num_examples': 10000, 'score': 32695.61033463478, 'total_duration': 35411.12110495567, 'accumulated_submission_time': 32695.61033463478, 'accumulated_eval_time': 2697.52627825737, 'accumulated_logging_time': 9.089037656784058}
I0307 19:32:17.427790 140113510135552 logging_writer.py:48] [83025] accumulated_eval_time=2697.53, accumulated_logging_time=9.08904, accumulated_submission_time=32695.6, global_step=83025, preemption_count=0, score=32695.6, test/accuracy=0.5501, test/loss=2.04798, test/num_examples=10000, total_duration=35411.1, train/accuracy=0.738899, train/loss=0.9883, validation/accuracy=0.66716, validation/loss=1.35493, validation/num_examples=50000
I0307 19:32:48.064577 140113501742848 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.9608001708984375, loss=1.4035112857818604
I0307 19:33:31.461561 140113510135552 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.192044496536255, loss=1.500594973564148
I0307 19:34:15.018510 140113501742848 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.499316930770874, loss=1.5643178224563599
I0307 19:35:00.277381 140113510135552 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1743216514587402, loss=1.5167490243911743
I0307 19:35:44.018874 140113501742848 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.0530855655670166, loss=1.4349991083145142
I0307 19:36:23.948400 140113510135552 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.1322450637817383, loss=1.4952324628829956
I0307 19:37:08.135702 140113501742848 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.255974054336548, loss=1.4155641794204712
I0307 19:37:52.665255 140113510135552 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.2530016899108887, loss=1.4437377452850342
I0307 19:38:35.034463 140113501742848 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.1377713680267334, loss=1.4659268856048584
I0307 19:39:17.258812 140113510135552 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.0823168754577637, loss=1.4837948083877563
I0307 19:39:56.695134 140113501742848 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.257268190383911, loss=1.4124488830566406
I0307 19:40:36.440754 140113510135552 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.2909762859344482, loss=1.4621479511260986
I0307 19:40:47.581333 140269360194752 spec.py:321] Evaluating on the training split.
I0307 19:41:00.751939 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 19:41:22.985415 140269360194752 spec.py:349] Evaluating on the test split.
I0307 19:41:24.738242 140269360194752 submission_runner.py:469] Time since start: 35958.60s, 	Step: 84227, 	{'train/accuracy': 0.7450773119926453, 'train/loss': 0.9722169041633606, 'validation/accuracy': 0.6714199781417847, 'validation/loss': 1.335518717765808, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.0356361865997314, 'test/num_examples': 10000, 'score': 33205.59354376793, 'total_duration': 35958.5955247879, 'accumulated_submission_time': 33205.59354376793, 'accumulated_eval_time': 2734.6829755306244, 'accumulated_logging_time': 9.282243013381958}
I0307 19:41:24.815683 140113501742848 logging_writer.py:48] [84227] accumulated_eval_time=2734.68, accumulated_logging_time=9.28224, accumulated_submission_time=33205.6, global_step=84227, preemption_count=0, score=33205.6, test/accuracy=0.5491, test/loss=2.03564, test/num_examples=10000, total_duration=35958.6, train/accuracy=0.745077, train/loss=0.972217, validation/accuracy=0.67142, validation/loss=1.33552, validation/num_examples=50000
I0307 19:42:14.503261 140113510135552 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.2702436447143555, loss=1.496233582496643
I0307 19:43:16.300751 140113501742848 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.283334493637085, loss=1.5040732622146606
I0307 19:43:59.456690 140113510135552 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.1343495845794678, loss=1.4083938598632812
I0307 19:44:44.439762 140113501742848 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.084517002105713, loss=1.4186241626739502
I0307 19:45:28.074697 140113510135552 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.126708507537842, loss=1.4285191297531128
I0307 19:46:10.082551 140113501742848 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.270406723022461, loss=1.5096650123596191
I0307 19:47:08.007839 140113510135552 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.2126755714416504, loss=1.441864013671875
I0307 19:48:03.077531 140113501742848 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.2543301582336426, loss=1.5321815013885498
I0307 19:48:46.578465 140113510135552 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.3745779991149902, loss=1.5151139497756958
I0307 19:49:26.539791 140113501742848 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.1551408767700195, loss=1.3944767713546753
I0307 19:49:55.173766 140269360194752 spec.py:321] Evaluating on the training split.
I0307 19:50:08.480401 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 19:50:26.573909 140269360194752 spec.py:349] Evaluating on the test split.
I0307 19:50:28.351169 140269360194752 submission_runner.py:469] Time since start: 36502.21s, 	Step: 85269, 	{'train/accuracy': 0.7497209906578064, 'train/loss': 0.9389574527740479, 'validation/accuracy': 0.6784600019454956, 'validation/loss': 1.3218321800231934, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.0571517944335938, 'test/num_examples': 10000, 'score': 33715.77509355545, 'total_duration': 36502.20847797394, 'accumulated_submission_time': 33715.77509355545, 'accumulated_eval_time': 2767.8602056503296, 'accumulated_logging_time': 9.414495944976807}
I0307 19:50:28.459424 140113510135552 logging_writer.py:48] [85269] accumulated_eval_time=2767.86, accumulated_logging_time=9.4145, accumulated_submission_time=33715.8, global_step=85269, preemption_count=0, score=33715.8, test/accuracy=0.5475, test/loss=2.05715, test/num_examples=10000, total_duration=36502.2, train/accuracy=0.749721, train/loss=0.938957, validation/accuracy=0.67846, validation/loss=1.32183, validation/num_examples=50000
I0307 19:50:41.254006 140113501742848 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.2000515460968018, loss=1.4873319864273071
I0307 19:51:21.631008 140113510135552 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.111081600189209, loss=1.4410431385040283
I0307 19:52:01.462981 140113501742848 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.1710596084594727, loss=1.479225754737854
I0307 19:52:46.257345 140113510135552 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.211467742919922, loss=1.4364069700241089
I0307 19:53:29.102067 140113501742848 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.1437370777130127, loss=1.4438550472259521
I0307 19:54:12.719559 140113510135552 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.484201192855835, loss=1.5720359086990356
I0307 19:54:59.597629 140113501742848 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.1443636417388916, loss=1.4959561824798584
I0307 19:55:43.243721 140113510135552 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.4342286586761475, loss=1.556230068206787
I0307 19:56:23.108699 140113501742848 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.091282367706299, loss=1.4364582300186157
I0307 19:57:08.119591 140113510135552 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.1667349338531494, loss=1.457592487335205
I0307 19:57:52.677127 140113501742848 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.2560770511627197, loss=1.47371244430542
I0307 19:58:33.284839 140113510135552 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.1935698986053467, loss=1.4183945655822754
I0307 19:58:58.799263 140269360194752 spec.py:321] Evaluating on the training split.
I0307 19:59:12.184079 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 19:59:33.421210 140269360194752 spec.py:349] Evaluating on the test split.
I0307 19:59:35.170820 140269360194752 submission_runner.py:469] Time since start: 37049.03s, 	Step: 86461, 	{'train/accuracy': 0.7471300959587097, 'train/loss': 0.9460687041282654, 'validation/accuracy': 0.6688399910926819, 'validation/loss': 1.3449113368988037, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.1063642501831055, 'test/num_examples': 10000, 'score': 34225.94098520279, 'total_duration': 37049.028126716614, 'accumulated_submission_time': 34225.94098520279, 'accumulated_eval_time': 2804.2315702438354, 'accumulated_logging_time': 9.556679487228394}
I0307 19:59:35.286157 140113501742848 logging_writer.py:48] [86461] accumulated_eval_time=2804.23, accumulated_logging_time=9.55668, accumulated_submission_time=34225.9, global_step=86461, preemption_count=0, score=34225.9, test/accuracy=0.5364, test/loss=2.10636, test/num_examples=10000, total_duration=37049, train/accuracy=0.74713, train/loss=0.946069, validation/accuracy=0.66884, validation/loss=1.34491, validation/num_examples=50000
I0307 19:59:51.339839 140113510135552 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.278235912322998, loss=1.6186810731887817
I0307 20:00:36.744841 140113501742848 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.0914089679718018, loss=1.4383838176727295
I0307 20:01:23.895399 140113510135552 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.9366852045059204, loss=1.3115662336349487
I0307 20:02:09.994472 140113501742848 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.2676637172698975, loss=1.498124599456787
I0307 20:02:56.110035 140113510135552 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.195885419845581, loss=1.43966805934906
I0307 20:03:41.834303 140113501742848 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.1203479766845703, loss=1.3704426288604736
I0307 20:04:27.855597 140113510135552 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.11207914352417, loss=1.4434435367584229
I0307 20:05:11.084645 140113501742848 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.2137954235076904, loss=1.539117455482483
I0307 20:05:51.443216 140113510135552 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.2425923347473145, loss=1.3810410499572754
I0307 20:06:37.365176 140113501742848 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.203167676925659, loss=1.4685078859329224
I0307 20:07:23.866884 140113510135552 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.142298698425293, loss=1.4137706756591797
I0307 20:08:05.536141 140269360194752 spec.py:321] Evaluating on the training split.
I0307 20:08:19.220559 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 20:08:41.395401 140269360194752 spec.py:349] Evaluating on the test split.
I0307 20:08:43.130008 140269360194752 submission_runner.py:469] Time since start: 37596.99s, 	Step: 87571, 	{'train/accuracy': 0.7589684128761292, 'train/loss': 0.9046853184700012, 'validation/accuracy': 0.6720199584960938, 'validation/loss': 1.325779914855957, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.0510897636413574, 'test/num_examples': 10000, 'score': 34736.0363342762, 'total_duration': 37596.98733854294, 'accumulated_submission_time': 34736.0363342762, 'accumulated_eval_time': 2841.825268507004, 'accumulated_logging_time': 9.698861360549927}
I0307 20:08:43.229627 140113501742848 logging_writer.py:48] [87571] accumulated_eval_time=2841.83, accumulated_logging_time=9.69886, accumulated_submission_time=34736, global_step=87571, preemption_count=0, score=34736, test/accuracy=0.5407, test/loss=2.05109, test/num_examples=10000, total_duration=37597, train/accuracy=0.758968, train/loss=0.904685, validation/accuracy=0.67202, validation/loss=1.32578, validation/num_examples=50000
I0307 20:08:55.437093 140113510135552 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.038747549057007, loss=1.4999849796295166
I0307 20:09:35.449236 140113501742848 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.284472703933716, loss=1.4105572700500488
I0307 20:10:16.117815 140113510135552 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.377676010131836, loss=1.4297490119934082
I0307 20:10:59.257102 140113501742848 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.217346668243408, loss=1.4056724309921265
I0307 20:11:45.819996 140113510135552 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.1024670600891113, loss=1.4455369710922241
I0307 20:12:34.300295 140113501742848 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.1803393363952637, loss=1.4298176765441895
I0307 20:13:28.152489 140113510135552 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.300494909286499, loss=1.4157607555389404
I0307 20:14:13.157796 140113501742848 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.0080151557922363, loss=1.3884541988372803
I0307 20:14:57.110049 140113510135552 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.2936086654663086, loss=1.5011829137802124
I0307 20:15:46.827626 140113501742848 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.1856119632720947, loss=1.4079607725143433
I0307 20:16:46.859145 140113510135552 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.060929775238037, loss=1.3755111694335938
I0307 20:17:13.138563 140269360194752 spec.py:321] Evaluating on the training split.
I0307 20:17:26.495501 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 20:17:47.979476 140269360194752 spec.py:349] Evaluating on the test split.
I0307 20:17:49.725271 140269360194752 submission_runner.py:469] Time since start: 38143.58s, 	Step: 88647, 	{'train/accuracy': 0.7711256146430969, 'train/loss': 0.8675334453582764, 'validation/accuracy': 0.6724199652671814, 'validation/loss': 1.342900276184082, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.102644920349121, 'test/num_examples': 10000, 'score': 35245.79747009277, 'total_duration': 38143.58253598213, 'accumulated_submission_time': 35245.79747009277, 'accumulated_eval_time': 2878.411741733551, 'accumulated_logging_time': 9.825964450836182}
I0307 20:17:49.842266 140113501742848 logging_writer.py:48] [88647] accumulated_eval_time=2878.41, accumulated_logging_time=9.82596, accumulated_submission_time=35245.8, global_step=88647, preemption_count=0, score=35245.8, test/accuracy=0.5436, test/loss=2.10264, test/num_examples=10000, total_duration=38143.6, train/accuracy=0.771126, train/loss=0.867533, validation/accuracy=0.67242, validation/loss=1.3429, validation/num_examples=50000
I0307 20:18:11.777742 140113510135552 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.374821901321411, loss=1.5002195835113525
I0307 20:18:54.106842 140113501742848 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.2799487113952637, loss=1.3738653659820557
I0307 20:19:35.724799 140113510135552 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.597262144088745, loss=1.488455057144165
I0307 20:20:15.130861 140113501742848 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.510460138320923, loss=1.5698519945144653
I0307 20:20:55.391625 140113510135552 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.1903951168060303, loss=1.3764337301254272
I0307 20:21:35.481203 140113501742848 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.084670305252075, loss=1.4741652011871338
I0307 20:22:28.316131 140113510135552 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.2359960079193115, loss=1.5135488510131836
I0307 20:23:18.529984 140113501742848 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.170814037322998, loss=1.37236487865448
I0307 20:24:08.810110 140113510135552 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.078328847885132, loss=1.4169642925262451
I0307 20:25:02.376389 140113501742848 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.319758176803589, loss=1.4479482173919678
I0307 20:25:51.756237 140113510135552 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.1465401649475098, loss=1.5681331157684326
I0307 20:26:19.969651 140269360194752 spec.py:321] Evaluating on the training split.
I0307 20:26:33.569407 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 20:26:54.353210 140269360194752 spec.py:349] Evaluating on the test split.
I0307 20:26:56.076570 140269360194752 submission_runner.py:469] Time since start: 38689.93s, 	Step: 89772, 	{'train/accuracy': 0.7669004797935486, 'train/loss': 0.885750412940979, 'validation/accuracy': 0.6752200126647949, 'validation/loss': 1.312709927558899, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.0189590454101562, 'test/num_examples': 10000, 'score': 35755.64154100418, 'total_duration': 38689.93389034271, 'accumulated_submission_time': 35755.64154100418, 'accumulated_eval_time': 2914.518481016159, 'accumulated_logging_time': 10.097168207168579}
I0307 20:26:56.159861 140113501742848 logging_writer.py:48] [89772] accumulated_eval_time=2914.52, accumulated_logging_time=10.0972, accumulated_submission_time=35755.6, global_step=89772, preemption_count=0, score=35755.6, test/accuracy=0.5506, test/loss=2.01896, test/num_examples=10000, total_duration=38689.9, train/accuracy=0.7669, train/loss=0.88575, validation/accuracy=0.67522, validation/loss=1.31271, validation/num_examples=50000
I0307 20:27:07.994900 140113510135552 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.2603719234466553, loss=1.5478869676589966
I0307 20:27:57.225833 140113501742848 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.0643694400787354, loss=1.452517032623291
I0307 20:28:50.959306 140113510135552 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.344254493713379, loss=1.3805606365203857
I0307 20:29:37.689359 140113501742848 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.32749080657959, loss=1.4060144424438477
I0307 20:30:18.177166 140113510135552 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.140380620956421, loss=1.3782565593719482
I0307 20:31:00.075734 140113501742848 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.2554268836975098, loss=1.4958879947662354
I0307 20:31:43.548229 140113510135552 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.2977654933929443, loss=1.492047905921936
I0307 20:32:24.778584 140113501742848 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.315800666809082, loss=1.3971400260925293
I0307 20:33:16.204940 140113510135552 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.1105434894561768, loss=1.3448837995529175
I0307 20:34:08.699206 140113501742848 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.1936352252960205, loss=1.4697203636169434
I0307 20:34:55.222300 140113510135552 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.1085803508758545, loss=1.4170029163360596
I0307 20:35:26.465421 140269360194752 spec.py:321] Evaluating on the training split.
I0307 20:35:40.060487 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 20:36:00.258830 140269360194752 spec.py:349] Evaluating on the test split.
I0307 20:36:02.031494 140269360194752 submission_runner.py:469] Time since start: 39235.89s, 	Step: 90868, 	{'train/accuracy': 0.75, 'train/loss': 0.9514040350914001, 'validation/accuracy': 0.6718599796295166, 'validation/loss': 1.3213268518447876, 'validation/num_examples': 50000, 'test/accuracy': 0.5414000153541565, 'test/loss': 2.0719118118286133, 'test/num_examples': 10000, 'score': 36265.79408097267, 'total_duration': 39235.888822078705, 'accumulated_submission_time': 36265.79408097267, 'accumulated_eval_time': 2950.0843873023987, 'accumulated_logging_time': 10.208319902420044}
I0307 20:36:02.129696 140113501742848 logging_writer.py:48] [90868] accumulated_eval_time=2950.08, accumulated_logging_time=10.2083, accumulated_submission_time=36265.8, global_step=90868, preemption_count=0, score=36265.8, test/accuracy=0.5414, test/loss=2.07191, test/num_examples=10000, total_duration=39235.9, train/accuracy=0.75, train/loss=0.951404, validation/accuracy=0.67186, validation/loss=1.32133, validation/num_examples=50000
I0307 20:36:16.142366 140113510135552 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.115013599395752, loss=1.4327667951583862
I0307 20:37:05.914934 140113501742848 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.259005069732666, loss=1.4059958457946777
I0307 20:37:46.073266 140113510135552 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.2839016914367676, loss=1.3836511373519897
I0307 20:38:39.353446 140113501742848 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.498997926712036, loss=1.3456764221191406
I0307 20:39:31.913025 140113510135552 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.2417166233062744, loss=1.3866006135940552
I0307 20:41:00.735409 140113501742848 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.3177497386932373, loss=1.4518641233444214
I0307 20:41:45.624878 140113510135552 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.2058708667755127, loss=1.4250824451446533
I0307 20:43:12.904787 140113501742848 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.534367084503174, loss=1.4597697257995605
I0307 20:44:32.484488 140269360194752 spec.py:321] Evaluating on the training split.
I0307 20:44:45.587907 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 20:45:05.921338 140269360194752 spec.py:349] Evaluating on the test split.
I0307 20:45:07.919998 140269360194752 submission_runner.py:469] Time since start: 39781.78s, 	Step: 91693, 	{'train/accuracy': 0.7562180757522583, 'train/loss': 0.9143868088722229, 'validation/accuracy': 0.6722999811172485, 'validation/loss': 1.3276240825653076, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.054367780685425, 'test/num_examples': 10000, 'score': 36775.99167513847, 'total_duration': 39781.77725291252, 'accumulated_submission_time': 36775.99167513847, 'accumulated_eval_time': 2985.5196537971497, 'accumulated_logging_time': 10.369922876358032}
I0307 20:45:08.034856 140113510135552 logging_writer.py:48] [91693] accumulated_eval_time=2985.52, accumulated_logging_time=10.3699, accumulated_submission_time=36776, global_step=91693, preemption_count=0, score=36776, test/accuracy=0.544, test/loss=2.05437, test/num_examples=10000, total_duration=39781.8, train/accuracy=0.756218, train/loss=0.914387, validation/accuracy=0.6723, validation/loss=1.32762, validation/num_examples=50000
I0307 20:45:11.163601 140113501742848 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.3167014122009277, loss=1.4459547996520996
I0307 20:46:14.889636 140113510135552 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.4235215187072754, loss=1.4426822662353516
I0307 20:47:21.525154 140113501742848 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3643360137939453, loss=1.4102641344070435
I0307 20:48:35.501864 140113510135552 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.173940420150757, loss=1.3605502843856812
I0307 20:49:35.849316 140113501742848 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.1309804916381836, loss=1.3407374620437622
I0307 20:50:33.819397 140113510135552 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.313613176345825, loss=1.4088655710220337
I0307 20:51:24.155762 140113501742848 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.622825860977173, loss=1.4927785396575928
I0307 20:52:17.244752 140113510135552 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.1171576976776123, loss=1.3274421691894531
I0307 20:53:38.744648 140269360194752 spec.py:321] Evaluating on the training split.
I0307 20:53:52.514330 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 20:54:16.441534 140269360194752 spec.py:349] Evaluating on the test split.
I0307 20:54:18.171451 140269360194752 submission_runner.py:469] Time since start: 40332.03s, 	Step: 92483, 	{'train/accuracy': 0.8000836968421936, 'train/loss': 0.7523419260978699, 'validation/accuracy': 0.6754199862480164, 'validation/loss': 1.3226666450500488, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.045224189758301, 'test/num_examples': 10000, 'score': 37286.58005976677, 'total_duration': 40332.02872085571, 'accumulated_submission_time': 37286.58005976677, 'accumulated_eval_time': 3024.94624209404, 'accumulated_logging_time': 10.518640518188477}
I0307 20:54:18.268654 140113501742848 logging_writer.py:48] [92483] accumulated_eval_time=3024.95, accumulated_logging_time=10.5186, accumulated_submission_time=37286.6, global_step=92483, preemption_count=0, score=37286.6, test/accuracy=0.5487, test/loss=2.04522, test/num_examples=10000, total_duration=40332, train/accuracy=0.800084, train/loss=0.752342, validation/accuracy=0.67542, validation/loss=1.32267, validation/num_examples=50000
I0307 20:54:25.487780 140113510135552 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.5751516819000244, loss=1.4557514190673828
I0307 20:55:16.389513 140113501742848 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.410426378250122, loss=1.4481463432312012
I0307 20:56:06.752487 140113510135552 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.2051827907562256, loss=1.370949625968933
I0307 20:57:01.620695 140113501742848 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.354749917984009, loss=1.4263930320739746
I0307 20:57:51.979256 140113510135552 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.479257106781006, loss=1.4774646759033203
I0307 20:59:00.799183 140113501742848 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.103647470474243, loss=1.4407024383544922
I0307 20:59:40.749380 140113510135552 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.2837913036346436, loss=1.5040667057037354
I0307 21:00:24.962261 140113501742848 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.420482635498047, loss=1.465390920639038
I0307 21:01:11.507824 140113510135552 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.1481266021728516, loss=1.4249898195266724
I0307 21:02:05.514400 140113501742848 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.1924026012420654, loss=1.2995829582214355
I0307 21:02:48.732172 140269360194752 spec.py:321] Evaluating on the training split.
I0307 21:03:00.599815 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 21:03:23.588712 140269360194752 spec.py:349] Evaluating on the test split.
I0307 21:03:25.308165 140269360194752 submission_runner.py:469] Time since start: 40879.17s, 	Step: 93451, 	{'train/accuracy': 0.7322424650192261, 'train/loss': 1.0223578214645386, 'validation/accuracy': 0.6579999923706055, 'validation/loss': 1.4051156044006348, 'validation/num_examples': 50000, 'test/accuracy': 0.5319000482559204, 'test/loss': 2.1515700817108154, 'test/num_examples': 10000, 'score': 37796.891820430756, 'total_duration': 40879.16563129425, 'accumulated_submission_time': 37796.891820430756, 'accumulated_eval_time': 3061.5222041606903, 'accumulated_logging_time': 10.657094240188599}
I0307 21:03:25.439934 140113510135552 logging_writer.py:48] [93451] accumulated_eval_time=3061.52, accumulated_logging_time=10.6571, accumulated_submission_time=37796.9, global_step=93451, preemption_count=0, score=37796.9, test/accuracy=0.5319, test/loss=2.15157, test/num_examples=10000, total_duration=40879.2, train/accuracy=0.732242, train/loss=1.02236, validation/accuracy=0.658, validation/loss=1.40512, validation/num_examples=50000
I0307 21:03:45.378581 140113501742848 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.3025009632110596, loss=1.3219047784805298
I0307 21:04:44.995277 140113510135552 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.390077590942383, loss=1.4295545816421509
I0307 21:05:49.688488 140113501742848 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.2367093563079834, loss=1.3685226440429688
I0307 21:06:35.731298 140113510135552 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.16188645362854, loss=1.3961303234100342
I0307 21:07:15.994513 140113501742848 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.3044960498809814, loss=1.3997875452041626
I0307 21:07:54.688878 140113510135552 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.528353691101074, loss=1.4345144033432007
I0307 21:08:34.305286 140113501742848 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.2526581287384033, loss=1.4431763887405396
I0307 21:09:13.376684 140113510135552 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.1795482635498047, loss=1.4148229360580444
I0307 21:09:51.901138 140113501742848 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.2825255393981934, loss=1.441206693649292
I0307 21:10:30.946545 140113510135552 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.1108357906341553, loss=1.32694673538208
I0307 21:11:10.490516 140113501742848 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.3235630989074707, loss=1.4365533590316772
I0307 21:11:49.278228 140113510135552 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.1676859855651855, loss=1.3654193878173828
I0307 21:11:55.573000 140269360194752 spec.py:321] Evaluating on the training split.
I0307 21:12:06.882214 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 21:12:27.102402 140269360194752 spec.py:349] Evaluating on the test split.
I0307 21:12:28.818180 140269360194752 submission_runner.py:469] Time since start: 41422.68s, 	Step: 94617, 	{'train/accuracy': 0.750996470451355, 'train/loss': 0.9445841312408447, 'validation/accuracy': 0.6700599789619446, 'validation/loss': 1.3443413972854614, 'validation/num_examples': 50000, 'test/accuracy': 0.5450000166893005, 'test/loss': 2.0809412002563477, 'test/num_examples': 10000, 'score': 38306.863139629364, 'total_duration': 41422.6756465435, 'accumulated_submission_time': 38306.863139629364, 'accumulated_eval_time': 3094.767350912094, 'accumulated_logging_time': 10.809594631195068}
I0307 21:12:28.907144 140113501742848 logging_writer.py:48] [94617] accumulated_eval_time=3094.77, accumulated_logging_time=10.8096, accumulated_submission_time=38306.9, global_step=94617, preemption_count=0, score=38306.9, test/accuracy=0.545, test/loss=2.08094, test/num_examples=10000, total_duration=41422.7, train/accuracy=0.750996, train/loss=0.944584, validation/accuracy=0.67006, validation/loss=1.34434, validation/num_examples=50000
I0307 21:13:02.069550 140113510135552 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.4951164722442627, loss=1.4872369766235352
I0307 21:13:40.931835 140113501742848 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.3146910667419434, loss=1.4342103004455566
I0307 21:14:20.437359 140113510135552 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.5900309085845947, loss=1.4907281398773193
I0307 21:15:00.165996 140113501742848 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.010878801345825, loss=1.3449159860610962
I0307 21:15:39.986547 140113510135552 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.1046359539031982, loss=1.3814408779144287
I0307 21:16:18.319492 140113501742848 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.399343967437744, loss=1.3850533962249756
I0307 21:16:58.232254 140113510135552 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.3889594078063965, loss=1.362922191619873
I0307 21:17:37.307773 140113501742848 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.3984527587890625, loss=1.435117483139038
I0307 21:18:16.393414 140113510135552 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.396789789199829, loss=1.4460179805755615
I0307 21:18:55.341939 140113501742848 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.4327104091644287, loss=1.4530545473098755
I0307 21:19:34.068737 140113510135552 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.4032444953918457, loss=1.467468023300171
I0307 21:20:12.931611 140113501742848 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.312713623046875, loss=1.3865740299224854
I0307 21:20:51.992486 140113510135552 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.3679749965667725, loss=1.5048234462738037
I0307 21:20:59.151007 140269360194752 spec.py:321] Evaluating on the training split.
I0307 21:21:09.930872 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 21:21:26.333552 140269360194752 spec.py:349] Evaluating on the test split.
I0307 21:21:28.060783 140269360194752 submission_runner.py:469] Time since start: 41961.92s, 	Step: 95919, 	{'train/accuracy': 0.7483457922935486, 'train/loss': 0.9407371282577515, 'validation/accuracy': 0.6692799925804138, 'validation/loss': 1.3309261798858643, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.055803060531616, 'test/num_examples': 10000, 'score': 38816.94192481041, 'total_duration': 41961.91824746132, 'accumulated_submission_time': 38816.94192481041, 'accumulated_eval_time': 3123.677096605301, 'accumulated_logging_time': 10.915040493011475}
I0307 21:21:28.148062 140113501742848 logging_writer.py:48] [95919] accumulated_eval_time=3123.68, accumulated_logging_time=10.915, accumulated_submission_time=38816.9, global_step=95919, preemption_count=0, score=38816.9, test/accuracy=0.541, test/loss=2.0558, test/num_examples=10000, total_duration=41961.9, train/accuracy=0.748346, train/loss=0.940737, validation/accuracy=0.66928, validation/loss=1.33093, validation/num_examples=50000
I0307 21:22:00.092788 140113510135552 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.360382318496704, loss=1.3880759477615356
I0307 21:22:39.636839 140113501742848 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.257668972015381, loss=1.4070420265197754
I0307 21:23:19.323025 140113510135552 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.687338352203369, loss=1.4391093254089355
I0307 21:23:59.734438 140113501742848 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.2201197147369385, loss=1.2814979553222656
I0307 21:24:39.105988 140113510135552 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.431467294692993, loss=1.4670302867889404
I0307 21:25:18.529457 140113501742848 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.387380599975586, loss=1.4313652515411377
I0307 21:25:58.358717 140113510135552 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.293424367904663, loss=1.388875126838684
I0307 21:26:37.992922 140113501742848 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.5946226119995117, loss=1.4228829145431519
I0307 21:27:17.375295 140113510135552 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.270611524581909, loss=1.433042287826538
I0307 21:27:56.800860 140113501742848 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.439061164855957, loss=1.4537702798843384
I0307 21:28:36.110052 140113510135552 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.264796018600464, loss=1.3474299907684326
I0307 21:29:15.938120 140113501742848 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.3516252040863037, loss=1.3747062683105469
I0307 21:29:55.613862 140113510135552 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.4887149333953857, loss=1.4047868251800537
I0307 21:29:58.067820 140269360194752 spec.py:321] Evaluating on the training split.
I0307 21:30:08.931877 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 21:30:31.773624 140269360194752 spec.py:349] Evaluating on the test split.
I0307 21:30:33.494070 140269360194752 submission_runner.py:469] Time since start: 42507.35s, 	Step: 97207, 	{'train/accuracy': 0.7700294852256775, 'train/loss': 0.8650060892105103, 'validation/accuracy': 0.6903199553489685, 'validation/loss': 1.2660845518112183, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9759856462478638, 'test/num_examples': 10000, 'score': 39326.68496346474, 'total_duration': 42507.3515226841, 'accumulated_submission_time': 39326.68496346474, 'accumulated_eval_time': 3159.1033103466034, 'accumulated_logging_time': 11.028908252716064}
I0307 21:30:33.576426 140113501742848 logging_writer.py:48] [97207] accumulated_eval_time=3159.1, accumulated_logging_time=11.0289, accumulated_submission_time=39326.7, global_step=97207, preemption_count=0, score=39326.7, test/accuracy=0.5613, test/loss=1.97599, test/num_examples=10000, total_duration=42507.4, train/accuracy=0.770029, train/loss=0.865006, validation/accuracy=0.69032, validation/loss=1.26608, validation/num_examples=50000
I0307 21:31:11.067473 140113510135552 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.3105902671813965, loss=1.331369161605835
I0307 21:31:50.868350 140113501742848 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.281376361846924, loss=1.3888094425201416
I0307 21:32:31.205736 140113510135552 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.359455108642578, loss=1.291489601135254
I0307 21:33:10.957792 140113501742848 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.5691070556640625, loss=1.3978869915008545
I0307 21:33:50.633038 140113510135552 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.4855661392211914, loss=1.3661046028137207
I0307 21:34:29.877584 140113501742848 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.3830723762512207, loss=1.3578946590423584
I0307 21:35:09.681388 140113510135552 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.400745153427124, loss=1.4664483070373535
I0307 21:35:48.881023 140113501742848 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.2557225227355957, loss=1.3172861337661743
I0307 21:36:28.610247 140113510135552 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.4179110527038574, loss=1.3597949743270874
I0307 21:37:07.836898 140113501742848 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.39713716506958, loss=1.3365730047225952
I0307 21:37:47.569148 140113510135552 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.3700921535491943, loss=1.3617743253707886
I0307 21:38:27.290759 140113501742848 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.3165271282196045, loss=1.3891868591308594
I0307 21:39:03.694983 140269360194752 spec.py:321] Evaluating on the training split.
I0307 21:39:14.579795 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 21:39:35.256302 140269360194752 spec.py:349] Evaluating on the test split.
I0307 21:39:36.970259 140269360194752 submission_runner.py:469] Time since start: 43050.83s, 	Step: 98493, 	{'train/accuracy': 0.7604033350944519, 'train/loss': 0.9039230346679688, 'validation/accuracy': 0.6824199557304382, 'validation/loss': 1.2977700233459473, 'validation/num_examples': 50000, 'test/accuracy': 0.5514000058174133, 'test/loss': 2.0156543254852295, 'test/num_examples': 10000, 'score': 39836.60939192772, 'total_duration': 43050.827724933624, 'accumulated_submission_time': 39836.60939192772, 'accumulated_eval_time': 3192.3785531520844, 'accumulated_logging_time': 11.154204845428467}
I0307 21:39:37.066376 140113510135552 logging_writer.py:48] [98493] accumulated_eval_time=3192.38, accumulated_logging_time=11.1542, accumulated_submission_time=39836.6, global_step=98493, preemption_count=0, score=39836.6, test/accuracy=0.5514, test/loss=2.01565, test/num_examples=10000, total_duration=43050.8, train/accuracy=0.760403, train/loss=0.903923, validation/accuracy=0.68242, validation/loss=1.29777, validation/num_examples=50000
I0307 21:39:40.241360 140113501742848 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.4519383907318115, loss=1.3153581619262695
I0307 21:40:19.674286 140113510135552 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.694298028945923, loss=1.5067981481552124
I0307 21:40:59.709430 140113501742848 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.40031099319458, loss=1.2755264043807983
I0307 21:41:40.019857 140113510135552 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.522562026977539, loss=1.3344430923461914
I0307 21:42:20.036820 140113501742848 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.23150897026062, loss=1.277320146560669
I0307 21:42:59.991740 140113510135552 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.4072158336639404, loss=1.3879868984222412
I0307 21:43:39.527759 140113501742848 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.647418260574341, loss=1.411963701248169
I0307 21:44:19.123924 140113510135552 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.4494988918304443, loss=1.4458439350128174
I0307 21:44:58.549284 140113501742848 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.4109606742858887, loss=1.3621635437011719
I0307 21:45:38.016569 140113510135552 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.561413288116455, loss=1.415069341659546
I0307 21:46:17.920913 140113501742848 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.2607154846191406, loss=1.3392784595489502
I0307 21:46:57.526588 140113510135552 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.450667142868042, loss=1.4974632263183594
I0307 21:47:36.922979 140113501742848 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.4975216388702393, loss=1.3701586723327637
I0307 21:48:07.352992 140269360194752 spec.py:321] Evaluating on the training split.
I0307 21:48:18.382120 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 21:48:40.046271 140269360194752 spec.py:349] Evaluating on the test split.
I0307 21:48:41.788429 140269360194752 submission_runner.py:469] Time since start: 43595.65s, 	Step: 99778, 	{'train/accuracy': 0.7584303021430969, 'train/loss': 0.9036785364151001, 'validation/accuracy': 0.6762199997901917, 'validation/loss': 1.312163233757019, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.032076597213745, 'test/num_examples': 10000, 'score': 40346.70287466049, 'total_duration': 43595.645894765854, 'accumulated_submission_time': 40346.70287466049, 'accumulated_eval_time': 3226.813953638077, 'accumulated_logging_time': 11.288993120193481}
I0307 21:48:41.870812 140113510135552 logging_writer.py:48] [99778] accumulated_eval_time=3226.81, accumulated_logging_time=11.289, accumulated_submission_time=40346.7, global_step=99778, preemption_count=0, score=40346.7, test/accuracy=0.5489, test/loss=2.03208, test/num_examples=10000, total_duration=43595.6, train/accuracy=0.75843, train/loss=0.903679, validation/accuracy=0.67622, validation/loss=1.31216, validation/num_examples=50000
I0307 21:48:50.940526 140113501742848 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.366899013519287, loss=1.3792942762374878
I0307 21:49:30.611407 140113510135552 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.4812307357788086, loss=1.3847254514694214
I0307 21:50:10.656413 140113501742848 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.47937273979187, loss=1.4965598583221436
I0307 21:50:50.580600 140113510135552 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.3661417961120605, loss=1.4565114974975586
I0307 21:51:30.234272 140113501742848 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.221290349960327, loss=1.3632192611694336
I0307 21:52:09.927153 140113510135552 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.3304319381713867, loss=1.4750418663024902
I0307 21:52:49.477344 140113501742848 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.405587673187256, loss=1.382472276687622
I0307 21:53:29.054440 140113510135552 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.321967601776123, loss=1.3019193410873413
I0307 21:54:08.450422 140113501742848 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.3660385608673096, loss=1.2830421924591064
I0307 21:54:47.822546 140113510135552 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.4529104232788086, loss=1.2793086767196655
I0307 21:55:27.433953 140113501742848 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.117067575454712, loss=1.2704510688781738
I0307 21:56:07.401685 140113510135552 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.500568151473999, loss=1.316770315170288
I0307 21:56:47.056575 140113501742848 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.35186767578125, loss=1.2728298902511597
I0307 21:57:11.837175 140269360194752 spec.py:321] Evaluating on the training split.
I0307 21:57:22.631804 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 21:57:42.317679 140269360194752 spec.py:349] Evaluating on the test split.
I0307 21:57:44.035618 140269360194752 submission_runner.py:469] Time since start: 44137.89s, 	Step: 101064, 	{'train/accuracy': 0.7598652839660645, 'train/loss': 0.8957650065422058, 'validation/accuracy': 0.6802799701690674, 'validation/loss': 1.3044817447662354, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.010774612426758, 'test/num_examples': 10000, 'score': 40856.48312306404, 'total_duration': 44137.893067359924, 'accumulated_submission_time': 40856.48312306404, 'accumulated_eval_time': 3259.0123479366302, 'accumulated_logging_time': 11.401992082595825}
I0307 21:57:44.154734 140113510135552 logging_writer.py:48] [101064] accumulated_eval_time=3259.01, accumulated_logging_time=11.402, accumulated_submission_time=40856.5, global_step=101064, preemption_count=0, score=40856.5, test/accuracy=0.5527, test/loss=2.01077, test/num_examples=10000, total_duration=44137.9, train/accuracy=0.759865, train/loss=0.895765, validation/accuracy=0.68028, validation/loss=1.30448, validation/num_examples=50000
I0307 21:57:58.912361 140113501742848 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.3777594566345215, loss=1.3381679058074951
I0307 21:58:38.450180 140113510135552 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.475990056991577, loss=1.3101136684417725
I0307 21:59:17.774132 140113501742848 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.8016042709350586, loss=1.3247424364089966
I0307 21:59:57.056293 140113510135552 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.346132516860962, loss=1.450688362121582
I0307 22:00:36.222203 140113501742848 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.388315439224243, loss=1.3766638040542603
I0307 22:01:15.352666 140113510135552 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.358935594558716, loss=1.3991111516952515
I0307 22:01:54.554402 140113501742848 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.5543649196624756, loss=1.438428521156311
I0307 22:02:33.923310 140113510135552 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.4881818294525146, loss=1.3774813413619995
I0307 22:03:13.247886 140113501742848 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.446382522583008, loss=1.2800859212875366
I0307 22:03:52.687200 140113510135552 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.3765954971313477, loss=1.33847975730896
I0307 22:04:31.980734 140113501742848 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.303126573562622, loss=1.3851217031478882
I0307 22:05:11.618010 140113510135552 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.3431670665740967, loss=1.3443447351455688
I0307 22:05:51.164600 140113501742848 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.2346956729888916, loss=1.2811245918273926
I0307 22:06:14.181975 140269360194752 spec.py:321] Evaluating on the training split.
I0307 22:06:25.050975 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 22:06:44.709639 140269360194752 spec.py:349] Evaluating on the test split.
I0307 22:06:46.431630 140269360194752 submission_runner.py:469] Time since start: 44680.29s, 	Step: 102359, 	{'train/accuracy': 0.7714245915412903, 'train/loss': 0.8496510982513428, 'validation/accuracy': 0.6862999796867371, 'validation/loss': 1.2820326089859009, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.984736442565918, 'test/num_examples': 10000, 'score': 41366.31562757492, 'total_duration': 44680.28909254074, 'accumulated_submission_time': 41366.31562757492, 'accumulated_eval_time': 3291.261964559555, 'accumulated_logging_time': 11.561526775360107}
I0307 22:06:46.555666 140113510135552 logging_writer.py:48] [102359] accumulated_eval_time=3291.26, accumulated_logging_time=11.5615, accumulated_submission_time=41366.3, global_step=102359, preemption_count=0, score=41366.3, test/accuracy=0.5589, test/loss=1.98474, test/num_examples=10000, total_duration=44680.3, train/accuracy=0.771425, train/loss=0.849651, validation/accuracy=0.6863, validation/loss=1.28203, validation/num_examples=50000
I0307 22:07:03.355181 140113501742848 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.397061586380005, loss=1.3738889694213867
I0307 22:07:43.106133 140113510135552 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.431098461151123, loss=1.4187661409378052
2025-03-07 22:08:09.407193: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:08:22.691715 140113501742848 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.4837093353271484, loss=1.4097317457199097
I0307 22:09:01.960439 140113510135552 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.4477248191833496, loss=1.2955907583236694
I0307 22:09:41.221581 140113501742848 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.3409156799316406, loss=1.398005485534668
I0307 22:10:20.981510 140113510135552 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.451408624649048, loss=1.4069279432296753
I0307 22:11:00.538965 140113501742848 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.3046553134918213, loss=1.2657032012939453
I0307 22:11:40.362045 140113510135552 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.281717538833618, loss=1.2560193538665771
I0307 22:12:19.603425 140113501742848 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.538107395172119, loss=1.4306690692901611
I0307 22:12:59.158447 140113510135552 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.476529598236084, loss=1.424328088760376
I0307 22:13:38.199740 140113501742848 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.805405378341675, loss=1.439163327217102
I0307 22:14:17.568362 140113510135552 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.5391628742218018, loss=1.3241554498672485
I0307 22:14:56.770014 140113501742848 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.514373540878296, loss=1.3811005353927612
I0307 22:15:16.534766 140269360194752 spec.py:321] Evaluating on the training split.
I0307 22:15:27.369465 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 22:15:46.304484 140269360194752 spec.py:349] Evaluating on the test split.
I0307 22:15:48.021982 140269360194752 submission_runner.py:469] Time since start: 45221.88s, 	Step: 103652, 	{'train/accuracy': 0.7772639989852905, 'train/loss': 0.8298704028129578, 'validation/accuracy': 0.6933799982070923, 'validation/loss': 1.249065637588501, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9656987190246582, 'test/num_examples': 10000, 'score': 41876.10270237923, 'total_duration': 45221.87944674492, 'accumulated_submission_time': 41876.10270237923, 'accumulated_eval_time': 3322.7491459846497, 'accumulated_logging_time': 11.72409725189209}
I0307 22:15:48.088880 140113510135552 logging_writer.py:48] [103652] accumulated_eval_time=3322.75, accumulated_logging_time=11.7241, accumulated_submission_time=41876.1, global_step=103652, preemption_count=0, score=41876.1, test/accuracy=0.5661, test/loss=1.9657, test/num_examples=10000, total_duration=45221.9, train/accuracy=0.777264, train/loss=0.82987, validation/accuracy=0.69338, validation/loss=1.24907, validation/num_examples=50000
I0307 22:16:07.421766 140113501742848 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.419379472732544, loss=1.324931263923645
I0307 22:16:46.845957 140113510135552 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.375375986099243, loss=1.1977139711380005
2025-03-07 22:16:54.099339: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:17:26.628881 140113501742848 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.4009714126586914, loss=1.4064466953277588
I0307 22:18:06.084409 140113510135552 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.5939366817474365, loss=1.391143560409546
I0307 22:18:45.231634 140113501742848 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.5176308155059814, loss=1.3150945901870728
I0307 22:19:24.667495 140113510135552 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.7992451190948486, loss=1.3278539180755615
I0307 22:20:04.140136 140113501742848 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.55444598197937, loss=1.3323713541030884
I0307 22:20:43.392911 140113510135552 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.6461894512176514, loss=1.4064767360687256
I0307 22:21:23.029240 140113501742848 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.6685402393341064, loss=1.3732880353927612
I0307 22:22:02.714807 140113510135552 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.479217767715454, loss=1.278913974761963
I0307 22:22:42.171951 140113501742848 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.528973340988159, loss=1.3700110912322998
I0307 22:23:21.699356 140113510135552 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.4796524047851562, loss=1.3823866844177246
I0307 22:24:01.420487 140113501742848 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.6753761768341064, loss=1.3473272323608398
I0307 22:24:18.112793 140269360194752 spec.py:321] Evaluating on the training split.
I0307 22:24:29.179947 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 22:24:51.332833 140269360194752 spec.py:349] Evaluating on the test split.
I0307 22:24:53.482527 140269360194752 submission_runner.py:469] Time since start: 45767.34s, 	Step: 104943, 	{'train/accuracy': 0.7734375, 'train/loss': 0.8351027369499207, 'validation/accuracy': 0.6850000023841858, 'validation/loss': 1.2898337841033936, 'validation/num_examples': 50000, 'test/accuracy': 0.5534000396728516, 'test/loss': 2.0251309871673584, 'test/num_examples': 10000, 'score': 42385.95104813576, 'total_duration': 45767.339889764786, 'accumulated_submission_time': 42385.95104813576, 'accumulated_eval_time': 3358.118741750717, 'accumulated_logging_time': 11.811901330947876}
I0307 22:24:53.542853 140113510135552 logging_writer.py:48] [104943] accumulated_eval_time=3358.12, accumulated_logging_time=11.8119, accumulated_submission_time=42386, global_step=104943, preemption_count=0, score=42386, test/accuracy=0.5534, test/loss=2.02513, test/num_examples=10000, total_duration=45767.3, train/accuracy=0.773438, train/loss=0.835103, validation/accuracy=0.685, validation/loss=1.28983, validation/num_examples=50000
I0307 22:25:16.568707 140113501742848 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.489281415939331, loss=1.2792390584945679
2025-03-07 22:25:43.883634: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:25:56.326936 140113510135552 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.560051679611206, loss=1.3076179027557373
I0307 22:26:36.025265 140113501742848 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.4253766536712646, loss=1.2608635425567627
I0307 22:27:15.373970 140113510135552 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.426447868347168, loss=1.4605076313018799
I0307 22:27:54.085338 140113501742848 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.45837140083313, loss=1.3596060276031494
I0307 22:28:33.338256 140113510135552 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.470350980758667, loss=1.3498064279556274
I0307 22:29:13.114002 140113501742848 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.3354885578155518, loss=1.3021198511123657
I0307 22:29:52.807830 140113510135552 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.580775260925293, loss=1.4176607131958008
I0307 22:30:32.320029 140113501742848 logging_writer.py:48] [105800] global_step=105800, grad_norm=3.0850107669830322, loss=1.4407175779342651
I0307 22:31:12.068855 140113510135552 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.357538938522339, loss=1.21402907371521
I0307 22:31:51.625594 140113501742848 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.5808069705963135, loss=1.293308138847351
I0307 22:32:31.220775 140113510135552 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.5137462615966797, loss=1.354917287826538
I0307 22:33:11.029809 140113501742848 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.491863965988159, loss=1.453770399093628
I0307 22:33:23.582310 140269360194752 spec.py:321] Evaluating on the training split.
I0307 22:33:34.621846 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 22:33:56.416548 140269360194752 spec.py:349] Evaluating on the test split.
I0307 22:33:58.104908 140269360194752 submission_runner.py:469] Time since start: 46311.96s, 	Step: 106233, 	{'train/accuracy': 0.7788584232330322, 'train/loss': 0.8187527656555176, 'validation/accuracy': 0.6909399628639221, 'validation/loss': 1.2568820714950562, 'validation/num_examples': 50000, 'test/accuracy': 0.5570000410079956, 'test/loss': 2.028005838394165, 'test/num_examples': 10000, 'score': 42895.79862046242, 'total_duration': 46311.962373018265, 'accumulated_submission_time': 42895.79862046242, 'accumulated_eval_time': 3392.6413123607635, 'accumulated_logging_time': 11.910680294036865}
I0307 22:33:58.259663 140113510135552 logging_writer.py:48] [106233] accumulated_eval_time=3392.64, accumulated_logging_time=11.9107, accumulated_submission_time=42895.8, global_step=106233, preemption_count=0, score=42895.8, test/accuracy=0.557, test/loss=2.02801, test/num_examples=10000, total_duration=46312, train/accuracy=0.778858, train/loss=0.818753, validation/accuracy=0.69094, validation/loss=1.25688, validation/num_examples=50000
I0307 22:34:25.253022 140113501742848 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.489223003387451, loss=1.328693151473999
2025-03-07 22:34:33.755609: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:35:04.784080 140113510135552 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.596668243408203, loss=1.3900665044784546
I0307 22:35:44.385490 140113501742848 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.4460604190826416, loss=1.2663284540176392
I0307 22:36:23.780472 140113510135552 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.5130488872528076, loss=1.2666457891464233
I0307 22:37:03.708694 140113501742848 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.6720142364501953, loss=1.3547672033309937
I0307 22:37:43.605959 140113510135552 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.348202705383301, loss=1.3237133026123047
I0307 22:38:23.196711 140113501742848 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.610379219055176, loss=1.3079001903533936
I0307 22:39:02.831803 140113510135552 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.438209295272827, loss=1.3029143810272217
I0307 22:39:42.775910 140113501742848 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.6906516551971436, loss=1.3088730573654175
I0307 22:40:22.475798 140113510135552 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.625044822692871, loss=1.3327701091766357
I0307 22:41:02.403518 140113501742848 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.5146191120147705, loss=1.2927422523498535
I0307 22:41:42.461564 140113510135552 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.6059794425964355, loss=1.3735429048538208
I0307 22:42:22.289553 140113501742848 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.4598805904388428, loss=1.3227053880691528
I0307 22:42:28.326122 140269360194752 spec.py:321] Evaluating on the training split.
I0307 22:42:39.684363 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 22:43:00.785457 140269360194752 spec.py:349] Evaluating on the test split.
I0307 22:43:02.532093 140269360194752 submission_runner.py:469] Time since start: 46856.39s, 	Step: 107516, 	{'train/accuracy': 0.7691127061843872, 'train/loss': 0.8598592281341553, 'validation/accuracy': 0.6838399767875671, 'validation/loss': 1.2917050123214722, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 2.003894090652466, 'test/num_examples': 10000, 'score': 43405.666311979294, 'total_duration': 46856.389553785324, 'accumulated_submission_time': 43405.666311979294, 'accumulated_eval_time': 3426.847248315811, 'accumulated_logging_time': 12.112124681472778}
I0307 22:43:02.639881 140113510135552 logging_writer.py:48] [107516] accumulated_eval_time=3426.85, accumulated_logging_time=12.1121, accumulated_submission_time=43405.7, global_step=107516, preemption_count=0, score=43405.7, test/accuracy=0.5605, test/loss=2.00389, test/num_examples=10000, total_duration=46856.4, train/accuracy=0.769113, train/loss=0.859859, validation/accuracy=0.68384, validation/loss=1.29171, validation/num_examples=50000
2025-03-07 22:43:25.220625: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:43:36.706645 140113501742848 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.849835157394409, loss=1.3573434352874756
I0307 22:44:16.653079 140113510135552 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.380648136138916, loss=1.3721623420715332
I0307 22:44:55.925701 140113501742848 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.342188596725464, loss=1.28782057762146
I0307 22:45:35.593187 140113510135552 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.4520201683044434, loss=1.3924758434295654
I0307 22:46:15.594749 140113501742848 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.5217065811157227, loss=1.2434558868408203
I0307 22:46:55.514431 140113510135552 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.337972402572632, loss=1.1207480430603027
I0307 22:47:35.505284 140113501742848 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.321239709854126, loss=1.297824740409851
I0307 22:48:15.566022 140113510135552 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.4987876415252686, loss=1.4116673469543457
I0307 22:48:55.574632 140113501742848 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.6860406398773193, loss=1.2992554903030396
I0307 22:49:35.456075 140113510135552 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.611248731613159, loss=1.1790266036987305
I0307 22:50:15.355649 140113501742848 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.488372564315796, loss=1.3896820545196533
I0307 22:50:55.394475 140113510135552 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.5257301330566406, loss=1.2501428127288818
I0307 22:51:32.629709 140269360194752 spec.py:321] Evaluating on the training split.
I0307 22:51:44.212373 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 22:52:02.386209 140269360194752 spec.py:349] Evaluating on the test split.
I0307 22:52:04.095040 140269360194752 submission_runner.py:469] Time since start: 47397.95s, 	Step: 108795, 	{'train/accuracy': 0.7829440236091614, 'train/loss': 0.8076595067977905, 'validation/accuracy': 0.6936999559402466, 'validation/loss': 1.2444788217544556, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 1.9626424312591553, 'test/num_examples': 10000, 'score': 43915.451953172684, 'total_duration': 47397.95249414444, 'accumulated_submission_time': 43915.451953172684, 'accumulated_eval_time': 3458.3125355243683, 'accumulated_logging_time': 12.270926475524902}
I0307 22:52:04.195686 140113501742848 logging_writer.py:48] [108795] accumulated_eval_time=3458.31, accumulated_logging_time=12.2709, accumulated_submission_time=43915.5, global_step=108795, preemption_count=0, score=43915.5, test/accuracy=0.5642, test/loss=1.96264, test/num_examples=10000, total_duration=47398, train/accuracy=0.782944, train/loss=0.80766, validation/accuracy=0.6937, validation/loss=1.24448, validation/num_examples=50000
I0307 22:52:06.594910 140113510135552 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.6182069778442383, loss=1.2831823825836182
2025-03-07 22:52:15.453771: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:52:46.125012 140113501742848 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.6240322589874268, loss=1.3583905696868896
I0307 22:53:25.839876 140113510135552 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.6609299182891846, loss=1.3501733541488647
I0307 22:54:05.968274 140113501742848 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.5279552936553955, loss=1.3465080261230469
I0307 22:54:45.714995 140113510135552 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.642564535140991, loss=1.257062315940857
I0307 22:55:25.687540 140113501742848 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.3148934841156006, loss=1.216140866279602
I0307 22:56:05.443834 140113510135552 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.5865676403045654, loss=1.273219108581543
I0307 22:56:45.115748 140113501742848 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.9578425884246826, loss=1.333531379699707
I0307 22:57:24.939979 140113510135552 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.4517319202423096, loss=1.3753199577331543
I0307 22:58:04.720333 140113501742848 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.468799114227295, loss=1.2935184240341187
I0307 22:58:44.539926 140113510135552 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.534628391265869, loss=1.2603716850280762
I0307 22:59:24.618547 140113501742848 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.4232139587402344, loss=1.249514102935791
I0307 23:00:04.502067 140113510135552 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.5624005794525146, loss=1.3192819356918335
I0307 23:00:34.182935 140269360194752 spec.py:321] Evaluating on the training split.
2025-03-07 23:00:34.976273: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:00:45.482165 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 23:01:07.966264 140269360194752 spec.py:349] Evaluating on the test split.
I0307 23:01:09.680987 140269360194752 submission_runner.py:469] Time since start: 47943.54s, 	Step: 110076, 	{'train/accuracy': 0.7724409699440002, 'train/loss': 0.8382278084754944, 'validation/accuracy': 0.6887399554252625, 'validation/loss': 1.271635890007019, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.9834569692611694, 'test/num_examples': 10000, 'score': 44425.26453781128, 'total_duration': 47943.53843379021, 'accumulated_submission_time': 44425.26453781128, 'accumulated_eval_time': 3493.8105325698853, 'accumulated_logging_time': 12.391745567321777}
I0307 23:01:09.784714 140113501742848 logging_writer.py:48] [110076] accumulated_eval_time=3493.81, accumulated_logging_time=12.3917, accumulated_submission_time=44425.3, global_step=110076, preemption_count=0, score=44425.3, test/accuracy=0.5638, test/loss=1.98346, test/num_examples=10000, total_duration=47943.5, train/accuracy=0.772441, train/loss=0.838228, validation/accuracy=0.68874, validation/loss=1.27164, validation/num_examples=50000
I0307 23:01:19.738285 140113510135552 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.696150779724121, loss=1.2263966798782349
I0307 23:01:59.394592 140113501742848 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.9517886638641357, loss=1.4741003513336182
I0307 23:02:39.696187 140113510135552 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.431953191757202, loss=1.3549575805664062
I0307 23:03:19.676973 140113501742848 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.3527307510375977, loss=1.1982592344284058
I0307 23:03:59.724931 140113510135552 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.485059976577759, loss=1.332180380821228
I0307 23:04:39.606521 140113501742848 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.5627200603485107, loss=1.2995561361312866
I0307 23:05:19.413108 140113510135552 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.704798936843872, loss=1.3551002740859985
I0307 23:05:59.106916 140113501742848 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.410167694091797, loss=1.1913952827453613
I0307 23:06:39.216773 140113510135552 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.560405969619751, loss=1.2910977602005005
I0307 23:07:19.208098 140113501742848 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.616602897644043, loss=1.2240656614303589
I0307 23:07:59.394620 140113510135552 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.711975336074829, loss=1.3376104831695557
I0307 23:08:39.786275 140113501742848 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.53497576713562, loss=1.3061902523040771
I0307 23:09:19.837516 140113510135552 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.5356569290161133, loss=1.2618199586868286
2025-03-07 23:09:31.233924: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:09:39.789717 140269360194752 spec.py:321] Evaluating on the training split.
I0307 23:09:51.136275 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 23:10:14.457478 140269360194752 spec.py:349] Evaluating on the test split.
I0307 23:10:16.176356 140269360194752 submission_runner.py:469] Time since start: 48490.03s, 	Step: 111351, 	{'train/accuracy': 0.7835817933082581, 'train/loss': 0.8117202520370483, 'validation/accuracy': 0.6911799907684326, 'validation/loss': 1.25642728805542, 'validation/num_examples': 50000, 'test/accuracy': 0.5667999982833862, 'test/loss': 1.955601453781128, 'test/num_examples': 10000, 'score': 44935.08746623993, 'total_duration': 48490.03379917145, 'accumulated_submission_time': 44935.08746623993, 'accumulated_eval_time': 3530.197116136551, 'accumulated_logging_time': 12.525977611541748}
I0307 23:10:16.281136 140113501742848 logging_writer.py:48] [111351] accumulated_eval_time=3530.2, accumulated_logging_time=12.526, accumulated_submission_time=44935.1, global_step=111351, preemption_count=0, score=44935.1, test/accuracy=0.5668, test/loss=1.9556, test/num_examples=10000, total_duration=48490, train/accuracy=0.783582, train/loss=0.81172, validation/accuracy=0.69118, validation/loss=1.25643, validation/num_examples=50000
I0307 23:10:36.229450 140113510135552 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.752570629119873, loss=1.2508083581924438
I0307 23:11:15.884308 140113501742848 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.3972744941711426, loss=1.2827808856964111
I0307 23:11:55.254142 140113510135552 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.732156753540039, loss=1.374326229095459
I0307 23:12:35.155536 140113501742848 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.7238047122955322, loss=1.2439292669296265
I0307 23:13:14.667910 140113510135552 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.5798110961914062, loss=1.2997738122940063
I0307 23:13:54.452550 140113501742848 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.7952678203582764, loss=1.405004858970642
I0307 23:14:34.026609 140113510135552 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.6451799869537354, loss=1.3201866149902344
I0307 23:15:13.771632 140113501742848 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.59499192237854, loss=1.282938838005066
I0307 23:15:53.535986 140113510135552 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.6243672370910645, loss=1.331741213798523
I0307 23:16:33.194567 140113501742848 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.702787399291992, loss=1.314480185508728
I0307 23:17:13.329793 140113510135552 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.475452184677124, loss=1.2465345859527588
I0307 23:17:53.222208 140113501742848 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.5394985675811768, loss=1.2023470401763916
2025-03-07 23:18:24.087057: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:18:33.308617 140113510135552 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.6543631553649902, loss=1.2525861263275146
I0307 23:18:46.500186 140269360194752 spec.py:321] Evaluating on the training split.
I0307 23:18:57.404824 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 23:19:16.604320 140269360194752 spec.py:349] Evaluating on the test split.
I0307 23:19:18.330844 140269360194752 submission_runner.py:469] Time since start: 49032.19s, 	Step: 112634, 	{'train/accuracy': 0.790437638759613, 'train/loss': 0.7606332302093506, 'validation/accuracy': 0.697219967842102, 'validation/loss': 1.2452595233917236, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.984863519668579, 'test/num_examples': 10000, 'score': 45445.11686396599, 'total_duration': 49032.18830418587, 'accumulated_submission_time': 45445.11686396599, 'accumulated_eval_time': 3562.0277366638184, 'accumulated_logging_time': 12.664294481277466}
I0307 23:19:18.436303 140113501742848 logging_writer.py:48] [112634] accumulated_eval_time=3562.03, accumulated_logging_time=12.6643, accumulated_submission_time=45445.1, global_step=112634, preemption_count=0, score=45445.1, test/accuracy=0.5696, test/loss=1.98486, test/num_examples=10000, total_duration=49032.2, train/accuracy=0.790438, train/loss=0.760633, validation/accuracy=0.69722, validation/loss=1.24526, validation/num_examples=50000
I0307 23:19:45.425112 140113510135552 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.5661134719848633, loss=1.302130937576294
I0307 23:20:25.132429 140113501742848 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.460792064666748, loss=1.2448395490646362
I0307 23:21:04.601787 140113510135552 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.746753454208374, loss=1.2993402481079102
I0307 23:21:44.183006 140113501742848 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.493238925933838, loss=1.3023197650909424
I0307 23:22:23.443959 140113510135552 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.690581798553467, loss=1.2970188856124878
I0307 23:23:02.819921 140113501742848 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.5051498413085938, loss=1.199034333229065
I0307 23:23:42.375679 140113510135552 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.648540735244751, loss=1.342461109161377
I0307 23:24:22.152324 140113501742848 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.661536455154419, loss=1.2824667692184448
I0307 23:25:01.515764 140113510135552 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.493622303009033, loss=1.3208564519882202
I0307 23:25:41.137128 140113501742848 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.501671075820923, loss=1.387589693069458
I0307 23:26:21.041729 140113510135552 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.48024320602417, loss=1.17393958568573
I0307 23:27:01.314553 140113501742848 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.4828901290893555, loss=1.1960523128509521
2025-03-07 23:27:12.889940: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:27:41.698508 140113510135552 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.575439929962158, loss=1.2385773658752441
I0307 23:27:48.673346 140269360194752 spec.py:321] Evaluating on the training split.
I0307 23:27:59.261176 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 23:28:21.684646 140269360194752 spec.py:349] Evaluating on the test split.
I0307 23:28:23.403606 140269360194752 submission_runner.py:469] Time since start: 49577.26s, 	Step: 113919, 	{'train/accuracy': 0.7876474857330322, 'train/loss': 0.7735476493835449, 'validation/accuracy': 0.6972999572753906, 'validation/loss': 1.234043836593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5649999976158142, 'test/loss': 1.988587737083435, 'test/num_examples': 10000, 'score': 45955.16851425171, 'total_duration': 49577.261051893234, 'accumulated_submission_time': 45955.16851425171, 'accumulated_eval_time': 3596.7579474449158, 'accumulated_logging_time': 12.797095775604248}
I0307 23:28:23.491539 140113501742848 logging_writer.py:48] [113919] accumulated_eval_time=3596.76, accumulated_logging_time=12.7971, accumulated_submission_time=45955.2, global_step=113919, preemption_count=0, score=45955.2, test/accuracy=0.565, test/loss=1.98859, test/num_examples=10000, total_duration=49577.3, train/accuracy=0.787647, train/loss=0.773548, validation/accuracy=0.6973, validation/loss=1.23404, validation/num_examples=50000
I0307 23:28:55.766432 140113510135552 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.9361696243286133, loss=1.2983171939849854
I0307 23:29:35.697300 140113501742848 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.6910810470581055, loss=1.350942611694336
I0307 23:30:15.475188 140113510135552 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.620435953140259, loss=1.2845585346221924
I0307 23:30:55.370851 140113501742848 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.5107192993164062, loss=1.235260009765625
I0307 23:31:34.996485 140113510135552 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.639503002166748, loss=1.2725743055343628
I0307 23:32:14.445478 140113501742848 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.9317009449005127, loss=1.2671231031417847
I0307 23:32:53.872079 140113510135552 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.928546905517578, loss=1.3427786827087402
I0307 23:33:33.484481 140113501742848 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.6715598106384277, loss=1.311479091644287
I0307 23:34:13.609009 140113510135552 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.825134038925171, loss=1.2525521516799927
I0307 23:34:53.454897 140113501742848 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.823922634124756, loss=1.2139374017715454
I0307 23:35:33.711602 140113510135552 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.8093695640563965, loss=1.2292680740356445
2025-03-07 23:36:06.210600: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:36:13.695007 140113501742848 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.675114631652832, loss=1.3231940269470215
I0307 23:36:53.330861 140113510135552 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.5916948318481445, loss=1.221561074256897
I0307 23:36:53.671876 140269360194752 spec.py:321] Evaluating on the training split.
I0307 23:37:04.658390 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 23:37:26.516784 140269360194752 spec.py:349] Evaluating on the test split.
I0307 23:37:28.233440 140269360194752 submission_runner.py:469] Time since start: 50122.09s, 	Step: 115202, 	{'train/accuracy': 0.7921516299247742, 'train/loss': 0.7752204537391663, 'validation/accuracy': 0.6965599656105042, 'validation/loss': 1.230233907699585, 'validation/num_examples': 50000, 'test/accuracy': 0.5667000412940979, 'test/loss': 1.9672342538833618, 'test/num_examples': 10000, 'score': 46465.16586327553, 'total_duration': 50122.09089612961, 'accumulated_submission_time': 46465.16586327553, 'accumulated_eval_time': 3631.3194675445557, 'accumulated_logging_time': 12.909730911254883}
I0307 23:37:28.380890 140113501742848 logging_writer.py:48] [115202] accumulated_eval_time=3631.32, accumulated_logging_time=12.9097, accumulated_submission_time=46465.2, global_step=115202, preemption_count=0, score=46465.2, test/accuracy=0.5667, test/loss=1.96723, test/num_examples=10000, total_duration=50122.1, train/accuracy=0.792152, train/loss=0.77522, validation/accuracy=0.69656, validation/loss=1.23023, validation/num_examples=50000
I0307 23:38:07.680977 140113510135552 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.674528121948242, loss=1.2633575201034546
I0307 23:38:47.224877 140113501742848 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.7062675952911377, loss=1.3158767223358154
I0307 23:39:27.026298 140113510135552 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.7826340198516846, loss=1.2879893779754639
I0307 23:40:06.513929 140113501742848 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.819772720336914, loss=1.246948480606079
I0307 23:40:45.909135 140113510135552 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.789588689804077, loss=1.27146315574646
I0307 23:41:25.242505 140113501742848 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.7692086696624756, loss=1.2991381883621216
I0307 23:42:04.878706 140113510135552 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.5741729736328125, loss=1.2704766988754272
I0307 23:42:44.430894 140113501742848 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.630728006362915, loss=1.349219799041748
I0307 23:43:24.214231 140113510135552 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.5640974044799805, loss=1.2994359731674194
I0307 23:44:04.421553 140113501742848 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.892991781234741, loss=1.3541909456253052
I0307 23:44:44.573288 140113510135552 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.9395253658294678, loss=1.2950023412704468
2025-03-07 23:44:56.283408: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:45:24.810039 140113501742848 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.938873529434204, loss=1.2662403583526611
I0307 23:45:58.383367 140269360194752 spec.py:321] Evaluating on the training split.
I0307 23:46:09.108860 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 23:46:28.333521 140269360194752 spec.py:349] Evaluating on the test split.
I0307 23:46:30.231245 140269360194752 submission_runner.py:469] Time since start: 50664.09s, 	Step: 116485, 	{'train/accuracy': 0.7828842401504517, 'train/loss': 0.79611736536026, 'validation/accuracy': 0.6855999827384949, 'validation/loss': 1.2866547107696533, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 2.025076389312744, 'test/num_examples': 10000, 'score': 46974.98157310486, 'total_duration': 50664.08869457245, 'accumulated_submission_time': 46974.98157310486, 'accumulated_eval_time': 3663.16729593277, 'accumulated_logging_time': 13.08717393875122}
I0307 23:46:30.320752 140113510135552 logging_writer.py:48] [116485] accumulated_eval_time=3663.17, accumulated_logging_time=13.0872, accumulated_submission_time=46975, global_step=116485, preemption_count=0, score=46975, test/accuracy=0.5581, test/loss=2.02508, test/num_examples=10000, total_duration=50664.1, train/accuracy=0.782884, train/loss=0.796117, validation/accuracy=0.6856, validation/loss=1.28665, validation/num_examples=50000
I0307 23:46:36.618235 140113501742848 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.6682722568511963, loss=1.2034512758255005
I0307 23:47:15.778353 140113510135552 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.6915340423583984, loss=1.2225329875946045
I0307 23:47:55.860303 140113501742848 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.6464340686798096, loss=1.2783684730529785
I0307 23:48:35.129172 140113510135552 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.832643508911133, loss=1.1896488666534424
I0307 23:49:14.649550 140113501742848 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.74902081489563, loss=1.2581181526184082
I0307 23:49:54.430378 140113510135552 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.6633753776550293, loss=1.3504446744918823
I0307 23:50:33.555701 140113501742848 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.6899001598358154, loss=1.2279388904571533
I0307 23:51:13.154729 140113510135552 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.7684166431427, loss=1.2817981243133545
I0307 23:51:52.547639 140113501742848 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.492893695831299, loss=1.2178375720977783
I0307 23:52:32.771285 140113510135552 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.964042901992798, loss=1.3407858610153198
I0307 23:53:12.734108 140113501742848 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.8456358909606934, loss=1.3458517789840698
2025-03-07 23:53:44.682959: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:53:52.490015 140113510135552 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.526109218597412, loss=1.2129114866256714
I0307 23:54:32.298073 140113501742848 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.6017608642578125, loss=1.192335844039917
I0307 23:55:00.243024 140269360194752 spec.py:321] Evaluating on the training split.
I0307 23:55:11.121612 140269360194752 spec.py:333] Evaluating on the validation split.
I0307 23:55:29.218480 140269360194752 spec.py:349] Evaluating on the test split.
I0307 23:55:30.941468 140269360194752 submission_runner.py:469] Time since start: 51204.80s, 	Step: 117772, 	{'train/accuracy': 0.8014987111091614, 'train/loss': 0.722296953201294, 'validation/accuracy': 0.7025799751281738, 'validation/loss': 1.2125309705734253, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.9241734743118286, 'test/num_examples': 10000, 'score': 47484.701085329056, 'total_duration': 51204.798934698105, 'accumulated_submission_time': 47484.701085329056, 'accumulated_eval_time': 3693.865710258484, 'accumulated_logging_time': 13.221311330795288}
I0307 23:55:31.043142 140113510135552 logging_writer.py:48] [117772] accumulated_eval_time=3693.87, accumulated_logging_time=13.2213, accumulated_submission_time=47484.7, global_step=117772, preemption_count=0, score=47484.7, test/accuracy=0.5755, test/loss=1.92417, test/num_examples=10000, total_duration=51204.8, train/accuracy=0.801499, train/loss=0.722297, validation/accuracy=0.70258, validation/loss=1.21253, validation/num_examples=50000
I0307 23:55:42.659602 140113501742848 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.794428825378418, loss=1.3112961053848267
I0307 23:56:21.808199 140113510135552 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.592798948287964, loss=1.2006964683532715
I0307 23:57:01.242154 140113501742848 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.0290799140930176, loss=1.2701095342636108
I0307 23:57:41.119513 140113510135552 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.5924196243286133, loss=1.220109224319458
I0307 23:58:20.827902 140113501742848 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.724696159362793, loss=1.2210817337036133
I0307 23:59:00.523435 140113510135552 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.7705183029174805, loss=1.271667718887329
I0307 23:59:40.332796 140113501742848 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.6266303062438965, loss=1.1818294525146484
I0308 00:00:20.095420 140113510135552 logging_writer.py:48] [118500] global_step=118500, grad_norm=3.074918508529663, loss=1.3925241231918335
I0308 00:01:00.156617 140113501742848 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.7489891052246094, loss=1.1917033195495605
I0308 00:01:40.173114 140113510135552 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.8290505409240723, loss=1.2057957649230957
I0308 00:02:19.954473 140113501742848 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.7596545219421387, loss=1.2258179187774658
2025-03-08 00:02:32.566518: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:02:59.960502 140113510135552 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.824181079864502, loss=1.2000234127044678
I0308 00:03:39.376071 140113501742848 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.942063331604004, loss=1.3342000246047974
I0308 00:04:01.199411 140269360194752 spec.py:321] Evaluating on the training split.
I0308 00:04:11.761404 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 00:04:33.036946 140269360194752 spec.py:349] Evaluating on the test split.
I0308 00:04:34.758408 140269360194752 submission_runner.py:469] Time since start: 51748.62s, 	Step: 119057, 	{'train/accuracy': 0.7967554330825806, 'train/loss': 0.7357949614524841, 'validation/accuracy': 0.697439968585968, 'validation/loss': 1.229734182357788, 'validation/num_examples': 50000, 'test/accuracy': 0.5702000260353088, 'test/loss': 1.9597786664962769, 'test/num_examples': 10000, 'score': 47994.67843961716, 'total_duration': 51748.61587405205, 'accumulated_submission_time': 47994.67843961716, 'accumulated_eval_time': 3727.4246740341187, 'accumulated_logging_time': 13.346659421920776}
I0308 00:04:34.822978 140113510135552 logging_writer.py:48] [119057] accumulated_eval_time=3727.42, accumulated_logging_time=13.3467, accumulated_submission_time=47994.7, global_step=119057, preemption_count=0, score=47994.7, test/accuracy=0.5702, test/loss=1.95978, test/num_examples=10000, total_duration=51748.6, train/accuracy=0.796755, train/loss=0.735795, validation/accuracy=0.69744, validation/loss=1.22973, validation/num_examples=50000
I0308 00:04:51.960552 140113501742848 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.707120895385742, loss=1.281032681465149
I0308 00:05:31.444912 140113510135552 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.948918581008911, loss=1.2201404571533203
I0308 00:06:10.861322 140113501742848 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.89937424659729, loss=1.2251591682434082
I0308 00:06:50.563127 140113510135552 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.918952226638794, loss=1.3447924852371216
I0308 00:07:30.046981 140113501742848 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.846204996109009, loss=1.2904175519943237
I0308 00:08:09.594832 140113510135552 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.6258342266082764, loss=1.2515071630477905
I0308 00:08:48.857197 140113501742848 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.968275308609009, loss=1.2131810188293457
I0308 00:09:28.617847 140113510135552 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.6700022220611572, loss=1.166639804840088
I0308 00:10:07.564337 140113501742848 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.8473780155181885, loss=1.2221055030822754
I0308 00:10:47.625494 140113510135552 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.8894762992858887, loss=1.2510000467300415
2025-03-08 00:11:21.141346: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:11:27.530231 140113501742848 logging_writer.py:48] [120100] global_step=120100, grad_norm=3.1360559463500977, loss=1.1993955373764038
I0308 00:12:07.244327 140113510135552 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.69024920463562, loss=1.2564796209335327
I0308 00:12:47.114796 140113501742848 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.760500431060791, loss=1.291520595550537
I0308 00:13:04.821790 140269360194752 spec.py:321] Evaluating on the training split.
I0308 00:13:15.407895 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 00:13:35.744869 140269360194752 spec.py:349] Evaluating on the test split.
I0308 00:13:37.467629 140269360194752 submission_runner.py:469] Time since start: 52291.33s, 	Step: 120346, 	{'train/accuracy': 0.8049864172935486, 'train/loss': 0.7096879482269287, 'validation/accuracy': 0.7060799598693848, 'validation/loss': 1.1964306831359863, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.9114309549331665, 'test/num_examples': 10000, 'score': 48504.49068021774, 'total_duration': 52291.32509112358, 'accumulated_submission_time': 48504.49068021774, 'accumulated_eval_time': 3760.0704922676086, 'accumulated_logging_time': 13.443628549575806}
I0308 00:13:37.558523 140113510135552 logging_writer.py:48] [120346] accumulated_eval_time=3760.07, accumulated_logging_time=13.4436, accumulated_submission_time=48504.5, global_step=120346, preemption_count=0, score=48504.5, test/accuracy=0.5789, test/loss=1.91143, test/num_examples=10000, total_duration=52291.3, train/accuracy=0.804986, train/loss=0.709688, validation/accuracy=0.70608, validation/loss=1.19643, validation/num_examples=50000
I0308 00:13:59.381634 140113501742848 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.6209914684295654, loss=1.1878814697265625
I0308 00:14:39.192311 140113510135552 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.7739577293395996, loss=1.2144476175308228
I0308 00:15:18.707281 140113501742848 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.9070117473602295, loss=1.2075181007385254
I0308 00:15:58.649075 140113510135552 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.6039726734161377, loss=1.1026191711425781
I0308 00:16:38.226453 140113501742848 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.7774031162261963, loss=1.1621711254119873
I0308 00:17:17.958567 140113510135552 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.886219024658203, loss=1.2798558473587036
I0308 00:17:57.905736 140113501742848 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.6139414310455322, loss=1.1199567317962646
I0308 00:18:37.670482 140113510135552 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.867180585861206, loss=1.2782784700393677
I0308 00:19:17.578059 140113501742848 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.692011594772339, loss=1.1795754432678223
I0308 00:19:57.678094 140113510135552 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.729506015777588, loss=1.0861587524414062
2025-03-08 00:20:10.969388: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:20:37.554690 140113501742848 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.8819773197174072, loss=1.2712006568908691
I0308 00:21:17.189475 140113510135552 logging_writer.py:48] [121500] global_step=121500, grad_norm=3.1795411109924316, loss=1.286115050315857
I0308 00:21:56.772338 140113501742848 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.812272071838379, loss=1.145517349243164
I0308 00:22:07.749942 140269360194752 spec.py:321] Evaluating on the training split.
I0308 00:22:18.829498 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 00:22:41.614794 140269360194752 spec.py:349] Evaluating on the test split.
I0308 00:22:43.320932 140269360194752 submission_runner.py:469] Time since start: 52837.18s, 	Step: 121629, 	{'train/accuracy': 0.8121811151504517, 'train/loss': 0.6799030303955078, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.1968052387237549, 'validation/num_examples': 50000, 'test/accuracy': 0.5857000350952148, 'test/loss': 1.9047188758850098, 'test/num_examples': 10000, 'score': 49014.4986975193, 'total_duration': 52837.1783952713, 'accumulated_submission_time': 49014.4986975193, 'accumulated_eval_time': 3795.641449689865, 'accumulated_logging_time': 13.564247608184814}
I0308 00:22:43.393421 140113510135552 logging_writer.py:48] [121629] accumulated_eval_time=3795.64, accumulated_logging_time=13.5642, accumulated_submission_time=49014.5, global_step=121629, preemption_count=0, score=49014.5, test/accuracy=0.5857, test/loss=1.90472, test/num_examples=10000, total_duration=52837.2, train/accuracy=0.812181, train/loss=0.679903, validation/accuracy=0.70642, validation/loss=1.19681, validation/num_examples=50000
I0308 00:23:12.296241 140113501742848 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.794069528579712, loss=1.095594882965088
I0308 00:23:51.104774 140113510135552 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.204660177230835, loss=1.2733261585235596
I0308 00:24:30.580176 140113501742848 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.6627323627471924, loss=1.149841070175171
I0308 00:25:09.953410 140113510135552 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.8163559436798096, loss=1.2145520448684692
I0308 00:25:48.892492 140113501742848 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.9509525299072266, loss=1.2993210554122925
I0308 00:26:27.977078 140113510135552 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.833883047103882, loss=1.2000194787979126
I0308 00:27:06.970895 140113501742848 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.096022129058838, loss=1.2233421802520752
I0308 00:27:46.614339 140113510135552 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.5998435020446777, loss=1.2078297138214111
I0308 00:28:26.151651 140113501742848 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.8990440368652344, loss=1.2950303554534912
2025-03-08 00:29:01.076890: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:29:05.757023 140113510135552 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.6670563220977783, loss=1.2445156574249268
I0308 00:29:45.127913 140113501742848 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.1232452392578125, loss=1.3979476690292358
I0308 00:30:24.139339 140113510135552 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.81360125541687, loss=1.2302674055099487
I0308 00:31:02.990244 140113501742848 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.7715532779693604, loss=1.1838600635528564
I0308 00:31:13.664310 140269360194752 spec.py:321] Evaluating on the training split.
I0308 00:31:24.609549 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 00:31:45.581310 140269360194752 spec.py:349] Evaluating on the test split.
I0308 00:31:47.375499 140269360194752 submission_runner.py:469] Time since start: 53381.23s, 	Step: 122929, 	{'train/accuracy': 0.8142139315605164, 'train/loss': 0.6730453968048096, 'validation/accuracy': 0.7076999545097351, 'validation/loss': 1.194522500038147, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 1.9256930351257324, 'test/num_examples': 10000, 'score': 49524.02431964874, 'total_duration': 53381.2328877449, 'accumulated_submission_time': 49524.02431964874, 'accumulated_eval_time': 3829.3525273799896, 'accumulated_logging_time': 14.227649211883545}
I0308 00:31:47.510347 140113510135552 logging_writer.py:48] [122929] accumulated_eval_time=3829.35, accumulated_logging_time=14.2276, accumulated_submission_time=49524, global_step=122929, preemption_count=0, score=49524, test/accuracy=0.581, test/loss=1.92569, test/num_examples=10000, total_duration=53381.2, train/accuracy=0.814214, train/loss=0.673045, validation/accuracy=0.7077, validation/loss=1.19452, validation/num_examples=50000
I0308 00:32:15.454479 140113501742848 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.0330052375793457, loss=1.3454182147979736
I0308 00:32:54.374043 140113510135552 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.784198760986328, loss=1.1200703382492065
I0308 00:33:33.358488 140113501742848 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.167267322540283, loss=1.2110341787338257
I0308 00:34:12.498558 140113510135552 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.7465953826904297, loss=1.2262290716171265
I0308 00:34:51.074555 140113501742848 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.0245532989501953, loss=1.151484727859497
I0308 00:35:30.168037 140113510135552 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.8354437351226807, loss=1.3115298748016357
I0308 00:36:09.354512 140113501742848 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.1593871116638184, loss=1.237694501876831
I0308 00:36:49.234675 140113510135552 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.8588221073150635, loss=1.2723182439804077
I0308 00:37:28.580475 140113501742848 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.8812167644500732, loss=1.238139271736145
2025-03-08 00:37:42.568429: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:38:07.841859 140113510135552 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.890432596206665, loss=1.2773463726043701
I0308 00:38:46.854392 140113501742848 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.927386522293091, loss=1.0981450080871582
I0308 00:39:26.053098 140113510135552 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.88407564163208, loss=1.1685796976089478
I0308 00:40:05.000066 140113501742848 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.7044270038604736, loss=1.2183990478515625
I0308 00:40:17.621438 140269360194752 spec.py:321] Evaluating on the training split.
I0308 00:40:28.415408 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 00:40:51.148320 140269360194752 spec.py:349] Evaluating on the test split.
I0308 00:40:52.887600 140269360194752 submission_runner.py:469] Time since start: 53926.75s, 	Step: 124233, 	{'train/accuracy': 0.8074776530265808, 'train/loss': 0.6959711909294128, 'validation/accuracy': 0.7015599608421326, 'validation/loss': 1.2140841484069824, 'validation/num_examples': 50000, 'test/accuracy': 0.5718000531196594, 'test/loss': 1.9616262912750244, 'test/num_examples': 10000, 'score': 50033.93161869049, 'total_duration': 53926.7450504303, 'accumulated_submission_time': 50033.93161869049, 'accumulated_eval_time': 3864.6186389923096, 'accumulated_logging_time': 14.410242557525635}
I0308 00:40:52.989502 140113510135552 logging_writer.py:48] [124233] accumulated_eval_time=3864.62, accumulated_logging_time=14.4102, accumulated_submission_time=50033.9, global_step=124233, preemption_count=0, score=50033.9, test/accuracy=0.5718, test/loss=1.96163, test/num_examples=10000, total_duration=53926.7, train/accuracy=0.807478, train/loss=0.695971, validation/accuracy=0.70156, validation/loss=1.21408, validation/num_examples=50000
I0308 00:41:19.403165 140113501742848 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.823488473892212, loss=1.164616346359253
I0308 00:41:58.944725 140113510135552 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.763123035430908, loss=1.1948652267456055
I0308 00:42:38.005723 140113501742848 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.9551541805267334, loss=1.1518105268478394
I0308 00:43:17.417430 140113510135552 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.1029417514801025, loss=1.1741840839385986
I0308 00:43:56.166950 140113501742848 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.4324400424957275, loss=1.2925999164581299
I0308 00:44:35.146905 140113510135552 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.8545963764190674, loss=1.2006486654281616
I0308 00:45:14.271102 140113501742848 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.868485689163208, loss=1.2117925882339478
I0308 00:45:54.461346 140113510135552 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.0856802463531494, loss=1.221657395362854
2025-03-08 00:46:29.149289: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:46:34.241374 140113501742848 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.0875906944274902, loss=1.275528073310852
I0308 00:47:13.546171 140113510135552 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.929777145385742, loss=1.161655306816101
I0308 00:47:52.584058 140113501742848 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.0488128662109375, loss=1.301642894744873
I0308 00:48:31.612669 140113510135552 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.777186155319214, loss=1.149996280670166
I0308 00:49:11.412631 140113501742848 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.9147274494171143, loss=1.142151117324829
I0308 00:49:22.960783 140269360194752 spec.py:321] Evaluating on the training split.
I0308 00:49:33.795520 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 00:49:55.067775 140269360194752 spec.py:349] Evaluating on the test split.
I0308 00:49:56.819246 140269360194752 submission_runner.py:469] Time since start: 54470.68s, 	Step: 125530, 	{'train/accuracy': 0.813895046710968, 'train/loss': 0.6682764887809753, 'validation/accuracy': 0.7048400044441223, 'validation/loss': 1.2078776359558105, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 1.9193578958511353, 'test/num_examples': 10000, 'score': 50543.704063653946, 'total_duration': 54470.67670702934, 'accumulated_submission_time': 50543.704063653946, 'accumulated_eval_time': 3898.4770679473877, 'accumulated_logging_time': 14.555945873260498}
I0308 00:49:56.899987 140113510135552 logging_writer.py:48] [125530] accumulated_eval_time=3898.48, accumulated_logging_time=14.5559, accumulated_submission_time=50543.7, global_step=125530, preemption_count=0, score=50543.7, test/accuracy=0.5775, test/loss=1.91936, test/num_examples=10000, total_duration=54470.7, train/accuracy=0.813895, train/loss=0.668276, validation/accuracy=0.70484, validation/loss=1.20788, validation/num_examples=50000
I0308 00:50:24.694177 140113501742848 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.968397378921509, loss=1.169346809387207
I0308 00:51:03.964764 140113510135552 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.311666488647461, loss=1.267841100692749
I0308 00:51:43.192385 140113501742848 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.947401762008667, loss=1.229252815246582
I0308 00:52:22.610225 140113510135552 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.892510414123535, loss=1.1910409927368164
I0308 00:53:01.701711 140113501742848 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.9283573627471924, loss=1.2690083980560303
I0308 00:53:40.774608 140113510135552 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.8812568187713623, loss=1.1771785020828247
I0308 00:54:20.433215 140113501742848 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.027863025665283, loss=1.1388660669326782
I0308 00:55:00.533091 140113510135552 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.0377721786499023, loss=1.2474451065063477
2025-03-08 00:55:15.686275: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:55:39.767630 140113501742848 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.2408134937286377, loss=1.163075566291809
I0308 00:56:18.133735 140113510135552 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.005272150039673, loss=1.1298213005065918
I0308 00:56:57.452375 140113501742848 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.967353105545044, loss=1.1951498985290527
I0308 00:57:36.504335 140113510135552 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.00111985206604, loss=1.1986725330352783
I0308 00:58:15.788491 140113501742848 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.1483511924743652, loss=1.185509443283081
I0308 00:58:26.883229 140269360194752 spec.py:321] Evaluating on the training split.
I0308 00:58:37.867552 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 00:58:54.327898 140269360194752 spec.py:349] Evaluating on the test split.
I0308 00:58:56.094334 140269360194752 submission_runner.py:469] Time since start: 55009.95s, 	Step: 126829, 	{'train/accuracy': 0.8191167116165161, 'train/loss': 0.6490090489387512, 'validation/accuracy': 0.7081999778747559, 'validation/loss': 1.2002969980239868, 'validation/num_examples': 50000, 'test/accuracy': 0.5841000080108643, 'test/loss': 1.9353251457214355, 'test/num_examples': 10000, 'score': 51053.50607872009, 'total_duration': 55009.9517827034, 'accumulated_submission_time': 51053.50607872009, 'accumulated_eval_time': 3927.6881256103516, 'accumulated_logging_time': 14.661714553833008}
I0308 00:58:56.167191 140113510135552 logging_writer.py:48] [126829] accumulated_eval_time=3927.69, accumulated_logging_time=14.6617, accumulated_submission_time=51053.5, global_step=126829, preemption_count=0, score=51053.5, test/accuracy=0.5841, test/loss=1.93533, test/num_examples=10000, total_duration=55010, train/accuracy=0.819117, train/loss=0.649009, validation/accuracy=0.7082, validation/loss=1.2003, validation/num_examples=50000
I0308 00:59:24.246643 140113501742848 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.779587745666504, loss=1.0945345163345337
I0308 01:00:03.365378 140113510135552 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.933037519454956, loss=1.1217260360717773
I0308 01:00:42.590136 140113501742848 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.0435104370117188, loss=1.1719110012054443
I0308 01:01:22.063556 140113510135552 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.9315884113311768, loss=1.1538349390029907
I0308 01:02:01.219703 140113501742848 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.0399694442749023, loss=1.0952465534210205
I0308 01:02:40.854075 140113510135552 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.0740973949432373, loss=1.1453931331634521
I0308 01:03:21.093936 140113501742848 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.4298906326293945, loss=1.2155770063400269
2025-03-08 01:03:56.266191: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:04:00.317509 140113510135552 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.4805526733398438, loss=1.1766306161880493
I0308 01:04:39.742824 140113501742848 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.981013536453247, loss=1.1431961059570312
I0308 01:05:18.954422 140113510135552 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.960583448410034, loss=1.188253402709961
I0308 01:05:58.485019 140113501742848 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.8756155967712402, loss=1.1211367845535278
I0308 01:06:38.306337 140113510135552 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.9411187171936035, loss=1.1082977056503296
I0308 01:07:17.375298 140113501742848 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.214717149734497, loss=1.2016770839691162
I0308 01:07:26.497685 140269360194752 spec.py:321] Evaluating on the training split.
I0308 01:07:37.086876 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 01:07:58.697657 140269360194752 spec.py:349] Evaluating on the test split.
I0308 01:08:00.435303 140269360194752 submission_runner.py:469] Time since start: 55554.29s, 	Step: 128124, 	{'train/accuracy': 0.8212690949440002, 'train/loss': 0.6420150399208069, 'validation/accuracy': 0.7077800035476685, 'validation/loss': 1.1788913011550903, 'validation/num_examples': 50000, 'test/accuracy': 0.584600031375885, 'test/loss': 1.8811419010162354, 'test/num_examples': 10000, 'score': 51563.65951156616, 'total_duration': 55554.292761564255, 'accumulated_submission_time': 51563.65951156616, 'accumulated_eval_time': 3961.6257083415985, 'accumulated_logging_time': 14.754802227020264}
I0308 01:08:00.514091 140113510135552 logging_writer.py:48] [128124] accumulated_eval_time=3961.63, accumulated_logging_time=14.7548, accumulated_submission_time=51563.7, global_step=128124, preemption_count=0, score=51563.7, test/accuracy=0.5846, test/loss=1.88114, test/num_examples=10000, total_duration=55554.3, train/accuracy=0.821269, train/loss=0.642015, validation/accuracy=0.70778, validation/loss=1.17889, validation/num_examples=50000
I0308 01:08:30.762048 140113501742848 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.0059213638305664, loss=1.184950828552246
I0308 01:09:09.971874 140113510135552 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.2662880420684814, loss=1.2035915851593018
I0308 01:09:48.954578 140113501742848 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.2356483936309814, loss=1.1447468996047974
I0308 01:10:27.819040 140113510135552 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.1437597274780273, loss=1.179018497467041
I0308 01:11:07.037579 140113501742848 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.462721109390259, loss=1.118242859840393
I0308 01:11:46.395748 140113510135552 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.4656989574432373, loss=1.237383246421814
I0308 01:12:26.247134 140113501742848 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.174922466278076, loss=1.2467906475067139
2025-03-08 01:12:41.912984: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:13:05.903310 140113510135552 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.089268684387207, loss=1.1859714984893799
I0308 01:13:44.650799 140113501742848 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.8913075923919678, loss=1.1247520446777344
I0308 01:14:24.242644 140113510135552 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.8178956508636475, loss=1.065463900566101
I0308 01:15:03.372663 140113501742848 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.2502856254577637, loss=1.1615053415298462
I0308 01:15:42.700457 140113510135552 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.0542006492614746, loss=1.1211638450622559
I0308 01:16:22.017524 140113501742848 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.9488706588745117, loss=1.1170495748519897
I0308 01:16:30.485094 140269360194752 spec.py:321] Evaluating on the training split.
I0308 01:16:41.209539 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 01:17:03.178457 140269360194752 spec.py:349] Evaluating on the test split.
I0308 01:17:04.933096 140269360194752 submission_runner.py:469] Time since start: 56098.79s, 	Step: 129422, 	{'train/accuracy': 0.8216477632522583, 'train/loss': 0.6355151534080505, 'validation/accuracy': 0.7138199806213379, 'validation/loss': 1.1791808605194092, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 1.8831253051757812, 'test/num_examples': 10000, 'score': 52073.43739271164, 'total_duration': 56098.79053258896, 'accumulated_submission_time': 52073.43739271164, 'accumulated_eval_time': 3996.07364821434, 'accumulated_logging_time': 14.870981454849243}
I0308 01:17:05.019866 140113510135552 logging_writer.py:48] [129422] accumulated_eval_time=3996.07, accumulated_logging_time=14.871, accumulated_submission_time=52073.4, global_step=129422, preemption_count=0, score=52073.4, test/accuracy=0.5869, test/loss=1.88313, test/num_examples=10000, total_duration=56098.8, train/accuracy=0.821648, train/loss=0.635515, validation/accuracy=0.71382, validation/loss=1.17918, validation/num_examples=50000
I0308 01:17:36.039645 140113501742848 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.0876522064208984, loss=1.118940830230713
I0308 01:18:14.928957 140113510135552 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.3178517818450928, loss=1.2418068647384644
I0308 01:18:54.292499 140113501742848 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.940838098526001, loss=1.1189788579940796
I0308 01:19:33.882661 140113510135552 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.9567785263061523, loss=1.1373757123947144
I0308 01:20:13.515019 140113501742848 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.2338643074035645, loss=1.1390641927719116
I0308 01:20:53.420395 140113510135552 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.7686479091644287, loss=1.0454083681106567
2025-03-08 01:21:29.801402: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:21:33.182908 140113501742848 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.0011982917785645, loss=1.1131240129470825
I0308 01:22:12.771302 140113510135552 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.1349263191223145, loss=1.1303200721740723
I0308 01:22:52.085492 140113501742848 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.315441608428955, loss=1.1352828741073608
I0308 01:23:30.921676 140113510135552 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.5314078330993652, loss=1.2180557250976562
I0308 01:24:09.984548 140113501742848 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.899075984954834, loss=1.1612064838409424
I0308 01:24:48.990758 140113510135552 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.141019582748413, loss=1.1355936527252197
I0308 01:25:28.170930 140113501742848 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.044940948486328, loss=1.202952265739441
I0308 01:25:35.128145 140269360194752 spec.py:321] Evaluating on the training split.
I0308 01:25:45.984373 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 01:26:09.331161 140269360194752 spec.py:349] Evaluating on the test split.
I0308 01:26:11.084813 140269360194752 submission_runner.py:469] Time since start: 56644.94s, 	Step: 130719, 	{'train/accuracy': 0.8334861397743225, 'train/loss': 0.5989682078361511, 'validation/accuracy': 0.7184000015258789, 'validation/loss': 1.1502035856246948, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.8482716083526611, 'test/num_examples': 10000, 'score': 52583.355182886124, 'total_duration': 56644.942281246185, 'accumulated_submission_time': 52583.355182886124, 'accumulated_eval_time': 4032.0302834510803, 'accumulated_logging_time': 14.991738319396973}
I0308 01:26:11.159907 140113510135552 logging_writer.py:48] [130719] accumulated_eval_time=4032.03, accumulated_logging_time=14.9917, accumulated_submission_time=52583.4, global_step=130719, preemption_count=0, score=52583.4, test/accuracy=0.5927, test/loss=1.84827, test/num_examples=10000, total_duration=56644.9, train/accuracy=0.833486, train/loss=0.598968, validation/accuracy=0.7184, validation/loss=1.1502, validation/num_examples=50000
I0308 01:26:42.984982 140113501742848 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.05279541015625, loss=1.224618911743164
I0308 01:27:22.033895 140113510135552 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.849803924560547, loss=1.1258513927459717
I0308 01:28:01.374508 140113501742848 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.2208094596862793, loss=1.097623348236084
I0308 01:28:40.602694 140113510135552 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.3151350021362305, loss=1.0747696161270142
I0308 01:29:20.167456 140113501742848 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.183753252029419, loss=1.1823104619979858
I0308 01:30:00.080253 140113510135552 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.3330628871917725, loss=1.0951863527297974
2025-03-08 01:30:16.997461: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:30:40.159024 140113501742848 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.2993035316467285, loss=1.1158264875411987
I0308 01:31:19.391644 140113510135552 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.2189064025878906, loss=1.2634210586547852
I0308 01:31:58.974317 140113501742848 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.3127615451812744, loss=1.1980299949645996
I0308 01:32:38.805565 140113510135552 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.227999210357666, loss=1.1525418758392334
I0308 01:33:18.330614 140113501742848 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.1170032024383545, loss=1.0578805208206177
I0308 01:33:57.714959 140113510135552 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.2664241790771484, loss=1.1716718673706055
I0308 01:34:37.377629 140113501742848 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.490963935852051, loss=1.1657129526138306
I0308 01:34:41.246079 140269360194752 spec.py:321] Evaluating on the training split.
I0308 01:34:52.105964 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 01:35:12.061911 140269360194752 spec.py:349] Evaluating on the test split.
I0308 01:35:13.809005 140269360194752 submission_runner.py:469] Time since start: 57187.67s, 	Step: 132011, 	{'train/accuracy': 0.8279256820678711, 'train/loss': 0.6220022439956665, 'validation/accuracy': 0.7115600109100342, 'validation/loss': 1.166405200958252, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.8730454444885254, 'test/num_examples': 10000, 'score': 53093.251718997955, 'total_duration': 57187.666471004486, 'accumulated_submission_time': 53093.251718997955, 'accumulated_eval_time': 4064.59317445755, 'accumulated_logging_time': 15.10022497177124}
I0308 01:35:13.923598 140113510135552 logging_writer.py:48] [132011] accumulated_eval_time=4064.59, accumulated_logging_time=15.1002, accumulated_submission_time=53093.3, global_step=132011, preemption_count=0, score=53093.3, test/accuracy=0.5862, test/loss=1.87305, test/num_examples=10000, total_duration=57187.7, train/accuracy=0.827926, train/loss=0.622002, validation/accuracy=0.71156, validation/loss=1.16641, validation/num_examples=50000
I0308 01:35:49.467343 140113501742848 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.20538592338562, loss=1.1104469299316406
I0308 01:36:29.105130 140113510135552 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.0751564502716064, loss=1.096003770828247
I0308 01:37:08.693867 140113501742848 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.0627095699310303, loss=1.11903977394104
I0308 01:37:47.987492 140113510135552 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.059425115585327, loss=1.1981301307678223
I0308 01:38:27.901606 140113501742848 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.0967085361480713, loss=1.135548710823059
2025-03-08 01:39:05.495655: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:39:07.991929 140113510135552 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.32171368598938, loss=1.0994945764541626
I0308 01:39:47.346251 140113501742848 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.077162981033325, loss=1.1589641571044922
I0308 01:40:26.737480 140113510135552 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2507882118225098, loss=1.0510767698287964
I0308 01:41:05.789189 140113501742848 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.0826168060302734, loss=1.1028852462768555
I0308 01:41:44.839327 140113510135552 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.1593756675720215, loss=1.17202627658844
I0308 01:42:24.313749 140113501742848 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.349940299987793, loss=1.12837815284729
I0308 01:43:03.627000 140113510135552 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.2518606185913086, loss=1.0347909927368164
I0308 01:43:42.699907 140113501742848 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.154306650161743, loss=1.0951311588287354
I0308 01:43:43.952736 140269360194752 spec.py:321] Evaluating on the training split.
I0308 01:43:54.677665 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 01:44:19.207482 140269360194752 spec.py:349] Evaluating on the test split.
I0308 01:44:20.951344 140269360194752 submission_runner.py:469] Time since start: 57734.81s, 	Step: 133304, 	{'train/accuracy': 0.83500075340271, 'train/loss': 0.5890035033226013, 'validation/accuracy': 0.7152799963951111, 'validation/loss': 1.1624947786331177, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.8634424209594727, 'test/num_examples': 10000, 'score': 53603.08537530899, 'total_duration': 57734.80880069733, 'accumulated_submission_time': 53603.08537530899, 'accumulated_eval_time': 4101.591737031937, 'accumulated_logging_time': 15.255291223526001}
I0308 01:44:21.043544 140113510135552 logging_writer.py:48] [133304] accumulated_eval_time=4101.59, accumulated_logging_time=15.2553, accumulated_submission_time=53603.1, global_step=133304, preemption_count=0, score=53603.1, test/accuracy=0.592, test/loss=1.86344, test/num_examples=10000, total_duration=57734.8, train/accuracy=0.835001, train/loss=0.589004, validation/accuracy=0.71528, validation/loss=1.16249, validation/num_examples=50000
I0308 01:44:59.270601 140113501742848 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.2547426223754883, loss=1.0553960800170898
I0308 01:45:38.686320 140113510135552 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.2219998836517334, loss=1.075697422027588
I0308 01:46:18.212598 140113501742848 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.2373440265655518, loss=1.1808401346206665
I0308 01:46:57.601990 140113510135552 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.182492733001709, loss=1.2210657596588135
I0308 01:47:37.304815 140113501742848 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.288618564605713, loss=1.127906084060669
2025-03-08 01:47:55.036209: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:48:17.163827 140113510135552 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.3318889141082764, loss=1.206501841545105
I0308 01:48:56.438413 140113501742848 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.2321279048919678, loss=1.0911896228790283
I0308 01:49:36.178504 140113510135552 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.12304949760437, loss=1.024664282798767
I0308 01:50:15.421921 140113501742848 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.5337517261505127, loss=1.1760436296463013
I0308 01:50:54.694823 140113510135552 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.681241750717163, loss=1.1407995223999023
I0308 01:51:34.197805 140113501742848 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.2106802463531494, loss=1.1049940586090088
I0308 01:52:13.115885 140113510135552 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.5166525840759277, loss=1.1942957639694214
I0308 01:52:51.271270 140269360194752 spec.py:321] Evaluating on the training split.
I0308 01:53:02.341369 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 01:53:24.269516 140269360194752 spec.py:349] Evaluating on the test split.
I0308 01:53:26.021466 140269360194752 submission_runner.py:469] Time since start: 58279.88s, 	Step: 134599, 	{'train/accuracy': 0.8369140625, 'train/loss': 0.581257700920105, 'validation/accuracy': 0.7176199555397034, 'validation/loss': 1.159517526626587, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8773891925811768, 'test/num_examples': 10000, 'score': 54113.132682561874, 'total_duration': 58279.87891817093, 'accumulated_submission_time': 54113.132682561874, 'accumulated_eval_time': 4136.3418889045715, 'accumulated_logging_time': 15.37397027015686}
I0308 01:53:26.169663 140113501742848 logging_writer.py:48] [134599] accumulated_eval_time=4136.34, accumulated_logging_time=15.374, accumulated_submission_time=54113.1, global_step=134599, preemption_count=0, score=54113.1, test/accuracy=0.5907, test/loss=1.87739, test/num_examples=10000, total_duration=58279.9, train/accuracy=0.836914, train/loss=0.581258, validation/accuracy=0.71762, validation/loss=1.15952, validation/num_examples=50000
I0308 01:53:26.925513 140113510135552 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.1849420070648193, loss=1.133406162261963
I0308 01:54:06.397510 140113501742848 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.2908196449279785, loss=1.1889593601226807
I0308 01:54:45.792877 140113510135552 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.3429720401763916, loss=1.107252597808838
I0308 01:55:25.250738 140113501742848 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.9120471477508545, loss=0.9713793992996216
I0308 01:56:05.196817 140113510135552 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.199288845062256, loss=1.06523597240448
2025-03-08 01:56:43.223025: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 01:56:44.638062 140113501742848 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.3072850704193115, loss=1.186010718345642
I0308 01:57:24.343884 140113510135552 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.0889201164245605, loss=1.1413486003875732
I0308 01:58:03.559052 140113501742848 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.142298460006714, loss=1.0863330364227295
I0308 01:58:42.674330 140113510135552 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.1913957595825195, loss=1.1103991270065308
I0308 01:59:21.509086 140113501742848 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.3700456619262695, loss=1.1530243158340454
I0308 02:00:00.665147 140113510135552 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.2183120250701904, loss=1.076020359992981
I0308 02:00:39.899827 140113501742848 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.307708501815796, loss=1.091872215270996
I0308 02:01:19.524469 140113510135552 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.4064905643463135, loss=1.0727723836898804
I0308 02:01:56.189873 140269360194752 spec.py:321] Evaluating on the training split.
I0308 02:02:07.206316 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 02:02:27.983159 140269360194752 spec.py:349] Evaluating on the test split.
I0308 02:02:29.735132 140269360194752 submission_runner.py:469] Time since start: 58823.59s, 	Step: 135895, 	{'train/accuracy': 0.8416972160339355, 'train/loss': 0.5584900379180908, 'validation/accuracy': 0.7178399562835693, 'validation/loss': 1.1549780368804932, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8753383159637451, 'test/num_examples': 10000, 'score': 54622.9719285965, 'total_duration': 58823.592566251755, 'accumulated_submission_time': 54622.9719285965, 'accumulated_eval_time': 4169.887081384659, 'accumulated_logging_time': 15.546517372131348}
I0308 02:02:29.811979 140113501742848 logging_writer.py:48] [135895] accumulated_eval_time=4169.89, accumulated_logging_time=15.5465, accumulated_submission_time=54623, global_step=135895, preemption_count=0, score=54623, test/accuracy=0.5907, test/loss=1.87534, test/num_examples=10000, total_duration=58823.6, train/accuracy=0.841697, train/loss=0.55849, validation/accuracy=0.71784, validation/loss=1.15498, validation/num_examples=50000
I0308 02:02:32.152299 140113510135552 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.2270755767822266, loss=1.0572280883789062
I0308 02:03:11.590781 140113501742848 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.1968345642089844, loss=1.0490341186523438
I0308 02:03:51.106177 140113510135552 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.13348650932312, loss=1.087760329246521
I0308 02:04:30.907680 140113501742848 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.4297068119049072, loss=1.148841381072998
I0308 02:05:10.788016 140113510135552 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.252232074737549, loss=1.1220859289169312
2025-03-08 02:05:29.784810: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:05:50.505062 140113501742848 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.3649802207946777, loss=1.1850662231445312
I0308 02:06:29.958534 140113510135552 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.1893718242645264, loss=1.0827471017837524
I0308 02:07:08.789786 140113501742848 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.588475465774536, loss=1.1934502124786377
I0308 02:07:47.436563 140113510135552 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.18005108833313, loss=1.0369101762771606
I0308 02:08:26.507449 140113501742848 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.216731309890747, loss=1.095387578010559
I0308 02:09:05.521627 140113510135552 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.1050655841827393, loss=1.1199285984039307
I0308 02:09:44.465463 140113501742848 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.1296422481536865, loss=1.0019962787628174
I0308 02:10:23.099338 140113510135552 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.230140447616577, loss=1.0930272340774536
I0308 02:11:00.060881 140269360194752 spec.py:321] Evaluating on the training split.
I0308 02:11:10.637335 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 02:11:32.921503 140269360194752 spec.py:349] Evaluating on the test split.
I0308 02:11:34.679629 140269360194752 submission_runner.py:469] Time since start: 59368.54s, 	Step: 137196, 	{'train/accuracy': 0.844168484210968, 'train/loss': 0.5514883399009705, 'validation/accuracy': 0.7198799848556519, 'validation/loss': 1.1460130214691162, 'validation/num_examples': 50000, 'test/accuracy': 0.5911000370979309, 'test/loss': 1.8841530084609985, 'test/num_examples': 10000, 'score': 55133.01143240929, 'total_duration': 59368.53708696365, 'accumulated_submission_time': 55133.01143240929, 'accumulated_eval_time': 4204.505786418915, 'accumulated_logging_time': 15.671289682388306}
I0308 02:11:34.788978 140113501742848 logging_writer.py:48] [137196] accumulated_eval_time=4204.51, accumulated_logging_time=15.6713, accumulated_submission_time=55133, global_step=137196, preemption_count=0, score=55133, test/accuracy=0.5911, test/loss=1.88415, test/num_examples=10000, total_duration=59368.5, train/accuracy=0.844168, train/loss=0.551488, validation/accuracy=0.71988, validation/loss=1.14601, validation/num_examples=50000
I0308 02:11:36.908167 140113510135552 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.3016412258148193, loss=1.105077862739563
I0308 02:12:16.475171 140113501742848 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.3257062435150146, loss=1.1230058670043945
I0308 02:12:55.433235 140113510135552 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.738372564315796, loss=1.057817816734314
I0308 02:13:35.663265 140113501742848 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.190553665161133, loss=1.0435705184936523
2025-03-08 02:14:14.825510: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:14:15.493789 140113510135552 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.463364839553833, loss=1.1293482780456543
I0308 02:14:54.861885 140113501742848 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.323993682861328, loss=1.1117498874664307
I0308 02:15:34.073463 140113510135552 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.5026843547821045, loss=1.0274516344070435
I0308 02:16:13.622017 140113501742848 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.4631540775299072, loss=1.1249747276306152
I0308 02:16:52.747530 140113510135552 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.4271926879882812, loss=1.1571621894836426
I0308 02:17:32.557001 140113501742848 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.4087793827056885, loss=1.15236234664917
I0308 02:18:12.018558 140113510135552 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.2599263191223145, loss=1.0297421216964722
I0308 02:18:51.277114 140113501742848 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.32474684715271, loss=1.033082365989685
I0308 02:19:31.246984 140113510135552 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.454784393310547, loss=1.0796490907669067
I0308 02:20:04.977629 140269360194752 spec.py:321] Evaluating on the training split.
I0308 02:20:15.772208 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 02:20:39.739676 140269360194752 spec.py:349] Evaluating on the test split.
I0308 02:20:41.488421 140269360194752 submission_runner.py:469] Time since start: 59915.35s, 	Step: 138486, 	{'train/accuracy': 0.8429527878761292, 'train/loss': 0.5433604121208191, 'validation/accuracy': 0.7161399722099304, 'validation/loss': 1.1705085039138794, 'validation/num_examples': 50000, 'test/accuracy': 0.593000054359436, 'test/loss': 1.8821163177490234, 'test/num_examples': 10000, 'score': 55643.017302036285, 'total_duration': 59915.34588932991, 'accumulated_submission_time': 55643.017302036285, 'accumulated_eval_time': 4241.016545295715, 'accumulated_logging_time': 15.809410333633423}
I0308 02:20:41.592664 140113501742848 logging_writer.py:48] [138486] accumulated_eval_time=4241.02, accumulated_logging_time=15.8094, accumulated_submission_time=55643, global_step=138486, preemption_count=0, score=55643, test/accuracy=0.593, test/loss=1.88212, test/num_examples=10000, total_duration=59915.3, train/accuracy=0.842953, train/loss=0.54336, validation/accuracy=0.71614, validation/loss=1.17051, validation/num_examples=50000
I0308 02:20:47.532063 140113510135552 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.421539545059204, loss=1.0899722576141357
I0308 02:21:27.141660 140113501742848 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.0994279384613037, loss=1.0414780378341675
I0308 02:22:06.644188 140113510135552 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.1579842567443848, loss=1.0509800910949707
I0308 02:22:46.969362 140113501742848 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.760890245437622, loss=1.1686149835586548
2025-03-08 02:23:07.132929: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:23:27.146937 140113510135552 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.271916151046753, loss=1.0344613790512085
I0308 02:24:06.434615 140113501742848 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.3781518936157227, loss=1.0696508884429932
I0308 02:24:46.455028 140113510135552 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.1536567211151123, loss=0.9935362935066223
I0308 02:25:26.396407 140113501742848 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.5951170921325684, loss=1.1162402629852295
I0308 02:26:06.169315 140113510135552 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.2781291007995605, loss=1.0766366720199585
I0308 02:26:46.220946 140113501742848 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.443098545074463, loss=1.1486204862594604
I0308 02:27:26.618809 140113510135552 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.762840747833252, loss=1.2131069898605347
I0308 02:28:06.726505 140113501742848 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.4585797786712646, loss=1.0626291036605835
I0308 02:28:46.674393 140113510135552 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.3666930198669434, loss=1.1037590503692627
I0308 02:29:11.798120 140269360194752 spec.py:321] Evaluating on the training split.
I0308 02:29:22.984988 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 02:29:45.105226 140269360194752 spec.py:349] Evaluating on the test split.
I0308 02:29:46.846282 140269360194752 submission_runner.py:469] Time since start: 60460.70s, 	Step: 139763, 	{'train/accuracy': 0.8525190949440002, 'train/loss': 0.5119024515151978, 'validation/accuracy': 0.7247799634933472, 'validation/loss': 1.1486928462982178, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.8840183019638062, 'test/num_examples': 10000, 'score': 56153.03397631645, 'total_duration': 60460.70373916626, 'accumulated_submission_time': 56153.03397631645, 'accumulated_eval_time': 4276.064665317535, 'accumulated_logging_time': 15.949503660202026}
I0308 02:29:47.006867 140113501742848 logging_writer.py:48] [139763] accumulated_eval_time=4276.06, accumulated_logging_time=15.9495, accumulated_submission_time=56153, global_step=139763, preemption_count=0, score=56153, test/accuracy=0.6007, test/loss=1.88402, test/num_examples=10000, total_duration=60460.7, train/accuracy=0.852519, train/loss=0.511902, validation/accuracy=0.72478, validation/loss=1.14869, validation/num_examples=50000
I0308 02:30:02.122230 140113510135552 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.532073497772217, loss=1.0642057657241821
I0308 02:30:42.109913 140113501742848 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.431964159011841, loss=1.1032098531723022
I0308 02:31:22.515642 140113510135552 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.304081678390503, loss=1.013206958770752
I0308 02:32:02.467112 140113501742848 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.411940813064575, loss=1.0374304056167603
2025-03-08 02:32:06.292057: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:32:43.217408 140113510135552 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.729318618774414, loss=1.034691572189331
I0308 02:33:22.960406 140113501742848 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.6165316104888916, loss=0.98974609375
I0308 02:34:02.662358 140113510135552 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.5776731967926025, loss=1.1370041370391846
I0308 02:34:42.911779 140113501742848 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.3656022548675537, loss=1.0724378824234009
I0308 02:35:23.314895 140113510135552 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.3729629516601562, loss=1.1160482168197632
I0308 02:36:03.217790 140113501742848 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.250352382659912, loss=1.0209120512008667
I0308 02:36:42.866402 140113510135552 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.61189603805542, loss=1.1335862874984741
I0308 02:37:22.821470 140113501742848 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.3686680793762207, loss=1.0235815048217773
I0308 02:38:02.773298 140113510135552 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.3015999794006348, loss=0.9719517230987549
I0308 02:38:16.946897 140269360194752 spec.py:321] Evaluating on the training split.
I0308 02:38:27.759424 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 02:38:49.679943 140269360194752 spec.py:349] Evaluating on the test split.
I0308 02:38:51.405889 140269360194752 submission_runner.py:469] Time since start: 61005.26s, 	Step: 141037, 	{'train/accuracy': 0.8427335619926453, 'train/loss': 0.5529648661613464, 'validation/accuracy': 0.7173599600791931, 'validation/loss': 1.163743495941162, 'validation/num_examples': 50000, 'test/accuracy': 0.59170001745224, 'test/loss': 1.8985487222671509, 'test/num_examples': 10000, 'score': 56662.78712630272, 'total_duration': 61005.26335000992, 'accumulated_submission_time': 56662.78712630272, 'accumulated_eval_time': 4310.523617506027, 'accumulated_logging_time': 16.143863677978516}
I0308 02:38:51.517229 140113501742848 logging_writer.py:48] [141037] accumulated_eval_time=4310.52, accumulated_logging_time=16.1439, accumulated_submission_time=56662.8, global_step=141037, preemption_count=0, score=56662.8, test/accuracy=0.5917, test/loss=1.89855, test/num_examples=10000, total_duration=61005.3, train/accuracy=0.842734, train/loss=0.552965, validation/accuracy=0.71736, validation/loss=1.16374, validation/num_examples=50000
I0308 02:39:16.729449 140113510135552 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.8718717098236084, loss=1.0822614431381226
I0308 02:39:56.487043 140113501742848 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.4414000511169434, loss=1.1560354232788086
I0308 02:40:36.856431 140113510135552 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.4575562477111816, loss=0.9903532266616821
2025-03-08 02:40:57.825937: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:41:17.047672 140113501742848 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.1998322010040283, loss=1.042143702507019
I0308 02:41:56.667431 140113510135552 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.3602054119110107, loss=1.0493648052215576
I0308 02:42:35.760944 140113501742848 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.5102291107177734, loss=1.050389289855957
I0308 02:43:15.424270 140113510135552 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.3506577014923096, loss=1.0503431558609009
I0308 02:43:54.913524 140113501742848 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.4949839115142822, loss=1.0598034858703613
I0308 02:44:34.940165 140113510135552 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.5238072872161865, loss=0.9750880599021912
I0308 02:45:15.025476 140113501742848 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.3664166927337646, loss=1.106328010559082
I0308 02:45:55.202315 140113510135552 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.738689661026001, loss=1.0686044692993164
I0308 02:46:35.450628 140113501742848 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.5784404277801514, loss=1.0621743202209473
I0308 02:47:15.920309 140113510135552 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.5675208568573, loss=1.0570356845855713
I0308 02:47:21.453191 140269360194752 spec.py:321] Evaluating on the training split.
I0308 02:47:32.340665 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 02:47:52.594047 140269360194752 spec.py:349] Evaluating on the test split.
I0308 02:47:54.320719 140269360194752 submission_runner.py:469] Time since start: 61548.18s, 	Step: 142315, 	{'train/accuracy': 0.8552893400192261, 'train/loss': 0.5058260560035706, 'validation/accuracy': 0.7188999652862549, 'validation/loss': 1.1465741395950317, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.8645789623260498, 'test/num_examples': 10000, 'score': 57172.52275586128, 'total_duration': 61548.17818212509, 'accumulated_submission_time': 57172.52275586128, 'accumulated_eval_time': 4343.391114234924, 'accumulated_logging_time': 16.300902605056763}
I0308 02:47:54.460772 140113501742848 logging_writer.py:48] [142315] accumulated_eval_time=4343.39, accumulated_logging_time=16.3009, accumulated_submission_time=57172.5, global_step=142315, preemption_count=0, score=57172.5, test/accuracy=0.5955, test/loss=1.86458, test/num_examples=10000, total_duration=61548.2, train/accuracy=0.855289, train/loss=0.505826, validation/accuracy=0.7189, validation/loss=1.14657, validation/num_examples=50000
I0308 02:48:28.797065 140113510135552 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.254047155380249, loss=0.9561185240745544
I0308 02:49:09.108138 140113501742848 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.342618465423584, loss=1.0336030721664429
I0308 02:49:49.427378 140113510135552 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.8312113285064697, loss=1.0567957162857056
2025-03-08 02:49:51.688115: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:50:29.627054 140113501742848 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.548504114151001, loss=1.0220869779586792
I0308 02:51:09.286775 140113510135552 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.4785351753234863, loss=1.0556824207305908
I0308 02:51:48.569101 140113501742848 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.497256278991699, loss=1.0267903804779053
I0308 02:52:27.659647 140113510135552 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.637699842453003, loss=1.0674684047698975
I0308 02:53:07.496498 140113501742848 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.473149061203003, loss=1.0598530769348145
I0308 02:53:47.422651 140113510135552 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.693084239959717, loss=0.9999113082885742
I0308 02:54:27.084356 140113501742848 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.678901433944702, loss=1.0592879056930542
I0308 02:55:07.021061 140113510135552 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.6490283012390137, loss=0.9865152835845947
I0308 02:55:47.322817 140113501742848 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.6293089389801025, loss=1.1020066738128662
I0308 02:56:24.465852 140269360194752 spec.py:321] Evaluating on the training split.
I0308 02:56:35.022087 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 02:56:58.682571 140269360194752 spec.py:349] Evaluating on the test split.
I0308 02:57:00.414227 140269360194752 submission_runner.py:469] Time since start: 62094.27s, 	Step: 143593, 	{'train/accuracy': 0.8539739847183228, 'train/loss': 0.5155287384986877, 'validation/accuracy': 0.7179399728775024, 'validation/loss': 1.158629298210144, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8788204193115234, 'test/num_examples': 10000, 'score': 57682.3403673172, 'total_duration': 62094.271691799164, 'accumulated_submission_time': 57682.3403673172, 'accumulated_eval_time': 4379.339458227158, 'accumulated_logging_time': 16.476032495498657}
I0308 02:57:00.547245 140113510135552 logging_writer.py:48] [143593] accumulated_eval_time=4379.34, accumulated_logging_time=16.476, accumulated_submission_time=57682.3, global_step=143593, preemption_count=0, score=57682.3, test/accuracy=0.5933, test/loss=1.87882, test/num_examples=10000, total_duration=62094.3, train/accuracy=0.853974, train/loss=0.515529, validation/accuracy=0.71794, validation/loss=1.15863, validation/num_examples=50000
I0308 02:57:03.707039 140113501742848 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.420240879058838, loss=0.9676461219787598
I0308 02:57:43.687232 140113510135552 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.289886474609375, loss=0.9844344258308411
I0308 02:58:23.339407 140113501742848 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.4343018531799316, loss=1.0388236045837402
2025-03-08 02:58:49.192142: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:59:04.244334 140113510135552 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.5357534885406494, loss=0.9730687737464905
I0308 02:59:44.625466 140113501742848 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.4806885719299316, loss=1.0739355087280273
I0308 03:00:24.731492 140113510135552 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.514338254928589, loss=0.9512280225753784
I0308 03:01:04.579108 140113501742848 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.963221788406372, loss=1.0552334785461426
I0308 03:01:44.476500 140113510135552 logging_writer.py:48] [144300] global_step=144300, grad_norm=4.0274338722229, loss=1.086840271949768
I0308 03:02:23.978438 140113501742848 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.823298215866089, loss=1.031830906867981
I0308 03:03:03.864355 140113510135552 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.911418914794922, loss=1.0061047077178955
I0308 03:03:43.423934 140113501742848 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.520905017852783, loss=0.9888100624084473
I0308 03:04:23.188598 140113510135552 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.444246292114258, loss=0.9845272898674011
I0308 03:05:03.146084 140113501742848 logging_writer.py:48] [144800] global_step=144800, grad_norm=4.12065315246582, loss=1.075172781944275
I0308 03:05:30.440793 140269360194752 spec.py:321] Evaluating on the training split.
I0308 03:05:41.184078 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 03:06:03.653291 140269360194752 spec.py:349] Evaluating on the test split.
I0308 03:06:05.482277 140269360194752 submission_runner.py:469] Time since start: 62639.34s, 	Step: 144869, 	{'train/accuracy': 0.8717514276504517, 'train/loss': 0.45605209469795227, 'validation/accuracy': 0.7270199656486511, 'validation/loss': 1.1128953695297241, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.8197840452194214, 'test/num_examples': 10000, 'score': 58192.048651218414, 'total_duration': 62639.33973765373, 'accumulated_submission_time': 58192.048651218414, 'accumulated_eval_time': 4414.380907058716, 'accumulated_logging_time': 16.63759422302246}
I0308 03:06:05.615375 140113510135552 logging_writer.py:48] [144869] accumulated_eval_time=4414.38, accumulated_logging_time=16.6376, accumulated_submission_time=58192, global_step=144869, preemption_count=0, score=58192, test/accuracy=0.6024, test/loss=1.81978, test/num_examples=10000, total_duration=62639.3, train/accuracy=0.871751, train/loss=0.456052, validation/accuracy=0.72702, validation/loss=1.1129, validation/num_examples=50000
I0308 03:06:18.354312 140113501742848 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.4106409549713135, loss=0.898409366607666
I0308 03:06:58.131531 140113510135552 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.654277801513672, loss=0.9568789005279541
I0308 03:07:38.427640 140113501742848 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.7014358043670654, loss=0.9180042743682861
2025-03-08 03:07:43.050510: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:08:18.385128 140113510135552 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.376917600631714, loss=1.0142226219177246
I0308 03:08:58.924429 140113501742848 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.5511367321014404, loss=1.0233979225158691
I0308 03:09:38.200847 140113510135552 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.968485116958618, loss=1.0164682865142822
I0308 03:10:18.586768 140113501742848 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.477532148361206, loss=0.9996680617332458
I0308 03:10:58.667622 140113510135552 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.8677427768707275, loss=0.997357964515686
I0308 03:11:38.449413 140113501742848 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.754305124282837, loss=1.0476255416870117
I0308 03:12:17.871093 140113510135552 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.596750259399414, loss=0.9507359862327576
I0308 03:12:57.532663 140113501742848 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.8680179119110107, loss=1.0476410388946533
I0308 03:13:37.670926 140113510135552 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.6167218685150146, loss=1.052831768989563
I0308 03:14:17.655111 140113501742848 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.5985329151153564, loss=0.9836759567260742
I0308 03:14:35.597284 140269360194752 spec.py:321] Evaluating on the training split.
I0308 03:14:46.332569 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 03:15:05.206253 140269360194752 spec.py:349] Evaluating on the test split.
I0308 03:15:06.950500 140269360194752 submission_runner.py:469] Time since start: 63180.81s, 	Step: 146146, 	{'train/accuracy': 0.8713328838348389, 'train/loss': 0.45249906182289124, 'validation/accuracy': 0.7279599905014038, 'validation/loss': 1.1116821765899658, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.8199840784072876, 'test/num_examples': 10000, 'score': 58701.83473944664, 'total_duration': 63180.807953596115, 'accumulated_submission_time': 58701.83473944664, 'accumulated_eval_time': 4445.734078407288, 'accumulated_logging_time': 16.812229871749878}
I0308 03:15:07.022514 140113510135552 logging_writer.py:48] [146146] accumulated_eval_time=4445.73, accumulated_logging_time=16.8122, accumulated_submission_time=58701.8, global_step=146146, preemption_count=0, score=58701.8, test/accuracy=0.6006, test/loss=1.81998, test/num_examples=10000, total_duration=63180.8, train/accuracy=0.871333, train/loss=0.452499, validation/accuracy=0.72796, validation/loss=1.11168, validation/num_examples=50000
I0308 03:15:28.841973 140113501742848 logging_writer.py:48] [146200] global_step=146200, grad_norm=4.207067966461182, loss=1.1047353744506836
I0308 03:16:08.164582 140113510135552 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.699190855026245, loss=1.0090843439102173
2025-03-08 03:16:34.246360: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:16:48.692215 140113501742848 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.4649765491485596, loss=0.9153962135314941
I0308 03:17:28.101229 140113510135552 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.022177696228027, loss=1.0177552700042725
I0308 03:18:07.602895 140113501742848 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.7148008346557617, loss=0.9887162446975708
I0308 03:18:47.482247 140113510135552 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.7652394771575928, loss=0.8939451575279236
I0308 03:19:27.117403 140113501742848 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.380582571029663, loss=0.8819249868392944
I0308 03:20:06.692545 140113510135552 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.4592275619506836, loss=1.0072780847549438
I0308 03:20:46.367375 140113501742848 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.7670884132385254, loss=1.0551183223724365
I0308 03:21:26.898424 140113510135552 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.79256010055542, loss=0.893290638923645
I0308 03:22:06.943680 140113501742848 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.638686180114746, loss=0.9558132886886597
I0308 03:22:46.990287 140113510135552 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.7911341190338135, loss=1.0956813097000122
I0308 03:23:27.139736 140113501742848 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.781787395477295, loss=1.0801304578781128
I0308 03:23:37.200590 140269360194752 spec.py:321] Evaluating on the training split.
I0308 03:23:48.372733 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 03:24:10.493270 140269360194752 spec.py:349] Evaluating on the test split.
I0308 03:24:12.235536 140269360194752 submission_runner.py:469] Time since start: 63726.09s, 	Step: 147426, 	{'train/accuracy': 0.8796436190605164, 'train/loss': 0.4141806364059448, 'validation/accuracy': 0.7302199602127075, 'validation/loss': 1.1202220916748047, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.8784282207489014, 'test/num_examples': 10000, 'score': 59211.79710316658, 'total_duration': 63726.09299612045, 'accumulated_submission_time': 59211.79710316658, 'accumulated_eval_time': 4480.768987417221, 'accumulated_logging_time': 16.9438316822052}
I0308 03:24:12.305202 140113510135552 logging_writer.py:48] [147426] accumulated_eval_time=4480.77, accumulated_logging_time=16.9438, accumulated_submission_time=59211.8, global_step=147426, preemption_count=0, score=59211.8, test/accuracy=0.6055, test/loss=1.87843, test/num_examples=10000, total_duration=63726.1, train/accuracy=0.879644, train/loss=0.414181, validation/accuracy=0.73022, validation/loss=1.12022, validation/num_examples=50000
I0308 03:24:41.877885 140113501742848 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.890904188156128, loss=0.947507381439209
I0308 03:25:21.482850 140113510135552 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.855741500854492, loss=1.0342302322387695
2025-03-08 03:25:28.092161: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:26:02.048061 140113501742848 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.8200910091400146, loss=0.9705245494842529
I0308 03:26:41.932059 140113510135552 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.873749017715454, loss=1.0185259580612183
I0308 03:27:21.846789 140113501742848 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.4377715587615967, loss=0.977496862411499
I0308 03:28:01.917177 140113510135552 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.604454517364502, loss=0.9578253030776978
I0308 03:28:41.665360 140113501742848 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.823983669281006, loss=1.0068773031234741
I0308 03:29:21.473577 140113510135552 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.5407285690307617, loss=0.8896802663803101
I0308 03:30:01.480589 140113501742848 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.916151285171509, loss=0.9036642909049988
I0308 03:30:41.243163 140113510135552 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.761397123336792, loss=0.9958497881889343
I0308 03:31:21.153328 140113501742848 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.590669870376587, loss=0.9048138856887817
I0308 03:32:00.981514 140113510135552 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.9264230728149414, loss=1.0180087089538574
I0308 03:32:40.941124 140113501742848 logging_writer.py:48] [148700] global_step=148700, grad_norm=4.074453353881836, loss=0.9710040092468262
I0308 03:32:42.627802 140269360194752 spec.py:321] Evaluating on the training split.
I0308 03:32:53.871297 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 03:33:13.078102 140269360194752 spec.py:349] Evaluating on the test split.
I0308 03:33:14.819467 140269360194752 submission_runner.py:469] Time since start: 64268.68s, 	Step: 148705, 	{'train/accuracy': 0.8806600570678711, 'train/loss': 0.4179016351699829, 'validation/accuracy': 0.728119969367981, 'validation/loss': 1.1285237073898315, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.8571628332138062, 'test/num_examples': 10000, 'score': 59721.9285428524, 'total_duration': 64268.67691755295, 'accumulated_submission_time': 59721.9285428524, 'accumulated_eval_time': 4512.960601806641, 'accumulated_logging_time': 17.049315690994263}
I0308 03:33:14.903343 140113510135552 logging_writer.py:48] [148705] accumulated_eval_time=4512.96, accumulated_logging_time=17.0493, accumulated_submission_time=59721.9, global_step=148705, preemption_count=0, score=59721.9, test/accuracy=0.6041, test/loss=1.85716, test/num_examples=10000, total_duration=64268.7, train/accuracy=0.88066, train/loss=0.417902, validation/accuracy=0.72812, validation/loss=1.12852, validation/num_examples=50000
I0308 03:33:52.993432 140113501742848 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.6726174354553223, loss=1.0014842748641968
2025-03-08 03:34:17.847516: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:34:32.876357 140113510135552 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.7456002235412598, loss=0.9974154233932495
I0308 03:35:13.069875 140113501742848 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.9825327396392822, loss=0.9437830448150635
I0308 03:35:52.804993 140113510135552 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.5718116760253906, loss=0.945676326751709
I0308 03:36:32.092335 140113501742848 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.7189388275146484, loss=0.9507669806480408
I0308 03:37:11.772098 140113510135552 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.751729726791382, loss=1.0168992280960083
I0308 03:37:51.753215 140113501742848 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.720454692840576, loss=1.1122806072235107
I0308 03:38:31.508211 140113510135552 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.59002685546875, loss=1.0180779695510864
I0308 03:39:11.257266 140113501742848 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.903440237045288, loss=0.9911472797393799
I0308 03:39:50.885850 140113510135552 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.71399188041687, loss=0.9758082628250122
I0308 03:40:30.885645 140113501742848 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.8876562118530273, loss=0.9279723763465881
I0308 03:41:11.488178 140113510135552 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.5552783012390137, loss=0.9771955609321594
I0308 03:41:45.030711 140269360194752 spec.py:321] Evaluating on the training split.
I0308 03:41:56.046578 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 03:42:18.658227 140269360194752 spec.py:349] Evaluating on the test split.
I0308 03:42:20.397283 140269360194752 submission_runner.py:469] Time since start: 64814.25s, 	Step: 149985, 	{'train/accuracy': 0.8879743218421936, 'train/loss': 0.3844203054904938, 'validation/accuracy': 0.7346000075340271, 'validation/loss': 1.0968949794769287, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.8292913436889648, 'test/num_examples': 10000, 'score': 60231.869604349136, 'total_duration': 64814.25472855568, 'accumulated_submission_time': 60231.869604349136, 'accumulated_eval_time': 4548.327118396759, 'accumulated_logging_time': 17.165669202804565}
I0308 03:42:20.546028 140113501742848 logging_writer.py:48] [149985] accumulated_eval_time=4548.33, accumulated_logging_time=17.1657, accumulated_submission_time=60231.9, global_step=149985, preemption_count=0, score=60231.9, test/accuracy=0.605, test/loss=1.82929, test/num_examples=10000, total_duration=64814.3, train/accuracy=0.887974, train/loss=0.38442, validation/accuracy=0.7346, validation/loss=1.09689, validation/num_examples=50000
I0308 03:42:27.010841 140113510135552 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.803565263748169, loss=0.950879693031311
I0308 03:43:06.540452 140113501742848 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.9501845836639404, loss=0.9849994778633118
2025-03-08 03:43:10.775687: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:43:46.391144 140113510135552 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.6889915466308594, loss=0.9538451433181763
I0308 03:44:25.855400 140113501742848 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.5250473022460938, loss=0.8687152862548828
I0308 03:45:05.400472 140113510135552 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.9806809425354004, loss=0.9516366720199585
I0308 03:45:44.642773 140113501742848 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.8971002101898193, loss=0.9080798625946045
I0308 03:46:24.284769 140113510135552 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.9346721172332764, loss=0.9143810272216797
I0308 03:47:04.045603 140113501742848 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.574150562286377, loss=0.9285311102867126
I0308 03:47:43.773576 140113510135552 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.426030397415161, loss=0.8757164478302002
I0308 03:48:23.289019 140113501742848 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.9250102043151855, loss=0.9884206056594849
I0308 03:49:03.113366 140113510135552 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.046815395355225, loss=0.963325023651123
I0308 03:49:42.597445 140113501742848 logging_writer.py:48] [151100] global_step=151100, grad_norm=4.203383922576904, loss=0.9692071676254272
I0308 03:50:22.155585 140113510135552 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.8703415393829346, loss=0.9534966349601746
I0308 03:50:50.461692 140269360194752 spec.py:321] Evaluating on the training split.
I0308 03:51:01.621834 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 03:51:22.644970 140269360194752 spec.py:349] Evaluating on the test split.
I0308 03:51:24.754958 140269360194752 submission_runner.py:469] Time since start: 65358.61s, 	Step: 151273, 	{'train/accuracy': 0.8902861475944519, 'train/loss': 0.38157209753990173, 'validation/accuracy': 0.7341600060462952, 'validation/loss': 1.0990909337997437, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.8257477283477783, 'test/num_examples': 10000, 'score': 60741.602796792984, 'total_duration': 65358.6123790741, 'accumulated_submission_time': 60741.602796792984, 'accumulated_eval_time': 4582.6203083992, 'accumulated_logging_time': 17.34152913093567}
I0308 03:51:24.854791 140113501742848 logging_writer.py:48] [151273] accumulated_eval_time=4582.62, accumulated_logging_time=17.3415, accumulated_submission_time=60741.6, global_step=151273, preemption_count=0, score=60741.6, test/accuracy=0.6104, test/loss=1.82575, test/num_examples=10000, total_duration=65358.6, train/accuracy=0.890286, train/loss=0.381572, validation/accuracy=0.73416, validation/loss=1.09909, validation/num_examples=50000
I0308 03:51:36.228212 140113510135552 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.9107720851898193, loss=0.8843939900398254
2025-03-08 03:52:00.852658: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:52:15.495577 140113501742848 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.708379030227661, loss=0.9726933836936951
I0308 03:52:55.480871 140113510135552 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.283832550048828, loss=0.9283170104026794
I0308 03:53:35.181288 140113501742848 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.007662773132324, loss=1.027480125427246
I0308 03:54:14.815745 140113510135552 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.750059127807617, loss=0.9042647480964661
I0308 03:54:54.031362 140113501742848 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.861029624938965, loss=1.043416976928711
I0308 03:55:33.546192 140113510135552 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.1356072425842285, loss=0.938239574432373
I0308 03:56:12.808280 140113501742848 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.353023052215576, loss=1.0073593854904175
I0308 03:56:52.074287 140113510135552 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.279181003570557, loss=0.9861012697219849
I0308 03:57:31.656354 140113501742848 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.8713417053222656, loss=0.9896062612533569
I0308 03:58:11.487868 140113510135552 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.081442356109619, loss=0.8836658000946045
I0308 03:58:51.156274 140113501742848 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.00276517868042, loss=0.9355047941207886
I0308 03:59:31.842337 140113510135552 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.306550979614258, loss=1.1537985801696777
I0308 03:59:54.789806 140269360194752 spec.py:321] Evaluating on the training split.
I0308 04:00:05.751802 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 04:00:24.239363 140269360194752 spec.py:349] Evaluating on the test split.
I0308 04:00:26.023837 140269360194752 submission_runner.py:469] Time since start: 65899.88s, 	Step: 152558, 	{'train/accuracy': 0.8884725570678711, 'train/loss': 0.3839569389820099, 'validation/accuracy': 0.7287999987602234, 'validation/loss': 1.1154820919036865, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.858032464981079, 'test/num_examples': 10000, 'score': 61251.35122394562, 'total_duration': 65899.88117957115, 'accumulated_submission_time': 61251.35122394562, 'accumulated_eval_time': 4613.8541967868805, 'accumulated_logging_time': 17.47600269317627}
I0308 04:00:26.140835 140113501742848 logging_writer.py:48] [152558] accumulated_eval_time=4613.85, accumulated_logging_time=17.476, accumulated_submission_time=61251.4, global_step=152558, preemption_count=0, score=61251.4, test/accuracy=0.6, test/loss=1.85803, test/num_examples=10000, total_duration=65899.9, train/accuracy=0.888473, train/loss=0.383957, validation/accuracy=0.7288, validation/loss=1.11548, validation/num_examples=50000
I0308 04:00:43.371910 140113510135552 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.9940743446350098, loss=1.0276837348937988
2025-03-08 04:00:48.235928: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:01:22.907438 140113501742848 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.668131113052368, loss=0.8843689560890198
I0308 04:02:02.524491 140113510135552 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.062532424926758, loss=0.8997483253479004
I0308 04:02:42.087850 140113501742848 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.5339815616607666, loss=0.8274106979370117
I0308 04:03:21.375536 140113510135552 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.195168972015381, loss=0.8794422149658203
I0308 04:04:00.801093 140113501742848 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.986790418624878, loss=0.8578097820281982
I0308 04:04:40.699628 140113510135552 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.516859292984009, loss=0.7289105653762817
I0308 04:05:20.056720 140113501742848 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.827636241912842, loss=0.829210638999939
I0308 04:05:59.512168 140113510135552 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.161622047424316, loss=1.0007212162017822
I0308 04:06:39.045876 140113501742848 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.975776195526123, loss=0.8720587491989136
I0308 04:07:18.947639 140113510135552 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.148968696594238, loss=0.9485455751419067
I0308 04:07:58.323152 140113501742848 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.006524562835693, loss=0.8826175928115845
I0308 04:08:38.264158 140113510135552 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.80764102935791, loss=0.899235725402832
I0308 04:08:56.115339 140269360194752 spec.py:321] Evaluating on the training split.
I0308 04:09:07.759275 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 04:09:29.944537 140269360194752 spec.py:349] Evaluating on the test split.
I0308 04:09:31.693689 140269360194752 submission_runner.py:469] Time since start: 66445.55s, 	Step: 153846, 	{'train/accuracy': 0.8997528553009033, 'train/loss': 0.3465679883956909, 'validation/accuracy': 0.7373200058937073, 'validation/loss': 1.0923019647598267, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.8296637535095215, 'test/num_examples': 10000, 'score': 61761.13899111748, 'total_duration': 66445.55115318298, 'accumulated_submission_time': 61761.13899111748, 'accumulated_eval_time': 4649.432509183884, 'accumulated_logging_time': 17.625362873077393}
I0308 04:09:31.838873 140113501742848 logging_writer.py:48] [153846] accumulated_eval_time=4649.43, accumulated_logging_time=17.6254, accumulated_submission_time=61761.1, global_step=153846, preemption_count=0, score=61761.1, test/accuracy=0.6111, test/loss=1.82966, test/num_examples=10000, total_duration=66445.6, train/accuracy=0.899753, train/loss=0.346568, validation/accuracy=0.73732, validation/loss=1.0923, validation/num_examples=50000
2025-03-08 04:09:39.444617: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:09:53.701422 140113510135552 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.9595983028411865, loss=0.8491827845573425
I0308 04:10:33.294005 140113501742848 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.897671699523926, loss=0.8281528949737549
I0308 04:11:12.930211 140113510135552 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.034005165100098, loss=0.8728700876235962
I0308 04:11:52.728479 140113501742848 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.8746516704559326, loss=0.9575617909431458
I0308 04:12:32.152037 140113510135552 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.8435065746307373, loss=0.9004971385002136
I0308 04:13:11.898956 140113501742848 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.2329182624816895, loss=0.9439239501953125
I0308 04:13:51.403446 140113510135552 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.075691223144531, loss=0.9600186347961426
I0308 04:14:31.020780 140113501742848 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.8662939071655273, loss=0.8610013127326965
I0308 04:15:10.621864 140113510135552 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.9203848838806152, loss=0.8314229249954224
I0308 04:15:50.135725 140113501742848 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.9115183353424072, loss=0.9817540645599365
I0308 04:16:30.297981 140113510135552 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.914921283721924, loss=0.8969448208808899
I0308 04:17:09.853374 140113501742848 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.6610541343688965, loss=0.8420290946960449
I0308 04:17:48.737472 140113510135552 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.9291012287139893, loss=0.9384806752204895
2025-03-08 04:17:58.044267: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:18:02.042765 140269360194752 spec.py:321] Evaluating on the training split.
I0308 04:18:13.848300 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 04:18:34.057785 140269360194752 spec.py:349] Evaluating on the test split.
I0308 04:18:35.795205 140269360194752 submission_runner.py:469] Time since start: 66989.65s, 	Step: 155134, 	{'train/accuracy': 0.9030413031578064, 'train/loss': 0.3380180299282074, 'validation/accuracy': 0.737019956111908, 'validation/loss': 1.0878331661224365, 'validation/num_examples': 50000, 'test/accuracy': 0.6136000156402588, 'test/loss': 1.8199772834777832, 'test/num_examples': 10000, 'score': 62271.167729616165, 'total_duration': 66989.65263462067, 'accumulated_submission_time': 62271.167729616165, 'accumulated_eval_time': 4683.184887886047, 'accumulated_logging_time': 17.79177737236023}
I0308 04:18:35.893789 140113501742848 logging_writer.py:48] [155134] accumulated_eval_time=4683.18, accumulated_logging_time=17.7918, accumulated_submission_time=62271.2, global_step=155134, preemption_count=0, score=62271.2, test/accuracy=0.6136, test/loss=1.81998, test/num_examples=10000, total_duration=66989.7, train/accuracy=0.903041, train/loss=0.338018, validation/accuracy=0.73702, validation/loss=1.08783, validation/num_examples=50000
I0308 04:19:02.181011 140113510135552 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.405645370483398, loss=0.8925333619117737
I0308 04:19:41.274225 140113501742848 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.8249804973602295, loss=0.8343613147735596
I0308 04:20:20.353634 140113510135552 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.865478038787842, loss=0.8631481528282166
I0308 04:20:59.660455 140113501742848 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.102777481079102, loss=0.882446825504303
I0308 04:21:38.907028 140113510135552 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.044028282165527, loss=0.8642536401748657
I0308 04:22:18.103719 140113501742848 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.002788543701172, loss=0.8089820146560669
I0308 04:22:57.266953 140113510135552 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.00955057144165, loss=0.8725763559341431
I0308 04:23:36.724153 140113501742848 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.8044092655181885, loss=0.8244740962982178
I0308 04:24:16.234906 140113510135552 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.342928409576416, loss=0.9004875421524048
I0308 04:24:55.646450 140113501742848 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.272533893585205, loss=0.8705366849899292
I0308 04:25:35.481724 140113510135552 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.2634782791137695, loss=0.8442981243133545
I0308 04:26:15.250720 140113501742848 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.935943126678467, loss=0.8860083818435669
2025-03-08 04:26:42.130748: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:26:55.250258 140113510135552 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.148647785186768, loss=0.833336591720581
I0308 04:27:06.129471 140269360194752 spec.py:321] Evaluating on the training split.
I0308 04:27:16.969663 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 04:27:37.533327 140269360194752 spec.py:349] Evaluating on the test split.
I0308 04:27:39.636620 140269360194752 submission_runner.py:469] Time since start: 67533.49s, 	Step: 156428, 	{'train/accuracy': 0.8995535373687744, 'train/loss': 0.34427598118782043, 'validation/accuracy': 0.7343199849128723, 'validation/loss': 1.1086012125015259, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.8398256301879883, 'test/num_examples': 10000, 'score': 62781.21112012863, 'total_duration': 67533.49406647682, 'accumulated_submission_time': 62781.21112012863, 'accumulated_eval_time': 4716.691983222961, 'accumulated_logging_time': 17.92684841156006}
I0308 04:27:39.764711 140113501742848 logging_writer.py:48] [156428] accumulated_eval_time=4716.69, accumulated_logging_time=17.9268, accumulated_submission_time=62781.2, global_step=156428, preemption_count=0, score=62781.2, test/accuracy=0.6107, test/loss=1.83983, test/num_examples=10000, total_duration=67533.5, train/accuracy=0.899554, train/loss=0.344276, validation/accuracy=0.73432, validation/loss=1.1086, validation/num_examples=50000
I0308 04:28:08.454717 140113510135552 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.981194257736206, loss=0.9093539118766785
I0308 04:28:47.709061 140113501742848 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.6148476600646973, loss=0.786572277545929
I0308 04:29:27.453569 140113510135552 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.3599419593811035, loss=0.9938494563102722
I0308 04:30:07.583884 140113501742848 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.827326774597168, loss=0.7770652770996094
I0308 04:30:46.877436 140113510135552 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.116787433624268, loss=0.8241859078407288
I0308 04:31:26.351778 140113501742848 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.213296890258789, loss=0.85296630859375
I0308 04:32:05.997936 140113510135552 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.073351860046387, loss=0.9073308110237122
I0308 04:32:45.527676 140113501742848 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.234502792358398, loss=0.8944995999336243
I0308 04:33:25.372959 140113510135552 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.346688270568848, loss=0.8302425146102905
I0308 04:34:04.800007 140113501742848 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.7386748790740967, loss=0.8533910512924194
I0308 04:34:44.842101 140113510135552 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.210164546966553, loss=0.7737120389938354
I0308 04:35:24.520606 140113501742848 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.9356424808502197, loss=0.8451720476150513
2025-03-08 04:35:32.212445: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:36:03.936738 140113510135552 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.098484039306641, loss=0.9518113732337952
I0308 04:36:09.856582 140269360194752 spec.py:321] Evaluating on the training split.
I0308 04:36:20.893049 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 04:36:42.853908 140269360194752 spec.py:349] Evaluating on the test split.
I0308 04:36:44.579008 140269360194752 submission_runner.py:469] Time since start: 68078.44s, 	Step: 157716, 	{'train/accuracy': 0.9099968075752258, 'train/loss': 0.30783137679100037, 'validation/accuracy': 0.7361999750137329, 'validation/loss': 1.1030312776565552, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.847373127937317, 'test/num_examples': 10000, 'score': 63291.10856127739, 'total_duration': 68078.4364669323, 'accumulated_submission_time': 63291.10856127739, 'accumulated_eval_time': 4751.414372205734, 'accumulated_logging_time': 18.090118646621704}
I0308 04:36:44.656936 140113501742848 logging_writer.py:48] [157716] accumulated_eval_time=4751.41, accumulated_logging_time=18.0901, accumulated_submission_time=63291.1, global_step=157716, preemption_count=0, score=63291.1, test/accuracy=0.6105, test/loss=1.84737, test/num_examples=10000, total_duration=68078.4, train/accuracy=0.909997, train/loss=0.307831, validation/accuracy=0.7362, validation/loss=1.10303, validation/num_examples=50000
I0308 04:37:18.044078 140113510135552 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.8362982273101807, loss=0.824613630771637
I0308 04:37:57.248656 140113501742848 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.149970054626465, loss=0.9191984534263611
I0308 04:38:36.601541 140113510135552 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.657824993133545, loss=1.003442645072937
I0308 04:39:16.283597 140113501742848 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.518963813781738, loss=0.8758608102798462
I0308 04:39:55.767470 140113510135552 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.943925619125366, loss=0.8658944964408875
I0308 04:40:35.539413 140113501742848 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.415358543395996, loss=0.9052472114562988
I0308 04:41:15.003113 140113510135552 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.517592906951904, loss=0.88808673620224
I0308 04:41:54.301182 140113501742848 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.332458972930908, loss=0.8553865551948547
I0308 04:42:34.109394 140113510135552 logging_writer.py:48] [158600] global_step=158600, grad_norm=3.990334987640381, loss=0.8263831734657288
I0308 04:43:13.712064 140113501742848 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.96879506111145, loss=0.7774604558944702
I0308 04:43:53.480905 140113510135552 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.846536159515381, loss=0.8599761724472046
2025-03-08 04:44:22.735297: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:44:32.186644 140113501742848 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.353209018707275, loss=0.947724461555481
I0308 04:45:10.048691 140113510135552 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.334239959716797, loss=0.8606666326522827
I0308 04:45:14.617012 140269360194752 spec.py:321] Evaluating on the training split.
I0308 04:45:26.747473 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 04:45:47.690640 140269360194752 spec.py:349] Evaluating on the test split.
I0308 04:45:49.435600 140269360194752 submission_runner.py:469] Time since start: 68623.29s, 	Step: 159013, 	{'train/accuracy': 0.9157166481018066, 'train/loss': 0.29050204157829285, 'validation/accuracy': 0.7391600012779236, 'validation/loss': 1.0969388484954834, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.8542455434799194, 'test/num_examples': 10000, 'score': 63800.86756539345, 'total_duration': 68623.29306054115, 'accumulated_submission_time': 63800.86756539345, 'accumulated_eval_time': 4786.232919216156, 'accumulated_logging_time': 18.210217475891113}
I0308 04:45:49.602082 140113501742848 logging_writer.py:48] [159013] accumulated_eval_time=4786.23, accumulated_logging_time=18.2102, accumulated_submission_time=63800.9, global_step=159013, preemption_count=0, score=63800.9, test/accuracy=0.609, test/loss=1.85425, test/num_examples=10000, total_duration=68623.3, train/accuracy=0.915717, train/loss=0.290502, validation/accuracy=0.73916, validation/loss=1.09694, validation/num_examples=50000
I0308 04:46:24.367047 140113510135552 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.406167030334473, loss=0.9438238739967346
I0308 04:47:03.717071 140113501742848 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.349288463592529, loss=0.9048657417297363
I0308 04:47:43.483520 140113510135552 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.9417426586151123, loss=0.8110513091087341
I0308 04:48:22.776354 140113501742848 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.499247074127197, loss=0.8726335167884827
I0308 04:49:02.594047 140113510135552 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.203535556793213, loss=0.7381933927536011
I0308 04:49:42.405933 140113501742848 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.084898471832275, loss=0.7635013461112976
I0308 04:50:22.427114 140113510135552 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.558859825134277, loss=0.8983364105224609
I0308 04:51:01.809274 140113501742848 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.469206809997559, loss=0.9046569466590881
I0308 04:51:40.876870 140113510135552 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.134082317352295, loss=0.8122484087944031
I0308 04:52:20.522443 140113501742848 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.107983589172363, loss=0.91196608543396
I0308 04:53:00.362934 140113510135552 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.984369993209839, loss=0.8191866874694824
2025-03-08 04:53:08.154255: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:53:39.777201 140113501742848 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.155155658721924, loss=0.7921993136405945
I0308 04:54:19.536000 140113510135552 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.503890037536621, loss=0.9102585911750793
I0308 04:54:19.546344 140269360194752 spec.py:321] Evaluating on the training split.
I0308 04:54:30.659887 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 04:54:50.976869 140269360194752 spec.py:349] Evaluating on the test split.
I0308 04:54:52.694616 140269360194752 submission_runner.py:469] Time since start: 69166.55s, 	Step: 160301, 	{'train/accuracy': 0.9168925285339355, 'train/loss': 0.2939058840274811, 'validation/accuracy': 0.7389999628067017, 'validation/loss': 1.0933477878570557, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.8359034061431885, 'test/num_examples': 10000, 'score': 64310.629964113235, 'total_duration': 69166.55206394196, 'accumulated_submission_time': 64310.629964113235, 'accumulated_eval_time': 4819.381118774414, 'accumulated_logging_time': 18.402047157287598}
I0308 04:54:52.746834 140113501742848 logging_writer.py:48] [160301] accumulated_eval_time=4819.38, accumulated_logging_time=18.402, accumulated_submission_time=64310.6, global_step=160301, preemption_count=0, score=64310.6, test/accuracy=0.609, test/loss=1.8359, test/num_examples=10000, total_duration=69166.6, train/accuracy=0.916893, train/loss=0.293906, validation/accuracy=0.739, validation/loss=1.09335, validation/num_examples=50000
I0308 04:55:32.596790 140113510135552 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.403537273406982, loss=0.8703144192695618
I0308 04:56:12.393861 140113501742848 logging_writer.py:48] [160500] global_step=160500, grad_norm=3.9706811904907227, loss=0.8391150832176208
I0308 04:56:51.751942 140113510135552 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.284676551818848, loss=0.7993603944778442
I0308 04:57:31.391404 140113501742848 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.223697662353516, loss=0.8824266791343689
I0308 04:58:10.977699 140113510135552 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.157190322875977, loss=0.8994781970977783
I0308 04:58:50.404109 140113501742848 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.9528005123138428, loss=0.7976953387260437
I0308 04:59:30.431800 140113510135552 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.0387396812438965, loss=0.8172625303268433
I0308 05:00:09.717958 140113501742848 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.298076629638672, loss=0.816422700881958
I0308 05:00:49.312481 140113510135552 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.43197774887085, loss=0.7803170084953308
I0308 05:01:29.371534 140113501742848 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.8380377292633057, loss=0.7928615808486938
2025-03-08 05:01:57.390297: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:02:09.039141 140113510135552 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.182254314422607, loss=0.8345567584037781
I0308 05:02:48.823671 140113501742848 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.663802146911621, loss=0.8283306956291199
I0308 05:03:22.710053 140269360194752 spec.py:321] Evaluating on the training split.
I0308 05:03:34.261537 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 05:03:58.490812 140269360194752 spec.py:349] Evaluating on the test split.
I0308 05:04:00.214667 140269360194752 submission_runner.py:469] Time since start: 69714.07s, 	Step: 161587, 	{'train/accuracy': 0.9212173223495483, 'train/loss': 0.2696397304534912, 'validation/accuracy': 0.7414199709892273, 'validation/loss': 1.083556890487671, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.838761806488037, 'test/num_examples': 10000, 'score': 64820.405883550644, 'total_duration': 69714.07211208344, 'accumulated_submission_time': 64820.405883550644, 'accumulated_eval_time': 4856.885681629181, 'accumulated_logging_time': 18.480733394622803}
I0308 05:04:00.290564 140113510135552 logging_writer.py:48] [161587] accumulated_eval_time=4856.89, accumulated_logging_time=18.4807, accumulated_submission_time=64820.4, global_step=161587, preemption_count=0, score=64820.4, test/accuracy=0.6135, test/loss=1.83876, test/num_examples=10000, total_duration=69714.1, train/accuracy=0.921217, train/loss=0.26964, validation/accuracy=0.74142, validation/loss=1.08356, validation/num_examples=50000
I0308 05:04:05.658267 140113501742848 logging_writer.py:48] [161600] global_step=161600, grad_norm=3.88958477973938, loss=0.7722837924957275
I0308 05:04:45.214386 140113510135552 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.301607131958008, loss=0.8395716547966003
I0308 05:05:24.543061 140113501742848 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.975588321685791, loss=0.7510091066360474
I0308 05:06:04.319465 140113510135552 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.150041580200195, loss=0.9042206406593323
I0308 05:06:43.395043 140113501742848 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.0591912269592285, loss=0.8141450881958008
I0308 05:07:22.906823 140113510135552 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.09392786026001, loss=0.822688102722168
I0308 05:08:02.371790 140113501742848 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.961176872253418, loss=0.7763155102729797
I0308 05:08:41.885779 140113510135552 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.273900985717773, loss=0.8922785520553589
I0308 05:09:21.470727 140113501742848 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.37917947769165, loss=0.7813264727592468
I0308 05:10:01.112615 140113510135552 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.093480587005615, loss=0.7401379942893982
I0308 05:10:40.597452 140113501742848 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.661525249481201, loss=0.8807752132415771
2025-03-08 05:10:50.368571: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:11:19.783792 140113510135552 logging_writer.py:48] [162700] global_step=162700, grad_norm=5.351044178009033, loss=0.8202934265136719
I0308 05:11:59.056363 140113501742848 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.3327765464782715, loss=0.8519071936607361
I0308 05:12:30.382180 140269360194752 spec.py:321] Evaluating on the training split.
I0308 05:12:41.291574 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 05:13:02.277343 140269360194752 spec.py:349] Evaluating on the test split.
I0308 05:13:04.011077 140269360194752 submission_runner.py:469] Time since start: 70257.87s, 	Step: 162881, 	{'train/accuracy': 0.9265385866165161, 'train/loss': 0.25606390833854675, 'validation/accuracy': 0.7444599866867065, 'validation/loss': 1.0853649377822876, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.8269352912902832, 'test/num_examples': 10000, 'score': 65330.30572605133, 'total_duration': 70257.86854076385, 'accumulated_submission_time': 65330.30572605133, 'accumulated_eval_time': 4890.514559745789, 'accumulated_logging_time': 18.592165231704712}
I0308 05:13:04.096531 140113510135552 logging_writer.py:48] [162881] accumulated_eval_time=4890.51, accumulated_logging_time=18.5922, accumulated_submission_time=65330.3, global_step=162881, preemption_count=0, score=65330.3, test/accuracy=0.6205, test/loss=1.82694, test/num_examples=10000, total_duration=70257.9, train/accuracy=0.926539, train/loss=0.256064, validation/accuracy=0.74446, validation/loss=1.08536, validation/num_examples=50000
I0308 05:13:11.874892 140113501742848 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.747803211212158, loss=0.8616593480110168
I0308 05:13:51.241029 140113510135552 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.1915998458862305, loss=0.7922519445419312
I0308 05:14:30.453910 140113501742848 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.359599590301514, loss=0.800532877445221
I0308 05:15:09.994461 140113510135552 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.42021369934082, loss=0.8205280900001526
I0308 05:15:48.991153 140113501742848 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.43480110168457, loss=0.8735570311546326
I0308 05:16:28.161473 140113510135552 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.127299785614014, loss=0.8182281255722046
I0308 05:17:07.878971 140113501742848 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.3091230392456055, loss=0.8075810670852661
I0308 05:17:47.165649 140113510135552 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.090662479400635, loss=0.8552645444869995
I0308 05:18:26.617153 140113501742848 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.246490001678467, loss=0.7954021096229553
I0308 05:19:06.199335 140113510135552 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.198765754699707, loss=0.8509796857833862
2025-03-08 05:19:35.061568: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:19:45.879501 140113501742848 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.08205509185791, loss=0.7669384479522705
I0308 05:20:25.635207 140113510135552 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.035091400146484, loss=0.7922718524932861
I0308 05:21:05.355287 140113501742848 logging_writer.py:48] [164100] global_step=164100, grad_norm=3.973344326019287, loss=0.8076192736625671
I0308 05:21:34.192591 140269360194752 spec.py:321] Evaluating on the training split.
I0308 05:21:45.063087 140269360194752 spec.py:333] Evaluating on the validation split.
I0308 05:22:03.518115 140269360194752 spec.py:349] Evaluating on the test split.
I0308 05:22:05.276110 140269360194752 submission_runner.py:469] Time since start: 70799.13s, 	Step: 164174, 	{'train/accuracy': 0.9280930757522583, 'train/loss': 0.25396662950515747, 'validation/accuracy': 0.7455399632453918, 'validation/loss': 1.0867712497711182, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.8309104442596436, 'test/num_examples': 10000, 'score': 65840.21555304527, 'total_duration': 70799.13355851173, 'accumulated_submission_time': 65840.21555304527, 'accumulated_eval_time': 4921.598035573959, 'accumulated_logging_time': 18.710416793823242}
I0308 05:22:05.349926 140113510135552 logging_writer.py:48] [164174] accumulated_eval_time=4921.6, accumulated_logging_time=18.7104, accumulated_submission_time=65840.2, global_step=164174, preemption_count=0, score=65840.2, test/accuracy=0.6206, test/loss=1.83091, test/num_examples=10000, total_duration=70799.1, train/accuracy=0.928093, train/loss=0.253967, validation/accuracy=0.74554, validation/loss=1.08677, validation/num_examples=50000
I0308 05:22:16.358798 140113501742848 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.229207515716553, loss=0.7849122285842896
I0308 05:22:55.823386 140113510135552 logging_writer.py:48] [164300] global_step=164300, grad_norm=3.994497776031494, loss=0.7894632816314697
I0308 05:23:35.361925 140113501742848 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.4706034660339355, loss=0.7447136044502258
I0308 05:24:15.186203 140113510135552 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.299576282501221, loss=0.772548496723175
I0308 05:24:54.652551 140113501742848 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.520112037658691, loss=0.8289357423782349
I0308 05:25:33.866483 140113510135552 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.632938385009766, loss=0.8119581341743469
I0308 05:26:13.337102 140113501742848 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.437999725341797, loss=0.7627930045127869
I0308 05:26:52.692147 140113510135552 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.881601333618164, loss=0.8403946161270142
I0308 05:27:32.032653 140113501742848 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.036900997161865, loss=0.7887706756591797
I0308 05:28:11.741901 140113510135552 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.079575061798096, loss=0.7741187214851379
2025-03-08 05:28:20.797283: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:28:51.197685 140113501742848 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.176636219024658, loss=0.7688851356506348
I0308 05:29:30.474592 140113510135552 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.192947864532471, loss=0.7058550119400024
I0308 05:30:09.602896 140113501742848 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.486426830291748, loss=0.7807411551475525
I0308 05:30:35.395730 140113510135552 logging_writer.py:48] [165467] global_step=165467, preemption_count=0, score=66350
I0308 05:30:36.958868 140269360194752 submission_runner.py:646] Tuning trial 5/5
I0308 05:30:36.981152 140269360194752 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0308 05:30:36.985227 140269360194752 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009167729294858873, 'train/loss': 6.912870407104492, 'validation/accuracy': 0.0009399999980814755, 'validation/loss': 6.913017272949219, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.913171291351318, 'test/num_examples': 10000, 'score': 58.6150918006897, 'total_duration': 143.4340283870697, 'accumulated_submission_time': 58.6150918006897, 'accumulated_eval_time': 84.81869649887085, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1339, {'train/accuracy': 0.15929926931858063, 'train/loss': 4.339805603027344, 'validation/accuracy': 0.13293999433517456, 'validation/loss': 4.56612491607666, 'validation/num_examples': 50000, 'test/accuracy': 0.09910000115633011, 'test/loss': 5.026389122009277, 'test/num_examples': 10000, 'score': 568.4661321640015, 'total_duration': 685.5391473770142, 'accumulated_submission_time': 568.4661321640015, 'accumulated_eval_time': 116.86762952804565, 'accumulated_logging_time': 0.03965926170349121, 'global_step': 1339, 'preemption_count': 0}), (2666, {'train/accuracy': 0.3227439224720001, 'train/loss': 3.161668062210083, 'validation/accuracy': 0.26872000098228455, 'validation/loss': 3.495527982711792, 'validation/num_examples': 50000, 'test/accuracy': 0.19840000569820404, 'test/loss': 4.071988582611084, 'test/num_examples': 10000, 'score': 1078.3304996490479, 'total_duration': 1239.279393196106, 'accumulated_submission_time': 1078.3304996490479, 'accumulated_eval_time': 160.56041884422302, 'accumulated_logging_time': 0.07461929321289062, 'global_step': 2666, 'preemption_count': 0}), (3997, {'train/accuracy': 0.44738519191741943, 'train/loss': 2.45143723487854, 'validation/accuracy': 0.3901599943637848, 'validation/loss': 2.781484842300415, 'validation/num_examples': 50000, 'test/accuracy': 0.28690001368522644, 'test/loss': 3.502551794052124, 'test/num_examples': 10000, 'score': 1588.2982215881348, 'total_duration': 1789.2242217063904, 'accumulated_submission_time': 1588.2982215881348, 'accumulated_eval_time': 200.36492466926575, 'accumulated_logging_time': 0.12267780303955078, 'global_step': 3997, 'preemption_count': 0}), (5326, {'train/accuracy': 0.5285993218421936, 'train/loss': 2.0123300552368164, 'validation/accuracy': 0.4635799825191498, 'validation/loss': 2.3488736152648926, 'validation/num_examples': 50000, 'test/accuracy': 0.3582000136375427, 'test/loss': 3.0545520782470703, 'test/num_examples': 10000, 'score': 2098.2309913635254, 'total_duration': 2334.445417404175, 'accumulated_submission_time': 2098.2309913635254, 'accumulated_eval_time': 235.52203559875488, 'accumulated_logging_time': 0.13881325721740723, 'global_step': 5326, 'preemption_count': 0}), (6655, {'train/accuracy': 0.5837053656578064, 'train/loss': 1.7373634576797485, 'validation/accuracy': 0.5206400156021118, 'validation/loss': 2.0676352977752686, 'validation/num_examples': 50000, 'test/accuracy': 0.40390002727508545, 'test/loss': 2.8003625869750977, 'test/num_examples': 10000, 'score': 2608.3175547122955, 'total_duration': 2879.148097515106, 'accumulated_submission_time': 2608.3175547122955, 'accumulated_eval_time': 270.0064697265625, 'accumulated_logging_time': 0.15894794464111328, 'global_step': 6655, 'preemption_count': 0}), (7984, {'train/accuracy': 0.5759526491165161, 'train/loss': 1.7723933458328247, 'validation/accuracy': 0.5143200159072876, 'validation/loss': 2.112335205078125, 'validation/num_examples': 50000, 'test/accuracy': 0.4068000316619873, 'test/loss': 2.8483917713165283, 'test/num_examples': 10000, 'score': 3118.4493594169617, 'total_duration': 3426.7095811367035, 'accumulated_submission_time': 3118.4493594169617, 'accumulated_eval_time': 307.3126537799835, 'accumulated_logging_time': 0.1769120693206787, 'global_step': 7984, 'preemption_count': 0}), (9315, {'train/accuracy': 0.6282286047935486, 'train/loss': 1.5050336122512817, 'validation/accuracy': 0.5659399628639221, 'validation/loss': 1.846015453338623, 'validation/num_examples': 50000, 'test/accuracy': 0.43560001254081726, 'test/loss': 2.6101667881011963, 'test/num_examples': 10000, 'score': 3628.3923115730286, 'total_duration': 3974.701777458191, 'accumulated_submission_time': 3628.3923115730286, 'accumulated_eval_time': 345.2049353122711, 'accumulated_logging_time': 0.22958660125732422, 'global_step': 9315, 'preemption_count': 0}), (10652, {'train/accuracy': 0.6330716013908386, 'train/loss': 1.4845244884490967, 'validation/accuracy': 0.5704399943351746, 'validation/loss': 1.81955087184906, 'validation/num_examples': 50000, 'test/accuracy': 0.4433000087738037, 'test/loss': 2.587620735168457, 'test/num_examples': 10000, 'score': 4138.432383060455, 'total_duration': 4526.649419307709, 'accumulated_submission_time': 4138.432383060455, 'accumulated_eval_time': 386.835266828537, 'accumulated_logging_time': 0.3533949851989746, 'global_step': 10652, 'preemption_count': 0}), (11980, {'train/accuracy': 0.6530413031578064, 'train/loss': 1.4080978631973267, 'validation/accuracy': 0.5837599635124207, 'validation/loss': 1.747476577758789, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.503089427947998, 'test/num_examples': 10000, 'score': 4648.388277292252, 'total_duration': 5080.775400400162, 'accumulated_submission_time': 4648.388277292252, 'accumulated_eval_time': 430.79916405677795, 'accumulated_logging_time': 0.4122931957244873, 'global_step': 11980, 'preemption_count': 0}), (13305, {'train/accuracy': 0.6422991156578064, 'train/loss': 1.4485238790512085, 'validation/accuracy': 0.5745999813079834, 'validation/loss': 1.811006784439087, 'validation/num_examples': 50000, 'test/accuracy': 0.45570001006126404, 'test/loss': 2.5664117336273193, 'test/num_examples': 10000, 'score': 5158.23425412178, 'total_duration': 5635.634290218353, 'accumulated_submission_time': 5158.23425412178, 'accumulated_eval_time': 475.5239474773407, 'accumulated_logging_time': 0.5558822154998779, 'global_step': 13305, 'preemption_count': 0}), (14624, {'train/accuracy': 0.6447703838348389, 'train/loss': 1.4218053817749023, 'validation/accuracy': 0.5844399929046631, 'validation/loss': 1.7558026313781738, 'validation/num_examples': 50000, 'test/accuracy': 0.457800030708313, 'test/loss': 2.5262722969055176, 'test/num_examples': 10000, 'score': 5668.10534119606, 'total_duration': 6191.2318522930145, 'accumulated_submission_time': 5668.10534119606, 'accumulated_eval_time': 520.9528930187225, 'accumulated_logging_time': 0.7134666442871094, 'global_step': 14624, 'preemption_count': 0}), (15940, {'train/accuracy': 0.6671914458274841, 'train/loss': 1.3173516988754272, 'validation/accuracy': 0.5956999659538269, 'validation/loss': 1.6901887655258179, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4337799549102783, 'test/num_examples': 10000, 'score': 6177.7375519275665, 'total_duration': 6742.823451757431, 'accumulated_submission_time': 6177.7375519275665, 'accumulated_eval_time': 562.4926252365112, 'accumulated_logging_time': 0.9937787055969238, 'global_step': 15940, 'preemption_count': 0}), (17256, {'train/accuracy': 0.6512874364852905, 'train/loss': 1.3997085094451904, 'validation/accuracy': 0.5823999643325806, 'validation/loss': 1.7467749118804932, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.470266580581665, 'test/num_examples': 10000, 'score': 6687.620101690292, 'total_duration': 7299.134912014008, 'accumulated_submission_time': 6687.620101690292, 'accumulated_eval_time': 608.5871965885162, 'accumulated_logging_time': 1.1844618320465088, 'global_step': 17256, 'preemption_count': 0}), (18578, {'train/accuracy': 0.6806241869926453, 'train/loss': 1.2696762084960938, 'validation/accuracy': 0.6096199750900269, 'validation/loss': 1.6212488412857056, 'validation/num_examples': 50000, 'test/accuracy': 0.4790000319480896, 'test/loss': 2.3717098236083984, 'test/num_examples': 10000, 'score': 7197.6196620464325, 'total_duration': 7849.326790332794, 'accumulated_submission_time': 7197.6196620464325, 'accumulated_eval_time': 648.504426240921, 'accumulated_logging_time': 1.3333828449249268, 'global_step': 18578, 'preemption_count': 0}), (19898, {'train/accuracy': 0.6729910373687744, 'train/loss': 1.3033865690231323, 'validation/accuracy': 0.6078199744224548, 'validation/loss': 1.64429771900177, 'validation/num_examples': 50000, 'test/accuracy': 0.48990002274513245, 'test/loss': 2.365725517272949, 'test/num_examples': 10000, 'score': 7707.5985741615295, 'total_duration': 8407.273492097855, 'accumulated_submission_time': 7707.5985741615295, 'accumulated_eval_time': 696.186240196228, 'accumulated_logging_time': 1.4901890754699707, 'global_step': 19898, 'preemption_count': 0}), (21204, {'train/accuracy': 0.6843510866165161, 'train/loss': 1.237943172454834, 'validation/accuracy': 0.6148599982261658, 'validation/loss': 1.613425612449646, 'validation/num_examples': 50000, 'test/accuracy': 0.4823000133037567, 'test/loss': 2.364466428756714, 'test/num_examples': 10000, 'score': 8217.763018369675, 'total_duration': 8965.202175617218, 'accumulated_submission_time': 8217.763018369675, 'accumulated_eval_time': 743.7722883224487, 'accumulated_logging_time': 1.527381181716919, 'global_step': 21204, 'preemption_count': 0}), (22513, {'train/accuracy': 0.6660953164100647, 'train/loss': 1.3299880027770996, 'validation/accuracy': 0.5976399779319763, 'validation/loss': 1.6900964975357056, 'validation/num_examples': 50000, 'test/accuracy': 0.47440001368522644, 'test/loss': 2.4104833602905273, 'test/num_examples': 10000, 'score': 8727.635385751724, 'total_duration': 9514.198991298676, 'accumulated_submission_time': 8727.635385751724, 'accumulated_eval_time': 782.6991384029388, 'accumulated_logging_time': 1.5777478218078613, 'global_step': 22513, 'preemption_count': 0}), (23768, {'train/accuracy': 0.66015625, 'train/loss': 1.3520725965499878, 'validation/accuracy': 0.5965200066566467, 'validation/loss': 1.7051085233688354, 'validation/num_examples': 50000, 'test/accuracy': 0.48000001907348633, 'test/loss': 2.4384641647338867, 'test/num_examples': 10000, 'score': 9237.437793016434, 'total_duration': 10068.803575515747, 'accumulated_submission_time': 9237.437793016434, 'accumulated_eval_time': 827.2006297111511, 'accumulated_logging_time': 1.7460052967071533, 'global_step': 23768, 'preemption_count': 0}), (25077, {'train/accuracy': 0.6676897406578064, 'train/loss': 1.3109707832336426, 'validation/accuracy': 0.6008999943733215, 'validation/loss': 1.6706515550613403, 'validation/num_examples': 50000, 'test/accuracy': 0.47510001063346863, 'test/loss': 2.416644334793091, 'test/num_examples': 10000, 'score': 9747.373111486435, 'total_duration': 10624.217602729797, 'accumulated_submission_time': 9747.373111486435, 'accumulated_eval_time': 872.4149825572968, 'accumulated_logging_time': 1.874401330947876, 'global_step': 25077, 'preemption_count': 0}), (26384, {'train/accuracy': 0.6811822056770325, 'train/loss': 1.259221076965332, 'validation/accuracy': 0.6119399666786194, 'validation/loss': 1.6104940176010132, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.3759024143218994, 'test/num_examples': 10000, 'score': 10257.216561079025, 'total_duration': 11178.408034563065, 'accumulated_submission_time': 10257.216561079025, 'accumulated_eval_time': 916.5076866149902, 'accumulated_logging_time': 1.9939005374908447, 'global_step': 26384, 'preemption_count': 0}), (27704, {'train/accuracy': 0.6849290132522583, 'train/loss': 1.2387564182281494, 'validation/accuracy': 0.616320013999939, 'validation/loss': 1.5950056314468384, 'validation/num_examples': 50000, 'test/accuracy': 0.4926000237464905, 'test/loss': 2.350342273712158, 'test/num_examples': 10000, 'score': 10766.4341006279, 'total_duration': 11731.897221326828, 'accumulated_submission_time': 10766.4341006279, 'accumulated_eval_time': 959.8398950099945, 'accumulated_logging_time': 2.8018834590911865, 'global_step': 27704, 'preemption_count': 0}), (29025, {'train/accuracy': 0.6882174611091614, 'train/loss': 1.2280173301696777, 'validation/accuracy': 0.6187799572944641, 'validation/loss': 1.5952154397964478, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.3284976482391357, 'test/num_examples': 10000, 'score': 11276.176182508469, 'total_duration': 12284.668390989304, 'accumulated_submission_time': 11276.176182508469, 'accumulated_eval_time': 1002.5912687778473, 'accumulated_logging_time': 2.948348045349121, 'global_step': 29025, 'preemption_count': 0}), (30339, {'train/accuracy': 0.6851682066917419, 'train/loss': 1.2412374019622803, 'validation/accuracy': 0.6202999949455261, 'validation/loss': 1.5862261056900024, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.3271164894104004, 'test/num_examples': 10000, 'score': 11785.983068227768, 'total_duration': 12835.990358352661, 'accumulated_submission_time': 11785.983068227768, 'accumulated_eval_time': 1043.8447053432465, 'accumulated_logging_time': 3.077840566635132, 'global_step': 30339, 'preemption_count': 0}), (31656, {'train/accuracy': 0.689871609210968, 'train/loss': 1.216988444328308, 'validation/accuracy': 0.619879961013794, 'validation/loss': 1.5646140575408936, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.253394842147827, 'test/num_examples': 10000, 'score': 12295.836554527283, 'total_duration': 13382.991485595703, 'accumulated_submission_time': 12295.836554527283, 'accumulated_eval_time': 1080.6946363449097, 'accumulated_logging_time': 3.245150327682495, 'global_step': 31656, 'preemption_count': 0}), (32974, {'train/accuracy': 0.6886360049247742, 'train/loss': 1.223679542541504, 'validation/accuracy': 0.621399998664856, 'validation/loss': 1.5662318468093872, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.338261842727661, 'test/num_examples': 10000, 'score': 12805.7665848732, 'total_duration': 13949.34369301796, 'accumulated_submission_time': 12805.7665848732, 'accumulated_eval_time': 1136.8119888305664, 'accumulated_logging_time': 3.419074296951294, 'global_step': 32974, 'preemption_count': 0}), (34292, {'train/accuracy': 0.6939373016357422, 'train/loss': 1.2007282972335815, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.5255661010742188, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.266035556793213, 'test/num_examples': 10000, 'score': 13315.649727582932, 'total_duration': 14514.021225452423, 'accumulated_submission_time': 13315.649727582932, 'accumulated_eval_time': 1191.3865106105804, 'accumulated_logging_time': 3.5079076290130615, 'global_step': 34292, 'preemption_count': 0}), (35614, {'train/accuracy': 0.6938974857330322, 'train/loss': 1.19839346408844, 'validation/accuracy': 0.625220000743866, 'validation/loss': 1.543534278869629, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2703428268432617, 'test/num_examples': 10000, 'score': 13825.470371246338, 'total_duration': 15064.475488901138, 'accumulated_submission_time': 13825.470371246338, 'accumulated_eval_time': 1231.7749123573303, 'accumulated_logging_time': 3.619274854660034, 'global_step': 35614, 'preemption_count': 0}), (36934, {'train/accuracy': 0.6949537396430969, 'train/loss': 1.1953582763671875, 'validation/accuracy': 0.6283199787139893, 'validation/loss': 1.5368438959121704, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2652556896209717, 'test/num_examples': 10000, 'score': 14335.530648946762, 'total_duration': 15626.66927909851, 'accumulated_submission_time': 14335.530648946762, 'accumulated_eval_time': 1283.6372187137604, 'accumulated_logging_time': 3.7568199634552, 'global_step': 36934, 'preemption_count': 0}), (38171, {'train/accuracy': 0.6978435516357422, 'train/loss': 1.1887882947921753, 'validation/accuracy': 0.6301199793815613, 'validation/loss': 1.5258427858352661, 'validation/num_examples': 50000, 'test/accuracy': 0.503600001335144, 'test/loss': 2.2854816913604736, 'test/num_examples': 10000, 'score': 14845.296821832657, 'total_duration': 16194.19715476036, 'accumulated_submission_time': 14845.296821832657, 'accumulated_eval_time': 1341.1132242679596, 'accumulated_logging_time': 3.909550189971924, 'global_step': 38171, 'preemption_count': 0}), (39485, {'train/accuracy': 0.6980627775192261, 'train/loss': 1.1838842630386353, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.5314708948135376, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2674360275268555, 'test/num_examples': 10000, 'score': 15355.186982154846, 'total_duration': 16753.81883621216, 'accumulated_submission_time': 15355.186982154846, 'accumulated_eval_time': 1390.6154713630676, 'accumulated_logging_time': 4.0049965381622314, 'global_step': 39485, 'preemption_count': 0}), (40802, {'train/accuracy': 0.7097018361091614, 'train/loss': 1.1230547428131104, 'validation/accuracy': 0.6408799886703491, 'validation/loss': 1.482406497001648, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.2179179191589355, 'test/num_examples': 10000, 'score': 15865.208394289017, 'total_duration': 17313.76481819153, 'accumulated_submission_time': 15865.208394289017, 'accumulated_eval_time': 1440.2522711753845, 'accumulated_logging_time': 4.158671140670776, 'global_step': 40802, 'preemption_count': 0}), (42118, {'train/accuracy': 0.6983816623687744, 'train/loss': 1.167810320854187, 'validation/accuracy': 0.6308000087738037, 'validation/loss': 1.523690938949585, 'validation/num_examples': 50000, 'test/accuracy': 0.5017000436782837, 'test/loss': 2.2804453372955322, 'test/num_examples': 10000, 'score': 16375.017258882523, 'total_duration': 17868.437598466873, 'accumulated_submission_time': 16375.017258882523, 'accumulated_eval_time': 1484.8731899261475, 'accumulated_logging_time': 4.269007444381714, 'global_step': 42118, 'preemption_count': 0}), (43435, {'train/accuracy': 0.7061343789100647, 'train/loss': 1.1475305557250977, 'validation/accuracy': 0.6379199624061584, 'validation/loss': 1.4962568283081055, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.2154202461242676, 'test/num_examples': 10000, 'score': 16884.944253206253, 'total_duration': 18412.305977106094, 'accumulated_submission_time': 16884.944253206253, 'accumulated_eval_time': 1518.5584359169006, 'accumulated_logging_time': 4.390644311904907, 'global_step': 43435, 'preemption_count': 0}), (44751, {'train/accuracy': 0.7056361436843872, 'train/loss': 1.1494941711425781, 'validation/accuracy': 0.6333799958229065, 'validation/loss': 1.5034329891204834, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.183959484100342, 'test/num_examples': 10000, 'score': 17394.775994062424, 'total_duration': 18968.783734560013, 'accumulated_submission_time': 17394.775994062424, 'accumulated_eval_time': 1564.9447996616364, 'accumulated_logging_time': 4.51846170425415, 'global_step': 44751, 'preemption_count': 0}), (46066, {'train/accuracy': 0.7078284025192261, 'train/loss': 1.1308703422546387, 'validation/accuracy': 0.6412799954414368, 'validation/loss': 1.4750566482543945, 'validation/num_examples': 50000, 'test/accuracy': 0.5138000249862671, 'test/loss': 2.216357469558716, 'test/num_examples': 10000, 'score': 17904.899745464325, 'total_duration': 19525.015421628952, 'accumulated_submission_time': 17904.899745464325, 'accumulated_eval_time': 1610.8338239192963, 'accumulated_logging_time': 4.603740692138672, 'global_step': 46066, 'preemption_count': 0}), (47380, {'train/accuracy': 0.7111766338348389, 'train/loss': 1.115090250968933, 'validation/accuracy': 0.6467399597167969, 'validation/loss': 1.455227255821228, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.187701463699341, 'test/num_examples': 10000, 'score': 18414.812789201736, 'total_duration': 20080.279507637024, 'accumulated_submission_time': 18414.812789201736, 'accumulated_eval_time': 1655.9438173770905, 'accumulated_logging_time': 4.706700086593628, 'global_step': 47380, 'preemption_count': 0}), (48696, {'train/accuracy': 0.7036033272743225, 'train/loss': 1.1529393196105957, 'validation/accuracy': 0.6352399587631226, 'validation/loss': 1.5055887699127197, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.2442681789398193, 'test/num_examples': 10000, 'score': 18924.855459690094, 'total_duration': 20625.278146266937, 'accumulated_submission_time': 18924.855459690094, 'accumulated_eval_time': 1690.6081550121307, 'accumulated_logging_time': 4.8652544021606445, 'global_step': 48696, 'preemption_count': 0}), (50017, {'train/accuracy': 0.7104990482330322, 'train/loss': 1.1222102642059326, 'validation/accuracy': 0.6417999863624573, 'validation/loss': 1.4830840826034546, 'validation/num_examples': 50000, 'test/accuracy': 0.5064000487327576, 'test/loss': 2.255596876144409, 'test/num_examples': 10000, 'score': 19434.673849105835, 'total_duration': 21182.41639828682, 'accumulated_submission_time': 19434.673849105835, 'accumulated_eval_time': 1737.6073410511017, 'accumulated_logging_time': 5.051330804824829, 'global_step': 50017, 'preemption_count': 0}), (51332, {'train/accuracy': 0.7073301672935486, 'train/loss': 1.131919503211975, 'validation/accuracy': 0.6402199864387512, 'validation/loss': 1.47739839553833, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2346036434173584, 'test/num_examples': 10000, 'score': 19944.40364098549, 'total_duration': 21732.246586322784, 'accumulated_submission_time': 19944.40364098549, 'accumulated_eval_time': 1777.4099655151367, 'accumulated_logging_time': 5.213693857192993, 'global_step': 51332, 'preemption_count': 0}), (52651, {'train/accuracy': 0.6966677308082581, 'train/loss': 1.1785019636154175, 'validation/accuracy': 0.6322799921035767, 'validation/loss': 1.5149649381637573, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.2282726764678955, 'test/num_examples': 10000, 'score': 20454.419619083405, 'total_duration': 22283.516021966934, 'accumulated_submission_time': 20454.419619083405, 'accumulated_eval_time': 1818.3890566825867, 'accumulated_logging_time': 5.356108903884888, 'global_step': 52651, 'preemption_count': 0}), (53965, {'train/accuracy': 0.7091039419174194, 'train/loss': 1.1363623142242432, 'validation/accuracy': 0.6449199914932251, 'validation/loss': 1.4615753889083862, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.2126657962799072, 'test/num_examples': 10000, 'score': 20964.578180789948, 'total_duration': 22830.6040391922, 'accumulated_submission_time': 20964.578180789948, 'accumulated_eval_time': 1855.0715210437775, 'accumulated_logging_time': 5.467150449752808, 'global_step': 53965, 'preemption_count': 0}), (55280, {'train/accuracy': 0.7094627022743225, 'train/loss': 1.1254081726074219, 'validation/accuracy': 0.6401399970054626, 'validation/loss': 1.4917874336242676, 'validation/num_examples': 50000, 'test/accuracy': 0.517300009727478, 'test/loss': 2.218022584915161, 'test/num_examples': 10000, 'score': 21474.393956899643, 'total_duration': 23382.731903076172, 'accumulated_submission_time': 21474.393956899643, 'accumulated_eval_time': 1897.1430563926697, 'accumulated_logging_time': 5.57506251335144, 'global_step': 55280, 'preemption_count': 0}), (56525, {'train/accuracy': 0.7144650816917419, 'train/loss': 1.1070650815963745, 'validation/accuracy': 0.6505599617958069, 'validation/loss': 1.439046859741211, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.1772494316101074, 'test/num_examples': 10000, 'score': 21984.491243124008, 'total_duration': 23933.158789396286, 'accumulated_submission_time': 21984.491243124008, 'accumulated_eval_time': 1937.2310678958893, 'accumulated_logging_time': 5.685719013214111, 'global_step': 56525, 'preemption_count': 0}), (57836, {'train/accuracy': 0.717793345451355, 'train/loss': 1.070703387260437, 'validation/accuracy': 0.652459979057312, 'validation/loss': 1.4229189157485962, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1448540687561035, 'test/num_examples': 10000, 'score': 22494.473898887634, 'total_duration': 24482.028872966766, 'accumulated_submission_time': 22494.473898887634, 'accumulated_eval_time': 1975.8591182231903, 'accumulated_logging_time': 5.807201862335205, 'global_step': 57836, 'preemption_count': 0}), (59135, {'train/accuracy': 0.7215202450752258, 'train/loss': 1.073554515838623, 'validation/accuracy': 0.6504600048065186, 'validation/loss': 1.4403984546661377, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.1634931564331055, 'test/num_examples': 10000, 'score': 23004.26758122444, 'total_duration': 25032.068063259125, 'accumulated_submission_time': 23004.26758122444, 'accumulated_eval_time': 2015.8443415164948, 'accumulated_logging_time': 5.917670249938965, 'global_step': 59135, 'preemption_count': 0}), (60428, {'train/accuracy': 0.7120535373687744, 'train/loss': 1.1217913627624512, 'validation/accuracy': 0.643779993057251, 'validation/loss': 1.468058705329895, 'validation/num_examples': 50000, 'test/accuracy': 0.5224000215530396, 'test/loss': 2.160226821899414, 'test/num_examples': 10000, 'score': 23514.374061346054, 'total_duration': 25578.937777996063, 'accumulated_submission_time': 23514.374061346054, 'accumulated_eval_time': 2052.315071105957, 'accumulated_logging_time': 6.05895209312439, 'global_step': 60428, 'preemption_count': 0}), (61714, {'train/accuracy': 0.7234534025192261, 'train/loss': 1.0633902549743652, 'validation/accuracy': 0.6550799608230591, 'validation/loss': 1.4097949266433716, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.108109474182129, 'test/num_examples': 10000, 'score': 24024.109657764435, 'total_duration': 26127.42178440094, 'accumulated_submission_time': 24024.109657764435, 'accumulated_eval_time': 2090.7561309337616, 'accumulated_logging_time': 6.214584827423096, 'global_step': 61714, 'preemption_count': 0}), (62999, {'train/accuracy': 0.7166174650192261, 'train/loss': 1.0886846780776978, 'validation/accuracy': 0.6485599875450134, 'validation/loss': 1.4411242008209229, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.166180372238159, 'test/num_examples': 10000, 'score': 24534.139717817307, 'total_duration': 26675.91220641136, 'accumulated_submission_time': 24534.139717817307, 'accumulated_eval_time': 2128.9455440044403, 'accumulated_logging_time': 6.331876516342163, 'global_step': 62999, 'preemption_count': 0}), (64290, {'train/accuracy': 0.7254264950752258, 'train/loss': 1.0430907011032104, 'validation/accuracy': 0.6535199880599976, 'validation/loss': 1.4232927560806274, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.1406497955322266, 'test/num_examples': 10000, 'score': 25044.27539539337, 'total_duration': 27223.770844459534, 'accumulated_submission_time': 25044.27539539337, 'accumulated_eval_time': 2166.374985933304, 'accumulated_logging_time': 6.471105575561523, 'global_step': 64290, 'preemption_count': 0}), (65574, {'train/accuracy': 0.7141461968421936, 'train/loss': 1.1031270027160645, 'validation/accuracy': 0.6446599960327148, 'validation/loss': 1.4697213172912598, 'validation/num_examples': 50000, 'test/accuracy': 0.5164000391960144, 'test/loss': 2.213465452194214, 'test/num_examples': 10000, 'score': 25554.087022304535, 'total_duration': 27767.756018161774, 'accumulated_submission_time': 25554.087022304535, 'accumulated_eval_time': 2200.294060945511, 'accumulated_logging_time': 6.5701744556427, 'global_step': 65574, 'preemption_count': 0}), (66761, {'train/accuracy': 0.7280372977256775, 'train/loss': 1.04458487033844, 'validation/accuracy': 0.652899980545044, 'validation/loss': 1.4218896627426147, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.1242177486419678, 'test/num_examples': 10000, 'score': 26064.145768642426, 'total_duration': 28319.94919037819, 'accumulated_submission_time': 26064.145768642426, 'accumulated_eval_time': 2242.11728143692, 'accumulated_logging_time': 6.736119270324707, 'global_step': 66761, 'preemption_count': 0}), (68044, {'train/accuracy': 0.7293526530265808, 'train/loss': 1.0390655994415283, 'validation/accuracy': 0.6547799706459045, 'validation/loss': 1.4202443361282349, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.1513993740081787, 'test/num_examples': 10000, 'score': 26574.129656791687, 'total_duration': 28866.996738910675, 'accumulated_submission_time': 26574.129656791687, 'accumulated_eval_time': 2278.824225664139, 'accumulated_logging_time': 6.938032627105713, 'global_step': 68044, 'preemption_count': 0}), (69296, {'train/accuracy': 0.7253667116165161, 'train/loss': 1.0567028522491455, 'validation/accuracy': 0.6482399702072144, 'validation/loss': 1.449615240097046, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.192610502243042, 'test/num_examples': 10000, 'score': 27083.97568178177, 'total_duration': 29413.650495052338, 'accumulated_submission_time': 27083.97568178177, 'accumulated_eval_time': 2315.288168668747, 'accumulated_logging_time': 7.128679275512695, 'global_step': 69296, 'preemption_count': 0}), (70572, {'train/accuracy': 0.7393574714660645, 'train/loss': 1.0069780349731445, 'validation/accuracy': 0.6589999794960022, 'validation/loss': 1.3940340280532837, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.0888493061065674, 'test/num_examples': 10000, 'score': 27593.986520051956, 'total_duration': 29959.30716943741, 'accumulated_submission_time': 27593.986520051956, 'accumulated_eval_time': 2350.641744375229, 'accumulated_logging_time': 7.267857789993286, 'global_step': 70572, 'preemption_count': 0}), (71844, {'train/accuracy': 0.7378826141357422, 'train/loss': 1.0114717483520508, 'validation/accuracy': 0.6562199592590332, 'validation/loss': 1.416731834411621, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.1171469688415527, 'test/num_examples': 10000, 'score': 28104.007316589355, 'total_duration': 30501.58286857605, 'accumulated_submission_time': 28104.007316589355, 'accumulated_eval_time': 2382.5699660778046, 'accumulated_logging_time': 7.438830137252808, 'global_step': 71844, 'preemption_count': 0}), (73108, {'train/accuracy': 0.7170957922935486, 'train/loss': 1.0847793817520142, 'validation/accuracy': 0.6380400061607361, 'validation/loss': 1.5005325078964233, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.293741464614868, 'test/num_examples': 10000, 'score': 28614.14536833763, 'total_duration': 31046.706951379776, 'accumulated_submission_time': 28614.14536833763, 'accumulated_eval_time': 2417.302140235901, 'accumulated_logging_time': 7.5397725105285645, 'global_step': 73108, 'preemption_count': 0}), (74371, {'train/accuracy': 0.7361088991165161, 'train/loss': 1.0086668729782104, 'validation/accuracy': 0.6509199738502502, 'validation/loss': 1.4272668361663818, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.1644039154052734, 'test/num_examples': 10000, 'score': 29124.007921934128, 'total_duration': 31588.812475442886, 'accumulated_submission_time': 29124.007921934128, 'accumulated_eval_time': 2449.228042125702, 'accumulated_logging_time': 7.704905986785889, 'global_step': 74371, 'preemption_count': 0}), (75619, {'train/accuracy': 0.7685546875, 'train/loss': 0.8727958798408508, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.329728603363037, 'validation/num_examples': 50000, 'test/accuracy': 0.5484000444412231, 'test/loss': 2.065916061401367, 'test/num_examples': 10000, 'score': 29633.580072164536, 'total_duration': 32137.35546541214, 'accumulated_submission_time': 29633.580072164536, 'accumulated_eval_time': 2487.4017350673676, 'accumulated_logging_time': 8.353960514068604, 'global_step': 75619, 'preemption_count': 0}), (76891, {'train/accuracy': 0.7476482391357422, 'train/loss': 0.9540586471557617, 'validation/accuracy': 0.6596399545669556, 'validation/loss': 1.3977417945861816, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.172443389892578, 'test/num_examples': 10000, 'score': 30143.65061402321, 'total_duration': 32681.122876167297, 'accumulated_submission_time': 30143.65061402321, 'accumulated_eval_time': 2520.8206589221954, 'accumulated_logging_time': 8.47841191291809, 'global_step': 76891, 'preemption_count': 0}), (78146, {'train/accuracy': 0.7475486397743225, 'train/loss': 0.959528923034668, 'validation/accuracy': 0.6498799920082092, 'validation/loss': 1.437365174293518, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.175877332687378, 'test/num_examples': 10000, 'score': 30653.61021065712, 'total_duration': 33227.571417331696, 'accumulated_submission_time': 30653.61021065712, 'accumulated_eval_time': 2557.068140029907, 'accumulated_logging_time': 8.571596622467041, 'global_step': 78146, 'preemption_count': 0}), (79414, {'train/accuracy': 0.767996609210968, 'train/loss': 0.8771113157272339, 'validation/accuracy': 0.659339964389801, 'validation/loss': 1.4158713817596436, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.182405948638916, 'test/num_examples': 10000, 'score': 31163.34557890892, 'total_duration': 33770.92522621155, 'accumulated_submission_time': 31163.34557890892, 'accumulated_eval_time': 2590.387677192688, 'accumulated_logging_time': 8.721405744552612, 'global_step': 79414, 'preemption_count': 0}), (80660, {'train/accuracy': 0.7736766338348389, 'train/loss': 0.8552953004837036, 'validation/accuracy': 0.6590999960899353, 'validation/loss': 1.4029873609542847, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.1389479637145996, 'test/num_examples': 10000, 'score': 31673.28662133217, 'total_duration': 34317.52774596214, 'accumulated_submission_time': 31673.28662133217, 'accumulated_eval_time': 2626.759115457535, 'accumulated_logging_time': 8.864092111587524, 'global_step': 80660, 'preemption_count': 0}), (81888, {'train/accuracy': 0.7417091727256775, 'train/loss': 0.9829739928245544, 'validation/accuracy': 0.6683599948883057, 'validation/loss': 1.3575916290283203, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.074643850326538, 'test/num_examples': 10000, 'score': 32183.258754968643, 'total_duration': 34864.22689676285, 'accumulated_submission_time': 32183.258754968643, 'accumulated_eval_time': 2663.212234735489, 'accumulated_logging_time': 8.993061304092407, 'global_step': 81888, 'preemption_count': 0}), (83025, {'train/accuracy': 0.7388990521430969, 'train/loss': 0.988299548625946, 'validation/accuracy': 0.6671599745750427, 'validation/loss': 1.3549338579177856, 'validation/num_examples': 50000, 'test/accuracy': 0.5501000285148621, 'test/loss': 2.047982931137085, 'test/num_examples': 10000, 'score': 32695.61033463478, 'total_duration': 35411.12110495567, 'accumulated_submission_time': 32695.61033463478, 'accumulated_eval_time': 2697.52627825737, 'accumulated_logging_time': 9.089037656784058, 'global_step': 83025, 'preemption_count': 0}), (84227, {'train/accuracy': 0.7450773119926453, 'train/loss': 0.9722169041633606, 'validation/accuracy': 0.6714199781417847, 'validation/loss': 1.335518717765808, 'validation/num_examples': 50000, 'test/accuracy': 0.5491000413894653, 'test/loss': 2.0356361865997314, 'test/num_examples': 10000, 'score': 33205.59354376793, 'total_duration': 35958.5955247879, 'accumulated_submission_time': 33205.59354376793, 'accumulated_eval_time': 2734.6829755306244, 'accumulated_logging_time': 9.282243013381958, 'global_step': 84227, 'preemption_count': 0}), (85269, {'train/accuracy': 0.7497209906578064, 'train/loss': 0.9389574527740479, 'validation/accuracy': 0.6784600019454956, 'validation/loss': 1.3218321800231934, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.0571517944335938, 'test/num_examples': 10000, 'score': 33715.77509355545, 'total_duration': 36502.20847797394, 'accumulated_submission_time': 33715.77509355545, 'accumulated_eval_time': 2767.8602056503296, 'accumulated_logging_time': 9.414495944976807, 'global_step': 85269, 'preemption_count': 0}), (86461, {'train/accuracy': 0.7471300959587097, 'train/loss': 0.9460687041282654, 'validation/accuracy': 0.6688399910926819, 'validation/loss': 1.3449113368988037, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.1063642501831055, 'test/num_examples': 10000, 'score': 34225.94098520279, 'total_duration': 37049.028126716614, 'accumulated_submission_time': 34225.94098520279, 'accumulated_eval_time': 2804.2315702438354, 'accumulated_logging_time': 9.556679487228394, 'global_step': 86461, 'preemption_count': 0}), (87571, {'train/accuracy': 0.7589684128761292, 'train/loss': 0.9046853184700012, 'validation/accuracy': 0.6720199584960938, 'validation/loss': 1.325779914855957, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.0510897636413574, 'test/num_examples': 10000, 'score': 34736.0363342762, 'total_duration': 37596.98733854294, 'accumulated_submission_time': 34736.0363342762, 'accumulated_eval_time': 2841.825268507004, 'accumulated_logging_time': 9.698861360549927, 'global_step': 87571, 'preemption_count': 0}), (88647, {'train/accuracy': 0.7711256146430969, 'train/loss': 0.8675334453582764, 'validation/accuracy': 0.6724199652671814, 'validation/loss': 1.342900276184082, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.102644920349121, 'test/num_examples': 10000, 'score': 35245.79747009277, 'total_duration': 38143.58253598213, 'accumulated_submission_time': 35245.79747009277, 'accumulated_eval_time': 2878.411741733551, 'accumulated_logging_time': 9.825964450836182, 'global_step': 88647, 'preemption_count': 0}), (89772, {'train/accuracy': 0.7669004797935486, 'train/loss': 0.885750412940979, 'validation/accuracy': 0.6752200126647949, 'validation/loss': 1.312709927558899, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.0189590454101562, 'test/num_examples': 10000, 'score': 35755.64154100418, 'total_duration': 38689.93389034271, 'accumulated_submission_time': 35755.64154100418, 'accumulated_eval_time': 2914.518481016159, 'accumulated_logging_time': 10.097168207168579, 'global_step': 89772, 'preemption_count': 0}), (90868, {'train/accuracy': 0.75, 'train/loss': 0.9514040350914001, 'validation/accuracy': 0.6718599796295166, 'validation/loss': 1.3213268518447876, 'validation/num_examples': 50000, 'test/accuracy': 0.5414000153541565, 'test/loss': 2.0719118118286133, 'test/num_examples': 10000, 'score': 36265.79408097267, 'total_duration': 39235.888822078705, 'accumulated_submission_time': 36265.79408097267, 'accumulated_eval_time': 2950.0843873023987, 'accumulated_logging_time': 10.208319902420044, 'global_step': 90868, 'preemption_count': 0}), (91693, {'train/accuracy': 0.7562180757522583, 'train/loss': 0.9143868088722229, 'validation/accuracy': 0.6722999811172485, 'validation/loss': 1.3276240825653076, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.054367780685425, 'test/num_examples': 10000, 'score': 36775.99167513847, 'total_duration': 39781.77725291252, 'accumulated_submission_time': 36775.99167513847, 'accumulated_eval_time': 2985.5196537971497, 'accumulated_logging_time': 10.369922876358032, 'global_step': 91693, 'preemption_count': 0}), (92483, {'train/accuracy': 0.8000836968421936, 'train/loss': 0.7523419260978699, 'validation/accuracy': 0.6754199862480164, 'validation/loss': 1.3226666450500488, 'validation/num_examples': 50000, 'test/accuracy': 0.5487000346183777, 'test/loss': 2.045224189758301, 'test/num_examples': 10000, 'score': 37286.58005976677, 'total_duration': 40332.02872085571, 'accumulated_submission_time': 37286.58005976677, 'accumulated_eval_time': 3024.94624209404, 'accumulated_logging_time': 10.518640518188477, 'global_step': 92483, 'preemption_count': 0}), (93451, {'train/accuracy': 0.7322424650192261, 'train/loss': 1.0223578214645386, 'validation/accuracy': 0.6579999923706055, 'validation/loss': 1.4051156044006348, 'validation/num_examples': 50000, 'test/accuracy': 0.5319000482559204, 'test/loss': 2.1515700817108154, 'test/num_examples': 10000, 'score': 37796.891820430756, 'total_duration': 40879.16563129425, 'accumulated_submission_time': 37796.891820430756, 'accumulated_eval_time': 3061.5222041606903, 'accumulated_logging_time': 10.657094240188599, 'global_step': 93451, 'preemption_count': 0}), (94617, {'train/accuracy': 0.750996470451355, 'train/loss': 0.9445841312408447, 'validation/accuracy': 0.6700599789619446, 'validation/loss': 1.3443413972854614, 'validation/num_examples': 50000, 'test/accuracy': 0.5450000166893005, 'test/loss': 2.0809412002563477, 'test/num_examples': 10000, 'score': 38306.863139629364, 'total_duration': 41422.6756465435, 'accumulated_submission_time': 38306.863139629364, 'accumulated_eval_time': 3094.767350912094, 'accumulated_logging_time': 10.809594631195068, 'global_step': 94617, 'preemption_count': 0}), (95919, {'train/accuracy': 0.7483457922935486, 'train/loss': 0.9407371282577515, 'validation/accuracy': 0.6692799925804138, 'validation/loss': 1.3309261798858643, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.055803060531616, 'test/num_examples': 10000, 'score': 38816.94192481041, 'total_duration': 41961.91824746132, 'accumulated_submission_time': 38816.94192481041, 'accumulated_eval_time': 3123.677096605301, 'accumulated_logging_time': 10.915040493011475, 'global_step': 95919, 'preemption_count': 0}), (97207, {'train/accuracy': 0.7700294852256775, 'train/loss': 0.8650060892105103, 'validation/accuracy': 0.6903199553489685, 'validation/loss': 1.2660845518112183, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9759856462478638, 'test/num_examples': 10000, 'score': 39326.68496346474, 'total_duration': 42507.3515226841, 'accumulated_submission_time': 39326.68496346474, 'accumulated_eval_time': 3159.1033103466034, 'accumulated_logging_time': 11.028908252716064, 'global_step': 97207, 'preemption_count': 0}), (98493, {'train/accuracy': 0.7604033350944519, 'train/loss': 0.9039230346679688, 'validation/accuracy': 0.6824199557304382, 'validation/loss': 1.2977700233459473, 'validation/num_examples': 50000, 'test/accuracy': 0.5514000058174133, 'test/loss': 2.0156543254852295, 'test/num_examples': 10000, 'score': 39836.60939192772, 'total_duration': 43050.827724933624, 'accumulated_submission_time': 39836.60939192772, 'accumulated_eval_time': 3192.3785531520844, 'accumulated_logging_time': 11.154204845428467, 'global_step': 98493, 'preemption_count': 0}), (99778, {'train/accuracy': 0.7584303021430969, 'train/loss': 0.9036785364151001, 'validation/accuracy': 0.6762199997901917, 'validation/loss': 1.312163233757019, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.032076597213745, 'test/num_examples': 10000, 'score': 40346.70287466049, 'total_duration': 43595.645894765854, 'accumulated_submission_time': 40346.70287466049, 'accumulated_eval_time': 3226.813953638077, 'accumulated_logging_time': 11.288993120193481, 'global_step': 99778, 'preemption_count': 0}), (101064, {'train/accuracy': 0.7598652839660645, 'train/loss': 0.8957650065422058, 'validation/accuracy': 0.6802799701690674, 'validation/loss': 1.3044817447662354, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.010774612426758, 'test/num_examples': 10000, 'score': 40856.48312306404, 'total_duration': 44137.893067359924, 'accumulated_submission_time': 40856.48312306404, 'accumulated_eval_time': 3259.0123479366302, 'accumulated_logging_time': 11.401992082595825, 'global_step': 101064, 'preemption_count': 0}), (102359, {'train/accuracy': 0.7714245915412903, 'train/loss': 0.8496510982513428, 'validation/accuracy': 0.6862999796867371, 'validation/loss': 1.2820326089859009, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 1.984736442565918, 'test/num_examples': 10000, 'score': 41366.31562757492, 'total_duration': 44680.28909254074, 'accumulated_submission_time': 41366.31562757492, 'accumulated_eval_time': 3291.261964559555, 'accumulated_logging_time': 11.561526775360107, 'global_step': 102359, 'preemption_count': 0}), (103652, {'train/accuracy': 0.7772639989852905, 'train/loss': 0.8298704028129578, 'validation/accuracy': 0.6933799982070923, 'validation/loss': 1.249065637588501, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9656987190246582, 'test/num_examples': 10000, 'score': 41876.10270237923, 'total_duration': 45221.87944674492, 'accumulated_submission_time': 41876.10270237923, 'accumulated_eval_time': 3322.7491459846497, 'accumulated_logging_time': 11.72409725189209, 'global_step': 103652, 'preemption_count': 0}), (104943, {'train/accuracy': 0.7734375, 'train/loss': 0.8351027369499207, 'validation/accuracy': 0.6850000023841858, 'validation/loss': 1.2898337841033936, 'validation/num_examples': 50000, 'test/accuracy': 0.5534000396728516, 'test/loss': 2.0251309871673584, 'test/num_examples': 10000, 'score': 42385.95104813576, 'total_duration': 45767.339889764786, 'accumulated_submission_time': 42385.95104813576, 'accumulated_eval_time': 3358.118741750717, 'accumulated_logging_time': 11.811901330947876, 'global_step': 104943, 'preemption_count': 0}), (106233, {'train/accuracy': 0.7788584232330322, 'train/loss': 0.8187527656555176, 'validation/accuracy': 0.6909399628639221, 'validation/loss': 1.2568820714950562, 'validation/num_examples': 50000, 'test/accuracy': 0.5570000410079956, 'test/loss': 2.028005838394165, 'test/num_examples': 10000, 'score': 42895.79862046242, 'total_duration': 46311.962373018265, 'accumulated_submission_time': 42895.79862046242, 'accumulated_eval_time': 3392.6413123607635, 'accumulated_logging_time': 11.910680294036865, 'global_step': 106233, 'preemption_count': 0}), (107516, {'train/accuracy': 0.7691127061843872, 'train/loss': 0.8598592281341553, 'validation/accuracy': 0.6838399767875671, 'validation/loss': 1.2917050123214722, 'validation/num_examples': 50000, 'test/accuracy': 0.5605000257492065, 'test/loss': 2.003894090652466, 'test/num_examples': 10000, 'score': 43405.666311979294, 'total_duration': 46856.389553785324, 'accumulated_submission_time': 43405.666311979294, 'accumulated_eval_time': 3426.847248315811, 'accumulated_logging_time': 12.112124681472778, 'global_step': 107516, 'preemption_count': 0}), (108795, {'train/accuracy': 0.7829440236091614, 'train/loss': 0.8076595067977905, 'validation/accuracy': 0.6936999559402466, 'validation/loss': 1.2444788217544556, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 1.9626424312591553, 'test/num_examples': 10000, 'score': 43915.451953172684, 'total_duration': 47397.95249414444, 'accumulated_submission_time': 43915.451953172684, 'accumulated_eval_time': 3458.3125355243683, 'accumulated_logging_time': 12.270926475524902, 'global_step': 108795, 'preemption_count': 0}), (110076, {'train/accuracy': 0.7724409699440002, 'train/loss': 0.8382278084754944, 'validation/accuracy': 0.6887399554252625, 'validation/loss': 1.271635890007019, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.9834569692611694, 'test/num_examples': 10000, 'score': 44425.26453781128, 'total_duration': 47943.53843379021, 'accumulated_submission_time': 44425.26453781128, 'accumulated_eval_time': 3493.8105325698853, 'accumulated_logging_time': 12.391745567321777, 'global_step': 110076, 'preemption_count': 0}), (111351, {'train/accuracy': 0.7835817933082581, 'train/loss': 0.8117202520370483, 'validation/accuracy': 0.6911799907684326, 'validation/loss': 1.25642728805542, 'validation/num_examples': 50000, 'test/accuracy': 0.5667999982833862, 'test/loss': 1.955601453781128, 'test/num_examples': 10000, 'score': 44935.08746623993, 'total_duration': 48490.03379917145, 'accumulated_submission_time': 44935.08746623993, 'accumulated_eval_time': 3530.197116136551, 'accumulated_logging_time': 12.525977611541748, 'global_step': 111351, 'preemption_count': 0}), (112634, {'train/accuracy': 0.790437638759613, 'train/loss': 0.7606332302093506, 'validation/accuracy': 0.697219967842102, 'validation/loss': 1.2452595233917236, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.984863519668579, 'test/num_examples': 10000, 'score': 45445.11686396599, 'total_duration': 49032.18830418587, 'accumulated_submission_time': 45445.11686396599, 'accumulated_eval_time': 3562.0277366638184, 'accumulated_logging_time': 12.664294481277466, 'global_step': 112634, 'preemption_count': 0}), (113919, {'train/accuracy': 0.7876474857330322, 'train/loss': 0.7735476493835449, 'validation/accuracy': 0.6972999572753906, 'validation/loss': 1.234043836593628, 'validation/num_examples': 50000, 'test/accuracy': 0.5649999976158142, 'test/loss': 1.988587737083435, 'test/num_examples': 10000, 'score': 45955.16851425171, 'total_duration': 49577.261051893234, 'accumulated_submission_time': 45955.16851425171, 'accumulated_eval_time': 3596.7579474449158, 'accumulated_logging_time': 12.797095775604248, 'global_step': 113919, 'preemption_count': 0}), (115202, {'train/accuracy': 0.7921516299247742, 'train/loss': 0.7752204537391663, 'validation/accuracy': 0.6965599656105042, 'validation/loss': 1.230233907699585, 'validation/num_examples': 50000, 'test/accuracy': 0.5667000412940979, 'test/loss': 1.9672342538833618, 'test/num_examples': 10000, 'score': 46465.16586327553, 'total_duration': 50122.09089612961, 'accumulated_submission_time': 46465.16586327553, 'accumulated_eval_time': 3631.3194675445557, 'accumulated_logging_time': 12.909730911254883, 'global_step': 115202, 'preemption_count': 0}), (116485, {'train/accuracy': 0.7828842401504517, 'train/loss': 0.79611736536026, 'validation/accuracy': 0.6855999827384949, 'validation/loss': 1.2866547107696533, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 2.025076389312744, 'test/num_examples': 10000, 'score': 46974.98157310486, 'total_duration': 50664.08869457245, 'accumulated_submission_time': 46974.98157310486, 'accumulated_eval_time': 3663.16729593277, 'accumulated_logging_time': 13.08717393875122, 'global_step': 116485, 'preemption_count': 0}), (117772, {'train/accuracy': 0.8014987111091614, 'train/loss': 0.722296953201294, 'validation/accuracy': 0.7025799751281738, 'validation/loss': 1.2125309705734253, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 1.9241734743118286, 'test/num_examples': 10000, 'score': 47484.701085329056, 'total_duration': 51204.798934698105, 'accumulated_submission_time': 47484.701085329056, 'accumulated_eval_time': 3693.865710258484, 'accumulated_logging_time': 13.221311330795288, 'global_step': 117772, 'preemption_count': 0}), (119057, {'train/accuracy': 0.7967554330825806, 'train/loss': 0.7357949614524841, 'validation/accuracy': 0.697439968585968, 'validation/loss': 1.229734182357788, 'validation/num_examples': 50000, 'test/accuracy': 0.5702000260353088, 'test/loss': 1.9597786664962769, 'test/num_examples': 10000, 'score': 47994.67843961716, 'total_duration': 51748.61587405205, 'accumulated_submission_time': 47994.67843961716, 'accumulated_eval_time': 3727.4246740341187, 'accumulated_logging_time': 13.346659421920776, 'global_step': 119057, 'preemption_count': 0}), (120346, {'train/accuracy': 0.8049864172935486, 'train/loss': 0.7096879482269287, 'validation/accuracy': 0.7060799598693848, 'validation/loss': 1.1964306831359863, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.9114309549331665, 'test/num_examples': 10000, 'score': 48504.49068021774, 'total_duration': 52291.32509112358, 'accumulated_submission_time': 48504.49068021774, 'accumulated_eval_time': 3760.0704922676086, 'accumulated_logging_time': 13.443628549575806, 'global_step': 120346, 'preemption_count': 0}), (121629, {'train/accuracy': 0.8121811151504517, 'train/loss': 0.6799030303955078, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.1968052387237549, 'validation/num_examples': 50000, 'test/accuracy': 0.5857000350952148, 'test/loss': 1.9047188758850098, 'test/num_examples': 10000, 'score': 49014.4986975193, 'total_duration': 52837.1783952713, 'accumulated_submission_time': 49014.4986975193, 'accumulated_eval_time': 3795.641449689865, 'accumulated_logging_time': 13.564247608184814, 'global_step': 121629, 'preemption_count': 0}), (122929, {'train/accuracy': 0.8142139315605164, 'train/loss': 0.6730453968048096, 'validation/accuracy': 0.7076999545097351, 'validation/loss': 1.194522500038147, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 1.9256930351257324, 'test/num_examples': 10000, 'score': 49524.02431964874, 'total_duration': 53381.2328877449, 'accumulated_submission_time': 49524.02431964874, 'accumulated_eval_time': 3829.3525273799896, 'accumulated_logging_time': 14.227649211883545, 'global_step': 122929, 'preemption_count': 0}), (124233, {'train/accuracy': 0.8074776530265808, 'train/loss': 0.6959711909294128, 'validation/accuracy': 0.7015599608421326, 'validation/loss': 1.2140841484069824, 'validation/num_examples': 50000, 'test/accuracy': 0.5718000531196594, 'test/loss': 1.9616262912750244, 'test/num_examples': 10000, 'score': 50033.93161869049, 'total_duration': 53926.7450504303, 'accumulated_submission_time': 50033.93161869049, 'accumulated_eval_time': 3864.6186389923096, 'accumulated_logging_time': 14.410242557525635, 'global_step': 124233, 'preemption_count': 0}), (125530, {'train/accuracy': 0.813895046710968, 'train/loss': 0.6682764887809753, 'validation/accuracy': 0.7048400044441223, 'validation/loss': 1.2078776359558105, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 1.9193578958511353, 'test/num_examples': 10000, 'score': 50543.704063653946, 'total_duration': 54470.67670702934, 'accumulated_submission_time': 50543.704063653946, 'accumulated_eval_time': 3898.4770679473877, 'accumulated_logging_time': 14.555945873260498, 'global_step': 125530, 'preemption_count': 0}), (126829, {'train/accuracy': 0.8191167116165161, 'train/loss': 0.6490090489387512, 'validation/accuracy': 0.7081999778747559, 'validation/loss': 1.2002969980239868, 'validation/num_examples': 50000, 'test/accuracy': 0.5841000080108643, 'test/loss': 1.9353251457214355, 'test/num_examples': 10000, 'score': 51053.50607872009, 'total_duration': 55009.9517827034, 'accumulated_submission_time': 51053.50607872009, 'accumulated_eval_time': 3927.6881256103516, 'accumulated_logging_time': 14.661714553833008, 'global_step': 126829, 'preemption_count': 0}), (128124, {'train/accuracy': 0.8212690949440002, 'train/loss': 0.6420150399208069, 'validation/accuracy': 0.7077800035476685, 'validation/loss': 1.1788913011550903, 'validation/num_examples': 50000, 'test/accuracy': 0.584600031375885, 'test/loss': 1.8811419010162354, 'test/num_examples': 10000, 'score': 51563.65951156616, 'total_duration': 55554.292761564255, 'accumulated_submission_time': 51563.65951156616, 'accumulated_eval_time': 3961.6257083415985, 'accumulated_logging_time': 14.754802227020264, 'global_step': 128124, 'preemption_count': 0}), (129422, {'train/accuracy': 0.8216477632522583, 'train/loss': 0.6355151534080505, 'validation/accuracy': 0.7138199806213379, 'validation/loss': 1.1791808605194092, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 1.8831253051757812, 'test/num_examples': 10000, 'score': 52073.43739271164, 'total_duration': 56098.79053258896, 'accumulated_submission_time': 52073.43739271164, 'accumulated_eval_time': 3996.07364821434, 'accumulated_logging_time': 14.870981454849243, 'global_step': 129422, 'preemption_count': 0}), (130719, {'train/accuracy': 0.8334861397743225, 'train/loss': 0.5989682078361511, 'validation/accuracy': 0.7184000015258789, 'validation/loss': 1.1502035856246948, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.8482716083526611, 'test/num_examples': 10000, 'score': 52583.355182886124, 'total_duration': 56644.942281246185, 'accumulated_submission_time': 52583.355182886124, 'accumulated_eval_time': 4032.0302834510803, 'accumulated_logging_time': 14.991738319396973, 'global_step': 130719, 'preemption_count': 0}), (132011, {'train/accuracy': 0.8279256820678711, 'train/loss': 0.6220022439956665, 'validation/accuracy': 0.7115600109100342, 'validation/loss': 1.166405200958252, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.8730454444885254, 'test/num_examples': 10000, 'score': 53093.251718997955, 'total_duration': 57187.666471004486, 'accumulated_submission_time': 53093.251718997955, 'accumulated_eval_time': 4064.59317445755, 'accumulated_logging_time': 15.10022497177124, 'global_step': 132011, 'preemption_count': 0}), (133304, {'train/accuracy': 0.83500075340271, 'train/loss': 0.5890035033226013, 'validation/accuracy': 0.7152799963951111, 'validation/loss': 1.1624947786331177, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.8634424209594727, 'test/num_examples': 10000, 'score': 53603.08537530899, 'total_duration': 57734.80880069733, 'accumulated_submission_time': 53603.08537530899, 'accumulated_eval_time': 4101.591737031937, 'accumulated_logging_time': 15.255291223526001, 'global_step': 133304, 'preemption_count': 0}), (134599, {'train/accuracy': 0.8369140625, 'train/loss': 0.581257700920105, 'validation/accuracy': 0.7176199555397034, 'validation/loss': 1.159517526626587, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8773891925811768, 'test/num_examples': 10000, 'score': 54113.132682561874, 'total_duration': 58279.87891817093, 'accumulated_submission_time': 54113.132682561874, 'accumulated_eval_time': 4136.3418889045715, 'accumulated_logging_time': 15.37397027015686, 'global_step': 134599, 'preemption_count': 0}), (135895, {'train/accuracy': 0.8416972160339355, 'train/loss': 0.5584900379180908, 'validation/accuracy': 0.7178399562835693, 'validation/loss': 1.1549780368804932, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8753383159637451, 'test/num_examples': 10000, 'score': 54622.9719285965, 'total_duration': 58823.592566251755, 'accumulated_submission_time': 54622.9719285965, 'accumulated_eval_time': 4169.887081384659, 'accumulated_logging_time': 15.546517372131348, 'global_step': 135895, 'preemption_count': 0}), (137196, {'train/accuracy': 0.844168484210968, 'train/loss': 0.5514883399009705, 'validation/accuracy': 0.7198799848556519, 'validation/loss': 1.1460130214691162, 'validation/num_examples': 50000, 'test/accuracy': 0.5911000370979309, 'test/loss': 1.8841530084609985, 'test/num_examples': 10000, 'score': 55133.01143240929, 'total_duration': 59368.53708696365, 'accumulated_submission_time': 55133.01143240929, 'accumulated_eval_time': 4204.505786418915, 'accumulated_logging_time': 15.671289682388306, 'global_step': 137196, 'preemption_count': 0}), (138486, {'train/accuracy': 0.8429527878761292, 'train/loss': 0.5433604121208191, 'validation/accuracy': 0.7161399722099304, 'validation/loss': 1.1705085039138794, 'validation/num_examples': 50000, 'test/accuracy': 0.593000054359436, 'test/loss': 1.8821163177490234, 'test/num_examples': 10000, 'score': 55643.017302036285, 'total_duration': 59915.34588932991, 'accumulated_submission_time': 55643.017302036285, 'accumulated_eval_time': 4241.016545295715, 'accumulated_logging_time': 15.809410333633423, 'global_step': 138486, 'preemption_count': 0}), (139763, {'train/accuracy': 0.8525190949440002, 'train/loss': 0.5119024515151978, 'validation/accuracy': 0.7247799634933472, 'validation/loss': 1.1486928462982178, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.8840183019638062, 'test/num_examples': 10000, 'score': 56153.03397631645, 'total_duration': 60460.70373916626, 'accumulated_submission_time': 56153.03397631645, 'accumulated_eval_time': 4276.064665317535, 'accumulated_logging_time': 15.949503660202026, 'global_step': 139763, 'preemption_count': 0}), (141037, {'train/accuracy': 0.8427335619926453, 'train/loss': 0.5529648661613464, 'validation/accuracy': 0.7173599600791931, 'validation/loss': 1.163743495941162, 'validation/num_examples': 50000, 'test/accuracy': 0.59170001745224, 'test/loss': 1.8985487222671509, 'test/num_examples': 10000, 'score': 56662.78712630272, 'total_duration': 61005.26335000992, 'accumulated_submission_time': 56662.78712630272, 'accumulated_eval_time': 4310.523617506027, 'accumulated_logging_time': 16.143863677978516, 'global_step': 141037, 'preemption_count': 0}), (142315, {'train/accuracy': 0.8552893400192261, 'train/loss': 0.5058260560035706, 'validation/accuracy': 0.7188999652862549, 'validation/loss': 1.1465741395950317, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.8645789623260498, 'test/num_examples': 10000, 'score': 57172.52275586128, 'total_duration': 61548.17818212509, 'accumulated_submission_time': 57172.52275586128, 'accumulated_eval_time': 4343.391114234924, 'accumulated_logging_time': 16.300902605056763, 'global_step': 142315, 'preemption_count': 0}), (143593, {'train/accuracy': 0.8539739847183228, 'train/loss': 0.5155287384986877, 'validation/accuracy': 0.7179399728775024, 'validation/loss': 1.158629298210144, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8788204193115234, 'test/num_examples': 10000, 'score': 57682.3403673172, 'total_duration': 62094.271691799164, 'accumulated_submission_time': 57682.3403673172, 'accumulated_eval_time': 4379.339458227158, 'accumulated_logging_time': 16.476032495498657, 'global_step': 143593, 'preemption_count': 0}), (144869, {'train/accuracy': 0.8717514276504517, 'train/loss': 0.45605209469795227, 'validation/accuracy': 0.7270199656486511, 'validation/loss': 1.1128953695297241, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.8197840452194214, 'test/num_examples': 10000, 'score': 58192.048651218414, 'total_duration': 62639.33973765373, 'accumulated_submission_time': 58192.048651218414, 'accumulated_eval_time': 4414.380907058716, 'accumulated_logging_time': 16.63759422302246, 'global_step': 144869, 'preemption_count': 0}), (146146, {'train/accuracy': 0.8713328838348389, 'train/loss': 0.45249906182289124, 'validation/accuracy': 0.7279599905014038, 'validation/loss': 1.1116821765899658, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.8199840784072876, 'test/num_examples': 10000, 'score': 58701.83473944664, 'total_duration': 63180.807953596115, 'accumulated_submission_time': 58701.83473944664, 'accumulated_eval_time': 4445.734078407288, 'accumulated_logging_time': 16.812229871749878, 'global_step': 146146, 'preemption_count': 0}), (147426, {'train/accuracy': 0.8796436190605164, 'train/loss': 0.4141806364059448, 'validation/accuracy': 0.7302199602127075, 'validation/loss': 1.1202220916748047, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.8784282207489014, 'test/num_examples': 10000, 'score': 59211.79710316658, 'total_duration': 63726.09299612045, 'accumulated_submission_time': 59211.79710316658, 'accumulated_eval_time': 4480.768987417221, 'accumulated_logging_time': 16.9438316822052, 'global_step': 147426, 'preemption_count': 0}), (148705, {'train/accuracy': 0.8806600570678711, 'train/loss': 0.4179016351699829, 'validation/accuracy': 0.728119969367981, 'validation/loss': 1.1285237073898315, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.8571628332138062, 'test/num_examples': 10000, 'score': 59721.9285428524, 'total_duration': 64268.67691755295, 'accumulated_submission_time': 59721.9285428524, 'accumulated_eval_time': 4512.960601806641, 'accumulated_logging_time': 17.049315690994263, 'global_step': 148705, 'preemption_count': 0}), (149985, {'train/accuracy': 0.8879743218421936, 'train/loss': 0.3844203054904938, 'validation/accuracy': 0.7346000075340271, 'validation/loss': 1.0968949794769287, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.8292913436889648, 'test/num_examples': 10000, 'score': 60231.869604349136, 'total_duration': 64814.25472855568, 'accumulated_submission_time': 60231.869604349136, 'accumulated_eval_time': 4548.327118396759, 'accumulated_logging_time': 17.165669202804565, 'global_step': 149985, 'preemption_count': 0}), (151273, {'train/accuracy': 0.8902861475944519, 'train/loss': 0.38157209753990173, 'validation/accuracy': 0.7341600060462952, 'validation/loss': 1.0990909337997437, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.8257477283477783, 'test/num_examples': 10000, 'score': 60741.602796792984, 'total_duration': 65358.6123790741, 'accumulated_submission_time': 60741.602796792984, 'accumulated_eval_time': 4582.6203083992, 'accumulated_logging_time': 17.34152913093567, 'global_step': 151273, 'preemption_count': 0}), (152558, {'train/accuracy': 0.8884725570678711, 'train/loss': 0.3839569389820099, 'validation/accuracy': 0.7287999987602234, 'validation/loss': 1.1154820919036865, 'validation/num_examples': 50000, 'test/accuracy': 0.6000000238418579, 'test/loss': 1.858032464981079, 'test/num_examples': 10000, 'score': 61251.35122394562, 'total_duration': 65899.88117957115, 'accumulated_submission_time': 61251.35122394562, 'accumulated_eval_time': 4613.8541967868805, 'accumulated_logging_time': 17.47600269317627, 'global_step': 152558, 'preemption_count': 0}), (153846, {'train/accuracy': 0.8997528553009033, 'train/loss': 0.3465679883956909, 'validation/accuracy': 0.7373200058937073, 'validation/loss': 1.0923019647598267, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.8296637535095215, 'test/num_examples': 10000, 'score': 61761.13899111748, 'total_duration': 66445.55115318298, 'accumulated_submission_time': 61761.13899111748, 'accumulated_eval_time': 4649.432509183884, 'accumulated_logging_time': 17.625362873077393, 'global_step': 153846, 'preemption_count': 0}), (155134, {'train/accuracy': 0.9030413031578064, 'train/loss': 0.3380180299282074, 'validation/accuracy': 0.737019956111908, 'validation/loss': 1.0878331661224365, 'validation/num_examples': 50000, 'test/accuracy': 0.6136000156402588, 'test/loss': 1.8199772834777832, 'test/num_examples': 10000, 'score': 62271.167729616165, 'total_duration': 66989.65263462067, 'accumulated_submission_time': 62271.167729616165, 'accumulated_eval_time': 4683.184887886047, 'accumulated_logging_time': 17.79177737236023, 'global_step': 155134, 'preemption_count': 0}), (156428, {'train/accuracy': 0.8995535373687744, 'train/loss': 0.34427598118782043, 'validation/accuracy': 0.7343199849128723, 'validation/loss': 1.1086012125015259, 'validation/num_examples': 50000, 'test/accuracy': 0.6107000112533569, 'test/loss': 1.8398256301879883, 'test/num_examples': 10000, 'score': 62781.21112012863, 'total_duration': 67533.49406647682, 'accumulated_submission_time': 62781.21112012863, 'accumulated_eval_time': 4716.691983222961, 'accumulated_logging_time': 17.92684841156006, 'global_step': 156428, 'preemption_count': 0}), (157716, {'train/accuracy': 0.9099968075752258, 'train/loss': 0.30783137679100037, 'validation/accuracy': 0.7361999750137329, 'validation/loss': 1.1030312776565552, 'validation/num_examples': 50000, 'test/accuracy': 0.6105000376701355, 'test/loss': 1.847373127937317, 'test/num_examples': 10000, 'score': 63291.10856127739, 'total_duration': 68078.4364669323, 'accumulated_submission_time': 63291.10856127739, 'accumulated_eval_time': 4751.414372205734, 'accumulated_logging_time': 18.090118646621704, 'global_step': 157716, 'preemption_count': 0}), (159013, {'train/accuracy': 0.9157166481018066, 'train/loss': 0.29050204157829285, 'validation/accuracy': 0.7391600012779236, 'validation/loss': 1.0969388484954834, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.8542455434799194, 'test/num_examples': 10000, 'score': 63800.86756539345, 'total_duration': 68623.29306054115, 'accumulated_submission_time': 63800.86756539345, 'accumulated_eval_time': 4786.232919216156, 'accumulated_logging_time': 18.210217475891113, 'global_step': 159013, 'preemption_count': 0}), (160301, {'train/accuracy': 0.9168925285339355, 'train/loss': 0.2939058840274811, 'validation/accuracy': 0.7389999628067017, 'validation/loss': 1.0933477878570557, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.8359034061431885, 'test/num_examples': 10000, 'score': 64310.629964113235, 'total_duration': 69166.55206394196, 'accumulated_submission_time': 64310.629964113235, 'accumulated_eval_time': 4819.381118774414, 'accumulated_logging_time': 18.402047157287598, 'global_step': 160301, 'preemption_count': 0}), (161587, {'train/accuracy': 0.9212173223495483, 'train/loss': 0.2696397304534912, 'validation/accuracy': 0.7414199709892273, 'validation/loss': 1.083556890487671, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.838761806488037, 'test/num_examples': 10000, 'score': 64820.405883550644, 'total_duration': 69714.07211208344, 'accumulated_submission_time': 64820.405883550644, 'accumulated_eval_time': 4856.885681629181, 'accumulated_logging_time': 18.480733394622803, 'global_step': 161587, 'preemption_count': 0}), (162881, {'train/accuracy': 0.9265385866165161, 'train/loss': 0.25606390833854675, 'validation/accuracy': 0.7444599866867065, 'validation/loss': 1.0853649377822876, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.8269352912902832, 'test/num_examples': 10000, 'score': 65330.30572605133, 'total_duration': 70257.86854076385, 'accumulated_submission_time': 65330.30572605133, 'accumulated_eval_time': 4890.514559745789, 'accumulated_logging_time': 18.592165231704712, 'global_step': 162881, 'preemption_count': 0}), (164174, {'train/accuracy': 0.9280930757522583, 'train/loss': 0.25396662950515747, 'validation/accuracy': 0.7455399632453918, 'validation/loss': 1.0867712497711182, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.8309104442596436, 'test/num_examples': 10000, 'score': 65840.21555304527, 'total_duration': 70799.13355851173, 'accumulated_submission_time': 65840.21555304527, 'accumulated_eval_time': 4921.598035573959, 'accumulated_logging_time': 18.710416793823242, 'global_step': 164174, 'preemption_count': 0})], 'global_step': 165467}
I0308 05:30:36.985515 140269360194752 submission_runner.py:649] Timing: 66350.04545640945
I0308 05:30:36.985560 140269360194752 submission_runner.py:651] Total number of evals: 130
I0308 05:30:36.985589 140269360194752 submission_runner.py:652] ====================
I0308 05:30:36.985836 140269360194752 submission_runner.py:750] Final imagenet_resnet score: 4
