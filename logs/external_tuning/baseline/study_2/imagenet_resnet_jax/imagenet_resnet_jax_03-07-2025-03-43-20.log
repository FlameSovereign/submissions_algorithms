python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=848369103 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-03-43-20.log
2025-03-07 03:43:33.980536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741319014.277180       8 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741319014.393142       8 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 03:44:14.356062 140393707492544 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax.
I0307 03:44:16.745577 140393707492544 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 03:44:16.748988 140393707492544 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 03:44:16.769827 140393707492544 submission_runner.py:606] Using RNG seed 848369103
I0307 03:44:20.999564 140393707492544 submission_runner.py:615] --- Tuning run 1/5 ---
I0307 03:44:20.999808 140393707492544 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_1.
I0307 03:44:21.000009 140393707492544 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_1/hparams.json.
I0307 03:44:21.233631 140393707492544 submission_runner.py:218] Initializing dataset.
I0307 03:44:22.318752 140393707492544 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:44:22.644609 140393707492544 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:44:22.935296 140393707492544 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:44:24.483274 140393707492544 submission_runner.py:229] Initializing model.
I0307 03:44:48.213724 140393707492544 submission_runner.py:272] Initializing optimizer.
I0307 03:44:49.299139 140393707492544 submission_runner.py:279] Initializing metrics bundle.
I0307 03:44:49.299391 140393707492544 submission_runner.py:301] Initializing checkpoint and logger.
I0307 03:44:49.300476 140393707492544 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0307 03:44:49.300601 140393707492544 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0307 03:44:49.836863 140393707492544 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_1/flags_0.json.
I0307 03:44:50.119240 140393707492544 submission_runner.py:337] Starting training loop.
I0307 03:45:46.323346 140256787560192 logging_writer.py:48] [0] global_step=0, grad_norm=0.6202682256698608, loss=6.932528972625732
I0307 03:45:46.688714 140393707492544 spec.py:321] Evaluating on the training split.
I0307 03:45:47.165158 140393707492544 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:45:47.189050 140393707492544 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:45:47.231865 140393707492544 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:46:06.493792 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 03:46:07.065986 140393707492544 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:46:07.074441 140393707492544 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:46:07.121552 140393707492544 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:46:54.280477 140393707492544 spec.py:349] Evaluating on the test split.
I0307 03:46:54.791831 140393707492544 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 03:46:54.819740 140393707492544 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 03:46:54.858189 140393707492544 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 03:47:11.610408 140393707492544 submission_runner.py:469] Time since start: 141.49s, 	Step: 1, 	{'train/accuracy': 0.0009167729294858873, 'train/loss': 6.91182279586792, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912854194641113, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.912630081176758, 'test/num_examples': 10000, 'score': 56.56920671463013, 'total_duration': 141.49113082885742, 'accumulated_submission_time': 56.56920671463013, 'accumulated_eval_time': 84.92164707183838, 'accumulated_logging_time': 0}
I0307 03:47:11.639765 140238131296000 logging_writer.py:48] [1] accumulated_eval_time=84.9216, accumulated_logging_time=0, accumulated_submission_time=56.5692, global_step=1, preemption_count=0, score=56.5692, test/accuracy=0.0008, test/loss=6.91263, test/num_examples=10000, total_duration=141.491, train/accuracy=0.000916773, train/loss=6.91182, validation/accuracy=0.00112, validation/loss=6.91285, validation/num_examples=50000
I0307 03:47:48.097687 140238122903296 logging_writer.py:48] [100] global_step=100, grad_norm=0.6362558007240295, loss=6.909478664398193
I0307 03:48:25.092427 140238131296000 logging_writer.py:48] [200] global_step=200, grad_norm=0.6048820614814758, loss=6.864731311798096
I0307 03:49:02.344908 140238122903296 logging_writer.py:48] [300] global_step=300, grad_norm=0.6488522291183472, loss=6.784732341766357
I0307 03:49:39.633328 140238131296000 logging_writer.py:48] [400] global_step=400, grad_norm=0.6943344473838806, loss=6.690184593200684
I0307 03:50:17.468597 140238122903296 logging_writer.py:48] [500] global_step=500, grad_norm=0.719430685043335, loss=6.638962745666504
I0307 03:50:55.473002 140238131296000 logging_writer.py:48] [600] global_step=600, grad_norm=0.742348849773407, loss=6.528713703155518
I0307 03:51:32.748195 140238122903296 logging_writer.py:48] [700] global_step=700, grad_norm=0.7854829430580139, loss=6.468485355377197
I0307 03:52:10.217298 140238131296000 logging_writer.py:48] [800] global_step=800, grad_norm=0.8151260018348694, loss=6.341543197631836
I0307 03:52:47.637261 140238122903296 logging_writer.py:48] [900] global_step=900, grad_norm=1.1707643270492554, loss=6.28414249420166
I0307 03:53:24.940351 140238131296000 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.012929677963257, loss=6.184823036193848
I0307 03:54:02.602742 140238122903296 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2310227155685425, loss=6.071354389190674
I0307 03:54:39.656918 140238131296000 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.9281944036483765, loss=6.0710649490356445
I0307 03:55:16.468698 140238122903296 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.2711071968078613, loss=5.887270927429199
I0307 03:55:41.803576 140393707492544 spec.py:321] Evaluating on the training split.
I0307 03:55:54.618878 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 03:56:16.261756 140393707492544 spec.py:349] Evaluating on the test split.
I0307 03:56:18.452025 140393707492544 submission_runner.py:469] Time since start: 688.33s, 	Step: 1368, 	{'train/accuracy': 0.0647321417927742, 'train/loss': 5.50593376159668, 'validation/accuracy': 0.05753999948501587, 'validation/loss': 5.589382171630859, 'validation/num_examples': 50000, 'test/accuracy': 0.040800001472234726, 'test/loss': 5.779533386230469, 'test/num_examples': 10000, 'score': 566.5438151359558, 'total_duration': 688.332738161087, 'accumulated_submission_time': 566.5438151359558, 'accumulated_eval_time': 121.57005262374878, 'accumulated_logging_time': 0.03919839859008789}
I0307 03:56:18.482357 140237678286592 logging_writer.py:48] [1368] accumulated_eval_time=121.57, accumulated_logging_time=0.0391984, accumulated_submission_time=566.544, global_step=1368, preemption_count=0, score=566.544, test/accuracy=0.0408, test/loss=5.77953, test/num_examples=10000, total_duration=688.333, train/accuracy=0.0647321, train/loss=5.50593, validation/accuracy=0.05754, validation/loss=5.58938, validation/num_examples=50000
I0307 03:56:30.936449 140237686679296 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.8996813297271729, loss=5.896853446960449
I0307 03:57:08.265216 140237678286592 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.58847713470459, loss=5.772598743438721
I0307 03:57:45.826312 140237686679296 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.228382110595703, loss=5.746158123016357
I0307 03:58:23.203010 140237678286592 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.876541018486023, loss=5.754790306091309
I0307 03:59:01.060892 140237686679296 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.225048065185547, loss=5.660632610321045
I0307 03:59:38.375866 140237678286592 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.1977274417877197, loss=5.560844898223877
I0307 04:00:16.091457 140237686679296 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.4117043018341064, loss=5.522372245788574
I0307 04:00:53.976152 140237678286592 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.561189651489258, loss=5.515624046325684
I0307 04:01:31.887161 140237686679296 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.0507264137268066, loss=5.454944133758545
I0307 04:02:09.880012 140237678286592 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.780999183654785, loss=5.415966033935547
I0307 04:02:47.924132 140237686679296 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.876652956008911, loss=5.341308116912842
I0307 04:03:25.941873 140237678286592 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.555086612701416, loss=5.29252290725708
I0307 04:04:04.265079 140237686679296 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.9764037132263184, loss=5.297334671020508
I0307 04:04:42.634314 140237678286592 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.7916977405548096, loss=5.186338424682617
I0307 04:04:48.816802 140393707492544 spec.py:321] Evaluating on the training split.
I0307 04:05:01.247406 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 04:05:23.595931 140393707492544 spec.py:349] Evaluating on the test split.
I0307 04:05:25.408103 140393707492544 submission_runner.py:469] Time since start: 1235.29s, 	Step: 2717, 	{'train/accuracy': 0.15676817297935486, 'train/loss': 4.436342239379883, 'validation/accuracy': 0.1378999948501587, 'validation/loss': 4.576536655426025, 'validation/num_examples': 50000, 'test/accuracy': 0.09760000556707382, 'test/loss': 4.984498977661133, 'test/num_examples': 10000, 'score': 1076.7115275859833, 'total_duration': 1235.2888140678406, 'accumulated_submission_time': 1076.7115275859833, 'accumulated_eval_time': 158.16130113601685, 'accumulated_logging_time': 0.07754254341125488}
I0307 04:05:25.429369 140237686679296 logging_writer.py:48] [2717] accumulated_eval_time=158.161, accumulated_logging_time=0.0775425, accumulated_submission_time=1076.71, global_step=2717, preemption_count=0, score=1076.71, test/accuracy=0.0976, test/loss=4.9845, test/num_examples=10000, total_duration=1235.29, train/accuracy=0.156768, train/loss=4.43634, validation/accuracy=0.1379, validation/loss=4.57654, validation/num_examples=50000
I0307 04:05:58.118180 140237678286592 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.098679065704346, loss=5.265782356262207
I0307 04:06:36.756260 140237686679296 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.121120452880859, loss=5.019757270812988
I0307 04:07:14.976197 140237678286592 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.4954774379730225, loss=5.0232439041137695
I0307 04:07:53.082016 140237686679296 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.5942373275756836, loss=5.1003570556640625
I0307 04:08:31.458979 140237678286592 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.3977668285369873, loss=4.947600841522217
I0307 04:09:09.712866 140237686679296 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.1628775596618652, loss=4.999529838562012
I0307 04:09:48.209338 140237678286592 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.610116958618164, loss=4.925786972045898
I0307 04:10:26.419560 140237686679296 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.841397762298584, loss=4.783037185668945
I0307 04:11:04.495734 140237678286592 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.843879699707031, loss=4.743556499481201
I0307 04:11:42.860637 140237686679296 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.291508674621582, loss=4.754148483276367
I0307 04:12:21.010159 140237678286592 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.535109758377075, loss=4.757413864135742
I0307 04:12:59.243871 140237686679296 logging_writer.py:48] [3900] global_step=3900, grad_norm=6.38149881362915, loss=4.62786340713501
I0307 04:13:37.359220 140237678286592 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.082390785217285, loss=4.6135711669921875
I0307 04:13:55.704048 140393707492544 spec.py:321] Evaluating on the training split.
I0307 04:14:07.181226 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 04:14:32.319311 140393707492544 spec.py:349] Evaluating on the test split.
I0307 04:14:34.132353 140393707492544 submission_runner.py:469] Time since start: 1784.01s, 	Step: 4048, 	{'train/accuracy': 0.26251593232154846, 'train/loss': 3.619271755218506, 'validation/accuracy': 0.2300799936056137, 'validation/loss': 3.8207767009735107, 'validation/num_examples': 50000, 'test/accuracy': 0.1680000126361847, 'test/loss': 4.3531413078308105, 'test/num_examples': 10000, 'score': 1586.8293161392212, 'total_duration': 1784.0130641460419, 'accumulated_submission_time': 1586.8293161392212, 'accumulated_eval_time': 196.58955931663513, 'accumulated_logging_time': 0.10748410224914551}
I0307 04:14:34.172226 140237686679296 logging_writer.py:48] [4048] accumulated_eval_time=196.59, accumulated_logging_time=0.107484, accumulated_submission_time=1586.83, global_step=4048, preemption_count=0, score=1586.83, test/accuracy=0.168, test/loss=4.35314, test/num_examples=10000, total_duration=1784.01, train/accuracy=0.262516, train/loss=3.61927, validation/accuracy=0.23008, validation/loss=3.82078, validation/num_examples=50000
I0307 04:14:54.821619 140237678286592 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.001585483551025, loss=4.596473693847656
I0307 04:15:32.638622 140237686679296 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.433298587799072, loss=4.50613260269165
I0307 04:16:10.800899 140237678286592 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.912297487258911, loss=4.515514850616455
I0307 04:16:49.118745 140237686679296 logging_writer.py:48] [4400] global_step=4400, grad_norm=5.40854024887085, loss=4.442078113555908
I0307 04:17:26.608835 140237678286592 logging_writer.py:48] [4500] global_step=4500, grad_norm=6.966238975524902, loss=4.474717140197754
I0307 04:18:05.118091 140237686679296 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.522454738616943, loss=4.4409284591674805
I0307 04:18:43.578605 140237678286592 logging_writer.py:48] [4700] global_step=4700, grad_norm=5.671330451965332, loss=4.530478000640869
I0307 04:19:22.328317 140237686679296 logging_writer.py:48] [4800] global_step=4800, grad_norm=5.395707130432129, loss=4.264945983886719
I0307 04:20:00.728341 140237678286592 logging_writer.py:48] [4900] global_step=4900, grad_norm=5.8068108558654785, loss=4.306220531463623
I0307 04:20:39.150299 140237686679296 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.4223504066467285, loss=4.208405494689941
I0307 04:21:17.649787 140237678286592 logging_writer.py:48] [5100] global_step=5100, grad_norm=6.259925365447998, loss=4.297712326049805
I0307 04:21:55.864682 140237686679296 logging_writer.py:48] [5200] global_step=5200, grad_norm=5.362920761108398, loss=4.203536033630371
I0307 04:22:34.124094 140237678286592 logging_writer.py:48] [5300] global_step=5300, grad_norm=5.548031330108643, loss=4.227999687194824
I0307 04:23:04.418530 140393707492544 spec.py:321] Evaluating on the training split.
I0307 04:23:15.716601 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 04:23:37.494884 140393707492544 spec.py:349] Evaluating on the test split.
I0307 04:23:39.475598 140393707492544 submission_runner.py:469] Time since start: 2329.36s, 	Step: 5380, 	{'train/accuracy': 0.33751195669174194, 'train/loss': 3.120251417160034, 'validation/accuracy': 0.3039799928665161, 'validation/loss': 3.3272545337677, 'validation/num_examples': 50000, 'test/accuracy': 0.22720001637935638, 'test/loss': 3.9319283962249756, 'test/num_examples': 10000, 'score': 2096.9253208637238, 'total_duration': 2329.3563158512115, 'accumulated_submission_time': 2096.9253208637238, 'accumulated_eval_time': 231.6465826034546, 'accumulated_logging_time': 0.155487060546875}
I0307 04:23:39.535748 140237686679296 logging_writer.py:48] [5380] accumulated_eval_time=231.647, accumulated_logging_time=0.155487, accumulated_submission_time=2096.93, global_step=5380, preemption_count=0, score=2096.93, test/accuracy=0.2272, test/loss=3.93193, test/num_examples=10000, total_duration=2329.36, train/accuracy=0.337512, train/loss=3.12025, validation/accuracy=0.30398, validation/loss=3.32725, validation/num_examples=50000
I0307 04:23:48.016858 140237678286592 logging_writer.py:48] [5400] global_step=5400, grad_norm=6.219124794006348, loss=4.1255083084106445
I0307 04:24:26.445335 140237686679296 logging_writer.py:48] [5500] global_step=5500, grad_norm=4.754327297210693, loss=4.209436416625977
I0307 04:25:05.106070 140237678286592 logging_writer.py:48] [5600] global_step=5600, grad_norm=6.474018573760986, loss=4.122152328491211
I0307 04:25:43.554309 140237686679296 logging_writer.py:48] [5700] global_step=5700, grad_norm=5.785545825958252, loss=4.136604309082031
I0307 04:26:22.096669 140237678286592 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.806042432785034, loss=4.118386745452881
I0307 04:27:00.206647 140237686679296 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.8394389152526855, loss=4.0587849617004395
I0307 04:27:38.956951 140237678286592 logging_writer.py:48] [6000] global_step=6000, grad_norm=6.547352313995361, loss=4.046379089355469
I0307 04:28:17.076129 140237686679296 logging_writer.py:48] [6100] global_step=6100, grad_norm=5.479892253875732, loss=3.9729862213134766
I0307 04:28:55.716499 140237678286592 logging_writer.py:48] [6200] global_step=6200, grad_norm=6.4038567543029785, loss=3.869457483291626
I0307 04:29:33.819411 140237686679296 logging_writer.py:48] [6300] global_step=6300, grad_norm=4.058222770690918, loss=3.8674557209014893
I0307 04:30:12.242610 140237678286592 logging_writer.py:48] [6400] global_step=6400, grad_norm=5.8220391273498535, loss=3.919844388961792
I0307 04:30:50.458283 140237686679296 logging_writer.py:48] [6500] global_step=6500, grad_norm=7.173405647277832, loss=3.9111287593841553
I0307 04:31:28.668204 140237678286592 logging_writer.py:48] [6600] global_step=6600, grad_norm=6.5006937980651855, loss=3.9083619117736816
I0307 04:32:06.940741 140237686679296 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.260604381561279, loss=3.8592779636383057
I0307 04:32:09.686060 140393707492544 spec.py:321] Evaluating on the training split.
I0307 04:32:21.434734 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 04:32:44.218388 140393707492544 spec.py:349] Evaluating on the test split.
I0307 04:32:46.080799 140393707492544 submission_runner.py:469] Time since start: 2875.96s, 	Step: 6708, 	{'train/accuracy': 0.40858179330825806, 'train/loss': 2.7329723834991455, 'validation/accuracy': 0.3676999807357788, 'validation/loss': 2.9532580375671387, 'validation/num_examples': 50000, 'test/accuracy': 0.28360000252723694, 'test/loss': 3.584315299987793, 'test/num_examples': 10000, 'score': 2606.9397070407867, 'total_duration': 2875.9615302085876, 'accumulated_submission_time': 2606.9397070407867, 'accumulated_eval_time': 268.04128313064575, 'accumulated_logging_time': 0.22454524040222168}
I0307 04:32:46.108361 140237678286592 logging_writer.py:48] [6708] accumulated_eval_time=268.041, accumulated_logging_time=0.224545, accumulated_submission_time=2606.94, global_step=6708, preemption_count=0, score=2606.94, test/accuracy=0.2836, test/loss=3.58432, test/num_examples=10000, total_duration=2875.96, train/accuracy=0.408582, train/loss=2.73297, validation/accuracy=0.3677, validation/loss=2.95326, validation/num_examples=50000
I0307 04:33:22.102483 140237686679296 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.6967082023620605, loss=3.766366958618164
I0307 04:34:00.377921 140237678286592 logging_writer.py:48] [6900] global_step=6900, grad_norm=6.402060508728027, loss=3.9176511764526367
I0307 04:34:38.401253 140237686679296 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.834676742553711, loss=3.8122241497039795
I0307 04:35:16.786853 140237678286592 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.333340644836426, loss=3.802813768386841
I0307 04:35:54.502864 140237686679296 logging_writer.py:48] [7200] global_step=7200, grad_norm=5.089655876159668, loss=3.7814834117889404
I0307 04:36:32.540859 140237678286592 logging_writer.py:48] [7300] global_step=7300, grad_norm=6.812779903411865, loss=3.750652313232422
I0307 04:37:11.084390 140237686679296 logging_writer.py:48] [7400] global_step=7400, grad_norm=5.9284772872924805, loss=3.722545862197876
I0307 04:37:49.451333 140237678286592 logging_writer.py:48] [7500] global_step=7500, grad_norm=5.522374153137207, loss=3.7429487705230713
I0307 04:38:28.116349 140237686679296 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.59632682800293, loss=3.6634819507598877
I0307 04:39:06.847108 140237678286592 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.6226911544799805, loss=3.6510095596313477
I0307 04:39:45.570007 140237686679296 logging_writer.py:48] [7800] global_step=7800, grad_norm=6.265903949737549, loss=3.552351713180542
I0307 04:40:24.420397 140237678286592 logging_writer.py:48] [7900] global_step=7900, grad_norm=6.699706077575684, loss=3.6468052864074707
I0307 04:41:03.024732 140237686679296 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.7891294956207275, loss=3.72617769241333
I0307 04:41:16.397476 140393707492544 spec.py:321] Evaluating on the training split.
I0307 04:41:30.900436 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 04:41:54.301364 140393707492544 spec.py:349] Evaluating on the test split.
I0307 04:41:56.143746 140393707492544 submission_runner.py:469] Time since start: 3426.02s, 	Step: 8036, 	{'train/accuracy': 0.4738321006298065, 'train/loss': 2.394381523132324, 'validation/accuracy': 0.42993998527526855, 'validation/loss': 2.622504711151123, 'validation/num_examples': 50000, 'test/accuracy': 0.3343000113964081, 'test/loss': 3.2575578689575195, 'test/num_examples': 10000, 'score': 3117.0869011878967, 'total_duration': 3426.024457216263, 'accumulated_submission_time': 3117.0869011878967, 'accumulated_eval_time': 307.7875051498413, 'accumulated_logging_time': 0.2605764865875244}
I0307 04:41:56.186912 140237678286592 logging_writer.py:48] [8036] accumulated_eval_time=307.788, accumulated_logging_time=0.260576, accumulated_submission_time=3117.09, global_step=8036, preemption_count=0, score=3117.09, test/accuracy=0.3343, test/loss=3.25756, test/num_examples=10000, total_duration=3426.02, train/accuracy=0.473832, train/loss=2.39438, validation/accuracy=0.42994, validation/loss=2.6225, validation/num_examples=50000
I0307 04:42:21.673837 140237686679296 logging_writer.py:48] [8100] global_step=8100, grad_norm=6.063807487487793, loss=3.546290636062622
I0307 04:43:00.267471 140237678286592 logging_writer.py:48] [8200] global_step=8200, grad_norm=9.543957710266113, loss=3.5516905784606934
I0307 04:43:38.895057 140237686679296 logging_writer.py:48] [8300] global_step=8300, grad_norm=4.231390476226807, loss=3.531686305999756
I0307 04:44:17.589531 140237678286592 logging_writer.py:48] [8400] global_step=8400, grad_norm=8.663583755493164, loss=3.5047807693481445
I0307 04:44:56.319016 140237686679296 logging_writer.py:48] [8500] global_step=8500, grad_norm=5.621513366699219, loss=3.495389461517334
I0307 04:45:34.829266 140237678286592 logging_writer.py:48] [8600] global_step=8600, grad_norm=5.131852149963379, loss=3.4102916717529297
I0307 04:46:13.422668 140237686679296 logging_writer.py:48] [8700] global_step=8700, grad_norm=5.960093021392822, loss=3.54042387008667
I0307 04:46:52.088967 140237678286592 logging_writer.py:48] [8800] global_step=8800, grad_norm=7.3007731437683105, loss=3.4841864109039307
I0307 04:47:30.746848 140237686679296 logging_writer.py:48] [8900] global_step=8900, grad_norm=7.186751842498779, loss=3.3962202072143555
I0307 04:48:09.162440 140237678286592 logging_writer.py:48] [9000] global_step=9000, grad_norm=4.262548446655273, loss=3.5289993286132812
I0307 04:48:47.647610 140237686679296 logging_writer.py:48] [9100] global_step=9100, grad_norm=5.014713287353516, loss=3.2916440963745117
I0307 04:49:26.597909 140237678286592 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.6951639652252197, loss=3.485438823699951
I0307 04:50:04.999874 140237686679296 logging_writer.py:48] [9300] global_step=9300, grad_norm=6.862725257873535, loss=3.4399285316467285
I0307 04:50:26.453135 140393707492544 spec.py:321] Evaluating on the training split.
I0307 04:50:41.491730 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 04:51:04.749328 140393707492544 spec.py:349] Evaluating on the test split.
I0307 04:51:06.555077 140393707492544 submission_runner.py:469] Time since start: 3976.44s, 	Step: 9357, 	{'train/accuracy': 0.5187539458274841, 'train/loss': 2.121927499771118, 'validation/accuracy': 0.46633997559547424, 'validation/loss': 2.3913583755493164, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.0530731678009033, 'test/num_examples': 10000, 'score': 3627.2021667957306, 'total_duration': 3976.4358086586, 'accumulated_submission_time': 3627.2021667957306, 'accumulated_eval_time': 347.88941049575806, 'accumulated_logging_time': 0.33215832710266113}
I0307 04:51:06.663429 140237678286592 logging_writer.py:48] [9357] accumulated_eval_time=347.889, accumulated_logging_time=0.332158, accumulated_submission_time=3627.2, global_step=9357, preemption_count=0, score=3627.2, test/accuracy=0.3613, test/loss=3.05307, test/num_examples=10000, total_duration=3976.44, train/accuracy=0.518754, train/loss=2.12193, validation/accuracy=0.46634, validation/loss=2.39136, validation/num_examples=50000
I0307 04:51:23.996937 140237686679296 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.889369487762451, loss=3.572981357574463
I0307 04:52:02.865645 140237678286592 logging_writer.py:48] [9500] global_step=9500, grad_norm=6.4890828132629395, loss=3.3936080932617188
I0307 04:52:40.497117 140237686679296 logging_writer.py:48] [9600] global_step=9600, grad_norm=6.8391947746276855, loss=3.4460432529449463
I0307 04:53:17.712027 140237678286592 logging_writer.py:48] [9700] global_step=9700, grad_norm=7.184378623962402, loss=3.401723623275757
I0307 04:53:55.416687 140237686679296 logging_writer.py:48] [9800] global_step=9800, grad_norm=6.463693141937256, loss=3.3047704696655273
I0307 04:54:33.353319 140237678286592 logging_writer.py:48] [9900] global_step=9900, grad_norm=6.426818370819092, loss=3.4200439453125
I0307 04:55:11.140753 140237686679296 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.198579788208008, loss=3.3973402976989746
I0307 04:55:49.239843 140237678286592 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.327723741531372, loss=3.3558027744293213
I0307 04:56:27.458022 140237686679296 logging_writer.py:48] [10200] global_step=10200, grad_norm=5.756371021270752, loss=3.3607797622680664
I0307 04:57:05.866550 140237678286592 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.875500679016113, loss=3.328616142272949
I0307 04:57:43.884300 140237686679296 logging_writer.py:48] [10400] global_step=10400, grad_norm=6.254083633422852, loss=3.2835893630981445
I0307 04:58:21.964157 140237678286592 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.882829666137695, loss=3.23188853263855
I0307 04:59:00.431224 140237686679296 logging_writer.py:48] [10600] global_step=10600, grad_norm=5.17378044128418, loss=3.2760183811187744
I0307 04:59:36.929088 140393707492544 spec.py:321] Evaluating on the training split.
I0307 04:59:51.194736 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 05:00:16.391803 140393707492544 spec.py:349] Evaluating on the test split.
I0307 05:00:18.202073 140393707492544 submission_runner.py:469] Time since start: 4528.08s, 	Step: 10697, 	{'train/accuracy': 0.5465362071990967, 'train/loss': 2.0194756984710693, 'validation/accuracy': 0.49573999643325806, 'validation/loss': 2.251650333404541, 'validation/num_examples': 50000, 'test/accuracy': 0.3846000134944916, 'test/loss': 2.921189785003662, 'test/num_examples': 10000, 'score': 4137.313571929932, 'total_duration': 4528.082790374756, 'accumulated_submission_time': 4137.313571929932, 'accumulated_eval_time': 389.16234731674194, 'accumulated_logging_time': 0.4484844207763672}
I0307 05:00:18.214851 140237678286592 logging_writer.py:48] [10697] accumulated_eval_time=389.162, accumulated_logging_time=0.448484, accumulated_submission_time=4137.31, global_step=10697, preemption_count=0, score=4137.31, test/accuracy=0.3846, test/loss=2.92119, test/num_examples=10000, total_duration=4528.08, train/accuracy=0.546536, train/loss=2.01948, validation/accuracy=0.49574, validation/loss=2.25165, validation/num_examples=50000
I0307 05:00:20.104914 140237686679296 logging_writer.py:48] [10700] global_step=10700, grad_norm=7.836803436279297, loss=3.34261417388916
I0307 05:00:58.161427 140237678286592 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.6760339736938477, loss=3.227107048034668
I0307 05:01:36.684360 140237686679296 logging_writer.py:48] [10900] global_step=10900, grad_norm=6.639444828033447, loss=3.275007486343384
I0307 05:02:15.097477 140237678286592 logging_writer.py:48] [11000] global_step=11000, grad_norm=3.9328339099884033, loss=3.258338689804077
I0307 05:02:53.269412 140237686679296 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.561166286468506, loss=3.3592207431793213
I0307 05:03:29.941050 140237678286592 logging_writer.py:48] [11200] global_step=11200, grad_norm=6.093831539154053, loss=3.2934153079986572
I0307 05:04:07.510580 140237686679296 logging_writer.py:48] [11300] global_step=11300, grad_norm=6.827207565307617, loss=3.1825642585754395
I0307 05:04:45.718301 140237678286592 logging_writer.py:48] [11400] global_step=11400, grad_norm=6.7902140617370605, loss=3.285005569458008
I0307 05:05:24.888495 140237686679296 logging_writer.py:48] [11500] global_step=11500, grad_norm=4.443902492523193, loss=3.280527353286743
I0307 05:06:03.362871 140237678286592 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.706519603729248, loss=3.180602788925171
I0307 05:06:41.936314 140237686679296 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.735777378082275, loss=3.228698253631592
I0307 05:07:20.586108 140237678286592 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.529268741607666, loss=3.290107488632202
I0307 05:07:59.028550 140237686679296 logging_writer.py:48] [11900] global_step=11900, grad_norm=5.68017578125, loss=3.092550277709961
I0307 05:08:37.819406 140237678286592 logging_writer.py:48] [12000] global_step=12000, grad_norm=6.007710933685303, loss=3.2094664573669434
I0307 05:08:48.497573 140393707492544 spec.py:321] Evaluating on the training split.
I0307 05:09:03.466834 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 05:09:23.574923 140393707492544 spec.py:349] Evaluating on the test split.
I0307 05:09:25.387290 140393707492544 submission_runner.py:469] Time since start: 5075.27s, 	Step: 12029, 	{'train/accuracy': 0.5726442933082581, 'train/loss': 1.9059715270996094, 'validation/accuracy': 0.5229399800300598, 'validation/loss': 2.145876169204712, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.832265853881836, 'test/num_examples': 10000, 'score': 4647.442705869675, 'total_duration': 5075.26800942421, 'accumulated_submission_time': 4647.442705869675, 'accumulated_eval_time': 426.05201482772827, 'accumulated_logging_time': 0.4694032669067383}
I0307 05:09:25.512259 140237686679296 logging_writer.py:48] [12029] accumulated_eval_time=426.052, accumulated_logging_time=0.469403, accumulated_submission_time=4647.44, global_step=12029, preemption_count=0, score=4647.44, test/accuracy=0.3976, test/loss=2.83227, test/num_examples=10000, total_duration=5075.27, train/accuracy=0.572644, train/loss=1.90597, validation/accuracy=0.52294, validation/loss=2.14588, validation/num_examples=50000
I0307 05:09:53.236234 140237678286592 logging_writer.py:48] [12100] global_step=12100, grad_norm=6.13707971572876, loss=3.3260488510131836
I0307 05:10:31.664334 140237686679296 logging_writer.py:48] [12200] global_step=12200, grad_norm=5.865999698638916, loss=3.223701238632202
I0307 05:11:10.272053 140237678286592 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.888221263885498, loss=3.1093575954437256
I0307 05:11:48.987975 140237686679296 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.64625883102417, loss=3.1550796031951904
I0307 05:12:28.006448 140237678286592 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.8983006477355957, loss=3.1931848526000977
I0307 05:13:06.899278 140237686679296 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.95199728012085, loss=3.1545724868774414
I0307 05:13:45.538694 140237678286592 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.711345195770264, loss=3.168245792388916
I0307 05:14:24.153786 140237686679296 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.90162992477417, loss=3.142141580581665
I0307 05:15:02.311944 140237678286592 logging_writer.py:48] [12900] global_step=12900, grad_norm=7.364778995513916, loss=3.2974250316619873
I0307 05:15:40.626906 140237686679296 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.896432399749756, loss=3.1477601528167725
I0307 05:16:19.294710 140237678286592 logging_writer.py:48] [13100] global_step=13100, grad_norm=5.982175350189209, loss=3.1502609252929688
I0307 05:16:57.678231 140237686679296 logging_writer.py:48] [13200] global_step=13200, grad_norm=6.959403038024902, loss=3.1993465423583984
I0307 05:17:36.061852 140237678286592 logging_writer.py:48] [13300] global_step=13300, grad_norm=6.307599067687988, loss=3.0088977813720703
I0307 05:17:55.687264 140393707492544 spec.py:321] Evaluating on the training split.
I0307 05:18:13.179555 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 05:18:35.514294 140393707492544 spec.py:349] Evaluating on the test split.
I0307 05:18:37.373215 140393707492544 submission_runner.py:469] Time since start: 5627.25s, 	Step: 13352, 	{'train/accuracy': 0.5810546875, 'train/loss': 1.839288353919983, 'validation/accuracy': 0.5356999635696411, 'validation/loss': 2.062808036804199, 'validation/num_examples': 50000, 'test/accuracy': 0.41920003294944763, 'test/loss': 2.725562572479248, 'test/num_examples': 10000, 'score': 5157.469700574875, 'total_duration': 5627.253938674927, 'accumulated_submission_time': 5157.469700574875, 'accumulated_eval_time': 467.73792481422424, 'accumulated_logging_time': 0.6028783321380615}
I0307 05:18:37.493842 140237686679296 logging_writer.py:48] [13352] accumulated_eval_time=467.738, accumulated_logging_time=0.602878, accumulated_submission_time=5157.47, global_step=13352, preemption_count=0, score=5157.47, test/accuracy=0.4192, test/loss=2.72556, test/num_examples=10000, total_duration=5627.25, train/accuracy=0.581055, train/loss=1.83929, validation/accuracy=0.5357, validation/loss=2.06281, validation/num_examples=50000
I0307 05:18:59.050476 140237678286592 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.7622599601745605, loss=3.1516757011413574
I0307 05:19:37.453473 140237686679296 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.061789035797119, loss=3.0191845893859863
I0307 05:20:16.421771 140237678286592 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.7695631980896, loss=3.136446952819824
I0307 05:20:55.263680 140237686679296 logging_writer.py:48] [13700] global_step=13700, grad_norm=5.590153217315674, loss=3.1389830112457275
I0307 05:21:34.229065 140237678286592 logging_writer.py:48] [13800] global_step=13800, grad_norm=5.369077205657959, loss=3.043548107147217
I0307 05:22:12.809359 140237686679296 logging_writer.py:48] [13900] global_step=13900, grad_norm=9.267340660095215, loss=2.986905574798584
I0307 05:22:51.438678 140237678286592 logging_writer.py:48] [14000] global_step=14000, grad_norm=7.0103440284729, loss=3.1646883487701416
I0307 05:23:29.767901 140237686679296 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.14547061920166, loss=3.077138900756836
I0307 05:24:08.107729 140237678286592 logging_writer.py:48] [14200] global_step=14200, grad_norm=5.380791664123535, loss=3.098579168319702
I0307 05:24:46.712881 140237686679296 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.044316053390503, loss=2.965977191925049
I0307 05:25:25.401123 140237678286592 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.581728219985962, loss=3.0544676780700684
I0307 05:26:03.924074 140237686679296 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.1352458000183105, loss=3.057159662246704
I0307 05:26:42.707375 140237678286592 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.965930461883545, loss=3.0465850830078125
I0307 05:27:07.669196 140393707492544 spec.py:321] Evaluating on the training split.
I0307 05:27:25.692411 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 05:27:51.234288 140393707492544 spec.py:349] Evaluating on the test split.
I0307 05:27:53.042186 140393707492544 submission_runner.py:469] Time since start: 6182.92s, 	Step: 14666, 	{'train/accuracy': 0.6036550998687744, 'train/loss': 1.734601616859436, 'validation/accuracy': 0.5449399948120117, 'validation/loss': 1.9952954053878784, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.656996965408325, 'test/num_examples': 10000, 'score': 5667.497880935669, 'total_duration': 6182.922917604446, 'accumulated_submission_time': 5667.497880935669, 'accumulated_eval_time': 513.1108808517456, 'accumulated_logging_time': 0.7315492630004883}
I0307 05:27:53.171859 140237686679296 logging_writer.py:48] [14666] accumulated_eval_time=513.111, accumulated_logging_time=0.731549, accumulated_submission_time=5667.5, global_step=14666, preemption_count=0, score=5667.5, test/accuracy=0.4295, test/loss=2.657, test/num_examples=10000, total_duration=6182.92, train/accuracy=0.603655, train/loss=1.7346, validation/accuracy=0.54494, validation/loss=1.9953, validation/num_examples=50000
I0307 05:28:06.798390 140237678286592 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.5642008781433105, loss=3.137716770172119
I0307 05:28:45.004234 140237686679296 logging_writer.py:48] [14800] global_step=14800, grad_norm=5.7847795486450195, loss=3.1262218952178955
I0307 05:29:23.826470 140237678286592 logging_writer.py:48] [14900] global_step=14900, grad_norm=6.701359272003174, loss=3.126908779144287
I0307 05:30:02.097140 140237686679296 logging_writer.py:48] [15000] global_step=15000, grad_norm=5.2253499031066895, loss=3.105588436126709
I0307 05:30:41.381363 140237678286592 logging_writer.py:48] [15100] global_step=15100, grad_norm=7.979267597198486, loss=3.135255813598633
I0307 05:31:19.941251 140237686679296 logging_writer.py:48] [15200] global_step=15200, grad_norm=7.49713134765625, loss=3.1292037963867188
I0307 05:31:58.869587 140237678286592 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.782952308654785, loss=3.0936503410339355
I0307 05:32:37.601077 140237686679296 logging_writer.py:48] [15400] global_step=15400, grad_norm=6.181324481964111, loss=3.048245429992676
I0307 05:33:16.877279 140237678286592 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.9700193405151367, loss=3.041415214538574
I0307 05:33:54.832019 140237686679296 logging_writer.py:48] [15600] global_step=15600, grad_norm=5.513481140136719, loss=2.962737560272217
I0307 05:34:36.543829 140237678286592 logging_writer.py:48] [15700] global_step=15700, grad_norm=6.367307662963867, loss=2.982783079147339
I0307 05:35:16.738984 140237686679296 logging_writer.py:48] [15800] global_step=15800, grad_norm=5.225562572479248, loss=2.9845683574676514
I0307 05:35:56.563564 140237678286592 logging_writer.py:48] [15900] global_step=15900, grad_norm=4.9166388511657715, loss=2.8647851943969727
I0307 05:36:23.221842 140393707492544 spec.py:321] Evaluating on the training split.
I0307 05:36:42.567538 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 05:37:02.530563 140393707492544 spec.py:349] Evaluating on the test split.
I0307 05:37:04.396668 140393707492544 submission_runner.py:469] Time since start: 6734.28s, 	Step: 15970, 	{'train/accuracy': 0.6106504797935486, 'train/loss': 1.7090407609939575, 'validation/accuracy': 0.5572400093078613, 'validation/loss': 1.9529874324798584, 'validation/num_examples': 50000, 'test/accuracy': 0.4408000111579895, 'test/loss': 2.6234328746795654, 'test/num_examples': 10000, 'score': 6177.396770238876, 'total_duration': 6734.277386665344, 'accumulated_submission_time': 6177.396770238876, 'accumulated_eval_time': 554.2856616973877, 'accumulated_logging_time': 0.869286060333252}
I0307 05:37:04.481353 140237686679296 logging_writer.py:48] [15970] accumulated_eval_time=554.286, accumulated_logging_time=0.869286, accumulated_submission_time=6177.4, global_step=15970, preemption_count=0, score=6177.4, test/accuracy=0.4408, test/loss=2.62343, test/num_examples=10000, total_duration=6734.28, train/accuracy=0.61065, train/loss=1.70904, validation/accuracy=0.55724, validation/loss=1.95299, validation/num_examples=50000
I0307 05:37:17.198560 140237678286592 logging_writer.py:48] [16000] global_step=16000, grad_norm=5.3812994956970215, loss=3.051523208618164
I0307 05:37:55.543425 140237686679296 logging_writer.py:48] [16100] global_step=16100, grad_norm=4.0571513175964355, loss=3.0630409717559814
I0307 05:38:34.090361 140237678286592 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.152245998382568, loss=3.051860809326172
I0307 05:39:13.241354 140237686679296 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.347898006439209, loss=3.025718927383423
I0307 05:39:51.301662 140237678286592 logging_writer.py:48] [16400] global_step=16400, grad_norm=4.793010234832764, loss=3.086190700531006
I0307 05:40:29.677838 140237686679296 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.86271595954895, loss=2.958721399307251
I0307 05:41:08.007389 140237678286592 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.773324012756348, loss=2.9632515907287598
I0307 05:41:46.315500 140237686679296 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.522719860076904, loss=2.9994966983795166
I0307 05:42:24.704653 140237678286592 logging_writer.py:48] [16800] global_step=16800, grad_norm=5.9327239990234375, loss=3.0332295894622803
I0307 05:43:03.271437 140237686679296 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.8636040687561035, loss=2.927216053009033
I0307 05:43:41.610193 140237678286592 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.791567325592041, loss=3.0471994876861572
I0307 05:44:20.041080 140237686679296 logging_writer.py:48] [17100] global_step=17100, grad_norm=6.067712306976318, loss=2.948570966720581
I0307 05:44:58.344230 140237678286592 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.984513521194458, loss=2.9989869594573975
I0307 05:45:34.500785 140393707492544 spec.py:321] Evaluating on the training split.
I0307 05:45:47.762969 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 05:46:10.134088 140393707492544 spec.py:349] Evaluating on the test split.
I0307 05:46:11.976799 140393707492544 submission_runner.py:469] Time since start: 7281.86s, 	Step: 17294, 	{'train/accuracy': 0.6128427982330322, 'train/loss': 1.682786226272583, 'validation/accuracy': 0.5596799850463867, 'validation/loss': 1.942488431930542, 'validation/num_examples': 50000, 'test/accuracy': 0.4328000247478485, 'test/loss': 2.6385648250579834, 'test/num_examples': 10000, 'score': 6687.268934011459, 'total_duration': 7281.857532262802, 'accumulated_submission_time': 6687.268934011459, 'accumulated_eval_time': 591.7616419792175, 'accumulated_logging_time': 0.963083028793335}
I0307 05:46:12.060804 140237686679296 logging_writer.py:48] [17294] accumulated_eval_time=591.762, accumulated_logging_time=0.963083, accumulated_submission_time=6687.27, global_step=17294, preemption_count=0, score=6687.27, test/accuracy=0.4328, test/loss=2.63856, test/num_examples=10000, total_duration=7281.86, train/accuracy=0.612843, train/loss=1.68279, validation/accuracy=0.55968, validation/loss=1.94249, validation/num_examples=50000
I0307 05:46:15.080161 140237678286592 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.359939098358154, loss=2.9943106174468994
I0307 05:46:53.021255 140237686679296 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.393450975418091, loss=3.0544400215148926
I0307 05:47:31.862348 140237678286592 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.588134288787842, loss=3.090143918991089
I0307 05:48:10.133711 140237686679296 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.293710708618164, loss=2.995943069458008
I0307 05:48:48.854254 140237678286592 logging_writer.py:48] [17700] global_step=17700, grad_norm=5.54825496673584, loss=3.0469722747802734
I0307 05:49:27.230772 140237686679296 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.572277069091797, loss=2.990873336791992
I0307 05:50:05.900152 140237678286592 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.4801149368286133, loss=3.0600943565368652
I0307 05:50:43.897704 140237686679296 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.4845738410949707, loss=2.9202258586883545
I0307 05:51:23.065302 140237678286592 logging_writer.py:48] [18100] global_step=18100, grad_norm=5.777547359466553, loss=2.9850308895111084
I0307 05:52:01.514136 140237686679296 logging_writer.py:48] [18200] global_step=18200, grad_norm=5.515402317047119, loss=2.937859058380127
I0307 05:52:39.829075 140237678286592 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.542312145233154, loss=3.0883564949035645
I0307 05:53:18.355655 140237686679296 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.7497332096099854, loss=3.0540976524353027
I0307 05:53:57.076694 140237678286592 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.129596710205078, loss=3.1208674907684326
I0307 05:54:35.335571 140237686679296 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.991421937942505, loss=2.940868377685547
I0307 05:54:42.277438 140393707492544 spec.py:321] Evaluating on the training split.
I0307 05:54:55.176013 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 05:55:17.564204 140393707492544 spec.py:349] Evaluating on the test split.
I0307 05:55:19.404508 140393707492544 submission_runner.py:469] Time since start: 7829.29s, 	Step: 18619, 	{'train/accuracy': 0.6204758882522583, 'train/loss': 1.6513203382492065, 'validation/accuracy': 0.5702799558639526, 'validation/loss': 1.8937433958053589, 'validation/num_examples': 50000, 'test/accuracy': 0.44530001282691956, 'test/loss': 2.5417776107788086, 'test/num_examples': 10000, 'score': 7197.350691795349, 'total_duration': 7829.285220146179, 'accumulated_submission_time': 7197.350691795349, 'accumulated_eval_time': 628.8886682987213, 'accumulated_logging_time': 1.054701566696167}
I0307 05:55:19.446532 140237678286592 logging_writer.py:48] [18619] accumulated_eval_time=628.889, accumulated_logging_time=1.0547, accumulated_submission_time=7197.35, global_step=18619, preemption_count=0, score=7197.35, test/accuracy=0.4453, test/loss=2.54178, test/num_examples=10000, total_duration=7829.29, train/accuracy=0.620476, train/loss=1.65132, validation/accuracy=0.57028, validation/loss=1.89374, validation/num_examples=50000
I0307 05:55:51.638807 140237686679296 logging_writer.py:48] [18700] global_step=18700, grad_norm=5.3100199699401855, loss=3.0394155979156494
I0307 05:56:30.638865 140237678286592 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.0530030727386475, loss=2.897829055786133
I0307 05:57:09.204511 140237686679296 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.4296751022338867, loss=2.9832334518432617
I0307 05:57:47.574386 140237678286592 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.3751168251037598, loss=2.9423606395721436
I0307 05:58:25.961678 140237686679296 logging_writer.py:48] [19100] global_step=19100, grad_norm=6.331875324249268, loss=2.8929996490478516
I0307 05:59:04.674104 140237678286592 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.735938549041748, loss=2.929157018661499
I0307 05:59:43.788731 140237686679296 logging_writer.py:48] [19300] global_step=19300, grad_norm=4.325897216796875, loss=2.907766819000244
I0307 06:00:22.183262 140237678286592 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.130901336669922, loss=3.0084455013275146
I0307 06:01:00.853701 140237686679296 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.195379734039307, loss=2.880277156829834
I0307 06:01:39.613801 140237678286592 logging_writer.py:48] [19600] global_step=19600, grad_norm=4.12898588180542, loss=2.9599976539611816
I0307 06:02:18.399358 140237686679296 logging_writer.py:48] [19700] global_step=19700, grad_norm=6.55571174621582, loss=2.939419746398926
I0307 06:02:57.038869 140237678286592 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.2697689533233643, loss=2.999755382537842
I0307 06:03:49.501667 140393707492544 spec.py:321] Evaluating on the training split.
I0307 06:04:08.115169 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 06:04:31.537601 140393707492544 spec.py:349] Evaluating on the test split.
I0307 06:04:33.359824 140393707492544 submission_runner.py:469] Time since start: 8383.24s, 	Step: 19899, 	{'train/accuracy': 0.6251793503761292, 'train/loss': 1.6402405500411987, 'validation/accuracy': 0.5752599835395813, 'validation/loss': 1.8779237270355225, 'validation/num_examples': 50000, 'test/accuracy': 0.4529000222682953, 'test/loss': 2.5515594482421875, 'test/num_examples': 10000, 'score': 7707.2713577747345, 'total_duration': 8383.240555524826, 'accumulated_submission_time': 7707.2713577747345, 'accumulated_eval_time': 672.7467930316925, 'accumulated_logging_time': 1.104966402053833}
I0307 06:04:33.451811 140237686679296 logging_writer.py:48] [19899] accumulated_eval_time=672.747, accumulated_logging_time=1.10497, accumulated_submission_time=7707.27, global_step=19899, preemption_count=0, score=7707.27, test/accuracy=0.4529, test/loss=2.55156, test/num_examples=10000, total_duration=8383.24, train/accuracy=0.625179, train/loss=1.64024, validation/accuracy=0.57526, validation/loss=1.87792, validation/num_examples=50000
I0307 06:04:34.494468 140237678286592 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.313699722290039, loss=2.9108798503875732
I0307 06:05:12.904022 140237686679296 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.910175085067749, loss=2.9717767238616943
I0307 06:05:51.691495 140237678286592 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.390440940856934, loss=2.999652862548828
I0307 06:06:29.689236 140237686679296 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.9326863288879395, loss=2.9526946544647217
I0307 06:07:07.419851 140237678286592 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.998368263244629, loss=3.10581374168396
I0307 06:07:45.549650 140237686679296 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.49462628364563, loss=3.110220432281494
I0307 06:08:24.335513 140237678286592 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.247157573699951, loss=2.937626838684082
I0307 06:09:02.325951 140237686679296 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.0989389419555664, loss=2.911635160446167
I0307 06:09:40.716384 140237678286592 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.9248218536376953, loss=2.9959540367126465
I0307 06:10:18.945574 140237686679296 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.877925395965576, loss=2.887640953063965
I0307 06:10:57.373725 140237678286592 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.915771245956421, loss=3.0053820610046387
I0307 06:11:35.487545 140237686679296 logging_writer.py:48] [21000] global_step=21000, grad_norm=4.309767246246338, loss=2.8480515480041504
I0307 06:12:13.733656 140237678286592 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.065153121948242, loss=2.869713068008423
I0307 06:12:52.277738 140237686679296 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.83417010307312, loss=2.947362184524536
I0307 06:13:03.432748 140393707492544 spec.py:321] Evaluating on the training split.
I0307 06:13:17.623214 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 06:13:40.521514 140393707492544 spec.py:349] Evaluating on the test split.
I0307 06:13:42.318077 140393707492544 submission_runner.py:469] Time since start: 8932.20s, 	Step: 21230, 	{'train/accuracy': 0.6267936825752258, 'train/loss': 1.629669189453125, 'validation/accuracy': 0.5759599804878235, 'validation/loss': 1.87746000289917, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.53212833404541, 'test/num_examples': 10000, 'score': 8217.10722565651, 'total_duration': 8932.19880771637, 'accumulated_submission_time': 8217.10722565651, 'accumulated_eval_time': 711.6320838928223, 'accumulated_logging_time': 1.2052326202392578}
I0307 06:13:42.411638 140237678286592 logging_writer.py:48] [21230] accumulated_eval_time=711.632, accumulated_logging_time=1.20523, accumulated_submission_time=8217.11, global_step=21230, preemption_count=0, score=8217.11, test/accuracy=0.4533, test/loss=2.53213, test/num_examples=10000, total_duration=8932.2, train/accuracy=0.626794, train/loss=1.62967, validation/accuracy=0.57596, validation/loss=1.87746, validation/num_examples=50000
I0307 06:14:10.621390 140237686679296 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.189098834991455, loss=3.009291410446167
I0307 06:14:48.870157 140237678286592 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.3018436431884766, loss=3.0029263496398926
I0307 06:15:27.496599 140237686679296 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.988400459289551, loss=2.924074411392212
I0307 06:16:06.518423 140237678286592 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.051151752471924, loss=2.9919466972351074
I0307 06:16:45.214467 140237686679296 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.459794282913208, loss=2.907848834991455
I0307 06:17:24.025700 140237678286592 logging_writer.py:48] [21800] global_step=21800, grad_norm=4.273447513580322, loss=2.931218147277832
I0307 06:18:02.789851 140237686679296 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.096435546875, loss=2.8630504608154297
I0307 06:18:41.271745 140237678286592 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.5062150955200195, loss=2.8515074253082275
I0307 06:19:19.984435 140237686679296 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.7035717964172363, loss=3.004323720932007
I0307 06:19:58.277434 140237678286592 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.3647141456604004, loss=2.8900811672210693
I0307 06:20:37.209910 140237686679296 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.4449636936187744, loss=2.953665256500244
I0307 06:21:15.860310 140237678286592 logging_writer.py:48] [22400] global_step=22400, grad_norm=4.944125175476074, loss=2.8849892616271973
I0307 06:21:54.564246 140237686679296 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.2773795127868652, loss=2.943347692489624
I0307 06:22:12.449804 140393707492544 spec.py:321] Evaluating on the training split.
I0307 06:22:30.342003 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 06:22:52.848891 140393707492544 spec.py:349] Evaluating on the test split.
I0307 06:22:54.709914 140393707492544 submission_runner.py:469] Time since start: 9484.59s, 	Step: 22547, 	{'train/accuracy': 0.6214724183082581, 'train/loss': 1.6483502388000488, 'validation/accuracy': 0.5716800093650818, 'validation/loss': 1.8884778022766113, 'validation/num_examples': 50000, 'test/accuracy': 0.44600000977516174, 'test/loss': 2.585023880004883, 'test/num_examples': 10000, 'score': 8727.009164333344, 'total_duration': 9484.590646028519, 'accumulated_submission_time': 8727.009164333344, 'accumulated_eval_time': 753.8921568393707, 'accumulated_logging_time': 1.3064463138580322}
I0307 06:22:54.741830 140237678286592 logging_writer.py:48] [22547] accumulated_eval_time=753.892, accumulated_logging_time=1.30645, accumulated_submission_time=8727.01, global_step=22547, preemption_count=0, score=8727.01, test/accuracy=0.446, test/loss=2.58502, test/num_examples=10000, total_duration=9484.59, train/accuracy=0.621472, train/loss=1.64835, validation/accuracy=0.57168, validation/loss=1.88848, validation/num_examples=50000
I0307 06:23:15.820423 140237686679296 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.1917614936828613, loss=2.97908353805542
I0307 06:23:54.149247 140237678286592 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.4112460613250732, loss=2.983001232147217
I0307 06:24:32.950833 140237686679296 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.172572612762451, loss=2.9781265258789062
I0307 06:25:11.924275 140237678286592 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.8008615970611572, loss=2.8969056606292725
I0307 06:25:50.517034 140237686679296 logging_writer.py:48] [23000] global_step=23000, grad_norm=4.2767720222473145, loss=2.893932819366455
I0307 06:26:28.493321 140237678286592 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.096451759338379, loss=2.9021213054656982
I0307 06:27:06.880840 140237686679296 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.909696102142334, loss=2.869405746459961
I0307 06:27:45.264864 140237678286592 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.958780288696289, loss=2.9112470149993896
I0307 06:28:23.706053 140237686679296 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.3913397789001465, loss=2.875494956970215
I0307 06:29:02.305712 140237678286592 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.6585350036621094, loss=2.8556034564971924
I0307 06:29:41.013731 140237686679296 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.6586697101593018, loss=2.916353225708008
I0307 06:30:19.648648 140237678286592 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.033419370651245, loss=2.8359968662261963
I0307 06:30:58.287286 140237686679296 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.4421582221984863, loss=2.9466793537139893
I0307 06:31:25.023267 140393707492544 spec.py:321] Evaluating on the training split.
I0307 06:31:41.530658 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 06:32:04.742890 140393707492544 spec.py:349] Evaluating on the test split.
I0307 06:32:06.558579 140393707492544 submission_runner.py:469] Time since start: 10036.44s, 	Step: 23870, 	{'train/accuracy': 0.6290457248687744, 'train/loss': 1.6290956735610962, 'validation/accuracy': 0.5796799659729004, 'validation/loss': 1.865731954574585, 'validation/num_examples': 50000, 'test/accuracy': 0.4644000232219696, 'test/loss': 2.5260605812072754, 'test/num_examples': 10000, 'score': 9237.150210142136, 'total_duration': 10036.43930220604, 'accumulated_submission_time': 9237.150210142136, 'accumulated_eval_time': 795.4274299144745, 'accumulated_logging_time': 1.353196144104004}
I0307 06:32:06.602282 140237678286592 logging_writer.py:48] [23870] accumulated_eval_time=795.427, accumulated_logging_time=1.3532, accumulated_submission_time=9237.15, global_step=23870, preemption_count=0, score=9237.15, test/accuracy=0.4644, test/loss=2.52606, test/num_examples=10000, total_duration=10036.4, train/accuracy=0.629046, train/loss=1.6291, validation/accuracy=0.57968, validation/loss=1.86573, validation/num_examples=50000
I0307 06:32:18.901892 140237686679296 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.1521499156951904, loss=2.892456293106079
I0307 06:32:57.579687 140237678286592 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.550520896911621, loss=2.9550623893737793
I0307 06:33:36.042788 140237686679296 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.941986083984375, loss=2.88712215423584
I0307 06:34:14.338182 140237678286592 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.0971641540527344, loss=2.8914387226104736
I0307 06:34:52.774990 140237686679296 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.897692918777466, loss=2.859389066696167
I0307 06:35:31.257110 140237678286592 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.2875046730041504, loss=2.966853380203247
I0307 06:36:10.121852 140237686679296 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.1149797439575195, loss=2.9760611057281494
I0307 06:36:48.302875 140237678286592 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.659377098083496, loss=2.8759524822235107
I0307 06:37:26.884989 140237686679296 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.4150118827819824, loss=2.8976047039031982
I0307 06:38:05.303307 140237678286592 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.765876054763794, loss=2.878772735595703
I0307 06:38:43.610279 140237686679296 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.790741443634033, loss=2.9373316764831543
I0307 06:39:22.426405 140237678286592 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.2352614402770996, loss=2.8849353790283203
I0307 06:40:01.392125 140237686679296 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.2034451961517334, loss=2.809375762939453
I0307 06:40:36.583079 140393707492544 spec.py:321] Evaluating on the training split.
I0307 06:40:53.033355 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 06:41:17.314850 140393707492544 spec.py:349] Evaluating on the test split.
I0307 06:41:19.171166 140393707492544 submission_runner.py:469] Time since start: 10589.05s, 	Step: 25191, 	{'train/accuracy': 0.6312978267669678, 'train/loss': 1.596471905708313, 'validation/accuracy': 0.5849999785423279, 'validation/loss': 1.8211485147476196, 'validation/num_examples': 50000, 'test/accuracy': 0.46720001101493835, 'test/loss': 2.4841697216033936, 'test/num_examples': 10000, 'score': 9746.980960845947, 'total_duration': 10589.051862478256, 'accumulated_submission_time': 9746.980960845947, 'accumulated_eval_time': 838.0154654979706, 'accumulated_logging_time': 1.4236249923706055}
I0307 06:41:19.235290 140237678286592 logging_writer.py:48] [25191] accumulated_eval_time=838.015, accumulated_logging_time=1.42362, accumulated_submission_time=9746.98, global_step=25191, preemption_count=0, score=9746.98, test/accuracy=0.4672, test/loss=2.48417, test/num_examples=10000, total_duration=10589.1, train/accuracy=0.631298, train/loss=1.59647, validation/accuracy=0.585, validation/loss=1.82115, validation/num_examples=50000
I0307 06:41:23.547796 140237686679296 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.616755723953247, loss=2.92868971824646
I0307 06:42:02.051944 140237678286592 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.8990142345428467, loss=2.8549485206604004
I0307 06:42:40.378392 140237686679296 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.8354125022888184, loss=2.8395419120788574
I0307 06:43:18.600654 140237678286592 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.160630226135254, loss=2.805058002471924
I0307 06:43:57.004680 140237686679296 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.596400499343872, loss=2.8033089637756348
I0307 06:44:35.476628 140237678286592 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.2611162662506104, loss=2.8520748615264893
I0307 06:45:14.173992 140237686679296 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.286092519760132, loss=2.806668996810913
I0307 06:45:53.271739 140237678286592 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.427633047103882, loss=2.8889904022216797
I0307 06:46:31.682601 140237686679296 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.1745450496673584, loss=2.8508365154266357
I0307 06:47:10.330511 140237678286592 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.6886260509490967, loss=2.983203649520874
I0307 06:47:48.994946 140237686679296 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.766066312789917, loss=2.8666634559631348
I0307 06:48:27.879729 140237678286592 logging_writer.py:48] [26300] global_step=26300, grad_norm=4.520242214202881, loss=2.900477647781372
I0307 06:49:06.607646 140237686679296 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.622702121734619, loss=2.8453359603881836
I0307 06:49:45.520974 140237678286592 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.7485289573669434, loss=2.8931922912597656
I0307 06:49:49.348119 140393707492544 spec.py:321] Evaluating on the training split.
I0307 06:50:05.196537 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 06:50:29.832866 140393707492544 spec.py:349] Evaluating on the test split.
I0307 06:50:31.637214 140393707492544 submission_runner.py:469] Time since start: 11141.52s, 	Step: 26510, 	{'train/accuracy': 0.6360809803009033, 'train/loss': 1.5859854221343994, 'validation/accuracy': 0.5837399959564209, 'validation/loss': 1.829034447669983, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.477566957473755, 'test/num_examples': 10000, 'score': 10256.957358837128, 'total_duration': 11141.51793885231, 'accumulated_submission_time': 10256.957358837128, 'accumulated_eval_time': 880.304535150528, 'accumulated_logging_time': 1.4954111576080322}
I0307 06:50:31.695391 140237686679296 logging_writer.py:48] [26510] accumulated_eval_time=880.305, accumulated_logging_time=1.49541, accumulated_submission_time=10257, global_step=26510, preemption_count=0, score=10257, test/accuracy=0.4688, test/loss=2.47757, test/num_examples=10000, total_duration=11141.5, train/accuracy=0.636081, train/loss=1.58599, validation/accuracy=0.58374, validation/loss=1.82903, validation/num_examples=50000
I0307 06:51:07.270489 140237678286592 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.531956434249878, loss=2.8344762325286865
I0307 06:51:45.556481 140237686679296 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.7360353469848633, loss=2.8626232147216797
I0307 06:52:24.282140 140237678286592 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.0198562145233154, loss=2.864145040512085
I0307 06:53:02.809041 140237686679296 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.423262357711792, loss=2.8943772315979004
I0307 06:53:41.376485 140237678286592 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.465843439102173, loss=2.798675060272217
I0307 06:54:20.186465 140237686679296 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.1558051109313965, loss=2.770948648452759
I0307 06:54:58.751149 140237678286592 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.973679780960083, loss=2.84570574760437
I0307 06:55:37.263969 140237686679296 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.6632115840911865, loss=2.879913091659546
I0307 06:56:15.815087 140237678286592 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.790203809738159, loss=2.8649964332580566
I0307 06:56:54.626992 140237686679296 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.1745636463165283, loss=2.788790464401245
I0307 06:57:33.249794 140237678286592 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.3396530151367188, loss=2.9360389709472656
I0307 06:58:12.004081 140237686679296 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.1805577278137207, loss=2.8489015102386475
I0307 06:58:50.429797 140237678286592 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.0374348163604736, loss=2.8550291061401367
I0307 06:59:01.808901 140393707492544 spec.py:321] Evaluating on the training split.
I0307 06:59:17.670144 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 06:59:41.988440 140393707492544 spec.py:349] Evaluating on the test split.
I0307 06:59:43.830220 140393707492544 submission_runner.py:469] Time since start: 11693.71s, 	Step: 27831, 	{'train/accuracy': 0.6398875713348389, 'train/loss': 1.5636855363845825, 'validation/accuracy': 0.5902199745178223, 'validation/loss': 1.8054137229919434, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.473846435546875, 'test/num_examples': 10000, 'score': 10766.931269407272, 'total_duration': 11693.710929393768, 'accumulated_submission_time': 10766.931269407272, 'accumulated_eval_time': 922.3257946968079, 'accumulated_logging_time': 1.561805009841919}
I0307 06:59:43.895272 140237686679296 logging_writer.py:48] [27831] accumulated_eval_time=922.326, accumulated_logging_time=1.56181, accumulated_submission_time=10766.9, global_step=27831, preemption_count=0, score=10766.9, test/accuracy=0.4649, test/loss=2.47385, test/num_examples=10000, total_duration=11693.7, train/accuracy=0.639888, train/loss=1.56369, validation/accuracy=0.59022, validation/loss=1.80541, validation/num_examples=50000
I0307 07:00:10.679094 140237678286592 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.9134814739227295, loss=2.874009370803833
I0307 07:00:49.267102 140237686679296 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.9440622329711914, loss=2.8351032733917236
I0307 07:01:27.530007 140237678286592 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.908506393432617, loss=2.8421499729156494
I0307 07:02:05.987112 140237686679296 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.784548044204712, loss=2.9279825687408447
I0307 07:02:44.250643 140237678286592 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.080596446990967, loss=2.8424415588378906
I0307 07:03:22.670969 140237686679296 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.017456531524658, loss=2.9036202430725098
I0307 07:04:01.074930 140237678286592 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.6067185401916504, loss=2.8185510635375977
I0307 07:04:39.565832 140237686679296 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.088670253753662, loss=2.822592258453369
I0307 07:05:18.260177 140237678286592 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.0750176906585693, loss=2.954359531402588
I0307 07:05:57.217937 140237686679296 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.8875925540924072, loss=2.9249813556671143
I0307 07:06:35.752868 140237678286592 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.0068771839141846, loss=2.955481767654419
I0307 07:07:14.392354 140237686679296 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.991058111190796, loss=2.8985235691070557
I0307 07:07:52.985506 140237678286592 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.8267858028411865, loss=2.92592716217041
I0307 07:08:13.954697 140393707492544 spec.py:321] Evaluating on the training split.
I0307 07:08:29.025671 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 07:08:51.729887 140393707492544 spec.py:349] Evaluating on the test split.
I0307 07:08:53.560183 140393707492544 submission_runner.py:469] Time since start: 12243.44s, 	Step: 29156, 	{'train/accuracy': 0.6448501348495483, 'train/loss': 1.5242646932601929, 'validation/accuracy': 0.5925799608230591, 'validation/loss': 1.7726174592971802, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.427928924560547, 'test/num_examples': 10000, 'score': 11276.84908080101, 'total_duration': 12243.440912485123, 'accumulated_submission_time': 11276.84908080101, 'accumulated_eval_time': 961.9312443733215, 'accumulated_logging_time': 1.6346564292907715}
I0307 07:08:53.603952 140237686679296 logging_writer.py:48] [29156] accumulated_eval_time=961.931, accumulated_logging_time=1.63466, accumulated_submission_time=11276.8, global_step=29156, preemption_count=0, score=11276.8, test/accuracy=0.4775, test/loss=2.42793, test/num_examples=10000, total_duration=12243.4, train/accuracy=0.64485, train/loss=1.52426, validation/accuracy=0.59258, validation/loss=1.77262, validation/num_examples=50000
I0307 07:09:11.458012 140237678286592 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.6474990844726562, loss=2.905937671661377
I0307 07:09:49.852076 140237686679296 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.524770498275757, loss=2.8318111896514893
I0307 07:10:28.192449 140237678286592 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.0403618812561035, loss=2.825366973876953
I0307 07:11:06.742457 140237686679296 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.1215627193450928, loss=2.9533586502075195
I0307 07:11:45.343352 140237678286592 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.833645820617676, loss=2.884408473968506
I0307 07:12:23.954743 140237686679296 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.297523021697998, loss=2.7439520359039307
I0307 07:13:02.835648 140237678286592 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.318577766418457, loss=2.9543533325195312
I0307 07:13:41.637362 140237686679296 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.208765983581543, loss=2.8674566745758057
I0307 07:14:20.050492 140237678286592 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.684915065765381, loss=2.8348300457000732
I0307 07:14:58.808620 140237686679296 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.317659854888916, loss=2.7935171127319336
I0307 07:15:37.443167 140237678286592 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.9720356464385986, loss=2.799994707107544
I0307 07:16:15.966618 140237686679296 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.227536678314209, loss=2.8519790172576904
I0307 07:16:54.710401 140237678286592 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.330735921859741, loss=2.8013193607330322
I0307 07:17:23.645402 140393707492544 spec.py:321] Evaluating on the training split.
I0307 07:17:40.882764 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 07:18:04.977562 140393707492544 spec.py:349] Evaluating on the test split.
I0307 07:18:06.784113 140393707492544 submission_runner.py:469] Time since start: 12796.66s, 	Step: 30476, 	{'train/accuracy': 0.6464444994926453, 'train/loss': 1.575693964958191, 'validation/accuracy': 0.5981799960136414, 'validation/loss': 1.8023936748504639, 'validation/num_examples': 50000, 'test/accuracy': 0.47870001196861267, 'test/loss': 2.4508919715881348, 'test/num_examples': 10000, 'score': 11786.757893323898, 'total_duration': 12796.664817810059, 'accumulated_submission_time': 11786.757893323898, 'accumulated_eval_time': 1005.0699033737183, 'accumulated_logging_time': 1.6861090660095215}
I0307 07:18:06.824790 140237686679296 logging_writer.py:48] [30476] accumulated_eval_time=1005.07, accumulated_logging_time=1.68611, accumulated_submission_time=11786.8, global_step=30476, preemption_count=0, score=11786.8, test/accuracy=0.4787, test/loss=2.45089, test/num_examples=10000, total_duration=12796.7, train/accuracy=0.646444, train/loss=1.57569, validation/accuracy=0.59818, validation/loss=1.80239, validation/num_examples=50000
I0307 07:18:16.767655 140237678286592 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.3190340995788574, loss=2.753321886062622
I0307 07:18:55.014967 140237686679296 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.98711895942688, loss=2.861424446105957
I0307 07:19:33.506270 140237678286592 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.737217426300049, loss=2.8513870239257812
I0307 07:20:11.598786 140237686679296 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.761916160583496, loss=2.843722343444824
I0307 07:20:50.285297 140237678286592 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.0440242290496826, loss=2.9220004081726074
I0307 07:21:29.267203 140237686679296 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.2148454189300537, loss=2.745600700378418
I0307 07:22:08.047030 140237678286592 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.0706026554107666, loss=2.8068554401397705
I0307 07:22:46.694925 140237686679296 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.0045247077941895, loss=2.8259503841400146
I0307 07:23:25.835911 140237678286592 logging_writer.py:48] [31300] global_step=31300, grad_norm=4.441456317901611, loss=2.7810728549957275
I0307 07:24:04.469867 140237686679296 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.791222333908081, loss=2.8557488918304443
I0307 07:24:43.029141 140237678286592 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.6862783432006836, loss=2.8478500843048096
I0307 07:25:21.867204 140237686679296 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.304321050643921, loss=2.875859260559082
I0307 07:26:00.504485 140237678286592 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.1117215156555176, loss=2.8901925086975098
I0307 07:26:37.069988 140393707492544 spec.py:321] Evaluating on the training split.
I0307 07:26:54.186212 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 07:27:15.314452 140393707492544 spec.py:349] Evaluating on the test split.
I0307 07:27:17.155678 140393707492544 submission_runner.py:469] Time since start: 13347.04s, 	Step: 31795, 	{'train/accuracy': 0.6410833597183228, 'train/loss': 1.5624748468399048, 'validation/accuracy': 0.5927799940109253, 'validation/loss': 1.7792936563491821, 'validation/num_examples': 50000, 'test/accuracy': 0.46810001134872437, 'test/loss': 2.470116138458252, 'test/num_examples': 10000, 'score': 12296.850430250168, 'total_duration': 13347.036401987076, 'accumulated_submission_time': 12296.850430250168, 'accumulated_eval_time': 1045.155550956726, 'accumulated_logging_time': 1.7511441707611084}
I0307 07:27:17.199192 140237686679296 logging_writer.py:48] [31795] accumulated_eval_time=1045.16, accumulated_logging_time=1.75114, accumulated_submission_time=12296.9, global_step=31795, preemption_count=0, score=12296.9, test/accuracy=0.4681, test/loss=2.47012, test/num_examples=10000, total_duration=13347, train/accuracy=0.641083, train/loss=1.56247, validation/accuracy=0.59278, validation/loss=1.77929, validation/num_examples=50000
I0307 07:27:19.746217 140237678286592 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.2912259101867676, loss=2.785081624984741
I0307 07:27:58.429943 140237686679296 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.752941846847534, loss=2.842965841293335
I0307 07:28:37.098931 140237678286592 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.6733486652374268, loss=2.7015905380249023
I0307 07:29:15.861972 140237686679296 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.9188687801361084, loss=2.7508115768432617
I0307 07:29:54.631125 140237678286592 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.4753658771514893, loss=2.849670886993408
I0307 07:30:33.927666 140237686679296 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.844043493270874, loss=2.69411039352417
I0307 07:31:14.139378 140237678286592 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.6057770252227783, loss=2.8405449390411377
I0307 07:31:53.281888 140237686679296 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.228005886077881, loss=2.705493927001953
I0307 07:32:32.447685 140237678286592 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.120086669921875, loss=2.841496467590332
I0307 07:33:11.187110 140237686679296 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.859405279159546, loss=2.8701062202453613
I0307 07:33:50.102120 140237678286592 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.926880121231079, loss=2.766993522644043
I0307 07:34:28.740473 140237686679296 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.310471773147583, loss=2.870821714401245
I0307 07:35:07.667528 140237678286592 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.6499545574188232, loss=2.766873836517334
I0307 07:35:46.612948 140237686679296 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.1092634201049805, loss=2.811924457550049
I0307 07:35:47.458113 140393707492544 spec.py:321] Evaluating on the training split.
I0307 07:35:59.107559 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 07:36:24.067328 140393707492544 spec.py:349] Evaluating on the test split.
I0307 07:36:26.218188 140393707492544 submission_runner.py:469] Time since start: 13896.10s, 	Step: 33103, 	{'train/accuracy': 0.6491150856018066, 'train/loss': 1.5205764770507812, 'validation/accuracy': 0.6026600003242493, 'validation/loss': 1.742527723312378, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.3952646255493164, 'test/num_examples': 10000, 'score': 12806.972704172134, 'total_duration': 13896.098922252655, 'accumulated_submission_time': 12806.972704172134, 'accumulated_eval_time': 1083.9155917167664, 'accumulated_logging_time': 1.8032684326171875}
I0307 07:36:26.281633 140237678286592 logging_writer.py:48] [33103] accumulated_eval_time=1083.92, accumulated_logging_time=1.80327, accumulated_submission_time=12807, global_step=33103, preemption_count=0, score=12807, test/accuracy=0.4811, test/loss=2.39526, test/num_examples=10000, total_duration=13896.1, train/accuracy=0.649115, train/loss=1.52058, validation/accuracy=0.60266, validation/loss=1.74253, validation/num_examples=50000
I0307 07:37:04.138918 140237686679296 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.9231443405151367, loss=2.751162528991699
I0307 07:37:42.878441 140237678286592 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.964270830154419, loss=2.8912460803985596
I0307 07:38:21.433568 140237686679296 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.385986328125, loss=2.8333983421325684
I0307 07:39:00.502956 140237678286592 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.8127458095550537, loss=2.770015001296997
I0307 07:39:39.319205 140237686679296 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.5292673110961914, loss=2.882378339767456
I0307 07:40:17.963073 140237678286592 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.0142507553100586, loss=2.819774866104126
I0307 07:40:56.910756 140237686679296 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.8700826168060303, loss=2.8247063159942627
I0307 07:41:35.656577 140237678286592 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.4784579277038574, loss=2.8816018104553223
I0307 07:42:14.441298 140237686679296 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.945199728012085, loss=2.820333242416382
I0307 07:42:53.294053 140237678286592 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.326469898223877, loss=2.7456750869750977
I0307 07:43:31.978269 140237686679296 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.0763192176818848, loss=2.853201389312744
I0307 07:44:10.765458 140237678286592 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.1635847091674805, loss=2.9041361808776855
I0307 07:44:49.255414 140237686679296 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.2375330924987793, loss=2.79616379737854
I0307 07:44:56.336319 140393707492544 spec.py:321] Evaluating on the training split.
I0307 07:45:09.214856 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 07:45:31.837606 140393707492544 spec.py:349] Evaluating on the test split.
I0307 07:45:33.672358 140393707492544 submission_runner.py:469] Time since start: 14443.55s, 	Step: 34419, 	{'train/accuracy': 0.646882951259613, 'train/loss': 1.5322587490081787, 'validation/accuracy': 0.5976200103759766, 'validation/loss': 1.7603105306625366, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.4205799102783203, 'test/num_examples': 10000, 'score': 13316.89038491249, 'total_duration': 14443.553050518036, 'accumulated_submission_time': 13316.89038491249, 'accumulated_eval_time': 1121.2515542507172, 'accumulated_logging_time': 1.8746492862701416}
I0307 07:45:33.729193 140237678286592 logging_writer.py:48] [34419] accumulated_eval_time=1121.25, accumulated_logging_time=1.87465, accumulated_submission_time=13316.9, global_step=34419, preemption_count=0, score=13316.9, test/accuracy=0.478, test/loss=2.42058, test/num_examples=10000, total_duration=14443.6, train/accuracy=0.646883, train/loss=1.53226, validation/accuracy=0.59762, validation/loss=1.76031, validation/num_examples=50000
I0307 07:46:05.730065 140237686679296 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.7790305614471436, loss=2.7509496212005615
I0307 07:46:44.189382 140237678286592 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.7173573970794678, loss=2.8342597484588623
I0307 07:47:22.614937 140237686679296 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.8508691787719727, loss=2.78523325920105
I0307 07:48:01.401658 140237678286592 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.665914535522461, loss=2.822275400161743
I0307 07:48:40.238405 140237686679296 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.378922939300537, loss=2.8659772872924805
I0307 07:49:18.612207 140237678286592 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.1228830814361572, loss=2.767185688018799
I0307 07:49:57.626013 140237686679296 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.79012393951416, loss=2.873682737350464
I0307 07:50:36.427417 140237678286592 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.051863193511963, loss=2.7631828784942627
I0307 07:51:15.116400 140237686679296 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.9363198280334473, loss=2.9031269550323486
I0307 07:51:54.194555 140237678286592 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.012568235397339, loss=2.8603737354278564
I0307 07:52:33.014990 140237686679296 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.385505199432373, loss=2.831723690032959
I0307 07:53:11.693865 140237678286592 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.83370041847229, loss=2.8787405490875244
I0307 07:53:50.760823 140237686679296 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.0863654613494873, loss=2.805353879928589
I0307 07:54:04.051912 140393707492544 spec.py:321] Evaluating on the training split.
I0307 07:54:15.804252 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 07:54:46.937061 140393707492544 spec.py:349] Evaluating on the test split.
I0307 07:54:48.717577 140393707492544 submission_runner.py:469] Time since start: 14998.60s, 	Step: 35735, 	{'train/accuracy': 0.6544762253761292, 'train/loss': 1.5279052257537842, 'validation/accuracy': 0.6057800054550171, 'validation/loss': 1.7456291913986206, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.426006555557251, 'test/num_examples': 10000, 'score': 13827.075783252716, 'total_duration': 14998.59830904007, 'accumulated_submission_time': 13827.075783252716, 'accumulated_eval_time': 1165.9171843528748, 'accumulated_logging_time': 1.9393372535705566}
I0307 07:54:48.784096 140237678286592 logging_writer.py:48] [35735] accumulated_eval_time=1165.92, accumulated_logging_time=1.93934, accumulated_submission_time=13827.1, global_step=35735, preemption_count=0, score=13827.1, test/accuracy=0.4804, test/loss=2.42601, test/num_examples=10000, total_duration=14998.6, train/accuracy=0.654476, train/loss=1.52791, validation/accuracy=0.60578, validation/loss=1.74563, validation/num_examples=50000
I0307 07:55:14.478255 140237686679296 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.3487000465393066, loss=2.7940480709075928
I0307 07:55:52.897402 140237678286592 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.9650979042053223, loss=2.877804756164551
I0307 07:56:31.205129 140237686679296 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.2351858615875244, loss=2.8035287857055664
I0307 07:57:09.685510 140237678286592 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.206507921218872, loss=2.8514254093170166
I0307 07:57:48.340980 140237686679296 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.874988079071045, loss=2.740974187850952
I0307 07:58:27.310022 140237678286592 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.991462469100952, loss=2.754457712173462
I0307 07:59:06.276351 140237686679296 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.1763648986816406, loss=2.8066630363464355
I0307 07:59:45.232254 140237678286592 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.9483654499053955, loss=2.806288957595825
I0307 08:00:24.029122 140237686679296 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.4098658561706543, loss=2.8233540058135986
I0307 08:01:02.771406 140237678286592 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.148489236831665, loss=2.7891666889190674
I0307 08:01:41.548968 140237686679296 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.983895778656006, loss=2.7286887168884277
I0307 08:02:20.511198 140237678286592 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.862003803253174, loss=2.7489349842071533
I0307 08:02:59.445656 140237686679296 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.8791942596435547, loss=2.9064712524414062
I0307 08:03:19.040383 140393707492544 spec.py:321] Evaluating on the training split.
I0307 08:03:31.046078 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 08:03:58.162751 140393707492544 spec.py:349] Evaluating on the test split.
I0307 08:03:59.948367 140393707492544 submission_runner.py:469] Time since start: 15549.83s, 	Step: 37052, 	{'train/accuracy': 0.6554129123687744, 'train/loss': 1.4934836626052856, 'validation/accuracy': 0.6114999651908875, 'validation/loss': 1.7066612243652344, 'validation/num_examples': 50000, 'test/accuracy': 0.48660001158714294, 'test/loss': 2.3942503929138184, 'test/num_examples': 10000, 'score': 14337.196107625961, 'total_duration': 15549.829090356827, 'accumulated_submission_time': 14337.196107625961, 'accumulated_eval_time': 1206.8251340389252, 'accumulated_logging_time': 2.0135011672973633}
I0307 08:04:00.008927 140237678286592 logging_writer.py:48] [37052] accumulated_eval_time=1206.83, accumulated_logging_time=2.0135, accumulated_submission_time=14337.2, global_step=37052, preemption_count=0, score=14337.2, test/accuracy=0.4866, test/loss=2.39425, test/num_examples=10000, total_duration=15549.8, train/accuracy=0.655413, train/loss=1.49348, validation/accuracy=0.6115, validation/loss=1.70666, validation/num_examples=50000
I0307 08:04:18.973152 140237686679296 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.8830182552337646, loss=2.8048782348632812
I0307 08:04:57.455297 140237678286592 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.7693676948547363, loss=2.791740894317627
I0307 08:05:36.082817 140237686679296 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.9895429611206055, loss=2.896437406539917
I0307 08:06:14.623640 140237678286592 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.7997488975524902, loss=2.7986626625061035
I0307 08:06:53.485978 140237686679296 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.628354549407959, loss=2.718010902404785
I0307 08:07:32.053107 140237678286592 logging_writer.py:48] [37600] global_step=37600, grad_norm=4.0031962394714355, loss=2.8180482387542725
I0307 08:08:10.282190 140237686679296 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.0082802772521973, loss=2.852092742919922
I0307 08:08:48.860642 140237678286592 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.78482723236084, loss=2.8062167167663574
I0307 08:09:27.337753 140237686679296 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.8389639854431152, loss=2.800281047821045
I0307 08:10:06.077878 140237678286592 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.9221243858337402, loss=2.7999587059020996
I0307 08:10:44.526796 140237686679296 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.6283302307128906, loss=2.843190908432007
I0307 08:11:23.315573 140237678286592 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.2906434535980225, loss=2.8524363040924072
I0307 08:12:01.726521 140237686679296 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.542074203491211, loss=2.7900328636169434
I0307 08:12:30.013285 140393707492544 spec.py:321] Evaluating on the training split.
I0307 08:12:41.923725 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 08:13:12.288575 140393707492544 spec.py:349] Evaluating on the test split.
I0307 08:13:14.075894 140393707492544 submission_runner.py:469] Time since start: 16103.96s, 	Step: 38375, 	{'train/accuracy': 0.6594985723495483, 'train/loss': 1.485409140586853, 'validation/accuracy': 0.6105799674987793, 'validation/loss': 1.7028491497039795, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.384251832962036, 'test/num_examples': 10000, 'score': 14847.059845685959, 'total_duration': 16103.9566116333, 'accumulated_submission_time': 14847.059845685959, 'accumulated_eval_time': 1250.8877024650574, 'accumulated_logging_time': 2.0823917388916016}
I0307 08:13:14.118524 140237678286592 logging_writer.py:48] [38375] accumulated_eval_time=1250.89, accumulated_logging_time=2.08239, accumulated_submission_time=14847.1, global_step=38375, preemption_count=0, score=14847.1, test/accuracy=0.482, test/loss=2.38425, test/num_examples=10000, total_duration=16104, train/accuracy=0.659499, train/loss=1.48541, validation/accuracy=0.61058, validation/loss=1.70285, validation/num_examples=50000
I0307 08:13:24.231117 140237686679296 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.870885133743286, loss=2.804912805557251
I0307 08:14:02.939124 140237678286592 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.9869906902313232, loss=2.7999000549316406
I0307 08:14:41.361412 140237686679296 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.2847728729248047, loss=2.783688545227051
I0307 08:15:20.192752 140237678286592 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.0222861766815186, loss=2.8558924198150635
I0307 08:15:59.078605 140237686679296 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.0419063568115234, loss=2.78373646736145
I0307 08:16:37.841112 140237678286592 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.140573263168335, loss=2.750009536743164
I0307 08:17:15.869262 140237686679296 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.8156120777130127, loss=2.8269903659820557
I0307 08:17:55.121335 140237678286592 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.122959852218628, loss=2.7421700954437256
I0307 08:18:33.690848 140237686679296 logging_writer.py:48] [39200] global_step=39200, grad_norm=2.845569372177124, loss=2.9237020015716553
I0307 08:19:12.010452 140237678286592 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.9978132247924805, loss=2.7773172855377197
I0307 08:19:50.716543 140237686679296 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.2451059818267822, loss=2.8123345375061035
I0307 08:20:29.725130 140237678286592 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.027484893798828, loss=2.7743241786956787
I0307 08:21:08.265277 140237686679296 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.255718469619751, loss=2.7788612842559814
I0307 08:21:44.182240 140393707492544 spec.py:321] Evaluating on the training split.
I0307 08:21:56.145221 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 08:22:27.223764 140393707492544 spec.py:349] Evaluating on the test split.
I0307 08:22:29.012649 140393707492544 submission_runner.py:469] Time since start: 16658.89s, 	Step: 39693, 	{'train/accuracy': 0.6515066623687744, 'train/loss': 1.513933777809143, 'validation/accuracy': 0.605239987373352, 'validation/loss': 1.7309751510620117, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.419649124145508, 'test/num_examples': 10000, 'score': 15356.989696025848, 'total_duration': 16658.893371105194, 'accumulated_submission_time': 15356.989696025848, 'accumulated_eval_time': 1295.7180805206299, 'accumulated_logging_time': 2.133098602294922}
I0307 08:22:29.099859 140237678286592 logging_writer.py:48] [39693] accumulated_eval_time=1295.72, accumulated_logging_time=2.1331, accumulated_submission_time=15357, global_step=39693, preemption_count=0, score=15357, test/accuracy=0.4762, test/loss=2.41965, test/num_examples=10000, total_duration=16658.9, train/accuracy=0.651507, train/loss=1.51393, validation/accuracy=0.60524, validation/loss=1.73098, validation/num_examples=50000
I0307 08:22:32.203936 140237686679296 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.4160006046295166, loss=2.7159814834594727
I0307 08:23:10.609762 140237678286592 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.422424554824829, loss=2.855696678161621
I0307 08:23:49.588233 140237686679296 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.134181022644043, loss=2.7901792526245117
I0307 08:24:28.346688 140237678286592 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.577061176300049, loss=2.9212629795074463
I0307 08:25:07.196376 140237686679296 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.011159658432007, loss=2.8078935146331787
I0307 08:25:46.197673 140237678286592 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.863065719604492, loss=2.897920846939087
I0307 08:26:24.803138 140237686679296 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.9413154125213623, loss=2.8064963817596436
I0307 08:27:03.509004 140237678286592 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.493483066558838, loss=2.816504716873169
I0307 08:27:42.307442 140237686679296 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.5847153663635254, loss=2.71580171585083
I0307 08:28:21.177422 140237678286592 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.2250335216522217, loss=2.8110218048095703
I0307 08:28:59.680411 140237686679296 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.002960443496704, loss=2.7188377380371094
I0307 08:29:38.268512 140237678286592 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.5831565856933594, loss=2.8895256519317627
I0307 08:30:16.970471 140237686679296 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.6800038814544678, loss=2.708333969116211
I0307 08:30:56.015792 140237678286592 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.2141501903533936, loss=2.7058279514312744
I0307 08:30:59.162277 140393707492544 spec.py:321] Evaluating on the training split.
I0307 08:31:10.860004 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 08:31:41.178801 140393707492544 spec.py:349] Evaluating on the test split.
I0307 08:31:42.971592 140393707492544 submission_runner.py:469] Time since start: 17212.85s, 	Step: 41009, 	{'train/accuracy': 0.6520049571990967, 'train/loss': 1.4856503009796143, 'validation/accuracy': 0.6104599833488464, 'validation/loss': 1.6889045238494873, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.3565027713775635, 'test/num_examples': 10000, 'score': 15866.921926498413, 'total_duration': 17212.8523144722, 'accumulated_submission_time': 15866.921926498413, 'accumulated_eval_time': 1339.5273489952087, 'accumulated_logging_time': 2.2279880046844482}
I0307 08:31:43.045391 140237686679296 logging_writer.py:48] [41009] accumulated_eval_time=1339.53, accumulated_logging_time=2.22799, accumulated_submission_time=15866.9, global_step=41009, preemption_count=0, score=15866.9, test/accuracy=0.4862, test/loss=2.3565, test/num_examples=10000, total_duration=17212.9, train/accuracy=0.652005, train/loss=1.48565, validation/accuracy=0.61046, validation/loss=1.6889, validation/num_examples=50000
I0307 08:32:18.377830 140237678286592 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.3243353366851807, loss=2.830770492553711
I0307 08:32:56.972701 140237686679296 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.1792454719543457, loss=2.8852691650390625
I0307 08:33:35.675801 140237678286592 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.8082995414733887, loss=2.7614543437957764
I0307 08:34:14.189345 140237686679296 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.0121772289276123, loss=2.8010880947113037
I0307 08:34:53.029017 140237678286592 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.922053337097168, loss=2.7686760425567627
I0307 08:35:31.560884 140237686679296 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.195176362991333, loss=2.8098955154418945
I0307 08:36:10.695183 140237678286592 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.010902166366577, loss=2.7926924228668213
I0307 08:36:49.279650 140237686679296 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.1873321533203125, loss=2.693765878677368
I0307 08:37:28.205355 140237678286592 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.9561352729797363, loss=2.8353190422058105
I0307 08:38:06.887557 140237686679296 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.1614019870758057, loss=2.775275230407715
I0307 08:38:45.722503 140237678286592 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.3313350677490234, loss=2.8650946617126465
I0307 08:39:24.822828 140237686679296 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.425257682800293, loss=2.7058746814727783
I0307 08:40:03.267319 140237678286592 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.0834150314331055, loss=2.7232165336608887
I0307 08:40:13.016049 140393707492544 spec.py:321] Evaluating on the training split.
I0307 08:40:25.262435 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 08:40:50.926885 140393707492544 spec.py:349] Evaluating on the test split.
I0307 08:40:52.742645 140393707492544 submission_runner.py:469] Time since start: 17762.62s, 	Step: 42326, 	{'train/accuracy': 0.6617307066917419, 'train/loss': 1.4724243879318237, 'validation/accuracy': 0.609279990196228, 'validation/loss': 1.7118030786514282, 'validation/num_examples': 50000, 'test/accuracy': 0.48730000853538513, 'test/loss': 2.3597989082336426, 'test/num_examples': 10000, 'score': 16376.759145259857, 'total_duration': 17762.623378276825, 'accumulated_submission_time': 16376.759145259857, 'accumulated_eval_time': 1379.25390791893, 'accumulated_logging_time': 2.3094470500946045}
I0307 08:40:52.815228 140237686679296 logging_writer.py:48] [42326] accumulated_eval_time=1379.25, accumulated_logging_time=2.30945, accumulated_submission_time=16376.8, global_step=42326, preemption_count=0, score=16376.8, test/accuracy=0.4873, test/loss=2.3598, test/num_examples=10000, total_duration=17762.6, train/accuracy=0.661731, train/loss=1.47242, validation/accuracy=0.60928, validation/loss=1.7118, validation/num_examples=50000
I0307 08:41:22.108183 140237678286592 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.6576101779937744, loss=2.749723434448242
I0307 08:42:00.492429 140237686679296 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.130366325378418, loss=2.713486671447754
I0307 08:42:39.296919 140237678286592 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.582496404647827, loss=2.8363826274871826
I0307 08:43:17.764505 140237686679296 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.4900529384613037, loss=2.773683547973633
I0307 08:43:56.352899 140237678286592 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.0664443969726562, loss=2.830458402633667
I0307 08:44:34.815683 140237686679296 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.089064359664917, loss=2.724381446838379
I0307 08:45:13.349552 140237678286592 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.027796745300293, loss=2.6852009296417236
I0307 08:45:51.982612 140237686679296 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.033900260925293, loss=2.766927719116211
I0307 08:46:30.716775 140237678286592 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.1527435779571533, loss=2.7282347679138184
I0307 08:47:09.137084 140237686679296 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.7553999423980713, loss=2.797135829925537
I0307 08:47:47.777563 140237678286592 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.2004778385162354, loss=2.758193016052246
I0307 08:48:26.196555 140237686679296 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.35198974609375, loss=2.8248820304870605
I0307 08:49:04.591903 140237678286592 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.891223430633545, loss=2.8527884483337402
I0307 08:49:22.828656 140393707492544 spec.py:321] Evaluating on the training split.
I0307 08:49:34.819947 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 08:50:00.343375 140393707492544 spec.py:349] Evaluating on the test split.
I0307 08:50:02.176050 140393707492544 submission_runner.py:469] Time since start: 18312.06s, 	Step: 43648, 	{'train/accuracy': 0.6495137214660645, 'train/loss': 1.5253076553344727, 'validation/accuracy': 0.6074599623680115, 'validation/loss': 1.7194430828094482, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.420499801635742, 'test/num_examples': 10000, 'score': 16886.63295531273, 'total_duration': 18312.05678343773, 'accumulated_submission_time': 16886.63295531273, 'accumulated_eval_time': 1418.6012780666351, 'accumulated_logging_time': 2.3899149894714355}
I0307 08:50:02.306427 140237686679296 logging_writer.py:48] [43648] accumulated_eval_time=1418.6, accumulated_logging_time=2.38991, accumulated_submission_time=16886.6, global_step=43648, preemption_count=0, score=16886.6, test/accuracy=0.478, test/loss=2.4205, test/num_examples=10000, total_duration=18312.1, train/accuracy=0.649514, train/loss=1.52531, validation/accuracy=0.60746, validation/loss=1.71944, validation/num_examples=50000
I0307 08:50:22.817673 140237678286592 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.5782370567321777, loss=2.8648946285247803
I0307 08:51:01.944027 140237686679296 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.673238515853882, loss=2.7738983631134033
I0307 08:51:40.438594 140237678286592 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.0102710723876953, loss=2.7725467681884766
I0307 08:52:18.962302 140237686679296 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.0675978660583496, loss=2.7530102729797363
I0307 08:52:57.547143 140237678286592 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.594097852706909, loss=2.6983628273010254
I0307 08:53:36.258941 140237686679296 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.8478877544403076, loss=2.8323707580566406
I0307 08:54:15.059598 140237678286592 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.3782219886779785, loss=2.8840980529785156
I0307 08:54:53.792892 140237686679296 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.062314987182617, loss=2.7597453594207764
I0307 08:55:32.771457 140237678286592 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.9008591175079346, loss=2.7016396522521973
I0307 08:56:11.081206 140237686679296 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.8803467750549316, loss=2.7135720252990723
I0307 08:56:49.827442 140237678286592 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.8951659202575684, loss=2.6894257068634033
I0307 08:57:28.428941 140237686679296 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.2059237957000732, loss=2.7432661056518555
I0307 08:58:07.182125 140237678286592 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.298366069793701, loss=2.7463345527648926
I0307 08:58:32.558078 140393707492544 spec.py:321] Evaluating on the training split.
I0307 08:58:45.224668 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 08:59:13.337522 140393707492544 spec.py:349] Evaluating on the test split.
I0307 08:59:15.124761 140393707492544 submission_runner.py:469] Time since start: 18865.01s, 	Step: 44967, 	{'train/accuracy': 0.659598171710968, 'train/loss': 1.4718120098114014, 'validation/accuracy': 0.6124799847602844, 'validation/loss': 1.6863794326782227, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3506157398223877, 'test/num_examples': 10000, 'score': 17396.74730324745, 'total_duration': 18865.005488157272, 'accumulated_submission_time': 17396.74730324745, 'accumulated_eval_time': 1461.1679220199585, 'accumulated_logging_time': 2.5292623043060303}
I0307 08:59:15.219328 140237686679296 logging_writer.py:48] [44967] accumulated_eval_time=1461.17, accumulated_logging_time=2.52926, accumulated_submission_time=17396.7, global_step=44967, preemption_count=0, score=17396.7, test/accuracy=0.4867, test/loss=2.35062, test/num_examples=10000, total_duration=18865, train/accuracy=0.659598, train/loss=1.47181, validation/accuracy=0.61248, validation/loss=1.68638, validation/num_examples=50000
I0307 08:59:28.411503 140237678286592 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.844301223754883, loss=2.813511371612549
I0307 09:00:07.632876 140237686679296 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.1797585487365723, loss=2.7513816356658936
I0307 09:00:47.100189 140237678286592 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.741487979888916, loss=2.8432393074035645
I0307 09:01:25.297297 140237686679296 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.4330437183380127, loss=2.8650057315826416
I0307 09:02:03.780023 140237678286592 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.9431991577148438, loss=2.842951536178589
I0307 09:02:42.555544 140237686679296 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.8383634090423584, loss=2.8268468379974365
I0307 09:03:21.243004 140237678286592 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.1307506561279297, loss=2.7870235443115234
I0307 09:04:00.023872 140237686679296 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.026085138320923, loss=2.793203592300415
I0307 09:04:38.579376 140237678286592 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.2286674976348877, loss=2.7072653770446777
I0307 09:05:17.125680 140237686679296 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.9487204551696777, loss=2.795891761779785
I0307 09:05:55.934857 140237678286592 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.810241460800171, loss=2.7226691246032715
I0307 09:06:34.546697 140237686679296 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.124150276184082, loss=2.737746000289917
I0307 09:07:13.257471 140237678286592 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.680051803588867, loss=2.7017107009887695
I0307 09:07:45.200945 140393707492544 spec.py:321] Evaluating on the training split.
I0307 09:07:57.593360 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 09:08:24.883985 140393707492544 spec.py:349] Evaluating on the test split.
I0307 09:08:26.683127 140393707492544 submission_runner.py:469] Time since start: 19416.56s, 	Step: 46283, 	{'train/accuracy': 0.6578643321990967, 'train/loss': 1.4481370449066162, 'validation/accuracy': 0.6146999597549438, 'validation/loss': 1.6504967212677002, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.333427906036377, 'test/num_examples': 10000, 'score': 17906.59534907341, 'total_duration': 19416.56383371353, 'accumulated_submission_time': 17906.59534907341, 'accumulated_eval_time': 1502.6500434875488, 'accumulated_logging_time': 2.6325485706329346}
I0307 09:08:26.811516 140237686679296 logging_writer.py:48] [46283] accumulated_eval_time=1502.65, accumulated_logging_time=2.63255, accumulated_submission_time=17906.6, global_step=46283, preemption_count=0, score=17906.6, test/accuracy=0.491, test/loss=2.33343, test/num_examples=10000, total_duration=19416.6, train/accuracy=0.657864, train/loss=1.44814, validation/accuracy=0.6147, validation/loss=1.6505, validation/num_examples=50000
I0307 09:08:33.902101 140237678286592 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.9341201782226562, loss=2.7783584594726562
I0307 09:09:13.527606 140237686679296 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.006352663040161, loss=2.6430323123931885
I0307 09:09:52.089210 140237678286592 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.512733221054077, loss=2.742119312286377
I0307 09:10:30.466600 140237686679296 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.8183183670043945, loss=2.712669610977173
I0307 09:11:09.156901 140237678286592 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.8469955921173096, loss=2.6674392223358154
I0307 09:11:47.658800 140237686679296 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.80194354057312, loss=2.7936742305755615
I0307 09:12:26.226470 140237678286592 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.629051923751831, loss=2.7854599952697754
I0307 09:13:04.580296 140237686679296 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.2335572242736816, loss=2.7712512016296387
I0307 09:13:42.710556 140237678286592 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.8796749114990234, loss=2.7174835205078125
I0307 09:14:21.009235 140237686679296 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.2518138885498047, loss=2.8536548614501953
I0307 09:14:59.562951 140237678286592 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.6565515995025635, loss=2.65700626373291
I0307 09:15:38.238726 140237686679296 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.6240317821502686, loss=2.7687697410583496
I0307 09:16:16.729001 140237678286592 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.268787384033203, loss=2.7518532276153564
I0307 09:16:55.550065 140237686679296 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.584895133972168, loss=2.6982474327087402
I0307 09:16:56.748272 140393707492544 spec.py:321] Evaluating on the training split.
I0307 09:17:08.620421 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 09:17:37.146717 140393707492544 spec.py:349] Evaluating on the test split.
I0307 09:17:38.926427 140393707492544 submission_runner.py:469] Time since start: 19968.81s, 	Step: 47604, 	{'train/accuracy': 0.6633250713348389, 'train/loss': 1.4739528894424438, 'validation/accuracy': 0.6157799959182739, 'validation/loss': 1.6829516887664795, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.3608720302581787, 'test/num_examples': 10000, 'score': 18416.387121915817, 'total_duration': 19968.807157993317, 'accumulated_submission_time': 18416.387121915817, 'accumulated_eval_time': 1544.8281593322754, 'accumulated_logging_time': 2.7714359760284424}
I0307 09:17:39.001062 140237678286592 logging_writer.py:48] [47604] accumulated_eval_time=1544.83, accumulated_logging_time=2.77144, accumulated_submission_time=18416.4, global_step=47604, preemption_count=0, score=18416.4, test/accuracy=0.492, test/loss=2.36087, test/num_examples=10000, total_duration=19968.8, train/accuracy=0.663325, train/loss=1.47395, validation/accuracy=0.61578, validation/loss=1.68295, validation/num_examples=50000
I0307 09:18:16.783088 140237686679296 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.3909518718719482, loss=2.70743465423584
I0307 09:18:55.415498 140237678286592 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.200916051864624, loss=2.7982177734375
I0307 09:19:34.320018 140237686679296 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.927888870239258, loss=2.8205935955047607
I0307 09:20:12.756401 140237678286592 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.062227487564087, loss=2.7932491302490234
I0307 09:20:51.152363 140237686679296 logging_writer.py:48] [48100] global_step=48100, grad_norm=2.9933574199676514, loss=2.724067211151123
I0307 09:21:29.956078 140237678286592 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.0506842136383057, loss=2.6488094329833984
I0307 09:22:08.746847 140237686679296 logging_writer.py:48] [48300] global_step=48300, grad_norm=2.810670852661133, loss=2.769932985305786
I0307 09:22:47.492380 140237678286592 logging_writer.py:48] [48400] global_step=48400, grad_norm=2.95648455619812, loss=2.8134679794311523
I0307 09:23:26.386400 140237686679296 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.154059648513794, loss=2.7322158813476562
I0307 09:24:04.954775 140237678286592 logging_writer.py:48] [48600] global_step=48600, grad_norm=2.9473345279693604, loss=2.7313199043273926
I0307 09:24:43.346378 140237686679296 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.478728771209717, loss=2.8005847930908203
I0307 09:25:22.758893 140237678286592 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.7359414100646973, loss=2.7469682693481445
I0307 09:26:01.373883 140237686679296 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.6101760864257812, loss=2.6971356868743896
I0307 09:26:09.153883 140393707492544 spec.py:321] Evaluating on the training split.
I0307 09:26:20.546991 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 09:26:46.381068 140393707492544 spec.py:349] Evaluating on the test split.
I0307 09:26:48.192569 140393707492544 submission_runner.py:469] Time since start: 20518.07s, 	Step: 48921, 	{'train/accuracy': 0.6593391299247742, 'train/loss': 1.4737554788589478, 'validation/accuracy': 0.6145600080490112, 'validation/loss': 1.6812688112258911, 'validation/num_examples': 50000, 'test/accuracy': 0.4832000136375427, 'test/loss': 2.3895442485809326, 'test/num_examples': 10000, 'score': 18926.40757584572, 'total_duration': 20518.0732858181, 'accumulated_submission_time': 18926.40757584572, 'accumulated_eval_time': 1583.8667945861816, 'accumulated_logging_time': 2.8539810180664062}
I0307 09:26:48.260155 140237678286592 logging_writer.py:48] [48921] accumulated_eval_time=1583.87, accumulated_logging_time=2.85398, accumulated_submission_time=18926.4, global_step=48921, preemption_count=0, score=18926.4, test/accuracy=0.4832, test/loss=2.38954, test/num_examples=10000, total_duration=20518.1, train/accuracy=0.659339, train/loss=1.47376, validation/accuracy=0.61456, validation/loss=1.68127, validation/num_examples=50000
I0307 09:27:19.053811 140237686679296 logging_writer.py:48] [49000] global_step=49000, grad_norm=2.9308595657348633, loss=2.786942958831787
I0307 09:27:57.505247 140237678286592 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.8773305416107178, loss=2.8222408294677734
I0307 09:28:36.159372 140237686679296 logging_writer.py:48] [49200] global_step=49200, grad_norm=2.8190531730651855, loss=2.781695604324341
I0307 09:29:14.771229 140237678286592 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.0425302982330322, loss=2.791024684906006
I0307 09:29:53.637553 140237686679296 logging_writer.py:48] [49400] global_step=49400, grad_norm=2.616699457168579, loss=2.739640235900879
I0307 09:30:32.085340 140237678286592 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.18168568611145, loss=2.7216129302978516
I0307 09:31:10.839686 140237686679296 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.72224760055542, loss=2.6602296829223633
I0307 09:31:49.512794 140237678286592 logging_writer.py:48] [49700] global_step=49700, grad_norm=2.872345209121704, loss=2.8074965476989746
I0307 09:32:28.213004 140237686679296 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.813114643096924, loss=2.7653322219848633
I0307 09:33:06.733196 140237678286592 logging_writer.py:48] [49900] global_step=49900, grad_norm=2.9071598052978516, loss=2.726957082748413
I0307 09:33:44.837386 140237686679296 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.2605338096618652, loss=2.8384814262390137
I0307 09:34:23.898632 140237678286592 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.2220346927642822, loss=2.8042519092559814
I0307 09:35:02.303553 140237686679296 logging_writer.py:48] [50200] global_step=50200, grad_norm=2.6606836318969727, loss=2.68820858001709
I0307 09:35:18.441023 140393707492544 spec.py:321] Evaluating on the training split.
I0307 09:35:30.296551 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 09:35:56.880597 140393707492544 spec.py:349] Evaluating on the test split.
I0307 09:35:58.676611 140393707492544 submission_runner.py:469] Time since start: 21068.56s, 	Step: 50243, 	{'train/accuracy': 0.6617107391357422, 'train/loss': 1.4694215059280396, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.6839659214019775, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.3257341384887695, 'test/num_examples': 10000, 'score': 19436.45262861252, 'total_duration': 21068.557340860367, 'accumulated_submission_time': 19436.45262861252, 'accumulated_eval_time': 1624.1023440361023, 'accumulated_logging_time': 2.9294593334198}
I0307 09:35:58.735300 140237678286592 logging_writer.py:48] [50243] accumulated_eval_time=1624.1, accumulated_logging_time=2.92946, accumulated_submission_time=19436.5, global_step=50243, preemption_count=0, score=19436.5, test/accuracy=0.4956, test/loss=2.32573, test/num_examples=10000, total_duration=21068.6, train/accuracy=0.661711, train/loss=1.46942, validation/accuracy=0.61268, validation/loss=1.68397, validation/num_examples=50000
I0307 09:36:21.034705 140237686679296 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.5805864334106445, loss=2.7771449089050293
I0307 09:36:59.461174 140237678286592 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.018094539642334, loss=2.8255348205566406
I0307 09:37:38.009588 140237686679296 logging_writer.py:48] [50500] global_step=50500, grad_norm=2.892758369445801, loss=2.7375001907348633
I0307 09:38:16.653782 140237678286592 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.087733507156372, loss=2.809666872024536
I0307 09:38:55.003680 140237686679296 logging_writer.py:48] [50700] global_step=50700, grad_norm=2.9476726055145264, loss=2.7372074127197266
I0307 09:39:33.220268 140237678286592 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.753006935119629, loss=2.7468788623809814
I0307 09:40:11.737071 140237686679296 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.167556047439575, loss=2.7315380573272705
I0307 09:40:50.120573 140237678286592 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.213202714920044, loss=2.7880027294158936
I0307 09:41:28.668926 140237686679296 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.710324764251709, loss=2.7711904048919678
I0307 09:42:06.887487 140237678286592 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.3218069076538086, loss=2.7257308959960938
I0307 09:42:45.851018 140237686679296 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.4580559730529785, loss=2.6301164627075195
I0307 09:43:24.396073 140237678286592 logging_writer.py:48] [51400] global_step=51400, grad_norm=2.5031120777130127, loss=2.6452274322509766
I0307 09:44:02.958240 140237686679296 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.575080156326294, loss=2.747523307800293
I0307 09:44:28.893693 140393707492544 spec.py:321] Evaluating on the training split.
I0307 09:44:40.950886 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 09:45:08.168120 140393707492544 spec.py:349] Evaluating on the test split.
I0307 09:45:09.958690 140393707492544 submission_runner.py:469] Time since start: 21619.84s, 	Step: 51568, 	{'train/accuracy': 0.6576650142669678, 'train/loss': 1.491856336593628, 'validation/accuracy': 0.6123799681663513, 'validation/loss': 1.7056300640106201, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.3611185550689697, 'test/num_examples': 10000, 'score': 19946.46866297722, 'total_duration': 21619.83942103386, 'accumulated_submission_time': 19946.46866297722, 'accumulated_eval_time': 1665.1673040390015, 'accumulated_logging_time': 2.997260332107544}
I0307 09:45:10.011617 140237678286592 logging_writer.py:48] [51568] accumulated_eval_time=1665.17, accumulated_logging_time=2.99726, accumulated_submission_time=19946.5, global_step=51568, preemption_count=0, score=19946.5, test/accuracy=0.4958, test/loss=2.36112, test/num_examples=10000, total_duration=21619.8, train/accuracy=0.657665, train/loss=1.49186, validation/accuracy=0.61238, validation/loss=1.70563, validation/num_examples=50000
I0307 09:45:22.848518 140237686679296 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.2598488330841064, loss=2.7413976192474365
I0307 09:46:01.611995 140237678286592 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.173593759536743, loss=2.684554100036621
I0307 09:46:40.219494 140237686679296 logging_writer.py:48] [51800] global_step=51800, grad_norm=2.8298587799072266, loss=2.756528854370117
I0307 09:47:18.587706 140237678286592 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.6702523231506348, loss=2.7851054668426514
I0307 09:47:57.407956 140237686679296 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.109387159347534, loss=2.708214282989502
I0307 09:48:36.141158 140237678286592 logging_writer.py:48] [52100] global_step=52100, grad_norm=2.7974345684051514, loss=2.6596286296844482
I0307 09:49:14.805998 140237686679296 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.078307867050171, loss=2.807142496109009
I0307 09:49:53.539199 140237678286592 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.1456499099731445, loss=2.837646484375
I0307 09:50:31.768843 140237686679296 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.1002538204193115, loss=2.75105619430542
I0307 09:51:10.438464 140237678286592 logging_writer.py:48] [52500] global_step=52500, grad_norm=2.866457939147949, loss=2.7937753200531006
I0307 09:52:27.658150 140237686679296 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.1936185359954834, loss=2.793564796447754
I0307 09:53:06.923929 140237678286592 logging_writer.py:48] [52700] global_step=52700, grad_norm=2.957685708999634, loss=2.6674623489379883
I0307 09:53:40.197433 140393707492544 spec.py:321] Evaluating on the training split.
I0307 09:53:51.616023 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 09:54:18.106729 140393707492544 spec.py:349] Evaluating on the test split.
I0307 09:54:19.900389 140393707492544 submission_runner.py:469] Time since start: 22169.78s, 	Step: 52784, 	{'train/accuracy': 0.6667928695678711, 'train/loss': 1.4553090333938599, 'validation/accuracy': 0.6168000102043152, 'validation/loss': 1.6777008771896362, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.350822687149048, 'test/num_examples': 10000, 'score': 20456.509650230408, 'total_duration': 22169.78112077713, 'accumulated_submission_time': 20456.509650230408, 'accumulated_eval_time': 1704.8702244758606, 'accumulated_logging_time': 3.0712063312530518}
I0307 09:54:20.015771 140237686679296 logging_writer.py:48] [52784] accumulated_eval_time=1704.87, accumulated_logging_time=3.07121, accumulated_submission_time=20456.5, global_step=52784, preemption_count=0, score=20456.5, test/accuracy=0.4919, test/loss=2.35082, test/num_examples=10000, total_duration=22169.8, train/accuracy=0.666793, train/loss=1.45531, validation/accuracy=0.6168, validation/loss=1.6777, validation/num_examples=50000
I0307 09:54:26.731413 140237678286592 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.180790901184082, loss=2.744833469390869
I0307 09:55:05.914073 140237686679296 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.959887981414795, loss=2.6633615493774414
I0307 09:55:45.129796 140237678286592 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.623955011367798, loss=2.708183526992798
I0307 09:56:23.928103 140237686679296 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.449913740158081, loss=2.761730432510376
I0307 09:57:02.949263 140237678286592 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.476651430130005, loss=2.75667667388916
I0307 09:57:42.195049 140237686679296 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.289475440979004, loss=2.7277672290802
I0307 09:58:21.272260 140237678286592 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.134456157684326, loss=2.7335314750671387
I0307 09:59:00.639101 140237686679296 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.296719551086426, loss=2.8303775787353516
I0307 09:59:40.092829 140237678286592 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.262162208557129, loss=2.7453842163085938
I0307 10:00:19.397593 140237686679296 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.0034284591674805, loss=2.7766993045806885
I0307 10:00:59.149342 140237678286592 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.317513942718506, loss=2.758456230163574
I0307 10:01:38.530952 140237686679296 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.262179374694824, loss=2.748901605606079
I0307 10:02:17.583967 140237678286592 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.277822971343994, loss=2.6919660568237305
I0307 10:02:50.105256 140393707492544 spec.py:321] Evaluating on the training split.
I0307 10:03:02.449666 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 10:03:27.194144 140393707492544 spec.py:349] Evaluating on the test split.
I0307 10:03:28.979648 140393707492544 submission_runner.py:469] Time since start: 22718.86s, 	Step: 54083, 	{'train/accuracy': 0.6694634556770325, 'train/loss': 1.4290324449539185, 'validation/accuracy': 0.6240999698638916, 'validation/loss': 1.6453121900558472, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.3095548152923584, 'test/num_examples': 10000, 'score': 20966.437312602997, 'total_duration': 22718.860379219055, 'accumulated_submission_time': 20966.437312602997, 'accumulated_eval_time': 1743.7445893287659, 'accumulated_logging_time': 3.201098680496216}
I0307 10:03:29.032664 140237686679296 logging_writer.py:48] [54083] accumulated_eval_time=1743.74, accumulated_logging_time=3.2011, accumulated_submission_time=20966.4, global_step=54083, preemption_count=0, score=20966.4, test/accuracy=0.4961, test/loss=2.30955, test/num_examples=10000, total_duration=22718.9, train/accuracy=0.669463, train/loss=1.42903, validation/accuracy=0.6241, validation/loss=1.64531, validation/num_examples=50000
I0307 10:03:36.091546 140237678286592 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.040461540222168, loss=2.7975215911865234
I0307 10:04:15.309134 140237686679296 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.833928346633911, loss=2.7986934185028076
I0307 10:04:55.049776 140237678286592 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.5460329055786133, loss=2.7266008853912354
I0307 10:05:34.818469 140237686679296 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.0920217037200928, loss=2.7193939685821533
I0307 10:06:13.389607 140237678286592 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.07377290725708, loss=2.7470409870147705
I0307 10:06:53.200532 140237686679296 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.709758758544922, loss=2.69692063331604
I0307 10:07:33.103682 140237678286592 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.129427671432495, loss=2.8680083751678467
I0307 10:08:13.569508 140237686679296 logging_writer.py:48] [54800] global_step=54800, grad_norm=2.995878219604492, loss=2.689436197280884
I0307 10:08:53.377383 140237678286592 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.99035382270813, loss=2.698869466781616
I0307 10:09:33.877969 140237686679296 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.3266828060150146, loss=2.769042491912842
I0307 10:10:13.336761 140237678286592 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.208355188369751, loss=2.6752688884735107
I0307 10:10:53.118275 140237686679296 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.8092901706695557, loss=2.756948471069336
I0307 10:11:32.163133 140237678286592 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.8168721199035645, loss=2.6483211517333984
I0307 10:11:59.093812 140393707492544 spec.py:321] Evaluating on the training split.
I0307 10:12:10.572908 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 10:12:38.778373 140393707492544 spec.py:349] Evaluating on the test split.
I0307 10:12:40.554465 140393707492544 submission_runner.py:469] Time since start: 23270.44s, 	Step: 55368, 	{'train/accuracy': 0.6736686825752258, 'train/loss': 1.373186469078064, 'validation/accuracy': 0.6240999698638916, 'validation/loss': 1.615986943244934, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.295732021331787, 'test/num_examples': 10000, 'score': 21476.33947277069, 'total_duration': 23270.435178995132, 'accumulated_submission_time': 21476.33947277069, 'accumulated_eval_time': 1785.2051935195923, 'accumulated_logging_time': 3.26275897026062}
I0307 10:12:40.687911 140237686679296 logging_writer.py:48] [55368] accumulated_eval_time=1785.21, accumulated_logging_time=3.26276, accumulated_submission_time=21476.3, global_step=55368, preemption_count=0, score=21476.3, test/accuracy=0.4956, test/loss=2.29573, test/num_examples=10000, total_duration=23270.4, train/accuracy=0.673669, train/loss=1.37319, validation/accuracy=0.6241, validation/loss=1.61599, validation/num_examples=50000
I0307 10:12:53.463560 140237678286592 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.304891347885132, loss=2.705854892730713
I0307 10:13:32.327812 140237686679296 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.7196245193481445, loss=2.7659034729003906
I0307 10:14:11.773199 140237678286592 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.0522537231445312, loss=2.6411194801330566
I0307 10:14:50.807085 140237686679296 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.0426366329193115, loss=2.837913990020752
I0307 10:15:30.092767 140237678286592 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.1667392253875732, loss=2.7513363361358643
I0307 10:16:09.597256 140237686679296 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.1650259494781494, loss=2.806601047515869
I0307 10:16:49.408061 140237678286592 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.046109199523926, loss=2.7138237953186035
I0307 10:17:29.484585 140237686679296 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.0193605422973633, loss=2.640782356262207
I0307 10:18:09.311955 140237678286592 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.2212111949920654, loss=2.6587271690368652
2025-03-07 10:18:39.231050: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:18:49.442468 140237686679296 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.566663980484009, loss=2.6472043991088867
I0307 10:19:28.942545 140237678286592 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.044826030731201, loss=2.743156671524048
I0307 10:20:08.584023 140237686679296 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.0768532752990723, loss=2.666855573654175
I0307 10:20:48.149863 140237678286592 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.1512207984924316, loss=2.755903482437134
I0307 10:21:10.615219 140393707492544 spec.py:321] Evaluating on the training split.
I0307 10:21:22.300940 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 10:21:46.917346 140393707492544 spec.py:349] Evaluating on the test split.
I0307 10:21:48.704390 140393707492544 submission_runner.py:469] Time since start: 23818.59s, 	Step: 56657, 	{'train/accuracy': 0.6736487150192261, 'train/loss': 1.3920156955718994, 'validation/accuracy': 0.6235799789428711, 'validation/loss': 1.62201726436615, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.298546314239502, 'test/num_examples': 10000, 'score': 21986.10379600525, 'total_duration': 23818.58509993553, 'accumulated_submission_time': 21986.10379600525, 'accumulated_eval_time': 1823.294312953949, 'accumulated_logging_time': 3.4053735733032227}
I0307 10:21:48.799358 140237686679296 logging_writer.py:48] [56657] accumulated_eval_time=1823.29, accumulated_logging_time=3.40537, accumulated_submission_time=21986.1, global_step=56657, preemption_count=0, score=21986.1, test/accuracy=0.4997, test/loss=2.29855, test/num_examples=10000, total_duration=23818.6, train/accuracy=0.673649, train/loss=1.39202, validation/accuracy=0.62358, validation/loss=1.62202, validation/num_examples=50000
I0307 10:22:06.044404 140237678286592 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.9137632846832275, loss=2.7430872917175293
I0307 10:22:45.830537 140237686679296 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.406658411026001, loss=2.7339255809783936
I0307 10:23:25.710795 140237678286592 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.78229022026062, loss=2.739898204803467
I0307 10:24:05.779012 140237686679296 logging_writer.py:48] [57000] global_step=57000, grad_norm=2.970109701156616, loss=2.7111384868621826
I0307 10:24:45.441940 140237678286592 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.2463531494140625, loss=2.7055108547210693
I0307 10:25:25.468594 140237686679296 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.3356168270111084, loss=2.6761374473571777
I0307 10:26:05.606443 140237678286592 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.0588877201080322, loss=2.6931474208831787
I0307 10:26:45.562263 140237686679296 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.8839194774627686, loss=2.735313653945923
I0307 10:27:25.355041 140237678286592 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.9198365211486816, loss=2.6628220081329346
I0307 10:28:05.303084 140237686679296 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.407355546951294, loss=2.7102630138397217
I0307 10:28:45.178510 140237678286592 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.448357343673706, loss=2.770346164703369
I0307 10:29:24.750118 140237686679296 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.2692487239837646, loss=2.6659858226776123
I0307 10:30:03.553970 140237678286592 logging_writer.py:48] [57900] global_step=57900, grad_norm=4.0759735107421875, loss=2.8067307472229004
I0307 10:30:19.023521 140393707492544 spec.py:321] Evaluating on the training split.
I0307 10:30:30.968953 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 10:30:59.763572 140393707492544 spec.py:349] Evaluating on the test split.
I0307 10:31:01.543247 140393707492544 submission_runner.py:469] Time since start: 24371.42s, 	Step: 57940, 	{'train/accuracy': 0.6677694320678711, 'train/loss': 1.4304031133651733, 'validation/accuracy': 0.6202600002288818, 'validation/loss': 1.6514286994934082, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.327284336090088, 'test/num_examples': 10000, 'score': 22496.170170545578, 'total_duration': 24371.42397737503, 'accumulated_submission_time': 22496.170170545578, 'accumulated_eval_time': 1865.8140029907227, 'accumulated_logging_time': 3.5080671310424805}
I0307 10:31:01.632832 140237686679296 logging_writer.py:48] [57940] accumulated_eval_time=1865.81, accumulated_logging_time=3.50807, accumulated_submission_time=22496.2, global_step=57940, preemption_count=0, score=22496.2, test/accuracy=0.4939, test/loss=2.32728, test/num_examples=10000, total_duration=24371.4, train/accuracy=0.667769, train/loss=1.4304, validation/accuracy=0.62026, validation/loss=1.65143, validation/num_examples=50000
I0307 10:31:25.697412 140237678286592 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.9819202423095703, loss=2.7863380908966064
I0307 10:32:05.901033 140237686679296 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.0598530769348145, loss=2.7246010303497314
I0307 10:32:46.077894 140237678286592 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.040193796157837, loss=2.664095163345337
I0307 10:33:26.272017 140237686679296 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.1366894245147705, loss=2.7593345642089844
I0307 10:34:05.392935 140237678286592 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.019712209701538, loss=2.6961183547973633
I0307 10:34:45.442637 140237686679296 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.593397855758667, loss=2.6001415252685547
I0307 10:35:25.335218 140237678286592 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.203150510787964, loss=2.752858877182007
I0307 10:36:05.244377 140237686679296 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.960789680480957, loss=2.680412769317627
I0307 10:36:45.208127 140237678286592 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.8804407119750977, loss=2.696864604949951
I0307 10:37:24.102129 140237686679296 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.5819387435913086, loss=2.7786495685577393
I0307 10:38:03.472974 140237678286592 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.114018440246582, loss=2.7311675548553467
I0307 10:38:43.543633 140237686679296 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.4098379611968994, loss=2.7245073318481445
I0307 10:39:23.115814 140237678286592 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.230116605758667, loss=2.714688301086426
I0307 10:39:31.575089 140393707492544 spec.py:321] Evaluating on the training split.
I0307 10:39:43.166833 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 10:40:08.945982 140393707492544 spec.py:349] Evaluating on the test split.
I0307 10:40:10.731378 140393707492544 submission_runner.py:469] Time since start: 24920.61s, 	Step: 59222, 	{'train/accuracy': 0.6744459271430969, 'train/loss': 1.4181233644485474, 'validation/accuracy': 0.6239399909973145, 'validation/loss': 1.6500818729400635, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2889773845672607, 'test/num_examples': 10000, 'score': 23005.943830490112, 'total_duration': 24920.612103939056, 'accumulated_submission_time': 23005.943830490112, 'accumulated_eval_time': 1904.9702589511871, 'accumulated_logging_time': 3.614800453186035}
I0307 10:40:10.790953 140237686679296 logging_writer.py:48] [59222] accumulated_eval_time=1904.97, accumulated_logging_time=3.6148, accumulated_submission_time=23005.9, global_step=59222, preemption_count=0, score=23005.9, test/accuracy=0.5065, test/loss=2.28898, test/num_examples=10000, total_duration=24920.6, train/accuracy=0.674446, train/loss=1.41812, validation/accuracy=0.62394, validation/loss=1.65008, validation/num_examples=50000
I0307 10:40:42.318166 140237678286592 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.0120460987091064, loss=2.7230722904205322
I0307 10:41:22.620081 140237686679296 logging_writer.py:48] [59400] global_step=59400, grad_norm=2.9230167865753174, loss=2.6407392024993896
I0307 10:42:02.967212 140237678286592 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.78651762008667, loss=2.770772695541382
I0307 10:42:42.584012 140237686679296 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.3706414699554443, loss=2.6228861808776855
I0307 10:43:22.120018 140237678286592 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.982795476913452, loss=2.6802918910980225
I0307 10:44:02.268531 140237686679296 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.8602583408355713, loss=2.730490207672119
I0307 10:44:42.091679 140237678286592 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.9779703617095947, loss=2.7043237686157227
I0307 10:45:21.537348 140237686679296 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.240648031234741, loss=2.6338250637054443
I0307 10:46:00.999568 140237678286592 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.6541526317596436, loss=2.6644272804260254
I0307 10:46:40.849673 140237686679296 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.4301517009735107, loss=2.6520283222198486
I0307 10:47:20.815270 140237678286592 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.3234527111053467, loss=2.7752668857574463
I0307 10:48:00.829807 140237686679296 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.0123841762542725, loss=2.6491692066192627
I0307 10:48:41.012389 140237678286592 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.8061957359313965, loss=2.6323986053466797
I0307 10:48:41.024048 140393707492544 spec.py:321] Evaluating on the training split.
I0307 10:48:53.300160 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 10:49:19.574559 140393707492544 spec.py:349] Evaluating on the test split.
I0307 10:49:21.350950 140393707492544 submission_runner.py:469] Time since start: 25471.23s, 	Step: 60501, 	{'train/accuracy': 0.6765584945678711, 'train/loss': 1.3821467161178589, 'validation/accuracy': 0.6243000030517578, 'validation/loss': 1.6189907789230347, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2862048149108887, 'test/num_examples': 10000, 'score': 23516.01775741577, 'total_duration': 25471.231680631638, 'accumulated_submission_time': 23516.01775741577, 'accumulated_eval_time': 1945.2971069812775, 'accumulated_logging_time': 3.683279514312744}
I0307 10:49:21.421003 140237686679296 logging_writer.py:48] [60501] accumulated_eval_time=1945.3, accumulated_logging_time=3.68328, accumulated_submission_time=23516, global_step=60501, preemption_count=0, score=23516, test/accuracy=0.5001, test/loss=2.2862, test/num_examples=10000, total_duration=25471.2, train/accuracy=0.676558, train/loss=1.38215, validation/accuracy=0.6243, validation/loss=1.61899, validation/num_examples=50000
I0307 10:50:01.150651 140237678286592 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.120945692062378, loss=2.8010549545288086
I0307 10:50:41.163704 140237686679296 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.441340923309326, loss=2.7052340507507324
I0307 10:51:21.558971 140237678286592 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.3947744369506836, loss=2.718406915664673
I0307 10:52:01.818093 140237686679296 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.2247817516326904, loss=2.690589666366577
I0307 10:52:41.941865 140237678286592 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.3912065029144287, loss=2.708315134048462
I0307 10:53:21.929419 140237686679296 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.103829860687256, loss=2.666313648223877
I0307 10:54:04.559228 140237678286592 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.1821906566619873, loss=2.6519341468811035
I0307 10:54:47.964643 140237686679296 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.5180954933166504, loss=2.7330198287963867
I0307 10:55:27.485336 140237678286592 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.0188965797424316, loss=2.672316312789917
I0307 10:56:07.221692 140237686679296 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.1482973098754883, loss=2.5676586627960205
I0307 10:56:46.549215 140237678286592 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.9046802520751953, loss=2.666069746017456
I0307 10:57:26.657740 140237686679296 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.3827366828918457, loss=2.6989073753356934
I0307 10:57:51.542769 140393707492544 spec.py:321] Evaluating on the training split.
I0307 10:58:03.427545 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 10:58:31.317737 140393707492544 spec.py:349] Evaluating on the test split.
I0307 10:58:33.099067 140393707492544 submission_runner.py:469] Time since start: 26022.98s, 	Step: 61762, 	{'train/accuracy': 0.6795878410339355, 'train/loss': 1.3922882080078125, 'validation/accuracy': 0.6300599575042725, 'validation/loss': 1.6184196472167969, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.2597270011901855, 'test/num_examples': 10000, 'score': 24025.98553752899, 'total_duration': 26022.97979950905, 'accumulated_submission_time': 24025.98553752899, 'accumulated_eval_time': 1986.8533709049225, 'accumulated_logging_time': 3.761902093887329}
I0307 10:58:33.221287 140237678286592 logging_writer.py:48] [61762] accumulated_eval_time=1986.85, accumulated_logging_time=3.7619, accumulated_submission_time=24026, global_step=61762, preemption_count=0, score=24026, test/accuracy=0.5091, test/loss=2.25973, test/num_examples=10000, total_duration=26023, train/accuracy=0.679588, train/loss=1.39229, validation/accuracy=0.63006, validation/loss=1.61842, validation/num_examples=50000
I0307 10:58:48.873861 140237686679296 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.627333402633667, loss=2.7391064167022705
I0307 10:59:28.837064 140237678286592 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.889348030090332, loss=2.69975209236145
I0307 11:00:09.182572 140237686679296 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.9706151485443115, loss=2.672901153564453
I0307 11:00:49.315760 140237678286592 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.0082883834838867, loss=2.6485366821289062
I0307 11:01:28.953931 140237686679296 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.5592422485351562, loss=2.6745777130126953
I0307 11:02:08.788553 140237678286592 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.783082962036133, loss=2.7146434783935547
I0307 11:02:56.889639 140237686679296 logging_writer.py:48] [62400] global_step=62400, grad_norm=4.0758376121521, loss=2.6068625450134277
I0307 11:03:41.720665 140237678286592 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.1665337085723877, loss=2.737516403198242
I0307 11:04:22.022855 140237686679296 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.632601022720337, loss=2.756424903869629
I0307 11:05:01.746100 140237678286592 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.878603935241699, loss=2.687757730484009
I0307 11:05:41.344413 140237686679296 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.2649686336517334, loss=2.7348597049713135
I0307 11:06:20.892088 140237678286592 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.919037342071533, loss=2.593010425567627
I0307 11:07:00.975911 140237686679296 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.2447586059570312, loss=2.6606087684631348
I0307 11:07:03.471924 140393707492544 spec.py:321] Evaluating on the training split.
I0307 11:07:14.765975 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 11:07:41.615745 140393707492544 spec.py:349] Evaluating on the test split.
I0307 11:07:43.432012 140393707492544 submission_runner.py:469] Time since start: 26573.31s, 	Step: 63007, 	{'train/accuracy': 0.6781927347183228, 'train/loss': 1.4180994033813477, 'validation/accuracy': 0.6233999729156494, 'validation/loss': 1.6588274240493774, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.3485636711120605, 'test/num_examples': 10000, 'score': 24536.08448243141, 'total_duration': 26573.312735795975, 'accumulated_submission_time': 24536.08448243141, 'accumulated_eval_time': 2026.8134157657623, 'accumulated_logging_time': 3.89278244972229}
I0307 11:07:43.502140 140237678286592 logging_writer.py:48] [63007] accumulated_eval_time=2026.81, accumulated_logging_time=3.89278, accumulated_submission_time=24536.1, global_step=63007, preemption_count=0, score=24536.1, test/accuracy=0.4973, test/loss=2.34856, test/num_examples=10000, total_duration=26573.3, train/accuracy=0.678193, train/loss=1.4181, validation/accuracy=0.6234, validation/loss=1.65883, validation/num_examples=50000
I0307 11:08:21.057417 140237686679296 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.978815793991089, loss=2.7160677909851074
I0307 11:09:01.097728 140237678286592 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.142704963684082, loss=2.656456470489502
I0307 11:09:41.455253 140237686679296 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.9200680255889893, loss=2.698270797729492
I0307 11:10:21.707450 140237678286592 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.8414249420166016, loss=2.7637104988098145
I0307 11:11:01.297326 140237686679296 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.01106333732605, loss=2.773038625717163
I0307 11:11:40.934428 140237678286592 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.1154258251190186, loss=2.6640286445617676
I0307 11:12:21.640077 140237686679296 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.002873182296753, loss=2.653385639190674
I0307 11:13:01.382404 140237678286592 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.7477242946624756, loss=2.700547695159912
I0307 11:13:40.960184 140237686679296 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.2090039253234863, loss=2.714486837387085
I0307 11:14:20.927362 140237678286592 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.170659065246582, loss=2.624659299850464
I0307 11:15:01.121669 140237686679296 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.3240060806274414, loss=2.7426486015319824
I0307 11:15:41.475630 140237678286592 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.7883424758911133, loss=2.712486982345581
I0307 11:16:13.627361 140393707492544 spec.py:321] Evaluating on the training split.
I0307 11:16:25.254299 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 11:16:53.314571 140393707492544 spec.py:349] Evaluating on the test split.
I0307 11:16:55.091996 140393707492544 submission_runner.py:469] Time since start: 27124.97s, 	Step: 64281, 	{'train/accuracy': 0.6554527878761292, 'train/loss': 1.5121690034866333, 'validation/accuracy': 0.6066200137138367, 'validation/loss': 1.750923991203308, 'validation/num_examples': 50000, 'test/accuracy': 0.48510003089904785, 'test/loss': 2.431643009185791, 'test/num_examples': 10000, 'score': 25046.053644657135, 'total_duration': 27124.972723007202, 'accumulated_submission_time': 25046.053644657135, 'accumulated_eval_time': 2068.2780256271362, 'accumulated_logging_time': 3.9706714153289795}
I0307 11:16:55.150936 140237686679296 logging_writer.py:48] [64281] accumulated_eval_time=2068.28, accumulated_logging_time=3.97067, accumulated_submission_time=25046.1, global_step=64281, preemption_count=0, score=25046.1, test/accuracy=0.4851, test/loss=2.43164, test/num_examples=10000, total_duration=27125, train/accuracy=0.655453, train/loss=1.51217, validation/accuracy=0.60662, validation/loss=1.75092, validation/num_examples=50000
I0307 11:17:03.129537 140237678286592 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.41511607170105, loss=2.677155017852783
I0307 11:17:43.062409 140237686679296 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.9379053115844727, loss=2.626267194747925
I0307 11:18:23.064701 140237678286592 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.159968376159668, loss=2.6784727573394775
I0307 11:19:03.781153 140237686679296 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.4866058826446533, loss=2.684774398803711
I0307 11:19:43.595644 140237678286592 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.6952104568481445, loss=2.614583969116211
I0307 11:20:23.399665 140237686679296 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.194751262664795, loss=2.7378745079040527
I0307 11:21:03.811168 140237678286592 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.460332155227661, loss=2.6736176013946533
I0307 11:21:43.799757 140237686679296 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.4881162643432617, loss=2.7300305366516113
2025-03-07 11:21:57.823076: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:22:23.734920 140237678286592 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.6294939517974854, loss=2.760166645050049
I0307 11:23:03.729260 140237686679296 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.644198417663574, loss=2.7073090076446533
I0307 11:23:43.742214 140237678286592 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.2670557498931885, loss=2.675475597381592
I0307 11:24:23.694438 140237686679296 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.057737350463867, loss=2.750157356262207
I0307 11:25:03.586659 140237678286592 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.347440004348755, loss=2.7488272190093994
I0307 11:25:25.309747 140393707492544 spec.py:321] Evaluating on the training split.
I0307 11:25:37.065650 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 11:26:04.693918 140393707492544 spec.py:349] Evaluating on the test split.
I0307 11:26:06.475781 140393707492544 submission_runner.py:469] Time since start: 27676.36s, 	Step: 65556, 	{'train/accuracy': 0.685945451259613, 'train/loss': 1.3673322200775146, 'validation/accuracy': 0.6327599883079529, 'validation/loss': 1.608644962310791, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.258028507232666, 'test/num_examples': 10000, 'score': 25556.05849671364, 'total_duration': 27676.356492996216, 'accumulated_submission_time': 25556.05849671364, 'accumulated_eval_time': 2109.4440019130707, 'accumulated_logging_time': 4.037358522415161}
I0307 11:26:06.546302 140237686679296 logging_writer.py:48] [65556] accumulated_eval_time=2109.44, accumulated_logging_time=4.03736, accumulated_submission_time=25556.1, global_step=65556, preemption_count=0, score=25556.1, test/accuracy=0.5125, test/loss=2.25803, test/num_examples=10000, total_duration=27676.4, train/accuracy=0.685945, train/loss=1.36733, validation/accuracy=0.63276, validation/loss=1.60864, validation/num_examples=50000
I0307 11:26:24.461013 140237678286592 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.8548007011413574, loss=2.743187427520752
I0307 11:27:03.884782 140237686679296 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.46352219581604, loss=2.621446132659912
I0307 11:27:44.138226 140237678286592 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.097781181335449, loss=2.6043450832366943
I0307 11:28:24.240844 140237686679296 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.136000156402588, loss=2.6180388927459717
I0307 11:29:03.748060 140237678286592 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.9087157249450684, loss=2.6871700286865234
I0307 11:29:43.332080 140237686679296 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.3752570152282715, loss=2.7632012367248535
I0307 11:30:23.647447 140237678286592 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.2221262454986572, loss=2.6944656372070312
I0307 11:31:04.050014 140237686679296 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.978034019470215, loss=2.6831400394439697
I0307 11:31:43.051446 140237678286592 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.1888587474823, loss=2.780527114868164
I0307 11:32:22.477322 140237686679296 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.988081216812134, loss=2.7467939853668213
I0307 11:33:02.940640 140237678286592 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.3943538665771484, loss=2.708045482635498
I0307 11:33:43.013153 140237686679296 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.0542640686035156, loss=2.7344260215759277
I0307 11:34:22.744761 140237678286592 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.0798354148864746, loss=2.714122772216797
I0307 11:34:36.616534 140393707492544 spec.py:321] Evaluating on the training split.
I0307 11:34:48.366764 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 11:35:17.405539 140393707492544 spec.py:349] Evaluating on the test split.
I0307 11:35:19.227675 140393707492544 submission_runner.py:469] Time since start: 28229.11s, 	Step: 66836, 	{'train/accuracy': 0.6857461333274841, 'train/loss': 1.3545249700546265, 'validation/accuracy': 0.6339199542999268, 'validation/loss': 1.6100269556045532, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.2672336101531982, 'test/num_examples': 10000, 'score': 26065.972586393356, 'total_duration': 28229.108406305313, 'accumulated_submission_time': 26065.972586393356, 'accumulated_eval_time': 2152.055104494095, 'accumulated_logging_time': 4.1164069175720215}
I0307 11:35:19.320432 140237686679296 logging_writer.py:48] [66836] accumulated_eval_time=2152.06, accumulated_logging_time=4.11641, accumulated_submission_time=26066, global_step=66836, preemption_count=0, score=26066, test/accuracy=0.5136, test/loss=2.26723, test/num_examples=10000, total_duration=28229.1, train/accuracy=0.685746, train/loss=1.35452, validation/accuracy=0.63392, validation/loss=1.61003, validation/num_examples=50000
I0307 11:35:45.044350 140237678286592 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.324007749557495, loss=2.59238862991333
I0307 11:36:24.860888 140237686679296 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.259446859359741, loss=2.651940107345581
I0307 11:37:05.003690 140237678286592 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.379100799560547, loss=2.6864094734191895
I0307 11:37:44.881816 140237686679296 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.814964771270752, loss=2.5792551040649414
I0307 11:38:24.768122 140237678286592 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.28509783744812, loss=2.78566837310791
I0307 11:39:04.909854 140237686679296 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.6736629009246826, loss=2.715972661972046
I0307 11:39:45.171398 140237678286592 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.893847942352295, loss=2.634971857070923
I0307 11:40:24.788896 140237686679296 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.1312482357025146, loss=2.6322999000549316
I0307 11:41:04.809451 140237678286592 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.1921489238739014, loss=2.5879180431365967
I0307 11:41:44.565254 140237686679296 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.5248525142669678, loss=2.695930242538452
I0307 11:42:24.931478 140237678286592 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.214437961578369, loss=2.6942200660705566
I0307 11:43:05.365777 140237686679296 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.109764337539673, loss=2.7595014572143555
I0307 11:43:45.800323 140237678286592 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.246884822845459, loss=2.6747307777404785
I0307 11:43:49.485508 140393707492544 spec.py:321] Evaluating on the training split.
I0307 11:44:01.310138 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 11:44:28.332171 140393707492544 spec.py:349] Evaluating on the test split.
I0307 11:44:30.128389 140393707492544 submission_runner.py:469] Time since start: 28780.01s, 	Step: 68110, 	{'train/accuracy': 0.6877192258834839, 'train/loss': 1.354539394378662, 'validation/accuracy': 0.6323800086975098, 'validation/loss': 1.5963611602783203, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.278628349304199, 'test/num_examples': 10000, 'score': 26575.983340024948, 'total_duration': 28780.009119272232, 'accumulated_submission_time': 26575.983340024948, 'accumulated_eval_time': 2192.697955608368, 'accumulated_logging_time': 4.217088460922241}
I0307 11:44:30.197403 140237686679296 logging_writer.py:48] [68110] accumulated_eval_time=2192.7, accumulated_logging_time=4.21709, accumulated_submission_time=26576, global_step=68110, preemption_count=0, score=26576, test/accuracy=0.5037, test/loss=2.27863, test/num_examples=10000, total_duration=28780, train/accuracy=0.687719, train/loss=1.35454, validation/accuracy=0.63238, validation/loss=1.59636, validation/num_examples=50000
I0307 11:45:05.987403 140237678286592 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.0526082515716553, loss=2.705885171890259
I0307 11:45:46.154443 140237686679296 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.5388002395629883, loss=2.669562339782715
I0307 11:46:26.665286 140237678286592 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.1767544746398926, loss=2.7435734272003174
I0307 11:47:06.465072 140237686679296 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.3594422340393066, loss=2.6245832443237305
I0307 11:47:46.610110 140237678286592 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.1644835472106934, loss=2.734510660171509
I0307 11:48:26.446745 140237686679296 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.5437803268432617, loss=2.703310966491699
I0307 11:49:06.758284 140237678286592 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.8725993633270264, loss=2.576709508895874
I0307 11:49:46.095362 140237686679296 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.231293201446533, loss=2.66668963432312
I0307 11:50:25.592448 140237678286592 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.0569159984588623, loss=2.5936059951782227
I0307 11:51:05.277546 140237686679296 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.3899588584899902, loss=2.657743453979492
I0307 11:51:45.456976 140237678286592 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.2506892681121826, loss=2.741588592529297
I0307 11:52:25.356355 140237686679296 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.5376546382904053, loss=2.7111854553222656
I0307 11:53:00.361063 140393707492544 spec.py:321] Evaluating on the training split.
I0307 11:53:12.280377 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 11:53:38.154718 140393707492544 spec.py:349] Evaluating on the test split.
I0307 11:53:39.967996 140393707492544 submission_runner.py:469] Time since start: 29329.85s, 	Step: 69388, 	{'train/accuracy': 0.6885562539100647, 'train/loss': 1.3444920778274536, 'validation/accuracy': 0.6308799982070923, 'validation/loss': 1.5962424278259277, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.273098945617676, 'test/num_examples': 10000, 'score': 27085.99235892296, 'total_duration': 29329.848727226257, 'accumulated_submission_time': 27085.99235892296, 'accumulated_eval_time': 2232.304853439331, 'accumulated_logging_time': 4.294145822525024}
I0307 11:53:40.063536 140237678286592 logging_writer.py:48] [69388] accumulated_eval_time=2232.3, accumulated_logging_time=4.29415, accumulated_submission_time=27086, global_step=69388, preemption_count=0, score=27086, test/accuracy=0.508, test/loss=2.2731, test/num_examples=10000, total_duration=29329.8, train/accuracy=0.688556, train/loss=1.34449, validation/accuracy=0.63088, validation/loss=1.59624, validation/num_examples=50000
I0307 11:53:45.274625 140237686679296 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.1246256828308105, loss=2.7246415615081787
I0307 11:54:25.719281 140237678286592 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.0615217685699463, loss=2.6352384090423584
I0307 11:55:05.511114 140237686679296 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.247981309890747, loss=2.6099636554718018
I0307 11:55:45.020077 140237678286592 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.14107084274292, loss=2.6647446155548096
I0307 11:56:25.008261 140237686679296 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.747689962387085, loss=2.592787742614746
I0307 11:57:05.180784 140237678286592 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.2887356281280518, loss=2.53007173538208
I0307 11:57:44.845084 140237686679296 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.2009003162384033, loss=2.6999993324279785
2025-03-07 11:58:03.105892: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:58:24.723592 140237678286592 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.321489095687866, loss=2.6388115882873535
I0307 11:59:05.312373 140237686679296 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.1284282207489014, loss=2.5862629413604736
I0307 11:59:45.230017 140237678286592 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.4098145961761475, loss=2.6769869327545166
I0307 12:00:24.809198 140237686679296 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.3874752521514893, loss=2.597219467163086
I0307 12:01:04.889650 140237678286592 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.5381762981414795, loss=2.6156153678894043
I0307 12:01:45.360830 140237686679296 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.3600974082946777, loss=2.723806381225586
I0307 12:02:10.168280 140393707492544 spec.py:321] Evaluating on the training split.
I0307 12:02:21.880721 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 12:02:47.912061 140393707492544 spec.py:349] Evaluating on the test split.
I0307 12:02:49.741710 140393707492544 submission_runner.py:469] Time since start: 29879.62s, 	Step: 70664, 	{'train/accuracy': 0.6712372303009033, 'train/loss': 1.4403884410858154, 'validation/accuracy': 0.6182999610900879, 'validation/loss': 1.6814872026443481, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.324831008911133, 'test/num_examples': 10000, 'score': 27595.943027973175, 'total_duration': 29879.622443437576, 'accumulated_submission_time': 27595.943027973175, 'accumulated_eval_time': 2271.8782544136047, 'accumulated_logging_time': 4.397392988204956}
I0307 12:02:49.812015 140237678286592 logging_writer.py:48] [70664] accumulated_eval_time=2271.88, accumulated_logging_time=4.39739, accumulated_submission_time=27595.9, global_step=70664, preemption_count=0, score=27595.9, test/accuracy=0.4973, test/loss=2.32483, test/num_examples=10000, total_duration=29879.6, train/accuracy=0.671237, train/loss=1.44039, validation/accuracy=0.6183, validation/loss=1.68149, validation/num_examples=50000
I0307 12:03:04.809682 140237686679296 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.330787420272827, loss=2.634448528289795
I0307 12:03:43.685451 140237678286592 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.286151647567749, loss=2.7629616260528564
I0307 12:04:23.766783 140237686679296 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.1745493412017822, loss=2.541032075881958
I0307 12:05:05.322074 140237678286592 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.7399656772613525, loss=2.6468749046325684
I0307 12:05:47.871017 140237686679296 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.260720729827881, loss=2.583890199661255
I0307 12:06:29.660110 140237678286592 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.347001552581787, loss=2.567539691925049
2025-03-07 12:07:09.443142: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:07:10.998849 140237686679296 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.2352983951568604, loss=2.6616368293762207
I0307 12:07:50.864474 140237678286592 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.6365625858306885, loss=2.6475422382354736
I0307 12:08:31.046271 140237686679296 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.2707221508026123, loss=2.6836371421813965
I0307 12:09:10.681458 140237678286592 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.239194869995117, loss=2.6485981941223145
I0307 12:09:50.315211 140237686679296 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.4068007469177246, loss=2.6030306816101074
I0307 12:10:30.420557 140237678286592 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.9722681045532227, loss=2.618713617324829
I0307 12:11:10.674926 140237686679296 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.2730941772460938, loss=2.5312893390655518
I0307 12:11:19.816974 140393707492544 spec.py:321] Evaluating on the training split.
I0307 12:11:31.344079 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 12:11:58.125050 140393707492544 spec.py:349] Evaluating on the test split.
I0307 12:11:59.904104 140393707492544 submission_runner.py:469] Time since start: 30429.78s, 	Step: 71924, 	{'train/accuracy': 0.6946348547935486, 'train/loss': 1.3102076053619385, 'validation/accuracy': 0.6383199691772461, 'validation/loss': 1.5735831260681152, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.2401814460754395, 'test/num_examples': 10000, 'score': 28105.794216394424, 'total_duration': 30429.78483438492, 'accumulated_submission_time': 28105.794216394424, 'accumulated_eval_time': 2311.965348482132, 'accumulated_logging_time': 4.47649884223938}
I0307 12:11:59.965672 140237678286592 logging_writer.py:48] [71924] accumulated_eval_time=2311.97, accumulated_logging_time=4.4765, accumulated_submission_time=28105.8, global_step=71924, preemption_count=0, score=28105.8, test/accuracy=0.5158, test/loss=2.24018, test/num_examples=10000, total_duration=30429.8, train/accuracy=0.694635, train/loss=1.31021, validation/accuracy=0.63832, validation/loss=1.57358, validation/num_examples=50000
I0307 12:12:30.890995 140237686679296 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.448909282684326, loss=2.632791042327881
I0307 12:13:13.016908 140237678286592 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.4057445526123047, loss=2.687593460083008
I0307 12:13:54.573433 140237686679296 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.779942750930786, loss=2.59017014503479
I0307 12:14:52.249484 140237678286592 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.6601979732513428, loss=2.669064998626709
I0307 12:16:03.550584 140237686679296 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.1076455116271973, loss=2.5969059467315674
I0307 12:16:47.544873 140237678286592 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.0553152561187744, loss=2.6659483909606934
I0307 12:17:27.284003 140237686679296 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.090543508529663, loss=2.533559560775757
I0307 12:18:06.826915 140237678286592 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.394212245941162, loss=2.711064338684082
I0307 12:18:46.675626 140237686679296 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.6716251373291016, loss=2.761279821395874
I0307 12:19:26.232373 140237678286592 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.8609251976013184, loss=2.602745294570923
I0307 12:20:06.704472 140237686679296 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.1984784603118896, loss=2.680345296859741
I0307 12:20:29.913648 140393707492544 spec.py:321] Evaluating on the training split.
I0307 12:20:42.376921 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 12:21:08.180410 140393707492544 spec.py:349] Evaluating on the test split.
I0307 12:21:10.005458 140393707492544 submission_runner.py:469] Time since start: 30979.89s, 	Step: 73060, 	{'train/accuracy': 0.6994379758834839, 'train/loss': 1.2880041599273682, 'validation/accuracy': 0.6383199691772461, 'validation/loss': 1.5783789157867432, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.258270025253296, 'test/num_examples': 10000, 'score': 28615.571096897125, 'total_duration': 30979.886177539825, 'accumulated_submission_time': 28615.571096897125, 'accumulated_eval_time': 2352.057108402252, 'accumulated_logging_time': 4.577669620513916}
I0307 12:21:10.071070 140237678286592 logging_writer.py:48] [73060] accumulated_eval_time=2352.06, accumulated_logging_time=4.57767, accumulated_submission_time=28615.6, global_step=73060, preemption_count=0, score=28615.6, test/accuracy=0.5058, test/loss=2.25827, test/num_examples=10000, total_duration=30979.9, train/accuracy=0.699438, train/loss=1.288, validation/accuracy=0.63832, validation/loss=1.57838, validation/num_examples=50000
I0307 12:21:26.201792 140237686679296 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.1020331382751465, loss=2.6425585746765137
I0307 12:22:05.823859 140237678286592 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.475385904312134, loss=2.689561605453491
I0307 12:22:45.982260 140237686679296 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.3895411491394043, loss=2.695833683013916
I0307 12:23:30.341155 140237678286592 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.0095787048339844, loss=2.6055984497070312
I0307 12:24:12.339375 140237686679296 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.79998779296875, loss=2.6189138889312744
I0307 12:24:56.257398 140237678286592 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.154569625854492, loss=2.617518424987793
I0307 12:25:39.942902 140237686679296 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.487626552581787, loss=2.6806650161743164
I0307 12:26:19.168221 140237678286592 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.1764795780181885, loss=2.6778769493103027
I0307 12:27:44.214738 140237686679296 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.0578091144561768, loss=2.743549346923828
I0307 12:28:28.491981 140237678286592 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.291977643966675, loss=2.5722057819366455
I0307 12:29:14.790097 140237686679296 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.383728504180908, loss=2.7058262825012207
I0307 12:29:40.329869 140393707492544 spec.py:321] Evaluating on the training split.
I0307 12:29:52.950759 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 12:30:15.965287 140393707492544 spec.py:349] Evaluating on the test split.
I0307 12:30:17.747972 140393707492544 submission_runner.py:469] Time since start: 31527.63s, 	Step: 74156, 	{'train/accuracy': 0.7185506820678711, 'train/loss': 1.2132364511489868, 'validation/accuracy': 0.6328799724578857, 'validation/loss': 1.5887466669082642, 'validation/num_examples': 50000, 'test/accuracy': 0.5145000219345093, 'test/loss': 2.2378978729248047, 'test/num_examples': 10000, 'score': 29125.696088552475, 'total_duration': 31527.628700971603, 'accumulated_submission_time': 29125.696088552475, 'accumulated_eval_time': 2389.4751737117767, 'accumulated_logging_time': 4.651588439941406}
I0307 12:30:17.819239 140237678286592 logging_writer.py:48] [74156] accumulated_eval_time=2389.48, accumulated_logging_time=4.65159, accumulated_submission_time=29125.7, global_step=74156, preemption_count=0, score=29125.7, test/accuracy=0.5145, test/loss=2.2379, test/num_examples=10000, total_duration=31527.6, train/accuracy=0.718551, train/loss=1.21324, validation/accuracy=0.63288, validation/loss=1.58875, validation/num_examples=50000
I0307 12:30:35.789399 140237686679296 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.039461135864258, loss=2.5524120330810547
I0307 12:31:16.323420 140237678286592 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.2188875675201416, loss=2.6409058570861816
I0307 12:31:56.822178 140237686679296 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.559812307357788, loss=2.7344794273376465
I0307 12:32:38.305020 140237678286592 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.438253402709961, loss=2.5846970081329346
I0307 12:33:18.584058 140237686679296 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.111253499984741, loss=2.655459403991699
I0307 12:34:02.109041 140237678286592 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.2469139099121094, loss=2.641700267791748
I0307 12:34:49.890757 140237686679296 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.2434539794921875, loss=2.630211591720581
I0307 12:35:45.915307 140237678286592 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.2941336631774902, loss=2.6054775714874268
I0307 12:36:28.394210 140237686679296 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.712799072265625, loss=2.669447898864746
I0307 12:37:09.355359 140237678286592 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.8837881088256836, loss=2.56168270111084
I0307 12:37:48.309854 140237686679296 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.4327805042266846, loss=2.717115640640259
I0307 12:38:29.019806 140237678286592 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.3215994834899902, loss=2.5803260803222656
I0307 12:38:48.084388 140393707492544 spec.py:321] Evaluating on the training split.
I0307 12:39:00.450490 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 12:39:24.413703 140393707492544 spec.py:349] Evaluating on the test split.
I0307 12:39:26.181740 140393707492544 submission_runner.py:469] Time since start: 32076.06s, 	Step: 75346, 	{'train/accuracy': 0.6737284660339355, 'train/loss': 1.406254768371582, 'validation/accuracy': 0.6254199743270874, 'validation/loss': 1.6341502666473389, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.269862174987793, 'test/num_examples': 10000, 'score': 29635.822702407837, 'total_duration': 32076.062469005585, 'accumulated_submission_time': 29635.822702407837, 'accumulated_eval_time': 2427.5725004673004, 'accumulated_logging_time': 4.73053240776062}
I0307 12:39:26.266298 140237686679296 logging_writer.py:48] [75346] accumulated_eval_time=2427.57, accumulated_logging_time=4.73053, accumulated_submission_time=29635.8, global_step=75346, preemption_count=0, score=29635.8, test/accuracy=0.5035, test/loss=2.26986, test/num_examples=10000, total_duration=32076.1, train/accuracy=0.673728, train/loss=1.40625, validation/accuracy=0.62542, validation/loss=1.63415, validation/num_examples=50000
I0307 12:39:47.869929 140237678286592 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.6174983978271484, loss=2.6662659645080566
I0307 12:40:32.013998 140237686679296 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.289478302001953, loss=2.6897928714752197
I0307 12:41:16.138446 140237678286592 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.037452220916748, loss=2.6136927604675293
I0307 12:41:58.257271 140237686679296 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.6235949993133545, loss=2.7083687782287598
I0307 12:42:40.620189 140237678286592 logging_writer.py:48] [75800] global_step=75800, grad_norm=4.1717634201049805, loss=2.6269729137420654
I0307 12:43:21.966296 140237686679296 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.353790760040283, loss=2.6234350204467773
I0307 12:44:01.725841 140237678286592 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.495513677597046, loss=2.646815538406372
I0307 12:44:44.482023 140237686679296 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.0456814765930176, loss=2.636486053466797
I0307 12:45:31.483605 140237678286592 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.7860705852508545, loss=2.6242733001708984
I0307 12:46:12.295784 140237686679296 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.4776434898376465, loss=2.7358946800231934
2025-03-07 12:46:13.806367: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:46:54.360702 140237678286592 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.4483819007873535, loss=2.533797025680542
I0307 12:47:37.870527 140237686679296 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.2874257564544678, loss=2.677734613418579
I0307 12:47:56.498925 140393707492544 spec.py:321] Evaluating on the training split.
I0307 12:48:08.965408 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 12:48:34.098097 140393707492544 spec.py:349] Evaluating on the test split.
I0307 12:48:35.871173 140393707492544 submission_runner.py:469] Time since start: 32625.75s, 	Step: 76540, 	{'train/accuracy': 0.6850685477256775, 'train/loss': 1.3532170057296753, 'validation/accuracy': 0.6418799757957458, 'validation/loss': 1.5587762594223022, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.2022647857666016, 'test/num_examples': 10000, 'score': 30145.91132569313, 'total_duration': 32625.751860141754, 'accumulated_submission_time': 30145.91132569313, 'accumulated_eval_time': 2466.944679260254, 'accumulated_logging_time': 4.823166847229004}
I0307 12:48:35.967452 140237678286592 logging_writer.py:48] [76540] accumulated_eval_time=2466.94, accumulated_logging_time=4.82317, accumulated_submission_time=30145.9, global_step=76540, preemption_count=0, score=30145.9, test/accuracy=0.5191, test/loss=2.20226, test/num_examples=10000, total_duration=32625.8, train/accuracy=0.685069, train/loss=1.35322, validation/accuracy=0.64188, validation/loss=1.55878, validation/num_examples=50000
I0307 12:49:00.105919 140237686679296 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.896043539047241, loss=2.686417579650879
I0307 12:49:37.628046 140237678286592 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.604973554611206, loss=2.622645139694214
I0307 12:50:17.896593 140237686679296 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.624473810195923, loss=2.591756582260132
I0307 12:51:00.042541 140237678286592 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.3999221324920654, loss=2.6861565113067627
I0307 12:51:40.789475 140237686679296 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.09940767288208, loss=2.6037347316741943
I0307 12:52:22.185504 140237678286592 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.153593063354492, loss=2.5959863662719727
I0307 12:53:05.320558 140237686679296 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.863152503967285, loss=2.6659297943115234
I0307 12:53:46.786911 140237678286592 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.801603317260742, loss=2.6775259971618652
I0307 12:54:32.492014 140237686679296 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.2865333557128906, loss=2.6028811931610107
I0307 12:55:13.771946 140237678286592 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.526648759841919, loss=2.6801304817199707
I0307 12:55:53.463614 140237686679296 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.3235576152801514, loss=2.6385912895202637
I0307 12:56:33.508054 140237678286592 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.7254128456115723, loss=2.5827009677886963
I0307 12:57:05.906300 140393707492544 spec.py:321] Evaluating on the training split.
I0307 12:57:18.028037 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 12:57:41.915016 140393707492544 spec.py:349] Evaluating on the test split.
I0307 12:57:43.732156 140393707492544 submission_runner.py:469] Time since start: 33173.61s, 	Step: 77782, 	{'train/accuracy': 0.6861248016357422, 'train/loss': 1.3418529033660889, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.5507181882858276, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.2120394706726074, 'test/num_examples': 10000, 'score': 30655.70121073723, 'total_duration': 33173.612880945206, 'accumulated_submission_time': 30655.70121073723, 'accumulated_eval_time': 2504.7704935073853, 'accumulated_logging_time': 4.927473783493042}
I0307 12:57:43.797673 140237686679296 logging_writer.py:48] [77782] accumulated_eval_time=2504.77, accumulated_logging_time=4.92747, accumulated_submission_time=30655.7, global_step=77782, preemption_count=0, score=30655.7, test/accuracy=0.5211, test/loss=2.21204, test/num_examples=10000, total_duration=33173.6, train/accuracy=0.686125, train/loss=1.34185, validation/accuracy=0.64166, validation/loss=1.55072, validation/num_examples=50000
I0307 12:57:51.398646 140237678286592 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.4930708408355713, loss=2.6615679264068604
I0307 12:58:32.912065 140237686679296 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.3571481704711914, loss=2.594768524169922
I0307 12:59:19.829696 140237678286592 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.297853946685791, loss=2.5577030181884766
I0307 13:00:04.497000 140237686679296 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.1853630542755127, loss=2.635826826095581
I0307 13:00:49.238606 140237678286592 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.364440441131592, loss=2.5782206058502197
I0307 13:01:34.487662 140237686679296 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.608365774154663, loss=2.5557806491851807
I0307 13:02:17.589015 140237678286592 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.846858501434326, loss=2.619633913040161
I0307 13:02:56.768692 140237686679296 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.2595701217651367, loss=2.691314220428467
I0307 13:03:37.321907 140237678286592 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.864466667175293, loss=2.5998406410217285
I0307 13:04:21.849657 140237686679296 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.860621452331543, loss=2.653885841369629
I0307 13:05:01.979282 140237678286592 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.4683780670166016, loss=2.6581132411956787
I0307 13:05:43.154556 140237686679296 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.6846160888671875, loss=2.666477680206299
I0307 13:06:13.801477 140393707492544 spec.py:321] Evaluating on the training split.
I0307 13:06:26.164054 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 13:06:47.759537 140393707492544 spec.py:349] Evaluating on the test split.
I0307 13:06:49.539030 140393707492544 submission_runner.py:469] Time since start: 33719.42s, 	Step: 78972, 	{'train/accuracy': 0.693359375, 'train/loss': 1.3174878358840942, 'validation/accuracy': 0.6416199803352356, 'validation/loss': 1.5526129007339478, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.2029266357421875, 'test/num_examples': 10000, 'score': 31165.5601875782, 'total_duration': 33719.41975212097, 'accumulated_submission_time': 31165.5601875782, 'accumulated_eval_time': 2540.508004426956, 'accumulated_logging_time': 5.001620292663574}
I0307 13:06:49.664374 140237678286592 logging_writer.py:48] [78972] accumulated_eval_time=2540.51, accumulated_logging_time=5.00162, accumulated_submission_time=31165.6, global_step=78972, preemption_count=0, score=31165.6, test/accuracy=0.5182, test/loss=2.20293, test/num_examples=10000, total_duration=33719.4, train/accuracy=0.693359, train/loss=1.31749, validation/accuracy=0.64162, validation/loss=1.55261, validation/num_examples=50000
I0307 13:07:01.099483 140237686679296 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.593600034713745, loss=2.734255075454712
I0307 13:07:51.832925 140237678286592 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.304140090942383, loss=2.660712480545044
I0307 13:08:43.867471 140237686679296 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.178203821182251, loss=2.556758165359497
I0307 13:09:29.377523 140237678286592 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.37288761138916, loss=2.556328773498535
I0307 13:10:26.595710 140237686679296 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.647160053253174, loss=2.6724400520324707
I0307 13:11:16.343460 140237678286592 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.3206753730773926, loss=2.4577245712280273
I0307 13:12:04.548388 140237686679296 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.6427478790283203, loss=2.6951382160186768
I0307 13:12:50.387937 140237678286592 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.321648597717285, loss=2.5859503746032715
I0307 13:13:35.046452 140237686679296 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.942121982574463, loss=2.580021381378174
I0307 13:14:22.270573 140237678286592 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.6203372478485107, loss=2.645092010498047
I0307 13:15:10.665098 140237686679296 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.4307656288146973, loss=2.6214957237243652
I0307 13:15:19.691940 140393707492544 spec.py:321] Evaluating on the training split.
I0307 13:15:32.245268 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 13:15:55.175321 140393707492544 spec.py:349] Evaluating on the test split.
I0307 13:15:56.953924 140393707492544 submission_runner.py:469] Time since start: 34266.83s, 	Step: 80022, 	{'train/accuracy': 0.6858657598495483, 'train/loss': 1.3517900705337524, 'validation/accuracy': 0.6343599557876587, 'validation/loss': 1.585518717765808, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2630302906036377, 'test/num_examples': 10000, 'score': 31675.460896015167, 'total_duration': 34266.834636449814, 'accumulated_submission_time': 31675.460896015167, 'accumulated_eval_time': 2577.769932985306, 'accumulated_logging_time': 5.135424852371216}
I0307 13:15:57.008278 140237678286592 logging_writer.py:48] [80022] accumulated_eval_time=2577.77, accumulated_logging_time=5.13542, accumulated_submission_time=31675.5, global_step=80022, preemption_count=0, score=31675.5, test/accuracy=0.5094, test/loss=2.26303, test/num_examples=10000, total_duration=34266.8, train/accuracy=0.685866, train/loss=1.35179, validation/accuracy=0.63436, validation/loss=1.58552, validation/num_examples=50000
I0307 13:16:30.181697 140237686679296 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.3328728675842285, loss=2.6181812286376953
I0307 13:17:12.662926 140237678286592 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.4541587829589844, loss=2.4884114265441895
I0307 13:18:40.477959 140237686679296 logging_writer.py:48] [80300] global_step=80300, grad_norm=3.8087925910949707, loss=2.6276376247406006
I0307 13:19:22.446059 140237678286592 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.0146331787109375, loss=2.644536018371582
I0307 13:20:01.057407 140237686679296 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.500775098800659, loss=2.5885372161865234
I0307 13:21:19.344751 140237678286592 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.4323816299438477, loss=2.5065951347351074
I0307 13:22:04.811995 140237686679296 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.762402296066284, loss=2.7077317237854004
I0307 13:22:50.202709 140237678286592 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.9826745986938477, loss=2.5258257389068604
I0307 13:23:34.783416 140237686679296 logging_writer.py:48] [80900] global_step=80900, grad_norm=3.345316171646118, loss=2.5711004734039307
I0307 13:24:14.324208 140237678286592 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.722363233566284, loss=2.556555986404419
I0307 13:24:27.030642 140393707492544 spec.py:321] Evaluating on the training split.
I0307 13:24:39.435432 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 13:25:01.090989 140393707492544 spec.py:349] Evaluating on the test split.
I0307 13:25:02.894261 140393707492544 submission_runner.py:469] Time since start: 34812.77s, 	Step: 81032, 	{'train/accuracy': 0.7059351205825806, 'train/loss': 1.266436219215393, 'validation/accuracy': 0.6407399773597717, 'validation/loss': 1.5582534074783325, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.1871933937072754, 'test/num_examples': 10000, 'score': 32185.357806682587, 'total_duration': 34812.774980545044, 'accumulated_submission_time': 32185.357806682587, 'accumulated_eval_time': 2613.6335051059723, 'accumulated_logging_time': 5.198646306991577}
I0307 13:25:02.947454 140237686679296 logging_writer.py:48] [81032] accumulated_eval_time=2613.63, accumulated_logging_time=5.19865, accumulated_submission_time=32185.4, global_step=81032, preemption_count=0, score=32185.4, test/accuracy=0.5234, test/loss=2.18719, test/num_examples=10000, total_duration=34812.8, train/accuracy=0.705935, train/loss=1.26644, validation/accuracy=0.64074, validation/loss=1.55825, validation/num_examples=50000
I0307 13:25:30.319154 140237678286592 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.8087587356567383, loss=2.587554931640625
I0307 13:26:44.705546 140237686679296 logging_writer.py:48] [81200] global_step=81200, grad_norm=3.3404037952423096, loss=2.597968578338623
I0307 13:27:35.777268 140237678286592 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.0492122173309326, loss=2.510179042816162
I0307 13:28:20.696197 140237686679296 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.7598788738250732, loss=2.621248960494995
I0307 13:29:03.407271 140237678286592 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.6957733631134033, loss=2.722856283187866
I0307 13:29:52.121574 140237686679296 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.271979808807373, loss=2.5361201763153076
I0307 13:30:46.053033 140237678286592 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.8304941654205322, loss=2.66420316696167
I0307 13:31:33.205448 140237686679296 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.691664457321167, loss=2.616990327835083
I0307 13:32:30.269028 140237678286592 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.618140459060669, loss=2.5691416263580322
I0307 13:33:16.216484 140237686679296 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.993518352508545, loss=2.656132221221924
I0307 13:33:33.353607 140393707492544 spec.py:321] Evaluating on the training split.
I0307 13:33:46.219942 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 13:34:07.824733 140393707492544 spec.py:349] Evaluating on the test split.
I0307 13:34:09.589856 140393707492544 submission_runner.py:469] Time since start: 35359.47s, 	Step: 82040, 	{'train/accuracy': 0.7329599857330322, 'train/loss': 1.153250813484192, 'validation/accuracy': 0.6435999870300293, 'validation/loss': 1.5510573387145996, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.237302541732788, 'test/num_examples': 10000, 'score': 32695.638719558716, 'total_duration': 35359.470586299896, 'accumulated_submission_time': 32695.638719558716, 'accumulated_eval_time': 2649.869715690613, 'accumulated_logging_time': 5.260061740875244}
I0307 13:34:09.691503 140237678286592 logging_writer.py:48] [82040] accumulated_eval_time=2649.87, accumulated_logging_time=5.26006, accumulated_submission_time=32695.6, global_step=82040, preemption_count=0, score=32695.6, test/accuracy=0.5136, test/loss=2.2373, test/num_examples=10000, total_duration=35359.5, train/accuracy=0.73296, train/loss=1.15325, validation/accuracy=0.6436, validation/loss=1.55106, validation/num_examples=50000
I0307 13:34:35.167737 140237686679296 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.7340502738952637, loss=2.606548309326172
I0307 13:35:19.995312 140237678286592 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.157801389694214, loss=2.6075210571289062
I0307 13:36:08.340950 140237686679296 logging_writer.py:48] [82300] global_step=82300, grad_norm=4.336080551147461, loss=2.6456897258758545
I0307 13:37:02.442632 140237678286592 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.754054546356201, loss=2.5279312133789062
I0307 13:37:51.430563 140237686679296 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.973288059234619, loss=2.73305082321167
2025-03-07 13:38:18.630441: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:39:04.895538 140237678286592 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.262050151824951, loss=2.6371617317199707
I0307 13:40:31.799194 140237686679296 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.533799171447754, loss=2.637831687927246
I0307 13:41:09.908363 140237678286592 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.814486265182495, loss=2.65466570854187
I0307 13:41:49.802343 140237686679296 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.5764284133911133, loss=2.66959810256958
I0307 13:42:36.870016 140237678286592 logging_writer.py:48] [83000] global_step=83000, grad_norm=3.570019006729126, loss=2.593648910522461
I0307 13:42:39.904757 140393707492544 spec.py:321] Evaluating on the training split.
I0307 13:42:52.682265 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 13:43:15.599987 140393707492544 spec.py:349] Evaluating on the test split.
I0307 13:43:17.371193 140393707492544 submission_runner.py:469] Time since start: 35907.25s, 	Step: 83005, 	{'train/accuracy': 0.6928212642669678, 'train/loss': 1.349096655845642, 'validation/accuracy': 0.6462199687957764, 'validation/loss': 1.5621826648712158, 'validation/num_examples': 50000, 'test/accuracy': 0.5187000036239624, 'test/loss': 2.2379472255706787, 'test/num_examples': 10000, 'score': 33205.73368763924, 'total_duration': 35907.25191640854, 'accumulated_submission_time': 33205.73368763924, 'accumulated_eval_time': 2687.3361155986786, 'accumulated_logging_time': 5.369216203689575}
I0307 13:43:17.424259 140237686679296 logging_writer.py:48] [83005] accumulated_eval_time=2687.34, accumulated_logging_time=5.36922, accumulated_submission_time=33205.7, global_step=83005, preemption_count=0, score=33205.7, test/accuracy=0.5187, test/loss=2.23795, test/num_examples=10000, total_duration=35907.3, train/accuracy=0.692821, train/loss=1.3491, validation/accuracy=0.64622, validation/loss=1.56218, validation/num_examples=50000
I0307 13:44:07.107506 140237678286592 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.615817070007324, loss=2.5881848335266113
I0307 13:44:52.041301 140237686679296 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.8855247497558594, loss=2.5898995399475098
I0307 13:45:36.970953 140237678286592 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.4587810039520264, loss=2.4822208881378174
I0307 13:46:30.580697 140237686679296 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.5279619693756104, loss=2.623952627182007
I0307 13:47:16.403430 140237678286592 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.0865516662597656, loss=2.640962839126587
I0307 13:48:01.380525 140237686679296 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.9397432804107666, loss=2.5507686138153076
I0307 13:48:44.678628 140237678286592 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.6129698753356934, loss=2.5497806072235107
I0307 13:49:24.941336 140237686679296 logging_writer.py:48] [83800] global_step=83800, grad_norm=3.832930564880371, loss=2.5971927642822266
I0307 13:50:07.380984 140237678286592 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.016451358795166, loss=2.609543800354004
I0307 13:50:50.770754 140237686679296 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.818490505218506, loss=2.577716588973999
I0307 13:51:32.743168 140237678286592 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.664846897125244, loss=2.5777859687805176
I0307 13:51:47.503796 140393707492544 spec.py:321] Evaluating on the training split.
I0307 13:51:59.833167 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 13:52:22.458631 140393707492544 spec.py:349] Evaluating on the test split.
I0307 13:52:24.237239 140393707492544 submission_runner.py:469] Time since start: 36454.12s, 	Step: 84139, 	{'train/accuracy': 0.6945351958274841, 'train/loss': 1.3159548044204712, 'validation/accuracy': 0.6390399932861328, 'validation/loss': 1.5624115467071533, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.2333319187164307, 'test/num_examples': 10000, 'score': 33715.67713141441, 'total_duration': 36454.11797332764, 'accumulated_submission_time': 33715.67713141441, 'accumulated_eval_time': 2724.0695250034332, 'accumulated_logging_time': 5.430817365646362}
I0307 13:52:24.304774 140237686679296 logging_writer.py:48] [84139] accumulated_eval_time=2724.07, accumulated_logging_time=5.43082, accumulated_submission_time=33715.7, global_step=84139, preemption_count=0, score=33715.7, test/accuracy=0.5159, test/loss=2.23333, test/num_examples=10000, total_duration=36454.1, train/accuracy=0.694535, train/loss=1.31595, validation/accuracy=0.63904, validation/loss=1.56241, validation/num_examples=50000
I0307 13:52:48.986020 140237678286592 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.350151777267456, loss=2.5984935760498047
I0307 13:53:31.910125 140237686679296 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.2961254119873047, loss=2.5857255458831787
I0307 13:54:50.634562 140237678286592 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.754559278488159, loss=2.627777576446533
I0307 13:56:15.046896 140237686679296 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.8225815296173096, loss=2.6022439002990723
I0307 13:57:02.920405 140237678286592 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.468421697616577, loss=2.5755507946014404
I0307 13:57:44.426163 140237686679296 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.4809467792510986, loss=2.6627743244171143
I0307 13:58:26.539280 140237678286592 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.376828908920288, loss=2.5707924365997314
I0307 13:59:26.309642 140237686679296 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.333479404449463, loss=2.5236942768096924
I0307 14:00:10.809345 140237678286592 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.401102304458618, loss=2.57485032081604
I0307 14:00:53.542797 140237686679296 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.7463536262512207, loss=2.6222991943359375
I0307 14:00:54.265726 140393707492544 spec.py:321] Evaluating on the training split.
I0307 14:01:08.479927 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 14:01:31.428001 140393707492544 spec.py:349] Evaluating on the test split.
I0307 14:01:33.223033 140393707492544 submission_runner.py:469] Time since start: 37003.10s, 	Step: 85103, 	{'train/accuracy': 0.7028061151504517, 'train/loss': 1.2598367929458618, 'validation/accuracy': 0.6434999704360962, 'validation/loss': 1.5392330884933472, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.214064836502075, 'test/num_examples': 10000, 'score': 34225.52469873428, 'total_duration': 37003.10376429558, 'accumulated_submission_time': 34225.52469873428, 'accumulated_eval_time': 2763.0267927646637, 'accumulated_logging_time': 5.506056547164917}
I0307 14:01:33.364615 140237678286592 logging_writer.py:48] [85103] accumulated_eval_time=2763.03, accumulated_logging_time=5.50606, accumulated_submission_time=34225.5, global_step=85103, preemption_count=0, score=34225.5, test/accuracy=0.5129, test/loss=2.21406, test/num_examples=10000, total_duration=37003.1, train/accuracy=0.702806, train/loss=1.25984, validation/accuracy=0.6435, validation/loss=1.53923, validation/num_examples=50000
I0307 14:02:19.198505 140237686679296 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.51782488822937, loss=2.612384557723999
I0307 14:03:13.124794 140237678286592 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.2449874877929688, loss=2.6371164321899414
I0307 14:04:16.376211 140237686679296 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.2304270267486572, loss=2.6446003913879395
I0307 14:05:01.207870 140237678286592 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.7855968475341797, loss=2.6391005516052246
I0307 14:05:47.489036 140237686679296 logging_writer.py:48] [85600] global_step=85600, grad_norm=4.003610610961914, loss=2.5231966972351074
I0307 14:06:41.662314 140237678286592 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.7619571685791016, loss=2.6432292461395264
I0307 14:07:29.374409 140237686679296 logging_writer.py:48] [85800] global_step=85800, grad_norm=4.235905647277832, loss=2.5537493228912354
I0307 14:08:46.524059 140237678286592 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.361442804336548, loss=2.625532865524292
I0307 14:09:41.823898 140237686679296 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.1818201541900635, loss=2.609861135482788
I0307 14:10:03.404642 140393707492544 spec.py:321] Evaluating on the training split.
I0307 14:10:15.816212 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 14:10:37.408163 140393707492544 spec.py:349] Evaluating on the test split.
I0307 14:10:39.202988 140393707492544 submission_runner.py:469] Time since start: 37549.08s, 	Step: 86041, 	{'train/accuracy': 0.7290138602256775, 'train/loss': 1.177551507949829, 'validation/accuracy': 0.6460399627685547, 'validation/loss': 1.5545614957809448, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.1816036701202393, 'test/num_examples': 10000, 'score': 34735.45169377327, 'total_duration': 37549.08370709419, 'accumulated_submission_time': 34735.45169377327, 'accumulated_eval_time': 2798.825093984604, 'accumulated_logging_time': 5.656623125076294}
I0307 14:10:39.241498 140237678286592 logging_writer.py:48] [86041] accumulated_eval_time=2798.83, accumulated_logging_time=5.65662, accumulated_submission_time=34735.5, global_step=86041, preemption_count=0, score=34735.5, test/accuracy=0.5309, test/loss=2.1816, test/num_examples=10000, total_duration=37549.1, train/accuracy=0.729014, train/loss=1.17755, validation/accuracy=0.64604, validation/loss=1.55456, validation/num_examples=50000
I0307 14:11:08.132736 140237686679296 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.28798770904541, loss=2.655848979949951
I0307 14:12:35.247237 140237678286592 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.407186269760132, loss=2.5601754188537598
I0307 14:13:15.799525 140237686679296 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.915806293487549, loss=2.551443338394165
I0307 14:14:42.413524 140237678286592 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.3873960971832275, loss=2.58308482170105
I0307 14:15:27.782992 140237686679296 logging_writer.py:48] [86500] global_step=86500, grad_norm=3.3919544219970703, loss=2.5797553062438965
I0307 14:17:15.328476 140237678286592 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.7149085998535156, loss=2.6411094665527344
I0307 14:18:31.031998 140237686679296 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.413700580596924, loss=2.5845048427581787
I0307 14:19:09.426426 140393707492544 spec.py:321] Evaluating on the training split.
I0307 14:19:21.152179 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 14:19:39.727632 140393707492544 spec.py:349] Evaluating on the test split.
I0307 14:19:41.535674 140393707492544 submission_runner.py:469] Time since start: 38091.42s, 	Step: 86774, 	{'train/accuracy': 0.7005341053009033, 'train/loss': 1.3050864934921265, 'validation/accuracy': 0.6470400094985962, 'validation/loss': 1.541648268699646, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.1699681282043457, 'test/num_examples': 10000, 'score': 35245.549488306046, 'total_duration': 38091.41640520096, 'accumulated_submission_time': 35245.549488306046, 'accumulated_eval_time': 2830.934319257736, 'accumulated_logging_time': 5.703082323074341}
I0307 14:19:41.653026 140237678286592 logging_writer.py:48] [86774] accumulated_eval_time=2830.93, accumulated_logging_time=5.70308, accumulated_submission_time=35245.5, global_step=86774, preemption_count=0, score=35245.5, test/accuracy=0.5321, test/loss=2.16997, test/num_examples=10000, total_duration=38091.4, train/accuracy=0.700534, train/loss=1.30509, validation/accuracy=0.64704, validation/loss=1.54165, validation/num_examples=50000
I0307 14:19:54.065306 140237686679296 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.0586652755737305, loss=2.5955519676208496
I0307 14:21:16.830772 140237678286592 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.665990114212036, loss=2.5406174659729004
I0307 14:22:17.510740 140237686679296 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.086141347885132, loss=2.4852216243743896
I0307 14:23:02.163674 140237678286592 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.562303304672241, loss=2.647404432296753
I0307 14:23:44.371301 140237686679296 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.31955885887146, loss=2.572558879852295
I0307 14:24:25.038446 140237678286592 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.1004819869995117, loss=2.566413640975952
I0307 14:25:54.516240 140237686679296 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.4406516551971436, loss=2.6120333671569824
I0307 14:26:59.933396 140237678286592 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.471383571624756, loss=2.5722906589508057
I0307 14:27:48.823476 140237686679296 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.366570234298706, loss=2.5847790241241455
I0307 14:28:11.852990 140393707492544 spec.py:321] Evaluating on the training split.
I0307 14:28:22.412324 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 14:28:45.060069 140393707492544 spec.py:349] Evaluating on the test split.
I0307 14:28:46.823717 140393707492544 submission_runner.py:469] Time since start: 38636.70s, 	Step: 87660, 	{'train/accuracy': 0.7178531289100647, 'train/loss': 1.2251883745193481, 'validation/accuracy': 0.6545599699020386, 'validation/loss': 1.5065555572509766, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.161987066268921, 'test/num_examples': 10000, 'score': 35755.64194726944, 'total_duration': 38636.70443224907, 'accumulated_submission_time': 35755.64194726944, 'accumulated_eval_time': 2865.9050023555756, 'accumulated_logging_time': 5.8283281326293945}
I0307 14:28:46.915746 140237678286592 logging_writer.py:48] [87660] accumulated_eval_time=2865.91, accumulated_logging_time=5.82833, accumulated_submission_time=35755.6, global_step=87660, preemption_count=0, score=35755.6, test/accuracy=0.5308, test/loss=2.16199, test/num_examples=10000, total_duration=38636.7, train/accuracy=0.717853, train/loss=1.22519, validation/accuracy=0.65456, validation/loss=1.50656, validation/num_examples=50000
I0307 14:29:03.286309 140237686679296 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.117594242095947, loss=2.7176411151885986
I0307 14:30:03.546178 140237678286592 logging_writer.py:48] [87800] global_step=87800, grad_norm=3.9739253520965576, loss=2.5322601795196533
I0307 14:31:26.950697 140237686679296 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.378998756408691, loss=2.5128777027130127
I0307 14:32:21.990979 140237678286592 logging_writer.py:48] [88000] global_step=88000, grad_norm=3.2891969680786133, loss=2.613068103790283
I0307 14:33:54.482032 140237686679296 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.5406687259674072, loss=2.5816876888275146
I0307 14:34:55.769627 140237678286592 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.7251901626586914, loss=2.633963108062744
I0307 14:36:03.695367 140237686679296 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.4219772815704346, loss=2.58571195602417
I0307 14:36:58.868957 140237678286592 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.4871318340301514, loss=2.620697259902954
I0307 14:37:16.928538 140393707492544 spec.py:321] Evaluating on the training split.
I0307 14:37:28.483546 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 14:37:48.835905 140393707492544 spec.py:349] Evaluating on the test split.
I0307 14:37:50.601426 140393707492544 submission_runner.py:469] Time since start: 39180.48s, 	Step: 88440, 	{'train/accuracy': 0.6963488459587097, 'train/loss': 1.3131103515625, 'validation/accuracy': 0.6462000012397766, 'validation/loss': 1.5422579050064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.1805808544158936, 'test/num_examples': 10000, 'score': 36265.561046123505, 'total_duration': 39180.48215460777, 'accumulated_submission_time': 36265.561046123505, 'accumulated_eval_time': 2899.577857732773, 'accumulated_logging_time': 5.928080320358276}
I0307 14:37:50.655089 140237686679296 logging_writer.py:48] [88440] accumulated_eval_time=2899.58, accumulated_logging_time=5.92808, accumulated_submission_time=36265.6, global_step=88440, preemption_count=0, score=36265.6, test/accuracy=0.526, test/loss=2.18058, test/num_examples=10000, total_duration=39180.5, train/accuracy=0.696349, train/loss=1.31311, validation/accuracy=0.6462, validation/loss=1.54226, validation/num_examples=50000
I0307 14:38:14.949863 140237678286592 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.450360059738159, loss=2.574505567550659
I0307 14:39:00.675976 140237686679296 logging_writer.py:48] [88600] global_step=88600, grad_norm=3.843716859817505, loss=2.5871222019195557
I0307 14:39:57.810877 140237678286592 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.8470373153686523, loss=2.530576229095459
I0307 14:41:03.446075 140237686679296 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.31734299659729, loss=2.572040557861328
2025-03-07 14:41:14.554766: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:42:37.754877 140237678286592 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.226897954940796, loss=2.5682590007781982
I0307 14:43:49.781110 140237686679296 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.625861406326294, loss=2.600114345550537
I0307 14:45:08.213810 140237678286592 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.8678135871887207, loss=2.7070415019989014
I0307 14:46:21.014306 140393707492544 spec.py:321] Evaluating on the training split.
I0307 14:46:33.868888 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 14:46:54.627529 140393707492544 spec.py:349] Evaluating on the test split.
I0307 14:46:56.394187 140393707492544 submission_runner.py:469] Time since start: 39726.27s, 	Step: 89194, 	{'train/accuracy': 0.7027263641357422, 'train/loss': 1.2801790237426758, 'validation/accuracy': 0.6464799642562866, 'validation/loss': 1.5340098142623901, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.1986050605773926, 'test/num_examples': 10000, 'score': 36775.806440114975, 'total_duration': 39726.27491426468, 'accumulated_submission_time': 36775.806440114975, 'accumulated_eval_time': 2934.9577057361603, 'accumulated_logging_time': 6.0126330852508545}
I0307 14:46:56.429814 140237686679296 logging_writer.py:48] [89194] accumulated_eval_time=2934.96, accumulated_logging_time=6.01263, accumulated_submission_time=36775.8, global_step=89194, preemption_count=0, score=36775.8, test/accuracy=0.5196, test/loss=2.19861, test/num_examples=10000, total_duration=39726.3, train/accuracy=0.702726, train/loss=1.28018, validation/accuracy=0.64648, validation/loss=1.53401, validation/num_examples=50000
I0307 14:46:59.194861 140237678286592 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.379767894744873, loss=2.5983734130859375
I0307 14:48:03.427495 140237686679296 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.2260286808013916, loss=2.550544261932373
I0307 14:49:35.783760 140237678286592 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.9279181957244873, loss=2.625971794128418
I0307 14:50:25.175508 140237686679296 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.4940989017486572, loss=2.642223596572876
I0307 14:51:54.493438 140237678286592 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.030294418334961, loss=2.5980496406555176
I0307 14:52:52.695778 140237686679296 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.579441547393799, loss=2.547184705734253
I0307 14:53:45.645997 140237678286592 logging_writer.py:48] [89800] global_step=89800, grad_norm=3.256896495819092, loss=2.5575902462005615
I0307 14:54:49.331886 140237686679296 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.788982391357422, loss=2.5462167263031006
I0307 14:55:26.672577 140393707492544 spec.py:321] Evaluating on the training split.
I0307 14:55:37.868582 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 14:55:57.728451 140393707492544 spec.py:349] Evaluating on the test split.
I0307 14:55:59.535172 140393707492544 submission_runner.py:469] Time since start: 40269.42s, 	Step: 89971, 	{'train/accuracy': 0.744140625, 'train/loss': 1.0952011346817017, 'validation/accuracy': 0.650119960308075, 'validation/loss': 1.511588215827942, 'validation/num_examples': 50000, 'test/accuracy': 0.523900032043457, 'test/loss': 2.1890480518341064, 'test/num_examples': 10000, 'score': 37285.95492887497, 'total_duration': 40269.415907382965, 'accumulated_submission_time': 37285.95492887497, 'accumulated_eval_time': 2967.8202724456787, 'accumulated_logging_time': 6.056591749191284}
I0307 14:55:59.569859 140237678286592 logging_writer.py:48] [89971] accumulated_eval_time=2967.82, accumulated_logging_time=6.05659, accumulated_submission_time=37286, global_step=89971, preemption_count=0, score=37286, test/accuracy=0.5239, test/loss=2.18905, test/num_examples=10000, total_duration=40269.4, train/accuracy=0.744141, train/loss=1.0952, validation/accuracy=0.65012, validation/loss=1.51159, validation/num_examples=50000
I0307 14:56:11.693609 140237686679296 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.4144208431243896, loss=2.564150810241699
2025-03-07 14:56:51.706340: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:57:15.535245 140237678286592 logging_writer.py:48] [90100] global_step=90100, grad_norm=3.314872980117798, loss=2.5317513942718506
I0307 14:58:06.727729 140237686679296 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.448822021484375, loss=2.6128456592559814
I0307 14:59:42.813067 140237678286592 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.9182944297790527, loss=2.5764381885528564
I0307 15:00:35.725416 140237686679296 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.9585471153259277, loss=2.558547258377075
I0307 15:01:25.397333 140237678286592 logging_writer.py:48] [90500] global_step=90500, grad_norm=3.37502384185791, loss=2.5542571544647217
I0307 15:02:21.576696 140237686679296 logging_writer.py:48] [90600] global_step=90600, grad_norm=3.840580701828003, loss=2.5662453174591064
I0307 15:03:18.166621 140237678286592 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.1482715606689453, loss=2.5196969509124756
I0307 15:04:15.738088 140237686679296 logging_writer.py:48] [90800] global_step=90800, grad_norm=3.3607969284057617, loss=2.574655771255493
I0307 15:04:30.269407 140393707492544 spec.py:321] Evaluating on the training split.
I0307 15:04:41.055052 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 15:05:01.724181 140393707492544 spec.py:349] Evaluating on the test split.
I0307 15:05:03.499300 140393707492544 submission_runner.py:469] Time since start: 40813.38s, 	Step: 90825, 	{'train/accuracy': 0.6939971446990967, 'train/loss': 1.3381785154342651, 'validation/accuracy': 0.6429799795150757, 'validation/loss': 1.5819443464279175, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.234989881515503, 'test/num_examples': 10000, 'score': 37796.54807496071, 'total_duration': 40813.380023002625, 'accumulated_submission_time': 37796.54807496071, 'accumulated_eval_time': 3001.0501289367676, 'accumulated_logging_time': 6.099925994873047}
I0307 15:05:03.543905 140237678286592 logging_writer.py:48] [90825] accumulated_eval_time=3001.05, accumulated_logging_time=6.09993, accumulated_submission_time=37796.5, global_step=90825, preemption_count=0, score=37796.5, test/accuracy=0.5202, test/loss=2.23499, test/num_examples=10000, total_duration=40813.4, train/accuracy=0.693997, train/loss=1.33818, validation/accuracy=0.64298, validation/loss=1.58194, validation/num_examples=50000
I0307 15:06:43.865395 140237686679296 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.695960283279419, loss=2.5544586181640625
I0307 15:07:52.445453 140237678286592 logging_writer.py:48] [91000] global_step=91000, grad_norm=3.3726797103881836, loss=2.518256664276123
I0307 15:08:45.313919 140237686679296 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.2086169719696045, loss=2.595160961151123
I0307 15:09:44.807325 140237678286592 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.3257951736450195, loss=2.511932611465454
I0307 15:11:07.475266 140237686679296 logging_writer.py:48] [91300] global_step=91300, grad_norm=4.4454498291015625, loss=2.489159107208252
I0307 15:12:36.123043 140237678286592 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.815049886703491, loss=2.4951791763305664
I0307 15:13:33.989146 140393707492544 spec.py:321] Evaluating on the training split.
I0307 15:13:45.315194 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 15:14:05.745488 140393707492544 spec.py:349] Evaluating on the test split.
I0307 15:14:07.542528 140393707492544 submission_runner.py:469] Time since start: 41357.42s, 	Step: 91495, 	{'train/accuracy': 0.7225565910339355, 'train/loss': 1.2032887935638428, 'validation/accuracy': 0.653719961643219, 'validation/loss': 1.5242154598236084, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.1811347007751465, 'test/num_examples': 10000, 'score': 38306.91541457176, 'total_duration': 41357.423248529434, 'accumulated_submission_time': 38306.91541457176, 'accumulated_eval_time': 3034.603466272354, 'accumulated_logging_time': 6.15294337272644}
I0307 15:14:07.585316 140237686679296 logging_writer.py:48] [91495] accumulated_eval_time=3034.6, accumulated_logging_time=6.15294, accumulated_submission_time=38306.9, global_step=91495, preemption_count=0, score=38306.9, test/accuracy=0.529, test/loss=2.18113, test/num_examples=10000, total_duration=41357.4, train/accuracy=0.722557, train/loss=1.20329, validation/accuracy=0.65372, validation/loss=1.52422, validation/num_examples=50000
I0307 15:14:09.917447 140237678286592 logging_writer.py:48] [91500] global_step=91500, grad_norm=3.781733751296997, loss=2.44281268119812
I0307 15:15:09.744062 140237686679296 logging_writer.py:48] [91600] global_step=91600, grad_norm=3.292959451675415, loss=2.5038275718688965
I0307 15:17:54.475122 140237678286592 logging_writer.py:48] [91700] global_step=91700, grad_norm=3.2666375637054443, loss=2.5421032905578613
I0307 15:19:50.283746 140237686679296 logging_writer.py:48] [91800] global_step=91800, grad_norm=4.324710369110107, loss=2.4148433208465576
I0307 15:21:17.335458 140237678286592 logging_writer.py:48] [91900] global_step=91900, grad_norm=3.3428494930267334, loss=2.443943500518799
I0307 15:22:29.272774 140237686679296 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.347020387649536, loss=2.641514539718628
I0307 15:22:37.682912 140393707492544 spec.py:321] Evaluating on the training split.
I0307 15:22:48.438503 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 15:23:07.698115 140393707492544 spec.py:349] Evaluating on the test split.
I0307 15:23:09.497924 140393707492544 submission_runner.py:469] Time since start: 41899.38s, 	Step: 92011, 	{'train/accuracy': 0.7049385905265808, 'train/loss': 1.3060574531555176, 'validation/accuracy': 0.6490399837493896, 'validation/loss': 1.5620735883712769, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.249709129333496, 'test/num_examples': 10000, 'score': 38816.94752573967, 'total_duration': 41899.378648757935, 'accumulated_submission_time': 38816.94752573967, 'accumulated_eval_time': 3066.4184379577637, 'accumulated_logging_time': 6.203336715698242}
I0307 15:23:09.536266 140237678286592 logging_writer.py:48] [92011] accumulated_eval_time=3066.42, accumulated_logging_time=6.20334, accumulated_submission_time=38816.9, global_step=92011, preemption_count=0, score=38816.9, test/accuracy=0.5178, test/loss=2.24971, test/num_examples=10000, total_duration=41899.4, train/accuracy=0.704939, train/loss=1.30606, validation/accuracy=0.64904, validation/loss=1.56207, validation/num_examples=50000
I0307 15:24:12.465963 140237686679296 logging_writer.py:48] [92100] global_step=92100, grad_norm=3.6920244693756104, loss=2.529348134994507
I0307 15:25:35.152793 140237678286592 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.7375125885009766, loss=2.533430337905884
I0307 15:26:23.447276 140237686679296 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.6064085960388184, loss=2.5698461532592773
I0307 15:27:41.089652 140237678286592 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.74997615814209, loss=2.528343677520752
I0307 15:28:55.635093 140237686679296 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.6774775981903076, loss=2.64064621925354
I0307 15:29:56.579419 140237678286592 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.6924986839294434, loss=2.5856757164001465
I0307 15:31:11.780251 140237686679296 logging_writer.py:48] [92700] global_step=92700, grad_norm=3.8408524990081787, loss=2.6277658939361572
I0307 15:31:39.913446 140393707492544 spec.py:321] Evaluating on the training split.
I0307 15:31:50.581238 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 15:32:10.880071 140393707492544 spec.py:349] Evaluating on the test split.
I0307 15:32:12.630165 140393707492544 submission_runner.py:469] Time since start: 42442.51s, 	Step: 92737, 	{'train/accuracy': 0.729910671710968, 'train/loss': 1.178239107131958, 'validation/accuracy': 0.6497399806976318, 'validation/loss': 1.54465913772583, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.2249951362609863, 'test/num_examples': 10000, 'score': 39327.23157310486, 'total_duration': 42442.51089644432, 'accumulated_submission_time': 39327.23157310486, 'accumulated_eval_time': 3099.135130405426, 'accumulated_logging_time': 6.254950284957886}
I0307 15:32:12.704686 140237678286592 logging_writer.py:48] [92737] accumulated_eval_time=3099.14, accumulated_logging_time=6.25495, accumulated_submission_time=39327.2, global_step=92737, preemption_count=0, score=39327.2, test/accuracy=0.521, test/loss=2.225, test/num_examples=10000, total_duration=42442.5, train/accuracy=0.729911, train/loss=1.17824, validation/accuracy=0.64974, validation/loss=1.54466, validation/num_examples=50000
I0307 15:32:44.503078 140237686679296 logging_writer.py:48] [92800] global_step=92800, grad_norm=3.581616163253784, loss=2.520477056503296
I0307 15:33:34.456236 140237678286592 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.6411850452423096, loss=2.584667921066284
I0307 15:34:24.056746 140237686679296 logging_writer.py:48] [93000] global_step=93000, grad_norm=3.313417434692383, loss=2.4887890815734863
I0307 15:37:21.905021 140237678286592 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.109598636627197, loss=2.442387580871582
I0307 15:39:32.361441 140237686679296 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.9192328453063965, loss=2.601571559906006
I0307 15:40:43.145087 140393707492544 spec.py:321] Evaluating on the training split.
I0307 15:40:53.103035 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 15:41:15.132944 140393707492544 spec.py:349] Evaluating on the test split.
I0307 15:41:16.945184 140393707492544 submission_runner.py:469] Time since start: 42986.83s, 	Step: 93268, 	{'train/accuracy': 0.7146045565605164, 'train/loss': 1.2338337898254395, 'validation/accuracy': 0.6588999629020691, 'validation/loss': 1.49222731590271, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.154205799102783, 'test/num_examples': 10000, 'score': 39837.59973573685, 'total_duration': 42986.82591319084, 'accumulated_submission_time': 39837.59973573685, 'accumulated_eval_time': 3132.935188770294, 'accumulated_logging_time': 6.344084739685059}
I0307 15:41:16.988318 140237678286592 logging_writer.py:48] [93268] accumulated_eval_time=3132.94, accumulated_logging_time=6.34408, accumulated_submission_time=39837.6, global_step=93268, preemption_count=0, score=39837.6, test/accuracy=0.5307, test/loss=2.15421, test/num_examples=10000, total_duration=42986.8, train/accuracy=0.714605, train/loss=1.23383, validation/accuracy=0.6589, validation/loss=1.49223, validation/num_examples=50000
I0307 15:41:37.435803 140237686679296 logging_writer.py:48] [93300] global_step=93300, grad_norm=3.339959144592285, loss=2.552229404449463
I0307 15:42:45.948786 140237678286592 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.4814093112945557, loss=2.5626959800720215
I0307 15:43:56.727441 140237686679296 logging_writer.py:48] [93500] global_step=93500, grad_norm=3.457031011581421, loss=2.5646984577178955
I0307 15:45:18.518014 140237678286592 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.6310536861419678, loss=2.4809226989746094
I0307 15:47:01.412141 140237686679296 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.644216537475586, loss=2.550804615020752
I0307 15:48:26.053261 140237678286592 logging_writer.py:48] [93800] global_step=93800, grad_norm=3.795999526977539, loss=2.4909005165100098
I0307 15:49:43.984270 140237686679296 logging_writer.py:48] [93900] global_step=93900, grad_norm=3.25758695602417, loss=2.498100519180298
I0307 15:49:47.116515 140393707492544 spec.py:321] Evaluating on the training split.
I0307 15:49:57.642269 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 15:50:16.926044 140393707492544 spec.py:349] Evaluating on the test split.
I0307 15:50:18.726716 140393707492544 submission_runner.py:469] Time since start: 43528.61s, 	Step: 93905, 	{'train/accuracy': 0.699617326259613, 'train/loss': 1.2866594791412354, 'validation/accuracy': 0.6510599851608276, 'validation/loss': 1.5093066692352295, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.1932735443115234, 'test/num_examples': 10000, 'score': 40347.65095090866, 'total_duration': 43528.60744023323, 'accumulated_submission_time': 40347.65095090866, 'accumulated_eval_time': 3164.545352935791, 'accumulated_logging_time': 6.395532846450806}
I0307 15:50:18.785624 140237678286592 logging_writer.py:48] [93905] accumulated_eval_time=3164.55, accumulated_logging_time=6.39553, accumulated_submission_time=40347.7, global_step=93905, preemption_count=0, score=40347.7, test/accuracy=0.5299, test/loss=2.19327, test/num_examples=10000, total_duration=43528.6, train/accuracy=0.699617, train/loss=1.28666, validation/accuracy=0.65106, validation/loss=1.50931, validation/num_examples=50000
I0307 15:51:26.014317 140237686679296 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.593580961227417, loss=2.4161343574523926
I0307 15:53:22.338686 140237678286592 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.4675261974334717, loss=2.601743221282959
I0307 15:54:42.404089 140237686679296 logging_writer.py:48] [94200] global_step=94200, grad_norm=3.6628646850585938, loss=2.5462515354156494
I0307 15:55:52.303519 140237678286592 logging_writer.py:48] [94300] global_step=94300, grad_norm=3.657748222351074, loss=2.475109338760376
I0307 15:57:11.590745 140237686679296 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.399211406707764, loss=2.4556596279144287
I0307 15:58:48.881114 140393707492544 spec.py:321] Evaluating on the training split.
I0307 15:58:59.205764 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 15:59:21.786304 140393707492544 spec.py:349] Evaluating on the test split.
I0307 15:59:23.547724 140393707492544 submission_runner.py:469] Time since start: 44073.43s, 	Step: 94497, 	{'train/accuracy': 0.7049983739852905, 'train/loss': 1.280906081199646, 'validation/accuracy': 0.6481800079345703, 'validation/loss': 1.5424549579620361, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.210353374481201, 'test/num_examples': 10000, 'score': 40857.67301392555, 'total_duration': 44073.42842292786, 'accumulated_submission_time': 40857.67301392555, 'accumulated_eval_time': 3199.2119233608246, 'accumulated_logging_time': 6.463046073913574}
I0307 15:59:23.600030 140237678286592 logging_writer.py:48] [94497] accumulated_eval_time=3199.21, accumulated_logging_time=6.46305, accumulated_submission_time=40857.7, global_step=94497, preemption_count=0, score=40857.7, test/accuracy=0.5219, test/loss=2.21035, test/num_examples=10000, total_duration=44073.4, train/accuracy=0.704998, train/loss=1.28091, validation/accuracy=0.64818, validation/loss=1.54245, validation/num_examples=50000
I0307 15:59:25.171891 140237686679296 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.787987470626831, loss=2.736738443374634
I0307 16:01:20.798902 140237678286592 logging_writer.py:48] [94600] global_step=94600, grad_norm=3.7728404998779297, loss=2.642119884490967
I0307 16:02:44.917957 140237686679296 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.5580103397369385, loss=2.5565757751464844
I0307 16:04:11.624935 140237678286592 logging_writer.py:48] [94800] global_step=94800, grad_norm=3.944075107574463, loss=2.6218245029449463
I0307 16:06:11.796677 140237686679296 logging_writer.py:48] [94900] global_step=94900, grad_norm=3.410287618637085, loss=2.552464246749878
I0307 16:07:17.796479 140237678286592 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.9019055366516113, loss=2.5910251140594482
I0307 16:07:53.961850 140393707492544 spec.py:321] Evaluating on the training split.
I0307 16:08:05.770188 140393707492544 spec.py:333] Evaluating on the validation split.
2025-03-07 16:08:07.471163: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:08:25.153173 140393707492544 spec.py:349] Evaluating on the test split.
I0307 16:08:26.923059 140393707492544 submission_runner.py:469] Time since start: 44616.80s, 	Step: 95059, 	{'train/accuracy': 0.7104392647743225, 'train/loss': 1.248903751373291, 'validation/accuracy': 0.6591799855232239, 'validation/loss': 1.47475266456604, 'validation/num_examples': 50000, 'test/accuracy': 0.5375000238418579, 'test/loss': 2.127592086791992, 'test/num_examples': 10000, 'score': 41367.95849776268, 'total_duration': 44616.80377650261, 'accumulated_submission_time': 41367.95849776268, 'accumulated_eval_time': 3232.173087835312, 'accumulated_logging_time': 6.531429767608643}
I0307 16:08:27.038903 140237686679296 logging_writer.py:48] [95059] accumulated_eval_time=3232.17, accumulated_logging_time=6.53143, accumulated_submission_time=41368, global_step=95059, preemption_count=0, score=41368, test/accuracy=0.5375, test/loss=2.12759, test/num_examples=10000, total_duration=44616.8, train/accuracy=0.710439, train/loss=1.2489, validation/accuracy=0.65918, validation/loss=1.47475, validation/num_examples=50000
I0307 16:09:18.758923 140237678286592 logging_writer.py:48] [95100] global_step=95100, grad_norm=3.4036672115325928, loss=2.4858243465423584
I0307 16:10:49.031800 140237686679296 logging_writer.py:48] [95200] global_step=95200, grad_norm=3.4132838249206543, loss=2.5681190490722656
I0307 16:12:10.498649 140237678286592 logging_writer.py:48] [95300] global_step=95300, grad_norm=4.017390251159668, loss=2.5095551013946533
I0307 16:13:32.213728 140237686679296 logging_writer.py:48] [95400] global_step=95400, grad_norm=3.4749035835266113, loss=2.596705675125122
I0307 16:14:58.387356 140237678286592 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.132155895233154, loss=2.5418481826782227
I0307 16:16:30.300559 140237686679296 logging_writer.py:48] [95600] global_step=95600, grad_norm=3.678492546081543, loss=2.6454660892486572
I0307 16:16:57.412736 140393707492544 spec.py:321] Evaluating on the training split.
I0307 16:17:08.461952 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 16:17:28.941653 140393707492544 spec.py:349] Evaluating on the test split.
I0307 16:17:30.705911 140393707492544 submission_runner.py:469] Time since start: 45160.59s, 	Step: 95634, 	{'train/accuracy': 0.7330994606018066, 'train/loss': 1.1398533582687378, 'validation/accuracy': 0.6633399724960327, 'validation/loss': 1.4489723443984985, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.0861051082611084, 'test/num_examples': 10000, 'score': 41878.2607088089, 'total_duration': 45160.58663845062, 'accumulated_submission_time': 41878.2607088089, 'accumulated_eval_time': 3265.4662256240845, 'accumulated_logging_time': 6.655385971069336}
I0307 16:17:30.778837 140237678286592 logging_writer.py:48] [95634] accumulated_eval_time=3265.47, accumulated_logging_time=6.65539, accumulated_submission_time=41878.3, global_step=95634, preemption_count=0, score=41878.3, test/accuracy=0.5407, test/loss=2.08611, test/num_examples=10000, total_duration=45160.6, train/accuracy=0.733099, train/loss=1.13985, validation/accuracy=0.66334, validation/loss=1.44897, validation/num_examples=50000
I0307 16:18:03.952960 140237686679296 logging_writer.py:48] [95700] global_step=95700, grad_norm=3.6049985885620117, loss=2.5411806106567383
I0307 16:19:57.756956 140237678286592 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.82354736328125, loss=2.5179738998413086
I0307 16:21:06.480610 140237686679296 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.752532482147217, loss=2.59299373626709
I0307 16:21:58.537597 140237678286592 logging_writer.py:48] [96000] global_step=96000, grad_norm=3.6968934535980225, loss=2.5639710426330566
I0307 16:23:03.301313 140237686679296 logging_writer.py:48] [96100] global_step=96100, grad_norm=3.8881101608276367, loss=2.5451889038085938
I0307 16:24:31.345570 140237678286592 logging_writer.py:48] [96200] global_step=96200, grad_norm=3.686772108078003, loss=2.532047748565674
I0307 16:25:52.581085 140237686679296 logging_writer.py:48] [96300] global_step=96300, grad_norm=3.7396538257598877, loss=2.4998598098754883
I0307 16:26:01.362987 140393707492544 spec.py:321] Evaluating on the training split.
I0307 16:26:11.713729 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 16:26:32.355412 140393707492544 spec.py:349] Evaluating on the test split.
I0307 16:26:34.154015 140393707492544 submission_runner.py:469] Time since start: 45704.03s, 	Step: 96311, 	{'train/accuracy': 0.7125119566917419, 'train/loss': 1.2434674501419067, 'validation/accuracy': 0.6585400104522705, 'validation/loss': 1.495395541191101, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.150344133377075, 'test/num_examples': 10000, 'score': 42388.76323437691, 'total_duration': 45704.03474211693, 'accumulated_submission_time': 42388.76323437691, 'accumulated_eval_time': 3298.257212162018, 'accumulated_logging_time': 6.736063003540039}
I0307 16:26:34.217260 140237678286592 logging_writer.py:48] [96311] accumulated_eval_time=3298.26, accumulated_logging_time=6.73606, accumulated_submission_time=42388.8, global_step=96311, preemption_count=0, score=42388.8, test/accuracy=0.533, test/loss=2.15034, test/num_examples=10000, total_duration=45704, train/accuracy=0.712512, train/loss=1.24347, validation/accuracy=0.65854, validation/loss=1.4954, validation/num_examples=50000
I0307 16:28:22.277354 140237686679296 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.355249404907227, loss=2.5610811710357666
I0307 16:30:19.455111 140237678286592 logging_writer.py:48] [96500] global_step=96500, grad_norm=3.1529743671417236, loss=2.5151541233062744
I0307 16:31:50.107934 140237686679296 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.580152750015259, loss=2.513190507888794
I0307 16:33:21.957304 140237678286592 logging_writer.py:48] [96700] global_step=96700, grad_norm=3.679067373275757, loss=2.509465456008911
I0307 16:35:04.883869 140393707492544 spec.py:321] Evaluating on the training split.
I0307 16:35:15.369027 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 16:35:35.791632 140393707492544 spec.py:349] Evaluating on the test split.
I0307 16:35:37.590003 140393707492544 submission_runner.py:469] Time since start: 46247.47s, 	Step: 96752, 	{'train/accuracy': 0.7551020383834839, 'train/loss': 1.0866751670837402, 'validation/accuracy': 0.6674000024795532, 'validation/loss': 1.4685901403427124, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.105558395385742, 'test/num_examples': 10000, 'score': 42899.36148262024, 'total_duration': 46247.470718860626, 'accumulated_submission_time': 42899.36148262024, 'accumulated_eval_time': 3330.9632999897003, 'accumulated_logging_time': 6.817828416824341}
I0307 16:35:37.634580 140237686679296 logging_writer.py:48] [96752] accumulated_eval_time=3330.96, accumulated_logging_time=6.81783, accumulated_submission_time=42899.4, global_step=96752, preemption_count=0, score=42899.4, test/accuracy=0.5436, test/loss=2.10556, test/num_examples=10000, total_duration=46247.5, train/accuracy=0.755102, train/loss=1.08668, validation/accuracy=0.6674, validation/loss=1.46859, validation/num_examples=50000
I0307 16:36:55.755431 140237678286592 logging_writer.py:48] [96800] global_step=96800, grad_norm=3.4578144550323486, loss=2.5293774604797363
I0307 16:39:35.684609 140237686679296 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.4390790462493896, loss=2.563169479370117
I0307 16:42:59.117422 140237678286592 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.6568000316619873, loss=2.5451242923736572
I0307 16:44:08.576506 140393707492544 spec.py:321] Evaluating on the training split.
I0307 16:44:18.679424 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 16:44:41.418048 140393707492544 spec.py:349] Evaluating on the test split.
I0307 16:44:43.170059 140393707492544 submission_runner.py:469] Time since start: 46793.05s, 	Step: 97033, 	{'train/accuracy': 0.7271205186843872, 'train/loss': 1.1751289367675781, 'validation/accuracy': 0.6590399742126465, 'validation/loss': 1.473814606666565, 'validation/num_examples': 50000, 'test/accuracy': 0.5325000286102295, 'test/loss': 2.1544113159179688, 'test/num_examples': 10000, 'score': 43410.26461362839, 'total_duration': 46793.050782203674, 'accumulated_submission_time': 43410.26461362839, 'accumulated_eval_time': 3365.5568146705627, 'accumulated_logging_time': 6.8711864948272705}
I0307 16:44:43.231678 140237686679296 logging_writer.py:48] [97033] accumulated_eval_time=3365.56, accumulated_logging_time=6.87119, accumulated_submission_time=43410.3, global_step=97033, preemption_count=0, score=43410.3, test/accuracy=0.5325, test/loss=2.15441, test/num_examples=10000, total_duration=46793.1, train/accuracy=0.727121, train/loss=1.17513, validation/accuracy=0.65904, validation/loss=1.47381, validation/num_examples=50000
I0307 16:46:07.580204 140237678286592 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.7537736892700195, loss=2.6470417976379395
I0307 16:48:43.838582 140237686679296 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.436765670776367, loss=2.511453628540039
I0307 16:49:37.369540 140237678286592 logging_writer.py:48] [97300] global_step=97300, grad_norm=3.62746000289917, loss=2.4945387840270996
I0307 16:51:09.314237 140237686679296 logging_writer.py:48] [97400] global_step=97400, grad_norm=3.5865283012390137, loss=2.517543077468872
I0307 16:53:13.470976 140393707492544 spec.py:321] Evaluating on the training split.
I0307 16:53:23.723581 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 16:53:43.033868 140393707492544 spec.py:349] Evaluating on the test split.
I0307 16:53:44.823690 140393707492544 submission_runner.py:469] Time since start: 47334.70s, 	Step: 97477, 	{'train/accuracy': 0.7231743931770325, 'train/loss': 1.1722335815429688, 'validation/accuracy': 0.6620399951934814, 'validation/loss': 1.4404419660568237, 'validation/num_examples': 50000, 'test/accuracy': 0.5397000312805176, 'test/loss': 2.1169655323028564, 'test/num_examples': 10000, 'score': 43920.425899505615, 'total_duration': 47334.70442152023, 'accumulated_submission_time': 43920.425899505615, 'accumulated_eval_time': 3396.9095027446747, 'accumulated_logging_time': 6.96275782585144}
I0307 16:53:44.924523 140237678286592 logging_writer.py:48] [97477] accumulated_eval_time=3396.91, accumulated_logging_time=6.96276, accumulated_submission_time=43920.4, global_step=97477, preemption_count=0, score=43920.4, test/accuracy=0.5397, test/loss=2.11697, test/num_examples=10000, total_duration=47334.7, train/accuracy=0.723174, train/loss=1.17223, validation/accuracy=0.66204, validation/loss=1.44044, validation/num_examples=50000
I0307 16:54:06.918595 140237686679296 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.800877094268799, loss=2.4463131427764893
I0307 16:56:34.686658 140237678286592 logging_writer.py:48] [97600] global_step=97600, grad_norm=3.94702410697937, loss=2.537321090698242
I0307 16:58:25.965006 140237686679296 logging_writer.py:48] [97700] global_step=97700, grad_norm=3.859138011932373, loss=2.544285297393799
I0307 17:00:50.204209 140237678286592 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.8532330989837646, loss=2.5589749813079834
I0307 17:02:15.678807 140393707492544 spec.py:321] Evaluating on the training split.
I0307 17:02:26.061242 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 17:02:47.495151 140393707492544 spec.py:349] Evaluating on the test split.
I0307 17:02:49.290485 140393707492544 submission_runner.py:469] Time since start: 47879.17s, 	Step: 97895, 	{'train/accuracy': 0.7117546200752258, 'train/loss': 1.2318885326385498, 'validation/accuracy': 0.6568199992179871, 'validation/loss': 1.4732506275177002, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.130251169204712, 'test/num_examples': 10000, 'score': 44431.06924343109, 'total_duration': 47879.17121720314, 'accumulated_submission_time': 44431.06924343109, 'accumulated_eval_time': 3430.521162509918, 'accumulated_logging_time': 7.130736351013184}
I0307 17:02:49.363325 140237686679296 logging_writer.py:48] [97895] accumulated_eval_time=3430.52, accumulated_logging_time=7.13074, accumulated_submission_time=44431.1, global_step=97895, preemption_count=0, score=44431.1, test/accuracy=0.5323, test/loss=2.13025, test/num_examples=10000, total_duration=47879.2, train/accuracy=0.711755, train/loss=1.23189, validation/accuracy=0.65682, validation/loss=1.47325, validation/num_examples=50000
I0307 17:02:51.721076 140237678286592 logging_writer.py:48] [97900] global_step=97900, grad_norm=3.637636661529541, loss=2.5356662273406982
I0307 17:04:27.100010 140237686679296 logging_writer.py:48] [98000] global_step=98000, grad_norm=3.4055635929107666, loss=2.4743919372558594
I0307 17:06:17.833694 140237678286592 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.124743938446045, loss=2.5004751682281494
I0307 17:09:07.107890 140237686679296 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.951580762863159, loss=2.580012083053589
I0307 17:11:22.731618 140393707492544 spec.py:321] Evaluating on the training split.
I0307 17:11:32.708575 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 17:11:55.845760 140393707492544 spec.py:349] Evaluating on the test split.
I0307 17:11:57.634954 140393707492544 submission_runner.py:469] Time since start: 48427.52s, 	Step: 98277, 	{'train/accuracy': 0.7464524507522583, 'train/loss': 1.0883313417434692, 'validation/accuracy': 0.6632399559020996, 'validation/loss': 1.4610949754714966, 'validation/num_examples': 50000, 'test/accuracy': 0.5336000323295593, 'test/loss': 2.124410629272461, 'test/num_examples': 10000, 'score': 44944.380278110504, 'total_duration': 48427.51564669609, 'accumulated_submission_time': 44944.380278110504, 'accumulated_eval_time': 3465.4244379997253, 'accumulated_logging_time': 7.218156576156616}
I0307 17:11:57.677137 140237678286592 logging_writer.py:48] [98277] accumulated_eval_time=3465.42, accumulated_logging_time=7.21816, accumulated_submission_time=44944.4, global_step=98277, preemption_count=0, score=44944.4, test/accuracy=0.5336, test/loss=2.12441, test/num_examples=10000, total_duration=48427.5, train/accuracy=0.746452, train/loss=1.08833, validation/accuracy=0.66324, validation/loss=1.46109, validation/num_examples=50000
I0307 17:12:29.869130 140237686679296 logging_writer.py:48] [98300] global_step=98300, grad_norm=3.777167797088623, loss=2.508800506591797
I0307 17:17:30.807581 140237678286592 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.7055866718292236, loss=2.561674118041992
I0307 17:18:53.849626 140237686679296 logging_writer.py:48] [98500] global_step=98500, grad_norm=3.8316597938537598, loss=2.4311814308166504
I0307 17:20:29.211340 140393707492544 spec.py:321] Evaluating on the training split.
I0307 17:20:39.498973 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 17:21:04.357590 140393707492544 spec.py:349] Evaluating on the test split.
I0307 17:21:06.118057 140393707492544 submission_runner.py:469] Time since start: 48976.00s, 	Step: 98574, 	{'train/accuracy': 0.7293327450752258, 'train/loss': 1.1673786640167236, 'validation/accuracy': 0.6615399718284607, 'validation/loss': 1.4706826210021973, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.150257110595703, 'test/num_examples': 10000, 'score': 45455.86292171478, 'total_duration': 48975.998775959015, 'accumulated_submission_time': 45455.86292171478, 'accumulated_eval_time': 3502.3311240673065, 'accumulated_logging_time': 7.278835296630859}
I0307 17:21:06.140352 140237678286592 logging_writer.py:48] [98574] accumulated_eval_time=3502.33, accumulated_logging_time=7.27884, accumulated_submission_time=45455.9, global_step=98574, preemption_count=0, score=45455.9, test/accuracy=0.5365, test/loss=2.15026, test/num_examples=10000, total_duration=48976, train/accuracy=0.729333, train/loss=1.16738, validation/accuracy=0.66154, validation/loss=1.47068, validation/num_examples=50000
I0307 17:22:41.033396 140237686679296 logging_writer.py:48] [98600] global_step=98600, grad_norm=4.1864166259765625, loss=2.5332961082458496
I0307 17:29:39.553444 140393707492544 spec.py:321] Evaluating on the training split.
I0307 17:29:50.252972 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 17:30:14.813978 140393707492544 spec.py:349] Evaluating on the test split.
I0307 17:30:16.590672 140393707492544 submission_runner.py:469] Time since start: 49526.47s, 	Step: 98698, 	{'train/accuracy': 0.7310666441917419, 'train/loss': 1.1544603109359741, 'validation/accuracy': 0.6657599806785583, 'validation/loss': 1.452366828918457, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.1250152587890625, 'test/num_examples': 10000, 'score': 45969.2556245327, 'total_duration': 49526.471385002136, 'accumulated_submission_time': 45969.2556245327, 'accumulated_eval_time': 3539.368305206299, 'accumulated_logging_time': 7.309299468994141}
I0307 17:30:16.614222 140237678286592 logging_writer.py:48] [98698] accumulated_eval_time=3539.37, accumulated_logging_time=7.3093, accumulated_submission_time=45969.3, global_step=98698, preemption_count=0, score=45969.3, test/accuracy=0.5365, test/loss=2.12502, test/num_examples=10000, total_duration=49526.5, train/accuracy=0.731067, train/loss=1.15446, validation/accuracy=0.66576, validation/loss=1.45237, validation/num_examples=50000
I0307 17:30:17.765114 140237686679296 logging_writer.py:48] [98700] global_step=98700, grad_norm=3.730163335800171, loss=2.5425379276275635
I0307 17:34:05.814988 140237678286592 logging_writer.py:48] [98800] global_step=98800, grad_norm=3.7152726650238037, loss=2.4517343044281006
I0307 17:35:45.282039 140237686679296 logging_writer.py:48] [98900] global_step=98900, grad_norm=3.7117879390716553, loss=2.5047717094421387
I0307 17:37:55.158622 140237678286592 logging_writer.py:48] [99000] global_step=99000, grad_norm=3.7328896522521973, loss=2.5255589485168457
I0307 17:38:46.607824 140393707492544 spec.py:321] Evaluating on the training split.
I0307 17:38:57.156366 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 17:39:17.152880 140393707492544 spec.py:349] Evaluating on the test split.
I0307 17:39:18.953803 140393707492544 submission_runner.py:469] Time since start: 50068.83s, 	Step: 99034, 	{'train/accuracy': 0.7171157598495483, 'train/loss': 1.2178411483764648, 'validation/accuracy': 0.6614399552345276, 'validation/loss': 1.466336965560913, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.1209664344787598, 'test/num_examples': 10000, 'score': 46479.20488524437, 'total_duration': 50068.834535360336, 'accumulated_submission_time': 46479.20488524437, 'accumulated_eval_time': 3571.7142674922943, 'accumulated_logging_time': 7.3412628173828125}
I0307 17:39:19.003658 140237686679296 logging_writer.py:48] [99034] accumulated_eval_time=3571.71, accumulated_logging_time=7.34126, accumulated_submission_time=46479.2, global_step=99034, preemption_count=0, score=46479.2, test/accuracy=0.539, test/loss=2.12097, test/num_examples=10000, total_duration=50068.8, train/accuracy=0.717116, train/loss=1.21784, validation/accuracy=0.66144, validation/loss=1.46634, validation/num_examples=50000
I0307 17:43:02.979831 140237678286592 logging_writer.py:48] [99100] global_step=99100, grad_norm=3.889556407928467, loss=2.5194904804229736
I0307 17:47:51.264608 140393707492544 spec.py:321] Evaluating on the training split.
I0307 17:48:01.467473 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 17:48:23.189925 140393707492544 spec.py:349] Evaluating on the test split.
I0307 17:48:24.964433 140393707492544 submission_runner.py:469] Time since start: 50614.85s, 	Step: 99168, 	{'train/accuracy': 0.7145846486091614, 'train/loss': 1.2254630327224731, 'validation/accuracy': 0.6559399962425232, 'validation/loss': 1.487714171409607, 'validation/num_examples': 50000, 'test/accuracy': 0.5264000296592712, 'test/loss': 2.176237106323242, 'test/num_examples': 10000, 'score': 46991.432193279266, 'total_duration': 50614.84516477585, 'accumulated_submission_time': 46991.432193279266, 'accumulated_eval_time': 3605.4140617847443, 'accumulated_logging_time': 7.410963535308838}
I0307 17:48:24.985381 140237686679296 logging_writer.py:48] [99168] accumulated_eval_time=3605.41, accumulated_logging_time=7.41096, accumulated_submission_time=46991.4, global_step=99168, preemption_count=0, score=46991.4, test/accuracy=0.5264, test/loss=2.17624, test/num_examples=10000, total_duration=50614.8, train/accuracy=0.714585, train/loss=1.22546, validation/accuracy=0.65594, validation/loss=1.48771, validation/num_examples=50000
I0307 17:49:06.916267 140237678286592 logging_writer.py:48] [99200] global_step=99200, grad_norm=3.8436410427093506, loss=2.5080692768096924
I0307 17:55:37.347544 140237686679296 logging_writer.py:48] [99300] global_step=99300, grad_norm=3.9090564250946045, loss=2.570399761199951
I0307 17:56:55.330600 140393707492544 spec.py:321] Evaluating on the training split.
I0307 17:57:05.688545 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 17:57:25.324529 140393707492544 spec.py:349] Evaluating on the test split.
I0307 17:57:27.148922 140393707492544 submission_runner.py:469] Time since start: 51157.03s, 	Step: 99319, 	{'train/accuracy': 0.7199457883834839, 'train/loss': 1.200421690940857, 'validation/accuracy': 0.6664800047874451, 'validation/loss': 1.4445267915725708, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.1042182445526123, 'test/num_examples': 10000, 'score': 47501.75345444679, 'total_duration': 51157.02963376045, 'accumulated_submission_time': 47501.75345444679, 'accumulated_eval_time': 3637.2323274612427, 'accumulated_logging_time': 7.440806150436401}
I0307 17:57:27.170423 140237678286592 logging_writer.py:48] [99319] accumulated_eval_time=3637.23, accumulated_logging_time=7.44081, accumulated_submission_time=47501.8, global_step=99319, preemption_count=0, score=47501.8, test/accuracy=0.5402, test/loss=2.10422, test/num_examples=10000, total_duration=51157, train/accuracy=0.719946, train/loss=1.20042, validation/accuracy=0.66648, validation/loss=1.44453, validation/num_examples=50000
I0307 18:00:43.461860 140237686679296 logging_writer.py:48] [99400] global_step=99400, grad_norm=3.9456520080566406, loss=2.5606369972229004
I0307 18:04:21.168458 140237678286592 logging_writer.py:48] [99500] global_step=99500, grad_norm=3.4703845977783203, loss=2.4870173931121826
I0307 18:05:57.200248 140393707492544 spec.py:321] Evaluating on the training split.
I0307 18:06:07.452377 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 18:06:27.624075 140393707492544 spec.py:349] Evaluating on the test split.
I0307 18:06:29.404778 140393707492544 submission_runner.py:469] Time since start: 51699.29s, 	Step: 99552, 	{'train/accuracy': 0.7219586968421936, 'train/loss': 1.1822550296783447, 'validation/accuracy': 0.6655399799346924, 'validation/loss': 1.4443949460983276, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.0771679878234863, 'test/num_examples': 10000, 'score': 48011.75184464455, 'total_duration': 51699.28551030159, 'accumulated_submission_time': 48011.75184464455, 'accumulated_eval_time': 3669.436831474304, 'accumulated_logging_time': 7.47026515007019}
I0307 18:06:29.425036 140237686679296 logging_writer.py:48] [99552] accumulated_eval_time=3669.44, accumulated_logging_time=7.47027, accumulated_submission_time=48011.8, global_step=99552, preemption_count=0, score=48011.8, test/accuracy=0.5445, test/loss=2.07717, test/num_examples=10000, total_duration=51699.3, train/accuracy=0.721959, train/loss=1.18226, validation/accuracy=0.66554, validation/loss=1.44439, validation/num_examples=50000
I0307 18:07:24.715866 140237678286592 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.914242744445801, loss=2.4643924236297607
I0307 18:08:53.629721 140237686679296 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.005997180938721, loss=2.484581232070923
I0307 18:10:16.970853 140237678286592 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.8027377128601074, loss=2.469899892807007
I0307 18:13:46.273139 140237686679296 logging_writer.py:48] [99900] global_step=99900, grad_norm=3.7432243824005127, loss=2.388036012649536
I0307 18:15:01.366764 140393707492544 spec.py:321] Evaluating on the training split.
I0307 18:15:12.022979 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 18:15:36.082462 140393707492544 spec.py:349] Evaluating on the test split.
I0307 18:15:37.874855 140393707492544 submission_runner.py:469] Time since start: 52247.76s, 	Step: 99927, 	{'train/accuracy': 0.7403539419174194, 'train/loss': 1.1030845642089844, 'validation/accuracy': 0.6631999611854553, 'validation/loss': 1.4522411823272705, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.1274261474609375, 'test/num_examples': 10000, 'score': 48523.62880039215, 'total_duration': 52247.7555706501, 'accumulated_submission_time': 48523.62880039215, 'accumulated_eval_time': 3705.9448721408844, 'accumulated_logging_time': 7.514772415161133}
I0307 18:15:37.944831 140237678286592 logging_writer.py:48] [99927] accumulated_eval_time=3705.94, accumulated_logging_time=7.51477, accumulated_submission_time=48523.6, global_step=99927, preemption_count=0, score=48523.6, test/accuracy=0.5338, test/loss=2.12743, test/num_examples=10000, total_duration=52247.8, train/accuracy=0.740354, train/loss=1.10308, validation/accuracy=0.6632, validation/loss=1.45224, validation/num_examples=50000
I0307 18:18:42.031105 140237686679296 logging_writer.py:48] [100000] global_step=100000, grad_norm=3.8839850425720215, loss=2.513078451156616
2025-03-07 18:19:44.419393: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:20:05.502400 140237678286592 logging_writer.py:48] [100100] global_step=100100, grad_norm=3.8756396770477295, loss=2.6797845363616943
I0307 18:21:22.326806 140237686679296 logging_writer.py:48] [100200] global_step=100200, grad_norm=3.9042205810546875, loss=2.461573600769043
I0307 18:24:08.761349 140393707492544 spec.py:321] Evaluating on the training split.
I0307 18:24:18.735046 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 18:24:41.314742 140393707492544 spec.py:349] Evaluating on the test split.
I0307 18:24:43.125535 140393707492544 submission_runner.py:469] Time since start: 52793.01s, 	Step: 100293, 	{'train/accuracy': 0.7253866195678711, 'train/loss': 1.1838643550872803, 'validation/accuracy': 0.6637799739837646, 'validation/loss': 1.45968759059906, 'validation/num_examples': 50000, 'test/accuracy': 0.5396000146865845, 'test/loss': 2.1015052795410156, 'test/num_examples': 10000, 'score': 49034.39515399933, 'total_duration': 52793.00626826286, 'accumulated_submission_time': 49034.39515399933, 'accumulated_eval_time': 3740.3090307712555, 'accumulated_logging_time': 7.594053268432617}
I0307 18:24:43.206539 140237678286592 logging_writer.py:48] [100293] accumulated_eval_time=3740.31, accumulated_logging_time=7.59405, accumulated_submission_time=49034.4, global_step=100293, preemption_count=0, score=49034.4, test/accuracy=0.5396, test/loss=2.10151, test/num_examples=10000, total_duration=52793, train/accuracy=0.725387, train/loss=1.18386, validation/accuracy=0.66378, validation/loss=1.45969, validation/num_examples=50000
I0307 18:24:46.333145 140237686679296 logging_writer.py:48] [100300] global_step=100300, grad_norm=3.85233473777771, loss=2.5306522846221924
I0307 18:26:31.278509 140237678286592 logging_writer.py:48] [100400] global_step=100400, grad_norm=3.9095470905303955, loss=2.4493305683135986
I0307 18:28:22.239379 140237686679296 logging_writer.py:48] [100500] global_step=100500, grad_norm=3.791958808898926, loss=2.4409730434417725
I0307 18:30:12.904635 140237678286592 logging_writer.py:48] [100600] global_step=100600, grad_norm=3.825371026992798, loss=2.4352383613586426
I0307 18:33:15.696351 140393707492544 spec.py:321] Evaluating on the training split.
I0307 18:33:25.499157 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 18:33:50.114340 140393707492544 spec.py:349] Evaluating on the test split.
I0307 18:33:51.878050 140393707492544 submission_runner.py:469] Time since start: 53341.76s, 	Step: 100680, 	{'train/accuracy': 0.722078263759613, 'train/loss': 1.1988540887832642, 'validation/accuracy': 0.6609599590301514, 'validation/loss': 1.4627281427383423, 'validation/num_examples': 50000, 'test/accuracy': 0.5355000495910645, 'test/loss': 2.1217551231384277, 'test/num_examples': 10000, 'score': 49546.82700634003, 'total_duration': 53341.758776426315, 'accumulated_submission_time': 49546.82700634003, 'accumulated_eval_time': 3776.4906940460205, 'accumulated_logging_time': 7.692488431930542}
I0307 18:33:51.973068 140237686679296 logging_writer.py:48] [100680] accumulated_eval_time=3776.49, accumulated_logging_time=7.69249, accumulated_submission_time=49546.8, global_step=100680, preemption_count=0, score=49546.8, test/accuracy=0.5355, test/loss=2.12176, test/num_examples=10000, total_duration=53341.8, train/accuracy=0.722078, train/loss=1.19885, validation/accuracy=0.66096, validation/loss=1.46273, validation/num_examples=50000
I0307 18:35:01.350747 140237678286592 logging_writer.py:48] [100700] global_step=100700, grad_norm=3.8355841636657715, loss=2.5863959789276123
I0307 18:39:18.964569 140237686679296 logging_writer.py:48] [100800] global_step=100800, grad_norm=3.5509986877441406, loss=2.5357203483581543
I0307 18:42:24.045869 140393707492544 spec.py:321] Evaluating on the training split.
I0307 18:42:34.693913 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 18:42:54.613350 140393707492544 spec.py:349] Evaluating on the test split.
I0307 18:42:56.379568 140393707492544 submission_runner.py:469] Time since start: 53886.26s, 	Step: 100844, 	{'train/accuracy': 0.7101601958274841, 'train/loss': 1.2198963165283203, 'validation/accuracy': 0.6584799885749817, 'validation/loss': 1.4676671028137207, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.120868444442749, 'test/num_examples': 10000, 'score': 50058.8605761528, 'total_duration': 53886.2602994442, 'accumulated_submission_time': 50058.8605761528, 'accumulated_eval_time': 3808.824368953705, 'accumulated_logging_time': 7.808620452880859}
I0307 18:42:56.399921 140237678286592 logging_writer.py:48] [100844] accumulated_eval_time=3808.82, accumulated_logging_time=7.80862, accumulated_submission_time=50058.9, global_step=100844, preemption_count=0, score=50058.9, test/accuracy=0.5343, test/loss=2.12087, test/num_examples=10000, total_duration=53886.3, train/accuracy=0.71016, train/loss=1.2199, validation/accuracy=0.65848, validation/loss=1.46767, validation/num_examples=50000
I0307 18:43:47.644970 140237686679296 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.0111517906188965, loss=2.464097738265991
I0307 18:45:05.227669 140237678286592 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.078813552856445, loss=2.5361878871917725
I0307 18:46:07.952169 140237686679296 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.132978916168213, loss=2.5586585998535156
I0307 18:47:10.590188 140237678286592 logging_writer.py:48] [101200] global_step=101200, grad_norm=3.8823258876800537, loss=2.394045829772949
I0307 18:49:11.659547 140237686679296 logging_writer.py:48] [101300] global_step=101300, grad_norm=3.4088025093078613, loss=2.457393169403076
I0307 18:51:27.525053 140393707492544 spec.py:321] Evaluating on the training split.
I0307 18:51:37.867459 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 18:51:57.025333 140393707492544 spec.py:349] Evaluating on the test split.
I0307 18:51:58.817378 140393707492544 submission_runner.py:469] Time since start: 54428.70s, 	Step: 101393, 	{'train/accuracy': 0.7495017647743225, 'train/loss': 1.0902628898620605, 'validation/accuracy': 0.6667199730873108, 'validation/loss': 1.4508531093597412, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.1181185245513916, 'test/num_examples': 10000, 'score': 50569.91860842705, 'total_duration': 54428.698112010956, 'accumulated_submission_time': 50569.91860842705, 'accumulated_eval_time': 3840.116666316986, 'accumulated_logging_time': 7.836751461029053}
I0307 18:51:58.851944 140237678286592 logging_writer.py:48] [101393] accumulated_eval_time=3840.12, accumulated_logging_time=7.83675, accumulated_submission_time=50569.9, global_step=101393, preemption_count=0, score=50569.9, test/accuracy=0.5388, test/loss=2.11812, test/num_examples=10000, total_duration=54428.7, train/accuracy=0.749502, train/loss=1.09026, validation/accuracy=0.66672, validation/loss=1.45085, validation/num_examples=50000
I0307 18:52:01.992092 140237686679296 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.575465202331543, loss=2.47291898727417
I0307 18:54:20.526058 140237678286592 logging_writer.py:48] [101500] global_step=101500, grad_norm=3.823368549346924, loss=2.4914486408233643
I0307 18:56:47.453650 140237686679296 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.251538276672363, loss=2.519695520401001
I0307 18:59:14.015297 140237678286592 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.049383163452148, loss=2.438110828399658
I0307 19:00:30.087641 140393707492544 spec.py:321] Evaluating on the training split.
I0307 19:00:40.793656 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 19:01:01.540334 140393707492544 spec.py:349] Evaluating on the test split.
I0307 19:01:03.298657 140393707492544 submission_runner.py:469] Time since start: 54973.18s, 	Step: 101753, 	{'train/accuracy': 0.7218191623687744, 'train/loss': 1.2033320665359497, 'validation/accuracy': 0.6624199748039246, 'validation/loss': 1.4756213426589966, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.1381893157958984, 'test/num_examples': 10000, 'score': 51081.10740876198, 'total_duration': 54973.17938923836, 'accumulated_submission_time': 51081.10740876198, 'accumulated_eval_time': 3873.3276517391205, 'accumulated_logging_time': 7.879081964492798}
I0307 19:01:03.340523 140237686679296 logging_writer.py:48] [101753] accumulated_eval_time=3873.33, accumulated_logging_time=7.87908, accumulated_submission_time=51081.1, global_step=101753, preemption_count=0, score=51081.1, test/accuracy=0.5312, test/loss=2.13819, test/num_examples=10000, total_duration=54973.2, train/accuracy=0.721819, train/loss=1.20333, validation/accuracy=0.66242, validation/loss=1.47562, validation/num_examples=50000
I0307 19:01:59.724314 140237678286592 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.217933654785156, loss=2.60798978805542
I0307 19:06:19.603123 140237686679296 logging_writer.py:48] [101900] global_step=101900, grad_norm=3.9466686248779297, loss=2.5421924591064453
I0307 19:09:02.314893 140237678286592 logging_writer.py:48] [102000] global_step=102000, grad_norm=3.9088387489318848, loss=2.510599136352539
I0307 19:09:34.418381 140393707492544 spec.py:321] Evaluating on the training split.
I0307 19:09:44.720974 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 19:10:05.475011 140393707492544 spec.py:349] Evaluating on the test split.
I0307 19:10:07.252270 140393707492544 submission_runner.py:469] Time since start: 55517.13s, 	Step: 102015, 	{'train/accuracy': 0.7227160334587097, 'train/loss': 1.2286900281906128, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.5014077425003052, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.162128210067749, 'test/num_examples': 10000, 'score': 51592.13065266609, 'total_duration': 55517.132984638214, 'accumulated_submission_time': 51592.13065266609, 'accumulated_eval_time': 3906.1614899635315, 'accumulated_logging_time': 7.946697950363159}
I0307 19:10:07.274914 140237686679296 logging_writer.py:48] [102015] accumulated_eval_time=3906.16, accumulated_logging_time=7.9467, accumulated_submission_time=51592.1, global_step=102015, preemption_count=0, score=51592.1, test/accuracy=0.5372, test/loss=2.16213, test/num_examples=10000, total_duration=55517.1, train/accuracy=0.722716, train/loss=1.22869, validation/accuracy=0.66106, validation/loss=1.50141, validation/num_examples=50000
I0307 19:12:39.697390 140237678286592 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.115320205688477, loss=2.520433187484741
I0307 19:14:29.689121 140237686679296 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.0726823806762695, loss=2.4766316413879395
I0307 19:15:55.305214 140237678286592 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.226083755493164, loss=2.5422840118408203
I0307 19:17:19.937977 140237686679296 logging_writer.py:48] [102400] global_step=102400, grad_norm=3.858163595199585, loss=2.482367515563965
I0307 19:18:39.209492 140393707492544 spec.py:321] Evaluating on the training split.
I0307 19:18:49.242402 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 19:19:12.497288 140393707492544 spec.py:349] Evaluating on the test split.
I0307 19:19:14.328028 140393707492544 submission_runner.py:469] Time since start: 56064.21s, 	Step: 102485, 	{'train/accuracy': 0.7192482352256775, 'train/loss': 1.2020204067230225, 'validation/accuracy': 0.6642599701881409, 'validation/loss': 1.4513390064239502, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.098752737045288, 'test/num_examples': 10000, 'score': 52104.00662469864, 'total_duration': 56064.208756923676, 'accumulated_submission_time': 52104.00662469864, 'accumulated_eval_time': 3941.2799973487854, 'accumulated_logging_time': 7.978222846984863}
I0307 19:19:14.373057 140237678286592 logging_writer.py:48] [102485] accumulated_eval_time=3941.28, accumulated_logging_time=7.97822, accumulated_submission_time=52104, global_step=102485, preemption_count=0, score=52104, test/accuracy=0.5417, test/loss=2.09875, test/num_examples=10000, total_duration=56064.2, train/accuracy=0.719248, train/loss=1.20202, validation/accuracy=0.66426, validation/loss=1.45134, validation/num_examples=50000
I0307 19:19:27.967880 140237686679296 logging_writer.py:48] [102500] global_step=102500, grad_norm=3.6788692474365234, loss=2.611746072769165
2025-03-07 19:22:16.344975: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:23:10.208715 140237678286592 logging_writer.py:48] [102600] global_step=102600, grad_norm=3.834225654602051, loss=2.472029685974121
I0307 19:26:59.098457 140237686679296 logging_writer.py:48] [102700] global_step=102700, grad_norm=3.867008924484253, loss=2.499295949935913
I0307 19:27:46.039455 140393707492544 spec.py:321] Evaluating on the training split.
I0307 19:27:56.045551 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 19:28:18.052079 140393707492544 spec.py:349] Evaluating on the test split.
I0307 19:28:19.836792 140393707492544 submission_runner.py:469] Time since start: 56609.72s, 	Step: 102728, 	{'train/accuracy': 0.7487045526504517, 'train/loss': 1.0697017908096313, 'validation/accuracy': 0.6563999652862549, 'validation/loss': 1.480522632598877, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.160945415496826, 'test/num_examples': 10000, 'score': 52615.63793873787, 'total_duration': 56609.717522382736, 'accumulated_submission_time': 52615.63793873787, 'accumulated_eval_time': 3975.0773010253906, 'accumulated_logging_time': 8.031734943389893}
I0307 19:28:19.858401 140237678286592 logging_writer.py:48] [102728] accumulated_eval_time=3975.08, accumulated_logging_time=8.03173, accumulated_submission_time=52615.6, global_step=102728, preemption_count=0, score=52615.6, test/accuracy=0.5281, test/loss=2.16095, test/num_examples=10000, total_duration=56609.7, train/accuracy=0.748705, train/loss=1.0697, validation/accuracy=0.6564, validation/loss=1.48052, validation/num_examples=50000
I0307 19:30:36.502916 140237686679296 logging_writer.py:48] [102800] global_step=102800, grad_norm=3.91648268699646, loss=2.490854263305664
I0307 19:34:12.516374 140237678286592 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.431552886962891, loss=2.5347163677215576
I0307 19:36:50.886965 140393707492544 spec.py:321] Evaluating on the training split.
I0307 19:37:01.492333 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 19:37:20.475060 140393707492544 spec.py:349] Evaluating on the test split.
I0307 19:37:22.282682 140393707492544 submission_runner.py:469] Time since start: 57152.16s, 	Step: 102989, 	{'train/accuracy': 0.737723171710968, 'train/loss': 1.1194742918014526, 'validation/accuracy': 0.6657800078392029, 'validation/loss': 1.4356077909469604, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.073362112045288, 'test/num_examples': 10000, 'score': 53126.63229942322, 'total_duration': 57152.16341352463, 'accumulated_submission_time': 53126.63229942322, 'accumulated_eval_time': 4006.4729900360107, 'accumulated_logging_time': 8.061196088790894}
I0307 19:37:22.303847 140237686679296 logging_writer.py:48] [102989] accumulated_eval_time=4006.47, accumulated_logging_time=8.0612, accumulated_submission_time=53126.6, global_step=102989, preemption_count=0, score=53126.6, test/accuracy=0.5454, test/loss=2.07336, test/num_examples=10000, total_duration=57152.2, train/accuracy=0.737723, train/loss=1.11947, validation/accuracy=0.66578, validation/loss=1.43561, validation/num_examples=50000
I0307 19:37:27.170701 140237678286592 logging_writer.py:48] [103000] global_step=103000, grad_norm=3.8744847774505615, loss=2.497209072113037
I0307 19:38:55.020317 140237686679296 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.198435306549072, loss=2.4734792709350586
I0307 19:40:26.921806 140237678286592 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.163356304168701, loss=2.5728259086608887
I0307 19:41:59.004308 140237686679296 logging_writer.py:48] [103300] global_step=103300, grad_norm=3.677706003189087, loss=2.484649658203125
I0307 19:43:30.247633 140237678286592 logging_writer.py:48] [103400] global_step=103400, grad_norm=3.9628255367279053, loss=2.4525904655456543
I0307 19:45:53.554097 140393707492544 spec.py:321] Evaluating on the training split.
I0307 19:46:03.572360 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 19:46:28.284410 140393707492544 spec.py:349] Evaluating on the test split.
I0307 19:46:30.060699 140393707492544 submission_runner.py:469] Time since start: 57699.94s, 	Step: 103482, 	{'train/accuracy': 0.7254862785339355, 'train/loss': 1.1640537977218628, 'validation/accuracy': 0.6676200032234192, 'validation/loss': 1.4284816980361938, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.080355405807495, 'test/num_examples': 10000, 'score': 53637.82115983963, 'total_duration': 57699.94142770767, 'accumulated_submission_time': 53637.82115983963, 'accumulated_eval_time': 4042.9795672893524, 'accumulated_logging_time': 8.09032654762268}
I0307 19:46:30.106289 140237686679296 logging_writer.py:48] [103482] accumulated_eval_time=4042.98, accumulated_logging_time=8.09033, accumulated_submission_time=53637.8, global_step=103482, preemption_count=0, score=53637.8, test/accuracy=0.5458, test/loss=2.08036, test/num_examples=10000, total_duration=57699.9, train/accuracy=0.725486, train/loss=1.16405, validation/accuracy=0.66762, validation/loss=1.42848, validation/num_examples=50000
I0307 19:47:30.894230 140237678286592 logging_writer.py:48] [103500] global_step=103500, grad_norm=3.4439361095428467, loss=2.5168116092681885
I0307 19:54:41.301083 140237686679296 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.181036472320557, loss=2.5729434490203857
I0307 19:55:02.870517 140393707492544 spec.py:321] Evaluating on the training split.
I0307 19:55:12.902582 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 19:55:40.657292 140393707492544 spec.py:349] Evaluating on the test split.
I0307 19:55:42.423490 140393707492544 submission_runner.py:469] Time since start: 58252.30s, 	Step: 103606, 	{'train/accuracy': 0.7208625674247742, 'train/loss': 1.1963168382644653, 'validation/accuracy': 0.662559986114502, 'validation/loss': 1.4623286724090576, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.100508213043213, 'test/num_examples': 10000, 'score': 54150.564057826996, 'total_duration': 58252.30419373512, 'accumulated_submission_time': 54150.564057826996, 'accumulated_eval_time': 4082.5324823856354, 'accumulated_logging_time': 8.1444571018219}
I0307 19:55:42.446268 140237678286592 logging_writer.py:48] [103606] accumulated_eval_time=4082.53, accumulated_logging_time=8.14446, accumulated_submission_time=54150.6, global_step=103606, preemption_count=0, score=54150.6, test/accuracy=0.544, test/loss=2.10051, test/num_examples=10000, total_duration=58252.3, train/accuracy=0.720863, train/loss=1.19632, validation/accuracy=0.66256, validation/loss=1.46233, validation/num_examples=50000
I0307 20:02:10.341496 140237686679296 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.003283977508545, loss=2.4934334754943848
I0307 20:04:15.063739 140393707492544 spec.py:321] Evaluating on the training split.
I0307 20:04:25.057950 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 20:04:46.982994 140393707492544 spec.py:349] Evaluating on the test split.
I0307 20:04:48.796484 140393707492544 submission_runner.py:469] Time since start: 58798.68s, 	Step: 103730, 	{'train/accuracy': 0.7204639315605164, 'train/loss': 1.2202650308609009, 'validation/accuracy': 0.6645599603652954, 'validation/loss': 1.4677965641021729, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.0924293994903564, 'test/num_examples': 10000, 'score': 54663.0922563076, 'total_duration': 58798.67719864845, 'accumulated_submission_time': 54663.0922563076, 'accumulated_eval_time': 4116.265195131302, 'accumulated_logging_time': 8.2440664768219}
I0307 20:04:48.817867 140237678286592 logging_writer.py:48] [103730] accumulated_eval_time=4116.27, accumulated_logging_time=8.24407, accumulated_submission_time=54663.1, global_step=103730, preemption_count=0, score=54663.1, test/accuracy=0.5424, test/loss=2.09243, test/num_examples=10000, total_duration=58798.7, train/accuracy=0.720464, train/loss=1.22027, validation/accuracy=0.66456, validation/loss=1.4678, validation/num_examples=50000
I0307 20:06:16.720628 140237686679296 logging_writer.py:48] [103800] global_step=103800, grad_norm=3.950716972351074, loss=2.4372267723083496
I0307 20:08:43.396320 140237678286592 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.054391384124756, loss=2.4976577758789062
I0307 20:11:08.944001 140237686679296 logging_writer.py:48] [104000] global_step=104000, grad_norm=3.8216447830200195, loss=2.5319371223449707
I0307 20:13:20.295170 140393707492544 spec.py:321] Evaluating on the training split.
I0307 20:13:31.198125 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 20:13:53.786483 140393707492544 spec.py:349] Evaluating on the test split.
I0307 20:13:55.565331 140393707492544 submission_runner.py:469] Time since start: 59345.45s, 	Step: 104079, 	{'train/accuracy': 0.7202447056770325, 'train/loss': 1.2129395008087158, 'validation/accuracy': 0.6656399965286255, 'validation/loss': 1.457554817199707, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.112750768661499, 'test/num_examples': 10000, 'score': 55174.524755477905, 'total_duration': 59345.44605708122, 'accumulated_submission_time': 55174.524755477905, 'accumulated_eval_time': 4151.535323381424, 'accumulated_logging_time': 8.273392915725708}
I0307 20:13:55.623368 140237678286592 logging_writer.py:48] [104079] accumulated_eval_time=4151.54, accumulated_logging_time=8.27339, accumulated_submission_time=55174.5, global_step=104079, preemption_count=0, score=55174.5, test/accuracy=0.5409, test/loss=2.11275, test/num_examples=10000, total_duration=59345.4, train/accuracy=0.720245, train/loss=1.21294, validation/accuracy=0.66564, validation/loss=1.45755, validation/num_examples=50000
I0307 20:14:21.952092 140237686679296 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.550385475158691, loss=2.50189471244812
I0307 20:16:47.705477 140237678286592 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.422317028045654, loss=2.4613754749298096
I0307 20:19:12.549270 140237686679296 logging_writer.py:48] [104300] global_step=104300, grad_norm=3.66683292388916, loss=2.4231793880462646
I0307 20:21:38.131133 140237678286592 logging_writer.py:48] [104400] global_step=104400, grad_norm=3.9567534923553467, loss=2.535240411758423
I0307 20:22:25.980479 140393707492544 spec.py:321] Evaluating on the training split.
I0307 20:22:36.048299 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 20:22:57.964279 140393707492544 spec.py:349] Evaluating on the test split.
I0307 20:22:59.717625 140393707492544 submission_runner.py:469] Time since start: 59889.60s, 	Step: 104434, 	{'train/accuracy': 0.7488042116165161, 'train/loss': 1.0662858486175537, 'validation/accuracy': 0.6728799939155579, 'validation/loss': 1.408199667930603, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.04132342338562, 'test/num_examples': 10000, 'score': 55684.83670759201, 'total_duration': 59889.59834122658, 'accumulated_submission_time': 55684.83670759201, 'accumulated_eval_time': 4185.27242064476, 'accumulated_logging_time': 8.340363502502441}
I0307 20:22:59.809915 140237686679296 logging_writer.py:48] [104434] accumulated_eval_time=4185.27, accumulated_logging_time=8.34036, accumulated_submission_time=55684.8, global_step=104434, preemption_count=0, score=55684.8, test/accuracy=0.5451, test/loss=2.04132, test/num_examples=10000, total_duration=59889.6, train/accuracy=0.748804, train/loss=1.06629, validation/accuracy=0.67288, validation/loss=1.4082, validation/num_examples=50000
I0307 20:24:22.735847 140237678286592 logging_writer.py:48] [104500] global_step=104500, grad_norm=4.4669108390808105, loss=2.513103485107422
I0307 20:26:47.544635 140237686679296 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.120935440063477, loss=2.445314884185791
I0307 20:28:50.643254 140237678286592 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.065998554229736, loss=2.5207273960113525
I0307 20:30:06.502025 140237686679296 logging_writer.py:48] [104800] global_step=104800, grad_norm=3.5934696197509766, loss=2.3776588439941406
I0307 20:31:23.725417 140237678286592 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.201284408569336, loss=2.53501558303833
I0307 20:31:29.818485 140393707492544 spec.py:321] Evaluating on the training split.
I0307 20:31:40.419589 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 20:32:02.280281 140393707492544 spec.py:349] Evaluating on the test split.
I0307 20:32:04.039215 140393707492544 submission_runner.py:469] Time since start: 60433.92s, 	Step: 104909, 	{'train/accuracy': 0.7283760905265808, 'train/loss': 1.1514848470687866, 'validation/accuracy': 0.6637799739837646, 'validation/loss': 1.4325437545776367, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.0986835956573486, 'test/num_examples': 10000, 'score': 56194.78654503822, 'total_duration': 60433.91994857788, 'accumulated_submission_time': 56194.78654503822, 'accumulated_eval_time': 4219.493124723434, 'accumulated_logging_time': 8.441278219223022}
I0307 20:32:04.076183 140237686679296 logging_writer.py:48] [104909] accumulated_eval_time=4219.49, accumulated_logging_time=8.44128, accumulated_submission_time=56194.8, global_step=104909, preemption_count=0, score=56194.8, test/accuracy=0.5385, test/loss=2.09868, test/num_examples=10000, total_duration=60433.9, train/accuracy=0.728376, train/loss=1.15148, validation/accuracy=0.66378, validation/loss=1.43254, validation/num_examples=50000
I0307 20:33:07.924017 140237678286592 logging_writer.py:48] [105000] global_step=105000, grad_norm=4.572478294372559, loss=2.4607136249542236
I0307 20:34:25.329057 140237686679296 logging_writer.py:48] [105100] global_step=105100, grad_norm=3.573070526123047, loss=2.484622001647949
I0307 20:36:48.102217 140237678286592 logging_writer.py:48] [105200] global_step=105200, grad_norm=3.679715394973755, loss=2.4938392639160156
I0307 20:40:34.859581 140393707492544 spec.py:321] Evaluating on the training split.
I0307 20:40:45.082787 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 20:41:10.693071 140393707492544 spec.py:349] Evaluating on the test split.
I0307 20:41:12.467675 140393707492544 submission_runner.py:469] Time since start: 60982.35s, 	Step: 105254, 	{'train/accuracy': 0.7225565910339355, 'train/loss': 1.1946988105773926, 'validation/accuracy': 0.6688599586486816, 'validation/loss': 1.433799386024475, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.1130459308624268, 'test/num_examples': 10000, 'score': 56705.52609848976, 'total_duration': 60982.34839582443, 'accumulated_submission_time': 56705.52609848976, 'accumulated_eval_time': 4257.101182937622, 'accumulated_logging_time': 8.486390352249146}
I0307 20:41:12.521022 140237686679296 logging_writer.py:48] [105254] accumulated_eval_time=4257.1, accumulated_logging_time=8.48639, accumulated_submission_time=56705.5, global_step=105254, preemption_count=0, score=56705.5, test/accuracy=0.5353, test/loss=2.11305, test/num_examples=10000, total_duration=60982.3, train/accuracy=0.722557, train/loss=1.1947, validation/accuracy=0.66886, validation/loss=1.4338, validation/num_examples=50000
I0307 20:44:12.596872 140237678286592 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.25819730758667, loss=2.5128173828125
I0307 20:49:04.221526 140237686679296 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.170720100402832, loss=2.5728118419647217
I0307 20:49:43.172286 140393707492544 spec.py:321] Evaluating on the training split.
I0307 20:49:53.625584 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 20:50:13.672152 140393707492544 spec.py:349] Evaluating on the test split.
I0307 20:50:15.480041 140393707492544 submission_runner.py:469] Time since start: 61525.36s, 	Step: 105443, 	{'train/accuracy': 0.7262834906578064, 'train/loss': 1.2042301893234253, 'validation/accuracy': 0.6696400046348572, 'validation/loss': 1.4560136795043945, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.113462209701538, 'test/num_examples': 10000, 'score': 57216.15009188652, 'total_duration': 61525.36075305939, 'accumulated_submission_time': 57216.15009188652, 'accumulated_eval_time': 4289.408891439438, 'accumulated_logging_time': 8.547240495681763}
I0307 20:50:15.504597 140237678286592 logging_writer.py:48] [105443] accumulated_eval_time=4289.41, accumulated_logging_time=8.54724, accumulated_submission_time=57216.2, global_step=105443, preemption_count=0, score=57216.2, test/accuracy=0.5407, test/loss=2.11346, test/num_examples=10000, total_duration=61525.4, train/accuracy=0.726283, train/loss=1.20423, validation/accuracy=0.66964, validation/loss=1.45601, validation/num_examples=50000
I0307 20:50:58.857965 140237686679296 logging_writer.py:48] [105500] global_step=105500, grad_norm=3.32468843460083, loss=2.434910535812378
I0307 20:52:31.138435 140237678286592 logging_writer.py:48] [105600] global_step=105600, grad_norm=3.7223267555236816, loss=2.4592010974884033
I0307 20:54:03.531883 140237686679296 logging_writer.py:48] [105700] global_step=105700, grad_norm=3.727952241897583, loss=2.4324746131896973
I0307 20:55:37.256097 140237678286592 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.194421768188477, loss=2.545719861984253
I0307 20:57:09.805551 140237686679296 logging_writer.py:48] [105900] global_step=105900, grad_norm=3.857799530029297, loss=2.512406349182129
I0307 20:58:43.657632 140237678286592 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.496206760406494, loss=2.5052993297576904
I0307 20:58:45.626712 140393707492544 spec.py:321] Evaluating on the training split.
I0307 20:58:56.550597 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 20:59:17.146900 140393707492544 spec.py:349] Evaluating on the test split.
I0307 20:59:18.928944 140393707492544 submission_runner.py:469] Time since start: 62068.81s, 	Step: 106003, 	{'train/accuracy': 0.7354711294174194, 'train/loss': 1.140196681022644, 'validation/accuracy': 0.6699199676513672, 'validation/loss': 1.4432976245880127, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.1022963523864746, 'test/num_examples': 10000, 'score': 57726.20394182205, 'total_duration': 62068.80967712402, 'accumulated_submission_time': 57726.20394182205, 'accumulated_eval_time': 4322.711086988449, 'accumulated_logging_time': 8.580191373825073}
I0307 20:59:19.025082 140237686679296 logging_writer.py:48] [106003] accumulated_eval_time=4322.71, accumulated_logging_time=8.58019, accumulated_submission_time=57726.2, global_step=106003, preemption_count=0, score=57726.2, test/accuracy=0.5416, test/loss=2.1023, test/num_examples=10000, total_duration=62068.8, train/accuracy=0.735471, train/loss=1.1402, validation/accuracy=0.66992, validation/loss=1.4433, validation/num_examples=50000
I0307 21:00:40.949154 140237678286592 logging_writer.py:48] [106100] global_step=106100, grad_norm=3.9795944690704346, loss=2.4848814010620117
I0307 21:02:16.395800 140237686679296 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.158579349517822, loss=2.4780333042144775
I0307 21:03:50.139316 140237678286592 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.268260478973389, loss=2.4915196895599365
I0307 21:05:21.749655 140237686679296 logging_writer.py:48] [106400] global_step=106400, grad_norm=3.854017734527588, loss=2.5445361137390137
I0307 21:06:52.117578 140237678286592 logging_writer.py:48] [106500] global_step=106500, grad_norm=3.658622980117798, loss=2.4311041831970215
I0307 21:07:49.043832 140393707492544 spec.py:321] Evaluating on the training split.
I0307 21:07:59.402972 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 21:08:19.483146 140393707492544 spec.py:349] Evaluating on the test split.
I0307 21:08:21.272466 140393707492544 submission_runner.py:469] Time since start: 62611.15s, 	Step: 106564, 	{'train/accuracy': 0.7276785373687744, 'train/loss': 1.1678117513656616, 'validation/accuracy': 0.6691399812698364, 'validation/loss': 1.4272679090499878, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.067735195159912, 'test/num_examples': 10000, 'score': 58236.15479302406, 'total_duration': 62611.15320158005, 'accumulated_submission_time': 58236.15479302406, 'accumulated_eval_time': 4354.939691781998, 'accumulated_logging_time': 8.68401026725769}
I0307 21:08:21.377184 140237686679296 logging_writer.py:48] [106564] accumulated_eval_time=4354.94, accumulated_logging_time=8.68401, accumulated_submission_time=58236.2, global_step=106564, preemption_count=0, score=58236.2, test/accuracy=0.5475, test/loss=2.06774, test/num_examples=10000, total_duration=62611.2, train/accuracy=0.727679, train/loss=1.16781, validation/accuracy=0.66914, validation/loss=1.42727, validation/num_examples=50000
I0307 21:08:44.257384 140237678286592 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.858497619628906, loss=2.5849077701568604
I0307 21:10:15.473611 140237686679296 logging_writer.py:48] [106700] global_step=106700, grad_norm=3.838007926940918, loss=2.4347567558288574
I0307 21:11:46.319020 140237678286592 logging_writer.py:48] [106800] global_step=106800, grad_norm=4.015160083770752, loss=2.5611743927001953
I0307 21:13:16.766714 140237686679296 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.122817516326904, loss=2.4210307598114014
I0307 21:14:46.516539 140237678286592 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.9564688205718994, loss=2.5540900230407715
I0307 21:16:16.858162 140237686679296 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.044083118438721, loss=2.5286216735839844
I0307 21:16:52.017287 140393707492544 spec.py:321] Evaluating on the training split.
I0307 21:17:02.370012 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 21:17:21.281578 140393707492544 spec.py:349] Evaluating on the test split.
I0307 21:17:23.080474 140393707492544 submission_runner.py:469] Time since start: 63152.96s, 	Step: 107140, 	{'train/accuracy': 0.7466118931770325, 'train/loss': 1.0786799192428589, 'validation/accuracy': 0.6642599701881409, 'validation/loss': 1.4460047483444214, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.1041030883789062, 'test/num_examples': 10000, 'score': 58746.72533559799, 'total_duration': 63152.96120882034, 'accumulated_submission_time': 58746.72533559799, 'accumulated_eval_time': 4386.002843618393, 'accumulated_logging_time': 8.796464204788208}
I0307 21:17:23.122745 140237678286592 logging_writer.py:48] [107140] accumulated_eval_time=4386, accumulated_logging_time=8.79646, accumulated_submission_time=58746.7, global_step=107140, preemption_count=0, score=58746.7, test/accuracy=0.5394, test/loss=2.1041, test/num_examples=10000, total_duration=63153, train/accuracy=0.746612, train/loss=1.07868, validation/accuracy=0.66426, validation/loss=1.446, validation/num_examples=50000
I0307 21:18:08.249088 140237686679296 logging_writer.py:48] [107200] global_step=107200, grad_norm=3.760857582092285, loss=2.503246307373047
I0307 21:19:38.344758 140237678286592 logging_writer.py:48] [107300] global_step=107300, grad_norm=3.7037744522094727, loss=2.4376425743103027
I0307 21:21:08.574834 140237686679296 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.732773303985596, loss=2.438206195831299
I0307 21:22:38.534508 140237678286592 logging_writer.py:48] [107500] global_step=107500, grad_norm=3.7951691150665283, loss=2.4424972534179688
2025-03-07 21:23:51.831957: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:24:10.213913 140237686679296 logging_writer.py:48] [107600] global_step=107600, grad_norm=3.853945016860962, loss=2.4943604469299316
I0307 21:25:40.355568 140237678286592 logging_writer.py:48] [107700] global_step=107700, grad_norm=3.9807302951812744, loss=2.596273422241211
I0307 21:25:53.081499 140393707492544 spec.py:321] Evaluating on the training split.
I0307 21:26:03.342547 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 21:26:24.033736 140393707492544 spec.py:349] Evaluating on the test split.
I0307 21:26:25.821150 140393707492544 submission_runner.py:469] Time since start: 63695.70s, 	Step: 107715, 	{'train/accuracy': 0.7308274507522583, 'train/loss': 1.1577601432800293, 'validation/accuracy': 0.6692599654197693, 'validation/loss': 1.4366854429244995, 'validation/num_examples': 50000, 'test/accuracy': 0.5412999987602234, 'test/loss': 2.1156527996063232, 'test/num_examples': 10000, 'score': 59256.584094285965, 'total_duration': 63695.701869249344, 'accumulated_submission_time': 59256.584094285965, 'accumulated_eval_time': 4418.74244594574, 'accumulated_logging_time': 8.87801480293274}
I0307 21:26:25.909696 140237686679296 logging_writer.py:48] [107715] accumulated_eval_time=4418.74, accumulated_logging_time=8.87801, accumulated_submission_time=59256.6, global_step=107715, preemption_count=0, score=59256.6, test/accuracy=0.5413, test/loss=2.11565, test/num_examples=10000, total_duration=63695.7, train/accuracy=0.730827, train/loss=1.15776, validation/accuracy=0.66926, validation/loss=1.43669, validation/num_examples=50000
I0307 21:27:34.331573 140237678286592 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.614690780639648, loss=2.578995704650879
I0307 21:31:26.488872 140237686679296 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.370129108428955, loss=2.6014797687530518
I0307 21:34:56.844843 140393707492544 spec.py:321] Evaluating on the training split.
I0307 21:35:07.213327 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 21:35:29.849088 140393707492544 spec.py:349] Evaluating on the test split.
I0307 21:35:31.657553 140393707492544 submission_runner.py:469] Time since start: 64241.54s, 	Step: 107950, 	{'train/accuracy': 0.7295918464660645, 'train/loss': 1.143438458442688, 'validation/accuracy': 0.6731599569320679, 'validation/loss': 1.4076335430145264, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.050673246383667, 'test/num_examples': 10000, 'score': 59767.48608708382, 'total_duration': 64241.538284778595, 'accumulated_submission_time': 59767.48608708382, 'accumulated_eval_time': 4453.555124044418, 'accumulated_logging_time': 8.974353551864624}
I0307 21:35:31.680038 140237678286592 logging_writer.py:48] [107950] accumulated_eval_time=4453.56, accumulated_logging_time=8.97435, accumulated_submission_time=59767.5, global_step=107950, preemption_count=0, score=59767.5, test/accuracy=0.5525, test/loss=2.05067, test/num_examples=10000, total_duration=64241.5, train/accuracy=0.729592, train/loss=1.14344, validation/accuracy=0.67316, validation/loss=1.40763, validation/num_examples=50000
I0307 21:38:49.988700 140237686679296 logging_writer.py:48] [108000] global_step=108000, grad_norm=3.6905806064605713, loss=2.432750940322876
I0307 21:44:02.309491 140393707492544 spec.py:321] Evaluating on the training split.
I0307 21:44:12.233465 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 21:44:36.444639 140393707492544 spec.py:349] Evaluating on the test split.
I0307 21:44:38.211004 140393707492544 submission_runner.py:469] Time since start: 64788.09s, 	Step: 108074, 	{'train/accuracy': 0.7307078838348389, 'train/loss': 1.1458221673965454, 'validation/accuracy': 0.6693599820137024, 'validation/loss': 1.4208639860153198, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.05647349357605, 'test/num_examples': 10000, 'score': 60278.09408926964, 'total_duration': 64788.09173464775, 'accumulated_submission_time': 60278.09408926964, 'accumulated_eval_time': 4489.4566123485565, 'accumulated_logging_time': 9.005134344100952}
I0307 21:44:38.233649 140237678286592 logging_writer.py:48] [108074] accumulated_eval_time=4489.46, accumulated_logging_time=9.00513, accumulated_submission_time=60278.1, global_step=108074, preemption_count=0, score=60278.1, test/accuracy=0.5539, test/loss=2.05647, test/num_examples=10000, total_duration=64788.1, train/accuracy=0.730708, train/loss=1.14582, validation/accuracy=0.66936, validation/loss=1.42086, validation/num_examples=50000
I0307 21:46:13.519642 140237686679296 logging_writer.py:48] [108100] global_step=108100, grad_norm=3.415797710418701, loss=2.3829829692840576
I0307 21:53:09.382102 140393707492544 spec.py:321] Evaluating on the training split.
I0307 21:53:19.303527 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 21:53:44.352494 140393707492544 spec.py:349] Evaluating on the test split.
I0307 21:53:46.169832 140393707492544 submission_runner.py:469] Time since start: 65336.05s, 	Step: 108198, 	{'train/accuracy': 0.7258051633834839, 'train/loss': 1.1665153503417969, 'validation/accuracy': 0.6667799949645996, 'validation/loss': 1.4418220520019531, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0967540740966797, 'test/num_examples': 10000, 'score': 60789.18449521065, 'total_duration': 65336.05056142807, 'accumulated_submission_time': 60789.18449521065, 'accumulated_eval_time': 4526.244311094284, 'accumulated_logging_time': 9.072465181350708}
I0307 21:53:46.192486 140237678286592 logging_writer.py:48] [108198] accumulated_eval_time=4526.24, accumulated_logging_time=9.07247, accumulated_submission_time=60789.2, global_step=108198, preemption_count=0, score=60789.2, test/accuracy=0.5377, test/loss=2.09675, test/num_examples=10000, total_duration=65336.1, train/accuracy=0.725805, train/loss=1.16652, validation/accuracy=0.66678, validation/loss=1.44182, validation/num_examples=50000
I0307 21:53:47.342544 140237686679296 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.36112642288208, loss=2.5485081672668457
I0307 22:00:46.777285 140237678286592 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.496201515197754, loss=2.5304298400878906
I0307 22:02:17.389391 140393707492544 spec.py:321] Evaluating on the training split.
I0307 22:02:27.398519 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 22:02:51.308042 140393707492544 spec.py:349] Evaluating on the test split.
I0307 22:02:53.129760 140393707492544 submission_runner.py:469] Time since start: 65883.01s, 	Step: 108322, 	{'train/accuracy': 0.7287747263908386, 'train/loss': 1.1600546836853027, 'validation/accuracy': 0.6708799600601196, 'validation/loss': 1.415154218673706, 'validation/num_examples': 50000, 'test/accuracy': 0.5455999970436096, 'test/loss': 2.070884943008423, 'test/num_examples': 10000, 'score': 61300.36177492142, 'total_duration': 65883.01047825813, 'accumulated_submission_time': 61300.36177492142, 'accumulated_eval_time': 4561.984658479691, 'accumulated_logging_time': 9.1028311252594}
I0307 22:02:53.154022 140237686679296 logging_writer.py:48] [108322] accumulated_eval_time=4561.98, accumulated_logging_time=9.10283, accumulated_submission_time=61300.4, global_step=108322, preemption_count=0, score=61300.4, test/accuracy=0.5456, test/loss=2.07088, test/num_examples=10000, total_duration=65883, train/accuracy=0.728775, train/loss=1.16005, validation/accuracy=0.67088, validation/loss=1.41515, validation/num_examples=50000
I0307 22:08:10.645165 140237678286592 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.105504035949707, loss=2.405515432357788
I0307 22:11:24.944806 140393707492544 spec.py:321] Evaluating on the training split.
I0307 22:11:35.639431 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 22:11:55.009766 140393707492544 spec.py:349] Evaluating on the test split.
I0307 22:11:56.803012 140393707492544 submission_runner.py:469] Time since start: 66426.68s, 	Step: 108446, 	{'train/accuracy': 0.7276985049247742, 'train/loss': 1.1638103723526, 'validation/accuracy': 0.669979989528656, 'validation/loss': 1.4223161935806274, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.0719552040100098, 'test/num_examples': 10000, 'score': 61812.13180017471, 'total_duration': 66426.68374371529, 'accumulated_submission_time': 61812.13180017471, 'accumulated_eval_time': 4593.8428428173065, 'accumulated_logging_time': 9.134752035140991}
I0307 22:11:56.825703 140237686679296 logging_writer.py:48] [108446] accumulated_eval_time=4593.84, accumulated_logging_time=9.13475, accumulated_submission_time=61812.1, global_step=108446, preemption_count=0, score=61812.1, test/accuracy=0.5457, test/loss=2.07196, test/num_examples=10000, total_duration=66426.7, train/accuracy=0.727699, train/loss=1.16381, validation/accuracy=0.66998, validation/loss=1.42232, validation/num_examples=50000
I0307 22:12:32.950421 140237678286592 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.153036117553711, loss=2.533959150314331
I0307 22:13:54.692703 140237686679296 logging_writer.py:48] [108600] global_step=108600, grad_norm=3.786263942718506, loss=2.470386028289795
I0307 22:15:19.255254 140237678286592 logging_writer.py:48] [108700] global_step=108700, grad_norm=3.99099063873291, loss=2.4972822666168213
I0307 22:16:43.153741 140237686679296 logging_writer.py:48] [108800] global_step=108800, grad_norm=3.513038158416748, loss=2.390967845916748
I0307 22:18:00.293704 140237678286592 logging_writer.py:48] [108900] global_step=108900, grad_norm=3.7453818321228027, loss=2.4212138652801514
I0307 22:19:13.843757 140237686679296 logging_writer.py:48] [109000] global_step=109000, grad_norm=3.8065974712371826, loss=2.5179758071899414
I0307 22:20:26.831551 140393707492544 spec.py:321] Evaluating on the training split.
I0307 22:20:38.472712 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 22:21:01.631189 140393707492544 spec.py:349] Evaluating on the test split.
I0307 22:21:03.425443 140393707492544 submission_runner.py:469] Time since start: 66973.31s, 	Step: 109100, 	{'train/accuracy': 0.7307278513908386, 'train/loss': 1.1489639282226562, 'validation/accuracy': 0.6688799858093262, 'validation/loss': 1.4371027946472168, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0681846141815186, 'test/num_examples': 10000, 'score': 62322.05839467049, 'total_duration': 66973.30617070198, 'accumulated_submission_time': 62322.05839467049, 'accumulated_eval_time': 4630.436698913574, 'accumulated_logging_time': 9.166320085525513}
I0307 22:21:03.524528 140237678286592 logging_writer.py:48] [109100] accumulated_eval_time=4630.44, accumulated_logging_time=9.16632, accumulated_submission_time=62322.1, global_step=109100, preemption_count=0, score=62322.1, test/accuracy=0.5433, test/loss=2.06818, test/num_examples=10000, total_duration=66973.3, train/accuracy=0.730728, train/loss=1.14896, validation/accuracy=0.66888, validation/loss=1.4371, validation/num_examples=50000
I0307 22:21:03.877072 140237686679296 logging_writer.py:48] [109100] global_step=109100, grad_norm=3.84247088432312, loss=2.520463705062866
I0307 22:22:12.527389 140237678286592 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.11837100982666, loss=2.500131607055664
I0307 22:23:26.512744 140237686679296 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.3446736335754395, loss=2.390007972717285
I0307 22:24:40.891644 140237678286592 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.22847318649292, loss=2.555129051208496
I0307 22:25:54.948306 140237686679296 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.70937967300415, loss=2.5060184001922607
I0307 22:27:08.990314 140237678286592 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.261394023895264, loss=2.4306535720825195
I0307 22:28:23.085621 140237686679296 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.2138671875, loss=2.3870739936828613
I0307 22:29:33.832924 140393707492544 spec.py:321] Evaluating on the training split.
I0307 22:29:43.893945 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 22:30:02.889189 140393707492544 spec.py:349] Evaluating on the test split.
I0307 22:30:04.727190 140393707492544 submission_runner.py:469] Time since start: 67514.61s, 	Step: 109747, 	{'train/accuracy': 0.7344746589660645, 'train/loss': 1.1202000379562378, 'validation/accuracy': 0.6791999936103821, 'validation/loss': 1.3814679384231567, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.0251922607421875, 'test/num_examples': 10000, 'score': 62832.286561489105, 'total_duration': 67514.60792064667, 'accumulated_submission_time': 62832.286561489105, 'accumulated_eval_time': 4661.330931186676, 'accumulated_logging_time': 9.274696111679077}
I0307 22:30:04.794384 140237678286592 logging_writer.py:48] [109747] accumulated_eval_time=4661.33, accumulated_logging_time=9.2747, accumulated_submission_time=62832.3, global_step=109747, preemption_count=0, score=62832.3, test/accuracy=0.5506, test/loss=2.02519, test/num_examples=10000, total_duration=67514.6, train/accuracy=0.734475, train/loss=1.1202, validation/accuracy=0.6792, validation/loss=1.38147, validation/num_examples=50000
I0307 22:31:18.676771 140237686679296 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.2327961921691895, loss=2.431880474090576
I0307 22:34:08.348490 140237678286592 logging_writer.py:48] [109900] global_step=109900, grad_norm=3.467811346054077, loss=2.4452924728393555
I0307 22:37:04.110281 140237686679296 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.06317138671875, loss=2.442270040512085
I0307 22:38:36.104335 140393707492544 spec.py:321] Evaluating on the training split.
I0307 22:38:45.508262 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 22:39:05.398735 140393707492544 spec.py:349] Evaluating on the test split.
I0307 22:39:07.203510 140393707492544 submission_runner.py:469] Time since start: 68057.08s, 	Step: 110054, 	{'train/accuracy': 0.760164201259613, 'train/loss': 1.0102070569992065, 'validation/accuracy': 0.6690599918365479, 'validation/loss': 1.4101848602294922, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.077561140060425, 'test/num_examples': 10000, 'score': 63343.556574344635, 'total_duration': 68057.08423185349, 'accumulated_submission_time': 63343.556574344635, 'accumulated_eval_time': 4692.430089712143, 'accumulated_logging_time': 9.3498694896698}
I0307 22:39:07.254227 140237678286592 logging_writer.py:48] [110054] accumulated_eval_time=4692.43, accumulated_logging_time=9.34987, accumulated_submission_time=63343.6, global_step=110054, preemption_count=0, score=63343.6, test/accuracy=0.5442, test/loss=2.07756, test/num_examples=10000, total_duration=68057.1, train/accuracy=0.760164, train/loss=1.01021, validation/accuracy=0.66906, validation/loss=1.41018, validation/num_examples=50000
2025-03-07 22:39:37.532567: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:40:03.048091 140237686679296 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.065702438354492, loss=2.430061101913452
I0307 22:42:23.542756 140237678286592 logging_writer.py:48] [110200] global_step=110200, grad_norm=3.7219765186309814, loss=2.4985573291778564
I0307 22:44:43.637789 140237686679296 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.110191345214844, loss=2.4660613536834717
I0307 22:47:04.645993 140237678286592 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.580552101135254, loss=2.5412662029266357
I0307 22:47:38.461019 140393707492544 spec.py:321] Evaluating on the training split.
I0307 22:47:48.747277 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 22:48:10.701076 140393707492544 spec.py:349] Evaluating on the test split.
I0307 22:48:12.482117 140393707492544 submission_runner.py:469] Time since start: 68602.36s, 	Step: 110425, 	{'train/accuracy': 0.7470703125, 'train/loss': 1.0757482051849365, 'validation/accuracy': 0.6789999604225159, 'validation/loss': 1.386796474456787, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.053622245788574, 'test/num_examples': 10000, 'score': 63854.71561408043, 'total_duration': 68602.3628358841, 'accumulated_submission_time': 63854.71561408043, 'accumulated_eval_time': 4726.451149463654, 'accumulated_logging_time': 9.408446073532104}
I0307 22:48:12.565467 140237686679296 logging_writer.py:48] [110425] accumulated_eval_time=4726.45, accumulated_logging_time=9.40845, accumulated_submission_time=63854.7, global_step=110425, preemption_count=0, score=63854.7, test/accuracy=0.5504, test/loss=2.05362, test/num_examples=10000, total_duration=68602.4, train/accuracy=0.74707, train/loss=1.07575, validation/accuracy=0.679, validation/loss=1.3868, validation/num_examples=50000
I0307 22:49:46.608515 140237678286592 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.227410793304443, loss=2.4937477111816406
I0307 22:52:09.273323 140237686679296 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.089635848999023, loss=2.3585729598999023
I0307 22:54:31.663191 140237678286592 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.0528244972229, loss=2.3899588584899902
I0307 22:56:42.764721 140393707492544 spec.py:321] Evaluating on the training split.
I0307 22:56:52.733207 140393707492544 spec.py:333] Evaluating on the validation split.
2025-03-07 22:56:54.758827: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 1073741824 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 22:56:54.800378: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 966367744 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 22:56:54.832650: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 869731072 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 22:56:54.842742: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 782758144 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 22:56:55.466924: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 704482304 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 22:56:55.493080: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 634034176 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 22:56:55.517042: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 570630912 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
I0307 22:57:14.232118 140393707492544 spec.py:349] Evaluating on the test split.
I0307 22:57:16.038617 140393707492544 submission_runner.py:469] Time since start: 69145.92s, 	Step: 110793, 	{'train/accuracy': 0.7336973547935486, 'train/loss': 1.1514904499053955, 'validation/accuracy': 0.6708999872207642, 'validation/loss': 1.4322401285171509, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.0793941020965576, 'test/num_examples': 10000, 'score': 64364.8362364769, 'total_duration': 69145.91933512688, 'accumulated_submission_time': 64364.8362364769, 'accumulated_eval_time': 4759.72500872612, 'accumulated_logging_time': 9.533119916915894}
I0307 22:57:16.089941 140237686679296 logging_writer.py:48] [110793] accumulated_eval_time=4759.73, accumulated_logging_time=9.53312, accumulated_submission_time=64364.8, global_step=110793, preemption_count=0, score=64364.8, test/accuracy=0.5517, test/loss=2.07939, test/num_examples=10000, total_duration=69145.9, train/accuracy=0.733697, train/loss=1.15149, validation/accuracy=0.6709, validation/loss=1.43224, validation/num_examples=50000
I0307 22:57:19.180373 140237678286592 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.035857677459717, loss=2.3675310611724854
I0307 22:59:34.504090 140237686679296 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.012976169586182, loss=2.3619439601898193
I0307 23:02:12.428951 140237678286592 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.35152530670166, loss=2.453232765197754
I0307 23:05:08.482427 140237686679296 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.2490434646606445, loss=2.477489948272705
I0307 23:05:46.905134 140393707492544 spec.py:321] Evaluating on the training split.
I0307 23:05:57.007982 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 23:06:19.320116 140393707492544 spec.py:349] Evaluating on the test split.
I0307 23:06:21.073325 140393707492544 submission_runner.py:469] Time since start: 69690.95s, 	Step: 111122, 	{'train/accuracy': 0.7323421239852905, 'train/loss': 1.14013671875, 'validation/accuracy': 0.6732000112533569, 'validation/loss': 1.4115277528762817, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.061681032180786, 'test/num_examples': 10000, 'score': 64875.60917925835, 'total_duration': 69690.9540605545, 'accumulated_submission_time': 64875.60917925835, 'accumulated_eval_time': 4793.893170118332, 'accumulated_logging_time': 9.592237949371338}
I0307 23:06:21.129499 140237678286592 logging_writer.py:48] [111122] accumulated_eval_time=4793.89, accumulated_logging_time=9.59224, accumulated_submission_time=64875.6, global_step=111122, preemption_count=0, score=64875.6, test/accuracy=0.5475, test/loss=2.06168, test/num_examples=10000, total_duration=69691, train/accuracy=0.732342, train/loss=1.14014, validation/accuracy=0.6732, validation/loss=1.41153, validation/num_examples=50000
I0307 23:08:23.859025 140237686679296 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.752403259277344, loss=2.4213850498199463
I0307 23:11:19.422110 140237678286592 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.054352760314941, loss=2.4649903774261475
I0307 23:13:51.489193 140237686679296 logging_writer.py:48] [111400] global_step=111400, grad_norm=5.07385778427124, loss=2.3754963874816895
I0307 23:14:52.073694 140393707492544 spec.py:321] Evaluating on the training split.
I0307 23:15:01.614428 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 23:15:25.315743 140393707492544 spec.py:349] Evaluating on the test split.
I0307 23:15:27.075686 140393707492544 submission_runner.py:469] Time since start: 70236.96s, 	Step: 111444, 	{'train/accuracy': 0.7626554369926453, 'train/loss': 1.007634162902832, 'validation/accuracy': 0.672819972038269, 'validation/loss': 1.4071537256240845, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.071359872817993, 'test/num_examples': 10000, 'score': 65386.51100897789, 'total_duration': 70236.95640730858, 'accumulated_submission_time': 65386.51100897789, 'accumulated_eval_time': 4828.895120859146, 'accumulated_logging_time': 9.656341314315796}
I0307 23:15:27.120601 140237678286592 logging_writer.py:48] [111444] accumulated_eval_time=4828.9, accumulated_logging_time=9.65634, accumulated_submission_time=65386.5, global_step=111444, preemption_count=0, score=65386.5, test/accuracy=0.5504, test/loss=2.07136, test/num_examples=10000, total_duration=70237, train/accuracy=0.762655, train/loss=1.00763, validation/accuracy=0.67282, validation/loss=1.40715, validation/num_examples=50000
I0307 23:16:31.274124 140237686679296 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.598896503448486, loss=2.471980094909668
I0307 23:18:52.537402 140237678286592 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.158226490020752, loss=2.437407970428467
I0307 23:21:14.492712 140237686679296 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.594004154205322, loss=2.382270097732544
I0307 23:23:36.771756 140237678286592 logging_writer.py:48] [111800] global_step=111800, grad_norm=3.7679250240325928, loss=2.498086452484131
I0307 23:23:58.223263 140393707492544 spec.py:321] Evaluating on the training split.
I0307 23:24:08.620607 140393707492544 spec.py:333] Evaluating on the validation split.
I0307 23:24:29.156964 140393707492544 spec.py:349] Evaluating on the test split.
I0307 23:24:30.900787 140393707492544 submission_runner.py:469] Time since start: 70780.78s, 	Step: 111816, 	{'train/accuracy': 0.7431042790412903, 'train/loss': 1.1029917001724243, 'validation/accuracy': 0.6697799563407898, 'validation/loss': 1.4257930517196655, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.0903494358062744, 'test/num_examples': 10000, 'score': 65897.54716444016, 'total_duration': 70780.78152060509, 'accumulated_submission_time': 65897.54716444016, 'accumulated_eval_time': 4861.572622060776, 'accumulated_logging_time': 9.72568941116333}
I0307 23:24:30.963615 140237686679296 logging_writer.py:48] [111816] accumulated_eval_time=4861.57, accumulated_logging_time=9.72569, accumulated_submission_time=65897.5, global_step=111816, preemption_count=0, score=65897.5, test/accuracy=0.5478, test/loss=2.09035, test/num_examples=10000, total_duration=70780.8, train/accuracy=0.743104, train/loss=1.10299, validation/accuracy=0.66978, validation/loss=1.42579, validation/num_examples=50000
I0307 23:26:18.799374 140237678286592 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.5691986083984375, loss=2.3848321437835693
I0307 23:28:39.964265 140237686679296 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.187755107879639, loss=2.4628257751464844
I0307 23:31:02.129884 140237678286592 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.636017322540283, loss=2.4831924438476562
I0307 23:33:01.270557 140237686679296 logging_writer.py:48] [112185] global_step=112185, preemption_count=0, score=66407.8
I0307 23:33:02.864569 140393707492544 submission_runner.py:646] Tuning trial 1/5
I0307 23:33:02.883939 140393707492544 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 23:33:02.887840 140393707492544 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009167729294858873, 'train/loss': 6.91182279586792, 'validation/accuracy': 0.0011199999134987593, 'validation/loss': 6.912854194641113, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.912630081176758, 'test/num_examples': 10000, 'score': 56.56920671463013, 'total_duration': 141.49113082885742, 'accumulated_submission_time': 56.56920671463013, 'accumulated_eval_time': 84.92164707183838, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1368, {'train/accuracy': 0.0647321417927742, 'train/loss': 5.50593376159668, 'validation/accuracy': 0.05753999948501587, 'validation/loss': 5.589382171630859, 'validation/num_examples': 50000, 'test/accuracy': 0.040800001472234726, 'test/loss': 5.779533386230469, 'test/num_examples': 10000, 'score': 566.5438151359558, 'total_duration': 688.332738161087, 'accumulated_submission_time': 566.5438151359558, 'accumulated_eval_time': 121.57005262374878, 'accumulated_logging_time': 0.03919839859008789, 'global_step': 1368, 'preemption_count': 0}), (2717, {'train/accuracy': 0.15676817297935486, 'train/loss': 4.436342239379883, 'validation/accuracy': 0.1378999948501587, 'validation/loss': 4.576536655426025, 'validation/num_examples': 50000, 'test/accuracy': 0.09760000556707382, 'test/loss': 4.984498977661133, 'test/num_examples': 10000, 'score': 1076.7115275859833, 'total_duration': 1235.2888140678406, 'accumulated_submission_time': 1076.7115275859833, 'accumulated_eval_time': 158.16130113601685, 'accumulated_logging_time': 0.07754254341125488, 'global_step': 2717, 'preemption_count': 0}), (4048, {'train/accuracy': 0.26251593232154846, 'train/loss': 3.619271755218506, 'validation/accuracy': 0.2300799936056137, 'validation/loss': 3.8207767009735107, 'validation/num_examples': 50000, 'test/accuracy': 0.1680000126361847, 'test/loss': 4.3531413078308105, 'test/num_examples': 10000, 'score': 1586.8293161392212, 'total_duration': 1784.0130641460419, 'accumulated_submission_time': 1586.8293161392212, 'accumulated_eval_time': 196.58955931663513, 'accumulated_logging_time': 0.10748410224914551, 'global_step': 4048, 'preemption_count': 0}), (5380, {'train/accuracy': 0.33751195669174194, 'train/loss': 3.120251417160034, 'validation/accuracy': 0.3039799928665161, 'validation/loss': 3.3272545337677, 'validation/num_examples': 50000, 'test/accuracy': 0.22720001637935638, 'test/loss': 3.9319283962249756, 'test/num_examples': 10000, 'score': 2096.9253208637238, 'total_duration': 2329.3563158512115, 'accumulated_submission_time': 2096.9253208637238, 'accumulated_eval_time': 231.6465826034546, 'accumulated_logging_time': 0.155487060546875, 'global_step': 5380, 'preemption_count': 0}), (6708, {'train/accuracy': 0.40858179330825806, 'train/loss': 2.7329723834991455, 'validation/accuracy': 0.3676999807357788, 'validation/loss': 2.9532580375671387, 'validation/num_examples': 50000, 'test/accuracy': 0.28360000252723694, 'test/loss': 3.584315299987793, 'test/num_examples': 10000, 'score': 2606.9397070407867, 'total_duration': 2875.9615302085876, 'accumulated_submission_time': 2606.9397070407867, 'accumulated_eval_time': 268.04128313064575, 'accumulated_logging_time': 0.22454524040222168, 'global_step': 6708, 'preemption_count': 0}), (8036, {'train/accuracy': 0.4738321006298065, 'train/loss': 2.394381523132324, 'validation/accuracy': 0.42993998527526855, 'validation/loss': 2.622504711151123, 'validation/num_examples': 50000, 'test/accuracy': 0.3343000113964081, 'test/loss': 3.2575578689575195, 'test/num_examples': 10000, 'score': 3117.0869011878967, 'total_duration': 3426.024457216263, 'accumulated_submission_time': 3117.0869011878967, 'accumulated_eval_time': 307.7875051498413, 'accumulated_logging_time': 0.2605764865875244, 'global_step': 8036, 'preemption_count': 0}), (9357, {'train/accuracy': 0.5187539458274841, 'train/loss': 2.121927499771118, 'validation/accuracy': 0.46633997559547424, 'validation/loss': 2.3913583755493164, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.0530731678009033, 'test/num_examples': 10000, 'score': 3627.2021667957306, 'total_duration': 3976.4358086586, 'accumulated_submission_time': 3627.2021667957306, 'accumulated_eval_time': 347.88941049575806, 'accumulated_logging_time': 0.33215832710266113, 'global_step': 9357, 'preemption_count': 0}), (10697, {'train/accuracy': 0.5465362071990967, 'train/loss': 2.0194756984710693, 'validation/accuracy': 0.49573999643325806, 'validation/loss': 2.251650333404541, 'validation/num_examples': 50000, 'test/accuracy': 0.3846000134944916, 'test/loss': 2.921189785003662, 'test/num_examples': 10000, 'score': 4137.313571929932, 'total_duration': 4528.082790374756, 'accumulated_submission_time': 4137.313571929932, 'accumulated_eval_time': 389.16234731674194, 'accumulated_logging_time': 0.4484844207763672, 'global_step': 10697, 'preemption_count': 0}), (12029, {'train/accuracy': 0.5726442933082581, 'train/loss': 1.9059715270996094, 'validation/accuracy': 0.5229399800300598, 'validation/loss': 2.145876169204712, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.832265853881836, 'test/num_examples': 10000, 'score': 4647.442705869675, 'total_duration': 5075.26800942421, 'accumulated_submission_time': 4647.442705869675, 'accumulated_eval_time': 426.05201482772827, 'accumulated_logging_time': 0.4694032669067383, 'global_step': 12029, 'preemption_count': 0}), (13352, {'train/accuracy': 0.5810546875, 'train/loss': 1.839288353919983, 'validation/accuracy': 0.5356999635696411, 'validation/loss': 2.062808036804199, 'validation/num_examples': 50000, 'test/accuracy': 0.41920003294944763, 'test/loss': 2.725562572479248, 'test/num_examples': 10000, 'score': 5157.469700574875, 'total_duration': 5627.253938674927, 'accumulated_submission_time': 5157.469700574875, 'accumulated_eval_time': 467.73792481422424, 'accumulated_logging_time': 0.6028783321380615, 'global_step': 13352, 'preemption_count': 0}), (14666, {'train/accuracy': 0.6036550998687744, 'train/loss': 1.734601616859436, 'validation/accuracy': 0.5449399948120117, 'validation/loss': 1.9952954053878784, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.656996965408325, 'test/num_examples': 10000, 'score': 5667.497880935669, 'total_duration': 6182.922917604446, 'accumulated_submission_time': 5667.497880935669, 'accumulated_eval_time': 513.1108808517456, 'accumulated_logging_time': 0.7315492630004883, 'global_step': 14666, 'preemption_count': 0}), (15970, {'train/accuracy': 0.6106504797935486, 'train/loss': 1.7090407609939575, 'validation/accuracy': 0.5572400093078613, 'validation/loss': 1.9529874324798584, 'validation/num_examples': 50000, 'test/accuracy': 0.4408000111579895, 'test/loss': 2.6234328746795654, 'test/num_examples': 10000, 'score': 6177.396770238876, 'total_duration': 6734.277386665344, 'accumulated_submission_time': 6177.396770238876, 'accumulated_eval_time': 554.2856616973877, 'accumulated_logging_time': 0.869286060333252, 'global_step': 15970, 'preemption_count': 0}), (17294, {'train/accuracy': 0.6128427982330322, 'train/loss': 1.682786226272583, 'validation/accuracy': 0.5596799850463867, 'validation/loss': 1.942488431930542, 'validation/num_examples': 50000, 'test/accuracy': 0.4328000247478485, 'test/loss': 2.6385648250579834, 'test/num_examples': 10000, 'score': 6687.268934011459, 'total_duration': 7281.857532262802, 'accumulated_submission_time': 6687.268934011459, 'accumulated_eval_time': 591.7616419792175, 'accumulated_logging_time': 0.963083028793335, 'global_step': 17294, 'preemption_count': 0}), (18619, {'train/accuracy': 0.6204758882522583, 'train/loss': 1.6513203382492065, 'validation/accuracy': 0.5702799558639526, 'validation/loss': 1.8937433958053589, 'validation/num_examples': 50000, 'test/accuracy': 0.44530001282691956, 'test/loss': 2.5417776107788086, 'test/num_examples': 10000, 'score': 7197.350691795349, 'total_duration': 7829.285220146179, 'accumulated_submission_time': 7197.350691795349, 'accumulated_eval_time': 628.8886682987213, 'accumulated_logging_time': 1.054701566696167, 'global_step': 18619, 'preemption_count': 0}), (19899, {'train/accuracy': 0.6251793503761292, 'train/loss': 1.6402405500411987, 'validation/accuracy': 0.5752599835395813, 'validation/loss': 1.8779237270355225, 'validation/num_examples': 50000, 'test/accuracy': 0.4529000222682953, 'test/loss': 2.5515594482421875, 'test/num_examples': 10000, 'score': 7707.2713577747345, 'total_duration': 8383.240555524826, 'accumulated_submission_time': 7707.2713577747345, 'accumulated_eval_time': 672.7467930316925, 'accumulated_logging_time': 1.104966402053833, 'global_step': 19899, 'preemption_count': 0}), (21230, {'train/accuracy': 0.6267936825752258, 'train/loss': 1.629669189453125, 'validation/accuracy': 0.5759599804878235, 'validation/loss': 1.87746000289917, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.53212833404541, 'test/num_examples': 10000, 'score': 8217.10722565651, 'total_duration': 8932.19880771637, 'accumulated_submission_time': 8217.10722565651, 'accumulated_eval_time': 711.6320838928223, 'accumulated_logging_time': 1.2052326202392578, 'global_step': 21230, 'preemption_count': 0}), (22547, {'train/accuracy': 0.6214724183082581, 'train/loss': 1.6483502388000488, 'validation/accuracy': 0.5716800093650818, 'validation/loss': 1.8884778022766113, 'validation/num_examples': 50000, 'test/accuracy': 0.44600000977516174, 'test/loss': 2.585023880004883, 'test/num_examples': 10000, 'score': 8727.009164333344, 'total_duration': 9484.590646028519, 'accumulated_submission_time': 8727.009164333344, 'accumulated_eval_time': 753.8921568393707, 'accumulated_logging_time': 1.3064463138580322, 'global_step': 22547, 'preemption_count': 0}), (23870, {'train/accuracy': 0.6290457248687744, 'train/loss': 1.6290956735610962, 'validation/accuracy': 0.5796799659729004, 'validation/loss': 1.865731954574585, 'validation/num_examples': 50000, 'test/accuracy': 0.4644000232219696, 'test/loss': 2.5260605812072754, 'test/num_examples': 10000, 'score': 9237.150210142136, 'total_duration': 10036.43930220604, 'accumulated_submission_time': 9237.150210142136, 'accumulated_eval_time': 795.4274299144745, 'accumulated_logging_time': 1.353196144104004, 'global_step': 23870, 'preemption_count': 0}), (25191, {'train/accuracy': 0.6312978267669678, 'train/loss': 1.596471905708313, 'validation/accuracy': 0.5849999785423279, 'validation/loss': 1.8211485147476196, 'validation/num_examples': 50000, 'test/accuracy': 0.46720001101493835, 'test/loss': 2.4841697216033936, 'test/num_examples': 10000, 'score': 9746.980960845947, 'total_duration': 10589.051862478256, 'accumulated_submission_time': 9746.980960845947, 'accumulated_eval_time': 838.0154654979706, 'accumulated_logging_time': 1.4236249923706055, 'global_step': 25191, 'preemption_count': 0}), (26510, {'train/accuracy': 0.6360809803009033, 'train/loss': 1.5859854221343994, 'validation/accuracy': 0.5837399959564209, 'validation/loss': 1.829034447669983, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.477566957473755, 'test/num_examples': 10000, 'score': 10256.957358837128, 'total_duration': 11141.51793885231, 'accumulated_submission_time': 10256.957358837128, 'accumulated_eval_time': 880.304535150528, 'accumulated_logging_time': 1.4954111576080322, 'global_step': 26510, 'preemption_count': 0}), (27831, {'train/accuracy': 0.6398875713348389, 'train/loss': 1.5636855363845825, 'validation/accuracy': 0.5902199745178223, 'validation/loss': 1.8054137229919434, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.473846435546875, 'test/num_examples': 10000, 'score': 10766.931269407272, 'total_duration': 11693.710929393768, 'accumulated_submission_time': 10766.931269407272, 'accumulated_eval_time': 922.3257946968079, 'accumulated_logging_time': 1.561805009841919, 'global_step': 27831, 'preemption_count': 0}), (29156, {'train/accuracy': 0.6448501348495483, 'train/loss': 1.5242646932601929, 'validation/accuracy': 0.5925799608230591, 'validation/loss': 1.7726174592971802, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.427928924560547, 'test/num_examples': 10000, 'score': 11276.84908080101, 'total_duration': 12243.440912485123, 'accumulated_submission_time': 11276.84908080101, 'accumulated_eval_time': 961.9312443733215, 'accumulated_logging_time': 1.6346564292907715, 'global_step': 29156, 'preemption_count': 0}), (30476, {'train/accuracy': 0.6464444994926453, 'train/loss': 1.575693964958191, 'validation/accuracy': 0.5981799960136414, 'validation/loss': 1.8023936748504639, 'validation/num_examples': 50000, 'test/accuracy': 0.47870001196861267, 'test/loss': 2.4508919715881348, 'test/num_examples': 10000, 'score': 11786.757893323898, 'total_duration': 12796.664817810059, 'accumulated_submission_time': 11786.757893323898, 'accumulated_eval_time': 1005.0699033737183, 'accumulated_logging_time': 1.6861090660095215, 'global_step': 30476, 'preemption_count': 0}), (31795, {'train/accuracy': 0.6410833597183228, 'train/loss': 1.5624748468399048, 'validation/accuracy': 0.5927799940109253, 'validation/loss': 1.7792936563491821, 'validation/num_examples': 50000, 'test/accuracy': 0.46810001134872437, 'test/loss': 2.470116138458252, 'test/num_examples': 10000, 'score': 12296.850430250168, 'total_duration': 13347.036401987076, 'accumulated_submission_time': 12296.850430250168, 'accumulated_eval_time': 1045.155550956726, 'accumulated_logging_time': 1.7511441707611084, 'global_step': 31795, 'preemption_count': 0}), (33103, {'train/accuracy': 0.6491150856018066, 'train/loss': 1.5205764770507812, 'validation/accuracy': 0.6026600003242493, 'validation/loss': 1.742527723312378, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.3952646255493164, 'test/num_examples': 10000, 'score': 12806.972704172134, 'total_duration': 13896.098922252655, 'accumulated_submission_time': 12806.972704172134, 'accumulated_eval_time': 1083.9155917167664, 'accumulated_logging_time': 1.8032684326171875, 'global_step': 33103, 'preemption_count': 0}), (34419, {'train/accuracy': 0.646882951259613, 'train/loss': 1.5322587490081787, 'validation/accuracy': 0.5976200103759766, 'validation/loss': 1.7603105306625366, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.4205799102783203, 'test/num_examples': 10000, 'score': 13316.89038491249, 'total_duration': 14443.553050518036, 'accumulated_submission_time': 13316.89038491249, 'accumulated_eval_time': 1121.2515542507172, 'accumulated_logging_time': 1.8746492862701416, 'global_step': 34419, 'preemption_count': 0}), (35735, {'train/accuracy': 0.6544762253761292, 'train/loss': 1.5279052257537842, 'validation/accuracy': 0.6057800054550171, 'validation/loss': 1.7456291913986206, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.426006555557251, 'test/num_examples': 10000, 'score': 13827.075783252716, 'total_duration': 14998.59830904007, 'accumulated_submission_time': 13827.075783252716, 'accumulated_eval_time': 1165.9171843528748, 'accumulated_logging_time': 1.9393372535705566, 'global_step': 35735, 'preemption_count': 0}), (37052, {'train/accuracy': 0.6554129123687744, 'train/loss': 1.4934836626052856, 'validation/accuracy': 0.6114999651908875, 'validation/loss': 1.7066612243652344, 'validation/num_examples': 50000, 'test/accuracy': 0.48660001158714294, 'test/loss': 2.3942503929138184, 'test/num_examples': 10000, 'score': 14337.196107625961, 'total_duration': 15549.829090356827, 'accumulated_submission_time': 14337.196107625961, 'accumulated_eval_time': 1206.8251340389252, 'accumulated_logging_time': 2.0135011672973633, 'global_step': 37052, 'preemption_count': 0}), (38375, {'train/accuracy': 0.6594985723495483, 'train/loss': 1.485409140586853, 'validation/accuracy': 0.6105799674987793, 'validation/loss': 1.7028491497039795, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.384251832962036, 'test/num_examples': 10000, 'score': 14847.059845685959, 'total_duration': 16103.9566116333, 'accumulated_submission_time': 14847.059845685959, 'accumulated_eval_time': 1250.8877024650574, 'accumulated_logging_time': 2.0823917388916016, 'global_step': 38375, 'preemption_count': 0}), (39693, {'train/accuracy': 0.6515066623687744, 'train/loss': 1.513933777809143, 'validation/accuracy': 0.605239987373352, 'validation/loss': 1.7309751510620117, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.419649124145508, 'test/num_examples': 10000, 'score': 15356.989696025848, 'total_duration': 16658.893371105194, 'accumulated_submission_time': 15356.989696025848, 'accumulated_eval_time': 1295.7180805206299, 'accumulated_logging_time': 2.133098602294922, 'global_step': 39693, 'preemption_count': 0}), (41009, {'train/accuracy': 0.6520049571990967, 'train/loss': 1.4856503009796143, 'validation/accuracy': 0.6104599833488464, 'validation/loss': 1.6889045238494873, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.3565027713775635, 'test/num_examples': 10000, 'score': 15866.921926498413, 'total_duration': 17212.8523144722, 'accumulated_submission_time': 15866.921926498413, 'accumulated_eval_time': 1339.5273489952087, 'accumulated_logging_time': 2.2279880046844482, 'global_step': 41009, 'preemption_count': 0}), (42326, {'train/accuracy': 0.6617307066917419, 'train/loss': 1.4724243879318237, 'validation/accuracy': 0.609279990196228, 'validation/loss': 1.7118030786514282, 'validation/num_examples': 50000, 'test/accuracy': 0.48730000853538513, 'test/loss': 2.3597989082336426, 'test/num_examples': 10000, 'score': 16376.759145259857, 'total_duration': 17762.623378276825, 'accumulated_submission_time': 16376.759145259857, 'accumulated_eval_time': 1379.25390791893, 'accumulated_logging_time': 2.3094470500946045, 'global_step': 42326, 'preemption_count': 0}), (43648, {'train/accuracy': 0.6495137214660645, 'train/loss': 1.5253076553344727, 'validation/accuracy': 0.6074599623680115, 'validation/loss': 1.7194430828094482, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.420499801635742, 'test/num_examples': 10000, 'score': 16886.63295531273, 'total_duration': 18312.05678343773, 'accumulated_submission_time': 16886.63295531273, 'accumulated_eval_time': 1418.6012780666351, 'accumulated_logging_time': 2.3899149894714355, 'global_step': 43648, 'preemption_count': 0}), (44967, {'train/accuracy': 0.659598171710968, 'train/loss': 1.4718120098114014, 'validation/accuracy': 0.6124799847602844, 'validation/loss': 1.6863794326782227, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.3506157398223877, 'test/num_examples': 10000, 'score': 17396.74730324745, 'total_duration': 18865.005488157272, 'accumulated_submission_time': 17396.74730324745, 'accumulated_eval_time': 1461.1679220199585, 'accumulated_logging_time': 2.5292623043060303, 'global_step': 44967, 'preemption_count': 0}), (46283, {'train/accuracy': 0.6578643321990967, 'train/loss': 1.4481370449066162, 'validation/accuracy': 0.6146999597549438, 'validation/loss': 1.6504967212677002, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.333427906036377, 'test/num_examples': 10000, 'score': 17906.59534907341, 'total_duration': 19416.56383371353, 'accumulated_submission_time': 17906.59534907341, 'accumulated_eval_time': 1502.6500434875488, 'accumulated_logging_time': 2.6325485706329346, 'global_step': 46283, 'preemption_count': 0}), (47604, {'train/accuracy': 0.6633250713348389, 'train/loss': 1.4739528894424438, 'validation/accuracy': 0.6157799959182739, 'validation/loss': 1.6829516887664795, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.3608720302581787, 'test/num_examples': 10000, 'score': 18416.387121915817, 'total_duration': 19968.807157993317, 'accumulated_submission_time': 18416.387121915817, 'accumulated_eval_time': 1544.8281593322754, 'accumulated_logging_time': 2.7714359760284424, 'global_step': 47604, 'preemption_count': 0}), (48921, {'train/accuracy': 0.6593391299247742, 'train/loss': 1.4737554788589478, 'validation/accuracy': 0.6145600080490112, 'validation/loss': 1.6812688112258911, 'validation/num_examples': 50000, 'test/accuracy': 0.4832000136375427, 'test/loss': 2.3895442485809326, 'test/num_examples': 10000, 'score': 18926.40757584572, 'total_duration': 20518.0732858181, 'accumulated_submission_time': 18926.40757584572, 'accumulated_eval_time': 1583.8667945861816, 'accumulated_logging_time': 2.8539810180664062, 'global_step': 48921, 'preemption_count': 0}), (50243, {'train/accuracy': 0.6617107391357422, 'train/loss': 1.4694215059280396, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.6839659214019775, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.3257341384887695, 'test/num_examples': 10000, 'score': 19436.45262861252, 'total_duration': 21068.557340860367, 'accumulated_submission_time': 19436.45262861252, 'accumulated_eval_time': 1624.1023440361023, 'accumulated_logging_time': 2.9294593334198, 'global_step': 50243, 'preemption_count': 0}), (51568, {'train/accuracy': 0.6576650142669678, 'train/loss': 1.491856336593628, 'validation/accuracy': 0.6123799681663513, 'validation/loss': 1.7056300640106201, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.3611185550689697, 'test/num_examples': 10000, 'score': 19946.46866297722, 'total_duration': 21619.83942103386, 'accumulated_submission_time': 19946.46866297722, 'accumulated_eval_time': 1665.1673040390015, 'accumulated_logging_time': 2.997260332107544, 'global_step': 51568, 'preemption_count': 0}), (52784, {'train/accuracy': 0.6667928695678711, 'train/loss': 1.4553090333938599, 'validation/accuracy': 0.6168000102043152, 'validation/loss': 1.6777008771896362, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.350822687149048, 'test/num_examples': 10000, 'score': 20456.509650230408, 'total_duration': 22169.78112077713, 'accumulated_submission_time': 20456.509650230408, 'accumulated_eval_time': 1704.8702244758606, 'accumulated_logging_time': 3.0712063312530518, 'global_step': 52784, 'preemption_count': 0}), (54083, {'train/accuracy': 0.6694634556770325, 'train/loss': 1.4290324449539185, 'validation/accuracy': 0.6240999698638916, 'validation/loss': 1.6453121900558472, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.3095548152923584, 'test/num_examples': 10000, 'score': 20966.437312602997, 'total_duration': 22718.860379219055, 'accumulated_submission_time': 20966.437312602997, 'accumulated_eval_time': 1743.7445893287659, 'accumulated_logging_time': 3.201098680496216, 'global_step': 54083, 'preemption_count': 0}), (55368, {'train/accuracy': 0.6736686825752258, 'train/loss': 1.373186469078064, 'validation/accuracy': 0.6240999698638916, 'validation/loss': 1.615986943244934, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.295732021331787, 'test/num_examples': 10000, 'score': 21476.33947277069, 'total_duration': 23270.435178995132, 'accumulated_submission_time': 21476.33947277069, 'accumulated_eval_time': 1785.2051935195923, 'accumulated_logging_time': 3.26275897026062, 'global_step': 55368, 'preemption_count': 0}), (56657, {'train/accuracy': 0.6736487150192261, 'train/loss': 1.3920156955718994, 'validation/accuracy': 0.6235799789428711, 'validation/loss': 1.62201726436615, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.298546314239502, 'test/num_examples': 10000, 'score': 21986.10379600525, 'total_duration': 23818.58509993553, 'accumulated_submission_time': 21986.10379600525, 'accumulated_eval_time': 1823.294312953949, 'accumulated_logging_time': 3.4053735733032227, 'global_step': 56657, 'preemption_count': 0}), (57940, {'train/accuracy': 0.6677694320678711, 'train/loss': 1.4304031133651733, 'validation/accuracy': 0.6202600002288818, 'validation/loss': 1.6514286994934082, 'validation/num_examples': 50000, 'test/accuracy': 0.49390003085136414, 'test/loss': 2.327284336090088, 'test/num_examples': 10000, 'score': 22496.170170545578, 'total_duration': 24371.42397737503, 'accumulated_submission_time': 22496.170170545578, 'accumulated_eval_time': 1865.8140029907227, 'accumulated_logging_time': 3.5080671310424805, 'global_step': 57940, 'preemption_count': 0}), (59222, {'train/accuracy': 0.6744459271430969, 'train/loss': 1.4181233644485474, 'validation/accuracy': 0.6239399909973145, 'validation/loss': 1.6500818729400635, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2889773845672607, 'test/num_examples': 10000, 'score': 23005.943830490112, 'total_duration': 24920.612103939056, 'accumulated_submission_time': 23005.943830490112, 'accumulated_eval_time': 1904.9702589511871, 'accumulated_logging_time': 3.614800453186035, 'global_step': 59222, 'preemption_count': 0}), (60501, {'train/accuracy': 0.6765584945678711, 'train/loss': 1.3821467161178589, 'validation/accuracy': 0.6243000030517578, 'validation/loss': 1.6189907789230347, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2862048149108887, 'test/num_examples': 10000, 'score': 23516.01775741577, 'total_duration': 25471.231680631638, 'accumulated_submission_time': 23516.01775741577, 'accumulated_eval_time': 1945.2971069812775, 'accumulated_logging_time': 3.683279514312744, 'global_step': 60501, 'preemption_count': 0}), (61762, {'train/accuracy': 0.6795878410339355, 'train/loss': 1.3922882080078125, 'validation/accuracy': 0.6300599575042725, 'validation/loss': 1.6184196472167969, 'validation/num_examples': 50000, 'test/accuracy': 0.5091000199317932, 'test/loss': 2.2597270011901855, 'test/num_examples': 10000, 'score': 24025.98553752899, 'total_duration': 26022.97979950905, 'accumulated_submission_time': 24025.98553752899, 'accumulated_eval_time': 1986.8533709049225, 'accumulated_logging_time': 3.761902093887329, 'global_step': 61762, 'preemption_count': 0}), (63007, {'train/accuracy': 0.6781927347183228, 'train/loss': 1.4180994033813477, 'validation/accuracy': 0.6233999729156494, 'validation/loss': 1.6588274240493774, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.3485636711120605, 'test/num_examples': 10000, 'score': 24536.08448243141, 'total_duration': 26573.312735795975, 'accumulated_submission_time': 24536.08448243141, 'accumulated_eval_time': 2026.8134157657623, 'accumulated_logging_time': 3.89278244972229, 'global_step': 63007, 'preemption_count': 0}), (64281, {'train/accuracy': 0.6554527878761292, 'train/loss': 1.5121690034866333, 'validation/accuracy': 0.6066200137138367, 'validation/loss': 1.750923991203308, 'validation/num_examples': 50000, 'test/accuracy': 0.48510003089904785, 'test/loss': 2.431643009185791, 'test/num_examples': 10000, 'score': 25046.053644657135, 'total_duration': 27124.972723007202, 'accumulated_submission_time': 25046.053644657135, 'accumulated_eval_time': 2068.2780256271362, 'accumulated_logging_time': 3.9706714153289795, 'global_step': 64281, 'preemption_count': 0}), (65556, {'train/accuracy': 0.685945451259613, 'train/loss': 1.3673322200775146, 'validation/accuracy': 0.6327599883079529, 'validation/loss': 1.608644962310791, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.258028507232666, 'test/num_examples': 10000, 'score': 25556.05849671364, 'total_duration': 27676.356492996216, 'accumulated_submission_time': 25556.05849671364, 'accumulated_eval_time': 2109.4440019130707, 'accumulated_logging_time': 4.037358522415161, 'global_step': 65556, 'preemption_count': 0}), (66836, {'train/accuracy': 0.6857461333274841, 'train/loss': 1.3545249700546265, 'validation/accuracy': 0.6339199542999268, 'validation/loss': 1.6100269556045532, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.2672336101531982, 'test/num_examples': 10000, 'score': 26065.972586393356, 'total_duration': 28229.108406305313, 'accumulated_submission_time': 26065.972586393356, 'accumulated_eval_time': 2152.055104494095, 'accumulated_logging_time': 4.1164069175720215, 'global_step': 66836, 'preemption_count': 0}), (68110, {'train/accuracy': 0.6877192258834839, 'train/loss': 1.354539394378662, 'validation/accuracy': 0.6323800086975098, 'validation/loss': 1.5963611602783203, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.278628349304199, 'test/num_examples': 10000, 'score': 26575.983340024948, 'total_duration': 28780.009119272232, 'accumulated_submission_time': 26575.983340024948, 'accumulated_eval_time': 2192.697955608368, 'accumulated_logging_time': 4.217088460922241, 'global_step': 68110, 'preemption_count': 0}), (69388, {'train/accuracy': 0.6885562539100647, 'train/loss': 1.3444920778274536, 'validation/accuracy': 0.6308799982070923, 'validation/loss': 1.5962424278259277, 'validation/num_examples': 50000, 'test/accuracy': 0.5080000162124634, 'test/loss': 2.273098945617676, 'test/num_examples': 10000, 'score': 27085.99235892296, 'total_duration': 29329.848727226257, 'accumulated_submission_time': 27085.99235892296, 'accumulated_eval_time': 2232.304853439331, 'accumulated_logging_time': 4.294145822525024, 'global_step': 69388, 'preemption_count': 0}), (70664, {'train/accuracy': 0.6712372303009033, 'train/loss': 1.4403884410858154, 'validation/accuracy': 0.6182999610900879, 'validation/loss': 1.6814872026443481, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.324831008911133, 'test/num_examples': 10000, 'score': 27595.943027973175, 'total_duration': 29879.622443437576, 'accumulated_submission_time': 27595.943027973175, 'accumulated_eval_time': 2271.8782544136047, 'accumulated_logging_time': 4.397392988204956, 'global_step': 70664, 'preemption_count': 0}), (71924, {'train/accuracy': 0.6946348547935486, 'train/loss': 1.3102076053619385, 'validation/accuracy': 0.6383199691772461, 'validation/loss': 1.5735831260681152, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.2401814460754395, 'test/num_examples': 10000, 'score': 28105.794216394424, 'total_duration': 30429.78483438492, 'accumulated_submission_time': 28105.794216394424, 'accumulated_eval_time': 2311.965348482132, 'accumulated_logging_time': 4.47649884223938, 'global_step': 71924, 'preemption_count': 0}), (73060, {'train/accuracy': 0.6994379758834839, 'train/loss': 1.2880041599273682, 'validation/accuracy': 0.6383199691772461, 'validation/loss': 1.5783789157867432, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.258270025253296, 'test/num_examples': 10000, 'score': 28615.571096897125, 'total_duration': 30979.886177539825, 'accumulated_submission_time': 28615.571096897125, 'accumulated_eval_time': 2352.057108402252, 'accumulated_logging_time': 4.577669620513916, 'global_step': 73060, 'preemption_count': 0}), (74156, {'train/accuracy': 0.7185506820678711, 'train/loss': 1.2132364511489868, 'validation/accuracy': 0.6328799724578857, 'validation/loss': 1.5887466669082642, 'validation/num_examples': 50000, 'test/accuracy': 0.5145000219345093, 'test/loss': 2.2378978729248047, 'test/num_examples': 10000, 'score': 29125.696088552475, 'total_duration': 31527.628700971603, 'accumulated_submission_time': 29125.696088552475, 'accumulated_eval_time': 2389.4751737117767, 'accumulated_logging_time': 4.651588439941406, 'global_step': 74156, 'preemption_count': 0}), (75346, {'train/accuracy': 0.6737284660339355, 'train/loss': 1.406254768371582, 'validation/accuracy': 0.6254199743270874, 'validation/loss': 1.6341502666473389, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.269862174987793, 'test/num_examples': 10000, 'score': 29635.822702407837, 'total_duration': 32076.062469005585, 'accumulated_submission_time': 29635.822702407837, 'accumulated_eval_time': 2427.5725004673004, 'accumulated_logging_time': 4.73053240776062, 'global_step': 75346, 'preemption_count': 0}), (76540, {'train/accuracy': 0.6850685477256775, 'train/loss': 1.3532170057296753, 'validation/accuracy': 0.6418799757957458, 'validation/loss': 1.5587762594223022, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.2022647857666016, 'test/num_examples': 10000, 'score': 30145.91132569313, 'total_duration': 32625.751860141754, 'accumulated_submission_time': 30145.91132569313, 'accumulated_eval_time': 2466.944679260254, 'accumulated_logging_time': 4.823166847229004, 'global_step': 76540, 'preemption_count': 0}), (77782, {'train/accuracy': 0.6861248016357422, 'train/loss': 1.3418529033660889, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.5507181882858276, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.2120394706726074, 'test/num_examples': 10000, 'score': 30655.70121073723, 'total_duration': 33173.612880945206, 'accumulated_submission_time': 30655.70121073723, 'accumulated_eval_time': 2504.7704935073853, 'accumulated_logging_time': 4.927473783493042, 'global_step': 77782, 'preemption_count': 0}), (78972, {'train/accuracy': 0.693359375, 'train/loss': 1.3174878358840942, 'validation/accuracy': 0.6416199803352356, 'validation/loss': 1.5526129007339478, 'validation/num_examples': 50000, 'test/accuracy': 0.5182000398635864, 'test/loss': 2.2029266357421875, 'test/num_examples': 10000, 'score': 31165.5601875782, 'total_duration': 33719.41975212097, 'accumulated_submission_time': 31165.5601875782, 'accumulated_eval_time': 2540.508004426956, 'accumulated_logging_time': 5.001620292663574, 'global_step': 78972, 'preemption_count': 0}), (80022, {'train/accuracy': 0.6858657598495483, 'train/loss': 1.3517900705337524, 'validation/accuracy': 0.6343599557876587, 'validation/loss': 1.585518717765808, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2630302906036377, 'test/num_examples': 10000, 'score': 31675.460896015167, 'total_duration': 34266.834636449814, 'accumulated_submission_time': 31675.460896015167, 'accumulated_eval_time': 2577.769932985306, 'accumulated_logging_time': 5.135424852371216, 'global_step': 80022, 'preemption_count': 0}), (81032, {'train/accuracy': 0.7059351205825806, 'train/loss': 1.266436219215393, 'validation/accuracy': 0.6407399773597717, 'validation/loss': 1.5582534074783325, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.1871933937072754, 'test/num_examples': 10000, 'score': 32185.357806682587, 'total_duration': 34812.774980545044, 'accumulated_submission_time': 32185.357806682587, 'accumulated_eval_time': 2613.6335051059723, 'accumulated_logging_time': 5.198646306991577, 'global_step': 81032, 'preemption_count': 0}), (82040, {'train/accuracy': 0.7329599857330322, 'train/loss': 1.153250813484192, 'validation/accuracy': 0.6435999870300293, 'validation/loss': 1.5510573387145996, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.237302541732788, 'test/num_examples': 10000, 'score': 32695.638719558716, 'total_duration': 35359.470586299896, 'accumulated_submission_time': 32695.638719558716, 'accumulated_eval_time': 2649.869715690613, 'accumulated_logging_time': 5.260061740875244, 'global_step': 82040, 'preemption_count': 0}), (83005, {'train/accuracy': 0.6928212642669678, 'train/loss': 1.349096655845642, 'validation/accuracy': 0.6462199687957764, 'validation/loss': 1.5621826648712158, 'validation/num_examples': 50000, 'test/accuracy': 0.5187000036239624, 'test/loss': 2.2379472255706787, 'test/num_examples': 10000, 'score': 33205.73368763924, 'total_duration': 35907.25191640854, 'accumulated_submission_time': 33205.73368763924, 'accumulated_eval_time': 2687.3361155986786, 'accumulated_logging_time': 5.369216203689575, 'global_step': 83005, 'preemption_count': 0}), (84139, {'train/accuracy': 0.6945351958274841, 'train/loss': 1.3159548044204712, 'validation/accuracy': 0.6390399932861328, 'validation/loss': 1.5624115467071533, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.2333319187164307, 'test/num_examples': 10000, 'score': 33715.67713141441, 'total_duration': 36454.11797332764, 'accumulated_submission_time': 33715.67713141441, 'accumulated_eval_time': 2724.0695250034332, 'accumulated_logging_time': 5.430817365646362, 'global_step': 84139, 'preemption_count': 0}), (85103, {'train/accuracy': 0.7028061151504517, 'train/loss': 1.2598367929458618, 'validation/accuracy': 0.6434999704360962, 'validation/loss': 1.5392330884933472, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.214064836502075, 'test/num_examples': 10000, 'score': 34225.52469873428, 'total_duration': 37003.10376429558, 'accumulated_submission_time': 34225.52469873428, 'accumulated_eval_time': 2763.0267927646637, 'accumulated_logging_time': 5.506056547164917, 'global_step': 85103, 'preemption_count': 0}), (86041, {'train/accuracy': 0.7290138602256775, 'train/loss': 1.177551507949829, 'validation/accuracy': 0.6460399627685547, 'validation/loss': 1.5545614957809448, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.1816036701202393, 'test/num_examples': 10000, 'score': 34735.45169377327, 'total_duration': 37549.08370709419, 'accumulated_submission_time': 34735.45169377327, 'accumulated_eval_time': 2798.825093984604, 'accumulated_logging_time': 5.656623125076294, 'global_step': 86041, 'preemption_count': 0}), (86774, {'train/accuracy': 0.7005341053009033, 'train/loss': 1.3050864934921265, 'validation/accuracy': 0.6470400094985962, 'validation/loss': 1.541648268699646, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.1699681282043457, 'test/num_examples': 10000, 'score': 35245.549488306046, 'total_duration': 38091.41640520096, 'accumulated_submission_time': 35245.549488306046, 'accumulated_eval_time': 2830.934319257736, 'accumulated_logging_time': 5.703082323074341, 'global_step': 86774, 'preemption_count': 0}), (87660, {'train/accuracy': 0.7178531289100647, 'train/loss': 1.2251883745193481, 'validation/accuracy': 0.6545599699020386, 'validation/loss': 1.5065555572509766, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.161987066268921, 'test/num_examples': 10000, 'score': 35755.64194726944, 'total_duration': 38636.70443224907, 'accumulated_submission_time': 35755.64194726944, 'accumulated_eval_time': 2865.9050023555756, 'accumulated_logging_time': 5.8283281326293945, 'global_step': 87660, 'preemption_count': 0}), (88440, {'train/accuracy': 0.6963488459587097, 'train/loss': 1.3131103515625, 'validation/accuracy': 0.6462000012397766, 'validation/loss': 1.5422579050064087, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.1805808544158936, 'test/num_examples': 10000, 'score': 36265.561046123505, 'total_duration': 39180.48215460777, 'accumulated_submission_time': 36265.561046123505, 'accumulated_eval_time': 2899.577857732773, 'accumulated_logging_time': 5.928080320358276, 'global_step': 88440, 'preemption_count': 0}), (89194, {'train/accuracy': 0.7027263641357422, 'train/loss': 1.2801790237426758, 'validation/accuracy': 0.6464799642562866, 'validation/loss': 1.5340098142623901, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.1986050605773926, 'test/num_examples': 10000, 'score': 36775.806440114975, 'total_duration': 39726.27491426468, 'accumulated_submission_time': 36775.806440114975, 'accumulated_eval_time': 2934.9577057361603, 'accumulated_logging_time': 6.0126330852508545, 'global_step': 89194, 'preemption_count': 0}), (89971, {'train/accuracy': 0.744140625, 'train/loss': 1.0952011346817017, 'validation/accuracy': 0.650119960308075, 'validation/loss': 1.511588215827942, 'validation/num_examples': 50000, 'test/accuracy': 0.523900032043457, 'test/loss': 2.1890480518341064, 'test/num_examples': 10000, 'score': 37285.95492887497, 'total_duration': 40269.415907382965, 'accumulated_submission_time': 37285.95492887497, 'accumulated_eval_time': 2967.8202724456787, 'accumulated_logging_time': 6.056591749191284, 'global_step': 89971, 'preemption_count': 0}), (90825, {'train/accuracy': 0.6939971446990967, 'train/loss': 1.3381785154342651, 'validation/accuracy': 0.6429799795150757, 'validation/loss': 1.5819443464279175, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.234989881515503, 'test/num_examples': 10000, 'score': 37796.54807496071, 'total_duration': 40813.380023002625, 'accumulated_submission_time': 37796.54807496071, 'accumulated_eval_time': 3001.0501289367676, 'accumulated_logging_time': 6.099925994873047, 'global_step': 90825, 'preemption_count': 0}), (91495, {'train/accuracy': 0.7225565910339355, 'train/loss': 1.2032887935638428, 'validation/accuracy': 0.653719961643219, 'validation/loss': 1.5242154598236084, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.1811347007751465, 'test/num_examples': 10000, 'score': 38306.91541457176, 'total_duration': 41357.423248529434, 'accumulated_submission_time': 38306.91541457176, 'accumulated_eval_time': 3034.603466272354, 'accumulated_logging_time': 6.15294337272644, 'global_step': 91495, 'preemption_count': 0}), (92011, {'train/accuracy': 0.7049385905265808, 'train/loss': 1.3060574531555176, 'validation/accuracy': 0.6490399837493896, 'validation/loss': 1.5620735883712769, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.249709129333496, 'test/num_examples': 10000, 'score': 38816.94752573967, 'total_duration': 41899.378648757935, 'accumulated_submission_time': 38816.94752573967, 'accumulated_eval_time': 3066.4184379577637, 'accumulated_logging_time': 6.203336715698242, 'global_step': 92011, 'preemption_count': 0}), (92737, {'train/accuracy': 0.729910671710968, 'train/loss': 1.178239107131958, 'validation/accuracy': 0.6497399806976318, 'validation/loss': 1.54465913772583, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.2249951362609863, 'test/num_examples': 10000, 'score': 39327.23157310486, 'total_duration': 42442.51089644432, 'accumulated_submission_time': 39327.23157310486, 'accumulated_eval_time': 3099.135130405426, 'accumulated_logging_time': 6.254950284957886, 'global_step': 92737, 'preemption_count': 0}), (93268, {'train/accuracy': 0.7146045565605164, 'train/loss': 1.2338337898254395, 'validation/accuracy': 0.6588999629020691, 'validation/loss': 1.49222731590271, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.154205799102783, 'test/num_examples': 10000, 'score': 39837.59973573685, 'total_duration': 42986.82591319084, 'accumulated_submission_time': 39837.59973573685, 'accumulated_eval_time': 3132.935188770294, 'accumulated_logging_time': 6.344084739685059, 'global_step': 93268, 'preemption_count': 0}), (93905, {'train/accuracy': 0.699617326259613, 'train/loss': 1.2866594791412354, 'validation/accuracy': 0.6510599851608276, 'validation/loss': 1.5093066692352295, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.1932735443115234, 'test/num_examples': 10000, 'score': 40347.65095090866, 'total_duration': 43528.60744023323, 'accumulated_submission_time': 40347.65095090866, 'accumulated_eval_time': 3164.545352935791, 'accumulated_logging_time': 6.395532846450806, 'global_step': 93905, 'preemption_count': 0}), (94497, {'train/accuracy': 0.7049983739852905, 'train/loss': 1.280906081199646, 'validation/accuracy': 0.6481800079345703, 'validation/loss': 1.5424549579620361, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.210353374481201, 'test/num_examples': 10000, 'score': 40857.67301392555, 'total_duration': 44073.42842292786, 'accumulated_submission_time': 40857.67301392555, 'accumulated_eval_time': 3199.2119233608246, 'accumulated_logging_time': 6.463046073913574, 'global_step': 94497, 'preemption_count': 0}), (95059, {'train/accuracy': 0.7104392647743225, 'train/loss': 1.248903751373291, 'validation/accuracy': 0.6591799855232239, 'validation/loss': 1.47475266456604, 'validation/num_examples': 50000, 'test/accuracy': 0.5375000238418579, 'test/loss': 2.127592086791992, 'test/num_examples': 10000, 'score': 41367.95849776268, 'total_duration': 44616.80377650261, 'accumulated_submission_time': 41367.95849776268, 'accumulated_eval_time': 3232.173087835312, 'accumulated_logging_time': 6.531429767608643, 'global_step': 95059, 'preemption_count': 0}), (95634, {'train/accuracy': 0.7330994606018066, 'train/loss': 1.1398533582687378, 'validation/accuracy': 0.6633399724960327, 'validation/loss': 1.4489723443984985, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.0861051082611084, 'test/num_examples': 10000, 'score': 41878.2607088089, 'total_duration': 45160.58663845062, 'accumulated_submission_time': 41878.2607088089, 'accumulated_eval_time': 3265.4662256240845, 'accumulated_logging_time': 6.655385971069336, 'global_step': 95634, 'preemption_count': 0}), (96311, {'train/accuracy': 0.7125119566917419, 'train/loss': 1.2434674501419067, 'validation/accuracy': 0.6585400104522705, 'validation/loss': 1.495395541191101, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.150344133377075, 'test/num_examples': 10000, 'score': 42388.76323437691, 'total_duration': 45704.03474211693, 'accumulated_submission_time': 42388.76323437691, 'accumulated_eval_time': 3298.257212162018, 'accumulated_logging_time': 6.736063003540039, 'global_step': 96311, 'preemption_count': 0}), (96752, {'train/accuracy': 0.7551020383834839, 'train/loss': 1.0866751670837402, 'validation/accuracy': 0.6674000024795532, 'validation/loss': 1.4685901403427124, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.105558395385742, 'test/num_examples': 10000, 'score': 42899.36148262024, 'total_duration': 46247.470718860626, 'accumulated_submission_time': 42899.36148262024, 'accumulated_eval_time': 3330.9632999897003, 'accumulated_logging_time': 6.817828416824341, 'global_step': 96752, 'preemption_count': 0}), (97033, {'train/accuracy': 0.7271205186843872, 'train/loss': 1.1751289367675781, 'validation/accuracy': 0.6590399742126465, 'validation/loss': 1.473814606666565, 'validation/num_examples': 50000, 'test/accuracy': 0.5325000286102295, 'test/loss': 2.1544113159179688, 'test/num_examples': 10000, 'score': 43410.26461362839, 'total_duration': 46793.050782203674, 'accumulated_submission_time': 43410.26461362839, 'accumulated_eval_time': 3365.5568146705627, 'accumulated_logging_time': 6.8711864948272705, 'global_step': 97033, 'preemption_count': 0}), (97477, {'train/accuracy': 0.7231743931770325, 'train/loss': 1.1722335815429688, 'validation/accuracy': 0.6620399951934814, 'validation/loss': 1.4404419660568237, 'validation/num_examples': 50000, 'test/accuracy': 0.5397000312805176, 'test/loss': 2.1169655323028564, 'test/num_examples': 10000, 'score': 43920.425899505615, 'total_duration': 47334.70442152023, 'accumulated_submission_time': 43920.425899505615, 'accumulated_eval_time': 3396.9095027446747, 'accumulated_logging_time': 6.96275782585144, 'global_step': 97477, 'preemption_count': 0}), (97895, {'train/accuracy': 0.7117546200752258, 'train/loss': 1.2318885326385498, 'validation/accuracy': 0.6568199992179871, 'validation/loss': 1.4732506275177002, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.130251169204712, 'test/num_examples': 10000, 'score': 44431.06924343109, 'total_duration': 47879.17121720314, 'accumulated_submission_time': 44431.06924343109, 'accumulated_eval_time': 3430.521162509918, 'accumulated_logging_time': 7.130736351013184, 'global_step': 97895, 'preemption_count': 0}), (98277, {'train/accuracy': 0.7464524507522583, 'train/loss': 1.0883313417434692, 'validation/accuracy': 0.6632399559020996, 'validation/loss': 1.4610949754714966, 'validation/num_examples': 50000, 'test/accuracy': 0.5336000323295593, 'test/loss': 2.124410629272461, 'test/num_examples': 10000, 'score': 44944.380278110504, 'total_duration': 48427.51564669609, 'accumulated_submission_time': 44944.380278110504, 'accumulated_eval_time': 3465.4244379997253, 'accumulated_logging_time': 7.218156576156616, 'global_step': 98277, 'preemption_count': 0}), (98574, {'train/accuracy': 0.7293327450752258, 'train/loss': 1.1673786640167236, 'validation/accuracy': 0.6615399718284607, 'validation/loss': 1.4706826210021973, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.150257110595703, 'test/num_examples': 10000, 'score': 45455.86292171478, 'total_duration': 48975.998775959015, 'accumulated_submission_time': 45455.86292171478, 'accumulated_eval_time': 3502.3311240673065, 'accumulated_logging_time': 7.278835296630859, 'global_step': 98574, 'preemption_count': 0}), (98698, {'train/accuracy': 0.7310666441917419, 'train/loss': 1.1544603109359741, 'validation/accuracy': 0.6657599806785583, 'validation/loss': 1.452366828918457, 'validation/num_examples': 50000, 'test/accuracy': 0.5365000367164612, 'test/loss': 2.1250152587890625, 'test/num_examples': 10000, 'score': 45969.2556245327, 'total_duration': 49526.471385002136, 'accumulated_submission_time': 45969.2556245327, 'accumulated_eval_time': 3539.368305206299, 'accumulated_logging_time': 7.309299468994141, 'global_step': 98698, 'preemption_count': 0}), (99034, {'train/accuracy': 0.7171157598495483, 'train/loss': 1.2178411483764648, 'validation/accuracy': 0.6614399552345276, 'validation/loss': 1.466336965560913, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.1209664344787598, 'test/num_examples': 10000, 'score': 46479.20488524437, 'total_duration': 50068.834535360336, 'accumulated_submission_time': 46479.20488524437, 'accumulated_eval_time': 3571.7142674922943, 'accumulated_logging_time': 7.3412628173828125, 'global_step': 99034, 'preemption_count': 0}), (99168, {'train/accuracy': 0.7145846486091614, 'train/loss': 1.2254630327224731, 'validation/accuracy': 0.6559399962425232, 'validation/loss': 1.487714171409607, 'validation/num_examples': 50000, 'test/accuracy': 0.5264000296592712, 'test/loss': 2.176237106323242, 'test/num_examples': 10000, 'score': 46991.432193279266, 'total_duration': 50614.84516477585, 'accumulated_submission_time': 46991.432193279266, 'accumulated_eval_time': 3605.4140617847443, 'accumulated_logging_time': 7.410963535308838, 'global_step': 99168, 'preemption_count': 0}), (99319, {'train/accuracy': 0.7199457883834839, 'train/loss': 1.200421690940857, 'validation/accuracy': 0.6664800047874451, 'validation/loss': 1.4445267915725708, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.1042182445526123, 'test/num_examples': 10000, 'score': 47501.75345444679, 'total_duration': 51157.02963376045, 'accumulated_submission_time': 47501.75345444679, 'accumulated_eval_time': 3637.2323274612427, 'accumulated_logging_time': 7.440806150436401, 'global_step': 99319, 'preemption_count': 0}), (99552, {'train/accuracy': 0.7219586968421936, 'train/loss': 1.1822550296783447, 'validation/accuracy': 0.6655399799346924, 'validation/loss': 1.4443949460983276, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.0771679878234863, 'test/num_examples': 10000, 'score': 48011.75184464455, 'total_duration': 51699.28551030159, 'accumulated_submission_time': 48011.75184464455, 'accumulated_eval_time': 3669.436831474304, 'accumulated_logging_time': 7.47026515007019, 'global_step': 99552, 'preemption_count': 0}), (99927, {'train/accuracy': 0.7403539419174194, 'train/loss': 1.1030845642089844, 'validation/accuracy': 0.6631999611854553, 'validation/loss': 1.4522411823272705, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.1274261474609375, 'test/num_examples': 10000, 'score': 48523.62880039215, 'total_duration': 52247.7555706501, 'accumulated_submission_time': 48523.62880039215, 'accumulated_eval_time': 3705.9448721408844, 'accumulated_logging_time': 7.514772415161133, 'global_step': 99927, 'preemption_count': 0}), (100293, {'train/accuracy': 0.7253866195678711, 'train/loss': 1.1838643550872803, 'validation/accuracy': 0.6637799739837646, 'validation/loss': 1.45968759059906, 'validation/num_examples': 50000, 'test/accuracy': 0.5396000146865845, 'test/loss': 2.1015052795410156, 'test/num_examples': 10000, 'score': 49034.39515399933, 'total_duration': 52793.00626826286, 'accumulated_submission_time': 49034.39515399933, 'accumulated_eval_time': 3740.3090307712555, 'accumulated_logging_time': 7.594053268432617, 'global_step': 100293, 'preemption_count': 0}), (100680, {'train/accuracy': 0.722078263759613, 'train/loss': 1.1988540887832642, 'validation/accuracy': 0.6609599590301514, 'validation/loss': 1.4627281427383423, 'validation/num_examples': 50000, 'test/accuracy': 0.5355000495910645, 'test/loss': 2.1217551231384277, 'test/num_examples': 10000, 'score': 49546.82700634003, 'total_duration': 53341.758776426315, 'accumulated_submission_time': 49546.82700634003, 'accumulated_eval_time': 3776.4906940460205, 'accumulated_logging_time': 7.692488431930542, 'global_step': 100680, 'preemption_count': 0}), (100844, {'train/accuracy': 0.7101601958274841, 'train/loss': 1.2198963165283203, 'validation/accuracy': 0.6584799885749817, 'validation/loss': 1.4676671028137207, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.120868444442749, 'test/num_examples': 10000, 'score': 50058.8605761528, 'total_duration': 53886.2602994442, 'accumulated_submission_time': 50058.8605761528, 'accumulated_eval_time': 3808.824368953705, 'accumulated_logging_time': 7.808620452880859, 'global_step': 100844, 'preemption_count': 0}), (101393, {'train/accuracy': 0.7495017647743225, 'train/loss': 1.0902628898620605, 'validation/accuracy': 0.6667199730873108, 'validation/loss': 1.4508531093597412, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.1181185245513916, 'test/num_examples': 10000, 'score': 50569.91860842705, 'total_duration': 54428.698112010956, 'accumulated_submission_time': 50569.91860842705, 'accumulated_eval_time': 3840.116666316986, 'accumulated_logging_time': 7.836751461029053, 'global_step': 101393, 'preemption_count': 0}), (101753, {'train/accuracy': 0.7218191623687744, 'train/loss': 1.2033320665359497, 'validation/accuracy': 0.6624199748039246, 'validation/loss': 1.4756213426589966, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.1381893157958984, 'test/num_examples': 10000, 'score': 51081.10740876198, 'total_duration': 54973.17938923836, 'accumulated_submission_time': 51081.10740876198, 'accumulated_eval_time': 3873.3276517391205, 'accumulated_logging_time': 7.879081964492798, 'global_step': 101753, 'preemption_count': 0}), (102015, {'train/accuracy': 0.7227160334587097, 'train/loss': 1.2286900281906128, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.5014077425003052, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.162128210067749, 'test/num_examples': 10000, 'score': 51592.13065266609, 'total_duration': 55517.132984638214, 'accumulated_submission_time': 51592.13065266609, 'accumulated_eval_time': 3906.1614899635315, 'accumulated_logging_time': 7.946697950363159, 'global_step': 102015, 'preemption_count': 0}), (102485, {'train/accuracy': 0.7192482352256775, 'train/loss': 1.2020204067230225, 'validation/accuracy': 0.6642599701881409, 'validation/loss': 1.4513390064239502, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.098752737045288, 'test/num_examples': 10000, 'score': 52104.00662469864, 'total_duration': 56064.208756923676, 'accumulated_submission_time': 52104.00662469864, 'accumulated_eval_time': 3941.2799973487854, 'accumulated_logging_time': 7.978222846984863, 'global_step': 102485, 'preemption_count': 0}), (102728, {'train/accuracy': 0.7487045526504517, 'train/loss': 1.0697017908096313, 'validation/accuracy': 0.6563999652862549, 'validation/loss': 1.480522632598877, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.160945415496826, 'test/num_examples': 10000, 'score': 52615.63793873787, 'total_duration': 56609.717522382736, 'accumulated_submission_time': 52615.63793873787, 'accumulated_eval_time': 3975.0773010253906, 'accumulated_logging_time': 8.031734943389893, 'global_step': 102728, 'preemption_count': 0}), (102989, {'train/accuracy': 0.737723171710968, 'train/loss': 1.1194742918014526, 'validation/accuracy': 0.6657800078392029, 'validation/loss': 1.4356077909469604, 'validation/num_examples': 50000, 'test/accuracy': 0.5454000234603882, 'test/loss': 2.073362112045288, 'test/num_examples': 10000, 'score': 53126.63229942322, 'total_duration': 57152.16341352463, 'accumulated_submission_time': 53126.63229942322, 'accumulated_eval_time': 4006.4729900360107, 'accumulated_logging_time': 8.061196088790894, 'global_step': 102989, 'preemption_count': 0}), (103482, {'train/accuracy': 0.7254862785339355, 'train/loss': 1.1640537977218628, 'validation/accuracy': 0.6676200032234192, 'validation/loss': 1.4284816980361938, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.080355405807495, 'test/num_examples': 10000, 'score': 53637.82115983963, 'total_duration': 57699.94142770767, 'accumulated_submission_time': 53637.82115983963, 'accumulated_eval_time': 4042.9795672893524, 'accumulated_logging_time': 8.09032654762268, 'global_step': 103482, 'preemption_count': 0}), (103606, {'train/accuracy': 0.7208625674247742, 'train/loss': 1.1963168382644653, 'validation/accuracy': 0.662559986114502, 'validation/loss': 1.4623286724090576, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.100508213043213, 'test/num_examples': 10000, 'score': 54150.564057826996, 'total_duration': 58252.30419373512, 'accumulated_submission_time': 54150.564057826996, 'accumulated_eval_time': 4082.5324823856354, 'accumulated_logging_time': 8.1444571018219, 'global_step': 103606, 'preemption_count': 0}), (103730, {'train/accuracy': 0.7204639315605164, 'train/loss': 1.2202650308609009, 'validation/accuracy': 0.6645599603652954, 'validation/loss': 1.4677965641021729, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.0924293994903564, 'test/num_examples': 10000, 'score': 54663.0922563076, 'total_duration': 58798.67719864845, 'accumulated_submission_time': 54663.0922563076, 'accumulated_eval_time': 4116.265195131302, 'accumulated_logging_time': 8.2440664768219, 'global_step': 103730, 'preemption_count': 0}), (104079, {'train/accuracy': 0.7202447056770325, 'train/loss': 1.2129395008087158, 'validation/accuracy': 0.6656399965286255, 'validation/loss': 1.457554817199707, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.112750768661499, 'test/num_examples': 10000, 'score': 55174.524755477905, 'total_duration': 59345.44605708122, 'accumulated_submission_time': 55174.524755477905, 'accumulated_eval_time': 4151.535323381424, 'accumulated_logging_time': 8.273392915725708, 'global_step': 104079, 'preemption_count': 0}), (104434, {'train/accuracy': 0.7488042116165161, 'train/loss': 1.0662858486175537, 'validation/accuracy': 0.6728799939155579, 'validation/loss': 1.408199667930603, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.04132342338562, 'test/num_examples': 10000, 'score': 55684.83670759201, 'total_duration': 59889.59834122658, 'accumulated_submission_time': 55684.83670759201, 'accumulated_eval_time': 4185.27242064476, 'accumulated_logging_time': 8.340363502502441, 'global_step': 104434, 'preemption_count': 0}), (104909, {'train/accuracy': 0.7283760905265808, 'train/loss': 1.1514848470687866, 'validation/accuracy': 0.6637799739837646, 'validation/loss': 1.4325437545776367, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.0986835956573486, 'test/num_examples': 10000, 'score': 56194.78654503822, 'total_duration': 60433.91994857788, 'accumulated_submission_time': 56194.78654503822, 'accumulated_eval_time': 4219.493124723434, 'accumulated_logging_time': 8.441278219223022, 'global_step': 104909, 'preemption_count': 0}), (105254, {'train/accuracy': 0.7225565910339355, 'train/loss': 1.1946988105773926, 'validation/accuracy': 0.6688599586486816, 'validation/loss': 1.433799386024475, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.1130459308624268, 'test/num_examples': 10000, 'score': 56705.52609848976, 'total_duration': 60982.34839582443, 'accumulated_submission_time': 56705.52609848976, 'accumulated_eval_time': 4257.101182937622, 'accumulated_logging_time': 8.486390352249146, 'global_step': 105254, 'preemption_count': 0}), (105443, {'train/accuracy': 0.7262834906578064, 'train/loss': 1.2042301893234253, 'validation/accuracy': 0.6696400046348572, 'validation/loss': 1.4560136795043945, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.113462209701538, 'test/num_examples': 10000, 'score': 57216.15009188652, 'total_duration': 61525.36075305939, 'accumulated_submission_time': 57216.15009188652, 'accumulated_eval_time': 4289.408891439438, 'accumulated_logging_time': 8.547240495681763, 'global_step': 105443, 'preemption_count': 0}), (106003, {'train/accuracy': 0.7354711294174194, 'train/loss': 1.140196681022644, 'validation/accuracy': 0.6699199676513672, 'validation/loss': 1.4432976245880127, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.1022963523864746, 'test/num_examples': 10000, 'score': 57726.20394182205, 'total_duration': 62068.80967712402, 'accumulated_submission_time': 57726.20394182205, 'accumulated_eval_time': 4322.711086988449, 'accumulated_logging_time': 8.580191373825073, 'global_step': 106003, 'preemption_count': 0}), (106564, {'train/accuracy': 0.7276785373687744, 'train/loss': 1.1678117513656616, 'validation/accuracy': 0.6691399812698364, 'validation/loss': 1.4272679090499878, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.067735195159912, 'test/num_examples': 10000, 'score': 58236.15479302406, 'total_duration': 62611.15320158005, 'accumulated_submission_time': 58236.15479302406, 'accumulated_eval_time': 4354.939691781998, 'accumulated_logging_time': 8.68401026725769, 'global_step': 106564, 'preemption_count': 0}), (107140, {'train/accuracy': 0.7466118931770325, 'train/loss': 1.0786799192428589, 'validation/accuracy': 0.6642599701881409, 'validation/loss': 1.4460047483444214, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.1041030883789062, 'test/num_examples': 10000, 'score': 58746.72533559799, 'total_duration': 63152.96120882034, 'accumulated_submission_time': 58746.72533559799, 'accumulated_eval_time': 4386.002843618393, 'accumulated_logging_time': 8.796464204788208, 'global_step': 107140, 'preemption_count': 0}), (107715, {'train/accuracy': 0.7308274507522583, 'train/loss': 1.1577601432800293, 'validation/accuracy': 0.6692599654197693, 'validation/loss': 1.4366854429244995, 'validation/num_examples': 50000, 'test/accuracy': 0.5412999987602234, 'test/loss': 2.1156527996063232, 'test/num_examples': 10000, 'score': 59256.584094285965, 'total_duration': 63695.701869249344, 'accumulated_submission_time': 59256.584094285965, 'accumulated_eval_time': 4418.74244594574, 'accumulated_logging_time': 8.87801480293274, 'global_step': 107715, 'preemption_count': 0}), (107950, {'train/accuracy': 0.7295918464660645, 'train/loss': 1.143438458442688, 'validation/accuracy': 0.6731599569320679, 'validation/loss': 1.4076335430145264, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.050673246383667, 'test/num_examples': 10000, 'score': 59767.48608708382, 'total_duration': 64241.538284778595, 'accumulated_submission_time': 59767.48608708382, 'accumulated_eval_time': 4453.555124044418, 'accumulated_logging_time': 8.974353551864624, 'global_step': 107950, 'preemption_count': 0}), (108074, {'train/accuracy': 0.7307078838348389, 'train/loss': 1.1458221673965454, 'validation/accuracy': 0.6693599820137024, 'validation/loss': 1.4208639860153198, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.05647349357605, 'test/num_examples': 10000, 'score': 60278.09408926964, 'total_duration': 64788.09173464775, 'accumulated_submission_time': 60278.09408926964, 'accumulated_eval_time': 4489.4566123485565, 'accumulated_logging_time': 9.005134344100952, 'global_step': 108074, 'preemption_count': 0}), (108198, {'train/accuracy': 0.7258051633834839, 'train/loss': 1.1665153503417969, 'validation/accuracy': 0.6667799949645996, 'validation/loss': 1.4418220520019531, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.0967540740966797, 'test/num_examples': 10000, 'score': 60789.18449521065, 'total_duration': 65336.05056142807, 'accumulated_submission_time': 60789.18449521065, 'accumulated_eval_time': 4526.244311094284, 'accumulated_logging_time': 9.072465181350708, 'global_step': 108198, 'preemption_count': 0}), (108322, {'train/accuracy': 0.7287747263908386, 'train/loss': 1.1600546836853027, 'validation/accuracy': 0.6708799600601196, 'validation/loss': 1.415154218673706, 'validation/num_examples': 50000, 'test/accuracy': 0.5455999970436096, 'test/loss': 2.070884943008423, 'test/num_examples': 10000, 'score': 61300.36177492142, 'total_duration': 65883.01047825813, 'accumulated_submission_time': 61300.36177492142, 'accumulated_eval_time': 4561.984658479691, 'accumulated_logging_time': 9.1028311252594, 'global_step': 108322, 'preemption_count': 0}), (108446, {'train/accuracy': 0.7276985049247742, 'train/loss': 1.1638103723526, 'validation/accuracy': 0.669979989528656, 'validation/loss': 1.4223161935806274, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.0719552040100098, 'test/num_examples': 10000, 'score': 61812.13180017471, 'total_duration': 66426.68374371529, 'accumulated_submission_time': 61812.13180017471, 'accumulated_eval_time': 4593.8428428173065, 'accumulated_logging_time': 9.134752035140991, 'global_step': 108446, 'preemption_count': 0}), (109100, {'train/accuracy': 0.7307278513908386, 'train/loss': 1.1489639282226562, 'validation/accuracy': 0.6688799858093262, 'validation/loss': 1.4371027946472168, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.0681846141815186, 'test/num_examples': 10000, 'score': 62322.05839467049, 'total_duration': 66973.30617070198, 'accumulated_submission_time': 62322.05839467049, 'accumulated_eval_time': 4630.436698913574, 'accumulated_logging_time': 9.166320085525513, 'global_step': 109100, 'preemption_count': 0}), (109747, {'train/accuracy': 0.7344746589660645, 'train/loss': 1.1202000379562378, 'validation/accuracy': 0.6791999936103821, 'validation/loss': 1.3814679384231567, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 2.0251922607421875, 'test/num_examples': 10000, 'score': 62832.286561489105, 'total_duration': 67514.60792064667, 'accumulated_submission_time': 62832.286561489105, 'accumulated_eval_time': 4661.330931186676, 'accumulated_logging_time': 9.274696111679077, 'global_step': 109747, 'preemption_count': 0}), (110054, {'train/accuracy': 0.760164201259613, 'train/loss': 1.0102070569992065, 'validation/accuracy': 0.6690599918365479, 'validation/loss': 1.4101848602294922, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.077561140060425, 'test/num_examples': 10000, 'score': 63343.556574344635, 'total_duration': 68057.08423185349, 'accumulated_submission_time': 63343.556574344635, 'accumulated_eval_time': 4692.430089712143, 'accumulated_logging_time': 9.3498694896698, 'global_step': 110054, 'preemption_count': 0}), (110425, {'train/accuracy': 0.7470703125, 'train/loss': 1.0757482051849365, 'validation/accuracy': 0.6789999604225159, 'validation/loss': 1.386796474456787, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.053622245788574, 'test/num_examples': 10000, 'score': 63854.71561408043, 'total_duration': 68602.3628358841, 'accumulated_submission_time': 63854.71561408043, 'accumulated_eval_time': 4726.451149463654, 'accumulated_logging_time': 9.408446073532104, 'global_step': 110425, 'preemption_count': 0}), (110793, {'train/accuracy': 0.7336973547935486, 'train/loss': 1.1514904499053955, 'validation/accuracy': 0.6708999872207642, 'validation/loss': 1.4322401285171509, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.0793941020965576, 'test/num_examples': 10000, 'score': 64364.8362364769, 'total_duration': 69145.91933512688, 'accumulated_submission_time': 64364.8362364769, 'accumulated_eval_time': 4759.72500872612, 'accumulated_logging_time': 9.533119916915894, 'global_step': 110793, 'preemption_count': 0}), (111122, {'train/accuracy': 0.7323421239852905, 'train/loss': 1.14013671875, 'validation/accuracy': 0.6732000112533569, 'validation/loss': 1.4115277528762817, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.061681032180786, 'test/num_examples': 10000, 'score': 64875.60917925835, 'total_duration': 69690.9540605545, 'accumulated_submission_time': 64875.60917925835, 'accumulated_eval_time': 4793.893170118332, 'accumulated_logging_time': 9.592237949371338, 'global_step': 111122, 'preemption_count': 0}), (111444, {'train/accuracy': 0.7626554369926453, 'train/loss': 1.007634162902832, 'validation/accuracy': 0.672819972038269, 'validation/loss': 1.4071537256240845, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.071359872817993, 'test/num_examples': 10000, 'score': 65386.51100897789, 'total_duration': 70236.95640730858, 'accumulated_submission_time': 65386.51100897789, 'accumulated_eval_time': 4828.895120859146, 'accumulated_logging_time': 9.656341314315796, 'global_step': 111444, 'preemption_count': 0}), (111816, {'train/accuracy': 0.7431042790412903, 'train/loss': 1.1029917001724243, 'validation/accuracy': 0.6697799563407898, 'validation/loss': 1.4257930517196655, 'validation/num_examples': 50000, 'test/accuracy': 0.5478000044822693, 'test/loss': 2.0903494358062744, 'test/num_examples': 10000, 'score': 65897.54716444016, 'total_duration': 70780.78152060509, 'accumulated_submission_time': 65897.54716444016, 'accumulated_eval_time': 4861.572622060776, 'accumulated_logging_time': 9.72568941116333, 'global_step': 111816, 'preemption_count': 0})], 'global_step': 112185}
I0307 23:33:02.888075 140393707492544 submission_runner.py:649] Timing: 66407.78679919243
I0307 23:33:02.888124 140393707492544 submission_runner.py:651] Total number of evals: 130
I0307 23:33:02.888153 140393707492544 submission_runner.py:652] ====================
I0307 23:33:02.888380 140393707492544 submission_runner.py:750] Final imagenet_resnet score: 0
