python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-877515129 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-09-36-14.log
2025-03-07 09:36:33.294971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741340193.918332       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741340194.286503       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 09:37:29.248135 140413841560768 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax.
I0307 09:37:31.660965 140413841560768 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 09:37:31.663887 140413841560768 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 09:37:31.672788 140413841560768 submission_runner.py:606] Using RNG seed -877515129
I0307 09:37:34.827248 140413841560768 submission_runner.py:615] --- Tuning run 4/5 ---
I0307 09:37:34.827501 140413841560768 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_4.
I0307 09:37:34.827723 140413841560768 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_4/hparams.json.
I0307 09:37:35.085632 140413841560768 submission_runner.py:218] Initializing dataset.
I0307 09:37:36.921561 140413841560768 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:37:37.271658 140413841560768 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:37:37.647065 140413841560768 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:37:39.648372 140413841560768 submission_runner.py:229] Initializing model.
I0307 09:38:05.052016 140413841560768 submission_runner.py:272] Initializing optimizer.
I0307 09:38:06.232630 140413841560768 submission_runner.py:279] Initializing metrics bundle.
I0307 09:38:06.232894 140413841560768 submission_runner.py:301] Initializing checkpoint and logger.
I0307 09:38:06.234144 140413841560768 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0307 09:38:06.234254 140413841560768 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_4/meta_data_0.json.
I0307 09:38:06.783828 140413841560768 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/imagenet_resnet_jax/trial_4/flags_0.json.
I0307 09:38:07.170290 140413841560768 submission_runner.py:337] Starting training loop.
I0307 09:39:05.778278 140272331642624 logging_writer.py:48] [0] global_step=0, grad_norm=0.6812232732772827, loss=6.930140018463135
I0307 09:39:06.122029 140413841560768 spec.py:321] Evaluating on the training split.
I0307 09:39:06.584001 140413841560768 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:39:06.607825 140413841560768 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:39:06.649975 140413841560768 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:39:26.914398 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 09:39:27.352132 140413841560768 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:39:27.360355 140413841560768 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 09:39:27.409013 140413841560768 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 09:40:07.956079 140413841560768 spec.py:349] Evaluating on the test split.
I0307 09:40:08.423143 140413841560768 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:40:08.466190 140413841560768 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 09:40:08.513357 140413841560768 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 09:40:40.743021 140413841560768 submission_runner.py:469] Time since start: 153.57s, 	Step: 1, 	{'train/accuracy': 0.000996492337435484, 'train/loss': 6.911397457122803, 'validation/accuracy': 0.0008999999845400453, 'validation/loss': 6.9117817878723145, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.911991119384766, 'test/num_examples': 10000, 'score': 58.95149374008179, 'total_duration': 153.57268047332764, 'accumulated_submission_time': 58.95149374008179, 'accumulated_eval_time': 94.62093567848206, 'accumulated_logging_time': 0}
I0307 09:40:40.798496 140257626404608 logging_writer.py:48] [1] accumulated_eval_time=94.6209, accumulated_logging_time=0, accumulated_submission_time=58.9515, global_step=1, preemption_count=0, score=58.9515, test/accuracy=0.0011, test/loss=6.91199, test/num_examples=10000, total_duration=153.573, train/accuracy=0.000996492, train/loss=6.9114, validation/accuracy=0.0009, validation/loss=6.91178, validation/num_examples=50000
I0307 09:41:17.154743 140257618011904 logging_writer.py:48] [100] global_step=100, grad_norm=0.7852025628089905, loss=6.677527904510498
I0307 09:41:54.479156 140257626404608 logging_writer.py:48] [200] global_step=200, grad_norm=0.9162800908088684, loss=6.283400535583496
I0307 09:42:32.339208 140257618011904 logging_writer.py:48] [300] global_step=300, grad_norm=2.0460872650146484, loss=6.05780029296875
I0307 09:43:10.160347 140257626404608 logging_writer.py:48] [400] global_step=400, grad_norm=2.766341209411621, loss=5.754971027374268
I0307 09:43:48.524781 140257618011904 logging_writer.py:48] [500] global_step=500, grad_norm=5.2135725021362305, loss=5.6332221031188965
I0307 09:44:26.996869 140257626404608 logging_writer.py:48] [600] global_step=600, grad_norm=4.39957332611084, loss=5.391834735870361
I0307 09:45:05.216190 140257618011904 logging_writer.py:48] [700] global_step=700, grad_norm=2.3355038166046143, loss=5.113019943237305
I0307 09:45:43.476439 140257626404608 logging_writer.py:48] [800] global_step=800, grad_norm=4.588608264923096, loss=5.0122528076171875
I0307 09:46:21.571766 140257618011904 logging_writer.py:48] [900] global_step=900, grad_norm=2.8520500659942627, loss=4.765174865722656
I0307 09:47:00.156949 140257626404608 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.9600906372070312, loss=4.6853203773498535
I0307 09:47:38.271360 140257618011904 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.911640167236328, loss=4.544067859649658
I0307 09:48:17.226925 140257626404608 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.3217697143554688, loss=4.557408332824707
I0307 09:48:55.197802 140257618011904 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.2646732330322266, loss=4.162847995758057
I0307 09:49:11.129879 140413841560768 spec.py:321] Evaluating on the training split.
I0307 09:49:22.955529 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 09:49:49.926873 140413841560768 spec.py:349] Evaluating on the test split.
I0307 09:49:52.107912 140413841560768 submission_runner.py:469] Time since start: 704.94s, 	Step: 1342, 	{'train/accuracy': 0.1727917641401291, 'train/loss': 4.282511234283447, 'validation/accuracy': 0.14531999826431274, 'validation/loss': 4.559546947479248, 'validation/num_examples': 50000, 'test/accuracy': 0.10680000483989716, 'test/loss': 5.042964458465576, 'test/num_examples': 10000, 'score': 569.0983226299286, 'total_duration': 704.9375741481781, 'accumulated_submission_time': 569.0983226299286, 'accumulated_eval_time': 135.59893321990967, 'accumulated_logging_time': 0.06599211692810059}
I0307 09:49:52.130957 140257634797312 logging_writer.py:48] [1342] accumulated_eval_time=135.599, accumulated_logging_time=0.0659921, accumulated_submission_time=569.098, global_step=1342, preemption_count=0, score=569.098, test/accuracy=0.1068, test/loss=5.04296, test/num_examples=10000, total_duration=704.938, train/accuracy=0.172792, train/loss=4.28251, validation/accuracy=0.14532, validation/loss=4.55955, validation/num_examples=50000
I0307 09:50:14.969183 140257643190016 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.2198686599731445, loss=4.155579566955566
I0307 09:50:53.610395 140257634797312 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.113687753677368, loss=4.131833076477051
I0307 09:51:31.568952 140257643190016 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.1086206436157227, loss=3.9849436283111572
I0307 09:52:09.738199 140257634797312 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.5449917316436768, loss=3.9247660636901855
I0307 09:52:48.021644 140257643190016 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.0216288566589355, loss=3.6688072681427
I0307 09:53:27.858794 140257634797312 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.782555341720581, loss=3.652446985244751
I0307 09:54:09.902052 140257643190016 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.6250510215759277, loss=3.6019153594970703
I0307 09:54:50.478215 140257634797312 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.8011335134506226, loss=3.5836081504821777
I0307 09:55:30.365096 140257643190016 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.662208914756775, loss=3.4391543865203857
I0307 09:56:11.075863 140257634797312 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.9329192638397217, loss=3.5353188514709473
I0307 09:56:50.314141 140257643190016 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.5949167013168335, loss=3.252615213394165
I0307 09:57:30.059337 140257634797312 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.4162116050720215, loss=3.443325996398926
I0307 09:58:08.637135 140257643190016 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.435170292854309, loss=3.347548007965088
I0307 09:58:22.281057 140413841560768 spec.py:321] Evaluating on the training split.
I0307 09:58:33.658272 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 09:59:04.784742 140413841560768 spec.py:349] Evaluating on the test split.
I0307 09:59:06.575940 140413841560768 submission_runner.py:469] Time since start: 1259.41s, 	Step: 2636, 	{'train/accuracy': 0.36039140820503235, 'train/loss': 2.9724817276000977, 'validation/accuracy': 0.30587998032569885, 'validation/loss': 3.3215339183807373, 'validation/num_examples': 50000, 'test/accuracy': 0.22420001029968262, 'test/loss': 3.9959349632263184, 'test/num_examples': 10000, 'score': 1079.069636106491, 'total_duration': 1259.4055924415588, 'accumulated_submission_time': 1079.069636106491, 'accumulated_eval_time': 179.89376163482666, 'accumulated_logging_time': 0.10499739646911621}
I0307 09:59:06.600031 140257634797312 logging_writer.py:48] [2636] accumulated_eval_time=179.894, accumulated_logging_time=0.104997, accumulated_submission_time=1079.07, global_step=2636, preemption_count=0, score=1079.07, test/accuracy=0.2242, test/loss=3.99593, test/num_examples=10000, total_duration=1259.41, train/accuracy=0.360391, train/loss=2.97248, validation/accuracy=0.30588, validation/loss=3.32153, validation/num_examples=50000
I0307 09:59:31.877306 140257643190016 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.5171775817871094, loss=3.266209125518799
I0307 10:00:10.216398 140257634797312 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.3068389892578125, loss=3.2881948947906494
I0307 10:00:48.979095 140257643190016 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.359471321105957, loss=3.254810333251953
I0307 10:01:27.346251 140257634797312 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4331098794937134, loss=3.0564780235290527
I0307 10:02:05.732522 140257643190016 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1288472414016724, loss=3.0669028759002686
I0307 10:02:43.990273 140257634797312 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.3572211265563965, loss=3.025153875350952
I0307 10:03:22.530628 140257643190016 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0600014925003052, loss=3.2408745288848877
I0307 10:04:01.005794 140257634797312 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.248895525932312, loss=2.9532313346862793
I0307 10:04:39.333599 140257643190016 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.384798526763916, loss=3.067525863647461
I0307 10:05:17.646565 140257634797312 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.2367737293243408, loss=2.8850839138031006
I0307 10:05:55.898534 140257643190016 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.4465179443359375, loss=2.8669142723083496
I0307 10:06:34.310705 140257634797312 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.1631572246551514, loss=3.084266185760498
I0307 10:07:12.990480 140257643190016 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8759052753448486, loss=2.9783968925476074
I0307 10:07:36.748352 140413841560768 spec.py:321] Evaluating on the training split.
I0307 10:07:47.902148 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 10:08:10.228717 140413841560768 spec.py:349] Evaluating on the test split.
I0307 10:08:12.067166 140413841560768 submission_runner.py:469] Time since start: 1804.90s, 	Step: 3963, 	{'train/accuracy': 0.3452048599720001, 'train/loss': 3.0669162273406982, 'validation/accuracy': 0.2925799787044525, 'validation/loss': 3.444424867630005, 'validation/num_examples': 50000, 'test/accuracy': 0.21470001339912415, 'test/loss': 4.206352710723877, 'test/num_examples': 10000, 'score': 1589.0803759098053, 'total_duration': 1804.8968324661255, 'accumulated_submission_time': 1589.0803759098053, 'accumulated_eval_time': 215.21254181861877, 'accumulated_logging_time': 0.1381206512451172}
I0307 10:08:12.078151 140257634797312 logging_writer.py:48] [3963] accumulated_eval_time=215.213, accumulated_logging_time=0.138121, accumulated_submission_time=1589.08, global_step=3963, preemption_count=0, score=1589.08, test/accuracy=0.2147, test/loss=4.20635, test/num_examples=10000, total_duration=1804.9, train/accuracy=0.345205, train/loss=3.06692, validation/accuracy=0.29258, validation/loss=3.44442, validation/num_examples=50000
I0307 10:08:26.876159 140257643190016 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7883609533309937, loss=2.861818790435791
I0307 10:09:05.477799 140257634797312 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9579064249992371, loss=2.9464235305786133
I0307 10:09:44.234358 140257643190016 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.7620934247970581, loss=2.905259370803833
I0307 10:10:22.936912 140257634797312 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9078940749168396, loss=2.8611979484558105
I0307 10:11:01.395782 140257643190016 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8200103044509888, loss=2.9223415851593018
I0307 10:11:39.648630 140257634797312 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9794849753379822, loss=2.7247402667999268
I0307 10:12:17.966769 140257643190016 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9488435983657837, loss=2.924175977706909
I0307 10:12:56.665451 140257634797312 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.0620012283325195, loss=2.6769626140594482
I0307 10:13:35.307549 140257643190016 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8966834545135498, loss=2.75398588180542
I0307 10:14:13.689326 140257634797312 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.9871001243591309, loss=2.79846453666687
I0307 10:14:51.865921 140257643190016 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.893862247467041, loss=2.7859153747558594
I0307 10:15:30.617726 140257634797312 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.119163155555725, loss=2.7828617095947266
I0307 10:16:09.330399 140257643190016 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.097344994544983, loss=2.71879506111145
I0307 10:16:42.069529 140413841560768 spec.py:321] Evaluating on the training split.
I0307 10:16:53.026757 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 10:17:15.545933 140413841560768 spec.py:349] Evaluating on the test split.
I0307 10:17:17.394984 140413841560768 submission_runner.py:469] Time since start: 2350.22s, 	Step: 5286, 	{'train/accuracy': 0.43189969658851624, 'train/loss': 2.571624755859375, 'validation/accuracy': 0.374019980430603, 'validation/loss': 2.90041184425354, 'validation/num_examples': 50000, 'test/accuracy': 0.2818000018596649, 'test/loss': 3.609036922454834, 'test/num_examples': 10000, 'score': 2098.938943386078, 'total_duration': 2350.2246432304382, 'accumulated_submission_time': 2098.938943386078, 'accumulated_eval_time': 250.53794384002686, 'accumulated_logging_time': 0.15760183334350586}
I0307 10:17:17.448080 140257634797312 logging_writer.py:48] [5286] accumulated_eval_time=250.538, accumulated_logging_time=0.157602, accumulated_submission_time=2098.94, global_step=5286, preemption_count=0, score=2098.94, test/accuracy=0.2818, test/loss=3.60904, test/num_examples=10000, total_duration=2350.22, train/accuracy=0.4319, train/loss=2.57162, validation/accuracy=0.37402, validation/loss=2.90041, validation/num_examples=50000
I0307 10:17:23.210019 140257643190016 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8609215617179871, loss=2.825599431991577
I0307 10:18:01.435052 140257634797312 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8837796449661255, loss=2.7686917781829834
I0307 10:18:39.862543 140257643190016 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8980749249458313, loss=2.6930344104766846
I0307 10:19:18.265573 140257634797312 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9763368368148804, loss=2.6242518424987793
I0307 10:19:56.778971 140257643190016 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.981157660484314, loss=2.4708213806152344
I0307 10:20:35.554773 140257634797312 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9035496115684509, loss=2.6290271282196045
I0307 10:21:14.215796 140257643190016 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8974061012268066, loss=2.6482908725738525
I0307 10:21:52.684518 140257634797312 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0901216268539429, loss=2.7876827716827393
I0307 10:22:31.682806 140257643190016 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8695419430732727, loss=2.610621929168701
I0307 10:23:10.068223 140257634797312 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8447015881538391, loss=2.608881711959839
I0307 10:23:48.757884 140257643190016 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.0011913776397705, loss=2.5781493186950684
I0307 10:24:27.047470 140257634797312 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.0460759401321411, loss=2.461761236190796
I0307 10:25:05.893459 140257643190016 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.876930832862854, loss=2.683563232421875
I0307 10:25:44.440019 140257634797312 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9879511594772339, loss=2.493130683898926
I0307 10:25:47.628670 140413841560768 spec.py:321] Evaluating on the training split.
I0307 10:25:59.162766 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 10:26:20.654682 140413841560768 spec.py:349] Evaluating on the test split.
I0307 10:26:22.493005 140413841560768 submission_runner.py:469] Time since start: 2895.32s, 	Step: 6609, 	{'train/accuracy': 0.4371013939380646, 'train/loss': 2.521240234375, 'validation/accuracy': 0.3926999866962433, 'validation/loss': 2.80679988861084, 'validation/num_examples': 50000, 'test/accuracy': 0.3006000220775604, 'test/loss': 3.475165843963623, 'test/num_examples': 10000, 'score': 2608.9872546195984, 'total_duration': 2895.3226742744446, 'accumulated_submission_time': 2608.9872546195984, 'accumulated_eval_time': 285.4022305011749, 'accumulated_logging_time': 0.22725439071655273}
I0307 10:26:22.568098 140257643190016 logging_writer.py:48] [6609] accumulated_eval_time=285.402, accumulated_logging_time=0.227254, accumulated_submission_time=2608.99, global_step=6609, preemption_count=0, score=2608.99, test/accuracy=0.3006, test/loss=3.47517, test/num_examples=10000, total_duration=2895.32, train/accuracy=0.437101, train/loss=2.52124, validation/accuracy=0.3927, validation/loss=2.8068, validation/num_examples=50000
I0307 10:26:58.513802 140257634797312 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8987325429916382, loss=2.6832597255706787
I0307 10:27:36.898514 140257643190016 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9384991526603699, loss=2.5617005825042725
I0307 10:28:15.338527 140257634797312 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9728371500968933, loss=2.5285637378692627
I0307 10:28:53.718721 140257643190016 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.996816098690033, loss=2.6746277809143066
I0307 10:29:31.871561 140257634797312 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.028541088104248, loss=2.4834837913513184
I0307 10:30:10.407759 140257643190016 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.905360221862793, loss=2.606011390686035
I0307 10:30:48.561117 140257634797312 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.1255147457122803, loss=2.4060001373291016
I0307 10:31:26.938022 140257643190016 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0218950510025024, loss=2.619877576828003
I0307 10:32:05.099750 140257634797312 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.0201536417007446, loss=2.582548141479492
I0307 10:32:43.583949 140257643190016 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9938094615936279, loss=2.5858943462371826
I0307 10:33:22.091183 140257634797312 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8933505415916443, loss=2.617902994155884
I0307 10:34:01.137071 140257643190016 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.0502325296401978, loss=2.4954349994659424
I0307 10:34:39.436245 140257634797312 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9859735369682312, loss=2.4924697875976562
I0307 10:34:52.863137 140413841560768 spec.py:321] Evaluating on the training split.
I0307 10:35:04.260999 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 10:35:26.562194 140413841560768 spec.py:349] Evaluating on the test split.
I0307 10:35:28.437263 140413841560768 submission_runner.py:469] Time since start: 3441.27s, 	Step: 7936, 	{'train/accuracy': 0.38125795125961304, 'train/loss': 2.9028635025024414, 'validation/accuracy': 0.34926000237464905, 'validation/loss': 3.1355762481689453, 'validation/num_examples': 50000, 'test/accuracy': 0.26440000534057617, 'test/loss': 3.837865114212036, 'test/num_examples': 10000, 'score': 3119.1585879325867, 'total_duration': 3441.2669138908386, 'accumulated_submission_time': 3119.1585879325867, 'accumulated_eval_time': 320.97629475593567, 'accumulated_logging_time': 0.3117954730987549}
I0307 10:35:28.497927 140257643190016 logging_writer.py:48] [7936] accumulated_eval_time=320.976, accumulated_logging_time=0.311795, accumulated_submission_time=3119.16, global_step=7936, preemption_count=0, score=3119.16, test/accuracy=0.2644, test/loss=3.83787, test/num_examples=10000, total_duration=3441.27, train/accuracy=0.381258, train/loss=2.90286, validation/accuracy=0.34926, validation/loss=3.13558, validation/num_examples=50000
I0307 10:35:53.563026 140257634797312 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8423014879226685, loss=2.532191038131714
I0307 10:36:31.864324 140257643190016 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0223616361618042, loss=2.5114989280700684
I0307 10:37:10.496060 140257634797312 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9850615859031677, loss=2.514167308807373
I0307 10:37:48.711864 140257643190016 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9435745477676392, loss=2.5636520385742188
I0307 10:38:27.405100 140257634797312 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.9588219523429871, loss=2.51229190826416
I0307 10:39:05.922548 140257643190016 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.9793916940689087, loss=2.4433352947235107
I0307 10:39:44.336647 140257634797312 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0327019691467285, loss=2.463801383972168
I0307 10:40:22.797908 140257643190016 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0169907808303833, loss=2.469743490219116
I0307 10:41:01.327785 140257634797312 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9907956719398499, loss=2.4775686264038086
I0307 10:41:39.681869 140257643190016 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9346864819526672, loss=2.333829641342163
I0307 10:42:18.319719 140257634797312 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0217562913894653, loss=2.622626543045044
I0307 10:42:56.754812 140257643190016 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1021918058395386, loss=2.437887668609619
I0307 10:43:35.041438 140257634797312 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.933712363243103, loss=2.355520725250244
I0307 10:43:58.604733 140413841560768 spec.py:321] Evaluating on the training split.
I0307 10:44:11.105299 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 10:44:31.692408 140413841560768 spec.py:349] Evaluating on the test split.
I0307 10:44:33.532685 140413841560768 submission_runner.py:469] Time since start: 3986.36s, 	Step: 9261, 	{'train/accuracy': 0.3960060477256775, 'train/loss': 2.7425317764282227, 'validation/accuracy': 0.35694000124931335, 'validation/loss': 3.0479462146759033, 'validation/num_examples': 50000, 'test/accuracy': 0.28220000863075256, 'test/loss': 3.7197985649108887, 'test/num_examples': 10000, 'score': 3629.141862630844, 'total_duration': 3986.362341403961, 'accumulated_submission_time': 3629.141862630844, 'accumulated_eval_time': 355.90418553352356, 'accumulated_logging_time': 0.38298916816711426}
I0307 10:44:33.568249 140257643190016 logging_writer.py:48] [9261] accumulated_eval_time=355.904, accumulated_logging_time=0.382989, accumulated_submission_time=3629.14, global_step=9261, preemption_count=0, score=3629.14, test/accuracy=0.2822, test/loss=3.7198, test/num_examples=10000, total_duration=3986.36, train/accuracy=0.396006, train/loss=2.74253, validation/accuracy=0.35694, validation/loss=3.04795, validation/num_examples=50000
I0307 10:44:49.117827 140257634797312 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.4494549036026, loss=2.5593786239624023
I0307 10:45:27.674260 140257643190016 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0517204999923706, loss=2.610250234603882
I0307 10:46:06.504800 140257634797312 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9063758850097656, loss=2.4348061084747314
I0307 10:46:45.170551 140257643190016 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.1372638940811157, loss=2.4054219722747803
I0307 10:47:23.913738 140257634797312 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.9795547723770142, loss=2.3423564434051514
I0307 10:48:01.999578 140257643190016 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.144511342048645, loss=2.5767807960510254
I0307 10:48:40.433384 140257634797312 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.0414228439331055, loss=2.513108968734741
I0307 10:49:18.733929 140257643190016 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.970259964466095, loss=2.5268893241882324
I0307 10:49:57.438005 140257634797312 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9978733658790588, loss=2.436415672302246
I0307 10:50:36.304865 140257643190016 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9348564147949219, loss=2.4092233180999756
I0307 10:51:14.103913 140257634797312 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.0266244411468506, loss=2.47965669631958
I0307 10:51:51.516580 140257643190016 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.0129601955413818, loss=2.46520733833313
I0307 10:52:29.260440 140257634797312 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.0500264167785645, loss=2.4797277450561523
I0307 10:53:03.644647 140413841560768 spec.py:321] Evaluating on the training split.
I0307 10:53:20.459561 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 10:53:41.395361 140413841560768 spec.py:349] Evaluating on the test split.
I0307 10:53:43.228883 140413841560768 submission_runner.py:469] Time since start: 4536.06s, 	Step: 10592, 	{'train/accuracy': 0.37637513875961304, 'train/loss': 2.8603925704956055, 'validation/accuracy': 0.346019983291626, 'validation/loss': 3.0767273902893066, 'validation/num_examples': 50000, 'test/accuracy': 0.26900002360343933, 'test/loss': 3.727496385574341, 'test/num_examples': 10000, 'score': 4139.016051530838, 'total_duration': 4536.058547735214, 'accumulated_submission_time': 4139.016051530838, 'accumulated_eval_time': 395.4883773326874, 'accumulated_logging_time': 0.48174214363098145}
I0307 10:53:43.279892 140257643190016 logging_writer.py:48] [10592] accumulated_eval_time=395.488, accumulated_logging_time=0.481742, accumulated_submission_time=4139.02, global_step=10592, preemption_count=0, score=4139.02, test/accuracy=0.269, test/loss=3.7275, test/num_examples=10000, total_duration=4536.06, train/accuracy=0.376375, train/loss=2.86039, validation/accuracy=0.34602, validation/loss=3.07673, validation/num_examples=50000
I0307 10:53:46.782449 140257634797312 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9639300107955933, loss=2.3935298919677734
I0307 10:54:25.700486 140257643190016 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.0526939630508423, loss=2.579500675201416
I0307 10:55:04.862113 140257634797312 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.9842941164970398, loss=2.4205942153930664
I0307 10:55:44.071750 140257643190016 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.0850552320480347, loss=2.4975383281707764
I0307 10:56:23.258755 140257634797312 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.0462124347686768, loss=2.3955979347229004
I0307 10:57:02.349806 140257643190016 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.0953022241592407, loss=2.475463628768921
I0307 10:57:41.225135 140257634797312 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9970545172691345, loss=2.3669211864471436
I0307 10:58:19.416444 140257643190016 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.070891261100769, loss=2.4154810905456543
I0307 10:58:58.563227 140257634797312 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0677241086959839, loss=2.4230568408966064
I0307 10:59:37.499101 140257643190016 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.036520004272461, loss=2.5194249153137207
I0307 11:00:15.817182 140257634797312 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.070114016532898, loss=2.5280230045318604
I0307 11:00:53.953242 140257643190016 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8672027587890625, loss=2.3743114471435547
I0307 11:01:32.508219 140257634797312 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.031351923942566, loss=2.437159538269043
I0307 11:02:11.742805 140257643190016 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.1037230491638184, loss=2.386012077331543
I0307 11:02:13.322800 140413841560768 spec.py:321] Evaluating on the training split.
I0307 11:02:30.572101 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 11:02:46.765027 140413841560768 spec.py:349] Evaluating on the test split.
I0307 11:02:48.578125 140413841560768 submission_runner.py:469] Time since start: 5081.41s, 	Step: 11905, 	{'train/accuracy': 0.3257533311843872, 'train/loss': 3.3719565868377686, 'validation/accuracy': 0.29989999532699585, 'validation/loss': 3.5589704513549805, 'validation/num_examples': 50000, 'test/accuracy': 0.22510001063346863, 'test/loss': 4.310132026672363, 'test/num_examples': 10000, 'score': 4648.88237452507, 'total_duration': 5081.407785415649, 'accumulated_submission_time': 4648.88237452507, 'accumulated_eval_time': 430.74365282058716, 'accumulated_logging_time': 0.5416533946990967}
I0307 11:02:48.654697 140257634797312 logging_writer.py:48] [11905] accumulated_eval_time=430.744, accumulated_logging_time=0.541653, accumulated_submission_time=4648.88, global_step=11905, preemption_count=0, score=4648.88, test/accuracy=0.2251, test/loss=4.31013, test/num_examples=10000, total_duration=5081.41, train/accuracy=0.325753, train/loss=3.37196, validation/accuracy=0.2999, validation/loss=3.55897, validation/num_examples=50000
I0307 11:03:26.176599 140257643190016 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0805732011795044, loss=2.3812339305877686
I0307 11:04:04.798912 140257634797312 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9409651756286621, loss=2.314734935760498
I0307 11:04:43.371634 140257643190016 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0032399892807007, loss=2.428499698638916
I0307 11:05:21.647799 140257634797312 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.0177615880966187, loss=2.578080892562866
I0307 11:06:00.569091 140257643190016 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.087715983390808, loss=2.539477586746216
I0307 11:06:39.556173 140257634797312 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.0351508855819702, loss=2.3678689002990723
I0307 11:07:18.817116 140257643190016 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9726170301437378, loss=2.293445587158203
I0307 11:07:57.759050 140257634797312 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.0273339748382568, loss=2.383175849914551
I0307 11:08:36.264606 140257643190016 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9601232409477234, loss=2.417806386947632
I0307 11:09:14.884833 140257634797312 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.1564126014709473, loss=2.4688751697540283
I0307 11:09:53.732206 140257643190016 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9747778177261353, loss=2.420365333557129
I0307 11:10:32.238727 140257634797312 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0520429611206055, loss=2.4508373737335205
I0307 11:11:10.973023 140257643190016 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.0375361442565918, loss=2.492114305496216
I0307 11:11:18.803113 140413841560768 spec.py:321] Evaluating on the training split.
I0307 11:11:36.726012 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 11:11:57.818552 140413841560768 spec.py:349] Evaluating on the test split.
I0307 11:11:59.607084 140413841560768 submission_runner.py:469] Time since start: 5632.44s, 	Step: 13221, 	{'train/accuracy': 0.3149513602256775, 'train/loss': 3.410829782485962, 'validation/accuracy': 0.28453999757766724, 'validation/loss': 3.683342695236206, 'validation/num_examples': 50000, 'test/accuracy': 0.22590000927448273, 'test/loss': 4.321931838989258, 'test/num_examples': 10000, 'score': 5158.88205242157, 'total_duration': 5632.436633110046, 'accumulated_submission_time': 5158.88205242157, 'accumulated_eval_time': 471.5474600791931, 'accumulated_logging_time': 0.6273565292358398}
I0307 11:11:59.761331 140257634797312 logging_writer.py:48] [13221] accumulated_eval_time=471.547, accumulated_logging_time=0.627357, accumulated_submission_time=5158.88, global_step=13221, preemption_count=0, score=5158.88, test/accuracy=0.2259, test/loss=4.32193, test/num_examples=10000, total_duration=5632.44, train/accuracy=0.314951, train/loss=3.41083, validation/accuracy=0.28454, validation/loss=3.68334, validation/num_examples=50000
I0307 11:12:30.152144 140257643190016 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.138153314590454, loss=2.294267416000366
I0307 11:13:08.788670 140257634797312 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0128989219665527, loss=2.526656150817871
I0307 11:13:47.760351 140257643190016 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0227571725845337, loss=2.3697509765625
I0307 11:14:26.716815 140257634797312 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.145126223564148, loss=2.4642956256866455
I0307 11:15:05.506581 140257643190016 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0176060199737549, loss=2.4574220180511475
I0307 11:15:44.787856 140257634797312 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1605809926986694, loss=2.5205485820770264
I0307 11:16:23.391568 140257643190016 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0537596940994263, loss=2.272894859313965
I0307 11:17:02.084620 140257634797312 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9198927283287048, loss=2.3993945121765137
I0307 11:17:40.948953 140257643190016 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.9959467053413391, loss=2.4114415645599365
I0307 11:18:19.454948 140257634797312 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9561250805854797, loss=2.292447090148926
I0307 11:18:58.091731 140257643190016 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.9445004463195801, loss=2.3327791690826416
I0307 11:19:36.683264 140257634797312 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.046759843826294, loss=2.302797555923462
I0307 11:20:15.323296 140257643190016 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.368761420249939, loss=2.2402262687683105
I0307 11:20:29.660611 140413841560768 spec.py:321] Evaluating on the training split.
I0307 11:20:46.645989 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 11:21:09.219864 140413841560768 spec.py:349] Evaluating on the test split.
I0307 11:21:11.034321 140413841560768 submission_runner.py:469] Time since start: 6183.86s, 	Step: 14538, 	{'train/accuracy': 0.2760084569454193, 'train/loss': 3.6868908405303955, 'validation/accuracy': 0.2556400001049042, 'validation/loss': 3.885582447052002, 'validation/num_examples': 50000, 'test/accuracy': 0.18730001151561737, 'test/loss': 4.582221031188965, 'test/num_examples': 10000, 'score': 5668.583002567291, 'total_duration': 6183.863875627518, 'accumulated_submission_time': 5668.583002567291, 'accumulated_eval_time': 512.9210107326508, 'accumulated_logging_time': 0.8443193435668945}
I0307 11:21:11.184910 140257634797312 logging_writer.py:48] [14538] accumulated_eval_time=512.921, accumulated_logging_time=0.844319, accumulated_submission_time=5668.58, global_step=14538, preemption_count=0, score=5668.58, test/accuracy=0.1873, test/loss=4.58222, test/num_examples=10000, total_duration=6183.86, train/accuracy=0.276008, train/loss=3.68689, validation/accuracy=0.25564, validation/loss=3.88558, validation/num_examples=50000
I0307 11:21:35.961085 140257643190016 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9424672722816467, loss=2.319962739944458
I0307 11:22:14.464372 140257634797312 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0278520584106445, loss=2.324418544769287
I0307 11:22:52.892465 140257643190016 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0260014533996582, loss=2.5128302574157715
I0307 11:23:31.610694 140257634797312 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.05947744846344, loss=2.527101993560791
I0307 11:24:10.683975 140257643190016 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.0884482860565186, loss=2.378159523010254
I0307 11:24:50.265678 140257634797312 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9247177839279175, loss=2.4291436672210693
I0307 11:25:28.643968 140257643190016 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9620804190635681, loss=2.2829504013061523
I0307 11:26:07.563329 140257634797312 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.952743649482727, loss=2.342432737350464
I0307 11:26:46.781730 140257643190016 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.0840243101119995, loss=2.468377113342285
I0307 11:27:25.670871 140257634797312 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.970989465713501, loss=2.417398452758789
I0307 11:28:04.478980 140257643190016 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0084038972854614, loss=2.315396547317505
I0307 11:28:42.822536 140257634797312 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.190327525138855, loss=2.4409284591674805
I0307 11:29:21.892226 140257643190016 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9880706667900085, loss=2.246556520462036
I0307 11:29:41.118107 140413841560768 spec.py:321] Evaluating on the training split.
I0307 11:30:01.777115 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 11:30:25.706743 140413841560768 spec.py:349] Evaluating on the test split.
I0307 11:30:27.536978 140413841560768 submission_runner.py:469] Time since start: 6740.37s, 	Step: 15851, 	{'train/accuracy': 0.36172670125961304, 'train/loss': 3.021711587905884, 'validation/accuracy': 0.33761999011039734, 'validation/loss': 3.2474756240844727, 'validation/num_examples': 50000, 'test/accuracy': 0.24390001595020294, 'test/loss': 4.107375144958496, 'test/num_examples': 10000, 'score': 6178.349545717239, 'total_duration': 6740.366504192352, 'accumulated_submission_time': 6178.349545717239, 'accumulated_eval_time': 559.3396933078766, 'accumulated_logging_time': 1.0217478275299072}
I0307 11:30:27.652859 140257634797312 logging_writer.py:48] [15851] accumulated_eval_time=559.34, accumulated_logging_time=1.02175, accumulated_submission_time=6178.35, global_step=15851, preemption_count=0, score=6178.35, test/accuracy=0.2439, test/loss=4.10738, test/num_examples=10000, total_duration=6740.37, train/accuracy=0.361727, train/loss=3.02171, validation/accuracy=0.33762, validation/loss=3.24748, validation/num_examples=50000
I0307 11:30:47.079650 140257643190016 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.1688432693481445, loss=2.3883235454559326
I0307 11:31:26.095905 140257634797312 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9666608572006226, loss=2.2323973178863525
I0307 11:32:04.756932 140257643190016 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.1480422019958496, loss=2.3679518699645996
I0307 11:32:43.088500 140257634797312 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.0597907304763794, loss=2.408018112182617
I0307 11:33:23.174399 140257643190016 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.97968989610672, loss=2.2401134967803955
I0307 11:34:02.066362 140257634797312 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.0895594358444214, loss=2.4711899757385254
I0307 11:34:40.818635 140257643190016 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.033921241760254, loss=2.354487419128418
I0307 11:35:19.499095 140257634797312 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0225751399993896, loss=2.3743295669555664
I0307 11:35:58.011108 140257643190016 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0151466131210327, loss=2.419429063796997
I0307 11:36:36.985229 140257634797312 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9716806411743164, loss=2.3274028301239014
I0307 11:37:16.036013 140257643190016 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.1269632577896118, loss=2.34442400932312
I0307 11:37:55.151288 140257634797312 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1678171157836914, loss=2.363353729248047
I0307 11:38:33.961541 140257643190016 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.1343525648117065, loss=2.2751529216766357
I0307 11:38:57.651139 140413841560768 spec.py:321] Evaluating on the training split.
I0307 11:39:18.032790 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 11:39:42.288877 140413841560768 spec.py:349] Evaluating on the test split.
I0307 11:39:44.140338 140413841560768 submission_runner.py:469] Time since start: 7296.97s, 	Step: 17162, 	{'train/accuracy': 0.4166533648967743, 'train/loss': 2.736692428588867, 'validation/accuracy': 0.38279998302459717, 'validation/loss': 2.994539976119995, 'validation/num_examples': 50000, 'test/accuracy': 0.28519999980926514, 'test/loss': 3.809859275817871, 'test/num_examples': 10000, 'score': 6688.176451683044, 'total_duration': 7296.969889640808, 'accumulated_submission_time': 6688.176451683044, 'accumulated_eval_time': 605.8287394046783, 'accumulated_logging_time': 1.1723618507385254}
I0307 11:39:44.411359 140257634797312 logging_writer.py:48] [17162] accumulated_eval_time=605.829, accumulated_logging_time=1.17236, accumulated_submission_time=6688.18, global_step=17162, preemption_count=0, score=6688.18, test/accuracy=0.2852, test/loss=3.80986, test/num_examples=10000, total_duration=7296.97, train/accuracy=0.416653, train/loss=2.73669, validation/accuracy=0.3828, validation/loss=2.99454, validation/num_examples=50000
I0307 11:39:59.611881 140257643190016 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.1198079586029053, loss=2.3332459926605225
I0307 11:40:38.003492 140257634797312 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0621858835220337, loss=2.270632266998291
I0307 11:41:16.572947 140257643190016 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.077156901359558, loss=2.3771204948425293
I0307 11:41:55.254973 140257634797312 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2854632139205933, loss=2.3237173557281494
I0307 11:42:34.395856 140257643190016 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0040403604507446, loss=2.3983817100524902
I0307 11:43:13.104428 140257634797312 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.1789205074310303, loss=2.426948308944702
I0307 11:43:51.896729 140257643190016 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.13520085811615, loss=2.3232929706573486
I0307 11:44:30.682523 140257634797312 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0452429056167603, loss=2.3492541313171387
I0307 11:45:09.396040 140257643190016 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0109736919403076, loss=2.40714693069458
I0307 11:45:48.352240 140257634797312 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.2554283142089844, loss=2.326557159423828
I0307 11:46:27.168406 140257643190016 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.0207463502883911, loss=2.2955660820007324
I0307 11:47:05.929471 140257634797312 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.0224604606628418, loss=2.31874418258667
I0307 11:47:44.689753 140257643190016 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.089754343032837, loss=2.1913914680480957
I0307 11:48:14.442825 140413841560768 spec.py:321] Evaluating on the training split.
I0307 11:48:28.667045 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 11:48:50.001156 140413841560768 spec.py:349] Evaluating on the test split.
I0307 11:48:52.021852 140413841560768 submission_runner.py:469] Time since start: 7844.82s, 	Step: 18478, 	{'train/accuracy': 0.35674425959587097, 'train/loss': 3.0802149772644043, 'validation/accuracy': 0.33188000321388245, 'validation/loss': 3.2801218032836914, 'validation/num_examples': 50000, 'test/accuracy': 0.2590000033378601, 'test/loss': 3.9830384254455566, 'test/num_examples': 10000, 'score': 7197.967222929001, 'total_duration': 7844.824070215225, 'accumulated_submission_time': 7197.967222929001, 'accumulated_eval_time': 643.3802750110626, 'accumulated_logging_time': 1.55641508102417}
I0307 11:48:52.139192 140257634797312 logging_writer.py:48] [18478] accumulated_eval_time=643.38, accumulated_logging_time=1.55642, accumulated_submission_time=7197.97, global_step=18478, preemption_count=0, score=7197.97, test/accuracy=0.259, test/loss=3.98304, test/num_examples=10000, total_duration=7844.82, train/accuracy=0.356744, train/loss=3.08021, validation/accuracy=0.33188, validation/loss=3.28012, validation/num_examples=50000
I0307 11:49:01.150470 140257643190016 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0911511182785034, loss=2.4390907287597656
I0307 11:49:39.715763 140257634797312 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0273865461349487, loss=2.385993480682373
I0307 11:50:18.212197 140257643190016 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.008441686630249, loss=2.3428821563720703
I0307 11:50:57.274068 140257634797312 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0070676803588867, loss=2.3675789833068848
I0307 11:51:35.889671 140257643190016 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9864134192466736, loss=2.364441394805908
I0307 11:52:14.165577 140257634797312 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.0071932077407837, loss=2.3782315254211426
I0307 11:52:52.634927 140257643190016 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0184615850448608, loss=2.5048298835754395
I0307 11:53:31.634036 140257634797312 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.196664571762085, loss=2.2556424140930176
I0307 11:54:10.772567 140257643190016 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.1846131086349487, loss=2.4195661544799805
I0307 11:54:49.923907 140257634797312 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0245397090911865, loss=2.3772037029266357
I0307 11:55:28.850643 140257643190016 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0377461910247803, loss=2.3622968196868896
I0307 11:56:07.672832 140257634797312 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9522143602371216, loss=2.5528364181518555
I0307 11:56:46.462773 140257643190016 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0629186630249023, loss=2.272498607635498
I0307 11:57:22.181361 140413841560768 spec.py:321] Evaluating on the training split.
I0307 11:57:38.361467 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 11:58:06.639227 140413841560768 spec.py:349] Evaluating on the test split.
I0307 11:58:08.481951 140413841560768 submission_runner.py:469] Time since start: 8401.31s, 	Step: 19793, 	{'train/accuracy': 0.23495295643806458, 'train/loss': 4.069524765014648, 'validation/accuracy': 0.2222599983215332, 'validation/loss': 4.186558723449707, 'validation/num_examples': 50000, 'test/accuracy': 0.17020000517368317, 'test/loss': 4.826508045196533, 'test/num_examples': 10000, 'score': 7707.762309789658, 'total_duration': 8401.311625003815, 'accumulated_submission_time': 7707.762309789658, 'accumulated_eval_time': 689.6808261871338, 'accumulated_logging_time': 1.795959234237671}
I0307 11:58:08.544527 140257634797312 logging_writer.py:48] [19793] accumulated_eval_time=689.681, accumulated_logging_time=1.79596, accumulated_submission_time=7707.76, global_step=19793, preemption_count=0, score=7707.76, test/accuracy=0.1702, test/loss=4.82651, test/num_examples=10000, total_duration=8401.31, train/accuracy=0.234953, train/loss=4.06952, validation/accuracy=0.22226, validation/loss=4.18656, validation/num_examples=50000
I0307 11:58:11.608809 140257643190016 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1131700277328491, loss=2.4459898471832275
I0307 11:58:50.628342 140257634797312 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0992388725280762, loss=2.4273762702941895
I0307 11:59:28.837666 140257643190016 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.0449713468551636, loss=2.3438234329223633
I0307 12:00:08.475328 140257634797312 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.1043543815612793, loss=2.4138619899749756
I0307 12:00:47.145220 140257643190016 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.036711573600769, loss=2.2664270401000977
I0307 12:01:26.204913 140257634797312 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0552928447723389, loss=2.3264822959899902
I0307 12:02:06.250421 140257643190016 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.2118788957595825, loss=2.4332921504974365
I0307 12:02:45.557671 140257634797312 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.134920597076416, loss=2.381798505783081
I0307 12:03:24.421732 140257643190016 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.095994472503662, loss=2.2350666522979736
I0307 12:04:03.085489 140257634797312 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.2280811071395874, loss=2.3814139366149902
I0307 12:04:41.885992 140257643190016 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.9669419527053833, loss=2.257399320602417
I0307 12:05:20.623085 140257634797312 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.099472999572754, loss=2.242889881134033
I0307 12:06:00.382135 140257643190016 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0101544857025146, loss=2.299010753631592
I0307 12:06:38.652308 140413841560768 spec.py:321] Evaluating on the training split.
I0307 12:06:55.108654 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 12:07:17.443345 140413841560768 spec.py:349] Evaluating on the test split.
I0307 12:07:19.279414 140413841560768 submission_runner.py:469] Time since start: 8952.11s, 	Step: 21100, 	{'train/accuracy': 0.25221219658851624, 'train/loss': 3.9189140796661377, 'validation/accuracy': 0.23273999989032745, 'validation/loss': 4.085951328277588, 'validation/num_examples': 50000, 'test/accuracy': 0.16530001163482666, 'test/loss': 4.810108184814453, 'test/num_examples': 10000, 'score': 8217.72189283371, 'total_duration': 8952.10895895958, 'accumulated_submission_time': 8217.72189283371, 'accumulated_eval_time': 730.3077661991119, 'accumulated_logging_time': 1.8665201663970947}
I0307 12:07:19.361832 140257634797312 logging_writer.py:48] [21100] accumulated_eval_time=730.308, accumulated_logging_time=1.86652, accumulated_submission_time=8217.72, global_step=21100, preemption_count=0, score=8217.72, test/accuracy=0.1653, test/loss=4.81011, test/num_examples=10000, total_duration=8952.11, train/accuracy=0.252212, train/loss=3.91891, validation/accuracy=0.23274, validation/loss=4.08595, validation/num_examples=50000
I0307 12:07:19.836913 140257643190016 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.1010220050811768, loss=2.2543063163757324
I0307 12:07:58.868826 140257634797312 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.935178279876709, loss=2.3195550441741943
I0307 12:08:38.926742 140257643190016 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.9903981685638428, loss=2.302562713623047
I0307 12:09:17.925981 140257634797312 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9813664555549622, loss=2.3137145042419434
I0307 12:09:56.950394 140257643190016 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0137566328048706, loss=2.2807743549346924
I0307 12:10:36.335969 140257634797312 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.2675703763961792, loss=2.526329517364502
I0307 12:11:16.566896 140257643190016 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0370551347732544, loss=2.366283655166626
I0307 12:11:56.299338 140257634797312 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.0070178508758545, loss=2.4257652759552
I0307 12:12:35.651440 140257643190016 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.0702941417694092, loss=2.3459129333496094
I0307 12:13:14.527229 140257634797312 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0880253314971924, loss=2.2349441051483154
I0307 12:13:53.189123 140257643190016 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.2264899015426636, loss=2.27126407623291
I0307 12:14:32.073773 140257634797312 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.0765187740325928, loss=2.203289747238159
I0307 12:15:10.866649 140257643190016 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0038551092147827, loss=2.2996795177459717
I0307 12:15:49.484001 140413841560768 spec.py:321] Evaluating on the training split.
I0307 12:16:03.237536 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 12:16:24.822649 140413841560768 spec.py:349] Evaluating on the test split.
I0307 12:16:26.651941 140413841560768 submission_runner.py:469] Time since start: 9499.48s, 	Step: 22400, 	{'train/accuracy': 0.3220065236091614, 'train/loss': 3.3162057399749756, 'validation/accuracy': 0.2998200058937073, 'validation/loss': 3.505038022994995, 'validation/num_examples': 50000, 'test/accuracy': 0.2183000147342682, 'test/loss': 4.377758502960205, 'test/num_examples': 10000, 'score': 8727.663914442062, 'total_duration': 9499.48161816597, 'accumulated_submission_time': 8727.663914442062, 'accumulated_eval_time': 767.475672006607, 'accumulated_logging_time': 1.9921505451202393}
I0307 12:16:26.684218 140257634797312 logging_writer.py:48] [22400] accumulated_eval_time=767.476, accumulated_logging_time=1.99215, accumulated_submission_time=8727.66, global_step=22400, preemption_count=0, score=8727.66, test/accuracy=0.2183, test/loss=4.37776, test/num_examples=10000, total_duration=9499.48, train/accuracy=0.322007, train/loss=3.31621, validation/accuracy=0.29982, validation/loss=3.50504, validation/num_examples=50000
I0307 12:16:27.114423 140257643190016 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.0464013814926147, loss=2.3328123092651367
I0307 12:17:05.892413 140257634797312 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.3305319547653198, loss=2.3000621795654297
I0307 12:17:45.329679 140257643190016 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.1761873960494995, loss=2.419543743133545
I0307 12:18:23.842432 140257634797312 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.990897536277771, loss=2.2406625747680664
I0307 12:19:02.827626 140257643190016 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.167765736579895, loss=2.528792142868042
I0307 12:19:41.097274 140257634797312 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.131648063659668, loss=2.3525338172912598
I0307 12:20:19.747853 140257643190016 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0619131326675415, loss=2.284167766571045
I0307 12:20:58.584790 140257634797312 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0602282285690308, loss=2.1991677284240723
I0307 12:21:36.994168 140257643190016 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.0520955324172974, loss=2.428327798843384
I0307 12:22:15.804043 140257634797312 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.035364031791687, loss=2.3318867683410645
I0307 12:22:54.302608 140257643190016 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.2196694612503052, loss=2.2936246395111084
I0307 12:23:33.047427 140257634797312 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0932884216308594, loss=2.357387065887451
I0307 12:24:11.468436 140257643190016 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.173836588859558, loss=2.3896119594573975
I0307 12:24:50.118124 140257634797312 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.0193918943405151, loss=2.210737466812134
I0307 12:24:56.736260 140413841560768 spec.py:321] Evaluating on the training split.
I0307 12:25:16.193651 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 12:25:39.611045 140413841560768 spec.py:349] Evaluating on the test split.
I0307 12:25:41.448856 140413841560768 submission_runner.py:469] Time since start: 10054.28s, 	Step: 23718, 	{'train/accuracy': 0.25554049015045166, 'train/loss': 3.87467360496521, 'validation/accuracy': 0.24177999794483185, 'validation/loss': 4.040432929992676, 'validation/num_examples': 50000, 'test/accuracy': 0.1827000081539154, 'test/loss': 4.756398677825928, 'test/num_examples': 10000, 'score': 9237.5722489357, 'total_duration': 10054.27838087082, 'accumulated_submission_time': 9237.5722489357, 'accumulated_eval_time': 812.1880791187286, 'accumulated_logging_time': 2.0323283672332764}
I0307 12:25:41.536618 140257643190016 logging_writer.py:48] [23718] accumulated_eval_time=812.188, accumulated_logging_time=2.03233, accumulated_submission_time=9237.57, global_step=23718, preemption_count=0, score=9237.57, test/accuracy=0.1827, test/loss=4.7564, test/num_examples=10000, total_duration=10054.3, train/accuracy=0.25554, train/loss=3.87467, validation/accuracy=0.24178, validation/loss=4.04043, validation/num_examples=50000
I0307 12:26:14.569383 140257634797312 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.13664972782135, loss=2.206921100616455
I0307 12:26:53.393502 140257643190016 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.0182164907455444, loss=2.3804500102996826
I0307 12:27:32.252530 140257634797312 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.0504608154296875, loss=2.3046445846557617
I0307 12:28:11.244669 140257643190016 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9940778613090515, loss=2.292297124862671
I0307 12:28:50.565842 140257634797312 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.9939169883728027, loss=2.27083158493042
I0307 12:29:29.130795 140257643190016 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.2230900526046753, loss=2.481964111328125
I0307 12:30:08.078722 140257634797312 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0392757654190063, loss=2.2642621994018555
I0307 12:30:46.608538 140257643190016 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0780731439590454, loss=2.4537901878356934
I0307 12:31:25.060942 140257634797312 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0627973079681396, loss=2.4191508293151855
I0307 12:32:03.995287 140257643190016 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.1294275522232056, loss=2.291206121444702
I0307 12:32:42.899044 140257634797312 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.053652048110962, loss=2.2376766204833984
I0307 12:33:21.678632 140257643190016 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0114482641220093, loss=2.2206523418426514
I0307 12:34:00.856013 140257634797312 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0100207328796387, loss=2.3072550296783447
I0307 12:34:11.476710 140413841560768 spec.py:321] Evaluating on the training split.
I0307 12:34:32.651316 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 12:34:54.849304 140413841560768 spec.py:349] Evaluating on the test split.
I0307 12:34:56.663477 140413841560768 submission_runner.py:469] Time since start: 10609.49s, 	Step: 25027, 	{'train/accuracy': 0.17245295643806458, 'train/loss': 5.346203327178955, 'validation/accuracy': 0.16779999434947968, 'validation/loss': 5.412228584289551, 'validation/num_examples': 50000, 'test/accuracy': 0.11410000175237656, 'test/loss': 6.4161882400512695, 'test/num_examples': 10000, 'score': 9747.356564998627, 'total_duration': 10609.492995023727, 'accumulated_submission_time': 9747.356564998627, 'accumulated_eval_time': 857.374653339386, 'accumulated_logging_time': 2.1465256214141846}
I0307 12:34:56.717216 140257643190016 logging_writer.py:48] [25027] accumulated_eval_time=857.375, accumulated_logging_time=2.14653, accumulated_submission_time=9747.36, global_step=25027, preemption_count=0, score=9747.36, test/accuracy=0.1141, test/loss=6.41619, test/num_examples=10000, total_duration=10609.5, train/accuracy=0.172453, train/loss=5.3462, validation/accuracy=0.1678, validation/loss=5.41223, validation/num_examples=50000
I0307 12:35:25.985058 140257634797312 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.1124638319015503, loss=2.329901695251465
I0307 12:36:04.628566 140257643190016 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.1093029975891113, loss=2.3055667877197266
I0307 12:36:43.034111 140257634797312 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.1160815954208374, loss=2.304797649383545
I0307 12:37:22.209544 140257643190016 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.057294487953186, loss=2.279991626739502
I0307 12:38:01.424181 140257634797312 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.0825998783111572, loss=2.3087708950042725
I0307 12:38:40.371723 140257643190016 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1108347177505493, loss=2.329883098602295
I0307 12:39:19.197009 140257634797312 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.273037314414978, loss=2.471883773803711
I0307 12:39:58.185392 140257643190016 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.144980549812317, loss=2.325902223587036
I0307 12:40:36.663890 140257634797312 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0888205766677856, loss=2.3257217407226562
I0307 12:41:15.516012 140257643190016 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.089574933052063, loss=2.2284891605377197
I0307 12:41:54.347032 140257634797312 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.0285013914108276, loss=2.225752592086792
I0307 12:42:33.181368 140257643190016 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0573219060897827, loss=2.2690610885620117
I0307 12:43:13.230665 140257634797312 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.0724812746047974, loss=2.2959206104278564
I0307 12:43:26.883003 140413841560768 spec.py:321] Evaluating on the training split.
I0307 12:43:43.820614 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 12:44:04.187631 140413841560768 spec.py:349] Evaluating on the test split.
I0307 12:44:06.016744 140413841560768 submission_runner.py:469] Time since start: 11158.85s, 	Step: 26335, 	{'train/accuracy': 0.2770647406578064, 'train/loss': 3.9628350734710693, 'validation/accuracy': 0.265859991312027, 'validation/loss': 4.100792407989502, 'validation/num_examples': 50000, 'test/accuracy': 0.19350001215934753, 'test/loss': 5.097689628601074, 'test/num_examples': 10000, 'score': 10257.385878324509, 'total_duration': 11158.846274852753, 'accumulated_submission_time': 10257.385878324509, 'accumulated_eval_time': 896.5082101821899, 'accumulated_logging_time': 2.2085933685302734}
I0307 12:44:06.182190 140257643190016 logging_writer.py:48] [26335] accumulated_eval_time=896.508, accumulated_logging_time=2.20859, accumulated_submission_time=10257.4, global_step=26335, preemption_count=0, score=10257.4, test/accuracy=0.1935, test/loss=5.09769, test/num_examples=10000, total_duration=11158.8, train/accuracy=0.277065, train/loss=3.96284, validation/accuracy=0.26586, validation/loss=4.10079, validation/num_examples=50000
I0307 12:44:32.267189 140257634797312 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.362710952758789, loss=2.5080387592315674
I0307 12:45:10.573866 140257643190016 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.0712499618530273, loss=2.229933261871338
I0307 12:45:49.618990 140257634797312 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.077679991722107, loss=2.5010733604431152
I0307 12:46:28.245024 140257643190016 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.3146404027938843, loss=2.4957964420318604
I0307 12:47:07.285439 140257634797312 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.206620216369629, loss=2.4465041160583496
I0307 12:47:46.136142 140257643190016 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1706722974777222, loss=2.3608238697052
I0307 12:48:24.856549 140257634797312 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.140336275100708, loss=2.3781495094299316
I0307 12:49:04.197221 140257643190016 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1370069980621338, loss=2.2397823333740234
I0307 12:49:42.871971 140257634797312 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1662100553512573, loss=2.413893222808838
I0307 12:50:21.394963 140257643190016 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.1447373628616333, loss=2.3400018215179443
I0307 12:51:00.140303 140257634797312 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.1430941820144653, loss=2.3314566612243652
I0307 12:51:39.172176 140257643190016 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1367793083190918, loss=2.244953155517578
I0307 12:52:18.342009 140257634797312 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.9973724484443665, loss=2.3059475421905518
I0307 12:52:36.204192 140413841560768 spec.py:321] Evaluating on the training split.
I0307 12:52:53.015362 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 12:53:14.635536 140413841560768 spec.py:349] Evaluating on the test split.
I0307 12:53:16.552628 140413841560768 submission_runner.py:469] Time since start: 11709.38s, 	Step: 27647, 	{'train/accuracy': 0.3230229616165161, 'train/loss': 3.3251378536224365, 'validation/accuracy': 0.29659998416900635, 'validation/loss': 3.530597448348999, 'validation/num_examples': 50000, 'test/accuracy': 0.22910000383853912, 'test/loss': 4.166884422302246, 'test/num_examples': 10000, 'score': 10766.883116006851, 'total_duration': 11709.382200479507, 'accumulated_submission_time': 10766.883116006851, 'accumulated_eval_time': 936.856507062912, 'accumulated_logging_time': 2.7665841579437256}
I0307 12:53:16.667441 140257643190016 logging_writer.py:48] [27647] accumulated_eval_time=936.857, accumulated_logging_time=2.76658, accumulated_submission_time=10766.9, global_step=27647, preemption_count=0, score=10766.9, test/accuracy=0.2291, test/loss=4.16688, test/num_examples=10000, total_duration=11709.4, train/accuracy=0.323023, train/loss=3.32514, validation/accuracy=0.2966, validation/loss=3.5306, validation/num_examples=50000
I0307 12:53:37.348347 140257634797312 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.0804166793823242, loss=2.1803598403930664
I0307 12:54:16.255841 140257643190016 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.1381498575210571, loss=2.371823787689209
I0307 12:54:54.678802 140257634797312 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0904408693313599, loss=2.332597494125366
I0307 12:55:33.283501 140257643190016 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1781079769134521, loss=2.383667230606079
I0307 12:56:12.088424 140257634797312 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3222523927688599, loss=2.404669761657715
I0307 12:56:50.700093 140257643190016 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.1086164712905884, loss=2.279420852661133
I0307 12:57:29.275190 140257634797312 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.1498292684555054, loss=2.3821210861206055
I0307 12:58:08.276765 140257643190016 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.025618314743042, loss=2.1140782833099365
I0307 12:58:47.297542 140257634797312 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.0851579904556274, loss=2.385751247406006
I0307 12:59:25.748716 140257643190016 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.086391806602478, loss=2.2812252044677734
I0307 13:00:04.464875 140257634797312 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.1748381853103638, loss=2.304933786392212
I0307 13:00:43.916557 140257643190016 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.1493825912475586, loss=2.3678596019744873
I0307 13:01:23.356685 140257634797312 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.0931220054626465, loss=2.4343183040618896
I0307 13:01:46.910162 140413841560768 spec.py:321] Evaluating on the training split.
I0307 13:02:03.380043 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 13:02:27.097727 140413841560768 spec.py:349] Evaluating on the test split.
I0307 13:02:28.950338 140413841560768 submission_runner.py:469] Time since start: 12261.78s, 	Step: 28962, 	{'train/accuracy': 0.32830435037612915, 'train/loss': 3.302366018295288, 'validation/accuracy': 0.3099599778652191, 'validation/loss': 3.4873781204223633, 'validation/num_examples': 50000, 'test/accuracy': 0.22910000383853912, 'test/loss': 4.304723739624023, 'test/num_examples': 10000, 'score': 11276.933552742004, 'total_duration': 12261.779875278473, 'accumulated_submission_time': 11276.933552742004, 'accumulated_eval_time': 978.8965072631836, 'accumulated_logging_time': 2.9419424533843994}
I0307 13:02:29.014751 140257643190016 logging_writer.py:48] [28962] accumulated_eval_time=978.897, accumulated_logging_time=2.94194, accumulated_submission_time=11276.9, global_step=28962, preemption_count=0, score=11276.9, test/accuracy=0.2291, test/loss=4.30472, test/num_examples=10000, total_duration=12261.8, train/accuracy=0.328304, train/loss=3.30237, validation/accuracy=0.30996, validation/loss=3.48738, validation/num_examples=50000
I0307 13:02:44.075200 140257634797312 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.2096000909805298, loss=2.2441189289093018
I0307 13:03:22.908372 140257643190016 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9852133989334106, loss=2.3517684936523438
I0307 13:04:01.232257 140257634797312 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.1155407428741455, loss=2.3063392639160156
I0307 13:04:40.269489 140257643190016 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0745601654052734, loss=2.265672206878662
I0307 13:05:19.158890 140257634797312 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.2171802520751953, loss=2.274266004562378
I0307 13:05:58.089781 140257643190016 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.1457210779190063, loss=2.194000720977783
I0307 13:06:36.837325 140257634797312 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.999093234539032, loss=2.2783915996551514
I0307 13:07:15.395462 140257643190016 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0599404573440552, loss=2.3722851276397705
I0307 13:07:53.914360 140257634797312 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.1234848499298096, loss=2.250997543334961
I0307 13:08:32.454530 140257643190016 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.2010599374771118, loss=2.2967076301574707
I0307 13:09:11.202459 140257634797312 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.1841480731964111, loss=2.1814963817596436
I0307 13:09:51.479397 140257643190016 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.062538504600525, loss=2.0742573738098145
I0307 13:10:30.790494 140257634797312 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.2850611209869385, loss=2.415950298309326
I0307 13:10:59.050180 140413841560768 spec.py:321] Evaluating on the training split.
I0307 13:11:18.700084 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 13:11:39.347281 140413841560768 spec.py:349] Evaluating on the test split.
I0307 13:11:41.195863 140413841560768 submission_runner.py:469] Time since start: 12814.02s, 	Step: 30275, 	{'train/accuracy': 0.08810985088348389, 'train/loss': 6.9213714599609375, 'validation/accuracy': 0.08069999516010284, 'validation/loss': 7.091775894165039, 'validation/num_examples': 50000, 'test/accuracy': 0.06630000472068787, 'test/loss': 7.594058036804199, 'test/num_examples': 10000, 'score': 11786.804682731628, 'total_duration': 12814.02454996109, 'accumulated_submission_time': 11786.804682731628, 'accumulated_eval_time': 1021.0411665439606, 'accumulated_logging_time': 3.036323070526123}
I0307 13:11:41.315937 140257643190016 logging_writer.py:48] [30275] accumulated_eval_time=1021.04, accumulated_logging_time=3.03632, accumulated_submission_time=11786.8, global_step=30275, preemption_count=0, score=11786.8, test/accuracy=0.0663, test/loss=7.59406, test/num_examples=10000, total_duration=12814, train/accuracy=0.0881099, train/loss=6.92137, validation/accuracy=0.0807, validation/loss=7.09178, validation/num_examples=50000
I0307 13:11:51.366523 140257634797312 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.1481516361236572, loss=2.3638906478881836
I0307 13:12:30.560179 140257643190016 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.1768919229507446, loss=2.4201769828796387
I0307 13:13:09.999862 140257634797312 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.03304922580719, loss=2.306108236312866
I0307 13:13:48.692415 140257643190016 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.027632474899292, loss=2.2731783390045166
I0307 13:14:27.420818 140257634797312 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.2200180292129517, loss=2.3285446166992188
I0307 13:15:06.381200 140257643190016 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.084648847579956, loss=2.1941537857055664
I0307 13:15:44.845222 140257634797312 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.1337991952896118, loss=2.3307926654815674
I0307 13:16:23.477604 140257643190016 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.153362512588501, loss=2.181948184967041
I0307 13:17:02.322138 140257634797312 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1212897300720215, loss=2.3057174682617188
I0307 13:17:40.726082 140257643190016 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.397874116897583, loss=2.2471141815185547
I0307 13:18:19.727526 140257634797312 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.2335277795791626, loss=2.363067865371704
I0307 13:18:59.120741 140257643190016 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.0313066244125366, loss=2.2662131786346436
I0307 13:19:38.328337 140257634797312 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1813448667526245, loss=2.326171398162842
I0307 13:20:11.375428 140413841560768 spec.py:321] Evaluating on the training split.
I0307 13:20:27.769573 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 13:20:47.614189 140413841560768 spec.py:349] Evaluating on the test split.
I0307 13:20:49.448805 140413841560768 submission_runner.py:469] Time since start: 13362.28s, 	Step: 31586, 	{'train/accuracy': 0.2460339516401291, 'train/loss': 4.076464653015137, 'validation/accuracy': 0.22613999247550964, 'validation/loss': 4.313343048095703, 'validation/num_examples': 50000, 'test/accuracy': 0.172200009226799, 'test/loss': 5.027352809906006, 'test/num_examples': 10000, 'score': 12296.698233127594, 'total_duration': 13362.27832865715, 'accumulated_submission_time': 12296.698233127594, 'accumulated_eval_time': 1059.114381313324, 'accumulated_logging_time': 3.191221237182617}
I0307 13:20:49.572622 140257634797312 logging_writer.py:48] [31586] accumulated_eval_time=1059.11, accumulated_logging_time=3.19122, accumulated_submission_time=12296.7, global_step=31586, preemption_count=0, score=12296.7, test/accuracy=0.1722, test/loss=5.02735, test/num_examples=10000, total_duration=13362.3, train/accuracy=0.246034, train/loss=4.07646, validation/accuracy=0.22614, validation/loss=4.31334, validation/num_examples=50000
I0307 13:20:55.610098 140257643190016 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.2560062408447266, loss=2.3623533248901367
I0307 13:21:34.111160 140257634797312 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.2378450632095337, loss=2.3363304138183594
I0307 13:22:12.766613 140257643190016 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.1659529209136963, loss=2.3132810592651367
I0307 13:22:51.106778 140257634797312 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.2448312044143677, loss=2.441953420639038
I0307 13:23:29.660442 140257643190016 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.084368109703064, loss=2.268784761428833
I0307 13:24:08.337426 140257634797312 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0433446168899536, loss=2.3086912631988525
I0307 13:24:47.007390 140257643190016 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.1991925239562988, loss=2.3730080127716064
I0307 13:25:25.567161 140257634797312 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0298558473587036, loss=2.1325788497924805
I0307 13:26:03.737881 140257643190016 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.304012656211853, loss=2.3698413372039795
I0307 13:26:42.305087 140257634797312 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0354422330856323, loss=2.0865681171417236
I0307 13:27:21.794749 140257643190016 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0673112869262695, loss=2.2228851318359375
I0307 13:28:00.509477 140257634797312 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9845669865608215, loss=2.1917903423309326
I0307 13:28:39.344347 140257643190016 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.204733967781067, loss=2.343576192855835
I0307 13:29:17.971912 140257634797312 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.1918033361434937, loss=2.2895870208740234
I0307 13:29:19.522460 140413841560768 spec.py:321] Evaluating on the training split.
I0307 13:29:31.684754 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 13:30:03.098132 140413841560768 spec.py:349] Evaluating on the test split.
I0307 13:30:04.884923 140413841560768 submission_runner.py:469] Time since start: 13917.71s, 	Step: 32905, 	{'train/accuracy': 0.3020966053009033, 'train/loss': 3.5442140102386475, 'validation/accuracy': 0.280379980802536, 'validation/loss': 3.7143263816833496, 'validation/num_examples': 50000, 'test/accuracy': 0.2111000120639801, 'test/loss': 4.476617336273193, 'test/num_examples': 10000, 'score': 12806.461260318756, 'total_duration': 13917.714373111725, 'accumulated_submission_time': 12806.461260318756, 'accumulated_eval_time': 1104.4765815734863, 'accumulated_logging_time': 3.373443841934204}
I0307 13:30:04.993532 140257643190016 logging_writer.py:48] [32905] accumulated_eval_time=1104.48, accumulated_logging_time=3.37344, accumulated_submission_time=12806.5, global_step=32905, preemption_count=0, score=12806.5, test/accuracy=0.2111, test/loss=4.47662, test/num_examples=10000, total_duration=13917.7, train/accuracy=0.302097, train/loss=3.54421, validation/accuracy=0.28038, validation/loss=3.71433, validation/num_examples=50000
I0307 13:30:42.323516 140257634797312 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.103308916091919, loss=2.318037509918213
I0307 13:31:20.564912 140257643190016 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.3057886362075806, loss=2.3336150646209717
I0307 13:31:58.984155 140257634797312 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.1872926950454712, loss=2.3951971530914307
I0307 13:32:37.870307 140257643190016 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1631749868392944, loss=2.1672940254211426
I0307 13:33:16.591171 140257634797312 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.2088373899459839, loss=2.3321802616119385
I0307 13:33:55.079859 140257643190016 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.109100103378296, loss=2.294020175933838
I0307 13:34:33.661568 140257634797312 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.3603829145431519, loss=2.2888498306274414
I0307 13:35:12.313113 140257643190016 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.1972445249557495, loss=2.2334799766540527
I0307 13:35:51.436990 140257634797312 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.239719271659851, loss=2.385971784591675
I0307 13:36:29.983346 140257643190016 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.1923853158950806, loss=2.215879201889038
I0307 13:37:08.894663 140257634797312 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.2467488050460815, loss=2.3093059062957764
I0307 13:37:47.360638 140257643190016 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.1250849962234497, loss=2.3987889289855957
I0307 13:38:25.991455 140257634797312 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.1308356523513794, loss=2.273423433303833
I0307 13:38:35.235282 140413841560768 spec.py:321] Evaluating on the training split.
I0307 13:38:48.101585 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 13:39:14.749474 140413841560768 spec.py:349] Evaluating on the test split.
I0307 13:39:16.569615 140413841560768 submission_runner.py:469] Time since start: 14469.40s, 	Step: 34225, 	{'train/accuracy': 0.24300462007522583, 'train/loss': 4.0306596755981445, 'validation/accuracy': 0.2237199991941452, 'validation/loss': 4.230849266052246, 'validation/num_examples': 50000, 'test/accuracy': 0.16440001130104065, 'test/loss': 5.0166449546813965, 'test/num_examples': 10000, 'score': 13316.517487764359, 'total_duration': 14469.399162054062, 'accumulated_submission_time': 13316.517487764359, 'accumulated_eval_time': 1145.8107497692108, 'accumulated_logging_time': 3.533261299133301}
I0307 13:39:16.638981 140257643190016 logging_writer.py:48] [34225] accumulated_eval_time=1145.81, accumulated_logging_time=3.53326, accumulated_submission_time=13316.5, global_step=34225, preemption_count=0, score=13316.5, test/accuracy=0.1644, test/loss=5.01664, test/num_examples=10000, total_duration=14469.4, train/accuracy=0.243005, train/loss=4.03066, validation/accuracy=0.22372, validation/loss=4.23085, validation/num_examples=50000
I0307 13:39:46.126658 140257634797312 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0523067712783813, loss=2.224940538406372
I0307 13:40:24.705809 140257643190016 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.123322606086731, loss=2.25692081451416
I0307 13:41:03.339558 140257634797312 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.0881192684173584, loss=2.374959945678711
I0307 13:41:41.061201 140257643190016 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.2106883525848389, loss=2.3333935737609863
I0307 13:42:19.351469 140257634797312 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.2230561971664429, loss=2.2101449966430664
I0307 13:42:58.010015 140257643190016 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.1590189933776855, loss=2.4136078357696533
I0307 13:43:36.493396 140257634797312 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.1404937505722046, loss=2.2454867362976074
I0307 13:44:15.219907 140257643190016 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.152482271194458, loss=2.2586798667907715
I0307 13:44:53.989561 140257634797312 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1308090686798096, loss=2.208550453186035
I0307 13:45:32.427752 140257643190016 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.531299352645874, loss=2.2428948879241943
I0307 13:46:11.085559 140257634797312 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.039438009262085, loss=2.2291953563690186
I0307 13:46:49.626938 140257643190016 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.1455248594284058, loss=2.234868049621582
I0307 13:47:28.429184 140257634797312 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.0085341930389404, loss=2.270968437194824
I0307 13:47:46.928836 140413841560768 spec.py:321] Evaluating on the training split.
I0307 13:47:59.687974 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 13:48:21.455986 140413841560768 spec.py:349] Evaluating on the test split.
I0307 13:48:23.231341 140413841560768 submission_runner.py:469] Time since start: 15016.06s, 	Step: 35549, 	{'train/accuracy': 0.10782046616077423, 'train/loss': 5.862038612365723, 'validation/accuracy': 0.09572000056505203, 'validation/loss': 6.023454666137695, 'validation/num_examples': 50000, 'test/accuracy': 0.07830000668764114, 'test/loss': 6.451637268066406, 'test/num_examples': 10000, 'score': 13826.651160001755, 'total_duration': 15016.060870170593, 'accumulated_submission_time': 13826.651160001755, 'accumulated_eval_time': 1182.113071680069, 'accumulated_logging_time': 3.6243035793304443}
I0307 13:48:23.281960 140257643190016 logging_writer.py:48] [35549] accumulated_eval_time=1182.11, accumulated_logging_time=3.6243, accumulated_submission_time=13826.7, global_step=35549, preemption_count=0, score=13826.7, test/accuracy=0.0783, test/loss=6.45164, test/num_examples=10000, total_duration=15016.1, train/accuracy=0.10782, train/loss=5.86204, validation/accuracy=0.09572, validation/loss=6.02345, validation/num_examples=50000
I0307 13:48:43.284183 140257634797312 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.2540751695632935, loss=2.369579315185547
I0307 13:49:22.268019 140257643190016 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.1786130666732788, loss=2.389850378036499
I0307 13:50:01.133832 140257634797312 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1129095554351807, loss=2.3246688842773438
I0307 13:50:39.772321 140257643190016 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.114040493965149, loss=2.1554462909698486
I0307 13:51:18.726961 140257634797312 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.334315299987793, loss=2.2559921741485596
I0307 13:51:57.719864 140257643190016 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.078833818435669, loss=2.3362419605255127
I0307 13:52:36.497479 140257634797312 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1898868083953857, loss=2.24824595451355
I0307 13:53:15.741045 140257643190016 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.2107806205749512, loss=2.2664554119110107
I0307 13:53:55.069420 140257634797312 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.0676326751708984, loss=2.2769055366516113
I0307 13:54:33.937114 140257643190016 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0686076879501343, loss=2.240219831466675
I0307 13:55:12.631649 140257634797312 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.1133790016174316, loss=2.2257883548736572
I0307 13:55:51.143860 140257643190016 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.2533732652664185, loss=2.374805450439453
I0307 13:56:29.679412 140257634797312 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.158862590789795, loss=2.3652994632720947
I0307 13:56:53.566834 140413841560768 spec.py:321] Evaluating on the training split.
I0307 13:57:06.193344 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 13:57:29.925432 140413841560768 spec.py:349] Evaluating on the test split.
I0307 13:57:31.759478 140413841560768 submission_runner.py:469] Time since start: 15564.59s, 	Step: 36862, 	{'train/accuracy': 0.33067601919174194, 'train/loss': 3.2387871742248535, 'validation/accuracy': 0.3121199905872345, 'validation/loss': 3.3640401363372803, 'validation/num_examples': 50000, 'test/accuracy': 0.23770001530647278, 'test/loss': 4.094586372375488, 'test/num_examples': 10000, 'score': 14336.78462934494, 'total_duration': 15564.589007139206, 'accumulated_submission_time': 14336.78462934494, 'accumulated_eval_time': 1220.30553150177, 'accumulated_logging_time': 3.695484161376953}
I0307 13:57:31.912310 140257643190016 logging_writer.py:48] [36862] accumulated_eval_time=1220.31, accumulated_logging_time=3.69548, accumulated_submission_time=14336.8, global_step=36862, preemption_count=0, score=14336.8, test/accuracy=0.2377, test/loss=4.09459, test/num_examples=10000, total_duration=15564.6, train/accuracy=0.330676, train/loss=3.23879, validation/accuracy=0.31212, validation/loss=3.36404, validation/num_examples=50000
I0307 13:57:47.223008 140257634797312 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.9893944263458252, loss=2.2447972297668457
I0307 13:58:25.793210 140257643190016 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.035080075263977, loss=2.2000973224639893
I0307 13:59:04.483151 140257634797312 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.1057060956954956, loss=2.3173768520355225
I0307 13:59:43.145092 140257643190016 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1135711669921875, loss=2.3184854984283447
I0307 14:00:21.772175 140257634797312 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.175610065460205, loss=2.2900378704071045
I0307 14:01:00.771627 140257643190016 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.149909496307373, loss=2.359839916229248
I0307 14:01:39.273869 140257634797312 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.086320400238037, loss=2.377473831176758
I0307 14:02:18.828458 140257643190016 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1785458326339722, loss=2.3820688724517822
I0307 14:02:57.460974 140257634797312 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.1973180770874023, loss=2.2105467319488525
I0307 14:03:35.796840 140257643190016 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1528905630111694, loss=2.420886993408203
I0307 14:04:14.316947 140257634797312 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.325803518295288, loss=2.311462879180908
I0307 14:04:53.178882 140257643190016 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0903656482696533, loss=2.3332161903381348
I0307 14:05:31.991228 140257634797312 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0774980783462524, loss=2.186980724334717
I0307 14:06:01.989461 140413841560768 spec.py:321] Evaluating on the training split.
I0307 14:06:14.934484 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 14:06:46.638282 140413841560768 spec.py:349] Evaluating on the test split.
I0307 14:06:48.467605 140413841560768 submission_runner.py:469] Time since start: 16121.30s, 	Step: 38178, 	{'train/accuracy': 0.20216836035251617, 'train/loss': 4.433493137359619, 'validation/accuracy': 0.1929199993610382, 'validation/loss': 4.585607528686523, 'validation/num_examples': 50000, 'test/accuracy': 0.15139999985694885, 'test/loss': 5.0769734382629395, 'test/num_examples': 10000, 'score': 14846.681279182434, 'total_duration': 16121.297149896622, 'accumulated_submission_time': 14846.681279182434, 'accumulated_eval_time': 1266.783516407013, 'accumulated_logging_time': 3.897552490234375}
I0307 14:06:48.562717 140257643190016 logging_writer.py:48] [38178] accumulated_eval_time=1266.78, accumulated_logging_time=3.89755, accumulated_submission_time=14846.7, global_step=38178, preemption_count=0, score=14846.7, test/accuracy=0.1514, test/loss=5.07697, test/num_examples=10000, total_duration=16121.3, train/accuracy=0.202168, train/loss=4.43349, validation/accuracy=0.19292, validation/loss=4.58561, validation/num_examples=50000
I0307 14:06:57.588866 140257634797312 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1015980243682861, loss=2.2819621562957764
I0307 14:07:36.248258 140257643190016 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.093291997909546, loss=2.2864789962768555
I0307 14:08:15.230394 140257634797312 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.403802514076233, loss=2.252009630203247
I0307 14:08:53.775377 140257643190016 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.128029227256775, loss=2.303861618041992
I0307 14:09:32.399903 140257634797312 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.1260335445404053, loss=2.3460965156555176
I0307 14:10:10.754126 140257643190016 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.3454331159591675, loss=2.2929749488830566
I0307 14:10:49.512681 140257634797312 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.071323037147522, loss=2.217472553253174
I0307 14:11:28.140889 140257643190016 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.059238314628601, loss=2.2079968452453613
I0307 14:12:06.712497 140257634797312 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1655914783477783, loss=2.415440082550049
I0307 14:12:45.451245 140257643190016 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.2699955701828003, loss=2.2231757640838623
I0307 14:13:24.392909 140257634797312 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.1785105466842651, loss=2.097430944442749
I0307 14:14:02.692001 140257643190016 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.183359146118164, loss=2.3025991916656494
I0307 14:14:41.340103 140257634797312 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.0506789684295654, loss=2.1719021797180176
I0307 14:15:18.525482 140413841560768 spec.py:321] Evaluating on the training split.
I0307 14:15:31.449635 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 14:15:54.662122 140413841560768 spec.py:349] Evaluating on the test split.
I0307 14:15:56.593394 140413841560768 submission_runner.py:469] Time since start: 16669.42s, 	Step: 39498, 	{'train/accuracy': 0.2537866532802582, 'train/loss': 4.071917533874512, 'validation/accuracy': 0.25071999430656433, 'validation/loss': 4.174068927764893, 'validation/num_examples': 50000, 'test/accuracy': 0.17990000545978546, 'test/loss': 5.063265800476074, 'test/num_examples': 10000, 'score': 15356.487592697144, 'total_duration': 16669.422901153564, 'accumulated_submission_time': 15356.487592697144, 'accumulated_eval_time': 1304.8512241840363, 'accumulated_logging_time': 4.014764070510864}
I0307 14:15:56.667689 140257643190016 logging_writer.py:48] [39498] accumulated_eval_time=1304.85, accumulated_logging_time=4.01476, accumulated_submission_time=15356.5, global_step=39498, preemption_count=0, score=15356.5, test/accuracy=0.1799, test/loss=5.06327, test/num_examples=10000, total_duration=16669.4, train/accuracy=0.253787, train/loss=4.07192, validation/accuracy=0.25072, validation/loss=4.17407, validation/num_examples=50000
I0307 14:15:57.853072 140257634797312 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.2024717330932617, loss=2.292851686477661
I0307 14:16:36.495571 140257643190016 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.328953742980957, loss=2.300426959991455
I0307 14:17:15.472098 140257634797312 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.2343474626541138, loss=2.32027006149292
I0307 14:17:53.966771 140257643190016 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.2112162113189697, loss=2.258934736251831
I0307 14:18:32.468878 140257634797312 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.2287981510162354, loss=2.329958915710449
I0307 14:19:11.061992 140257643190016 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.1353856325149536, loss=2.226158380508423
I0307 14:19:50.776825 140257634797312 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.2313834428787231, loss=2.379074811935425
I0307 14:20:29.589416 140257643190016 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.2625946998596191, loss=2.370190382003784
I0307 14:21:07.914765 140257634797312 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.140377163887024, loss=2.121019124984741
I0307 14:21:46.071384 140257643190016 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.2241605520248413, loss=2.3308446407318115
I0307 14:22:24.959225 140257634797312 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.2958945035934448, loss=2.117375373840332
I0307 14:23:03.413199 140257643190016 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.1249094009399414, loss=2.225841522216797
I0307 14:23:42.028915 140257634797312 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.1891968250274658, loss=2.232103109359741
I0307 14:24:20.398509 140257643190016 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.1820054054260254, loss=2.3072948455810547
I0307 14:24:26.877794 140413841560768 spec.py:321] Evaluating on the training split.
I0307 14:24:40.177968 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 14:25:01.979945 140413841560768 spec.py:349] Evaluating on the test split.
I0307 14:25:03.818335 140413841560768 submission_runner.py:469] Time since start: 17216.65s, 	Step: 40818, 	{'train/accuracy': 0.1706194132566452, 'train/loss': 5.035391807556152, 'validation/accuracy': 0.1625399887561798, 'validation/loss': 5.1285080909729, 'validation/num_examples': 50000, 'test/accuracy': 0.11860000342130661, 'test/loss': 5.980480670928955, 'test/num_examples': 10000, 'score': 15866.528507947922, 'total_duration': 17216.6478536129, 'accumulated_submission_time': 15866.528507947922, 'accumulated_eval_time': 1341.791568994522, 'accumulated_logging_time': 4.1220502853393555}
I0307 14:25:03.915167 140257634797312 logging_writer.py:48] [40818] accumulated_eval_time=1341.79, accumulated_logging_time=4.12205, accumulated_submission_time=15866.5, global_step=40818, preemption_count=0, score=15866.5, test/accuracy=0.1186, test/loss=5.98048, test/num_examples=10000, total_duration=17216.6, train/accuracy=0.170619, train/loss=5.03539, validation/accuracy=0.16254, validation/loss=5.12851, validation/num_examples=50000
I0307 14:25:35.765301 140257643190016 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.224278211593628, loss=2.319413423538208
I0307 14:26:14.016410 140257634797312 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.2053062915802002, loss=2.2744204998016357
I0307 14:26:52.665032 140257643190016 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1876394748687744, loss=2.2906064987182617
I0307 14:27:31.289442 140257634797312 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.0649688243865967, loss=2.127220630645752
I0307 14:28:10.547598 140257643190016 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.074142575263977, loss=2.328423023223877
I0307 14:28:49.814620 140257634797312 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.2030553817749023, loss=2.151353359222412
I0307 14:29:28.631217 140257643190016 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.2064576148986816, loss=2.200817108154297
I0307 14:30:07.380739 140257634797312 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.2248207330703735, loss=2.1886515617370605
I0307 14:30:46.074081 140257643190016 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1809841394424438, loss=2.3112001419067383
I0307 14:31:25.045570 140257634797312 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.2657216787338257, loss=2.199704170227051
I0307 14:32:03.624214 140257643190016 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1309716701507568, loss=2.4046835899353027
I0307 14:32:42.007709 140257634797312 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.1838701963424683, loss=2.3535876274108887
I0307 14:33:20.766414 140257643190016 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.1951943635940552, loss=2.1492972373962402
I0307 14:33:33.947220 140413841560768 spec.py:321] Evaluating on the training split.
I0307 14:33:46.507903 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 14:34:10.330127 140413841560768 spec.py:349] Evaluating on the test split.
I0307 14:34:12.163203 140413841560768 submission_runner.py:469] Time since start: 17764.99s, 	Step: 42135, 	{'train/accuracy': 0.11567282676696777, 'train/loss': 5.940239429473877, 'validation/accuracy': 0.11229999363422394, 'validation/loss': 6.0063252449035645, 'validation/num_examples': 50000, 'test/accuracy': 0.07880000025033951, 'test/loss': 6.625520706176758, 'test/num_examples': 10000, 'score': 16376.38707447052, 'total_duration': 17764.992713451385, 'accumulated_submission_time': 16376.38707447052, 'accumulated_eval_time': 1380.0073475837708, 'accumulated_logging_time': 4.25922417640686}
I0307 14:34:12.309436 140257634797312 logging_writer.py:48] [42135] accumulated_eval_time=1380.01, accumulated_logging_time=4.25922, accumulated_submission_time=16376.4, global_step=42135, preemption_count=0, score=16376.4, test/accuracy=0.0788, test/loss=6.62552, test/num_examples=10000, total_duration=17765, train/accuracy=0.115673, train/loss=5.94024, validation/accuracy=0.1123, validation/loss=6.00633, validation/num_examples=50000
I0307 14:34:38.009551 140257643190016 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.2202762365341187, loss=2.203463554382324
I0307 14:35:16.303692 140257634797312 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1955819129943848, loss=2.3789377212524414
I0307 14:35:55.082618 140257643190016 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.13676917552948, loss=2.1648216247558594
I0307 14:36:33.906857 140257634797312 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.20913565158844, loss=2.30883526802063
I0307 14:37:12.728265 140257643190016 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.1477289199829102, loss=2.3438751697540283
I0307 14:37:51.597316 140257634797312 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.1644697189331055, loss=2.2757270336151123
I0307 14:38:30.136430 140257643190016 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.1251381635665894, loss=2.0914673805236816
I0307 14:39:08.607250 140257634797312 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.210810661315918, loss=2.378448247909546
I0307 14:39:46.798944 140257643190016 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.1021575927734375, loss=2.3900437355041504
I0307 14:40:25.473163 140257634797312 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.1438605785369873, loss=2.1401560306549072
I0307 14:41:03.669764 140257643190016 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.201966643333435, loss=2.255350112915039
I0307 14:41:42.053158 140257634797312 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.2903821468353271, loss=2.2325551509857178
I0307 14:42:20.575129 140257643190016 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.204972743988037, loss=2.3945772647857666
I0307 14:42:42.422355 140413841560768 spec.py:321] Evaluating on the training split.
I0307 14:42:55.438272 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 14:43:19.593301 140413841560768 spec.py:349] Evaluating on the test split.
I0307 14:43:21.427896 140413841560768 submission_runner.py:469] Time since start: 18314.26s, 	Step: 43457, 	{'train/accuracy': 0.0633370503783226, 'train/loss': 7.920056343078613, 'validation/accuracy': 0.05791999772191048, 'validation/loss': 7.952990531921387, 'validation/num_examples': 50000, 'test/accuracy': 0.04070000350475311, 'test/loss': 8.67208480834961, 'test/num_examples': 10000, 'score': 16886.3216483593, 'total_duration': 18314.257409095764, 'accumulated_submission_time': 16886.3216483593, 'accumulated_eval_time': 1419.0126914978027, 'accumulated_logging_time': 4.453425645828247}
I0307 14:43:21.542541 140257634797312 logging_writer.py:48] [43457] accumulated_eval_time=1419.01, accumulated_logging_time=4.45343, accumulated_submission_time=16886.3, global_step=43457, preemption_count=0, score=16886.3, test/accuracy=0.0407, test/loss=8.67208, test/num_examples=10000, total_duration=18314.3, train/accuracy=0.0633371, train/loss=7.92006, validation/accuracy=0.05792, validation/loss=7.95299, validation/num_examples=50000
I0307 14:43:38.722370 140257643190016 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.2813801765441895, loss=2.310265064239502
I0307 14:44:17.286179 140257634797312 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.19929039478302, loss=2.2294626235961914
I0307 14:44:56.437371 140257643190016 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.2069758176803589, loss=2.3151187896728516
I0307 14:45:36.451838 140257634797312 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.1868224143981934, loss=2.2082507610321045
I0307 14:46:15.412146 140257643190016 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2200490236282349, loss=2.171346664428711
I0307 14:46:53.986434 140257634797312 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1516692638397217, loss=2.2086400985717773
I0307 14:47:32.802448 140257643190016 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.177245855331421, loss=2.161781072616577
I0307 14:48:11.360186 140257634797312 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.2066948413848877, loss=2.2615227699279785
I0307 14:48:49.945902 140257643190016 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.2006664276123047, loss=2.204575538635254
I0307 14:49:28.715852 140257634797312 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1796143054962158, loss=2.3185842037200928
I0307 14:50:07.402532 140257643190016 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.0902448892593384, loss=2.1448042392730713
I0307 14:50:45.870016 140257634797312 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1385325193405151, loss=2.395157814025879
I0307 14:51:24.468468 140257643190016 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.2245339155197144, loss=2.2033133506774902
I0307 14:51:51.436563 140413841560768 spec.py:321] Evaluating on the training split.
I0307 14:52:04.183339 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 14:52:30.087719 140413841560768 spec.py:349] Evaluating on the test split.
I0307 14:52:31.919718 140413841560768 submission_runner.py:469] Time since start: 18864.75s, 	Step: 44771, 	{'train/accuracy': 0.0276227667927742, 'train/loss': 7.577892303466797, 'validation/accuracy': 0.02725999988615513, 'validation/loss': 7.615306854248047, 'validation/num_examples': 50000, 'test/accuracy': 0.01850000023841858, 'test/loss': 7.915430068969727, 'test/num_examples': 10000, 'score': 17396.043432474136, 'total_duration': 18864.749269485474, 'accumulated_submission_time': 17396.043432474136, 'accumulated_eval_time': 1459.4956834316254, 'accumulated_logging_time': 4.602795600891113}
I0307 14:52:31.998013 140257634797312 logging_writer.py:48] [44771] accumulated_eval_time=1459.5, accumulated_logging_time=4.6028, accumulated_submission_time=17396, global_step=44771, preemption_count=0, score=17396, test/accuracy=0.0185, test/loss=7.91543, test/num_examples=10000, total_duration=18864.7, train/accuracy=0.0276228, train/loss=7.57789, validation/accuracy=0.02726, validation/loss=7.61531, validation/num_examples=50000
I0307 14:52:43.609547 140257643190016 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.3150426149368286, loss=2.3312692642211914
I0307 14:53:22.353211 140257634797312 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.274031400680542, loss=2.282045841217041
I0307 14:54:02.033092 140257643190016 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1526070833206177, loss=2.1827023029327393
I0307 14:54:42.933019 140257634797312 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.120192527770996, loss=2.208686351776123
I0307 14:55:21.743107 140257643190016 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.2656162977218628, loss=2.2315902709960938
I0307 14:56:00.576085 140257634797312 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.1718378067016602, loss=2.292029619216919
I0307 14:56:39.196554 140257643190016 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1967793703079224, loss=2.385136842727661
I0307 14:57:18.028166 140257634797312 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0699000358581543, loss=2.1749684810638428
I0307 14:57:56.813509 140257643190016 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.145890235900879, loss=2.243004083633423
I0307 14:58:35.601125 140257634797312 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.2268407344818115, loss=2.3708457946777344
I0307 14:59:14.408176 140257643190016 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.2358511686325073, loss=2.418639898300171
I0307 14:59:53.214284 140257634797312 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.1903059482574463, loss=2.371399402618408
I0307 15:00:31.574921 140257643190016 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1477831602096558, loss=2.1868035793304443
I0307 15:01:02.151145 140413841560768 spec.py:321] Evaluating on the training split.
I0307 15:01:15.184367 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 15:01:36.839945 140413841560768 spec.py:349] Evaluating on the test split.
I0307 15:01:38.667682 140413841560768 submission_runner.py:469] Time since start: 19411.50s, 	Step: 46080, 	{'train/accuracy': 0.2875278890132904, 'train/loss': 3.640925168991089, 'validation/accuracy': 0.2669999897480011, 'validation/loss': 3.8019394874572754, 'validation/num_examples': 50000, 'test/accuracy': 0.20350000262260437, 'test/loss': 4.463255882263184, 'test/num_examples': 10000, 'score': 17906.037656784058, 'total_duration': 19411.497208595276, 'accumulated_submission_time': 17906.037656784058, 'accumulated_eval_time': 1496.012038230896, 'accumulated_logging_time': 4.707248210906982}
I0307 15:01:38.723806 140257634797312 logging_writer.py:48] [46080] accumulated_eval_time=1496.01, accumulated_logging_time=4.70725, accumulated_submission_time=17906, global_step=46080, preemption_count=0, score=17906, test/accuracy=0.2035, test/loss=4.46326, test/num_examples=10000, total_duration=19411.5, train/accuracy=0.287528, train/loss=3.64093, validation/accuracy=0.267, validation/loss=3.80194, validation/num_examples=50000
I0307 15:01:47.229940 140257643190016 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.4142215251922607, loss=2.2351279258728027
I0307 15:02:26.109391 140257634797312 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.180273175239563, loss=2.1154983043670654
I0307 15:03:06.151962 140257643190016 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.172085165977478, loss=2.1580874919891357
I0307 15:03:45.750765 140257634797312 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.1366597414016724, loss=2.1230273246765137
I0307 15:04:24.648813 140257643190016 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1750884056091309, loss=2.2681210041046143
I0307 15:05:03.849858 140257634797312 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.0992907285690308, loss=2.19348406791687
I0307 15:05:42.435610 140257643190016 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.1907864809036255, loss=2.2621519565582275
I0307 15:06:21.065507 140257634797312 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.2703529596328735, loss=2.1816468238830566
I0307 15:06:59.428635 140257643190016 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.2528225183486938, loss=2.309237480163574
I0307 15:07:37.499899 140257634797312 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1965274810791016, loss=2.262467861175537
I0307 15:08:15.808938 140257643190016 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.2210997343063354, loss=2.2750558853149414
I0307 15:08:54.086599 140257634797312 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.2160836458206177, loss=2.181938648223877
I0307 15:09:32.538359 140257643190016 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.2146910429000854, loss=2.2242743968963623
I0307 15:10:08.994079 140413841560768 spec.py:321] Evaluating on the training split.
I0307 15:10:21.695977 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 15:10:40.938852 140413841560768 spec.py:349] Evaluating on the test split.
I0307 15:10:42.720459 140413841560768 submission_runner.py:469] Time since start: 19955.55s, 	Step: 47396, 	{'train/accuracy': 0.13255341351032257, 'train/loss': 5.313976764678955, 'validation/accuracy': 0.12441999465227127, 'validation/loss': 5.409339427947998, 'validation/num_examples': 50000, 'test/accuracy': 0.09480000287294388, 'test/loss': 5.978036880493164, 'test/num_examples': 10000, 'score': 18416.14288687706, 'total_duration': 19955.549971818924, 'accumulated_submission_time': 18416.14288687706, 'accumulated_eval_time': 1529.7382249832153, 'accumulated_logging_time': 4.789154767990112}
I0307 15:10:42.800118 140257634797312 logging_writer.py:48] [47396] accumulated_eval_time=1529.74, accumulated_logging_time=4.78915, accumulated_submission_time=18416.1, global_step=47396, preemption_count=0, score=18416.1, test/accuracy=0.0948, test/loss=5.97804, test/num_examples=10000, total_duration=19955.5, train/accuracy=0.132553, train/loss=5.31398, validation/accuracy=0.12442, validation/loss=5.40934, validation/num_examples=50000
I0307 15:10:44.899953 140257643190016 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1051979064941406, loss=2.192988872528076
I0307 15:11:23.747603 140257634797312 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.325683832168579, loss=2.1840333938598633
I0307 15:12:03.464814 140257643190016 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.2033122777938843, loss=2.2250633239746094
I0307 15:12:42.139419 140257634797312 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2835794687271118, loss=2.2113707065582275
I0307 15:13:20.995099 140257643190016 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.3830877542495728, loss=2.4233415126800537
I0307 15:13:59.683578 140257634797312 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.142073392868042, loss=2.218421697616577
I0307 15:14:38.134979 140257643190016 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.2338553667068481, loss=2.2322616577148438
I0307 15:15:16.608040 140257634797312 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.4249513149261475, loss=2.2491722106933594
I0307 15:15:55.475610 140257643190016 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.2632136344909668, loss=2.253222942352295
I0307 15:16:34.016833 140257634797312 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.1443428993225098, loss=2.181394100189209
I0307 15:17:12.721643 140257643190016 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.3933742046356201, loss=2.2281134128570557
I0307 15:17:51.887984 140257634797312 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2721424102783203, loss=2.2451424598693848
I0307 15:18:30.249838 140257643190016 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.193053960800171, loss=2.193060874938965
I0307 15:19:09.043458 140257634797312 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.210196852684021, loss=2.25598406791687
I0307 15:19:12.989766 140413841560768 spec.py:321] Evaluating on the training split.
I0307 15:19:25.931339 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 15:19:45.760726 140413841560768 spec.py:349] Evaluating on the test split.
I0307 15:19:47.546575 140413841560768 submission_runner.py:469] Time since start: 20500.38s, 	Step: 48711, 	{'train/accuracy': 0.31058672070503235, 'train/loss': 3.350979804992676, 'validation/accuracy': 0.2925199866294861, 'validation/loss': 3.4927120208740234, 'validation/num_examples': 50000, 'test/accuracy': 0.22290000319480896, 'test/loss': 4.135767936706543, 'test/num_examples': 10000, 'score': 18926.149178504944, 'total_duration': 20500.376094579697, 'accumulated_submission_time': 18926.149178504944, 'accumulated_eval_time': 1564.2948393821716, 'accumulated_logging_time': 4.915104627609253}
I0307 15:19:47.620493 140257643190016 logging_writer.py:48] [48711] accumulated_eval_time=1564.29, accumulated_logging_time=4.9151, accumulated_submission_time=18926.1, global_step=48711, preemption_count=0, score=18926.1, test/accuracy=0.2229, test/loss=4.13577, test/num_examples=10000, total_duration=20500.4, train/accuracy=0.310587, train/loss=3.35098, validation/accuracy=0.29252, validation/loss=3.49271, validation/num_examples=50000
I0307 15:20:22.696264 140257634797312 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.426139235496521, loss=2.1198651790618896
I0307 15:21:01.506266 140257643190016 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.1964353322982788, loss=2.293825626373291
I0307 15:21:40.242837 140257634797312 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.1887630224227905, loss=2.1353325843811035
I0307 15:22:18.733181 140257643190016 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.1795003414154053, loss=2.1301429271698
I0307 15:22:57.318735 140257634797312 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.3231518268585205, loss=2.392526865005493
I0307 15:23:36.074688 140257643190016 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.2144166231155396, loss=2.2023026943206787
I0307 15:24:14.418210 140257634797312 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.2863636016845703, loss=2.2707552909851074
I0307 15:24:52.663137 140257643190016 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1836494207382202, loss=2.3591480255126953
I0307 15:25:30.795771 140257634797312 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.2303574085235596, loss=2.249375343322754
I0307 15:26:09.754326 140257643190016 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.6121214628219604, loss=2.450383424758911
I0307 15:26:48.288771 140257634797312 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.2183434963226318, loss=2.3074915409088135
I0307 15:27:26.943756 140257643190016 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.3414247035980225, loss=2.319303274154663
I0307 15:28:05.631532 140257634797312 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.4005740880966187, loss=2.240788459777832
I0307 15:28:17.582292 140413841560768 spec.py:321] Evaluating on the training split.
I0307 15:28:30.099059 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 15:28:59.511352 140413841560768 spec.py:349] Evaluating on the test split.
I0307 15:29:01.252601 140413841560768 submission_runner.py:469] Time since start: 21054.08s, 	Step: 50031, 	{'train/accuracy': 0.26189810037612915, 'train/loss': 3.9904892444610596, 'validation/accuracy': 0.24539999663829803, 'validation/loss': 4.136928558349609, 'validation/num_examples': 50000, 'test/accuracy': 0.17630000412464142, 'test/loss': 4.876581192016602, 'test/num_examples': 10000, 'score': 19435.944265127182, 'total_duration': 21054.082141160965, 'accumulated_submission_time': 19435.944265127182, 'accumulated_eval_time': 1607.9649820327759, 'accumulated_logging_time': 5.024606704711914}
I0307 15:29:01.346764 140257643190016 logging_writer.py:48] [50031] accumulated_eval_time=1607.96, accumulated_logging_time=5.02461, accumulated_submission_time=19435.9, global_step=50031, preemption_count=0, score=19435.9, test/accuracy=0.1763, test/loss=4.87658, test/num_examples=10000, total_duration=21054.1, train/accuracy=0.261898, train/loss=3.99049, validation/accuracy=0.2454, validation/loss=4.13693, validation/num_examples=50000
I0307 15:29:28.889124 140257634797312 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.1624391078948975, loss=2.305795431137085
I0307 15:30:07.656568 140257643190016 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.1800305843353271, loss=2.3177998065948486
I0307 15:30:46.408309 140257634797312 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.246669054031372, loss=2.1659584045410156
I0307 15:31:25.124232 140257643190016 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2226231098175049, loss=2.2613348960876465
I0307 15:32:03.627022 140257634797312 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.238156795501709, loss=2.279010534286499
I0307 15:32:42.127688 140257643190016 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1017060279846191, loss=2.15096378326416
I0307 15:33:20.998685 140257634797312 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.1984121799468994, loss=2.267103672027588
I0307 15:33:59.780543 140257643190016 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.3101555109024048, loss=2.324822187423706
I0307 15:34:38.215231 140257634797312 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.27658212184906, loss=2.1420352458953857
I0307 15:35:16.729669 140257643190016 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.3410017490386963, loss=2.2030856609344482
I0307 15:35:55.447980 140257634797312 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.268248438835144, loss=2.1537680625915527
I0307 15:36:34.179991 140257643190016 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.2157078981399536, loss=2.1867618560791016
I0307 15:37:13.565837 140257634797312 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.1191004514694214, loss=2.1751716136932373
I0307 15:37:31.497162 140413841560768 spec.py:321] Evaluating on the training split.
I0307 15:37:44.519093 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 15:38:07.880049 140413841560768 spec.py:349] Evaluating on the test split.
I0307 15:38:09.693355 140413841560768 submission_runner.py:469] Time since start: 21602.52s, 	Step: 51348, 	{'train/accuracy': 0.24914300441741943, 'train/loss': 4.0262651443481445, 'validation/accuracy': 0.22981999814510345, 'validation/loss': 4.26649284362793, 'validation/num_examples': 50000, 'test/accuracy': 0.17180000245571136, 'test/loss': 4.9555487632751465, 'test/num_examples': 10000, 'score': 19945.93607735634, 'total_duration': 21602.522869825363, 'accumulated_submission_time': 19945.93607735634, 'accumulated_eval_time': 1646.1609761714935, 'accumulated_logging_time': 5.146812200546265}
I0307 15:38:09.798926 140257643190016 logging_writer.py:48] [51348] accumulated_eval_time=1646.16, accumulated_logging_time=5.14681, accumulated_submission_time=19945.9, global_step=51348, preemption_count=0, score=19945.9, test/accuracy=0.1718, test/loss=4.95555, test/num_examples=10000, total_duration=21602.5, train/accuracy=0.249143, train/loss=4.02627, validation/accuracy=0.22982, validation/loss=4.26649, validation/num_examples=50000
I0307 15:38:29.983667 140257634797312 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.1567192077636719, loss=2.28778076171875
I0307 15:39:08.546066 140257643190016 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.2204229831695557, loss=2.2679367065429688
I0307 15:39:46.813606 140257634797312 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2343469858169556, loss=2.344219923019409
I0307 15:40:25.227251 140257643190016 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.1909133195877075, loss=2.295865535736084
I0307 15:41:03.717656 140257634797312 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.3725357055664062, loss=2.2647957801818848
I0307 15:41:42.211443 140257643190016 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2336865663528442, loss=2.372279405593872
I0307 15:42:20.869883 140257634797312 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2256944179534912, loss=2.220613479614258
I0307 15:42:59.361061 140257643190016 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.190938115119934, loss=2.1353089809417725
I0307 15:43:38.032397 140257634797312 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.249186635017395, loss=2.260890483856201
I0307 15:44:16.548292 140257643190016 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.161441445350647, loss=2.31299090385437
I0307 15:44:54.940710 140257634797312 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.1919587850570679, loss=2.2724616527557373
I0307 15:45:33.637726 140257643190016 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2971148490905762, loss=2.2268126010894775
I0307 15:46:12.466346 140257634797312 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.338407278060913, loss=2.2170252799987793
I0307 15:46:39.836586 140413841560768 spec.py:321] Evaluating on the training split.
I0307 15:46:52.454403 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 15:47:15.024672 140413841560768 spec.py:349] Evaluating on the test split.
I0307 15:47:16.964202 140413841560768 submission_runner.py:469] Time since start: 22149.79s, 	Step: 52672, 	{'train/accuracy': 0.3948700428009033, 'train/loss': 2.825113296508789, 'validation/accuracy': 0.3696399927139282, 'validation/loss': 3.0120792388916016, 'validation/num_examples': 50000, 'test/accuracy': 0.26250001788139343, 'test/loss': 3.9060709476470947, 'test/num_examples': 10000, 'score': 20455.814186811447, 'total_duration': 22149.79374241829, 'accumulated_submission_time': 20455.814186811447, 'accumulated_eval_time': 1683.2884595394135, 'accumulated_logging_time': 5.277775764465332}
I0307 15:47:17.058590 140257643190016 logging_writer.py:48] [52672] accumulated_eval_time=1683.29, accumulated_logging_time=5.27778, accumulated_submission_time=20455.8, global_step=52672, preemption_count=0, score=20455.8, test/accuracy=0.2625, test/loss=3.90607, test/num_examples=10000, total_duration=22149.8, train/accuracy=0.39487, train/loss=2.82511, validation/accuracy=0.36964, validation/loss=3.01208, validation/num_examples=50000
I0307 15:47:28.246242 140257634797312 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.250170111656189, loss=2.2188720703125
I0307 15:48:06.623243 140257643190016 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.4520395994186401, loss=2.3046486377716064
I0307 15:48:45.395876 140257634797312 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.3452539443969727, loss=2.444868326187134
I0307 15:49:24.117102 140257643190016 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.2145183086395264, loss=2.1890716552734375
I0307 15:50:02.539413 140257634797312 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.3384108543395996, loss=2.3214399814605713
I0307 15:50:41.231869 140257643190016 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.4429110288619995, loss=2.1806488037109375
I0307 15:51:19.649099 140257634797312 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.2390201091766357, loss=2.338109016418457
I0307 15:51:58.359851 140257643190016 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.2347421646118164, loss=2.1625051498413086
I0307 15:52:36.936751 140257634797312 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.11054265499115, loss=2.108593463897705
I0307 15:53:15.700730 140257643190016 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3693372011184692, loss=2.151170015335083
I0307 15:53:54.229422 140257634797312 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.3225034475326538, loss=2.2741549015045166
I0307 15:54:33.320641 140257643190016 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.2331273555755615, loss=2.1762425899505615
I0307 15:55:12.365256 140257634797312 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.251007318496704, loss=2.241457462310791
I0307 15:55:46.966474 140413841560768 spec.py:321] Evaluating on the training split.
I0307 15:55:59.638629 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 15:56:17.828242 140413841560768 spec.py:349] Evaluating on the test split.
I0307 15:56:19.636453 140413841560768 submission_runner.py:469] Time since start: 22692.47s, 	Step: 53991, 	{'train/accuracy': 0.3551100194454193, 'train/loss': 3.0897536277770996, 'validation/accuracy': 0.33267998695373535, 'validation/loss': 3.2748899459838867, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 4.059902191162109, 'test/num_examples': 10000, 'score': 20965.567807912827, 'total_duration': 22692.465978622437, 'accumulated_submission_time': 20965.567807912827, 'accumulated_eval_time': 1715.9582483768463, 'accumulated_logging_time': 5.396424770355225}
I0307 15:56:19.771179 140257643190016 logging_writer.py:48] [53991] accumulated_eval_time=1715.96, accumulated_logging_time=5.39642, accumulated_submission_time=20965.6, global_step=53991, preemption_count=0, score=20965.6, test/accuracy=0.2457, test/loss=4.0599, test/num_examples=10000, total_duration=22692.5, train/accuracy=0.35511, train/loss=3.08975, validation/accuracy=0.33268, validation/loss=3.27489, validation/num_examples=50000
I0307 15:56:23.823265 140257634797312 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.2239340543746948, loss=2.218719482421875
I0307 15:57:02.463844 140257643190016 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.2057311534881592, loss=2.1907663345336914
I0307 15:57:41.166587 140257634797312 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.2230401039123535, loss=2.200603485107422
I0307 15:58:19.834481 140257643190016 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.2482476234436035, loss=2.243589162826538
I0307 15:58:58.523221 140257634797312 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2627860307693481, loss=2.2723476886749268
I0307 15:59:37.215155 140257643190016 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2581884860992432, loss=2.2398929595947266
I0307 16:00:16.029475 140257634797312 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.24549400806427, loss=2.2145261764526367
I0307 16:00:54.786946 140257643190016 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.3105138540267944, loss=2.3509621620178223
I0307 16:01:33.099734 140257634797312 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.3254165649414062, loss=2.1368207931518555
I0307 16:02:11.603273 140257643190016 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.5584325790405273, loss=2.234121084213257
I0307 16:02:50.592028 140257634797312 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2923178672790527, loss=2.240875720977783
I0307 16:03:30.451967 140257643190016 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.3860177993774414, loss=2.2978556156158447
I0307 16:04:08.740965 140257634797312 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.3678478002548218, loss=2.2476251125335693
I0307 16:04:47.278040 140257643190016 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.2290925979614258, loss=2.1128358840942383
I0307 16:04:50.019253 140413841560768 spec.py:321] Evaluating on the training split.
I0307 16:05:02.863590 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 16:05:24.972081 140413841560768 spec.py:349] Evaluating on the test split.
I0307 16:05:26.793649 140413841560768 submission_runner.py:469] Time since start: 23239.62s, 	Step: 55308, 	{'train/accuracy': 0.22875477373600006, 'train/loss': 4.165148735046387, 'validation/accuracy': 0.20763999223709106, 'validation/loss': 4.339871883392334, 'validation/num_examples': 50000, 'test/accuracy': 0.1592000126838684, 'test/loss': 4.965682029724121, 'test/num_examples': 10000, 'score': 21475.6614010334, 'total_duration': 23239.623193979263, 'accumulated_submission_time': 21475.6614010334, 'accumulated_eval_time': 1752.7324776649475, 'accumulated_logging_time': 5.556323766708374}
I0307 16:05:26.879720 140257634797312 logging_writer.py:48] [55308] accumulated_eval_time=1752.73, accumulated_logging_time=5.55632, accumulated_submission_time=21475.7, global_step=55308, preemption_count=0, score=21475.7, test/accuracy=0.1592, test/loss=4.96568, test/num_examples=10000, total_duration=23239.6, train/accuracy=0.228755, train/loss=4.16515, validation/accuracy=0.20764, validation/loss=4.33987, validation/num_examples=50000
I0307 16:06:02.597208 140257643190016 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.2476590871810913, loss=2.2386703491210938
I0307 16:06:41.061651 140257634797312 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2646076679229736, loss=2.199918746948242
I0307 16:07:19.809001 140257643190016 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.3372173309326172, loss=2.1837785243988037
I0307 16:07:58.868405 140257634797312 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.2053940296173096, loss=2.2364702224731445
I0307 16:08:37.845127 140257643190016 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.2869282960891724, loss=2.061871290206909
I0307 16:09:16.548842 140257634797312 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.295472264289856, loss=2.2382423877716064
I0307 16:09:55.050575 140257643190016 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.2930344343185425, loss=2.263516664505005
I0307 16:10:33.713062 140257634797312 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.2357419729232788, loss=2.189708948135376
I0307 16:11:12.692413 140257643190016 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.3308570384979248, loss=2.325122356414795
I0307 16:11:52.275674 140257634797312 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.3428229093551636, loss=2.20064115524292
I0307 16:12:31.418669 140257643190016 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2681045532226562, loss=2.233048915863037
I0307 16:13:10.224603 140257634797312 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.264760971069336, loss=2.375960350036621
I0307 16:13:48.934065 140257643190016 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.271846890449524, loss=2.2222607135772705
I0307 16:13:56.957021 140413841560768 spec.py:321] Evaluating on the training split.
I0307 16:14:09.519098 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 16:14:31.375822 140413841560768 spec.py:349] Evaluating on the test split.
I0307 16:14:33.191905 140413841560768 submission_runner.py:469] Time since start: 23786.02s, 	Step: 56622, 	{'train/accuracy': 0.3228236436843872, 'train/loss': 3.305924654006958, 'validation/accuracy': 0.3044999837875366, 'validation/loss': 3.4389808177948, 'validation/num_examples': 50000, 'test/accuracy': 0.21770000457763672, 'test/loss': 4.280907154083252, 'test/num_examples': 10000, 'score': 21985.569434404373, 'total_duration': 23786.021417856216, 'accumulated_submission_time': 21985.569434404373, 'accumulated_eval_time': 1788.9671611785889, 'accumulated_logging_time': 5.67985200881958}
I0307 16:14:33.355076 140257634797312 logging_writer.py:48] [56622] accumulated_eval_time=1788.97, accumulated_logging_time=5.67985, accumulated_submission_time=21985.6, global_step=56622, preemption_count=0, score=21985.6, test/accuracy=0.2177, test/loss=4.28091, test/num_examples=10000, total_duration=23786, train/accuracy=0.322824, train/loss=3.30592, validation/accuracy=0.3045, validation/loss=3.43898, validation/num_examples=50000
I0307 16:15:04.119502 140257643190016 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2001020908355713, loss=2.2698757648468018
I0307 16:15:42.715298 140257634797312 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2714720964431763, loss=2.250251293182373
I0307 16:16:21.593695 140257643190016 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.317172884941101, loss=2.235541582107544
I0307 16:17:00.374214 140257634797312 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.2828688621520996, loss=2.280895709991455
I0307 16:17:39.272952 140257643190016 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.1488548517227173, loss=2.209174156188965
I0307 16:19:58.112139 140257634797312 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.4477145671844482, loss=2.220623254776001
I0307 16:21:48.484718 140257643190016 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.3194067478179932, loss=2.2706875801086426
I0307 16:23:03.951432 140413841560768 spec.py:321] Evaluating on the training split.
I0307 16:23:17.565694 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 16:23:38.700016 140413841560768 spec.py:349] Evaluating on the test split.
I0307 16:23:40.519304 140413841560768 submission_runner.py:469] Time since start: 24333.35s, 	Step: 57369, 	{'train/accuracy': 0.3434111773967743, 'train/loss': 3.1648361682891846, 'validation/accuracy': 0.3187199831008911, 'validation/loss': 3.40744948387146, 'validation/num_examples': 50000, 'test/accuracy': 0.23340001702308655, 'test/loss': 4.210224151611328, 'test/num_examples': 10000, 'score': 22496.056319475174, 'total_duration': 24333.348866939545, 'accumulated_submission_time': 22496.056319475174, 'accumulated_eval_time': 1825.5348870754242, 'accumulated_logging_time': 5.8730082511901855}
I0307 16:23:40.541686 140257634797312 logging_writer.py:48] [57369] accumulated_eval_time=1825.53, accumulated_logging_time=5.87301, accumulated_submission_time=22496.1, global_step=57369, preemption_count=0, score=22496.1, test/accuracy=0.2334, test/loss=4.21022, test/num_examples=10000, total_duration=24333.3, train/accuracy=0.343411, train/loss=3.16484, validation/accuracy=0.31872, validation/loss=3.40745, validation/num_examples=50000
I0307 16:24:03.677427 140257643190016 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.3333297967910767, loss=2.303905725479126
I0307 16:25:48.399896 140257634797312 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.2901344299316406, loss=2.3186917304992676
2025-03-07 16:26:25.971715: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:27:37.783406 140257643190016 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.5112285614013672, loss=2.2984321117401123
I0307 16:29:25.924439 140257634797312 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.3205748796463013, loss=2.294247627258301
I0307 16:31:13.714219 140257643190016 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.3237395286560059, loss=2.2233383655548096
I0307 16:32:13.098647 140413841560768 spec.py:321] Evaluating on the training split.
I0307 16:32:24.381404 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 16:32:55.300479 140413841560768 spec.py:349] Evaluating on the test split.
I0307 16:32:57.169947 140413841560768 submission_runner.py:469] Time since start: 24890.00s, 	Step: 57834, 	{'train/accuracy': 0.38028138875961304, 'train/loss': 2.857783555984497, 'validation/accuracy': 0.3585599958896637, 'validation/loss': 3.0040712356567383, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.806081771850586, 'test/num_examples': 10000, 'score': 23008.537341833115, 'total_duration': 24889.999618768692, 'accumulated_submission_time': 23008.537341833115, 'accumulated_eval_time': 1869.6061589717865, 'accumulated_logging_time': 5.920804977416992}
I0307 16:32:57.223069 140257634797312 logging_writer.py:48] [57834] accumulated_eval_time=1869.61, accumulated_logging_time=5.9208, accumulated_submission_time=23008.5, global_step=57834, preemption_count=0, score=23008.5, test/accuracy=0.2657, test/loss=3.80608, test/num_examples=10000, total_duration=24890, train/accuracy=0.380281, train/loss=2.85778, validation/accuracy=0.35856, validation/loss=3.00407, validation/num_examples=50000
I0307 16:37:21.379977 140257643190016 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2373669147491455, loss=2.279567241668701
I0307 16:41:27.235183 140413841560768 spec.py:321] Evaluating on the training split.
I0307 16:41:38.252186 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 16:42:01.141914 140413841560768 spec.py:349] Evaluating on the test split.
I0307 16:42:02.946587 140413841560768 submission_runner.py:469] Time since start: 25435.78s, 	Step: 57959, 	{'train/accuracy': 0.22616389393806458, 'train/loss': 4.178374290466309, 'validation/accuracy': 0.20841999351978302, 'validation/loss': 4.313455104827881, 'validation/num_examples': 50000, 'test/accuracy': 0.1575000137090683, 'test/loss': 4.945911407470703, 'test/num_examples': 10000, 'score': 23518.526952028275, 'total_duration': 25435.776247024536, 'accumulated_submission_time': 23518.526952028275, 'accumulated_eval_time': 1905.3175201416016, 'accumulated_logging_time': 5.982558012008667}
I0307 16:42:02.962253 140257634797312 logging_writer.py:48] [57959] accumulated_eval_time=1905.32, accumulated_logging_time=5.98256, accumulated_submission_time=23518.5, global_step=57959, preemption_count=0, score=23518.5, test/accuracy=0.1575, test/loss=4.94591, test/num_examples=10000, total_duration=25435.8, train/accuracy=0.226164, train/loss=4.17837, validation/accuracy=0.20842, validation/loss=4.31346, validation/num_examples=50000
I0307 16:44:45.123550 140257643190016 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.3511854410171509, loss=2.308617115020752
I0307 16:50:36.364737 140413841560768 spec.py:321] Evaluating on the training split.
I0307 16:50:47.395872 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 16:51:14.795759 140413841560768 spec.py:349] Evaluating on the test split.
I0307 16:51:16.605361 140413841560768 submission_runner.py:469] Time since start: 25989.43s, 	Step: 58084, 	{'train/accuracy': 0.3465600907802582, 'train/loss': 3.1448400020599365, 'validation/accuracy': 0.32510000467300415, 'validation/loss': 3.3172545433044434, 'validation/num_examples': 50000, 'test/accuracy': 0.248400017619133, 'test/loss': 4.014395713806152, 'test/num_examples': 10000, 'score': 24031.906992912292, 'total_duration': 25989.434996843338, 'accumulated_submission_time': 24031.906992912292, 'accumulated_eval_time': 1945.5580773353577, 'accumulated_logging_time': 6.0060083866119385}
I0307 16:51:16.621002 140257634797312 logging_writer.py:48] [58084] accumulated_eval_time=1945.56, accumulated_logging_time=6.00601, accumulated_submission_time=24031.9, global_step=58084, preemption_count=0, score=24031.9, test/accuracy=0.2484, test/loss=4.0144, test/num_examples=10000, total_duration=25989.4, train/accuracy=0.34656, train/loss=3.14484, validation/accuracy=0.3251, validation/loss=3.31725, validation/num_examples=50000
I0307 16:52:11.574127 140257643190016 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.289354681968689, loss=2.156273365020752
I0307 16:59:15.717576 140257634797312 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.3565466403961182, loss=2.195033073425293
I0307 16:59:49.627243 140413841560768 spec.py:321] Evaluating on the training split.
I0307 17:00:00.480228 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 17:00:24.309922 140413841560768 spec.py:349] Evaluating on the test split.
I0307 17:00:26.074630 140413841560768 submission_runner.py:469] Time since start: 26538.90s, 	Step: 58209, 	{'train/accuracy': 0.11766581237316132, 'train/loss': 5.678391456604004, 'validation/accuracy': 0.11631999909877777, 'validation/loss': 5.718416213989258, 'validation/num_examples': 50000, 'test/accuracy': 0.0828000009059906, 'test/loss': 6.362591743469238, 'test/num_examples': 10000, 'score': 24544.891598939896, 'total_duration': 26538.904282331467, 'accumulated_submission_time': 24544.891598939896, 'accumulated_eval_time': 1982.005404472351, 'accumulated_logging_time': 6.0293285846710205}
I0307 17:00:26.090111 140257643190016 logging_writer.py:48] [58209] accumulated_eval_time=1982.01, accumulated_logging_time=6.02933, accumulated_submission_time=24544.9, global_step=58209, preemption_count=0, score=24544.9, test/accuracy=0.0828, test/loss=6.36259, test/num_examples=10000, total_duration=26538.9, train/accuracy=0.117666, train/loss=5.67839, validation/accuracy=0.11632, validation/loss=5.71842, validation/num_examples=50000
I0307 17:06:36.840738 140257634797312 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.2264494895935059, loss=2.1762278079986572
I0307 17:08:57.589151 140413841560768 spec.py:321] Evaluating on the training split.
I0307 17:09:08.410244 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 17:09:27.337894 140413841560768 spec.py:349] Evaluating on the test split.
I0307 17:09:29.121823 140413841560768 submission_runner.py:469] Time since start: 27081.95s, 	Step: 58352, 	{'train/accuracy': 0.24248644709587097, 'train/loss': 4.11922550201416, 'validation/accuracy': 0.2253599911928177, 'validation/loss': 4.246465682983398, 'validation/num_examples': 50000, 'test/accuracy': 0.15960000455379486, 'test/loss': 5.080419540405273, 'test/num_examples': 10000, 'score': 25056.365832567215, 'total_duration': 27081.95148229599, 'accumulated_submission_time': 25056.365832567215, 'accumulated_eval_time': 2013.538027048111, 'accumulated_logging_time': 6.053134441375732}
I0307 17:09:29.138151 140257643190016 logging_writer.py:48] [58352] accumulated_eval_time=2013.54, accumulated_logging_time=6.05313, accumulated_submission_time=25056.4, global_step=58352, preemption_count=0, score=25056.4, test/accuracy=0.1596, test/loss=5.08042, test/num_examples=10000, total_duration=27082, train/accuracy=0.242486, train/loss=4.11923, validation/accuracy=0.22536, validation/loss=4.24647, validation/num_examples=50000
I0307 17:10:53.156354 140257634797312 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.277894139289856, loss=2.166189193725586
I0307 17:13:35.673225 140257643190016 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.3057397603988647, loss=2.177622079849243
I0307 17:16:23.740283 140257634797312 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.229620099067688, loss=2.2644481658935547
I0307 17:18:00.363012 140413841560768 spec.py:321] Evaluating on the training split.
I0307 17:18:10.914175 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 17:18:28.356569 140413841560768 spec.py:349] Evaluating on the test split.
I0307 17:18:30.167885 140413841560768 submission_runner.py:469] Time since start: 27623.00s, 	Step: 58659, 	{'train/accuracy': 0.2133091539144516, 'train/loss': 4.2986931800842285, 'validation/accuracy': 0.19991999864578247, 'validation/loss': 4.448964595794678, 'validation/num_examples': 50000, 'test/accuracy': 0.15380001068115234, 'test/loss': 5.150226593017578, 'test/num_examples': 10000, 'score': 25567.54655981064, 'total_duration': 27622.99753499031, 'accumulated_submission_time': 25567.54655981064, 'accumulated_eval_time': 2043.3428399562836, 'accumulated_logging_time': 6.0776989459991455}
I0307 17:18:30.186014 140257643190016 logging_writer.py:48] [58659] accumulated_eval_time=2043.34, accumulated_logging_time=6.0777, accumulated_submission_time=25567.5, global_step=58659, preemption_count=0, score=25567.5, test/accuracy=0.1538, test/loss=5.15023, test/num_examples=10000, total_duration=27623, train/accuracy=0.213309, train/loss=4.29869, validation/accuracy=0.19992, validation/loss=4.44896, validation/num_examples=50000
I0307 17:19:26.136606 140257634797312 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.3778901100158691, loss=2.1773531436920166
I0307 17:22:20.385627 140257643190016 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.2979793548583984, loss=2.246776819229126
I0307 17:24:44.071269 140257634797312 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.3017606735229492, loss=2.1914374828338623
I0307 17:27:00.711704 140413841560768 spec.py:321] Evaluating on the training split.
I0307 17:27:11.990613 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 17:27:31.165729 140413841560768 spec.py:349] Evaluating on the test split.
I0307 17:27:32.973521 140413841560768 submission_runner.py:469] Time since start: 28165.80s, 	Step: 58996, 	{'train/accuracy': 0.11714763939380646, 'train/loss': 5.317147731781006, 'validation/accuracy': 0.10991999506950378, 'validation/loss': 5.358582973480225, 'validation/num_examples': 50000, 'test/accuracy': 0.07830000668764114, 'test/loss': 5.9857964515686035, 'test/num_examples': 10000, 'score': 26078.026283740997, 'total_duration': 28165.803186655045, 'accumulated_submission_time': 26078.026283740997, 'accumulated_eval_time': 2075.6046130657196, 'accumulated_logging_time': 6.105093002319336}
I0307 17:27:33.046777 140257643190016 logging_writer.py:48] [58996] accumulated_eval_time=2075.6, accumulated_logging_time=6.10509, accumulated_submission_time=26078, global_step=58996, preemption_count=0, score=26078, test/accuracy=0.0783, test/loss=5.9858, test/num_examples=10000, total_duration=28165.8, train/accuracy=0.117148, train/loss=5.31715, validation/accuracy=0.10992, validation/loss=5.35858, validation/num_examples=50000
I0307 17:27:35.009690 140257634797312 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2954015731811523, loss=2.3122575283050537
I0307 17:29:49.092392 140257643190016 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.363524317741394, loss=2.1733686923980713
I0307 17:32:10.682805 140257634797312 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.27842116355896, loss=2.2674808502197266
I0307 17:34:33.399252 140257643190016 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.2241021394729614, loss=2.207192897796631
I0307 17:36:03.312031 140413841560768 spec.py:321] Evaluating on the training split.
I0307 17:36:14.581814 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 17:36:35.764653 140413841560768 spec.py:349] Evaluating on the test split.
I0307 17:36:37.555923 140413841560768 submission_runner.py:469] Time since start: 28710.39s, 	Step: 59364, 	{'train/accuracy': 0.24918286502361298, 'train/loss': 3.97361159324646, 'validation/accuracy': 0.24108000099658966, 'validation/loss': 4.0884504318237305, 'validation/num_examples': 50000, 'test/accuracy': 0.17190000414848328, 'test/loss': 4.903674602508545, 'test/num_examples': 10000, 'score': 26588.208255767822, 'total_duration': 28710.385571956635, 'accumulated_submission_time': 26588.208255767822, 'accumulated_eval_time': 2109.848464488983, 'accumulated_logging_time': 6.2210023403167725}
I0307 17:36:37.601324 140257634797312 logging_writer.py:48] [59364] accumulated_eval_time=2109.85, accumulated_logging_time=6.221, accumulated_submission_time=26588.2, global_step=59364, preemption_count=0, score=26588.2, test/accuracy=0.1719, test/loss=4.90367, test/num_examples=10000, total_duration=28710.4, train/accuracy=0.249183, train/loss=3.97361, validation/accuracy=0.24108, validation/loss=4.08845, validation/num_examples=50000
I0307 17:37:15.288756 140257643190016 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.2352561950683594, loss=2.163339853286743
I0307 17:39:39.390885 140257634797312 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.3581476211547852, loss=2.2028584480285645
I0307 17:41:59.174150 140257643190016 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.1690473556518555, loss=2.253199338912964
I0307 17:44:19.239539 140257634797312 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.3882133960723877, loss=2.1700668334960938
I0307 17:45:08.350750 140413841560768 spec.py:321] Evaluating on the training split.
I0307 17:45:19.504194 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 17:45:42.634479 140413841560768 spec.py:349] Evaluating on the test split.
I0307 17:45:44.433331 140413841560768 submission_runner.py:469] Time since start: 29257.26s, 	Step: 59736, 	{'train/accuracy': 0.28993940353393555, 'train/loss': 3.7358789443969727, 'validation/accuracy': 0.2728399932384491, 'validation/loss': 3.9174323081970215, 'validation/num_examples': 50000, 'test/accuracy': 0.20040000975131989, 'test/loss': 4.754363059997559, 'test/num_examples': 10000, 'score': 27098.892266988754, 'total_duration': 29257.262996196747, 'accumulated_submission_time': 27098.892266988754, 'accumulated_eval_time': 2145.9310023784637, 'accumulated_logging_time': 6.290436744689941}
I0307 17:45:44.511529 140257643190016 logging_writer.py:48] [59736] accumulated_eval_time=2145.93, accumulated_logging_time=6.29044, accumulated_submission_time=27098.9, global_step=59736, preemption_count=0, score=27098.9, test/accuracy=0.2004, test/loss=4.75436, test/num_examples=10000, total_duration=29257.3, train/accuracy=0.289939, train/loss=3.73588, validation/accuracy=0.27284, validation/loss=3.91743, validation/num_examples=50000
I0307 17:47:04.964619 140257634797312 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.3107227087020874, loss=2.330984354019165
I0307 17:49:29.240073 140257643190016 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.3534191846847534, loss=2.3054873943328857
I0307 17:51:53.729658 140257634797312 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.3302487134933472, loss=2.199507474899292
2025-03-07 17:52:47.639903: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:54:15.256067 140413841560768 spec.py:321] Evaluating on the training split.
I0307 17:54:26.386580 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 17:54:47.706478 140413841560768 spec.py:349] Evaluating on the test split.
I0307 17:54:49.509220 140413841560768 submission_runner.py:469] Time since start: 29802.34s, 	Step: 60098, 	{'train/accuracy': 0.3938934803009033, 'train/loss': 2.868344306945801, 'validation/accuracy': 0.3738199770450592, 'validation/loss': 3.0190305709838867, 'validation/num_examples': 50000, 'test/accuracy': 0.2800000011920929, 'test/loss': 3.819106340408325, 'test/num_examples': 10000, 'score': 27609.56351208687, 'total_duration': 29802.338884830475, 'accumulated_submission_time': 27609.56351208687, 'accumulated_eval_time': 2180.184112548828, 'accumulated_logging_time': 6.402388095855713}
I0307 17:54:49.569303 140257643190016 logging_writer.py:48] [60098] accumulated_eval_time=2180.18, accumulated_logging_time=6.40239, accumulated_submission_time=27609.6, global_step=60098, preemption_count=0, score=27609.6, test/accuracy=0.28, test/loss=3.81911, test/num_examples=10000, total_duration=29802.3, train/accuracy=0.393893, train/loss=2.86834, validation/accuracy=0.37382, validation/loss=3.01903, validation/num_examples=50000
I0307 17:54:50.739840 140257634797312 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.289872407913208, loss=2.2105414867401123
I0307 17:57:02.205528 140257643190016 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.2809538841247559, loss=2.219726324081421
I0307 17:59:23.511498 140257634797312 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.3088700771331787, loss=2.379509449005127
I0307 18:01:44.774640 140257643190016 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.297629952430725, loss=2.043593168258667
I0307 18:03:19.595847 140413841560768 spec.py:321] Evaluating on the training split.
I0307 18:03:30.708508 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 18:03:50.004495 140413841560768 spec.py:349] Evaluating on the test split.
I0307 18:03:51.820576 140413841560768 submission_runner.py:469] Time since start: 30344.65s, 	Step: 60468, 	{'train/accuracy': 0.24487802386283875, 'train/loss': 4.541502475738525, 'validation/accuracy': 0.2254999876022339, 'validation/loss': 4.7774224281311035, 'validation/num_examples': 50000, 'test/accuracy': 0.16920000314712524, 'test/loss': 5.528901100158691, 'test/num_examples': 10000, 'score': 28119.510808944702, 'total_duration': 30344.65024638176, 'accumulated_submission_time': 28119.510808944702, 'accumulated_eval_time': 2212.408802509308, 'accumulated_logging_time': 6.499581336975098}
I0307 18:03:51.899970 140257634797312 logging_writer.py:48] [60468] accumulated_eval_time=2212.41, accumulated_logging_time=6.49958, accumulated_submission_time=28119.5, global_step=60468, preemption_count=0, score=28119.5, test/accuracy=0.1692, test/loss=5.5289, test/num_examples=10000, total_duration=30344.7, train/accuracy=0.244878, train/loss=4.5415, validation/accuracy=0.2255, validation/loss=4.77742, validation/num_examples=50000
I0307 18:04:20.727615 140257643190016 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.2093418836593628, loss=2.3017070293426514
I0307 18:06:46.306158 140257634797312 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.3331741094589233, loss=2.368584632873535
I0307 18:09:08.353963 140257643190016 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.2854987382888794, loss=2.234220504760742
I0307 18:11:30.760945 140257634797312 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.3782340288162231, loss=2.202843189239502
I0307 18:12:23.050968 140413841560768 spec.py:321] Evaluating on the training split.
I0307 18:12:33.937022 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 18:12:58.429447 140413841560768 spec.py:349] Evaluating on the test split.
I0307 18:13:00.175631 140413841560768 submission_runner.py:469] Time since start: 30893.01s, 	Step: 60838, 	{'train/accuracy': 0.30966994166374207, 'train/loss': 3.4514143466949463, 'validation/accuracy': 0.28968000411987305, 'validation/loss': 3.607893943786621, 'validation/num_examples': 50000, 'test/accuracy': 0.21710000932216644, 'test/loss': 4.354828834533691, 'test/num_examples': 10000, 'score': 28630.59779214859, 'total_duration': 30893.00526356697, 'accumulated_submission_time': 28630.59779214859, 'accumulated_eval_time': 2249.5333909988403, 'accumulated_logging_time': 6.602743864059448}
I0307 18:13:00.213476 140257643190016 logging_writer.py:48] [60838] accumulated_eval_time=2249.53, accumulated_logging_time=6.60274, accumulated_submission_time=28630.6, global_step=60838, preemption_count=0, score=28630.6, test/accuracy=0.2171, test/loss=4.35483, test/num_examples=10000, total_duration=30893, train/accuracy=0.30967, train/loss=3.45141, validation/accuracy=0.28968, validation/loss=3.60789, validation/num_examples=50000
I0307 18:14:16.604111 140257634797312 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.213036060333252, loss=2.189671277999878
I0307 18:20:29.220701 140257643190016 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2279690504074097, loss=2.1834912300109863
I0307 18:21:33.001280 140413841560768 spec.py:321] Evaluating on the training split.
I0307 18:21:43.729412 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 18:22:07.498010 140413841560768 spec.py:349] Evaluating on the test split.
I0307 18:22:09.290152 140413841560768 submission_runner.py:469] Time since start: 31442.12s, 	Step: 61016, 	{'train/accuracy': 0.11898118257522583, 'train/loss': 5.718156337738037, 'validation/accuracy': 0.1084199994802475, 'validation/loss': 5.873864650726318, 'validation/num_examples': 50000, 'test/accuracy': 0.08230000734329224, 'test/loss': 6.502752304077148, 'test/num_examples': 10000, 'score': 29143.32395339012, 'total_duration': 31442.119812488556, 'accumulated_submission_time': 29143.32395339012, 'accumulated_eval_time': 2285.82221865654, 'accumulated_logging_time': 6.683839321136475}
I0307 18:22:09.309060 140257634797312 logging_writer.py:48] [61016] accumulated_eval_time=2285.82, accumulated_logging_time=6.68384, accumulated_submission_time=29143.3, global_step=61016, preemption_count=0, score=29143.3, test/accuracy=0.0823, test/loss=6.50275, test/num_examples=10000, total_duration=31442.1, train/accuracy=0.118981, train/loss=5.71816, validation/accuracy=0.10842, validation/loss=5.87386, validation/num_examples=50000
I0307 18:27:55.774288 140257643190016 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2222723960876465, loss=2.2181756496429443
I0307 18:30:41.162245 140413841560768 spec.py:321] Evaluating on the training split.
I0307 18:30:51.968242 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 18:31:16.145909 140413841560768 spec.py:349] Evaluating on the test split.
I0307 18:31:17.952066 140413841560768 submission_runner.py:469] Time since start: 31990.78s, 	Step: 61140, 	{'train/accuracy': 0.32894212007522583, 'train/loss': 3.3118197917938232, 'validation/accuracy': 0.31200000643730164, 'validation/loss': 3.466230869293213, 'validation/num_examples': 50000, 'test/accuracy': 0.22960001230239868, 'test/loss': 4.346792221069336, 'test/num_examples': 10000, 'score': 29655.156173706055, 'total_duration': 31990.78170990944, 'accumulated_submission_time': 29655.156173706055, 'accumulated_eval_time': 2322.611976623535, 'accumulated_logging_time': 6.711446523666382}
I0307 18:31:17.971307 140257634797312 logging_writer.py:48] [61140] accumulated_eval_time=2322.61, accumulated_logging_time=6.71145, accumulated_submission_time=29655.2, global_step=61140, preemption_count=0, score=29655.2, test/accuracy=0.2296, test/loss=4.34679, test/num_examples=10000, total_duration=31990.8, train/accuracy=0.328942, train/loss=3.31182, validation/accuracy=0.312, validation/loss=3.46623, validation/num_examples=50000
I0307 18:35:19.897855 140257643190016 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.264906406402588, loss=2.1747701168060303
I0307 18:39:50.508748 140413841560768 spec.py:321] Evaluating on the training split.
I0307 18:40:01.241081 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 18:40:18.545570 140413841560768 spec.py:349] Evaluating on the test split.
I0307 18:40:20.343144 140413841560768 submission_runner.py:469] Time since start: 32533.17s, 	Step: 61264, 	{'train/accuracy': 0.2950015962123871, 'train/loss': 3.533273458480835, 'validation/accuracy': 0.2795400023460388, 'validation/loss': 3.654658079147339, 'validation/num_examples': 50000, 'test/accuracy': 0.203000009059906, 'test/loss': 4.428502559661865, 'test/num_examples': 10000, 'score': 30167.671818971634, 'total_duration': 32533.172789096832, 'accumulated_submission_time': 30167.671818971634, 'accumulated_eval_time': 2352.446312904358, 'accumulated_logging_time': 6.739759206771851}
I0307 18:40:20.362709 140257634797312 logging_writer.py:48] [61264] accumulated_eval_time=2352.45, accumulated_logging_time=6.73976, accumulated_submission_time=30167.7, global_step=61264, preemption_count=0, score=30167.7, test/accuracy=0.203, test/loss=4.4285, test/num_examples=10000, total_duration=32533.2, train/accuracy=0.295002, train/loss=3.53327, validation/accuracy=0.27954, validation/loss=3.65466, validation/num_examples=50000
2025-03-07 18:41:49.333668: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:42:41.093931 140257643190016 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.393164873123169, loss=2.3612375259399414
I0307 18:48:51.392484 140413841560768 spec.py:321] Evaluating on the training split.
I0307 18:49:02.283045 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 18:49:26.163322 140413841560768 spec.py:349] Evaluating on the test split.
I0307 18:49:28.066958 140413841560768 submission_runner.py:469] Time since start: 33080.90s, 	Step: 61388, 	{'train/accuracy': 0.22668208181858063, 'train/loss': 4.139625072479248, 'validation/accuracy': 0.2146800011396408, 'validation/loss': 4.234930038452148, 'validation/num_examples': 50000, 'test/accuracy': 0.1600000113248825, 'test/loss': 4.81683874130249, 'test/num_examples': 10000, 'score': 30678.638236761093, 'total_duration': 33080.8965845108, 'accumulated_submission_time': 30678.638236761093, 'accumulated_eval_time': 2389.1207072734833, 'accumulated_logging_time': 6.80924129486084}
I0307 18:49:28.090854 140257634797312 logging_writer.py:48] [61388] accumulated_eval_time=2389.12, accumulated_logging_time=6.80924, accumulated_submission_time=30678.6, global_step=61388, preemption_count=0, score=30678.6, test/accuracy=0.16, test/loss=4.81684, test/num_examples=10000, total_duration=33080.9, train/accuracy=0.226682, train/loss=4.13963, validation/accuracy=0.21468, validation/loss=4.23493, validation/num_examples=50000
I0307 18:50:08.187925 140257643190016 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.2508543729782104, loss=2.2948169708251953
I0307 18:57:15.412871 140257634797312 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.2075310945510864, loss=2.22404408454895
I0307 18:57:58.521671 140413841560768 spec.py:321] Evaluating on the training split.
I0307 18:58:09.351775 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 18:58:31.482443 140413841560768 spec.py:349] Evaluating on the test split.
I0307 18:58:33.255344 140413841560768 submission_runner.py:469] Time since start: 33626.09s, 	Step: 61511, 	{'train/accuracy': 0.25924745202064514, 'train/loss': 3.8297600746154785, 'validation/accuracy': 0.24361999332904816, 'validation/loss': 3.985414981842041, 'validation/num_examples': 50000, 'test/accuracy': 0.17750000953674316, 'test/loss': 4.7761054039001465, 'test/num_examples': 10000, 'score': 31189.044130563736, 'total_duration': 33626.085010290146, 'accumulated_submission_time': 31189.044130563736, 'accumulated_eval_time': 2423.8543360233307, 'accumulated_logging_time': 6.844536066055298}
I0307 18:58:33.286595 140257643190016 logging_writer.py:48] [61511] accumulated_eval_time=2423.85, accumulated_logging_time=6.84454, accumulated_submission_time=31189, global_step=61511, preemption_count=0, score=31189, test/accuracy=0.1775, test/loss=4.77611, test/num_examples=10000, total_duration=33626.1, train/accuracy=0.259247, train/loss=3.82976, validation/accuracy=0.24362, validation/loss=3.98541, validation/num_examples=50000
I0307 19:04:36.180181 140257634797312 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.3345526456832886, loss=2.2360949516296387
I0307 19:07:07.302220 140413841560768 spec.py:321] Evaluating on the training split.
I0307 19:07:17.964648 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 19:07:47.248226 140413841560768 spec.py:349] Evaluating on the test split.
I0307 19:07:49.037267 140413841560768 submission_runner.py:469] Time since start: 34181.87s, 	Step: 61637, 	{'train/accuracy': 0.22939252853393555, 'train/loss': 4.198792934417725, 'validation/accuracy': 0.21447999775409698, 'validation/loss': 4.342655658721924, 'validation/num_examples': 50000, 'test/accuracy': 0.16290000081062317, 'test/loss': 4.998435974121094, 'test/num_examples': 10000, 'score': 31703.03137922287, 'total_duration': 34181.86692881584, 'accumulated_submission_time': 31703.03137922287, 'accumulated_eval_time': 2465.5893456935883, 'accumulated_logging_time': 6.891858339309692}
I0307 19:07:49.055935 140257643190016 logging_writer.py:48] [61637] accumulated_eval_time=2465.59, accumulated_logging_time=6.89186, accumulated_submission_time=31703, global_step=61637, preemption_count=0, score=31703, test/accuracy=0.1629, test/loss=4.99844, test/num_examples=10000, total_duration=34181.9, train/accuracy=0.229393, train/loss=4.19879, validation/accuracy=0.21448, validation/loss=4.34266, validation/num_examples=50000
I0307 19:12:02.879396 140257634797312 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.2208865880966187, loss=2.1015284061431885
I0307 19:16:20.192801 140413841560768 spec.py:321] Evaluating on the training split.
I0307 19:16:30.894211 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 19:16:54.276960 140413841560768 spec.py:349] Evaluating on the test split.
I0307 19:16:56.011792 140413841560768 submission_runner.py:469] Time since start: 34728.84s, 	Step: 61762, 	{'train/accuracy': 0.23648755252361298, 'train/loss': 4.070553779602051, 'validation/accuracy': 0.23157998919487, 'validation/loss': 4.162125587463379, 'validation/num_examples': 50000, 'test/accuracy': 0.17310000956058502, 'test/loss': 4.836390495300293, 'test/num_examples': 10000, 'score': 32214.105890989304, 'total_duration': 34728.841467380524, 'accumulated_submission_time': 32214.105890989304, 'accumulated_eval_time': 2501.408307790756, 'accumulated_logging_time': 6.9596168994903564}
I0307 19:16:56.027863 140257643190016 logging_writer.py:48] [61762] accumulated_eval_time=2501.41, accumulated_logging_time=6.95962, accumulated_submission_time=32214.1, global_step=61762, preemption_count=0, score=32214.1, test/accuracy=0.1731, test/loss=4.83639, test/num_examples=10000, total_duration=34728.8, train/accuracy=0.236488, train/loss=4.07055, validation/accuracy=0.23158, validation/loss=4.16213, validation/num_examples=50000
I0307 19:19:25.261425 140257634797312 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.3145705461502075, loss=2.275740385055542
I0307 19:25:29.883426 140413841560768 spec.py:321] Evaluating on the training split.
I0307 19:25:40.742153 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 19:26:01.278048 140413841560768 spec.py:349] Evaluating on the test split.
I0307 19:26:03.073702 140413841560768 submission_runner.py:469] Time since start: 35275.90s, 	Step: 61887, 	{'train/accuracy': 0.21016022562980652, 'train/loss': 4.872672080993652, 'validation/accuracy': 0.19581998884677887, 'validation/loss': 4.975985050201416, 'validation/num_examples': 50000, 'test/accuracy': 0.14000000059604645, 'test/loss': 5.754014492034912, 'test/num_examples': 10000, 'score': 32727.940685272217, 'total_duration': 35275.90333819389, 'accumulated_submission_time': 32727.940685272217, 'accumulated_eval_time': 2534.5985140800476, 'accumulated_logging_time': 6.983231067657471}
I0307 19:26:03.092771 140257643190016 logging_writer.py:48] [61887] accumulated_eval_time=2534.6, accumulated_logging_time=6.98323, accumulated_submission_time=32727.9, global_step=61887, preemption_count=0, score=32727.9, test/accuracy=0.14, test/loss=5.75401, test/num_examples=10000, total_duration=35275.9, train/accuracy=0.21016, train/loss=4.87267, validation/accuracy=0.19582, validation/loss=4.97599, validation/num_examples=50000
I0307 19:26:45.947945 140257634797312 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.427557110786438, loss=2.2504658699035645
I0307 19:33:49.461190 140257643190016 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.4199341535568237, loss=2.2282564640045166
I0307 19:34:36.273642 140413841560768 spec.py:321] Evaluating on the training split.
I0307 19:34:47.288151 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 19:35:09.992783 140413841560768 spec.py:349] Evaluating on the test split.
I0307 19:35:11.731643 140413841560768 submission_runner.py:469] Time since start: 35824.56s, 	Step: 62012, 	{'train/accuracy': 0.17582111060619354, 'train/loss': 5.048666954040527, 'validation/accuracy': 0.15717999637126923, 'validation/loss': 5.288033485412598, 'validation/num_examples': 50000, 'test/accuracy': 0.11220000684261322, 'test/loss': 6.034156322479248, 'test/num_examples': 10000, 'score': 33241.09929347038, 'total_duration': 35824.561291217804, 'accumulated_submission_time': 33241.09929347038, 'accumulated_eval_time': 2570.0564539432526, 'accumulated_logging_time': 7.010974645614624}
I0307 19:35:11.748136 140257634797312 logging_writer.py:48] [62012] accumulated_eval_time=2570.06, accumulated_logging_time=7.01097, accumulated_submission_time=33241.1, global_step=62012, preemption_count=0, score=33241.1, test/accuracy=0.1122, test/loss=6.03416, test/num_examples=10000, total_duration=35824.6, train/accuracy=0.175821, train/loss=5.04867, validation/accuracy=0.15718, validation/loss=5.28803, validation/num_examples=50000
I0307 19:41:08.036118 140257643190016 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.352504014968872, loss=2.107463836669922
I0307 19:43:43.402803 140413841560768 spec.py:321] Evaluating on the training split.
I0307 19:43:53.973292 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 19:44:18.534683 140413841560768 spec.py:349] Evaluating on the test split.
I0307 19:44:20.333860 140413841560768 submission_runner.py:469] Time since start: 36373.16s, 	Step: 62138, 	{'train/accuracy': 0.20623405277729034, 'train/loss': 4.628417015075684, 'validation/accuracy': 0.18199999630451202, 'validation/loss': 4.891508102416992, 'validation/num_examples': 50000, 'test/accuracy': 0.13990001380443573, 'test/loss': 5.502305507659912, 'test/num_examples': 10000, 'score': 33752.73250865936, 'total_duration': 36373.163524866104, 'accumulated_submission_time': 33752.73250865936, 'accumulated_eval_time': 2606.987471103668, 'accumulated_logging_time': 7.036464691162109}
I0307 19:44:20.353630 140257634797312 logging_writer.py:48] [62138] accumulated_eval_time=2606.99, accumulated_logging_time=7.03646, accumulated_submission_time=33752.7, global_step=62138, preemption_count=0, score=33752.7, test/accuracy=0.1399, test/loss=5.50231, test/num_examples=10000, total_duration=36373.2, train/accuracy=0.206234, train/loss=4.62842, validation/accuracy=0.182, validation/loss=4.89151, validation/num_examples=50000
I0307 19:46:00.878872 140257643190016 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.5602329969406128, loss=2.277348041534424
I0307 19:48:26.579130 140257634797312 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.3236433267593384, loss=2.1892268657684326
I0307 19:50:52.151330 140257643190016 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.2788552045822144, loss=2.2777304649353027
I0307 19:52:51.136787 140413841560768 spec.py:321] Evaluating on the training split.
I0307 19:53:02.520915 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 19:53:22.725391 140413841560768 spec.py:349] Evaluating on the test split.
I0307 19:53:24.498187 140413841560768 submission_runner.py:469] Time since start: 36917.33s, 	Step: 62482, 	{'train/accuracy': 0.30592313408851624, 'train/loss': 3.4500646591186523, 'validation/accuracy': 0.2951599955558777, 'validation/loss': 3.5575101375579834, 'validation/num_examples': 50000, 'test/accuracy': 0.2087000161409378, 'test/loss': 4.413336753845215, 'test/num_examples': 10000, 'score': 34263.46985626221, 'total_duration': 36917.32784366608, 'accumulated_submission_time': 34263.46985626221, 'accumulated_eval_time': 2640.348819732666, 'accumulated_logging_time': 7.0649003982543945}
I0307 19:53:24.572613 140257634797312 logging_writer.py:48] [62482] accumulated_eval_time=2640.35, accumulated_logging_time=7.0649, accumulated_submission_time=34263.5, global_step=62482, preemption_count=0, score=34263.5, test/accuracy=0.2087, test/loss=4.41334, test/num_examples=10000, total_duration=36917.3, train/accuracy=0.305923, train/loss=3.45006, validation/accuracy=0.29516, validation/loss=3.55751, validation/num_examples=50000
I0307 19:53:36.388871 140257643190016 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.2472004890441895, loss=2.0983972549438477
I0307 19:56:02.473915 140257634797312 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.2552490234375, loss=2.2268075942993164
I0307 19:58:23.963355 140257643190016 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.4231432676315308, loss=2.229703426361084
I0307 20:00:46.086488 140257634797312 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3303598165512085, loss=2.027864694595337
I0307 20:01:55.637781 140413841560768 spec.py:321] Evaluating on the training split.
I0307 20:02:06.713387 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 20:02:25.971436 140413841560768 spec.py:349] Evaluating on the test split.
I0307 20:02:27.726599 140413841560768 submission_runner.py:469] Time since start: 37460.56s, 	Step: 62850, 	{'train/accuracy': 0.34482619166374207, 'train/loss': 3.135535955429077, 'validation/accuracy': 0.32453998923301697, 'validation/loss': 3.2905848026275635, 'validation/num_examples': 50000, 'test/accuracy': 0.2410000115633011, 'test/loss': 4.088424205780029, 'test/num_examples': 10000, 'score': 34774.485895872116, 'total_duration': 37460.55626320839, 'accumulated_submission_time': 34774.485895872116, 'accumulated_eval_time': 2672.4375896453857, 'accumulated_logging_time': 7.147935152053833}
I0307 20:02:40.820850 140257643190016 logging_writer.py:48] [62850] accumulated_eval_time=2672.44, accumulated_logging_time=7.14794, accumulated_submission_time=34774.5, global_step=62850, preemption_count=0, score=34774.5, test/accuracy=0.241, test/loss=4.08842, test/num_examples=10000, total_duration=37460.6, train/accuracy=0.344826, train/loss=3.13554, validation/accuracy=0.32454, validation/loss=3.29058, validation/num_examples=50000
I0307 20:03:39.456069 140257634797312 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.1751704216003418, loss=2.008455276489258
I0307 20:06:02.965670 140257643190016 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.2196390628814697, loss=2.2027080059051514
I0307 20:08:25.499034 140257634797312 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.2980403900146484, loss=2.2053325176239014
I0307 20:10:48.001010 140257643190016 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.4469119310379028, loss=2.1253886222839355
I0307 20:10:57.906995 140413841560768 spec.py:321] Evaluating on the training split.
I0307 20:11:09.206530 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 20:11:27.034997 140413841560768 spec.py:349] Evaluating on the test split.
I0307 20:11:28.804205 140413841560768 submission_runner.py:469] Time since start: 38001.63s, 	Step: 63208, 	{'train/accuracy': 0.36485570669174194, 'train/loss': 3.080524206161499, 'validation/accuracy': 0.3482399880886078, 'validation/loss': 3.221979856491089, 'validation/num_examples': 50000, 'test/accuracy': 0.26440000534057617, 'test/loss': 4.006695747375488, 'test/num_examples': 10000, 'score': 35271.52393269539, 'total_duration': 38001.63388586044, 'accumulated_submission_time': 35271.52393269539, 'accumulated_eval_time': 2703.3347663879395, 'accumulated_logging_time': 20.2498722076416}
I0307 20:11:28.853012 140257634797312 logging_writer.py:48] [63208] accumulated_eval_time=2703.33, accumulated_logging_time=20.2499, accumulated_submission_time=35271.5, global_step=63208, preemption_count=0, score=35271.5, test/accuracy=0.2644, test/loss=4.0067, test/num_examples=10000, total_duration=38001.6, train/accuracy=0.364856, train/loss=3.08052, validation/accuracy=0.34824, validation/loss=3.22198, validation/num_examples=50000
I0307 20:13:26.007822 140257643190016 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.1910960674285889, loss=2.1880252361297607
I0307 20:16:00.907691 140257634797312 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.2636910676956177, loss=2.2104060649871826
I0307 20:20:01.518533 140413841560768 spec.py:321] Evaluating on the training split.
I0307 20:20:12.399235 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 20:20:38.297622 140413841560768 spec.py:349] Evaluating on the test split.
I0307 20:20:40.037777 140413841560768 submission_runner.py:469] Time since start: 38552.87s, 	Step: 63458, 	{'train/accuracy': 0.35146284103393555, 'train/loss': 3.148709297180176, 'validation/accuracy': 0.32161998748779297, 'validation/loss': 3.3509581089019775, 'validation/num_examples': 50000, 'test/accuracy': 0.2451000064611435, 'test/loss': 4.087538719177246, 'test/num_examples': 10000, 'score': 35784.15509390831, 'total_duration': 38552.86743712425, 'accumulated_submission_time': 35784.15509390831, 'accumulated_eval_time': 2741.8539640903473, 'accumulated_logging_time': 20.306491374969482}
I0307 20:20:40.056200 140257643190016 logging_writer.py:48] [63458] accumulated_eval_time=2741.85, accumulated_logging_time=20.3065, accumulated_submission_time=35784.2, global_step=63458, preemption_count=0, score=35784.2, test/accuracy=0.2451, test/loss=4.08754, test/num_examples=10000, total_duration=38552.9, train/accuracy=0.351463, train/loss=3.14871, validation/accuracy=0.32162, validation/loss=3.35096, validation/num_examples=50000
I0307 20:23:24.406326 140257634797312 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.302488088607788, loss=2.1807892322540283
I0307 20:29:14.012246 140413841560768 spec.py:321] Evaluating on the training split.
I0307 20:29:24.774123 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 20:29:52.482637 140413841560768 spec.py:349] Evaluating on the test split.
I0307 20:29:54.393338 140413841560768 submission_runner.py:469] Time since start: 39107.22s, 	Step: 63583, 	{'train/accuracy': 0.3989357352256775, 'train/loss': 2.81195068359375, 'validation/accuracy': 0.342739999294281, 'validation/loss': 3.1508748531341553, 'validation/num_examples': 50000, 'test/accuracy': 0.2596000134944916, 'test/loss': 3.8776979446411133, 'test/num_examples': 10000, 'score': 36298.05703806877, 'total_duration': 39107.223012924194, 'accumulated_submission_time': 36298.05703806877, 'accumulated_eval_time': 2782.235025167465, 'accumulated_logging_time': 20.365324020385742}
I0307 20:29:54.439071 140257643190016 logging_writer.py:48] [63583] accumulated_eval_time=2782.24, accumulated_logging_time=20.3653, accumulated_submission_time=36298.1, global_step=63583, preemption_count=0, score=36298.1, test/accuracy=0.2596, test/loss=3.8777, test/num_examples=10000, total_duration=39107.2, train/accuracy=0.398936, train/loss=2.81195, validation/accuracy=0.34274, validation/loss=3.15087, validation/num_examples=50000
I0307 20:30:55.316955 140257634797312 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3100757598876953, loss=2.192060947418213
I0307 20:37:59.282576 140257643190016 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.3093883991241455, loss=2.238309860229492
I0307 20:38:24.574680 140413841560768 spec.py:321] Evaluating on the training split.
I0307 20:38:35.367382 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 20:39:00.888991 140413841560768 spec.py:349] Evaluating on the test split.
I0307 20:39:02.669228 140413841560768 submission_runner.py:469] Time since start: 39655.50s, 	Step: 63707, 	{'train/accuracy': 0.32649075984954834, 'train/loss': 3.299198627471924, 'validation/accuracy': 0.2880599796772003, 'validation/loss': 3.576810598373413, 'validation/num_examples': 50000, 'test/accuracy': 0.2143000066280365, 'test/loss': 4.212899208068848, 'test/num_examples': 10000, 'score': 36808.17079281807, 'total_duration': 39655.49816226959, 'accumulated_submission_time': 36808.17079281807, 'accumulated_eval_time': 2820.3287954330444, 'accumulated_logging_time': 20.419416904449463}
I0307 20:39:02.687830 140257634797312 logging_writer.py:48] [63707] accumulated_eval_time=2820.33, accumulated_logging_time=20.4194, accumulated_submission_time=36808.2, global_step=63707, preemption_count=0, score=36808.2, test/accuracy=0.2143, test/loss=4.2129, test/num_examples=10000, total_duration=39655.5, train/accuracy=0.326491, train/loss=3.2992, validation/accuracy=0.28806, validation/loss=3.57681, validation/num_examples=50000
I0307 20:45:29.068400 140257643190016 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.3300684690475464, loss=2.2459633350372314
I0307 20:47:33.158001 140413841560768 spec.py:321] Evaluating on the training split.
I0307 20:47:42.729204 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 20:48:10.310521 140413841560768 spec.py:349] Evaluating on the test split.
I0307 20:48:12.075282 140413841560768 submission_runner.py:469] Time since start: 40204.90s, 	Step: 63830, 	{'train/accuracy': 0.11778539419174194, 'train/loss': 5.5755085945129395, 'validation/accuracy': 0.10815999656915665, 'validation/loss': 5.711451053619385, 'validation/num_examples': 50000, 'test/accuracy': 0.07670000195503235, 'test/loss': 6.375582218170166, 'test/num_examples': 10000, 'score': 37318.61796140671, 'total_duration': 40204.90491318703, 'accumulated_submission_time': 37318.61796140671, 'accumulated_eval_time': 2859.24599814415, 'accumulated_logging_time': 20.447150468826294}
I0307 20:48:12.095455 140257634797312 logging_writer.py:48] [63830] accumulated_eval_time=2859.25, accumulated_logging_time=20.4472, accumulated_submission_time=37318.6, global_step=63830, preemption_count=0, score=37318.6, test/accuracy=0.0767, test/loss=6.37558, test/num_examples=10000, total_duration=40204.9, train/accuracy=0.117785, train/loss=5.57551, validation/accuracy=0.10816, validation/loss=5.71145, validation/num_examples=50000
I0307 20:53:03.897214 140257643190016 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3359440565109253, loss=2.1996583938598633
I0307 20:56:42.288704 140413841560768 spec.py:321] Evaluating on the training split.
I0307 20:56:53.093071 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 20:57:15.453349 140413841560768 spec.py:349] Evaluating on the test split.
I0307 20:57:17.201412 140413841560768 submission_runner.py:469] Time since start: 40750.03s, 	Step: 63952, 	{'train/accuracy': 0.2541254758834839, 'train/loss': 3.9942924976348877, 'validation/accuracy': 0.23715999722480774, 'validation/loss': 4.153016090393066, 'validation/num_examples': 50000, 'test/accuracy': 0.16910000145435333, 'test/loss': 4.971282958984375, 'test/num_examples': 10000, 'score': 37828.72723984718, 'total_duration': 40750.031073093414, 'accumulated_submission_time': 37828.72723984718, 'accumulated_eval_time': 2894.1586589813232, 'accumulated_logging_time': 20.538596630096436}
I0307 20:57:17.219145 140257634797312 logging_writer.py:48] [63952] accumulated_eval_time=2894.16, accumulated_logging_time=20.5386, accumulated_submission_time=37828.7, global_step=63952, preemption_count=0, score=37828.7, test/accuracy=0.1691, test/loss=4.97128, test/num_examples=10000, total_duration=40750, train/accuracy=0.254125, train/loss=3.99429, validation/accuracy=0.23716, validation/loss=4.15302, validation/num_examples=50000
I0307 21:00:25.847963 140257643190016 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.2474976778030396, loss=2.162660837173462
I0307 21:05:51.182155 140413841560768 spec.py:321] Evaluating on the training split.
I0307 21:06:02.071562 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 21:06:23.216699 140413841560768 spec.py:349] Evaluating on the test split.
I0307 21:06:25.016962 140413841560768 submission_runner.py:469] Time since start: 41297.85s, 	Step: 64078, 	{'train/accuracy': 0.4133649468421936, 'train/loss': 2.6875948905944824, 'validation/accuracy': 0.37790000438690186, 'validation/loss': 2.9126312732696533, 'validation/num_examples': 50000, 'test/accuracy': 0.2930000126361847, 'test/loss': 3.635350227355957, 'test/num_examples': 10000, 'score': 38342.6691300869, 'total_duration': 41297.84662628174, 'accumulated_submission_time': 38342.6691300869, 'accumulated_eval_time': 2927.993425130844, 'accumulated_logging_time': 20.563955545425415}
I0307 21:06:25.036894 140257634797312 logging_writer.py:48] [64078] accumulated_eval_time=2927.99, accumulated_logging_time=20.564, accumulated_submission_time=38342.7, global_step=64078, preemption_count=0, score=38342.7, test/accuracy=0.293, test/loss=3.63535, test/num_examples=10000, total_duration=41297.8, train/accuracy=0.413365, train/loss=2.68759, validation/accuracy=0.3779, validation/loss=2.91263, validation/num_examples=50000
I0307 21:07:45.464214 140257643190016 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.349301815032959, loss=2.2496798038482666
I0307 21:14:46.149462 140257634797312 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.2778840065002441, loss=2.3453993797302246
I0307 21:14:58.765349 140413841560768 spec.py:321] Evaluating on the training split.
I0307 21:15:09.796650 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 21:15:32.140345 140413841560768 spec.py:349] Evaluating on the test split.
I0307 21:15:33.929946 140413841560768 submission_runner.py:469] Time since start: 41846.76s, 	Step: 64204, 	{'train/accuracy': 0.2576729953289032, 'train/loss': 3.983649253845215, 'validation/accuracy': 0.2425599992275238, 'validation/loss': 4.106703758239746, 'validation/num_examples': 50000, 'test/accuracy': 0.1714000105857849, 'test/loss': 4.9204936027526855, 'test/num_examples': 10000, 'score': 38856.37526202202, 'total_duration': 41846.759601831436, 'accumulated_submission_time': 38856.37526202202, 'accumulated_eval_time': 2963.1579632759094, 'accumulated_logging_time': 20.592565536499023}
I0307 21:15:33.950198 140257643190016 logging_writer.py:48] [64204] accumulated_eval_time=2963.16, accumulated_logging_time=20.5926, accumulated_submission_time=38856.4, global_step=64204, preemption_count=0, score=38856.4, test/accuracy=0.1714, test/loss=4.92049, test/num_examples=10000, total_duration=41846.8, train/accuracy=0.257673, train/loss=3.98365, validation/accuracy=0.24256, validation/loss=4.1067, validation/num_examples=50000
I0307 21:22:12.112197 140257634797312 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.2983479499816895, loss=2.196808099746704
I0307 21:24:06.159049 140413841560768 spec.py:321] Evaluating on the training split.
I0307 21:24:17.161602 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 21:24:40.668889 140413841560768 spec.py:349] Evaluating on the test split.
I0307 21:24:42.414293 140413841560768 submission_runner.py:469] Time since start: 42395.24s, 	Step: 64328, 	{'train/accuracy': 0.23176418244838715, 'train/loss': 4.285702228546143, 'validation/accuracy': 0.21781998872756958, 'validation/loss': 4.485436916351318, 'validation/num_examples': 50000, 'test/accuracy': 0.1728000044822693, 'test/loss': 5.184473991394043, 'test/num_examples': 10000, 'score': 39368.5624165535, 'total_duration': 42395.243967056274, 'accumulated_submission_time': 39368.5624165535, 'accumulated_eval_time': 2999.4131710529327, 'accumulated_logging_time': 20.621379613876343}
I0307 21:24:42.430654 140257643190016 logging_writer.py:48] [64328] accumulated_eval_time=2999.41, accumulated_logging_time=20.6214, accumulated_submission_time=39368.6, global_step=64328, preemption_count=0, score=39368.6, test/accuracy=0.1728, test/loss=5.18447, test/num_examples=10000, total_duration=42395.2, train/accuracy=0.231764, train/loss=4.2857, validation/accuracy=0.21782, validation/loss=4.48544, validation/num_examples=50000
I0307 21:29:30.690446 140257634797312 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.5315577983856201, loss=2.2866358757019043
I0307 21:33:16.009743 140413841560768 spec.py:321] Evaluating on the training split.
I0307 21:33:26.872554 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 21:33:46.946831 140413841560768 spec.py:349] Evaluating on the test split.
I0307 21:33:48.764712 140413841560768 submission_runner.py:469] Time since start: 42941.59s, 	Step: 64456, 	{'train/accuracy': 0.34448739886283875, 'train/loss': 3.149181365966797, 'validation/accuracy': 0.32249999046325684, 'validation/loss': 3.318387985229492, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 4.10207462310791, 'test/num_examples': 10000, 'score': 39880.809448719025, 'total_duration': 42941.59438085556, 'accumulated_submission_time': 39880.809448719025, 'accumulated_eval_time': 3032.1681022644043, 'accumulated_logging_time': 21.95696473121643}
I0307 21:33:48.782793 140257643190016 logging_writer.py:48] [64456] accumulated_eval_time=3032.17, accumulated_logging_time=21.957, accumulated_submission_time=39880.8, global_step=64456, preemption_count=0, score=39880.8, test/accuracy=0.2416, test/loss=4.10207, test/num_examples=10000, total_duration=42941.6, train/accuracy=0.344487, train/loss=3.14918, validation/accuracy=0.3225, validation/loss=3.31839, validation/num_examples=50000
I0307 21:36:39.263246 140257634797312 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3117296695709229, loss=2.146243095397949
I0307 21:42:21.378046 140413841560768 spec.py:321] Evaluating on the training split.
I0307 21:42:32.158669 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 21:42:50.848234 140413841560768 spec.py:349] Evaluating on the test split.
I0307 21:42:52.740419 140413841560768 submission_runner.py:469] Time since start: 43485.57s, 	Step: 64583, 	{'train/accuracy': 0.31022799015045166, 'train/loss': 3.421186685562134, 'validation/accuracy': 0.28963997960090637, 'validation/loss': 3.599924087524414, 'validation/num_examples': 50000, 'test/accuracy': 0.21710000932216644, 'test/loss': 4.373508453369141, 'test/num_examples': 10000, 'score': 40393.38170218468, 'total_duration': 43485.57009410858, 'accumulated_submission_time': 40393.38170218468, 'accumulated_eval_time': 3063.5304415225983, 'accumulated_logging_time': 21.982789754867554}
I0307 21:42:52.774810 140257643190016 logging_writer.py:48] [64583] accumulated_eval_time=3063.53, accumulated_logging_time=21.9828, accumulated_submission_time=40393.4, global_step=64583, preemption_count=0, score=40393.4, test/accuracy=0.2171, test/loss=4.37351, test/num_examples=10000, total_duration=43485.6, train/accuracy=0.310228, train/loss=3.42119, validation/accuracy=0.28964, validation/loss=3.59992, validation/num_examples=50000
I0307 21:43:50.423655 140257634797312 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.2793376445770264, loss=2.180600166320801
I0307 21:48:29.373954 140257643190016 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.4088011980056763, loss=2.1538310050964355
I0307 21:50:17.821277 140257634797312 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.3467689752578735, loss=2.136657953262329
I0307 21:51:23.011461 140413841560768 spec.py:321] Evaluating on the training split.
I0307 21:51:34.071183 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 21:51:52.269655 140413841560768 spec.py:349] Evaluating on the test split.
I0307 21:51:54.037747 140413841560768 submission_runner.py:469] Time since start: 44026.87s, 	Step: 64861, 	{'train/accuracy': 0.35724249482154846, 'train/loss': 3.256249189376831, 'validation/accuracy': 0.3403399884700775, 'validation/loss': 3.3704471588134766, 'validation/num_examples': 50000, 'test/accuracy': 0.25130000710487366, 'test/loss': 4.261990547180176, 'test/num_examples': 10000, 'score': 40903.57956242561, 'total_duration': 44026.867421627045, 'accumulated_submission_time': 40903.57956242561, 'accumulated_eval_time': 3094.556693792343, 'accumulated_logging_time': 22.025065660476685}
I0307 21:51:54.054749 140257643190016 logging_writer.py:48] [64861] accumulated_eval_time=3094.56, accumulated_logging_time=22.0251, accumulated_submission_time=40903.6, global_step=64861, preemption_count=0, score=40903.6, test/accuracy=0.2513, test/loss=4.26199, test/num_examples=10000, total_duration=44026.9, train/accuracy=0.357242, train/loss=3.25625, validation/accuracy=0.34034, validation/loss=3.37045, validation/num_examples=50000
I0307 21:52:23.040723 140257634797312 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.4075751304626465, loss=2.1111016273498535
I0307 21:54:14.452476 140257643190016 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.303550124168396, loss=2.2937123775482178
2025-03-07 21:55:01.861851: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:56:06.452511 140257634797312 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.3082549571990967, loss=2.235513687133789
I0307 21:57:54.293996 140257643190016 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.3049557209014893, loss=2.173895835876465
I0307 21:59:41.919304 140257634797312 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.2917019128799438, loss=2.1735873222351074
I0307 22:00:24.745416 140413841560768 spec.py:321] Evaluating on the training split.
I0307 22:00:35.752468 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 22:00:55.616703 140413841560768 spec.py:349] Evaluating on the test split.
I0307 22:00:57.364114 140413841560768 submission_runner.py:469] Time since start: 44570.19s, 	Step: 65341, 	{'train/accuracy': 0.3435506820678711, 'train/loss': 3.2389893531799316, 'validation/accuracy': 0.3141999840736389, 'validation/loss': 3.526843309402466, 'validation/num_examples': 50000, 'test/accuracy': 0.22300000488758087, 'test/loss': 4.413634777069092, 'test/num_examples': 10000, 'score': 41414.20679354668, 'total_duration': 44570.19377946854, 'accumulated_submission_time': 41414.20679354668, 'accumulated_eval_time': 3127.1753418445587, 'accumulated_logging_time': 22.049418926239014}
I0307 22:00:57.405447 140257643190016 logging_writer.py:48] [65341] accumulated_eval_time=3127.18, accumulated_logging_time=22.0494, accumulated_submission_time=41414.2, global_step=65341, preemption_count=0, score=41414.2, test/accuracy=0.223, test/loss=4.41363, test/num_examples=10000, total_duration=44570.2, train/accuracy=0.343551, train/loss=3.23899, validation/accuracy=0.3142, validation/loss=3.52684, validation/num_examples=50000
I0307 22:01:49.360108 140257634797312 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.2916737794876099, loss=2.1460180282592773
I0307 22:03:35.638890 140257643190016 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.2084933519363403, loss=2.2589175701141357
I0307 22:05:22.392207 140257634797312 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.3575626611709595, loss=2.1580381393432617
I0307 22:07:09.888339 140257643190016 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.4001522064208984, loss=2.156033992767334
I0307 22:08:57.542049 140257634797312 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.3611071109771729, loss=2.058891773223877
I0307 22:09:27.831966 140413841560768 spec.py:321] Evaluating on the training split.
I0307 22:09:39.127831 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 22:09:58.795317 140413841560768 spec.py:349] Evaluating on the test split.
I0307 22:10:00.597853 140413841560768 submission_runner.py:469] Time since start: 45113.43s, 	Step: 65829, 	{'train/accuracy': 0.34197622537612915, 'train/loss': 3.1787047386169434, 'validation/accuracy': 0.32311999797821045, 'validation/loss': 3.3160715103149414, 'validation/num_examples': 50000, 'test/accuracy': 0.2427000105381012, 'test/loss': 4.056772708892822, 'test/num_examples': 10000, 'score': 41924.55049228668, 'total_duration': 45113.42751765251, 'accumulated_submission_time': 41924.55049228668, 'accumulated_eval_time': 3159.9411783218384, 'accumulated_logging_time': 22.116044521331787}
I0307 22:10:00.626816 140257643190016 logging_writer.py:48] [65829] accumulated_eval_time=3159.94, accumulated_logging_time=22.116, accumulated_submission_time=41924.6, global_step=65829, preemption_count=0, score=41924.6, test/accuracy=0.2427, test/loss=4.05677, test/num_examples=10000, total_duration=45113.4, train/accuracy=0.341976, train/loss=3.1787, validation/accuracy=0.32312, validation/loss=3.31607, validation/num_examples=50000
I0307 22:11:07.047920 140257634797312 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.4587080478668213, loss=2.2417173385620117
I0307 22:12:53.897436 140257643190016 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.4004549980163574, loss=2.193085193634033
I0307 22:14:41.600594 140257634797312 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.4411373138427734, loss=2.347717761993408
I0307 22:16:29.345400 140257643190016 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.2327628135681152, loss=2.045758008956909
2025-03-07 22:18:10.167890: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:18:18.715967 140257634797312 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3478089570999146, loss=2.099266529083252
I0307 22:18:30.640006 140413841560768 spec.py:321] Evaluating on the training split.
I0307 22:18:41.943249 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 22:19:02.323741 140413841560768 spec.py:349] Evaluating on the test split.
I0307 22:19:04.044790 140413841560768 submission_runner.py:469] Time since start: 45656.87s, 	Step: 66312, 	{'train/accuracy': 0.3198740482330322, 'train/loss': 3.409698486328125, 'validation/accuracy': 0.30747997760772705, 'validation/loss': 3.55702543258667, 'validation/num_examples': 50000, 'test/accuracy': 0.21800000965595245, 'test/loss': 4.414551258087158, 'test/num_examples': 10000, 'score': 42434.50054335594, 'total_duration': 45656.874467372894, 'accumulated_submission_time': 42434.50054335594, 'accumulated_eval_time': 3193.345922231674, 'accumulated_logging_time': 22.153114080429077}
I0307 22:19:04.082101 140257643190016 logging_writer.py:48] [66312] accumulated_eval_time=3193.35, accumulated_logging_time=22.1531, accumulated_submission_time=42434.5, global_step=66312, preemption_count=0, score=42434.5, test/accuracy=0.218, test/loss=4.41455, test/num_examples=10000, total_duration=45656.9, train/accuracy=0.319874, train/loss=3.4097, validation/accuracy=0.30748, validation/loss=3.55703, validation/num_examples=50000
I0307 22:20:28.143166 140257634797312 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.1843725442886353, loss=2.1705610752105713
I0307 22:22:14.666899 140257643190016 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.377125859260559, loss=2.259826898574829
I0307 22:24:02.180956 140257634797312 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.4022215604782104, loss=2.182006597518921
I0307 22:25:49.773540 140257643190016 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.423518419265747, loss=2.2160582542419434
I0307 22:27:34.068640 140413841560768 spec.py:321] Evaluating on the training split.
I0307 22:27:45.179646 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 22:28:06.627448 140413841560768 spec.py:349] Evaluating on the test split.
I0307 22:28:08.548319 140413841560768 submission_runner.py:469] Time since start: 46201.38s, 	Step: 66798, 	{'train/accuracy': 0.34036192297935486, 'train/loss': 3.174377679824829, 'validation/accuracy': 0.31167998909950256, 'validation/loss': 3.406909704208374, 'validation/num_examples': 50000, 'test/accuracy': 0.23250001668930054, 'test/loss': 4.173073768615723, 'test/num_examples': 10000, 'score': 42944.424287080765, 'total_duration': 46201.377992391586, 'accumulated_submission_time': 42944.424287080765, 'accumulated_eval_time': 3227.8255751132965, 'accumulated_logging_time': 22.19819140434265}
I0307 22:28:08.625278 140257634797312 logging_writer.py:48] [66798] accumulated_eval_time=3227.83, accumulated_logging_time=22.1982, accumulated_submission_time=42944.4, global_step=66798, preemption_count=0, score=42944.4, test/accuracy=0.2325, test/loss=4.17307, test/num_examples=10000, total_duration=46201.4, train/accuracy=0.340362, train/loss=3.17438, validation/accuracy=0.31168, validation/loss=3.40691, validation/num_examples=50000
I0307 22:28:09.728807 140257643190016 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.3363770246505737, loss=2.2284626960754395
I0307 22:29:47.312659 140257634797312 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3315587043762207, loss=2.2688162326812744
I0307 22:31:34.572898 140257643190016 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.3413852453231812, loss=2.0944364070892334
I0307 22:33:21.660156 140257634797312 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.380842685699463, loss=2.1998844146728516
I0307 22:35:09.273349 140257643190016 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.4809436798095703, loss=2.0703675746917725
I0307 22:36:38.560200 140413841560768 spec.py:321] Evaluating on the training split.
I0307 22:36:49.704756 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 22:37:07.213431 140413841560768 spec.py:349] Evaluating on the test split.
I0307 22:37:08.979839 140413841560768 submission_runner.py:469] Time since start: 46741.81s, 	Step: 67284, 	{'train/accuracy': 0.3095105290412903, 'train/loss': 3.5035314559936523, 'validation/accuracy': 0.2874999940395355, 'validation/loss': 3.653413772583008, 'validation/num_examples': 50000, 'test/accuracy': 0.21630001068115234, 'test/loss': 4.420182228088379, 'test/num_examples': 10000, 'score': 43454.29529118538, 'total_duration': 46741.80951166153, 'accumulated_submission_time': 43454.29529118538, 'accumulated_eval_time': 3258.2451775074005, 'accumulated_logging_time': 22.28388738632202}
I0307 22:37:09.074881 140257634797312 logging_writer.py:48] [67284] accumulated_eval_time=3258.25, accumulated_logging_time=22.2839, accumulated_submission_time=43454.3, global_step=67284, preemption_count=0, score=43454.3, test/accuracy=0.2163, test/loss=4.42018, test/num_examples=10000, total_duration=46741.8, train/accuracy=0.309511, train/loss=3.50353, validation/accuracy=0.2875, validation/loss=3.65341, validation/num_examples=50000
I0307 22:37:15.757496 140257643190016 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.4388624429702759, loss=2.2554266452789307
I0307 22:39:02.633994 140257634797312 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3112601041793823, loss=2.158991813659668
I0307 22:40:50.442844 140257643190016 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.525182843208313, loss=2.1591174602508545
2025-03-07 22:41:38.372347: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:42:40.043051 140257634797312 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.2582030296325684, loss=2.2500505447387695
I0307 22:44:27.736079 140257643190016 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.295746088027954, loss=2.1698968410491943
I0307 22:45:39.978247 140413841560768 spec.py:321] Evaluating on the training split.
I0307 22:45:51.446176 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 22:46:10.699961 140413841560768 spec.py:349] Evaluating on the test split.
I0307 22:46:12.483911 140413841560768 submission_runner.py:469] Time since start: 47285.31s, 	Step: 67768, 	{'train/accuracy': 0.2647082209587097, 'train/loss': 4.042666435241699, 'validation/accuracy': 0.2523399889469147, 'validation/loss': 4.150810718536377, 'validation/num_examples': 50000, 'test/accuracy': 0.1868000030517578, 'test/loss': 5.01791524887085, 'test/num_examples': 10000, 'score': 43965.13685297966, 'total_duration': 47285.313568115234, 'accumulated_submission_time': 43965.13685297966, 'accumulated_eval_time': 3290.7507905960083, 'accumulated_logging_time': 22.386687755584717}
I0307 22:46:12.563058 140257634797312 logging_writer.py:48] [67768] accumulated_eval_time=3290.75, accumulated_logging_time=22.3867, accumulated_submission_time=43965.1, global_step=67768, preemption_count=0, score=43965.1, test/accuracy=0.1868, test/loss=5.01792, test/num_examples=10000, total_duration=47285.3, train/accuracy=0.264708, train/loss=4.04267, validation/accuracy=0.25234, validation/loss=4.15081, validation/num_examples=50000
I0307 22:46:34.662684 140257643190016 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.380994200706482, loss=2.126002550125122
I0307 22:48:24.203878 140257634797312 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.4546077251434326, loss=2.0786256790161133
I0307 22:50:11.783459 140257643190016 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3103232383728027, loss=2.064023017883301
I0307 22:51:59.994513 140257634797312 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.3298230171203613, loss=2.1712472438812256
I0307 22:53:48.404652 140257643190016 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.3971314430236816, loss=2.2561373710632324
I0307 22:54:43.520935 140413841560768 spec.py:321] Evaluating on the training split.
I0307 22:54:54.806493 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 22:55:13.316785 140413841560768 spec.py:349] Evaluating on the test split.
I0307 22:55:15.102697 140413841560768 submission_runner.py:469] Time since start: 47827.93s, 	Step: 68252, 	{'train/accuracy': 0.3085339665412903, 'train/loss': 3.499007225036621, 'validation/accuracy': 0.2822200059890747, 'validation/loss': 3.7292985916137695, 'validation/num_examples': 50000, 'test/accuracy': 0.20590001344680786, 'test/loss': 4.527698040008545, 'test/num_examples': 10000, 'score': 44476.030771017075, 'total_duration': 47827.932343006134, 'accumulated_submission_time': 44476.030771017075, 'accumulated_eval_time': 3322.3324859142303, 'accumulated_logging_time': 22.474384784698486}
I0307 22:55:15.168248 140257634797312 logging_writer.py:48] [68252] accumulated_eval_time=3322.33, accumulated_logging_time=22.4744, accumulated_submission_time=44476, global_step=68252, preemption_count=0, score=44476, test/accuracy=0.2059, test/loss=4.5277, test/num_examples=10000, total_duration=47827.9, train/accuracy=0.308534, train/loss=3.49901, validation/accuracy=0.28222, validation/loss=3.7293, validation/num_examples=50000
I0307 22:55:55.936001 140257643190016 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.2906514406204224, loss=2.0669684410095215
I0307 22:57:43.219899 140257634797312 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3834729194641113, loss=2.0395936965942383
I0307 22:59:30.432477 140257643190016 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.3626649379730225, loss=2.234683036804199
I0307 23:01:18.293809 140257634797312 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.335209608078003, loss=2.164912700653076
I0307 23:03:06.035622 140257643190016 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.4222332239151, loss=2.0994749069213867
I0307 23:03:46.074053 140413841560768 spec.py:321] Evaluating on the training split.
I0307 23:03:57.849200 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 23:04:16.188819 140413841560768 spec.py:349] Evaluating on the test split.
I0307 23:04:17.957657 140413841560768 submission_runner.py:469] Time since start: 48370.79s, 	Step: 68738, 	{'train/accuracy': 0.16860650479793549, 'train/loss': 4.9217729568481445, 'validation/accuracy': 0.15981999039649963, 'validation/loss': 5.040174961090088, 'validation/num_examples': 50000, 'test/accuracy': 0.11860000342130661, 'test/loss': 5.589081764221191, 'test/num_examples': 10000, 'score': 44986.87431430817, 'total_duration': 48370.787288188934, 'accumulated_submission_time': 44986.87431430817, 'accumulated_eval_time': 3354.216007709503, 'accumulated_logging_time': 22.548347234725952}
I0307 23:04:18.032852 140257634797312 logging_writer.py:48] [68738] accumulated_eval_time=3354.22, accumulated_logging_time=22.5483, accumulated_submission_time=44986.9, global_step=68738, preemption_count=0, score=44986.9, test/accuracy=0.1186, test/loss=5.58908, test/num_examples=10000, total_duration=48370.8, train/accuracy=0.168607, train/loss=4.92177, validation/accuracy=0.15982, validation/loss=5.04017, validation/num_examples=50000
2025-03-07 23:05:09.977683: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:05:16.000632 140257643190016 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.249815583229065, loss=2.107215166091919
I0307 23:07:03.146415 140257634797312 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.3997201919555664, loss=2.3285229206085205
I0307 23:08:50.245527 140257643190016 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.3845545053482056, loss=2.0203120708465576
I0307 23:10:37.917499 140257634797312 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.399265170097351, loss=2.203244209289551
I0307 23:12:25.018013 140257643190016 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3318029642105103, loss=2.1261281967163086
I0307 23:12:48.628721 140413841560768 spec.py:321] Evaluating on the training split.
I0307 23:12:59.240443 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 23:13:17.654457 140413841560768 spec.py:349] Evaluating on the test split.
I0307 23:13:19.418292 140413841560768 submission_runner.py:469] Time since start: 48912.25s, 	Step: 69223, 	{'train/accuracy': 0.18329480290412903, 'train/loss': 4.912752151489258, 'validation/accuracy': 0.17749999463558197, 'validation/loss': 5.034994602203369, 'validation/num_examples': 50000, 'test/accuracy': 0.133200004696846, 'test/loss': 5.7967071533203125, 'test/num_examples': 10000, 'score': 45497.40640044212, 'total_duration': 48912.247970342636, 'accumulated_submission_time': 45497.40640044212, 'accumulated_eval_time': 3385.005548477173, 'accumulated_logging_time': 22.632166624069214}
I0307 23:13:19.450255 140257634797312 logging_writer.py:48] [69223] accumulated_eval_time=3385.01, accumulated_logging_time=22.6322, accumulated_submission_time=45497.4, global_step=69223, preemption_count=0, score=45497.4, test/accuracy=0.1332, test/loss=5.79671, test/num_examples=10000, total_duration=48912.2, train/accuracy=0.183295, train/loss=4.91275, validation/accuracy=0.1775, validation/loss=5.03499, validation/num_examples=50000
I0307 23:14:31.517995 140257643190016 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2368104457855225, loss=2.2583534717559814
I0307 23:16:18.631644 140257634797312 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.2474778890609741, loss=2.2491579055786133
I0307 23:18:06.142391 140257643190016 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.3005764484405518, loss=2.1230013370513916
I0307 23:19:53.714164 140257634797312 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.4011296033859253, loss=2.1611547470092773
I0307 23:21:41.007825 140257643190016 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.4725658893585205, loss=2.2125096321105957
I0307 23:21:49.723047 140413841560768 spec.py:321] Evaluating on the training split.
I0307 23:22:01.099918 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 23:22:21.372007 140413841560768 spec.py:349] Evaluating on the test split.
I0307 23:22:23.148202 140413841560768 submission_runner.py:469] Time since start: 49455.98s, 	Step: 69709, 	{'train/accuracy': 0.3179408311843872, 'train/loss': 3.543314218521118, 'validation/accuracy': 0.29203999042510986, 'validation/loss': 3.7925686836242676, 'validation/num_examples': 50000, 'test/accuracy': 0.22930000722408295, 'test/loss': 4.426686763763428, 'test/num_examples': 10000, 'score': 46007.61543250084, 'total_duration': 49455.97787928581, 'accumulated_submission_time': 46007.61543250084, 'accumulated_eval_time': 3418.4306683540344, 'accumulated_logging_time': 22.672513723373413}
I0307 23:22:23.206995 140257634797312 logging_writer.py:48] [69709] accumulated_eval_time=3418.43, accumulated_logging_time=22.6725, accumulated_submission_time=46007.6, global_step=69709, preemption_count=0, score=46007.6, test/accuracy=0.2293, test/loss=4.42669, test/num_examples=10000, total_duration=49456, train/accuracy=0.317941, train/loss=3.54331, validation/accuracy=0.29204, validation/loss=3.79257, validation/num_examples=50000
I0307 23:23:53.733613 140257643190016 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.351809024810791, loss=2.151093006134033
I0307 23:25:41.565826 140257634797312 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3063462972640991, loss=2.23091459274292
I0307 23:27:29.486823 140257643190016 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.3768258094787598, loss=2.1716134548187256
2025-03-07 23:28:19.857179: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 23:29:18.914846 140257634797312 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.605675458908081, loss=2.1232144832611084
I0307 23:30:53.581381 140413841560768 spec.py:321] Evaluating on the training split.
I0307 23:31:05.015453 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 23:31:22.135536 140413841560768 spec.py:349] Evaluating on the test split.
I0307 23:31:24.195122 140413841560768 submission_runner.py:469] Time since start: 49997.02s, 	Step: 70189, 	{'train/accuracy': 0.4380779564380646, 'train/loss': 2.5422377586364746, 'validation/accuracy': 0.41359999775886536, 'validation/loss': 2.7150917053222656, 'validation/num_examples': 50000, 'test/accuracy': 0.303600013256073, 'test/loss': 3.5382556915283203, 'test/num_examples': 10000, 'score': 46517.926050424576, 'total_duration': 49997.02480340004, 'accumulated_submission_time': 46517.926050424576, 'accumulated_eval_time': 3449.0443818569183, 'accumulated_logging_time': 22.73910689353943}
I0307 23:31:24.245324 140257643190016 logging_writer.py:48] [70189] accumulated_eval_time=3449.04, accumulated_logging_time=22.7391, accumulated_submission_time=46517.9, global_step=70189, preemption_count=0, score=46517.9, test/accuracy=0.3036, test/loss=3.53826, test/num_examples=10000, total_duration=49997, train/accuracy=0.438078, train/loss=2.54224, validation/accuracy=0.4136, validation/loss=2.71509, validation/num_examples=50000
I0307 23:31:28.906965 140257634797312 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.4286245107650757, loss=2.2435495853424072
I0307 23:33:11.772262 140257643190016 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.3131929636001587, loss=2.1386327743530273
I0307 23:38:40.572138 140257634797312 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.278212308883667, loss=2.2822773456573486
I0307 23:39:55.433536 140413841560768 spec.py:321] Evaluating on the training split.
I0307 23:40:05.918829 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 23:40:29.642822 140413841560768 spec.py:349] Evaluating on the test split.
I0307 23:40:31.433002 140413841560768 submission_runner.py:469] Time since start: 50544.26s, 	Step: 70419, 	{'train/accuracy': 0.4275151491165161, 'train/loss': 2.607390880584717, 'validation/accuracy': 0.39800000190734863, 'validation/loss': 2.7952003479003906, 'validation/num_examples': 50000, 'test/accuracy': 0.301800012588501, 'test/loss': 3.5714099407196045, 'test/num_examples': 10000, 'score': 47029.08199071884, 'total_duration': 50544.26266884804, 'accumulated_submission_time': 47029.08199071884, 'accumulated_eval_time': 3485.0438079833984, 'accumulated_logging_time': 22.79692816734314}
I0307 23:40:31.451169 140257643190016 logging_writer.py:48] [70419] accumulated_eval_time=3485.04, accumulated_logging_time=22.7969, accumulated_submission_time=47029.1, global_step=70419, preemption_count=0, score=47029.1, test/accuracy=0.3018, test/loss=3.57141, test/num_examples=10000, total_duration=50544.3, train/accuracy=0.427515, train/loss=2.60739, validation/accuracy=0.398, validation/loss=2.7952, validation/num_examples=50000
I0307 23:46:01.927836 140257634797312 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.3690394163131714, loss=2.253502607345581
I0307 23:49:02.091741 140413841560768 spec.py:321] Evaluating on the training split.
I0307 23:49:12.981058 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 23:49:31.893762 140413841560768 spec.py:349] Evaluating on the test split.
I0307 23:49:33.636634 140413841560768 submission_runner.py:469] Time since start: 51086.47s, 	Step: 70543, 	{'train/accuracy': 0.31527024507522583, 'train/loss': 3.4249825477600098, 'validation/accuracy': 0.2894199788570404, 'validation/loss': 3.62431001663208, 'validation/num_examples': 50000, 'test/accuracy': 0.22350001335144043, 'test/loss': 4.273886680603027, 'test/num_examples': 10000, 'score': 47539.701190948486, 'total_duration': 51086.46629238129, 'accumulated_submission_time': 47539.701190948486, 'accumulated_eval_time': 3516.588651895523, 'accumulated_logging_time': 22.822726726531982}
I0307 23:49:33.655105 140257643190016 logging_writer.py:48] [70543] accumulated_eval_time=3516.59, accumulated_logging_time=22.8227, accumulated_submission_time=47539.7, global_step=70543, preemption_count=0, score=47539.7, test/accuracy=0.2235, test/loss=4.27389, test/num_examples=10000, total_duration=51086.5, train/accuracy=0.31527, train/loss=3.42498, validation/accuracy=0.28942, validation/loss=3.62431, validation/num_examples=50000
I0307 23:53:18.053910 140257634797312 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.5325344800949097, loss=2.236705780029297
I0307 23:58:04.189190 140413841560768 spec.py:321] Evaluating on the training split.
I0307 23:58:14.885114 140413841560768 spec.py:333] Evaluating on the validation split.
I0307 23:58:41.736353 140413841560768 spec.py:349] Evaluating on the test split.
I0307 23:58:43.533675 140413841560768 submission_runner.py:469] Time since start: 51636.36s, 	Step: 70669, 	{'train/accuracy': 0.21430563926696777, 'train/loss': 4.538547039031982, 'validation/accuracy': 0.20347999036312103, 'validation/loss': 4.599852085113525, 'validation/num_examples': 50000, 'test/accuracy': 0.14420001208782196, 'test/loss': 5.534135341644287, 'test/num_examples': 10000, 'score': 48050.213551044464, 'total_duration': 51636.363298892975, 'accumulated_submission_time': 48050.213551044464, 'accumulated_eval_time': 3555.933057785034, 'accumulated_logging_time': 22.84971785545349}
I0307 23:58:43.568356 140257643190016 logging_writer.py:48] [70669] accumulated_eval_time=3555.93, accumulated_logging_time=22.8497, accumulated_submission_time=48050.2, global_step=70669, preemption_count=0, score=48050.2, test/accuracy=0.1442, test/loss=5.53414, test/num_examples=10000, total_duration=51636.4, train/accuracy=0.214306, train/loss=4.53855, validation/accuracy=0.20348, validation/loss=4.59985, validation/num_examples=50000
I0308 00:00:43.969203 140257634797312 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.540568232536316, loss=2.0990467071533203
I0308 00:07:16.794385 140413841560768 spec.py:321] Evaluating on the training split.
I0308 00:07:27.693604 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 00:07:50.201947 140413841560768 spec.py:349] Evaluating on the test split.
I0308 00:07:52.166798 140413841560768 submission_runner.py:469] Time since start: 52185.00s, 	Step: 70793, 	{'train/accuracy': 0.17984694242477417, 'train/loss': 5.031060218811035, 'validation/accuracy': 0.16103999316692352, 'validation/loss': 5.190260887145996, 'validation/num_examples': 50000, 'test/accuracy': 0.1257999986410141, 'test/loss': 5.690762996673584, 'test/num_examples': 10000, 'score': 48563.416789770126, 'total_duration': 52184.99644088745, 'accumulated_submission_time': 48563.416789770126, 'accumulated_eval_time': 3591.3054065704346, 'accumulated_logging_time': 22.893696784973145}
I0308 00:07:52.187763 140257643190016 logging_writer.py:48] [70793] accumulated_eval_time=3591.31, accumulated_logging_time=22.8937, accumulated_submission_time=48563.4, global_step=70793, preemption_count=0, score=48563.4, test/accuracy=0.1258, test/loss=5.69076, test/num_examples=10000, total_duration=52185, train/accuracy=0.179847, train/loss=5.03106, validation/accuracy=0.16104, validation/loss=5.19026, validation/num_examples=50000
I0308 00:08:10.988010 140257634797312 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3419508934020996, loss=2.1707992553710938
I0308 00:15:14.093597 140257643190016 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.4115138053894043, loss=2.2317698001861572
I0308 00:16:25.363684 140413841560768 spec.py:321] Evaluating on the training split.
I0308 00:16:36.095085 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 00:16:55.447170 140413841560768 spec.py:349] Evaluating on the test split.
I0308 00:16:57.175835 140413841560768 submission_runner.py:469] Time since start: 52730.01s, 	Step: 70918, 	{'train/accuracy': 0.36505499482154846, 'train/loss': 2.9559011459350586, 'validation/accuracy': 0.3509199917316437, 'validation/loss': 3.0791754722595215, 'validation/num_examples': 50000, 'test/accuracy': 0.25, 'test/loss': 3.983215093612671, 'test/num_examples': 10000, 'score': 49076.571224689484, 'total_duration': 52730.00550556183, 'accumulated_submission_time': 49076.571224689484, 'accumulated_eval_time': 3623.1175224781036, 'accumulated_logging_time': 22.923500299453735}
I0308 00:16:57.194115 140257634797312 logging_writer.py:48] [70918] accumulated_eval_time=3623.12, accumulated_logging_time=22.9235, accumulated_submission_time=49076.6, global_step=70918, preemption_count=0, score=49076.6, test/accuracy=0.25, test/loss=3.98322, test/num_examples=10000, total_duration=52730, train/accuracy=0.365055, train/loss=2.9559, validation/accuracy=0.35092, validation/loss=3.07918, validation/num_examples=50000
I0308 00:22:27.974231 140257643190016 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.6489779949188232, loss=2.2792468070983887
I0308 00:25:29.168318 140413841560768 spec.py:321] Evaluating on the training split.
I0308 00:25:39.981083 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 00:26:03.253055 140413841560768 spec.py:349] Evaluating on the test split.
I0308 00:26:05.041947 140413841560768 submission_runner.py:469] Time since start: 53277.87s, 	Step: 71044, 	{'train/accuracy': 0.21867027878761292, 'train/loss': 4.413930416107178, 'validation/accuracy': 0.20603999495506287, 'validation/loss': 4.537769317626953, 'validation/num_examples': 50000, 'test/accuracy': 0.15280000865459442, 'test/loss': 5.239365100860596, 'test/num_examples': 10000, 'score': 49588.523330926895, 'total_duration': 53277.87161183357, 'accumulated_submission_time': 49588.523330926895, 'accumulated_eval_time': 3658.9911205768585, 'accumulated_logging_time': 22.949737310409546}
I0308 00:26:05.064696 140257634797312 logging_writer.py:48] [71044] accumulated_eval_time=3658.99, accumulated_logging_time=22.9497, accumulated_submission_time=49588.5, global_step=71044, preemption_count=0, score=49588.5, test/accuracy=0.1528, test/loss=5.23937, test/num_examples=10000, total_duration=53277.9, train/accuracy=0.21867, train/loss=4.41393, validation/accuracy=0.20604, validation/loss=4.53777, validation/num_examples=50000
I0308 00:29:49.184124 140257643190016 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.548479437828064, loss=2.1570029258728027
I0308 00:34:37.184993 140413841560768 spec.py:321] Evaluating on the training split.
I0308 00:34:48.013282 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 00:35:10.559552 140413841560768 spec.py:349] Evaluating on the test split.
I0308 00:35:12.345664 140413841560768 submission_runner.py:469] Time since start: 53825.18s, 	Step: 71169, 	{'train/accuracy': 0.32057157158851624, 'train/loss': 3.423633337020874, 'validation/accuracy': 0.2821599841117859, 'validation/loss': 3.7573115825653076, 'validation/num_examples': 50000, 'test/accuracy': 0.2078000158071518, 'test/loss': 4.524680137634277, 'test/num_examples': 10000, 'score': 50100.62102603912, 'total_duration': 53825.17532014847, 'accumulated_submission_time': 50100.62102603912, 'accumulated_eval_time': 3694.1517481803894, 'accumulated_logging_time': 22.98146414756775}
I0308 00:35:12.367344 140257634797312 logging_writer.py:48] [71169] accumulated_eval_time=3694.15, accumulated_logging_time=22.9815, accumulated_submission_time=50100.6, global_step=71169, preemption_count=0, score=50100.6, test/accuracy=0.2078, test/loss=4.52468, test/num_examples=10000, total_duration=53825.2, train/accuracy=0.320572, train/loss=3.42363, validation/accuracy=0.28216, validation/loss=3.75731, validation/num_examples=50000
I0308 00:37:10.677826 140257643190016 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.5597810745239258, loss=2.1458137035369873
I0308 00:43:43.479062 140413841560768 spec.py:321] Evaluating on the training split.
I0308 00:43:54.129088 140413841560768 spec.py:333] Evaluating on the validation split.
2025-03-08 00:44:07.066590: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 00:44:13.700060 140413841560768 spec.py:349] Evaluating on the test split.
I0308 00:44:15.441087 140413841560768 submission_runner.py:469] Time since start: 54368.27s, 	Step: 71294, 	{'train/accuracy': 0.37147241830825806, 'train/loss': 2.9358925819396973, 'validation/accuracy': 0.33932000398635864, 'validation/loss': 3.1985857486724854, 'validation/num_examples': 50000, 'test/accuracy': 0.26010000705718994, 'test/loss': 3.9811501502990723, 'test/num_examples': 10000, 'score': 50611.71167373657, 'total_duration': 54368.27076411247, 'accumulated_submission_time': 50611.71167373657, 'accumulated_eval_time': 3726.1137471199036, 'accumulated_logging_time': 23.011420965194702}
I0308 00:44:15.460126 140257634797312 logging_writer.py:48] [71294] accumulated_eval_time=3726.11, accumulated_logging_time=23.0114, accumulated_submission_time=50611.7, global_step=71294, preemption_count=0, score=50611.7, test/accuracy=0.2601, test/loss=3.98115, test/num_examples=10000, total_duration=54368.3, train/accuracy=0.371472, train/loss=2.93589, validation/accuracy=0.33932, validation/loss=3.19859, validation/num_examples=50000
I0308 00:44:23.609067 140257643190016 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3078619241714478, loss=2.1460700035095215
I0308 00:51:26.509746 140257634797312 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.3844339847564697, loss=2.088682174682617
I0308 00:52:46.621008 140413841560768 spec.py:321] Evaluating on the training split.
I0308 00:52:57.349409 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 00:53:16.653058 140413841560768 spec.py:349] Evaluating on the test split.
I0308 00:53:18.394772 140413841560768 submission_runner.py:469] Time since start: 54911.22s, 	Step: 71420, 	{'train/accuracy': 0.378627210855484, 'train/loss': 2.951539993286133, 'validation/accuracy': 0.34891998767852783, 'validation/loss': 3.20268177986145, 'validation/num_examples': 50000, 'test/accuracy': 0.25870001316070557, 'test/loss': 4.002931594848633, 'test/num_examples': 10000, 'score': 51122.83813548088, 'total_duration': 54911.224437475204, 'accumulated_submission_time': 51122.83813548088, 'accumulated_eval_time': 3757.887465238571, 'accumulated_logging_time': 23.05109143257141}
I0308 00:53:18.415043 140257643190016 logging_writer.py:48] [71420] accumulated_eval_time=3757.89, accumulated_logging_time=23.0511, accumulated_submission_time=51122.8, global_step=71420, preemption_count=0, score=51122.8, test/accuracy=0.2587, test/loss=4.00293, test/num_examples=10000, total_duration=54911.2, train/accuracy=0.378627, train/loss=2.95154, validation/accuracy=0.34892, validation/loss=3.20268, validation/num_examples=50000
I0308 00:58:39.391763 140257634797312 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3943736553192139, loss=2.1934189796447754
I0308 01:01:50.487935 140413841560768 spec.py:321] Evaluating on the training split.
I0308 01:02:00.892940 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 01:02:19.878187 140413841560768 spec.py:349] Evaluating on the test split.
I0308 01:02:21.627712 140413841560768 submission_runner.py:469] Time since start: 55454.46s, 	Step: 71546, 	{'train/accuracy': 0.417988657951355, 'train/loss': 2.722219944000244, 'validation/accuracy': 0.38439998030662537, 'validation/loss': 2.965337038040161, 'validation/num_examples': 50000, 'test/accuracy': 0.2882000207901001, 'test/loss': 3.747056484222412, 'test/num_examples': 10000, 'score': 51634.89011383057, 'total_duration': 55454.45736122131, 'accumulated_submission_time': 51634.89011383057, 'accumulated_eval_time': 3789.027185201645, 'accumulated_logging_time': 23.07946276664734}
I0308 01:02:21.646675 140257643190016 logging_writer.py:48] [71546] accumulated_eval_time=3789.03, accumulated_logging_time=23.0795, accumulated_submission_time=51634.9, global_step=71546, preemption_count=0, score=51634.9, test/accuracy=0.2882, test/loss=3.74706, test/num_examples=10000, total_duration=55454.5, train/accuracy=0.417989, train/loss=2.72222, validation/accuracy=0.3844, validation/loss=2.96534, validation/num_examples=50000
I0308 01:05:53.918092 140257634797312 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.6611382961273193, loss=2.154344081878662
I0308 01:10:52.061613 140413841560768 spec.py:321] Evaluating on the training split.
I0308 01:11:02.803034 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 01:11:24.557255 140413841560768 spec.py:349] Evaluating on the test split.
I0308 01:11:26.349774 140413841560768 submission_runner.py:469] Time since start: 55999.18s, 	Step: 71672, 	{'train/accuracy': 0.24326370656490326, 'train/loss': 4.042903423309326, 'validation/accuracy': 0.2270599901676178, 'validation/loss': 4.189541816711426, 'validation/num_examples': 50000, 'test/accuracy': 0.16840000450611115, 'test/loss': 4.919600963592529, 'test/num_examples': 10000, 'score': 52145.28388285637, 'total_duration': 55999.17945146561, 'accumulated_submission_time': 52145.28388285637, 'accumulated_eval_time': 3823.3153166770935, 'accumulated_logging_time': 23.10611057281494}
I0308 01:11:26.368928 140257643190016 logging_writer.py:48] [71672] accumulated_eval_time=3823.32, accumulated_logging_time=23.1061, accumulated_submission_time=52145.3, global_step=71672, preemption_count=0, score=52145.3, test/accuracy=0.1684, test/loss=4.9196, test/num_examples=10000, total_duration=55999.2, train/accuracy=0.243264, train/loss=4.0429, validation/accuracy=0.22706, validation/loss=4.18954, validation/num_examples=50000
I0308 01:13:09.108770 140257634797312 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.436927318572998, loss=2.1579387187957764
I0308 01:19:59.656125 140413841560768 spec.py:321] Evaluating on the training split.
I0308 01:20:10.377663 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 01:20:29.810519 140413841560768 spec.py:349] Evaluating on the test split.
I0308 01:20:31.552479 140413841560768 submission_runner.py:469] Time since start: 56544.38s, 	Step: 71798, 	{'train/accuracy': 0.3282645046710968, 'train/loss': 3.399852752685547, 'validation/accuracy': 0.3055199980735779, 'validation/loss': 3.5449717044830322, 'validation/num_examples': 50000, 'test/accuracy': 0.2232000082731247, 'test/loss': 4.391471862792969, 'test/num_examples': 10000, 'score': 52658.53072857857, 'total_duration': 56544.382128953934, 'accumulated_submission_time': 52658.53072857857, 'accumulated_eval_time': 3855.211621761322, 'accumulated_logging_time': 23.15228772163391}
I0308 01:20:31.570982 140257643190016 logging_writer.py:48] [71798] accumulated_eval_time=3855.21, accumulated_logging_time=23.1523, accumulated_submission_time=52658.5, global_step=71798, preemption_count=0, score=52658.5, test/accuracy=0.2232, test/loss=4.39147, test/num_examples=10000, total_duration=56544.4, train/accuracy=0.328265, train/loss=3.39985, validation/accuracy=0.30552, validation/loss=3.54497, validation/num_examples=50000
I0308 01:20:32.692897 140257634797312 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.4787769317626953, loss=2.2189905643463135
I0308 01:27:26.440422 140257643190016 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.3885776996612549, loss=2.204665422439575
I0308 01:29:03.870916 140413841560768 spec.py:321] Evaluating on the training split.
I0308 01:29:14.488160 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 01:29:35.759058 140413841560768 spec.py:349] Evaluating on the test split.
I0308 01:29:37.498095 140413841560768 submission_runner.py:469] Time since start: 57090.33s, 	Step: 71924, 	{'train/accuracy': 0.24071268737316132, 'train/loss': 4.292496681213379, 'validation/accuracy': 0.21753999590873718, 'validation/loss': 4.522789478302002, 'validation/num_examples': 50000, 'test/accuracy': 0.17260000109672546, 'test/loss': 5.19539737701416, 'test/num_examples': 10000, 'score': 53170.809384822845, 'total_duration': 57090.327771902084, 'accumulated_submission_time': 53170.809384822845, 'accumulated_eval_time': 3888.8387672901154, 'accumulated_logging_time': 23.178457021713257}
I0308 01:29:37.516903 140257634797312 logging_writer.py:48] [71924] accumulated_eval_time=3888.84, accumulated_logging_time=23.1785, accumulated_submission_time=53170.8, global_step=71924, preemption_count=0, score=53170.8, test/accuracy=0.1726, test/loss=5.1954, test/num_examples=10000, total_duration=57090.3, train/accuracy=0.240713, train/loss=4.2925, validation/accuracy=0.21754, validation/loss=4.52279, validation/num_examples=50000
I0308 01:34:41.124004 140257643190016 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.4130349159240723, loss=2.1031906604766846
I0308 01:38:09.998145 140413841560768 spec.py:321] Evaluating on the training split.
I0308 01:38:20.771690 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 01:38:40.179081 140413841560768 spec.py:349] Evaluating on the test split.
I0308 01:38:41.920686 140413841560768 submission_runner.py:469] Time since start: 57634.75s, 	Step: 72050, 	{'train/accuracy': 0.15943877398967743, 'train/loss': 5.236638069152832, 'validation/accuracy': 0.15143999457359314, 'validation/loss': 5.319343566894531, 'validation/num_examples': 50000, 'test/accuracy': 0.11460000276565552, 'test/loss': 5.964763641357422, 'test/num_examples': 10000, 'score': 53683.269191265106, 'total_duration': 57634.75035023689, 'accumulated_submission_time': 53683.269191265106, 'accumulated_eval_time': 3920.7612686157227, 'accumulated_logging_time': 23.206202507019043}
I0308 01:38:41.939862 140257634797312 logging_writer.py:48] [72050] accumulated_eval_time=3920.76, accumulated_logging_time=23.2062, accumulated_submission_time=53683.3, global_step=72050, preemption_count=0, score=53683.3, test/accuracy=0.1146, test/loss=5.96476, test/num_examples=10000, total_duration=57634.8, train/accuracy=0.159439, train/loss=5.23664, validation/accuracy=0.15144, validation/loss=5.31934, validation/num_examples=50000
I0308 01:41:55.237370 140257643190016 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3434263467788696, loss=2.067018508911133
I0308 01:47:11.947191 140413841560768 spec.py:321] Evaluating on the training split.
I0308 01:47:22.761946 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 01:47:42.759884 140413841560768 spec.py:349] Evaluating on the test split.
I0308 01:47:44.496896 140413841560768 submission_runner.py:469] Time since start: 58177.33s, 	Step: 72176, 	{'train/accuracy': 0.1287667453289032, 'train/loss': 5.811068058013916, 'validation/accuracy': 0.124719999730587, 'validation/loss': 5.818170070648193, 'validation/num_examples': 50000, 'test/accuracy': 0.08990000188350677, 'test/loss': 6.3484015464782715, 'test/num_examples': 10000, 'score': 54193.23250865936, 'total_duration': 58177.32654285431, 'accumulated_submission_time': 54193.23250865936, 'accumulated_eval_time': 3953.3109164237976, 'accumulated_logging_time': 23.255741357803345}
I0308 01:47:44.535220 140257634797312 logging_writer.py:48] [72176] accumulated_eval_time=3953.31, accumulated_logging_time=23.2557, accumulated_submission_time=54193.2, global_step=72176, preemption_count=0, score=54193.2, test/accuracy=0.0899, test/loss=6.3484, test/num_examples=10000, total_duration=58177.3, train/accuracy=0.128767, train/loss=5.81107, validation/accuracy=0.12472, validation/loss=5.81817, validation/num_examples=50000
I0308 01:49:08.743676 140257643190016 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.286717414855957, loss=2.1320641040802
I0308 01:56:09.959778 140257634797312 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3489059209823608, loss=2.1529314517974854
I0308 01:56:18.374148 140413841560768 spec.py:321] Evaluating on the training split.
I0308 01:56:28.933384 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 01:56:48.762873 140413841560768 spec.py:349] Evaluating on the test split.
I0308 01:56:50.501703 140413841560768 submission_runner.py:469] Time since start: 58723.33s, 	Step: 72303, 	{'train/accuracy': 0.3047672212123871, 'train/loss': 3.601977586746216, 'validation/accuracy': 0.2881999909877777, 'validation/loss': 3.759565830230713, 'validation/num_examples': 50000, 'test/accuracy': 0.22230000793933868, 'test/loss': 4.540050983428955, 'test/num_examples': 10000, 'score': 54707.049211740494, 'total_duration': 58723.331384420395, 'accumulated_submission_time': 54707.049211740494, 'accumulated_eval_time': 3985.4384474754333, 'accumulated_logging_time': 23.30252504348755}
I0308 01:56:50.520479 140257643190016 logging_writer.py:48] [72303] accumulated_eval_time=3985.44, accumulated_logging_time=23.3025, accumulated_submission_time=54707, global_step=72303, preemption_count=0, score=54707, test/accuracy=0.2223, test/loss=4.54005, test/num_examples=10000, total_duration=58723.3, train/accuracy=0.304767, train/loss=3.60198, validation/accuracy=0.2882, validation/loss=3.75957, validation/num_examples=50000
I0308 02:03:25.152954 140257634797312 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.7346930503845215, loss=2.1251707077026367
I0308 02:05:23.109296 140413841560768 spec.py:321] Evaluating on the training split.
I0308 02:05:33.729398 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 02:05:54.373185 140413841560768 spec.py:349] Evaluating on the test split.
I0308 02:05:56.113842 140413841560768 submission_runner.py:469] Time since start: 59268.94s, 	Step: 72429, 	{'train/accuracy': 0.23636798560619354, 'train/loss': 4.132009506225586, 'validation/accuracy': 0.21319998800754547, 'validation/loss': 4.33515739440918, 'validation/num_examples': 50000, 'test/accuracy': 0.15980000793933868, 'test/loss': 4.946207046508789, 'test/num_examples': 10000, 'score': 55219.61724591255, 'total_duration': 59268.94351649284, 'accumulated_submission_time': 55219.61724591255, 'accumulated_eval_time': 4018.442964076996, 'accumulated_logging_time': 23.328914642333984}
I0308 02:05:56.133063 140257643190016 logging_writer.py:48] [72429] accumulated_eval_time=4018.44, accumulated_logging_time=23.3289, accumulated_submission_time=55219.6, global_step=72429, preemption_count=0, score=55219.6, test/accuracy=0.1598, test/loss=4.94621, test/num_examples=10000, total_duration=59268.9, train/accuracy=0.236368, train/loss=4.13201, validation/accuracy=0.2132, validation/loss=4.33516, validation/num_examples=50000
I0308 02:10:40.158848 140257634797312 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2933201789855957, loss=2.1908411979675293
2025-03-08 02:13:51.327781: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:14:26.231464 140413841560768 spec.py:321] Evaluating on the training split.
I0308 02:14:36.927242 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 02:14:55.823367 140413841560768 spec.py:349] Evaluating on the test split.
I0308 02:14:57.553458 140413841560768 submission_runner.py:469] Time since start: 59810.38s, 	Step: 72573, 	{'train/accuracy': 0.3298588991165161, 'train/loss': 3.3318350315093994, 'validation/accuracy': 0.31158000230789185, 'validation/loss': 3.4608922004699707, 'validation/num_examples': 50000, 'test/accuracy': 0.22660000622272491, 'test/loss': 4.267693996429443, 'test/num_examples': 10000, 'score': 55729.69266605377, 'total_duration': 59810.38313770294, 'accumulated_submission_time': 55729.69266605377, 'accumulated_eval_time': 4049.7649302482605, 'accumulated_logging_time': 23.355398178100586}
I0308 02:14:57.572098 140257643190016 logging_writer.py:48] [72573] accumulated_eval_time=4049.76, accumulated_logging_time=23.3554, accumulated_submission_time=55729.7, global_step=72573, preemption_count=0, score=55729.7, test/accuracy=0.2266, test/loss=4.26769, test/num_examples=10000, total_duration=59810.4, train/accuracy=0.329859, train/loss=3.33184, validation/accuracy=0.31158, validation/loss=3.46089, validation/num_examples=50000
I0308 02:15:22.644066 140257634797312 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.3590538501739502, loss=2.181539297103882
I0308 02:17:47.392904 140257643190016 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.4591362476348877, loss=2.152562379837036
I0308 02:20:09.315145 140257634797312 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.363494873046875, loss=2.1709794998168945
I0308 02:22:31.936668 140257643190016 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.5087498426437378, loss=2.1017768383026123
I0308 02:23:28.760381 140413841560768 spec.py:321] Evaluating on the training split.
I0308 02:23:39.843962 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 02:23:57.916840 140413841560768 spec.py:349] Evaluating on the test split.
I0308 02:23:59.642117 140413841560768 submission_runner.py:469] Time since start: 60352.47s, 	Step: 72941, 	{'train/accuracy': 0.4194834232330322, 'train/loss': 2.6997995376586914, 'validation/accuracy': 0.3963799774646759, 'validation/loss': 2.849249839782715, 'validation/num_examples': 50000, 'test/accuracy': 0.29200002551078796, 'test/loss': 3.7030029296875, 'test/num_examples': 10000, 'score': 56240.83129096031, 'total_duration': 60352.4717798233, 'accumulated_submission_time': 56240.83129096031, 'accumulated_eval_time': 4080.646626472473, 'accumulated_logging_time': 23.381823539733887}
I0308 02:23:59.681455 140257634797312 logging_writer.py:48] [72941] accumulated_eval_time=4080.65, accumulated_logging_time=23.3818, accumulated_submission_time=56240.8, global_step=72941, preemption_count=0, score=56240.8, test/accuracy=0.292, test/loss=3.703, test/num_examples=10000, total_duration=60352.5, train/accuracy=0.419483, train/loss=2.6998, validation/accuracy=0.39638, validation/loss=2.84925, validation/num_examples=50000
I0308 02:25:10.135390 140257643190016 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.329649806022644, loss=2.051898717880249
I0308 02:27:32.027004 140257634797312 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.3028459548950195, loss=2.0913925170898438
I0308 02:29:53.200958 140257643190016 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.4258865118026733, loss=2.1584510803222656
I0308 02:32:14.831327 140257634797312 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.4734599590301514, loss=2.1209051609039307
I0308 02:32:30.647341 140413841560768 spec.py:321] Evaluating on the training split.
I0308 02:32:41.704126 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 02:33:01.561235 140413841560768 spec.py:349] Evaluating on the test split.
I0308 02:33:03.324885 140413841560768 submission_runner.py:469] Time since start: 60896.15s, 	Step: 73312, 	{'train/accuracy': 0.2647082209587097, 'train/loss': 3.9943199157714844, 'validation/accuracy': 0.2400199919939041, 'validation/loss': 4.203547477722168, 'validation/num_examples': 50000, 'test/accuracy': 0.1811000108718872, 'test/loss': 4.935858726501465, 'test/num_examples': 10000, 'score': 56751.74811768532, 'total_duration': 60896.15454006195, 'accumulated_submission_time': 56751.74811768532, 'accumulated_eval_time': 4113.324114322662, 'accumulated_logging_time': 23.429174423217773}
I0308 02:33:03.381495 140257643190016 logging_writer.py:48] [73312] accumulated_eval_time=4113.32, accumulated_logging_time=23.4292, accumulated_submission_time=56751.7, global_step=73312, preemption_count=0, score=56751.7, test/accuracy=0.1811, test/loss=4.93586, test/num_examples=10000, total_duration=60896.2, train/accuracy=0.264708, train/loss=3.99432, validation/accuracy=0.24002, validation/loss=4.20355, validation/num_examples=50000
I0308 02:34:56.044786 140257634797312 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.3554686307907104, loss=2.0681886672973633
I0308 02:37:18.990054 140257643190016 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.4486063718795776, loss=2.070582389831543
I0308 02:39:41.432342 140257634797312 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.455580711364746, loss=2.0416882038116455
I0308 02:41:34.318870 140413841560768 spec.py:321] Evaluating on the training split.
I0308 02:41:45.280919 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 02:42:03.890243 140413841560768 spec.py:349] Evaluating on the test split.
I0308 02:42:05.633090 140413841560768 submission_runner.py:469] Time since start: 61438.46s, 	Step: 73680, 	{'train/accuracy': 0.3001634180545807, 'train/loss': 3.525435447692871, 'validation/accuracy': 0.2801399827003479, 'validation/loss': 3.6794707775115967, 'validation/num_examples': 50000, 'test/accuracy': 0.210300013422966, 'test/loss': 4.376433372497559, 'test/num_examples': 10000, 'score': 57262.63800191879, 'total_duration': 61438.46275138855, 'accumulated_submission_time': 57262.63800191879, 'accumulated_eval_time': 4144.638286113739, 'accumulated_logging_time': 23.492982149124146}
I0308 02:42:05.716588 140257643190016 logging_writer.py:48] [73680] accumulated_eval_time=4144.64, accumulated_logging_time=23.493, accumulated_submission_time=57262.6, global_step=73680, preemption_count=0, score=57262.6, test/accuracy=0.2103, test/loss=4.37643, test/num_examples=10000, total_duration=61438.5, train/accuracy=0.300163, train/loss=3.52544, validation/accuracy=0.28014, validation/loss=3.67947, validation/num_examples=50000
I0308 02:42:18.223391 140257634797312 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3755725622177124, loss=2.097508192062378
2025-03-08 02:44:42.959621: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 02:44:44.411020 140257643190016 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3989373445510864, loss=2.1739161014556885
I0308 02:47:06.994090 140257634797312 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.4882886409759521, loss=2.1606948375701904
I0308 02:49:29.386896 140257643190016 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.3352454900741577, loss=2.1148712635040283
I0308 02:50:36.583983 140413841560768 spec.py:321] Evaluating on the training split.
I0308 02:50:47.710239 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 02:51:07.552096 140413841560768 spec.py:349] Evaluating on the test split.
I0308 02:51:09.274987 140413841560768 submission_runner.py:469] Time since start: 61982.10s, 	Step: 74048, 	{'train/accuracy': 0.3238799273967743, 'train/loss': 3.609135150909424, 'validation/accuracy': 0.30115997791290283, 'validation/loss': 3.7780611515045166, 'validation/num_examples': 50000, 'test/accuracy': 0.21480001509189606, 'test/loss': 4.721311569213867, 'test/num_examples': 10000, 'score': 57773.43185329437, 'total_duration': 61982.10466837883, 'accumulated_submission_time': 57773.43185329437, 'accumulated_eval_time': 4177.329257965088, 'accumulated_logging_time': 23.609025955200195}
I0308 02:51:09.323301 140257634797312 logging_writer.py:48] [74048] accumulated_eval_time=4177.33, accumulated_logging_time=23.609, accumulated_submission_time=57773.4, global_step=74048, preemption_count=0, score=57773.4, test/accuracy=0.2148, test/loss=4.72131, test/num_examples=10000, total_duration=61982.1, train/accuracy=0.32388, train/loss=3.60914, validation/accuracy=0.30116, validation/loss=3.77806, validation/num_examples=50000
I0308 02:52:09.211706 140257643190016 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.3862669467926025, loss=2.11578106880188
I0308 02:54:31.352089 140257634797312 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3622852563858032, loss=2.0040152072906494
I0308 02:56:53.663291 140257643190016 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.4850424528121948, loss=2.1972100734710693
I0308 02:59:17.054231 140257634797312 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.4299747943878174, loss=2.1833157539367676
I0308 02:59:39.951478 140413841560768 spec.py:321] Evaluating on the training split.
I0308 02:59:50.913844 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 03:00:10.444591 140413841560768 spec.py:349] Evaluating on the test split.
I0308 03:00:12.177356 140413841560768 submission_runner.py:469] Time since start: 62525.01s, 	Step: 74417, 	{'train/accuracy': 0.38960856199264526, 'train/loss': 2.8841300010681152, 'validation/accuracy': 0.35599997639656067, 'validation/loss': 3.1400105953216553, 'validation/num_examples': 50000, 'test/accuracy': 0.2564000189304352, 'test/loss': 4.028597354888916, 'test/num_examples': 10000, 'score': 58284.013051986694, 'total_duration': 62525.00703930855, 'accumulated_submission_time': 58284.013051986694, 'accumulated_eval_time': 4209.555104494095, 'accumulated_logging_time': 23.664847135543823}
I0308 03:00:12.284021 140257643190016 logging_writer.py:48] [74417] accumulated_eval_time=4209.56, accumulated_logging_time=23.6648, accumulated_submission_time=58284, global_step=74417, preemption_count=0, score=58284, test/accuracy=0.2564, test/loss=4.0286, test/num_examples=10000, total_duration=62525, train/accuracy=0.389609, train/loss=2.88413, validation/accuracy=0.356, validation/loss=3.14001, validation/num_examples=50000
I0308 03:01:56.395341 140257634797312 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3986859321594238, loss=2.1313419342041016
I0308 03:04:17.979264 140257643190016 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.41974675655365, loss=2.1448590755462646
I0308 03:06:38.916553 140257634797312 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4764066934585571, loss=2.0349769592285156
I0308 03:08:42.412167 140413841560768 spec.py:321] Evaluating on the training split.
I0308 03:08:53.447343 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 03:09:13.421610 140413841560768 spec.py:349] Evaluating on the test split.
I0308 03:09:15.156008 140413841560768 submission_runner.py:469] Time since start: 63067.99s, 	Step: 74788, 	{'train/accuracy': 0.23208306729793549, 'train/loss': 4.297539710998535, 'validation/accuracy': 0.21371999382972717, 'validation/loss': 4.47291374206543, 'validation/num_examples': 50000, 'test/accuracy': 0.16350001096725464, 'test/loss': 5.1035542488098145, 'test/num_examples': 10000, 'score': 58794.092004299164, 'total_duration': 63067.98567914963, 'accumulated_submission_time': 58794.092004299164, 'accumulated_eval_time': 4242.298907518387, 'accumulated_logging_time': 23.77896547317505}
I0308 03:09:15.198151 140257643190016 logging_writer.py:48] [74788] accumulated_eval_time=4242.3, accumulated_logging_time=23.779, accumulated_submission_time=58794.1, global_step=74788, preemption_count=0, score=58794.1, test/accuracy=0.1635, test/loss=5.10355, test/num_examples=10000, total_duration=63068, train/accuracy=0.232083, train/loss=4.29754, validation/accuracy=0.21372, validation/loss=4.47291, validation/num_examples=50000
I0308 03:09:20.202174 140257634797312 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2789140939712524, loss=2.1097769737243652
I0308 03:11:40.769261 140257643190016 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.3647016286849976, loss=2.088571310043335
I0308 03:14:02.709199 140257634797312 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.309671401977539, loss=2.021207094192505
2025-03-08 03:15:15.336262: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:16:26.541131 140257643190016 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.2614853382110596, loss=2.15279221534729
I0308 03:17:45.738435 140413841560768 spec.py:321] Evaluating on the training split.
I0308 03:17:56.791266 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 03:18:17.219266 140413841560768 spec.py:349] Evaluating on the test split.
I0308 03:18:18.953534 140413841560768 submission_runner.py:469] Time since start: 63611.78s, 	Step: 75157, 	{'train/accuracy': 0.41976243257522583, 'train/loss': 2.6942365169525146, 'validation/accuracy': 0.3905999958515167, 'validation/loss': 2.8923380374908447, 'validation/num_examples': 50000, 'test/accuracy': 0.3020000159740448, 'test/loss': 3.6271145343780518, 'test/num_examples': 10000, 'score': 59304.58326411247, 'total_duration': 63611.78315758705, 'accumulated_submission_time': 59304.58326411247, 'accumulated_eval_time': 4275.51393532753, 'accumulated_logging_time': 23.829071521759033}
I0308 03:18:19.025866 140257634797312 logging_writer.py:48] [75157] accumulated_eval_time=4275.51, accumulated_logging_time=23.8291, accumulated_submission_time=59304.6, global_step=75157, preemption_count=0, score=59304.6, test/accuracy=0.302, test/loss=3.62711, test/num_examples=10000, total_duration=63611.8, train/accuracy=0.419762, train/loss=2.69424, validation/accuracy=0.3906, validation/loss=2.89234, validation/num_examples=50000
I0308 03:19:06.487941 140257643190016 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.4974994659423828, loss=2.188464879989624
I0308 03:21:28.823669 140257634797312 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4414153099060059, loss=2.191185235977173
I0308 03:23:50.890200 140257643190016 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.4640693664550781, loss=2.1916019916534424
I0308 03:26:12.960555 140257634797312 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.4109019041061401, loss=2.1604816913604736
I0308 03:26:50.301772 140413841560768 spec.py:321] Evaluating on the training split.
I0308 03:27:01.298261 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 03:27:20.242035 140413841560768 spec.py:349] Evaluating on the test split.
I0308 03:27:21.957630 140413841560768 submission_runner.py:469] Time since start: 64154.79s, 	Step: 75527, 	{'train/accuracy': 0.22761878371238708, 'train/loss': 4.414113521575928, 'validation/accuracy': 0.21965999901294708, 'validation/loss': 4.435965061187744, 'validation/num_examples': 50000, 'test/accuracy': 0.14820000529289246, 'test/loss': 5.41902494430542, 'test/num_examples': 10000, 'score': 59815.80926847458, 'total_duration': 64154.78731274605, 'accumulated_submission_time': 59815.80926847458, 'accumulated_eval_time': 4307.169762849808, 'accumulated_logging_time': 23.910183906555176}
I0308 03:27:22.019049 140257643190016 logging_writer.py:48] [75527] accumulated_eval_time=4307.17, accumulated_logging_time=23.9102, accumulated_submission_time=59815.8, global_step=75527, preemption_count=0, score=59815.8, test/accuracy=0.1482, test/loss=5.41902, test/num_examples=10000, total_duration=64154.8, train/accuracy=0.227619, train/loss=4.41411, validation/accuracy=0.21966, validation/loss=4.43597, validation/num_examples=50000
I0308 03:28:52.459810 140257634797312 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.3569464683532715, loss=2.1501903533935547
I0308 03:31:15.127912 140257643190016 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.3816708326339722, loss=2.122262477874756
I0308 03:33:38.146819 140257634797312 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.3312222957611084, loss=2.114069938659668
I0308 03:35:53.262206 140413841560768 spec.py:321] Evaluating on the training split.
I0308 03:36:04.108905 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 03:36:22.177948 140413841560768 spec.py:349] Evaluating on the test split.
I0308 03:36:23.906733 140413841560768 submission_runner.py:469] Time since start: 64696.74s, 	Step: 75896, 	{'train/accuracy': 0.1622488796710968, 'train/loss': 4.961266040802002, 'validation/accuracy': 0.14763998985290527, 'validation/loss': 5.162595272064209, 'validation/num_examples': 50000, 'test/accuracy': 0.10950000584125519, 'test/loss': 5.7070488929748535, 'test/num_examples': 10000, 'score': 60327.00371026993, 'total_duration': 64696.736402988434, 'accumulated_submission_time': 60327.00371026993, 'accumulated_eval_time': 4337.814252376556, 'accumulated_logging_time': 23.979315996170044}
I0308 03:36:23.935214 140257643190016 logging_writer.py:48] [75896] accumulated_eval_time=4337.81, accumulated_logging_time=23.9793, accumulated_submission_time=60327, global_step=75896, preemption_count=0, score=60327, test/accuracy=0.1095, test/loss=5.70705, test/num_examples=10000, total_duration=64696.7, train/accuracy=0.162249, train/loss=4.96127, validation/accuracy=0.14764, validation/loss=5.1626, validation/num_examples=50000
I0308 03:36:25.833623 140257634797312 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3464351892471313, loss=2.1710870265960693
I0308 03:38:37.096711 140257643190016 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3416868448257446, loss=2.081338405609131
I0308 03:40:58.770040 140257634797312 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.6853383779525757, loss=2.136854410171509
I0308 03:43:21.879204 140257643190016 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.5040614604949951, loss=2.161149501800537
I0308 03:44:54.453943 140413841560768 spec.py:321] Evaluating on the training split.
I0308 03:45:05.516183 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 03:45:24.607668 140413841560768 spec.py:349] Evaluating on the test split.
I0308 03:45:26.358542 140413841560768 submission_runner.py:469] Time since start: 65239.19s, 	Step: 76266, 	{'train/accuracy': 0.27638712525367737, 'train/loss': 3.7996342182159424, 'validation/accuracy': 0.2616199851036072, 'validation/loss': 3.94929575920105, 'validation/num_examples': 50000, 'test/accuracy': 0.19360001385211945, 'test/loss': 4.697072982788086, 'test/num_examples': 10000, 'score': 60837.44729280472, 'total_duration': 65239.18815803528, 'accumulated_submission_time': 60837.44729280472, 'accumulated_eval_time': 4369.718756437302, 'accumulated_logging_time': 24.042822122573853}
I0308 03:45:26.429326 140257634797312 logging_writer.py:48] [76266] accumulated_eval_time=4369.72, accumulated_logging_time=24.0428, accumulated_submission_time=60837.4, global_step=76266, preemption_count=0, score=60837.4, test/accuracy=0.1936, test/loss=4.69707, test/num_examples=10000, total_duration=65239.2, train/accuracy=0.276387, train/loss=3.79963, validation/accuracy=0.26162, validation/loss=3.9493, validation/num_examples=50000
I0308 03:46:01.932366 140257643190016 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.5674749612808228, loss=2.1278932094573975
2025-03-08 03:46:05.010304: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 03:48:25.119588 140257634797312 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.2640070915222168, loss=2.0773863792419434
I0308 03:50:47.658218 140257643190016 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.4489299058914185, loss=2.187617301940918
I0308 03:53:11.112579 140257634797312 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.432104229927063, loss=2.0848844051361084
I0308 03:53:56.593465 140413841560768 spec.py:321] Evaluating on the training split.
I0308 03:54:07.442517 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 03:54:25.004599 140413841560768 spec.py:349] Evaluating on the test split.
I0308 03:54:26.770579 140413841560768 submission_runner.py:469] Time since start: 65779.60s, 	Step: 76633, 	{'train/accuracy': 0.36981824040412903, 'train/loss': 2.9992830753326416, 'validation/accuracy': 0.3512599766254425, 'validation/loss': 3.1624698638916016, 'validation/num_examples': 50000, 'test/accuracy': 0.2596000134944916, 'test/loss': 3.9757516384124756, 'test/num_examples': 10000, 'score': 61347.53976511955, 'total_duration': 65779.60022091866, 'accumulated_submission_time': 61347.53976511955, 'accumulated_eval_time': 4399.895801067352, 'accumulated_logging_time': 24.143393993377686}
I0308 03:54:26.857898 140257643190016 logging_writer.py:48] [76633] accumulated_eval_time=4399.9, accumulated_logging_time=24.1434, accumulated_submission_time=61347.5, global_step=76633, preemption_count=0, score=61347.5, test/accuracy=0.2596, test/loss=3.97575, test/num_examples=10000, total_duration=65779.6, train/accuracy=0.369818, train/loss=2.99928, validation/accuracy=0.35126, validation/loss=3.16247, validation/num_examples=50000
I0308 03:55:48.550858 140257634797312 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.357010841369629, loss=2.098673105239868
I0308 03:58:11.022523 140257643190016 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.4154928922653198, loss=2.028411865234375
I0308 04:00:33.472513 140257634797312 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.3127490282058716, loss=2.123415231704712
I0308 04:02:56.198035 140257643190016 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4738490581512451, loss=2.17879056930542
I0308 04:02:57.630120 140413841560768 spec.py:321] Evaluating on the training split.
I0308 04:03:08.666542 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 04:03:26.743954 140413841560768 spec.py:349] Evaluating on the test split.
I0308 04:03:28.461316 140413841560768 submission_runner.py:469] Time since start: 66321.29s, 	Step: 77002, 	{'train/accuracy': 0.36535394191741943, 'train/loss': 3.113682270050049, 'validation/accuracy': 0.3440600037574768, 'validation/loss': 3.3049702644348145, 'validation/num_examples': 50000, 'test/accuracy': 0.2532000243663788, 'test/loss': 4.126364707946777, 'test/num_examples': 10000, 'score': 61858.23664236069, 'total_duration': 66321.29099607468, 'accumulated_submission_time': 61858.23664236069, 'accumulated_eval_time': 4430.72696685791, 'accumulated_logging_time': 24.263946294784546}
I0308 04:03:28.519943 140257634797312 logging_writer.py:48] [77002] accumulated_eval_time=4430.73, accumulated_logging_time=24.2639, accumulated_submission_time=61858.2, global_step=77002, preemption_count=0, score=61858.2, test/accuracy=0.2532, test/loss=4.12636, test/num_examples=10000, total_duration=66321.3, train/accuracy=0.365354, train/loss=3.11368, validation/accuracy=0.34406, validation/loss=3.30497, validation/num_examples=50000
I0308 04:05:33.435895 140257643190016 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.3475514650344849, loss=2.0259323120117188
I0308 04:07:55.637261 140257634797312 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.3834030628204346, loss=2.174248218536377
I0308 04:10:18.701159 140257643190016 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.4951913356781006, loss=2.0897650718688965
I0308 04:11:59.206020 140413841560768 spec.py:321] Evaluating on the training split.
I0308 04:12:10.220752 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 04:12:27.568184 140413841560768 spec.py:349] Evaluating on the test split.
I0308 04:12:29.324241 140413841560768 submission_runner.py:469] Time since start: 66862.15s, 	Step: 77371, 	{'train/accuracy': 0.34091994166374207, 'train/loss': 3.1935079097747803, 'validation/accuracy': 0.3044999837875366, 'validation/loss': 3.497637987136841, 'validation/num_examples': 50000, 'test/accuracy': 0.22770000994205475, 'test/loss': 4.2392258644104, 'test/num_examples': 10000, 'score': 62368.87443304062, 'total_duration': 66862.15390968323, 'accumulated_submission_time': 62368.87443304062, 'accumulated_eval_time': 4460.845156431198, 'accumulated_logging_time': 24.330100774765015}
I0308 04:12:29.361434 140257634797312 logging_writer.py:48] [77371] accumulated_eval_time=4460.85, accumulated_logging_time=24.3301, accumulated_submission_time=62368.9, global_step=77371, preemption_count=0, score=62368.9, test/accuracy=0.2277, test/loss=4.23923, test/num_examples=10000, total_duration=66862.2, train/accuracy=0.34092, train/loss=3.19351, validation/accuracy=0.3045, validation/loss=3.49764, validation/num_examples=50000
I0308 04:12:56.104546 140257643190016 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4114713668823242, loss=2.1149518489837646
I0308 04:15:23.188666 140257634797312 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.372855305671692, loss=2.1670265197753906
2025-03-08 04:16:39.565040: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:17:47.981495 140257643190016 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.4245808124542236, loss=2.140085220336914
I0308 04:20:11.726555 140257634797312 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.6327273845672607, loss=2.141713857650757
I0308 04:21:00.155318 140413841560768 spec.py:321] Evaluating on the training split.
I0308 04:21:11.257269 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 04:21:28.661107 140413841560768 spec.py:349] Evaluating on the test split.
I0308 04:21:30.385369 140413841560768 submission_runner.py:469] Time since start: 67403.22s, 	Step: 77735, 	{'train/accuracy': 0.3770129084587097, 'train/loss': 2.9615671634674072, 'validation/accuracy': 0.3563399910926819, 'validation/loss': 3.1165823936462402, 'validation/num_examples': 50000, 'test/accuracy': 0.2630999982357025, 'test/loss': 3.92667555809021, 'test/num_examples': 10000, 'score': 62879.58875656128, 'total_duration': 67403.21505022049, 'accumulated_submission_time': 62879.58875656128, 'accumulated_eval_time': 4491.075173854828, 'accumulated_logging_time': 24.406001567840576}
I0308 04:21:30.453747 140257643190016 logging_writer.py:48] [77735] accumulated_eval_time=4491.08, accumulated_logging_time=24.406, accumulated_submission_time=62879.6, global_step=77735, preemption_count=0, score=62879.6, test/accuracy=0.2631, test/loss=3.92668, test/num_examples=10000, total_duration=67403.2, train/accuracy=0.377013, train/loss=2.96157, validation/accuracy=0.35634, validation/loss=3.11658, validation/num_examples=50000
I0308 04:22:53.475486 140257634797312 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.6944090127944946, loss=2.1311376094818115
I0308 04:25:15.999744 140257643190016 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.463313341140747, loss=2.212679624557495
I0308 04:27:38.594616 140257634797312 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.4325859546661377, loss=2.150775671005249
I0308 04:30:01.543537 140257643190016 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.5007579326629639, loss=2.085231304168701
I0308 04:30:01.622793 140413841560768 spec.py:321] Evaluating on the training split.
I0308 04:30:12.661141 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 04:30:31.626045 140413841560768 spec.py:349] Evaluating on the test split.
I0308 04:30:33.369919 140413841560768 submission_runner.py:469] Time since start: 67946.20s, 	Step: 78101, 	{'train/accuracy': 0.38368940353393555, 'train/loss': 2.9835238456726074, 'validation/accuracy': 0.36479997634887695, 'validation/loss': 3.0990540981292725, 'validation/num_examples': 50000, 'test/accuracy': 0.26100000739097595, 'test/loss': 3.994112730026245, 'test/num_examples': 10000, 'score': 63390.70301914215, 'total_duration': 67946.19958806038, 'accumulated_submission_time': 63390.70301914215, 'accumulated_eval_time': 4522.822245836258, 'accumulated_logging_time': 24.488791465759277}
I0308 04:30:33.418038 140257634797312 logging_writer.py:48] [78101] accumulated_eval_time=4522.82, accumulated_logging_time=24.4888, accumulated_submission_time=63390.7, global_step=78101, preemption_count=0, score=63390.7, test/accuracy=0.261, test/loss=3.99411, test/num_examples=10000, total_duration=67946.2, train/accuracy=0.383689, train/loss=2.98352, validation/accuracy=0.3648, validation/loss=3.09905, validation/num_examples=50000
I0308 04:32:40.968873 140257643190016 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.4685860872268677, loss=2.1327972412109375
I0308 04:35:02.594002 140257634797312 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.3599016666412354, loss=2.1239986419677734
I0308 04:37:24.642901 140257643190016 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.3430107831954956, loss=2.0403976440429688
I0308 04:39:04.042925 140413841560768 spec.py:321] Evaluating on the training split.
I0308 04:39:15.011545 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 04:39:34.604425 140413841560768 spec.py:349] Evaluating on the test split.
I0308 04:39:36.372886 140413841560768 submission_runner.py:469] Time since start: 68489.20s, 	Step: 78471, 	{'train/accuracy': 0.3400629758834839, 'train/loss': 3.285909414291382, 'validation/accuracy': 0.321260005235672, 'validation/loss': 3.401979684829712, 'validation/num_examples': 50000, 'test/accuracy': 0.23920001089572906, 'test/loss': 4.214888095855713, 'test/num_examples': 10000, 'score': 63901.27969813347, 'total_duration': 68489.20250988007, 'accumulated_submission_time': 63901.27969813347, 'accumulated_eval_time': 4555.152124881744, 'accumulated_logging_time': 24.544800519943237}
I0308 04:39:36.470529 140257634797312 logging_writer.py:48] [78471] accumulated_eval_time=4555.15, accumulated_logging_time=24.5448, accumulated_submission_time=63901.3, global_step=78471, preemption_count=0, score=63901.3, test/accuracy=0.2392, test/loss=4.21489, test/num_examples=10000, total_duration=68489.2, train/accuracy=0.340063, train/loss=3.28591, validation/accuracy=0.32126, validation/loss=3.40198, validation/num_examples=50000
I0308 04:40:06.272743 140257643190016 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.4520655870437622, loss=1.9911623001098633
I0308 04:42:28.021863 140257634797312 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.38801109790802, loss=1.9332133531570435
I0308 04:44:49.945229 140257643190016 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.5249567031860352, loss=2.1357502937316895
I0308 04:47:11.890241 140257634797312 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.5769706964492798, loss=2.106088876724243
2025-03-08 04:47:18.201729: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 04:48:07.601639 140413841560768 spec.py:321] Evaluating on the training split.
I0308 04:48:18.752296 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 04:48:37.276474 140413841560768 spec.py:349] Evaluating on the test split.
I0308 04:48:39.018507 140413841560768 submission_runner.py:469] Time since start: 69031.85s, 	Step: 78839, 	{'train/accuracy': 0.35710299015045166, 'train/loss': 3.1243834495544434, 'validation/accuracy': 0.3249799907207489, 'validation/loss': 3.3870983123779297, 'validation/num_examples': 50000, 'test/accuracy': 0.24540001153945923, 'test/loss': 4.179543972015381, 'test/num_examples': 10000, 'score': 64412.32405233383, 'total_duration': 69031.8481874466, 'accumulated_submission_time': 64412.32405233383, 'accumulated_eval_time': 4586.568967819214, 'accumulated_logging_time': 24.686896562576294}
I0308 04:48:39.088193 140257643190016 logging_writer.py:48] [78839] accumulated_eval_time=4586.57, accumulated_logging_time=24.6869, accumulated_submission_time=64412.3, global_step=78839, preemption_count=0, score=64412.3, test/accuracy=0.2454, test/loss=4.17954, test/num_examples=10000, total_duration=69031.8, train/accuracy=0.357103, train/loss=3.12438, validation/accuracy=0.32498, validation/loss=3.3871, validation/num_examples=50000
I0308 04:49:51.944295 140257634797312 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.3759026527404785, loss=2.094087600708008
I0308 04:52:13.153941 140257643190016 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.391017198562622, loss=2.0891518592834473
I0308 04:54:36.824904 140257634797312 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.4133366346359253, loss=2.1180920600891113
I0308 04:56:59.897068 140257643190016 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.6777613162994385, loss=2.180140972137451
I0308 04:57:09.944879 140413841560768 spec.py:321] Evaluating on the training split.
I0308 04:57:20.763728 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 04:57:39.196325 140413841560768 spec.py:349] Evaluating on the test split.
I0308 04:57:40.948362 140413841560768 submission_runner.py:469] Time since start: 69573.78s, 	Step: 79208, 	{'train/accuracy': 0.3011200428009033, 'train/loss': 3.619767665863037, 'validation/accuracy': 0.2773599922657013, 'validation/loss': 3.804957151412964, 'validation/num_examples': 50000, 'test/accuracy': 0.20610001683235168, 'test/loss': 4.544443607330322, 'test/num_examples': 10000, 'score': 64923.132544994354, 'total_duration': 69573.7780251503, 'accumulated_submission_time': 64923.132544994354, 'accumulated_eval_time': 4617.572402238846, 'accumulated_logging_time': 24.7644202709198}
I0308 04:57:41.020311 140257634797312 logging_writer.py:48] [79208] accumulated_eval_time=4617.57, accumulated_logging_time=24.7644, accumulated_submission_time=64923.1, global_step=79208, preemption_count=0, score=64923.1, test/accuracy=0.2061, test/loss=4.54444, test/num_examples=10000, total_duration=69573.8, train/accuracy=0.30112, train/loss=3.61977, validation/accuracy=0.27736, validation/loss=3.80496, validation/num_examples=50000
I0308 04:59:37.646681 140257643190016 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.5074721574783325, loss=2.202817678451538
I0308 05:02:01.097284 140257634797312 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.5386804342269897, loss=2.2779085636138916
I0308 05:04:24.307722 140257643190016 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.3380496501922607, loss=2.0159265995025635
I0308 05:06:11.574146 140413841560768 spec.py:321] Evaluating on the training split.
I0308 05:06:22.676324 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 05:06:43.823431 140413841560768 spec.py:349] Evaluating on the test split.
I0308 05:06:45.563626 140413841560768 submission_runner.py:469] Time since start: 70118.39s, 	Step: 79577, 	{'train/accuracy': 0.14680324494838715, 'train/loss': 5.444582939147949, 'validation/accuracy': 0.13731999695301056, 'validation/loss': 5.536393642425537, 'validation/num_examples': 50000, 'test/accuracy': 0.09370000660419464, 'test/loss': 6.311817646026611, 'test/num_examples': 10000, 'score': 65433.63714981079, 'total_duration': 70118.39328622818, 'accumulated_submission_time': 65433.63714981079, 'accumulated_eval_time': 4651.561834812164, 'accumulated_logging_time': 24.8439519405365}
I0308 05:06:45.600552 140257634797312 logging_writer.py:48] [79577] accumulated_eval_time=4651.56, accumulated_logging_time=24.844, accumulated_submission_time=65433.6, global_step=79577, preemption_count=0, score=65433.6, test/accuracy=0.0937, test/loss=6.31182, test/num_examples=10000, total_duration=70118.4, train/accuracy=0.146803, train/loss=5.44458, validation/accuracy=0.13732, validation/loss=5.53639, validation/num_examples=50000
I0308 05:07:03.594641 140257643190016 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.4400640726089478, loss=2.066049814224243
I0308 05:09:26.811989 140257634797312 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.3874956369400024, loss=2.2002620697021484
I0308 05:11:49.598878 140257643190016 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.3303425312042236, loss=2.1156387329101562
I0308 05:14:11.995328 140257634797312 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.6773496866226196, loss=2.110077381134033
I0308 05:15:15.983460 140413841560768 spec.py:321] Evaluating on the training split.
I0308 05:15:26.973293 140413841560768 spec.py:333] Evaluating on the validation split.
I0308 05:15:46.519049 140413841560768 spec.py:349] Evaluating on the test split.
I0308 05:15:48.247358 140413841560768 submission_runner.py:469] Time since start: 70661.08s, 	Step: 79946, 	{'train/accuracy': 0.25841039419174194, 'train/loss': 3.9799463748931885, 'validation/accuracy': 0.24701999127864838, 'validation/loss': 4.066519737243652, 'validation/num_examples': 50000, 'test/accuracy': 0.17430001497268677, 'test/loss': 4.86047887802124, 'test/num_examples': 10000, 'score': 65943.97033858299, 'total_duration': 70661.07703733444, 'accumulated_submission_time': 65943.97033858299, 'accumulated_eval_time': 4683.825715065002, 'accumulated_logging_time': 24.889578819274902}
I0308 05:15:48.347927 140257643190016 logging_writer.py:48] [79946] accumulated_eval_time=4683.83, accumulated_logging_time=24.8896, accumulated_submission_time=65944, global_step=79946, preemption_count=0, score=65944, test/accuracy=0.1743, test/loss=4.86048, test/num_examples=10000, total_duration=70661.1, train/accuracy=0.25841, train/loss=3.97995, validation/accuracy=0.24702, validation/loss=4.06652, validation/num_examples=50000
I0308 05:16:52.893658 140257634797312 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.5838278532028198, loss=2.004983425140381
2025-03-08 05:18:12.360297: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0308 05:19:17.192779 140257643190016 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.4961450099945068, loss=2.086203098297119
I0308 05:21:39.707152 140257634797312 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.4096759557724, loss=2.088874101638794
I0308 05:24:01.889460 140257643190016 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4546377658843994, loss=2.1375653743743896
I0308 05:24:19.087124 140257634797312 logging_writer.py:48] [80313] global_step=80313, preemption_count=0, score=66454.6
I0308 05:24:20.701452 140413841560768 submission_runner.py:646] Tuning trial 4/5
I0308 05:24:20.721828 140413841560768 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0308 05:24:20.725980 140413841560768 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000996492337435484, 'train/loss': 6.911397457122803, 'validation/accuracy': 0.0008999999845400453, 'validation/loss': 6.9117817878723145, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.911991119384766, 'test/num_examples': 10000, 'score': 58.95149374008179, 'total_duration': 153.57268047332764, 'accumulated_submission_time': 58.95149374008179, 'accumulated_eval_time': 94.62093567848206, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1342, {'train/accuracy': 0.1727917641401291, 'train/loss': 4.282511234283447, 'validation/accuracy': 0.14531999826431274, 'validation/loss': 4.559546947479248, 'validation/num_examples': 50000, 'test/accuracy': 0.10680000483989716, 'test/loss': 5.042964458465576, 'test/num_examples': 10000, 'score': 569.0983226299286, 'total_duration': 704.9375741481781, 'accumulated_submission_time': 569.0983226299286, 'accumulated_eval_time': 135.59893321990967, 'accumulated_logging_time': 0.06599211692810059, 'global_step': 1342, 'preemption_count': 0}), (2636, {'train/accuracy': 0.36039140820503235, 'train/loss': 2.9724817276000977, 'validation/accuracy': 0.30587998032569885, 'validation/loss': 3.3215339183807373, 'validation/num_examples': 50000, 'test/accuracy': 0.22420001029968262, 'test/loss': 3.9959349632263184, 'test/num_examples': 10000, 'score': 1079.069636106491, 'total_duration': 1259.4055924415588, 'accumulated_submission_time': 1079.069636106491, 'accumulated_eval_time': 179.89376163482666, 'accumulated_logging_time': 0.10499739646911621, 'global_step': 2636, 'preemption_count': 0}), (3963, {'train/accuracy': 0.3452048599720001, 'train/loss': 3.0669162273406982, 'validation/accuracy': 0.2925799787044525, 'validation/loss': 3.444424867630005, 'validation/num_examples': 50000, 'test/accuracy': 0.21470001339912415, 'test/loss': 4.206352710723877, 'test/num_examples': 10000, 'score': 1589.0803759098053, 'total_duration': 1804.8968324661255, 'accumulated_submission_time': 1589.0803759098053, 'accumulated_eval_time': 215.21254181861877, 'accumulated_logging_time': 0.1381206512451172, 'global_step': 3963, 'preemption_count': 0}), (5286, {'train/accuracy': 0.43189969658851624, 'train/loss': 2.571624755859375, 'validation/accuracy': 0.374019980430603, 'validation/loss': 2.90041184425354, 'validation/num_examples': 50000, 'test/accuracy': 0.2818000018596649, 'test/loss': 3.609036922454834, 'test/num_examples': 10000, 'score': 2098.938943386078, 'total_duration': 2350.2246432304382, 'accumulated_submission_time': 2098.938943386078, 'accumulated_eval_time': 250.53794384002686, 'accumulated_logging_time': 0.15760183334350586, 'global_step': 5286, 'preemption_count': 0}), (6609, {'train/accuracy': 0.4371013939380646, 'train/loss': 2.521240234375, 'validation/accuracy': 0.3926999866962433, 'validation/loss': 2.80679988861084, 'validation/num_examples': 50000, 'test/accuracy': 0.3006000220775604, 'test/loss': 3.475165843963623, 'test/num_examples': 10000, 'score': 2608.9872546195984, 'total_duration': 2895.3226742744446, 'accumulated_submission_time': 2608.9872546195984, 'accumulated_eval_time': 285.4022305011749, 'accumulated_logging_time': 0.22725439071655273, 'global_step': 6609, 'preemption_count': 0}), (7936, {'train/accuracy': 0.38125795125961304, 'train/loss': 2.9028635025024414, 'validation/accuracy': 0.34926000237464905, 'validation/loss': 3.1355762481689453, 'validation/num_examples': 50000, 'test/accuracy': 0.26440000534057617, 'test/loss': 3.837865114212036, 'test/num_examples': 10000, 'score': 3119.1585879325867, 'total_duration': 3441.2669138908386, 'accumulated_submission_time': 3119.1585879325867, 'accumulated_eval_time': 320.97629475593567, 'accumulated_logging_time': 0.3117954730987549, 'global_step': 7936, 'preemption_count': 0}), (9261, {'train/accuracy': 0.3960060477256775, 'train/loss': 2.7425317764282227, 'validation/accuracy': 0.35694000124931335, 'validation/loss': 3.0479462146759033, 'validation/num_examples': 50000, 'test/accuracy': 0.28220000863075256, 'test/loss': 3.7197985649108887, 'test/num_examples': 10000, 'score': 3629.141862630844, 'total_duration': 3986.362341403961, 'accumulated_submission_time': 3629.141862630844, 'accumulated_eval_time': 355.90418553352356, 'accumulated_logging_time': 0.38298916816711426, 'global_step': 9261, 'preemption_count': 0}), (10592, {'train/accuracy': 0.37637513875961304, 'train/loss': 2.8603925704956055, 'validation/accuracy': 0.346019983291626, 'validation/loss': 3.0767273902893066, 'validation/num_examples': 50000, 'test/accuracy': 0.26900002360343933, 'test/loss': 3.727496385574341, 'test/num_examples': 10000, 'score': 4139.016051530838, 'total_duration': 4536.058547735214, 'accumulated_submission_time': 4139.016051530838, 'accumulated_eval_time': 395.4883773326874, 'accumulated_logging_time': 0.48174214363098145, 'global_step': 10592, 'preemption_count': 0}), (11905, {'train/accuracy': 0.3257533311843872, 'train/loss': 3.3719565868377686, 'validation/accuracy': 0.29989999532699585, 'validation/loss': 3.5589704513549805, 'validation/num_examples': 50000, 'test/accuracy': 0.22510001063346863, 'test/loss': 4.310132026672363, 'test/num_examples': 10000, 'score': 4648.88237452507, 'total_duration': 5081.407785415649, 'accumulated_submission_time': 4648.88237452507, 'accumulated_eval_time': 430.74365282058716, 'accumulated_logging_time': 0.5416533946990967, 'global_step': 11905, 'preemption_count': 0}), (13221, {'train/accuracy': 0.3149513602256775, 'train/loss': 3.410829782485962, 'validation/accuracy': 0.28453999757766724, 'validation/loss': 3.683342695236206, 'validation/num_examples': 50000, 'test/accuracy': 0.22590000927448273, 'test/loss': 4.321931838989258, 'test/num_examples': 10000, 'score': 5158.88205242157, 'total_duration': 5632.436633110046, 'accumulated_submission_time': 5158.88205242157, 'accumulated_eval_time': 471.5474600791931, 'accumulated_logging_time': 0.6273565292358398, 'global_step': 13221, 'preemption_count': 0}), (14538, {'train/accuracy': 0.2760084569454193, 'train/loss': 3.6868908405303955, 'validation/accuracy': 0.2556400001049042, 'validation/loss': 3.885582447052002, 'validation/num_examples': 50000, 'test/accuracy': 0.18730001151561737, 'test/loss': 4.582221031188965, 'test/num_examples': 10000, 'score': 5668.583002567291, 'total_duration': 6183.863875627518, 'accumulated_submission_time': 5668.583002567291, 'accumulated_eval_time': 512.9210107326508, 'accumulated_logging_time': 0.8443193435668945, 'global_step': 14538, 'preemption_count': 0}), (15851, {'train/accuracy': 0.36172670125961304, 'train/loss': 3.021711587905884, 'validation/accuracy': 0.33761999011039734, 'validation/loss': 3.2474756240844727, 'validation/num_examples': 50000, 'test/accuracy': 0.24390001595020294, 'test/loss': 4.107375144958496, 'test/num_examples': 10000, 'score': 6178.349545717239, 'total_duration': 6740.366504192352, 'accumulated_submission_time': 6178.349545717239, 'accumulated_eval_time': 559.3396933078766, 'accumulated_logging_time': 1.0217478275299072, 'global_step': 15851, 'preemption_count': 0}), (17162, {'train/accuracy': 0.4166533648967743, 'train/loss': 2.736692428588867, 'validation/accuracy': 0.38279998302459717, 'validation/loss': 2.994539976119995, 'validation/num_examples': 50000, 'test/accuracy': 0.28519999980926514, 'test/loss': 3.809859275817871, 'test/num_examples': 10000, 'score': 6688.176451683044, 'total_duration': 7296.969889640808, 'accumulated_submission_time': 6688.176451683044, 'accumulated_eval_time': 605.8287394046783, 'accumulated_logging_time': 1.1723618507385254, 'global_step': 17162, 'preemption_count': 0}), (18478, {'train/accuracy': 0.35674425959587097, 'train/loss': 3.0802149772644043, 'validation/accuracy': 0.33188000321388245, 'validation/loss': 3.2801218032836914, 'validation/num_examples': 50000, 'test/accuracy': 0.2590000033378601, 'test/loss': 3.9830384254455566, 'test/num_examples': 10000, 'score': 7197.967222929001, 'total_duration': 7844.824070215225, 'accumulated_submission_time': 7197.967222929001, 'accumulated_eval_time': 643.3802750110626, 'accumulated_logging_time': 1.55641508102417, 'global_step': 18478, 'preemption_count': 0}), (19793, {'train/accuracy': 0.23495295643806458, 'train/loss': 4.069524765014648, 'validation/accuracy': 0.2222599983215332, 'validation/loss': 4.186558723449707, 'validation/num_examples': 50000, 'test/accuracy': 0.17020000517368317, 'test/loss': 4.826508045196533, 'test/num_examples': 10000, 'score': 7707.762309789658, 'total_duration': 8401.311625003815, 'accumulated_submission_time': 7707.762309789658, 'accumulated_eval_time': 689.6808261871338, 'accumulated_logging_time': 1.795959234237671, 'global_step': 19793, 'preemption_count': 0}), (21100, {'train/accuracy': 0.25221219658851624, 'train/loss': 3.9189140796661377, 'validation/accuracy': 0.23273999989032745, 'validation/loss': 4.085951328277588, 'validation/num_examples': 50000, 'test/accuracy': 0.16530001163482666, 'test/loss': 4.810108184814453, 'test/num_examples': 10000, 'score': 8217.72189283371, 'total_duration': 8952.10895895958, 'accumulated_submission_time': 8217.72189283371, 'accumulated_eval_time': 730.3077661991119, 'accumulated_logging_time': 1.8665201663970947, 'global_step': 21100, 'preemption_count': 0}), (22400, {'train/accuracy': 0.3220065236091614, 'train/loss': 3.3162057399749756, 'validation/accuracy': 0.2998200058937073, 'validation/loss': 3.505038022994995, 'validation/num_examples': 50000, 'test/accuracy': 0.2183000147342682, 'test/loss': 4.377758502960205, 'test/num_examples': 10000, 'score': 8727.663914442062, 'total_duration': 9499.48161816597, 'accumulated_submission_time': 8727.663914442062, 'accumulated_eval_time': 767.475672006607, 'accumulated_logging_time': 1.9921505451202393, 'global_step': 22400, 'preemption_count': 0}), (23718, {'train/accuracy': 0.25554049015045166, 'train/loss': 3.87467360496521, 'validation/accuracy': 0.24177999794483185, 'validation/loss': 4.040432929992676, 'validation/num_examples': 50000, 'test/accuracy': 0.1827000081539154, 'test/loss': 4.756398677825928, 'test/num_examples': 10000, 'score': 9237.5722489357, 'total_duration': 10054.27838087082, 'accumulated_submission_time': 9237.5722489357, 'accumulated_eval_time': 812.1880791187286, 'accumulated_logging_time': 2.0323283672332764, 'global_step': 23718, 'preemption_count': 0}), (25027, {'train/accuracy': 0.17245295643806458, 'train/loss': 5.346203327178955, 'validation/accuracy': 0.16779999434947968, 'validation/loss': 5.412228584289551, 'validation/num_examples': 50000, 'test/accuracy': 0.11410000175237656, 'test/loss': 6.4161882400512695, 'test/num_examples': 10000, 'score': 9747.356564998627, 'total_duration': 10609.492995023727, 'accumulated_submission_time': 9747.356564998627, 'accumulated_eval_time': 857.374653339386, 'accumulated_logging_time': 2.1465256214141846, 'global_step': 25027, 'preemption_count': 0}), (26335, {'train/accuracy': 0.2770647406578064, 'train/loss': 3.9628350734710693, 'validation/accuracy': 0.265859991312027, 'validation/loss': 4.100792407989502, 'validation/num_examples': 50000, 'test/accuracy': 0.19350001215934753, 'test/loss': 5.097689628601074, 'test/num_examples': 10000, 'score': 10257.385878324509, 'total_duration': 11158.846274852753, 'accumulated_submission_time': 10257.385878324509, 'accumulated_eval_time': 896.5082101821899, 'accumulated_logging_time': 2.2085933685302734, 'global_step': 26335, 'preemption_count': 0}), (27647, {'train/accuracy': 0.3230229616165161, 'train/loss': 3.3251378536224365, 'validation/accuracy': 0.29659998416900635, 'validation/loss': 3.530597448348999, 'validation/num_examples': 50000, 'test/accuracy': 0.22910000383853912, 'test/loss': 4.166884422302246, 'test/num_examples': 10000, 'score': 10766.883116006851, 'total_duration': 11709.382200479507, 'accumulated_submission_time': 10766.883116006851, 'accumulated_eval_time': 936.856507062912, 'accumulated_logging_time': 2.7665841579437256, 'global_step': 27647, 'preemption_count': 0}), (28962, {'train/accuracy': 0.32830435037612915, 'train/loss': 3.302366018295288, 'validation/accuracy': 0.3099599778652191, 'validation/loss': 3.4873781204223633, 'validation/num_examples': 50000, 'test/accuracy': 0.22910000383853912, 'test/loss': 4.304723739624023, 'test/num_examples': 10000, 'score': 11276.933552742004, 'total_duration': 12261.779875278473, 'accumulated_submission_time': 11276.933552742004, 'accumulated_eval_time': 978.8965072631836, 'accumulated_logging_time': 2.9419424533843994, 'global_step': 28962, 'preemption_count': 0}), (30275, {'train/accuracy': 0.08810985088348389, 'train/loss': 6.9213714599609375, 'validation/accuracy': 0.08069999516010284, 'validation/loss': 7.091775894165039, 'validation/num_examples': 50000, 'test/accuracy': 0.06630000472068787, 'test/loss': 7.594058036804199, 'test/num_examples': 10000, 'score': 11786.804682731628, 'total_duration': 12814.02454996109, 'accumulated_submission_time': 11786.804682731628, 'accumulated_eval_time': 1021.0411665439606, 'accumulated_logging_time': 3.036323070526123, 'global_step': 30275, 'preemption_count': 0}), (31586, {'train/accuracy': 0.2460339516401291, 'train/loss': 4.076464653015137, 'validation/accuracy': 0.22613999247550964, 'validation/loss': 4.313343048095703, 'validation/num_examples': 50000, 'test/accuracy': 0.172200009226799, 'test/loss': 5.027352809906006, 'test/num_examples': 10000, 'score': 12296.698233127594, 'total_duration': 13362.27832865715, 'accumulated_submission_time': 12296.698233127594, 'accumulated_eval_time': 1059.114381313324, 'accumulated_logging_time': 3.191221237182617, 'global_step': 31586, 'preemption_count': 0}), (32905, {'train/accuracy': 0.3020966053009033, 'train/loss': 3.5442140102386475, 'validation/accuracy': 0.280379980802536, 'validation/loss': 3.7143263816833496, 'validation/num_examples': 50000, 'test/accuracy': 0.2111000120639801, 'test/loss': 4.476617336273193, 'test/num_examples': 10000, 'score': 12806.461260318756, 'total_duration': 13917.714373111725, 'accumulated_submission_time': 12806.461260318756, 'accumulated_eval_time': 1104.4765815734863, 'accumulated_logging_time': 3.373443841934204, 'global_step': 32905, 'preemption_count': 0}), (34225, {'train/accuracy': 0.24300462007522583, 'train/loss': 4.0306596755981445, 'validation/accuracy': 0.2237199991941452, 'validation/loss': 4.230849266052246, 'validation/num_examples': 50000, 'test/accuracy': 0.16440001130104065, 'test/loss': 5.0166449546813965, 'test/num_examples': 10000, 'score': 13316.517487764359, 'total_duration': 14469.399162054062, 'accumulated_submission_time': 13316.517487764359, 'accumulated_eval_time': 1145.8107497692108, 'accumulated_logging_time': 3.533261299133301, 'global_step': 34225, 'preemption_count': 0}), (35549, {'train/accuracy': 0.10782046616077423, 'train/loss': 5.862038612365723, 'validation/accuracy': 0.09572000056505203, 'validation/loss': 6.023454666137695, 'validation/num_examples': 50000, 'test/accuracy': 0.07830000668764114, 'test/loss': 6.451637268066406, 'test/num_examples': 10000, 'score': 13826.651160001755, 'total_duration': 15016.060870170593, 'accumulated_submission_time': 13826.651160001755, 'accumulated_eval_time': 1182.113071680069, 'accumulated_logging_time': 3.6243035793304443, 'global_step': 35549, 'preemption_count': 0}), (36862, {'train/accuracy': 0.33067601919174194, 'train/loss': 3.2387871742248535, 'validation/accuracy': 0.3121199905872345, 'validation/loss': 3.3640401363372803, 'validation/num_examples': 50000, 'test/accuracy': 0.23770001530647278, 'test/loss': 4.094586372375488, 'test/num_examples': 10000, 'score': 14336.78462934494, 'total_duration': 15564.589007139206, 'accumulated_submission_time': 14336.78462934494, 'accumulated_eval_time': 1220.30553150177, 'accumulated_logging_time': 3.695484161376953, 'global_step': 36862, 'preemption_count': 0}), (38178, {'train/accuracy': 0.20216836035251617, 'train/loss': 4.433493137359619, 'validation/accuracy': 0.1929199993610382, 'validation/loss': 4.585607528686523, 'validation/num_examples': 50000, 'test/accuracy': 0.15139999985694885, 'test/loss': 5.0769734382629395, 'test/num_examples': 10000, 'score': 14846.681279182434, 'total_duration': 16121.297149896622, 'accumulated_submission_time': 14846.681279182434, 'accumulated_eval_time': 1266.783516407013, 'accumulated_logging_time': 3.897552490234375, 'global_step': 38178, 'preemption_count': 0}), (39498, {'train/accuracy': 0.2537866532802582, 'train/loss': 4.071917533874512, 'validation/accuracy': 0.25071999430656433, 'validation/loss': 4.174068927764893, 'validation/num_examples': 50000, 'test/accuracy': 0.17990000545978546, 'test/loss': 5.063265800476074, 'test/num_examples': 10000, 'score': 15356.487592697144, 'total_duration': 16669.422901153564, 'accumulated_submission_time': 15356.487592697144, 'accumulated_eval_time': 1304.8512241840363, 'accumulated_logging_time': 4.014764070510864, 'global_step': 39498, 'preemption_count': 0}), (40818, {'train/accuracy': 0.1706194132566452, 'train/loss': 5.035391807556152, 'validation/accuracy': 0.1625399887561798, 'validation/loss': 5.1285080909729, 'validation/num_examples': 50000, 'test/accuracy': 0.11860000342130661, 'test/loss': 5.980480670928955, 'test/num_examples': 10000, 'score': 15866.528507947922, 'total_duration': 17216.6478536129, 'accumulated_submission_time': 15866.528507947922, 'accumulated_eval_time': 1341.791568994522, 'accumulated_logging_time': 4.1220502853393555, 'global_step': 40818, 'preemption_count': 0}), (42135, {'train/accuracy': 0.11567282676696777, 'train/loss': 5.940239429473877, 'validation/accuracy': 0.11229999363422394, 'validation/loss': 6.0063252449035645, 'validation/num_examples': 50000, 'test/accuracy': 0.07880000025033951, 'test/loss': 6.625520706176758, 'test/num_examples': 10000, 'score': 16376.38707447052, 'total_duration': 17764.992713451385, 'accumulated_submission_time': 16376.38707447052, 'accumulated_eval_time': 1380.0073475837708, 'accumulated_logging_time': 4.25922417640686, 'global_step': 42135, 'preemption_count': 0}), (43457, {'train/accuracy': 0.0633370503783226, 'train/loss': 7.920056343078613, 'validation/accuracy': 0.05791999772191048, 'validation/loss': 7.952990531921387, 'validation/num_examples': 50000, 'test/accuracy': 0.04070000350475311, 'test/loss': 8.67208480834961, 'test/num_examples': 10000, 'score': 16886.3216483593, 'total_duration': 18314.257409095764, 'accumulated_submission_time': 16886.3216483593, 'accumulated_eval_time': 1419.0126914978027, 'accumulated_logging_time': 4.453425645828247, 'global_step': 43457, 'preemption_count': 0}), (44771, {'train/accuracy': 0.0276227667927742, 'train/loss': 7.577892303466797, 'validation/accuracy': 0.02725999988615513, 'validation/loss': 7.615306854248047, 'validation/num_examples': 50000, 'test/accuracy': 0.01850000023841858, 'test/loss': 7.915430068969727, 'test/num_examples': 10000, 'score': 17396.043432474136, 'total_duration': 18864.749269485474, 'accumulated_submission_time': 17396.043432474136, 'accumulated_eval_time': 1459.4956834316254, 'accumulated_logging_time': 4.602795600891113, 'global_step': 44771, 'preemption_count': 0}), (46080, {'train/accuracy': 0.2875278890132904, 'train/loss': 3.640925168991089, 'validation/accuracy': 0.2669999897480011, 'validation/loss': 3.8019394874572754, 'validation/num_examples': 50000, 'test/accuracy': 0.20350000262260437, 'test/loss': 4.463255882263184, 'test/num_examples': 10000, 'score': 17906.037656784058, 'total_duration': 19411.497208595276, 'accumulated_submission_time': 17906.037656784058, 'accumulated_eval_time': 1496.012038230896, 'accumulated_logging_time': 4.707248210906982, 'global_step': 46080, 'preemption_count': 0}), (47396, {'train/accuracy': 0.13255341351032257, 'train/loss': 5.313976764678955, 'validation/accuracy': 0.12441999465227127, 'validation/loss': 5.409339427947998, 'validation/num_examples': 50000, 'test/accuracy': 0.09480000287294388, 'test/loss': 5.978036880493164, 'test/num_examples': 10000, 'score': 18416.14288687706, 'total_duration': 19955.549971818924, 'accumulated_submission_time': 18416.14288687706, 'accumulated_eval_time': 1529.7382249832153, 'accumulated_logging_time': 4.789154767990112, 'global_step': 47396, 'preemption_count': 0}), (48711, {'train/accuracy': 0.31058672070503235, 'train/loss': 3.350979804992676, 'validation/accuracy': 0.2925199866294861, 'validation/loss': 3.4927120208740234, 'validation/num_examples': 50000, 'test/accuracy': 0.22290000319480896, 'test/loss': 4.135767936706543, 'test/num_examples': 10000, 'score': 18926.149178504944, 'total_duration': 20500.376094579697, 'accumulated_submission_time': 18926.149178504944, 'accumulated_eval_time': 1564.2948393821716, 'accumulated_logging_time': 4.915104627609253, 'global_step': 48711, 'preemption_count': 0}), (50031, {'train/accuracy': 0.26189810037612915, 'train/loss': 3.9904892444610596, 'validation/accuracy': 0.24539999663829803, 'validation/loss': 4.136928558349609, 'validation/num_examples': 50000, 'test/accuracy': 0.17630000412464142, 'test/loss': 4.876581192016602, 'test/num_examples': 10000, 'score': 19435.944265127182, 'total_duration': 21054.082141160965, 'accumulated_submission_time': 19435.944265127182, 'accumulated_eval_time': 1607.9649820327759, 'accumulated_logging_time': 5.024606704711914, 'global_step': 50031, 'preemption_count': 0}), (51348, {'train/accuracy': 0.24914300441741943, 'train/loss': 4.0262651443481445, 'validation/accuracy': 0.22981999814510345, 'validation/loss': 4.26649284362793, 'validation/num_examples': 50000, 'test/accuracy': 0.17180000245571136, 'test/loss': 4.9555487632751465, 'test/num_examples': 10000, 'score': 19945.93607735634, 'total_duration': 21602.522869825363, 'accumulated_submission_time': 19945.93607735634, 'accumulated_eval_time': 1646.1609761714935, 'accumulated_logging_time': 5.146812200546265, 'global_step': 51348, 'preemption_count': 0}), (52672, {'train/accuracy': 0.3948700428009033, 'train/loss': 2.825113296508789, 'validation/accuracy': 0.3696399927139282, 'validation/loss': 3.0120792388916016, 'validation/num_examples': 50000, 'test/accuracy': 0.26250001788139343, 'test/loss': 3.9060709476470947, 'test/num_examples': 10000, 'score': 20455.814186811447, 'total_duration': 22149.79374241829, 'accumulated_submission_time': 20455.814186811447, 'accumulated_eval_time': 1683.2884595394135, 'accumulated_logging_time': 5.277775764465332, 'global_step': 52672, 'preemption_count': 0}), (53991, {'train/accuracy': 0.3551100194454193, 'train/loss': 3.0897536277770996, 'validation/accuracy': 0.33267998695373535, 'validation/loss': 3.2748899459838867, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 4.059902191162109, 'test/num_examples': 10000, 'score': 20965.567807912827, 'total_duration': 22692.465978622437, 'accumulated_submission_time': 20965.567807912827, 'accumulated_eval_time': 1715.9582483768463, 'accumulated_logging_time': 5.396424770355225, 'global_step': 53991, 'preemption_count': 0}), (55308, {'train/accuracy': 0.22875477373600006, 'train/loss': 4.165148735046387, 'validation/accuracy': 0.20763999223709106, 'validation/loss': 4.339871883392334, 'validation/num_examples': 50000, 'test/accuracy': 0.1592000126838684, 'test/loss': 4.965682029724121, 'test/num_examples': 10000, 'score': 21475.6614010334, 'total_duration': 23239.623193979263, 'accumulated_submission_time': 21475.6614010334, 'accumulated_eval_time': 1752.7324776649475, 'accumulated_logging_time': 5.556323766708374, 'global_step': 55308, 'preemption_count': 0}), (56622, {'train/accuracy': 0.3228236436843872, 'train/loss': 3.305924654006958, 'validation/accuracy': 0.3044999837875366, 'validation/loss': 3.4389808177948, 'validation/num_examples': 50000, 'test/accuracy': 0.21770000457763672, 'test/loss': 4.280907154083252, 'test/num_examples': 10000, 'score': 21985.569434404373, 'total_duration': 23786.021417856216, 'accumulated_submission_time': 21985.569434404373, 'accumulated_eval_time': 1788.9671611785889, 'accumulated_logging_time': 5.67985200881958, 'global_step': 56622, 'preemption_count': 0}), (57369, {'train/accuracy': 0.3434111773967743, 'train/loss': 3.1648361682891846, 'validation/accuracy': 0.3187199831008911, 'validation/loss': 3.40744948387146, 'validation/num_examples': 50000, 'test/accuracy': 0.23340001702308655, 'test/loss': 4.210224151611328, 'test/num_examples': 10000, 'score': 22496.056319475174, 'total_duration': 24333.348866939545, 'accumulated_submission_time': 22496.056319475174, 'accumulated_eval_time': 1825.5348870754242, 'accumulated_logging_time': 5.8730082511901855, 'global_step': 57369, 'preemption_count': 0}), (57834, {'train/accuracy': 0.38028138875961304, 'train/loss': 2.857783555984497, 'validation/accuracy': 0.3585599958896637, 'validation/loss': 3.0040712356567383, 'validation/num_examples': 50000, 'test/accuracy': 0.26570001244544983, 'test/loss': 3.806081771850586, 'test/num_examples': 10000, 'score': 23008.537341833115, 'total_duration': 24889.999618768692, 'accumulated_submission_time': 23008.537341833115, 'accumulated_eval_time': 1869.6061589717865, 'accumulated_logging_time': 5.920804977416992, 'global_step': 57834, 'preemption_count': 0}), (57959, {'train/accuracy': 0.22616389393806458, 'train/loss': 4.178374290466309, 'validation/accuracy': 0.20841999351978302, 'validation/loss': 4.313455104827881, 'validation/num_examples': 50000, 'test/accuracy': 0.1575000137090683, 'test/loss': 4.945911407470703, 'test/num_examples': 10000, 'score': 23518.526952028275, 'total_duration': 25435.776247024536, 'accumulated_submission_time': 23518.526952028275, 'accumulated_eval_time': 1905.3175201416016, 'accumulated_logging_time': 5.982558012008667, 'global_step': 57959, 'preemption_count': 0}), (58084, {'train/accuracy': 0.3465600907802582, 'train/loss': 3.1448400020599365, 'validation/accuracy': 0.32510000467300415, 'validation/loss': 3.3172545433044434, 'validation/num_examples': 50000, 'test/accuracy': 0.248400017619133, 'test/loss': 4.014395713806152, 'test/num_examples': 10000, 'score': 24031.906992912292, 'total_duration': 25989.434996843338, 'accumulated_submission_time': 24031.906992912292, 'accumulated_eval_time': 1945.5580773353577, 'accumulated_logging_time': 6.0060083866119385, 'global_step': 58084, 'preemption_count': 0}), (58209, {'train/accuracy': 0.11766581237316132, 'train/loss': 5.678391456604004, 'validation/accuracy': 0.11631999909877777, 'validation/loss': 5.718416213989258, 'validation/num_examples': 50000, 'test/accuracy': 0.0828000009059906, 'test/loss': 6.362591743469238, 'test/num_examples': 10000, 'score': 24544.891598939896, 'total_duration': 26538.904282331467, 'accumulated_submission_time': 24544.891598939896, 'accumulated_eval_time': 1982.005404472351, 'accumulated_logging_time': 6.0293285846710205, 'global_step': 58209, 'preemption_count': 0}), (58352, {'train/accuracy': 0.24248644709587097, 'train/loss': 4.11922550201416, 'validation/accuracy': 0.2253599911928177, 'validation/loss': 4.246465682983398, 'validation/num_examples': 50000, 'test/accuracy': 0.15960000455379486, 'test/loss': 5.080419540405273, 'test/num_examples': 10000, 'score': 25056.365832567215, 'total_duration': 27081.95148229599, 'accumulated_submission_time': 25056.365832567215, 'accumulated_eval_time': 2013.538027048111, 'accumulated_logging_time': 6.053134441375732, 'global_step': 58352, 'preemption_count': 0}), (58659, {'train/accuracy': 0.2133091539144516, 'train/loss': 4.2986931800842285, 'validation/accuracy': 0.19991999864578247, 'validation/loss': 4.448964595794678, 'validation/num_examples': 50000, 'test/accuracy': 0.15380001068115234, 'test/loss': 5.150226593017578, 'test/num_examples': 10000, 'score': 25567.54655981064, 'total_duration': 27622.99753499031, 'accumulated_submission_time': 25567.54655981064, 'accumulated_eval_time': 2043.3428399562836, 'accumulated_logging_time': 6.0776989459991455, 'global_step': 58659, 'preemption_count': 0}), (58996, {'train/accuracy': 0.11714763939380646, 'train/loss': 5.317147731781006, 'validation/accuracy': 0.10991999506950378, 'validation/loss': 5.358582973480225, 'validation/num_examples': 50000, 'test/accuracy': 0.07830000668764114, 'test/loss': 5.9857964515686035, 'test/num_examples': 10000, 'score': 26078.026283740997, 'total_duration': 28165.803186655045, 'accumulated_submission_time': 26078.026283740997, 'accumulated_eval_time': 2075.6046130657196, 'accumulated_logging_time': 6.105093002319336, 'global_step': 58996, 'preemption_count': 0}), (59364, {'train/accuracy': 0.24918286502361298, 'train/loss': 3.97361159324646, 'validation/accuracy': 0.24108000099658966, 'validation/loss': 4.0884504318237305, 'validation/num_examples': 50000, 'test/accuracy': 0.17190000414848328, 'test/loss': 4.903674602508545, 'test/num_examples': 10000, 'score': 26588.208255767822, 'total_duration': 28710.385571956635, 'accumulated_submission_time': 26588.208255767822, 'accumulated_eval_time': 2109.848464488983, 'accumulated_logging_time': 6.2210023403167725, 'global_step': 59364, 'preemption_count': 0}), (59736, {'train/accuracy': 0.28993940353393555, 'train/loss': 3.7358789443969727, 'validation/accuracy': 0.2728399932384491, 'validation/loss': 3.9174323081970215, 'validation/num_examples': 50000, 'test/accuracy': 0.20040000975131989, 'test/loss': 4.754363059997559, 'test/num_examples': 10000, 'score': 27098.892266988754, 'total_duration': 29257.262996196747, 'accumulated_submission_time': 27098.892266988754, 'accumulated_eval_time': 2145.9310023784637, 'accumulated_logging_time': 6.290436744689941, 'global_step': 59736, 'preemption_count': 0}), (60098, {'train/accuracy': 0.3938934803009033, 'train/loss': 2.868344306945801, 'validation/accuracy': 0.3738199770450592, 'validation/loss': 3.0190305709838867, 'validation/num_examples': 50000, 'test/accuracy': 0.2800000011920929, 'test/loss': 3.819106340408325, 'test/num_examples': 10000, 'score': 27609.56351208687, 'total_duration': 29802.338884830475, 'accumulated_submission_time': 27609.56351208687, 'accumulated_eval_time': 2180.184112548828, 'accumulated_logging_time': 6.402388095855713, 'global_step': 60098, 'preemption_count': 0}), (60468, {'train/accuracy': 0.24487802386283875, 'train/loss': 4.541502475738525, 'validation/accuracy': 0.2254999876022339, 'validation/loss': 4.7774224281311035, 'validation/num_examples': 50000, 'test/accuracy': 0.16920000314712524, 'test/loss': 5.528901100158691, 'test/num_examples': 10000, 'score': 28119.510808944702, 'total_duration': 30344.65024638176, 'accumulated_submission_time': 28119.510808944702, 'accumulated_eval_time': 2212.408802509308, 'accumulated_logging_time': 6.499581336975098, 'global_step': 60468, 'preemption_count': 0}), (60838, {'train/accuracy': 0.30966994166374207, 'train/loss': 3.4514143466949463, 'validation/accuracy': 0.28968000411987305, 'validation/loss': 3.607893943786621, 'validation/num_examples': 50000, 'test/accuracy': 0.21710000932216644, 'test/loss': 4.354828834533691, 'test/num_examples': 10000, 'score': 28630.59779214859, 'total_duration': 30893.00526356697, 'accumulated_submission_time': 28630.59779214859, 'accumulated_eval_time': 2249.5333909988403, 'accumulated_logging_time': 6.602743864059448, 'global_step': 60838, 'preemption_count': 0}), (61016, {'train/accuracy': 0.11898118257522583, 'train/loss': 5.718156337738037, 'validation/accuracy': 0.1084199994802475, 'validation/loss': 5.873864650726318, 'validation/num_examples': 50000, 'test/accuracy': 0.08230000734329224, 'test/loss': 6.502752304077148, 'test/num_examples': 10000, 'score': 29143.32395339012, 'total_duration': 31442.119812488556, 'accumulated_submission_time': 29143.32395339012, 'accumulated_eval_time': 2285.82221865654, 'accumulated_logging_time': 6.683839321136475, 'global_step': 61016, 'preemption_count': 0}), (61140, {'train/accuracy': 0.32894212007522583, 'train/loss': 3.3118197917938232, 'validation/accuracy': 0.31200000643730164, 'validation/loss': 3.466230869293213, 'validation/num_examples': 50000, 'test/accuracy': 0.22960001230239868, 'test/loss': 4.346792221069336, 'test/num_examples': 10000, 'score': 29655.156173706055, 'total_duration': 31990.78170990944, 'accumulated_submission_time': 29655.156173706055, 'accumulated_eval_time': 2322.611976623535, 'accumulated_logging_time': 6.711446523666382, 'global_step': 61140, 'preemption_count': 0}), (61264, {'train/accuracy': 0.2950015962123871, 'train/loss': 3.533273458480835, 'validation/accuracy': 0.2795400023460388, 'validation/loss': 3.654658079147339, 'validation/num_examples': 50000, 'test/accuracy': 0.203000009059906, 'test/loss': 4.428502559661865, 'test/num_examples': 10000, 'score': 30167.671818971634, 'total_duration': 32533.172789096832, 'accumulated_submission_time': 30167.671818971634, 'accumulated_eval_time': 2352.446312904358, 'accumulated_logging_time': 6.739759206771851, 'global_step': 61264, 'preemption_count': 0}), (61388, {'train/accuracy': 0.22668208181858063, 'train/loss': 4.139625072479248, 'validation/accuracy': 0.2146800011396408, 'validation/loss': 4.234930038452148, 'validation/num_examples': 50000, 'test/accuracy': 0.1600000113248825, 'test/loss': 4.81683874130249, 'test/num_examples': 10000, 'score': 30678.638236761093, 'total_duration': 33080.8965845108, 'accumulated_submission_time': 30678.638236761093, 'accumulated_eval_time': 2389.1207072734833, 'accumulated_logging_time': 6.80924129486084, 'global_step': 61388, 'preemption_count': 0}), (61511, {'train/accuracy': 0.25924745202064514, 'train/loss': 3.8297600746154785, 'validation/accuracy': 0.24361999332904816, 'validation/loss': 3.985414981842041, 'validation/num_examples': 50000, 'test/accuracy': 0.17750000953674316, 'test/loss': 4.7761054039001465, 'test/num_examples': 10000, 'score': 31189.044130563736, 'total_duration': 33626.085010290146, 'accumulated_submission_time': 31189.044130563736, 'accumulated_eval_time': 2423.8543360233307, 'accumulated_logging_time': 6.844536066055298, 'global_step': 61511, 'preemption_count': 0}), (61637, {'train/accuracy': 0.22939252853393555, 'train/loss': 4.198792934417725, 'validation/accuracy': 0.21447999775409698, 'validation/loss': 4.342655658721924, 'validation/num_examples': 50000, 'test/accuracy': 0.16290000081062317, 'test/loss': 4.998435974121094, 'test/num_examples': 10000, 'score': 31703.03137922287, 'total_duration': 34181.86692881584, 'accumulated_submission_time': 31703.03137922287, 'accumulated_eval_time': 2465.5893456935883, 'accumulated_logging_time': 6.891858339309692, 'global_step': 61637, 'preemption_count': 0}), (61762, {'train/accuracy': 0.23648755252361298, 'train/loss': 4.070553779602051, 'validation/accuracy': 0.23157998919487, 'validation/loss': 4.162125587463379, 'validation/num_examples': 50000, 'test/accuracy': 0.17310000956058502, 'test/loss': 4.836390495300293, 'test/num_examples': 10000, 'score': 32214.105890989304, 'total_duration': 34728.841467380524, 'accumulated_submission_time': 32214.105890989304, 'accumulated_eval_time': 2501.408307790756, 'accumulated_logging_time': 6.9596168994903564, 'global_step': 61762, 'preemption_count': 0}), (61887, {'train/accuracy': 0.21016022562980652, 'train/loss': 4.872672080993652, 'validation/accuracy': 0.19581998884677887, 'validation/loss': 4.975985050201416, 'validation/num_examples': 50000, 'test/accuracy': 0.14000000059604645, 'test/loss': 5.754014492034912, 'test/num_examples': 10000, 'score': 32727.940685272217, 'total_duration': 35275.90333819389, 'accumulated_submission_time': 32727.940685272217, 'accumulated_eval_time': 2534.5985140800476, 'accumulated_logging_time': 6.983231067657471, 'global_step': 61887, 'preemption_count': 0}), (62012, {'train/accuracy': 0.17582111060619354, 'train/loss': 5.048666954040527, 'validation/accuracy': 0.15717999637126923, 'validation/loss': 5.288033485412598, 'validation/num_examples': 50000, 'test/accuracy': 0.11220000684261322, 'test/loss': 6.034156322479248, 'test/num_examples': 10000, 'score': 33241.09929347038, 'total_duration': 35824.561291217804, 'accumulated_submission_time': 33241.09929347038, 'accumulated_eval_time': 2570.0564539432526, 'accumulated_logging_time': 7.010974645614624, 'global_step': 62012, 'preemption_count': 0}), (62138, {'train/accuracy': 0.20623405277729034, 'train/loss': 4.628417015075684, 'validation/accuracy': 0.18199999630451202, 'validation/loss': 4.891508102416992, 'validation/num_examples': 50000, 'test/accuracy': 0.13990001380443573, 'test/loss': 5.502305507659912, 'test/num_examples': 10000, 'score': 33752.73250865936, 'total_duration': 36373.163524866104, 'accumulated_submission_time': 33752.73250865936, 'accumulated_eval_time': 2606.987471103668, 'accumulated_logging_time': 7.036464691162109, 'global_step': 62138, 'preemption_count': 0}), (62482, {'train/accuracy': 0.30592313408851624, 'train/loss': 3.4500646591186523, 'validation/accuracy': 0.2951599955558777, 'validation/loss': 3.5575101375579834, 'validation/num_examples': 50000, 'test/accuracy': 0.2087000161409378, 'test/loss': 4.413336753845215, 'test/num_examples': 10000, 'score': 34263.46985626221, 'total_duration': 36917.32784366608, 'accumulated_submission_time': 34263.46985626221, 'accumulated_eval_time': 2640.348819732666, 'accumulated_logging_time': 7.0649003982543945, 'global_step': 62482, 'preemption_count': 0}), (62850, {'train/accuracy': 0.34482619166374207, 'train/loss': 3.135535955429077, 'validation/accuracy': 0.32453998923301697, 'validation/loss': 3.2905848026275635, 'validation/num_examples': 50000, 'test/accuracy': 0.2410000115633011, 'test/loss': 4.088424205780029, 'test/num_examples': 10000, 'score': 34774.485895872116, 'total_duration': 37460.55626320839, 'accumulated_submission_time': 34774.485895872116, 'accumulated_eval_time': 2672.4375896453857, 'accumulated_logging_time': 7.147935152053833, 'global_step': 62850, 'preemption_count': 0}), (63208, {'train/accuracy': 0.36485570669174194, 'train/loss': 3.080524206161499, 'validation/accuracy': 0.3482399880886078, 'validation/loss': 3.221979856491089, 'validation/num_examples': 50000, 'test/accuracy': 0.26440000534057617, 'test/loss': 4.006695747375488, 'test/num_examples': 10000, 'score': 35271.52393269539, 'total_duration': 38001.63388586044, 'accumulated_submission_time': 35271.52393269539, 'accumulated_eval_time': 2703.3347663879395, 'accumulated_logging_time': 20.2498722076416, 'global_step': 63208, 'preemption_count': 0}), (63458, {'train/accuracy': 0.35146284103393555, 'train/loss': 3.148709297180176, 'validation/accuracy': 0.32161998748779297, 'validation/loss': 3.3509581089019775, 'validation/num_examples': 50000, 'test/accuracy': 0.2451000064611435, 'test/loss': 4.087538719177246, 'test/num_examples': 10000, 'score': 35784.15509390831, 'total_duration': 38552.86743712425, 'accumulated_submission_time': 35784.15509390831, 'accumulated_eval_time': 2741.8539640903473, 'accumulated_logging_time': 20.306491374969482, 'global_step': 63458, 'preemption_count': 0}), (63583, {'train/accuracy': 0.3989357352256775, 'train/loss': 2.81195068359375, 'validation/accuracy': 0.342739999294281, 'validation/loss': 3.1508748531341553, 'validation/num_examples': 50000, 'test/accuracy': 0.2596000134944916, 'test/loss': 3.8776979446411133, 'test/num_examples': 10000, 'score': 36298.05703806877, 'total_duration': 39107.223012924194, 'accumulated_submission_time': 36298.05703806877, 'accumulated_eval_time': 2782.235025167465, 'accumulated_logging_time': 20.365324020385742, 'global_step': 63583, 'preemption_count': 0}), (63707, {'train/accuracy': 0.32649075984954834, 'train/loss': 3.299198627471924, 'validation/accuracy': 0.2880599796772003, 'validation/loss': 3.576810598373413, 'validation/num_examples': 50000, 'test/accuracy': 0.2143000066280365, 'test/loss': 4.212899208068848, 'test/num_examples': 10000, 'score': 36808.17079281807, 'total_duration': 39655.49816226959, 'accumulated_submission_time': 36808.17079281807, 'accumulated_eval_time': 2820.3287954330444, 'accumulated_logging_time': 20.419416904449463, 'global_step': 63707, 'preemption_count': 0}), (63830, {'train/accuracy': 0.11778539419174194, 'train/loss': 5.5755085945129395, 'validation/accuracy': 0.10815999656915665, 'validation/loss': 5.711451053619385, 'validation/num_examples': 50000, 'test/accuracy': 0.07670000195503235, 'test/loss': 6.375582218170166, 'test/num_examples': 10000, 'score': 37318.61796140671, 'total_duration': 40204.90491318703, 'accumulated_submission_time': 37318.61796140671, 'accumulated_eval_time': 2859.24599814415, 'accumulated_logging_time': 20.447150468826294, 'global_step': 63830, 'preemption_count': 0}), (63952, {'train/accuracy': 0.2541254758834839, 'train/loss': 3.9942924976348877, 'validation/accuracy': 0.23715999722480774, 'validation/loss': 4.153016090393066, 'validation/num_examples': 50000, 'test/accuracy': 0.16910000145435333, 'test/loss': 4.971282958984375, 'test/num_examples': 10000, 'score': 37828.72723984718, 'total_duration': 40750.031073093414, 'accumulated_submission_time': 37828.72723984718, 'accumulated_eval_time': 2894.1586589813232, 'accumulated_logging_time': 20.538596630096436, 'global_step': 63952, 'preemption_count': 0}), (64078, {'train/accuracy': 0.4133649468421936, 'train/loss': 2.6875948905944824, 'validation/accuracy': 0.37790000438690186, 'validation/loss': 2.9126312732696533, 'validation/num_examples': 50000, 'test/accuracy': 0.2930000126361847, 'test/loss': 3.635350227355957, 'test/num_examples': 10000, 'score': 38342.6691300869, 'total_duration': 41297.84662628174, 'accumulated_submission_time': 38342.6691300869, 'accumulated_eval_time': 2927.993425130844, 'accumulated_logging_time': 20.563955545425415, 'global_step': 64078, 'preemption_count': 0}), (64204, {'train/accuracy': 0.2576729953289032, 'train/loss': 3.983649253845215, 'validation/accuracy': 0.2425599992275238, 'validation/loss': 4.106703758239746, 'validation/num_examples': 50000, 'test/accuracy': 0.1714000105857849, 'test/loss': 4.9204936027526855, 'test/num_examples': 10000, 'score': 38856.37526202202, 'total_duration': 41846.759601831436, 'accumulated_submission_time': 38856.37526202202, 'accumulated_eval_time': 2963.1579632759094, 'accumulated_logging_time': 20.592565536499023, 'global_step': 64204, 'preemption_count': 0}), (64328, {'train/accuracy': 0.23176418244838715, 'train/loss': 4.285702228546143, 'validation/accuracy': 0.21781998872756958, 'validation/loss': 4.485436916351318, 'validation/num_examples': 50000, 'test/accuracy': 0.1728000044822693, 'test/loss': 5.184473991394043, 'test/num_examples': 10000, 'score': 39368.5624165535, 'total_duration': 42395.243967056274, 'accumulated_submission_time': 39368.5624165535, 'accumulated_eval_time': 2999.4131710529327, 'accumulated_logging_time': 20.621379613876343, 'global_step': 64328, 'preemption_count': 0}), (64456, {'train/accuracy': 0.34448739886283875, 'train/loss': 3.149181365966797, 'validation/accuracy': 0.32249999046325684, 'validation/loss': 3.318387985229492, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 4.10207462310791, 'test/num_examples': 10000, 'score': 39880.809448719025, 'total_duration': 42941.59438085556, 'accumulated_submission_time': 39880.809448719025, 'accumulated_eval_time': 3032.1681022644043, 'accumulated_logging_time': 21.95696473121643, 'global_step': 64456, 'preemption_count': 0}), (64583, {'train/accuracy': 0.31022799015045166, 'train/loss': 3.421186685562134, 'validation/accuracy': 0.28963997960090637, 'validation/loss': 3.599924087524414, 'validation/num_examples': 50000, 'test/accuracy': 0.21710000932216644, 'test/loss': 4.373508453369141, 'test/num_examples': 10000, 'score': 40393.38170218468, 'total_duration': 43485.57009410858, 'accumulated_submission_time': 40393.38170218468, 'accumulated_eval_time': 3063.5304415225983, 'accumulated_logging_time': 21.982789754867554, 'global_step': 64583, 'preemption_count': 0}), (64861, {'train/accuracy': 0.35724249482154846, 'train/loss': 3.256249189376831, 'validation/accuracy': 0.3403399884700775, 'validation/loss': 3.3704471588134766, 'validation/num_examples': 50000, 'test/accuracy': 0.25130000710487366, 'test/loss': 4.261990547180176, 'test/num_examples': 10000, 'score': 40903.57956242561, 'total_duration': 44026.867421627045, 'accumulated_submission_time': 40903.57956242561, 'accumulated_eval_time': 3094.556693792343, 'accumulated_logging_time': 22.025065660476685, 'global_step': 64861, 'preemption_count': 0}), (65341, {'train/accuracy': 0.3435506820678711, 'train/loss': 3.2389893531799316, 'validation/accuracy': 0.3141999840736389, 'validation/loss': 3.526843309402466, 'validation/num_examples': 50000, 'test/accuracy': 0.22300000488758087, 'test/loss': 4.413634777069092, 'test/num_examples': 10000, 'score': 41414.20679354668, 'total_duration': 44570.19377946854, 'accumulated_submission_time': 41414.20679354668, 'accumulated_eval_time': 3127.1753418445587, 'accumulated_logging_time': 22.049418926239014, 'global_step': 65341, 'preemption_count': 0}), (65829, {'train/accuracy': 0.34197622537612915, 'train/loss': 3.1787047386169434, 'validation/accuracy': 0.32311999797821045, 'validation/loss': 3.3160715103149414, 'validation/num_examples': 50000, 'test/accuracy': 0.2427000105381012, 'test/loss': 4.056772708892822, 'test/num_examples': 10000, 'score': 41924.55049228668, 'total_duration': 45113.42751765251, 'accumulated_submission_time': 41924.55049228668, 'accumulated_eval_time': 3159.9411783218384, 'accumulated_logging_time': 22.116044521331787, 'global_step': 65829, 'preemption_count': 0}), (66312, {'train/accuracy': 0.3198740482330322, 'train/loss': 3.409698486328125, 'validation/accuracy': 0.30747997760772705, 'validation/loss': 3.55702543258667, 'validation/num_examples': 50000, 'test/accuracy': 0.21800000965595245, 'test/loss': 4.414551258087158, 'test/num_examples': 10000, 'score': 42434.50054335594, 'total_duration': 45656.874467372894, 'accumulated_submission_time': 42434.50054335594, 'accumulated_eval_time': 3193.345922231674, 'accumulated_logging_time': 22.153114080429077, 'global_step': 66312, 'preemption_count': 0}), (66798, {'train/accuracy': 0.34036192297935486, 'train/loss': 3.174377679824829, 'validation/accuracy': 0.31167998909950256, 'validation/loss': 3.406909704208374, 'validation/num_examples': 50000, 'test/accuracy': 0.23250001668930054, 'test/loss': 4.173073768615723, 'test/num_examples': 10000, 'score': 42944.424287080765, 'total_duration': 46201.377992391586, 'accumulated_submission_time': 42944.424287080765, 'accumulated_eval_time': 3227.8255751132965, 'accumulated_logging_time': 22.19819140434265, 'global_step': 66798, 'preemption_count': 0}), (67284, {'train/accuracy': 0.3095105290412903, 'train/loss': 3.5035314559936523, 'validation/accuracy': 0.2874999940395355, 'validation/loss': 3.653413772583008, 'validation/num_examples': 50000, 'test/accuracy': 0.21630001068115234, 'test/loss': 4.420182228088379, 'test/num_examples': 10000, 'score': 43454.29529118538, 'total_duration': 46741.80951166153, 'accumulated_submission_time': 43454.29529118538, 'accumulated_eval_time': 3258.2451775074005, 'accumulated_logging_time': 22.28388738632202, 'global_step': 67284, 'preemption_count': 0}), (67768, {'train/accuracy': 0.2647082209587097, 'train/loss': 4.042666435241699, 'validation/accuracy': 0.2523399889469147, 'validation/loss': 4.150810718536377, 'validation/num_examples': 50000, 'test/accuracy': 0.1868000030517578, 'test/loss': 5.01791524887085, 'test/num_examples': 10000, 'score': 43965.13685297966, 'total_duration': 47285.313568115234, 'accumulated_submission_time': 43965.13685297966, 'accumulated_eval_time': 3290.7507905960083, 'accumulated_logging_time': 22.386687755584717, 'global_step': 67768, 'preemption_count': 0}), (68252, {'train/accuracy': 0.3085339665412903, 'train/loss': 3.499007225036621, 'validation/accuracy': 0.2822200059890747, 'validation/loss': 3.7292985916137695, 'validation/num_examples': 50000, 'test/accuracy': 0.20590001344680786, 'test/loss': 4.527698040008545, 'test/num_examples': 10000, 'score': 44476.030771017075, 'total_duration': 47827.932343006134, 'accumulated_submission_time': 44476.030771017075, 'accumulated_eval_time': 3322.3324859142303, 'accumulated_logging_time': 22.474384784698486, 'global_step': 68252, 'preemption_count': 0}), (68738, {'train/accuracy': 0.16860650479793549, 'train/loss': 4.9217729568481445, 'validation/accuracy': 0.15981999039649963, 'validation/loss': 5.040174961090088, 'validation/num_examples': 50000, 'test/accuracy': 0.11860000342130661, 'test/loss': 5.589081764221191, 'test/num_examples': 10000, 'score': 44986.87431430817, 'total_duration': 48370.787288188934, 'accumulated_submission_time': 44986.87431430817, 'accumulated_eval_time': 3354.216007709503, 'accumulated_logging_time': 22.548347234725952, 'global_step': 68738, 'preemption_count': 0}), (69223, {'train/accuracy': 0.18329480290412903, 'train/loss': 4.912752151489258, 'validation/accuracy': 0.17749999463558197, 'validation/loss': 5.034994602203369, 'validation/num_examples': 50000, 'test/accuracy': 0.133200004696846, 'test/loss': 5.7967071533203125, 'test/num_examples': 10000, 'score': 45497.40640044212, 'total_duration': 48912.247970342636, 'accumulated_submission_time': 45497.40640044212, 'accumulated_eval_time': 3385.005548477173, 'accumulated_logging_time': 22.632166624069214, 'global_step': 69223, 'preemption_count': 0}), (69709, {'train/accuracy': 0.3179408311843872, 'train/loss': 3.543314218521118, 'validation/accuracy': 0.29203999042510986, 'validation/loss': 3.7925686836242676, 'validation/num_examples': 50000, 'test/accuracy': 0.22930000722408295, 'test/loss': 4.426686763763428, 'test/num_examples': 10000, 'score': 46007.61543250084, 'total_duration': 49455.97787928581, 'accumulated_submission_time': 46007.61543250084, 'accumulated_eval_time': 3418.4306683540344, 'accumulated_logging_time': 22.672513723373413, 'global_step': 69709, 'preemption_count': 0}), (70189, {'train/accuracy': 0.4380779564380646, 'train/loss': 2.5422377586364746, 'validation/accuracy': 0.41359999775886536, 'validation/loss': 2.7150917053222656, 'validation/num_examples': 50000, 'test/accuracy': 0.303600013256073, 'test/loss': 3.5382556915283203, 'test/num_examples': 10000, 'score': 46517.926050424576, 'total_duration': 49997.02480340004, 'accumulated_submission_time': 46517.926050424576, 'accumulated_eval_time': 3449.0443818569183, 'accumulated_logging_time': 22.73910689353943, 'global_step': 70189, 'preemption_count': 0}), (70419, {'train/accuracy': 0.4275151491165161, 'train/loss': 2.607390880584717, 'validation/accuracy': 0.39800000190734863, 'validation/loss': 2.7952003479003906, 'validation/num_examples': 50000, 'test/accuracy': 0.301800012588501, 'test/loss': 3.5714099407196045, 'test/num_examples': 10000, 'score': 47029.08199071884, 'total_duration': 50544.26266884804, 'accumulated_submission_time': 47029.08199071884, 'accumulated_eval_time': 3485.0438079833984, 'accumulated_logging_time': 22.79692816734314, 'global_step': 70419, 'preemption_count': 0}), (70543, {'train/accuracy': 0.31527024507522583, 'train/loss': 3.4249825477600098, 'validation/accuracy': 0.2894199788570404, 'validation/loss': 3.62431001663208, 'validation/num_examples': 50000, 'test/accuracy': 0.22350001335144043, 'test/loss': 4.273886680603027, 'test/num_examples': 10000, 'score': 47539.701190948486, 'total_duration': 51086.46629238129, 'accumulated_submission_time': 47539.701190948486, 'accumulated_eval_time': 3516.588651895523, 'accumulated_logging_time': 22.822726726531982, 'global_step': 70543, 'preemption_count': 0}), (70669, {'train/accuracy': 0.21430563926696777, 'train/loss': 4.538547039031982, 'validation/accuracy': 0.20347999036312103, 'validation/loss': 4.599852085113525, 'validation/num_examples': 50000, 'test/accuracy': 0.14420001208782196, 'test/loss': 5.534135341644287, 'test/num_examples': 10000, 'score': 48050.213551044464, 'total_duration': 51636.363298892975, 'accumulated_submission_time': 48050.213551044464, 'accumulated_eval_time': 3555.933057785034, 'accumulated_logging_time': 22.84971785545349, 'global_step': 70669, 'preemption_count': 0}), (70793, {'train/accuracy': 0.17984694242477417, 'train/loss': 5.031060218811035, 'validation/accuracy': 0.16103999316692352, 'validation/loss': 5.190260887145996, 'validation/num_examples': 50000, 'test/accuracy': 0.1257999986410141, 'test/loss': 5.690762996673584, 'test/num_examples': 10000, 'score': 48563.416789770126, 'total_duration': 52184.99644088745, 'accumulated_submission_time': 48563.416789770126, 'accumulated_eval_time': 3591.3054065704346, 'accumulated_logging_time': 22.893696784973145, 'global_step': 70793, 'preemption_count': 0}), (70918, {'train/accuracy': 0.36505499482154846, 'train/loss': 2.9559011459350586, 'validation/accuracy': 0.3509199917316437, 'validation/loss': 3.0791754722595215, 'validation/num_examples': 50000, 'test/accuracy': 0.25, 'test/loss': 3.983215093612671, 'test/num_examples': 10000, 'score': 49076.571224689484, 'total_duration': 52730.00550556183, 'accumulated_submission_time': 49076.571224689484, 'accumulated_eval_time': 3623.1175224781036, 'accumulated_logging_time': 22.923500299453735, 'global_step': 70918, 'preemption_count': 0}), (71044, {'train/accuracy': 0.21867027878761292, 'train/loss': 4.413930416107178, 'validation/accuracy': 0.20603999495506287, 'validation/loss': 4.537769317626953, 'validation/num_examples': 50000, 'test/accuracy': 0.15280000865459442, 'test/loss': 5.239365100860596, 'test/num_examples': 10000, 'score': 49588.523330926895, 'total_duration': 53277.87161183357, 'accumulated_submission_time': 49588.523330926895, 'accumulated_eval_time': 3658.9911205768585, 'accumulated_logging_time': 22.949737310409546, 'global_step': 71044, 'preemption_count': 0}), (71169, {'train/accuracy': 0.32057157158851624, 'train/loss': 3.423633337020874, 'validation/accuracy': 0.2821599841117859, 'validation/loss': 3.7573115825653076, 'validation/num_examples': 50000, 'test/accuracy': 0.2078000158071518, 'test/loss': 4.524680137634277, 'test/num_examples': 10000, 'score': 50100.62102603912, 'total_duration': 53825.17532014847, 'accumulated_submission_time': 50100.62102603912, 'accumulated_eval_time': 3694.1517481803894, 'accumulated_logging_time': 22.98146414756775, 'global_step': 71169, 'preemption_count': 0}), (71294, {'train/accuracy': 0.37147241830825806, 'train/loss': 2.9358925819396973, 'validation/accuracy': 0.33932000398635864, 'validation/loss': 3.1985857486724854, 'validation/num_examples': 50000, 'test/accuracy': 0.26010000705718994, 'test/loss': 3.9811501502990723, 'test/num_examples': 10000, 'score': 50611.71167373657, 'total_duration': 54368.27076411247, 'accumulated_submission_time': 50611.71167373657, 'accumulated_eval_time': 3726.1137471199036, 'accumulated_logging_time': 23.011420965194702, 'global_step': 71294, 'preemption_count': 0}), (71420, {'train/accuracy': 0.378627210855484, 'train/loss': 2.951539993286133, 'validation/accuracy': 0.34891998767852783, 'validation/loss': 3.20268177986145, 'validation/num_examples': 50000, 'test/accuracy': 0.25870001316070557, 'test/loss': 4.002931594848633, 'test/num_examples': 10000, 'score': 51122.83813548088, 'total_duration': 54911.224437475204, 'accumulated_submission_time': 51122.83813548088, 'accumulated_eval_time': 3757.887465238571, 'accumulated_logging_time': 23.05109143257141, 'global_step': 71420, 'preemption_count': 0}), (71546, {'train/accuracy': 0.417988657951355, 'train/loss': 2.722219944000244, 'validation/accuracy': 0.38439998030662537, 'validation/loss': 2.965337038040161, 'validation/num_examples': 50000, 'test/accuracy': 0.2882000207901001, 'test/loss': 3.747056484222412, 'test/num_examples': 10000, 'score': 51634.89011383057, 'total_duration': 55454.45736122131, 'accumulated_submission_time': 51634.89011383057, 'accumulated_eval_time': 3789.027185201645, 'accumulated_logging_time': 23.07946276664734, 'global_step': 71546, 'preemption_count': 0}), (71672, {'train/accuracy': 0.24326370656490326, 'train/loss': 4.042903423309326, 'validation/accuracy': 0.2270599901676178, 'validation/loss': 4.189541816711426, 'validation/num_examples': 50000, 'test/accuracy': 0.16840000450611115, 'test/loss': 4.919600963592529, 'test/num_examples': 10000, 'score': 52145.28388285637, 'total_duration': 55999.17945146561, 'accumulated_submission_time': 52145.28388285637, 'accumulated_eval_time': 3823.3153166770935, 'accumulated_logging_time': 23.10611057281494, 'global_step': 71672, 'preemption_count': 0}), (71798, {'train/accuracy': 0.3282645046710968, 'train/loss': 3.399852752685547, 'validation/accuracy': 0.3055199980735779, 'validation/loss': 3.5449717044830322, 'validation/num_examples': 50000, 'test/accuracy': 0.2232000082731247, 'test/loss': 4.391471862792969, 'test/num_examples': 10000, 'score': 52658.53072857857, 'total_duration': 56544.382128953934, 'accumulated_submission_time': 52658.53072857857, 'accumulated_eval_time': 3855.211621761322, 'accumulated_logging_time': 23.15228772163391, 'global_step': 71798, 'preemption_count': 0}), (71924, {'train/accuracy': 0.24071268737316132, 'train/loss': 4.292496681213379, 'validation/accuracy': 0.21753999590873718, 'validation/loss': 4.522789478302002, 'validation/num_examples': 50000, 'test/accuracy': 0.17260000109672546, 'test/loss': 5.19539737701416, 'test/num_examples': 10000, 'score': 53170.809384822845, 'total_duration': 57090.327771902084, 'accumulated_submission_time': 53170.809384822845, 'accumulated_eval_time': 3888.8387672901154, 'accumulated_logging_time': 23.178457021713257, 'global_step': 71924, 'preemption_count': 0}), (72050, {'train/accuracy': 0.15943877398967743, 'train/loss': 5.236638069152832, 'validation/accuracy': 0.15143999457359314, 'validation/loss': 5.319343566894531, 'validation/num_examples': 50000, 'test/accuracy': 0.11460000276565552, 'test/loss': 5.964763641357422, 'test/num_examples': 10000, 'score': 53683.269191265106, 'total_duration': 57634.75035023689, 'accumulated_submission_time': 53683.269191265106, 'accumulated_eval_time': 3920.7612686157227, 'accumulated_logging_time': 23.206202507019043, 'global_step': 72050, 'preemption_count': 0}), (72176, {'train/accuracy': 0.1287667453289032, 'train/loss': 5.811068058013916, 'validation/accuracy': 0.124719999730587, 'validation/loss': 5.818170070648193, 'validation/num_examples': 50000, 'test/accuracy': 0.08990000188350677, 'test/loss': 6.3484015464782715, 'test/num_examples': 10000, 'score': 54193.23250865936, 'total_duration': 58177.32654285431, 'accumulated_submission_time': 54193.23250865936, 'accumulated_eval_time': 3953.3109164237976, 'accumulated_logging_time': 23.255741357803345, 'global_step': 72176, 'preemption_count': 0}), (72303, {'train/accuracy': 0.3047672212123871, 'train/loss': 3.601977586746216, 'validation/accuracy': 0.2881999909877777, 'validation/loss': 3.759565830230713, 'validation/num_examples': 50000, 'test/accuracy': 0.22230000793933868, 'test/loss': 4.540050983428955, 'test/num_examples': 10000, 'score': 54707.049211740494, 'total_duration': 58723.331384420395, 'accumulated_submission_time': 54707.049211740494, 'accumulated_eval_time': 3985.4384474754333, 'accumulated_logging_time': 23.30252504348755, 'global_step': 72303, 'preemption_count': 0}), (72429, {'train/accuracy': 0.23636798560619354, 'train/loss': 4.132009506225586, 'validation/accuracy': 0.21319998800754547, 'validation/loss': 4.33515739440918, 'validation/num_examples': 50000, 'test/accuracy': 0.15980000793933868, 'test/loss': 4.946207046508789, 'test/num_examples': 10000, 'score': 55219.61724591255, 'total_duration': 59268.94351649284, 'accumulated_submission_time': 55219.61724591255, 'accumulated_eval_time': 4018.442964076996, 'accumulated_logging_time': 23.328914642333984, 'global_step': 72429, 'preemption_count': 0}), (72573, {'train/accuracy': 0.3298588991165161, 'train/loss': 3.3318350315093994, 'validation/accuracy': 0.31158000230789185, 'validation/loss': 3.4608922004699707, 'validation/num_examples': 50000, 'test/accuracy': 0.22660000622272491, 'test/loss': 4.267693996429443, 'test/num_examples': 10000, 'score': 55729.69266605377, 'total_duration': 59810.38313770294, 'accumulated_submission_time': 55729.69266605377, 'accumulated_eval_time': 4049.7649302482605, 'accumulated_logging_time': 23.355398178100586, 'global_step': 72573, 'preemption_count': 0}), (72941, {'train/accuracy': 0.4194834232330322, 'train/loss': 2.6997995376586914, 'validation/accuracy': 0.3963799774646759, 'validation/loss': 2.849249839782715, 'validation/num_examples': 50000, 'test/accuracy': 0.29200002551078796, 'test/loss': 3.7030029296875, 'test/num_examples': 10000, 'score': 56240.83129096031, 'total_duration': 60352.4717798233, 'accumulated_submission_time': 56240.83129096031, 'accumulated_eval_time': 4080.646626472473, 'accumulated_logging_time': 23.381823539733887, 'global_step': 72941, 'preemption_count': 0}), (73312, {'train/accuracy': 0.2647082209587097, 'train/loss': 3.9943199157714844, 'validation/accuracy': 0.2400199919939041, 'validation/loss': 4.203547477722168, 'validation/num_examples': 50000, 'test/accuracy': 0.1811000108718872, 'test/loss': 4.935858726501465, 'test/num_examples': 10000, 'score': 56751.74811768532, 'total_duration': 60896.15454006195, 'accumulated_submission_time': 56751.74811768532, 'accumulated_eval_time': 4113.324114322662, 'accumulated_logging_time': 23.429174423217773, 'global_step': 73312, 'preemption_count': 0}), (73680, {'train/accuracy': 0.3001634180545807, 'train/loss': 3.525435447692871, 'validation/accuracy': 0.2801399827003479, 'validation/loss': 3.6794707775115967, 'validation/num_examples': 50000, 'test/accuracy': 0.210300013422966, 'test/loss': 4.376433372497559, 'test/num_examples': 10000, 'score': 57262.63800191879, 'total_duration': 61438.46275138855, 'accumulated_submission_time': 57262.63800191879, 'accumulated_eval_time': 4144.638286113739, 'accumulated_logging_time': 23.492982149124146, 'global_step': 73680, 'preemption_count': 0}), (74048, {'train/accuracy': 0.3238799273967743, 'train/loss': 3.609135150909424, 'validation/accuracy': 0.30115997791290283, 'validation/loss': 3.7780611515045166, 'validation/num_examples': 50000, 'test/accuracy': 0.21480001509189606, 'test/loss': 4.721311569213867, 'test/num_examples': 10000, 'score': 57773.43185329437, 'total_duration': 61982.10466837883, 'accumulated_submission_time': 57773.43185329437, 'accumulated_eval_time': 4177.329257965088, 'accumulated_logging_time': 23.609025955200195, 'global_step': 74048, 'preemption_count': 0}), (74417, {'train/accuracy': 0.38960856199264526, 'train/loss': 2.8841300010681152, 'validation/accuracy': 0.35599997639656067, 'validation/loss': 3.1400105953216553, 'validation/num_examples': 50000, 'test/accuracy': 0.2564000189304352, 'test/loss': 4.028597354888916, 'test/num_examples': 10000, 'score': 58284.013051986694, 'total_duration': 62525.00703930855, 'accumulated_submission_time': 58284.013051986694, 'accumulated_eval_time': 4209.555104494095, 'accumulated_logging_time': 23.664847135543823, 'global_step': 74417, 'preemption_count': 0}), (74788, {'train/accuracy': 0.23208306729793549, 'train/loss': 4.297539710998535, 'validation/accuracy': 0.21371999382972717, 'validation/loss': 4.47291374206543, 'validation/num_examples': 50000, 'test/accuracy': 0.16350001096725464, 'test/loss': 5.1035542488098145, 'test/num_examples': 10000, 'score': 58794.092004299164, 'total_duration': 63067.98567914963, 'accumulated_submission_time': 58794.092004299164, 'accumulated_eval_time': 4242.298907518387, 'accumulated_logging_time': 23.77896547317505, 'global_step': 74788, 'preemption_count': 0}), (75157, {'train/accuracy': 0.41976243257522583, 'train/loss': 2.6942365169525146, 'validation/accuracy': 0.3905999958515167, 'validation/loss': 2.8923380374908447, 'validation/num_examples': 50000, 'test/accuracy': 0.3020000159740448, 'test/loss': 3.6271145343780518, 'test/num_examples': 10000, 'score': 59304.58326411247, 'total_duration': 63611.78315758705, 'accumulated_submission_time': 59304.58326411247, 'accumulated_eval_time': 4275.51393532753, 'accumulated_logging_time': 23.829071521759033, 'global_step': 75157, 'preemption_count': 0}), (75527, {'train/accuracy': 0.22761878371238708, 'train/loss': 4.414113521575928, 'validation/accuracy': 0.21965999901294708, 'validation/loss': 4.435965061187744, 'validation/num_examples': 50000, 'test/accuracy': 0.14820000529289246, 'test/loss': 5.41902494430542, 'test/num_examples': 10000, 'score': 59815.80926847458, 'total_duration': 64154.78731274605, 'accumulated_submission_time': 59815.80926847458, 'accumulated_eval_time': 4307.169762849808, 'accumulated_logging_time': 23.910183906555176, 'global_step': 75527, 'preemption_count': 0}), (75896, {'train/accuracy': 0.1622488796710968, 'train/loss': 4.961266040802002, 'validation/accuracy': 0.14763998985290527, 'validation/loss': 5.162595272064209, 'validation/num_examples': 50000, 'test/accuracy': 0.10950000584125519, 'test/loss': 5.7070488929748535, 'test/num_examples': 10000, 'score': 60327.00371026993, 'total_duration': 64696.736402988434, 'accumulated_submission_time': 60327.00371026993, 'accumulated_eval_time': 4337.814252376556, 'accumulated_logging_time': 23.979315996170044, 'global_step': 75896, 'preemption_count': 0}), (76266, {'train/accuracy': 0.27638712525367737, 'train/loss': 3.7996342182159424, 'validation/accuracy': 0.2616199851036072, 'validation/loss': 3.94929575920105, 'validation/num_examples': 50000, 'test/accuracy': 0.19360001385211945, 'test/loss': 4.697072982788086, 'test/num_examples': 10000, 'score': 60837.44729280472, 'total_duration': 65239.18815803528, 'accumulated_submission_time': 60837.44729280472, 'accumulated_eval_time': 4369.718756437302, 'accumulated_logging_time': 24.042822122573853, 'global_step': 76266, 'preemption_count': 0}), (76633, {'train/accuracy': 0.36981824040412903, 'train/loss': 2.9992830753326416, 'validation/accuracy': 0.3512599766254425, 'validation/loss': 3.1624698638916016, 'validation/num_examples': 50000, 'test/accuracy': 0.2596000134944916, 'test/loss': 3.9757516384124756, 'test/num_examples': 10000, 'score': 61347.53976511955, 'total_duration': 65779.60022091866, 'accumulated_submission_time': 61347.53976511955, 'accumulated_eval_time': 4399.895801067352, 'accumulated_logging_time': 24.143393993377686, 'global_step': 76633, 'preemption_count': 0}), (77002, {'train/accuracy': 0.36535394191741943, 'train/loss': 3.113682270050049, 'validation/accuracy': 0.3440600037574768, 'validation/loss': 3.3049702644348145, 'validation/num_examples': 50000, 'test/accuracy': 0.2532000243663788, 'test/loss': 4.126364707946777, 'test/num_examples': 10000, 'score': 61858.23664236069, 'total_duration': 66321.29099607468, 'accumulated_submission_time': 61858.23664236069, 'accumulated_eval_time': 4430.72696685791, 'accumulated_logging_time': 24.263946294784546, 'global_step': 77002, 'preemption_count': 0}), (77371, {'train/accuracy': 0.34091994166374207, 'train/loss': 3.1935079097747803, 'validation/accuracy': 0.3044999837875366, 'validation/loss': 3.497637987136841, 'validation/num_examples': 50000, 'test/accuracy': 0.22770000994205475, 'test/loss': 4.2392258644104, 'test/num_examples': 10000, 'score': 62368.87443304062, 'total_duration': 66862.15390968323, 'accumulated_submission_time': 62368.87443304062, 'accumulated_eval_time': 4460.845156431198, 'accumulated_logging_time': 24.330100774765015, 'global_step': 77371, 'preemption_count': 0}), (77735, {'train/accuracy': 0.3770129084587097, 'train/loss': 2.9615671634674072, 'validation/accuracy': 0.3563399910926819, 'validation/loss': 3.1165823936462402, 'validation/num_examples': 50000, 'test/accuracy': 0.2630999982357025, 'test/loss': 3.92667555809021, 'test/num_examples': 10000, 'score': 62879.58875656128, 'total_duration': 67403.21505022049, 'accumulated_submission_time': 62879.58875656128, 'accumulated_eval_time': 4491.075173854828, 'accumulated_logging_time': 24.406001567840576, 'global_step': 77735, 'preemption_count': 0}), (78101, {'train/accuracy': 0.38368940353393555, 'train/loss': 2.9835238456726074, 'validation/accuracy': 0.36479997634887695, 'validation/loss': 3.0990540981292725, 'validation/num_examples': 50000, 'test/accuracy': 0.26100000739097595, 'test/loss': 3.994112730026245, 'test/num_examples': 10000, 'score': 63390.70301914215, 'total_duration': 67946.19958806038, 'accumulated_submission_time': 63390.70301914215, 'accumulated_eval_time': 4522.822245836258, 'accumulated_logging_time': 24.488791465759277, 'global_step': 78101, 'preemption_count': 0}), (78471, {'train/accuracy': 0.3400629758834839, 'train/loss': 3.285909414291382, 'validation/accuracy': 0.321260005235672, 'validation/loss': 3.401979684829712, 'validation/num_examples': 50000, 'test/accuracy': 0.23920001089572906, 'test/loss': 4.214888095855713, 'test/num_examples': 10000, 'score': 63901.27969813347, 'total_duration': 68489.20250988007, 'accumulated_submission_time': 63901.27969813347, 'accumulated_eval_time': 4555.152124881744, 'accumulated_logging_time': 24.544800519943237, 'global_step': 78471, 'preemption_count': 0}), (78839, {'train/accuracy': 0.35710299015045166, 'train/loss': 3.1243834495544434, 'validation/accuracy': 0.3249799907207489, 'validation/loss': 3.3870983123779297, 'validation/num_examples': 50000, 'test/accuracy': 0.24540001153945923, 'test/loss': 4.179543972015381, 'test/num_examples': 10000, 'score': 64412.32405233383, 'total_duration': 69031.8481874466, 'accumulated_submission_time': 64412.32405233383, 'accumulated_eval_time': 4586.568967819214, 'accumulated_logging_time': 24.686896562576294, 'global_step': 78839, 'preemption_count': 0}), (79208, {'train/accuracy': 0.3011200428009033, 'train/loss': 3.619767665863037, 'validation/accuracy': 0.2773599922657013, 'validation/loss': 3.804957151412964, 'validation/num_examples': 50000, 'test/accuracy': 0.20610001683235168, 'test/loss': 4.544443607330322, 'test/num_examples': 10000, 'score': 64923.132544994354, 'total_duration': 69573.7780251503, 'accumulated_submission_time': 64923.132544994354, 'accumulated_eval_time': 4617.572402238846, 'accumulated_logging_time': 24.7644202709198, 'global_step': 79208, 'preemption_count': 0}), (79577, {'train/accuracy': 0.14680324494838715, 'train/loss': 5.444582939147949, 'validation/accuracy': 0.13731999695301056, 'validation/loss': 5.536393642425537, 'validation/num_examples': 50000, 'test/accuracy': 0.09370000660419464, 'test/loss': 6.311817646026611, 'test/num_examples': 10000, 'score': 65433.63714981079, 'total_duration': 70118.39328622818, 'accumulated_submission_time': 65433.63714981079, 'accumulated_eval_time': 4651.561834812164, 'accumulated_logging_time': 24.8439519405365, 'global_step': 79577, 'preemption_count': 0}), (79946, {'train/accuracy': 0.25841039419174194, 'train/loss': 3.9799463748931885, 'validation/accuracy': 0.24701999127864838, 'validation/loss': 4.066519737243652, 'validation/num_examples': 50000, 'test/accuracy': 0.17430001497268677, 'test/loss': 4.86047887802124, 'test/num_examples': 10000, 'score': 65943.97033858299, 'total_duration': 70661.07703733444, 'accumulated_submission_time': 65943.97033858299, 'accumulated_eval_time': 4683.825715065002, 'accumulated_logging_time': 24.889578819274902, 'global_step': 79946, 'preemption_count': 0})], 'global_step': 80313}
I0308 05:24:20.726286 140413841560768 submission_runner.py:649] Timing: 66454.64535665512
I0308 05:24:20.726332 140413841560768 submission_runner.py:651] Total number of evals: 130
I0308 05:24:20.726362 140413841560768 submission_runner.py:652] ====================
I0308 05:24:20.726603 140413841560768 submission_runner.py:750] Final imagenet_resnet score: 3
