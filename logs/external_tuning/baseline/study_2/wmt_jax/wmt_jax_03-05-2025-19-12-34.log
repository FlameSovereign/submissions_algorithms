python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-1355215894 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-34.log
2025-03-05 19:12:35.494147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201955.516101       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201955.522549       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:42.209503 139629069661376 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax.
I0305 19:12:43.065225 139629069661376 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:43.068002 139629069661376 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:43.069512 139629069661376 submission_runner.py:606] Using RNG seed -1355215894
I0305 19:12:43.564394 139629069661376 submission_runner.py:615] --- Tuning run 3/5 ---
I0305 19:12:43.564557 139629069661376 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_3.
I0305 19:12:43.564774 139629069661376 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_3/hparams.json.
I0305 19:12:43.812834 139629069661376 submission_runner.py:218] Initializing dataset.
I0305 19:12:43.976867 139629069661376 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:43.982259 139629069661376 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:44.048293 139629069661376 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:45.417747 139629069661376 submission_runner.py:229] Initializing model.
I0305 19:13:24.276497 139629069661376 submission_runner.py:272] Initializing optimizer.
I0305 19:13:25.107575 139629069661376 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:25.107792 139629069661376 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:25.108686 139629069661376 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_3 with prefix checkpoint_
I0305 19:13:25.108781 139629069661376 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_3/meta_data_0.json.
I0305 19:13:25.108941 139629069661376 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:25.108989 139629069661376 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:25.300934 139629069661376 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_3/flags_0.json.
I0305 19:13:25.335275 139629069661376 submission_runner.py:337] Starting training loop.
I0305 19:13:50.764119 139492803401472 logging_writer.py:48] [0] global_step=0, grad_norm=6.396122932434082, loss=11.220495223999023
I0305 19:13:50.821864 139629069661376 spec.py:321] Evaluating on the training split.
I0305 19:13:50.823982 139629069661376 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:50.827459 139629069661376 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:50.858072 139629069661376 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:56.834328 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 19:19:00.963548 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 19:19:01.011994 139629069661376 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:01.017943 139629069661376 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:01.049165 139629069661376 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:06.122059 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 19:24:04.015477 139629069661376 spec.py:349] Evaluating on the test split.
I0305 19:24:04.017539 139629069661376 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:04.020930 139629069661376 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:04.050800 139629069661376 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:06.822297 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 19:29:04.713649 139629069661376 submission_runner.py:469] Time since start: 939.38s, 	Step: 1, 	{'train/accuracy': 0.0006161570199765265, 'train/loss': 11.21369457244873, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.19946575164795, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.181259155273438, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.486480712890625, 'total_duration': 939.3783233165741, 'accumulated_submission_time': 25.486480712890625, 'accumulated_eval_time': 913.8917396068573, 'accumulated_logging_time': 0}
I0305 19:29:04.720067 139485664716544 logging_writer.py:48] [1] accumulated_eval_time=913.892, accumulated_logging_time=0, accumulated_submission_time=25.4865, global_step=1, preemption_count=0, score=25.4865, test/accuracy=0.000718341, test/bleu=0, test/loss=11.1813, test/num_examples=3003, total_duration=939.378, train/accuracy=0.000616157, train/bleu=0, train/loss=11.2137, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.1995, validation/num_examples=3000
I0305 19:29:39.276130 139485656323840 logging_writer.py:48] [100] global_step=100, grad_norm=0.410114586353302, loss=8.729837417602539
I0305 19:30:13.797134 139485664716544 logging_writer.py:48] [200] global_step=200, grad_norm=0.17562370002269745, loss=8.332748413085938
I0305 19:30:48.269253 139485656323840 logging_writer.py:48] [300] global_step=300, grad_norm=0.1996089518070221, loss=8.054673194885254
I0305 19:31:22.836135 139485664716544 logging_writer.py:48] [400] global_step=400, grad_norm=0.2933734953403473, loss=7.595933437347412
I0305 19:31:57.416636 139485656323840 logging_writer.py:48] [500] global_step=500, grad_norm=0.3901461660861969, loss=7.222444534301758
I0305 19:32:31.987138 139485664716544 logging_writer.py:48] [600] global_step=600, grad_norm=0.6993204355239868, loss=6.895083904266357
I0305 19:33:06.599948 139485656323840 logging_writer.py:48] [700] global_step=700, grad_norm=0.47726303339004517, loss=6.588959693908691
I0305 19:33:41.185724 139485664716544 logging_writer.py:48] [800] global_step=800, grad_norm=0.6368511319160461, loss=6.329693794250488
I0305 19:34:15.766107 139485656323840 logging_writer.py:48] [900] global_step=900, grad_norm=0.7747336626052856, loss=6.1668829917907715
I0305 19:34:50.368896 139485664716544 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8075284957885742, loss=5.911403179168701
I0305 19:35:24.957609 139485656323840 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5882893800735474, loss=5.689502716064453
I0305 19:35:59.584327 139485664716544 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9362671375274658, loss=5.507638931274414
I0305 19:36:34.194841 139485656323840 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.8205429315567017, loss=5.3760600090026855
I0305 19:37:08.803415 139485664716544 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7531434893608093, loss=5.219480991363525
I0305 19:37:43.415433 139485656323840 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9989650845527649, loss=4.9756364822387695
I0305 19:38:18.004081 139485664716544 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6868440508842468, loss=4.834548473358154
I0305 19:38:52.638703 139485656323840 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.7326147556304932, loss=4.720107078552246
I0305 19:39:27.263679 139485664716544 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6902148723602295, loss=4.685697078704834
I0305 19:40:01.883617 139485656323840 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8494041562080383, loss=4.447993755340576
I0305 19:40:36.502094 139485664716544 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0338032245635986, loss=4.321579456329346
I0305 19:41:11.145477 139485656323840 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9639908671379089, loss=4.211021900177002
I0305 19:41:45.776977 139485664716544 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0059757232666016, loss=4.096301078796387
I0305 19:42:20.428082 139485656323840 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.0252366065979004, loss=3.9556124210357666
I0305 19:42:55.144227 139485664716544 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9504470825195312, loss=3.8741025924682617
I0305 19:43:04.844650 139629069661376 spec.py:321] Evaluating on the training split.
I0305 19:43:07.461773 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 19:46:50.315380 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 19:46:52.916032 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 19:50:14.860974 139629069661376 spec.py:349] Evaluating on the test split.
I0305 19:50:17.467157 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 19:53:27.875938 139629069661376 submission_runner.py:469] Time since start: 2402.54s, 	Step: 2429, 	{'train/accuracy': 0.4288758635520935, 'train/loss': 3.7705769538879395, 'train/bleu': 15.556901193491276, 'validation/accuracy': 0.4122438430786133, 'validation/loss': 3.9038379192352295, 'validation/bleu': 10.826612800200751, 'validation/num_examples': 3000, 'test/accuracy': 0.39797243475914, 'test/loss': 4.095420837402344, 'test/bleu': 8.810905968810859, 'test/num_examples': 3003, 'score': 865.4539229869843, 'total_duration': 2402.540598630905, 'accumulated_submission_time': 865.4539229869843, 'accumulated_eval_time': 1536.922968864441, 'accumulated_logging_time': 0.014814615249633789}
I0305 19:53:27.884687 139485362775808 logging_writer.py:48] [2429] accumulated_eval_time=1536.92, accumulated_logging_time=0.0148146, accumulated_submission_time=865.454, global_step=2429, preemption_count=0, score=865.454, test/accuracy=0.397972, test/bleu=8.81091, test/loss=4.09542, test/num_examples=3003, total_duration=2402.54, train/accuracy=0.428876, train/bleu=15.5569, train/loss=3.77058, validation/accuracy=0.412244, validation/bleu=10.8266, validation/loss=3.90384, validation/num_examples=3000
I0305 19:53:52.703370 139485354383104 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8349586129188538, loss=3.7860071659088135
I0305 19:54:27.236308 139485362775808 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9004833698272705, loss=3.669816017150879
I0305 19:55:01.857133 139485354383104 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7158071994781494, loss=3.6142990589141846
I0305 19:55:36.518109 139485362775808 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9815517663955688, loss=3.597071886062622
I0305 19:56:11.151529 139485354383104 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8991832137107849, loss=3.4737675189971924
I0305 19:56:45.844984 139485362775808 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.191799283027649, loss=3.4587504863739014
I0305 19:57:20.508085 139485354383104 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8660098314285278, loss=3.2928218841552734
I0305 19:57:55.162781 139485362775808 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.4710618257522583, loss=3.195967674255371
I0305 19:58:29.853634 139485354383104 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8275936841964722, loss=3.1171875
I0305 19:59:04.551387 139485362775808 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.790722131729126, loss=3.0840351581573486
I0305 19:59:39.192418 139485354383104 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.7700667381286621, loss=3.0304903984069824
I0305 20:00:13.836258 139485362775808 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6708385348320007, loss=3.057941198348999
I0305 20:00:48.496935 139485354383104 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6581811308860779, loss=2.9167990684509277
I0305 20:01:23.169864 139485362775808 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9607077240943909, loss=2.870311737060547
I0305 20:01:57.834505 139485354383104 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.756026566028595, loss=2.9627268314361572
I0305 20:02:32.469671 139485362775808 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6473998427391052, loss=2.826432704925537
I0305 20:03:07.128749 139485354383104 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6731814742088318, loss=2.802168130874634
I0305 20:03:41.788555 139485362775808 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6928496956825256, loss=2.8601319789886475
I0305 20:04:16.425552 139485354383104 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8540717959403992, loss=2.819350004196167
I0305 20:04:51.090767 139485362775808 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.729555070400238, loss=2.7928364276885986
I0305 20:05:25.713991 139485354383104 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6980520486831665, loss=2.7105274200439453
I0305 20:06:00.419159 139485362775808 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9505612850189209, loss=2.7061028480529785
I0305 20:06:35.057923 139485354383104 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.631261944770813, loss=2.8005199432373047
I0305 20:07:09.676847 139485362775808 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9103956818580627, loss=2.648972988128662
I0305 20:07:28.054857 139629069661376 spec.py:321] Evaluating on the training split.
I0305 20:07:30.682842 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:10:16.296666 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 20:10:18.908910 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:13:03.153756 139629069661376 spec.py:349] Evaluating on the test split.
I0305 20:13:05.769344 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:15:33.709180 139629069661376 submission_runner.py:469] Time since start: 3728.37s, 	Step: 4854, 	{'train/accuracy': 0.54524827003479, 'train/loss': 2.619779109954834, 'train/bleu': 25.27517063684932, 'validation/accuracy': 0.5513929724693298, 'validation/loss': 2.559023857116699, 'validation/bleu': 20.78442819427946, 'validation/num_examples': 3000, 'test/accuracy': 0.5495771169662476, 'test/loss': 2.5930731296539307, 'test/bleu': 19.194787685710793, 'test/num_examples': 3003, 'score': 1705.4736919403076, 'total_duration': 3728.3738470077515, 'accumulated_submission_time': 1705.4736919403076, 'accumulated_eval_time': 2022.577241897583, 'accumulated_logging_time': 0.03182816505432129}
I0305 20:15:33.718109 139485354383104 logging_writer.py:48] [4854] accumulated_eval_time=2022.58, accumulated_logging_time=0.0318282, accumulated_submission_time=1705.47, global_step=4854, preemption_count=0, score=1705.47, test/accuracy=0.549577, test/bleu=19.1948, test/loss=2.59307, test/num_examples=3003, total_duration=3728.37, train/accuracy=0.545248, train/bleu=25.2752, train/loss=2.61978, validation/accuracy=0.551393, validation/bleu=20.7844, validation/loss=2.55902, validation/num_examples=3000
I0305 20:15:49.958219 139485362775808 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8576439023017883, loss=2.661757230758667
I0305 20:16:24.462639 139485354383104 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6238301396369934, loss=2.495163917541504
I0305 20:16:59.072122 139485362775808 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6691578030586243, loss=2.562653064727783
I0305 20:17:33.705002 139485354383104 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5503997206687927, loss=2.550320863723755
I0305 20:18:08.372586 139485362775808 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6160078644752502, loss=2.5978379249572754
I0305 20:18:43.059079 139485354383104 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5447607636451721, loss=2.378324270248413
I0305 20:19:17.699536 139485362775808 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5408010482788086, loss=2.505352258682251
I0305 20:19:52.360042 139485354383104 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5609647631645203, loss=2.4116146564483643
I0305 20:20:27.022725 139485362775808 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5364646315574646, loss=2.4909157752990723
I0305 20:21:01.663350 139485354383104 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5825910568237305, loss=2.3579578399658203
I0305 20:21:36.327913 139485362775808 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5237250924110413, loss=2.328029155731201
I0305 20:22:10.980005 139485354383104 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5585798621177673, loss=2.33071231842041
I0305 20:22:45.628572 139485362775808 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5380414724349976, loss=2.539423942565918
I0305 20:23:20.295723 139485354383104 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5339804887771606, loss=2.322133779525757
I0305 20:23:54.927761 139485295634176 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5174865126609802, loss=2.4676551818847656
I0305 20:24:29.554703 139485287241472 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.49355995655059814, loss=2.3756096363067627
I0305 20:25:04.183698 139485295634176 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.47582241892814636, loss=2.456397294998169
I0305 20:25:38.786581 139485287241472 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5588399171829224, loss=2.3605947494506836
I0305 20:26:13.406789 139485295634176 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4695482850074768, loss=2.3394222259521484
I0305 20:26:48.017256 139485287241472 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4670718014240265, loss=2.4010891914367676
I0305 20:27:22.612394 139485295634176 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4748566746711731, loss=2.4047088623046875
I0305 20:27:57.217934 139485287241472 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5770893096923828, loss=2.385840892791748
I0305 20:28:31.821676 139485295634176 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4738800525665283, loss=2.3310439586639404
I0305 20:29:06.457803 139485287241472 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5364714860916138, loss=2.282379388809204
I0305 20:29:33.788796 139629069661376 spec.py:321] Evaluating on the training split.
I0305 20:29:36.410999 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:32:04.044518 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 20:32:06.652088 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:34:39.493283 139629069661376 spec.py:349] Evaluating on the test split.
I0305 20:34:42.104038 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:37:07.436515 139629069661376 submission_runner.py:469] Time since start: 5022.10s, 	Step: 7280, 	{'train/accuracy': 0.5875570774078369, 'train/loss': 2.212049961090088, 'train/bleu': 27.906779858640675, 'validation/accuracy': 0.5898820757865906, 'validation/loss': 2.1753923892974854, 'validation/bleu': 23.340932773479175, 'validation/num_examples': 3000, 'test/accuracy': 0.5919128656387329, 'test/loss': 2.1698529720306396, 'test/bleu': 22.152463632951903, 'test/num_examples': 3003, 'score': 2545.3964664936066, 'total_duration': 5022.101184844971, 'accumulated_submission_time': 2545.3964664936066, 'accumulated_eval_time': 2476.2249076366425, 'accumulated_logging_time': 0.04904675483703613}
I0305 20:37:07.445347 139485295634176 logging_writer.py:48] [7280] accumulated_eval_time=2476.22, accumulated_logging_time=0.0490468, accumulated_submission_time=2545.4, global_step=7280, preemption_count=0, score=2545.4, test/accuracy=0.591913, test/bleu=22.1525, test/loss=2.16985, test/num_examples=3003, total_duration=5022.1, train/accuracy=0.587557, train/bleu=27.9068, train/loss=2.21205, validation/accuracy=0.589882, validation/bleu=23.3409, validation/loss=2.17539, validation/num_examples=3000
I0305 20:37:14.671351 139485287241472 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4387883245944977, loss=2.246014356613159
I0305 20:37:49.053318 139485295634176 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4414612948894501, loss=2.324838638305664
I0305 20:38:23.560035 139485287241472 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4828549921512604, loss=2.314800500869751
I0305 20:38:58.067495 139485295634176 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.45324066281318665, loss=2.2649755477905273
I0305 20:39:32.631941 139485287241472 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.39410078525543213, loss=2.2119157314300537
I0305 20:40:07.204737 139485295634176 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.47907885909080505, loss=2.239482879638672
I0305 20:40:41.782220 139485287241472 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4434332251548767, loss=2.294616937637329
I0305 20:41:16.388054 139485295634176 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.42336079478263855, loss=2.182199716567993
I0305 20:41:50.971347 139485287241472 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.38973742723464966, loss=2.2412846088409424
I0305 20:42:25.568120 139485295634176 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.38436630368232727, loss=2.2178261280059814
I0305 20:43:00.190835 139485287241472 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.39235547184944153, loss=2.2780921459198
I0305 20:43:34.775238 139485295634176 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3800247013568878, loss=2.172313928604126
I0305 20:44:09.403592 139485287241472 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4135310649871826, loss=2.1311399936676025
I0305 20:44:43.994667 139485295634176 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4003666043281555, loss=2.1610007286071777
I0305 20:45:18.581595 139485287241472 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3603038489818573, loss=2.1330132484436035
I0305 20:45:53.155946 139485295634176 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.35197344422340393, loss=2.157381534576416
I0305 20:46:27.711178 139485287241472 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3512289226055145, loss=2.169196128845215
I0305 20:47:02.408551 139485295634176 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.38558369874954224, loss=2.200894594192505
I0305 20:47:37.022316 139485287241472 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4950694441795349, loss=2.213548421859741
I0305 20:48:11.606402 139485295634176 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.34295976161956787, loss=2.1582512855529785
I0305 20:48:46.162545 139485287241472 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3571166396141052, loss=2.1214213371276855
I0305 20:49:20.757134 139485295634176 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3565368354320526, loss=2.0784363746643066
I0305 20:49:55.319409 139485287241472 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3372696340084076, loss=2.1159327030181885
I0305 20:50:29.897336 139485295634176 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3817543387413025, loss=2.175015926361084
I0305 20:51:04.486303 139485287241472 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.31495368480682373, loss=2.1086089611053467
I0305 20:51:07.598080 139629069661376 spec.py:321] Evaluating on the training split.
I0305 20:51:10.213535 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:53:45.914412 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 20:53:48.522675 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:56:23.525059 139629069661376 spec.py:349] Evaluating on the test split.
I0305 20:56:26.133160 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 20:58:46.016721 139629069661376 submission_runner.py:469] Time since start: 6320.68s, 	Step: 9710, 	{'train/accuracy': 0.5940812826156616, 'train/loss': 2.128690242767334, 'train/bleu': 28.688456547332986, 'validation/accuracy': 0.6115615963935852, 'validation/loss': 1.9945515394210815, 'validation/bleu': 24.638138219074122, 'validation/num_examples': 3000, 'test/accuracy': 0.6168694496154785, 'test/loss': 1.9696519374847412, 'test/bleu': 23.798854738651972, 'test/num_examples': 3003, 'score': 3385.402242898941, 'total_duration': 6320.681403398514, 'accumulated_submission_time': 3385.402242898941, 'accumulated_eval_time': 2934.643504858017, 'accumulated_logging_time': 0.0671086311340332}
I0305 20:58:46.025692 139485295634176 logging_writer.py:48] [9710] accumulated_eval_time=2934.64, accumulated_logging_time=0.0671086, accumulated_submission_time=3385.4, global_step=9710, preemption_count=0, score=3385.4, test/accuracy=0.616869, test/bleu=23.7989, test/loss=1.96965, test/num_examples=3003, total_duration=6320.68, train/accuracy=0.594081, train/bleu=28.6885, train/loss=2.12869, validation/accuracy=0.611562, validation/bleu=24.6381, validation/loss=1.99455, validation/num_examples=3000
I0305 20:59:17.358214 139485287241472 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.31496530771255493, loss=2.136319875717163
I0305 20:59:51.806193 139485295634176 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.34912657737731934, loss=2.126814603805542
I0305 21:00:26.293258 139485287241472 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3316504657268524, loss=2.0847811698913574
I0305 21:01:00.885327 139485295634176 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3316519260406494, loss=2.13254714012146
I0305 21:01:35.417098 139485287241472 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.36833226680755615, loss=2.0942366123199463
I0305 21:02:09.957676 139485295634176 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.3159430921077728, loss=2.1103596687316895
I0305 21:02:44.499083 139485287241472 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3235858380794525, loss=2.0469021797180176
I0305 21:03:19.023772 139485295634176 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.34313642978668213, loss=2.0776052474975586
I0305 21:03:53.572662 139485287241472 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3022565543651581, loss=2.1091620922088623
I0305 21:04:28.109864 139485295634176 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.33045315742492676, loss=2.1272494792938232
I0305 21:05:02.672358 139485287241472 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3087178170681, loss=2.083972215652466
I0305 21:05:37.213782 139485295634176 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.3143315315246582, loss=2.0116961002349854
I0305 21:06:11.757496 139485287241472 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3096417486667633, loss=2.0497348308563232
I0305 21:06:46.300874 139485295634176 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3204954266548157, loss=2.027151107788086
I0305 21:07:20.847919 139485287241472 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.2921431362628937, loss=2.075841188430786
I0305 21:07:55.459010 139485295634176 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2843632698059082, loss=2.1174910068511963
I0305 21:08:30.028370 139485287241472 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.2856379449367523, loss=2.051126003265381
I0305 21:09:04.563790 139485295634176 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.28543078899383545, loss=1.9899890422821045
I0305 21:09:39.127891 139485287241472 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3136361241340637, loss=2.047750949859619
I0305 21:10:13.685512 139485295634176 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2910510301589966, loss=2.0556869506835938
I0305 21:10:48.255589 139485287241472 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2930634319782257, loss=2.1372320652008057
I0305 21:11:22.805547 139485295634176 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.27615466713905334, loss=2.096600294113159
I0305 21:11:57.357512 139485287241472 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2757808268070221, loss=2.108696222305298
I0305 21:12:31.926853 139485295634176 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.30599039793014526, loss=2.083242177963257
I0305 21:12:46.101293 139629069661376 spec.py:321] Evaluating on the training split.
I0305 21:12:48.719658 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 21:15:57.603342 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 21:16:00.211033 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 21:18:51.384715 139629069661376 spec.py:349] Evaluating on the test split.
I0305 21:18:53.989927 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 21:21:13.861178 139629069661376 submission_runner.py:469] Time since start: 7668.53s, 	Step: 12142, 	{'train/accuracy': 0.6039047837257385, 'train/loss': 2.0357227325439453, 'train/bleu': 28.78666351948088, 'validation/accuracy': 0.6217833161354065, 'validation/loss': 1.885305404663086, 'validation/bleu': 25.508397903086852, 'validation/num_examples': 3000, 'test/accuracy': 0.6304020285606384, 'test/loss': 1.8356561660766602, 'test/bleu': 24.77202011508364, 'test/num_examples': 3003, 'score': 4225.338691949844, 'total_duration': 7668.525817155838, 'accumulated_submission_time': 4225.338691949844, 'accumulated_eval_time': 3442.4033029079437, 'accumulated_logging_time': 0.08451223373413086}
I0305 21:21:13.870304 139485287241472 logging_writer.py:48] [12142] accumulated_eval_time=3442.4, accumulated_logging_time=0.0845122, accumulated_submission_time=4225.34, global_step=12142, preemption_count=0, score=4225.34, test/accuracy=0.630402, test/bleu=24.772, test/loss=1.83566, test/num_examples=3003, total_duration=7668.53, train/accuracy=0.603905, train/bleu=28.7867, train/loss=2.03572, validation/accuracy=0.621783, validation/bleu=25.5084, validation/loss=1.88531, validation/num_examples=3000
I0305 21:21:34.163913 139485295634176 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.27986064553260803, loss=2.0643105506896973
I0305 21:22:08.610611 139485287241472 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2701537609100342, loss=2.0206634998321533
I0305 21:22:43.123552 139485295634176 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.29985734820365906, loss=1.9399772882461548
I0305 21:23:17.654703 139485287241472 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.26453402638435364, loss=2.088973045349121
I0305 21:23:52.250627 139485295634176 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3331972062587738, loss=1.9665504693984985
I0305 21:24:26.895797 139485287241472 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.27150759100914, loss=2.0389444828033447
I0305 21:25:01.498745 139485295634176 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.250034898519516, loss=2.005396604537964
I0305 21:25:36.108440 139485287241472 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.27738043665885925, loss=2.0063908100128174
I0305 21:26:10.743929 139485295634176 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.31427082419395447, loss=2.0981264114379883
I0305 21:26:45.343862 139485287241472 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.27846547961235046, loss=2.0631728172302246
I0305 21:27:19.966086 139485295634176 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2949063181877136, loss=1.9375096559524536
I0305 21:27:54.575767 139485287241472 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2946266829967499, loss=1.96151864528656
I0305 21:28:29.189223 139485295634176 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.28653407096862793, loss=2.0579915046691895
I0305 21:29:03.752638 139485287241472 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.27776026725769043, loss=2.0320539474487305
I0305 21:29:38.349204 139485295634176 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.327724426984787, loss=1.9278584718704224
I0305 21:30:12.959449 139485287241472 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.27392032742500305, loss=1.9664161205291748
I0305 21:30:47.573716 139485295634176 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2907007336616516, loss=1.9321374893188477
I0305 21:31:22.169007 139485287241472 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3248870372772217, loss=1.926642894744873
I0305 21:31:56.742598 139485295634176 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.30165305733680725, loss=1.9987655878067017
I0305 21:32:31.321815 139485287241472 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3248050808906555, loss=2.002878427505493
I0305 21:33:05.894863 139485295634176 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.27346521615982056, loss=1.955202341079712
I0305 21:33:40.465542 139485287241472 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.35927510261535645, loss=1.8744961023330688
I0305 21:34:15.064068 139485295634176 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.3025398552417755, loss=1.947287917137146
I0305 21:34:49.694785 139485287241472 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.26548078656196594, loss=2.011920690536499
I0305 21:35:13.882948 139629069661376 spec.py:321] Evaluating on the training split.
I0305 21:35:16.499665 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 21:38:11.858185 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 21:38:14.453727 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 21:40:49.420570 139629069661376 spec.py:349] Evaluating on the test split.
I0305 21:40:52.028035 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 21:43:06.825720 139629069661376 submission_runner.py:469] Time since start: 8981.49s, 	Step: 14571, 	{'train/accuracy': 0.6147860884666443, 'train/loss': 1.9359149932861328, 'train/bleu': 29.970472072607677, 'validation/accuracy': 0.6356883645057678, 'validation/loss': 1.7836273908615112, 'validation/bleu': 26.744759991811442, 'validation/num_examples': 3000, 'test/accuracy': 0.6414899826049805, 'test/loss': 1.7355459928512573, 'test/bleu': 25.53281453509334, 'test/num_examples': 3003, 'score': 5065.210557937622, 'total_duration': 8981.490393161774, 'accumulated_submission_time': 5065.210557937622, 'accumulated_eval_time': 3915.346025943756, 'accumulated_logging_time': 0.10158371925354004}
I0305 21:43:06.835421 139485295634176 logging_writer.py:48] [14571] accumulated_eval_time=3915.35, accumulated_logging_time=0.101584, accumulated_submission_time=5065.21, global_step=14571, preemption_count=0, score=5065.21, test/accuracy=0.64149, test/bleu=25.5328, test/loss=1.73555, test/num_examples=3003, total_duration=8981.49, train/accuracy=0.614786, train/bleu=29.9705, train/loss=1.93591, validation/accuracy=0.635688, validation/bleu=26.7448, validation/loss=1.78363, validation/num_examples=3000
I0305 21:43:17.190231 139485287241472 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.33277428150177, loss=1.8986294269561768
I0305 21:43:51.620693 139485295634176 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3080114424228668, loss=1.9817020893096924
I0305 21:44:26.131250 139485287241472 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.31238797307014465, loss=1.8857464790344238
I0305 21:45:00.670821 139485295634176 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3386729061603546, loss=1.933784008026123
I0305 21:45:35.227311 139485287241472 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3182123005390167, loss=1.9456615447998047
I0305 21:46:09.812124 139485295634176 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3052912652492523, loss=1.8500744104385376
I0305 21:46:44.448436 139485287241472 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.4241391122341156, loss=1.8803192377090454
I0305 21:47:19.054979 139485295634176 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.38302215933799744, loss=1.9739432334899902
I0305 21:47:53.632005 139485287241472 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3845715820789337, loss=1.9938246011734009
I0305 21:48:28.259986 139485295634176 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3056073486804962, loss=1.9770119190216064
I0305 21:49:02.884633 139485287241472 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.28690454363822937, loss=1.8918958902359009
I0305 21:49:37.487315 139485295634176 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3127574920654297, loss=1.9745991230010986
I0305 21:50:12.106699 139485287241472 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.3449115753173828, loss=1.8743197917938232
I0305 21:50:46.734032 139485295634176 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3896988034248352, loss=1.8351351022720337
I0305 21:51:21.355579 139485287241472 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3460131287574768, loss=1.8816990852355957
I0305 21:51:55.975527 139485295634176 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3376322388648987, loss=1.9510290622711182
I0305 21:52:30.597584 139485287241472 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3463059961795807, loss=1.8372772932052612
I0305 21:53:05.202577 139485295634176 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2961888909339905, loss=1.8519809246063232
I0305 21:53:39.816821 139485287241472 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.32478296756744385, loss=1.8962762355804443
I0305 21:54:14.454705 139485295634176 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.32838624715805054, loss=1.9314098358154297
I0305 21:54:49.083165 139485287241472 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.35114967823028564, loss=1.9095619916915894
I0305 21:55:23.673947 139485295634176 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.40440335869789124, loss=1.9151842594146729
I0305 21:55:58.262938 139485287241472 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.312597393989563, loss=1.9383220672607422
I0305 21:56:32.869231 139485295634176 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3694019019603729, loss=1.8972691297531128
I0305 21:57:07.117288 139629069661376 spec.py:321] Evaluating on the training split.
I0305 21:57:09.730610 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:00:58.823950 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 22:01:01.426496 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:03:37.246683 139629069661376 spec.py:349] Evaluating on the test split.
I0305 22:03:39.845214 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:06:00.169942 139629069661376 submission_runner.py:469] Time since start: 10354.83s, 	Step: 17000, 	{'train/accuracy': 0.6206833720207214, 'train/loss': 1.8909770250320435, 'train/bleu': 30.060995666829, 'validation/accuracy': 0.6413739323616028, 'validation/loss': 1.7339715957641602, 'validation/bleu': 27.08305901709383, 'validation/num_examples': 3000, 'test/accuracy': 0.6501564383506775, 'test/loss': 1.6826587915420532, 'test/bleu': 26.340757724430667, 'test/num_examples': 3003, 'score': 5905.349917650223, 'total_duration': 10354.834623336792, 'accumulated_submission_time': 5905.349917650223, 'accumulated_eval_time': 4448.398643493652, 'accumulated_logging_time': 0.11999988555908203}
I0305 22:06:00.179505 139485287241472 logging_writer.py:48] [17000] accumulated_eval_time=4448.4, accumulated_logging_time=0.12, accumulated_submission_time=5905.35, global_step=17000, preemption_count=0, score=5905.35, test/accuracy=0.650156, test/bleu=26.3408, test/loss=1.68266, test/num_examples=3003, total_duration=10354.8, train/accuracy=0.620683, train/bleu=30.061, train/loss=1.89098, validation/accuracy=0.641374, validation/bleu=27.0831, validation/loss=1.73397, validation/num_examples=3000
I0305 22:06:00.539962 139485295634176 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.35790780186653137, loss=1.897923469543457
I0305 22:06:34.931164 139485287241472 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.400708943605423, loss=1.9259600639343262
I0305 22:07:09.405515 139485295634176 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3631305992603302, loss=1.9057812690734863
I0305 22:07:43.995022 139485287241472 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.3351609706878662, loss=1.855594515800476
I0305 22:08:18.591764 139485295634176 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.49025270342826843, loss=1.8408067226409912
I0305 22:08:53.240429 139485287241472 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.33969759941101074, loss=1.8863306045532227
I0305 22:09:27.851768 139485295634176 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.31454193592071533, loss=1.9746644496917725
I0305 22:10:02.416674 139485287241472 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.308098703622818, loss=1.8265411853790283
I0305 22:10:36.991142 139485295634176 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.35079869627952576, loss=1.9185612201690674
I0305 22:11:11.612937 139485287241472 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.353088915348053, loss=1.8953309059143066
I0305 22:11:46.224864 139485295634176 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3748806118965149, loss=1.9540513753890991
I0305 22:12:20.822943 139485287241472 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.34036433696746826, loss=1.9686505794525146
I0305 22:12:55.453190 139485295634176 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3740488290786743, loss=1.9047266244888306
I0305 22:13:30.045995 139485287241472 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3660261631011963, loss=1.847536563873291
I0305 22:14:04.655943 139485295634176 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3565666973590851, loss=1.9285281896591187
I0305 22:14:39.223897 139485287241472 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3709603250026703, loss=1.870554804801941
I0305 22:15:13.841316 139485295634176 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3570946156978607, loss=1.7747513055801392
I0305 22:15:48.446387 139485287241472 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.40686559677124023, loss=1.9120128154754639
I0305 22:16:23.046189 139485295634176 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.4086557626724243, loss=1.8481210470199585
I0305 22:16:57.853088 139485287241472 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.39530661702156067, loss=1.8756728172302246
I0305 22:17:32.640777 139485295634176 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.40650367736816406, loss=1.8489571809768677
I0305 22:18:07.368583 139485287241472 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3352454602718353, loss=1.9107133150100708
I0305 22:18:42.114732 139485295634176 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.398352712392807, loss=1.7964516878128052
I0305 22:19:16.828369 139485287241472 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.36799004673957825, loss=1.8231343030929565
I0305 22:19:51.565805 139485295634176 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4038671851158142, loss=1.905351996421814
I0305 22:20:00.270062 139629069661376 spec.py:321] Evaluating on the training split.
I0305 22:20:02.896393 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:23:39.960244 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 22:23:42.565786 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:26:28.982291 139629069661376 spec.py:349] Evaluating on the test split.
I0305 22:26:31.585435 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:29:00.788839 139629069661376 submission_runner.py:469] Time since start: 11735.45s, 	Step: 19426, 	{'train/accuracy': 0.6344527006149292, 'train/loss': 1.7864872217178345, 'train/bleu': 30.82396650855077, 'validation/accuracy': 0.6451066732406616, 'validation/loss': 1.701812505722046, 'validation/bleu': 27.43860230083909, 'validation/num_examples': 3000, 'test/accuracy': 0.6558915376663208, 'test/loss': 1.6511059999465942, 'test/bleu': 26.37713326447981, 'test/num_examples': 3003, 'score': 6745.300940513611, 'total_duration': 11735.453484773636, 'accumulated_submission_time': 6745.300940513611, 'accumulated_eval_time': 4988.9173402786255, 'accumulated_logging_time': 0.13750410079956055}
I0305 22:29:00.801549 139485287241472 logging_writer.py:48] [19426] accumulated_eval_time=4988.92, accumulated_logging_time=0.137504, accumulated_submission_time=6745.3, global_step=19426, preemption_count=0, score=6745.3, test/accuracy=0.655892, test/bleu=26.3771, test/loss=1.65111, test/num_examples=3003, total_duration=11735.5, train/accuracy=0.634453, train/bleu=30.824, train/loss=1.78649, validation/accuracy=0.645107, validation/bleu=27.4386, validation/loss=1.70181, validation/num_examples=3000
I0305 22:29:26.780094 139485295634176 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4082534909248352, loss=1.8723994493484497
I0305 22:30:01.393091 139485287241472 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3435758054256439, loss=1.8529977798461914
I0305 22:30:36.066331 139485295634176 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3470180034637451, loss=1.884334921836853
I0305 22:31:10.768366 139485287241472 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.37910738587379456, loss=1.896167516708374
I0305 22:31:45.480564 139485295634176 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.39184725284576416, loss=1.961971640586853
I0305 22:32:20.177904 139485287241472 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.468789279460907, loss=1.8069469928741455
I0305 22:32:54.964187 139485295634176 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3924524784088135, loss=1.7726404666900635
I0305 22:33:29.700201 139485287241472 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.38491877913475037, loss=1.8622928857803345
I0305 22:34:04.437193 139485295634176 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5029070377349854, loss=1.8049167394638062
I0305 22:34:39.164630 139485287241472 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.36955395340919495, loss=1.9519259929656982
I0305 22:35:13.898751 139485295634176 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.41527608036994934, loss=1.8165004253387451
I0305 22:35:48.649620 139485287241472 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.42344650626182556, loss=1.9341627359390259
I0305 22:36:23.362452 139485295634176 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.45206722617149353, loss=1.8586242198944092
I0305 22:36:58.114347 139485287241472 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.34283187985420227, loss=1.8530184030532837
I0305 22:37:32.855956 139485295634176 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3562595844268799, loss=1.7729737758636475
I0305 22:38:07.580263 139485287241472 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.36680305004119873, loss=1.8292070627212524
I0305 22:38:42.327378 139485295634176 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4270600974559784, loss=1.913399577140808
I0305 22:39:17.105900 139485287241472 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4151144325733185, loss=1.7694108486175537
I0305 22:39:51.849502 139485295634176 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.35332179069519043, loss=1.8427411317825317
I0305 22:40:26.555940 139485287241472 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.31550800800323486, loss=1.7563858032226562
I0305 22:41:01.306034 139485295634176 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.41980379819869995, loss=1.802268147468567
I0305 22:41:36.059815 139485287241472 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3886941373348236, loss=1.7996196746826172
I0305 22:42:10.774275 139485295634176 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.42505568265914917, loss=1.7678090333938599
I0305 22:42:45.524885 139485287241472 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.3587817847728729, loss=1.8900272846221924
I0305 22:43:00.804675 139629069661376 spec.py:321] Evaluating on the training split.
I0305 22:43:03.427395 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:46:53.627279 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 22:46:56.243082 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:49:57.892378 139629069661376 spec.py:349] Evaluating on the test split.
I0305 22:50:00.501624 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 22:52:47.939334 139629069661376 submission_runner.py:469] Time since start: 13162.60s, 	Step: 21845, 	{'train/accuracy': 0.6304247975349426, 'train/loss': 1.8210152387619019, 'train/bleu': 30.263021978033695, 'validation/accuracy': 0.647689938545227, 'validation/loss': 1.676596999168396, 'validation/bleu': 27.600635469041226, 'validation/num_examples': 3000, 'test/accuracy': 0.6587880849838257, 'test/loss': 1.6251347064971924, 'test/bleu': 26.85147099939014, 'test/num_examples': 3003, 'score': 7585.1607456207275, 'total_duration': 13162.6039788723, 'accumulated_submission_time': 7585.1607456207275, 'accumulated_eval_time': 5576.051920175552, 'accumulated_logging_time': 0.16083908081054688}
I0305 22:52:47.951888 139485295634176 logging_writer.py:48] [21845] accumulated_eval_time=5576.05, accumulated_logging_time=0.160839, accumulated_submission_time=7585.16, global_step=21845, preemption_count=0, score=7585.16, test/accuracy=0.658788, test/bleu=26.8515, test/loss=1.62513, test/num_examples=3003, total_duration=13162.6, train/accuracy=0.630425, train/bleu=30.263, train/loss=1.82102, validation/accuracy=0.64769, validation/bleu=27.6006, validation/loss=1.6766, validation/num_examples=3000
I0305 22:53:07.289720 139485287241472 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3804655075073242, loss=1.8701053857803345
I0305 22:53:41.884797 139485295634176 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.37256699800491333, loss=1.8504868745803833
I0305 22:54:16.552421 139485287241472 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.41265180706977844, loss=1.874474287033081
I0305 22:54:51.217718 139485295634176 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.393729031085968, loss=1.890515685081482
I0305 22:55:26.019203 139485287241472 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.41090595722198486, loss=1.7909327745437622
I0305 22:56:00.718028 139485295634176 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.3442821502685547, loss=1.8136848211288452
I0305 22:56:35.429672 139485287241472 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3955232501029968, loss=1.8342969417572021
I0305 22:57:10.137886 139485295634176 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.4246727526187897, loss=1.7723546028137207
I0305 22:57:44.859294 139485287241472 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.42177465558052063, loss=1.786666989326477
I0305 22:58:19.550137 139485295634176 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.36195605993270874, loss=1.7730023860931396
I0305 22:58:54.298387 139485287241472 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.36871984601020813, loss=1.7938411235809326
I0305 22:59:29.021145 139485295634176 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.37395599484443665, loss=1.825700044631958
I0305 23:00:03.764517 139485287241472 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.3691287636756897, loss=1.8647170066833496
I0305 23:00:38.512306 139485295634176 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.40910595655441284, loss=1.8068220615386963
I0305 23:01:13.216179 139485287241472 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.39282113313674927, loss=1.9019821882247925
I0305 23:01:47.887018 139485295634176 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.42697665095329285, loss=1.912474274635315
I0305 23:02:22.587732 139485287241472 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3909298777580261, loss=1.8492580652236938
I0305 23:02:57.278518 139485295634176 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.41942059993743896, loss=1.703154444694519
I0305 23:03:31.990111 139485287241472 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.48468348383903503, loss=1.8835346698760986
I0305 23:04:06.722846 139485295634176 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.37359458208084106, loss=1.7122962474822998
I0305 23:04:41.425307 139485287241472 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.4018026292324066, loss=1.7668278217315674
I0305 23:05:16.133857 139485295634176 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.35072436928749084, loss=1.7949566841125488
I0305 23:05:50.828932 139485287241472 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.39698976278305054, loss=1.820057988166809
I0305 23:06:25.537564 139485295634176 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.5912918448448181, loss=1.7732782363891602
I0305 23:06:48.116178 139629069661376 spec.py:321] Evaluating on the training split.
I0305 23:06:50.738850 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 23:11:03.481216 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 23:11:06.103206 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 23:14:22.881993 139629069661376 spec.py:349] Evaluating on the test split.
I0305 23:14:25.491584 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 23:17:27.478958 139629069661376 submission_runner.py:469] Time since start: 14642.14s, 	Step: 24266, 	{'train/accuracy': 0.6300191879272461, 'train/loss': 1.816698431968689, 'train/bleu': 30.368400848139316, 'validation/accuracy': 0.6515709757804871, 'validation/loss': 1.660017728805542, 'validation/bleu': 27.9377035280994, 'validation/num_examples': 3000, 'test/accuracy': 0.6619974374771118, 'test/loss': 1.6005034446716309, 'test/bleu': 26.87677742911548, 'test/num_examples': 3003, 'score': 8425.184804916382, 'total_duration': 14642.143604040146, 'accumulated_submission_time': 8425.184804916382, 'accumulated_eval_time': 6215.4146184921265, 'accumulated_logging_time': 0.18214893341064453}
I0305 23:17:27.490004 139485287241472 logging_writer.py:48] [24266] accumulated_eval_time=6215.41, accumulated_logging_time=0.182149, accumulated_submission_time=8425.18, global_step=24266, preemption_count=0, score=8425.18, test/accuracy=0.661997, test/bleu=26.8768, test/loss=1.6005, test/num_examples=3003, total_duration=14642.1, train/accuracy=0.630019, train/bleu=30.3684, train/loss=1.8167, validation/accuracy=0.651571, validation/bleu=27.9377, validation/loss=1.66002, validation/num_examples=3000
I0305 23:17:39.576910 139485295634176 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.49028080701828003, loss=1.886161208152771
I0305 23:18:14.119254 139485287241472 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4041103422641754, loss=1.8054172992706299
I0305 23:18:48.771019 139485295634176 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4142976999282837, loss=1.8313935995101929
I0305 23:19:23.458680 139485287241472 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.3194788992404938, loss=1.7915180921554565
I0305 23:19:58.124860 139485295634176 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.4442843198776245, loss=1.814515471458435
I0305 23:20:32.826675 139485287241472 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.49866002798080444, loss=1.8007533550262451
I0305 23:21:07.529882 139485295634176 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3739839196205139, loss=1.7394522428512573
I0305 23:21:42.214998 139485287241472 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.4079708158969879, loss=1.81261146068573
I0305 23:22:16.869371 139485295634176 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.38437017798423767, loss=1.848686695098877
I0305 23:22:51.440359 139485287241472 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.3898182809352875, loss=1.8214092254638672
I0305 23:23:26.063985 139485295634176 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4660671055316925, loss=1.799646258354187
I0305 23:24:00.647235 139485287241472 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3864063024520874, loss=1.8891448974609375
I0305 23:24:35.234221 139485295634176 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.37059101462364197, loss=1.8172872066497803
I0305 23:25:09.818606 139485287241472 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.41775205731391907, loss=1.8742766380310059
I0305 23:25:44.393713 139485295634176 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.34558945894241333, loss=1.8284659385681152
I0305 23:26:18.985143 139485287241472 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3857351243495941, loss=1.8341397047042847
I0305 23:26:53.566730 139485295634176 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.34414833784103394, loss=1.732925295829773
I0305 23:27:28.170346 139485287241472 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3931739032268524, loss=1.8511353731155396
I0305 23:28:02.722130 139485295634176 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.8017644286155701, loss=1.9450360536575317
I0305 23:28:37.328167 139485287241472 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.3831823468208313, loss=1.8972852230072021
I0305 23:29:11.937047 139485295634176 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.43990418314933777, loss=1.8188565969467163
I0305 23:29:46.531719 139485287241472 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.38439926505088806, loss=1.8268183469772339
I0305 23:30:21.124479 139485295634176 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.34851861000061035, loss=1.7378122806549072
I0305 23:30:55.731457 139485287241472 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.38344067335128784, loss=1.7783966064453125
I0305 23:31:27.579120 139629069661376 spec.py:321] Evaluating on the training split.
I0305 23:31:30.205962 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 23:34:43.472624 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 23:34:46.073764 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 23:37:20.682540 139629069661376 spec.py:349] Evaluating on the test split.
I0305 23:37:23.292863 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 23:40:13.123183 139629069661376 submission_runner.py:469] Time since start: 16007.79s, 	Step: 26693, 	{'train/accuracy': 0.6381562948226929, 'train/loss': 1.7634669542312622, 'train/bleu': 30.801172207068284, 'validation/accuracy': 0.6547598838806152, 'validation/loss': 1.641645073890686, 'validation/bleu': 27.654973346886848, 'validation/num_examples': 3000, 'test/accuracy': 0.6655312180519104, 'test/loss': 1.5759555101394653, 'test/bleu': 27.631221936801985, 'test/num_examples': 3003, 'score': 9265.13209104538, 'total_duration': 16007.787855863571, 'accumulated_submission_time': 9265.13209104538, 'accumulated_eval_time': 6740.958631277084, 'accumulated_logging_time': 0.20258116722106934}
I0305 23:40:13.134278 139485295634176 logging_writer.py:48] [26693] accumulated_eval_time=6740.96, accumulated_logging_time=0.202581, accumulated_submission_time=9265.13, global_step=26693, preemption_count=0, score=9265.13, test/accuracy=0.665531, test/bleu=27.6312, test/loss=1.57596, test/num_examples=3003, total_duration=16007.8, train/accuracy=0.638156, train/bleu=30.8012, train/loss=1.76347, validation/accuracy=0.65476, validation/bleu=27.655, validation/loss=1.64165, validation/num_examples=3000
I0305 23:40:15.887579 139485287241472 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.4117424190044403, loss=1.8192079067230225
I0305 23:40:50.336178 139485295634176 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.37177956104278564, loss=1.8092007637023926
I0305 23:41:24.823930 139485287241472 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.3760642409324646, loss=1.7800753116607666
I0305 23:41:59.375925 139485295634176 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4343303442001343, loss=1.8260713815689087
I0305 23:42:33.962249 139485287241472 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.3831340968608856, loss=1.8131517171859741
I0305 23:43:08.531207 139485295634176 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.4876878261566162, loss=1.7650443315505981
I0305 23:43:43.142011 139485287241472 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.39950671792030334, loss=1.730265498161316
I0305 23:44:17.771420 139485295634176 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.4070967733860016, loss=1.8191839456558228
I0305 23:44:52.374736 139485287241472 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.37123602628707886, loss=1.7977741956710815
I0305 23:45:26.963732 139485295634176 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.43231379985809326, loss=1.7895495891571045
I0305 23:46:01.563827 139485287241472 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.35675734281539917, loss=1.7463371753692627
I0305 23:46:36.199928 139485295634176 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.5075628757476807, loss=1.7330060005187988
I0305 23:47:10.814447 139485287241472 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.49724629521369934, loss=1.7983812093734741
I0305 23:47:45.382256 139485295634176 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.45916563272476196, loss=1.8817325830459595
I0305 23:48:20.000472 139485287241472 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.3848920464515686, loss=1.796717882156372
I0305 23:48:54.601567 139485295634176 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.41869521141052246, loss=1.7925845384597778
I0305 23:49:29.232938 139485287241472 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.7944925427436829, loss=1.782184362411499
I0305 23:50:03.894803 139485295634176 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.9854313731193542, loss=1.8370306491851807
I0305 23:50:38.505512 139485287241472 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.4404282867908478, loss=1.8697890043258667
I0305 23:51:13.146531 139485295634176 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.43723464012145996, loss=1.746038794517517
I0305 23:51:47.754683 139485287241472 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.4259093105792999, loss=1.8351396322250366
I0305 23:52:22.403207 139485295634176 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.41722336411476135, loss=1.842841386795044
I0305 23:52:56.990511 139485287241472 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.3680577874183655, loss=1.7974677085876465
I0305 23:53:31.580810 139485295634176 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.38696709275245667, loss=1.8094818592071533
I0305 23:54:06.186246 139485287241472 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.42240962386131287, loss=1.9069164991378784
I0305 23:54:13.458620 139629069661376 spec.py:321] Evaluating on the training split.
I0305 23:54:16.077833 139629069661376 workload.py:181] Translating evaluation dataset.
I0305 23:57:25.362010 139629069661376 spec.py:333] Evaluating on the validation split.
I0305 23:57:27.970867 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:00:24.074773 139629069661376 spec.py:349] Evaluating on the test split.
I0306 00:00:26.678871 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:03:06.632635 139629069661376 submission_runner.py:469] Time since start: 17381.30s, 	Step: 29122, 	{'train/accuracy': 0.6330106258392334, 'train/loss': 1.8009551763534546, 'train/bleu': 31.084494807019496, 'validation/accuracy': 0.6558228135108948, 'validation/loss': 1.6337389945983887, 'validation/bleu': 27.876580072890555, 'validation/num_examples': 3000, 'test/accuracy': 0.6659135818481445, 'test/loss': 1.5701812505722046, 'test/bleu': 27.329299125560674, 'test/num_examples': 3003, 'score': 10105.314846277237, 'total_duration': 17381.29731631279, 'accumulated_submission_time': 10105.314846277237, 'accumulated_eval_time': 7274.132599115372, 'accumulated_logging_time': 0.2228257656097412}
I0306 00:03:06.643806 139485295634176 logging_writer.py:48] [29122] accumulated_eval_time=7274.13, accumulated_logging_time=0.222826, accumulated_submission_time=10105.3, global_step=29122, preemption_count=0, score=10105.3, test/accuracy=0.665914, test/bleu=27.3293, test/loss=1.57018, test/num_examples=3003, total_duration=17381.3, train/accuracy=0.633011, train/bleu=31.0845, train/loss=1.80096, validation/accuracy=0.655823, validation/bleu=27.8766, validation/loss=1.63374, validation/num_examples=3000
I0306 00:03:33.843866 139485287241472 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.46073421835899353, loss=1.807157039642334
I0306 00:04:08.288356 139485295634176 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.4843716025352478, loss=1.7586373090744019
I0306 00:04:42.825049 139485287241472 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.39449483156204224, loss=1.7887099981307983
I0306 00:05:17.407748 139485295634176 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.37094607949256897, loss=1.747454285621643
I0306 00:05:51.996563 139485287241472 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3973924517631531, loss=1.7402678728103638
I0306 00:06:26.620597 139485295634176 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.3722504675388336, loss=1.7855582237243652
I0306 00:07:01.254904 139485287241472 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.4196052849292755, loss=1.6989156007766724
I0306 00:07:35.859885 139485295634176 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3938496708869934, loss=1.7981066703796387
I0306 00:08:10.469579 139485287241472 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.38886353373527527, loss=1.767905354499817
I0306 00:08:45.072448 139485295634176 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3567720651626587, loss=1.7398520708084106
I0306 00:09:19.650954 139485287241472 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.4362891614437103, loss=1.8540741205215454
I0306 00:09:54.270695 139485295634176 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.35846176743507385, loss=1.792954444885254
I0306 00:10:28.847796 139485287241472 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.4056150019168854, loss=1.8164714574813843
I0306 00:11:03.443406 139485295634176 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.41893646121025085, loss=1.8157620429992676
I0306 00:11:38.022614 139485287241472 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.41564977169036865, loss=1.8162615299224854
I0306 00:12:12.610951 139485295634176 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3763790428638458, loss=1.776686429977417
I0306 00:12:47.218482 139485287241472 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.379063218832016, loss=1.7815353870391846
I0306 00:13:21.841139 139485295634176 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3742595911026001, loss=1.7682517766952515
I0306 00:13:56.441009 139485287241472 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.4282636344432831, loss=1.8220958709716797
I0306 00:14:31.085741 139485295634176 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.43204009532928467, loss=1.7343190908432007
I0306 00:15:05.683162 139485287241472 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.4478779435157776, loss=1.742160439491272
I0306 00:15:40.311719 139485295634176 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3711688220500946, loss=1.688056230545044
I0306 00:16:14.987509 139485287241472 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.5020895004272461, loss=1.7542458772659302
I0306 00:16:49.704422 139485295634176 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3614960312843323, loss=1.7852028608322144
I0306 00:17:06.712466 139629069661376 spec.py:321] Evaluating on the training split.
I0306 00:17:09.339204 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:20:12.284105 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 00:20:14.890895 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:22:52.727597 139629069661376 spec.py:349] Evaluating on the test split.
I0306 00:22:55.332841 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:25:16.322180 139629069661376 submission_runner.py:469] Time since start: 18710.99s, 	Step: 31550, 	{'train/accuracy': 0.6594426035881042, 'train/loss': 1.6123465299606323, 'train/bleu': 32.65510227290665, 'validation/accuracy': 0.6556497812271118, 'validation/loss': 1.6266634464263916, 'validation/bleu': 27.988799619314513, 'validation/num_examples': 3000, 'test/accuracy': 0.6673849821090698, 'test/loss': 1.5585217475891113, 'test/bleu': 27.739321066917825, 'test/num_examples': 3003, 'score': 10945.24265384674, 'total_duration': 18710.986840963364, 'accumulated_submission_time': 10945.24265384674, 'accumulated_eval_time': 7763.742249488831, 'accumulated_logging_time': 0.24209904670715332}
I0306 00:25:16.334346 139485287241472 logging_writer.py:48] [31550] accumulated_eval_time=7763.74, accumulated_logging_time=0.242099, accumulated_submission_time=10945.2, global_step=31550, preemption_count=0, score=10945.2, test/accuracy=0.667385, test/bleu=27.7393, test/loss=1.55852, test/num_examples=3003, total_duration=18711, train/accuracy=0.659443, train/bleu=32.6551, train/loss=1.61235, validation/accuracy=0.65565, validation/bleu=27.9888, validation/loss=1.62666, validation/num_examples=3000
I0306 00:25:33.967186 139485295634176 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.4060678482055664, loss=1.737438678741455
I0306 00:26:08.482567 139485287241472 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9224868416786194, loss=1.7606905698776245
I0306 00:26:43.087723 139485295634176 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.40015703439712524, loss=1.750472903251648
I0306 00:27:17.789126 139485287241472 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.48435309529304504, loss=1.7571277618408203
I0306 00:27:52.476360 139485295634176 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.37079739570617676, loss=1.8541022539138794
I0306 00:28:27.207375 139485287241472 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3999137282371521, loss=1.7049214839935303
I0306 00:29:01.874396 139485295634176 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.39695778489112854, loss=1.8393045663833618
I0306 00:29:36.537480 139485287241472 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.38595935702323914, loss=1.7334198951721191
I0306 00:30:11.197978 139485295634176 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3913460075855255, loss=1.6784539222717285
I0306 00:30:45.880558 139485287241472 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.35006994009017944, loss=1.7614632844924927
I0306 00:31:20.542680 139485295634176 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.36526376008987427, loss=1.7111951112747192
I0306 00:31:55.169106 139485287241472 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3610880374908447, loss=1.7605869770050049
I0306 00:32:29.800704 139485295634176 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.3563825488090515, loss=1.8358709812164307
I0306 00:33:04.480655 139485287241472 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.41720181703567505, loss=1.8202005624771118
I0306 00:33:39.090953 139485295634176 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5169817805290222, loss=1.6943602561950684
I0306 00:34:13.683827 139485287241472 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.4529434144496918, loss=1.7274854183197021
I0306 00:34:48.253983 139485295634176 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.42138081789016724, loss=1.8020628690719604
I0306 00:35:22.850925 139485287241472 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.4311186671257019, loss=1.718031644821167
I0306 00:35:57.440140 139485295634176 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4076378047466278, loss=1.7905755043029785
I0306 00:36:31.980211 139485287241472 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.38011667132377625, loss=1.7573323249816895
I0306 00:37:06.552644 139485295634176 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.4146421253681183, loss=1.8470568656921387
I0306 00:37:41.128322 139485287241472 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.39631664752960205, loss=1.7784513235092163
I0306 00:38:15.719541 139485295634176 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.3663714528083801, loss=1.7338536977767944
I0306 00:38:50.319710 139485287241472 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.4779496192932129, loss=1.7705047130584717
I0306 00:39:16.599912 139629069661376 spec.py:321] Evaluating on the training split.
I0306 00:39:19.215490 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:43:30.749888 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 00:43:33.346587 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:46:36.222924 139629069661376 spec.py:349] Evaluating on the test split.
I0306 00:46:38.833723 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 00:49:13.963901 139629069661376 submission_runner.py:469] Time since start: 20148.63s, 	Step: 33977, 	{'train/accuracy': 0.6380448341369629, 'train/loss': 1.7544784545898438, 'train/bleu': 31.0528931223781, 'validation/accuracy': 0.6595432162284851, 'validation/loss': 1.6135351657867432, 'validation/bleu': 27.91781953936735, 'validation/num_examples': 3000, 'test/accuracy': 0.6691229343414307, 'test/loss': 1.5504486560821533, 'test/bleu': 27.819355708265213, 'test/num_examples': 3003, 'score': 11785.367548704147, 'total_duration': 20148.628581762314, 'accumulated_submission_time': 11785.367548704147, 'accumulated_eval_time': 8361.106194972992, 'accumulated_logging_time': 0.26242637634277344}
I0306 00:49:13.975903 139485295634176 logging_writer.py:48] [33977] accumulated_eval_time=8361.11, accumulated_logging_time=0.262426, accumulated_submission_time=11785.4, global_step=33977, preemption_count=0, score=11785.4, test/accuracy=0.669123, test/bleu=27.8194, test/loss=1.55045, test/num_examples=3003, total_duration=20148.6, train/accuracy=0.638045, train/bleu=31.0529, train/loss=1.75448, validation/accuracy=0.659543, validation/bleu=27.9178, validation/loss=1.61354, validation/num_examples=3000
I0306 00:49:22.226560 139485287241472 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.44858860969543457, loss=1.8429980278015137
I0306 00:49:56.672599 139485295634176 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.3792719542980194, loss=1.79230535030365
I0306 00:50:31.128226 139485287241472 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.43812069296836853, loss=1.7932791709899902
I0306 00:51:05.694206 139485295634176 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.38558968901634216, loss=1.7616304159164429
I0306 00:51:40.217240 139485287241472 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.5115485787391663, loss=1.7673840522766113
I0306 00:52:14.806235 139485295634176 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4175165295600891, loss=1.756651759147644
I0306 00:52:49.342952 139485287241472 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.3946326673030853, loss=1.8151592016220093
I0306 00:53:23.921661 139485295634176 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.4241415858268738, loss=1.7403942346572876
I0306 00:53:58.443817 139485287241472 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.36016547679901123, loss=1.7016736268997192
I0306 00:54:33.009510 139485295634176 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.417434424161911, loss=1.8116888999938965
I0306 00:55:07.546031 139485287241472 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.4207271635532379, loss=1.8330835103988647
I0306 00:55:42.092746 139485295634176 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.3935682475566864, loss=1.655451774597168
I0306 00:56:16.642808 139485287241472 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.40540289878845215, loss=1.817003846168518
I0306 00:56:51.200695 139485295634176 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.3745509386062622, loss=1.7510385513305664
I0306 00:57:25.746491 139485287241472 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.371589720249176, loss=1.8051615953445435
I0306 00:58:00.332794 139485295634176 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.39265015721321106, loss=1.7861144542694092
I0306 00:58:34.887673 139485287241472 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.41867688298225403, loss=1.785298466682434
I0306 00:59:09.440090 139485295634176 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.38912469148635864, loss=1.7879046201705933
I0306 00:59:43.993205 139485287241472 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.38941478729248047, loss=1.761078953742981
I0306 01:00:18.519669 139485295634176 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.34666094183921814, loss=1.6972465515136719
I0306 01:00:53.075603 139485287241472 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3564906418323517, loss=1.737527847290039
I0306 01:01:27.623587 139485295634176 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.3349650800228119, loss=1.6972986459732056
I0306 01:02:02.178740 139485287241472 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.4071686565876007, loss=1.7611699104309082
I0306 01:02:36.736323 139485295634176 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.0733476877212524, loss=1.9073927402496338
I0306 01:03:11.308826 139485287241472 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.9418870210647583, loss=1.750260829925537
I0306 01:03:14.086982 139629069661376 spec.py:321] Evaluating on the training split.
I0306 01:03:16.697939 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:07:07.715044 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 01:07:10.313802 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:10:01.374246 139629069661376 spec.py:349] Evaluating on the test split.
I0306 01:10:03.975081 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:13:03.788246 139629069661376 submission_runner.py:469] Time since start: 21578.45s, 	Step: 36409, 	{'train/accuracy': 0.6332497596740723, 'train/loss': 1.8029546737670898, 'train/bleu': 30.968016684484404, 'validation/accuracy': 0.6551306843757629, 'validation/loss': 1.6354548931121826, 'validation/bleu': 28.069561911623605, 'validation/num_examples': 3000, 'test/accuracy': 0.6643726229667664, 'test/loss': 1.5792622566223145, 'test/bleu': 27.233355359681738, 'test/num_examples': 3003, 'score': 12625.336913824081, 'total_duration': 21578.452902317047, 'accumulated_submission_time': 12625.336913824081, 'accumulated_eval_time': 8950.807389974594, 'accumulated_logging_time': 0.2835266590118408}
I0306 01:13:03.799998 139485295634176 logging_writer.py:48] [36409] accumulated_eval_time=8950.81, accumulated_logging_time=0.283527, accumulated_submission_time=12625.3, global_step=36409, preemption_count=0, score=12625.3, test/accuracy=0.664373, test/bleu=27.2334, test/loss=1.57926, test/num_examples=3003, total_duration=21578.5, train/accuracy=0.63325, train/bleu=30.968, train/loss=1.80295, validation/accuracy=0.655131, validation/bleu=28.0696, validation/loss=1.63545, validation/num_examples=3000
I0306 01:13:35.462610 139485287241472 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.43042024970054626, loss=1.71512770652771
I0306 01:14:09.895207 139485295634176 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.4018380641937256, loss=1.7707394361495972
I0306 01:14:44.381864 139485287241472 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.3820006251335144, loss=1.651759147644043
I0306 01:15:18.923124 139485295634176 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3323722779750824, loss=1.8174965381622314
I0306 01:15:53.456947 139485287241472 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3624449074268341, loss=1.7479331493377686
I0306 01:16:28.004514 139485295634176 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.38532280921936035, loss=1.7849905490875244
I0306 01:17:02.577812 139485287241472 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.440367192029953, loss=1.6957554817199707
I0306 01:17:37.122935 139485295634176 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.37906718254089355, loss=1.6860601902008057
I0306 01:18:11.647740 139485287241472 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.3772529363632202, loss=1.745492696762085
I0306 01:18:46.187934 139485295634176 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.39421001076698303, loss=1.7528928518295288
I0306 01:19:20.747616 139485287241472 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.3319101631641388, loss=1.767034649848938
I0306 01:19:55.323376 139485295634176 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.36869311332702637, loss=1.766777753829956
I0306 01:20:29.904531 139485287241472 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.43803974986076355, loss=1.7743841409683228
I0306 01:21:04.483249 139485295634176 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3528673052787781, loss=1.7737611532211304
I0306 01:21:39.036078 139485287241472 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.34818223118782043, loss=1.7321029901504517
I0306 01:22:13.575051 139485295634176 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.38775986433029175, loss=1.847404956817627
I0306 01:22:48.159287 139485287241472 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3599857985973358, loss=1.7219347953796387
I0306 01:23:22.700965 139485295634176 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3902841806411743, loss=1.8218169212341309
I0306 01:23:57.268596 139485287241472 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.3732658326625824, loss=1.7637375593185425
I0306 01:24:31.826538 139485295634176 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.34463369846343994, loss=1.7131227254867554
I0306 01:25:06.379088 139485287241472 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.37212619185447693, loss=1.791569471359253
I0306 01:25:40.935604 139485295634176 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.3704768717288971, loss=1.7467420101165771
I0306 01:26:15.466473 139485287241472 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3401137590408325, loss=1.7469195127487183
I0306 01:26:50.021935 139485295634176 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3637852072715759, loss=1.7932733297348022
I0306 01:27:03.829459 139629069661376 spec.py:321] Evaluating on the training split.
I0306 01:27:06.440629 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:30:23.527420 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 01:30:26.126995 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:33:02.660673 139629069661376 spec.py:349] Evaluating on the test split.
I0306 01:33:05.269630 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:35:48.140983 139629069661376 submission_runner.py:469] Time since start: 22942.81s, 	Step: 38841, 	{'train/accuracy': 0.644031822681427, 'train/loss': 1.7252724170684814, 'train/bleu': 31.59365271856774, 'validation/accuracy': 0.6603094935417175, 'validation/loss': 1.6036081314086914, 'validation/bleu': 28.70528621685984, 'validation/num_examples': 3000, 'test/accuracy': 0.6729927062988281, 'test/loss': 1.5313379764556885, 'test/bleu': 28.08534873260227, 'test/num_examples': 3003, 'score': 13465.22971701622, 'total_duration': 22942.8056640625, 'accumulated_submission_time': 13465.22971701622, 'accumulated_eval_time': 9475.118876934052, 'accumulated_logging_time': 0.30321764945983887}
I0306 01:35:48.152574 139485287241472 logging_writer.py:48] [38841] accumulated_eval_time=9475.12, accumulated_logging_time=0.303218, accumulated_submission_time=13465.2, global_step=38841, preemption_count=0, score=13465.2, test/accuracy=0.672993, test/bleu=28.0853, test/loss=1.53134, test/num_examples=3003, total_duration=22942.8, train/accuracy=0.644032, train/bleu=31.5937, train/loss=1.72527, validation/accuracy=0.660309, validation/bleu=28.7053, validation/loss=1.60361, validation/num_examples=3000
I0306 01:36:08.779205 139485295634176 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.37691426277160645, loss=1.8073481321334839
I0306 01:36:43.201317 139485287241472 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3567813038825989, loss=1.7388226985931396
I0306 01:37:17.691016 139485295634176 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.3670923113822937, loss=1.7709633111953735
I0306 01:37:52.256880 139485287241472 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3649444580078125, loss=1.8159422874450684
I0306 01:38:26.825835 139485295634176 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.38439831137657166, loss=1.7961667776107788
I0306 01:39:01.402049 139485287241472 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.3278273046016693, loss=1.7269980907440186
I0306 01:39:35.993729 139485295634176 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.35873717069625854, loss=1.8116952180862427
I0306 01:40:10.586116 139485287241472 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3665313124656677, loss=1.7658907175064087
I0306 01:40:45.180963 139485295634176 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.4595392346382141, loss=1.7689203023910522
I0306 01:41:19.777239 139485287241472 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.43422114849090576, loss=1.7660573720932007
I0306 01:41:54.442965 139485295634176 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.36802491545677185, loss=1.7195236682891846
I0306 01:42:29.054072 139485287241472 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.35321134328842163, loss=1.7777725458145142
I0306 01:43:03.662314 139485295634176 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.35465046763420105, loss=1.7785316705703735
I0306 01:43:38.235596 139485287241472 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3938552737236023, loss=1.7514169216156006
I0306 01:44:12.826801 139485295634176 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.38773414492607117, loss=1.7319890260696411
I0306 01:44:47.413030 139485287241472 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.3402566909790039, loss=1.8708714246749878
I0306 01:45:22.021156 139485295634176 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3411276638507843, loss=1.7193660736083984
I0306 01:45:56.619116 139485287241472 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.40041878819465637, loss=1.7752727270126343
I0306 01:46:31.197130 139485295634176 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3584243059158325, loss=1.780362844467163
I0306 01:47:05.738945 139485287241472 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.35388079285621643, loss=1.7940101623535156
I0306 01:47:40.324325 139485295634176 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3234459459781647, loss=1.7616358995437622
I0306 01:48:14.917888 139485287241472 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.37345385551452637, loss=1.8027381896972656
I0306 01:48:49.512754 139485295634176 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.3593309223651886, loss=1.6976312398910522
I0306 01:49:24.079094 139485287241472 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.35791391134262085, loss=1.6643965244293213
I0306 01:49:48.303395 139629069661376 spec.py:321] Evaluating on the training split.
I0306 01:49:50.915171 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:53:25.961779 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 01:53:28.564137 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:56:10.770758 139629069661376 spec.py:349] Evaluating on the test split.
I0306 01:56:13.373324 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 01:59:05.835883 139629069661376 submission_runner.py:469] Time since start: 24340.50s, 	Step: 41271, 	{'train/accuracy': 0.6406680941581726, 'train/loss': 1.7505210638046265, 'train/bleu': 31.086580752873505, 'validation/accuracy': 0.6600375771522522, 'validation/loss': 1.600296974182129, 'validation/bleu': 28.318280312187085, 'validation/num_examples': 3000, 'test/accuracy': 0.672610342502594, 'test/loss': 1.5272399187088013, 'test/bleu': 27.61382537255236, 'test/num_examples': 3003, 'score': 14305.239505290985, 'total_duration': 24340.500559568405, 'accumulated_submission_time': 14305.239505290985, 'accumulated_eval_time': 10032.651314973831, 'accumulated_logging_time': 0.32290196418762207}
I0306 01:59:05.847687 139485295634176 logging_writer.py:48] [41271] accumulated_eval_time=10032.7, accumulated_logging_time=0.322902, accumulated_submission_time=14305.2, global_step=41271, preemption_count=0, score=14305.2, test/accuracy=0.67261, test/bleu=27.6138, test/loss=1.52724, test/num_examples=3003, total_duration=24340.5, train/accuracy=0.640668, train/bleu=31.0866, train/loss=1.75052, validation/accuracy=0.660038, validation/bleu=28.3183, validation/loss=1.6003, validation/num_examples=3000
I0306 01:59:16.182006 139485287241472 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3945057690143585, loss=1.7506860494613647
I0306 01:59:50.647746 139485295634176 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.34621405601501465, loss=1.6651798486709595
I0306 02:00:25.183339 139485287241472 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.35750889778137207, loss=1.7345068454742432
I0306 02:00:59.725261 139485295634176 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.33807727694511414, loss=1.7470849752426147
I0306 02:01:34.291613 139485287241472 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3979407846927643, loss=1.7808672189712524
I0306 02:02:08.859511 139485295634176 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.3343341648578644, loss=1.7137227058410645
I0306 02:02:43.449302 139485287241472 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.41349515318870544, loss=1.7422834634780884
I0306 02:03:18.035562 139485295634176 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.38022086024284363, loss=1.675055980682373
I0306 02:03:52.558265 139485287241472 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.36707645654678345, loss=1.7237651348114014
I0306 02:04:27.131079 139485295634176 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3527539074420929, loss=1.727021336555481
I0306 02:05:01.697073 139485287241472 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3806985914707184, loss=1.7094835042953491
I0306 02:05:36.260451 139485295634176 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3883368670940399, loss=1.7607648372650146
I0306 02:06:10.794984 139485287241472 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.44670552015304565, loss=1.8218094110488892
I0306 02:06:45.353116 139485295634176 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.4148869514465332, loss=1.770769476890564
I0306 02:07:19.925350 139485287241472 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.4039340019226074, loss=1.760985016822815
I0306 02:07:54.515580 139485295634176 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.35045191645622253, loss=1.7036380767822266
I0306 02:08:29.123609 139485287241472 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3688773810863495, loss=1.7796372175216675
I0306 02:09:03.700741 139485295634176 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.37535712122917175, loss=1.8097764253616333
I0306 02:09:38.239803 139485287241472 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.35772064328193665, loss=1.7764995098114014
I0306 02:10:12.854981 139485295634176 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.41881415247917175, loss=1.778140902519226
I0306 02:10:47.437372 139485287241472 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3814423382282257, loss=1.760288953781128
I0306 02:11:21.988661 139485295634176 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.37442535161972046, loss=1.7085455656051636
I0306 02:11:56.566547 139485287241472 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.32990291714668274, loss=1.7096667289733887
I0306 02:12:31.175384 139485295634176 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.386254221200943, loss=1.7068003416061401
I0306 02:13:05.770129 139485287241472 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.45172739028930664, loss=1.8470795154571533
I0306 02:13:06.122645 139629069661376 spec.py:321] Evaluating on the training split.
I0306 02:13:08.734582 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 02:17:13.050169 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 02:17:15.644851 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 02:19:59.470259 139629069661376 spec.py:349] Evaluating on the test split.
I0306 02:20:02.071458 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 02:22:49.078403 139629069661376 submission_runner.py:469] Time since start: 25763.74s, 	Step: 43702, 	{'train/accuracy': 0.641524076461792, 'train/loss': 1.7379132509231567, 'train/bleu': 31.190737905214565, 'validation/accuracy': 0.6637208461761475, 'validation/loss': 1.5866236686706543, 'validation/bleu': 28.50399513128774, 'validation/num_examples': 3000, 'test/accuracy': 0.6762484312057495, 'test/loss': 1.5145207643508911, 'test/bleu': 28.305121424896882, 'test/num_examples': 3003, 'score': 15145.375562429428, 'total_duration': 25763.74303984642, 'accumulated_submission_time': 15145.375562429428, 'accumulated_eval_time': 10615.60698056221, 'accumulated_logging_time': 0.3427255153656006}
I0306 02:22:49.093275 139485295634176 logging_writer.py:48] [43702] accumulated_eval_time=10615.6, accumulated_logging_time=0.342726, accumulated_submission_time=15145.4, global_step=43702, preemption_count=0, score=15145.4, test/accuracy=0.676248, test/bleu=28.3051, test/loss=1.51452, test/num_examples=3003, total_duration=25763.7, train/accuracy=0.641524, train/bleu=31.1907, train/loss=1.73791, validation/accuracy=0.663721, validation/bleu=28.504, validation/loss=1.58662, validation/num_examples=3000
I0306 02:23:23.205277 139485287241472 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.4038543701171875, loss=1.7899311780929565
I0306 02:23:57.625725 139485295634176 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3613332211971283, loss=1.7966923713684082
I0306 02:24:32.039074 139485287241472 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3857389986515045, loss=1.7967844009399414
I0306 02:25:06.532204 139485295634176 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.33898305892944336, loss=1.6945775747299194
I0306 02:25:41.001350 139485287241472 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.3994779586791992, loss=1.7782628536224365
I0306 02:26:15.489759 139485295634176 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.40883395075798035, loss=1.6138198375701904
I0306 02:26:49.949438 139485287241472 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.36115869879722595, loss=1.7037407159805298
I0306 02:27:24.417096 139485295634176 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.38088080286979675, loss=1.8045151233673096
I0306 02:27:58.897480 139485287241472 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.36295655369758606, loss=1.7312285900115967
I0306 02:28:33.368748 139485295634176 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.37539902329444885, loss=1.6916590929031372
I0306 02:29:07.874094 139485287241472 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.4091755151748657, loss=1.744693636894226
I0306 02:29:42.374889 139485295634176 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.38043874502182007, loss=1.8067634105682373
I0306 02:30:16.874538 139485287241472 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.330244779586792, loss=1.720203161239624
I0306 02:30:51.330697 139485295634176 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.4010525047779083, loss=1.8051199913024902
I0306 02:31:25.825363 139485287241472 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.42488884925842285, loss=1.73934006690979
I0306 02:32:00.313441 139485295634176 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3888501822948456, loss=1.7633931636810303
I0306 02:32:34.825778 139485287241472 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.40392622351646423, loss=1.7241514921188354
I0306 02:33:09.274801 139485295634176 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.3677053451538086, loss=1.7224054336547852
I0306 02:33:43.745446 139485287241472 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.4537358283996582, loss=1.7608304023742676
I0306 02:34:18.230498 139485295634176 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.4145811200141907, loss=1.7668243646621704
I0306 02:34:52.745589 139485287241472 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.460267037153244, loss=1.8199479579925537
I0306 02:35:27.222594 139485295634176 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.37824979424476624, loss=1.655470371246338
I0306 02:36:01.697678 139485287241472 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.38688144087791443, loss=1.7232340574264526
I0306 02:36:36.181829 139485295634176 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.4218847453594208, loss=1.754785418510437
I0306 02:36:49.278658 139629069661376 spec.py:321] Evaluating on the training split.
I0306 02:36:51.888841 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 02:41:38.135713 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 02:41:40.746170 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 02:44:29.259718 139629069661376 spec.py:349] Evaluating on the test split.
I0306 02:44:31.860970 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 02:47:22.674720 139629069661376 submission_runner.py:469] Time since start: 27237.34s, 	Step: 46139, 	{'train/accuracy': 0.6466573476791382, 'train/loss': 1.7067011594772339, 'train/bleu': 31.18560964709687, 'validation/accuracy': 0.6643636226654053, 'validation/loss': 1.5744129419326782, 'validation/bleu': 28.764355914581728, 'validation/num_examples': 3000, 'test/accuracy': 0.675344705581665, 'test/loss': 1.5105329751968384, 'test/bleu': 28.090970119457992, 'test/num_examples': 3003, 'score': 15985.421673297882, 'total_duration': 27237.339407920837, 'accumulated_submission_time': 15985.421673297882, 'accumulated_eval_time': 11249.003007411957, 'accumulated_logging_time': 0.36733245849609375}
I0306 02:47:22.686951 139485287241472 logging_writer.py:48] [46139] accumulated_eval_time=11249, accumulated_logging_time=0.367332, accumulated_submission_time=15985.4, global_step=46139, preemption_count=0, score=15985.4, test/accuracy=0.675345, test/bleu=28.091, test/loss=1.51053, test/num_examples=3003, total_duration=27237.3, train/accuracy=0.646657, train/bleu=31.1856, train/loss=1.7067, validation/accuracy=0.664364, validation/bleu=28.7644, validation/loss=1.57441, validation/num_examples=3000
I0306 02:47:43.978616 139485295634176 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.38602250814437866, loss=1.8075470924377441
I0306 02:48:18.369519 139485287241472 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.42321693897247314, loss=1.794948935508728
I0306 02:48:52.769392 139485295634176 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.5084003806114197, loss=1.7509199380874634
I0306 02:49:27.252681 139485287241472 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.38146108388900757, loss=1.7902063131332397
I0306 02:50:01.736790 139485295634176 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.5183531641960144, loss=1.7099956274032593
I0306 02:50:36.225684 139485287241472 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3789883255958557, loss=1.7375671863555908
I0306 02:51:10.686602 139485295634176 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.40722551941871643, loss=1.799476146697998
I0306 02:51:45.175247 139485287241472 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.37269118428230286, loss=1.7651053667068481
I0306 02:52:19.661001 139485295634176 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3879096508026123, loss=1.8124167919158936
I0306 02:52:54.127295 139485287241472 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.40057849884033203, loss=1.6538234949111938
I0306 02:53:28.590991 139485295634176 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.3562657833099365, loss=1.7691329717636108
I0306 02:54:03.054402 139485287241472 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.35602867603302, loss=1.7261509895324707
I0306 02:54:37.520556 139485295634176 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.37123221158981323, loss=1.7055952548980713
I0306 02:55:12.016729 139485287241472 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.4003799259662628, loss=1.741203784942627
I0306 02:55:46.489660 139485295634176 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.4392220377922058, loss=1.6537983417510986
I0306 02:56:21.057714 139485287241472 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.38598814606666565, loss=1.657148003578186
I0306 02:56:55.597307 139485295634176 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.32996928691864014, loss=1.6631138324737549
I0306 02:57:30.065936 139485287241472 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.40976130962371826, loss=1.6790825128555298
I0306 02:58:04.554149 139485295634176 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.43064162135124207, loss=1.6895849704742432
I0306 02:58:39.072595 139485287241472 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3596455454826355, loss=1.7490315437316895
I0306 02:59:13.576539 139485295634176 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.33043938875198364, loss=1.7301490306854248
I0306 02:59:48.068186 139485287241472 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.3640388250350952, loss=1.7399414777755737
I0306 03:00:22.578315 139485295634176 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.3732098340988159, loss=1.7372525930404663
I0306 03:00:57.083948 139485287241472 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.36845752596855164, loss=1.692008137702942
I0306 03:01:22.935361 139629069661376 spec.py:321] Evaluating on the training split.
I0306 03:01:25.554292 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:04:39.690245 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 03:04:42.289600 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:07:28.567009 139629069661376 spec.py:349] Evaluating on the test split.
I0306 03:07:31.160692 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:10:05.808435 139629069661376 submission_runner.py:469] Time since start: 28600.47s, 	Step: 48576, 	{'train/accuracy': 0.6445617079734802, 'train/loss': 1.71452796459198, 'train/bleu': 31.336832431731086, 'validation/accuracy': 0.666192889213562, 'validation/loss': 1.570813536643982, 'validation/bleu': 28.719829313074563, 'validation/num_examples': 3000, 'test/accuracy': 0.6782412528991699, 'test/loss': 1.4970651865005493, 'test/bleu': 28.41594754520476, 'test/num_examples': 3003, 'score': 16825.53476881981, 'total_duration': 28600.473111629486, 'accumulated_submission_time': 16825.53476881981, 'accumulated_eval_time': 11771.8760368824, 'accumulated_logging_time': 0.3877696990966797}
I0306 03:10:05.821394 139485295634176 logging_writer.py:48] [48576] accumulated_eval_time=11771.9, accumulated_logging_time=0.38777, accumulated_submission_time=16825.5, global_step=48576, preemption_count=0, score=16825.5, test/accuracy=0.678241, test/bleu=28.4159, test/loss=1.49707, test/num_examples=3003, total_duration=28600.5, train/accuracy=0.644562, train/bleu=31.3368, train/loss=1.71453, validation/accuracy=0.666193, validation/bleu=28.7198, validation/loss=1.57081, validation/num_examples=3000
I0306 03:10:14.392847 139485287241472 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3957037925720215, loss=1.7333568334579468
I0306 03:10:48.746582 139485295634176 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3523325026035309, loss=1.7353564500808716
I0306 03:11:23.127591 139485287241472 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.4149077534675598, loss=1.673769474029541
I0306 03:11:57.569107 139485295634176 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.4232581555843353, loss=1.7356246709823608
I0306 03:12:32.004995 139485287241472 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.39072275161743164, loss=1.6593776941299438
I0306 03:13:06.499049 139485295634176 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.32899045944213867, loss=1.6711844205856323
I0306 03:13:40.967782 139485287241472 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.4455154538154602, loss=1.8249704837799072
I0306 03:14:15.425115 139485295634176 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.524603545665741, loss=1.7614420652389526
I0306 03:14:49.906944 139485287241472 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.34518179297447205, loss=1.7354841232299805
I0306 03:15:24.368647 139485295634176 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.34749117493629456, loss=1.6661962270736694
I0306 03:15:58.812902 139485287241472 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.4004407823085785, loss=1.7497949600219727
I0306 03:16:33.260536 139485295634176 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3784169852733612, loss=1.7007259130477905
I0306 03:17:07.687683 139485287241472 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.34289833903312683, loss=1.7745381593704224
I0306 03:17:42.191606 139485295634176 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.36741864681243896, loss=1.7103815078735352
I0306 03:18:16.651822 139485287241472 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3685171604156494, loss=1.7353790998458862
I0306 03:18:51.102401 139485295634176 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.3568330705165863, loss=1.7061553001403809
I0306 03:19:25.545641 139485287241472 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.43284299969673157, loss=1.6452558040618896
I0306 03:20:00.077570 139485295634176 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.3577762246131897, loss=1.6782933473587036
I0306 03:20:34.604658 139485287241472 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.39877885580062866, loss=1.6813515424728394
I0306 03:21:09.181368 139485295634176 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3542594909667969, loss=1.7336186170578003
I0306 03:21:43.748817 139485287241472 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.35878786444664, loss=1.679463267326355
I0306 03:22:18.264775 139485295634176 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.35930708050727844, loss=1.7057102918624878
I0306 03:22:52.798340 139485287241472 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.34994423389434814, loss=1.6217457056045532
I0306 03:23:27.364959 139485295634176 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.3658594787120819, loss=1.7512214183807373
I0306 03:24:01.902303 139485287241472 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.38818448781967163, loss=1.6786285638809204
I0306 03:24:06.059645 139629069661376 spec.py:321] Evaluating on the training split.
I0306 03:24:08.669705 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:27:32.918233 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 03:27:35.526136 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:30:17.769307 139629069661376 spec.py:349] Evaluating on the test split.
I0306 03:30:20.370899 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:32:59.055797 139629069661376 submission_runner.py:469] Time since start: 29973.72s, 	Step: 51013, 	{'train/accuracy': 0.6554715633392334, 'train/loss': 1.641449213027954, 'train/bleu': 32.2163874836394, 'validation/accuracy': 0.6674535870552063, 'validation/loss': 1.5638982057571411, 'validation/bleu': 28.855917279485706, 'validation/num_examples': 3000, 'test/accuracy': 0.6805931925773621, 'test/loss': 1.4873325824737549, 'test/bleu': 28.607490449108738, 'test/num_examples': 3003, 'score': 17665.63838148117, 'total_duration': 29973.72046995163, 'accumulated_submission_time': 17665.63838148117, 'accumulated_eval_time': 12304.872150182724, 'accumulated_logging_time': 0.4085354804992676}
I0306 03:32:59.068460 139485295634176 logging_writer.py:48] [51013] accumulated_eval_time=12304.9, accumulated_logging_time=0.408535, accumulated_submission_time=17665.6, global_step=51013, preemption_count=0, score=17665.6, test/accuracy=0.680593, test/bleu=28.6075, test/loss=1.48733, test/num_examples=3003, total_duration=29973.7, train/accuracy=0.655472, train/bleu=32.2164, train/loss=1.64145, validation/accuracy=0.667454, validation/bleu=28.8559, validation/loss=1.5639, validation/num_examples=3000
I0306 03:33:29.352209 139485287241472 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3605574071407318, loss=1.745282530784607
I0306 03:34:03.775839 139485295634176 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.39665332436561584, loss=1.6812208890914917
I0306 03:34:38.320416 139485287241472 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.4113236665725708, loss=1.7743371725082397
I0306 03:35:12.893130 139485295634176 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.35692131519317627, loss=1.7411280870437622
I0306 03:35:47.469839 139485287241472 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.332716703414917, loss=1.71293044090271
I0306 03:36:22.053028 139485295634176 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.352765291929245, loss=1.687188982963562
I0306 03:36:56.603278 139485287241472 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.36022329330444336, loss=1.6829394102096558
I0306 03:37:31.178571 139485295634176 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.37592124938964844, loss=1.7161635160446167
I0306 03:38:05.713865 139485287241472 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.4006645083427429, loss=1.7583119869232178
I0306 03:38:40.284380 139485295634176 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.4089573919773102, loss=1.7176496982574463
I0306 03:39:14.843394 139485287241472 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3412492871284485, loss=1.7249001264572144
I0306 03:39:49.405738 139485295634176 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.37718692421913147, loss=1.691810131072998
I0306 03:40:23.944261 139485287241472 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.3670250177383423, loss=1.6985912322998047
I0306 03:40:58.484280 139485295634176 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.3863644003868103, loss=1.6879181861877441
I0306 03:41:33.025810 139485287241472 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3800109028816223, loss=1.7425105571746826
I0306 03:42:07.577803 139485295634176 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3325755298137665, loss=1.6659127473831177
I0306 03:42:42.175605 139485287241472 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3868386745452881, loss=1.6756134033203125
I0306 03:43:16.765011 139485295634176 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.34112074971199036, loss=1.767244577407837
I0306 03:43:51.315940 139485287241472 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3980903625488281, loss=1.7295042276382446
I0306 03:44:25.890973 139485295634176 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.36453795433044434, loss=1.624567985534668
I0306 03:45:00.469408 139485287241472 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.40675801038742065, loss=1.7631795406341553
I0306 03:45:35.051416 139485295634176 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.368755578994751, loss=1.6770421266555786
I0306 03:46:09.631120 139485287241472 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.4040996730327606, loss=1.7677664756774902
I0306 03:46:44.189792 139485295634176 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.4021206498146057, loss=1.618031620979309
I0306 03:46:59.062004 139629069661376 spec.py:321] Evaluating on the training split.
I0306 03:47:01.681575 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:51:06.396983 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 03:51:09.007329 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:53:59.063258 139629069661376 spec.py:349] Evaluating on the test split.
I0306 03:54:01.659304 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 03:57:10.305679 139629069661376 submission_runner.py:469] Time since start: 31424.97s, 	Step: 53444, 	{'train/accuracy': 0.6479129791259766, 'train/loss': 1.6979836225509644, 'train/bleu': 31.993127864367104, 'validation/accuracy': 0.6690109372138977, 'validation/loss': 1.5569850206375122, 'validation/bleu': 28.822276867453446, 'validation/num_examples': 3000, 'test/accuracy': 0.6821573376655579, 'test/loss': 1.4822558164596558, 'test/bleu': 28.995730783884945, 'test/num_examples': 3003, 'score': 18505.494869709015, 'total_duration': 31424.970360040665, 'accumulated_submission_time': 18505.494869709015, 'accumulated_eval_time': 12916.115787029266, 'accumulated_logging_time': 0.42891979217529297}
I0306 03:57:10.318532 139485287241472 logging_writer.py:48] [53444] accumulated_eval_time=12916.1, accumulated_logging_time=0.42892, accumulated_submission_time=18505.5, global_step=53444, preemption_count=0, score=18505.5, test/accuracy=0.682157, test/bleu=28.9957, test/loss=1.48226, test/num_examples=3003, total_duration=31425, train/accuracy=0.647913, train/bleu=31.9931, train/loss=1.69798, validation/accuracy=0.669011, validation/bleu=28.8223, validation/loss=1.55699, validation/num_examples=3000
I0306 03:57:29.972475 139485295634176 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3536204397678375, loss=1.6361862421035767
I0306 03:58:04.372249 139485287241472 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.38369399309158325, loss=1.731031894683838
I0306 03:58:38.893926 139485295634176 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.369960218667984, loss=1.68540620803833
I0306 03:59:13.431084 139485287241472 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.32897135615348816, loss=1.631474256515503
I0306 03:59:47.980595 139485295634176 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.43141859769821167, loss=1.7726579904556274
I0306 04:00:22.516546 139485287241472 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3842748999595642, loss=1.7818615436553955
I0306 04:00:57.081741 139485295634176 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3748466372489929, loss=1.7264786958694458
I0306 04:01:31.640613 139485287241472 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.35017096996307373, loss=1.694196105003357
I0306 04:02:06.164967 139485295634176 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3726855218410492, loss=1.6778068542480469
I0306 04:02:40.727548 139485287241472 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.39730221033096313, loss=1.7040860652923584
I0306 04:03:15.280208 139485295634176 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.34208694100379944, loss=1.609908103942871
I0306 04:03:49.810265 139485287241472 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3432660400867462, loss=1.6274484395980835
I0306 04:04:24.350925 139485295634176 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.36337462067604065, loss=1.800277829170227
I0306 04:04:58.933833 139485287241472 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.36185747385025024, loss=1.6352571249008179
I0306 04:05:33.505162 139485295634176 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.36766326427459717, loss=1.7606359720230103
I0306 04:06:08.054000 139485287241472 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.39191582798957825, loss=1.659101963043213
I0306 04:06:42.642897 139485295634176 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.37826624512672424, loss=1.7513762712478638
I0306 04:07:17.227865 139485287241472 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.33621644973754883, loss=1.7108875513076782
I0306 04:07:51.798520 139485295634176 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3939080238342285, loss=1.737200379371643
I0306 04:08:26.363611 139485287241472 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3663186728954315, loss=1.6583470106124878
I0306 04:09:00.913988 139485295634176 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.37945881485939026, loss=1.6820003986358643
I0306 04:09:35.474161 139485287241472 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.37984785437583923, loss=1.739656925201416
I0306 04:10:10.060586 139485295634176 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.4033358097076416, loss=1.8246821165084839
I0306 04:10:44.654574 139485287241472 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.37094682455062866, loss=1.6233001947402954
I0306 04:11:10.594028 139629069661376 spec.py:321] Evaluating on the training split.
I0306 04:11:13.211690 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 04:14:47.436480 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 04:14:50.030285 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 04:17:25.756751 139629069661376 spec.py:349] Evaluating on the test split.
I0306 04:17:28.353529 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 04:20:13.202131 139629069661376 submission_runner.py:469] Time since start: 32807.87s, 	Step: 55876, 	{'train/accuracy': 0.6456969976425171, 'train/loss': 1.7108418941497803, 'train/bleu': 31.53783157345093, 'validation/accuracy': 0.6695671677589417, 'validation/loss': 1.5440362691879272, 'validation/bleu': 29.17804894591293, 'validation/num_examples': 3000, 'test/accuracy': 0.6823890805244446, 'test/loss': 1.4736313819885254, 'test/bleu': 28.66418574733848, 'test/num_examples': 3003, 'score': 19345.63218808174, 'total_duration': 32807.86678314209, 'accumulated_submission_time': 19345.63218808174, 'accumulated_eval_time': 13458.72381901741, 'accumulated_logging_time': 0.45050048828125}
I0306 04:20:13.216609 139485295634176 logging_writer.py:48] [55876] accumulated_eval_time=13458.7, accumulated_logging_time=0.4505, accumulated_submission_time=19345.6, global_step=55876, preemption_count=0, score=19345.6, test/accuracy=0.682389, test/bleu=28.6642, test/loss=1.47363, test/num_examples=3003, total_duration=32807.9, train/accuracy=0.645697, train/bleu=31.5378, train/loss=1.71084, validation/accuracy=0.669567, validation/bleu=29.178, validation/loss=1.54404, validation/num_examples=3000
I0306 04:20:21.828629 139485287241472 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.37514302134513855, loss=1.646471381187439
I0306 04:20:56.220566 139485295634176 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.3766760528087616, loss=1.7877564430236816
I0306 04:21:30.715024 139485287241472 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3530847430229187, loss=1.6934560537338257
I0306 04:22:05.206068 139485295634176 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.38427862524986267, loss=1.7722615003585815
I0306 04:22:39.736943 139485287241472 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.3702332377433777, loss=1.6868963241577148
I0306 04:23:14.275658 139485295634176 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.37606707215309143, loss=1.620813250541687
I0306 04:23:48.887564 139485287241472 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.41047123074531555, loss=1.6769740581512451
I0306 04:24:23.420302 139485295634176 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.39653921127319336, loss=1.683427333831787
I0306 04:24:57.938442 139485287241472 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.46832653880119324, loss=1.7371236085891724
I0306 04:25:32.477337 139485295634176 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3431338667869568, loss=1.6366480588912964
I0306 04:26:07.003023 139485287241472 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.38274067640304565, loss=1.6718977689743042
I0306 04:26:41.534653 139485295634176 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3582225441932678, loss=1.6880525350570679
I0306 04:27:16.076002 139485287241472 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.35099896788597107, loss=1.6332095861434937
I0306 04:27:50.597822 139485295634176 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.35938775539398193, loss=1.6934202909469604
I0306 04:28:25.121805 139485287241472 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3639971613883972, loss=1.744808554649353
I0306 04:28:59.673252 139485295634176 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.37222808599472046, loss=1.6849898099899292
I0306 04:29:34.182688 139485287241472 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3856408894062042, loss=1.6626909971237183
I0306 04:30:08.741033 139485295634176 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3856484293937683, loss=1.7416132688522339
I0306 04:30:43.297401 139485287241472 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.3677513599395752, loss=1.7086219787597656
I0306 04:31:17.855727 139485295634176 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.3618684709072113, loss=1.7098714113235474
I0306 04:31:52.398395 139485287241472 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.38830459117889404, loss=1.6692157983779907
I0306 04:32:26.974771 139485295634176 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3961530923843384, loss=1.6923153400421143
I0306 04:33:01.536868 139485287241472 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.36807164549827576, loss=1.6129438877105713
I0306 04:33:36.100302 139485295634176 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.38851025700569153, loss=1.6892434358596802
I0306 04:34:10.712819 139485287241472 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.4153596758842468, loss=1.714249849319458
I0306 04:34:13.498123 139629069661376 spec.py:321] Evaluating on the training split.
I0306 04:34:16.121928 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 04:38:30.349239 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 04:38:32.948953 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 04:41:27.606121 139629069661376 spec.py:349] Evaluating on the test split.
I0306 04:41:30.211214 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 04:44:14.556153 139629069661376 submission_runner.py:469] Time since start: 34249.22s, 	Step: 58309, 	{'train/accuracy': 0.6540137529373169, 'train/loss': 1.6488738059997559, 'train/bleu': 32.069103966589715, 'validation/accuracy': 0.6717796325683594, 'validation/loss': 1.5363051891326904, 'validation/bleu': 29.123070046912968, 'validation/num_examples': 3000, 'test/accuracy': 0.6833738684654236, 'test/loss': 1.4707404375076294, 'test/bleu': 28.560709831761415, 'test/num_examples': 3003, 'score': 20185.77959752083, 'total_duration': 34249.220818042755, 'accumulated_submission_time': 20185.77959752083, 'accumulated_eval_time': 14059.781799316406, 'accumulated_logging_time': 0.4727177619934082}
I0306 04:44:14.570187 139485295634176 logging_writer.py:48] [58309] accumulated_eval_time=14059.8, accumulated_logging_time=0.472718, accumulated_submission_time=20185.8, global_step=58309, preemption_count=0, score=20185.8, test/accuracy=0.683374, test/bleu=28.5607, test/loss=1.47074, test/num_examples=3003, total_duration=34249.2, train/accuracy=0.654014, train/bleu=32.0691, train/loss=1.64887, validation/accuracy=0.67178, validation/bleu=29.1231, validation/loss=1.53631, validation/num_examples=3000
I0306 04:44:46.221189 139485287241472 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.3945818245410919, loss=1.7020529508590698
I0306 04:45:20.665046 139485295634176 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.4519776999950409, loss=1.7227461338043213
I0306 04:45:55.169651 139485287241472 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3713531494140625, loss=1.681123971939087
I0306 04:46:29.714491 139485295634176 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3691841661930084, loss=1.694114089012146
I0306 04:47:04.265961 139485287241472 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.38007092475891113, loss=1.6894681453704834
I0306 04:47:38.794252 139485295634176 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3933197855949402, loss=1.663438081741333
I0306 04:48:13.367124 139485287241472 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.4024393558502197, loss=1.7119680643081665
I0306 04:48:47.881579 139485295634176 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.3559167981147766, loss=1.7387698888778687
I0306 04:49:22.440148 139485287241472 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.36294588446617126, loss=1.6709322929382324
I0306 04:49:56.980358 139485295634176 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.37630021572113037, loss=1.6834065914154053
I0306 04:50:31.509919 139485287241472 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3884707987308502, loss=1.7640644311904907
I0306 04:51:06.039036 139485295634176 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3866715729236603, loss=1.7190351486206055
I0306 04:51:40.579245 139485287241472 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.4348371922969818, loss=1.6544221639633179
I0306 04:52:15.141467 139485295634176 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3442179560661316, loss=1.6929200887680054
I0306 04:52:49.671694 139485287241472 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.4372578561306, loss=1.735658884048462
I0306 04:53:24.209609 139485295634176 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.38552412390708923, loss=1.61611008644104
I0306 04:53:58.762627 139485287241472 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.3900163769721985, loss=1.5481374263763428
I0306 04:54:33.290309 139485295634176 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.36299169063568115, loss=1.6696590185165405
I0306 04:55:07.800610 139485287241472 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3787706792354584, loss=1.7495781183242798
I0306 04:55:42.322891 139485295634176 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.37842467427253723, loss=1.594048261642456
I0306 04:56:16.876916 139485287241472 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.32369881868362427, loss=1.6465833187103271
I0306 04:56:51.432143 139485295634176 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.4501669406890869, loss=1.6068960428237915
I0306 04:57:25.985299 139485287241472 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.38993507623672485, loss=1.6660971641540527
I0306 04:58:00.497525 139485295634176 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.38524261116981506, loss=1.732803463935852
I0306 04:58:14.642611 139629069661376 spec.py:321] Evaluating on the training split.
I0306 04:58:17.258527 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:01:51.049296 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 05:01:53.649326 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:04:32.483861 139629069661376 spec.py:349] Evaluating on the test split.
I0306 05:04:35.091856 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:07:12.228108 139629069661376 submission_runner.py:469] Time since start: 35626.89s, 	Step: 60742, 	{'train/accuracy': 0.6510465145111084, 'train/loss': 1.6649905443191528, 'train/bleu': 31.73757337088702, 'validation/accuracy': 0.6714829802513123, 'validation/loss': 1.5333079099655151, 'validation/bleu': 29.144631008834217, 'validation/num_examples': 3000, 'test/accuracy': 0.6874058842658997, 'test/loss': 1.4529725313186646, 'test/bleu': 29.015509816789326, 'test/num_examples': 3003, 'score': 21025.717707395554, 'total_duration': 35626.89278316498, 'accumulated_submission_time': 21025.717707395554, 'accumulated_eval_time': 14597.367243289948, 'accumulated_logging_time': 0.49491024017333984}
I0306 05:07:12.241937 139485287241472 logging_writer.py:48] [60742] accumulated_eval_time=14597.4, accumulated_logging_time=0.49491, accumulated_submission_time=21025.7, global_step=60742, preemption_count=0, score=21025.7, test/accuracy=0.687406, test/bleu=29.0155, test/loss=1.45297, test/num_examples=3003, total_duration=35626.9, train/accuracy=0.651047, train/bleu=31.7376, train/loss=1.66499, validation/accuracy=0.671483, validation/bleu=29.1446, validation/loss=1.53331, validation/num_examples=3000
I0306 05:07:32.506356 139485295634176 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.3638584017753601, loss=1.5937929153442383
I0306 05:08:06.867266 139485287241472 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.39475303888320923, loss=1.6945592164993286
I0306 05:08:41.332648 139485295634176 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.37685152888298035, loss=1.708998441696167
I0306 05:09:15.858575 139485287241472 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.3891032338142395, loss=1.6972129344940186
I0306 05:09:50.359598 139485295634176 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3608010709285736, loss=1.6049200296401978
I0306 05:10:24.946520 139485287241472 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.37013188004493713, loss=1.715178370475769
I0306 05:10:59.466800 139485295634176 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.37303560972213745, loss=1.7104967832565308
I0306 05:11:33.998069 139485287241472 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.34706199169158936, loss=1.6602176427841187
I0306 05:12:08.523188 139485295634176 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.37198591232299805, loss=1.6703553199768066
I0306 05:12:43.039022 139485287241472 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.364605575799942, loss=1.6297821998596191
I0306 05:13:17.569984 139485295634176 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.4112548232078552, loss=1.7766401767730713
I0306 05:13:52.098818 139485287241472 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3990836441516876, loss=1.6744754314422607
I0306 05:14:26.644173 139485295634176 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.41440799832344055, loss=1.6260935068130493
I0306 05:15:01.262411 139485287241472 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.37260758876800537, loss=1.6885125637054443
I0306 05:15:35.799549 139485295634176 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.3849182724952698, loss=1.6599347591400146
I0306 05:16:10.323436 139485287241472 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.3879753351211548, loss=1.7795689105987549
I0306 05:16:44.891347 139485295634176 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.40182358026504517, loss=1.690261960029602
I0306 05:17:19.447357 139485287241472 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.36244305968284607, loss=1.6099249124526978
I0306 05:17:53.966853 139485295634176 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3729812800884247, loss=1.6825810670852661
I0306 05:18:28.502084 139485287241472 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.36039263010025024, loss=1.7218012809753418
I0306 05:19:02.999908 139485295634176 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.3508851230144501, loss=1.684108853340149
I0306 05:19:37.486485 139485287241472 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3437645137310028, loss=1.6202634572982788
I0306 05:20:11.985279 139485295634176 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.35510367155075073, loss=1.6359543800354004
I0306 05:20:46.470994 139485287241472 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.38211166858673096, loss=1.6815887689590454
I0306 05:21:12.425132 139629069661376 spec.py:321] Evaluating on the training split.
I0306 05:21:15.048260 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:24:44.046288 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 05:24:46.653449 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:27:40.742773 139629069661376 spec.py:349] Evaluating on the test split.
I0306 05:27:43.350428 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:30:57.619589 139629069661376 submission_runner.py:469] Time since start: 37052.28s, 	Step: 63176, 	{'train/accuracy': 0.6708105802536011, 'train/loss': 1.5459836721420288, 'train/bleu': 33.10576217857777, 'validation/accuracy': 0.6734482049942017, 'validation/loss': 1.5189322233200073, 'validation/bleu': 29.308808644526323, 'validation/num_examples': 3000, 'test/accuracy': 0.6857722401618958, 'test/loss': 1.4500166177749634, 'test/bleu': 29.057067708043235, 'test/num_examples': 3003, 'score': 21865.76618027687, 'total_duration': 37052.284259557724, 'accumulated_submission_time': 21865.76618027687, 'accumulated_eval_time': 15182.561658143997, 'accumulated_logging_time': 0.517808198928833}
I0306 05:30:57.634217 139485295634176 logging_writer.py:48] [63176] accumulated_eval_time=15182.6, accumulated_logging_time=0.517808, accumulated_submission_time=21865.8, global_step=63176, preemption_count=0, score=21865.8, test/accuracy=0.685772, test/bleu=29.0571, test/loss=1.45002, test/num_examples=3003, total_duration=37052.3, train/accuracy=0.670811, train/bleu=33.1058, train/loss=1.54598, validation/accuracy=0.673448, validation/bleu=29.3088, validation/loss=1.51893, validation/num_examples=3000
I0306 05:31:06.222742 139485287241472 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.385570764541626, loss=1.6800403594970703
I0306 05:31:40.551443 139485295634176 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.40017732977867126, loss=1.7767422199249268
I0306 05:32:14.944386 139485287241472 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.346651554107666, loss=1.6549078226089478
I0306 05:32:49.440018 139485295634176 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3565515875816345, loss=1.6307110786437988
I0306 05:33:23.924278 139485287241472 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3607091009616852, loss=1.6634103059768677
I0306 05:33:58.399583 139485295634176 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.34521612524986267, loss=1.6873372793197632
I0306 05:34:32.893617 139485287241472 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.38617101311683655, loss=1.7017879486083984
I0306 05:35:07.419051 139485295634176 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3891970217227936, loss=1.6853187084197998
I0306 05:35:41.934590 139485287241472 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3827240467071533, loss=1.6377540826797485
I0306 05:36:16.451262 139485295634176 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.5987101793289185, loss=1.6301584243774414
I0306 05:36:50.940097 139485287241472 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.3570055365562439, loss=1.6195476055145264
I0306 05:37:25.441332 139485295634176 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3933218717575073, loss=1.602878451347351
I0306 05:37:59.932296 139485287241472 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.362964004278183, loss=1.7090815305709839
I0306 05:38:34.470694 139485295634176 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.37816041707992554, loss=1.6354604959487915
I0306 05:39:09.003360 139485287241472 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.5076770186424255, loss=1.6665468215942383
I0306 05:39:43.529887 139485295634176 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.339371919631958, loss=1.724468469619751
I0306 05:40:18.056430 139485287241472 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.3705008029937744, loss=1.672093391418457
I0306 05:40:52.531503 139485295634176 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.4418504238128662, loss=1.7296212911605835
I0306 05:41:27.052985 139485287241472 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3694498538970947, loss=1.6664423942565918
I0306 05:42:01.559783 139485295634176 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3657035529613495, loss=1.608666181564331
I0306 05:42:36.085810 139485287241472 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.39521685242652893, loss=1.6428664922714233
I0306 05:43:10.580704 139485295634176 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.37627607583999634, loss=1.6099951267242432
I0306 05:43:45.079756 139485287241472 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.34860068559646606, loss=1.618111252784729
I0306 05:44:19.602935 139485295634176 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.36720678210258484, loss=1.6781562566757202
I0306 05:44:54.125265 139485287241472 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3863704204559326, loss=1.599290132522583
I0306 05:44:57.924221 139629069661376 spec.py:321] Evaluating on the training split.
I0306 05:45:00.542273 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:48:55.197892 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 05:48:57.786373 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:51:51.272536 139629069661376 spec.py:349] Evaluating on the test split.
I0306 05:51:53.882573 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 05:54:32.168542 139629069661376 submission_runner.py:469] Time since start: 38466.83s, 	Step: 65612, 	{'train/accuracy': 0.6556352972984314, 'train/loss': 1.6450740098953247, 'train/bleu': 32.32858308651338, 'validation/accuracy': 0.6741898059844971, 'validation/loss': 1.515145182609558, 'validation/bleu': 29.320158284295537, 'validation/num_examples': 3000, 'test/accuracy': 0.6878229975700378, 'test/loss': 1.4411917924880981, 'test/bleu': 29.242922385852413, 'test/num_examples': 3003, 'score': 22705.92084646225, 'total_duration': 38466.83321976662, 'accumulated_submission_time': 22705.92084646225, 'accumulated_eval_time': 15756.805934429169, 'accumulated_logging_time': 0.5413532257080078}
I0306 05:54:32.182904 139485295634176 logging_writer.py:48] [65612] accumulated_eval_time=15756.8, accumulated_logging_time=0.541353, accumulated_submission_time=22705.9, global_step=65612, preemption_count=0, score=22705.9, test/accuracy=0.687823, test/bleu=29.2429, test/loss=1.44119, test/num_examples=3003, total_duration=38466.8, train/accuracy=0.655635, train/bleu=32.3286, train/loss=1.64507, validation/accuracy=0.67419, validation/bleu=29.3202, validation/loss=1.51515, validation/num_examples=3000
I0306 05:55:02.733761 139485287241472 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.3812144100666046, loss=1.6849185228347778
I0306 05:55:37.127519 139485295634176 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.3608494699001312, loss=1.6162723302841187
I0306 05:56:11.552989 139485287241472 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.40434277057647705, loss=1.6641640663146973
I0306 05:56:46.023668 139485295634176 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.3442559540271759, loss=1.6381738185882568
I0306 05:57:20.537618 139485287241472 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3514549434185028, loss=1.6201095581054688
I0306 05:57:55.000899 139485295634176 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.372638076543808, loss=1.6567676067352295
I0306 05:58:29.431946 139485287241472 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.38371455669403076, loss=1.643844723701477
I0306 05:59:03.896210 139485295634176 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3707473576068878, loss=1.656884789466858
I0306 05:59:38.344090 139485287241472 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.37672188878059387, loss=1.679328441619873
I0306 06:00:12.833505 139485295634176 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.36786049604415894, loss=1.6703606843948364
I0306 06:00:47.273149 139485287241472 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3718312978744507, loss=1.5866633653640747
I0306 06:01:21.741669 139485295634176 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3715745806694031, loss=1.6322249174118042
I0306 06:01:56.195158 139485287241472 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.35011449456214905, loss=1.6395151615142822
I0306 06:02:30.636151 139485295634176 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3637099266052246, loss=1.539528489112854
I0306 06:03:05.140484 139485287241472 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3980158269405365, loss=1.6366597414016724
I0306 06:03:39.603973 139485295634176 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.38530269265174866, loss=1.584177851676941
I0306 06:04:14.079087 139485287241472 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.39471423625946045, loss=1.7622969150543213
I0306 06:04:48.553963 139485295634176 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.40591931343078613, loss=1.6841336488723755
I0306 06:05:23.041366 139485287241472 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3975732922554016, loss=1.6676262617111206
I0306 06:05:57.524867 139485295634176 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.38669687509536743, loss=1.719858169555664
I0306 06:06:31.979864 139485287241472 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.37410473823547363, loss=1.628406047821045
I0306 06:07:06.450169 139485295634176 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.4380025565624237, loss=1.685603380203247
I0306 06:07:40.932089 139485287241472 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.36764416098594666, loss=1.593492031097412
I0306 06:08:15.401790 139485295634176 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3664596676826477, loss=1.6021989583969116
I0306 06:08:32.316476 139629069661376 spec.py:321] Evaluating on the training split.
I0306 06:08:34.927437 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 06:11:59.945758 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 06:12:02.551874 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 06:14:53.233516 139629069661376 spec.py:349] Evaluating on the test split.
I0306 06:14:55.836500 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 06:17:35.935281 139629069661376 submission_runner.py:469] Time since start: 39850.60s, 	Step: 68050, 	{'train/accuracy': 0.6523852944374084, 'train/loss': 1.666360855102539, 'train/bleu': 31.95014051580668, 'validation/accuracy': 0.6741156578063965, 'validation/loss': 1.5079776048660278, 'validation/bleu': 29.472558254503635, 'validation/num_examples': 3000, 'test/accuracy': 0.6910207271575928, 'test/loss': 1.4314724206924438, 'test/bleu': 29.339185119097035, 'test/num_examples': 3003, 'score': 23545.919609069824, 'total_duration': 39850.59995150566, 'accumulated_submission_time': 23545.919609069824, 'accumulated_eval_time': 16300.424684047699, 'accumulated_logging_time': 0.5649464130401611}
I0306 06:17:35.949762 139485287241472 logging_writer.py:48] [68050] accumulated_eval_time=16300.4, accumulated_logging_time=0.564946, accumulated_submission_time=23545.9, global_step=68050, preemption_count=0, score=23545.9, test/accuracy=0.691021, test/bleu=29.3392, test/loss=1.43147, test/num_examples=3003, total_duration=39850.6, train/accuracy=0.652385, train/bleu=31.9501, train/loss=1.66636, validation/accuracy=0.674116, validation/bleu=29.4726, validation/loss=1.50798, validation/num_examples=3000
I0306 06:17:53.486343 139485295634176 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.41201505064964294, loss=1.6296411752700806
I0306 06:18:27.829459 139485287241472 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.4029984474182129, loss=1.698585867881775
I0306 06:19:02.254472 139485295634176 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3865855634212494, loss=1.7046207189559937
I0306 06:19:36.740209 139485287241472 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.397896409034729, loss=1.6997506618499756
I0306 06:20:11.212595 139485295634176 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3551429212093353, loss=1.6514778137207031
I0306 06:20:45.710475 139485287241472 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.38411515951156616, loss=1.6540428400039673
I0306 06:21:20.190517 139485295634176 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.3481586277484894, loss=1.6137292385101318
I0306 06:21:54.658226 139485287241472 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3623043894767761, loss=1.6492993831634521
I0306 06:22:29.163449 139485295634176 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.34452614188194275, loss=1.6378382444381714
I0306 06:23:03.671717 139485287241472 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.38789716362953186, loss=1.6583043336868286
I0306 06:23:38.226188 139485295634176 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3929468095302582, loss=1.68256676197052
I0306 06:24:12.760271 139485287241472 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.39375391602516174, loss=1.6159789562225342
I0306 06:24:47.308861 139485295634176 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.36533114314079285, loss=1.6012028455734253
I0306 06:25:21.875237 139485287241472 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3663657009601593, loss=1.531538724899292
I0306 06:25:56.438996 139485295634176 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.41978639364242554, loss=1.6402416229248047
I0306 06:26:30.997873 139485287241472 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3784888684749603, loss=1.6398966312408447
I0306 06:27:05.543539 139485295634176 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3807310461997986, loss=1.6224664449691772
I0306 06:27:40.097732 139485287241472 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3678677976131439, loss=1.5608288049697876
I0306 06:28:14.686536 139485295634176 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.4039113223552704, loss=1.6568528413772583
I0306 06:28:49.252854 139485287241472 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3521479070186615, loss=1.6188591718673706
I0306 06:29:23.821964 139485295634176 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3927588164806366, loss=1.5916558504104614
I0306 06:29:58.348248 139485287241472 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.39537113904953003, loss=1.591086983680725
I0306 06:30:32.896801 139485295634176 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.36714956164360046, loss=1.6044989824295044
I0306 06:31:07.449296 139485287241472 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.38941720128059387, loss=1.6189749240875244
I0306 06:31:36.128926 139629069661376 spec.py:321] Evaluating on the training split.
I0306 06:31:38.743167 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 06:35:28.864453 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 06:35:31.467957 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 06:38:19.061716 139629069661376 spec.py:349] Evaluating on the test split.
I0306 06:38:21.658203 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 06:41:21.786328 139629069661376 submission_runner.py:469] Time since start: 41276.45s, 	Step: 70484, 	{'train/accuracy': 0.6639842987060547, 'train/loss': 1.596910834312439, 'train/bleu': 33.22425828389636, 'validation/accuracy': 0.6769831776618958, 'validation/loss': 1.4955430030822754, 'validation/bleu': 29.64503239183237, 'validation/num_examples': 3000, 'test/accuracy': 0.6908701062202454, 'test/loss': 1.4201960563659668, 'test/bleu': 29.216032825318784, 'test/num_examples': 3003, 'score': 24385.96539258957, 'total_duration': 41276.45101213455, 'accumulated_submission_time': 24385.96539258957, 'accumulated_eval_time': 16886.082045793533, 'accumulated_logging_time': 0.587174654006958}
I0306 06:41:21.801337 139485295634176 logging_writer.py:48] [70484] accumulated_eval_time=16886.1, accumulated_logging_time=0.587175, accumulated_submission_time=24386, global_step=70484, preemption_count=0, score=24386, test/accuracy=0.69087, test/bleu=29.216, test/loss=1.4202, test/num_examples=3003, total_duration=41276.5, train/accuracy=0.663984, train/bleu=33.2243, train/loss=1.59691, validation/accuracy=0.676983, validation/bleu=29.645, validation/loss=1.49554, validation/num_examples=3000
I0306 06:41:27.647614 139485287241472 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.38049840927124023, loss=1.6428163051605225
I0306 06:42:02.054764 139485295634176 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.3699566125869751, loss=1.623010277748108
I0306 06:42:36.547221 139485287241472 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3817121982574463, loss=1.5671699047088623
I0306 06:43:11.054020 139485295634176 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.38870763778686523, loss=1.5797982215881348
I0306 06:43:45.592499 139485287241472 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.39594051241874695, loss=1.5601880550384521
I0306 06:44:20.144676 139485295634176 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.36686593294143677, loss=1.603216528892517
I0306 06:44:54.689335 139485287241472 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.4638523459434509, loss=1.6281883716583252
I0306 06:45:29.230676 139485295634176 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.3766791820526123, loss=1.6164606809616089
I0306 06:46:03.736807 139485287241472 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.41142553091049194, loss=1.6291559934616089
I0306 06:46:38.256850 139485295634176 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.4015448987483978, loss=1.6637085676193237
I0306 06:47:12.778080 139485287241472 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3698529005050659, loss=1.6104836463928223
I0306 06:47:47.305589 139485295634176 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.36994653940200806, loss=1.617830514907837
I0306 06:48:21.853589 139485287241472 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.40105754137039185, loss=1.6595885753631592
I0306 06:48:56.404221 139485295634176 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.3777620196342468, loss=1.6362357139587402
I0306 06:49:30.970493 139485287241472 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3888845443725586, loss=1.662986159324646
I0306 06:50:05.494275 139485295634176 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3672190010547638, loss=1.6055680513381958
I0306 06:50:40.011179 139485287241472 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.4140089750289917, loss=1.6165295839309692
I0306 06:51:14.539200 139485295634176 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.39999517798423767, loss=1.6356629133224487
I0306 06:51:49.060835 139485287241472 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.38482943177223206, loss=1.5282117128372192
I0306 06:52:23.815282 139485295634176 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.390642374753952, loss=1.6257439851760864
I0306 06:52:58.473372 139485287241472 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.4012795686721802, loss=1.5298312902450562
I0306 06:53:33.152017 139485295634176 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3833189010620117, loss=1.5073603391647339
I0306 06:54:07.827312 139485287241472 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.3512578308582306, loss=1.5398517847061157
I0306 06:54:42.513394 139485295634176 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.35279080271720886, loss=1.5146071910858154
I0306 06:55:17.198490 139485287241472 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3981277346611023, loss=1.6522413492202759
I0306 06:55:22.067805 139629069661376 spec.py:321] Evaluating on the training split.
I0306 06:55:24.694380 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 06:58:25.442933 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 06:58:28.051888 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:01:16.045742 139629069661376 spec.py:349] Evaluating on the test split.
I0306 07:01:18.662192 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:03:58.341928 139629069661376 submission_runner.py:469] Time since start: 42633.01s, 	Step: 72915, 	{'train/accuracy': 0.6623005867004395, 'train/loss': 1.6121453046798706, 'train/bleu': 32.95397298149998, 'validation/accuracy': 0.6780337691307068, 'validation/loss': 1.4897774457931519, 'validation/bleu': 29.562043954383505, 'validation/num_examples': 3000, 'test/accuracy': 0.6924921870231628, 'test/loss': 1.4109165668487549, 'test/bleu': 29.245773949115474, 'test/num_examples': 3003, 'score': 25226.096336841583, 'total_duration': 42633.00661087036, 'accumulated_submission_time': 25226.096336841583, 'accumulated_eval_time': 17402.356122016907, 'accumulated_logging_time': 0.6100254058837891}
I0306 07:03:58.358137 139485295634176 logging_writer.py:48] [72915] accumulated_eval_time=17402.4, accumulated_logging_time=0.610025, accumulated_submission_time=25226.1, global_step=72915, preemption_count=0, score=25226.1, test/accuracy=0.692492, test/bleu=29.2458, test/loss=1.41092, test/num_examples=3003, total_duration=42633, train/accuracy=0.662301, train/bleu=32.954, train/loss=1.61215, validation/accuracy=0.678034, validation/bleu=29.562, validation/loss=1.48978, validation/num_examples=3000
I0306 07:04:28.053049 139485287241472 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3772925138473511, loss=1.6598172187805176
I0306 07:05:02.602563 139485295634176 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3633182942867279, loss=1.5867581367492676
I0306 07:05:37.263200 139485287241472 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.40968820452690125, loss=1.6469162702560425
I0306 07:06:11.902000 139485295634176 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.40807464718818665, loss=1.661670446395874
I0306 07:06:46.575773 139485287241472 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3738953769207001, loss=1.6325371265411377
I0306 07:07:21.271140 139485295634176 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.37321117520332336, loss=1.5838760137557983
I0306 07:07:56.019908 139485287241472 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.376316100358963, loss=1.677640438079834
I0306 07:08:30.710991 139485295634176 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3561396896839142, loss=1.6457945108413696
I0306 07:09:05.393163 139485287241472 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.4225059151649475, loss=1.5999352931976318
I0306 07:09:40.058360 139485295634176 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.39156273007392883, loss=1.6036428213119507
I0306 07:10:14.732275 139485287241472 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.4131602942943573, loss=1.6266846656799316
I0306 07:10:49.397483 139485295634176 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.40906521677970886, loss=1.655268907546997
I0306 07:11:24.176668 139485287241472 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3854599595069885, loss=1.5965430736541748
I0306 07:11:58.836188 139485295634176 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.37508639693260193, loss=1.6190930604934692
I0306 07:12:33.536299 139485287241472 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.39390087127685547, loss=1.6141327619552612
I0306 07:13:08.215854 139485295634176 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.40248990058898926, loss=1.5986491441726685
I0306 07:13:42.898492 139485287241472 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.38687318563461304, loss=1.6723002195358276
I0306 07:14:17.569886 139485295634176 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3703237473964691, loss=1.6558724641799927
I0306 07:14:52.271275 139485287241472 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.4029284715652466, loss=1.584979772567749
I0306 07:15:26.941964 139485295634176 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.40067070722579956, loss=1.6282979249954224
I0306 07:16:01.609273 139485287241472 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.4247153103351593, loss=1.5780541896820068
I0306 07:16:36.319667 139485295634176 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3705991208553314, loss=1.5928606986999512
I0306 07:17:11.036971 139485287241472 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3993992209434509, loss=1.5974700450897217
I0306 07:17:45.756760 139485295634176 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.39317786693573, loss=1.5795797109603882
I0306 07:17:58.578619 139629069661376 spec.py:321] Evaluating on the training split.
I0306 07:18:01.204853 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:21:51.609075 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 07:21:54.207294 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:25:09.336019 139629069661376 spec.py:349] Evaluating on the test split.
I0306 07:25:11.946373 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:28:07.955648 139629069661376 submission_runner.py:469] Time since start: 44082.62s, 	Step: 75338, 	{'train/accuracy': 0.7071047425270081, 'train/loss': 1.3568321466445923, 'train/bleu': 35.8541383155911, 'validation/accuracy': 0.6799866557121277, 'validation/loss': 1.487004280090332, 'validation/bleu': 30.0304687842793, 'validation/num_examples': 3000, 'test/accuracy': 0.6945081949234009, 'test/loss': 1.4004243612289429, 'test/bleu': 29.805077926426073, 'test/num_examples': 3003, 'score': 26066.18368411064, 'total_duration': 44082.62029528618, 'accumulated_submission_time': 26066.18368411064, 'accumulated_eval_time': 18011.733072519302, 'accumulated_logging_time': 0.6341073513031006}
I0306 07:28:07.975730 139485287241472 logging_writer.py:48] [75338] accumulated_eval_time=18011.7, accumulated_logging_time=0.634107, accumulated_submission_time=26066.2, global_step=75338, preemption_count=0, score=26066.2, test/accuracy=0.694508, test/bleu=29.8051, test/loss=1.40042, test/num_examples=3003, total_duration=44082.6, train/accuracy=0.707105, train/bleu=35.8541, train/loss=1.35683, validation/accuracy=0.679987, validation/bleu=30.0305, validation/loss=1.487, validation/num_examples=3000
I0306 07:28:29.733096 139485295634176 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.38045743107795715, loss=1.6272306442260742
I0306 07:29:04.293533 139485287241472 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.36535075306892395, loss=1.5641416311264038
I0306 07:29:38.899884 139485295634176 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.4002512991428375, loss=1.545434832572937
I0306 07:30:13.564620 139485287241472 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3853360712528229, loss=1.595706820487976
I0306 07:30:48.184715 139485295634176 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.37262800335884094, loss=1.5636699199676514
I0306 07:31:22.855327 139485287241472 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.38659191131591797, loss=1.5991777181625366
I0306 07:31:57.495338 139485295634176 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.39126893877983093, loss=1.5662556886672974
I0306 07:32:32.134432 139485287241472 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.36693090200424194, loss=1.6187316179275513
I0306 07:33:06.793854 139485295634176 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.39400020241737366, loss=1.6453946828842163
I0306 07:33:41.436534 139485287241472 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.3857899606227875, loss=1.5896375179290771
I0306 07:34:16.103529 139485295634176 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.38005736470222473, loss=1.6334009170532227
I0306 07:34:50.799475 139485287241472 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.3947315812110901, loss=1.6182397603988647
I0306 07:35:25.466917 139485295634176 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.37789925932884216, loss=1.5956217050552368
I0306 07:36:00.159070 139485287241472 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.4031887650489807, loss=1.599717140197754
I0306 07:36:34.833144 139485295634176 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3970308303833008, loss=1.6781675815582275
I0306 07:37:09.525611 139485287241472 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.38630497455596924, loss=1.6193610429763794
I0306 07:37:44.213851 139485295634176 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.3761711120605469, loss=1.5840390920639038
I0306 07:38:18.866539 139485287241472 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.3809742331504822, loss=1.6229854822158813
I0306 07:38:53.544442 139485295634176 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.37065598368644714, loss=1.6279304027557373
I0306 07:39:28.242573 139485287241472 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.3964197337627411, loss=1.546884536743164
I0306 07:40:02.896695 139485295634176 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.38240867853164673, loss=1.5630038976669312
I0306 07:40:37.568427 139485287241472 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.3833102285861969, loss=1.6454296112060547
I0306 07:41:12.251495 139485295634176 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.39639776945114136, loss=1.627243161201477
I0306 07:41:46.929987 139485287241472 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3893842101097107, loss=1.6452537775039673
I0306 07:42:08.097436 139629069661376 spec.py:321] Evaluating on the training split.
I0306 07:42:10.724474 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:46:08.786270 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 07:46:11.388940 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:49:08.067011 139629069661376 spec.py:349] Evaluating on the test split.
I0306 07:49:10.670617 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 07:52:09.576440 139629069661376 submission_runner.py:469] Time since start: 45524.24s, 	Step: 77762, 	{'train/accuracy': 0.6659988760948181, 'train/loss': 1.5725090503692627, 'train/bleu': 33.087009210637206, 'validation/accuracy': 0.6815934777259827, 'validation/loss': 1.4700863361358643, 'validation/bleu': 30.034001688716202, 'validation/num_examples': 3000, 'test/accuracy': 0.6941490173339844, 'test/loss': 1.3987714052200317, 'test/bleu': 29.672812843596468, 'test/num_examples': 3003, 'score': 26906.169424057007, 'total_duration': 45524.24109983444, 'accumulated_submission_time': 26906.169424057007, 'accumulated_eval_time': 18613.212021827698, 'accumulated_logging_time': 0.6627371311187744}
I0306 07:52:09.596641 139485295634176 logging_writer.py:48] [77762] accumulated_eval_time=18613.2, accumulated_logging_time=0.662737, accumulated_submission_time=26906.2, global_step=77762, preemption_count=0, score=26906.2, test/accuracy=0.694149, test/bleu=29.6728, test/loss=1.39877, test/num_examples=3003, total_duration=45524.2, train/accuracy=0.665999, train/bleu=33.087, train/loss=1.57251, validation/accuracy=0.681593, validation/bleu=30.034, validation/loss=1.47009, validation/num_examples=3000
I0306 07:52:23.045213 139485287241472 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3931673467159271, loss=1.5122520923614502
I0306 07:52:57.574240 139485295634176 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.3910593092441559, loss=1.5843185186386108
I0306 07:53:32.164624 139485287241472 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3835708498954773, loss=1.567352294921875
I0306 07:54:06.791471 139485295634176 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.39989960193634033, loss=1.5867588520050049
I0306 07:54:41.458213 139485287241472 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.4632208049297333, loss=1.5910072326660156
I0306 07:55:16.106230 139485295634176 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.37507036328315735, loss=1.5139484405517578
I0306 07:55:50.725150 139485287241472 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.4032028019428253, loss=1.6618200540542603
I0306 07:56:25.364764 139485295634176 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3993901312351227, loss=1.6972423791885376
I0306 07:56:59.997705 139485287241472 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.40091031789779663, loss=1.5793536901474
I0306 07:57:34.690931 139485295634176 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4176735281944275, loss=1.5889556407928467
I0306 07:58:09.388954 139485287241472 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.39313453435897827, loss=1.5982259511947632
I0306 07:58:44.065917 139485295634176 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3836763799190521, loss=1.5134801864624023
I0306 07:59:18.756985 139485287241472 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3906303346157074, loss=1.6444064378738403
I0306 07:59:53.432063 139485295634176 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.39224714040756226, loss=1.4699580669403076
I0306 08:00:28.089569 139485287241472 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.41598695516586304, loss=1.6037075519561768
I0306 08:01:02.754254 139485295634176 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.3830307126045227, loss=1.5874183177947998
I0306 08:01:37.417700 139485287241472 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.39333900809288025, loss=1.499624252319336
I0306 08:02:12.103048 139485295634176 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.4026467502117157, loss=1.5924421548843384
I0306 08:02:46.766750 139485287241472 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.38756656646728516, loss=1.5572104454040527
I0306 08:03:21.428825 139485295634176 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.40969911217689514, loss=1.614205241203308
I0306 08:03:56.138299 139485287241472 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.4157573878765106, loss=1.5958080291748047
I0306 08:04:30.834185 139485295634176 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.40707966685295105, loss=1.574647068977356
I0306 08:05:05.471842 139485287241472 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3828170895576477, loss=1.6320595741271973
I0306 08:05:40.166863 139485295634176 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3821343779563904, loss=1.6119253635406494
I0306 08:06:09.646129 139629069661376 spec.py:321] Evaluating on the training split.
I0306 08:06:12.273789 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 08:10:26.000852 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 08:10:28.609274 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 08:13:19.229348 139629069661376 spec.py:349] Evaluating on the test split.
I0306 08:13:21.852364 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 08:16:11.877251 139629069661376 submission_runner.py:469] Time since start: 46966.54s, 	Step: 80186, 	{'train/accuracy': 0.6655203104019165, 'train/loss': 1.5883437395095825, 'train/bleu': 33.22163166887323, 'validation/accuracy': 0.6819766163825989, 'validation/loss': 1.4697943925857544, 'validation/bleu': 29.77887150526415, 'validation/num_examples': 3000, 'test/accuracy': 0.6966168284416199, 'test/loss': 1.3908458948135376, 'test/bleu': 29.816601216249484, 'test/num_examples': 3003, 'score': 27746.085688352585, 'total_duration': 46966.541915655136, 'accumulated_submission_time': 27746.085688352585, 'accumulated_eval_time': 19215.443091630936, 'accumulated_logging_time': 0.6914234161376953}
I0306 08:16:11.894377 139485287241472 logging_writer.py:48] [80186] accumulated_eval_time=19215.4, accumulated_logging_time=0.691423, accumulated_submission_time=27746.1, global_step=80186, preemption_count=0, score=27746.1, test/accuracy=0.696617, test/bleu=29.8166, test/loss=1.39085, test/num_examples=3003, total_duration=46966.5, train/accuracy=0.66552, train/bleu=33.2216, train/loss=1.58834, validation/accuracy=0.681977, validation/bleu=29.7789, validation/loss=1.46979, validation/num_examples=3000
I0306 08:16:17.069217 139485295634176 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.410828560590744, loss=1.6400994062423706
I0306 08:16:51.566516 139485287241472 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.40068256855010986, loss=1.5983147621154785
I0306 08:17:26.140842 139485295634176 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.42422956228256226, loss=1.5930018424987793
I0306 08:18:00.796467 139485287241472 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.449053555727005, loss=1.5604381561279297
I0306 08:18:35.450243 139485295634176 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.38476285338401794, loss=1.6042935848236084
I0306 08:19:10.112056 139485287241472 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.40427926182746887, loss=1.5627063512802124
I0306 08:19:44.778885 139485295634176 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.3950124979019165, loss=1.5391829013824463
I0306 08:20:19.465821 139485287241472 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.4069284498691559, loss=1.531772255897522
I0306 08:20:54.180341 139485295634176 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3817800283432007, loss=1.5772268772125244
I0306 08:21:28.887569 139485287241472 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.40058633685112, loss=1.5995897054672241
I0306 08:22:03.557602 139485295634176 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.39414694905281067, loss=1.5588685274124146
I0306 08:22:38.250980 139485287241472 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3976813852787018, loss=1.5327749252319336
I0306 08:23:12.892942 139485295634176 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.41062769293785095, loss=1.4819799661636353
I0306 08:23:47.578265 139485287241472 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.41392356157302856, loss=1.5026477575302124
I0306 08:24:22.232680 139485295634176 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.4643262028694153, loss=1.5975902080535889
I0306 08:24:56.893494 139485287241472 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.41126686334609985, loss=1.5496394634246826
I0306 08:25:31.591217 139485295634176 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.4150088131427765, loss=1.6269506216049194
I0306 08:26:06.256874 139485287241472 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.4433640241622925, loss=1.571286678314209
I0306 08:26:40.925568 139485295634176 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4291488528251648, loss=1.5351709127426147
I0306 08:27:15.630510 139485287241472 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.42707204818725586, loss=1.5552371740341187
I0306 08:27:50.339366 139485295634176 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.4501805901527405, loss=1.6153159141540527
I0306 08:28:25.021849 139485287241472 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.4030832052230835, loss=1.5567275285720825
I0306 08:28:59.689720 139485295634176 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4011344015598297, loss=1.5845768451690674
I0306 08:29:34.402699 139485287241472 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.38252970576286316, loss=1.5666413307189941
I0306 08:30:09.093023 139485295634176 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.395460307598114, loss=1.4866639375686646
I0306 08:30:12.221857 139629069661376 spec.py:321] Evaluating on the training split.
I0306 08:30:14.847952 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 08:34:34.536333 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 08:34:37.148287 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 08:37:23.592620 139629069661376 spec.py:349] Evaluating on the test split.
I0306 08:37:26.208227 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 08:40:16.835520 139629069661376 submission_runner.py:469] Time since start: 48411.50s, 	Step: 82610, 	{'train/accuracy': 0.6787692308425903, 'train/loss': 1.501219630241394, 'train/bleu': 34.15638195142007, 'validation/accuracy': 0.6833733320236206, 'validation/loss': 1.4623011350631714, 'validation/bleu': 30.132553181091396, 'validation/num_examples': 3000, 'test/accuracy': 0.6979029178619385, 'test/loss': 1.3808497190475464, 'test/bleu': 29.82716792614236, 'test/num_examples': 3003, 'score': 28586.279491901398, 'total_duration': 48411.500180482864, 'accumulated_submission_time': 28586.279491901398, 'accumulated_eval_time': 19820.05669569969, 'accumulated_logging_time': 0.7164402008056641}
I0306 08:40:16.853432 139485287241472 logging_writer.py:48] [82610] accumulated_eval_time=19820.1, accumulated_logging_time=0.71644, accumulated_submission_time=28586.3, global_step=82610, preemption_count=0, score=28586.3, test/accuracy=0.697903, test/bleu=29.8272, test/loss=1.38085, test/num_examples=3003, total_duration=48411.5, train/accuracy=0.678769, train/bleu=34.1564, train/loss=1.50122, validation/accuracy=0.683373, validation/bleu=30.1326, validation/loss=1.4623, validation/num_examples=3000
I0306 08:40:48.245510 139485295634176 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.4400463104248047, loss=1.5459439754486084
I0306 08:41:22.813355 139485287241472 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.4384514391422272, loss=1.5779259204864502
I0306 08:41:57.417576 139485295634176 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.43627482652664185, loss=1.5774332284927368
I0306 08:42:32.129977 139485287241472 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.4087182879447937, loss=1.5643161535263062
I0306 08:43:06.802118 139485295634176 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3947434425354004, loss=1.503983736038208
I0306 08:43:41.470796 139485287241472 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.43011897802352905, loss=1.6490823030471802
I0306 08:44:16.128588 139485295634176 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.4160477817058563, loss=1.5587077140808105
I0306 08:44:50.796231 139485287241472 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.40648767352104187, loss=1.6167653799057007
I0306 08:45:25.423819 139485295634176 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.40354660153388977, loss=1.553817629814148
I0306 08:46:00.086194 139485287241472 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.4098304212093353, loss=1.5930917263031006
I0306 08:46:34.769911 139485295634176 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.44559434056282043, loss=1.5905096530914307
I0306 08:47:09.417391 139485287241472 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.40245485305786133, loss=1.5922890901565552
I0306 08:47:44.067840 139485295634176 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.4165787696838379, loss=1.5411354303359985
I0306 08:48:18.767270 139485287241472 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.40846890211105347, loss=1.5757246017456055
I0306 08:48:53.383800 139485295634176 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.41807660460472107, loss=1.5216151475906372
I0306 08:49:28.023441 139485287241472 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.4237942695617676, loss=1.5304946899414062
I0306 08:50:02.627604 139485295634176 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.42479270696640015, loss=1.5451653003692627
I0306 08:50:37.192417 139485287241472 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4284922182559967, loss=1.5509878396987915
I0306 08:51:11.787560 139485295634176 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.41105470061302185, loss=1.536934733390808
I0306 08:51:46.400411 139485287241472 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.40916651487350464, loss=1.54410982131958
I0306 08:52:20.975692 139485295634176 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.43337780237197876, loss=1.6708658933639526
I0306 08:52:55.544763 139485287241472 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.40042775869369507, loss=1.5166399478912354
I0306 08:53:30.095465 139485295634176 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.42499643564224243, loss=1.5675482749938965
I0306 08:54:04.692493 139485287241472 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.4211284816265106, loss=1.6204932928085327
I0306 08:54:17.131395 139629069661376 spec.py:321] Evaluating on the training split.
I0306 08:54:19.748329 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 08:58:30.924410 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 08:58:33.529405 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:01:27.190656 139629069661376 spec.py:349] Evaluating on the test split.
I0306 09:01:29.791378 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:04:21.345505 139629069661376 submission_runner.py:469] Time since start: 49856.01s, 	Step: 85037, 	{'train/accuracy': 0.6714715361595154, 'train/loss': 1.5471140146255493, 'train/bleu': 33.579012171528916, 'validation/accuracy': 0.6855610609054565, 'validation/loss': 1.4485142230987549, 'validation/bleu': 30.1081861286188, 'validation/num_examples': 3000, 'test/accuracy': 0.7013440132141113, 'test/loss': 1.366485357284546, 'test/bleu': 30.416618790596697, 'test/num_examples': 3003, 'score': 29426.418329000473, 'total_duration': 49856.01018476486, 'accumulated_submission_time': 29426.418329000473, 'accumulated_eval_time': 20424.270758867264, 'accumulated_logging_time': 0.7420802116394043}
I0306 09:04:21.361880 139485295634176 logging_writer.py:48] [85037] accumulated_eval_time=20424.3, accumulated_logging_time=0.74208, accumulated_submission_time=29426.4, global_step=85037, preemption_count=0, score=29426.4, test/accuracy=0.701344, test/bleu=30.4166, test/loss=1.36649, test/num_examples=3003, total_duration=49856, train/accuracy=0.671472, train/bleu=33.579, train/loss=1.54711, validation/accuracy=0.685561, validation/bleu=30.1082, validation/loss=1.44851, validation/num_examples=3000
I0306 09:04:43.391322 139485287241472 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4049949049949646, loss=1.5843650102615356
I0306 09:05:17.774536 139485295634176 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.4403539299964905, loss=1.5615497827529907
I0306 09:05:52.202705 139485287241472 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.39797836542129517, loss=1.5432573556900024
I0306 09:06:26.701472 139485295634176 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.4429873824119568, loss=1.5271910429000854
I0306 09:07:01.191735 139485287241472 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.46550166606903076, loss=1.5284650325775146
I0306 09:07:35.747900 139485295634176 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.40798601508140564, loss=1.4321224689483643
I0306 09:08:10.282114 139485287241472 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.44497668743133545, loss=1.564884066581726
I0306 09:08:44.848695 139485295634176 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.4188213646411896, loss=1.5811450481414795
I0306 09:09:19.401107 139485287241472 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.4250144958496094, loss=1.5629138946533203
I0306 09:09:53.955066 139485295634176 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.4417005181312561, loss=1.5272107124328613
I0306 09:10:28.533010 139485287241472 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.4210260510444641, loss=1.5549932718276978
I0306 09:11:03.105581 139485295634176 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.4218975603580475, loss=1.4990612268447876
I0306 09:11:37.631919 139485287241472 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.45965296030044556, loss=1.5675315856933594
I0306 09:12:12.215420 139485295634176 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4374920725822449, loss=1.4951155185699463
I0306 09:12:46.750896 139485287241472 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.48189955949783325, loss=1.6142139434814453
I0306 09:13:21.266678 139485295634176 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.46758490800857544, loss=1.5378735065460205
I0306 09:13:55.792250 139485287241472 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.4635847210884094, loss=1.6036241054534912
I0306 09:14:30.388645 139485295634176 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.44146808981895447, loss=1.5804798603057861
I0306 09:15:04.905182 139485287241472 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4214472472667694, loss=1.5817838907241821
I0306 09:15:39.428437 139485295634176 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.4035581946372986, loss=1.4774787425994873
I0306 09:16:13.950015 139485287241472 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.41101354360580444, loss=1.5327295064926147
I0306 09:16:48.513372 139485295634176 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.4538814425468445, loss=1.5989444255828857
I0306 09:17:23.039188 139485287241472 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4414139688014984, loss=1.566321611404419
I0306 09:17:57.555037 139485295634176 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.44103407859802246, loss=1.465008020401001
I0306 09:18:21.380681 139629069661376 spec.py:321] Evaluating on the training split.
I0306 09:18:23.989294 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:22:02.838416 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 09:22:05.450343 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:24:48.960996 139629069661376 spec.py:349] Evaluating on the test split.
I0306 09:24:51.563208 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:27:35.121251 139629069661376 submission_runner.py:469] Time since start: 51249.79s, 	Step: 87470, 	{'train/accuracy': 0.6733237504959106, 'train/loss': 1.5478787422180176, 'train/bleu': 34.07862561112074, 'validation/accuracy': 0.6861295700073242, 'validation/loss': 1.4439963102340698, 'validation/bleu': 30.357072380601185, 'validation/num_examples': 3000, 'test/accuracy': 0.7019001245498657, 'test/loss': 1.3604204654693604, 'test/bleu': 30.34645853642891, 'test/num_examples': 3003, 'score': 30266.299453258514, 'total_duration': 51249.785928964615, 'accumulated_submission_time': 30266.299453258514, 'accumulated_eval_time': 20978.011278152466, 'accumulated_logging_time': 0.766226053237915}
I0306 09:27:35.138262 139485287241472 logging_writer.py:48] [87470] accumulated_eval_time=20978, accumulated_logging_time=0.766226, accumulated_submission_time=30266.3, global_step=87470, preemption_count=0, score=30266.3, test/accuracy=0.7019, test/bleu=30.3465, test/loss=1.36042, test/num_examples=3003, total_duration=51249.8, train/accuracy=0.673324, train/bleu=34.0786, train/loss=1.54788, validation/accuracy=0.68613, validation/bleu=30.3571, validation/loss=1.444, validation/num_examples=3000
I0306 09:27:45.816456 139485295634176 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4126308858394623, loss=1.52529776096344
I0306 09:28:20.202333 139485287241472 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.46031060814857483, loss=1.5136297941207886
I0306 09:28:54.675639 139485295634176 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.42834892868995667, loss=1.5118811130523682
I0306 09:29:29.178776 139485287241472 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.43226900696754456, loss=1.63199782371521
I0306 09:30:03.680662 139485295634176 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.4573782682418823, loss=1.4542813301086426
I0306 09:30:38.198256 139485287241472 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.41549888253211975, loss=1.4297173023223877
I0306 09:31:12.757454 139485295634176 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.4435045123100281, loss=1.4854460954666138
I0306 09:31:47.312862 139485287241472 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.4721642732620239, loss=1.5720484256744385
I0306 09:32:21.832892 139485295634176 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.48238101601600647, loss=1.5878432989120483
I0306 09:32:56.362965 139485287241472 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.46263599395751953, loss=1.5915716886520386
I0306 09:33:30.900170 139485295634176 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.45625314116477966, loss=1.5516846179962158
I0306 09:34:05.421812 139485287241472 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.4194067120552063, loss=1.5601853132247925
I0306 09:34:39.937305 139485295634176 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.46556317806243896, loss=1.584930419921875
I0306 09:35:14.482427 139485287241472 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4415131211280823, loss=1.5398125648498535
I0306 09:35:48.994557 139485295634176 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.45883551239967346, loss=1.5173472166061401
I0306 09:36:23.525689 139485287241472 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.44332394003868103, loss=1.5247281789779663
I0306 09:36:58.064290 139485295634176 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4280052185058594, loss=1.5018609762191772
I0306 09:37:32.577175 139485287241472 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.4584902226924896, loss=1.5132638216018677
I0306 09:38:07.117110 139485295634176 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.4282401204109192, loss=1.4543070793151855
I0306 09:38:41.616816 139485287241472 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.45360416173934937, loss=1.486114501953125
I0306 09:39:16.177870 139485295634176 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.5012369751930237, loss=1.6189568042755127
I0306 09:39:50.703088 139485287241472 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.46017712354660034, loss=1.549994945526123
I0306 09:40:25.240393 139485295634176 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4502866864204407, loss=1.5751127004623413
I0306 09:40:59.773121 139485287241472 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.4701550304889679, loss=1.5460646152496338
I0306 09:41:34.358746 139485295634176 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.46033257246017456, loss=1.5270568132400513
I0306 09:41:35.398947 139629069661376 spec.py:321] Evaluating on the training split.
I0306 09:41:38.016142 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:45:26.013187 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 09:45:28.614947 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:48:16.189586 139629069661376 spec.py:349] Evaluating on the test split.
I0306 09:48:18.794631 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 09:51:06.259073 139629069661376 submission_runner.py:469] Time since start: 52660.92s, 	Step: 89904, 	{'train/accuracy': 0.68186354637146, 'train/loss': 1.482079267501831, 'train/bleu': 34.32693509025636, 'validation/accuracy': 0.6874274015426636, 'validation/loss': 1.438246250152588, 'validation/bleu': 30.183784131444092, 'validation/num_examples': 3000, 'test/accuracy': 0.7033483982086182, 'test/loss': 1.3508611917495728, 'test/bleu': 30.383179845726477, 'test/num_examples': 3003, 'score': 31106.42396378517, 'total_duration': 52660.92375588417, 'accumulated_submission_time': 31106.42396378517, 'accumulated_eval_time': 21548.871358394623, 'accumulated_logging_time': 0.7919626235961914}
I0306 09:51:06.276692 139485287241472 logging_writer.py:48] [89904] accumulated_eval_time=21548.9, accumulated_logging_time=0.791963, accumulated_submission_time=31106.4, global_step=89904, preemption_count=0, score=31106.4, test/accuracy=0.703348, test/bleu=30.3832, test/loss=1.35086, test/num_examples=3003, total_duration=52660.9, train/accuracy=0.681864, train/bleu=34.3269, train/loss=1.48208, validation/accuracy=0.687427, validation/bleu=30.1838, validation/loss=1.43825, validation/num_examples=3000
I0306 09:51:39.594365 139485295634176 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.49812379479408264, loss=1.4981104135513306
I0306 09:52:13.975094 139485287241472 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.44266238808631897, loss=1.446501612663269
I0306 09:52:48.442569 139485295634176 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.45583048462867737, loss=1.4861993789672852
I0306 09:53:22.947005 139485287241472 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.4681485593318939, loss=1.5323647260665894
I0306 09:53:57.457219 139485295634176 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.4683520495891571, loss=1.536895513534546
I0306 09:54:31.970486 139485287241472 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4652637541294098, loss=1.4994778633117676
I0306 09:55:06.471420 139485295634176 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.4706118404865265, loss=1.5548925399780273
I0306 09:55:40.962065 139485287241472 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.45074525475502014, loss=1.4545546770095825
I0306 09:56:15.472230 139485295634176 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4589809775352478, loss=1.5678669214248657
I0306 09:56:49.982400 139485287241472 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4580371677875519, loss=1.4988526105880737
I0306 09:57:24.481581 139485295634176 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.46782803535461426, loss=1.5002944469451904
I0306 09:57:58.980612 139485287241472 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.45428189635276794, loss=1.4787476062774658
I0306 09:58:33.505585 139485295634176 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.4699820876121521, loss=1.5995506048202515
I0306 09:59:08.018496 139485287241472 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.453643798828125, loss=1.4432954788208008
I0306 09:59:42.532109 139485295634176 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4818015992641449, loss=1.5677671432495117
I0306 10:00:17.016181 139485287241472 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.44960707426071167, loss=1.5120031833648682
I0306 10:00:51.506754 139485295634176 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.45916134119033813, loss=1.5222018957138062
I0306 10:01:26.027282 139485287241472 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.4439071714878082, loss=1.5791622400283813
I0306 10:02:00.546522 139485295634176 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4565548598766327, loss=1.579865574836731
I0306 10:02:35.053590 139485287241472 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4794241189956665, loss=1.539228916168213
I0306 10:03:09.544577 139485295634176 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4601065516471863, loss=1.4802812337875366
I0306 10:03:44.085487 139485287241472 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.4812631607055664, loss=1.514567494392395
I0306 10:04:18.621057 139485295634176 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.45934900641441345, loss=1.5106301307678223
I0306 10:04:53.128295 139485287241472 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.47513166069984436, loss=1.5432130098342896
I0306 10:05:06.592201 139629069661376 spec.py:321] Evaluating on the training split.
I0306 10:05:09.202538 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:08:50.976346 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 10:08:53.581250 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:11:54.390082 139629069661376 spec.py:349] Evaluating on the test split.
I0306 10:11:56.999569 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:14:34.125797 139629069661376 submission_runner.py:469] Time since start: 54068.79s, 	Step: 92340, 	{'train/accuracy': 0.6789466142654419, 'train/loss': 1.5095155239105225, 'train/bleu': 34.49960671027544, 'validation/accuracy': 0.6882184147834778, 'validation/loss': 1.434462070465088, 'validation/bleu': 30.375925305243374, 'validation/num_examples': 3000, 'test/accuracy': 0.7045186161994934, 'test/loss': 1.3457274436950684, 'test/bleu': 30.594519714092794, 'test/num_examples': 3003, 'score': 31946.605330705643, 'total_duration': 54068.790437698364, 'accumulated_submission_time': 31946.605330705643, 'accumulated_eval_time': 22116.40487098694, 'accumulated_logging_time': 0.817626953125}
I0306 10:14:34.146190 139485295634176 logging_writer.py:48] [92340] accumulated_eval_time=22116.4, accumulated_logging_time=0.817627, accumulated_submission_time=31946.6, global_step=92340, preemption_count=0, score=31946.6, test/accuracy=0.704519, test/bleu=30.5945, test/loss=1.34573, test/num_examples=3003, total_duration=54068.8, train/accuracy=0.678947, train/bleu=34.4996, train/loss=1.50952, validation/accuracy=0.688218, validation/bleu=30.3759, validation/loss=1.43446, validation/num_examples=3000
I0306 10:14:55.119438 139485287241472 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4572189152240753, loss=1.4740420579910278
I0306 10:15:29.514473 139485295634176 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.4634138345718384, loss=1.5205045938491821
I0306 10:16:04.027717 139485287241472 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.5015648603439331, loss=1.4734760522842407
I0306 10:16:38.516600 139485295634176 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.4757647216320038, loss=1.4105886220932007
I0306 10:17:13.022530 139485287241472 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.4845702648162842, loss=1.5305472612380981
I0306 10:17:47.533603 139485295634176 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.4620656669139862, loss=1.413436770439148
I0306 10:18:22.020659 139485287241472 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.48079419136047363, loss=1.5390664339065552
I0306 10:18:56.531678 139485295634176 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.47852712869644165, loss=1.4916284084320068
I0306 10:19:31.028729 139485287241472 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.4751289188861847, loss=1.4486324787139893
I0306 10:20:05.556132 139485295634176 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4727041721343994, loss=1.4797327518463135
I0306 10:20:40.106174 139485287241472 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.47349461913108826, loss=1.5102697610855103
I0306 10:21:14.588479 139485295634176 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.4814334809780121, loss=1.45448637008667
I0306 10:21:49.087915 139485287241472 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.494628369808197, loss=1.4670408964157104
I0306 10:22:23.616153 139485295634176 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.5070874094963074, loss=1.4521299600601196
I0306 10:22:58.135029 139485287241472 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.4986884593963623, loss=1.5399445295333862
I0306 10:23:32.672630 139485295634176 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.46601402759552, loss=1.5142115354537964
I0306 10:24:07.223232 139485287241472 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.49320366978645325, loss=1.5125778913497925
I0306 10:24:41.738356 139485295634176 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.48697152733802795, loss=1.4697964191436768
I0306 10:25:16.275086 139485287241472 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4785759449005127, loss=1.5131431818008423
I0306 10:25:50.797498 139485295634176 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.4855518937110901, loss=1.540160894393921
I0306 10:26:25.351495 139485287241472 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4918898642063141, loss=1.5311119556427002
I0306 10:26:59.914489 139485295634176 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4791686534881592, loss=1.5298995971679688
I0306 10:27:34.470396 139485287241472 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.49144721031188965, loss=1.477777361869812
I0306 10:28:09.009047 139485295634176 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.5010895729064941, loss=1.448410153388977
I0306 10:28:34.243503 139629069661376 spec.py:321] Evaluating on the training split.
I0306 10:28:36.860186 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:32:13.513966 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 10:32:16.115465 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:34:58.205429 139629069661376 spec.py:349] Evaluating on the test split.
I0306 10:35:00.806970 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:37:36.354684 139629069661376 submission_runner.py:469] Time since start: 55451.02s, 	Step: 94774, 	{'train/accuracy': 0.6968047618865967, 'train/loss': 1.404218077659607, 'train/bleu': 35.49709408055989, 'validation/accuracy': 0.6890836358070374, 'validation/loss': 1.425716757774353, 'validation/bleu': 30.612033610576812, 'validation/num_examples': 3000, 'test/accuracy': 0.706071138381958, 'test/loss': 1.3368157148361206, 'test/bleu': 30.483672062793904, 'test/num_examples': 3003, 'score': 32786.5674738884, 'total_duration': 55451.01929616928, 'accumulated_submission_time': 32786.5674738884, 'accumulated_eval_time': 22658.51594042778, 'accumulated_logging_time': 0.8466439247131348}
I0306 10:37:36.375099 139485287241472 logging_writer.py:48] [94774] accumulated_eval_time=22658.5, accumulated_logging_time=0.846644, accumulated_submission_time=32786.6, global_step=94774, preemption_count=0, score=32786.6, test/accuracy=0.706071, test/bleu=30.4837, test/loss=1.33682, test/num_examples=3003, total_duration=55451, train/accuracy=0.696805, train/bleu=35.4971, train/loss=1.40422, validation/accuracy=0.689084, validation/bleu=30.612, validation/loss=1.42572, validation/num_examples=3000
I0306 10:37:45.658463 139485295634176 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.5006147027015686, loss=1.4375925064086914
I0306 10:38:20.071340 139485287241472 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4806206524372101, loss=1.4181681871414185
I0306 10:38:54.535450 139485295634176 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.5057501196861267, loss=1.6040207147598267
I0306 10:39:29.061552 139485287241472 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.4834619462490082, loss=1.4999951124191284
I0306 10:40:03.619716 139485295634176 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.5055312514305115, loss=1.4192578792572021
I0306 10:40:38.179639 139485287241472 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.4720185399055481, loss=1.4322161674499512
I0306 10:41:12.737811 139485295634176 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.4854986071586609, loss=1.4651756286621094
I0306 10:41:47.297445 139485287241472 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.4976170063018799, loss=1.580924391746521
I0306 10:42:21.880290 139485295634176 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.5013927221298218, loss=1.5111055374145508
I0306 10:42:56.428835 139485287241472 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.49800750613212585, loss=1.4551811218261719
I0306 10:43:31.005405 139485295634176 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.503031849861145, loss=1.455328106880188
I0306 10:44:05.543865 139485287241472 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.4917624592781067, loss=1.4856679439544678
I0306 10:44:40.060091 139485295634176 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.4817425012588501, loss=1.349353551864624
I0306 10:45:14.637164 139485287241472 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.5035889148712158, loss=1.4061917066574097
I0306 10:45:49.189592 139485295634176 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.5018759965896606, loss=1.4722341299057007
I0306 10:46:23.752062 139485287241472 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.5021771192550659, loss=1.468885898590088
I0306 10:46:58.297652 139485295634176 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5165597200393677, loss=1.410757064819336
I0306 10:47:32.867341 139485287241472 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.49994418025016785, loss=1.473904013633728
I0306 10:48:07.396873 139485295634176 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.5030534863471985, loss=1.4187538623809814
I0306 10:48:41.978923 139485287241472 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.5395973920822144, loss=1.5035074949264526
I0306 10:49:16.536523 139485295634176 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5331270694732666, loss=1.4917223453521729
I0306 10:49:51.084512 139485287241472 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.5233452916145325, loss=1.4603850841522217
I0306 10:50:25.642528 139485295634176 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5368037223815918, loss=1.4976922273635864
I0306 10:51:00.170520 139485287241472 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.48963314294815063, loss=1.4265835285186768
I0306 10:51:34.684861 139485295634176 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.506829023361206, loss=1.4155396223068237
I0306 10:51:36.411459 139629069661376 spec.py:321] Evaluating on the training split.
I0306 10:51:39.027598 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:55:27.168078 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 10:55:29.779583 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 10:58:15.850221 139629069661376 spec.py:349] Evaluating on the test split.
I0306 10:58:18.457224 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 11:00:58.646970 139629069661376 submission_runner.py:469] Time since start: 56853.31s, 	Step: 97206, 	{'train/accuracy': 0.6900190114974976, 'train/loss': 1.4364527463912964, 'train/bleu': 35.01110227645804, 'validation/accuracy': 0.69059157371521, 'validation/loss': 1.4213261604309082, 'validation/bleu': 30.70094834209247, 'validation/num_examples': 3000, 'test/accuracy': 0.7081450819969177, 'test/loss': 1.325734257698059, 'test/bleu': 30.678121408017475, 'test/num_examples': 3003, 'score': 33626.468497276306, 'total_duration': 56853.3116543293, 'accumulated_submission_time': 33626.468497276306, 'accumulated_eval_time': 23220.751407384872, 'accumulated_logging_time': 0.875568151473999}
I0306 11:00:58.665100 139485287241472 logging_writer.py:48] [97206] accumulated_eval_time=23220.8, accumulated_logging_time=0.875568, accumulated_submission_time=33626.5, global_step=97206, preemption_count=0, score=33626.5, test/accuracy=0.708145, test/bleu=30.6781, test/loss=1.32573, test/num_examples=3003, total_duration=56853.3, train/accuracy=0.690019, train/bleu=35.0111, train/loss=1.43645, validation/accuracy=0.690592, validation/bleu=30.7009, validation/loss=1.42133, validation/num_examples=3000
I0306 11:01:31.368761 139485295634176 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5259561538696289, loss=1.387786626815796
I0306 11:02:05.831329 139485287241472 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5053274631500244, loss=1.4908555746078491
I0306 11:02:40.340895 139485295634176 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.527114748954773, loss=1.465240716934204
I0306 11:03:14.885534 139485287241472 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.5546573996543884, loss=1.4796267747879028
I0306 11:03:49.426062 139485295634176 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.5388901233673096, loss=1.4705101251602173
I0306 11:04:23.954077 139485287241472 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.5264183282852173, loss=1.4627094268798828
I0306 11:04:58.514346 139485295634176 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.5152878165245056, loss=1.418818712234497
I0306 11:05:33.056355 139485287241472 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5102052092552185, loss=1.427367925643921
I0306 11:06:07.599747 139485295634176 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5711400508880615, loss=1.444872498512268
I0306 11:06:42.141993 139485287241472 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.5274083614349365, loss=1.4208773374557495
I0306 11:07:16.677281 139485295634176 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5338944792747498, loss=1.4917594194412231
I0306 11:07:51.221883 139485287241472 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.5213565826416016, loss=1.4366486072540283
I0306 11:08:25.740180 139485295634176 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5258045196533203, loss=1.4009162187576294
I0306 11:09:00.320297 139485287241472 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5305534601211548, loss=1.3514671325683594
I0306 11:09:34.869558 139485295634176 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.539794385433197, loss=1.4202951192855835
I0306 11:10:09.407860 139485287241472 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5142509937286377, loss=1.425584077835083
I0306 11:10:43.967022 139485295634176 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5409473776817322, loss=1.4588897228240967
I0306 11:11:18.520903 139485287241472 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5299351811408997, loss=1.4288760423660278
I0306 11:11:53.042974 139485295634176 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5576804280281067, loss=1.4636788368225098
I0306 11:12:27.588343 139485287241472 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5241168141365051, loss=1.443092703819275
I0306 11:13:02.123909 139485295634176 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.531975507736206, loss=1.4507251977920532
I0306 11:13:36.681798 139485287241472 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5343888401985168, loss=1.5168724060058594
I0306 11:14:11.239631 139485295634176 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.545495867729187, loss=1.4950573444366455
I0306 11:14:45.819236 139485287241472 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5551307201385498, loss=1.5068273544311523
I0306 11:14:58.961000 139629069661376 spec.py:321] Evaluating on the training split.
I0306 11:15:01.575120 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 11:19:10.365840 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 11:19:12.970381 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 11:22:05.266364 139629069661376 spec.py:349] Evaluating on the test split.
I0306 11:22:07.877060 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 11:25:02.577233 139629069661376 submission_runner.py:469] Time since start: 58297.24s, 	Step: 99639, 	{'train/accuracy': 0.6873375177383423, 'train/loss': 1.455028772354126, 'train/bleu': 34.648668799352336, 'validation/accuracy': 0.6912837028503418, 'validation/loss': 1.4166932106018066, 'validation/bleu': 30.612191116172603, 'validation/num_examples': 3000, 'test/accuracy': 0.7073224186897278, 'test/loss': 1.3296566009521484, 'test/bleu': 30.518567262327068, 'test/num_examples': 3003, 'score': 34466.63046216965, 'total_duration': 58297.24188733101, 'accumulated_submission_time': 34466.63046216965, 'accumulated_eval_time': 23824.367573738098, 'accumulated_logging_time': 0.9014079570770264}
I0306 11:25:02.598881 139485295634176 logging_writer.py:48] [99639] accumulated_eval_time=23824.4, accumulated_logging_time=0.901408, accumulated_submission_time=34466.6, global_step=99639, preemption_count=0, score=34466.6, test/accuracy=0.707322, test/bleu=30.5186, test/loss=1.32966, test/num_examples=3003, total_duration=58297.2, train/accuracy=0.687338, train/bleu=34.6487, train/loss=1.45503, validation/accuracy=0.691284, validation/bleu=30.6122, validation/loss=1.41669, validation/num_examples=3000
I0306 11:25:23.918615 139485287241472 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5147266387939453, loss=1.4487758874893188
I0306 11:25:58.327462 139485295634176 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5596114993095398, loss=1.5002743005752563
I0306 11:26:32.780692 139485287241472 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5170489549636841, loss=1.4000049829483032
I0306 11:27:07.310194 139485295634176 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5588764548301697, loss=1.4582582712173462
I0306 11:27:41.816836 139485287241472 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5261446237564087, loss=1.4548414945602417
I0306 11:28:16.320072 139485295634176 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5213554501533508, loss=1.3626528978347778
I0306 11:28:50.838033 139485287241472 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5350207090377808, loss=1.4457939863204956
I0306 11:29:25.379213 139485295634176 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.545120120048523, loss=1.4502756595611572
I0306 11:29:59.930436 139485287241472 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5311039686203003, loss=1.4799151420593262
I0306 11:30:34.479108 139485295634176 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5487513542175293, loss=1.476741909980774
I0306 11:31:09.021308 139485287241472 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5255385041236877, loss=1.4235831499099731
I0306 11:31:43.555709 139485295634176 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5643954277038574, loss=1.4799699783325195
I0306 11:32:18.098001 139485287241472 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5551748871803284, loss=1.420372724533081
I0306 11:32:52.653898 139485295634176 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5363945960998535, loss=1.440698266029358
I0306 11:33:27.205557 139485287241472 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5403449535369873, loss=1.4203017950057983
I0306 11:34:01.721963 139485295634176 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5456454157829285, loss=1.413861632347107
I0306 11:34:36.261271 139485287241472 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.5163083076477051, loss=1.319139838218689
I0306 11:35:10.777788 139485295634176 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5691042542457581, loss=1.5011193752288818
I0306 11:35:45.319280 139485287241472 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5493355989456177, loss=1.450352430343628
I0306 11:36:19.850676 139485295634176 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5619431734085083, loss=1.471613883972168
I0306 11:36:54.360534 139485287241472 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5422711968421936, loss=1.4486886262893677
I0306 11:37:28.875364 139485295634176 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5606030821800232, loss=1.4058648347854614
I0306 11:38:03.456466 139485287241472 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5671705007553101, loss=1.4605666399002075
I0306 11:38:37.993844 139485295634176 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5771890878677368, loss=1.4973053932189941
I0306 11:39:02.870738 139629069661376 spec.py:321] Evaluating on the training split.
I0306 11:39:05.494349 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 11:43:12.433210 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 11:43:15.040609 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 11:45:56.237593 139629069661376 spec.py:349] Evaluating on the test split.
I0306 11:45:58.844632 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 11:48:49.575410 139629069661376 submission_runner.py:469] Time since start: 59724.24s, 	Step: 102073, 	{'train/accuracy': 0.6997678875923157, 'train/loss': 1.384271264076233, 'train/bleu': 35.732994034266106, 'validation/accuracy': 0.6926062703132629, 'validation/loss': 1.4101861715316772, 'validation/bleu': 30.58568358791818, 'validation/num_examples': 3000, 'test/accuracy': 0.7094542980194092, 'test/loss': 1.3213461637496948, 'test/bleu': 30.693340247297034, 'test/num_examples': 3003, 'score': 35306.76857018471, 'total_duration': 59724.240082502365, 'accumulated_submission_time': 35306.76857018471, 'accumulated_eval_time': 24411.072201013565, 'accumulated_logging_time': 0.9317092895507812}
I0306 11:48:49.594044 139485287241472 logging_writer.py:48] [102073] accumulated_eval_time=24411.1, accumulated_logging_time=0.931709, accumulated_submission_time=35306.8, global_step=102073, preemption_count=0, score=35306.8, test/accuracy=0.709454, test/bleu=30.6933, test/loss=1.32135, test/num_examples=3003, total_duration=59724.2, train/accuracy=0.699768, train/bleu=35.733, train/loss=1.38427, validation/accuracy=0.692606, validation/bleu=30.5857, validation/loss=1.41019, validation/num_examples=3000
I0306 11:48:59.239747 139485295634176 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5631901025772095, loss=1.4420031309127808
I0306 11:49:33.685668 139485287241472 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.576535165309906, loss=1.3533505201339722
I0306 11:50:08.121905 139485295634176 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5808497667312622, loss=1.428123950958252
I0306 11:50:42.614902 139485287241472 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5437461733818054, loss=1.428199291229248
I0306 11:51:17.102926 139485295634176 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5691004991531372, loss=1.3838088512420654
I0306 11:51:51.573991 139485287241472 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5667833089828491, loss=1.394656777381897
I0306 11:52:26.131530 139485295634176 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5702213644981384, loss=1.3674635887145996
I0306 11:53:00.673102 139485287241472 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.575131356716156, loss=1.4258620738983154
I0306 11:53:35.241891 139485295634176 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5734585523605347, loss=1.4060698747634888
I0306 11:54:09.754941 139485287241472 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5921669602394104, loss=1.4984995126724243
I0306 11:54:44.309675 139485295634176 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.6017299890518188, loss=1.4945043325424194
I0306 11:55:18.860767 139485287241472 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.555165708065033, loss=1.3867542743682861
I0306 11:55:53.383702 139485295634176 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5816372036933899, loss=1.4144725799560547
I0306 11:56:27.936376 139485287241472 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5882279872894287, loss=1.3768502473831177
I0306 11:57:02.497647 139485295634176 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.6184778213500977, loss=1.4527961015701294
I0306 11:57:37.039144 139485287241472 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5947885513305664, loss=1.5044258832931519
I0306 11:58:11.580297 139485295634176 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5860818028450012, loss=1.461714267730713
I0306 11:58:46.128861 139485287241472 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5793332457542419, loss=1.3947222232818604
I0306 11:59:20.725139 139485295634176 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5904215574264526, loss=1.459913969039917
I0306 11:59:55.259087 139485287241472 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5990214943885803, loss=1.4234495162963867
I0306 12:00:29.820474 139485295634176 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.6032525300979614, loss=1.4257882833480835
I0306 12:01:04.388727 139485287241472 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.590380072593689, loss=1.4636194705963135
I0306 12:01:38.953920 139485295634176 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5679996013641357, loss=1.3667550086975098
I0306 12:02:13.514163 139485287241472 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5564115047454834, loss=1.3075807094573975
I0306 12:02:48.045238 139485295634176 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5696592926979065, loss=1.3762898445129395
I0306 12:02:49.783815 139629069661376 spec.py:321] Evaluating on the training split.
I0306 12:02:52.398722 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 12:06:37.049663 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 12:06:39.654981 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 12:09:17.937426 139629069661376 spec.py:349] Evaluating on the test split.
I0306 12:09:20.533145 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 12:12:12.583441 139629069661376 submission_runner.py:469] Time since start: 61127.25s, 	Step: 104506, 	{'train/accuracy': 0.689873456954956, 'train/loss': 1.439408779144287, 'train/bleu': 35.61598909437462, 'validation/accuracy': 0.6920006275177002, 'validation/loss': 1.4096968173980713, 'validation/bleu': 30.75859781455179, 'validation/num_examples': 3000, 'test/accuracy': 0.7110647559165955, 'test/loss': 1.3187589645385742, 'test/bleu': 30.95148247585919, 'test/num_examples': 3003, 'score': 36146.824863910675, 'total_duration': 61127.24808430672, 'accumulated_submission_time': 36146.824863910675, 'accumulated_eval_time': 24973.871740341187, 'accumulated_logging_time': 0.9583919048309326}
I0306 12:12:12.604747 139485287241472 logging_writer.py:48] [104506] accumulated_eval_time=24973.9, accumulated_logging_time=0.958392, accumulated_submission_time=36146.8, global_step=104506, preemption_count=0, score=36146.8, test/accuracy=0.711065, test/bleu=30.9515, test/loss=1.31876, test/num_examples=3003, total_duration=61127.2, train/accuracy=0.689873, train/bleu=35.616, train/loss=1.43941, validation/accuracy=0.692001, validation/bleu=30.7586, validation/loss=1.4097, validation/num_examples=3000
I0306 12:12:45.272984 139485295634176 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5842727422714233, loss=1.4492239952087402
I0306 12:13:19.700888 139485287241472 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.6336571574211121, loss=1.420548439025879
I0306 12:13:54.196187 139485295634176 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.5821533799171448, loss=1.415663242340088
I0306 12:14:28.738704 139485287241472 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.579786479473114, loss=1.4236640930175781
I0306 12:15:03.270636 139485295634176 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5841532945632935, loss=1.4197173118591309
I0306 12:15:37.802485 139485287241472 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5739876627922058, loss=1.3161827325820923
I0306 12:16:12.363003 139485295634176 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.6145938634872437, loss=1.3756840229034424
I0306 12:16:46.900535 139485287241472 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5758381485939026, loss=1.4854034185409546
I0306 12:17:21.445696 139485295634176 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5871569514274597, loss=1.4032529592514038
I0306 12:17:55.984164 139485287241472 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.5930754542350769, loss=1.4010498523712158
I0306 12:18:30.527204 139485295634176 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5982345342636108, loss=1.4377201795578003
I0306 12:19:05.086881 139485287241472 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5819534659385681, loss=1.363217830657959
I0306 12:19:39.588106 139485295634176 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.6039412021636963, loss=1.4190443754196167
I0306 12:20:14.084896 139485287241472 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.6223030686378479, loss=1.440216302871704
I0306 12:20:48.604553 139485295634176 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.6119840145111084, loss=1.3515459299087524
I0306 12:21:23.078780 139485287241472 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.589488685131073, loss=1.4011095762252808
I0306 12:21:57.616454 139485295634176 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.5784335732460022, loss=1.4316638708114624
I0306 12:22:32.073860 139485287241472 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.6276317834854126, loss=1.4415446519851685
I0306 12:23:06.606511 139485295634176 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5799209475517273, loss=1.4648640155792236
I0306 12:23:41.168082 139485287241472 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.6133914589881897, loss=1.4032511711120605
I0306 12:24:15.684260 139485295634176 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.6150979995727539, loss=1.363680124282837
I0306 12:24:50.073907 139485287241472 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.6133379340171814, loss=1.4405908584594727
I0306 12:25:24.455007 139485295634176 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.6124845743179321, loss=1.4547157287597656
I0306 12:25:58.790253 139485287241472 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.614253044128418, loss=1.3540818691253662
I0306 12:26:12.890459 139629069661376 spec.py:321] Evaluating on the training split.
I0306 12:26:15.506994 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 12:29:45.258754 139629069661376 spec.py:333] Evaluating on the validation split.
I0306 12:29:47.854145 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 12:32:30.447302 139629069661376 spec.py:349] Evaluating on the test split.
I0306 12:32:33.045514 139629069661376 workload.py:181] Translating evaluation dataset.
I0306 12:35:10.867461 139629069661376 submission_runner.py:469] Time since start: 62505.53s, 	Step: 106942, 	{'train/accuracy': 0.709425151348114, 'train/loss': 1.3388859033584595, 'train/bleu': 36.44139157972261, 'validation/accuracy': 0.6941512227058411, 'validation/loss': 1.403806209564209, 'validation/bleu': 30.879184951191704, 'validation/num_examples': 3000, 'test/accuracy': 0.7094427347183228, 'test/loss': 1.3157206773757935, 'test/bleu': 30.909889470674766, 'test/num_examples': 3003, 'score': 36986.97761964798, 'total_duration': 62505.53214287758, 'accumulated_submission_time': 36986.97761964798, 'accumulated_eval_time': 25511.84870672226, 'accumulated_logging_time': 0.9881687164306641}
I0306 12:35:10.886091 139485295634176 logging_writer.py:48] [106942] accumulated_eval_time=25511.8, accumulated_logging_time=0.988169, accumulated_submission_time=36987, global_step=106942, preemption_count=0, score=36987, test/accuracy=0.709443, test/bleu=30.9099, test/loss=1.31572, test/num_examples=3003, total_duration=62505.5, train/accuracy=0.709425, train/bleu=36.4414, train/loss=1.33889, validation/accuracy=0.694151, validation/bleu=30.8792, validation/loss=1.40381, validation/num_examples=3000
I0306 12:35:10.906107 139485287241472 logging_writer.py:48] [106942] global_step=106942, preemption_count=0, score=36987
I0306 12:35:10.930217 139629069661376 submission_runner.py:646] Tuning trial 3/5
I0306 12:35:10.930320 139629069661376 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0306 12:35:10.931825 139629069661376 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006161570199765265, 'train/loss': 11.21369457244873, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.19946575164795, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.181259155273438, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.486480712890625, 'total_duration': 939.3783233165741, 'accumulated_submission_time': 25.486480712890625, 'accumulated_eval_time': 913.8917396068573, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2429, {'train/accuracy': 0.4288758635520935, 'train/loss': 3.7705769538879395, 'train/bleu': 15.556901193491276, 'validation/accuracy': 0.4122438430786133, 'validation/loss': 3.9038379192352295, 'validation/bleu': 10.826612800200751, 'validation/num_examples': 3000, 'test/accuracy': 0.39797243475914, 'test/loss': 4.095420837402344, 'test/bleu': 8.810905968810859, 'test/num_examples': 3003, 'score': 865.4539229869843, 'total_duration': 2402.540598630905, 'accumulated_submission_time': 865.4539229869843, 'accumulated_eval_time': 1536.922968864441, 'accumulated_logging_time': 0.014814615249633789, 'global_step': 2429, 'preemption_count': 0}), (4854, {'train/accuracy': 0.54524827003479, 'train/loss': 2.619779109954834, 'train/bleu': 25.27517063684932, 'validation/accuracy': 0.5513929724693298, 'validation/loss': 2.559023857116699, 'validation/bleu': 20.78442819427946, 'validation/num_examples': 3000, 'test/accuracy': 0.5495771169662476, 'test/loss': 2.5930731296539307, 'test/bleu': 19.194787685710793, 'test/num_examples': 3003, 'score': 1705.4736919403076, 'total_duration': 3728.3738470077515, 'accumulated_submission_time': 1705.4736919403076, 'accumulated_eval_time': 2022.577241897583, 'accumulated_logging_time': 0.03182816505432129, 'global_step': 4854, 'preemption_count': 0}), (7280, {'train/accuracy': 0.5875570774078369, 'train/loss': 2.212049961090088, 'train/bleu': 27.906779858640675, 'validation/accuracy': 0.5898820757865906, 'validation/loss': 2.1753923892974854, 'validation/bleu': 23.340932773479175, 'validation/num_examples': 3000, 'test/accuracy': 0.5919128656387329, 'test/loss': 2.1698529720306396, 'test/bleu': 22.152463632951903, 'test/num_examples': 3003, 'score': 2545.3964664936066, 'total_duration': 5022.101184844971, 'accumulated_submission_time': 2545.3964664936066, 'accumulated_eval_time': 2476.2249076366425, 'accumulated_logging_time': 0.04904675483703613, 'global_step': 7280, 'preemption_count': 0}), (9710, {'train/accuracy': 0.5940812826156616, 'train/loss': 2.128690242767334, 'train/bleu': 28.688456547332986, 'validation/accuracy': 0.6115615963935852, 'validation/loss': 1.9945515394210815, 'validation/bleu': 24.638138219074122, 'validation/num_examples': 3000, 'test/accuracy': 0.6168694496154785, 'test/loss': 1.9696519374847412, 'test/bleu': 23.798854738651972, 'test/num_examples': 3003, 'score': 3385.402242898941, 'total_duration': 6320.681403398514, 'accumulated_submission_time': 3385.402242898941, 'accumulated_eval_time': 2934.643504858017, 'accumulated_logging_time': 0.0671086311340332, 'global_step': 9710, 'preemption_count': 0}), (12142, {'train/accuracy': 0.6039047837257385, 'train/loss': 2.0357227325439453, 'train/bleu': 28.78666351948088, 'validation/accuracy': 0.6217833161354065, 'validation/loss': 1.885305404663086, 'validation/bleu': 25.508397903086852, 'validation/num_examples': 3000, 'test/accuracy': 0.6304020285606384, 'test/loss': 1.8356561660766602, 'test/bleu': 24.77202011508364, 'test/num_examples': 3003, 'score': 4225.338691949844, 'total_duration': 7668.525817155838, 'accumulated_submission_time': 4225.338691949844, 'accumulated_eval_time': 3442.4033029079437, 'accumulated_logging_time': 0.08451223373413086, 'global_step': 12142, 'preemption_count': 0}), (14571, {'train/accuracy': 0.6147860884666443, 'train/loss': 1.9359149932861328, 'train/bleu': 29.970472072607677, 'validation/accuracy': 0.6356883645057678, 'validation/loss': 1.7836273908615112, 'validation/bleu': 26.744759991811442, 'validation/num_examples': 3000, 'test/accuracy': 0.6414899826049805, 'test/loss': 1.7355459928512573, 'test/bleu': 25.53281453509334, 'test/num_examples': 3003, 'score': 5065.210557937622, 'total_duration': 8981.490393161774, 'accumulated_submission_time': 5065.210557937622, 'accumulated_eval_time': 3915.346025943756, 'accumulated_logging_time': 0.10158371925354004, 'global_step': 14571, 'preemption_count': 0}), (17000, {'train/accuracy': 0.6206833720207214, 'train/loss': 1.8909770250320435, 'train/bleu': 30.060995666829, 'validation/accuracy': 0.6413739323616028, 'validation/loss': 1.7339715957641602, 'validation/bleu': 27.08305901709383, 'validation/num_examples': 3000, 'test/accuracy': 0.6501564383506775, 'test/loss': 1.6826587915420532, 'test/bleu': 26.340757724430667, 'test/num_examples': 3003, 'score': 5905.349917650223, 'total_duration': 10354.834623336792, 'accumulated_submission_time': 5905.349917650223, 'accumulated_eval_time': 4448.398643493652, 'accumulated_logging_time': 0.11999988555908203, 'global_step': 17000, 'preemption_count': 0}), (19426, {'train/accuracy': 0.6344527006149292, 'train/loss': 1.7864872217178345, 'train/bleu': 30.82396650855077, 'validation/accuracy': 0.6451066732406616, 'validation/loss': 1.701812505722046, 'validation/bleu': 27.43860230083909, 'validation/num_examples': 3000, 'test/accuracy': 0.6558915376663208, 'test/loss': 1.6511059999465942, 'test/bleu': 26.37713326447981, 'test/num_examples': 3003, 'score': 6745.300940513611, 'total_duration': 11735.453484773636, 'accumulated_submission_time': 6745.300940513611, 'accumulated_eval_time': 4988.9173402786255, 'accumulated_logging_time': 0.13750410079956055, 'global_step': 19426, 'preemption_count': 0}), (21845, {'train/accuracy': 0.6304247975349426, 'train/loss': 1.8210152387619019, 'train/bleu': 30.263021978033695, 'validation/accuracy': 0.647689938545227, 'validation/loss': 1.676596999168396, 'validation/bleu': 27.600635469041226, 'validation/num_examples': 3000, 'test/accuracy': 0.6587880849838257, 'test/loss': 1.6251347064971924, 'test/bleu': 26.85147099939014, 'test/num_examples': 3003, 'score': 7585.1607456207275, 'total_duration': 13162.6039788723, 'accumulated_submission_time': 7585.1607456207275, 'accumulated_eval_time': 5576.051920175552, 'accumulated_logging_time': 0.16083908081054688, 'global_step': 21845, 'preemption_count': 0}), (24266, {'train/accuracy': 0.6300191879272461, 'train/loss': 1.816698431968689, 'train/bleu': 30.368400848139316, 'validation/accuracy': 0.6515709757804871, 'validation/loss': 1.660017728805542, 'validation/bleu': 27.9377035280994, 'validation/num_examples': 3000, 'test/accuracy': 0.6619974374771118, 'test/loss': 1.6005034446716309, 'test/bleu': 26.87677742911548, 'test/num_examples': 3003, 'score': 8425.184804916382, 'total_duration': 14642.143604040146, 'accumulated_submission_time': 8425.184804916382, 'accumulated_eval_time': 6215.4146184921265, 'accumulated_logging_time': 0.18214893341064453, 'global_step': 24266, 'preemption_count': 0}), (26693, {'train/accuracy': 0.6381562948226929, 'train/loss': 1.7634669542312622, 'train/bleu': 30.801172207068284, 'validation/accuracy': 0.6547598838806152, 'validation/loss': 1.641645073890686, 'validation/bleu': 27.654973346886848, 'validation/num_examples': 3000, 'test/accuracy': 0.6655312180519104, 'test/loss': 1.5759555101394653, 'test/bleu': 27.631221936801985, 'test/num_examples': 3003, 'score': 9265.13209104538, 'total_duration': 16007.787855863571, 'accumulated_submission_time': 9265.13209104538, 'accumulated_eval_time': 6740.958631277084, 'accumulated_logging_time': 0.20258116722106934, 'global_step': 26693, 'preemption_count': 0}), (29122, {'train/accuracy': 0.6330106258392334, 'train/loss': 1.8009551763534546, 'train/bleu': 31.084494807019496, 'validation/accuracy': 0.6558228135108948, 'validation/loss': 1.6337389945983887, 'validation/bleu': 27.876580072890555, 'validation/num_examples': 3000, 'test/accuracy': 0.6659135818481445, 'test/loss': 1.5701812505722046, 'test/bleu': 27.329299125560674, 'test/num_examples': 3003, 'score': 10105.314846277237, 'total_duration': 17381.29731631279, 'accumulated_submission_time': 10105.314846277237, 'accumulated_eval_time': 7274.132599115372, 'accumulated_logging_time': 0.2228257656097412, 'global_step': 29122, 'preemption_count': 0}), (31550, {'train/accuracy': 0.6594426035881042, 'train/loss': 1.6123465299606323, 'train/bleu': 32.65510227290665, 'validation/accuracy': 0.6556497812271118, 'validation/loss': 1.6266634464263916, 'validation/bleu': 27.988799619314513, 'validation/num_examples': 3000, 'test/accuracy': 0.6673849821090698, 'test/loss': 1.5585217475891113, 'test/bleu': 27.739321066917825, 'test/num_examples': 3003, 'score': 10945.24265384674, 'total_duration': 18710.986840963364, 'accumulated_submission_time': 10945.24265384674, 'accumulated_eval_time': 7763.742249488831, 'accumulated_logging_time': 0.24209904670715332, 'global_step': 31550, 'preemption_count': 0}), (33977, {'train/accuracy': 0.6380448341369629, 'train/loss': 1.7544784545898438, 'train/bleu': 31.0528931223781, 'validation/accuracy': 0.6595432162284851, 'validation/loss': 1.6135351657867432, 'validation/bleu': 27.91781953936735, 'validation/num_examples': 3000, 'test/accuracy': 0.6691229343414307, 'test/loss': 1.5504486560821533, 'test/bleu': 27.819355708265213, 'test/num_examples': 3003, 'score': 11785.367548704147, 'total_duration': 20148.628581762314, 'accumulated_submission_time': 11785.367548704147, 'accumulated_eval_time': 8361.106194972992, 'accumulated_logging_time': 0.26242637634277344, 'global_step': 33977, 'preemption_count': 0}), (36409, {'train/accuracy': 0.6332497596740723, 'train/loss': 1.8029546737670898, 'train/bleu': 30.968016684484404, 'validation/accuracy': 0.6551306843757629, 'validation/loss': 1.6354548931121826, 'validation/bleu': 28.069561911623605, 'validation/num_examples': 3000, 'test/accuracy': 0.6643726229667664, 'test/loss': 1.5792622566223145, 'test/bleu': 27.233355359681738, 'test/num_examples': 3003, 'score': 12625.336913824081, 'total_duration': 21578.452902317047, 'accumulated_submission_time': 12625.336913824081, 'accumulated_eval_time': 8950.807389974594, 'accumulated_logging_time': 0.2835266590118408, 'global_step': 36409, 'preemption_count': 0}), (38841, {'train/accuracy': 0.644031822681427, 'train/loss': 1.7252724170684814, 'train/bleu': 31.59365271856774, 'validation/accuracy': 0.6603094935417175, 'validation/loss': 1.6036081314086914, 'validation/bleu': 28.70528621685984, 'validation/num_examples': 3000, 'test/accuracy': 0.6729927062988281, 'test/loss': 1.5313379764556885, 'test/bleu': 28.08534873260227, 'test/num_examples': 3003, 'score': 13465.22971701622, 'total_duration': 22942.8056640625, 'accumulated_submission_time': 13465.22971701622, 'accumulated_eval_time': 9475.118876934052, 'accumulated_logging_time': 0.30321764945983887, 'global_step': 38841, 'preemption_count': 0}), (41271, {'train/accuracy': 0.6406680941581726, 'train/loss': 1.7505210638046265, 'train/bleu': 31.086580752873505, 'validation/accuracy': 0.6600375771522522, 'validation/loss': 1.600296974182129, 'validation/bleu': 28.318280312187085, 'validation/num_examples': 3000, 'test/accuracy': 0.672610342502594, 'test/loss': 1.5272399187088013, 'test/bleu': 27.61382537255236, 'test/num_examples': 3003, 'score': 14305.239505290985, 'total_duration': 24340.500559568405, 'accumulated_submission_time': 14305.239505290985, 'accumulated_eval_time': 10032.651314973831, 'accumulated_logging_time': 0.32290196418762207, 'global_step': 41271, 'preemption_count': 0}), (43702, {'train/accuracy': 0.641524076461792, 'train/loss': 1.7379132509231567, 'train/bleu': 31.190737905214565, 'validation/accuracy': 0.6637208461761475, 'validation/loss': 1.5866236686706543, 'validation/bleu': 28.50399513128774, 'validation/num_examples': 3000, 'test/accuracy': 0.6762484312057495, 'test/loss': 1.5145207643508911, 'test/bleu': 28.305121424896882, 'test/num_examples': 3003, 'score': 15145.375562429428, 'total_duration': 25763.74303984642, 'accumulated_submission_time': 15145.375562429428, 'accumulated_eval_time': 10615.60698056221, 'accumulated_logging_time': 0.3427255153656006, 'global_step': 43702, 'preemption_count': 0}), (46139, {'train/accuracy': 0.6466573476791382, 'train/loss': 1.7067011594772339, 'train/bleu': 31.18560964709687, 'validation/accuracy': 0.6643636226654053, 'validation/loss': 1.5744129419326782, 'validation/bleu': 28.764355914581728, 'validation/num_examples': 3000, 'test/accuracy': 0.675344705581665, 'test/loss': 1.5105329751968384, 'test/bleu': 28.090970119457992, 'test/num_examples': 3003, 'score': 15985.421673297882, 'total_duration': 27237.339407920837, 'accumulated_submission_time': 15985.421673297882, 'accumulated_eval_time': 11249.003007411957, 'accumulated_logging_time': 0.36733245849609375, 'global_step': 46139, 'preemption_count': 0}), (48576, {'train/accuracy': 0.6445617079734802, 'train/loss': 1.71452796459198, 'train/bleu': 31.336832431731086, 'validation/accuracy': 0.666192889213562, 'validation/loss': 1.570813536643982, 'validation/bleu': 28.719829313074563, 'validation/num_examples': 3000, 'test/accuracy': 0.6782412528991699, 'test/loss': 1.4970651865005493, 'test/bleu': 28.41594754520476, 'test/num_examples': 3003, 'score': 16825.53476881981, 'total_duration': 28600.473111629486, 'accumulated_submission_time': 16825.53476881981, 'accumulated_eval_time': 11771.8760368824, 'accumulated_logging_time': 0.3877696990966797, 'global_step': 48576, 'preemption_count': 0}), (51013, {'train/accuracy': 0.6554715633392334, 'train/loss': 1.641449213027954, 'train/bleu': 32.2163874836394, 'validation/accuracy': 0.6674535870552063, 'validation/loss': 1.5638982057571411, 'validation/bleu': 28.855917279485706, 'validation/num_examples': 3000, 'test/accuracy': 0.6805931925773621, 'test/loss': 1.4873325824737549, 'test/bleu': 28.607490449108738, 'test/num_examples': 3003, 'score': 17665.63838148117, 'total_duration': 29973.72046995163, 'accumulated_submission_time': 17665.63838148117, 'accumulated_eval_time': 12304.872150182724, 'accumulated_logging_time': 0.4085354804992676, 'global_step': 51013, 'preemption_count': 0}), (53444, {'train/accuracy': 0.6479129791259766, 'train/loss': 1.6979836225509644, 'train/bleu': 31.993127864367104, 'validation/accuracy': 0.6690109372138977, 'validation/loss': 1.5569850206375122, 'validation/bleu': 28.822276867453446, 'validation/num_examples': 3000, 'test/accuracy': 0.6821573376655579, 'test/loss': 1.4822558164596558, 'test/bleu': 28.995730783884945, 'test/num_examples': 3003, 'score': 18505.494869709015, 'total_duration': 31424.970360040665, 'accumulated_submission_time': 18505.494869709015, 'accumulated_eval_time': 12916.115787029266, 'accumulated_logging_time': 0.42891979217529297, 'global_step': 53444, 'preemption_count': 0}), (55876, {'train/accuracy': 0.6456969976425171, 'train/loss': 1.7108418941497803, 'train/bleu': 31.53783157345093, 'validation/accuracy': 0.6695671677589417, 'validation/loss': 1.5440362691879272, 'validation/bleu': 29.17804894591293, 'validation/num_examples': 3000, 'test/accuracy': 0.6823890805244446, 'test/loss': 1.4736313819885254, 'test/bleu': 28.66418574733848, 'test/num_examples': 3003, 'score': 19345.63218808174, 'total_duration': 32807.86678314209, 'accumulated_submission_time': 19345.63218808174, 'accumulated_eval_time': 13458.72381901741, 'accumulated_logging_time': 0.45050048828125, 'global_step': 55876, 'preemption_count': 0}), (58309, {'train/accuracy': 0.6540137529373169, 'train/loss': 1.6488738059997559, 'train/bleu': 32.069103966589715, 'validation/accuracy': 0.6717796325683594, 'validation/loss': 1.5363051891326904, 'validation/bleu': 29.123070046912968, 'validation/num_examples': 3000, 'test/accuracy': 0.6833738684654236, 'test/loss': 1.4707404375076294, 'test/bleu': 28.560709831761415, 'test/num_examples': 3003, 'score': 20185.77959752083, 'total_duration': 34249.220818042755, 'accumulated_submission_time': 20185.77959752083, 'accumulated_eval_time': 14059.781799316406, 'accumulated_logging_time': 0.4727177619934082, 'global_step': 58309, 'preemption_count': 0}), (60742, {'train/accuracy': 0.6510465145111084, 'train/loss': 1.6649905443191528, 'train/bleu': 31.73757337088702, 'validation/accuracy': 0.6714829802513123, 'validation/loss': 1.5333079099655151, 'validation/bleu': 29.144631008834217, 'validation/num_examples': 3000, 'test/accuracy': 0.6874058842658997, 'test/loss': 1.4529725313186646, 'test/bleu': 29.015509816789326, 'test/num_examples': 3003, 'score': 21025.717707395554, 'total_duration': 35626.89278316498, 'accumulated_submission_time': 21025.717707395554, 'accumulated_eval_time': 14597.367243289948, 'accumulated_logging_time': 0.49491024017333984, 'global_step': 60742, 'preemption_count': 0}), (63176, {'train/accuracy': 0.6708105802536011, 'train/loss': 1.5459836721420288, 'train/bleu': 33.10576217857777, 'validation/accuracy': 0.6734482049942017, 'validation/loss': 1.5189322233200073, 'validation/bleu': 29.308808644526323, 'validation/num_examples': 3000, 'test/accuracy': 0.6857722401618958, 'test/loss': 1.4500166177749634, 'test/bleu': 29.057067708043235, 'test/num_examples': 3003, 'score': 21865.76618027687, 'total_duration': 37052.284259557724, 'accumulated_submission_time': 21865.76618027687, 'accumulated_eval_time': 15182.561658143997, 'accumulated_logging_time': 0.517808198928833, 'global_step': 63176, 'preemption_count': 0}), (65612, {'train/accuracy': 0.6556352972984314, 'train/loss': 1.6450740098953247, 'train/bleu': 32.32858308651338, 'validation/accuracy': 0.6741898059844971, 'validation/loss': 1.515145182609558, 'validation/bleu': 29.320158284295537, 'validation/num_examples': 3000, 'test/accuracy': 0.6878229975700378, 'test/loss': 1.4411917924880981, 'test/bleu': 29.242922385852413, 'test/num_examples': 3003, 'score': 22705.92084646225, 'total_duration': 38466.83321976662, 'accumulated_submission_time': 22705.92084646225, 'accumulated_eval_time': 15756.805934429169, 'accumulated_logging_time': 0.5413532257080078, 'global_step': 65612, 'preemption_count': 0}), (68050, {'train/accuracy': 0.6523852944374084, 'train/loss': 1.666360855102539, 'train/bleu': 31.95014051580668, 'validation/accuracy': 0.6741156578063965, 'validation/loss': 1.5079776048660278, 'validation/bleu': 29.472558254503635, 'validation/num_examples': 3000, 'test/accuracy': 0.6910207271575928, 'test/loss': 1.4314724206924438, 'test/bleu': 29.339185119097035, 'test/num_examples': 3003, 'score': 23545.919609069824, 'total_duration': 39850.59995150566, 'accumulated_submission_time': 23545.919609069824, 'accumulated_eval_time': 16300.424684047699, 'accumulated_logging_time': 0.5649464130401611, 'global_step': 68050, 'preemption_count': 0}), (70484, {'train/accuracy': 0.6639842987060547, 'train/loss': 1.596910834312439, 'train/bleu': 33.22425828389636, 'validation/accuracy': 0.6769831776618958, 'validation/loss': 1.4955430030822754, 'validation/bleu': 29.64503239183237, 'validation/num_examples': 3000, 'test/accuracy': 0.6908701062202454, 'test/loss': 1.4201960563659668, 'test/bleu': 29.216032825318784, 'test/num_examples': 3003, 'score': 24385.96539258957, 'total_duration': 41276.45101213455, 'accumulated_submission_time': 24385.96539258957, 'accumulated_eval_time': 16886.082045793533, 'accumulated_logging_time': 0.587174654006958, 'global_step': 70484, 'preemption_count': 0}), (72915, {'train/accuracy': 0.6623005867004395, 'train/loss': 1.6121453046798706, 'train/bleu': 32.95397298149998, 'validation/accuracy': 0.6780337691307068, 'validation/loss': 1.4897774457931519, 'validation/bleu': 29.562043954383505, 'validation/num_examples': 3000, 'test/accuracy': 0.6924921870231628, 'test/loss': 1.4109165668487549, 'test/bleu': 29.245773949115474, 'test/num_examples': 3003, 'score': 25226.096336841583, 'total_duration': 42633.00661087036, 'accumulated_submission_time': 25226.096336841583, 'accumulated_eval_time': 17402.356122016907, 'accumulated_logging_time': 0.6100254058837891, 'global_step': 72915, 'preemption_count': 0}), (75338, {'train/accuracy': 0.7071047425270081, 'train/loss': 1.3568321466445923, 'train/bleu': 35.8541383155911, 'validation/accuracy': 0.6799866557121277, 'validation/loss': 1.487004280090332, 'validation/bleu': 30.0304687842793, 'validation/num_examples': 3000, 'test/accuracy': 0.6945081949234009, 'test/loss': 1.4004243612289429, 'test/bleu': 29.805077926426073, 'test/num_examples': 3003, 'score': 26066.18368411064, 'total_duration': 44082.62029528618, 'accumulated_submission_time': 26066.18368411064, 'accumulated_eval_time': 18011.733072519302, 'accumulated_logging_time': 0.6341073513031006, 'global_step': 75338, 'preemption_count': 0}), (77762, {'train/accuracy': 0.6659988760948181, 'train/loss': 1.5725090503692627, 'train/bleu': 33.087009210637206, 'validation/accuracy': 0.6815934777259827, 'validation/loss': 1.4700863361358643, 'validation/bleu': 30.034001688716202, 'validation/num_examples': 3000, 'test/accuracy': 0.6941490173339844, 'test/loss': 1.3987714052200317, 'test/bleu': 29.672812843596468, 'test/num_examples': 3003, 'score': 26906.169424057007, 'total_duration': 45524.24109983444, 'accumulated_submission_time': 26906.169424057007, 'accumulated_eval_time': 18613.212021827698, 'accumulated_logging_time': 0.6627371311187744, 'global_step': 77762, 'preemption_count': 0}), (80186, {'train/accuracy': 0.6655203104019165, 'train/loss': 1.5883437395095825, 'train/bleu': 33.22163166887323, 'validation/accuracy': 0.6819766163825989, 'validation/loss': 1.4697943925857544, 'validation/bleu': 29.77887150526415, 'validation/num_examples': 3000, 'test/accuracy': 0.6966168284416199, 'test/loss': 1.3908458948135376, 'test/bleu': 29.816601216249484, 'test/num_examples': 3003, 'score': 27746.085688352585, 'total_duration': 46966.541915655136, 'accumulated_submission_time': 27746.085688352585, 'accumulated_eval_time': 19215.443091630936, 'accumulated_logging_time': 0.6914234161376953, 'global_step': 80186, 'preemption_count': 0}), (82610, {'train/accuracy': 0.6787692308425903, 'train/loss': 1.501219630241394, 'train/bleu': 34.15638195142007, 'validation/accuracy': 0.6833733320236206, 'validation/loss': 1.4623011350631714, 'validation/bleu': 30.132553181091396, 'validation/num_examples': 3000, 'test/accuracy': 0.6979029178619385, 'test/loss': 1.3808497190475464, 'test/bleu': 29.82716792614236, 'test/num_examples': 3003, 'score': 28586.279491901398, 'total_duration': 48411.500180482864, 'accumulated_submission_time': 28586.279491901398, 'accumulated_eval_time': 19820.05669569969, 'accumulated_logging_time': 0.7164402008056641, 'global_step': 82610, 'preemption_count': 0}), (85037, {'train/accuracy': 0.6714715361595154, 'train/loss': 1.5471140146255493, 'train/bleu': 33.579012171528916, 'validation/accuracy': 0.6855610609054565, 'validation/loss': 1.4485142230987549, 'validation/bleu': 30.1081861286188, 'validation/num_examples': 3000, 'test/accuracy': 0.7013440132141113, 'test/loss': 1.366485357284546, 'test/bleu': 30.416618790596697, 'test/num_examples': 3003, 'score': 29426.418329000473, 'total_duration': 49856.01018476486, 'accumulated_submission_time': 29426.418329000473, 'accumulated_eval_time': 20424.270758867264, 'accumulated_logging_time': 0.7420802116394043, 'global_step': 85037, 'preemption_count': 0}), (87470, {'train/accuracy': 0.6733237504959106, 'train/loss': 1.5478787422180176, 'train/bleu': 34.07862561112074, 'validation/accuracy': 0.6861295700073242, 'validation/loss': 1.4439963102340698, 'validation/bleu': 30.357072380601185, 'validation/num_examples': 3000, 'test/accuracy': 0.7019001245498657, 'test/loss': 1.3604204654693604, 'test/bleu': 30.34645853642891, 'test/num_examples': 3003, 'score': 30266.299453258514, 'total_duration': 51249.785928964615, 'accumulated_submission_time': 30266.299453258514, 'accumulated_eval_time': 20978.011278152466, 'accumulated_logging_time': 0.766226053237915, 'global_step': 87470, 'preemption_count': 0}), (89904, {'train/accuracy': 0.68186354637146, 'train/loss': 1.482079267501831, 'train/bleu': 34.32693509025636, 'validation/accuracy': 0.6874274015426636, 'validation/loss': 1.438246250152588, 'validation/bleu': 30.183784131444092, 'validation/num_examples': 3000, 'test/accuracy': 0.7033483982086182, 'test/loss': 1.3508611917495728, 'test/bleu': 30.383179845726477, 'test/num_examples': 3003, 'score': 31106.42396378517, 'total_duration': 52660.92375588417, 'accumulated_submission_time': 31106.42396378517, 'accumulated_eval_time': 21548.871358394623, 'accumulated_logging_time': 0.7919626235961914, 'global_step': 89904, 'preemption_count': 0}), (92340, {'train/accuracy': 0.6789466142654419, 'train/loss': 1.5095155239105225, 'train/bleu': 34.49960671027544, 'validation/accuracy': 0.6882184147834778, 'validation/loss': 1.434462070465088, 'validation/bleu': 30.375925305243374, 'validation/num_examples': 3000, 'test/accuracy': 0.7045186161994934, 'test/loss': 1.3457274436950684, 'test/bleu': 30.594519714092794, 'test/num_examples': 3003, 'score': 31946.605330705643, 'total_duration': 54068.790437698364, 'accumulated_submission_time': 31946.605330705643, 'accumulated_eval_time': 22116.40487098694, 'accumulated_logging_time': 0.817626953125, 'global_step': 92340, 'preemption_count': 0}), (94774, {'train/accuracy': 0.6968047618865967, 'train/loss': 1.404218077659607, 'train/bleu': 35.49709408055989, 'validation/accuracy': 0.6890836358070374, 'validation/loss': 1.425716757774353, 'validation/bleu': 30.612033610576812, 'validation/num_examples': 3000, 'test/accuracy': 0.706071138381958, 'test/loss': 1.3368157148361206, 'test/bleu': 30.483672062793904, 'test/num_examples': 3003, 'score': 32786.5674738884, 'total_duration': 55451.01929616928, 'accumulated_submission_time': 32786.5674738884, 'accumulated_eval_time': 22658.51594042778, 'accumulated_logging_time': 0.8466439247131348, 'global_step': 94774, 'preemption_count': 0}), (97206, {'train/accuracy': 0.6900190114974976, 'train/loss': 1.4364527463912964, 'train/bleu': 35.01110227645804, 'validation/accuracy': 0.69059157371521, 'validation/loss': 1.4213261604309082, 'validation/bleu': 30.70094834209247, 'validation/num_examples': 3000, 'test/accuracy': 0.7081450819969177, 'test/loss': 1.325734257698059, 'test/bleu': 30.678121408017475, 'test/num_examples': 3003, 'score': 33626.468497276306, 'total_duration': 56853.3116543293, 'accumulated_submission_time': 33626.468497276306, 'accumulated_eval_time': 23220.751407384872, 'accumulated_logging_time': 0.875568151473999, 'global_step': 97206, 'preemption_count': 0}), (99639, {'train/accuracy': 0.6873375177383423, 'train/loss': 1.455028772354126, 'train/bleu': 34.648668799352336, 'validation/accuracy': 0.6912837028503418, 'validation/loss': 1.4166932106018066, 'validation/bleu': 30.612191116172603, 'validation/num_examples': 3000, 'test/accuracy': 0.7073224186897278, 'test/loss': 1.3296566009521484, 'test/bleu': 30.518567262327068, 'test/num_examples': 3003, 'score': 34466.63046216965, 'total_duration': 58297.24188733101, 'accumulated_submission_time': 34466.63046216965, 'accumulated_eval_time': 23824.367573738098, 'accumulated_logging_time': 0.9014079570770264, 'global_step': 99639, 'preemption_count': 0}), (102073, {'train/accuracy': 0.6997678875923157, 'train/loss': 1.384271264076233, 'train/bleu': 35.732994034266106, 'validation/accuracy': 0.6926062703132629, 'validation/loss': 1.4101861715316772, 'validation/bleu': 30.58568358791818, 'validation/num_examples': 3000, 'test/accuracy': 0.7094542980194092, 'test/loss': 1.3213461637496948, 'test/bleu': 30.693340247297034, 'test/num_examples': 3003, 'score': 35306.76857018471, 'total_duration': 59724.240082502365, 'accumulated_submission_time': 35306.76857018471, 'accumulated_eval_time': 24411.072201013565, 'accumulated_logging_time': 0.9317092895507812, 'global_step': 102073, 'preemption_count': 0}), (104506, {'train/accuracy': 0.689873456954956, 'train/loss': 1.439408779144287, 'train/bleu': 35.61598909437462, 'validation/accuracy': 0.6920006275177002, 'validation/loss': 1.4096968173980713, 'validation/bleu': 30.75859781455179, 'validation/num_examples': 3000, 'test/accuracy': 0.7110647559165955, 'test/loss': 1.3187589645385742, 'test/bleu': 30.95148247585919, 'test/num_examples': 3003, 'score': 36146.824863910675, 'total_duration': 61127.24808430672, 'accumulated_submission_time': 36146.824863910675, 'accumulated_eval_time': 24973.871740341187, 'accumulated_logging_time': 0.9583919048309326, 'global_step': 104506, 'preemption_count': 0}), (106942, {'train/accuracy': 0.709425151348114, 'train/loss': 1.3388859033584595, 'train/bleu': 36.44139157972261, 'validation/accuracy': 0.6941512227058411, 'validation/loss': 1.403806209564209, 'validation/bleu': 30.879184951191704, 'validation/num_examples': 3000, 'test/accuracy': 0.7094427347183228, 'test/loss': 1.3157206773757935, 'test/bleu': 30.909889470674766, 'test/num_examples': 3003, 'score': 36986.97761964798, 'total_duration': 62505.53214287758, 'accumulated_submission_time': 36986.97761964798, 'accumulated_eval_time': 25511.84870672226, 'accumulated_logging_time': 0.9881687164306641, 'global_step': 106942, 'preemption_count': 0})], 'global_step': 106942}
I0306 12:35:10.931940 139629069661376 submission_runner.py:649] Timing: 36986.97761964798
I0306 12:35:10.931995 139629069661376 submission_runner.py:651] Total number of evals: 45
I0306 12:35:10.932029 139629069661376 submission_runner.py:652] ====================
I0306 12:35:10.932140 139629069661376 submission_runner.py:750] Final wmt score: 2
