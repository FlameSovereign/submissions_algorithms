python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=1186667789 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-36.log
2025-03-05 19:12:37.421774: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201957.443339       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201957.449958       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:44.154388 140014092387520 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax.
I0305 19:12:45.009686 140014092387520 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:45.012798 140014092387520 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:45.014579 140014092387520 submission_runner.py:606] Using RNG seed 1186667789
I0305 19:12:45.585355 140014092387520 submission_runner.py:615] --- Tuning run 2/5 ---
I0305 19:12:45.585545 140014092387520 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_2.
I0305 19:12:45.585731 140014092387520 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_2/hparams.json.
I0305 19:12:45.820611 140014092387520 submission_runner.py:218] Initializing dataset.
I0305 19:12:45.973656 140014092387520 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:46.049354 140014092387520 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:46.123227 140014092387520 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:47.460256 140014092387520 submission_runner.py:229] Initializing model.
I0305 19:13:27.179185 140014092387520 submission_runner.py:272] Initializing optimizer.
I0305 19:13:28.010040 140014092387520 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:28.010297 140014092387520 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:28.011338 140014092387520 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_2 with prefix checkpoint_
I0305 19:13:28.011451 140014092387520 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_2/meta_data_0.json.
I0305 19:13:28.011631 140014092387520 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:28.011681 140014092387520 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:28.196705 140014092387520 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_2/flags_0.json.
I0305 19:13:28.240836 140014092387520 submission_runner.py:337] Starting training loop.
I0305 19:13:53.351030 139878041884416 logging_writer.py:48] [0] global_step=0, grad_norm=4.643734455108643, loss=11.024527549743652
I0305 19:13:53.407422 140014092387520 spec.py:321] Evaluating on the training split.
I0305 19:13:53.409628 140014092387520 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:53.412812 140014092387520 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:53.443683 140014092387520 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:59.420321 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 19:19:01.815630 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 19:19:01.867601 140014092387520 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:01.879837 140014092387520 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:01.910260 140014092387520 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:06.909005 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 19:24:03.158372 140014092387520 spec.py:349] Evaluating on the test split.
I0305 19:24:03.160416 140014092387520 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:03.163309 140014092387520 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:03.194447 140014092387520 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:05.996459 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 19:29:02.548246 140014092387520 submission_runner.py:469] Time since start: 934.31s, 	Step: 1, 	{'train/accuracy': 0.000524216506164521, 'train/loss': 11.01574420928955, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.013504981994629, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.02798843383789, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.166481494903564, 'total_duration': 934.3073287010193, 'accumulated_submission_time': 25.166481494903564, 'accumulated_eval_time': 909.140741109848, 'accumulated_logging_time': 0}
I0305 19:29:02.555061 139871188354816 logging_writer.py:48] [1] accumulated_eval_time=909.141, accumulated_logging_time=0, accumulated_submission_time=25.1665, global_step=1, preemption_count=0, score=25.1665, test/accuracy=0.000718341, test/bleu=0, test/loss=11.028, test/num_examples=3003, total_duration=934.307, train/accuracy=0.000524217, train/bleu=0, train/loss=11.0157, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.0135, validation/num_examples=3000
I0305 19:29:37.264038 139871179962112 logging_writer.py:48] [100] global_step=100, grad_norm=0.24058526754379272, loss=8.997389793395996
I0305 19:30:11.884624 139871188354816 logging_writer.py:48] [200] global_step=200, grad_norm=0.40731683373451233, loss=8.605867385864258
I0305 19:30:46.601767 139871179962112 logging_writer.py:48] [300] global_step=300, grad_norm=0.6260031461715698, loss=8.27677059173584
I0305 19:31:21.324868 139871188354816 logging_writer.py:48] [400] global_step=400, grad_norm=0.6607136726379395, loss=7.937429428100586
I0305 19:31:56.046184 139871179962112 logging_writer.py:48] [500] global_step=500, grad_norm=0.5785030722618103, loss=7.803365707397461
I0305 19:32:30.743007 139871188354816 logging_writer.py:48] [600] global_step=600, grad_norm=0.4922850430011749, loss=7.626466751098633
I0305 19:33:05.443135 139871179962112 logging_writer.py:48] [700] global_step=700, grad_norm=0.7501625418663025, loss=7.327084541320801
I0305 19:33:40.092618 139871188354816 logging_writer.py:48] [800] global_step=800, grad_norm=0.638584554195404, loss=7.1828107833862305
I0305 19:34:14.780076 139871179962112 logging_writer.py:48] [900] global_step=900, grad_norm=0.5095987915992737, loss=7.066248416900635
I0305 19:34:49.437694 139871188354816 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5351152420043945, loss=6.872255325317383
I0305 19:35:24.110031 139871179962112 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5055910348892212, loss=6.693821430206299
I0305 19:35:58.810982 139871188354816 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0395137071609497, loss=6.639377117156982
I0305 19:36:33.538114 139871179962112 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6152240633964539, loss=6.5816330909729
I0305 19:37:08.195459 139871188354816 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.44851505756378174, loss=6.448582172393799
I0305 19:37:42.819319 139871179962112 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5506106615066528, loss=6.287137985229492
I0305 19:38:17.418187 139871188354816 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7617893815040588, loss=6.248904228210449
I0305 19:38:52.080316 139871179962112 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5635503530502319, loss=6.1110711097717285
I0305 19:39:26.718468 139871188354816 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6926586627960205, loss=6.074484348297119
I0305 19:40:01.369503 139871179962112 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6593906879425049, loss=5.90979528427124
I0305 19:40:36.044230 139871188354816 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5967385768890381, loss=5.7927632331848145
I0305 19:41:10.738656 139871179962112 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8072367906570435, loss=5.744834899902344
I0305 19:41:45.391011 139871188354816 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6210600733757019, loss=5.589395999908447
I0305 19:42:20.062988 139871179962112 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.795914351940155, loss=5.5466766357421875
I0305 19:42:54.766166 139871188354816 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.6847646832466125, loss=5.510944366455078
I0305 19:43:02.737259 140014092387520 spec.py:321] Evaluating on the training split.
I0305 19:43:05.337668 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 19:46:18.641448 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 19:46:21.241583 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 19:49:34.177517 140014092387520 spec.py:349] Evaluating on the test split.
I0305 19:49:36.767114 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 19:52:33.973783 140014092387520 submission_runner.py:469] Time since start: 2345.73s, 	Step: 2424, 	{'train/accuracy': 0.4466957449913025, 'train/loss': 3.7516324520111084, 'train/bleu': 16.690424241742072, 'validation/accuracy': 0.43770548701286316, 'validation/loss': 3.831123113632202, 'validation/bleu': 12.14541629922039, 'validation/num_examples': 3000, 'test/accuracy': 0.4279457926750183, 'test/loss': 3.985069751739502, 'test/bleu': 10.687958103130974, 'test/num_examples': 3003, 'score': 865.1917860507965, 'total_duration': 2345.732857942581, 'accumulated_submission_time': 865.1917860507965, 'accumulated_eval_time': 1480.3771858215332, 'accumulated_logging_time': 0.015112876892089844}
I0305 19:52:33.983720 139871179962112 logging_writer.py:48] [2424] accumulated_eval_time=1480.38, accumulated_logging_time=0.0151129, accumulated_submission_time=865.192, global_step=2424, preemption_count=0, score=865.192, test/accuracy=0.427946, test/bleu=10.688, test/loss=3.98507, test/num_examples=3003, total_duration=2345.73, train/accuracy=0.446696, train/bleu=16.6904, train/loss=3.75163, validation/accuracy=0.437705, validation/bleu=12.1454, validation/loss=3.83112, validation/num_examples=3000
I0305 19:53:00.571541 139871188354816 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.566796600818634, loss=5.453936576843262
I0305 19:53:35.189451 139871179962112 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6155505776405334, loss=5.311712741851807
I0305 19:54:09.832111 139871188354816 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8762566447257996, loss=5.429545879364014
I0305 19:54:44.513283 139871179962112 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.6086310148239136, loss=5.360841751098633
I0305 19:55:19.186184 139871188354816 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5541353821754456, loss=5.193263530731201
I0305 19:55:53.863828 139871179962112 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6541038155555725, loss=5.188573360443115
I0305 19:56:28.535500 139871188354816 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.45009368658065796, loss=5.066932201385498
I0305 19:57:03.204629 139871179962112 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.44471558928489685, loss=4.981021881103516
I0305 19:57:37.882259 139871188354816 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.47202053666114807, loss=4.970449447631836
I0305 19:58:12.551378 139871179962112 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.47870275378227234, loss=5.011008262634277
I0305 19:58:47.208903 139871188354816 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.487131267786026, loss=5.0076680183410645
I0305 19:59:21.891569 139871179962112 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.46083196997642517, loss=4.960983753204346
I0305 19:59:56.584524 139871188354816 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5036691427230835, loss=4.854641437530518
I0305 20:00:31.273490 139871179962112 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5382941961288452, loss=4.9224138259887695
I0305 20:01:05.951157 139871188354816 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.37115904688835144, loss=4.834537029266357
I0305 20:01:40.613710 139871179962112 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.46825307607650757, loss=4.894798278808594
I0305 20:02:15.281576 139871188354816 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.38328149914741516, loss=4.7705817222595215
I0305 20:02:49.948473 139871179962112 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.389240562915802, loss=4.779759883880615
I0305 20:03:24.608878 139871188354816 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.4092767536640167, loss=4.79192590713501
I0305 20:03:59.311987 139871179962112 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.3930078148841858, loss=4.735280990600586
I0305 20:04:34.012383 139871188354816 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3662479519844055, loss=4.76729679107666
I0305 20:05:08.744197 139871179962112 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.383249968290329, loss=4.657389163970947
I0305 20:05:43.403805 139871188354816 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3299025893211365, loss=4.751857757568359
I0305 20:06:18.071684 139871179962112 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.34240835905075073, loss=4.665285110473633
I0305 20:06:33.989070 140014092387520 spec.py:321] Evaluating on the training split.
I0305 20:06:36.587643 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:09:25.755040 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 20:09:28.349948 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:12:13.188546 140014092387520 spec.py:349] Evaluating on the test split.
I0305 20:12:15.778403 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:14:45.803890 140014092387520 submission_runner.py:469] Time since start: 3677.56s, 	Step: 4847, 	{'train/accuracy': 0.5477670431137085, 'train/loss': 2.785271644592285, 'train/bleu': 24.105470152832527, 'validation/accuracy': 0.5556077361106873, 'validation/loss': 2.712578296661377, 'validation/bleu': 20.44556085971932, 'validation/num_examples': 3000, 'test/accuracy': 0.5558452010154724, 'test/loss': 2.726165771484375, 'test/bleu': 18.905257571089965, 'test/num_examples': 3003, 'score': 1705.0442910194397, 'total_duration': 3677.563007593155, 'accumulated_submission_time': 1705.0442910194397, 'accumulated_eval_time': 1972.1919617652893, 'accumulated_logging_time': 0.03391599655151367}
I0305 20:14:45.812516 139871154784000 logging_writer.py:48] [4847] accumulated_eval_time=1972.19, accumulated_logging_time=0.033916, accumulated_submission_time=1705.04, global_step=4847, preemption_count=0, score=1705.04, test/accuracy=0.555845, test/bleu=18.9053, test/loss=2.72617, test/num_examples=3003, total_duration=3677.56, train/accuracy=0.547767, train/bleu=24.1055, train/loss=2.78527, validation/accuracy=0.555608, validation/bleu=20.4456, validation/loss=2.71258, validation/num_examples=3000
I0305 20:15:04.451145 139871146391296 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.349526047706604, loss=4.648802757263184
I0305 20:15:39.016392 139871154784000 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.32785141468048096, loss=4.647704601287842
I0305 20:16:13.631517 139871146391296 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.31145545840263367, loss=4.597428321838379
I0305 20:16:48.280481 139871154784000 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3799886405467987, loss=4.667436122894287
I0305 20:17:22.944211 139871146391296 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.2872903347015381, loss=4.534120559692383
I0305 20:17:57.594373 139871154784000 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.29842546582221985, loss=4.530466556549072
I0305 20:18:32.223781 139871146391296 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.28165629506111145, loss=4.599733352661133
I0305 20:19:06.882420 139871154784000 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.329607754945755, loss=4.635398864746094
I0305 20:19:41.617043 139871146391296 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.2827157974243164, loss=4.505339622497559
I0305 20:20:16.263703 139871154784000 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.32311809062957764, loss=4.624215602874756
I0305 20:20:50.905878 139871146391296 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.26006433367729187, loss=4.479511737823486
I0305 20:21:25.598373 139871154784000 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.2827151417732239, loss=4.497324466705322
I0305 20:22:00.253226 139871146391296 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.2673037350177765, loss=4.5217437744140625
I0305 20:22:34.938539 139871154784000 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.2895151972770691, loss=4.553616523742676
I0305 20:23:09.617522 139870634698496 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.2674787938594818, loss=4.55990743637085
I0305 20:23:44.235728 139870626305792 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.2707459628582001, loss=4.5063323974609375
I0305 20:24:18.869708 139870634698496 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.24463221430778503, loss=4.426549434661865
I0305 20:24:53.477547 139870626305792 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2541714906692505, loss=4.406388759613037
I0305 20:25:28.093564 139870634698496 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.2336185723543167, loss=4.464244365692139
I0305 20:26:02.705327 139870626305792 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.2730681896209717, loss=4.53499174118042
I0305 20:26:37.337687 139870634698496 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.22394821047782898, loss=4.431537628173828
I0305 20:27:11.977269 139870626305792 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.22677522897720337, loss=4.4188032150268555
I0305 20:27:46.662107 139870634698496 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.26017916202545166, loss=4.468926906585693
I0305 20:28:21.316855 139870626305792 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.26760995388031006, loss=4.323439598083496
I0305 20:28:45.908254 140014092387520 spec.py:321] Evaluating on the training split.
I0305 20:28:48.509232 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:31:26.106345 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 20:31:28.700390 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:34:09.748775 140014092387520 spec.py:349] Evaluating on the test split.
I0305 20:34:12.346934 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:36:45.671188 140014092387520 submission_runner.py:469] Time since start: 4997.43s, 	Step: 7272, 	{'train/accuracy': 0.5899104475975037, 'train/loss': 2.3937723636627197, 'train/bleu': 27.11121135665028, 'validation/accuracy': 0.5968531370162964, 'validation/loss': 2.344010591506958, 'validation/bleu': 23.33131986550044, 'validation/num_examples': 3000, 'test/accuracy': 0.5992701053619385, 'test/loss': 2.327810764312744, 'test/bleu': 21.932223744420213, 'test/num_examples': 3003, 'score': 2544.989545583725, 'total_duration': 4997.430301189423, 'accumulated_submission_time': 2544.989545583725, 'accumulated_eval_time': 2451.9548530578613, 'accumulated_logging_time': 0.05225253105163574}
I0305 20:36:45.679853 139870634698496 logging_writer.py:48] [7272] accumulated_eval_time=2451.95, accumulated_logging_time=0.0522525, accumulated_submission_time=2544.99, global_step=7272, preemption_count=0, score=2544.99, test/accuracy=0.59927, test/bleu=21.9322, test/loss=2.32781, test/num_examples=3003, total_duration=4997.43, train/accuracy=0.58991, train/bleu=27.1112, train/loss=2.39377, validation/accuracy=0.596853, validation/bleu=23.3313, validation/loss=2.34401, validation/num_examples=3000
I0305 20:36:55.679354 139870626305792 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.24933737516403198, loss=4.353630065917969
I0305 20:37:30.252552 139870634698496 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.23752643167972565, loss=4.380317211151123
I0305 20:38:04.899715 139870626305792 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.23845882713794708, loss=4.411165714263916
I0305 20:38:39.555735 139870634698496 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.2073839008808136, loss=4.360684871673584
I0305 20:39:14.193893 139870626305792 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.20851443707942963, loss=4.33819055557251
I0305 20:39:48.845187 139870634698496 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.23198603093624115, loss=4.336646556854248
I0305 20:40:23.476984 139870626305792 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.23811814188957214, loss=4.337450981140137
I0305 20:40:58.131547 139870634698496 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.2201080322265625, loss=4.324833393096924
I0305 20:41:32.770293 139870626305792 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.20606592297554016, loss=4.317414283752441
I0305 20:42:07.411275 139870634698496 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.20611125230789185, loss=4.356935977935791
I0305 20:42:42.074317 139870626305792 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.21042391657829285, loss=4.319174766540527
I0305 20:43:16.734292 139870634698496 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.20553798973560333, loss=4.386979579925537
I0305 20:43:51.403589 139870626305792 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.20340824127197266, loss=4.248300075531006
I0305 20:44:26.055945 139870634698496 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.18998384475708008, loss=4.26905632019043
I0305 20:45:00.692071 139870626305792 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.19503258168697357, loss=4.3080525398254395
I0305 20:45:35.288350 139870634698496 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.187785804271698, loss=4.312310218811035
I0305 20:46:09.900414 139870626305792 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.20058099925518036, loss=4.2598652839660645
I0305 20:46:44.504673 139870634698496 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.19572827219963074, loss=4.305285453796387
I0305 20:47:19.134810 139870626305792 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.18996243178844452, loss=4.313381195068359
I0305 20:47:53.750746 139870634698496 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.20868900418281555, loss=4.284556865692139
I0305 20:48:28.384836 139870626305792 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.17231270670890808, loss=4.253236770629883
I0305 20:49:03.049073 139870634698496 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.17901372909545898, loss=4.260609149932861
I0305 20:49:37.687435 139870626305792 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.19968923926353455, loss=4.255857944488525
I0305 20:50:12.331835 139870634698496 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.2013246864080429, loss=4.3134636878967285
I0305 20:50:45.929483 140014092387520 spec.py:321] Evaluating on the training split.
I0305 20:50:48.532606 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:53:36.320178 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 20:53:38.919926 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:56:19.809556 140014092387520 spec.py:349] Evaluating on the test split.
I0305 20:56:22.406147 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 20:58:52.929126 140014092387520 submission_runner.py:469] Time since start: 6324.69s, 	Step: 9698, 	{'train/accuracy': 0.6061737537384033, 'train/loss': 2.2678310871124268, 'train/bleu': 28.560014196608766, 'validation/accuracy': 0.6206215023994446, 'validation/loss': 2.154343843460083, 'validation/bleu': 25.446157752592445, 'validation/num_examples': 3000, 'test/accuracy': 0.6274127960205078, 'test/loss': 2.1260712146759033, 'test/bleu': 24.651664319412692, 'test/num_examples': 3003, 'score': 3385.091551542282, 'total_duration': 6324.688243150711, 'accumulated_submission_time': 3385.091551542282, 'accumulated_eval_time': 2938.954449892044, 'accumulated_logging_time': 0.0688619613647461}
I0305 20:58:52.937974 139870626305792 logging_writer.py:48] [9698] accumulated_eval_time=2938.95, accumulated_logging_time=0.068862, accumulated_submission_time=3385.09, global_step=9698, preemption_count=0, score=3385.09, test/accuracy=0.627413, test/bleu=24.6517, test/loss=2.12607, test/num_examples=3003, total_duration=6324.69, train/accuracy=0.606174, train/bleu=28.56, train/loss=2.26783, validation/accuracy=0.620622, validation/bleu=25.4462, validation/loss=2.15434, validation/num_examples=3000
I0305 20:58:53.992868 139870634698496 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.17397956550121307, loss=4.228569507598877
I0305 20:59:28.440093 139870626305792 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.18019768595695496, loss=4.218701362609863
I0305 21:00:02.960197 139870634698496 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.17594555020332336, loss=4.187819480895996
I0305 21:00:37.575311 139870626305792 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.19134163856506348, loss=4.293753147125244
I0305 21:01:12.215859 139870634698496 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.22180376946926117, loss=4.245710372924805
I0305 21:01:46.889322 139870626305792 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.1694408357143402, loss=4.227695941925049
I0305 21:02:21.544624 139870634698496 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.1934131234884262, loss=4.238493919372559
I0305 21:02:56.169976 139870626305792 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17600564658641815, loss=4.265903472900391
I0305 21:03:30.845456 139870634698496 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.17668481171131134, loss=4.295577526092529
I0305 21:04:05.487088 139870626305792 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.18484711647033691, loss=4.239630699157715
I0305 21:04:40.100817 139870634698496 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.19614683091640472, loss=4.223087310791016
I0305 21:05:14.718326 139870626305792 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1839599758386612, loss=4.217057704925537
I0305 21:05:49.323547 139870634698496 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.17623202502727509, loss=4.199873447418213
I0305 21:06:23.916819 139870626305792 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.1734427809715271, loss=4.223470687866211
I0305 21:06:58.513233 139870634698496 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.17210200428962708, loss=4.240353584289551
I0305 21:07:33.107130 139870626305792 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.2512625455856323, loss=4.231551170349121
I0305 21:08:07.721277 139870634698496 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.17105340957641602, loss=4.252542018890381
I0305 21:08:42.316393 139870626305792 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.173940509557724, loss=4.2018232345581055
I0305 21:09:16.967200 139870634698496 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17455843091011047, loss=4.216590404510498
I0305 21:09:51.644126 139870626305792 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.17438608407974243, loss=4.132931709289551
I0305 21:10:26.276622 139870634698496 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.16593672335147858, loss=4.172478199005127
I0305 21:11:00.922212 139870626305792 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.207800954580307, loss=4.176637172698975
I0305 21:11:35.548863 139870634698496 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.18530721962451935, loss=4.185762882232666
I0305 21:12:10.160453 139870626305792 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2201157659292221, loss=4.131905555725098
I0305 21:12:44.790053 139870634698496 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.16415773332118988, loss=4.154907703399658
I0305 21:12:53.083247 140014092387520 spec.py:321] Evaluating on the training split.
I0305 21:12:55.684369 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 21:15:50.936473 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 21:15:53.527752 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 21:18:34.854412 140014092387520 spec.py:349] Evaluating on the test split.
I0305 21:18:37.446385 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 21:21:03.967222 140014092387520 submission_runner.py:469] Time since start: 7655.73s, 	Step: 12125, 	{'train/accuracy': 0.615370512008667, 'train/loss': 2.1761953830718994, 'train/bleu': 29.589751985078458, 'validation/accuracy': 0.6325488686561584, 'validation/loss': 2.038947105407715, 'validation/bleu': 26.258069877797816, 'validation/num_examples': 3000, 'test/accuracy': 0.6422083377838135, 'test/loss': 1.9878355264663696, 'test/bleu': 25.681203334622175, 'test/num_examples': 3003, 'score': 4225.092828273773, 'total_duration': 7655.726309537888, 'accumulated_submission_time': 4225.092828273773, 'accumulated_eval_time': 3429.8383464813232, 'accumulated_logging_time': 0.08607339859008789}
I0305 21:21:03.976686 139870626305792 logging_writer.py:48] [12125] accumulated_eval_time=3429.84, accumulated_logging_time=0.0860734, accumulated_submission_time=4225.09, global_step=12125, preemption_count=0, score=4225.09, test/accuracy=0.642208, test/bleu=25.6812, test/loss=1.98784, test/num_examples=3003, total_duration=7655.73, train/accuracy=0.615371, train/bleu=29.5898, train/loss=2.1762, validation/accuracy=0.632549, validation/bleu=26.2581, validation/loss=2.03895, validation/num_examples=3000
I0305 21:21:30.209536 139870634698496 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.18159984052181244, loss=4.1844563484191895
I0305 21:22:04.740681 139870626305792 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.16311883926391602, loss=4.140347957611084
I0305 21:22:39.353915 139870634698496 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.1687277853488922, loss=4.1694560050964355
I0305 21:23:14.001848 139870626305792 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.16111963987350464, loss=4.220177173614502
I0305 21:23:48.606767 139870634698496 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.16835184395313263, loss=4.1904072761535645
I0305 21:24:23.232631 139870626305792 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.18681497871875763, loss=4.243283748626709
I0305 21:24:57.883686 139870634698496 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1890108734369278, loss=4.116302013397217
I0305 21:25:32.515059 139870626305792 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.17824454605579376, loss=4.124897480010986
I0305 21:26:07.158993 139870634698496 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.19620424509048462, loss=4.183093547821045
I0305 21:26:41.768385 139870626305792 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.1614433079957962, loss=4.158359527587891
I0305 21:27:16.389958 139870634698496 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.16870979964733124, loss=4.1500983238220215
I0305 21:27:50.993573 139870626305792 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.17044003307819366, loss=4.145582675933838
I0305 21:28:25.587639 139870634698496 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1630990356206894, loss=4.149837017059326
I0305 21:29:00.188049 139870626305792 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.22319909930229187, loss=4.180359840393066
I0305 21:29:34.791138 139870634698496 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.18087007105350494, loss=4.144742488861084
I0305 21:30:09.408822 139870626305792 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.1651376336812973, loss=4.078160285949707
I0305 21:30:44.046050 139870634698496 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.20265617966651917, loss=4.159217834472656
I0305 21:31:18.665462 139870626305792 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.15579073131084442, loss=4.117227077484131
I0305 21:31:53.283724 139870634698496 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.18000464141368866, loss=4.133530139923096
I0305 21:32:27.892998 139870626305792 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.15622933208942413, loss=4.103120803833008
I0305 21:33:02.511143 139870634698496 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.17668721079826355, loss=4.132108211517334
I0305 21:33:37.118625 139870626305792 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.14968112111091614, loss=4.110350608825684
I0305 21:34:11.755502 139870634698496 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.14477473497390747, loss=4.11750602722168
I0305 21:34:46.350570 139870626305792 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.16847580671310425, loss=4.117565631866455
I0305 21:35:04.050810 140014092387520 spec.py:321] Evaluating on the training split.
I0305 21:35:06.653553 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 21:37:48.058186 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 21:37:50.649020 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 21:40:31.213063 140014092387520 spec.py:349] Evaluating on the test split.
I0305 21:40:33.802853 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 21:43:00.228512 140014092387520 submission_runner.py:469] Time since start: 8971.99s, 	Step: 14552, 	{'train/accuracy': 0.6277811527252197, 'train/loss': 2.080852746963501, 'train/bleu': 30.269983873014414, 'validation/accuracy': 0.64398193359375, 'validation/loss': 1.9654031991958618, 'validation/bleu': 27.25049033763671, 'validation/num_examples': 3000, 'test/accuracy': 0.6517437100410461, 'test/loss': 1.916274905204773, 'test/bleu': 26.19114597247215, 'test/num_examples': 3003, 'score': 5065.029271602631, 'total_duration': 8971.987629652023, 'accumulated_submission_time': 5065.029271602631, 'accumulated_eval_time': 3906.0160126686096, 'accumulated_logging_time': 0.10339474678039551}
I0305 21:43:00.237671 139870634698496 logging_writer.py:48] [14552] accumulated_eval_time=3906.02, accumulated_logging_time=0.103395, accumulated_submission_time=5065.03, global_step=14552, preemption_count=0, score=5065.03, test/accuracy=0.651744, test/bleu=26.1911, test/loss=1.91627, test/num_examples=3003, total_duration=8971.99, train/accuracy=0.627781, train/bleu=30.27, train/loss=2.08085, validation/accuracy=0.643982, validation/bleu=27.2505, validation/loss=1.9654, validation/num_examples=3000
I0305 21:43:17.132823 139870626305792 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.19318857789039612, loss=4.1075119972229
I0305 21:43:51.665520 139870634698496 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.18816369771957397, loss=4.149539947509766
I0305 21:44:26.321865 139870626305792 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.17569056153297424, loss=4.1028313636779785
I0305 21:45:00.970968 139870634698496 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.18213799595832825, loss=4.1206231117248535
I0305 21:45:35.605581 139870626305792 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.1750897318124771, loss=4.08360481262207
I0305 21:46:10.261364 139870634698496 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.15388281643390656, loss=4.030141830444336
I0305 21:46:44.911260 139870626305792 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2508524954319, loss=4.0729780197143555
I0305 21:47:19.565131 139870634698496 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.16896986961364746, loss=4.062570571899414
I0305 21:47:54.212016 139870626305792 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.15219217538833618, loss=4.185762405395508
I0305 21:48:28.858945 139870634698496 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.16768410801887512, loss=4.200562000274658
I0305 21:49:03.435557 139870626305792 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.16854369640350342, loss=4.159080505371094
I0305 21:49:38.029262 139870634698496 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.15780062973499298, loss=4.121036052703857
I0305 21:50:12.633213 139870626305792 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.16791193187236786, loss=4.070505142211914
I0305 21:50:47.259298 139870634698496 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.18887336552143097, loss=4.051302909851074
I0305 21:51:21.866785 139870626305792 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.15089140832424164, loss=4.057375907897949
I0305 21:51:56.458001 139870634698496 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.16468092799186707, loss=4.102990627288818
I0305 21:52:31.102262 139870626305792 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.16549627482891083, loss=4.043540000915527
I0305 21:53:05.739070 139870634698496 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.16555112600326538, loss=4.059535980224609
I0305 21:53:40.366160 139870626305792 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.20709256827831268, loss=4.127621173858643
I0305 21:54:15.015600 139870634698496 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.15691615641117096, loss=4.036920070648193
I0305 21:54:49.650814 139870626305792 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.16286256909370422, loss=4.036446571350098
I0305 21:55:24.280687 139870634698496 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.15841645002365112, loss=4.068545818328857
I0305 21:55:58.898612 139870626305792 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.16915802657604218, loss=4.181685447692871
I0305 21:56:33.546310 139870634698496 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.16190028190612793, loss=4.042600631713867
I0305 21:57:00.565692 140014092387520 spec.py:321] Evaluating on the training split.
I0305 21:57:03.160336 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:01:04.227512 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 22:01:06.826366 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:03:50.322719 140014092387520 spec.py:349] Evaluating on the test split.
I0305 22:03:52.910850 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:06:18.246170 140014092387520 submission_runner.py:469] Time since start: 10370.01s, 	Step: 16979, 	{'train/accuracy': 0.6309562921524048, 'train/loss': 2.0344173908233643, 'train/bleu': 30.237332038175808, 'validation/accuracy': 0.650545060634613, 'validation/loss': 1.9043089151382446, 'validation/bleu': 27.329813303394587, 'validation/num_examples': 3000, 'test/accuracy': 0.6597844958305359, 'test/loss': 1.8375310897827148, 'test/bleu': 27.03137742347074, 'test/num_examples': 3003, 'score': 5905.220097780228, 'total_duration': 10370.005269289017, 'accumulated_submission_time': 5905.220097780228, 'accumulated_eval_time': 4463.696427345276, 'accumulated_logging_time': 0.12131905555725098}
I0305 22:06:18.256039 139870626305792 logging_writer.py:48] [16979] accumulated_eval_time=4463.7, accumulated_logging_time=0.121319, accumulated_submission_time=5905.22, global_step=16979, preemption_count=0, score=5905.22, test/accuracy=0.659784, test/bleu=27.0314, test/loss=1.83753, test/num_examples=3003, total_duration=10370, train/accuracy=0.630956, train/bleu=30.2373, train/loss=2.03442, validation/accuracy=0.650545, validation/bleu=27.3298, validation/loss=1.90431, validation/num_examples=3000
I0305 22:06:25.832366 139870634698496 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.16308654844760895, loss=4.096601486206055
I0305 22:07:00.316941 139870626305792 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.15608254075050354, loss=4.054485321044922
I0305 22:07:34.850622 139870634698496 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.17744706571102142, loss=4.033941745758057
I0305 22:08:09.459535 139870626305792 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.155928835272789, loss=4.070122718811035
I0305 22:08:44.054295 139870634698496 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.15524226427078247, loss=4.02144193649292
I0305 22:09:18.670607 139870626305792 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.17654940485954285, loss=4.055810451507568
I0305 22:09:53.251456 139870634698496 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.16857759654521942, loss=4.095699787139893
I0305 22:10:27.864350 139870626305792 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.21109411120414734, loss=4.049412250518799
I0305 22:11:02.463012 139870634698496 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.17461954057216644, loss=4.0510149002075195
I0305 22:11:37.096163 139870626305792 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.1791638284921646, loss=4.070907115936279
I0305 22:12:11.707045 139870634698496 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.23494713008403778, loss=4.068624496459961
I0305 22:12:46.303863 139870626305792 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.20450443029403687, loss=4.001544952392578
I0305 22:13:20.914487 139870634698496 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.22802133858203888, loss=4.069954872131348
I0305 22:13:55.521919 139870626305792 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.16634202003479004, loss=4.028279781341553
I0305 22:14:30.138403 139870634698496 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.15808945894241333, loss=4.053517818450928
I0305 22:15:04.771037 139870626305792 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.16210711002349854, loss=4.016324520111084
I0305 22:15:39.373714 139870634698496 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1509598344564438, loss=4.055668354034424
I0305 22:16:14.016733 139870626305792 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.19882921874523163, loss=4.045555114746094
I0305 22:16:48.690054 139870634698496 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.17837539315223694, loss=4.000118255615234
I0305 22:17:23.449625 139870626305792 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.19194336235523224, loss=4.038488864898682
I0305 22:17:58.115156 139870634698496 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.16946503520011902, loss=4.019809246063232
I0305 22:18:32.802484 139870626305792 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.15977488458156586, loss=4.092731475830078
I0305 22:19:07.456672 139870634698496 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.23653754591941833, loss=4.0879950523376465
I0305 22:19:42.130035 139870626305792 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.20970329642295837, loss=3.9973948001861572
I0305 22:20:16.832026 139870634698496 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1601753830909729, loss=3.9875376224517822
I0305 22:20:18.571131 140014092387520 spec.py:321] Evaluating on the training split.
I0305 22:20:21.178193 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:23:15.127350 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 22:23:17.721640 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:26:03.597116 140014092387520 spec.py:349] Evaluating on the test split.
I0305 22:26:06.197769 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:28:32.216557 140014092387520 submission_runner.py:469] Time since start: 11703.98s, 	Step: 19406, 	{'train/accuracy': 0.6474028825759888, 'train/loss': 1.9145114421844482, 'train/bleu': 31.30485040851925, 'validation/accuracy': 0.6537092328071594, 'validation/loss': 1.868304967880249, 'validation/bleu': 27.904660301480273, 'validation/num_examples': 3000, 'test/accuracy': 0.6646391153335571, 'test/loss': 1.8058494329452515, 'test/bleu': 27.24284537224183, 'test/num_examples': 3003, 'score': 6745.396078109741, 'total_duration': 11703.975671052933, 'accumulated_submission_time': 6745.396078109741, 'accumulated_eval_time': 4957.341809988022, 'accumulated_logging_time': 0.1385819911956787}
I0305 22:28:32.227560 139870626305792 logging_writer.py:48] [19406] accumulated_eval_time=4957.34, accumulated_logging_time=0.138582, accumulated_submission_time=6745.4, global_step=19406, preemption_count=0, score=6745.4, test/accuracy=0.664639, test/bleu=27.2428, test/loss=1.80585, test/num_examples=3003, total_duration=11704, train/accuracy=0.647403, train/bleu=31.3049, train/loss=1.91451, validation/accuracy=0.653709, validation/bleu=27.9047, validation/loss=1.8683, validation/num_examples=3000
I0305 22:29:05.017961 139870634698496 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.18552005290985107, loss=4.0179443359375
I0305 22:29:39.600217 139870626305792 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.17407244443893433, loss=4.06030797958374
I0305 22:30:14.268484 139870634698496 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.19537299871444702, loss=4.024579048156738
I0305 22:30:48.949145 139870626305792 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.18145303428173065, loss=3.965097427368164
I0305 22:31:23.648400 139870634698496 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.16690246760845184, loss=4.075831413269043
I0305 22:31:58.330311 139870626305792 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.15914113819599152, loss=3.9940404891967773
I0305 22:32:33.000161 139870634698496 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.21333345770835876, loss=3.959031820297241
I0305 22:33:07.669258 139870626305792 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.16215768456459045, loss=4.0423173904418945
I0305 22:33:42.359053 139870634698496 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.19530151784420013, loss=4.0337371826171875
I0305 22:34:17.033139 139870626305792 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.21293506026268005, loss=4.0426530838012695
I0305 22:34:51.699131 139870634698496 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.1586596965789795, loss=4.0279669761657715
I0305 22:35:26.363547 139870626305792 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.20496763288974762, loss=3.992771625518799
I0305 22:36:00.993308 139870634698496 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.17845690250396729, loss=4.030459403991699
I0305 22:36:35.641480 139870626305792 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.18124929070472717, loss=4.056223392486572
I0305 22:37:10.300249 139870634698496 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.16456077992916107, loss=4.019196033477783
I0305 22:37:44.966812 139870626305792 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.15732227265834808, loss=3.976504325866699
I0305 22:38:19.650100 139870634698496 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.17411646246910095, loss=4.165410041809082
I0305 22:38:54.311810 139870626305792 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.17583099007606506, loss=4.038769245147705
I0305 22:39:28.959368 139870634698496 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.17670239508152008, loss=4.060346603393555
I0305 22:40:03.623986 139870626305792 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.1708274483680725, loss=4.00800085067749
I0305 22:40:38.297690 139870634698496 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.16200220584869385, loss=4.007479667663574
I0305 22:41:12.963414 139870626305792 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.24157115817070007, loss=4.004642963409424
I0305 22:41:47.654438 139870634698496 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.2495553344488144, loss=3.970649480819702
I0305 22:42:22.357863 139870626305792 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.31633517146110535, loss=3.955301284790039
I0305 22:42:32.425548 140014092387520 spec.py:321] Evaluating on the training split.
I0305 22:42:35.031260 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:45:31.935857 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 22:45:34.529307 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:48:15.787422 140014092387520 spec.py:349] Evaluating on the test split.
I0305 22:48:18.385838 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 22:50:56.428962 140014092387520 submission_runner.py:469] Time since start: 13048.19s, 	Step: 21830, 	{'train/accuracy': 0.6396872401237488, 'train/loss': 1.979859709739685, 'train/bleu': 31.333566441154108, 'validation/accuracy': 0.6577138900756836, 'validation/loss': 1.8474589586257935, 'validation/bleu': 27.84406544527811, 'validation/num_examples': 3000, 'test/accuracy': 0.6688332557678223, 'test/loss': 1.7815340757369995, 'test/bleu': 27.97795885713411, 'test/num_examples': 3003, 'score': 7585.456572294235, 'total_duration': 13048.188078641891, 'accumulated_submission_time': 7585.456572294235, 'accumulated_eval_time': 5461.345185756683, 'accumulated_logging_time': 0.15760207176208496}
I0305 22:50:56.439339 139870634698496 logging_writer.py:48] [21830] accumulated_eval_time=5461.35, accumulated_logging_time=0.157602, accumulated_submission_time=7585.46, global_step=21830, preemption_count=0, score=7585.46, test/accuracy=0.668833, test/bleu=27.978, test/loss=1.78153, test/num_examples=3003, total_duration=13048.2, train/accuracy=0.639687, train/bleu=31.3336, train/loss=1.97986, validation/accuracy=0.657714, validation/bleu=27.8441, validation/loss=1.84746, validation/num_examples=3000
I0305 22:51:20.993380 139870626305792 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.17251157760620117, loss=4.0345611572265625
I0305 22:51:55.555717 139870634698496 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2124042958021164, loss=3.961486577987671
I0305 22:52:30.215022 139870626305792 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.23583318293094635, loss=4.032798767089844
I0305 22:53:04.859256 139870634698496 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.1608050912618637, loss=3.9649155139923096
I0305 22:53:39.540815 139870626305792 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.24956631660461426, loss=4.002753257751465
I0305 22:54:14.179528 139870634698496 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.18440712988376617, loss=4.017165184020996
I0305 22:54:48.809220 139870626305792 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.17792059481143951, loss=3.936269760131836
I0305 22:55:23.475002 139870634698496 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.1668292135000229, loss=4.016343593597412
I0305 22:55:58.069381 139870626305792 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.3143880367279053, loss=4.042222023010254
I0305 22:56:32.701757 139870634698496 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.1977020502090454, loss=3.9763855934143066
I0305 22:57:07.367160 139870626305792 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.20189736783504486, loss=4.001976013183594
I0305 22:57:42.035274 139870634698496 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.2586018145084381, loss=3.9661548137664795
I0305 22:58:16.706104 139870626305792 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.18202148377895355, loss=4.026866436004639
I0305 22:58:51.373409 139870634698496 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.1719844490289688, loss=3.9960763454437256
I0305 22:59:26.036998 139870626305792 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.2014082819223404, loss=3.971552848815918
I0305 23:00:00.791010 139870634698496 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.19450755417346954, loss=3.9705941677093506
I0305 23:00:35.457438 139870626305792 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.20014335215091705, loss=3.9863834381103516
I0305 23:01:10.099641 139870634698496 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.18100762367248535, loss=3.9174978733062744
I0305 23:01:44.766140 139870626305792 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.21741846203804016, loss=4.026398658752441
I0305 23:02:19.457150 139870634698496 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.1758580207824707, loss=3.9756643772125244
I0305 23:02:54.130019 139870626305792 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.19808202981948853, loss=4.019111633300781
I0305 23:03:28.799769 139870634698496 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.18123634159564972, loss=3.960118293762207
I0305 23:04:03.427876 139870626305792 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.16651633381843567, loss=3.994957208633423
I0305 23:04:38.057492 139870634698496 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.1761551797389984, loss=3.92020583152771
I0305 23:04:56.772144 140014092387520 spec.py:321] Evaluating on the training split.
I0305 23:04:59.377899 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:08:36.657728 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 23:08:39.256145 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:11:27.332264 140014092387520 spec.py:349] Evaluating on the test split.
I0305 23:11:29.919504 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:14:03.266590 140014092387520 submission_runner.py:469] Time since start: 14435.03s, 	Step: 24255, 	{'train/accuracy': 0.6438353061676025, 'train/loss': 1.9500995874404907, 'train/bleu': 31.284471929812018, 'validation/accuracy': 0.6619657278060913, 'validation/loss': 1.807058572769165, 'validation/bleu': 28.629246201900123, 'validation/num_examples': 3000, 'test/accuracy': 0.6737689971923828, 'test/loss': 1.7389591932296753, 'test/bleu': 27.779371335703487, 'test/num_examples': 3003, 'score': 8425.654890537262, 'total_duration': 14435.025680303574, 'accumulated_submission_time': 8425.654890537262, 'accumulated_eval_time': 6007.83956694603, 'accumulated_logging_time': 0.1757657527923584}
I0305 23:14:03.277583 139870626305792 logging_writer.py:48] [24255] accumulated_eval_time=6007.84, accumulated_logging_time=0.175766, accumulated_submission_time=8425.65, global_step=24255, preemption_count=0, score=8425.65, test/accuracy=0.673769, test/bleu=27.7794, test/loss=1.73896, test/num_examples=3003, total_duration=14435, train/accuracy=0.643835, train/bleu=31.2845, train/loss=1.9501, validation/accuracy=0.661966, validation/bleu=28.6292, validation/loss=1.80706, validation/num_examples=3000
I0305 23:14:19.159963 139870634698496 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2264835238456726, loss=4.0364990234375
I0305 23:14:53.702471 139870626305792 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.17683516442775726, loss=4.028824806213379
I0305 23:15:28.285647 139870634698496 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.1971484124660492, loss=3.9268548488616943
I0305 23:16:02.918988 139870626305792 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.23969633877277374, loss=3.9955689907073975
I0305 23:16:37.548191 139870634698496 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.21167436242103577, loss=3.97444224357605
I0305 23:17:12.176908 139870626305792 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.24221520125865936, loss=3.9685347080230713
I0305 23:17:46.815637 139870634698496 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.20564861595630646, loss=3.9510574340820312
I0305 23:18:21.509285 139870626305792 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.16658742725849152, loss=3.9275448322296143
I0305 23:18:56.201881 139870634698496 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.17564663290977478, loss=4.034000873565674
I0305 23:19:30.953149 139870626305792 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.19066287577152252, loss=3.931931257247925
I0305 23:20:05.679099 139870634698496 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.25742432475090027, loss=3.9786112308502197
I0305 23:20:40.431991 139870626305792 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.23607933521270752, loss=4.069587230682373
I0305 23:21:15.198761 139870634698496 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.1843007653951645, loss=4.007557392120361
I0305 23:21:49.957666 139870626305792 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.19483865797519684, loss=3.9497807025909424
I0305 23:22:24.696486 139870634698496 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.19221290946006775, loss=3.9552295207977295
I0305 23:22:59.436173 139870626305792 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.18475978076457977, loss=3.9522249698638916
I0305 23:23:34.186148 139870634698496 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.18317779898643494, loss=3.9193596839904785
I0305 23:24:08.965528 139870626305792 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2308448702096939, loss=4.05164098739624
I0305 23:24:43.741635 139870634698496 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.26955193281173706, loss=3.9133641719818115
I0305 23:25:18.527129 139870626305792 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.20207129418849945, loss=3.9734487533569336
I0305 23:25:53.261791 139870634698496 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.19177545607089996, loss=3.932253360748291
I0305 23:26:28.036003 139870626305792 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.22177916765213013, loss=4.00131368637085
I0305 23:27:02.799531 139870634698496 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.2268866002559662, loss=3.9367923736572266
I0305 23:27:37.567095 139870626305792 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.1925600916147232, loss=3.924023389816284
I0305 23:28:03.300999 140014092387520 spec.py:321] Evaluating on the training split.
I0305 23:28:05.914498 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:30:56.890896 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 23:30:59.488355 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:33:44.781618 140014092387520 spec.py:349] Evaluating on the test split.
I0305 23:33:47.381474 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:36:17.243611 140014092387520 submission_runner.py:469] Time since start: 15769.00s, 	Step: 26675, 	{'train/accuracy': 0.6491510272026062, 'train/loss': 1.903560996055603, 'train/bleu': 32.3970726365744, 'validation/accuracy': 0.6647220253944397, 'validation/loss': 1.8025007247924805, 'validation/bleu': 28.72615130128667, 'validation/num_examples': 3000, 'test/accuracy': 0.6767929792404175, 'test/loss': 1.7335381507873535, 'test/bleu': 28.323045010307812, 'test/num_examples': 3003, 'score': 9265.544337034225, 'total_duration': 15769.00270652771, 'accumulated_submission_time': 9265.544337034225, 'accumulated_eval_time': 6501.78210735321, 'accumulated_logging_time': 0.19534540176391602}
I0305 23:36:17.257496 139870634698496 logging_writer.py:48] [26675] accumulated_eval_time=6501.78, accumulated_logging_time=0.195345, accumulated_submission_time=9265.54, global_step=26675, preemption_count=0, score=9265.54, test/accuracy=0.676793, test/bleu=28.323, test/loss=1.73354, test/num_examples=3003, total_duration=15769, train/accuracy=0.649151, train/bleu=32.3971, train/loss=1.90356, validation/accuracy=0.664722, validation/bleu=28.7262, validation/loss=1.8025, validation/num_examples=3000
I0305 23:36:26.253659 139870626305792 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.20342351496219635, loss=3.963592290878296
I0305 23:37:00.884705 139870634698496 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.1845691204071045, loss=4.033663749694824
I0305 23:37:35.609152 139870626305792 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.1728619784116745, loss=3.961066961288452
I0305 23:38:10.353753 139870634698496 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.19812165200710297, loss=3.9989519119262695
I0305 23:38:45.102103 139870626305792 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.19131778180599213, loss=3.930983304977417
I0305 23:39:19.841032 139870634698496 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.17194589972496033, loss=3.905080556869507
I0305 23:39:54.586742 139870626305792 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.19798272848129272, loss=3.9468793869018555
I0305 23:40:29.331668 139870634698496 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.21574212610721588, loss=3.989790916442871
I0305 23:41:04.112476 139870626305792 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.22391915321350098, loss=3.965794086456299
I0305 23:41:38.866353 139870634698496 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.24906732141971588, loss=3.9405946731567383
I0305 23:42:13.643543 139870626305792 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.17496149241924286, loss=3.9549055099487305
I0305 23:42:48.428508 139870634698496 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.21241481602191925, loss=3.9593515396118164
I0305 23:43:23.197494 139870626305792 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.19320109486579895, loss=3.979701280593872
I0305 23:43:57.916596 139870634698496 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.2476862221956253, loss=3.9500224590301514
I0305 23:44:32.705676 139870626305792 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.1950182318687439, loss=3.9321417808532715
I0305 23:45:07.444918 139870634698496 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.2575743496417999, loss=3.9320976734161377
I0305 23:45:42.198048 139870626305792 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.2083098292350769, loss=3.9931273460388184
I0305 23:46:17.005795 139870634698496 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.19476096332073212, loss=3.9520390033721924
I0305 23:46:51.739073 139870626305792 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.22633425891399384, loss=4.028348445892334
I0305 23:47:26.518692 139870634698496 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.19369946420192719, loss=3.903196334838867
I0305 23:48:01.275359 139870626305792 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.20458540320396423, loss=3.9468674659729004
I0305 23:48:36.044333 139870634698496 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.29019895195961, loss=3.986931085586548
I0305 23:49:10.831535 139870626305792 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.19032764434814453, loss=3.9686920642852783
I0305 23:49:45.618227 139870634698496 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.21389304101467133, loss=3.9329118728637695
I0305 23:50:17.255812 140014092387520 spec.py:321] Evaluating on the training split.
I0305 23:50:19.869828 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:53:24.183570 140014092387520 spec.py:333] Evaluating on the validation split.
I0305 23:53:26.763098 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:56:21.529469 140014092387520 spec.py:349] Evaluating on the test split.
I0305 23:56:24.121949 140014092387520 workload.py:181] Translating evaluation dataset.
I0305 23:58:46.196865 140014092387520 submission_runner.py:469] Time since start: 17117.96s, 	Step: 29092, 	{'train/accuracy': 0.6483597755432129, 'train/loss': 1.908276915550232, 'train/bleu': 31.264425413605853, 'validation/accuracy': 0.6656613945960999, 'validation/loss': 1.7797433137893677, 'validation/bleu': 28.77879315421725, 'validation/num_examples': 3000, 'test/accuracy': 0.6782643795013428, 'test/loss': 1.7161427736282349, 'test/bleu': 28.20757514270011, 'test/num_examples': 3003, 'score': 10105.405197381973, 'total_duration': 17117.95597720146, 'accumulated_submission_time': 10105.405197381973, 'accumulated_eval_time': 7010.723140716553, 'accumulated_logging_time': 0.21793007850646973}
I0305 23:58:46.209143 139870626305792 logging_writer.py:48] [29092] accumulated_eval_time=7010.72, accumulated_logging_time=0.21793, accumulated_submission_time=10105.4, global_step=29092, preemption_count=0, score=10105.4, test/accuracy=0.678264, test/bleu=28.2076, test/loss=1.71614, test/num_examples=3003, total_duration=17118, train/accuracy=0.64836, train/bleu=31.2644, train/loss=1.90828, validation/accuracy=0.665661, validation/bleu=28.7788, validation/loss=1.77974, validation/num_examples=3000
I0305 23:58:49.334981 139870634698496 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.26668426394462585, loss=4.017286777496338
I0305 23:59:24.006804 139870626305792 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.32552018761634827, loss=3.956016778945923
I0305 23:59:58.711774 139870634698496 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1948048174381256, loss=3.890444755554199
I0306 00:00:33.476564 139870626305792 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.2208862155675888, loss=3.969231367111206
I0306 00:01:08.211083 139870634698496 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1951647698879242, loss=3.964747428894043
I0306 00:01:42.977060 139870626305792 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.18648748099803925, loss=3.8800461292266846
I0306 00:02:17.738713 139870634698496 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.24189914762973785, loss=3.954420328140259
I0306 00:02:52.515861 139870626305792 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.23115476965904236, loss=3.927429676055908
I0306 00:03:27.266988 139870634698496 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.23601134121418, loss=3.9602949619293213
I0306 00:04:02.014006 139870626305792 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.23122581839561462, loss=3.923316717147827
I0306 00:04:36.773016 139870634698496 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.22169630229473114, loss=3.9464187622070312
I0306 00:05:11.544634 139870626305792 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.3610801696777344, loss=3.939875841140747
I0306 00:05:46.307337 139870634698496 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.20408610999584198, loss=3.9645628929138184
I0306 00:06:21.050203 139870626305792 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.21207883954048157, loss=3.9597835540771484
I0306 00:06:55.772930 139870634698496 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.2641552984714508, loss=3.946202516555786
I0306 00:07:30.526068 139870626305792 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.2672271132469177, loss=4.016602039337158
I0306 00:08:05.267635 139870634698496 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.2012406438589096, loss=3.979053258895874
I0306 00:08:40.019636 139870626305792 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.1924022138118744, loss=3.9828789234161377
I0306 00:09:14.756275 139870634698496 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.2002444863319397, loss=3.899057388305664
I0306 00:09:49.515578 139870626305792 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.20347504317760468, loss=3.9399986267089844
I0306 00:10:24.264453 139870634698496 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.19579435884952545, loss=3.9011378288269043
I0306 00:10:59.022805 139870626305792 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.19393646717071533, loss=3.9569389820098877
I0306 00:11:33.779976 139870634698496 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.1930047869682312, loss=3.917262077331543
I0306 00:12:08.519792 139870626305792 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2062683403491974, loss=3.9362409114837646
I0306 00:12:43.256580 139870634698496 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.2124524563550949, loss=3.9532532691955566
I0306 00:12:46.399071 140014092387520 spec.py:321] Evaluating on the training split.
I0306 00:12:49.007393 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 00:17:34.932752 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 00:17:37.537541 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 00:21:41.981354 140014092387520 spec.py:349] Evaluating on the test split.
I0306 00:21:44.567950 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 00:25:39.576928 140014092387520 submission_runner.py:469] Time since start: 18731.34s, 	Step: 31510, 	{'train/accuracy': 0.6804373264312744, 'train/loss': 1.6987824440002441, 'train/bleu': 33.88130247241901, 'validation/accuracy': 0.6667367219924927, 'validation/loss': 1.7544461488723755, 'validation/bleu': 27.999507568288657, 'validation/num_examples': 3000, 'test/accuracy': 0.6793766617774963, 'test/loss': 1.6823879480361938, 'test/bleu': 28.388578945482298, 'test/num_examples': 3003, 'score': 10945.45983839035, 'total_duration': 18731.336015939713, 'accumulated_submission_time': 10945.45983839035, 'accumulated_eval_time': 7783.900926828384, 'accumulated_logging_time': 0.2381587028503418}
I0306 00:25:39.589507 139870626305792 logging_writer.py:48] [31510] accumulated_eval_time=7783.9, accumulated_logging_time=0.238159, accumulated_submission_time=10945.5, global_step=31510, preemption_count=0, score=10945.5, test/accuracy=0.679377, test/bleu=28.3886, test/loss=1.68239, test/num_examples=3003, total_duration=18731.3, train/accuracy=0.680437, train/bleu=33.8813, train/loss=1.69878, validation/accuracy=0.666737, validation/bleu=27.9995, validation/loss=1.75445, validation/num_examples=3000
I0306 00:26:11.072235 139870634698496 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.2775431275367737, loss=3.9216341972351074
I0306 00:26:45.727345 139870626305792 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.30777695775032043, loss=3.9173266887664795
I0306 00:27:20.445850 139870634698496 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.20040592551231384, loss=3.945441246032715
I0306 00:27:55.165664 139870626305792 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.18904145061969757, loss=3.9003677368164062
I0306 00:28:29.901793 139870634698496 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.19625814259052277, loss=3.9520623683929443
I0306 00:29:04.704896 139870626305792 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2106921225786209, loss=3.9730441570281982
I0306 00:29:39.421079 139870634698496 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.21105137467384338, loss=3.965914487838745
I0306 00:30:14.189067 139870626305792 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.23071719706058502, loss=3.978541851043701
I0306 00:30:48.961537 139870634698496 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2579449415206909, loss=3.9269356727600098
I0306 00:31:23.782438 139870626305792 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.20110441744327545, loss=3.9174861907958984
I0306 00:31:58.544960 139870634698496 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.27113664150238037, loss=3.940629720687866
I0306 00:32:33.283182 139870626305792 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.2447991818189621, loss=3.8854103088378906
I0306 00:33:08.021691 139870634698496 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.22950366139411926, loss=3.94934344291687
I0306 00:33:42.748333 139870626305792 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.304945707321167, loss=3.921647310256958
I0306 00:34:17.503537 139870634698496 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.23428232967853546, loss=3.914222478866577
I0306 00:34:52.229674 139870626305792 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.21852236986160278, loss=3.9301254749298096
I0306 00:35:26.969220 139870634698496 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.24784961342811584, loss=3.8905465602874756
I0306 00:36:01.696851 139870626305792 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.2239653319120407, loss=3.955230951309204
I0306 00:36:36.461441 139870634698496 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.21943847835063934, loss=3.835918664932251
I0306 00:37:11.167351 139870626305792 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.24210506677627563, loss=3.9403460025787354
I0306 00:37:45.933577 139870634698496 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.24971555173397064, loss=4.011336326599121
I0306 00:38:20.675886 139870626305792 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.207301065325737, loss=3.902444839477539
I0306 00:38:55.386852 139870634698496 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.197674959897995, loss=3.9091198444366455
I0306 00:39:30.087898 139870626305792 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.21081383526325226, loss=3.901698112487793
I0306 00:39:39.835917 140014092387520 spec.py:321] Evaluating on the training split.
I0306 00:39:42.440676 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 00:42:55.397391 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 00:42:57.984501 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 00:45:27.984029 140014092387520 spec.py:349] Evaluating on the test split.
I0306 00:45:30.570606 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 00:47:57.631206 140014092387520 submission_runner.py:469] Time since start: 20069.39s, 	Step: 33929, 	{'train/accuracy': 0.6599628925323486, 'train/loss': 1.8271431922912598, 'train/bleu': 32.339138907329854, 'validation/accuracy': 0.6692705154418945, 'validation/loss': 1.7412551641464233, 'validation/bleu': 28.76290182737091, 'validation/num_examples': 3000, 'test/accuracy': 0.6825280785560608, 'test/loss': 1.6659889221191406, 'test/bleu': 28.41589675117528, 'test/num_examples': 3003, 'score': 11785.570471286774, 'total_duration': 20069.390327453613, 'accumulated_submission_time': 11785.570471286774, 'accumulated_eval_time': 8281.696171045303, 'accumulated_logging_time': 0.2600553035736084}
I0306 00:47:57.642423 139870634698496 logging_writer.py:48] [33929] accumulated_eval_time=8281.7, accumulated_logging_time=0.260055, accumulated_submission_time=11785.6, global_step=33929, preemption_count=0, score=11785.6, test/accuracy=0.682528, test/bleu=28.4159, test/loss=1.66599, test/num_examples=3003, total_duration=20069.4, train/accuracy=0.659963, train/bleu=32.3391, train/loss=1.82714, validation/accuracy=0.669271, validation/bleu=28.7629, validation/loss=1.74126, validation/num_examples=3000
I0306 00:48:22.444884 139870626305792 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.21565476059913635, loss=3.9107561111450195
I0306 00:48:57.009150 139870634698496 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.21892410516738892, loss=3.897881507873535
I0306 00:49:31.602403 139870626305792 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.2200973778963089, loss=3.962345838546753
I0306 00:50:06.234281 139870634698496 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.29867419600486755, loss=3.9238688945770264
I0306 00:50:40.835156 139870626305792 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.21977433562278748, loss=3.9358315467834473
I0306 00:51:15.444977 139870634698496 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.20227199792861938, loss=3.871978759765625
I0306 00:51:50.095360 139870626305792 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2629275619983673, loss=3.945136547088623
I0306 00:52:24.740245 139870634698496 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.29347413778305054, loss=3.895848274230957
I0306 00:52:59.369674 139870626305792 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.20955008268356323, loss=3.924574375152588
I0306 00:53:34.007297 139870634698496 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.20832228660583496, loss=3.849151134490967
I0306 00:54:08.652513 139870626305792 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.24138601124286652, loss=3.915851593017578
I0306 00:54:43.297530 139870634698496 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2228396087884903, loss=3.8687081336975098
I0306 00:55:17.936251 139870626305792 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.21126636862754822, loss=3.953794240951538
I0306 00:55:52.561602 139870634698496 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.2747126519680023, loss=3.9467570781707764
I0306 00:56:27.202092 139870626305792 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.23893670737743378, loss=3.9221692085266113
I0306 00:57:01.896005 139870634698496 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2672048509120941, loss=3.8625731468200684
I0306 00:57:36.506748 139870626305792 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.25812193751335144, loss=3.929324150085449
I0306 00:58:11.103019 139870634698496 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.19994685053825378, loss=3.907989501953125
I0306 00:58:45.739697 139870626305792 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.2960784137248993, loss=3.928112030029297
I0306 00:59:20.348485 139870634698496 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.2604769468307495, loss=3.9296481609344482
I0306 00:59:54.958195 139870626305792 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.19775453209877014, loss=3.9054462909698486
I0306 01:00:29.582924 139870634698496 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.2070343792438507, loss=3.8579459190368652
I0306 01:01:04.206994 139870626305792 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.29481014609336853, loss=3.9185996055603027
I0306 01:01:38.833765 139870634698496 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.20684458315372467, loss=3.9295434951782227
I0306 01:01:57.859762 140014092387520 spec.py:321] Evaluating on the training split.
I0306 01:02:00.464560 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:04:47.027943 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 01:04:49.616592 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:07:24.028311 140014092387520 spec.py:349] Evaluating on the test split.
I0306 01:07:26.618029 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:09:44.475857 140014092387520 submission_runner.py:469] Time since start: 21376.23s, 	Step: 36356, 	{'train/accuracy': 0.654316782951355, 'train/loss': 1.874359130859375, 'train/bleu': 32.11816353812008, 'validation/accuracy': 0.6731515526771545, 'validation/loss': 1.7405656576156616, 'validation/bleu': 29.162475833081356, 'validation/num_examples': 3000, 'test/accuracy': 0.6834086179733276, 'test/loss': 1.6702252626419067, 'test/bleu': 28.720744835952587, 'test/num_examples': 3003, 'score': 12625.65418624878, 'total_duration': 21376.234978199005, 'accumulated_submission_time': 12625.65418624878, 'accumulated_eval_time': 8748.312220096588, 'accumulated_logging_time': 0.2798900604248047}
I0306 01:09:44.487115 139870626305792 logging_writer.py:48] [36356] accumulated_eval_time=8748.31, accumulated_logging_time=0.27989, accumulated_submission_time=12625.7, global_step=36356, preemption_count=0, score=12625.7, test/accuracy=0.683409, test/bleu=28.7207, test/loss=1.67023, test/num_examples=3003, total_duration=21376.2, train/accuracy=0.654317, train/bleu=32.1182, train/loss=1.87436, validation/accuracy=0.673152, validation/bleu=29.1625, validation/loss=1.74057, validation/num_examples=3000
I0306 01:10:00.010937 139870634698496 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.24520714581012726, loss=3.8968396186828613
I0306 01:10:34.490563 139870626305792 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.22701828181743622, loss=3.890331745147705
I0306 01:11:09.155645 139870634698496 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.21717551350593567, loss=3.93131685256958
I0306 01:11:43.725968 139870626305792 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.2542954981327057, loss=3.8250811100006104
I0306 01:12:18.314321 139870634698496 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.24781771004199982, loss=3.904510736465454
I0306 01:12:52.901664 139870626305792 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2249973863363266, loss=3.8932807445526123
I0306 01:13:27.544288 139870634698496 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.22953084111213684, loss=3.92590069770813
I0306 01:14:02.186057 139870626305792 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.23709270358085632, loss=3.8564419746398926
I0306 01:14:36.796169 139870634698496 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2602293789386749, loss=3.945664405822754
I0306 01:15:11.413222 139870626305792 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.23242177069187164, loss=3.9360108375549316
I0306 01:15:46.035106 139870634698496 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.22346775233745575, loss=3.9757795333862305
I0306 01:16:20.664663 139870626305792 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.28106793761253357, loss=3.9771900177001953
I0306 01:16:55.282664 139870634698496 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2651129364967346, loss=3.9163880348205566
I0306 01:17:29.940953 139870626305792 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.25000572204589844, loss=3.888618230819702
I0306 01:18:04.568947 139870634698496 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.28506898880004883, loss=3.98244047164917
I0306 01:18:39.189927 139870626305792 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2675561010837555, loss=3.888242483139038
I0306 01:19:13.825247 139870634698496 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.23946473002433777, loss=3.8676676750183105
I0306 01:19:48.468973 139870626305792 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.2755581736564636, loss=3.887681007385254
I0306 01:20:23.077898 139870634698496 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.2391766607761383, loss=3.9593234062194824
I0306 01:20:57.669676 139870626305792 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.22859926521778107, loss=3.9213240146636963
I0306 01:21:32.284210 139870634698496 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.26721957325935364, loss=3.8720622062683105
I0306 01:22:06.972490 139870626305792 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2308003008365631, loss=3.9017813205718994
I0306 01:22:41.610197 139870634698496 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.26437821984291077, loss=3.9081103801727295
I0306 01:23:16.240469 139870626305792 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.21448786556720734, loss=3.8748719692230225
I0306 01:23:44.619314 140014092387520 spec.py:321] Evaluating on the training split.
I0306 01:23:47.219446 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:26:35.478610 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 01:26:38.063136 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:29:18.451173 140014092387520 spec.py:349] Evaluating on the test split.
I0306 01:29:21.031806 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:31:49.424911 140014092387520 submission_runner.py:469] Time since start: 22701.18s, 	Step: 38783, 	{'train/accuracy': 0.6658881902694702, 'train/loss': 1.7860950231552124, 'train/bleu': 32.37594239737388, 'validation/accuracy': 0.6726447939872742, 'validation/loss': 1.727084755897522, 'validation/bleu': 29.374485709907372, 'validation/num_examples': 3000, 'test/accuracy': 0.6844977736473083, 'test/loss': 1.6515417098999023, 'test/bleu': 28.777301678435343, 'test/num_examples': 3003, 'score': 13465.653621196747, 'total_duration': 22701.18399143219, 'accumulated_submission_time': 13465.653621196747, 'accumulated_eval_time': 9233.117734670639, 'accumulated_logging_time': 0.30044102668762207}
I0306 01:31:49.436678 139870634698496 logging_writer.py:48] [38783] accumulated_eval_time=9233.12, accumulated_logging_time=0.300441, accumulated_submission_time=13465.7, global_step=38783, preemption_count=0, score=13465.7, test/accuracy=0.684498, test/bleu=28.7773, test/loss=1.65154, test/num_examples=3003, total_duration=22701.2, train/accuracy=0.665888, train/bleu=32.3759, train/loss=1.7861, validation/accuracy=0.672645, validation/bleu=29.3745, validation/loss=1.72708, validation/num_examples=3000
I0306 01:31:55.641456 139870626305792 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.26912832260131836, loss=3.9275872707366943
I0306 01:32:30.138594 139870634698496 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.28588035702705383, loss=3.903748035430908
I0306 01:33:04.718848 139870626305792 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.32369738817214966, loss=3.925915241241455
I0306 01:33:39.347874 139870634698496 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.24124713242053986, loss=3.841081380844116
I0306 01:34:13.957226 139870626305792 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3034081757068634, loss=3.8666088581085205
I0306 01:34:48.522478 139870634698496 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.23083944618701935, loss=3.919250965118408
I0306 01:35:23.082029 139870626305792 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.28502073884010315, loss=3.8878438472747803
I0306 01:35:57.662416 139870634698496 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.21194203197956085, loss=3.8739511966705322
I0306 01:36:32.254335 139870626305792 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.20917432010173798, loss=3.950092315673828
I0306 01:37:06.870489 139870634698496 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.24162520468235016, loss=3.908590793609619
I0306 01:37:41.452006 139870626305792 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.2560851275920868, loss=3.8269834518432617
I0306 01:38:16.064014 139870634698496 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.24498167634010315, loss=3.8934857845306396
I0306 01:38:50.710110 139870626305792 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.2287779152393341, loss=3.955869436264038
I0306 01:39:25.369672 139870634698496 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.22189965844154358, loss=3.857933759689331
I0306 01:39:59.985970 139870626305792 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.22425426542758942, loss=3.890170097351074
I0306 01:40:34.635045 139870634698496 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.29258131980895996, loss=3.870274543762207
I0306 01:41:09.281380 139870626305792 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.22607767581939697, loss=3.915653705596924
I0306 01:41:43.892779 139870634698496 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.2339133620262146, loss=3.902365207672119
I0306 01:42:18.490120 139870626305792 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.2445525974035263, loss=3.9238648414611816
I0306 01:42:53.125592 139870634698496 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.24891874194145203, loss=3.9188618659973145
I0306 01:43:27.772817 139870626305792 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.23385268449783325, loss=3.874070882797241
I0306 01:44:02.374000 139870634698496 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.25212860107421875, loss=3.87900710105896
I0306 01:44:37.040387 139870626305792 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.30324122309684753, loss=3.8533811569213867
I0306 01:45:11.646932 139870634698496 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.2566964328289032, loss=3.89254093170166
I0306 01:45:46.274364 139870626305792 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.23568576574325562, loss=3.9049618244171143
I0306 01:45:49.758651 140014092387520 spec.py:321] Evaluating on the training split.
I0306 01:45:52.358922 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:49:01.932607 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 01:49:04.516077 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:51:40.210699 140014092387520 spec.py:349] Evaluating on the test split.
I0306 01:51:42.800564 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 01:54:24.764374 140014092387520 submission_runner.py:469] Time since start: 24056.52s, 	Step: 41211, 	{'train/accuracy': 0.6578485369682312, 'train/loss': 1.8377385139465332, 'train/bleu': 32.55934789110335, 'validation/accuracy': 0.6763651967048645, 'validation/loss': 1.7149626016616821, 'validation/bleu': 29.51203362539935, 'validation/num_examples': 3000, 'test/accuracy': 0.6887035369873047, 'test/loss': 1.6402912139892578, 'test/bleu': 29.26120509627895, 'test/num_examples': 3003, 'score': 14305.846911907196, 'total_duration': 24056.523476839066, 'accumulated_submission_time': 14305.846911907196, 'accumulated_eval_time': 9748.123394727707, 'accumulated_logging_time': 0.319899320602417}
I0306 01:54:24.776553 139870634698496 logging_writer.py:48] [41211] accumulated_eval_time=9748.12, accumulated_logging_time=0.319899, accumulated_submission_time=14305.8, global_step=41211, preemption_count=0, score=14305.8, test/accuracy=0.688704, test/bleu=29.2612, test/loss=1.64029, test/num_examples=3003, total_duration=24056.5, train/accuracy=0.657849, train/bleu=32.5593, train/loss=1.83774, validation/accuracy=0.676365, validation/bleu=29.512, validation/loss=1.71496, validation/num_examples=3000
I0306 01:54:55.811481 139870626305792 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.48519426584243774, loss=3.854257822036743
I0306 01:55:30.358341 139870634698496 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2231217920780182, loss=3.854599714279175
I0306 01:56:04.937398 139870626305792 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.24251578748226166, loss=3.860433340072632
I0306 01:56:39.591034 139870634698496 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.27074676752090454, loss=3.900956630706787
I0306 01:57:14.199466 139870626305792 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2348162680864334, loss=3.860193967819214
I0306 01:57:48.791809 139870634698496 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.2524658441543579, loss=3.8514018058776855
I0306 01:58:23.431486 139870626305792 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.22333809733390808, loss=3.8321683406829834
I0306 01:58:58.069413 139870634698496 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2367054522037506, loss=3.8790202140808105
I0306 01:59:32.730238 139870626305792 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2474288046360016, loss=3.7897796630859375
I0306 02:00:07.360201 139870634698496 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.2392730712890625, loss=3.9530932903289795
I0306 02:00:41.989751 139870626305792 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.2354920357465744, loss=3.9048001766204834
I0306 02:01:16.610333 139870634698496 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2466178834438324, loss=3.8894903659820557
I0306 02:01:51.183366 139870626305792 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.24833709001541138, loss=3.9190726280212402
I0306 02:02:25.832898 139870634698496 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.23836903274059296, loss=3.9206221103668213
I0306 02:03:00.496655 139870626305792 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.29376891255378723, loss=3.873908042907715
I0306 02:03:35.161603 139870634698496 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.26707109808921814, loss=3.8761377334594727
I0306 02:04:09.804720 139870626305792 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.22636112570762634, loss=3.883791208267212
I0306 02:04:44.491704 139870634698496 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.24627038836479187, loss=3.919492244720459
I0306 02:05:19.168886 139870626305792 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.23619933426380157, loss=3.881578207015991
I0306 02:05:53.819301 139870634698496 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.2994634509086609, loss=3.868661880493164
I0306 02:06:28.449647 139870626305792 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.2881500720977783, loss=3.8894221782684326
I0306 02:07:03.061331 139870634698496 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.22957028448581696, loss=3.888866424560547
I0306 02:07:37.751800 139870626305792 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.23187138140201569, loss=3.8815951347351074
I0306 02:08:12.390171 139870634698496 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.2593050003051758, loss=3.8991289138793945
I0306 02:08:24.857996 140014092387520 spec.py:321] Evaluating on the training split.
I0306 02:08:27.452615 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:11:19.714064 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 02:11:22.300017 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:13:58.194428 140014092387520 spec.py:349] Evaluating on the test split.
I0306 02:14:00.781583 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:16:52.878565 140014092387520 submission_runner.py:469] Time since start: 25404.64s, 	Step: 43637, 	{'train/accuracy': 0.657191812992096, 'train/loss': 1.8295434713363647, 'train/bleu': 32.315905550746095, 'validation/accuracy': 0.6761921644210815, 'validation/loss': 1.7070823907852173, 'validation/bleu': 29.413222057987216, 'validation/num_examples': 3000, 'test/accuracy': 0.6892828345298767, 'test/loss': 1.62874174118042, 'test/bleu': 29.144494163907872, 'test/num_examples': 3003, 'score': 15145.796455144882, 'total_duration': 25404.637673139572, 'accumulated_submission_time': 15145.796455144882, 'accumulated_eval_time': 10256.143904209137, 'accumulated_logging_time': 0.3395676612854004}
I0306 02:16:52.890429 139870626305792 logging_writer.py:48] [43637] accumulated_eval_time=10256.1, accumulated_logging_time=0.339568, accumulated_submission_time=15145.8, global_step=43637, preemption_count=0, score=15145.8, test/accuracy=0.689283, test/bleu=29.1445, test/loss=1.62874, test/num_examples=3003, total_duration=25404.6, train/accuracy=0.657192, train/bleu=32.3159, train/loss=1.82954, validation/accuracy=0.676192, validation/bleu=29.4132, validation/loss=1.70708, validation/num_examples=3000
I0306 02:17:14.948413 139870634698496 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.30318522453308105, loss=3.8599863052368164
I0306 02:17:49.431754 139870626305792 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.24240553379058838, loss=3.865307569503784
I0306 02:18:24.022787 139870634698496 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.24561764299869537, loss=3.8472516536712646
I0306 02:18:58.595597 139870626305792 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2328307032585144, loss=3.931267023086548
I0306 02:19:33.149629 139870634698496 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.22810062766075134, loss=3.8066442012786865
I0306 02:20:07.762667 139870626305792 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.23850607872009277, loss=3.9434659481048584
I0306 02:20:42.347017 139870634698496 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2350136935710907, loss=3.8416404724121094
I0306 02:21:16.948503 139870626305792 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.30495327711105347, loss=3.846458673477173
I0306 02:21:51.525373 139870634698496 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.25957757234573364, loss=3.957007884979248
I0306 02:22:26.097773 139870626305792 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2500153183937073, loss=3.8963236808776855
I0306 02:23:00.697441 139870634698496 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2465267777442932, loss=3.8653006553649902
I0306 02:23:35.272796 139870626305792 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.2736107409000397, loss=3.8988027572631836
I0306 02:24:09.857611 139870634698496 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.24477925896644592, loss=3.9168894290924072
I0306 02:24:44.443607 139870626305792 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.28841832280158997, loss=3.9183342456817627
I0306 02:25:19.007998 139870634698496 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.22536267340183258, loss=3.8749547004699707
I0306 02:25:53.579731 139870626305792 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.23432867228984833, loss=3.8528130054473877
I0306 02:26:28.137690 139870634698496 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.24432164430618286, loss=3.918779134750366
I0306 02:27:02.716427 139870626305792 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.24266688525676727, loss=3.916004180908203
I0306 02:27:37.280410 139870634698496 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2570144236087799, loss=3.858104705810547
I0306 02:28:11.859895 139870626305792 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.23974716663360596, loss=3.8909661769866943
I0306 02:28:46.472401 139870634698496 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.23894266784191132, loss=3.867654800415039
I0306 02:29:21.083391 139870626305792 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2536690831184387, loss=3.882455587387085
I0306 02:29:55.639660 139870634698496 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.23571553826332092, loss=3.866811752319336
I0306 02:30:30.233619 139870626305792 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.24636448919773102, loss=3.90366530418396
I0306 02:30:53.033015 140014092387520 spec.py:321] Evaluating on the training split.
I0306 02:30:55.633694 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:33:56.364778 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 02:33:58.942324 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:36:31.744897 140014092387520 spec.py:349] Evaluating on the test split.
I0306 02:36:34.322357 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:38:55.787304 140014092387520 submission_runner.py:469] Time since start: 26727.55s, 	Step: 46067, 	{'train/accuracy': 0.6660265922546387, 'train/loss': 1.7800300121307373, 'train/bleu': 33.32514152016943, 'validation/accuracy': 0.6766865253448486, 'validation/loss': 1.695261001586914, 'validation/bleu': 29.49073337458618, 'validation/num_examples': 3000, 'test/accuracy': 0.6893175840377808, 'test/loss': 1.6159530878067017, 'test/bleu': 29.45351812516039, 'test/num_examples': 3003, 'score': 15985.809119939804, 'total_duration': 26727.546419620514, 'accumulated_submission_time': 15985.809119939804, 'accumulated_eval_time': 10738.898144721985, 'accumulated_logging_time': 0.35892677307128906}
I0306 02:38:55.800413 139870634698496 logging_writer.py:48] [46067] accumulated_eval_time=10738.9, accumulated_logging_time=0.358927, accumulated_submission_time=15985.8, global_step=46067, preemption_count=0, score=15985.8, test/accuracy=0.689318, test/bleu=29.4535, test/loss=1.61595, test/num_examples=3003, total_duration=26727.5, train/accuracy=0.666027, train/bleu=33.3251, train/loss=1.78003, validation/accuracy=0.676687, validation/bleu=29.4907, validation/loss=1.69526, validation/num_examples=3000
I0306 02:39:07.525060 139870626305792 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2549566924571991, loss=3.8411433696746826
I0306 02:39:41.985507 139870634698496 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2702001631259918, loss=3.9214000701904297
I0306 02:40:16.499172 139870626305792 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2651919722557068, loss=3.905954599380493
I0306 02:40:51.067584 139870634698496 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.2548905611038208, loss=3.8928823471069336
I0306 02:41:25.631489 139870626305792 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.27378058433532715, loss=3.8936009407043457
I0306 02:42:00.204380 139870634698496 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2556133568286896, loss=3.8207619190216064
I0306 02:42:34.785004 139870626305792 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.26358169317245483, loss=3.8471639156341553
I0306 02:43:09.315867 139870634698496 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.25276052951812744, loss=3.9464645385742188
I0306 02:43:43.854413 139870626305792 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2560860514640808, loss=3.891385316848755
I0306 02:44:18.393214 139870634698496 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.25287678837776184, loss=3.876145839691162
I0306 02:44:52.921569 139870626305792 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.23099353909492493, loss=3.9286253452301025
I0306 02:45:27.499611 139870634698496 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.24640116095542908, loss=3.8505051136016846
I0306 02:46:02.080763 139870626305792 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.26234468817710876, loss=3.885977029800415
I0306 02:46:36.638344 139870634698496 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.2383270561695099, loss=3.811228036880493
I0306 02:47:11.220762 139870626305792 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2669464647769928, loss=3.858987808227539
I0306 02:47:45.866346 139870634698496 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2623348534107208, loss=3.892618417739868
I0306 02:48:20.442482 139870626305792 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.25273561477661133, loss=3.894310474395752
I0306 02:48:55.027446 139870634698496 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2915724813938141, loss=3.8176422119140625
I0306 02:49:29.595492 139870626305792 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2613810896873474, loss=3.900644540786743
I0306 02:50:04.138751 139870634698496 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.25426337122917175, loss=3.9043660163879395
I0306 02:50:38.683618 139870626305792 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.25295156240463257, loss=3.8391289710998535
I0306 02:51:13.233281 139870634698496 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.23692277073860168, loss=3.9050862789154053
I0306 02:51:47.786032 139870626305792 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.23926109075546265, loss=3.847102642059326
I0306 02:52:22.300353 139870634698496 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.24267134070396423, loss=3.8887197971343994
I0306 02:52:55.794538 140014092387520 spec.py:321] Evaluating on the training split.
I0306 02:52:58.390083 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:55:35.770475 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 02:55:38.351887 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 02:58:09.314679 140014092387520 spec.py:349] Evaluating on the test split.
I0306 02:58:11.905539 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 03:00:31.587559 140014092387520 submission_runner.py:469] Time since start: 28023.35s, 	Step: 48498, 	{'train/accuracy': 0.6650455594062805, 'train/loss': 1.7754555940628052, 'train/bleu': 33.017636438170605, 'validation/accuracy': 0.6788619160652161, 'validation/loss': 1.6756508350372314, 'validation/bleu': 29.777172927643285, 'validation/num_examples': 3000, 'test/accuracy': 0.6922604441642761, 'test/loss': 1.5947095155715942, 'test/bleu': 29.57061145161476, 'test/num_examples': 3003, 'score': 16825.671855688095, 'total_duration': 28023.34668159485, 'accumulated_submission_time': 16825.671855688095, 'accumulated_eval_time': 11194.691133975983, 'accumulated_logging_time': 0.3797576427459717}
I0306 03:00:31.600817 139870626305792 logging_writer.py:48] [48498] accumulated_eval_time=11194.7, accumulated_logging_time=0.379758, accumulated_submission_time=16825.7, global_step=48498, preemption_count=0, score=16825.7, test/accuracy=0.69226, test/bleu=29.5706, test/loss=1.59471, test/num_examples=3003, total_duration=28023.3, train/accuracy=0.665046, train/bleu=33.0176, train/loss=1.77546, validation/accuracy=0.678862, validation/bleu=29.7772, validation/loss=1.67565, validation/num_examples=3000
I0306 03:00:32.652017 139870634698496 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.2644486427307129, loss=3.886138677597046
I0306 03:01:07.121927 139870626305792 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2706661522388458, loss=3.801470994949341
I0306 03:01:41.698603 139870634698496 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.26119890809059143, loss=3.867743968963623
I0306 03:02:16.279880 139870626305792 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.2391052395105362, loss=3.82709002494812
I0306 03:02:50.842865 139870634698496 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2943812310695648, loss=3.86810302734375
I0306 03:03:25.401581 139870626305792 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.24509559571743011, loss=3.8697867393493652
I0306 03:03:59.958236 139870634698496 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.27500271797180176, loss=3.8623476028442383
I0306 03:04:34.520721 139870626305792 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2685917913913727, loss=3.933685302734375
I0306 03:05:09.081177 139870634698496 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.281152606010437, loss=3.918877363204956
I0306 03:05:43.606158 139870626305792 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.27396583557128906, loss=3.914245843887329
I0306 03:06:18.177606 139870634698496 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.26917117834091187, loss=3.8838541507720947
I0306 03:06:52.756336 139870626305792 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.27392253279685974, loss=3.8841516971588135
I0306 03:07:27.322688 139870634698496 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.24502812325954437, loss=3.7889485359191895
I0306 03:08:01.919085 139870626305792 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.28211596608161926, loss=3.8525099754333496
I0306 03:08:36.586713 139870634698496 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.22869141399860382, loss=3.8169734477996826
I0306 03:09:11.171603 139870626305792 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.25701388716697693, loss=3.8376271724700928
I0306 03:09:45.746485 139870634698496 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.2478126734495163, loss=3.850076198577881
I0306 03:10:20.318619 139870626305792 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2577901780605316, loss=3.844479560852051
I0306 03:10:54.936797 139870634698496 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.24703222513198853, loss=3.8620829582214355
I0306 03:11:29.516010 139870626305792 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.2574462890625, loss=3.875711441040039
I0306 03:12:04.109828 139870634698496 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.25918513536453247, loss=3.85471248626709
I0306 03:12:38.717770 139870626305792 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.23773284256458282, loss=3.8141279220581055
I0306 03:13:13.352200 139870634698496 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.23657827079296112, loss=3.784291982650757
I0306 03:13:47.940369 139870626305792 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2769498825073242, loss=3.8148388862609863
I0306 03:14:22.526906 139870634698496 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2528471052646637, loss=3.890000343322754
I0306 03:14:31.898706 140014092387520 spec.py:321] Evaluating on the training split.
I0306 03:14:34.498989 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 03:17:31.892780 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 03:17:34.482367 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 03:20:17.752290 140014092387520 spec.py:349] Evaluating on the test split.
I0306 03:20:20.329436 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 03:22:55.712519 140014092387520 submission_runner.py:469] Time since start: 29367.47s, 	Step: 50928, 	{'train/accuracy': 0.678161084651947, 'train/loss': 1.7013061046600342, 'train/bleu': 33.73722193302117, 'validation/accuracy': 0.6770943999290466, 'validation/loss': 1.681348443031311, 'validation/bleu': 29.473870136680905, 'validation/num_examples': 3000, 'test/accuracy': 0.6920750737190247, 'test/loss': 1.6019251346588135, 'test/bleu': 29.406914407243583, 'test/num_examples': 3003, 'score': 17665.83458852768, 'total_duration': 29367.47161746025, 'accumulated_submission_time': 17665.83458852768, 'accumulated_eval_time': 11698.504893541336, 'accumulated_logging_time': 0.40401506423950195}
I0306 03:22:55.725514 139870626305792 logging_writer.py:48] [50928] accumulated_eval_time=11698.5, accumulated_logging_time=0.404015, accumulated_submission_time=17665.8, global_step=50928, preemption_count=0, score=17665.8, test/accuracy=0.692075, test/bleu=29.4069, test/loss=1.60193, test/num_examples=3003, total_duration=29367.5, train/accuracy=0.678161, train/bleu=33.7372, train/loss=1.70131, validation/accuracy=0.677094, validation/bleu=29.4739, validation/loss=1.68135, validation/num_examples=3000
I0306 03:23:20.871831 139870634698496 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.25026747584342957, loss=3.8169825077056885
I0306 03:23:55.355329 139870626305792 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.2887362837791443, loss=3.831500291824341
I0306 03:24:29.980580 139870634698496 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.23937036097049713, loss=3.785977840423584
I0306 03:25:04.586087 139870626305792 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2693929672241211, loss=3.9035329818725586
I0306 03:25:39.197973 139870634698496 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.2755701243877411, loss=3.8329880237579346
I0306 03:26:13.795653 139870626305792 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.28202977776527405, loss=3.8374550342559814
I0306 03:26:48.390000 139870634698496 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.2669748067855835, loss=3.8802802562713623
I0306 03:27:23.011031 139870626305792 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.26766759157180786, loss=3.8694376945495605
I0306 03:27:57.611434 139870634698496 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.24238264560699463, loss=3.9136760234832764
I0306 03:28:32.187885 139870626305792 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2789841890335083, loss=3.891838550567627
I0306 03:29:06.768928 139870634698496 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.23867669701576233, loss=3.8400959968566895
I0306 03:29:41.357916 139870626305792 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.25089597702026367, loss=3.7772815227508545
I0306 03:30:15.921784 139870634698496 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.25601431727409363, loss=3.8711063861846924
I0306 03:30:50.491456 139870626305792 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.26884257793426514, loss=3.84230375289917
I0306 03:31:25.089075 139870634698496 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.28092074394226074, loss=3.850132703781128
I0306 03:31:59.714871 139870626305792 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.254446804523468, loss=3.8400840759277344
I0306 03:32:34.345381 139870634698496 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.30089640617370605, loss=3.830674886703491
I0306 03:33:08.956204 139870626305792 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.31035080552101135, loss=3.848240852355957
I0306 03:33:43.579095 139870634698496 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.256639301776886, loss=3.8549344539642334
I0306 03:34:18.232751 139870626305792 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.29400476813316345, loss=3.8257648944854736
I0306 03:34:52.866583 139870634698496 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.25384560227394104, loss=3.886237859725952
I0306 03:35:27.490885 139870626305792 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2408663034439087, loss=3.8345229625701904
I0306 03:36:02.117859 139870634698496 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.2661759555339813, loss=3.839548349380493
I0306 03:36:36.722236 139870626305792 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.26426559686660767, loss=3.8401074409484863
I0306 03:36:55.742750 140014092387520 spec.py:321] Evaluating on the training split.
I0306 03:36:58.340942 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 03:40:15.768707 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 03:40:18.346004 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 03:42:51.496971 140014092387520 spec.py:349] Evaluating on the test split.
I0306 03:42:54.081113 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 03:45:27.664021 140014092387520 submission_runner.py:469] Time since start: 30719.42s, 	Step: 53356, 	{'train/accuracy': 0.6663511991500854, 'train/loss': 1.7673200368881226, 'train/bleu': 33.20184331635408, 'validation/accuracy': 0.6799001693725586, 'validation/loss': 1.6677190065383911, 'validation/bleu': 29.846394562757897, 'validation/num_examples': 3000, 'test/accuracy': 0.6934770345687866, 'test/loss': 1.5850940942764282, 'test/bleu': 29.66777480345962, 'test/num_examples': 3003, 'score': 18505.722554445267, 'total_duration': 30719.42313671112, 'accumulated_submission_time': 18505.722554445267, 'accumulated_eval_time': 12210.4261136055, 'accumulated_logging_time': 0.42496228218078613}
I0306 03:45:27.677253 139870634698496 logging_writer.py:48] [53356] accumulated_eval_time=12210.4, accumulated_logging_time=0.424962, accumulated_submission_time=18505.7, global_step=53356, preemption_count=0, score=18505.7, test/accuracy=0.693477, test/bleu=29.6678, test/loss=1.58509, test/num_examples=3003, total_duration=30719.4, train/accuracy=0.666351, train/bleu=33.2018, train/loss=1.76732, validation/accuracy=0.6799, validation/bleu=29.8464, validation/loss=1.66772, validation/num_examples=3000
I0306 03:45:43.178265 139870626305792 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.254146009683609, loss=3.8151462078094482
I0306 03:46:17.660465 139870634698496 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.2646231949329376, loss=3.823678970336914
I0306 03:46:52.240303 139870626305792 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.26360270380973816, loss=3.8137903213500977
I0306 03:47:26.847468 139870634698496 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.23704680800437927, loss=3.8135619163513184
I0306 03:48:01.465617 139870626305792 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.28490209579467773, loss=3.8654019832611084
I0306 03:48:36.103533 139870634698496 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.26344728469848633, loss=3.824002504348755
I0306 03:49:10.722735 139870626305792 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.26140713691711426, loss=3.8907179832458496
I0306 03:49:45.340809 139870634698496 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3051134943962097, loss=3.8803958892822266
I0306 03:50:19.971668 139870626305792 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.27737513184547424, loss=3.8663744926452637
I0306 03:50:54.548562 139870634698496 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.24525712430477142, loss=3.847203493118286
I0306 03:51:29.101046 139870626305792 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.25988972187042236, loss=3.850693464279175
I0306 03:52:03.666781 139870634698496 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.26988428831100464, loss=3.832580089569092
I0306 03:52:38.274372 139870626305792 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.2501790225505829, loss=3.8454411029815674
I0306 03:53:12.890306 139870634698496 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.25182729959487915, loss=3.8869552612304688
I0306 03:53:47.445367 139870626305792 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.25572457909584045, loss=3.809875965118408
I0306 03:54:22.017182 139870634698496 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.6377953290939331, loss=3.947660446166992
I0306 03:54:56.618704 139870626305792 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.24598953127861023, loss=3.8163418769836426
I0306 03:55:31.262353 139870634698496 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.8146548867225647, loss=3.840657949447632
I0306 03:56:05.869222 139870626305792 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2696947455406189, loss=3.832287549972534
I0306 03:56:40.472463 139870634698496 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.256368488073349, loss=3.859955310821533
I0306 03:57:15.113616 139870626305792 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.2605587840080261, loss=3.8120975494384766
I0306 03:57:49.734433 139870634698496 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.25628507137298584, loss=3.8330249786376953
I0306 03:58:24.396332 139870626305792 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2882769703865051, loss=3.831967830657959
I0306 03:58:59.008378 139870634698496 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.26030299067497253, loss=3.836216688156128
I0306 03:59:27.758927 140014092387520 spec.py:321] Evaluating on the training split.
I0306 03:59:30.354365 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:02:23.389434 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 04:02:25.979723 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:05:00.619134 140014092387520 spec.py:349] Evaluating on the test split.
I0306 04:05:03.209603 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:07:22.845515 140014092387520 submission_runner.py:469] Time since start: 32034.60s, 	Step: 55784, 	{'train/accuracy': 0.6593195199966431, 'train/loss': 1.8066797256469727, 'train/bleu': 33.46414713065126, 'validation/accuracy': 0.6803945302963257, 'validation/loss': 1.6639642715454102, 'validation/bleu': 29.814114746482282, 'validation/num_examples': 3000, 'test/accuracy': 0.6941837668418884, 'test/loss': 1.583396315574646, 'test/bleu': 29.442610156520477, 'test/num_examples': 3003, 'score': 19345.67029953003, 'total_duration': 32034.604627132416, 'accumulated_submission_time': 19345.67029953003, 'accumulated_eval_time': 12685.512647151947, 'accumulated_logging_time': 0.44696927070617676}
I0306 04:07:22.858508 139870626305792 logging_writer.py:48] [55784] accumulated_eval_time=12685.5, accumulated_logging_time=0.446969, accumulated_submission_time=19345.7, global_step=55784, preemption_count=0, score=19345.7, test/accuracy=0.694184, test/bleu=29.4426, test/loss=1.5834, test/num_examples=3003, total_duration=32034.6, train/accuracy=0.65932, train/bleu=33.4641, train/loss=1.80668, validation/accuracy=0.680395, validation/bleu=29.8141, validation/loss=1.66396, validation/num_examples=3000
I0306 04:07:28.727825 139870634698496 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2590809762477875, loss=3.7770025730133057
I0306 04:08:03.240523 139870626305792 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.25905269384384155, loss=3.760824680328369
I0306 04:08:37.797774 139870634698496 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.24199500679969788, loss=3.8447513580322266
I0306 04:09:12.377523 139870626305792 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.27382034063339233, loss=3.842970132827759
I0306 04:09:46.941755 139870634698496 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2681218087673187, loss=3.9354350566864014
I0306 04:10:21.529527 139870626305792 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.25509679317474365, loss=3.752748489379883
I0306 04:10:56.126376 139870634698496 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.2615153193473816, loss=3.8044660091400146
I0306 04:11:30.675509 139870626305792 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2701083719730377, loss=3.790637254714966
I0306 04:12:05.286869 139870634698496 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.23330898582935333, loss=3.802563428878784
I0306 04:12:39.883498 139870626305792 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.252711683511734, loss=3.7810819149017334
I0306 04:13:14.489670 139870634698496 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.24393804371356964, loss=3.791492462158203
I0306 04:13:49.110820 139870626305792 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.26836758852005005, loss=3.8193492889404297
I0306 04:14:23.726712 139870634698496 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.27759870886802673, loss=3.901667356491089
I0306 04:14:58.350029 139870626305792 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.27349603176116943, loss=3.858321189880371
I0306 04:15:33.002485 139870634698496 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2722316086292267, loss=3.8848824501037598
I0306 04:16:07.645138 139870626305792 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.25732940435409546, loss=3.856513500213623
I0306 04:16:42.254385 139870634698496 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.266850084066391, loss=3.8751752376556396
I0306 04:17:16.841530 139870626305792 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.27707841992378235, loss=3.763425827026367
I0306 04:17:51.406671 139870634698496 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.27471107244491577, loss=3.885810136795044
I0306 04:18:26.032192 139870626305792 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2709241807460785, loss=3.7993505001068115
I0306 04:19:00.620564 139870634698496 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.26476719975471497, loss=3.81054949760437
I0306 04:19:35.220509 139870626305792 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2536374032497406, loss=3.832984447479248
I0306 04:20:09.860345 139870634698496 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2719928026199341, loss=3.8528051376342773
I0306 04:20:44.477504 139870626305792 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.24781322479248047, loss=3.7732675075531006
I0306 04:21:19.084194 139870634698496 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.2592005133628845, loss=3.814465045928955
I0306 04:21:22.887026 140014092387520 spec.py:321] Evaluating on the training split.
I0306 04:21:25.481180 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:24:17.920472 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 04:24:20.507451 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:26:54.833099 140014092387520 spec.py:349] Evaluating on the test split.
I0306 04:26:57.420468 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:29:20.838501 140014092387520 submission_runner.py:469] Time since start: 33352.60s, 	Step: 58212, 	{'train/accuracy': 0.6740744113922119, 'train/loss': 1.7107826471328735, 'train/bleu': 33.75707023750471, 'validation/accuracy': 0.6798877716064453, 'validation/loss': 1.659110188484192, 'validation/bleu': 29.683342942430748, 'validation/num_examples': 3000, 'test/accuracy': 0.6946588158607483, 'test/loss': 1.576315999031067, 'test/bleu': 29.619676667709058, 'test/num_examples': 3003, 'score': 20185.56756925583, 'total_duration': 33352.59757947922, 'accumulated_submission_time': 20185.56756925583, 'accumulated_eval_time': 13163.464032173157, 'accumulated_logging_time': 0.4675753116607666}
I0306 04:29:20.851974 139870626305792 logging_writer.py:48] [58212] accumulated_eval_time=13163.5, accumulated_logging_time=0.467575, accumulated_submission_time=20185.6, global_step=58212, preemption_count=0, score=20185.6, test/accuracy=0.694659, test/bleu=29.6197, test/loss=1.57632, test/num_examples=3003, total_duration=33352.6, train/accuracy=0.674074, train/bleu=33.7571, train/loss=1.71078, validation/accuracy=0.679888, validation/bleu=29.6833, validation/loss=1.65911, validation/num_examples=3000
I0306 04:29:51.542727 139870634698496 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.29544898867607117, loss=3.8725106716156006
I0306 04:30:26.072912 139870626305792 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2581275403499603, loss=3.8680930137634277
I0306 04:31:00.657469 139870634698496 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.2765411138534546, loss=3.83183217048645
I0306 04:31:35.277785 139870626305792 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.27307331562042236, loss=3.797990322113037
I0306 04:32:09.912798 139870634698496 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.26986393332481384, loss=3.7868480682373047
I0306 04:32:44.489662 139870626305792 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2986806035041809, loss=3.8394176959991455
I0306 04:33:19.061948 139870634698496 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.297328919172287, loss=3.830824613571167
I0306 04:33:53.631435 139870626305792 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.28011417388916016, loss=3.8773021697998047
I0306 04:34:28.278243 139870634698496 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.2917415499687195, loss=3.8808741569519043
I0306 04:35:02.906183 139870626305792 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.2554568350315094, loss=3.756896734237671
I0306 04:35:37.499524 139870634698496 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.27449989318847656, loss=3.8043923377990723
I0306 04:36:12.151005 139870626305792 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.27848806977272034, loss=3.848515272140503
I0306 04:36:46.770962 139870634698496 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2812686264514923, loss=3.7993743419647217
I0306 04:37:21.360979 139870626305792 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.27150216698646545, loss=3.8280091285705566
I0306 04:37:56.005271 139870634698496 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.2681601643562317, loss=3.833608865737915
I0306 04:38:30.693064 139870626305792 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.26768869161605835, loss=3.778393268585205
I0306 04:39:05.321229 139870634698496 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.2883404791355133, loss=3.834658622741699
I0306 04:39:39.925073 139870626305792 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2596815228462219, loss=3.7851195335388184
I0306 04:40:14.557228 139870634698496 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.2796551585197449, loss=3.8381969928741455
I0306 04:40:49.160043 139870626305792 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.2561497688293457, loss=3.8137240409851074
I0306 04:41:23.737502 139870634698496 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.27788877487182617, loss=3.798156976699829
I0306 04:41:58.337278 139870626305792 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2727715075016022, loss=3.8469595909118652
I0306 04:42:32.928824 139870634698496 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2631778120994568, loss=3.7833423614501953
I0306 04:43:07.520434 139870626305792 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.3403915464878082, loss=3.791203498840332
I0306 04:43:21.005105 140014092387520 spec.py:321] Evaluating on the training split.
I0306 04:43:23.601144 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:46:56.524526 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 04:46:59.110059 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:49:35.893155 140014092387520 spec.py:349] Evaluating on the test split.
I0306 04:49:38.487418 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 04:52:14.323612 140014092387520 submission_runner.py:469] Time since start: 34726.08s, 	Step: 60640, 	{'train/accuracy': 0.6705814003944397, 'train/loss': 1.7417917251586914, 'train/bleu': 33.507083309313806, 'validation/accuracy': 0.6825575232505798, 'validation/loss': 1.6651276350021362, 'validation/bleu': 29.94895947917713, 'validation/num_examples': 3000, 'test/accuracy': 0.6970339417457581, 'test/loss': 1.5821985006332397, 'test/bleu': 29.730545639064797, 'test/num_examples': 3003, 'score': 21025.590893507004, 'total_duration': 34726.0827331543, 'accumulated_submission_time': 21025.590893507004, 'accumulated_eval_time': 13696.782493114471, 'accumulated_logging_time': 0.48986291885375977}
I0306 04:52:14.337601 139870634698496 logging_writer.py:48] [60640] accumulated_eval_time=13696.8, accumulated_logging_time=0.489863, accumulated_submission_time=21025.6, global_step=60640, preemption_count=0, score=21025.6, test/accuracy=0.697034, test/bleu=29.7305, test/loss=1.5822, test/num_examples=3003, total_duration=34726.1, train/accuracy=0.670581, train/bleu=33.5071, train/loss=1.74179, validation/accuracy=0.682558, validation/bleu=29.949, validation/loss=1.66513, validation/num_examples=3000
I0306 04:52:35.334865 139870626305792 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.2775958180427551, loss=3.8355982303619385
I0306 04:53:09.842639 139870634698496 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.27830132842063904, loss=3.8324995040893555
I0306 04:53:44.459426 139870626305792 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.2823559045791626, loss=3.7845828533172607
I0306 04:54:19.072634 139870634698496 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.25857076048851013, loss=3.845262050628662
I0306 04:54:53.695042 139870626305792 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.2795431315898895, loss=3.8672821521759033
I0306 04:55:28.340308 139870634698496 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.2839592397212982, loss=3.820590019226074
I0306 04:56:02.970901 139870626305792 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.26922178268432617, loss=3.8708953857421875
I0306 04:56:37.574212 139870634698496 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.29294389486312866, loss=3.8090693950653076
I0306 04:57:12.154753 139870626305792 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2704021632671356, loss=3.8262269496917725
I0306 04:57:46.735109 139870634698496 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2828894257545471, loss=3.8758327960968018
I0306 04:58:21.354989 139870626305792 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.27691102027893066, loss=3.870805025100708
I0306 04:58:55.929656 139870634698496 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.31792333722114563, loss=3.8290252685546875
I0306 04:59:30.517678 139870626305792 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3138211965560913, loss=3.7837204933166504
I0306 05:00:05.160020 139870634698496 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.27164196968078613, loss=3.813385486602783
I0306 05:00:39.766546 139870626305792 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.27685388922691345, loss=3.8194379806518555
I0306 05:01:14.381582 139870634698496 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.29288503527641296, loss=3.7953972816467285
I0306 05:01:48.997481 139870626305792 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.2586570680141449, loss=3.8259031772613525
I0306 05:02:23.587430 139870634698496 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.2757989466190338, loss=3.8599908351898193
I0306 05:02:58.191808 139870626305792 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.29745742678642273, loss=3.725463628768921
I0306 05:03:32.776961 139870634698496 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2905322015285492, loss=3.8071627616882324
I0306 05:04:07.356045 139870626305792 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.2750564217567444, loss=3.8312623500823975
I0306 05:04:41.941031 139870634698496 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2853470742702484, loss=3.7796783447265625
I0306 05:05:16.510148 139870626305792 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2582623362541199, loss=3.771472930908203
I0306 05:05:51.066591 139870634698496 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.27848726511001587, loss=3.8549067974090576
I0306 05:06:14.562154 140014092387520 spec.py:321] Evaluating on the training split.
I0306 05:06:17.162322 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 05:11:09.077957 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 05:11:11.676871 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 05:14:49.986102 140014092387520 spec.py:349] Evaluating on the test split.
I0306 05:14:52.567278 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 05:17:43.550973 140014092387520 submission_runner.py:469] Time since start: 36255.31s, 	Step: 63069, 	{'train/accuracy': 0.6917546391487122, 'train/loss': 1.6079250574111938, 'train/bleu': 34.63569596473616, 'validation/accuracy': 0.6841520071029663, 'validation/loss': 1.6403577327728271, 'validation/bleu': 30.072948329535862, 'validation/num_examples': 3000, 'test/accuracy': 0.6980419754981995, 'test/loss': 1.5595570802688599, 'test/bleu': 30.047292877075495, 'test/num_examples': 3003, 'score': 21865.68392586708, 'total_duration': 36255.31010055542, 'accumulated_submission_time': 21865.68392586708, 'accumulated_eval_time': 14385.771279096603, 'accumulated_logging_time': 0.5128803253173828}
I0306 05:17:43.565109 139870626305792 logging_writer.py:48] [63069] accumulated_eval_time=14385.8, accumulated_logging_time=0.51288, accumulated_submission_time=21865.7, global_step=63069, preemption_count=0, score=21865.7, test/accuracy=0.698042, test/bleu=30.0473, test/loss=1.55956, test/num_examples=3003, total_duration=36255.3, train/accuracy=0.691755, train/bleu=34.6357, train/loss=1.60793, validation/accuracy=0.684152, validation/bleu=30.0729, validation/loss=1.64036, validation/num_examples=3000
I0306 05:17:54.580786 139870634698496 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.25775566697120667, loss=3.85591983795166
I0306 05:18:29.023590 139870626305792 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2987835705280304, loss=3.834280490875244
I0306 05:19:03.551744 139870634698496 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.2758444845676422, loss=3.7823851108551025
I0306 05:19:38.128086 139870626305792 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2529204189777374, loss=3.787893772125244
I0306 05:20:12.653293 139870634698496 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.27486711740493774, loss=3.7559187412261963
I0306 05:20:47.197327 139870626305792 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.26958978176116943, loss=3.799797773361206
I0306 05:21:21.739381 139870634698496 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.32980233430862427, loss=3.8501718044281006
I0306 05:21:56.287034 139870626305792 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.27420109510421753, loss=3.7839181423187256
I0306 05:22:30.839452 139870634698496 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2662345767021179, loss=3.7406187057495117
I0306 05:23:05.399245 139870626305792 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2766857445240021, loss=3.8035340309143066
I0306 05:23:39.914043 139870634698496 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.2572057247161865, loss=3.7782208919525146
I0306 05:24:14.550673 139870626305792 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.27774184942245483, loss=3.80322265625
I0306 05:24:49.098967 139870634698496 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.2731856405735016, loss=3.7817389965057373
I0306 05:25:23.684982 139870626305792 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.2829539477825165, loss=3.8667404651641846
I0306 05:25:58.288547 139870634698496 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.266610711812973, loss=3.865572690963745
I0306 05:26:32.878669 139870626305792 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.27198392152786255, loss=3.703031063079834
I0306 05:27:07.438407 139870634698496 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.2789139449596405, loss=3.808034658432007
I0306 05:27:42.030881 139870626305792 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.26831886172294617, loss=3.738938808441162
I0306 05:28:16.631489 139870634698496 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.27067577838897705, loss=3.797320604324341
I0306 05:28:51.201868 139870626305792 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2942447364330292, loss=3.8313183784484863
I0306 05:29:25.779650 139870634698496 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2735404670238495, loss=3.7636280059814453
I0306 05:30:00.386227 139870626305792 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3219165802001953, loss=3.7738680839538574
I0306 05:30:34.993286 139870634698496 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2662978768348694, loss=3.7955539226531982
I0306 05:31:09.641214 139870626305792 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.2732383906841278, loss=3.8056108951568604
I0306 05:31:43.878637 140014092387520 spec.py:321] Evaluating on the training split.
I0306 05:31:46.470955 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 05:36:25.202799 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 05:36:27.795740 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 05:39:36.687195 140014092387520 spec.py:349] Evaluating on the test split.
I0306 05:39:39.282172 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 05:42:40.102604 140014092387520 submission_runner.py:469] Time since start: 37751.86s, 	Step: 65500, 	{'train/accuracy': 0.678013026714325, 'train/loss': 1.6905033588409424, 'train/bleu': 33.738987930062756, 'validation/accuracy': 0.6821743845939636, 'validation/loss': 1.6405622959136963, 'validation/bleu': 30.045239365827644, 'validation/num_examples': 3000, 'test/accuracy': 0.6986907720565796, 'test/loss': 1.554708480834961, 'test/bleu': 29.972031120604413, 'test/num_examples': 3003, 'score': 22705.866425275803, 'total_duration': 37751.861726522446, 'accumulated_submission_time': 22705.866425275803, 'accumulated_eval_time': 15041.995205879211, 'accumulated_logging_time': 0.5346004962921143}
I0306 05:42:40.117576 139870634698496 logging_writer.py:48] [65500] accumulated_eval_time=15042, accumulated_logging_time=0.5346, accumulated_submission_time=22705.9, global_step=65500, preemption_count=0, score=22705.9, test/accuracy=0.698691, test/bleu=29.972, test/loss=1.55471, test/num_examples=3003, total_duration=37751.9, train/accuracy=0.678013, train/bleu=33.739, train/loss=1.6905, validation/accuracy=0.682174, validation/bleu=30.0452, validation/loss=1.64056, validation/num_examples=3000
I0306 05:42:40.481579 139870626305792 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.29842492938041687, loss=3.8383066654205322
I0306 05:43:14.870171 139870634698496 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.27101194858551025, loss=3.841691017150879
I0306 05:43:49.310325 139870626305792 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.26005929708480835, loss=3.81561017036438
I0306 05:44:23.843786 139870634698496 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.2728542387485504, loss=3.80385684967041
I0306 05:44:58.395760 139870626305792 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.27390098571777344, loss=3.791675329208374
I0306 05:45:32.969256 139870634698496 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2766023576259613, loss=3.795081853866577
I0306 05:46:07.567511 139870626305792 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.27406394481658936, loss=3.737074613571167
I0306 05:46:42.118760 139870634698496 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.30030468106269836, loss=3.7416200637817383
I0306 05:47:16.691165 139870626305792 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.3269093334674835, loss=3.7500813007354736
I0306 05:47:51.216776 139870634698496 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2807104289531708, loss=3.81501841545105
I0306 05:48:25.796341 139870626305792 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2833947539329529, loss=3.7886321544647217
I0306 05:49:00.364215 139870634698496 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2762965261936188, loss=3.8116960525512695
I0306 05:49:34.943074 139870626305792 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.28535547852516174, loss=3.785447120666504
I0306 05:50:09.530476 139870634698496 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2881372570991516, loss=3.8013124465942383
I0306 05:50:44.076372 139870626305792 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2966945469379425, loss=3.824052333831787
I0306 05:51:18.635017 139870634698496 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2968890368938446, loss=3.8561036586761475
I0306 05:51:53.192838 139870626305792 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.29380959272384644, loss=3.8452303409576416
I0306 05:52:27.757039 139870634698496 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.30987825989723206, loss=3.7659380435943604
I0306 05:53:02.341889 139870626305792 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.27911731600761414, loss=3.8013112545013428
I0306 05:53:36.912306 139870634698496 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.27541646361351013, loss=3.7725908756256104
I0306 05:54:11.447760 139870626305792 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.2974436283111572, loss=3.76387882232666
I0306 05:54:45.999479 139870634698496 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2919907569885254, loss=3.8349878787994385
I0306 05:55:20.598398 139870626305792 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.29650264978408813, loss=3.8072259426116943
I0306 05:55:55.184069 139870634698496 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.28596052527427673, loss=3.730250120162964
I0306 05:56:29.718639 139870626305792 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.25342464447021484, loss=3.7301928997039795
I0306 05:56:40.446453 140014092387520 spec.py:321] Evaluating on the training split.
I0306 05:56:43.042212 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 05:59:48.351591 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 05:59:50.944486 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:02:30.166351 140014092387520 spec.py:349] Evaluating on the test split.
I0306 06:02:32.761168 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:04:59.072786 140014092387520 submission_runner.py:469] Time since start: 39090.83s, 	Step: 67932, 	{'train/accuracy': 0.6781975030899048, 'train/loss': 1.7058039903640747, 'train/bleu': 33.83729287863027, 'validation/accuracy': 0.6836081743240356, 'validation/loss': 1.6443802118301392, 'validation/bleu': 30.25696925145094, 'validation/num_examples': 3000, 'test/accuracy': 0.6991426348686218, 'test/loss': 1.5548205375671387, 'test/bleu': 30.270547114908364, 'test/num_examples': 3003, 'score': 23546.064761400223, 'total_duration': 39090.83188700676, 'accumulated_submission_time': 23546.064761400223, 'accumulated_eval_time': 15540.621483325958, 'accumulated_logging_time': 0.5572319030761719}
I0306 06:04:59.089326 139870634698496 logging_writer.py:48] [67932] accumulated_eval_time=15540.6, accumulated_logging_time=0.557232, accumulated_submission_time=23546.1, global_step=67932, preemption_count=0, score=23546.1, test/accuracy=0.699143, test/bleu=30.2705, test/loss=1.55482, test/num_examples=3003, total_duration=39090.8, train/accuracy=0.678198, train/bleu=33.8373, train/loss=1.7058, validation/accuracy=0.683608, validation/bleu=30.257, validation/loss=1.64438, validation/num_examples=3000
I0306 06:05:22.833981 139870626305792 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2636287212371826, loss=3.79345965385437
I0306 06:05:57.259508 139870634698496 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.2737710177898407, loss=3.7731704711914062
I0306 06:06:31.782171 139870626305792 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.2846822440624237, loss=3.7356481552124023
I0306 06:07:06.314328 139870634698496 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.2883252501487732, loss=3.7990036010742188
I0306 06:07:40.834035 139870626305792 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2718459963798523, loss=3.8153133392333984
I0306 06:08:15.416470 139870634698496 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.29276028275489807, loss=3.7570719718933105
I0306 06:08:49.952845 139870626305792 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2895655035972595, loss=3.863300085067749
I0306 06:09:24.539318 139870634698496 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.2721596360206604, loss=3.789714813232422
I0306 06:09:59.112860 139870626305792 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.289764404296875, loss=3.7838187217712402
I0306 06:10:33.667583 139870634698496 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.26647108793258667, loss=3.74894118309021
I0306 06:11:08.225414 139870626305792 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.26919135451316833, loss=3.7497916221618652
I0306 06:11:42.828224 139870634698496 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.27926966547966003, loss=3.7874643802642822
I0306 06:12:17.429958 139870626305792 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2888203263282776, loss=3.7347495555877686
I0306 06:12:52.023118 139870634698496 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3075714111328125, loss=3.7747318744659424
I0306 06:13:26.614066 139870626305792 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2750965356826782, loss=3.7932627201080322
I0306 06:14:01.214993 139870634698496 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.276281476020813, loss=3.751146078109741
I0306 06:14:35.838980 139870626305792 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2945913076400757, loss=3.7271528244018555
I0306 06:15:10.452021 139870634698496 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.30313897132873535, loss=3.7752716541290283
I0306 06:15:45.143985 139870626305792 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2768518328666687, loss=3.7160637378692627
I0306 06:16:19.768883 139870634698496 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2765272259712219, loss=3.744943141937256
I0306 06:16:54.393371 139870626305792 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.2690868079662323, loss=3.786627769470215
I0306 06:17:28.978414 139870634698496 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2919219732284546, loss=3.7633118629455566
I0306 06:18:03.576843 139870626305792 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2887597680091858, loss=3.7318716049194336
I0306 06:18:38.168617 139870634698496 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2679504454135895, loss=3.769496440887451
I0306 06:18:59.264886 140014092387520 spec.py:321] Evaluating on the training split.
I0306 06:19:01.872084 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:23:36.696389 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 06:23:39.293310 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:26:48.141668 140014092387520 spec.py:349] Evaluating on the test split.
I0306 06:26:50.729020 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:29:42.831120 140014092387520 submission_runner.py:469] Time since start: 40574.59s, 	Step: 70362, 	{'train/accuracy': 0.6845722794532776, 'train/loss': 1.6533809900283813, 'train/bleu': 34.62180642562419, 'validation/accuracy': 0.6854497790336609, 'validation/loss': 1.6352390050888062, 'validation/bleu': 30.327073951645982, 'validation/num_examples': 3000, 'test/accuracy': 0.7006256580352783, 'test/loss': 1.5441480875015259, 'test/bleu': 30.24846840007525, 'test/num_examples': 3003, 'score': 24386.10723233223, 'total_duration': 40574.590224027634, 'accumulated_submission_time': 24386.10723233223, 'accumulated_eval_time': 16184.187658786774, 'accumulated_logging_time': 0.5817821025848389}
I0306 06:29:42.845804 139870626305792 logging_writer.py:48] [70362] accumulated_eval_time=16184.2, accumulated_logging_time=0.581782, accumulated_submission_time=24386.1, global_step=70362, preemption_count=0, score=24386.1, test/accuracy=0.700626, test/bleu=30.2485, test/loss=1.54415, test/num_examples=3003, total_duration=40574.6, train/accuracy=0.684572, train/bleu=34.6218, train/loss=1.65338, validation/accuracy=0.68545, validation/bleu=30.3271, validation/loss=1.63524, validation/num_examples=3000
I0306 06:29:56.301977 139870634698496 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2632836103439331, loss=3.756459951400757
I0306 06:30:30.750733 139870626305792 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2790932059288025, loss=3.7954113483428955
I0306 06:31:05.252985 139870634698496 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.27757754921913147, loss=3.8076765537261963
I0306 06:31:39.818443 139870626305792 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2840679883956909, loss=3.79872465133667
I0306 06:32:14.392916 139870634698496 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.2994418740272522, loss=3.7896032333374023
I0306 06:32:48.983605 139870626305792 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.2872585952281952, loss=3.7805914878845215
I0306 06:33:23.579550 139870634698496 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.296207457780838, loss=3.7733101844787598
I0306 06:33:58.175211 139870626305792 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.26940155029296875, loss=3.770982265472412
I0306 06:34:32.794119 139870634698496 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.26917338371276855, loss=3.794097661972046
I0306 06:35:07.420653 139870626305792 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.2918854355812073, loss=3.7837376594543457
I0306 06:35:42.050477 139870634698496 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2898138761520386, loss=3.7636380195617676
I0306 06:36:16.655141 139870626305792 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.30505767464637756, loss=3.84018611907959
I0306 06:36:51.232582 139870634698496 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.2851948142051697, loss=3.7603201866149902
I0306 06:37:25.842212 139870626305792 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.3091332018375397, loss=3.762498140335083
I0306 06:38:00.473701 139870634698496 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.28130507469177246, loss=3.787402629852295
I0306 06:38:35.045244 139870626305792 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.32093068957328796, loss=3.823617935180664
I0306 06:39:09.647336 139870634698496 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2835596799850464, loss=3.819267988204956
I0306 06:39:44.222097 139870626305792 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.29407185316085815, loss=3.8156397342681885
I0306 06:40:18.844064 139870634698496 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.29262775182724, loss=3.811816453933716
I0306 06:40:53.454866 139870626305792 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.28942474722862244, loss=3.7658355236053467
I0306 06:41:28.279087 139870634698496 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.2724895179271698, loss=3.669975519180298
I0306 06:42:03.013832 139870626305792 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.27490538358688354, loss=3.723949670791626
I0306 06:42:37.729444 139870634698496 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.2689019739627838, loss=3.724210500717163
I0306 06:43:12.472990 139870626305792 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.289138525724411, loss=3.7487952709198
I0306 06:43:43.062344 140014092387520 spec.py:321] Evaluating on the training split.
I0306 06:43:45.681282 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:47:00.545129 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 06:47:03.131587 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:49:43.097763 140014092387520 spec.py:349] Evaluating on the test split.
I0306 06:49:45.696503 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 06:52:18.713738 140014092387520 submission_runner.py:469] Time since start: 41930.47s, 	Step: 72789, 	{'train/accuracy': 0.6809914708137512, 'train/loss': 1.675355076789856, 'train/bleu': 34.46936074608691, 'validation/accuracy': 0.6856970191001892, 'validation/loss': 1.6260801553726196, 'validation/bleu': 30.51372839924486, 'validation/num_examples': 3000, 'test/accuracy': 0.7015409469604492, 'test/loss': 1.5358240604400635, 'test/bleu': 30.29830131674638, 'test/num_examples': 3003, 'score': 25226.19104862213, 'total_duration': 41930.47286128998, 'accumulated_submission_time': 25226.19104862213, 'accumulated_eval_time': 16699.839008569717, 'accumulated_logging_time': 0.6043787002563477}
I0306 06:52:18.730473 139870634698496 logging_writer.py:48] [72789] accumulated_eval_time=16699.8, accumulated_logging_time=0.604379, accumulated_submission_time=25226.2, global_step=72789, preemption_count=0, score=25226.2, test/accuracy=0.701541, test/bleu=30.2983, test/loss=1.53582, test/num_examples=3003, total_duration=41930.5, train/accuracy=0.680991, train/bleu=34.4694, train/loss=1.67536, validation/accuracy=0.685697, validation/bleu=30.5137, validation/loss=1.62608, validation/num_examples=3000
I0306 06:52:22.879578 139870626305792 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3194369375705719, loss=3.8162548542022705
I0306 06:52:57.512183 139870634698496 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3177066445350647, loss=3.822404146194458
I0306 06:53:32.250807 139870626305792 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.29590532183647156, loss=3.794613838195801
I0306 06:54:06.968216 139870634698496 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2807636559009552, loss=3.775967836380005
I0306 06:54:41.765016 139870626305792 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.2849295139312744, loss=3.770155906677246
I0306 06:55:16.553166 139870634698496 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.2891155481338501, loss=3.711016893386841
I0306 06:55:51.303236 139870626305792 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2720545828342438, loss=3.7371692657470703
I0306 06:56:26.064222 139870634698496 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3011714816093445, loss=3.7948391437530518
I0306 06:57:00.791903 139870626305792 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.3232380747795105, loss=3.7466742992401123
I0306 06:57:35.538410 139870634698496 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.281975656747818, loss=3.768803834915161
I0306 06:58:10.269176 139870626305792 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.28632184863090515, loss=3.7864787578582764
I0306 06:58:45.015086 139870634698496 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.30751001834869385, loss=3.805514335632324
I0306 06:59:19.750400 139870626305792 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.2871007025241852, loss=3.8698222637176514
I0306 06:59:54.468157 139870634698496 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.32354608178138733, loss=3.7444396018981934
I0306 07:00:29.249596 139870626305792 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.27985647320747375, loss=3.7444539070129395
I0306 07:01:03.983031 139870634698496 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.27592259645462036, loss=3.781437873840332
I0306 07:01:38.709718 139870626305792 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.2908129394054413, loss=3.745298147201538
I0306 07:02:13.433693 139870634698496 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.30647554993629456, loss=3.7509307861328125
I0306 07:02:48.174120 139870626305792 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2897859513759613, loss=3.702101707458496
I0306 07:03:22.909469 139870634698496 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.30541282892227173, loss=3.7724101543426514
I0306 07:03:57.633516 139870626305792 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.26991891860961914, loss=3.778529644012451
I0306 07:04:32.378790 139870634698496 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.28519612550735474, loss=3.718501091003418
I0306 07:05:07.129258 139870626305792 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2946830689907074, loss=3.7787821292877197
I0306 07:05:41.890252 139870634698496 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3028815686702728, loss=3.7243947982788086
I0306 07:06:16.743467 139870626305792 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.29038888216018677, loss=3.7965755462646484
I0306 07:06:18.831164 140014092387520 spec.py:321] Evaluating on the training split.
I0306 07:06:21.444005 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 07:10:01.229962 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 07:10:03.822803 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 07:13:06.157179 140014092387520 spec.py:349] Evaluating on the test split.
I0306 07:13:08.755160 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 07:15:54.736363 140014092387520 submission_runner.py:469] Time since start: 43346.50s, 	Step: 75207, 	{'train/accuracy': 0.6794785857200623, 'train/loss': 1.6920408010482788, 'train/bleu': 34.42417625398327, 'validation/accuracy': 0.6880701184272766, 'validation/loss': 1.6248291730880737, 'validation/bleu': 30.416407374886056, 'validation/num_examples': 3000, 'test/accuracy': 0.70440274477005, 'test/loss': 1.531368613243103, 'test/bleu': 30.25555285865231, 'test/num_examples': 3003, 'score': 26066.15735888481, 'total_duration': 43346.49547815323, 'accumulated_submission_time': 26066.15735888481, 'accumulated_eval_time': 17275.74415397644, 'accumulated_logging_time': 0.6288051605224609}
I0306 07:15:54.752532 139870634698496 logging_writer.py:48] [75207] accumulated_eval_time=17275.7, accumulated_logging_time=0.628805, accumulated_submission_time=26066.2, global_step=75207, preemption_count=0, score=26066.2, test/accuracy=0.704403, test/bleu=30.2556, test/loss=1.53137, test/num_examples=3003, total_duration=43346.5, train/accuracy=0.679479, train/bleu=34.4242, train/loss=1.69204, validation/accuracy=0.68807, validation/bleu=30.4164, validation/loss=1.62483, validation/num_examples=3000
I0306 07:16:27.218590 139870626305792 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.29181334376335144, loss=3.7261033058166504
I0306 07:17:01.890998 139870634698496 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.29916828870773315, loss=3.747082233428955
I0306 07:17:36.606170 139870626305792 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.27675798535346985, loss=3.716552972793579
I0306 07:18:11.338098 139870634698496 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.319084495306015, loss=3.748999834060669
I0306 07:18:46.087706 139870626305792 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.31226667761802673, loss=3.7075233459472656
I0306 07:19:20.855246 139870634698496 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2992965281009674, loss=3.794893503189087
I0306 07:19:55.595870 139870626305792 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3016708195209503, loss=3.716850996017456
I0306 07:20:30.368812 139870634698496 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.2849806249141693, loss=3.729905843734741
I0306 07:21:05.102546 139870626305792 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.29939210414886475, loss=3.7101168632507324
I0306 07:21:39.861159 139870634698496 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.29434728622436523, loss=3.773244619369507
I0306 07:22:14.583686 139870626305792 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.28923848271369934, loss=3.749298572540283
I0306 07:22:49.352745 139870634698496 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3098536431789398, loss=3.7658298015594482
I0306 07:23:24.078854 139870626305792 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.2927933931350708, loss=3.7461907863616943
I0306 07:23:58.827885 139870634698496 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.29777956008911133, loss=3.7750167846679688
I0306 07:24:33.582972 139870626305792 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.27419471740722656, loss=3.7366886138916016
I0306 07:25:08.298921 139870634698496 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3052451014518738, loss=3.7506930828094482
I0306 07:25:43.052899 139870626305792 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.3187759816646576, loss=3.767767906188965
I0306 07:26:17.810971 139870634698496 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.28973403573036194, loss=3.743014335632324
I0306 07:26:52.573321 139870626305792 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.29307955503463745, loss=3.8569729328155518
I0306 07:27:27.339774 139870634698496 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2797596752643585, loss=3.7932348251342773
I0306 07:28:02.127106 139870626305792 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.2928259074687958, loss=3.7100770473480225
I0306 07:28:36.857810 139870634698496 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.2970759868621826, loss=3.7342820167541504
I0306 07:29:11.614966 139870626305792 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.3063567876815796, loss=3.6919710636138916
I0306 07:29:46.357332 139870634698496 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.2710157334804535, loss=3.6699180603027344
I0306 07:29:55.037020 140014092387520 spec.py:321] Evaluating on the training split.
I0306 07:29:57.647390 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 07:34:36.320345 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 07:34:38.919392 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 07:37:52.308261 140014092387520 spec.py:349] Evaluating on the test split.
I0306 07:37:54.898036 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 07:40:46.204797 140014092387520 submission_runner.py:469] Time since start: 44837.96s, 	Step: 77626, 	{'train/accuracy': 0.6874957084655762, 'train/loss': 1.6292890310287476, 'train/bleu': 34.73902798636497, 'validation/accuracy': 0.6861790418624878, 'validation/loss': 1.6188749074935913, 'validation/bleu': 30.518530648102807, 'validation/num_examples': 3000, 'test/accuracy': 0.7026879787445068, 'test/loss': 1.5254799127578735, 'test/bleu': 30.27720554256199, 'test/num_examples': 3003, 'score': 26906.30938053131, 'total_duration': 44837.9638838768, 'accumulated_submission_time': 26906.30938053131, 'accumulated_eval_time': 17926.911850452423, 'accumulated_logging_time': 0.652606725692749}
I0306 07:40:46.225759 139870626305792 logging_writer.py:48] [77626] accumulated_eval_time=17926.9, accumulated_logging_time=0.652607, accumulated_submission_time=26906.3, global_step=77626, preemption_count=0, score=26906.3, test/accuracy=0.702688, test/bleu=30.2772, test/loss=1.52548, test/num_examples=3003, total_duration=44838, train/accuracy=0.687496, train/bleu=34.739, train/loss=1.62929, validation/accuracy=0.686179, validation/bleu=30.5185, validation/loss=1.61887, validation/num_examples=3000
I0306 07:41:12.252695 139870634698496 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.28889310359954834, loss=3.7525460720062256
I0306 07:41:46.954120 139870626305792 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3106118142604828, loss=3.7301249504089355
I0306 07:42:21.679844 139870634698496 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.2947636544704437, loss=3.7368321418762207
I0306 07:42:56.430980 139870626305792 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.2830965518951416, loss=3.6999399662017822
I0306 07:43:31.176713 139870634698496 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.31352630257606506, loss=3.756430149078369
I0306 07:44:05.938209 139870626305792 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.29847973585128784, loss=3.7742719650268555
I0306 07:44:40.713693 139870634698496 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.29612305760383606, loss=3.6697003841400146
I0306 07:45:15.469181 139870626305792 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.3212723433971405, loss=3.724989891052246
I0306 07:45:50.210417 139870634698496 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2867366671562195, loss=3.7594492435455322
I0306 07:46:24.921872 139870626305792 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.29343751072883606, loss=3.7419440746307373
I0306 07:46:59.679260 139870634698496 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.3178871273994446, loss=3.7573182582855225
I0306 07:47:34.414479 139870626305792 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3084243834018707, loss=3.739725112915039
I0306 07:48:09.145669 139870634698496 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.30931341648101807, loss=3.8131802082061768
I0306 07:48:43.899989 139870626305792 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.33984068036079407, loss=3.7414371967315674
I0306 07:49:18.684673 139870634698496 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.29872700572013855, loss=3.7736475467681885
I0306 07:49:53.441223 139870626305792 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.30220749974250793, loss=3.6915442943573
I0306 07:50:28.206845 139870634698496 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2976801097393036, loss=3.6693637371063232
I0306 07:51:02.976969 139870626305792 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3087248206138611, loss=3.71498703956604
I0306 07:51:37.736095 139870634698496 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.31718865036964417, loss=3.7506825923919678
I0306 07:52:12.492316 139870626305792 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.2932835817337036, loss=3.6998555660247803
I0306 07:52:47.373773 139870634698496 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.30439913272857666, loss=3.7371060848236084
I0306 07:53:22.130891 139870626305792 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2837609052658081, loss=3.7256290912628174
I0306 07:53:56.902410 139870634698496 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.34548714756965637, loss=3.675906181335449
I0306 07:54:31.689350 139870626305792 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3069714307785034, loss=3.727004051208496
I0306 07:54:46.291737 140014092387520 spec.py:321] Evaluating on the training split.
I0306 07:54:48.898425 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 07:58:12.486690 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 07:58:15.082405 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:01:14.960163 140014092387520 spec.py:349] Evaluating on the test split.
I0306 08:01:17.551285 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:03:45.907425 140014092387520 submission_runner.py:469] Time since start: 46217.67s, 	Step: 80043, 	{'train/accuracy': 0.6864668726921082, 'train/loss': 1.6455543041229248, 'train/bleu': 34.74561324436292, 'validation/accuracy': 0.6876004338264465, 'validation/loss': 1.6194725036621094, 'validation/bleu': 30.508750137624247, 'validation/num_examples': 3000, 'test/accuracy': 0.7030819058418274, 'test/loss': 1.525663137435913, 'test/bleu': 30.104904671786333, 'test/num_examples': 3003, 'score': 27746.23920059204, 'total_duration': 46217.66652059555, 'accumulated_submission_time': 27746.23920059204, 'accumulated_eval_time': 18466.527469158173, 'accumulated_logging_time': 0.6828269958496094}
I0306 08:03:45.924546 139870634698496 logging_writer.py:48] [80043] accumulated_eval_time=18466.5, accumulated_logging_time=0.682827, accumulated_submission_time=27746.2, global_step=80043, preemption_count=0, score=27746.2, test/accuracy=0.703082, test/bleu=30.1049, test/loss=1.52566, test/num_examples=3003, total_duration=46217.7, train/accuracy=0.686467, train/bleu=34.7456, train/loss=1.64555, validation/accuracy=0.6876, validation/bleu=30.5088, validation/loss=1.61947, validation/num_examples=3000
I0306 08:04:05.982132 139870626305792 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.299187570810318, loss=3.785843849182129
I0306 08:04:40.617898 139870634698496 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3206574022769928, loss=3.7991409301757812
I0306 08:05:15.280192 139870626305792 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2961753308773041, loss=3.751666307449341
I0306 08:05:50.002754 139870634698496 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.29144999384880066, loss=3.7514824867248535
I0306 08:06:24.672140 139870626305792 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3122701942920685, loss=3.7934443950653076
I0306 08:06:59.398431 139870634698496 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.28716805577278137, loss=3.7110519409179688
I0306 08:07:34.104231 139870626305792 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.29316166043281555, loss=3.676806688308716
I0306 08:08:08.814091 139870634698496 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.29361236095428467, loss=3.7733423709869385
I0306 08:08:43.546057 139870626305792 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2965856194496155, loss=3.6992037296295166
I0306 08:09:18.282218 139870634698496 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3029816746711731, loss=3.8325765132904053
I0306 08:09:53.031843 139870626305792 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.29740986227989197, loss=3.735419273376465
I0306 08:10:27.801936 139870634698496 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.3125378489494324, loss=3.7116425037384033
I0306 08:11:02.597397 139870626305792 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.31084853410720825, loss=3.7809054851531982
I0306 08:11:37.339770 139870634698496 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.2986411154270172, loss=3.7573814392089844
I0306 08:12:12.097265 139870626305792 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.309370756149292, loss=3.7226598262786865
I0306 08:12:46.777389 139870634698496 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.30448147654533386, loss=3.7088217735290527
I0306 08:13:21.436588 139870626305792 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.3142252266407013, loss=3.7050912380218506
I0306 08:13:56.085876 139870634698496 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.30200424790382385, loss=3.707178831100464
I0306 08:14:30.757580 139870626305792 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.3044242560863495, loss=3.6925103664398193
I0306 08:15:05.398334 139870634698496 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3065949082374573, loss=3.7659990787506104
I0306 08:15:40.020297 139870626305792 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2909817397594452, loss=3.714272975921631
I0306 08:16:14.679328 139870634698496 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.32780781388282776, loss=3.7924604415893555
I0306 08:16:49.330565 139870626305792 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.29400599002838135, loss=3.7318291664123535
I0306 08:17:23.979520 139870634698496 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.3092613220214844, loss=3.7362420558929443
I0306 08:17:46.162875 140014092387520 spec.py:321] Evaluating on the training split.
I0306 08:17:48.769001 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:21:00.291826 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 08:21:02.880710 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:23:51.286196 140014092387520 spec.py:349] Evaluating on the test split.
I0306 08:23:53.879516 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:26:34.081793 140014092387520 submission_runner.py:469] Time since start: 47585.84s, 	Step: 82465, 	{'train/accuracy': 0.7023534774780273, 'train/loss': 1.5521113872528076, 'train/bleu': 35.83263091139065, 'validation/accuracy': 0.68921959400177, 'validation/loss': 1.6090657711029053, 'validation/bleu': 30.322149991395488, 'validation/num_examples': 3000, 'test/accuracy': 0.7057815194129944, 'test/loss': 1.5156477689743042, 'test/bleu': 30.52575065164402, 'test/num_examples': 3003, 'score': 28586.344668626785, 'total_duration': 47585.840898275375, 'accumulated_submission_time': 28586.344668626785, 'accumulated_eval_time': 18994.446326494217, 'accumulated_logging_time': 0.707528829574585}
I0306 08:26:34.099254 139870626305792 logging_writer.py:48] [82465] accumulated_eval_time=18994.4, accumulated_logging_time=0.707529, accumulated_submission_time=28586.3, global_step=82465, preemption_count=0, score=28586.3, test/accuracy=0.705782, test/bleu=30.5258, test/loss=1.51565, test/num_examples=3003, total_duration=47585.8, train/accuracy=0.702353, train/bleu=35.8326, train/loss=1.55211, validation/accuracy=0.68922, validation/bleu=30.3221, validation/loss=1.60907, validation/num_examples=3000
I0306 08:26:46.493522 139870634698496 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3019545376300812, loss=3.674405097961426
I0306 08:27:20.980692 139870626305792 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3041928708553314, loss=3.741300582885742
I0306 08:27:55.562636 139870634698496 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.31179115176200867, loss=3.768425226211548
I0306 08:28:30.189523 139870626305792 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.31380748748779297, loss=3.7413699626922607
I0306 08:29:04.805237 139870634698496 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.31361109018325806, loss=3.75009822845459
I0306 08:29:39.524537 139870626305792 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3006962537765503, loss=3.770653486251831
I0306 08:30:14.154394 139870634698496 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3119408190250397, loss=3.7505431175231934
I0306 08:30:48.764220 139870626305792 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.31306132674217224, loss=3.7346951961517334
I0306 08:31:23.404629 139870634698496 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.3213322162628174, loss=3.7311508655548096
I0306 08:31:58.008698 139870626305792 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.316241055727005, loss=3.767268180847168
I0306 08:32:32.651526 139870634698496 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.31288525462150574, loss=3.752756118774414
I0306 08:33:07.272043 139870626305792 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.3156659007072449, loss=3.738856554031372
I0306 08:33:41.898467 139870634698496 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.32446548342704773, loss=3.7970504760742188
I0306 08:34:16.515674 139870626305792 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.30901703238487244, loss=3.734684705734253
I0306 08:34:51.149975 139870634698496 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.327483594417572, loss=3.716576099395752
I0306 08:35:25.776909 139870626305792 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.30971309542655945, loss=3.721707820892334
I0306 08:36:00.389419 139870634698496 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.3035335838794708, loss=3.6974644660949707
I0306 08:36:35.028022 139870626305792 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.32061895728111267, loss=3.7469425201416016
I0306 08:37:09.661998 139870634698496 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.306234210729599, loss=3.6594748497009277
I0306 08:37:44.301634 139870626305792 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.31474509835243225, loss=3.686624765396118
I0306 08:38:18.928717 139870634698496 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.31196755170822144, loss=3.751149892807007
I0306 08:38:53.576354 139870626305792 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.3252544105052948, loss=3.7721011638641357
I0306 08:39:28.211764 139870634698496 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.30814605951309204, loss=3.64819598197937
I0306 08:40:02.828961 139870626305792 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.3115427494049072, loss=3.755768299102783
I0306 08:40:34.362266 140014092387520 spec.py:321] Evaluating on the training split.
I0306 08:40:36.971010 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:44:27.592389 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 08:44:30.189693 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:47:06.448085 140014092387520 spec.py:349] Evaluating on the test split.
I0306 08:47:09.045691 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 08:49:39.559925 140014092387520 submission_runner.py:469] Time since start: 48971.32s, 	Step: 84892, 	{'train/accuracy': 0.6925277709960938, 'train/loss': 1.6007498502731323, 'train/bleu': 35.20272484964355, 'validation/accuracy': 0.6891701817512512, 'validation/loss': 1.6060130596160889, 'validation/bleu': 30.57543923485164, 'validation/num_examples': 3000, 'test/accuracy': 0.7068706154823303, 'test/loss': 1.506286382675171, 'test/bleu': 30.824158321276325, 'test/num_examples': 3003, 'score': 29426.47685265541, 'total_duration': 48971.31904864311, 'accumulated_submission_time': 29426.47685265541, 'accumulated_eval_time': 19539.64394927025, 'accumulated_logging_time': 0.7331511974334717}
I0306 08:49:39.578419 139870634698496 logging_writer.py:48] [84892] accumulated_eval_time=19539.6, accumulated_logging_time=0.733151, accumulated_submission_time=29426.5, global_step=84892, preemption_count=0, score=29426.5, test/accuracy=0.706871, test/bleu=30.8242, test/loss=1.50629, test/num_examples=3003, total_duration=48971.3, train/accuracy=0.692528, train/bleu=35.2027, train/loss=1.60075, validation/accuracy=0.68917, validation/bleu=30.5754, validation/loss=1.60601, validation/num_examples=3000
I0306 08:49:42.687712 139870626305792 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.30726784467697144, loss=3.7000272274017334
I0306 08:50:17.174084 139870634698496 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.31662359833717346, loss=3.7342793941497803
I0306 08:50:51.731181 139870626305792 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.3178485035896301, loss=3.715548276901245
I0306 08:51:26.362904 139870634698496 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3013784885406494, loss=3.6986019611358643
I0306 08:52:01.009706 139870626305792 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.30080753564834595, loss=3.6923553943634033
I0306 08:52:35.634781 139870634698496 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.32438865303993225, loss=3.6796562671661377
I0306 08:53:10.253805 139870626305792 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.32590651512145996, loss=3.7084107398986816
I0306 08:53:44.935217 139870634698496 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.33922234177589417, loss=3.7556304931640625
I0306 08:54:19.578396 139870626305792 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.3177679181098938, loss=3.671745538711548
I0306 08:54:54.173497 139870634698496 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.31368035078048706, loss=3.651414632797241
I0306 08:55:28.759103 139870626305792 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.3317289352416992, loss=3.743616819381714
I0306 08:56:03.397683 139870634698496 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.32259729504585266, loss=3.6689717769622803
I0306 08:56:38.027613 139870626305792 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.318526029586792, loss=3.7596282958984375
I0306 08:57:12.681970 139870634698496 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.3128657341003418, loss=3.727076768875122
I0306 08:57:47.371209 139870626305792 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.312784880399704, loss=3.692396402359009
I0306 08:58:22.007238 139870634698496 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3124016523361206, loss=3.705475091934204
I0306 08:58:56.629891 139870626305792 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.32720842957496643, loss=3.7271382808685303
I0306 08:59:31.278095 139870634698496 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.3264724314212799, loss=3.682725429534912
I0306 09:00:05.912235 139870626305792 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3240949511528015, loss=3.71224308013916
I0306 09:00:40.554969 139870634698496 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3334946930408478, loss=3.680328607559204
I0306 09:01:15.201178 139870626305792 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.30833864212036133, loss=3.735027313232422
I0306 09:01:49.818423 139870634698496 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.31503838300704956, loss=3.6622376441955566
I0306 09:02:24.406612 139870626305792 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.34126415848731995, loss=3.679358959197998
I0306 09:02:59.050015 139870634698496 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3306884169578552, loss=3.7088067531585693
I0306 09:03:33.700029 139870626305792 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.33384737372398376, loss=3.7805445194244385
I0306 09:03:39.597256 140014092387520 spec.py:321] Evaluating on the training split.
I0306 09:03:42.210878 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:06:45.844653 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 09:06:48.427049 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:09:37.133686 140014092387520 spec.py:349] Evaluating on the test split.
I0306 09:09:39.726769 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:12:01.678434 140014092387520 submission_runner.py:469] Time since start: 50313.44s, 	Step: 87318, 	{'train/accuracy': 0.6939358115196228, 'train/loss': 1.6034772396087646, 'train/bleu': 35.35737138783944, 'validation/accuracy': 0.6900477409362793, 'validation/loss': 1.601252794265747, 'validation/bleu': 30.779533180808755, 'validation/num_examples': 3000, 'test/accuracy': 0.7064882516860962, 'test/loss': 1.5074666738510132, 'test/bleu': 30.53064323868315, 'test/num_examples': 3003, 'score': 30266.362763643265, 'total_duration': 50313.43754172325, 'accumulated_submission_time': 30266.362763643265, 'accumulated_eval_time': 20041.725078105927, 'accumulated_logging_time': 0.7603545188903809}
I0306 09:12:01.696163 139870634698496 logging_writer.py:48] [87318] accumulated_eval_time=20041.7, accumulated_logging_time=0.760355, accumulated_submission_time=30266.4, global_step=87318, preemption_count=0, score=30266.4, test/accuracy=0.706488, test/bleu=30.5306, test/loss=1.50747, test/num_examples=3003, total_duration=50313.4, train/accuracy=0.693936, train/bleu=35.3574, train/loss=1.60348, validation/accuracy=0.690048, validation/bleu=30.7795, validation/loss=1.60125, validation/num_examples=3000
I0306 09:12:30.346240 139870626305792 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3128179907798767, loss=3.683072566986084
I0306 09:13:04.927308 139870634698496 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.31988319754600525, loss=3.7496659755706787
I0306 09:13:39.489777 139870626305792 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.33657893538475037, loss=3.715867519378662
I0306 09:14:14.131388 139870634698496 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.31693756580352783, loss=3.683145046234131
I0306 09:14:48.703733 139870626305792 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.33748969435691833, loss=3.765827178955078
I0306 09:15:23.400894 139870634698496 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.31871652603149414, loss=3.725062131881714
I0306 09:15:58.089143 139870626305792 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.34612709283828735, loss=3.734736680984497
I0306 09:16:32.799453 139870634698496 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3408399522304535, loss=3.7580668926239014
I0306 09:17:07.501051 139870626305792 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3241288959980011, loss=3.714221954345703
I0306 09:17:42.200079 139870634698496 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.3459247946739197, loss=3.7524337768554688
I0306 09:18:16.896284 139870626305792 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.32947781682014465, loss=3.7098264694213867
I0306 09:18:51.592825 139870634698496 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.31875038146972656, loss=3.6566195487976074
I0306 09:19:26.313996 139870626305792 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3241957724094391, loss=3.695953369140625
I0306 09:20:00.989088 139870634698496 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.3390841484069824, loss=3.771692991256714
I0306 09:20:35.700273 139870626305792 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.353644460439682, loss=3.724386692047119
I0306 09:21:10.326733 139870634698496 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.32689306139945984, loss=3.704359292984009
I0306 09:21:44.957824 139870626305792 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3226127028465271, loss=3.670980930328369
I0306 09:22:19.619173 139870634698496 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.32787659764289856, loss=3.7063159942626953
I0306 09:22:54.271098 139870626305792 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.33673369884490967, loss=3.707921028137207
I0306 09:23:28.892165 139870634698496 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.33624720573425293, loss=3.6865038871765137
I0306 09:24:03.521602 139870626305792 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.35055652260780334, loss=3.6887292861938477
I0306 09:24:38.176692 139870634698496 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.3249763250350952, loss=3.6657676696777344
I0306 09:25:12.808078 139870626305792 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.3490656912326813, loss=3.690305233001709
I0306 09:25:47.446067 139870634698496 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.31501588225364685, loss=3.733302593231201
I0306 09:26:02.010893 140014092387520 spec.py:321] Evaluating on the training split.
I0306 09:26:04.607132 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:29:30.252474 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 09:29:32.836786 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:32:16.310740 140014092387520 spec.py:349] Evaluating on the test split.
I0306 09:32:18.896704 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:34:59.289387 140014092387520 submission_runner.py:469] Time since start: 51691.05s, 	Step: 89743, 	{'train/accuracy': 0.7043187022209167, 'train/loss': 1.5412328243255615, 'train/bleu': 35.74486855937609, 'validation/accuracy': 0.6905544996261597, 'validation/loss': 1.602715015411377, 'validation/bleu': 30.38012252055491, 'validation/num_examples': 3000, 'test/accuracy': 0.7081566452980042, 'test/loss': 1.502125859260559, 'test/bleu': 30.769510708107042, 'test/num_examples': 3003, 'score': 31106.542657852173, 'total_duration': 51691.04849886894, 'accumulated_submission_time': 31106.542657852173, 'accumulated_eval_time': 20579.003517866135, 'accumulated_logging_time': 0.7858483791351318}
I0306 09:34:59.306479 139870626305792 logging_writer.py:48] [89743] accumulated_eval_time=20579, accumulated_logging_time=0.785848, accumulated_submission_time=31106.5, global_step=89743, preemption_count=0, score=31106.5, test/accuracy=0.708157, test/bleu=30.7695, test/loss=1.50213, test/num_examples=3003, total_duration=51691, train/accuracy=0.704319, train/bleu=35.7449, train/loss=1.54123, validation/accuracy=0.690554, validation/bleu=30.3801, validation/loss=1.60272, validation/num_examples=3000
I0306 09:35:19.284730 139870634698496 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.34140127897262573, loss=3.7452120780944824
I0306 09:35:53.795331 139870626305792 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.3452281355857849, loss=3.7055158615112305
I0306 09:36:28.367202 139870634698496 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.34030720591545105, loss=3.644625663757324
I0306 09:37:02.997681 139870626305792 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.34482625126838684, loss=3.7579569816589355
I0306 09:37:37.617903 139870634698496 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.32129696011543274, loss=3.6643528938293457
I0306 09:38:12.219521 139870626305792 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.34765610098838806, loss=3.736091375350952
I0306 09:38:46.817422 139870634698496 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.33263567090034485, loss=3.6364338397979736
I0306 09:39:21.437791 139870626305792 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3477255403995514, loss=3.6766180992126465
I0306 09:39:56.036860 139870634698496 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.3342823088169098, loss=3.6965439319610596
I0306 09:40:30.599228 139870626305792 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.3313283324241638, loss=3.6895229816436768
I0306 09:41:05.183068 139870634698496 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3473827838897705, loss=3.706380844116211
I0306 09:41:39.757066 139870626305792 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.33435678482055664, loss=3.6546008586883545
I0306 09:42:14.368205 139870634698496 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.34255287051200867, loss=3.722625970840454
I0306 09:42:49.003874 139870626305792 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.32440194487571716, loss=3.6667885780334473
I0306 09:43:23.614118 139870634698496 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3482370376586914, loss=3.6649742126464844
I0306 09:43:58.236975 139870626305792 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.3240358829498291, loss=3.713597297668457
I0306 09:44:32.859334 139870634698496 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.33878612518310547, loss=3.7054848670959473
I0306 09:45:07.485072 139870626305792 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.32358452677726746, loss=3.6728992462158203
I0306 09:45:42.083027 139870634698496 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.35309872031211853, loss=3.703911542892456
I0306 09:46:16.729452 139870626305792 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.3231101930141449, loss=3.6976206302642822
I0306 09:46:51.364437 139870634698496 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.32578539848327637, loss=3.726073980331421
I0306 09:47:26.050493 139870626305792 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.336376816034317, loss=3.663787603378296
I0306 09:48:00.677076 139870634698496 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3404392898082733, loss=3.6856026649475098
I0306 09:48:35.262019 139870626305792 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3449271321296692, loss=3.6993091106414795
I0306 09:48:59.506990 140014092387520 spec.py:321] Evaluating on the training split.
I0306 09:49:02.101663 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:52:01.363133 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 09:52:03.956551 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:54:41.998912 140014092387520 spec.py:349] Evaluating on the test split.
I0306 09:54:44.597227 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 09:57:19.547225 140014092387520 submission_runner.py:469] Time since start: 53031.31s, 	Step: 92171, 	{'train/accuracy': 0.6977658867835999, 'train/loss': 1.5735212564468384, 'train/bleu': 35.42978390408443, 'validation/accuracy': 0.6913579106330872, 'validation/loss': 1.6029143333435059, 'validation/bleu': 30.57063039128615, 'validation/num_examples': 3000, 'test/accuracy': 0.707194983959198, 'test/loss': 1.5036488771438599, 'test/bleu': 30.802846268462485, 'test/num_examples': 3003, 'score': 31946.60865879059, 'total_duration': 53031.30634570122, 'accumulated_submission_time': 31946.60865879059, 'accumulated_eval_time': 21079.04371213913, 'accumulated_logging_time': 0.8108994960784912}
I0306 09:57:19.564786 139870634698496 logging_writer.py:48] [92171] accumulated_eval_time=21079, accumulated_logging_time=0.810899, accumulated_submission_time=31946.6, global_step=92171, preemption_count=0, score=31946.6, test/accuracy=0.707195, test/bleu=30.8028, test/loss=1.50365, test/num_examples=3003, total_duration=53031.3, train/accuracy=0.697766, train/bleu=35.4298, train/loss=1.57352, validation/accuracy=0.691358, validation/bleu=30.5706, validation/loss=1.60291, validation/num_examples=3000
I0306 09:57:29.906771 139870626305792 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3336458206176758, loss=3.6868832111358643
I0306 09:58:04.374657 139870634698496 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3428554832935333, loss=3.7578253746032715
I0306 09:58:38.909689 139870626305792 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.33993327617645264, loss=3.676393747329712
I0306 09:59:13.490252 139870634698496 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.3392348289489746, loss=3.662757635116577
I0306 09:59:48.099160 139870626305792 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.3241519629955292, loss=3.668107271194458
I0306 10:00:22.735412 139870634698496 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.3489302098751068, loss=3.6976516246795654
I0306 10:00:57.332283 139870626305792 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.321486234664917, loss=3.6410231590270996
I0306 10:01:31.939340 139870634698496 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.34175312519073486, loss=3.6748592853546143
I0306 10:02:06.558284 139870626305792 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3397567570209503, loss=3.694648027420044
I0306 10:02:41.137187 139870634698496 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.3214441239833832, loss=3.693142890930176
I0306 10:03:15.754615 139870626305792 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.32606565952301025, loss=3.60634708404541
I0306 10:03:50.387585 139870634698496 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3363204896450043, loss=3.6487205028533936
I0306 10:04:25.005371 139870626305792 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.35598647594451904, loss=3.6742050647735596
I0306 10:04:59.638763 139870634698496 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3294674754142761, loss=3.669647216796875
I0306 10:05:34.268640 139870626305792 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3652605414390564, loss=3.691288948059082
I0306 10:06:08.914182 139870634698496 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.3404214382171631, loss=3.6468729972839355
I0306 10:06:43.522853 139870626305792 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.3346792161464691, loss=3.6653964519500732
I0306 10:07:18.171774 139870634698496 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3455812335014343, loss=3.6699278354644775
I0306 10:07:52.789315 139870626305792 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.33312830328941345, loss=3.6811957359313965
I0306 10:08:27.410138 139870634698496 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.33257341384887695, loss=3.6541974544525146
I0306 10:09:02.060412 139870626305792 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3314684331417084, loss=3.7085628509521484
I0306 10:09:36.683597 139870634698496 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.35692939162254333, loss=3.7091426849365234
I0306 10:10:11.300580 139870626305792 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.34132975339889526, loss=3.639464855194092
I0306 10:10:45.899127 139870634698496 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3277287185192108, loss=3.636579990386963
I0306 10:11:19.847555 140014092387520 spec.py:321] Evaluating on the training split.
I0306 10:11:22.445367 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 10:14:40.654987 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 10:14:43.241917 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 10:17:21.760527 140014092387520 spec.py:349] Evaluating on the test split.
I0306 10:17:24.353539 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 10:20:17.213010 140014092387520 submission_runner.py:469] Time since start: 54408.97s, 	Step: 94599, 	{'train/accuracy': 0.7145909667015076, 'train/loss': 1.4945625066757202, 'train/bleu': 37.31204285384356, 'validation/accuracy': 0.6914814710617065, 'validation/loss': 1.5995135307312012, 'validation/bleu': 30.663545155943098, 'validation/num_examples': 3000, 'test/accuracy': 0.7078554034233093, 'test/loss': 1.5004310607910156, 'test/bleu': 30.582849364244517, 'test/num_examples': 3003, 'score': 32786.758513212204, 'total_duration': 54408.9720761776, 'accumulated_submission_time': 32786.758513212204, 'accumulated_eval_time': 21616.409068584442, 'accumulated_logging_time': 0.8370471000671387}
I0306 10:20:17.234569 139870626305792 logging_writer.py:48] [94599] accumulated_eval_time=21616.4, accumulated_logging_time=0.837047, accumulated_submission_time=32786.8, global_step=94599, preemption_count=0, score=32786.8, test/accuracy=0.707855, test/bleu=30.5828, test/loss=1.50043, test/num_examples=3003, total_duration=54409, train/accuracy=0.714591, train/bleu=37.312, train/loss=1.49456, validation/accuracy=0.691481, validation/bleu=30.6635, validation/loss=1.59951, validation/num_examples=3000
I0306 10:20:17.933714 139870634698496 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.34225183725357056, loss=3.6946167945861816
I0306 10:20:52.379004 139870626305792 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.36329385638237, loss=3.6741552352905273
I0306 10:21:26.947627 139870634698496 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.33530622720718384, loss=3.6391689777374268
I0306 10:22:01.543345 139870626305792 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.33254292607307434, loss=3.6366405487060547
I0306 10:22:36.125195 139870634698496 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3713104724884033, loss=3.7477223873138428
I0306 10:23:10.708231 139870626305792 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3379228115081787, loss=3.6718595027923584
I0306 10:23:45.421879 139870634698496 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3626101315021515, loss=3.6788711547851562
I0306 10:24:20.046884 139870626305792 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3544829189777374, loss=3.6252574920654297
I0306 10:24:54.646260 139870634698496 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3637215495109558, loss=3.6935620307922363
I0306 10:25:29.251200 139870626305792 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.32841843366622925, loss=3.6985573768615723
I0306 10:26:03.897265 139870634698496 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3507848381996155, loss=3.7022578716278076
I0306 10:26:38.526561 139870626305792 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.34533342719078064, loss=3.654393434524536
I0306 10:27:13.168048 139870634698496 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3642972409725189, loss=3.7069766521453857
I0306 10:27:47.796990 139870626305792 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3350408375263214, loss=3.7190663814544678
I0306 10:28:22.436736 139870634698496 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3353309631347656, loss=3.647031545639038
I0306 10:28:57.067283 139870626305792 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3488690257072449, loss=3.628004789352417
I0306 10:29:31.678724 139870634698496 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.3446587026119232, loss=3.6545028686523438
I0306 10:30:06.304693 139870626305792 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.37513938546180725, loss=3.6953635215759277
I0306 10:30:40.942178 139870634698496 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3379420340061188, loss=3.6368601322174072
I0306 10:31:15.562598 139870626305792 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3595570921897888, loss=3.677030324935913
I0306 10:31:50.216515 139870634698496 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.3466617465019226, loss=3.63838529586792
I0306 10:32:24.865844 139870626305792 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.33158907294273376, loss=3.6499149799346924
I0306 10:32:59.516629 139870634698496 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3500170409679413, loss=3.698681592941284
I0306 10:33:34.139935 139870626305792 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3499814569950104, loss=3.655057907104492
I0306 10:34:08.800469 139870634698496 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3740009367465973, loss=3.639049530029297
I0306 10:34:17.483772 140014092387520 spec.py:321] Evaluating on the training split.
I0306 10:34:20.097000 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 10:37:23.988891 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 10:37:26.571583 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 10:39:58.129959 140014092387520 spec.py:349] Evaluating on the test split.
I0306 10:40:00.727849 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 10:42:29.155901 140014092387520 submission_runner.py:469] Time since start: 55740.92s, 	Step: 97026, 	{'train/accuracy': 0.7064414024353027, 'train/loss': 1.5336871147155762, 'train/bleu': 36.26825426119099, 'validation/accuracy': 0.6917410492897034, 'validation/loss': 1.6006312370300293, 'validation/bleu': 30.576524362952135, 'validation/num_examples': 3000, 'test/accuracy': 0.7077511548995972, 'test/loss': 1.498480200767517, 'test/bleu': 30.46168828627666, 'test/num_examples': 3003, 'score': 33626.87296843529, 'total_duration': 55740.91502404213, 'accumulated_submission_time': 33626.87296843529, 'accumulated_eval_time': 22108.081154584885, 'accumulated_logging_time': 0.8669099807739258}
I0306 10:42:29.173885 139870626305792 logging_writer.py:48] [97026] accumulated_eval_time=22108.1, accumulated_logging_time=0.86691, accumulated_submission_time=33626.9, global_step=97026, preemption_count=0, score=33626.9, test/accuracy=0.707751, test/bleu=30.4617, test/loss=1.49848, test/num_examples=3003, total_duration=55740.9, train/accuracy=0.706441, train/bleu=36.2683, train/loss=1.53369, validation/accuracy=0.691741, validation/bleu=30.5765, validation/loss=1.60063, validation/num_examples=3000
I0306 10:42:55.026233 139870634698496 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.33370137214660645, loss=3.683222770690918
I0306 10:43:29.588823 139870626305792 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3527913987636566, loss=3.6604833602905273
I0306 10:44:04.166013 139870634698496 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.34135469794273376, loss=3.6930792331695557
I0306 10:44:38.797219 139870626305792 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3718455135822296, loss=3.6850759983062744
I0306 10:45:13.434233 139870634698496 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.35764700174331665, loss=3.668489933013916
I0306 10:45:48.064410 139870626305792 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.33899521827697754, loss=3.6476423740386963
I0306 10:46:22.705974 139870634698496 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.34114301204681396, loss=3.6767547130584717
I0306 10:46:57.354225 139870626305792 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.36264029145240784, loss=3.6707613468170166
I0306 10:47:31.977825 139870634698496 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.36155954003334045, loss=3.6210126876831055
I0306 10:48:06.575320 139870626305792 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.38666853308677673, loss=3.647076368331909
I0306 10:48:41.172759 139870634698496 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3518177568912506, loss=3.6596007347106934
I0306 10:49:15.796410 139870626305792 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.36226746439933777, loss=3.696465015411377
I0306 10:49:50.441842 139870634698496 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.3476064205169678, loss=3.672403335571289
I0306 10:50:25.067689 139870626305792 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.355544775724411, loss=3.6689186096191406
I0306 10:50:59.725535 139870634698496 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3579242527484894, loss=3.645434617996216
I0306 10:51:34.368579 139870626305792 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.36322927474975586, loss=3.6879475116729736
I0306 10:52:09.015236 139870634698496 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.33379697799682617, loss=3.687044620513916
I0306 10:52:43.642012 139870626305792 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.35186755657196045, loss=3.702599048614502
I0306 10:53:18.271591 139870634698496 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.36632242798805237, loss=3.671963691711426
I0306 10:53:52.901418 139870626305792 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3984428346157074, loss=3.6637539863586426
I0306 10:54:27.550813 139870634698496 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.3711279332637787, loss=3.683107852935791
I0306 10:55:02.188858 139870626305792 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.34367477893829346, loss=3.6149685382843018
I0306 10:55:36.805118 139870634698496 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3670894503593445, loss=3.6740341186523438
I0306 10:56:11.436074 139870626305792 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.36145347356796265, loss=3.684849977493286
I0306 10:56:29.453015 140014092387520 spec.py:321] Evaluating on the training split.
I0306 10:56:32.050837 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 10:59:32.039892 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 10:59:34.637647 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:02:07.330895 140014092387520 spec.py:349] Evaluating on the test split.
I0306 11:02:09.917005 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:04:43.250455 140014092387520 submission_runner.py:469] Time since start: 57075.01s, 	Step: 99453, 	{'train/accuracy': 0.706396222114563, 'train/loss': 1.5299201011657715, 'train/bleu': 35.85814231475738, 'validation/accuracy': 0.6916669011116028, 'validation/loss': 1.5949556827545166, 'validation/bleu': 30.507035768249672, 'validation/num_examples': 3000, 'test/accuracy': 0.7090488076210022, 'test/loss': 1.4936065673828125, 'test/bleu': 30.649132659897187, 'test/num_examples': 3003, 'score': 34467.01798629761, 'total_duration': 57075.00957465172, 'accumulated_submission_time': 34467.01798629761, 'accumulated_eval_time': 22601.87854719162, 'accumulated_logging_time': 0.8926870822906494}
I0306 11:04:43.268556 139870634698496 logging_writer.py:48] [99453] accumulated_eval_time=22601.9, accumulated_logging_time=0.892687, accumulated_submission_time=34467, global_step=99453, preemption_count=0, score=34467, test/accuracy=0.709049, test/bleu=30.6491, test/loss=1.49361, test/num_examples=3003, total_duration=57075, train/accuracy=0.706396, train/bleu=35.8581, train/loss=1.52992, validation/accuracy=0.691667, validation/bleu=30.507, validation/loss=1.59496, validation/num_examples=3000
I0306 11:04:59.818778 139870626305792 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.35757264494895935, loss=3.641897678375244
I0306 11:05:34.398813 139870634698496 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.36177825927734375, loss=3.603067636489868
I0306 11:06:09.013279 139870626305792 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.34926944971084595, loss=3.6520464420318604
I0306 11:06:43.592357 139870634698496 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.38788318634033203, loss=3.6657204627990723
I0306 11:07:18.192977 139870626305792 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.3524487316608429, loss=3.620725154876709
I0306 11:07:52.821478 139870634698496 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.35376057028770447, loss=3.6069562435150146
I0306 11:08:27.436219 139870626305792 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.3423868417739868, loss=3.6307871341705322
I0306 11:09:02.048911 139870634698496 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.35889923572540283, loss=3.666882276535034
I0306 11:09:36.649809 139870626305792 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.351283460855484, loss=3.626368522644043
I0306 11:10:11.205802 139870634698496 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.37563997507095337, loss=3.618191957473755
I0306 11:10:45.771219 139870626305792 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3648785948753357, loss=3.6523208618164062
I0306 11:11:20.313784 139870634698496 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3689256012439728, loss=3.687335968017578
I0306 11:11:54.857176 139870626305792 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.35809609293937683, loss=3.675224781036377
I0306 11:12:29.423049 139870634698496 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.35078445076942444, loss=3.6665585041046143
I0306 11:13:03.982833 139870626305792 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.3523871600627899, loss=3.6850452423095703
I0306 11:13:38.506551 139870634698496 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3475010097026825, loss=3.6354684829711914
I0306 11:14:13.074908 139870626305792 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3543733060359955, loss=3.6483988761901855
I0306 11:14:47.658692 139870634698496 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.3729071021080017, loss=3.6892285346984863
I0306 11:15:22.212457 139870626305792 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.3675786852836609, loss=3.6177494525909424
I0306 11:15:56.747452 139870634698496 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.34897565841674805, loss=3.643695831298828
I0306 11:16:31.276528 139870626305792 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.37767425179481506, loss=3.629318952560425
I0306 11:17:05.827494 139870634698496 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.36699622869491577, loss=3.6763432025909424
I0306 11:17:40.365889 139870626305792 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.370439350605011, loss=3.635125160217285
I0306 11:18:14.905478 139870634698496 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.3801307678222656, loss=3.64359188079834
I0306 11:18:43.560669 140014092387520 spec.py:321] Evaluating on the training split.
I0306 11:18:46.156823 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:22:07.542608 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 11:22:10.124736 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:25:31.533237 140014092387520 spec.py:349] Evaluating on the test split.
I0306 11:25:34.121853 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:28:48.053470 140014092387520 submission_runner.py:469] Time since start: 58519.81s, 	Step: 101884, 	{'train/accuracy': 0.7180216908454895, 'train/loss': 1.465099573135376, 'train/bleu': 36.908673009059676, 'validation/accuracy': 0.692284882068634, 'validation/loss': 1.5957337617874146, 'validation/bleu': 30.676344627889627, 'validation/num_examples': 3000, 'test/accuracy': 0.7088981866836548, 'test/loss': 1.4944958686828613, 'test/bleu': 30.507825230719238, 'test/num_examples': 3003, 'score': 35307.1790702343, 'total_duration': 58519.812579631805, 'accumulated_submission_time': 35307.1790702343, 'accumulated_eval_time': 23206.371296405792, 'accumulated_logging_time': 0.9188277721405029}
I0306 11:28:48.072950 139870626305792 logging_writer.py:48] [101884] accumulated_eval_time=23206.4, accumulated_logging_time=0.918828, accumulated_submission_time=35307.2, global_step=101884, preemption_count=0, score=35307.2, test/accuracy=0.708898, test/bleu=30.5078, test/loss=1.4945, test/num_examples=3003, total_duration=58519.8, train/accuracy=0.718022, train/bleu=36.9087, train/loss=1.4651, validation/accuracy=0.692285, validation/bleu=30.6763, validation/loss=1.59573, validation/num_examples=3000
I0306 11:28:53.919852 139870634698496 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.379631370306015, loss=3.6642167568206787
I0306 11:29:28.363386 139870626305792 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3518177568912506, loss=3.668321132659912
I0306 11:30:02.871604 139870634698496 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.355795294046402, loss=3.6619222164154053
I0306 11:30:37.388338 139870626305792 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3528349697589874, loss=3.625471830368042
I0306 11:31:11.939755 139870634698496 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.35159754753112793, loss=3.6619536876678467
I0306 11:31:46.474330 139870626305792 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.3641262650489807, loss=3.623445510864258
I0306 11:32:21.020366 139870634698496 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.37698185443878174, loss=3.6488149166107178
I0306 11:32:55.576215 139870626305792 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3618012070655823, loss=3.6553032398223877
I0306 11:33:30.124407 139870634698496 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.36711958050727844, loss=3.651952028274536
I0306 11:34:04.677934 139870626305792 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3712628483772278, loss=3.6047582626342773
I0306 11:34:39.182486 139870634698496 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3568897843360901, loss=3.6798789501190186
I0306 11:35:13.749066 139870626305792 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3757471740245819, loss=3.6402344703674316
I0306 11:35:48.292029 139870634698496 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.3835854232311249, loss=3.657907724380493
I0306 11:36:22.817582 139870626305792 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.36497920751571655, loss=3.6140754222869873
I0306 11:36:57.349313 139870634698496 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.37890350818634033, loss=3.617223024368286
I0306 11:37:31.901708 139870626305792 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.37380218505859375, loss=3.6478583812713623
I0306 11:38:06.436945 139870634698496 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3515739440917969, loss=3.589080810546875
I0306 11:38:40.962282 139870626305792 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3858972191810608, loss=3.6605398654937744
I0306 11:39:15.514915 139870634698496 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3757689297199249, loss=3.6849474906921387
I0306 11:39:50.041824 139870626305792 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.364266037940979, loss=3.635784864425659
I0306 11:40:24.576733 139870634698496 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3677161931991577, loss=3.5990099906921387
I0306 11:40:59.109462 139870626305792 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3563231825828552, loss=3.646428346633911
I0306 11:41:33.664359 139870634698496 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.37931376695632935, loss=3.646710157394409
I0306 11:42:08.253455 139870626305792 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3688422739505768, loss=3.6123294830322266
I0306 11:42:42.798577 139870634698496 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3625715672969818, loss=3.6297965049743652
I0306 11:42:48.321511 140014092387520 spec.py:321] Evaluating on the training split.
I0306 11:42:50.923922 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:45:49.787898 140014092387520 spec.py:333] Evaluating on the validation split.
I0306 11:45:52.369232 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:48:24.839926 140014092387520 spec.py:349] Evaluating on the test split.
I0306 11:48:27.432907 140014092387520 workload.py:181] Translating evaluation dataset.
I0306 11:51:06.371764 140014092387520 submission_runner.py:469] Time since start: 59858.13s, 	Step: 104317, 	{'train/accuracy': 0.7137287855148315, 'train/loss': 1.490756869316101, 'train/bleu': 36.84923538207688, 'validation/accuracy': 0.6928040385246277, 'validation/loss': 1.5952367782592773, 'validation/bleu': 30.84917590799016, 'validation/num_examples': 3000, 'test/accuracy': 0.7096860408782959, 'test/loss': 1.4918954372406006, 'test/bleu': 30.713737057467963, 'test/num_examples': 3003, 'score': 36147.29384422302, 'total_duration': 59858.13088798523, 'accumulated_submission_time': 36147.29384422302, 'accumulated_eval_time': 23704.42150592804, 'accumulated_logging_time': 0.9460732936859131}
I0306 11:51:06.390484 139870626305792 logging_writer.py:48] [104317] accumulated_eval_time=23704.4, accumulated_logging_time=0.946073, accumulated_submission_time=36147.3, global_step=104317, preemption_count=0, score=36147.3, test/accuracy=0.709686, test/bleu=30.7137, test/loss=1.4919, test/num_examples=3003, total_duration=59858.1, train/accuracy=0.713729, train/bleu=36.8492, train/loss=1.49076, validation/accuracy=0.692804, validation/bleu=30.8492, validation/loss=1.59524, validation/num_examples=3000
I0306 11:51:06.411084 139870634698496 logging_writer.py:48] [104317] global_step=104317, preemption_count=0, score=36147.3
I0306 11:51:06.434648 140014092387520 submission_runner.py:646] Tuning trial 2/5
I0306 11:51:06.434762 140014092387520 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0306 11:51:06.437637 140014092387520 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000524216506164521, 'train/loss': 11.01574420928955, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.013504981994629, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.02798843383789, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.166481494903564, 'total_duration': 934.3073287010193, 'accumulated_submission_time': 25.166481494903564, 'accumulated_eval_time': 909.140741109848, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2424, {'train/accuracy': 0.4466957449913025, 'train/loss': 3.7516324520111084, 'train/bleu': 16.690424241742072, 'validation/accuracy': 0.43770548701286316, 'validation/loss': 3.831123113632202, 'validation/bleu': 12.14541629922039, 'validation/num_examples': 3000, 'test/accuracy': 0.4279457926750183, 'test/loss': 3.985069751739502, 'test/bleu': 10.687958103130974, 'test/num_examples': 3003, 'score': 865.1917860507965, 'total_duration': 2345.732857942581, 'accumulated_submission_time': 865.1917860507965, 'accumulated_eval_time': 1480.3771858215332, 'accumulated_logging_time': 0.015112876892089844, 'global_step': 2424, 'preemption_count': 0}), (4847, {'train/accuracy': 0.5477670431137085, 'train/loss': 2.785271644592285, 'train/bleu': 24.105470152832527, 'validation/accuracy': 0.5556077361106873, 'validation/loss': 2.712578296661377, 'validation/bleu': 20.44556085971932, 'validation/num_examples': 3000, 'test/accuracy': 0.5558452010154724, 'test/loss': 2.726165771484375, 'test/bleu': 18.905257571089965, 'test/num_examples': 3003, 'score': 1705.0442910194397, 'total_duration': 3677.563007593155, 'accumulated_submission_time': 1705.0442910194397, 'accumulated_eval_time': 1972.1919617652893, 'accumulated_logging_time': 0.03391599655151367, 'global_step': 4847, 'preemption_count': 0}), (7272, {'train/accuracy': 0.5899104475975037, 'train/loss': 2.3937723636627197, 'train/bleu': 27.11121135665028, 'validation/accuracy': 0.5968531370162964, 'validation/loss': 2.344010591506958, 'validation/bleu': 23.33131986550044, 'validation/num_examples': 3000, 'test/accuracy': 0.5992701053619385, 'test/loss': 2.327810764312744, 'test/bleu': 21.932223744420213, 'test/num_examples': 3003, 'score': 2544.989545583725, 'total_duration': 4997.430301189423, 'accumulated_submission_time': 2544.989545583725, 'accumulated_eval_time': 2451.9548530578613, 'accumulated_logging_time': 0.05225253105163574, 'global_step': 7272, 'preemption_count': 0}), (9698, {'train/accuracy': 0.6061737537384033, 'train/loss': 2.2678310871124268, 'train/bleu': 28.560014196608766, 'validation/accuracy': 0.6206215023994446, 'validation/loss': 2.154343843460083, 'validation/bleu': 25.446157752592445, 'validation/num_examples': 3000, 'test/accuracy': 0.6274127960205078, 'test/loss': 2.1260712146759033, 'test/bleu': 24.651664319412692, 'test/num_examples': 3003, 'score': 3385.091551542282, 'total_duration': 6324.688243150711, 'accumulated_submission_time': 3385.091551542282, 'accumulated_eval_time': 2938.954449892044, 'accumulated_logging_time': 0.0688619613647461, 'global_step': 9698, 'preemption_count': 0}), (12125, {'train/accuracy': 0.615370512008667, 'train/loss': 2.1761953830718994, 'train/bleu': 29.589751985078458, 'validation/accuracy': 0.6325488686561584, 'validation/loss': 2.038947105407715, 'validation/bleu': 26.258069877797816, 'validation/num_examples': 3000, 'test/accuracy': 0.6422083377838135, 'test/loss': 1.9878355264663696, 'test/bleu': 25.681203334622175, 'test/num_examples': 3003, 'score': 4225.092828273773, 'total_duration': 7655.726309537888, 'accumulated_submission_time': 4225.092828273773, 'accumulated_eval_time': 3429.8383464813232, 'accumulated_logging_time': 0.08607339859008789, 'global_step': 12125, 'preemption_count': 0}), (14552, {'train/accuracy': 0.6277811527252197, 'train/loss': 2.080852746963501, 'train/bleu': 30.269983873014414, 'validation/accuracy': 0.64398193359375, 'validation/loss': 1.9654031991958618, 'validation/bleu': 27.25049033763671, 'validation/num_examples': 3000, 'test/accuracy': 0.6517437100410461, 'test/loss': 1.916274905204773, 'test/bleu': 26.19114597247215, 'test/num_examples': 3003, 'score': 5065.029271602631, 'total_duration': 8971.987629652023, 'accumulated_submission_time': 5065.029271602631, 'accumulated_eval_time': 3906.0160126686096, 'accumulated_logging_time': 0.10339474678039551, 'global_step': 14552, 'preemption_count': 0}), (16979, {'train/accuracy': 0.6309562921524048, 'train/loss': 2.0344173908233643, 'train/bleu': 30.237332038175808, 'validation/accuracy': 0.650545060634613, 'validation/loss': 1.9043089151382446, 'validation/bleu': 27.329813303394587, 'validation/num_examples': 3000, 'test/accuracy': 0.6597844958305359, 'test/loss': 1.8375310897827148, 'test/bleu': 27.03137742347074, 'test/num_examples': 3003, 'score': 5905.220097780228, 'total_duration': 10370.005269289017, 'accumulated_submission_time': 5905.220097780228, 'accumulated_eval_time': 4463.696427345276, 'accumulated_logging_time': 0.12131905555725098, 'global_step': 16979, 'preemption_count': 0}), (19406, {'train/accuracy': 0.6474028825759888, 'train/loss': 1.9145114421844482, 'train/bleu': 31.30485040851925, 'validation/accuracy': 0.6537092328071594, 'validation/loss': 1.868304967880249, 'validation/bleu': 27.904660301480273, 'validation/num_examples': 3000, 'test/accuracy': 0.6646391153335571, 'test/loss': 1.8058494329452515, 'test/bleu': 27.24284537224183, 'test/num_examples': 3003, 'score': 6745.396078109741, 'total_duration': 11703.975671052933, 'accumulated_submission_time': 6745.396078109741, 'accumulated_eval_time': 4957.341809988022, 'accumulated_logging_time': 0.1385819911956787, 'global_step': 19406, 'preemption_count': 0}), (21830, {'train/accuracy': 0.6396872401237488, 'train/loss': 1.979859709739685, 'train/bleu': 31.333566441154108, 'validation/accuracy': 0.6577138900756836, 'validation/loss': 1.8474589586257935, 'validation/bleu': 27.84406544527811, 'validation/num_examples': 3000, 'test/accuracy': 0.6688332557678223, 'test/loss': 1.7815340757369995, 'test/bleu': 27.97795885713411, 'test/num_examples': 3003, 'score': 7585.456572294235, 'total_duration': 13048.188078641891, 'accumulated_submission_time': 7585.456572294235, 'accumulated_eval_time': 5461.345185756683, 'accumulated_logging_time': 0.15760207176208496, 'global_step': 21830, 'preemption_count': 0}), (24255, {'train/accuracy': 0.6438353061676025, 'train/loss': 1.9500995874404907, 'train/bleu': 31.284471929812018, 'validation/accuracy': 0.6619657278060913, 'validation/loss': 1.807058572769165, 'validation/bleu': 28.629246201900123, 'validation/num_examples': 3000, 'test/accuracy': 0.6737689971923828, 'test/loss': 1.7389591932296753, 'test/bleu': 27.779371335703487, 'test/num_examples': 3003, 'score': 8425.654890537262, 'total_duration': 14435.025680303574, 'accumulated_submission_time': 8425.654890537262, 'accumulated_eval_time': 6007.83956694603, 'accumulated_logging_time': 0.1757657527923584, 'global_step': 24255, 'preemption_count': 0}), (26675, {'train/accuracy': 0.6491510272026062, 'train/loss': 1.903560996055603, 'train/bleu': 32.3970726365744, 'validation/accuracy': 0.6647220253944397, 'validation/loss': 1.8025007247924805, 'validation/bleu': 28.72615130128667, 'validation/num_examples': 3000, 'test/accuracy': 0.6767929792404175, 'test/loss': 1.7335381507873535, 'test/bleu': 28.323045010307812, 'test/num_examples': 3003, 'score': 9265.544337034225, 'total_duration': 15769.00270652771, 'accumulated_submission_time': 9265.544337034225, 'accumulated_eval_time': 6501.78210735321, 'accumulated_logging_time': 0.19534540176391602, 'global_step': 26675, 'preemption_count': 0}), (29092, {'train/accuracy': 0.6483597755432129, 'train/loss': 1.908276915550232, 'train/bleu': 31.264425413605853, 'validation/accuracy': 0.6656613945960999, 'validation/loss': 1.7797433137893677, 'validation/bleu': 28.77879315421725, 'validation/num_examples': 3000, 'test/accuracy': 0.6782643795013428, 'test/loss': 1.7161427736282349, 'test/bleu': 28.20757514270011, 'test/num_examples': 3003, 'score': 10105.405197381973, 'total_duration': 17117.95597720146, 'accumulated_submission_time': 10105.405197381973, 'accumulated_eval_time': 7010.723140716553, 'accumulated_logging_time': 0.21793007850646973, 'global_step': 29092, 'preemption_count': 0}), (31510, {'train/accuracy': 0.6804373264312744, 'train/loss': 1.6987824440002441, 'train/bleu': 33.88130247241901, 'validation/accuracy': 0.6667367219924927, 'validation/loss': 1.7544461488723755, 'validation/bleu': 27.999507568288657, 'validation/num_examples': 3000, 'test/accuracy': 0.6793766617774963, 'test/loss': 1.6823879480361938, 'test/bleu': 28.388578945482298, 'test/num_examples': 3003, 'score': 10945.45983839035, 'total_duration': 18731.336015939713, 'accumulated_submission_time': 10945.45983839035, 'accumulated_eval_time': 7783.900926828384, 'accumulated_logging_time': 0.2381587028503418, 'global_step': 31510, 'preemption_count': 0}), (33929, {'train/accuracy': 0.6599628925323486, 'train/loss': 1.8271431922912598, 'train/bleu': 32.339138907329854, 'validation/accuracy': 0.6692705154418945, 'validation/loss': 1.7412551641464233, 'validation/bleu': 28.76290182737091, 'validation/num_examples': 3000, 'test/accuracy': 0.6825280785560608, 'test/loss': 1.6659889221191406, 'test/bleu': 28.41589675117528, 'test/num_examples': 3003, 'score': 11785.570471286774, 'total_duration': 20069.390327453613, 'accumulated_submission_time': 11785.570471286774, 'accumulated_eval_time': 8281.696171045303, 'accumulated_logging_time': 0.2600553035736084, 'global_step': 33929, 'preemption_count': 0}), (36356, {'train/accuracy': 0.654316782951355, 'train/loss': 1.874359130859375, 'train/bleu': 32.11816353812008, 'validation/accuracy': 0.6731515526771545, 'validation/loss': 1.7405656576156616, 'validation/bleu': 29.162475833081356, 'validation/num_examples': 3000, 'test/accuracy': 0.6834086179733276, 'test/loss': 1.6702252626419067, 'test/bleu': 28.720744835952587, 'test/num_examples': 3003, 'score': 12625.65418624878, 'total_duration': 21376.234978199005, 'accumulated_submission_time': 12625.65418624878, 'accumulated_eval_time': 8748.312220096588, 'accumulated_logging_time': 0.2798900604248047, 'global_step': 36356, 'preemption_count': 0}), (38783, {'train/accuracy': 0.6658881902694702, 'train/loss': 1.7860950231552124, 'train/bleu': 32.37594239737388, 'validation/accuracy': 0.6726447939872742, 'validation/loss': 1.727084755897522, 'validation/bleu': 29.374485709907372, 'validation/num_examples': 3000, 'test/accuracy': 0.6844977736473083, 'test/loss': 1.6515417098999023, 'test/bleu': 28.777301678435343, 'test/num_examples': 3003, 'score': 13465.653621196747, 'total_duration': 22701.18399143219, 'accumulated_submission_time': 13465.653621196747, 'accumulated_eval_time': 9233.117734670639, 'accumulated_logging_time': 0.30044102668762207, 'global_step': 38783, 'preemption_count': 0}), (41211, {'train/accuracy': 0.6578485369682312, 'train/loss': 1.8377385139465332, 'train/bleu': 32.55934789110335, 'validation/accuracy': 0.6763651967048645, 'validation/loss': 1.7149626016616821, 'validation/bleu': 29.51203362539935, 'validation/num_examples': 3000, 'test/accuracy': 0.6887035369873047, 'test/loss': 1.6402912139892578, 'test/bleu': 29.26120509627895, 'test/num_examples': 3003, 'score': 14305.846911907196, 'total_duration': 24056.523476839066, 'accumulated_submission_time': 14305.846911907196, 'accumulated_eval_time': 9748.123394727707, 'accumulated_logging_time': 0.319899320602417, 'global_step': 41211, 'preemption_count': 0}), (43637, {'train/accuracy': 0.657191812992096, 'train/loss': 1.8295434713363647, 'train/bleu': 32.315905550746095, 'validation/accuracy': 0.6761921644210815, 'validation/loss': 1.7070823907852173, 'validation/bleu': 29.413222057987216, 'validation/num_examples': 3000, 'test/accuracy': 0.6892828345298767, 'test/loss': 1.62874174118042, 'test/bleu': 29.144494163907872, 'test/num_examples': 3003, 'score': 15145.796455144882, 'total_duration': 25404.637673139572, 'accumulated_submission_time': 15145.796455144882, 'accumulated_eval_time': 10256.143904209137, 'accumulated_logging_time': 0.3395676612854004, 'global_step': 43637, 'preemption_count': 0}), (46067, {'train/accuracy': 0.6660265922546387, 'train/loss': 1.7800300121307373, 'train/bleu': 33.32514152016943, 'validation/accuracy': 0.6766865253448486, 'validation/loss': 1.695261001586914, 'validation/bleu': 29.49073337458618, 'validation/num_examples': 3000, 'test/accuracy': 0.6893175840377808, 'test/loss': 1.6159530878067017, 'test/bleu': 29.45351812516039, 'test/num_examples': 3003, 'score': 15985.809119939804, 'total_duration': 26727.546419620514, 'accumulated_submission_time': 15985.809119939804, 'accumulated_eval_time': 10738.898144721985, 'accumulated_logging_time': 0.35892677307128906, 'global_step': 46067, 'preemption_count': 0}), (48498, {'train/accuracy': 0.6650455594062805, 'train/loss': 1.7754555940628052, 'train/bleu': 33.017636438170605, 'validation/accuracy': 0.6788619160652161, 'validation/loss': 1.6756508350372314, 'validation/bleu': 29.777172927643285, 'validation/num_examples': 3000, 'test/accuracy': 0.6922604441642761, 'test/loss': 1.5947095155715942, 'test/bleu': 29.57061145161476, 'test/num_examples': 3003, 'score': 16825.671855688095, 'total_duration': 28023.34668159485, 'accumulated_submission_time': 16825.671855688095, 'accumulated_eval_time': 11194.691133975983, 'accumulated_logging_time': 0.3797576427459717, 'global_step': 48498, 'preemption_count': 0}), (50928, {'train/accuracy': 0.678161084651947, 'train/loss': 1.7013061046600342, 'train/bleu': 33.73722193302117, 'validation/accuracy': 0.6770943999290466, 'validation/loss': 1.681348443031311, 'validation/bleu': 29.473870136680905, 'validation/num_examples': 3000, 'test/accuracy': 0.6920750737190247, 'test/loss': 1.6019251346588135, 'test/bleu': 29.406914407243583, 'test/num_examples': 3003, 'score': 17665.83458852768, 'total_duration': 29367.47161746025, 'accumulated_submission_time': 17665.83458852768, 'accumulated_eval_time': 11698.504893541336, 'accumulated_logging_time': 0.40401506423950195, 'global_step': 50928, 'preemption_count': 0}), (53356, {'train/accuracy': 0.6663511991500854, 'train/loss': 1.7673200368881226, 'train/bleu': 33.20184331635408, 'validation/accuracy': 0.6799001693725586, 'validation/loss': 1.6677190065383911, 'validation/bleu': 29.846394562757897, 'validation/num_examples': 3000, 'test/accuracy': 0.6934770345687866, 'test/loss': 1.5850940942764282, 'test/bleu': 29.66777480345962, 'test/num_examples': 3003, 'score': 18505.722554445267, 'total_duration': 30719.42313671112, 'accumulated_submission_time': 18505.722554445267, 'accumulated_eval_time': 12210.4261136055, 'accumulated_logging_time': 0.42496228218078613, 'global_step': 53356, 'preemption_count': 0}), (55784, {'train/accuracy': 0.6593195199966431, 'train/loss': 1.8066797256469727, 'train/bleu': 33.46414713065126, 'validation/accuracy': 0.6803945302963257, 'validation/loss': 1.6639642715454102, 'validation/bleu': 29.814114746482282, 'validation/num_examples': 3000, 'test/accuracy': 0.6941837668418884, 'test/loss': 1.583396315574646, 'test/bleu': 29.442610156520477, 'test/num_examples': 3003, 'score': 19345.67029953003, 'total_duration': 32034.604627132416, 'accumulated_submission_time': 19345.67029953003, 'accumulated_eval_time': 12685.512647151947, 'accumulated_logging_time': 0.44696927070617676, 'global_step': 55784, 'preemption_count': 0}), (58212, {'train/accuracy': 0.6740744113922119, 'train/loss': 1.7107826471328735, 'train/bleu': 33.75707023750471, 'validation/accuracy': 0.6798877716064453, 'validation/loss': 1.659110188484192, 'validation/bleu': 29.683342942430748, 'validation/num_examples': 3000, 'test/accuracy': 0.6946588158607483, 'test/loss': 1.576315999031067, 'test/bleu': 29.619676667709058, 'test/num_examples': 3003, 'score': 20185.56756925583, 'total_duration': 33352.59757947922, 'accumulated_submission_time': 20185.56756925583, 'accumulated_eval_time': 13163.464032173157, 'accumulated_logging_time': 0.4675753116607666, 'global_step': 58212, 'preemption_count': 0}), (60640, {'train/accuracy': 0.6705814003944397, 'train/loss': 1.7417917251586914, 'train/bleu': 33.507083309313806, 'validation/accuracy': 0.6825575232505798, 'validation/loss': 1.6651276350021362, 'validation/bleu': 29.94895947917713, 'validation/num_examples': 3000, 'test/accuracy': 0.6970339417457581, 'test/loss': 1.5821985006332397, 'test/bleu': 29.730545639064797, 'test/num_examples': 3003, 'score': 21025.590893507004, 'total_duration': 34726.0827331543, 'accumulated_submission_time': 21025.590893507004, 'accumulated_eval_time': 13696.782493114471, 'accumulated_logging_time': 0.48986291885375977, 'global_step': 60640, 'preemption_count': 0}), (63069, {'train/accuracy': 0.6917546391487122, 'train/loss': 1.6079250574111938, 'train/bleu': 34.63569596473616, 'validation/accuracy': 0.6841520071029663, 'validation/loss': 1.6403577327728271, 'validation/bleu': 30.072948329535862, 'validation/num_examples': 3000, 'test/accuracy': 0.6980419754981995, 'test/loss': 1.5595570802688599, 'test/bleu': 30.047292877075495, 'test/num_examples': 3003, 'score': 21865.68392586708, 'total_duration': 36255.31010055542, 'accumulated_submission_time': 21865.68392586708, 'accumulated_eval_time': 14385.771279096603, 'accumulated_logging_time': 0.5128803253173828, 'global_step': 63069, 'preemption_count': 0}), (65500, {'train/accuracy': 0.678013026714325, 'train/loss': 1.6905033588409424, 'train/bleu': 33.738987930062756, 'validation/accuracy': 0.6821743845939636, 'validation/loss': 1.6405622959136963, 'validation/bleu': 30.045239365827644, 'validation/num_examples': 3000, 'test/accuracy': 0.6986907720565796, 'test/loss': 1.554708480834961, 'test/bleu': 29.972031120604413, 'test/num_examples': 3003, 'score': 22705.866425275803, 'total_duration': 37751.861726522446, 'accumulated_submission_time': 22705.866425275803, 'accumulated_eval_time': 15041.995205879211, 'accumulated_logging_time': 0.5346004962921143, 'global_step': 65500, 'preemption_count': 0}), (67932, {'train/accuracy': 0.6781975030899048, 'train/loss': 1.7058039903640747, 'train/bleu': 33.83729287863027, 'validation/accuracy': 0.6836081743240356, 'validation/loss': 1.6443802118301392, 'validation/bleu': 30.25696925145094, 'validation/num_examples': 3000, 'test/accuracy': 0.6991426348686218, 'test/loss': 1.5548205375671387, 'test/bleu': 30.270547114908364, 'test/num_examples': 3003, 'score': 23546.064761400223, 'total_duration': 39090.83188700676, 'accumulated_submission_time': 23546.064761400223, 'accumulated_eval_time': 15540.621483325958, 'accumulated_logging_time': 0.5572319030761719, 'global_step': 67932, 'preemption_count': 0}), (70362, {'train/accuracy': 0.6845722794532776, 'train/loss': 1.6533809900283813, 'train/bleu': 34.62180642562419, 'validation/accuracy': 0.6854497790336609, 'validation/loss': 1.6352390050888062, 'validation/bleu': 30.327073951645982, 'validation/num_examples': 3000, 'test/accuracy': 0.7006256580352783, 'test/loss': 1.5441480875015259, 'test/bleu': 30.24846840007525, 'test/num_examples': 3003, 'score': 24386.10723233223, 'total_duration': 40574.590224027634, 'accumulated_submission_time': 24386.10723233223, 'accumulated_eval_time': 16184.187658786774, 'accumulated_logging_time': 0.5817821025848389, 'global_step': 70362, 'preemption_count': 0}), (72789, {'train/accuracy': 0.6809914708137512, 'train/loss': 1.675355076789856, 'train/bleu': 34.46936074608691, 'validation/accuracy': 0.6856970191001892, 'validation/loss': 1.6260801553726196, 'validation/bleu': 30.51372839924486, 'validation/num_examples': 3000, 'test/accuracy': 0.7015409469604492, 'test/loss': 1.5358240604400635, 'test/bleu': 30.29830131674638, 'test/num_examples': 3003, 'score': 25226.19104862213, 'total_duration': 41930.47286128998, 'accumulated_submission_time': 25226.19104862213, 'accumulated_eval_time': 16699.839008569717, 'accumulated_logging_time': 0.6043787002563477, 'global_step': 72789, 'preemption_count': 0}), (75207, {'train/accuracy': 0.6794785857200623, 'train/loss': 1.6920408010482788, 'train/bleu': 34.42417625398327, 'validation/accuracy': 0.6880701184272766, 'validation/loss': 1.6248291730880737, 'validation/bleu': 30.416407374886056, 'validation/num_examples': 3000, 'test/accuracy': 0.70440274477005, 'test/loss': 1.531368613243103, 'test/bleu': 30.25555285865231, 'test/num_examples': 3003, 'score': 26066.15735888481, 'total_duration': 43346.49547815323, 'accumulated_submission_time': 26066.15735888481, 'accumulated_eval_time': 17275.74415397644, 'accumulated_logging_time': 0.6288051605224609, 'global_step': 75207, 'preemption_count': 0}), (77626, {'train/accuracy': 0.6874957084655762, 'train/loss': 1.6292890310287476, 'train/bleu': 34.73902798636497, 'validation/accuracy': 0.6861790418624878, 'validation/loss': 1.6188749074935913, 'validation/bleu': 30.518530648102807, 'validation/num_examples': 3000, 'test/accuracy': 0.7026879787445068, 'test/loss': 1.5254799127578735, 'test/bleu': 30.27720554256199, 'test/num_examples': 3003, 'score': 26906.30938053131, 'total_duration': 44837.9638838768, 'accumulated_submission_time': 26906.30938053131, 'accumulated_eval_time': 17926.911850452423, 'accumulated_logging_time': 0.652606725692749, 'global_step': 77626, 'preemption_count': 0}), (80043, {'train/accuracy': 0.6864668726921082, 'train/loss': 1.6455543041229248, 'train/bleu': 34.74561324436292, 'validation/accuracy': 0.6876004338264465, 'validation/loss': 1.6194725036621094, 'validation/bleu': 30.508750137624247, 'validation/num_examples': 3000, 'test/accuracy': 0.7030819058418274, 'test/loss': 1.525663137435913, 'test/bleu': 30.104904671786333, 'test/num_examples': 3003, 'score': 27746.23920059204, 'total_duration': 46217.66652059555, 'accumulated_submission_time': 27746.23920059204, 'accumulated_eval_time': 18466.527469158173, 'accumulated_logging_time': 0.6828269958496094, 'global_step': 80043, 'preemption_count': 0}), (82465, {'train/accuracy': 0.7023534774780273, 'train/loss': 1.5521113872528076, 'train/bleu': 35.83263091139065, 'validation/accuracy': 0.68921959400177, 'validation/loss': 1.6090657711029053, 'validation/bleu': 30.322149991395488, 'validation/num_examples': 3000, 'test/accuracy': 0.7057815194129944, 'test/loss': 1.5156477689743042, 'test/bleu': 30.52575065164402, 'test/num_examples': 3003, 'score': 28586.344668626785, 'total_duration': 47585.840898275375, 'accumulated_submission_time': 28586.344668626785, 'accumulated_eval_time': 18994.446326494217, 'accumulated_logging_time': 0.707528829574585, 'global_step': 82465, 'preemption_count': 0}), (84892, {'train/accuracy': 0.6925277709960938, 'train/loss': 1.6007498502731323, 'train/bleu': 35.20272484964355, 'validation/accuracy': 0.6891701817512512, 'validation/loss': 1.6060130596160889, 'validation/bleu': 30.57543923485164, 'validation/num_examples': 3000, 'test/accuracy': 0.7068706154823303, 'test/loss': 1.506286382675171, 'test/bleu': 30.824158321276325, 'test/num_examples': 3003, 'score': 29426.47685265541, 'total_duration': 48971.31904864311, 'accumulated_submission_time': 29426.47685265541, 'accumulated_eval_time': 19539.64394927025, 'accumulated_logging_time': 0.7331511974334717, 'global_step': 84892, 'preemption_count': 0}), (87318, {'train/accuracy': 0.6939358115196228, 'train/loss': 1.6034772396087646, 'train/bleu': 35.35737138783944, 'validation/accuracy': 0.6900477409362793, 'validation/loss': 1.601252794265747, 'validation/bleu': 30.779533180808755, 'validation/num_examples': 3000, 'test/accuracy': 0.7064882516860962, 'test/loss': 1.5074666738510132, 'test/bleu': 30.53064323868315, 'test/num_examples': 3003, 'score': 30266.362763643265, 'total_duration': 50313.43754172325, 'accumulated_submission_time': 30266.362763643265, 'accumulated_eval_time': 20041.725078105927, 'accumulated_logging_time': 0.7603545188903809, 'global_step': 87318, 'preemption_count': 0}), (89743, {'train/accuracy': 0.7043187022209167, 'train/loss': 1.5412328243255615, 'train/bleu': 35.74486855937609, 'validation/accuracy': 0.6905544996261597, 'validation/loss': 1.602715015411377, 'validation/bleu': 30.38012252055491, 'validation/num_examples': 3000, 'test/accuracy': 0.7081566452980042, 'test/loss': 1.502125859260559, 'test/bleu': 30.769510708107042, 'test/num_examples': 3003, 'score': 31106.542657852173, 'total_duration': 51691.04849886894, 'accumulated_submission_time': 31106.542657852173, 'accumulated_eval_time': 20579.003517866135, 'accumulated_logging_time': 0.7858483791351318, 'global_step': 89743, 'preemption_count': 0}), (92171, {'train/accuracy': 0.6977658867835999, 'train/loss': 1.5735212564468384, 'train/bleu': 35.42978390408443, 'validation/accuracy': 0.6913579106330872, 'validation/loss': 1.6029143333435059, 'validation/bleu': 30.57063039128615, 'validation/num_examples': 3000, 'test/accuracy': 0.707194983959198, 'test/loss': 1.5036488771438599, 'test/bleu': 30.802846268462485, 'test/num_examples': 3003, 'score': 31946.60865879059, 'total_duration': 53031.30634570122, 'accumulated_submission_time': 31946.60865879059, 'accumulated_eval_time': 21079.04371213913, 'accumulated_logging_time': 0.8108994960784912, 'global_step': 92171, 'preemption_count': 0}), (94599, {'train/accuracy': 0.7145909667015076, 'train/loss': 1.4945625066757202, 'train/bleu': 37.31204285384356, 'validation/accuracy': 0.6914814710617065, 'validation/loss': 1.5995135307312012, 'validation/bleu': 30.663545155943098, 'validation/num_examples': 3000, 'test/accuracy': 0.7078554034233093, 'test/loss': 1.5004310607910156, 'test/bleu': 30.582849364244517, 'test/num_examples': 3003, 'score': 32786.758513212204, 'total_duration': 54408.9720761776, 'accumulated_submission_time': 32786.758513212204, 'accumulated_eval_time': 21616.409068584442, 'accumulated_logging_time': 0.8370471000671387, 'global_step': 94599, 'preemption_count': 0}), (97026, {'train/accuracy': 0.7064414024353027, 'train/loss': 1.5336871147155762, 'train/bleu': 36.26825426119099, 'validation/accuracy': 0.6917410492897034, 'validation/loss': 1.6006312370300293, 'validation/bleu': 30.576524362952135, 'validation/num_examples': 3000, 'test/accuracy': 0.7077511548995972, 'test/loss': 1.498480200767517, 'test/bleu': 30.46168828627666, 'test/num_examples': 3003, 'score': 33626.87296843529, 'total_duration': 55740.91502404213, 'accumulated_submission_time': 33626.87296843529, 'accumulated_eval_time': 22108.081154584885, 'accumulated_logging_time': 0.8669099807739258, 'global_step': 97026, 'preemption_count': 0}), (99453, {'train/accuracy': 0.706396222114563, 'train/loss': 1.5299201011657715, 'train/bleu': 35.85814231475738, 'validation/accuracy': 0.6916669011116028, 'validation/loss': 1.5949556827545166, 'validation/bleu': 30.507035768249672, 'validation/num_examples': 3000, 'test/accuracy': 0.7090488076210022, 'test/loss': 1.4936065673828125, 'test/bleu': 30.649132659897187, 'test/num_examples': 3003, 'score': 34467.01798629761, 'total_duration': 57075.00957465172, 'accumulated_submission_time': 34467.01798629761, 'accumulated_eval_time': 22601.87854719162, 'accumulated_logging_time': 0.8926870822906494, 'global_step': 99453, 'preemption_count': 0}), (101884, {'train/accuracy': 0.7180216908454895, 'train/loss': 1.465099573135376, 'train/bleu': 36.908673009059676, 'validation/accuracy': 0.692284882068634, 'validation/loss': 1.5957337617874146, 'validation/bleu': 30.676344627889627, 'validation/num_examples': 3000, 'test/accuracy': 0.7088981866836548, 'test/loss': 1.4944958686828613, 'test/bleu': 30.507825230719238, 'test/num_examples': 3003, 'score': 35307.1790702343, 'total_duration': 58519.812579631805, 'accumulated_submission_time': 35307.1790702343, 'accumulated_eval_time': 23206.371296405792, 'accumulated_logging_time': 0.9188277721405029, 'global_step': 101884, 'preemption_count': 0}), (104317, {'train/accuracy': 0.7137287855148315, 'train/loss': 1.490756869316101, 'train/bleu': 36.84923538207688, 'validation/accuracy': 0.6928040385246277, 'validation/loss': 1.5952367782592773, 'validation/bleu': 30.84917590799016, 'validation/num_examples': 3000, 'test/accuracy': 0.7096860408782959, 'test/loss': 1.4918954372406006, 'test/bleu': 30.713737057467963, 'test/num_examples': 3003, 'score': 36147.29384422302, 'total_duration': 59858.13088798523, 'accumulated_submission_time': 36147.29384422302, 'accumulated_eval_time': 23704.42150592804, 'accumulated_logging_time': 0.9460732936859131, 'global_step': 104317, 'preemption_count': 0})], 'global_step': 104317}
I0306 11:51:06.437731 140014092387520 submission_runner.py:649] Timing: 36147.29384422302
I0306 11:51:06.437767 140014092387520 submission_runner.py:651] Total number of evals: 44
I0306 11:51:06.437814 140014092387520 submission_runner.py:652] ====================
I0306 11:51:06.437928 140014092387520 submission_runner.py:750] Final wmt score: 1
