python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-1796777921 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-13-06.log
2025-03-05 19:13:07.597134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201987.619550       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201987.626398       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:13:14.527179 140248407626944 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax.
I0305 19:13:15.505518 140248407626944 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:13:15.508648 140248407626944 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:13:15.510308 140248407626944 submission_runner.py:606] Using RNG seed -1796777921
I0305 19:13:16.114149 140248407626944 submission_runner.py:615] --- Tuning run 4/5 ---
I0305 19:13:16.114330 140248407626944 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_4.
I0305 19:13:16.114530 140248407626944 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_4/hparams.json.
I0305 19:13:16.353399 140248407626944 submission_runner.py:218] Initializing dataset.
I0305 19:13:16.525250 140248407626944 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:16.549795 140248407626944 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:16.625442 140248407626944 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:17.997467 140248407626944 submission_runner.py:229] Initializing model.
I0305 19:14:00.183647 140248407626944 submission_runner.py:272] Initializing optimizer.
I0305 19:14:01.034654 140248407626944 submission_runner.py:279] Initializing metrics bundle.
I0305 19:14:01.034914 140248407626944 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:14:01.035973 140248407626944 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_4 with prefix checkpoint_
I0305 19:14:01.036077 140248407626944 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_4/meta_data_0.json.
I0305 19:14:01.036308 140248407626944 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:14:01.036371 140248407626944 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:14:01.229665 140248407626944 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_4/flags_0.json.
I0305 19:14:01.264219 140248407626944 submission_runner.py:337] Starting training loop.
I0305 19:14:27.625344 140112251307776 logging_writer.py:48] [0] global_step=0, grad_norm=5.730098724365234, loss=11.058897972106934
I0305 19:14:27.686009 140248407626944 spec.py:321] Evaluating on the training split.
I0305 19:14:27.688231 140248407626944 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:27.691797 140248407626944 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:14:27.724012 140248407626944 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:33.849572 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 19:19:38.906021 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 19:19:38.942440 140248407626944 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:38.977967 140248407626944 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:39.011020 140248407626944 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:44.200206 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 19:24:42.706495 140248407626944 spec.py:349] Evaluating on the test split.
I0305 19:24:42.708946 140248407626944 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:42.712264 140248407626944 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:42.744908 140248407626944 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:45.531457 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 19:29:43.988645 140248407626944 submission_runner.py:469] Time since start: 942.72s, 	Step: 1, 	{'train/accuracy': 0.0005826972774229944, 'train/loss': 11.045845985412598, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.047025680541992, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.040393829345703, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.42166519165039, 'total_duration': 942.7243485450745, 'accumulated_submission_time': 26.42166519165039, 'accumulated_eval_time': 916.302565574646, 'accumulated_logging_time': 0}
I0305 19:29:43.995932 140105321985792 logging_writer.py:48] [1] accumulated_eval_time=916.303, accumulated_logging_time=0, accumulated_submission_time=26.4217, global_step=1, preemption_count=0, score=26.4217, test/accuracy=0.000718341, test/bleu=0, test/loss=11.0404, test/num_examples=3003, total_duration=942.724, train/accuracy=0.000582697, train/bleu=0, train/loss=11.0458, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.047, validation/num_examples=3000
I0305 19:30:19.057989 140105313593088 logging_writer.py:48] [100] global_step=100, grad_norm=0.7115164399147034, loss=7.5423383712768555
I0305 19:30:54.015735 140105321985792 logging_writer.py:48] [200] global_step=200, grad_norm=0.7376587390899658, loss=6.528521537780762
I0305 19:31:29.009744 140105313593088 logging_writer.py:48] [300] global_step=300, grad_norm=0.47403424978256226, loss=5.820012092590332
I0305 19:32:04.014873 140105321985792 logging_writer.py:48] [400] global_step=400, grad_norm=0.4484783113002777, loss=5.368669033050537
I0305 19:32:39.048054 140105313593088 logging_writer.py:48] [500] global_step=500, grad_norm=0.480848103761673, loss=4.975229263305664
I0305 19:33:14.076249 140105321985792 logging_writer.py:48] [600] global_step=600, grad_norm=0.4783884882926941, loss=4.656709671020508
I0305 19:33:49.127215 140105313593088 logging_writer.py:48] [700] global_step=700, grad_norm=0.6655330657958984, loss=4.4443488121032715
I0305 19:34:24.167630 140105321985792 logging_writer.py:48] [800] global_step=800, grad_norm=0.4992346167564392, loss=4.140336036682129
I0305 19:34:59.222235 140105313593088 logging_writer.py:48] [900] global_step=900, grad_norm=0.445415198802948, loss=3.9546406269073486
I0305 19:35:34.269932 140105321985792 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4567277729511261, loss=3.761026382446289
I0305 19:36:09.290004 140105313593088 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.500423789024353, loss=3.730581760406494
I0305 19:36:44.321683 140105321985792 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3866436183452606, loss=3.614088535308838
I0305 19:37:19.365601 140105313593088 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.40226948261260986, loss=3.537330150604248
I0305 19:37:54.387926 140105321985792 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.3173845410346985, loss=3.4541678428649902
I0305 19:38:29.422684 140105313593088 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.3757283091545105, loss=3.3849234580993652
I0305 19:39:04.454549 140105321985792 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.2928368151187897, loss=3.235163450241089
I0305 19:39:39.459397 140105313593088 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.2874985933303833, loss=3.139904022216797
I0305 19:40:14.486117 140105321985792 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.2090381383895874, loss=3.0983939170837402
I0305 19:40:49.492588 140105313593088 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.21752069890499115, loss=3.010908842086792
I0305 19:41:24.519057 140105321985792 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.18534399569034576, loss=2.8903424739837646
I0305 19:41:59.525285 140105313593088 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.29779836535453796, loss=2.9069089889526367
I0305 19:42:34.572877 140105321985792 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4005890190601349, loss=2.7971999645233154
I0305 19:43:09.611110 140105313593088 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.21518658101558685, loss=2.8921303749084473
I0305 19:43:44.256072 140248407626944 spec.py:321] Evaluating on the training split.
I0305 19:43:46.857057 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 19:46:33.247935 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 19:46:35.841528 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 19:49:22.311740 140248407626944 spec.py:349] Evaluating on the test split.
I0305 19:49:24.908553 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 19:51:48.397203 140248407626944 submission_runner.py:469] Time since start: 2267.13s, 	Step: 2400, 	{'train/accuracy': 0.5243012309074402, 'train/loss': 2.668487071990967, 'train/bleu': 22.06324745666449, 'validation/accuracy': 0.523582935333252, 'validation/loss': 2.6468873023986816, 'validation/bleu': 18.140998608213064, 'validation/num_examples': 3000, 'test/accuracy': 0.5218630433082581, 'test/loss': 2.682077407836914, 'test/bleu': 16.73002282967136, 'test/num_examples': 3003, 'score': 866.5041997432709, 'total_duration': 2267.132890224457, 'accumulated_submission_time': 866.5041997432709, 'accumulated_eval_time': 1400.4436326026917, 'accumulated_logging_time': 0.016210079193115234}
I0305 19:51:48.406351 140105321985792 logging_writer.py:48] [2400] accumulated_eval_time=1400.44, accumulated_logging_time=0.0162101, accumulated_submission_time=866.504, global_step=2400, preemption_count=0, score=866.504, test/accuracy=0.521863, test/bleu=16.73, test/loss=2.68208, test/num_examples=3003, total_duration=2267.13, train/accuracy=0.524301, train/bleu=22.0632, train/loss=2.66849, validation/accuracy=0.523583, validation/bleu=18.141, validation/loss=2.64689, validation/num_examples=3000
I0305 19:51:48.764206 140105313593088 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3631499111652374, loss=2.733720541000366
I0305 19:52:23.695626 140105321985792 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.18582969903945923, loss=2.6876590251922607
I0305 19:52:58.671762 140105313593088 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2678320109844208, loss=2.5688881874084473
I0305 19:53:33.645544 140105321985792 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.30038681626319885, loss=2.5362563133239746
I0305 19:54:08.644077 140105313593088 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.321786105632782, loss=2.55204701423645
I0305 19:54:43.623075 140105321985792 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2604624629020691, loss=2.44303822517395
I0305 19:55:18.609158 140105313593088 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2054816335439682, loss=2.4708855152130127
I0305 19:55:53.602456 140105321985792 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.367025226354599, loss=2.4238033294677734
I0305 19:56:28.609865 140105313593088 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.4314717650413513, loss=2.3877453804016113
I0305 19:57:03.603521 140105321985792 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3262525200843811, loss=2.3113436698913574
I0305 19:57:38.606305 140105313593088 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.42240938544273376, loss=2.3347268104553223
I0305 19:58:13.617974 140105321985792 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.3336144983768463, loss=2.2894742488861084
I0305 19:58:48.633037 140105313593088 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.2675677537918091, loss=2.4074766635894775
I0305 19:59:23.638527 140105321985792 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.47711727023124695, loss=2.1901822090148926
I0305 19:59:58.630454 140105313593088 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.20251280069351196, loss=2.2302331924438477
I0305 20:00:33.622250 140105321985792 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.2971988618373871, loss=2.365375280380249
I0305 20:01:08.628283 140105313593088 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.4186493754386902, loss=2.3768553733825684
I0305 20:01:43.636723 140105321985792 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.3277311623096466, loss=2.2355222702026367
I0305 20:02:18.650696 140105313593088 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.2930203378200531, loss=2.2622413635253906
I0305 20:02:53.699184 140105321985792 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7843644618988037, loss=2.2018771171569824
I0305 20:03:28.772176 140105313593088 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.43507254123687744, loss=2.341951847076416
I0305 20:04:03.795081 140105321985792 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.4897444546222687, loss=2.287095785140991
I0305 20:04:38.819426 140105313593088 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3948933780193329, loss=2.2365946769714355
I0305 20:05:13.849898 140105321985792 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5234207510948181, loss=2.3095192909240723
I0305 20:05:48.522785 140248407626944 spec.py:321] Evaluating on the training split.
I0305 20:05:51.121989 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:08:58.541326 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 20:09:01.142589 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:11:28.490621 140248407626944 spec.py:349] Evaluating on the test split.
I0305 20:11:31.093957 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:14:03.558475 140248407626944 submission_runner.py:469] Time since start: 3602.29s, 	Step: 4800, 	{'train/accuracy': 0.5761494040489197, 'train/loss': 2.251298189163208, 'train/bleu': 26.57507033736089, 'validation/accuracy': 0.595295786857605, 'validation/loss': 2.0990915298461914, 'validation/bleu': 23.169889766266664, 'validation/num_examples': 3000, 'test/accuracy': 0.5996060967445374, 'test/loss': 2.0647213459014893, 'test/bleu': 21.939725157588853, 'test/num_examples': 3003, 'score': 1706.4510326385498, 'total_duration': 3602.2941720485687, 'accumulated_submission_time': 1706.4510326385498, 'accumulated_eval_time': 1895.479255437851, 'accumulated_logging_time': 0.03408670425415039}
I0305 20:14:03.569041 140105313593088 logging_writer.py:48] [4800] accumulated_eval_time=1895.48, accumulated_logging_time=0.0340867, accumulated_submission_time=1706.45, global_step=4800, preemption_count=0, score=1706.45, test/accuracy=0.599606, test/bleu=21.9397, test/loss=2.06472, test/num_examples=3003, total_duration=3602.29, train/accuracy=0.576149, train/bleu=26.5751, train/loss=2.2513, validation/accuracy=0.595296, validation/bleu=23.1699, validation/loss=2.09909, validation/num_examples=3000
I0305 20:14:03.935213 140105321985792 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.4584633708000183, loss=2.2141165733337402
I0305 20:14:38.853457 140105313593088 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.47468382120132446, loss=2.1428256034851074
I0305 20:15:13.869653 140105321985792 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.3961727023124695, loss=2.146749973297119
I0305 20:15:48.920223 140105313593088 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.370394766330719, loss=2.282376766204834
I0305 20:16:23.973780 140105321985792 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3465195894241333, loss=2.217193365097046
I0305 20:16:59.012940 140105313593088 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.635362982749939, loss=2.1741654872894287
I0305 20:17:34.035573 140105321985792 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.32364919781684875, loss=2.191983938217163
I0305 20:18:09.063049 140105313593088 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.4477282464504242, loss=2.14994478225708
I0305 20:18:44.103373 140105321985792 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.4751460552215576, loss=2.229858875274658
I0305 20:19:19.143417 140105313593088 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5860053300857544, loss=2.2388510704040527
I0305 20:19:54.182526 140105321985792 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2933861315250397, loss=2.1957523822784424
I0305 20:20:29.220622 140105313593088 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.33202192187309265, loss=2.1499717235565186
I0305 20:21:04.247390 140105321985792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7036488056182861, loss=2.1505627632141113
I0305 20:21:39.284343 140105313593088 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.2816430628299713, loss=2.1240086555480957
I0305 20:22:14.294684 140105321985792 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.34734684228897095, loss=2.1338748931884766
I0305 20:22:49.294128 140104668485376 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.40779924392700195, loss=2.231717586517334
I0305 20:23:24.219279 140104660092672 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9614494442939758, loss=2.215982675552368
I0305 20:23:59.147487 140104668485376 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.37934327125549316, loss=2.1030452251434326
I0305 20:24:34.078338 140104660092672 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.416230171918869, loss=2.134768009185791
I0305 20:25:09.037523 140104668485376 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.339770644903183, loss=2.223689556121826
I0305 20:25:43.986886 140104660092672 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.3379366397857666, loss=2.229361057281494
I0305 20:26:18.900637 140104668485376 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.3624919652938843, loss=2.1559629440307617
I0305 20:26:53.801872 140104660092672 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4774736166000366, loss=2.1558690071105957
I0305 20:27:28.695672 140104668485376 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6340687870979309, loss=2.1951239109039307
I0305 20:28:03.576463 140104660092672 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5887355208396912, loss=2.0884127616882324
I0305 20:28:03.585082 140248407626944 spec.py:321] Evaluating on the training split.
I0305 20:28:06.191387 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:31:08.955507 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 20:31:11.555285 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:33:51.933334 140248407626944 spec.py:349] Evaluating on the test split.
I0305 20:33:54.530011 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:36:27.572261 140248407626944 submission_runner.py:469] Time since start: 4946.31s, 	Step: 7201, 	{'train/accuracy': 0.5885499119758606, 'train/loss': 2.1332192420959473, 'train/bleu': 27.03930457150921, 'validation/accuracy': 0.603848934173584, 'validation/loss': 2.011162042617798, 'validation/bleu': 23.893339803280817, 'validation/num_examples': 3000, 'test/accuracy': 0.6076700091362, 'test/loss': 1.981911540031433, 'test/bleu': 22.898264565142487, 'test/num_examples': 3003, 'score': 2546.297681570053, 'total_duration': 4946.307964324951, 'accumulated_submission_time': 2546.297681570053, 'accumulated_eval_time': 2399.4663529396057, 'accumulated_logging_time': 0.05392956733703613}
I0305 20:36:27.582962 140104668485376 logging_writer.py:48] [7201] accumulated_eval_time=2399.47, accumulated_logging_time=0.0539296, accumulated_submission_time=2546.3, global_step=7201, preemption_count=0, score=2546.3, test/accuracy=0.60767, test/bleu=22.8983, test/loss=1.98191, test/num_examples=3003, total_duration=4946.31, train/accuracy=0.58855, train/bleu=27.0393, train/loss=2.13322, validation/accuracy=0.603849, validation/bleu=23.8933, validation/loss=2.01116, validation/num_examples=3000
I0305 20:37:02.424917 140104660092672 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.44640403985977173, loss=2.1964950561523438
I0305 20:37:37.344246 140104668485376 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5488253831863403, loss=2.118624687194824
I0305 20:38:12.227292 140104660092672 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.26233726739883423, loss=2.100580930709839
I0305 20:38:47.152738 140104668485376 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.3887370824813843, loss=2.1599528789520264
I0305 20:39:22.087548 140104660092672 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.652729868888855, loss=2.1692233085632324
I0305 20:39:57.075585 140104668485376 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.33234867453575134, loss=2.110136032104492
I0305 20:40:31.996241 140104660092672 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.554000198841095, loss=1.9636707305908203
I0305 20:41:06.947071 140104668485376 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3287140727043152, loss=2.2768352031707764
I0305 20:41:41.905149 140104660092672 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3010607957839966, loss=2.108690023422241
I0305 20:42:16.830624 140104668485376 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.326860249042511, loss=2.262192487716675
I0305 20:42:51.762955 140104660092672 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.30647042393684387, loss=2.149676561355591
I0305 20:43:26.685348 140104668485376 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.366905152797699, loss=2.0928194522857666
I0305 20:44:01.617592 140104660092672 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.35008999705314636, loss=2.0330331325531006
I0305 20:44:36.571614 140104668485376 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.38211286067962646, loss=2.2574641704559326
I0305 20:45:11.536908 140104660092672 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.2879425585269928, loss=2.166088581085205
I0305 20:45:46.478152 140104668485376 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.33041173219680786, loss=2.150297164916992
I0305 20:46:21.434356 140104660092672 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3616137206554413, loss=2.0225791931152344
I0305 20:46:56.325034 140104668485376 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5637381076812744, loss=2.167271852493286
I0305 20:47:31.247162 140104660092672 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7473936676979065, loss=2.227809190750122
I0305 20:48:06.165697 140104668485376 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3924708068370819, loss=2.146364688873291
I0305 20:48:41.102661 140104660092672 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.2859046459197998, loss=2.1369683742523193
I0305 20:49:16.027527 140104668485376 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5394838452339172, loss=2.1526424884796143
I0305 20:49:50.969939 140104660092672 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.40834540128707886, loss=2.130413055419922
I0305 20:50:25.918106 140104668485376 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.32449570298194885, loss=2.1472203731536865
I0305 20:50:27.670756 140248407626944 spec.py:321] Evaluating on the training split.
I0305 20:50:30.275335 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:53:41.860634 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 20:53:44.472828 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:56:39.228095 140248407626944 spec.py:349] Evaluating on the test split.
I0305 20:56:41.833877 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 20:59:15.695171 140248407626944 submission_runner.py:469] Time since start: 6314.43s, 	Step: 9606, 	{'train/accuracy': 0.592634379863739, 'train/loss': 2.1329338550567627, 'train/bleu': 27.754886105412552, 'validation/accuracy': 0.6082367300987244, 'validation/loss': 1.9942150115966797, 'validation/bleu': 24.553891809431896, 'validation/num_examples': 3000, 'test/accuracy': 0.6162785291671753, 'test/loss': 1.9350523948669434, 'test/bleu': 23.53525358070045, 'test/num_examples': 3003, 'score': 3386.2254931926727, 'total_duration': 6314.430859565735, 'accumulated_submission_time': 3386.2254931926727, 'accumulated_eval_time': 2927.4906842708588, 'accumulated_logging_time': 0.07434701919555664}
I0305 20:59:15.706159 140104660092672 logging_writer.py:48] [9606] accumulated_eval_time=2927.49, accumulated_logging_time=0.074347, accumulated_submission_time=3386.23, global_step=9606, preemption_count=0, score=3386.23, test/accuracy=0.616279, test/bleu=23.5353, test/loss=1.93505, test/num_examples=3003, total_duration=6314.43, train/accuracy=0.592634, train/bleu=27.7549, train/loss=2.13293, validation/accuracy=0.608237, validation/bleu=24.5539, validation/loss=1.99422, validation/num_examples=3000
I0305 20:59:48.861368 140104668485376 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.290601909160614, loss=2.0974044799804688
I0305 21:00:23.727604 140104660092672 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.549081563949585, loss=2.140838623046875
I0305 21:00:58.628908 140104668485376 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3365088701248169, loss=2.042356252670288
I0305 21:01:33.597976 140104660092672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6123021841049194, loss=2.136727809906006
I0305 21:02:08.516960 140104668485376 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.266716331243515, loss=2.0988047122955322
I0305 21:02:43.408165 140104660092672 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.33947861194610596, loss=2.1846749782562256
I0305 21:03:18.334749 140104668485376 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6085208654403687, loss=2.1239993572235107
I0305 21:03:53.236184 140104660092672 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7141340374946594, loss=2.2165591716766357
I0305 21:04:28.161435 140104668485376 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5334404110908508, loss=2.179316520690918
I0305 21:05:03.103792 140104660092672 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.27469876408576965, loss=2.1184892654418945
I0305 21:05:38.016176 140104668485376 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3216441571712494, loss=2.1173033714294434
I0305 21:06:12.916758 140104660092672 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.4063964784145355, loss=2.113556385040283
I0305 21:06:47.796096 140104668485376 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.369998037815094, loss=2.1221656799316406
I0305 21:07:22.649237 140104660092672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.32363128662109375, loss=2.0565037727355957
I0305 21:07:57.548134 140104668485376 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.5341132879257202, loss=2.052541971206665
I0305 21:08:32.390038 140104660092672 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3998287320137024, loss=2.151097297668457
I0305 21:09:07.309054 140104668485376 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3382916748523712, loss=2.101891040802002
I0305 21:09:42.252452 140104660092672 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.35101762413978577, loss=2.0436739921569824
I0305 21:10:17.161626 140104668485376 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.40242326259613037, loss=2.069535255432129
I0305 21:10:52.078045 140104660092672 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.39509686827659607, loss=2.1640961170196533
I0305 21:11:27.006777 140104668485376 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.46620607376098633, loss=2.0704455375671387
I0305 21:12:01.904004 140104660092672 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.6013889908790588, loss=2.0810046195983887
I0305 21:12:36.838033 140104668485376 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.5173302888870239, loss=2.086982011795044
I0305 21:13:11.705551 140104660092672 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0868033170700073, loss=2.203022003173828
I0305 21:13:15.900712 140248407626944 spec.py:321] Evaluating on the training split.
I0305 21:13:18.500976 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 21:17:33.658731 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 21:17:36.259574 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 21:21:11.940301 140248407626944 spec.py:349] Evaluating on the test split.
I0305 21:21:14.534075 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 21:24:10.128144 140248407626944 submission_runner.py:469] Time since start: 7808.86s, 	Step: 12013, 	{'train/accuracy': 0.5948514342308044, 'train/loss': 2.1006462574005127, 'train/bleu': 28.06506934391542, 'validation/accuracy': 0.610622227191925, 'validation/loss': 1.96293044090271, 'validation/bleu': 24.316434139255463, 'validation/num_examples': 3000, 'test/accuracy': 0.6173444390296936, 'test/loss': 1.9210034608840942, 'test/bleu': 23.64199541545895, 'test/num_examples': 3003, 'score': 4226.262289047241, 'total_duration': 7808.863825082779, 'accumulated_submission_time': 4226.262289047241, 'accumulated_eval_time': 3581.718025445938, 'accumulated_logging_time': 0.0946357250213623}
I0305 21:24:10.138737 140104668485376 logging_writer.py:48] [12013] accumulated_eval_time=3581.72, accumulated_logging_time=0.0946357, accumulated_submission_time=4226.26, global_step=12013, preemption_count=0, score=4226.26, test/accuracy=0.617344, test/bleu=23.642, test/loss=1.921, test/num_examples=3003, total_duration=7808.86, train/accuracy=0.594851, train/bleu=28.0651, train/loss=2.10065, validation/accuracy=0.610622, validation/bleu=24.3164, validation/loss=1.96293, validation/num_examples=3000
I0305 21:24:40.797027 140104660092672 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.315769761800766, loss=2.046039581298828
I0305 21:25:15.662104 140104668485376 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6246486306190491, loss=2.0904452800750732
I0305 21:25:50.502821 140104660092672 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.32109445333480835, loss=2.134948968887329
I0305 21:26:25.371637 140104668485376 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3635517358779907, loss=2.1664445400238037
I0305 21:27:00.224907 140104660092672 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5306106805801392, loss=2.094374179840088
I0305 21:27:35.158903 140104668485376 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6224602460861206, loss=2.083076000213623
I0305 21:28:10.079592 140104660092672 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.6063125729560852, loss=2.0482003688812256
I0305 21:28:45.000632 140104668485376 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.27707982063293457, loss=2.072458028793335
I0305 21:29:20.009761 140104660092672 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.7205343246459961, loss=2.1568405628204346
I0305 21:29:54.995118 140104668485376 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5696350336074829, loss=2.15828800201416
I0305 21:30:29.994548 140104660092672 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.36506056785583496, loss=2.0541553497314453
I0305 21:31:04.991489 140104668485376 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.307267427444458, loss=2.0768420696258545
I0305 21:31:40.007318 140104660092672 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3180486559867859, loss=2.05936861038208
I0305 21:32:14.964703 140104668485376 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.25382575392723083, loss=2.1496529579162598
I0305 21:32:49.948340 140104660092672 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.39753374457359314, loss=2.111760139465332
I0305 21:33:24.945543 140104668485376 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4237672686576843, loss=2.078172206878662
I0305 21:33:59.939861 140104660092672 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2741256058216095, loss=2.077756643295288
I0305 21:34:34.933773 140104668485376 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3377780318260193, loss=2.0153777599334717
I0305 21:35:09.942991 140104660092672 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.29449719190597534, loss=2.122255563735962
I0305 21:35:44.931995 140104668485376 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.4949653446674347, loss=2.0796101093292236
I0305 21:36:19.949358 140104660092672 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3553400933742523, loss=2.210210084915161
I0305 21:36:54.984569 140104668485376 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.3521592915058136, loss=2.0980634689331055
I0305 21:37:30.076114 140104660092672 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.5625194311141968, loss=2.0797369480133057
I0305 21:38:05.121689 140104668485376 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.7670286297798157, loss=2.158156394958496
I0305 21:38:10.384987 140248407626944 spec.py:321] Evaluating on the training split.
I0305 21:38:12.987855 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 21:41:27.757156 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 21:41:30.341221 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 21:44:18.747856 140248407626944 spec.py:349] Evaluating on the test split.
I0305 21:44:21.332409 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 21:46:55.072422 140248407626944 submission_runner.py:469] Time since start: 9173.81s, 	Step: 14416, 	{'train/accuracy': 0.5995975732803345, 'train/loss': 2.0686583518981934, 'train/bleu': 27.711498802894372, 'validation/accuracy': 0.6161718368530273, 'validation/loss': 1.9349751472473145, 'validation/bleu': 24.921011789710803, 'validation/num_examples': 3000, 'test/accuracy': 0.6221411228179932, 'test/loss': 1.8915694952011108, 'test/bleu': 24.013532325379224, 'test/num_examples': 3003, 'score': 5066.354030370712, 'total_duration': 9173.808131456375, 'accumulated_submission_time': 5066.354030370712, 'accumulated_eval_time': 4106.405398845673, 'accumulated_logging_time': 0.11338615417480469}
I0305 21:46:55.082700 140104660092672 logging_writer.py:48] [14416] accumulated_eval_time=4106.41, accumulated_logging_time=0.113386, accumulated_submission_time=5066.35, global_step=14416, preemption_count=0, score=5066.35, test/accuracy=0.622141, test/bleu=24.0135, test/loss=1.89157, test/num_examples=3003, total_duration=9173.81, train/accuracy=0.599598, train/bleu=27.7115, train/loss=2.06866, validation/accuracy=0.616172, validation/bleu=24.921, validation/loss=1.93498, validation/num_examples=3000
I0305 21:47:24.762640 140104668485376 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.4445441961288452, loss=2.090336799621582
I0305 21:47:59.698316 140104660092672 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.40975528955459595, loss=1.993971586227417
I0305 21:48:34.664664 140104668485376 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.3722650408744812, loss=2.1703124046325684
I0305 21:49:09.656529 140104660092672 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2846912443637848, loss=1.9600118398666382
I0305 21:49:44.630700 140104668485376 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.26890578866004944, loss=2.0714051723480225
I0305 21:50:19.619148 140104660092672 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.46360287070274353, loss=2.044597625732422
I0305 21:50:54.630891 140104668485376 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.32952260971069336, loss=2.0553736686706543
I0305 21:51:29.659935 140104660092672 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3367970585823059, loss=2.044296979904175
I0305 21:52:04.669838 140104668485376 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.32362812757492065, loss=2.0565285682678223
I0305 21:52:39.681528 140104660092672 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6264040470123291, loss=2.0615932941436768
I0305 21:53:14.690484 140104668485376 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.38555797934532166, loss=2.163316249847412
I0305 21:53:49.694760 140104660092672 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7840883731842041, loss=2.065553903579712
I0305 21:54:24.715642 140104668485376 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.35586225986480713, loss=2.0623939037323
I0305 21:54:59.711059 140104660092672 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.5597623586654663, loss=2.0141615867614746
I0305 21:55:34.722705 140104668485376 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.42035359144210815, loss=2.0062246322631836
I0305 21:56:09.758456 140104660092672 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5275857448577881, loss=1.9956861734390259
I0305 21:56:44.793034 140104668485376 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.34587228298187256, loss=2.17999267578125
I0305 21:57:19.815942 140104660092672 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3553723394870758, loss=2.069918632507324
I0305 21:57:54.825992 140104668485376 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.5792917609214783, loss=2.0318410396575928
I0305 21:58:29.827399 140104660092672 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.628183901309967, loss=2.0532643795013428
I0305 21:59:04.849619 140104668485376 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.4215409457683563, loss=2.0462021827697754
I0305 21:59:39.809840 140104660092672 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3658943772315979, loss=2.087833881378174
I0305 22:00:14.815147 140104668485376 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5560032725334167, loss=2.057971715927124
I0305 22:00:49.772324 140104660092672 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.2601669132709503, loss=2.1270105838775635
I0305 22:00:55.367906 140248407626944 spec.py:321] Evaluating on the training split.
I0305 22:00:57.971571 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:04:24.312263 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 22:04:26.901087 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:07:01.102263 140248407626944 spec.py:349] Evaluating on the test split.
I0305 22:07:03.704921 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:09:30.251726 140248407626944 submission_runner.py:469] Time since start: 10528.99s, 	Step: 16817, 	{'train/accuracy': 0.5967208743095398, 'train/loss': 2.0855557918548584, 'train/bleu': 27.791125013759444, 'validation/accuracy': 0.6140830516815186, 'validation/loss': 1.9277559518814087, 'validation/bleu': 24.711194840199852, 'validation/num_examples': 3000, 'test/accuracy': 0.6215502023696899, 'test/loss': 1.8961756229400635, 'test/bleu': 23.647379412036194, 'test/num_examples': 3003, 'score': 5906.49445939064, 'total_duration': 10528.987439870834, 'accumulated_submission_time': 5906.49445939064, 'accumulated_eval_time': 4621.289161682129, 'accumulated_logging_time': 0.13210725784301758}
I0305 22:09:30.261839 140104668485376 logging_writer.py:48] [16817] accumulated_eval_time=4621.29, accumulated_logging_time=0.132107, accumulated_submission_time=5906.49, global_step=16817, preemption_count=0, score=5906.49, test/accuracy=0.62155, test/bleu=23.6474, test/loss=1.89618, test/num_examples=3003, total_duration=10529, train/accuracy=0.596721, train/bleu=27.7911, train/loss=2.08556, validation/accuracy=0.614083, validation/bleu=24.7112, validation/loss=1.92776, validation/num_examples=3000
I0305 22:09:59.647947 140104660092672 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.31136736273765564, loss=2.073550224304199
I0305 22:10:34.611995 140104668485376 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.47552359104156494, loss=2.127152681350708
I0305 22:11:09.621639 140104660092672 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.5425376892089844, loss=2.070645570755005
I0305 22:11:44.654248 140104668485376 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.6210699677467346, loss=2.001570224761963
I0305 22:12:19.676036 140104660092672 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.4953887164592743, loss=2.126843214035034
I0305 22:12:54.718368 140104668485376 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.5956838726997375, loss=2.0059001445770264
I0305 22:13:29.710847 140104660092672 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.34428852796554565, loss=2.130764961242676
I0305 22:14:04.727427 140104668485376 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.38927948474884033, loss=2.090322256088257
I0305 22:14:39.807819 140104660092672 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2960212826728821, loss=2.0647084712982178
I0305 22:15:14.831789 140104668485376 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.4009782075881958, loss=2.076228618621826
I0305 22:15:49.828087 140104660092672 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5378538370132446, loss=2.024085283279419
I0305 22:16:24.860036 140104668485376 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.4532392919063568, loss=2.124081611633301
I0305 22:16:59.876331 140104660092672 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6668320894241333, loss=2.1485438346862793
I0305 22:17:34.867709 140104668485376 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.2981668710708618, loss=2.1373276710510254
I0305 22:18:09.873287 140104660092672 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.5984541773796082, loss=2.042818307876587
I0305 22:18:44.835181 140104668485376 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.685399055480957, loss=2.1400630474090576
I0305 22:19:19.833713 140104660092672 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5085468292236328, loss=2.057795763015747
I0305 22:19:54.830377 140104668485376 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.5239251255989075, loss=2.0594916343688965
I0305 22:20:29.780266 140104660092672 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3451371490955353, loss=2.0726704597473145
I0305 22:21:04.778365 140104668485376 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.31319424510002136, loss=1.9941920042037964
I0305 22:21:39.982520 140104660092672 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.4486369788646698, loss=2.1467909812927246
I0305 22:22:15.085098 140104668485376 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.2892748713493347, loss=2.0652151107788086
I0305 22:22:50.225030 140104660092672 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.5204548835754395, loss=2.0934789180755615
I0305 22:23:25.359616 140104668485376 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.6232854723930359, loss=2.0250139236450195
I0305 22:23:30.285811 140248407626944 spec.py:321] Evaluating on the training split.
I0305 22:23:32.893060 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:26:37.673217 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 22:26:40.279873 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:29:26.721286 140248407626944 spec.py:349] Evaluating on the test split.
I0305 22:29:29.317784 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:32:01.017030 140248407626944 submission_runner.py:469] Time since start: 11879.75s, 	Step: 19215, 	{'train/accuracy': 0.6089617609977722, 'train/loss': 1.972785472869873, 'train/bleu': 29.01144004101766, 'validation/accuracy': 0.6204113364219666, 'validation/loss': 1.9081695079803467, 'validation/bleu': 24.698706358958617, 'validation/num_examples': 3000, 'test/accuracy': 0.6271694898605347, 'test/loss': 1.8622543811798096, 'test/bleu': 24.1552987499265, 'test/num_examples': 3003, 'score': 6746.372478961945, 'total_duration': 11879.752734184265, 'accumulated_submission_time': 6746.372478961945, 'accumulated_eval_time': 5132.020322084427, 'accumulated_logging_time': 0.15047740936279297}
I0305 22:32:01.029458 140104660092672 logging_writer.py:48] [19215] accumulated_eval_time=5132.02, accumulated_logging_time=0.150477, accumulated_submission_time=6746.37, global_step=19215, preemption_count=0, score=6746.37, test/accuracy=0.627169, test/bleu=24.1553, test/loss=1.86225, test/num_examples=3003, total_duration=11879.8, train/accuracy=0.608962, train/bleu=29.0114, train/loss=1.97279, validation/accuracy=0.620411, validation/bleu=24.6987, validation/loss=1.90817, validation/num_examples=3000
I0305 22:32:31.193311 140104668485376 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.33707910776138306, loss=2.035160541534424
I0305 22:33:06.275617 140104660092672 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3653310239315033, loss=2.057474136352539
I0305 22:33:41.404909 140104668485376 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.32451751828193665, loss=2.059410572052002
I0305 22:34:16.521169 140104660092672 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3046162724494934, loss=2.077486753463745
I0305 22:34:51.663377 140104668485376 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.49334773421287537, loss=2.0070765018463135
I0305 22:35:26.764242 140104660092672 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.34617242217063904, loss=2.0597336292266846
I0305 22:36:01.921353 140104668485376 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5664016008377075, loss=2.0407164096832275
I0305 22:36:37.067380 140104660092672 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.38625049591064453, loss=1.991054892539978
I0305 22:37:12.197227 140104668485376 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.4594222605228424, loss=1.9906790256500244
I0305 22:37:47.308769 140104660092672 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6613579988479614, loss=2.0691142082214355
I0305 22:38:22.399453 140104668485376 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.2818280756473541, loss=2.0874013900756836
I0305 22:38:57.562597 140104660092672 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.5530345439910889, loss=2.0881454944610596
I0305 22:39:32.695307 140104668485376 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5995869040489197, loss=2.060145378112793
I0305 22:40:07.837273 140104660092672 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3192642331123352, loss=1.970380425453186
I0305 22:40:42.985827 140104668485376 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5040636658668518, loss=2.0310075283050537
I0305 22:41:18.152258 140104660092672 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.46543020009994507, loss=2.0815975666046143
I0305 22:41:53.291046 140104668485376 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.46814611554145813, loss=2.013932943344116
I0305 22:42:28.442755 140104660092672 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.29575902223587036, loss=2.0972578525543213
I0305 22:43:03.608756 140104668485376 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.6395073533058167, loss=2.1196672916412354
I0305 22:43:38.720799 140104660092672 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.34871962666511536, loss=2.0535576343536377
I0305 22:44:13.830398 140104668485376 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.35159677267074585, loss=2.061415433883667
I0305 22:44:48.940548 140104660092672 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.39312270283699036, loss=2.049808979034424
I0305 22:45:24.063408 140104668485376 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.6843059062957764, loss=2.084381580352783
I0305 22:45:59.191012 140104660092672 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3445921540260315, loss=2.0776641368865967
I0305 22:46:01.304514 140248407626944 spec.py:321] Evaluating on the training split.
I0305 22:46:03.908732 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:50:06.636690 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 22:50:09.237032 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:53:23.947045 140248407626944 spec.py:349] Evaluating on the test split.
I0305 22:53:26.546668 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 22:56:11.066179 140248407626944 submission_runner.py:469] Time since start: 13329.80s, 	Step: 21607, 	{'train/accuracy': 0.600994884967804, 'train/loss': 2.069777727127075, 'train/bleu': 28.01189501640959, 'validation/accuracy': 0.6183472275733948, 'validation/loss': 1.9050275087356567, 'validation/bleu': 24.4177373588781, 'validation/num_examples': 3000, 'test/accuracy': 0.6240528225898743, 'test/loss': 1.8706202507019043, 'test/bleu': 23.36249353772997, 'test/num_examples': 3003, 'score': 7586.503115653992, 'total_duration': 13329.801872253418, 'accumulated_submission_time': 7586.503115653992, 'accumulated_eval_time': 5741.78190612793, 'accumulated_logging_time': 0.17234587669372559}
I0305 22:56:11.078679 140104668485376 logging_writer.py:48] [21607] accumulated_eval_time=5741.78, accumulated_logging_time=0.172346, accumulated_submission_time=7586.5, global_step=21607, preemption_count=0, score=7586.5, test/accuracy=0.624053, test/bleu=23.3625, test/loss=1.87062, test/num_examples=3003, total_duration=13329.8, train/accuracy=0.600995, train/bleu=28.0119, train/loss=2.06978, validation/accuracy=0.618347, validation/bleu=24.4177, validation/loss=1.90503, validation/num_examples=3000
I0305 22:56:44.018924 140104660092672 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.3228459358215332, loss=2.023951530456543
I0305 22:57:19.070355 140104668485376 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.7853730916976929, loss=2.0316689014434814
I0305 22:57:54.166052 140104660092672 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.4515920579433441, loss=2.0462498664855957
I0305 22:58:29.273184 140104668485376 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.46377307176589966, loss=2.056528091430664
I0305 22:59:04.399507 140104660092672 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.31167295575141907, loss=2.0588550567626953
I0305 22:59:39.496149 140104668485376 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.7379427552223206, loss=2.0605874061584473
I0305 23:00:14.599692 140104660092672 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.31148409843444824, loss=2.0247950553894043
I0305 23:00:49.722566 140104668485376 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.32770100235939026, loss=2.0867531299591064
I0305 23:01:24.840849 140104660092672 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3815692663192749, loss=2.025528907775879
I0305 23:01:59.925603 140104668485376 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.33284157514572144, loss=2.031780242919922
I0305 23:02:35.059388 140104660092672 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.34632524847984314, loss=2.136794090270996
I0305 23:03:10.169700 140104668485376 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.29718562960624695, loss=2.0284337997436523
I0305 23:03:45.289935 140104660092672 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.40221768617630005, loss=2.0842292308807373
I0305 23:04:20.435188 140104668485376 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4551495313644409, loss=2.0354716777801514
I0305 23:04:55.567883 140104660092672 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2542024254798889, loss=2.0332281589508057
I0305 23:05:30.664453 140104668485376 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.35920068621635437, loss=2.086056709289551
I0305 23:06:05.801295 140104660092672 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3222106695175171, loss=1.997990369796753
I0305 23:06:40.914145 140104668485376 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.4372500479221344, loss=2.056119680404663
I0305 23:07:16.043063 140104660092672 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.44235897064208984, loss=2.099177122116089
I0305 23:07:51.140514 140104668485376 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.5609893798828125, loss=2.0276248455047607
I0305 23:08:26.272921 140104660092672 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.37998348474502563, loss=2.0694620609283447
I0305 23:09:01.425502 140104668485376 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.29236382246017456, loss=2.0143003463745117
I0305 23:09:36.575811 140104660092672 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3822334110736847, loss=2.0789427757263184
I0305 23:10:11.380316 140248407626944 spec.py:321] Evaluating on the training split.
I0305 23:10:13.984614 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 23:13:35.257263 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 23:13:37.861252 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 23:16:35.246856 140248407626944 spec.py:349] Evaluating on the test split.
I0305 23:16:37.849673 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 23:19:36.776866 140248407626944 submission_runner.py:469] Time since start: 14735.51s, 	Step: 24000, 	{'train/accuracy': 0.5988175272941589, 'train/loss': 2.0836374759674072, 'train/bleu': 28.698458458970528, 'validation/accuracy': 0.6217339038848877, 'validation/loss': 1.8910731077194214, 'validation/bleu': 25.055974462199096, 'validation/num_examples': 3000, 'test/accuracy': 0.6299038529396057, 'test/loss': 1.8353830575942993, 'test/bleu': 24.380433008049046, 'test/num_examples': 3003, 'score': 8426.663962602615, 'total_duration': 14735.512577295303, 'accumulated_submission_time': 8426.663962602615, 'accumulated_eval_time': 6307.178397893906, 'accumulated_logging_time': 0.19337987899780273}
I0305 23:19:36.788620 140104668485376 logging_writer.py:48] [24000] accumulated_eval_time=6307.18, accumulated_logging_time=0.19338, accumulated_submission_time=8426.66, global_step=24000, preemption_count=0, score=8426.66, test/accuracy=0.629904, test/bleu=24.3804, test/loss=1.83538, test/num_examples=3003, total_duration=14735.5, train/accuracy=0.598818, train/bleu=28.6985, train/loss=2.08364, validation/accuracy=0.621734, validation/bleu=25.056, validation/loss=1.89107, validation/num_examples=3000
I0305 23:19:37.157191 140104660092672 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.4583126902580261, loss=2.0872817039489746
I0305 23:20:12.210862 140104668485376 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3972281515598297, loss=2.1395294666290283
I0305 23:20:47.286229 140104660092672 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3044789135456085, loss=2.0910236835479736
I0305 23:21:22.405105 140104668485376 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.5033265948295593, loss=2.0005016326904297
I0305 23:21:57.522538 140104660092672 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.2830932140350342, loss=2.084928274154663
I0305 23:22:32.627732 140104668485376 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.38722047209739685, loss=2.0005569458007812
I0305 23:23:07.752707 140104660092672 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.3137493133544922, loss=1.9425830841064453
I0305 23:23:42.850972 140104668485376 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.3720085024833679, loss=2.0432379245758057
I0305 23:24:17.951819 140104660092672 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.5355788469314575, loss=2.018653392791748
I0305 23:24:53.071562 140104668485376 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.28987613320350647, loss=1.9377961158752441
I0305 23:25:28.160699 140104660092672 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.43527817726135254, loss=2.0283000469207764
I0305 23:26:03.225043 140104668485376 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.28967493772506714, loss=2.0776143074035645
I0305 23:26:38.188804 140104660092672 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4180136024951935, loss=1.9930919408798218
I0305 23:27:13.145605 140104668485376 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.3642824590206146, loss=2.041374921798706
I0305 23:27:48.163959 140104660092672 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3594887852668762, loss=2.08829402923584
I0305 23:28:23.136742 140104668485376 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.46825897693634033, loss=2.0127742290496826
I0305 23:28:58.137627 140104660092672 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.3479878902435303, loss=2.0079309940338135
I0305 23:29:33.133553 140104668485376 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.32117676734924316, loss=1.9711133241653442
I0305 23:30:08.151705 140104660092672 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4153321385383606, loss=2.129481792449951
I0305 23:30:43.159778 140104668485376 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3935451805591583, loss=2.008519411087036
I0305 23:31:18.178996 140104660092672 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.28239938616752625, loss=2.140720844268799
I0305 23:31:53.208814 140104668485376 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.39290520548820496, loss=2.0337016582489014
I0305 23:32:28.247005 140104660092672 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.3341318666934967, loss=2.0662407875061035
I0305 23:33:03.240017 140104668485376 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.45881107449531555, loss=1.9911173582077026
I0305 23:33:36.848623 140248407626944 spec.py:321] Evaluating on the training split.
I0305 23:33:39.456743 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 23:36:51.935507 140248407626944 spec.py:333] Evaluating on the validation split.
I0305 23:36:54.543899 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 23:40:07.687781 140248407626944 spec.py:349] Evaluating on the test split.
I0305 23:40:10.289613 140248407626944 workload.py:181] Translating evaluation dataset.
I0305 23:43:19.549036 140248407626944 submission_runner.py:469] Time since start: 16158.28s, 	Step: 26397, 	{'train/accuracy': 0.6073904037475586, 'train/loss': 2.0108675956726074, 'train/bleu': 28.470763039248457, 'validation/accuracy': 0.6224631071090698, 'validation/loss': 1.8763236999511719, 'validation/bleu': 24.98457005789974, 'validation/num_examples': 3000, 'test/accuracy': 0.6294288039207458, 'test/loss': 1.8344347476959229, 'test/bleu': 24.543198578889346, 'test/num_examples': 3003, 'score': 9266.580038070679, 'total_duration': 16158.284749269485, 'accumulated_submission_time': 9266.580038070679, 'accumulated_eval_time': 6889.878755331039, 'accumulated_logging_time': 0.21436476707458496}
I0305 23:43:19.561782 140104660092672 logging_writer.py:48] [26397] accumulated_eval_time=6889.88, accumulated_logging_time=0.214365, accumulated_submission_time=9266.58, global_step=26397, preemption_count=0, score=9266.58, test/accuracy=0.629429, test/bleu=24.5432, test/loss=1.83443, test/num_examples=3003, total_duration=16158.3, train/accuracy=0.60739, train/bleu=28.4708, train/loss=2.01087, validation/accuracy=0.622463, validation/bleu=24.9846, validation/loss=1.87632, validation/num_examples=3000
I0305 23:43:20.957040 140104668485376 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.25464755296707153, loss=2.056483507156372
I0305 23:43:55.861133 140104660092672 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3776048719882965, loss=2.0128085613250732
I0305 23:44:30.790502 140104668485376 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.33091381192207336, loss=2.068788528442383
I0305 23:45:05.734438 140104660092672 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.6049640774726868, loss=2.1160905361175537
I0305 23:45:40.725463 140104668485376 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.8134912848472595, loss=2.1542460918426514
I0305 23:46:15.729684 140104660092672 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2773943841457367, loss=1.99814772605896
I0305 23:46:50.718172 140104668485376 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.29364287853240967, loss=1.9598026275634766
I0305 23:47:25.673858 140104660092672 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.43375512957572937, loss=2.0217225551605225
I0305 23:48:00.660315 140104668485376 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3038766384124756, loss=1.9545437097549438
I0305 23:48:35.641619 140104660092672 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.30581170320510864, loss=2.069728136062622
I0305 23:49:10.622211 140104668485376 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5283961296081543, loss=2.0121402740478516
I0305 23:49:45.644746 140104660092672 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.30098432302474976, loss=2.007814645767212
I0305 23:50:20.633131 140104668485376 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.28945958614349365, loss=2.0921435356140137
I0305 23:50:55.595287 140104660092672 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.26686978340148926, loss=1.9815430641174316
I0305 23:51:30.583863 140104668485376 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.287482887506485, loss=2.041128158569336
I0305 23:52:05.582180 140104660092672 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.33123305439949036, loss=2.019665241241455
I0305 23:52:40.583907 140104668485376 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.30506590008735657, loss=2.021214723587036
I0305 23:53:15.565761 140104660092672 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.29681113362312317, loss=2.064549446105957
I0305 23:53:50.567425 140104668485376 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.3021695017814636, loss=1.9433718919754028
I0305 23:54:25.597819 140104660092672 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.41616034507751465, loss=2.0394046306610107
I0305 23:55:00.631736 140104668485376 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.2666030526161194, loss=1.993843913078308
I0305 23:55:35.641919 140104660092672 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3318706750869751, loss=2.0418660640716553
I0305 23:56:10.652090 140104668485376 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.2541213035583496, loss=1.946653962135315
I0305 23:56:45.650540 140104660092672 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.378274142742157, loss=2.034992218017578
I0305 23:57:19.610621 140248407626944 spec.py:321] Evaluating on the training split.
I0305 23:57:22.220448 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:00:27.897765 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 00:00:30.505107 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:03:19.130606 140248407626944 spec.py:349] Evaluating on the test split.
I0306 00:03:21.728103 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:05:59.893987 140248407626944 submission_runner.py:469] Time since start: 17518.63s, 	Step: 28798, 	{'train/accuracy': 0.6040464043617249, 'train/loss': 2.0616681575775146, 'train/bleu': 28.303358346541863, 'validation/accuracy': 0.6231182217597961, 'validation/loss': 1.8767093420028687, 'validation/bleu': 25.35758235315749, 'validation/num_examples': 3000, 'test/accuracy': 0.6336577534675598, 'test/loss': 1.808383822441101, 'test/bleu': 24.397016183603924, 'test/num_examples': 3003, 'score': 10106.486082792282, 'total_duration': 17518.629702091217, 'accumulated_submission_time': 10106.486082792282, 'accumulated_eval_time': 7410.162064552307, 'accumulated_logging_time': 0.23556137084960938}
I0306 00:05:59.906258 140104668485376 logging_writer.py:48] [28798] accumulated_eval_time=7410.16, accumulated_logging_time=0.235561, accumulated_submission_time=10106.5, global_step=28798, preemption_count=0, score=10106.5, test/accuracy=0.633658, test/bleu=24.397, test/loss=1.80838, test/num_examples=3003, total_duration=17518.6, train/accuracy=0.604046, train/bleu=28.3034, train/loss=2.06167, validation/accuracy=0.623118, validation/bleu=25.3576, validation/loss=1.87671, validation/num_examples=3000
I0306 00:06:00.975775 140104660092672 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3170613646507263, loss=2.0433402061462402
I0306 00:06:35.918551 140104668485376 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5319752097129822, loss=1.9710818529129028
I0306 00:07:10.867990 140104660092672 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6668068170547485, loss=2.0425915718078613
I0306 00:07:45.840993 140104668485376 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.4471120536327362, loss=2.0558626651763916
I0306 00:08:20.789517 140104660092672 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.2836496829986572, loss=2.0539791584014893
I0306 00:08:55.751811 140104668485376 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.42684903740882874, loss=1.9596811532974243
I0306 00:09:30.689264 140104660092672 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7333782315254211, loss=2.0560250282287598
I0306 00:10:05.638150 140104668485376 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.41559457778930664, loss=2.0553717613220215
I0306 00:10:40.606438 140104660092672 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.28576138615608215, loss=1.930078387260437
I0306 00:11:15.570688 140104668485376 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.3214566111564636, loss=1.947259783744812
I0306 00:11:50.548724 140104660092672 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5408133864402771, loss=1.9429106712341309
I0306 00:12:25.555160 140104668485376 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.4225669503211975, loss=1.9699375629425049
I0306 00:13:00.573885 140104660092672 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.3280291259288788, loss=1.9733576774597168
I0306 00:13:35.555845 140104668485376 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.28793245553970337, loss=2.0983047485351562
I0306 00:14:10.552002 140104660092672 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.30432066321372986, loss=1.9702051877975464
I0306 00:14:45.557526 140104668485376 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.43977493047714233, loss=2.0756423473358154
I0306 00:15:20.582001 140104660092672 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.30731651186943054, loss=2.01208758354187
I0306 00:15:55.585334 140104668485376 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.28474897146224976, loss=1.9957274198532104
I0306 00:16:30.616680 140104660092672 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.3999764621257782, loss=2.03069806098938
I0306 00:17:05.599097 140104668485376 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.6516909003257751, loss=2.1491732597351074
I0306 00:17:40.596696 140104660092672 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.46064651012420654, loss=1.9121359586715698
I0306 00:18:15.613697 140104668485376 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.38170120120048523, loss=1.9388810396194458
I0306 00:18:50.621063 140104660092672 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.2930896580219269, loss=1.935895562171936
I0306 00:19:25.600474 140104668485376 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.2695976793766022, loss=1.9883606433868408
I0306 00:19:59.930670 140248407626944 spec.py:321] Evaluating on the training split.
I0306 00:20:02.540782 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:23:19.852397 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 00:23:22.448004 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:26:19.556811 140248407626944 spec.py:349] Evaluating on the test split.
I0306 00:26:22.162830 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:29:21.751041 140248407626944 submission_runner.py:469] Time since start: 18920.49s, 	Step: 31199, 	{'train/accuracy': 0.6023253202438354, 'train/loss': 2.0468084812164307, 'train/bleu': 29.050422495080856, 'validation/accuracy': 0.6252194046974182, 'validation/loss': 1.8635061979293823, 'validation/bleu': 25.19420020542427, 'validation/num_examples': 3000, 'test/accuracy': 0.6330784559249878, 'test/loss': 1.8104184865951538, 'test/bleu': 25.023926394236796, 'test/num_examples': 3003, 'score': 10946.36867761612, 'total_duration': 18920.48673558235, 'accumulated_submission_time': 10946.36867761612, 'accumulated_eval_time': 7971.982358932495, 'accumulated_logging_time': 0.2559394836425781}
I0306 00:29:21.765164 140104660092672 logging_writer.py:48] [31199] accumulated_eval_time=7971.98, accumulated_logging_time=0.255939, accumulated_submission_time=10946.4, global_step=31199, preemption_count=0, score=10946.4, test/accuracy=0.633078, test/bleu=25.0239, test/loss=1.81042, test/num_examples=3003, total_duration=18920.5, train/accuracy=0.602325, train/bleu=29.0504, train/loss=2.04681, validation/accuracy=0.625219, validation/bleu=25.1942, validation/loss=1.86351, validation/num_examples=3000
I0306 00:29:22.471418 140104668485376 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.30000826716423035, loss=1.976728916168213
I0306 00:29:57.420478 140104660092672 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.7368358373641968, loss=1.9452983140945435
I0306 00:30:32.466774 140104668485376 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.5659025311470032, loss=2.050408124923706
I0306 00:31:07.591234 140104660092672 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3095977008342743, loss=2.0013844966888428
I0306 00:31:42.675849 140104668485376 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.44779133796691895, loss=1.931157112121582
I0306 00:32:17.742089 140104660092672 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.43514418601989746, loss=2.010392904281616
I0306 00:32:52.823628 140104668485376 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.30392560362815857, loss=1.9905108213424683
I0306 00:33:27.889270 140104660092672 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3282737731933594, loss=2.010061025619507
I0306 00:34:02.907226 140104668485376 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2928696274757385, loss=2.040848970413208
I0306 00:34:37.936530 140104660092672 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3269399106502533, loss=1.9314593076705933
I0306 00:35:12.955905 140104668485376 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.4601193070411682, loss=2.0081794261932373
I0306 00:35:48.014240 140104660092672 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.41858410835266113, loss=2.0118038654327393
I0306 00:36:23.055633 140104668485376 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2606959640979767, loss=1.8688786029815674
I0306 00:36:58.093420 140104660092672 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.37482041120529175, loss=1.9642741680145264
I0306 00:37:33.107635 140104668485376 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3172985017299652, loss=1.9416215419769287
I0306 00:38:08.143013 140104660092672 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.26375138759613037, loss=1.9496738910675049
I0306 00:38:43.165742 140104668485376 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.31225132942199707, loss=2.0422656536102295
I0306 00:39:18.160175 140104660092672 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.8423136472702026, loss=2.084441661834717
I0306 00:39:53.164674 140104668485376 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3609395921230316, loss=1.9272725582122803
I0306 00:40:28.165327 140104660092672 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.47100669145584106, loss=1.9404252767562866
I0306 00:41:03.176924 140104668485376 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.35677456855773926, loss=2.014401912689209
I0306 00:41:38.181253 140104660092672 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.2536073327064514, loss=1.9726694822311401
I0306 00:42:13.174388 140104668485376 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4123915433883667, loss=1.9655805826187134
I0306 00:42:48.146515 140104660092672 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3412278890609741, loss=2.0934321880340576
I0306 00:43:22.092116 140248407626944 spec.py:321] Evaluating on the training split.
I0306 00:43:24.696129 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:46:45.007637 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 00:46:47.597832 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:49:26.830221 140248407626944 spec.py:349] Evaluating on the test split.
I0306 00:49:29.433377 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 00:51:58.310597 140248407626944 submission_runner.py:469] Time since start: 20277.05s, 	Step: 33598, 	{'train/accuracy': 0.6076792478561401, 'train/loss': 2.0037922859191895, 'train/bleu': 28.84632768272457, 'validation/accuracy': 0.62728351354599, 'validation/loss': 1.8529053926467896, 'validation/bleu': 25.832449345547825, 'validation/num_examples': 3000, 'test/accuracy': 0.6383965015411377, 'test/loss': 1.7966763973236084, 'test/bleu': 25.50332373340371, 'test/num_examples': 3003, 'score': 11786.552738904953, 'total_duration': 20277.04630637169, 'accumulated_submission_time': 11786.552738904953, 'accumulated_eval_time': 8488.200779438019, 'accumulated_logging_time': 0.2782886028289795}
I0306 00:51:58.323814 140104668485376 logging_writer.py:48] [33598] accumulated_eval_time=8488.2, accumulated_logging_time=0.278289, accumulated_submission_time=11786.6, global_step=33598, preemption_count=0, score=11786.6, test/accuracy=0.638397, test/bleu=25.5033, test/loss=1.79668, test/num_examples=3003, total_duration=20277, train/accuracy=0.607679, train/bleu=28.8463, train/loss=2.00379, validation/accuracy=0.627284, validation/bleu=25.8324, validation/loss=1.85291, validation/num_examples=3000
I0306 00:51:59.376144 140104660092672 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.34950530529022217, loss=2.052400827407837
I0306 00:52:34.232402 140104668485376 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.2796155512332916, loss=2.016916036605835
I0306 00:53:09.084050 140104660092672 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.49294552206993103, loss=2.0102503299713135
I0306 00:53:44.027885 140104668485376 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5020411014556885, loss=2.021367073059082
I0306 00:54:18.950975 140104660092672 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.31922852993011475, loss=2.06598162651062
I0306 00:54:53.868165 140104668485376 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.27030694484710693, loss=1.9465501308441162
I0306 00:55:28.787081 140104660092672 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.30409106612205505, loss=2.0032618045806885
I0306 00:56:03.728290 140104668485376 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.35194793343544006, loss=2.064056396484375
I0306 00:56:38.681946 140104660092672 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.35901516675949097, loss=1.9509272575378418
I0306 00:57:13.634648 140104668485376 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4752863645553589, loss=2.0686399936676025
I0306 00:57:48.585479 140104660092672 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.29608026146888733, loss=2.0108625888824463
I0306 00:58:23.541162 140104668485376 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3828250467777252, loss=1.9935054779052734
I0306 00:58:58.555805 140104660092672 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.4895188510417938, loss=2.0346031188964844
I0306 00:59:33.510299 140104668485376 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.28632545471191406, loss=2.0010244846343994
I0306 01:00:08.477227 140104660092672 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.43548598885536194, loss=1.9887516498565674
I0306 01:00:43.395100 140104668485376 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.510291337966919, loss=1.9161142110824585
I0306 01:01:18.359501 140104660092672 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.5454928874969482, loss=2.031487464904785
I0306 01:01:53.360592 140104668485376 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6626936197280884, loss=1.967310905456543
I0306 01:02:28.342620 140104660092672 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.6369598507881165, loss=2.04787278175354
I0306 01:03:03.289864 140104668485376 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.48445966839790344, loss=2.0261898040771484
I0306 01:03:38.297486 140104660092672 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5840241312980652, loss=2.0561838150024414
I0306 01:04:13.204430 140104668485376 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.35541942715644836, loss=2.0172600746154785
I0306 01:04:48.134621 140104660092672 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.3400028645992279, loss=1.9947359561920166
I0306 01:05:23.053639 140104668485376 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.31691357493400574, loss=1.9497743844985962
I0306 01:05:57.985028 140104660092672 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.2455950379371643, loss=1.928142786026001
I0306 01:05:58.342576 140248407626944 spec.py:321] Evaluating on the training split.
I0306 01:06:00.946017 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:09:02.877393 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 01:09:05.479860 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:11:52.133585 140248407626944 spec.py:349] Evaluating on the test split.
I0306 01:11:54.726243 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:14:25.447819 140248407626944 submission_runner.py:469] Time since start: 21624.18s, 	Step: 36002, 	{'train/accuracy': 0.6086857318878174, 'train/loss': 2.0054149627685547, 'train/bleu': 28.876378522990866, 'validation/accuracy': 0.6295948624610901, 'validation/loss': 1.84092116355896, 'validation/bleu': 26.17329039950227, 'validation/num_examples': 3000, 'test/accuracy': 0.6351871490478516, 'test/loss': 1.7874221801757812, 'test/bleu': 24.81050768744868, 'test/num_examples': 3003, 'score': 12626.428744316101, 'total_duration': 21624.183537483215, 'accumulated_submission_time': 12626.428744316101, 'accumulated_eval_time': 8995.305968999863, 'accumulated_logging_time': 0.30034947395324707}
I0306 01:14:25.460311 140104668485376 logging_writer.py:48] [36002] accumulated_eval_time=8995.31, accumulated_logging_time=0.300349, accumulated_submission_time=12626.4, global_step=36002, preemption_count=0, score=12626.4, test/accuracy=0.635187, test/bleu=24.8105, test/loss=1.78742, test/num_examples=3003, total_duration=21624.2, train/accuracy=0.608686, train/bleu=28.8764, train/loss=2.00541, validation/accuracy=0.629595, validation/bleu=26.1733, validation/loss=1.84092, validation/num_examples=3000
I0306 01:15:00.040593 140104660092672 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.28987830877304077, loss=2.0149543285369873
I0306 01:15:34.967178 140104668485376 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.3400452733039856, loss=2.0067179203033447
I0306 01:16:09.925127 140104660092672 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.35319119691848755, loss=1.9875874519348145
I0306 01:16:44.926311 140104668485376 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3553459644317627, loss=1.9007441997528076
I0306 01:17:19.895247 140104660092672 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.333418607711792, loss=1.957663893699646
I0306 01:17:54.876121 140104668485376 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.40148505568504333, loss=1.9851878881454468
I0306 01:18:29.884492 140104660092672 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.33100593090057373, loss=1.8926018476486206
I0306 01:19:04.878053 140104668485376 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3058679401874542, loss=2.007714033126831
I0306 01:19:39.863802 140104660092672 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3581889569759369, loss=1.9641765356063843
I0306 01:20:14.855666 140104668485376 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.46490418910980225, loss=2.047067403793335
I0306 01:20:49.856911 140104660092672 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3019165098667145, loss=1.8982993364334106
I0306 01:21:24.847366 140104668485376 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.27721405029296875, loss=2.0025625228881836
I0306 01:21:59.777368 140104660092672 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4282114803791046, loss=1.9844586849212646
I0306 01:22:34.725170 140104668485376 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.28452253341674805, loss=1.956120252609253
I0306 01:23:09.633485 140104660092672 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.2742554247379303, loss=2.1145312786102295
I0306 01:23:44.565427 140104668485376 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.3156733810901642, loss=1.9729472398757935
I0306 01:24:19.587234 140104660092672 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.43030673265457153, loss=2.0082223415374756
I0306 01:24:54.583517 140104668485376 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.5339615941047668, loss=2.033097267150879
I0306 01:25:29.565484 140104660092672 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2807343304157257, loss=1.9316412210464478
I0306 01:26:04.576053 140104668485376 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2557794451713562, loss=1.9775112867355347
I0306 01:26:39.607078 140104660092672 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3236677646636963, loss=1.9601154327392578
I0306 01:27:14.610835 140104668485376 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.36680492758750916, loss=1.9329785108566284
I0306 01:27:49.630015 140104660092672 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.3322167694568634, loss=2.0253520011901855
I0306 01:28:24.615463 140104668485376 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.30505886673927307, loss=1.9166829586029053
I0306 01:28:25.673202 140248407626944 spec.py:321] Evaluating on the training split.
I0306 01:28:28.273014 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:31:26.521645 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 01:31:29.101365 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:34:19.742605 140248407626944 spec.py:349] Evaluating on the test split.
I0306 01:34:22.332475 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:36:53.376039 140248407626944 submission_runner.py:469] Time since start: 22972.11s, 	Step: 38404, 	{'train/accuracy': 0.6169231534004211, 'train/loss': 1.9332997798919678, 'train/bleu': 29.345306079455177, 'validation/accuracy': 0.6299780011177063, 'validation/loss': 1.8246445655822754, 'validation/bleu': 25.469598513026288, 'validation/num_examples': 3000, 'test/accuracy': 0.6377360820770264, 'test/loss': 1.7799433469772339, 'test/bleu': 24.733074118360015, 'test/num_examples': 3003, 'score': 13466.497159481049, 'total_duration': 22972.11173725128, 'accumulated_submission_time': 13466.497159481049, 'accumulated_eval_time': 9503.008731365204, 'accumulated_logging_time': 0.3213996887207031}
I0306 01:36:53.389997 140104660092672 logging_writer.py:48] [38404] accumulated_eval_time=9503.01, accumulated_logging_time=0.3214, accumulated_submission_time=13466.5, global_step=38404, preemption_count=0, score=13466.5, test/accuracy=0.637736, test/bleu=24.7331, test/loss=1.77994, test/num_examples=3003, total_duration=22972.1, train/accuracy=0.616923, train/bleu=29.3453, train/loss=1.9333, validation/accuracy=0.629978, validation/bleu=25.4696, validation/loss=1.82464, validation/num_examples=3000
I0306 01:37:27.280112 140104668485376 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.4737880229949951, loss=2.0367913246154785
I0306 01:38:02.223378 140104660092672 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.35248181223869324, loss=1.9033252000808716
I0306 01:38:37.281301 140104668485376 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3828296363353729, loss=1.975089192390442
I0306 01:39:12.352041 140104660092672 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.42917853593826294, loss=1.957556128501892
I0306 01:39:47.369076 140104668485376 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.47873052954673767, loss=2.0570156574249268
I0306 01:40:22.382983 140104660092672 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3635522425174713, loss=1.9944953918457031
I0306 01:40:57.404646 140104668485376 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.4271242022514343, loss=2.044694423675537
I0306 01:41:32.407834 140104660092672 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.31456491351127625, loss=1.932206630706787
I0306 01:42:07.412398 140104668485376 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.28884345293045044, loss=2.0183937549591064
I0306 01:42:42.439035 140104660092672 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.31055888533592224, loss=1.9070758819580078
I0306 01:43:17.417810 140104668485376 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2834946811199188, loss=1.9860436916351318
I0306 01:43:52.417306 140104660092672 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.6120399236679077, loss=2.029486656188965
I0306 01:44:27.363059 140104668485376 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.28664329648017883, loss=2.0368893146514893
I0306 01:45:02.344629 140104660092672 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.5049555897712708, loss=2.0057103633880615
I0306 01:45:37.281066 140104668485376 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.26360976696014404, loss=1.8978512287139893
I0306 01:46:12.206978 140104660092672 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.5007949471473694, loss=2.0103681087493896
I0306 01:46:47.139440 140104668485376 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.38559260964393616, loss=1.967836618423462
I0306 01:47:22.113154 140104660092672 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2620687186717987, loss=1.982099175453186
I0306 01:47:57.078811 140104668485376 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.3469109535217285, loss=2.056736707687378
I0306 01:48:32.041612 140104660092672 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6807302832603455, loss=1.9956947565078735
I0306 01:49:07.005139 140104668485376 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3436174690723419, loss=1.9845465421676636
I0306 01:49:42.039217 140104660092672 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3758399486541748, loss=1.9311423301696777
I0306 01:50:17.051167 140104668485376 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.445952445268631, loss=1.9497607946395874
I0306 01:50:52.104900 140104660092672 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.2429092824459076, loss=1.8860899209976196
I0306 01:50:53.514522 140248407626944 spec.py:321] Evaluating on the training split.
I0306 01:50:56.115684 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:54:04.741235 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 01:54:07.338418 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:56:57.564547 140248407626944 spec.py:349] Evaluating on the test split.
I0306 01:57:00.159022 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 01:59:54.379729 140248407626944 submission_runner.py:469] Time since start: 24353.12s, 	Step: 40805, 	{'train/accuracy': 0.607897937297821, 'train/loss': 1.9986627101898193, 'train/bleu': 29.005081207177977, 'validation/accuracy': 0.6303117275238037, 'validation/loss': 1.8237323760986328, 'validation/bleu': 25.89962378845285, 'validation/num_examples': 3000, 'test/accuracy': 0.6384196281433105, 'test/loss': 1.772047758102417, 'test/bleu': 24.974593643082358, 'test/num_examples': 3003, 'score': 14306.475115776062, 'total_duration': 24353.115441560745, 'accumulated_submission_time': 14306.475115776062, 'accumulated_eval_time': 10043.873878240585, 'accumulated_logging_time': 0.3443717956542969}
I0306 01:59:54.393275 140104668485376 logging_writer.py:48] [40805] accumulated_eval_time=10043.9, accumulated_logging_time=0.344372, accumulated_submission_time=14306.5, global_step=40805, preemption_count=0, score=14306.5, test/accuracy=0.63842, test/bleu=24.9746, test/loss=1.77205, test/num_examples=3003, total_duration=24353.1, train/accuracy=0.607898, train/bleu=29.0051, train/loss=1.99866, validation/accuracy=0.630312, validation/bleu=25.8996, validation/loss=1.82373, validation/num_examples=3000
I0306 02:00:27.900397 140104660092672 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.4845333397388458, loss=2.0205583572387695
I0306 02:01:02.863989 140104668485376 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.626295804977417, loss=1.9312806129455566
I0306 02:01:37.842626 140104660092672 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.4367740750312805, loss=1.9829641580581665
I0306 02:02:12.846644 140104668485376 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.22193491458892822, loss=1.957362174987793
I0306 02:02:47.862695 140104660092672 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.43655434250831604, loss=1.9719767570495605
I0306 02:03:22.887606 140104668485376 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.31335344910621643, loss=1.9307132959365845
I0306 02:03:57.879766 140104660092672 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.49306055903434753, loss=2.0415923595428467
I0306 02:04:32.861625 140104668485376 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.38799533247947693, loss=2.04550838470459
I0306 02:05:07.851295 140104660092672 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2711981236934662, loss=2.0287697315216064
I0306 02:05:42.858047 140104668485376 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.2961633503437042, loss=1.9468597173690796
I0306 02:06:17.867301 140104660092672 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.46184661984443665, loss=1.8931621313095093
I0306 02:06:52.909507 140104668485376 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.29756951332092285, loss=2.07893705368042
I0306 02:07:27.908831 140104660092672 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.3937939703464508, loss=1.9019502401351929
I0306 02:08:02.893841 140104668485376 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3455340266227722, loss=1.986771821975708
I0306 02:08:37.860459 140104660092672 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.5652633905410767, loss=1.9455451965332031
I0306 02:09:12.846265 140104668485376 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.447532057762146, loss=2.040034294128418
I0306 02:09:47.826260 140104660092672 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.32305923104286194, loss=1.954472541809082
I0306 02:10:22.808814 140104668485376 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.43898358941078186, loss=2.02968430519104
I0306 02:10:57.820934 140104660092672 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.27335047721862793, loss=1.945669174194336
I0306 02:11:32.806788 140104668485376 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.30197760462760925, loss=1.868580937385559
I0306 02:12:07.829510 140104660092672 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.29224976897239685, loss=1.9368926286697388
I0306 02:12:42.813840 140104668485376 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.45357435941696167, loss=1.9910142421722412
I0306 02:13:17.794592 140104660092672 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.4333784580230713, loss=1.9528765678405762
I0306 02:13:52.761961 140104668485376 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.31532204151153564, loss=1.9039368629455566
I0306 02:13:54.514835 140248407626944 spec.py:321] Evaluating on the training split.
I0306 02:13:57.110701 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 02:17:33.812656 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 02:17:36.410304 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 02:20:28.800537 140248407626944 spec.py:349] Evaluating on the test split.
I0306 02:20:31.395742 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 02:23:22.926991 140248407626944 submission_runner.py:469] Time since start: 25761.66s, 	Step: 43206, 	{'train/accuracy': 0.6099070906639099, 'train/loss': 1.996186375617981, 'train/bleu': 29.378764128653852, 'validation/accuracy': 0.6331421732902527, 'validation/loss': 1.8111562728881836, 'validation/bleu': 25.968980067543747, 'validation/num_examples': 3000, 'test/accuracy': 0.642022967338562, 'test/loss': 1.7490240335464478, 'test/bleu': 25.563272391854316, 'test/num_examples': 3003, 'score': 15146.454290151596, 'total_duration': 25761.662665843964, 'accumulated_submission_time': 15146.454290151596, 'accumulated_eval_time': 10612.285935878754, 'accumulated_logging_time': 0.36600255966186523}
I0306 02:23:22.942002 140104660092672 logging_writer.py:48] [43206] accumulated_eval_time=10612.3, accumulated_logging_time=0.366003, accumulated_submission_time=15146.5, global_step=43206, preemption_count=0, score=15146.5, test/accuracy=0.642023, test/bleu=25.5633, test/loss=1.74902, test/num_examples=3003, total_duration=25761.7, train/accuracy=0.609907, train/bleu=29.3788, train/loss=1.99619, validation/accuracy=0.633142, validation/bleu=25.969, validation/loss=1.81116, validation/num_examples=3000
I0306 02:23:56.147459 140104668485376 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3283933401107788, loss=1.9957154989242554
I0306 02:24:31.088884 140104660092672 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.33471232652664185, loss=1.9689875841140747
I0306 02:25:06.139260 140104668485376 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.27632805705070496, loss=2.001267671585083
I0306 02:25:41.186764 140104660092672 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.30030497908592224, loss=1.9848419427871704
I0306 02:26:16.217826 140104668485376 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.30526861548423767, loss=2.020270586013794
I0306 02:26:51.203099 140104660092672 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.30620846152305603, loss=1.8277935981750488
I0306 02:27:26.211627 140104668485376 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.2885649800300598, loss=1.944448709487915
I0306 02:28:01.234799 140104660092672 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.30624887347221375, loss=1.9706319570541382
I0306 02:28:36.207618 140104668485376 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.3035690188407898, loss=1.97927725315094
I0306 02:29:11.162875 140104660092672 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.5602536797523499, loss=2.000377893447876
I0306 02:29:46.146287 140104668485376 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.38728752732276917, loss=1.9511767625808716
I0306 02:30:21.136357 140104660092672 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.32145559787750244, loss=1.8964920043945312
I0306 02:30:56.149051 140104668485376 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.3439355194568634, loss=1.9305393695831299
I0306 02:31:31.149417 140104660092672 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2913149893283844, loss=1.9715282917022705
I0306 02:32:06.126464 140104668485376 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2904202342033386, loss=1.9248636960983276
I0306 02:32:41.135343 140104660092672 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.31158706545829773, loss=2.0510129928588867
I0306 02:33:16.133363 140104668485376 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.32917922735214233, loss=2.011312484741211
I0306 02:33:51.100502 140104660092672 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.300779789686203, loss=1.9280210733413696
I0306 02:34:26.069955 140104668485376 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.2988220751285553, loss=1.936208724975586
I0306 02:35:01.031003 140104660092672 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3825969994068146, loss=1.880637526512146
I0306 02:35:36.020767 140104668485376 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3359433114528656, loss=1.9387876987457275
I0306 02:36:11.038846 140104660092672 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3266165256500244, loss=1.9487277269363403
I0306 02:36:46.027645 140104668485376 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.5432462096214294, loss=1.9340921640396118
I0306 02:37:21.069148 140104660092672 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.32550710439682007, loss=1.9202731847763062
I0306 02:37:23.176268 140248407626944 spec.py:321] Evaluating on the training split.
I0306 02:37:25.778343 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 02:40:47.996549 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 02:40:50.594887 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 02:43:34.173087 140248407626944 spec.py:349] Evaluating on the test split.
I0306 02:43:36.763492 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 02:46:20.496231 140248407626944 submission_runner.py:469] Time since start: 27139.23s, 	Step: 45607, 	{'train/accuracy': 0.6180210709571838, 'train/loss': 1.938862681388855, 'train/bleu': 29.55189084693673, 'validation/accuracy': 0.6367760300636292, 'validation/loss': 1.7869443893432617, 'validation/bleu': 26.184702858415104, 'validation/num_examples': 3000, 'test/accuracy': 0.6419302821159363, 'test/loss': 1.7406638860702515, 'test/bleu': 25.08556920046382, 'test/num_examples': 3003, 'score': 15986.543020248413, 'total_duration': 27139.23193717003, 'accumulated_submission_time': 15986.543020248413, 'accumulated_eval_time': 11149.60583281517, 'accumulated_logging_time': 0.39058470726013184}
I0306 02:46:20.510076 140104668485376 logging_writer.py:48] [45607] accumulated_eval_time=11149.6, accumulated_logging_time=0.390585, accumulated_submission_time=15986.5, global_step=45607, preemption_count=0, score=15986.5, test/accuracy=0.64193, test/bleu=25.0856, test/loss=1.74066, test/num_examples=3003, total_duration=27139.2, train/accuracy=0.618021, train/bleu=29.5519, train/loss=1.93886, validation/accuracy=0.636776, validation/bleu=26.1847, validation/loss=1.78694, validation/num_examples=3000
I0306 02:46:53.332592 140104660092672 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.29672038555145264, loss=1.993415355682373
I0306 02:47:28.256656 140104668485376 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.31137022376060486, loss=2.006838083267212
I0306 02:48:03.268070 140104660092672 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.4230481684207916, loss=2.058539390563965
I0306 02:48:38.242878 140104668485376 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3424954414367676, loss=1.9500255584716797
I0306 02:49:13.239738 140104660092672 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.6596549153327942, loss=1.9054206609725952
I0306 02:49:48.222236 140104668485376 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.29293131828308105, loss=2.046578884124756
I0306 02:50:23.247676 140104660092672 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.6147670745849609, loss=1.9208228588104248
I0306 02:50:58.248677 140104668485376 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3181140124797821, loss=1.9373375177383423
I0306 02:51:33.319121 140104660092672 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.32772746682167053, loss=2.024195432662964
I0306 02:52:08.320498 140104668485376 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3778230845928192, loss=1.9517011642456055
I0306 02:52:43.341176 140104660092672 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3405373692512512, loss=1.864578127861023
I0306 02:53:18.360578 140104668485376 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2741749882698059, loss=2.0348434448242188
I0306 02:53:53.380239 140104660092672 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.41875776648521423, loss=2.0317580699920654
I0306 02:54:28.397662 140104668485376 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.38591307401657104, loss=1.9299370050430298
I0306 02:55:03.391728 140104660092672 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.534972071647644, loss=2.009445905685425
I0306 02:55:38.401407 140104668485376 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.45864373445510864, loss=1.9549479484558105
I0306 02:56:13.442898 140104660092672 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.5279331207275391, loss=1.945159912109375
I0306 02:56:48.481464 140104668485376 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3754868507385254, loss=1.9885469675064087
I0306 02:57:23.517608 140104660092672 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.35014328360557556, loss=1.9015462398529053
I0306 02:57:58.610117 140104668485376 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.4065111577510834, loss=2.019465208053589
I0306 02:58:33.604481 140104660092672 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.32319381833076477, loss=2.003645181655884
I0306 02:59:08.646856 140104668485376 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3526891767978668, loss=1.9914764165878296
I0306 02:59:43.656602 140104660092672 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.34516164660453796, loss=1.9354904890060425
I0306 03:00:18.654101 140104668485376 logging_writer.py:48] [48000] global_step=48000, grad_norm=4.198657512664795, loss=2.0247323513031006
I0306 03:00:20.763872 140248407626944 spec.py:321] Evaluating on the training split.
I0306 03:00:23.358805 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:03:27.924216 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 03:03:30.514745 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:06:32.164334 140248407626944 spec.py:349] Evaluating on the test split.
I0306 03:06:34.763974 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:09:09.355616 140248407626944 submission_runner.py:469] Time since start: 28508.09s, 	Step: 48007, 	{'train/accuracy': 0.61665278673172, 'train/loss': 1.9455482959747314, 'train/bleu': 28.975495628399077, 'validation/accuracy': 0.6345264911651611, 'validation/loss': 1.8003181219100952, 'validation/bleu': 26.03076655066841, 'validation/num_examples': 3000, 'test/accuracy': 0.6408759355545044, 'test/loss': 1.737618088722229, 'test/bleu': 25.14725041646701, 'test/num_examples': 3003, 'score': 16826.650903701782, 'total_duration': 28508.091321468353, 'accumulated_submission_time': 16826.650903701782, 'accumulated_eval_time': 11678.197509527206, 'accumulated_logging_time': 0.4134480953216553}
I0306 03:09:09.369092 140104660092672 logging_writer.py:48] [48007] accumulated_eval_time=11678.2, accumulated_logging_time=0.413448, accumulated_submission_time=16826.7, global_step=48007, preemption_count=0, score=16826.7, test/accuracy=0.640876, test/bleu=25.1473, test/loss=1.73762, test/num_examples=3003, total_duration=28508.1, train/accuracy=0.616653, train/bleu=28.9755, train/loss=1.94555, validation/accuracy=0.634526, validation/bleu=26.0308, validation/loss=1.80032, validation/num_examples=3000
I0306 03:09:42.215494 140104668485376 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.2898060381412506, loss=1.9768519401550293
I0306 03:10:17.141014 140104660092672 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.28818875551223755, loss=1.9596081972122192
I0306 03:10:52.127746 140104668485376 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.4710911810398102, loss=1.9888889789581299
I0306 03:11:27.091051 140104660092672 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.3161514103412628, loss=1.9711414575576782
I0306 03:12:02.069016 140104668485376 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.29383334517478943, loss=1.9622304439544678
I0306 03:12:37.103395 140104660092672 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.31950390338897705, loss=1.8511254787445068
I0306 03:13:12.157702 140104668485376 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3283860981464386, loss=1.935379981994629
I0306 03:13:47.186539 140104660092672 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.37724435329437256, loss=1.8899339437484741
I0306 03:14:22.139067 140104668485376 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2983468770980835, loss=1.8821030855178833
I0306 03:14:57.140046 140104660092672 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.5457049608230591, loss=1.9249370098114014
I0306 03:15:32.129879 140104668485376 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.35666656494140625, loss=1.9624898433685303
I0306 03:16:07.170433 140104660092672 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.336795449256897, loss=1.9785151481628418
I0306 03:16:42.186259 140104668485376 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3382331430912018, loss=1.982608437538147
I0306 03:17:17.193334 140104660092672 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2819540798664093, loss=1.9975180625915527
I0306 03:17:52.172435 140104668485376 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.44944772124290466, loss=1.9316774606704712
I0306 03:18:27.170000 140104660092672 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.29437384009361267, loss=2.018436908721924
I0306 03:19:02.169398 140104668485376 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.27988871932029724, loss=1.8039915561676025
I0306 03:19:37.192612 140104660092672 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.28547003865242004, loss=1.9850348234176636
I0306 03:20:12.192339 140104668485376 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3592210114002228, loss=1.9943767786026
I0306 03:20:47.193329 140104660092672 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.33300650119781494, loss=1.9320307970046997
I0306 03:21:22.194990 140104668485376 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.46772125363349915, loss=2.045539617538452
I0306 03:21:57.118967 140104660092672 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.30929145216941833, loss=1.9708924293518066
I0306 03:22:32.019972 140104668485376 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.4115675389766693, loss=1.9081264734268188
I0306 03:23:06.897065 140104660092672 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.5258285999298096, loss=1.9319685697555542
I0306 03:23:09.702023 140248407626944 spec.py:321] Evaluating on the training split.
I0306 03:23:12.304661 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:28:07.605148 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 03:28:10.200123 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:33:04.963618 140248407626944 spec.py:349] Evaluating on the test split.
I0306 03:33:07.560758 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:38:01.770691 140248407626944 submission_runner.py:469] Time since start: 30240.51s, 	Step: 50409, 	{'train/accuracy': 0.6405792236328125, 'train/loss': 1.7554317712783813, 'train/bleu': 30.948378319609148, 'validation/accuracy': 0.6364299654960632, 'validation/loss': 1.7731913328170776, 'validation/bleu': 23.44060198108962, 'validation/num_examples': 3000, 'test/accuracy': 0.6470745205879211, 'test/loss': 1.713481068611145, 'test/bleu': 24.99513706270935, 'test/num_examples': 3003, 'score': 17666.842764616013, 'total_duration': 30240.50640106201, 'accumulated_submission_time': 17666.842764616013, 'accumulated_eval_time': 12570.26611495018, 'accumulated_logging_time': 0.4349939823150635}
I0306 03:38:01.784806 140104668485376 logging_writer.py:48] [50409] accumulated_eval_time=12570.3, accumulated_logging_time=0.434994, accumulated_submission_time=17666.8, global_step=50409, preemption_count=0, score=17666.8, test/accuracy=0.647075, test/bleu=24.9951, test/loss=1.71348, test/num_examples=3003, total_duration=30240.5, train/accuracy=0.640579, train/bleu=30.9484, train/loss=1.75543, validation/accuracy=0.63643, validation/bleu=23.4406, validation/loss=1.77319, validation/num_examples=3000
I0306 03:38:33.821951 140104660092672 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3077884614467621, loss=1.898126482963562
I0306 03:39:08.675345 140104668485376 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.6032643914222717, loss=1.9754798412322998
I0306 03:39:43.566879 140104660092672 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.2923448085784912, loss=1.8751124143600464
I0306 03:40:18.460941 140104668485376 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2977822422981262, loss=1.8379477262496948
I0306 03:40:53.376144 140104660092672 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.30463480949401855, loss=1.9969395399093628
I0306 03:41:28.264075 140104668485376 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.44088536500930786, loss=1.8523117303848267
I0306 03:42:03.145138 140104660092672 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.4106181859970093, loss=1.9853297472000122
I0306 03:42:38.019784 140104668485376 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.47870001196861267, loss=1.9817395210266113
I0306 03:43:12.922358 140104660092672 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.32352766394615173, loss=2.0059921741485596
I0306 03:43:47.816257 140104668485376 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.30903616547584534, loss=1.894039511680603
I0306 03:44:22.716418 140104660092672 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.509068489074707, loss=1.8982603549957275
I0306 03:44:57.622290 140104668485376 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.34394630789756775, loss=1.953480839729309
I0306 03:45:32.527746 140104660092672 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.26754939556121826, loss=1.9127589464187622
I0306 03:46:07.416926 140104668485376 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.562242329120636, loss=1.992232084274292
I0306 03:46:42.312784 140104660092672 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3317807912826538, loss=1.985329508781433
I0306 03:47:17.180088 140104668485376 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3362501859664917, loss=1.9987597465515137
I0306 03:47:52.039361 140104660092672 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.5618369579315186, loss=1.8760966062545776
I0306 03:48:26.913421 140104668485376 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.38848409056663513, loss=1.9137412309646606
I0306 03:49:01.788696 140104660092672 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.26338768005371094, loss=1.854811191558838
I0306 03:49:36.643504 140104668485376 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.2921503484249115, loss=1.8938754796981812
I0306 03:50:11.510462 140104660092672 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.27966946363449097, loss=1.9181691408157349
I0306 03:50:46.372806 140104668485376 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.28331437706947327, loss=1.9431747198104858
I0306 03:51:21.263535 140104660092672 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.36240068078041077, loss=1.874300479888916
I0306 03:51:56.113236 140104668485376 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.34189602732658386, loss=1.97841477394104
I0306 03:52:02.057957 140248407626944 spec.py:321] Evaluating on the training split.
I0306 03:52:04.663986 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:56:36.666523 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 03:56:39.267646 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 03:59:24.151022 140248407626944 spec.py:349] Evaluating on the test split.
I0306 03:59:26.749059 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 04:02:06.314822 140248407626944 submission_runner.py:469] Time since start: 31685.05s, 	Step: 52818, 	{'train/accuracy': 0.621373176574707, 'train/loss': 1.909628987312317, 'train/bleu': 29.404804899780526, 'validation/accuracy': 0.64076828956604, 'validation/loss': 1.761560082435608, 'validation/bleu': 26.379700089759037, 'validation/num_examples': 3000, 'test/accuracy': 0.6488587856292725, 'test/loss': 1.7041263580322266, 'test/bleu': 25.814511785082797, 'test/num_examples': 3003, 'score': 18506.974251031876, 'total_duration': 31685.050538539886, 'accumulated_submission_time': 18506.974251031876, 'accumulated_eval_time': 13174.522924900055, 'accumulated_logging_time': 0.45745110511779785}
I0306 04:02:06.329834 140104660092672 logging_writer.py:48] [52818] accumulated_eval_time=13174.5, accumulated_logging_time=0.457451, accumulated_submission_time=18507, global_step=52818, preemption_count=0, score=18507, test/accuracy=0.648859, test/bleu=25.8145, test/loss=1.70413, test/num_examples=3003, total_duration=31685.1, train/accuracy=0.621373, train/bleu=29.4048, train/loss=1.90963, validation/accuracy=0.640768, validation/bleu=26.3797, validation/loss=1.76156, validation/num_examples=3000
I0306 04:02:35.221811 140104668485376 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3336784541606903, loss=1.8331903219223022
I0306 04:03:10.083998 140104660092672 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.4536479413509369, loss=2.036881923675537
I0306 04:03:44.965130 140104668485376 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.3212931156158447, loss=2.028580665588379
I0306 04:04:19.846482 140104660092672 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.3122813403606415, loss=2.0308291912078857
I0306 04:04:54.721533 140104668485376 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.4412728548049927, loss=2.0028107166290283
I0306 04:05:29.591950 140104660092672 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3461168110370636, loss=1.9837089776992798
I0306 04:06:04.430776 140104668485376 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.36723998188972473, loss=1.7974964380264282
I0306 04:06:39.304152 140104660092672 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.43600985407829285, loss=1.852338194847107
I0306 04:07:14.175653 140104668485376 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2801734209060669, loss=1.867763876914978
I0306 04:07:48.999491 140104660092672 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.34143683314323425, loss=1.883152961730957
I0306 04:08:23.873032 140104668485376 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.37993669509887695, loss=1.9077403545379639
I0306 04:08:58.749404 140104660092672 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3656325042247772, loss=1.8623936176300049
I0306 04:09:33.673783 140104668485376 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2822331488132477, loss=1.9725793600082397
I0306 04:10:08.611593 140104660092672 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.264748215675354, loss=1.9703770875930786
I0306 04:10:43.506966 140104668485376 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3512929379940033, loss=1.9980963468551636
I0306 04:11:18.406365 140104660092672 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.34221234917640686, loss=1.9476238489151
I0306 04:11:53.309838 140104668485376 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3641268312931061, loss=1.8468769788742065
I0306 04:12:28.182543 140104660092672 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.4156949520111084, loss=1.8887907266616821
I0306 04:13:03.095697 140104668485376 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.294030100107193, loss=1.9183324575424194
I0306 04:13:38.011659 140104660092672 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.27164891362190247, loss=1.94500732421875
I0306 04:14:12.933319 140104668485376 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.267825722694397, loss=2.0235981941223145
I0306 04:14:47.856286 140104660092672 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.28152552247047424, loss=1.829864263534546
I0306 04:15:22.794117 140104668485376 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.27292361855506897, loss=1.9460622072219849
I0306 04:15:57.686521 140104660092672 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.285553902387619, loss=1.8900268077850342
I0306 04:16:06.406350 140248407626944 spec.py:321] Evaluating on the training split.
I0306 04:16:09.008303 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 04:19:47.124740 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 04:19:49.724447 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 04:22:33.917882 140248407626944 spec.py:349] Evaluating on the test split.
I0306 04:22:36.518684 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 04:24:58.823752 140248407626944 submission_runner.py:469] Time since start: 33057.56s, 	Step: 55226, 	{'train/accuracy': 0.6187305450439453, 'train/loss': 1.9316372871398926, 'train/bleu': 29.710605439586914, 'validation/accuracy': 0.6412750482559204, 'validation/loss': 1.7546164989471436, 'validation/bleu': 26.281337249560597, 'validation/num_examples': 3000, 'test/accuracy': 0.649623453617096, 'test/loss': 1.695084571838379, 'test/bleu': 25.701607392065284, 'test/num_examples': 3003, 'score': 19346.908233880997, 'total_duration': 33057.55945301056, 'accumulated_submission_time': 19346.908233880997, 'accumulated_eval_time': 13706.940255641937, 'accumulated_logging_time': 0.4806942939758301}
I0306 04:24:58.838554 140104668485376 logging_writer.py:48] [55226] accumulated_eval_time=13706.9, accumulated_logging_time=0.480694, accumulated_submission_time=19346.9, global_step=55226, preemption_count=0, score=19346.9, test/accuracy=0.649623, test/bleu=25.7016, test/loss=1.69508, test/num_examples=3003, total_duration=33057.6, train/accuracy=0.618731, train/bleu=29.7106, train/loss=1.93164, validation/accuracy=0.641275, validation/bleu=26.2813, validation/loss=1.75462, validation/num_examples=3000
I0306 04:25:24.972519 140104660092672 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3098064661026001, loss=1.925474762916565
I0306 04:25:59.828925 140104668485376 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.28129518032073975, loss=1.9310567378997803
I0306 04:26:34.745493 140104660092672 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.30726706981658936, loss=1.9093996286392212
I0306 04:27:09.652890 140104668485376 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3506670892238617, loss=1.9230587482452393
I0306 04:27:44.558023 140104660092672 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.36800602078437805, loss=1.8835334777832031
I0306 04:28:19.453016 140104668485376 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2651592791080475, loss=1.8396663665771484
I0306 04:28:54.342252 140104660092672 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.48524513840675354, loss=1.9125844240188599
I0306 04:29:29.261004 140104668485376 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2665528357028961, loss=1.8711981773376465
I0306 04:30:04.174594 140104660092672 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.33737415075302124, loss=1.9675443172454834
I0306 04:30:39.058687 140104668485376 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2793768644332886, loss=1.801586627960205
I0306 04:31:13.966664 140104660092672 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.2756137549877167, loss=1.8544507026672363
I0306 04:31:48.880605 140104668485376 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.41347765922546387, loss=1.8855050802230835
I0306 04:32:23.813525 140104660092672 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3254067599773407, loss=1.9059125185012817
I0306 04:32:58.804207 140104668485376 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.3820491433143616, loss=1.8608733415603638
I0306 04:33:33.790376 140104660092672 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.352977454662323, loss=1.8839973211288452
I0306 04:34:08.743296 140104668485376 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.2867158353328705, loss=1.8943902254104614
I0306 04:34:43.734317 140104660092672 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.3152090311050415, loss=1.8904412984848022
I0306 04:35:18.704547 140104668485376 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3230583965778351, loss=1.9695862531661987
I0306 04:35:53.727812 140104660092672 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.39071956276893616, loss=1.885640263557434
I0306 04:36:28.738340 140104668485376 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.33392444252967834, loss=2.0023391246795654
I0306 04:37:03.753852 140104660092672 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3484903872013092, loss=1.9206596612930298
I0306 04:37:38.748156 140104668485376 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3034673035144806, loss=1.9051268100738525
I0306 04:38:13.725470 140104660092672 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.2732166051864624, loss=1.8019993305206299
I0306 04:38:48.694998 140104668485376 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2777540981769562, loss=1.8570435047149658
I0306 04:38:58.848091 140248407626944 spec.py:321] Evaluating on the training split.
I0306 04:39:01.451509 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 04:42:47.599558 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 04:42:50.197714 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 04:46:03.420943 140248407626944 spec.py:349] Evaluating on the test split.
I0306 04:46:06.010466 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 04:49:09.593074 140248407626944 submission_runner.py:469] Time since start: 34508.33s, 	Step: 57630, 	{'train/accuracy': 0.6252249479293823, 'train/loss': 1.862509846687317, 'train/bleu': 30.217214815072033, 'validation/accuracy': 0.6423380374908447, 'validation/loss': 1.7404899597167969, 'validation/bleu': 26.641212112606315, 'validation/num_examples': 3000, 'test/accuracy': 0.6520913243293762, 'test/loss': 1.6839854717254639, 'test/bleu': 26.357387294050795, 'test/num_examples': 3003, 'score': 20186.779103040695, 'total_duration': 34508.328780412674, 'accumulated_submission_time': 20186.779103040695, 'accumulated_eval_time': 14317.685172319412, 'accumulated_logging_time': 0.5038979053497314}
I0306 04:49:09.608088 140104660092672 logging_writer.py:48] [57630] accumulated_eval_time=14317.7, accumulated_logging_time=0.503898, accumulated_submission_time=20186.8, global_step=57630, preemption_count=0, score=20186.8, test/accuracy=0.652091, test/bleu=26.3574, test/loss=1.68399, test/num_examples=3003, total_duration=34508.3, train/accuracy=0.625225, train/bleu=30.2172, train/loss=1.86251, validation/accuracy=0.642338, validation/bleu=26.6412, validation/loss=1.74049, validation/num_examples=3000
I0306 04:49:34.360070 140104668485376 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.34545406699180603, loss=1.84612238407135
I0306 04:50:09.275450 140104660092672 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2855445146560669, loss=1.8994848728179932
I0306 04:50:44.215404 140104668485376 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.3185615539550781, loss=1.9004243612289429
I0306 04:51:19.176140 140104660092672 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.33387869596481323, loss=1.8526701927185059
I0306 04:51:54.115990 140104668485376 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.32095441222190857, loss=1.869274377822876
I0306 04:52:29.088067 140104660092672 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.36409929394721985, loss=1.9572523832321167
I0306 04:53:04.005280 140104668485376 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3233873248100281, loss=1.957736849784851
I0306 04:53:38.972160 140104660092672 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2808176577091217, loss=1.8507763147354126
I0306 04:54:13.878927 140104668485376 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.36112409830093384, loss=1.8241753578186035
I0306 04:54:48.836129 140104660092672 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.36722153425216675, loss=1.8434590101242065
I0306 04:55:23.772735 140104668485376 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.27343684434890747, loss=1.8408303260803223
I0306 04:55:58.730893 140104660092672 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3013516962528229, loss=1.9067165851593018
I0306 04:56:33.641250 140104668485376 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.28635308146476746, loss=1.8938586711883545
I0306 04:57:08.610787 140104660092672 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3618829548358917, loss=1.9819433689117432
I0306 04:57:43.524024 140104668485376 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.37923750281333923, loss=1.9800678491592407
I0306 04:58:18.461476 140104660092672 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.30876439809799194, loss=1.944333791732788
I0306 04:58:53.398400 140104668485376 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.3404652178287506, loss=1.9719204902648926
I0306 04:59:28.363857 140104660092672 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.36121776700019836, loss=1.9293392896652222
I0306 05:00:03.365629 140104668485376 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.28483107686042786, loss=1.9352619647979736
I0306 05:00:38.333241 140104660092672 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3076910972595215, loss=1.9452345371246338
I0306 05:01:13.340098 140104668485376 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.38708287477493286, loss=1.9038053750991821
I0306 05:01:48.322125 140104660092672 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.28654178977012634, loss=1.9122473001480103
I0306 05:02:23.318812 140104668485376 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3012774884700775, loss=1.7417374849319458
I0306 05:02:58.345902 140104660092672 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.34932950139045715, loss=1.9193652868270874
I0306 05:03:09.913899 140248407626944 spec.py:321] Evaluating on the training split.
I0306 05:03:12.527398 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:06:14.810871 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 05:06:17.413272 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:09:04.806039 140248407626944 spec.py:349] Evaluating on the test split.
I0306 05:09:07.408470 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:11:41.803996 140248407626944 submission_runner.py:469] Time since start: 35860.54s, 	Step: 60034, 	{'train/accuracy': 0.623226523399353, 'train/loss': 1.89035964012146, 'train/bleu': 29.752940184660318, 'validation/accuracy': 0.6430054903030396, 'validation/loss': 1.7353914976119995, 'validation/bleu': 26.792112275242218, 'validation/num_examples': 3000, 'test/accuracy': 0.6558915376663208, 'test/loss': 1.6648130416870117, 'test/bleu': 26.092712384391096, 'test/num_examples': 3003, 'score': 21026.94348526001, 'total_duration': 35860.53970384598, 'accumulated_submission_time': 21026.94348526001, 'accumulated_eval_time': 14829.575204849243, 'accumulated_logging_time': 0.5270743370056152}
I0306 05:11:41.819098 140104668485376 logging_writer.py:48] [60034] accumulated_eval_time=14829.6, accumulated_logging_time=0.527074, accumulated_submission_time=21026.9, global_step=60034, preemption_count=0, score=21026.9, test/accuracy=0.655892, test/bleu=26.0927, test/loss=1.66481, test/num_examples=3003, total_duration=35860.5, train/accuracy=0.623227, train/bleu=29.7529, train/loss=1.89036, validation/accuracy=0.643005, validation/bleu=26.7921, validation/loss=1.73539, validation/num_examples=3000
I0306 05:12:05.219873 140104660092672 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.33651962876319885, loss=1.99334716796875
I0306 05:12:40.154544 140104668485376 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3296670913696289, loss=1.793169379234314
I0306 05:13:15.088994 140104660092672 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.3768526613712311, loss=1.8651556968688965
I0306 05:13:50.035480 140104668485376 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2755105495452881, loss=1.9090471267700195
I0306 05:14:24.994996 140104660092672 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.40892931818962097, loss=1.8681236505508423
I0306 05:14:59.978916 140104668485376 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.34122800827026367, loss=1.8697447776794434
I0306 05:15:34.983770 140104660092672 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3030213713645935, loss=1.8770289421081543
I0306 05:16:09.990131 140104668485376 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2982349395751953, loss=1.8040382862091064
I0306 05:16:45.023497 140104660092672 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3510870635509491, loss=1.878408432006836
I0306 05:17:20.064113 140104668485376 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3107745349407196, loss=1.8400551080703735
I0306 05:17:55.089152 140104660092672 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.3194396197795868, loss=1.9397659301757812
I0306 05:18:30.081070 140104668485376 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3415631353855133, loss=1.819190502166748
I0306 05:19:05.121561 140104660092672 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2992299199104309, loss=1.8879872560501099
I0306 05:19:40.134577 140104668485376 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.35083723068237305, loss=1.935246467590332
I0306 05:20:15.132600 140104660092672 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3730422258377075, loss=1.8948386907577515
I0306 05:20:50.086838 140104668485376 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.30869829654693604, loss=1.8328500986099243
I0306 05:21:25.084499 140104660092672 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3333824574947357, loss=1.957362413406372
I0306 05:22:00.061053 140104668485376 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.2937849462032318, loss=1.826271414756775
I0306 05:22:35.027825 140104660092672 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.27971261739730835, loss=1.8343894481658936
I0306 05:23:10.007337 140104668485376 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3201380968093872, loss=1.8866697549819946
I0306 05:23:45.014287 140104660092672 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.32174405455589294, loss=1.8344087600708008
I0306 05:24:19.978394 140104668485376 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.35406142473220825, loss=1.9289088249206543
I0306 05:24:54.945118 140104660092672 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.3057411313056946, loss=1.8851195573806763
I0306 05:25:29.912488 140104668485376 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3170807361602783, loss=1.948715090751648
I0306 05:25:41.809640 140248407626944 spec.py:321] Evaluating on the training split.
I0306 05:25:44.408766 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:29:58.308784 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 05:30:00.907430 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:32:54.715867 140248407626944 spec.py:349] Evaluating on the test split.
I0306 05:32:57.310953 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:36:18.641961 140248407626944 submission_runner.py:469] Time since start: 37337.38s, 	Step: 62435, 	{'train/accuracy': 0.6214150786399841, 'train/loss': 1.8964927196502686, 'train/bleu': 29.592889104671457, 'validation/accuracy': 0.6461449265480042, 'validation/loss': 1.7099868059158325, 'validation/bleu': 26.796261819351862, 'validation/num_examples': 3000, 'test/accuracy': 0.6564360857009888, 'test/loss': 1.6444545984268188, 'test/bleu': 26.18130830435449, 'test/num_examples': 3003, 'score': 21866.79092860222, 'total_duration': 37337.37766337395, 'accumulated_submission_time': 21866.79092860222, 'accumulated_eval_time': 15466.407456874847, 'accumulated_logging_time': 0.5507338047027588}
I0306 05:36:18.657427 140104660092672 logging_writer.py:48] [62435] accumulated_eval_time=15466.4, accumulated_logging_time=0.550734, accumulated_submission_time=21866.8, global_step=62435, preemption_count=0, score=21866.8, test/accuracy=0.656436, test/bleu=26.1813, test/loss=1.64445, test/num_examples=3003, total_duration=37337.4, train/accuracy=0.621415, train/bleu=29.5929, train/loss=1.89649, validation/accuracy=0.646145, validation/bleu=26.7963, validation/loss=1.70999, validation/num_examples=3000
I0306 05:36:41.721794 140104668485376 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.30301833152770996, loss=1.831178069114685
I0306 05:37:16.642602 140104660092672 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.31296321749687195, loss=1.8601336479187012
I0306 05:37:51.629404 140104668485376 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.32483768463134766, loss=1.9051711559295654
I0306 05:38:26.628310 140104660092672 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.30858883261680603, loss=1.850389003753662
I0306 05:39:01.625648 140104668485376 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3660111129283905, loss=1.9022735357284546
I0306 05:39:36.607570 140104660092672 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3237304985523224, loss=1.8568013906478882
I0306 05:40:11.671331 140104668485376 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.2703307271003723, loss=1.836057424545288
I0306 05:40:46.676453 140104660092672 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2961479127407074, loss=1.8457293510437012
I0306 05:41:21.705879 140104668485376 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.2885167598724365, loss=1.8897838592529297
I0306 05:41:56.709792 140104660092672 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3147667348384857, loss=1.8641060590744019
I0306 05:42:31.685271 140104668485376 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3076953887939453, loss=1.8265243768692017
I0306 05:43:06.672333 140104660092672 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2973973751068115, loss=1.7758491039276123
I0306 05:43:41.667124 140104668485376 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.29189562797546387, loss=1.8215209245681763
I0306 05:44:16.668312 140104660092672 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.33651891350746155, loss=1.8102545738220215
I0306 05:44:51.663628 140104668485376 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3151341378688812, loss=1.855832576751709
I0306 05:45:26.674903 140104660092672 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2858085632324219, loss=1.8306595087051392
I0306 05:46:01.609472 140104668485376 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3256317377090454, loss=1.9317761659622192
I0306 05:46:36.554718 140104660092672 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.3437110483646393, loss=1.8918192386627197
I0306 05:47:11.517057 140104668485376 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.35940808057785034, loss=1.8641340732574463
I0306 05:47:46.468605 140104660092672 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.35132744908332825, loss=1.8481627702713013
I0306 05:48:21.419265 140104668485376 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.276294082403183, loss=1.8831902742385864
I0306 05:48:56.354427 140104660092672 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3754587769508362, loss=1.9111554622650146
I0306 05:49:31.302254 140104668485376 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3701455295085907, loss=1.8222777843475342
I0306 05:50:06.218629 140104660092672 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.35272902250289917, loss=1.9405057430267334
I0306 05:50:18.799452 140248407626944 spec.py:321] Evaluating on the training split.
I0306 05:50:21.403686 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:54:45.163863 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 05:54:47.769036 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 05:57:53.727141 140248407626944 spec.py:349] Evaluating on the test split.
I0306 05:57:56.325378 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 06:01:08.317970 140248407626944 submission_runner.py:469] Time since start: 38827.05s, 	Step: 64837, 	{'train/accuracy': 0.6294022798538208, 'train/loss': 1.8417770862579346, 'train/bleu': 30.20044197552171, 'validation/accuracy': 0.646837055683136, 'validation/loss': 1.7003849744796753, 'validation/bleu': 26.915054761490847, 'validation/num_examples': 3000, 'test/accuracy': 0.6591588258743286, 'test/loss': 1.6346181631088257, 'test/bleu': 26.55129252816776, 'test/num_examples': 3003, 'score': 22706.78688764572, 'total_duration': 38827.0536634922, 'accumulated_submission_time': 22706.78688764572, 'accumulated_eval_time': 16115.925898313522, 'accumulated_logging_time': 0.5742483139038086}
I0306 06:01:08.334362 140104668485376 logging_writer.py:48] [64837] accumulated_eval_time=16115.9, accumulated_logging_time=0.574248, accumulated_submission_time=22706.8, global_step=64837, preemption_count=0, score=22706.8, test/accuracy=0.659159, test/bleu=26.5513, test/loss=1.63462, test/num_examples=3003, total_duration=38827.1, train/accuracy=0.629402, train/bleu=30.2004, train/loss=1.84178, validation/accuracy=0.646837, validation/bleu=26.9151, validation/loss=1.70038, validation/num_examples=3000
I0306 06:01:30.678243 140104660092672 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2909051775932312, loss=1.9002364873886108
I0306 06:02:05.613797 140104668485376 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3276074230670929, loss=1.8762893676757812
I0306 06:02:40.595222 140104660092672 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3295612633228302, loss=1.837255835533142
I0306 06:03:15.596035 140104668485376 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3184870779514313, loss=1.9218487739562988
I0306 06:03:50.607200 140104660092672 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.29425308108329773, loss=1.8518309593200684
I0306 06:04:25.628667 140104668485376 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.32203465700149536, loss=1.8772914409637451
I0306 06:05:00.639210 140104660092672 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.313545823097229, loss=1.8869073390960693
I0306 06:05:35.653733 140104668485376 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.30463263392448425, loss=1.8047250509262085
I0306 06:06:10.636623 140104660092672 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.36047109961509705, loss=1.904922604560852
I0306 06:06:45.611775 140104668485376 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.30157971382141113, loss=1.8065769672393799
I0306 06:07:20.584605 140104660092672 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.300229549407959, loss=1.821853518486023
I0306 06:07:55.551942 140104668485376 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.27821165323257446, loss=1.7895177602767944
I0306 06:08:30.520871 140104660092672 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2961055636405945, loss=1.825427532196045
I0306 06:09:05.521323 140104668485376 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.29972395300865173, loss=1.8137556314468384
I0306 06:09:40.530845 140104660092672 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2785075902938843, loss=1.8716373443603516
I0306 06:10:15.395776 140104668485376 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2931046783924103, loss=1.7783116102218628
I0306 06:10:50.334732 140104660092672 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.35975274443626404, loss=1.9485512971878052
I0306 06:11:25.310520 140104668485376 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2955211400985718, loss=1.791746735572815
I0306 06:12:00.215335 140104660092672 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.33349111676216125, loss=1.9064136743545532
I0306 06:12:35.195068 140104668485376 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.28383126854896545, loss=1.814918041229248
I0306 06:13:10.147392 140104660092672 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.3040814995765686, loss=1.8730632066726685
I0306 06:13:45.145893 140104668485376 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.36336982250213623, loss=1.938120722770691
I0306 06:14:20.135005 140104660092672 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.29749569296836853, loss=1.8765721321105957
I0306 06:14:55.162392 140104668485376 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.30801084637641907, loss=1.8196192979812622
I0306 06:15:08.477326 140248407626944 spec.py:321] Evaluating on the training split.
I0306 06:15:11.080561 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 06:18:13.293863 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 06:18:15.889537 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 06:21:06.882819 140248407626944 spec.py:349] Evaluating on the test split.
I0306 06:21:09.475780 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 06:23:54.981112 140248407626944 submission_runner.py:469] Time since start: 40193.72s, 	Step: 67239, 	{'train/accuracy': 0.6307945251464844, 'train/loss': 1.847119688987732, 'train/bleu': 30.530028128689345, 'validation/accuracy': 0.649642825126648, 'validation/loss': 1.6922351121902466, 'validation/bleu': 27.193436361678348, 'validation/num_examples': 3000, 'test/accuracy': 0.6626694798469543, 'test/loss': 1.6143981218338013, 'test/bleu': 27.00615756210425, 'test/num_examples': 3003, 'score': 23546.788079977036, 'total_duration': 40193.716821432114, 'accumulated_submission_time': 23546.788079977036, 'accumulated_eval_time': 16642.429622888565, 'accumulated_logging_time': 0.5989913940429688}
I0306 06:23:54.998043 140104660092672 logging_writer.py:48] [67239] accumulated_eval_time=16642.4, accumulated_logging_time=0.598991, accumulated_submission_time=23546.8, global_step=67239, preemption_count=0, score=23546.8, test/accuracy=0.662669, test/bleu=27.0062, test/loss=1.6144, test/num_examples=3003, total_duration=40193.7, train/accuracy=0.630795, train/bleu=30.53, train/loss=1.84712, validation/accuracy=0.649643, validation/bleu=27.1934, validation/loss=1.69224, validation/num_examples=3000
I0306 06:24:16.668747 140104668485376 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.28298044204711914, loss=1.9069329500198364
I0306 06:24:51.621582 140104660092672 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.31953221559524536, loss=1.867163896560669
I0306 06:25:26.616937 140104668485376 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.2753107249736786, loss=1.8945260047912598
I0306 06:26:01.610376 140104660092672 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.37854835391044617, loss=1.9050514698028564
I0306 06:26:36.582897 140104668485376 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.2877632677555084, loss=1.8365862369537354
I0306 06:27:11.553211 140104660092672 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3132423162460327, loss=1.7726995944976807
I0306 06:27:46.538476 140104668485376 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.30787917971611023, loss=1.8543232679367065
I0306 06:28:21.488682 140104660092672 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.28611618280410767, loss=1.841294288635254
I0306 06:28:56.486047 140104668485376 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3749951124191284, loss=1.8654811382293701
I0306 06:29:31.468084 140104660092672 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.32799527049064636, loss=1.7850313186645508
I0306 06:30:06.459412 140104668485376 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.2855731248855591, loss=1.8829389810562134
I0306 06:30:41.427367 140104660092672 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3011032044887543, loss=1.8672937154769897
I0306 06:31:16.404869 140104668485376 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2813805639743805, loss=1.8146328926086426
I0306 06:31:51.424595 140104660092672 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.3352200388908386, loss=1.8832241296768188
I0306 06:32:26.420616 140104668485376 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.26697853207588196, loss=1.8492029905319214
I0306 06:33:01.400701 140104660092672 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2707582712173462, loss=1.8172663450241089
I0306 06:33:36.427198 140104668485376 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.298435777425766, loss=1.827848196029663
I0306 06:34:11.413475 140104660092672 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.35098370909690857, loss=1.908126950263977
I0306 06:34:46.354826 140104668485376 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2722233235836029, loss=1.7377346754074097
I0306 06:35:21.302581 140104660092672 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.34133949875831604, loss=1.8033804893493652
I0306 06:35:56.218574 140104668485376 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.33293527364730835, loss=1.8532928228378296
I0306 06:36:31.175729 140104660092672 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.281225323677063, loss=1.801439642906189
I0306 06:37:06.094016 140104668485376 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.35911625623703003, loss=1.775159478187561
I0306 06:37:41.034591 140104660092672 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.31892070174217224, loss=1.8109068870544434
I0306 06:37:54.990021 140248407626944 spec.py:321] Evaluating on the training split.
I0306 06:37:57.596487 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 06:41:33.670901 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 06:41:36.277380 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 06:44:26.617515 140248407626944 spec.py:349] Evaluating on the test split.
I0306 06:44:29.213442 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 06:47:13.237822 140248407626944 submission_runner.py:469] Time since start: 41591.97s, 	Step: 69641, 	{'train/accuracy': 0.6417231559753418, 'train/loss': 1.7461527585983276, 'train/bleu': 31.380935208801496, 'validation/accuracy': 0.6530171036720276, 'validation/loss': 1.6630244255065918, 'validation/bleu': 27.659766721102173, 'validation/num_examples': 3000, 'test/accuracy': 0.6612791419029236, 'test/loss': 1.6080572605133057, 'test/bleu': 26.98846945057267, 'test/num_examples': 3003, 'score': 24386.636229276657, 'total_duration': 41591.9735121727, 'accumulated_submission_time': 24386.636229276657, 'accumulated_eval_time': 17200.6773416996, 'accumulated_logging_time': 0.6243457794189453}
I0306 06:47:13.255235 140104668485376 logging_writer.py:48] [69641] accumulated_eval_time=17200.7, accumulated_logging_time=0.624346, accumulated_submission_time=24386.6, global_step=69641, preemption_count=0, score=24386.6, test/accuracy=0.661279, test/bleu=26.9885, test/loss=1.60806, test/num_examples=3003, total_duration=41592, train/accuracy=0.641723, train/bleu=31.3809, train/loss=1.74615, validation/accuracy=0.653017, validation/bleu=27.6598, validation/loss=1.66302, validation/num_examples=3000
I0306 06:47:34.121700 140104660092672 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3099822402000427, loss=1.8782734870910645
I0306 06:48:08.964591 140104668485376 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2566912770271301, loss=1.8244177103042603
I0306 06:48:43.837110 140104660092672 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.3012552559375763, loss=1.7731077671051025
I0306 06:49:18.747435 140104668485376 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3038565516471863, loss=1.7674164772033691
I0306 06:49:53.650766 140104660092672 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2811528742313385, loss=1.7636008262634277
I0306 06:50:28.583459 140104668485376 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.3042728304862976, loss=1.7412753105163574
I0306 06:51:03.519878 140104660092672 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2953416109085083, loss=1.8669573068618774
I0306 06:51:38.453438 140104668485376 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.30661261081695557, loss=1.8178730010986328
I0306 06:52:13.379429 140104660092672 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.3077395558357239, loss=1.862108826637268
I0306 06:52:48.280872 140104668485376 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.3087061643600464, loss=1.8872021436691284
I0306 06:53:23.142174 140104660092672 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3616616129875183, loss=1.8991276025772095
I0306 06:53:58.053552 140104668485376 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.3441283702850342, loss=1.8406418561935425
I0306 06:54:32.929961 140104660092672 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.34356385469436646, loss=1.852957010269165
I0306 06:55:07.847082 140104668485376 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3480648696422577, loss=1.8037654161453247
I0306 06:55:42.753296 140104660092672 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.29694706201553345, loss=1.829548954963684
I0306 06:56:17.688295 140104668485376 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.35174116492271423, loss=1.8128561973571777
I0306 06:56:52.608056 140104660092672 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.3199644684791565, loss=1.8450407981872559
I0306 06:57:27.544841 140104668485376 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.27501264214515686, loss=1.7953157424926758
I0306 06:58:02.405539 140104660092672 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2918422818183899, loss=1.740688681602478
I0306 06:58:37.297198 140104668485376 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.28259754180908203, loss=1.7657309770584106
I0306 06:59:12.161815 140104660092672 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.30774950981140137, loss=1.8680988550186157
I0306 06:59:47.073794 140104668485376 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.27902722358703613, loss=1.780907154083252
I0306 07:00:21.969725 140104660092672 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3693086504936218, loss=1.8631516695022583
I0306 07:00:56.868990 140104668485376 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.30299654603004456, loss=1.7932697534561157
I0306 07:01:13.274949 140248407626944 spec.py:321] Evaluating on the training split.
I0306 07:01:15.887468 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:04:45.387569 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 07:04:47.989326 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:08:04.629563 140248407626944 spec.py:349] Evaluating on the test split.
I0306 07:08:07.233361 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:10:51.143970 140248407626944 submission_runner.py:469] Time since start: 43009.88s, 	Step: 72048, 	{'train/accuracy': 0.6350510716438293, 'train/loss': 1.8069862127304077, 'train/bleu': 30.674610005409807, 'validation/accuracy': 0.6550441384315491, 'validation/loss': 1.654374361038208, 'validation/bleu': 27.568141518094126, 'validation/num_examples': 3000, 'test/accuracy': 0.6680338382720947, 'test/loss': 1.5831362009048462, 'test/bleu': 27.269447461818483, 'test/num_examples': 3003, 'score': 25226.513605117798, 'total_duration': 43009.87966322899, 'accumulated_submission_time': 25226.513605117798, 'accumulated_eval_time': 17778.546284914017, 'accumulated_logging_time': 0.6499409675598145}
I0306 07:10:51.161312 140104660092672 logging_writer.py:48] [72048] accumulated_eval_time=17778.5, accumulated_logging_time=0.649941, accumulated_submission_time=25226.5, global_step=72048, preemption_count=0, score=25226.5, test/accuracy=0.668034, test/bleu=27.2694, test/loss=1.58314, test/num_examples=3003, total_duration=43009.9, train/accuracy=0.635051, train/bleu=30.6746, train/loss=1.80699, validation/accuracy=0.655044, validation/bleu=27.5681, validation/loss=1.65437, validation/num_examples=3000
I0306 07:11:09.788985 140104668485376 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3001803755760193, loss=1.7760882377624512
I0306 07:11:44.779011 140104660092672 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3147132694721222, loss=1.8789759874343872
I0306 07:12:19.796168 140104668485376 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3580244779586792, loss=1.751057744026184
I0306 07:12:54.820046 140104660092672 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.28929999470710754, loss=1.7810871601104736
I0306 07:13:29.856312 140104668485376 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3236841857433319, loss=1.8338268995285034
I0306 07:14:04.870450 140104660092672 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3277742266654968, loss=1.8135360479354858
I0306 07:14:39.910761 140104668485376 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.32385873794555664, loss=1.7971097230911255
I0306 07:15:14.905380 140104660092672 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2673052251338959, loss=1.7864731550216675
I0306 07:15:49.896958 140104668485376 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3031303882598877, loss=1.8114662170410156
I0306 07:16:24.903069 140104660092672 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2945423424243927, loss=1.7752711772918701
I0306 07:16:59.899398 140104668485376 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.30852922797203064, loss=1.7082253694534302
I0306 07:17:34.899560 140104660092672 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.29039162397384644, loss=1.7403990030288696
I0306 07:18:09.936261 140104668485376 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.361374169588089, loss=1.839887261390686
I0306 07:18:44.967708 140104660092672 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3269292414188385, loss=1.8192493915557861
I0306 07:19:19.955701 140104668485376 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.31092503666877747, loss=1.808739185333252
I0306 07:19:54.986242 140104660092672 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.31476232409477234, loss=1.8515361547470093
I0306 07:20:30.034013 140104668485376 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.31756916642189026, loss=1.8238118886947632
I0306 07:21:05.086482 140104660092672 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.27285540103912354, loss=1.7231602668762207
I0306 07:21:40.093120 140104668485376 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.322374552488327, loss=1.7854888439178467
I0306 07:22:15.118295 140104660092672 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.2946469187736511, loss=1.8082894086837769
I0306 07:22:50.182082 140104668485376 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.33402761816978455, loss=1.8954225778579712
I0306 07:23:25.184849 140104660092672 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.31293076276779175, loss=1.7360081672668457
I0306 07:24:00.238916 140104668485376 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3102568984031677, loss=1.7481553554534912
I0306 07:24:35.314326 140104660092672 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.39083224534988403, loss=1.8482717275619507
I0306 07:24:51.426812 140248407626944 spec.py:321] Evaluating on the training split.
I0306 07:24:54.032382 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:28:19.827829 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 07:28:22.435862 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:31:13.020065 140248407626944 spec.py:349] Evaluating on the test split.
I0306 07:31:15.631381 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:34:13.115823 140248407626944 submission_runner.py:469] Time since start: 44411.85s, 	Step: 74447, 	{'train/accuracy': 0.6347497701644897, 'train/loss': 1.8110401630401611, 'train/bleu': 30.62346749624611, 'validation/accuracy': 0.6573060154914856, 'validation/loss': 1.6354496479034424, 'validation/bleu': 27.670441213807376, 'validation/num_examples': 3000, 'test/accuracy': 0.6668752431869507, 'test/loss': 1.570426344871521, 'test/bleu': 27.148600734014202, 'test/num_examples': 3003, 'score': 26066.559239387512, 'total_duration': 44411.85153913498, 'accumulated_submission_time': 26066.559239387512, 'accumulated_eval_time': 18340.23524069786, 'accumulated_logging_time': 0.7510364055633545}
I0306 07:34:13.133736 140104668485376 logging_writer.py:48] [74447] accumulated_eval_time=18340.2, accumulated_logging_time=0.751036, accumulated_submission_time=26066.6, global_step=74447, preemption_count=0, score=26066.6, test/accuracy=0.666875, test/bleu=27.1486, test/loss=1.57043, test/num_examples=3003, total_duration=44411.9, train/accuracy=0.63475, train/bleu=30.6235, train/loss=1.81104, validation/accuracy=0.657306, validation/bleu=27.6704, validation/loss=1.63545, validation/num_examples=3000
I0306 07:34:32.018894 140104660092672 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.27637970447540283, loss=1.7043704986572266
I0306 07:35:07.015584 140104668485376 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2696000635623932, loss=1.8333977460861206
I0306 07:35:42.050057 140104660092672 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3615671396255493, loss=1.7165405750274658
I0306 07:36:17.071060 140104668485376 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2916272282600403, loss=1.8293853998184204
I0306 07:36:52.094381 140104660092672 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3024168610572815, loss=1.787601351737976
I0306 07:37:27.158579 140104668485376 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.28740057349205017, loss=1.8008297681808472
I0306 07:38:02.159535 140104660092672 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.32862550020217896, loss=1.848827838897705
I0306 07:38:37.205730 140104668485376 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.30131372809410095, loss=1.7581946849822998
I0306 07:39:12.297248 140104660092672 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.30878686904907227, loss=1.8700131177902222
I0306 07:39:47.419687 140104668485376 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.3595849573612213, loss=1.8122122287750244
I0306 07:40:22.512202 140104660092672 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.32937943935394287, loss=1.8092265129089355
I0306 07:40:57.593927 140104668485376 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.3216873109340668, loss=1.7892112731933594
I0306 07:41:32.745504 140104660092672 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.32478198409080505, loss=1.798928141593933
I0306 07:42:07.862920 140104668485376 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3264267146587372, loss=1.7581876516342163
I0306 07:42:42.972347 140104660092672 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2799434959888458, loss=1.7396160364151
I0306 07:43:18.111572 140104668485376 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.30435657501220703, loss=1.7220052480697632
I0306 07:43:53.209030 140104660092672 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.32727745175361633, loss=1.7534279823303223
I0306 07:44:28.306534 140104668485376 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.3484876751899719, loss=1.8662025928497314
I0306 07:45:03.474711 140104660092672 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.2987487018108368, loss=1.7207703590393066
I0306 07:45:38.545302 140104668485376 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3432980179786682, loss=1.7885063886642456
I0306 07:46:13.667401 140104660092672 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.3352453410625458, loss=1.8100615739822388
I0306 07:46:48.744185 140104668485376 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3360282778739929, loss=1.776541829109192
I0306 07:47:23.843422 140104660092672 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.29463139176368713, loss=1.8292394876480103
I0306 07:47:58.962694 140104668485376 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3123689591884613, loss=1.7935901880264282
I0306 07:48:13.345260 140248407626944 spec.py:321] Evaluating on the training split.
I0306 07:48:15.955560 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:52:39.306531 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 07:52:41.924594 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:56:01.657805 140248407626944 spec.py:349] Evaluating on the test split.
I0306 07:56:04.268017 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 07:58:51.442008 140248407626944 submission_runner.py:469] Time since start: 45890.18s, 	Step: 76842, 	{'train/accuracy': 0.643849790096283, 'train/loss': 1.7367849349975586, 'train/bleu': 31.4240551171181, 'validation/accuracy': 0.6587027311325073, 'validation/loss': 1.6264591217041016, 'validation/bleu': 27.717167307785182, 'validation/num_examples': 3000, 'test/accuracy': 0.6707450151443481, 'test/loss': 1.5569945573806763, 'test/bleu': 27.190997661693533, 'test/num_examples': 3003, 'score': 26906.62972521782, 'total_duration': 45890.177701711655, 'accumulated_submission_time': 26906.62972521782, 'accumulated_eval_time': 18978.33191728592, 'accumulated_logging_time': 0.7785978317260742}
I0306 07:58:51.462495 140104660092672 logging_writer.py:48] [76842] accumulated_eval_time=18978.3, accumulated_logging_time=0.778598, accumulated_submission_time=26906.6, global_step=76842, preemption_count=0, score=26906.6, test/accuracy=0.670745, test/bleu=27.191, test/loss=1.55699, test/num_examples=3003, total_duration=45890.2, train/accuracy=0.64385, train/bleu=31.4241, train/loss=1.73678, validation/accuracy=0.658703, validation/bleu=27.7172, validation/loss=1.62646, validation/num_examples=3000
I0306 07:59:12.131444 140104668485376 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.298060804605484, loss=1.7854512929916382
I0306 07:59:47.175584 140104660092672 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.30641934275627136, loss=1.778240442276001
I0306 08:00:22.263091 140104668485376 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.3120059370994568, loss=1.763396143913269
I0306 08:00:57.391184 140104660092672 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.3166530430316925, loss=1.746930718421936
I0306 08:01:32.531807 140104668485376 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.30247700214385986, loss=1.714836835861206
I0306 08:02:07.641449 140104660092672 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.31056538224220276, loss=1.6507536172866821
I0306 08:02:42.779329 140104668485376 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.30994734168052673, loss=1.690592646598816
I0306 08:03:17.906961 140104660092672 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.29842957854270935, loss=1.8252315521240234
I0306 08:03:53.013464 140104668485376 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.32406020164489746, loss=1.747987151145935
I0306 08:04:28.132508 140104660092672 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.30225440859794617, loss=1.7982360124588013
I0306 08:05:03.245092 140104668485376 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.3020307421684265, loss=1.801134705543518
I0306 08:05:38.361811 140104660092672 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.27338549494743347, loss=1.7528676986694336
I0306 08:06:13.496861 140104668485376 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3032256066799164, loss=1.662764310836792
I0306 08:06:48.641119 140104660092672 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.31554993987083435, loss=1.8251879215240479
I0306 08:07:23.748208 140104668485376 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3010452687740326, loss=1.7008459568023682
I0306 08:07:58.846934 140104660092672 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.3123115599155426, loss=1.7613184452056885
I0306 08:08:33.976731 140104668485376 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2891980707645416, loss=1.7901750802993774
I0306 08:09:09.069128 140104660092672 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.31507039070129395, loss=1.7218949794769287
I0306 08:09:44.214401 140104668485376 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.32941532135009766, loss=1.7584724426269531
I0306 08:10:19.306189 140104660092672 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.29243046045303345, loss=1.7777174711227417
I0306 08:10:54.389752 140104668485376 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.34546005725860596, loss=1.7971937656402588
I0306 08:11:29.509099 140104660092672 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3231319785118103, loss=1.6889716386795044
I0306 08:12:04.599111 140104668485376 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.34125444293022156, loss=1.7883182764053345
I0306 08:12:39.735456 140104660092672 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.3439146876335144, loss=1.8437094688415527
I0306 08:12:51.683340 140248407626944 spec.py:321] Evaluating on the training split.
I0306 08:12:54.297233 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 08:16:19.427398 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 08:16:22.026831 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 08:19:18.650082 140248407626944 spec.py:349] Evaluating on the test split.
I0306 08:19:21.243845 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 08:22:26.892416 140248407626944 submission_runner.py:469] Time since start: 47305.63s, 	Step: 79235, 	{'train/accuracy': 0.6377763152122498, 'train/loss': 1.7833001613616943, 'train/bleu': 31.18229808110785, 'validation/accuracy': 0.6613106727600098, 'validation/loss': 1.6094623804092407, 'validation/bleu': 28.19021724030307, 'validation/num_examples': 3000, 'test/accuracy': 0.6720542311668396, 'test/loss': 1.5351009368896484, 'test/bleu': 27.277182572121234, 'test/num_examples': 3003, 'score': 27746.70812368393, 'total_duration': 47305.628126859665, 'accumulated_submission_time': 27746.70812368393, 'accumulated_eval_time': 19553.54093313217, 'accumulated_logging_time': 0.8071153163909912}
I0306 08:22:26.910621 140104668485376 logging_writer.py:48] [79235] accumulated_eval_time=19553.5, accumulated_logging_time=0.807115, accumulated_submission_time=27746.7, global_step=79235, preemption_count=0, score=27746.7, test/accuracy=0.672054, test/bleu=27.2772, test/loss=1.5351, test/num_examples=3003, total_duration=47305.6, train/accuracy=0.637776, train/bleu=31.1823, train/loss=1.7833, validation/accuracy=0.661311, validation/bleu=28.1902, validation/loss=1.60946, validation/num_examples=3000
I0306 08:22:50.010311 140104660092672 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2914624512195587, loss=1.7719172239303589
I0306 08:23:25.025707 140104668485376 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.31563156843185425, loss=1.754408597946167
I0306 08:24:00.087990 140104660092672 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3137255311012268, loss=1.7742457389831543
I0306 08:24:35.162761 140104668485376 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.30761563777923584, loss=1.8169461488723755
I0306 08:25:10.254391 140104660092672 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2812098562717438, loss=1.7763019800186157
I0306 08:25:45.361607 140104668485376 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.28584495186805725, loss=1.7312283515930176
I0306 08:26:20.450912 140104660092672 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.3002467751502991, loss=1.8410332202911377
I0306 08:26:55.583081 140104668485376 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2991993725299835, loss=1.7568988800048828
I0306 08:27:30.700055 140104660092672 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.37224364280700684, loss=1.7866812944412231
I0306 08:28:05.842160 140104668485376 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3173321485519409, loss=1.7291511297225952
I0306 08:28:40.974226 140104660092672 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.3288244605064392, loss=1.8147746324539185
I0306 08:29:16.101613 140104668485376 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.30151358246803284, loss=1.738215684890747
I0306 08:29:51.228138 140104660092672 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.27221959829330444, loss=1.7894536256790161
I0306 08:30:26.352379 140104668485376 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3054947555065155, loss=1.697628378868103
I0306 08:31:01.480284 140104660092672 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.2960328161716461, loss=1.7664278745651245
I0306 08:31:36.574869 140104668485376 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.3169836699962616, loss=1.7642182111740112
I0306 08:32:11.701252 140104660092672 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3153837323188782, loss=1.752488136291504
I0306 08:32:46.802871 140104668485376 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3031924366950989, loss=1.7435683012008667
I0306 08:33:21.927083 140104660092672 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.30848240852355957, loss=1.6915837526321411
I0306 08:33:57.023460 140104668485376 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.31521910429000854, loss=1.7499141693115234
I0306 08:34:32.136987 140104660092672 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3121231198310852, loss=1.751129150390625
I0306 08:35:07.212649 140104668485376 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.299198716878891, loss=1.7146283388137817
I0306 08:35:42.329203 140104660092672 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.3125903308391571, loss=1.7318142652511597
I0306 08:36:17.446333 140104668485376 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3185507357120514, loss=1.7426581382751465
I0306 08:36:26.920866 140248407626944 spec.py:321] Evaluating on the training split.
I0306 08:36:29.537312 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 08:40:53.562165 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 08:40:56.169423 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 08:44:07.901467 140248407626944 spec.py:349] Evaluating on the test split.
I0306 08:44:10.508760 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 08:47:17.087578 140248407626944 submission_runner.py:469] Time since start: 48795.82s, 	Step: 81628, 	{'train/accuracy': 0.6835156083106995, 'train/loss': 1.4777052402496338, 'train/bleu': 34.6312223314649, 'validation/accuracy': 0.6645613312721252, 'validation/loss': 1.5900806188583374, 'validation/bleu': 28.092138402436095, 'validation/num_examples': 3000, 'test/accuracy': 0.6757618188858032, 'test/loss': 1.5181536674499512, 'test/bleu': 27.73714469340843, 'test/num_examples': 3003, 'score': 28586.574621915817, 'total_duration': 48795.823291778564, 'accumulated_submission_time': 28586.574621915817, 'accumulated_eval_time': 20203.707585334778, 'accumulated_logging_time': 0.833693265914917}
I0306 08:47:17.106746 140104660092672 logging_writer.py:48] [81628] accumulated_eval_time=20203.7, accumulated_logging_time=0.833693, accumulated_submission_time=28586.6, global_step=81628, preemption_count=0, score=28586.6, test/accuracy=0.675762, test/bleu=27.7371, test/loss=1.51815, test/num_examples=3003, total_duration=48795.8, train/accuracy=0.683516, train/bleu=34.6312, train/loss=1.47771, validation/accuracy=0.664561, validation/bleu=28.0921, validation/loss=1.59008, validation/num_examples=3000
I0306 08:47:42.637318 140104668485376 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2885175347328186, loss=1.7664450407028198
I0306 08:48:17.668348 140104660092672 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.32553935050964355, loss=1.6847138404846191
I0306 08:48:52.720080 140104668485376 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.3276582360267639, loss=1.6587249040603638
I0306 08:49:27.798840 140104660092672 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3190712332725525, loss=1.707679271697998
I0306 08:50:02.872101 140104668485376 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3064289093017578, loss=1.7203662395477295
I0306 08:50:37.919039 140104660092672 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3279604911804199, loss=1.7475348711013794
I0306 08:51:12.962096 140104668485376 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.33377572894096375, loss=1.80710768699646
I0306 08:51:48.038054 140104660092672 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.3341232240200043, loss=1.8421885967254639
I0306 08:52:23.082504 140104668485376 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3233341574668884, loss=1.794093132019043
I0306 08:52:58.172932 140104660092672 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3442245125770569, loss=1.7420825958251953
I0306 08:53:33.229928 140104668485376 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.30745017528533936, loss=1.662579894065857
I0306 08:54:08.327557 140104660092672 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.31477370858192444, loss=1.777886152267456
I0306 08:54:43.396725 140104668485376 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.28728049993515015, loss=1.7227810621261597
I0306 08:55:18.514144 140104660092672 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.33519259095191956, loss=1.7815831899642944
I0306 08:55:53.555324 140104668485376 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3148391544818878, loss=1.717370867729187
I0306 08:56:28.638280 140104660092672 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.28548750281333923, loss=1.7621859312057495
I0306 08:57:03.686423 140104668485376 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.3233318328857422, loss=1.7821037769317627
I0306 08:57:38.761175 140104660092672 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3307524621486664, loss=1.7247710227966309
I0306 08:58:13.779846 140104668485376 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3123661279678345, loss=1.7801356315612793
I0306 08:58:48.852975 140104660092672 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.32181674242019653, loss=1.7544046640396118
I0306 08:59:23.881887 140104668485376 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.2979136109352112, loss=1.738114356994629
I0306 08:59:58.939941 140104660092672 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2979564666748047, loss=1.6987069845199585
I0306 09:00:33.968491 140104668485376 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.3219242990016937, loss=1.7363810539245605
I0306 09:01:08.987155 140104660092672 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.29722297191619873, loss=1.7099779844284058
I0306 09:01:17.415320 140248407626944 spec.py:321] Evaluating on the training split.
I0306 09:01:20.015374 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:04:59.133829 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 09:05:01.745299 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:07:42.841210 140248407626944 spec.py:349] Evaluating on the test split.
I0306 09:07:45.433993 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:10:42.859280 140248407626944 submission_runner.py:469] Time since start: 50201.59s, 	Step: 84025, 	{'train/accuracy': 0.6493034362792969, 'train/loss': 1.7085705995559692, 'train/bleu': 31.45303599077604, 'validation/accuracy': 0.668615460395813, 'validation/loss': 1.5697382688522339, 'validation/bleu': 28.70318347292197, 'validation/num_examples': 3000, 'test/accuracy': 0.6783918738365173, 'test/loss': 1.4986896514892578, 'test/bleu': 28.279428678066182, 'test/num_examples': 3003, 'score': 29426.739711523056, 'total_duration': 50201.59497499466, 'accumulated_submission_time': 29426.739711523056, 'accumulated_eval_time': 20769.151468753815, 'accumulated_logging_time': 0.8610711097717285}
I0306 09:10:42.876843 140104668485376 logging_writer.py:48] [84025] accumulated_eval_time=20769.2, accumulated_logging_time=0.861071, accumulated_submission_time=29426.7, global_step=84025, preemption_count=0, score=29426.7, test/accuracy=0.678392, test/bleu=28.2794, test/loss=1.49869, test/num_examples=3003, total_duration=50201.6, train/accuracy=0.649303, train/bleu=31.453, train/loss=1.70857, validation/accuracy=0.668615, validation/bleu=28.7032, validation/loss=1.56974, validation/num_examples=3000
I0306 09:11:09.398821 140104660092672 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.2993028163909912, loss=1.677567481994629
I0306 09:11:44.302679 140104668485376 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3191412091255188, loss=1.6810026168823242
I0306 09:12:19.265574 140104660092672 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.30497482419013977, loss=1.6834397315979004
I0306 09:12:54.223375 140104668485376 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.320276141166687, loss=1.7818608283996582
I0306 09:13:29.194530 140104660092672 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3014355003833771, loss=1.7157697677612305
I0306 09:14:04.179395 140104668485376 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.28963232040405273, loss=1.6642316579818726
I0306 09:14:39.166528 140104660092672 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.3054252862930298, loss=1.695630669593811
I0306 09:15:14.144470 140104668485376 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.31626376509666443, loss=1.6789189577102661
I0306 09:15:49.130949 140104660092672 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.3218042850494385, loss=1.6654589176177979
I0306 09:16:24.095647 140104668485376 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.29585227370262146, loss=1.6854430437088013
I0306 09:16:59.070239 140104660092672 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.3177512586116791, loss=1.7380459308624268
I0306 09:17:34.032508 140104668485376 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.32078811526298523, loss=1.6567661762237549
I0306 09:18:09.030470 140104660092672 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.30562281608581543, loss=1.6927294731140137
I0306 09:18:44.009252 140104668485376 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.30145537853240967, loss=1.6405236721038818
I0306 09:19:18.971383 140104660092672 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.31273335218429565, loss=1.745337963104248
I0306 09:19:53.945408 140104668485376 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.3348369300365448, loss=1.7748925685882568
I0306 09:20:28.896994 140104660092672 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.32934466004371643, loss=1.6916056871414185
I0306 09:21:03.879384 140104668485376 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.3437095284461975, loss=1.7605745792388916
I0306 09:21:38.847805 140104660092672 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.35095730423927307, loss=1.7743855714797974
I0306 09:22:13.826535 140104668485376 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.31112411618232727, loss=1.807813048362732
I0306 09:22:48.769068 140104660092672 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3432603180408478, loss=1.7821862697601318
I0306 09:23:23.779631 140104668485376 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.3031224310398102, loss=1.6885943412780762
I0306 09:23:58.685024 140104660092672 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.33895567059516907, loss=1.8633613586425781
I0306 09:24:33.613523 140104668485376 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3403378129005432, loss=1.689435601234436
I0306 09:24:43.048915 140248407626944 spec.py:321] Evaluating on the training split.
I0306 09:24:45.650315 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:29:10.626096 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 09:29:13.223916 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:32:45.958317 140248407626944 spec.py:349] Evaluating on the test split.
I0306 09:32:48.560430 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:36:01.375133 140248407626944 submission_runner.py:469] Time since start: 51720.11s, 	Step: 86428, 	{'train/accuracy': 0.6513230800628662, 'train/loss': 1.6993632316589355, 'train/bleu': 31.57597122100193, 'validation/accuracy': 0.6699997782707214, 'validation/loss': 1.5602314472198486, 'validation/bleu': 29.0313639459778, 'validation/num_examples': 3000, 'test/accuracy': 0.6820183396339417, 'test/loss': 1.4871288537979126, 'test/bleu': 28.512797079562908, 'test/num_examples': 3003, 'score': 30266.774632692337, 'total_duration': 51720.11083507538, 'accumulated_submission_time': 30266.774632692337, 'accumulated_eval_time': 21447.477617263794, 'accumulated_logging_time': 0.886648416519165}
I0306 09:36:01.393378 140104660092672 logging_writer.py:48] [86428] accumulated_eval_time=21447.5, accumulated_logging_time=0.886648, accumulated_submission_time=30266.8, global_step=86428, preemption_count=0, score=30266.8, test/accuracy=0.682018, test/bleu=28.5128, test/loss=1.48713, test/num_examples=3003, total_duration=51720.1, train/accuracy=0.651323, train/bleu=31.576, train/loss=1.69936, validation/accuracy=0.67, validation/bleu=29.0314, validation/loss=1.56023, validation/num_examples=3000
I0306 09:36:26.914929 140104668485376 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.32582882046699524, loss=1.6358466148376465
I0306 09:37:01.878916 140104660092672 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.3121819794178009, loss=1.6920232772827148
I0306 09:37:36.883741 140104668485376 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3207053542137146, loss=1.703437089920044
I0306 09:38:11.830672 140104660092672 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.32796746492385864, loss=1.7407035827636719
I0306 09:38:46.822814 140104668485376 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2986713647842407, loss=1.7003706693649292
I0306 09:39:21.784621 140104660092672 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.32361751794815063, loss=1.6767772436141968
I0306 09:39:56.734148 140104668485376 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.328559011220932, loss=1.694525957107544
I0306 09:40:31.699709 140104660092672 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3179508149623871, loss=1.7173757553100586
I0306 09:41:06.686829 140104668485376 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.34192970395088196, loss=1.6516685485839844
I0306 09:41:41.679968 140104660092672 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3464691936969757, loss=1.687535285949707
I0306 09:42:16.650235 140104668485376 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3200305998325348, loss=1.7027233839035034
I0306 09:42:51.608746 140104660092672 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.2927510142326355, loss=1.6732945442199707
I0306 09:43:26.594417 140104668485376 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.2973736822605133, loss=1.7322840690612793
I0306 09:44:01.550585 140104660092672 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3128635287284851, loss=1.6356366872787476
I0306 09:44:36.439050 140104668485376 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3405608534812927, loss=1.6964055299758911
I0306 09:45:11.339962 140104660092672 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.30218884348869324, loss=1.6089352369308472
I0306 09:45:46.204327 140104668485376 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3137068748474121, loss=1.634531021118164
I0306 09:46:21.097841 140104660092672 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.34814363718032837, loss=1.6102674007415771
I0306 09:46:55.986598 140104668485376 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.310666561126709, loss=1.6655977964401245
I0306 09:47:30.857769 140104660092672 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.3113609552383423, loss=1.732762336730957
I0306 09:48:05.783170 140104668485376 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3238009810447693, loss=1.7119635343551636
I0306 09:48:40.688034 140104660092672 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3025209307670593, loss=1.6945545673370361
I0306 09:49:15.585206 140104668485376 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.31120380759239197, loss=1.72493577003479
I0306 09:49:50.491475 140104660092672 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3410990834236145, loss=1.6940475702285767
I0306 09:50:01.669312 140248407626944 spec.py:321] Evaluating on the training split.
I0306 09:50:04.274020 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:54:00.250654 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 09:54:02.857960 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 09:57:10.906290 140248407626944 spec.py:349] Evaluating on the test split.
I0306 09:57:13.509188 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 10:00:12.132535 140248407626944 submission_runner.py:469] Time since start: 53170.87s, 	Step: 88833, 	{'train/accuracy': 0.6636746525764465, 'train/loss': 1.6096980571746826, 'train/bleu': 32.65140834115123, 'validation/accuracy': 0.6717919707298279, 'validation/loss': 1.5396769046783447, 'validation/bleu': 29.00127703822721, 'validation/num_examples': 3000, 'test/accuracy': 0.6835013628005981, 'test/loss': 1.4725807905197144, 'test/bleu': 28.460627210430083, 'test/num_examples': 3003, 'score': 31106.910621643066, 'total_duration': 53170.86822557449, 'accumulated_submission_time': 31106.910621643066, 'accumulated_eval_time': 22057.94075703621, 'accumulated_logging_time': 0.9131319522857666}
I0306 10:00:12.153166 140104668485376 logging_writer.py:48] [88833] accumulated_eval_time=22057.9, accumulated_logging_time=0.913132, accumulated_submission_time=31106.9, global_step=88833, preemption_count=0, score=31106.9, test/accuracy=0.683501, test/bleu=28.4606, test/loss=1.47258, test/num_examples=3003, total_duration=53170.9, train/accuracy=0.663675, train/bleu=32.6514, train/loss=1.6097, validation/accuracy=0.671792, validation/bleu=29.0013, validation/loss=1.53968, validation/num_examples=3000
I0306 10:00:35.802601 140104660092672 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3006209433078766, loss=1.6637345552444458
I0306 10:01:10.610505 140104668485376 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.29235437512397766, loss=1.7102186679840088
I0306 10:01:45.469716 140104660092672 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.30406519770622253, loss=1.6894190311431885
I0306 10:02:20.349178 140104668485376 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3145318329334259, loss=1.678792953491211
I0306 10:02:55.249896 140104660092672 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.3201703429222107, loss=1.6623047590255737
I0306 10:03:30.144387 140104668485376 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.31700122356414795, loss=1.645066738128662
I0306 10:04:05.035956 140104660092672 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.3113373816013336, loss=1.684584140777588
I0306 10:04:39.942719 140104668485376 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.31333306431770325, loss=1.637988805770874
I0306 10:05:14.830828 140104660092672 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.348823606967926, loss=1.704375982284546
I0306 10:05:49.748244 140104668485376 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.30992409586906433, loss=1.6716150045394897
I0306 10:06:24.637575 140104660092672 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.32596808671951294, loss=1.6643999814987183
I0306 10:06:59.520871 140104668485376 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.29625678062438965, loss=1.643682599067688
I0306 10:07:34.403013 140104660092672 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.31380340456962585, loss=1.7229772806167603
I0306 10:08:09.246766 140104668485376 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.338644415140152, loss=1.6390661001205444
I0306 10:08:44.127638 140104660092672 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3305332064628601, loss=1.6857794523239136
I0306 10:09:18.993281 140104668485376 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3117312490940094, loss=1.6138173341751099
I0306 10:09:53.831387 140104660092672 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.29778510332107544, loss=1.582589030265808
I0306 10:10:28.684129 140104668485376 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.29957902431488037, loss=1.656835675239563
I0306 10:11:03.546927 140104660092672 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.3108810484409332, loss=1.6908107995986938
I0306 10:11:38.400953 140104668485376 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3155631422996521, loss=1.6343382596969604
I0306 10:12:13.277311 140104660092672 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.3371390700340271, loss=1.6990963220596313
I0306 10:12:48.156210 140104668485376 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3318691551685333, loss=1.6717056035995483
I0306 10:13:23.005286 140104660092672 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.33090612292289734, loss=1.64356529712677
I0306 10:13:57.906749 140104668485376 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3182240426540375, loss=1.6463806629180908
I0306 10:14:12.197689 140248407626944 spec.py:321] Evaluating on the training split.
I0306 10:14:14.808709 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 10:18:18.843935 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 10:18:21.440469 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 10:21:21.228910 140248407626944 spec.py:349] Evaluating on the test split.
I0306 10:21:23.834744 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 10:24:31.234112 140248407626944 submission_runner.py:469] Time since start: 54629.97s, 	Step: 91242, 	{'train/accuracy': 0.6555089950561523, 'train/loss': 1.6580567359924316, 'train/bleu': 32.23229413014045, 'validation/accuracy': 0.6740291118621826, 'validation/loss': 1.5280110836029053, 'validation/bleu': 28.773842061603613, 'validation/num_examples': 3000, 'test/accuracy': 0.6861545443534851, 'test/loss': 1.4497209787368774, 'test/bleu': 28.794300777932847, 'test/num_examples': 3003, 'score': 31946.811557531357, 'total_duration': 54629.96982336044, 'accumulated_submission_time': 31946.811557531357, 'accumulated_eval_time': 22676.977138757706, 'accumulated_logging_time': 0.942638635635376}
I0306 10:24:31.252578 140104660092672 logging_writer.py:48] [91242] accumulated_eval_time=22677, accumulated_logging_time=0.942639, accumulated_submission_time=31946.8, global_step=91242, preemption_count=0, score=31946.8, test/accuracy=0.686155, test/bleu=28.7943, test/loss=1.44972, test/num_examples=3003, total_duration=54630, train/accuracy=0.655509, train/bleu=32.2323, train/loss=1.65806, validation/accuracy=0.674029, validation/bleu=28.7738, validation/loss=1.52801, validation/num_examples=3000
I0306 10:24:51.742440 140104668485376 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.33441951870918274, loss=1.655248999595642
I0306 10:25:26.524324 140104660092672 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.32156985998153687, loss=1.688436508178711
I0306 10:26:01.360166 140104668485376 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.33871129155158997, loss=1.6751593351364136
I0306 10:26:36.236934 140104660092672 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.32981938123703003, loss=1.6481868028640747
I0306 10:27:11.075296 140104668485376 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.32367879152297974, loss=1.6373565196990967
I0306 10:27:45.924831 140104660092672 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.30096352100372314, loss=1.6179063320159912
I0306 10:28:20.816926 140104668485376 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.31015896797180176, loss=1.567219614982605
I0306 10:28:55.697149 140104660092672 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.33110713958740234, loss=1.6475731134414673
I0306 10:29:30.588640 140104668485376 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.33840441703796387, loss=1.5989601612091064
I0306 10:30:05.445488 140104660092672 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3411610722541809, loss=1.6080200672149658
I0306 10:30:40.346858 140104668485376 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.33998170495033264, loss=1.6048451662063599
I0306 10:31:15.205061 140104660092672 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3335092067718506, loss=1.6694163084030151
I0306 10:31:50.070871 140104668485376 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.3264225721359253, loss=1.6200625896453857
I0306 10:32:24.946937 140104660092672 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.32975003123283386, loss=1.6501924991607666
I0306 10:32:59.845374 140104668485376 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.31242290139198303, loss=1.6809409856796265
I0306 10:33:34.751477 140104660092672 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.3267313838005066, loss=1.6322919130325317
I0306 10:34:09.648838 140104668485376 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.30834171175956726, loss=1.6082854270935059
I0306 10:34:44.543066 140104660092672 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.31451502442359924, loss=1.6671775579452515
I0306 10:35:19.458757 140104668485376 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.31424543261528015, loss=1.6097586154937744
I0306 10:35:54.371325 140104660092672 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.3403632640838623, loss=1.6533303260803223
I0306 10:36:29.257364 140104668485376 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3201310336589813, loss=1.6914137601852417
I0306 10:37:04.159204 140104660092672 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.34326618909835815, loss=1.6566526889801025
I0306 10:37:39.050277 140104668485376 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3377217948436737, loss=1.7320245504379272
I0306 10:38:13.917042 140104660092672 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.31777602434158325, loss=1.6504900455474854
I0306 10:38:31.354670 140248407626944 spec.py:321] Evaluating on the training split.
I0306 10:38:33.962412 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 10:42:33.390379 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 10:42:35.997569 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 10:45:52.152884 140248407626944 spec.py:349] Evaluating on the test split.
I0306 10:45:54.756671 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 10:48:54.873865 140248407626944 submission_runner.py:469] Time since start: 56093.61s, 	Step: 93651, 	{'train/accuracy': 0.6547471284866333, 'train/loss': 1.6660195589065552, 'train/bleu': 33.03640261403633, 'validation/accuracy': 0.6758584380149841, 'validation/loss': 1.5119266510009766, 'validation/bleu': 29.244105561495573, 'validation/num_examples': 3000, 'test/accuracy': 0.6886803507804871, 'test/loss': 1.4332627058029175, 'test/bleu': 29.1400584975005, 'test/num_examples': 3003, 'score': 32786.76872754097, 'total_duration': 56093.60956931114, 'accumulated_submission_time': 32786.76872754097, 'accumulated_eval_time': 23300.496270418167, 'accumulated_logging_time': 0.9697928428649902}
I0306 10:48:54.896106 140104668485376 logging_writer.py:48] [93651] accumulated_eval_time=23300.5, accumulated_logging_time=0.969793, accumulated_submission_time=32786.8, global_step=93651, preemption_count=0, score=32786.8, test/accuracy=0.68868, test/bleu=29.1401, test/loss=1.43326, test/num_examples=3003, total_duration=56093.6, train/accuracy=0.654747, train/bleu=33.0364, train/loss=1.66602, validation/accuracy=0.675858, validation/bleu=29.2441, validation/loss=1.51193, validation/num_examples=3000
I0306 10:49:12.335312 140104660092672 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.30467483401298523, loss=1.6276469230651855
I0306 10:49:47.164056 140104668485376 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.34100911021232605, loss=1.6585358381271362
I0306 10:50:22.035760 140104660092672 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.32501888275146484, loss=1.6090741157531738
I0306 10:50:56.915130 140104668485376 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.31612515449523926, loss=1.6259242296218872
I0306 10:51:31.827300 140104660092672 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2972617447376251, loss=1.5914076566696167
I0306 10:52:06.809213 140104668485376 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3076726794242859, loss=1.6455219984054565
I0306 10:52:41.794206 140104660092672 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.33773431181907654, loss=1.6182998418807983
I0306 10:53:16.752557 140104668485376 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3258012533187866, loss=1.6879874467849731
I0306 10:53:51.711724 140104660092672 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.32493022084236145, loss=1.6262465715408325
I0306 10:54:26.684734 140104668485376 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3164539933204651, loss=1.5827007293701172
I0306 10:55:01.675858 140104660092672 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.31713002920150757, loss=1.5919135808944702
I0306 10:55:36.654935 140104668485376 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3044677674770355, loss=1.563549518585205
I0306 10:56:11.654967 140104660092672 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.32154038548469543, loss=1.5505508184432983
I0306 10:56:46.604113 140104668485376 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3295387625694275, loss=1.5037684440612793
I0306 10:57:21.607661 140104660092672 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3055673837661743, loss=1.6414350271224976
I0306 10:57:56.568781 140104668485376 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.32906636595726013, loss=1.558259129524231
I0306 10:58:31.557425 140104660092672 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3242187798023224, loss=1.6232980489730835
I0306 10:59:06.571851 140104668485376 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3393307626247406, loss=1.5974117517471313
I0306 10:59:41.543317 140104660092672 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3219209313392639, loss=1.6040005683898926
I0306 11:00:16.516473 140104668485376 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.33697938919067383, loss=1.5619934797286987
I0306 11:00:51.488113 140104660092672 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3237163722515106, loss=1.5441044569015503
I0306 11:01:26.466167 140104668485376 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3232201337814331, loss=1.5655279159545898
I0306 11:02:01.458711 140104660092672 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.31592103838920593, loss=1.5952481031417847
I0306 11:02:36.444507 140104668485376 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3099517226219177, loss=1.6031877994537354
I0306 11:02:55.020632 140248407626944 spec.py:321] Evaluating on the training split.
I0306 11:02:57.624735 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 11:07:37.169909 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 11:07:39.773530 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 11:11:10.322706 140248407626944 spec.py:349] Evaluating on the test split.
I0306 11:11:12.931430 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 11:14:39.592827 140248407626944 submission_runner.py:469] Time since start: 57638.33s, 	Step: 96054, 	{'train/accuracy': 0.6671206951141357, 'train/loss': 1.5872409343719482, 'train/bleu': 33.25061949803364, 'validation/accuracy': 0.6793316006660461, 'validation/loss': 1.498511552810669, 'validation/bleu': 29.372826734289195, 'validation/num_examples': 3000, 'test/accuracy': 0.6936044692993164, 'test/loss': 1.4130539894104004, 'test/bleu': 29.45315474812473, 'test/num_examples': 3003, 'score': 33626.75097632408, 'total_duration': 57638.328526735306, 'accumulated_submission_time': 33626.75097632408, 'accumulated_eval_time': 24005.06839466095, 'accumulated_logging_time': 1.0006675720214844}
I0306 11:14:39.613222 140104660092672 logging_writer.py:48] [96054] accumulated_eval_time=24005.1, accumulated_logging_time=1.00067, accumulated_submission_time=33626.8, global_step=96054, preemption_count=0, score=33626.8, test/accuracy=0.693604, test/bleu=29.4532, test/loss=1.41305, test/num_examples=3003, total_duration=57638.3, train/accuracy=0.667121, train/bleu=33.2506, train/loss=1.58724, validation/accuracy=0.679332, validation/bleu=29.3728, validation/loss=1.49851, validation/num_examples=3000
I0306 11:14:56.047643 140104668485376 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.33045250177383423, loss=1.619215965270996
I0306 11:15:30.950848 140104660092672 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.33803656697273254, loss=1.5965121984481812
I0306 11:16:05.932356 140104668485376 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.32334402203559875, loss=1.618525505065918
I0306 11:16:40.903357 140104660092672 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.310433030128479, loss=1.5948238372802734
I0306 11:17:15.858965 140104668485376 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3294312059879303, loss=1.541857361793518
I0306 11:17:50.818460 140104660092672 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.3457493185997009, loss=1.611998438835144
I0306 11:18:25.794763 140104668485376 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.362430214881897, loss=1.6605713367462158
I0306 11:19:00.770477 140104660092672 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.33001166582107544, loss=1.5737251043319702
I0306 11:19:35.774521 140104668485376 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3290804326534271, loss=1.6123459339141846
I0306 11:20:10.778032 140104660092672 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.32265013456344604, loss=1.5547949075698853
I0306 11:20:45.731848 140104668485376 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3151227533817291, loss=1.5566059350967407
I0306 11:21:20.691432 140104660092672 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3005099296569824, loss=1.50799560546875
I0306 11:21:55.665332 140104668485376 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.32545679807662964, loss=1.590444803237915
I0306 11:22:30.657011 140104660092672 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.325515478849411, loss=1.5857558250427246
I0306 11:23:05.646823 140104668485376 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3491777181625366, loss=1.588112235069275
I0306 11:23:40.595429 140104660092672 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.35427457094192505, loss=1.6911524534225464
I0306 11:24:15.541681 140104668485376 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.33190786838531494, loss=1.6182719469070435
I0306 11:24:50.479095 140104660092672 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3734252452850342, loss=1.5576046705245972
I0306 11:25:25.442619 140104668485376 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3242848813533783, loss=1.5459518432617188
I0306 11:26:00.385324 140104660092672 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.32944828271865845, loss=1.6243118047714233
I0306 11:26:35.358654 140104668485376 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.34803879261016846, loss=1.6411813497543335
I0306 11:27:10.322190 140104660092672 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.3435019254684448, loss=1.5758413076400757
I0306 11:27:45.244818 140104668485376 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.3261135220527649, loss=1.58052659034729
I0306 11:28:20.218546 140104660092672 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.31626376509666443, loss=1.5438096523284912
I0306 11:28:39.805783 140248407626944 spec.py:321] Evaluating on the training split.
I0306 11:28:42.412028 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 11:32:50.870990 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 11:32:53.470817 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 11:36:30.067910 140248407626944 spec.py:349] Evaluating on the test split.
I0306 11:36:32.672745 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 11:40:10.187212 140248407626944 submission_runner.py:469] Time since start: 59168.92s, 	Step: 98457, 	{'train/accuracy': 0.6665906310081482, 'train/loss': 1.5995906591415405, 'train/bleu': 33.48670436823367, 'validation/accuracy': 0.6801473498344421, 'validation/loss': 1.4855279922485352, 'validation/bleu': 29.43370871277903, 'validation/num_examples': 3000, 'test/accuracy': 0.6942300796508789, 'test/loss': 1.4011098146438599, 'test/bleu': 29.50482910751763, 'test/num_examples': 3003, 'score': 34466.80168223381, 'total_duration': 59168.92292332649, 'accumulated_submission_time': 34466.80168223381, 'accumulated_eval_time': 24695.449775218964, 'accumulated_logging_time': 1.0296571254730225}
I0306 11:40:10.207242 140104668485376 logging_writer.py:48] [98457] accumulated_eval_time=24695.4, accumulated_logging_time=1.02966, accumulated_submission_time=34466.8, global_step=98457, preemption_count=0, score=34466.8, test/accuracy=0.69423, test/bleu=29.5048, test/loss=1.40111, test/num_examples=3003, total_duration=59168.9, train/accuracy=0.666591, train/bleu=33.4867, train/loss=1.59959, validation/accuracy=0.680147, validation/bleu=29.4337, validation/loss=1.48553, validation/num_examples=3000
I0306 11:40:25.575926 140104660092672 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.33429062366485596, loss=1.6156270503997803
I0306 11:41:00.453452 140104668485376 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3225962817668915, loss=1.4862958192825317
I0306 11:41:35.315873 140104660092672 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.3429604768753052, loss=1.5485271215438843
I0306 11:42:10.232075 140104668485376 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3126964271068573, loss=1.5180444717407227
I0306 11:42:45.176450 140104660092672 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.3586684465408325, loss=1.586078405380249
I0306 11:43:20.122855 140104668485376 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.31579262018203735, loss=1.6050214767456055
I0306 11:43:55.049181 140104660092672 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.32445329427719116, loss=1.5340993404388428
I0306 11:44:30.004323 140104668485376 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3626459836959839, loss=1.5824456214904785
I0306 11:45:04.975400 140104660092672 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.32341933250427246, loss=1.5935591459274292
I0306 11:45:39.915568 140104668485376 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3297414779663086, loss=1.525264024734497
I0306 11:46:14.898823 140104660092672 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.33013302087783813, loss=1.618851900100708
I0306 11:46:49.878938 140104668485376 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.36626890301704407, loss=1.6365243196487427
I0306 11:47:24.850809 140104660092672 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.3498602509498596, loss=1.6543009281158447
I0306 11:47:59.876142 140104668485376 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.31903335452079773, loss=1.5240380764007568
I0306 11:48:34.828537 140104660092672 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.322791188955307, loss=1.5586884021759033
I0306 11:49:09.836989 140104668485376 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.32721081376075745, loss=1.5450705289840698
I0306 11:49:44.833656 140104660092672 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.3560889959335327, loss=1.5597490072250366
I0306 11:50:19.811957 140104668485376 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3501245081424713, loss=1.6311001777648926
I0306 11:50:54.788389 140104660092672 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.3308427929878235, loss=1.5216500759124756
I0306 11:51:29.779153 140104668485376 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.3380300998687744, loss=1.4935041666030884
I0306 11:52:04.781567 140104660092672 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3290742337703705, loss=1.544175148010254
I0306 11:52:39.784258 140104668485376 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3463689088821411, loss=1.599043607711792
I0306 11:53:14.771991 140104660092672 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3250691592693329, loss=1.4771504402160645
I0306 11:53:49.796708 140104668485376 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.3684067726135254, loss=1.57463538646698
I0306 11:54:10.435606 140248407626944 spec.py:321] Evaluating on the training split.
I0306 11:54:13.046354 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 11:58:02.015913 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 11:58:04.614943 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:01:00.906658 140248407626944 spec.py:349] Evaluating on the test split.
I0306 12:01:03.512949 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:04:02.324304 140248407626944 submission_runner.py:469] Time since start: 60601.06s, 	Step: 100860, 	{'train/accuracy': 0.6865341663360596, 'train/loss': 1.4697033166885376, 'train/bleu': 34.37697609056703, 'validation/accuracy': 0.6838058829307556, 'validation/loss': 1.4683496952056885, 'validation/bleu': 29.81889250951529, 'validation/num_examples': 3000, 'test/accuracy': 0.698841392993927, 'test/loss': 1.3876018524169922, 'test/bleu': 29.560720436591545, 'test/num_examples': 3003, 'score': 35306.88824081421, 'total_duration': 60601.060000896454, 'accumulated_submission_time': 35306.88824081421, 'accumulated_eval_time': 25287.33839583397, 'accumulated_logging_time': 1.0578217506408691}
I0306 12:04:02.344411 140104660092672 logging_writer.py:48] [100860] accumulated_eval_time=25287.3, accumulated_logging_time=1.05782, accumulated_submission_time=35306.9, global_step=100860, preemption_count=0, score=35306.9, test/accuracy=0.698841, test/bleu=29.5607, test/loss=1.3876, test/num_examples=3003, total_duration=60601.1, train/accuracy=0.686534, train/bleu=34.377, train/loss=1.4697, validation/accuracy=0.683806, validation/bleu=29.8189, validation/loss=1.46835, validation/num_examples=3000
I0306 12:04:16.655197 140104668485376 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.33077874779701233, loss=1.5568307638168335
I0306 12:04:51.588994 140104660092672 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3342468738555908, loss=1.5688951015472412
I0306 12:05:26.538919 140104668485376 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.32690030336380005, loss=1.5254932641983032
I0306 12:06:01.522443 140104660092672 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.33303412795066833, loss=1.5280344486236572
I0306 12:06:36.533317 140104668485376 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.3323480784893036, loss=1.4433046579360962
I0306 12:07:11.526733 140104660092672 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.35038673877716064, loss=1.6001205444335938
I0306 12:07:46.519194 140104668485376 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.32052695751190186, loss=1.5475199222564697
I0306 12:08:21.505137 140104660092672 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.3603114187717438, loss=1.5360143184661865
I0306 12:08:56.504223 140104668485376 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.33485469222068787, loss=1.5730905532836914
I0306 12:09:31.519428 140104660092672 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.3539648950099945, loss=1.592619776725769
I0306 12:10:06.517491 140104668485376 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3390551209449768, loss=1.642957329750061
I0306 12:10:41.509114 140104660092672 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3577860891819, loss=1.6091008186340332
I0306 12:11:16.500743 140104668485376 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3513156771659851, loss=1.5272122621536255
I0306 12:11:51.515955 140104660092672 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.33591556549072266, loss=1.5491851568222046
I0306 12:12:26.448957 140104668485376 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.35080549120903015, loss=1.5900537967681885
I0306 12:13:01.419830 140104660092672 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.35248705744743347, loss=1.5812594890594482
I0306 12:13:36.341931 140104668485376 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.34491172432899475, loss=1.423823595046997
I0306 12:14:11.311537 140104660092672 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3386002480983734, loss=1.526834487915039
I0306 12:14:46.266839 140104668485376 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.354140043258667, loss=1.5870604515075684
I0306 12:15:21.296503 140104660092672 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3550657629966736, loss=1.569204330444336
I0306 12:15:56.254034 140104668485376 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.340656042098999, loss=1.540985107421875
I0306 12:16:31.258426 140104660092672 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3593778610229492, loss=1.5506179332733154
I0306 12:17:06.235117 140104668485376 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.34275269508361816, loss=1.5591124296188354
I0306 12:17:41.209286 140104660092672 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.35029616951942444, loss=1.5186793804168701
I0306 12:18:02.538479 140248407626944 spec.py:321] Evaluating on the training split.
I0306 12:18:05.145974 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:22:05.027441 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 12:22:07.632074 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:25:01.833172 140248407626944 spec.py:349] Evaluating on the test split.
I0306 12:25:04.433873 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:27:55.641608 140248407626944 submission_runner.py:469] Time since start: 62034.38s, 	Step: 103262, 	{'train/accuracy': 0.6788262724876404, 'train/loss': 1.5172978639602661, 'train/bleu': 34.018486393462084, 'validation/accuracy': 0.687204897403717, 'validation/loss': 1.4548075199127197, 'validation/bleu': 29.973279843165173, 'validation/num_examples': 3000, 'test/accuracy': 0.7016800045967102, 'test/loss': 1.3686797618865967, 'test/bleu': 30.039921547477793, 'test/num_examples': 3003, 'score': 36146.94039392471, 'total_duration': 62034.37730574608, 'accumulated_submission_time': 36146.94039392471, 'accumulated_eval_time': 25880.441450595856, 'accumulated_logging_time': 1.0862483978271484}
I0306 12:27:55.661726 140104668485376 logging_writer.py:48] [103262] accumulated_eval_time=25880.4, accumulated_logging_time=1.08625, accumulated_submission_time=36146.9, global_step=103262, preemption_count=0, score=36146.9, test/accuracy=0.70168, test/bleu=30.0399, test/loss=1.36868, test/num_examples=3003, total_duration=62034.4, train/accuracy=0.678826, train/bleu=34.0185, train/loss=1.5173, validation/accuracy=0.687205, validation/bleu=29.9733, validation/loss=1.45481, validation/num_examples=3000
I0306 12:28:09.262184 140104660092672 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.3384076952934265, loss=1.4715327024459839
I0306 12:28:44.181088 140104668485376 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.34799039363861084, loss=1.5606275796890259
I0306 12:29:19.071978 140104660092672 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3346354067325592, loss=1.5260157585144043
I0306 12:29:54.042537 140104668485376 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.35315823554992676, loss=1.6127879619598389
I0306 12:30:28.989478 140104660092672 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3419216573238373, loss=1.537975549697876
I0306 12:31:03.949532 140104668485376 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3561965227127075, loss=1.565933108329773
I0306 12:31:38.984845 140104660092672 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3491217792034149, loss=1.514189600944519
I0306 12:32:13.972914 140104668485376 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.35531318187713623, loss=1.4991576671600342
I0306 12:32:48.944599 140104660092672 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.3575044572353363, loss=1.591439127922058
I0306 12:33:23.916386 140104668485376 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.36730095744132996, loss=1.5074260234832764
I0306 12:33:58.882820 140104660092672 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3409616947174072, loss=1.5383528470993042
I0306 12:34:33.881223 140104668485376 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.33831414580345154, loss=1.4143397808074951
I0306 12:35:08.865274 140104660092672 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.36165377497673035, loss=1.5983856916427612
I0306 12:35:43.817122 140104668485376 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.35711991786956787, loss=1.4817211627960205
I0306 12:36:18.778136 140104660092672 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.35322925448417664, loss=1.5344371795654297
I0306 12:36:53.739952 140104668485376 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3467947244644165, loss=1.5529979467391968
I0306 12:37:28.723092 140104660092672 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.35219404101371765, loss=1.5461053848266602
I0306 12:38:03.718138 140104668485376 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3454935848712921, loss=1.5728319883346558
I0306 12:38:38.725851 140104660092672 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.3520266115665436, loss=1.5007822513580322
I0306 12:39:13.696047 140104668485376 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.35821521282196045, loss=1.4873631000518799
I0306 12:39:48.661415 140104660092672 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.3548473119735718, loss=1.521543025970459
I0306 12:40:23.587435 140104668485376 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.35878029465675354, loss=1.4604138135910034
I0306 12:40:58.564857 140104660092672 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.3727337419986725, loss=1.599766731262207
I0306 12:41:33.515383 140104668485376 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3607207238674164, loss=1.5082875490188599
I0306 12:41:55.904842 140248407626944 spec.py:321] Evaluating on the training split.
I0306 12:41:58.505378 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:46:03.621877 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 12:46:06.221405 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:49:10.361084 140248407626944 spec.py:349] Evaluating on the test split.
I0306 12:49:12.951054 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 12:51:48.209053 140248407626944 submission_runner.py:469] Time since start: 63466.94s, 	Step: 105665, 	{'train/accuracy': 0.6763718724250793, 'train/loss': 1.5338882207870483, 'train/bleu': 34.12100765658176, 'validation/accuracy': 0.6881071925163269, 'validation/loss': 1.4445780515670776, 'validation/bleu': 29.97703042088864, 'validation/num_examples': 3000, 'test/accuracy': 0.7015641331672668, 'test/loss': 1.360108733177185, 'test/bleu': 29.95942457831162, 'test/num_examples': 3003, 'score': 36987.0404446125, 'total_duration': 63466.944769620895, 'accumulated_submission_time': 36987.0404446125, 'accumulated_eval_time': 26472.745608329773, 'accumulated_logging_time': 1.1144263744354248}
I0306 12:51:48.229186 140104660092672 logging_writer.py:48] [105665] accumulated_eval_time=26472.7, accumulated_logging_time=1.11443, accumulated_submission_time=36987, global_step=105665, preemption_count=0, score=36987, test/accuracy=0.701564, test/bleu=29.9594, test/loss=1.36011, test/num_examples=3003, total_duration=63466.9, train/accuracy=0.676372, train/bleu=34.121, train/loss=1.53389, validation/accuracy=0.688107, validation/bleu=29.977, validation/loss=1.44458, validation/num_examples=3000
I0306 12:52:00.788404 140104668485376 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.3670129179954529, loss=1.5642976760864258
I0306 12:52:35.679556 140104660092672 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.3575173616409302, loss=1.517428994178772
I0306 12:53:10.626609 140104668485376 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.3545638918876648, loss=1.5167641639709473
I0306 12:53:45.540386 140104660092672 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.37483149766921997, loss=1.5424903631210327
I0306 12:54:20.525047 140104668485376 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.35763660073280334, loss=1.5094232559204102
I0306 12:54:55.458127 140104660092672 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.37972190976142883, loss=1.5342798233032227
I0306 12:55:30.452081 140104668485376 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.3557702302932739, loss=1.5724374055862427
I0306 12:56:05.403204 140104660092672 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.37674233317375183, loss=1.4683369398117065
I0306 12:56:40.364687 140104668485376 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.37276557087898254, loss=1.5629549026489258
I0306 12:57:15.327929 140104660092672 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3640482723712921, loss=1.4929430484771729
I0306 12:57:50.205437 140104668485376 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3487059473991394, loss=1.5320295095443726
I0306 12:58:25.099607 140104660092672 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.35602864623069763, loss=1.524053692817688
I0306 12:58:59.952442 140104668485376 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3579491972923279, loss=1.5015357732772827
I0306 12:59:34.834075 140104660092672 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.363934725522995, loss=1.4966551065444946
I0306 13:00:09.689074 140104668485376 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.381399005651474, loss=1.5022308826446533
I0306 13:00:44.564016 140104660092672 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3726690113544464, loss=1.505570411682129
I0306 13:01:19.444115 140104668485376 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3661685883998871, loss=1.5120267868041992
I0306 13:01:54.333717 140104660092672 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.35870498418807983, loss=1.523612141609192
I0306 13:02:29.253379 140104668485376 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.34578609466552734, loss=1.4655354022979736
I0306 13:03:04.132115 140104660092672 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.3809327781200409, loss=1.544959306716919
I0306 13:03:39.013955 140104668485376 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.34841111302375793, loss=1.4802820682525635
I0306 13:04:13.878912 140104660092672 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.3541969954967499, loss=1.4871125221252441
I0306 13:04:48.736241 140104668485376 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.351346492767334, loss=1.4427459239959717
I0306 13:05:23.649253 140104660092672 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.3784679174423218, loss=1.478540062904358
I0306 13:05:48.425271 140248407626944 spec.py:321] Evaluating on the training split.
I0306 13:05:51.033635 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 13:09:59.132067 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 13:10:01.741711 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 13:13:12.784098 140248407626944 spec.py:349] Evaluating on the test split.
I0306 13:13:15.386429 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 13:16:07.235469 140248407626944 submission_runner.py:469] Time since start: 64925.97s, 	Step: 108072, 	{'train/accuracy': 0.6887732148170471, 'train/loss': 1.457044005393982, 'train/bleu': 35.05783441037165, 'validation/accuracy': 0.6897881627082825, 'validation/loss': 1.4340277910232544, 'validation/bleu': 30.28818746987776, 'validation/num_examples': 3000, 'test/accuracy': 0.7047619223594666, 'test/loss': 1.3452433347702026, 'test/bleu': 30.443317200022946, 'test/num_examples': 3003, 'score': 37827.09180569649, 'total_duration': 64925.97117352486, 'accumulated_submission_time': 37827.09180569649, 'accumulated_eval_time': 27091.555745124817, 'accumulated_logging_time': 1.1432020664215088}
I0306 13:16:07.254967 140104668485376 logging_writer.py:48] [108072] accumulated_eval_time=27091.6, accumulated_logging_time=1.1432, accumulated_submission_time=37827.1, global_step=108072, preemption_count=0, score=37827.1, test/accuracy=0.704762, test/bleu=30.4433, test/loss=1.34524, test/num_examples=3003, total_duration=64926, train/accuracy=0.688773, train/bleu=35.0578, train/loss=1.45704, validation/accuracy=0.689788, validation/bleu=30.2882, validation/loss=1.43403, validation/num_examples=3000
I0306 13:16:17.343283 140104660092672 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.36375823616981506, loss=1.487376093864441
I0306 13:16:52.113293 140104668485376 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.39536991715431213, loss=1.5741387605667114
I0306 13:17:26.962018 140104660092672 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3599856495857239, loss=1.4722040891647339
I0306 13:18:01.793780 140104668485376 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.36552539467811584, loss=1.600083589553833
I0306 13:18:36.673663 140104660092672 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3667467534542084, loss=1.4288536310195923
I0306 13:19:11.579849 140104668485376 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.35920581221580505, loss=1.5161406993865967
I0306 13:19:46.501776 140104660092672 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3840278387069702, loss=1.534769058227539
I0306 13:20:21.400765 140104668485376 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3590052127838135, loss=1.4560022354125977
I0306 13:20:56.308112 140104660092672 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.36015620827674866, loss=1.4342724084854126
I0306 13:21:31.215704 140104668485376 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3802182376384735, loss=1.5693702697753906
I0306 13:22:06.094032 140104660092672 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3586491346359253, loss=1.4550584554672241
I0306 13:22:40.971554 140104668485376 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.3549054265022278, loss=1.4828245639801025
I0306 13:23:15.867504 140104660092672 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.375644326210022, loss=1.484572172164917
I0306 13:23:50.771006 140104668485376 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3814085125923157, loss=1.5662225484848022
I0306 13:24:25.701483 140104660092672 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.39047083258628845, loss=1.4943245649337769
I0306 13:25:00.599218 140104668485376 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.35894864797592163, loss=1.4757081270217896
I0306 13:25:35.488386 140104660092672 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.37949347496032715, loss=1.53261137008667
I0306 13:26:10.377374 140104668485376 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.3552536964416504, loss=1.401029348373413
I0306 13:26:45.289282 140104660092672 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.37306511402130127, loss=1.4529908895492554
I0306 13:27:20.190708 140104668485376 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.3561394214630127, loss=1.4257535934448242
I0306 13:27:55.080037 140104660092672 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.3592089116573334, loss=1.4924241304397583
I0306 13:28:29.935410 140104668485376 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3962010145187378, loss=1.4350061416625977
I0306 13:29:04.829565 140104660092672 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.36736950278282166, loss=1.4777716398239136
I0306 13:29:39.748991 140104668485376 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.36674803495407104, loss=1.5040299892425537
I0306 13:30:07.332573 140248407626944 spec.py:321] Evaluating on the training split.
I0306 13:30:09.944740 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 13:34:13.026462 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 13:34:15.634186 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 13:37:10.966445 140248407626944 spec.py:349] Evaluating on the test split.
I0306 13:37:13.562447 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 13:40:07.305546 140248407626944 submission_runner.py:469] Time since start: 66366.04s, 	Step: 110480, 	{'train/accuracy': 0.6907700300216675, 'train/loss': 1.455430269241333, 'train/bleu': 34.576012362174524, 'validation/accuracy': 0.6912713646888733, 'validation/loss': 1.425293207168579, 'validation/bleu': 30.387941172735605, 'validation/num_examples': 3000, 'test/accuracy': 0.7065461874008179, 'test/loss': 1.33441162109375, 'test/bleu': 30.22170547881406, 'test/num_examples': 3003, 'score': 38667.02536845207, 'total_duration': 66366.04125571251, 'accumulated_submission_time': 38667.02536845207, 'accumulated_eval_time': 27691.52865934372, 'accumulated_logging_time': 1.1720681190490723}
I0306 13:40:07.327069 140104660092672 logging_writer.py:48] [110480] accumulated_eval_time=27691.5, accumulated_logging_time=1.17207, accumulated_submission_time=38667, global_step=110480, preemption_count=0, score=38667, test/accuracy=0.706546, test/bleu=30.2217, test/loss=1.33441, test/num_examples=3003, total_duration=66366, train/accuracy=0.69077, train/bleu=34.576, train/loss=1.45543, validation/accuracy=0.691271, validation/bleu=30.3879, validation/loss=1.42529, validation/num_examples=3000
I0306 13:40:14.652076 140104668485376 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3567960560321808, loss=1.4480546712875366
I0306 13:40:49.489294 140104660092672 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.3863374888896942, loss=1.4335516691207886
I0306 13:41:24.329174 140104668485376 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.3785049319267273, loss=1.484610676765442
I0306 13:41:59.214147 140104660092672 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.38731086254119873, loss=1.5393232107162476
I0306 13:42:34.085566 140104668485376 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3597860336303711, loss=1.47822904586792
I0306 13:43:09.012151 140104660092672 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.37736526131629944, loss=1.431633472442627
I0306 13:43:43.907831 140104668485376 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.38587722182273865, loss=1.4730010032653809
I0306 13:44:18.834188 140104660092672 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3598076105117798, loss=1.4213112592697144
I0306 13:44:53.737962 140104668485376 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.38116368651390076, loss=1.439765453338623
I0306 13:45:28.648143 140104660092672 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.36162635684013367, loss=1.4616515636444092
I0306 13:46:03.564479 140104668485376 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3800494074821472, loss=1.5172536373138428
I0306 13:46:38.480803 140104660092672 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3728395998477936, loss=1.4893791675567627
I0306 13:47:13.432470 140104668485376 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3576711416244507, loss=1.4296025037765503
I0306 13:47:48.322012 140104660092672 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.37455812096595764, loss=1.4576442241668701
I0306 13:48:23.186422 140104668485376 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3537166118621826, loss=1.4991512298583984
I0306 13:48:58.045105 140104660092672 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.37587082386016846, loss=1.463134765625
I0306 13:49:32.923117 140104668485376 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.3684955835342407, loss=1.4600600004196167
I0306 13:50:07.813274 140104660092672 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.37728944420814514, loss=1.4262880086898804
I0306 13:50:42.702552 140104668485376 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.35711321234703064, loss=1.4559041261672974
I0306 13:51:17.595587 140104660092672 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3818811774253845, loss=1.493435263633728
I0306 13:51:52.428861 140104668485376 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.36519697308540344, loss=1.414495825767517
I0306 13:52:27.286623 140104660092672 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3754326403141022, loss=1.4659262895584106
I0306 13:53:02.122960 140104668485376 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.3826534152030945, loss=1.47910475730896
I0306 13:53:36.989801 140104660092672 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.38054218888282776, loss=1.4918748140335083
I0306 13:54:07.328696 140248407626944 spec.py:321] Evaluating on the training split.
I0306 13:54:09.932041 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 13:58:02.898056 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 13:58:05.499399 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:00:49.132400 140248407626944 spec.py:349] Evaluating on the test split.
I0306 14:00:51.729162 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:03:32.544444 140248407626944 submission_runner.py:469] Time since start: 67771.28s, 	Step: 112888, 	{'train/accuracy': 0.6914148330688477, 'train/loss': 1.4510936737060547, 'train/bleu': 34.893560258747556, 'validation/accuracy': 0.6927669048309326, 'validation/loss': 1.4183810949325562, 'validation/bleu': 30.68908206575, 'validation/num_examples': 3000, 'test/accuracy': 0.7070212364196777, 'test/loss': 1.3276269435882568, 'test/bleu': 30.497464849337014, 'test/num_examples': 3003, 'score': 39506.88290643692, 'total_duration': 67771.2801516056, 'accumulated_submission_time': 39506.88290643692, 'accumulated_eval_time': 28256.744346618652, 'accumulated_logging_time': 1.2026426792144775}
I0306 14:03:32.565295 140104668485376 logging_writer.py:48] [112888] accumulated_eval_time=28256.7, accumulated_logging_time=1.20264, accumulated_submission_time=39506.9, global_step=112888, preemption_count=0, score=39506.9, test/accuracy=0.707021, test/bleu=30.4975, test/loss=1.32763, test/num_examples=3003, total_duration=67771.3, train/accuracy=0.691415, train/bleu=34.8936, train/loss=1.45109, validation/accuracy=0.692767, validation/bleu=30.6891, validation/loss=1.41838, validation/num_examples=3000
I0306 14:03:37.095401 140104660092672 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.38184085488319397, loss=1.4910677671432495
I0306 14:04:11.987926 140104668485376 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.363579660654068, loss=1.4564173221588135
I0306 14:04:46.911690 140104660092672 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.38454678654670715, loss=1.4609787464141846
I0306 14:05:21.862210 140104668485376 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.3789992332458496, loss=1.4119843244552612
I0306 14:05:56.834572 140104660092672 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.39077311754226685, loss=1.4651823043823242
I0306 14:06:31.829918 140104668485376 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3700321912765503, loss=1.4304533004760742
I0306 14:07:06.818846 140104660092672 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3533543646335602, loss=1.359617829322815
I0306 14:07:41.808197 140104668485376 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.36592137813568115, loss=1.4330624341964722
I0306 14:08:16.824647 140104660092672 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.37617743015289307, loss=1.4455740451812744
I0306 14:08:51.793246 140104668485376 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.37152618169784546, loss=1.544534683227539
I0306 14:09:26.761819 140104660092672 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.3812364339828491, loss=1.4804548025131226
I0306 14:10:01.750173 140104668485376 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3702808618545532, loss=1.461695671081543
I0306 14:10:36.701646 140104660092672 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3711869418621063, loss=1.4164941310882568
I0306 14:11:11.664073 140104668485376 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.35947927832603455, loss=1.4345364570617676
I0306 14:11:46.633280 140104660092672 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.36192837357521057, loss=1.4401379823684692
I0306 14:12:21.647804 140104668485376 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3717011511325836, loss=1.4405157566070557
I0306 14:12:56.611045 140104660092672 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.38654959201812744, loss=1.4108487367630005
I0306 14:13:31.596535 140104668485376 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.3621174395084381, loss=1.4320151805877686
I0306 14:14:06.568202 140104660092672 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.38639208674430847, loss=1.368430256843567
I0306 14:14:41.516057 140104668485376 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.36500537395477295, loss=1.4354811906814575
I0306 14:15:16.496046 140104660092672 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.38525769114494324, loss=1.4943419694900513
I0306 14:15:51.448335 140104668485376 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.38706809282302856, loss=1.4622246026992798
I0306 14:16:26.454199 140104660092672 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.37562018632888794, loss=1.4281147718429565
I0306 14:17:01.412232 140104668485376 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.40731045603752136, loss=1.4000993967056274
I0306 14:17:32.561878 140248407626944 spec.py:321] Evaluating on the training split.
I0306 14:17:35.168757 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:21:30.124295 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 14:21:32.728532 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:24:29.563028 140248407626944 spec.py:349] Evaluating on the test split.
I0306 14:24:32.157565 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:27:29.262487 140248407626944 submission_runner.py:469] Time since start: 69208.00s, 	Step: 115290, 	{'train/accuracy': 0.6954032778739929, 'train/loss': 1.4250965118408203, 'train/bleu': 35.681854361707195, 'validation/accuracy': 0.6933231353759766, 'validation/loss': 1.4148573875427246, 'validation/bleu': 30.690845862097188, 'validation/num_examples': 3000, 'test/accuracy': 0.7082145810127258, 'test/loss': 1.3245909214019775, 'test/bleu': 30.328584945674155, 'test/num_examples': 3003, 'score': 40346.73822975159, 'total_duration': 69207.99819898605, 'accumulated_submission_time': 40346.73822975159, 'accumulated_eval_time': 28853.44492459297, 'accumulated_logging_time': 1.2314317226409912}
I0306 14:27:29.283470 140104660092672 logging_writer.py:48] [115290] accumulated_eval_time=28853.4, accumulated_logging_time=1.23143, accumulated_submission_time=40346.7, global_step=115290, preemption_count=0, score=40346.7, test/accuracy=0.708215, test/bleu=30.3286, test/loss=1.32459, test/num_examples=3003, total_duration=69208, train/accuracy=0.695403, train/bleu=35.6819, train/loss=1.4251, validation/accuracy=0.693323, validation/bleu=30.6908, validation/loss=1.41486, validation/num_examples=3000
I0306 14:27:33.120571 140104668485376 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.3628908097743988, loss=1.4838871955871582
I0306 14:28:08.007806 140104660092672 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3655546009540558, loss=1.4220479726791382
I0306 14:28:42.946896 140104668485376 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.3543657064437866, loss=1.4328500032424927
I0306 14:29:17.919977 140104660092672 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.38269758224487305, loss=1.4264589548110962
I0306 14:29:52.859365 140104668485376 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.37884753942489624, loss=1.4635878801345825
I0306 14:30:27.826646 140104660092672 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.38202419877052307, loss=1.4136403799057007
I0306 14:31:02.767398 140104668485376 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.3698301613330841, loss=1.4406912326812744
I0306 14:31:37.741933 140104660092672 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.39641815423965454, loss=1.4942240715026855
I0306 14:32:12.681312 140104668485376 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.39927124977111816, loss=1.5170683860778809
I0306 14:32:47.617893 140104660092672 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.37420591711997986, loss=1.3662126064300537
I0306 14:33:22.611357 140104668485376 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3683454096317291, loss=1.4444894790649414
I0306 14:33:57.565243 140104660092672 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.3760214149951935, loss=1.5198945999145508
I0306 14:34:32.545017 140104668485376 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3690882921218872, loss=1.3728214502334595
I0306 14:35:07.534201 140104660092672 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.3670315444469452, loss=1.3410898447036743
I0306 14:35:42.516824 140104668485376 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.3999004662036896, loss=1.441786766052246
I0306 14:36:17.532031 140104660092672 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.3677706718444824, loss=1.3823574781417847
I0306 14:36:52.548307 140104668485376 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.36903509497642517, loss=1.448764681816101
I0306 14:37:27.537111 140104660092672 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3598079979419708, loss=1.3772941827774048
I0306 14:38:02.493275 140104668485376 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.38356709480285645, loss=1.4233523607254028
I0306 14:38:37.480440 140104660092672 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.3849676847457886, loss=1.4661990404129028
I0306 14:39:12.464898 140104668485376 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.36546066403388977, loss=1.469712257385254
I0306 14:39:47.454313 140104660092672 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.3690255880355835, loss=1.4726805686950684
I0306 14:40:22.429852 140104668485376 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.37205052375793457, loss=1.439759612083435
I0306 14:40:57.389581 140104660092672 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3649121820926666, loss=1.4374920129776
I0306 14:41:29.557841 140248407626944 spec.py:321] Evaluating on the training split.
I0306 14:41:32.165715 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:45:17.026886 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 14:45:19.629269 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:48:14.375606 140248407626944 spec.py:349] Evaluating on the test split.
I0306 14:48:16.975856 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 14:51:09.135482 140248407626944 submission_runner.py:469] Time since start: 70627.87s, 	Step: 117693, 	{'train/accuracy': 0.6958362460136414, 'train/loss': 1.4230217933654785, 'train/bleu': 35.119542558351824, 'validation/accuracy': 0.6937680840492249, 'validation/loss': 1.414509654045105, 'validation/bleu': 30.667707931823198, 'validation/num_examples': 3000, 'test/accuracy': 0.7088286280632019, 'test/loss': 1.321641445159912, 'test/bleu': 30.554928100510434, 'test/num_examples': 3003, 'score': 41186.87511754036, 'total_duration': 70627.8711707592, 'accumulated_submission_time': 41186.87511754036, 'accumulated_eval_time': 29433.022482395172, 'accumulated_logging_time': 1.2613105773925781}
I0306 14:51:09.156849 140104668485376 logging_writer.py:48] [117693] accumulated_eval_time=29433, accumulated_logging_time=1.26131, accumulated_submission_time=41186.9, global_step=117693, preemption_count=0, score=41186.9, test/accuracy=0.708829, test/bleu=30.5549, test/loss=1.32164, test/num_examples=3003, total_duration=70627.9, train/accuracy=0.695836, train/bleu=35.1195, train/loss=1.42302, validation/accuracy=0.693768, validation/bleu=30.6677, validation/loss=1.41451, validation/num_examples=3000
I0306 14:51:11.950739 140104660092672 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.37324997782707214, loss=1.458078384399414
I0306 14:51:46.844847 140104668485376 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3748210668563843, loss=1.4010968208312988
I0306 14:52:21.779024 140104660092672 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.38973891735076904, loss=1.4390093088150024
I0306 14:52:56.770823 140104668485376 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.37398219108581543, loss=1.4475620985031128
I0306 14:53:31.725084 140104660092672 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.38234642148017883, loss=1.4590709209442139
I0306 14:54:06.708143 140104668485376 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.38546377420425415, loss=1.4835549592971802
I0306 14:54:41.706943 140104660092672 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.36202096939086914, loss=1.4369087219238281
I0306 14:55:16.638852 140104668485376 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3906670808792114, loss=1.4757161140441895
I0306 14:55:51.597311 140104660092672 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.37614884972572327, loss=1.4151653051376343
I0306 14:56:26.561123 140104668485376 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.36544227600097656, loss=1.4703969955444336
I0306 14:57:01.533282 140104660092672 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.3823987543582916, loss=1.4329646825790405
I0306 14:57:36.492372 140104668485376 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.3607527017593384, loss=1.4141415357589722
I0306 14:58:11.484982 140104660092672 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.3776053488254547, loss=1.4493904113769531
I0306 14:58:46.472918 140104668485376 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3877115845680237, loss=1.4399992227554321
I0306 14:59:21.459380 140104660092672 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.38862985372543335, loss=1.4733527898788452
I0306 14:59:56.435583 140104668485376 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.38864436745643616, loss=1.505663514137268
I0306 15:00:31.379508 140104660092672 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.3986970782279968, loss=1.503201961517334
I0306 15:01:06.372380 140104668485376 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3615071177482605, loss=1.4424546957015991
I0306 15:01:41.334584 140104660092672 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.36636805534362793, loss=1.4699949026107788
I0306 15:02:16.308995 140104668485376 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3702567517757416, loss=1.4046504497528076
I0306 15:02:51.282033 140104660092672 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.3747038245201111, loss=1.356715440750122
I0306 15:03:26.305449 140104668485376 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.37688449025154114, loss=1.4778441190719604
I0306 15:04:01.330531 140104660092672 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.36572933197021484, loss=1.407757043838501
I0306 15:04:36.337339 140104668485376 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.365337073802948, loss=1.466500997543335
I0306 15:05:09.244551 140248407626944 spec.py:321] Evaluating on the training split.
I0306 15:05:11.850176 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 15:08:48.710426 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 15:08:51.322183 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 15:11:44.116397 140248407626944 spec.py:349] Evaluating on the test split.
I0306 15:11:46.713028 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 15:14:38.009043 140248407626944 submission_runner.py:469] Time since start: 72036.74s, 	Step: 120095, 	{'train/accuracy': 0.6928597688674927, 'train/loss': 1.4398102760314941, 'train/bleu': 35.55839829008526, 'validation/accuracy': 0.6939781904220581, 'validation/loss': 1.413612961769104, 'validation/bleu': 30.751387102081758, 'validation/num_examples': 3000, 'test/accuracy': 0.7088518142700195, 'test/loss': 1.3207508325576782, 'test/bleu': 30.560136961566112, 'test/num_examples': 3003, 'score': 42026.82373261452, 'total_duration': 72036.74474716187, 'accumulated_submission_time': 42026.82373261452, 'accumulated_eval_time': 30001.786912441254, 'accumulated_logging_time': 1.2909040451049805}
I0306 15:14:38.030563 140104660092672 logging_writer.py:48] [120095] accumulated_eval_time=30001.8, accumulated_logging_time=1.2909, accumulated_submission_time=42026.8, global_step=120095, preemption_count=0, score=42026.8, test/accuracy=0.708852, test/bleu=30.5601, test/loss=1.32075, test/num_examples=3003, total_duration=72036.7, train/accuracy=0.69286, train/bleu=35.5584, train/loss=1.43981, validation/accuracy=0.693978, validation/bleu=30.7514, validation/loss=1.41361, validation/num_examples=3000
I0306 15:14:40.125322 140104668485376 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.3835984170436859, loss=1.363379955291748
I0306 15:15:15.066208 140104660092672 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.3765033185482025, loss=1.3803147077560425
I0306 15:15:50.007888 140104668485376 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3819875121116638, loss=1.518829584121704
I0306 15:16:24.948751 140104660092672 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.36118537187576294, loss=1.3736650943756104
I0306 15:16:59.904607 140104668485376 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3917306363582611, loss=1.4668595790863037
I0306 15:17:34.912926 140104660092672 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3679788112640381, loss=1.4056676626205444
I0306 15:18:09.883737 140104668485376 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.36152276396751404, loss=1.439064860343933
I0306 15:18:44.844493 140104660092672 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.3832666873931885, loss=1.4869108200073242
I0306 15:19:19.825628 140104668485376 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.3665475845336914, loss=1.4541370868682861
I0306 15:19:54.803789 140104660092672 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.36757948994636536, loss=1.4261846542358398
I0306 15:20:29.773362 140104668485376 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3762560188770294, loss=1.497096061706543
I0306 15:21:04.735105 140104660092672 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3779725134372711, loss=1.4300082921981812
I0306 15:21:39.706071 140104668485376 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.372767835855484, loss=1.4796888828277588
I0306 15:22:14.678323 140104660092672 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3521772623062134, loss=1.3897101879119873
I0306 15:22:49.673336 140104668485376 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.37438657879829407, loss=1.4921220541000366
I0306 15:23:24.681593 140104660092672 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.359353631734848, loss=1.4031673669815063
I0306 15:23:59.656481 140104668485376 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.36673229932785034, loss=1.394675850868225
I0306 15:24:34.632378 140104660092672 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.3775629997253418, loss=1.4671341180801392
I0306 15:25:09.598734 140104668485376 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3829366862773895, loss=1.442946195602417
I0306 15:25:44.582770 140104660092672 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.3606213331222534, loss=1.3717262744903564
I0306 15:26:19.561925 140104668485376 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.3916752338409424, loss=1.4886515140533447
I0306 15:26:54.522586 140104660092672 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.37833353877067566, loss=1.4207558631896973
I0306 15:27:29.549837 140104668485376 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.38233792781829834, loss=1.3599514961242676
I0306 15:28:04.517260 140104660092672 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.3734702467918396, loss=1.4098989963531494
I0306 15:28:38.113873 140248407626944 spec.py:321] Evaluating on the training split.
I0306 15:28:40.721934 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 15:32:29.364136 140248407626944 spec.py:333] Evaluating on the validation split.
I0306 15:32:31.969738 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 15:35:24.868740 140248407626944 spec.py:349] Evaluating on the test split.
I0306 15:35:27.466344 140248407626944 workload.py:181] Translating evaluation dataset.
I0306 15:38:18.512506 140248407626944 submission_runner.py:469] Time since start: 73457.25s, 	Step: 122497, 	{'train/accuracy': 0.6967914700508118, 'train/loss': 1.4181947708129883, 'train/bleu': 35.15719713891962, 'validation/accuracy': 0.6939781904220581, 'validation/loss': 1.413612961769104, 'validation/bleu': 30.751387102081758, 'validation/num_examples': 3000, 'test/accuracy': 0.7088518142700195, 'test/loss': 1.3207508325576782, 'test/bleu': 30.560136961566112, 'test/num_examples': 3003, 'score': 42866.767013549805, 'total_duration': 73457.24822068214, 'accumulated_submission_time': 42866.767013549805, 'accumulated_eval_time': 30582.185494422913, 'accumulated_logging_time': 1.3216969966888428}
I0306 15:38:18.534220 140104668485376 logging_writer.py:48] [122497] accumulated_eval_time=30582.2, accumulated_logging_time=1.3217, accumulated_submission_time=42866.8, global_step=122497, preemption_count=0, score=42866.8, test/accuracy=0.708852, test/bleu=30.5601, test/loss=1.32075, test/num_examples=3003, total_duration=73457.2, train/accuracy=0.696791, train/bleu=35.1572, train/loss=1.41819, validation/accuracy=0.693978, validation/bleu=30.7514, validation/loss=1.41361, validation/num_examples=3000
I0306 15:38:19.937397 140104660092672 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3809933662414551, loss=1.4950405359268188
I0306 15:38:54.872961 140104668485376 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.363659530878067, loss=1.3964155912399292
I0306 15:39:29.766723 140104660092672 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.37153708934783936, loss=1.4051250219345093
I0306 15:40:04.739322 140104668485376 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3644486665725708, loss=1.444406509399414
I0306 15:40:39.667812 140104660092672 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.35300424695014954, loss=1.4090484380722046
I0306 15:41:14.640524 140104668485376 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3943498134613037, loss=1.410991907119751
I0306 15:41:49.594864 140104660092672 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.3760972321033478, loss=1.4465503692626953
I0306 15:42:24.566715 140104668485376 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.386599600315094, loss=1.4659186601638794
I0306 15:42:59.532542 140104660092672 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.37164610624313354, loss=1.433603048324585
I0306 15:43:34.488115 140104668485376 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3782289922237396, loss=1.460642695426941
I0306 15:44:09.457526 140104660092672 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.3681040108203888, loss=1.4820654392242432
I0306 15:44:44.426212 140104668485376 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.37825489044189453, loss=1.415114164352417
I0306 15:45:19.418404 140104660092672 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3778017461299896, loss=1.428612232208252
I0306 15:45:54.435811 140104668485376 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.37300780415534973, loss=1.422983169555664
I0306 15:46:29.444873 140104660092672 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3599405586719513, loss=1.4361530542373657
I0306 15:47:04.398543 140104668485376 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.3781221807003021, loss=1.4471513032913208
I0306 15:47:39.387565 140104660092672 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.3737161457538605, loss=1.4456045627593994
I0306 15:48:14.370390 140104668485376 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.35536253452301025, loss=1.4085280895233154
I0306 15:48:49.361038 140104660092672 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.37949949502944946, loss=1.4331775903701782
I0306 15:49:24.345740 140104668485376 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.37678176164627075, loss=1.4056203365325928
I0306 15:49:59.330802 140104660092672 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.36318278312683105, loss=1.4339170455932617
I0306 15:50:34.301534 140104668485376 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.3634353280067444, loss=1.4462631940841675
I0306 15:51:09.314930 140104660092672 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.3642129898071289, loss=1.4199939966201782
I0306 15:51:44.291997 140104668485376 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.3624022305011749, loss=1.412455439567566
I0306 15:52:18.586712 140104660092672 logging_writer.py:48] [124899] global_step=124899, preemption_count=0, score=43706.7
I0306 15:52:18.612252 140248407626944 submission_runner.py:646] Tuning trial 4/5
I0306 15:52:18.612396 140248407626944 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0306 15:52:18.614865 140248407626944 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005826972774229944, 'train/loss': 11.045845985412598, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.047025680541992, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.040393829345703, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.42166519165039, 'total_duration': 942.7243485450745, 'accumulated_submission_time': 26.42166519165039, 'accumulated_eval_time': 916.302565574646, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2400, {'train/accuracy': 0.5243012309074402, 'train/loss': 2.668487071990967, 'train/bleu': 22.06324745666449, 'validation/accuracy': 0.523582935333252, 'validation/loss': 2.6468873023986816, 'validation/bleu': 18.140998608213064, 'validation/num_examples': 3000, 'test/accuracy': 0.5218630433082581, 'test/loss': 2.682077407836914, 'test/bleu': 16.73002282967136, 'test/num_examples': 3003, 'score': 866.5041997432709, 'total_duration': 2267.132890224457, 'accumulated_submission_time': 866.5041997432709, 'accumulated_eval_time': 1400.4436326026917, 'accumulated_logging_time': 0.016210079193115234, 'global_step': 2400, 'preemption_count': 0}), (4800, {'train/accuracy': 0.5761494040489197, 'train/loss': 2.251298189163208, 'train/bleu': 26.57507033736089, 'validation/accuracy': 0.595295786857605, 'validation/loss': 2.0990915298461914, 'validation/bleu': 23.169889766266664, 'validation/num_examples': 3000, 'test/accuracy': 0.5996060967445374, 'test/loss': 2.0647213459014893, 'test/bleu': 21.939725157588853, 'test/num_examples': 3003, 'score': 1706.4510326385498, 'total_duration': 3602.2941720485687, 'accumulated_submission_time': 1706.4510326385498, 'accumulated_eval_time': 1895.479255437851, 'accumulated_logging_time': 0.03408670425415039, 'global_step': 4800, 'preemption_count': 0}), (7201, {'train/accuracy': 0.5885499119758606, 'train/loss': 2.1332192420959473, 'train/bleu': 27.03930457150921, 'validation/accuracy': 0.603848934173584, 'validation/loss': 2.011162042617798, 'validation/bleu': 23.893339803280817, 'validation/num_examples': 3000, 'test/accuracy': 0.6076700091362, 'test/loss': 1.981911540031433, 'test/bleu': 22.898264565142487, 'test/num_examples': 3003, 'score': 2546.297681570053, 'total_duration': 4946.307964324951, 'accumulated_submission_time': 2546.297681570053, 'accumulated_eval_time': 2399.4663529396057, 'accumulated_logging_time': 0.05392956733703613, 'global_step': 7201, 'preemption_count': 0}), (9606, {'train/accuracy': 0.592634379863739, 'train/loss': 2.1329338550567627, 'train/bleu': 27.754886105412552, 'validation/accuracy': 0.6082367300987244, 'validation/loss': 1.9942150115966797, 'validation/bleu': 24.553891809431896, 'validation/num_examples': 3000, 'test/accuracy': 0.6162785291671753, 'test/loss': 1.9350523948669434, 'test/bleu': 23.53525358070045, 'test/num_examples': 3003, 'score': 3386.2254931926727, 'total_duration': 6314.430859565735, 'accumulated_submission_time': 3386.2254931926727, 'accumulated_eval_time': 2927.4906842708588, 'accumulated_logging_time': 0.07434701919555664, 'global_step': 9606, 'preemption_count': 0}), (12013, {'train/accuracy': 0.5948514342308044, 'train/loss': 2.1006462574005127, 'train/bleu': 28.06506934391542, 'validation/accuracy': 0.610622227191925, 'validation/loss': 1.96293044090271, 'validation/bleu': 24.316434139255463, 'validation/num_examples': 3000, 'test/accuracy': 0.6173444390296936, 'test/loss': 1.9210034608840942, 'test/bleu': 23.64199541545895, 'test/num_examples': 3003, 'score': 4226.262289047241, 'total_duration': 7808.863825082779, 'accumulated_submission_time': 4226.262289047241, 'accumulated_eval_time': 3581.718025445938, 'accumulated_logging_time': 0.0946357250213623, 'global_step': 12013, 'preemption_count': 0}), (14416, {'train/accuracy': 0.5995975732803345, 'train/loss': 2.0686583518981934, 'train/bleu': 27.711498802894372, 'validation/accuracy': 0.6161718368530273, 'validation/loss': 1.9349751472473145, 'validation/bleu': 24.921011789710803, 'validation/num_examples': 3000, 'test/accuracy': 0.6221411228179932, 'test/loss': 1.8915694952011108, 'test/bleu': 24.013532325379224, 'test/num_examples': 3003, 'score': 5066.354030370712, 'total_duration': 9173.808131456375, 'accumulated_submission_time': 5066.354030370712, 'accumulated_eval_time': 4106.405398845673, 'accumulated_logging_time': 0.11338615417480469, 'global_step': 14416, 'preemption_count': 0}), (16817, {'train/accuracy': 0.5967208743095398, 'train/loss': 2.0855557918548584, 'train/bleu': 27.791125013759444, 'validation/accuracy': 0.6140830516815186, 'validation/loss': 1.9277559518814087, 'validation/bleu': 24.711194840199852, 'validation/num_examples': 3000, 'test/accuracy': 0.6215502023696899, 'test/loss': 1.8961756229400635, 'test/bleu': 23.647379412036194, 'test/num_examples': 3003, 'score': 5906.49445939064, 'total_duration': 10528.987439870834, 'accumulated_submission_time': 5906.49445939064, 'accumulated_eval_time': 4621.289161682129, 'accumulated_logging_time': 0.13210725784301758, 'global_step': 16817, 'preemption_count': 0}), (19215, {'train/accuracy': 0.6089617609977722, 'train/loss': 1.972785472869873, 'train/bleu': 29.01144004101766, 'validation/accuracy': 0.6204113364219666, 'validation/loss': 1.9081695079803467, 'validation/bleu': 24.698706358958617, 'validation/num_examples': 3000, 'test/accuracy': 0.6271694898605347, 'test/loss': 1.8622543811798096, 'test/bleu': 24.1552987499265, 'test/num_examples': 3003, 'score': 6746.372478961945, 'total_duration': 11879.752734184265, 'accumulated_submission_time': 6746.372478961945, 'accumulated_eval_time': 5132.020322084427, 'accumulated_logging_time': 0.15047740936279297, 'global_step': 19215, 'preemption_count': 0}), (21607, {'train/accuracy': 0.600994884967804, 'train/loss': 2.069777727127075, 'train/bleu': 28.01189501640959, 'validation/accuracy': 0.6183472275733948, 'validation/loss': 1.9050275087356567, 'validation/bleu': 24.4177373588781, 'validation/num_examples': 3000, 'test/accuracy': 0.6240528225898743, 'test/loss': 1.8706202507019043, 'test/bleu': 23.36249353772997, 'test/num_examples': 3003, 'score': 7586.503115653992, 'total_duration': 13329.801872253418, 'accumulated_submission_time': 7586.503115653992, 'accumulated_eval_time': 5741.78190612793, 'accumulated_logging_time': 0.17234587669372559, 'global_step': 21607, 'preemption_count': 0}), (24000, {'train/accuracy': 0.5988175272941589, 'train/loss': 2.0836374759674072, 'train/bleu': 28.698458458970528, 'validation/accuracy': 0.6217339038848877, 'validation/loss': 1.8910731077194214, 'validation/bleu': 25.055974462199096, 'validation/num_examples': 3000, 'test/accuracy': 0.6299038529396057, 'test/loss': 1.8353830575942993, 'test/bleu': 24.380433008049046, 'test/num_examples': 3003, 'score': 8426.663962602615, 'total_duration': 14735.512577295303, 'accumulated_submission_time': 8426.663962602615, 'accumulated_eval_time': 6307.178397893906, 'accumulated_logging_time': 0.19337987899780273, 'global_step': 24000, 'preemption_count': 0}), (26397, {'train/accuracy': 0.6073904037475586, 'train/loss': 2.0108675956726074, 'train/bleu': 28.470763039248457, 'validation/accuracy': 0.6224631071090698, 'validation/loss': 1.8763236999511719, 'validation/bleu': 24.98457005789974, 'validation/num_examples': 3000, 'test/accuracy': 0.6294288039207458, 'test/loss': 1.8344347476959229, 'test/bleu': 24.543198578889346, 'test/num_examples': 3003, 'score': 9266.580038070679, 'total_duration': 16158.284749269485, 'accumulated_submission_time': 9266.580038070679, 'accumulated_eval_time': 6889.878755331039, 'accumulated_logging_time': 0.21436476707458496, 'global_step': 26397, 'preemption_count': 0}), (28798, {'train/accuracy': 0.6040464043617249, 'train/loss': 2.0616681575775146, 'train/bleu': 28.303358346541863, 'validation/accuracy': 0.6231182217597961, 'validation/loss': 1.8767093420028687, 'validation/bleu': 25.35758235315749, 'validation/num_examples': 3000, 'test/accuracy': 0.6336577534675598, 'test/loss': 1.808383822441101, 'test/bleu': 24.397016183603924, 'test/num_examples': 3003, 'score': 10106.486082792282, 'total_duration': 17518.629702091217, 'accumulated_submission_time': 10106.486082792282, 'accumulated_eval_time': 7410.162064552307, 'accumulated_logging_time': 0.23556137084960938, 'global_step': 28798, 'preemption_count': 0}), (31199, {'train/accuracy': 0.6023253202438354, 'train/loss': 2.0468084812164307, 'train/bleu': 29.050422495080856, 'validation/accuracy': 0.6252194046974182, 'validation/loss': 1.8635061979293823, 'validation/bleu': 25.19420020542427, 'validation/num_examples': 3000, 'test/accuracy': 0.6330784559249878, 'test/loss': 1.8104184865951538, 'test/bleu': 25.023926394236796, 'test/num_examples': 3003, 'score': 10946.36867761612, 'total_duration': 18920.48673558235, 'accumulated_submission_time': 10946.36867761612, 'accumulated_eval_time': 7971.982358932495, 'accumulated_logging_time': 0.2559394836425781, 'global_step': 31199, 'preemption_count': 0}), (33598, {'train/accuracy': 0.6076792478561401, 'train/loss': 2.0037922859191895, 'train/bleu': 28.84632768272457, 'validation/accuracy': 0.62728351354599, 'validation/loss': 1.8529053926467896, 'validation/bleu': 25.832449345547825, 'validation/num_examples': 3000, 'test/accuracy': 0.6383965015411377, 'test/loss': 1.7966763973236084, 'test/bleu': 25.50332373340371, 'test/num_examples': 3003, 'score': 11786.552738904953, 'total_duration': 20277.04630637169, 'accumulated_submission_time': 11786.552738904953, 'accumulated_eval_time': 8488.200779438019, 'accumulated_logging_time': 0.2782886028289795, 'global_step': 33598, 'preemption_count': 0}), (36002, {'train/accuracy': 0.6086857318878174, 'train/loss': 2.0054149627685547, 'train/bleu': 28.876378522990866, 'validation/accuracy': 0.6295948624610901, 'validation/loss': 1.84092116355896, 'validation/bleu': 26.17329039950227, 'validation/num_examples': 3000, 'test/accuracy': 0.6351871490478516, 'test/loss': 1.7874221801757812, 'test/bleu': 24.81050768744868, 'test/num_examples': 3003, 'score': 12626.428744316101, 'total_duration': 21624.183537483215, 'accumulated_submission_time': 12626.428744316101, 'accumulated_eval_time': 8995.305968999863, 'accumulated_logging_time': 0.30034947395324707, 'global_step': 36002, 'preemption_count': 0}), (38404, {'train/accuracy': 0.6169231534004211, 'train/loss': 1.9332997798919678, 'train/bleu': 29.345306079455177, 'validation/accuracy': 0.6299780011177063, 'validation/loss': 1.8246445655822754, 'validation/bleu': 25.469598513026288, 'validation/num_examples': 3000, 'test/accuracy': 0.6377360820770264, 'test/loss': 1.7799433469772339, 'test/bleu': 24.733074118360015, 'test/num_examples': 3003, 'score': 13466.497159481049, 'total_duration': 22972.11173725128, 'accumulated_submission_time': 13466.497159481049, 'accumulated_eval_time': 9503.008731365204, 'accumulated_logging_time': 0.3213996887207031, 'global_step': 38404, 'preemption_count': 0}), (40805, {'train/accuracy': 0.607897937297821, 'train/loss': 1.9986627101898193, 'train/bleu': 29.005081207177977, 'validation/accuracy': 0.6303117275238037, 'validation/loss': 1.8237323760986328, 'validation/bleu': 25.89962378845285, 'validation/num_examples': 3000, 'test/accuracy': 0.6384196281433105, 'test/loss': 1.772047758102417, 'test/bleu': 24.974593643082358, 'test/num_examples': 3003, 'score': 14306.475115776062, 'total_duration': 24353.115441560745, 'accumulated_submission_time': 14306.475115776062, 'accumulated_eval_time': 10043.873878240585, 'accumulated_logging_time': 0.3443717956542969, 'global_step': 40805, 'preemption_count': 0}), (43206, {'train/accuracy': 0.6099070906639099, 'train/loss': 1.996186375617981, 'train/bleu': 29.378764128653852, 'validation/accuracy': 0.6331421732902527, 'validation/loss': 1.8111562728881836, 'validation/bleu': 25.968980067543747, 'validation/num_examples': 3000, 'test/accuracy': 0.642022967338562, 'test/loss': 1.7490240335464478, 'test/bleu': 25.563272391854316, 'test/num_examples': 3003, 'score': 15146.454290151596, 'total_duration': 25761.662665843964, 'accumulated_submission_time': 15146.454290151596, 'accumulated_eval_time': 10612.285935878754, 'accumulated_logging_time': 0.36600255966186523, 'global_step': 43206, 'preemption_count': 0}), (45607, {'train/accuracy': 0.6180210709571838, 'train/loss': 1.938862681388855, 'train/bleu': 29.55189084693673, 'validation/accuracy': 0.6367760300636292, 'validation/loss': 1.7869443893432617, 'validation/bleu': 26.184702858415104, 'validation/num_examples': 3000, 'test/accuracy': 0.6419302821159363, 'test/loss': 1.7406638860702515, 'test/bleu': 25.08556920046382, 'test/num_examples': 3003, 'score': 15986.543020248413, 'total_duration': 27139.23193717003, 'accumulated_submission_time': 15986.543020248413, 'accumulated_eval_time': 11149.60583281517, 'accumulated_logging_time': 0.39058470726013184, 'global_step': 45607, 'preemption_count': 0}), (48007, {'train/accuracy': 0.61665278673172, 'train/loss': 1.9455482959747314, 'train/bleu': 28.975495628399077, 'validation/accuracy': 0.6345264911651611, 'validation/loss': 1.8003181219100952, 'validation/bleu': 26.03076655066841, 'validation/num_examples': 3000, 'test/accuracy': 0.6408759355545044, 'test/loss': 1.737618088722229, 'test/bleu': 25.14725041646701, 'test/num_examples': 3003, 'score': 16826.650903701782, 'total_duration': 28508.091321468353, 'accumulated_submission_time': 16826.650903701782, 'accumulated_eval_time': 11678.197509527206, 'accumulated_logging_time': 0.4134480953216553, 'global_step': 48007, 'preemption_count': 0}), (50409, {'train/accuracy': 0.6405792236328125, 'train/loss': 1.7554317712783813, 'train/bleu': 30.948378319609148, 'validation/accuracy': 0.6364299654960632, 'validation/loss': 1.7731913328170776, 'validation/bleu': 23.44060198108962, 'validation/num_examples': 3000, 'test/accuracy': 0.6470745205879211, 'test/loss': 1.713481068611145, 'test/bleu': 24.99513706270935, 'test/num_examples': 3003, 'score': 17666.842764616013, 'total_duration': 30240.50640106201, 'accumulated_submission_time': 17666.842764616013, 'accumulated_eval_time': 12570.26611495018, 'accumulated_logging_time': 0.4349939823150635, 'global_step': 50409, 'preemption_count': 0}), (52818, {'train/accuracy': 0.621373176574707, 'train/loss': 1.909628987312317, 'train/bleu': 29.404804899780526, 'validation/accuracy': 0.64076828956604, 'validation/loss': 1.761560082435608, 'validation/bleu': 26.379700089759037, 'validation/num_examples': 3000, 'test/accuracy': 0.6488587856292725, 'test/loss': 1.7041263580322266, 'test/bleu': 25.814511785082797, 'test/num_examples': 3003, 'score': 18506.974251031876, 'total_duration': 31685.050538539886, 'accumulated_submission_time': 18506.974251031876, 'accumulated_eval_time': 13174.522924900055, 'accumulated_logging_time': 0.45745110511779785, 'global_step': 52818, 'preemption_count': 0}), (55226, {'train/accuracy': 0.6187305450439453, 'train/loss': 1.9316372871398926, 'train/bleu': 29.710605439586914, 'validation/accuracy': 0.6412750482559204, 'validation/loss': 1.7546164989471436, 'validation/bleu': 26.281337249560597, 'validation/num_examples': 3000, 'test/accuracy': 0.649623453617096, 'test/loss': 1.695084571838379, 'test/bleu': 25.701607392065284, 'test/num_examples': 3003, 'score': 19346.908233880997, 'total_duration': 33057.55945301056, 'accumulated_submission_time': 19346.908233880997, 'accumulated_eval_time': 13706.940255641937, 'accumulated_logging_time': 0.4806942939758301, 'global_step': 55226, 'preemption_count': 0}), (57630, {'train/accuracy': 0.6252249479293823, 'train/loss': 1.862509846687317, 'train/bleu': 30.217214815072033, 'validation/accuracy': 0.6423380374908447, 'validation/loss': 1.7404899597167969, 'validation/bleu': 26.641212112606315, 'validation/num_examples': 3000, 'test/accuracy': 0.6520913243293762, 'test/loss': 1.6839854717254639, 'test/bleu': 26.357387294050795, 'test/num_examples': 3003, 'score': 20186.779103040695, 'total_duration': 34508.328780412674, 'accumulated_submission_time': 20186.779103040695, 'accumulated_eval_time': 14317.685172319412, 'accumulated_logging_time': 0.5038979053497314, 'global_step': 57630, 'preemption_count': 0}), (60034, {'train/accuracy': 0.623226523399353, 'train/loss': 1.89035964012146, 'train/bleu': 29.752940184660318, 'validation/accuracy': 0.6430054903030396, 'validation/loss': 1.7353914976119995, 'validation/bleu': 26.792112275242218, 'validation/num_examples': 3000, 'test/accuracy': 0.6558915376663208, 'test/loss': 1.6648130416870117, 'test/bleu': 26.092712384391096, 'test/num_examples': 3003, 'score': 21026.94348526001, 'total_duration': 35860.53970384598, 'accumulated_submission_time': 21026.94348526001, 'accumulated_eval_time': 14829.575204849243, 'accumulated_logging_time': 0.5270743370056152, 'global_step': 60034, 'preemption_count': 0}), (62435, {'train/accuracy': 0.6214150786399841, 'train/loss': 1.8964927196502686, 'train/bleu': 29.592889104671457, 'validation/accuracy': 0.6461449265480042, 'validation/loss': 1.7099868059158325, 'validation/bleu': 26.796261819351862, 'validation/num_examples': 3000, 'test/accuracy': 0.6564360857009888, 'test/loss': 1.6444545984268188, 'test/bleu': 26.18130830435449, 'test/num_examples': 3003, 'score': 21866.79092860222, 'total_duration': 37337.37766337395, 'accumulated_submission_time': 21866.79092860222, 'accumulated_eval_time': 15466.407456874847, 'accumulated_logging_time': 0.5507338047027588, 'global_step': 62435, 'preemption_count': 0}), (64837, {'train/accuracy': 0.6294022798538208, 'train/loss': 1.8417770862579346, 'train/bleu': 30.20044197552171, 'validation/accuracy': 0.646837055683136, 'validation/loss': 1.7003849744796753, 'validation/bleu': 26.915054761490847, 'validation/num_examples': 3000, 'test/accuracy': 0.6591588258743286, 'test/loss': 1.6346181631088257, 'test/bleu': 26.55129252816776, 'test/num_examples': 3003, 'score': 22706.78688764572, 'total_duration': 38827.0536634922, 'accumulated_submission_time': 22706.78688764572, 'accumulated_eval_time': 16115.925898313522, 'accumulated_logging_time': 0.5742483139038086, 'global_step': 64837, 'preemption_count': 0}), (67239, {'train/accuracy': 0.6307945251464844, 'train/loss': 1.847119688987732, 'train/bleu': 30.530028128689345, 'validation/accuracy': 0.649642825126648, 'validation/loss': 1.6922351121902466, 'validation/bleu': 27.193436361678348, 'validation/num_examples': 3000, 'test/accuracy': 0.6626694798469543, 'test/loss': 1.6143981218338013, 'test/bleu': 27.00615756210425, 'test/num_examples': 3003, 'score': 23546.788079977036, 'total_duration': 40193.716821432114, 'accumulated_submission_time': 23546.788079977036, 'accumulated_eval_time': 16642.429622888565, 'accumulated_logging_time': 0.5989913940429688, 'global_step': 67239, 'preemption_count': 0}), (69641, {'train/accuracy': 0.6417231559753418, 'train/loss': 1.7461527585983276, 'train/bleu': 31.380935208801496, 'validation/accuracy': 0.6530171036720276, 'validation/loss': 1.6630244255065918, 'validation/bleu': 27.659766721102173, 'validation/num_examples': 3000, 'test/accuracy': 0.6612791419029236, 'test/loss': 1.6080572605133057, 'test/bleu': 26.98846945057267, 'test/num_examples': 3003, 'score': 24386.636229276657, 'total_duration': 41591.9735121727, 'accumulated_submission_time': 24386.636229276657, 'accumulated_eval_time': 17200.6773416996, 'accumulated_logging_time': 0.6243457794189453, 'global_step': 69641, 'preemption_count': 0}), (72048, {'train/accuracy': 0.6350510716438293, 'train/loss': 1.8069862127304077, 'train/bleu': 30.674610005409807, 'validation/accuracy': 0.6550441384315491, 'validation/loss': 1.654374361038208, 'validation/bleu': 27.568141518094126, 'validation/num_examples': 3000, 'test/accuracy': 0.6680338382720947, 'test/loss': 1.5831362009048462, 'test/bleu': 27.269447461818483, 'test/num_examples': 3003, 'score': 25226.513605117798, 'total_duration': 43009.87966322899, 'accumulated_submission_time': 25226.513605117798, 'accumulated_eval_time': 17778.546284914017, 'accumulated_logging_time': 0.6499409675598145, 'global_step': 72048, 'preemption_count': 0}), (74447, {'train/accuracy': 0.6347497701644897, 'train/loss': 1.8110401630401611, 'train/bleu': 30.62346749624611, 'validation/accuracy': 0.6573060154914856, 'validation/loss': 1.6354496479034424, 'validation/bleu': 27.670441213807376, 'validation/num_examples': 3000, 'test/accuracy': 0.6668752431869507, 'test/loss': 1.570426344871521, 'test/bleu': 27.148600734014202, 'test/num_examples': 3003, 'score': 26066.559239387512, 'total_duration': 44411.85153913498, 'accumulated_submission_time': 26066.559239387512, 'accumulated_eval_time': 18340.23524069786, 'accumulated_logging_time': 0.7510364055633545, 'global_step': 74447, 'preemption_count': 0}), (76842, {'train/accuracy': 0.643849790096283, 'train/loss': 1.7367849349975586, 'train/bleu': 31.4240551171181, 'validation/accuracy': 0.6587027311325073, 'validation/loss': 1.6264591217041016, 'validation/bleu': 27.717167307785182, 'validation/num_examples': 3000, 'test/accuracy': 0.6707450151443481, 'test/loss': 1.5569945573806763, 'test/bleu': 27.190997661693533, 'test/num_examples': 3003, 'score': 26906.62972521782, 'total_duration': 45890.177701711655, 'accumulated_submission_time': 26906.62972521782, 'accumulated_eval_time': 18978.33191728592, 'accumulated_logging_time': 0.7785978317260742, 'global_step': 76842, 'preemption_count': 0}), (79235, {'train/accuracy': 0.6377763152122498, 'train/loss': 1.7833001613616943, 'train/bleu': 31.18229808110785, 'validation/accuracy': 0.6613106727600098, 'validation/loss': 1.6094623804092407, 'validation/bleu': 28.19021724030307, 'validation/num_examples': 3000, 'test/accuracy': 0.6720542311668396, 'test/loss': 1.5351009368896484, 'test/bleu': 27.277182572121234, 'test/num_examples': 3003, 'score': 27746.70812368393, 'total_duration': 47305.628126859665, 'accumulated_submission_time': 27746.70812368393, 'accumulated_eval_time': 19553.54093313217, 'accumulated_logging_time': 0.8071153163909912, 'global_step': 79235, 'preemption_count': 0}), (81628, {'train/accuracy': 0.6835156083106995, 'train/loss': 1.4777052402496338, 'train/bleu': 34.6312223314649, 'validation/accuracy': 0.6645613312721252, 'validation/loss': 1.5900806188583374, 'validation/bleu': 28.092138402436095, 'validation/num_examples': 3000, 'test/accuracy': 0.6757618188858032, 'test/loss': 1.5181536674499512, 'test/bleu': 27.73714469340843, 'test/num_examples': 3003, 'score': 28586.574621915817, 'total_duration': 48795.823291778564, 'accumulated_submission_time': 28586.574621915817, 'accumulated_eval_time': 20203.707585334778, 'accumulated_logging_time': 0.833693265914917, 'global_step': 81628, 'preemption_count': 0}), (84025, {'train/accuracy': 0.6493034362792969, 'train/loss': 1.7085705995559692, 'train/bleu': 31.45303599077604, 'validation/accuracy': 0.668615460395813, 'validation/loss': 1.5697382688522339, 'validation/bleu': 28.70318347292197, 'validation/num_examples': 3000, 'test/accuracy': 0.6783918738365173, 'test/loss': 1.4986896514892578, 'test/bleu': 28.279428678066182, 'test/num_examples': 3003, 'score': 29426.739711523056, 'total_duration': 50201.59497499466, 'accumulated_submission_time': 29426.739711523056, 'accumulated_eval_time': 20769.151468753815, 'accumulated_logging_time': 0.8610711097717285, 'global_step': 84025, 'preemption_count': 0}), (86428, {'train/accuracy': 0.6513230800628662, 'train/loss': 1.6993632316589355, 'train/bleu': 31.57597122100193, 'validation/accuracy': 0.6699997782707214, 'validation/loss': 1.5602314472198486, 'validation/bleu': 29.0313639459778, 'validation/num_examples': 3000, 'test/accuracy': 0.6820183396339417, 'test/loss': 1.4871288537979126, 'test/bleu': 28.512797079562908, 'test/num_examples': 3003, 'score': 30266.774632692337, 'total_duration': 51720.11083507538, 'accumulated_submission_time': 30266.774632692337, 'accumulated_eval_time': 21447.477617263794, 'accumulated_logging_time': 0.886648416519165, 'global_step': 86428, 'preemption_count': 0}), (88833, {'train/accuracy': 0.6636746525764465, 'train/loss': 1.6096980571746826, 'train/bleu': 32.65140834115123, 'validation/accuracy': 0.6717919707298279, 'validation/loss': 1.5396769046783447, 'validation/bleu': 29.00127703822721, 'validation/num_examples': 3000, 'test/accuracy': 0.6835013628005981, 'test/loss': 1.4725807905197144, 'test/bleu': 28.460627210430083, 'test/num_examples': 3003, 'score': 31106.910621643066, 'total_duration': 53170.86822557449, 'accumulated_submission_time': 31106.910621643066, 'accumulated_eval_time': 22057.94075703621, 'accumulated_logging_time': 0.9131319522857666, 'global_step': 88833, 'preemption_count': 0}), (91242, {'train/accuracy': 0.6555089950561523, 'train/loss': 1.6580567359924316, 'train/bleu': 32.23229413014045, 'validation/accuracy': 0.6740291118621826, 'validation/loss': 1.5280110836029053, 'validation/bleu': 28.773842061603613, 'validation/num_examples': 3000, 'test/accuracy': 0.6861545443534851, 'test/loss': 1.4497209787368774, 'test/bleu': 28.794300777932847, 'test/num_examples': 3003, 'score': 31946.811557531357, 'total_duration': 54629.96982336044, 'accumulated_submission_time': 31946.811557531357, 'accumulated_eval_time': 22676.977138757706, 'accumulated_logging_time': 0.942638635635376, 'global_step': 91242, 'preemption_count': 0}), (93651, {'train/accuracy': 0.6547471284866333, 'train/loss': 1.6660195589065552, 'train/bleu': 33.03640261403633, 'validation/accuracy': 0.6758584380149841, 'validation/loss': 1.5119266510009766, 'validation/bleu': 29.244105561495573, 'validation/num_examples': 3000, 'test/accuracy': 0.6886803507804871, 'test/loss': 1.4332627058029175, 'test/bleu': 29.1400584975005, 'test/num_examples': 3003, 'score': 32786.76872754097, 'total_duration': 56093.60956931114, 'accumulated_submission_time': 32786.76872754097, 'accumulated_eval_time': 23300.496270418167, 'accumulated_logging_time': 0.9697928428649902, 'global_step': 93651, 'preemption_count': 0}), (96054, {'train/accuracy': 0.6671206951141357, 'train/loss': 1.5872409343719482, 'train/bleu': 33.25061949803364, 'validation/accuracy': 0.6793316006660461, 'validation/loss': 1.498511552810669, 'validation/bleu': 29.372826734289195, 'validation/num_examples': 3000, 'test/accuracy': 0.6936044692993164, 'test/loss': 1.4130539894104004, 'test/bleu': 29.45315474812473, 'test/num_examples': 3003, 'score': 33626.75097632408, 'total_duration': 57638.328526735306, 'accumulated_submission_time': 33626.75097632408, 'accumulated_eval_time': 24005.06839466095, 'accumulated_logging_time': 1.0006675720214844, 'global_step': 96054, 'preemption_count': 0}), (98457, {'train/accuracy': 0.6665906310081482, 'train/loss': 1.5995906591415405, 'train/bleu': 33.48670436823367, 'validation/accuracy': 0.6801473498344421, 'validation/loss': 1.4855279922485352, 'validation/bleu': 29.43370871277903, 'validation/num_examples': 3000, 'test/accuracy': 0.6942300796508789, 'test/loss': 1.4011098146438599, 'test/bleu': 29.50482910751763, 'test/num_examples': 3003, 'score': 34466.80168223381, 'total_duration': 59168.92292332649, 'accumulated_submission_time': 34466.80168223381, 'accumulated_eval_time': 24695.449775218964, 'accumulated_logging_time': 1.0296571254730225, 'global_step': 98457, 'preemption_count': 0}), (100860, {'train/accuracy': 0.6865341663360596, 'train/loss': 1.4697033166885376, 'train/bleu': 34.37697609056703, 'validation/accuracy': 0.6838058829307556, 'validation/loss': 1.4683496952056885, 'validation/bleu': 29.81889250951529, 'validation/num_examples': 3000, 'test/accuracy': 0.698841392993927, 'test/loss': 1.3876018524169922, 'test/bleu': 29.560720436591545, 'test/num_examples': 3003, 'score': 35306.88824081421, 'total_duration': 60601.060000896454, 'accumulated_submission_time': 35306.88824081421, 'accumulated_eval_time': 25287.33839583397, 'accumulated_logging_time': 1.0578217506408691, 'global_step': 100860, 'preemption_count': 0}), (103262, {'train/accuracy': 0.6788262724876404, 'train/loss': 1.5172978639602661, 'train/bleu': 34.018486393462084, 'validation/accuracy': 0.687204897403717, 'validation/loss': 1.4548075199127197, 'validation/bleu': 29.973279843165173, 'validation/num_examples': 3000, 'test/accuracy': 0.7016800045967102, 'test/loss': 1.3686797618865967, 'test/bleu': 30.039921547477793, 'test/num_examples': 3003, 'score': 36146.94039392471, 'total_duration': 62034.37730574608, 'accumulated_submission_time': 36146.94039392471, 'accumulated_eval_time': 25880.441450595856, 'accumulated_logging_time': 1.0862483978271484, 'global_step': 103262, 'preemption_count': 0}), (105665, {'train/accuracy': 0.6763718724250793, 'train/loss': 1.5338882207870483, 'train/bleu': 34.12100765658176, 'validation/accuracy': 0.6881071925163269, 'validation/loss': 1.4445780515670776, 'validation/bleu': 29.97703042088864, 'validation/num_examples': 3000, 'test/accuracy': 0.7015641331672668, 'test/loss': 1.360108733177185, 'test/bleu': 29.95942457831162, 'test/num_examples': 3003, 'score': 36987.0404446125, 'total_duration': 63466.944769620895, 'accumulated_submission_time': 36987.0404446125, 'accumulated_eval_time': 26472.745608329773, 'accumulated_logging_time': 1.1144263744354248, 'global_step': 105665, 'preemption_count': 0}), (108072, {'train/accuracy': 0.6887732148170471, 'train/loss': 1.457044005393982, 'train/bleu': 35.05783441037165, 'validation/accuracy': 0.6897881627082825, 'validation/loss': 1.4340277910232544, 'validation/bleu': 30.28818746987776, 'validation/num_examples': 3000, 'test/accuracy': 0.7047619223594666, 'test/loss': 1.3452433347702026, 'test/bleu': 30.443317200022946, 'test/num_examples': 3003, 'score': 37827.09180569649, 'total_duration': 64925.97117352486, 'accumulated_submission_time': 37827.09180569649, 'accumulated_eval_time': 27091.555745124817, 'accumulated_logging_time': 1.1432020664215088, 'global_step': 108072, 'preemption_count': 0}), (110480, {'train/accuracy': 0.6907700300216675, 'train/loss': 1.455430269241333, 'train/bleu': 34.576012362174524, 'validation/accuracy': 0.6912713646888733, 'validation/loss': 1.425293207168579, 'validation/bleu': 30.387941172735605, 'validation/num_examples': 3000, 'test/accuracy': 0.7065461874008179, 'test/loss': 1.33441162109375, 'test/bleu': 30.22170547881406, 'test/num_examples': 3003, 'score': 38667.02536845207, 'total_duration': 66366.04125571251, 'accumulated_submission_time': 38667.02536845207, 'accumulated_eval_time': 27691.52865934372, 'accumulated_logging_time': 1.1720681190490723, 'global_step': 110480, 'preemption_count': 0}), (112888, {'train/accuracy': 0.6914148330688477, 'train/loss': 1.4510936737060547, 'train/bleu': 34.893560258747556, 'validation/accuracy': 0.6927669048309326, 'validation/loss': 1.4183810949325562, 'validation/bleu': 30.68908206575, 'validation/num_examples': 3000, 'test/accuracy': 0.7070212364196777, 'test/loss': 1.3276269435882568, 'test/bleu': 30.497464849337014, 'test/num_examples': 3003, 'score': 39506.88290643692, 'total_duration': 67771.2801516056, 'accumulated_submission_time': 39506.88290643692, 'accumulated_eval_time': 28256.744346618652, 'accumulated_logging_time': 1.2026426792144775, 'global_step': 112888, 'preemption_count': 0}), (115290, {'train/accuracy': 0.6954032778739929, 'train/loss': 1.4250965118408203, 'train/bleu': 35.681854361707195, 'validation/accuracy': 0.6933231353759766, 'validation/loss': 1.4148573875427246, 'validation/bleu': 30.690845862097188, 'validation/num_examples': 3000, 'test/accuracy': 0.7082145810127258, 'test/loss': 1.3245909214019775, 'test/bleu': 30.328584945674155, 'test/num_examples': 3003, 'score': 40346.73822975159, 'total_duration': 69207.99819898605, 'accumulated_submission_time': 40346.73822975159, 'accumulated_eval_time': 28853.44492459297, 'accumulated_logging_time': 1.2314317226409912, 'global_step': 115290, 'preemption_count': 0}), (117693, {'train/accuracy': 0.6958362460136414, 'train/loss': 1.4230217933654785, 'train/bleu': 35.119542558351824, 'validation/accuracy': 0.6937680840492249, 'validation/loss': 1.414509654045105, 'validation/bleu': 30.667707931823198, 'validation/num_examples': 3000, 'test/accuracy': 0.7088286280632019, 'test/loss': 1.321641445159912, 'test/bleu': 30.554928100510434, 'test/num_examples': 3003, 'score': 41186.87511754036, 'total_duration': 70627.8711707592, 'accumulated_submission_time': 41186.87511754036, 'accumulated_eval_time': 29433.022482395172, 'accumulated_logging_time': 1.2613105773925781, 'global_step': 117693, 'preemption_count': 0}), (120095, {'train/accuracy': 0.6928597688674927, 'train/loss': 1.4398102760314941, 'train/bleu': 35.55839829008526, 'validation/accuracy': 0.6939781904220581, 'validation/loss': 1.413612961769104, 'validation/bleu': 30.751387102081758, 'validation/num_examples': 3000, 'test/accuracy': 0.7088518142700195, 'test/loss': 1.3207508325576782, 'test/bleu': 30.560136961566112, 'test/num_examples': 3003, 'score': 42026.82373261452, 'total_duration': 72036.74474716187, 'accumulated_submission_time': 42026.82373261452, 'accumulated_eval_time': 30001.786912441254, 'accumulated_logging_time': 1.2909040451049805, 'global_step': 120095, 'preemption_count': 0}), (122497, {'train/accuracy': 0.6967914700508118, 'train/loss': 1.4181947708129883, 'train/bleu': 35.15719713891962, 'validation/accuracy': 0.6939781904220581, 'validation/loss': 1.413612961769104, 'validation/bleu': 30.751387102081758, 'validation/num_examples': 3000, 'test/accuracy': 0.7088518142700195, 'test/loss': 1.3207508325576782, 'test/bleu': 30.560136961566112, 'test/num_examples': 3003, 'score': 42866.767013549805, 'total_duration': 73457.24822068214, 'accumulated_submission_time': 42866.767013549805, 'accumulated_eval_time': 30582.185494422913, 'accumulated_logging_time': 1.3216969966888428, 'global_step': 122497, 'preemption_count': 0})], 'global_step': 124899}
I0306 15:52:18.614987 140248407626944 submission_runner.py:649] Timing: 43706.661986112595
I0306 15:52:18.615030 140248407626944 submission_runner.py:651] Total number of evals: 52
I0306 15:52:18.615063 140248407626944 submission_runner.py:652] ====================
I0306 15:52:18.615210 140248407626944 submission_runner.py:750] Final wmt score: 3
