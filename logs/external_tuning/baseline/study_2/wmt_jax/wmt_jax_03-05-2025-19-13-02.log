python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-663393617 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-13-02.log
2025-03-05 19:13:03.606702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201983.629079       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201983.635980       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:13:10.466695 139852506842304 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax.
I0305 19:13:11.443870 139852506842304 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:13:11.446964 139852506842304 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:13:11.448647 139852506842304 submission_runner.py:606] Using RNG seed -663393617
I0305 19:13:12.061324 139852506842304 submission_runner.py:615] --- Tuning run 1/5 ---
I0305 19:13:12.061506 139852506842304 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_1.
I0305 19:13:12.061696 139852506842304 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_1/hparams.json.
I0305 19:13:12.298190 139852506842304 submission_runner.py:218] Initializing dataset.
I0305 19:13:12.479605 139852506842304 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:12.510603 139852506842304 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:12.586220 139852506842304 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:13.973865 139852506842304 submission_runner.py:229] Initializing model.
I0305 19:13:55.786754 139852506842304 submission_runner.py:272] Initializing optimizer.
I0305 19:13:56.666150 139852506842304 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:56.666414 139852506842304 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:56.667435 139852506842304 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_1 with prefix checkpoint_
I0305 19:13:56.667541 139852506842304 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_1/meta_data_0.json.
I0305 19:13:56.667759 139852506842304 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:56.667812 139852506842304 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:56.858914 139852506842304 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_1/flags_0.json.
I0305 19:13:56.895277 139852506842304 submission_runner.py:337] Starting training loop.
I0305 19:14:22.842310 139716267493120 logging_writer.py:48] [0] global_step=0, grad_norm=5.433254241943359, loss=11.089016914367676
I0305 19:14:22.902584 139852506842304 spec.py:321] Evaluating on the training split.
I0305 19:14:22.904812 139852506842304 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:22.908617 139852506842304 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:14:22.942407 139852506842304 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:29.125104 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 19:19:35.032069 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 19:19:35.083353 139852506842304 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:35.090201 139852506842304 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:35.122238 139852506842304 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:40.306394 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 19:24:40.130352 139852506842304 spec.py:349] Evaluating on the test split.
I0305 19:24:40.132955 139852506842304 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:40.136637 139852506842304 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:40.169912 139852506842304 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:42.961404 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 19:29:42.678854 139852506842304 submission_runner.py:469] Time since start: 945.78s, 	Step: 1, 	{'train/accuracy': 0.0007001033518463373, 'train/loss': 11.072443962097168, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.089398384094238, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.093862533569336, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.007185459136963, 'total_duration': 945.7834956645966, 'accumulated_submission_time': 26.007185459136963, 'accumulated_eval_time': 919.7761988639832, 'accumulated_logging_time': 0}
I0305 19:29:42.686780 139709154051840 logging_writer.py:48] [1] accumulated_eval_time=919.776, accumulated_logging_time=0, accumulated_submission_time=26.0072, global_step=1, preemption_count=0, score=26.0072, test/accuracy=0.000718341, test/bleu=0, test/loss=11.0939, test/num_examples=3003, total_duration=945.783, train/accuracy=0.000700103, train/bleu=0, train/loss=11.0724, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.0894, validation/num_examples=3000
I0305 19:30:17.483016 139709145659136 logging_writer.py:48] [100] global_step=100, grad_norm=0.35970228910446167, loss=8.913162231445312
I0305 19:30:52.168327 139709154051840 logging_writer.py:48] [200] global_step=200, grad_norm=0.1630987972021103, loss=8.564688682556152
I0305 19:31:26.923119 139709145659136 logging_writer.py:48] [300] global_step=300, grad_norm=0.19120700657367706, loss=8.296892166137695
I0305 19:32:01.698031 139709154051840 logging_writer.py:48] [400] global_step=400, grad_norm=0.27128732204437256, loss=7.930752277374268
I0305 19:32:36.510580 139709145659136 logging_writer.py:48] [500] global_step=500, grad_norm=0.3792567551136017, loss=7.584674835205078
I0305 19:33:11.340440 139709154051840 logging_writer.py:48] [600] global_step=600, grad_norm=0.47656702995300293, loss=7.352880477905273
I0305 19:33:46.204736 139709145659136 logging_writer.py:48] [700] global_step=700, grad_norm=0.5787357687950134, loss=7.090738296508789
I0305 19:34:21.052567 139709154051840 logging_writer.py:48] [800] global_step=800, grad_norm=1.1713463068008423, loss=6.871852397918701
I0305 19:34:55.913033 139709145659136 logging_writer.py:48] [900] global_step=900, grad_norm=0.6559244990348816, loss=6.690353870391846
I0305 19:35:30.790395 139709154051840 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5827972292900085, loss=6.484797477722168
I0305 19:36:05.647890 139709145659136 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5355781316757202, loss=6.260437965393066
I0305 19:36:40.516270 139709154051840 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5985299944877625, loss=6.093109607696533
I0305 19:37:15.391341 139709145659136 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.9129658341407776, loss=6.006778240203857
I0305 19:37:50.245732 139709154051840 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6553893685340881, loss=5.865147113800049
I0305 19:38:25.107359 139709145659136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.582734227180481, loss=5.727118015289307
I0305 19:38:59.968147 139709154051840 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6030863523483276, loss=5.5525803565979
I0305 19:39:34.860997 139709145659136 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.594636857509613, loss=5.43776273727417
I0305 19:40:09.775012 139709154051840 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5906246304512024, loss=5.424025535583496
I0305 19:40:44.642290 139709145659136 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5798648595809937, loss=5.259311199188232
I0305 19:41:19.524500 139709154051840 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9084556698799133, loss=5.16171932220459
I0305 19:41:54.402039 139709145659136 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9091558456420898, loss=5.033843517303467
I0305 19:42:29.281785 139709154051840 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0223917961120605, loss=4.9370293617248535
I0305 19:43:04.188793 139709145659136 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8160740733146667, loss=4.849054336547852
I0305 19:43:39.087122 139709154051840 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8333131670951843, loss=4.867983341217041
I0305 19:43:42.937639 139852506842304 spec.py:321] Evaluating on the training split.
I0305 19:43:45.596384 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 19:47:17.981540 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 19:47:20.623089 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 19:50:30.644690 139852506842304 spec.py:349] Evaluating on the test split.
I0305 19:50:33.279735 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 19:53:38.220413 139852506842304 submission_runner.py:469] Time since start: 2381.33s, 	Step: 2412, 	{'train/accuracy': 0.42491084337234497, 'train/loss': 3.8637702465057373, 'train/bleu': 15.61919232914508, 'validation/accuracy': 0.4137764871120453, 'validation/loss': 3.9510624408721924, 'validation/bleu': 10.762993728862007, 'validation/num_examples': 3000, 'test/accuracy': 0.400359183549881, 'test/loss': 4.126694679260254, 'test/bleu': 9.021694400708748, 'test/num_examples': 3003, 'score': 866.0806493759155, 'total_duration': 2381.325071334839, 'accumulated_submission_time': 866.0806493759155, 'accumulated_eval_time': 1515.0589277744293, 'accumulated_logging_time': 0.01842665672302246}
I0305 19:53:38.229477 139708944332544 logging_writer.py:48] [2412] accumulated_eval_time=1515.06, accumulated_logging_time=0.0184267, accumulated_submission_time=866.081, global_step=2412, preemption_count=0, score=866.081, test/accuracy=0.400359, test/bleu=9.02169, test/loss=4.12669, test/num_examples=3003, total_duration=2381.33, train/accuracy=0.424911, train/bleu=15.6192, train/loss=3.86377, validation/accuracy=0.413776, validation/bleu=10.763, validation/loss=3.95106, validation/num_examples=3000
I0305 19:54:09.146866 139708935939840 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6225273013114929, loss=4.748279571533203
I0305 19:54:43.986876 139708944332544 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.480366587638855, loss=4.64115047454834
I0305 19:55:18.858487 139708935939840 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.006533145904541, loss=4.53400182723999
I0305 19:55:53.733358 139708944332544 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7304180264472961, loss=4.566551208496094
I0305 19:56:28.610798 139708935939840 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.8559765815734863, loss=4.389969348907471
I0305 19:57:03.504976 139708944332544 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8063653707504272, loss=4.406327724456787
I0305 19:57:38.431597 139708935939840 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8390734195709229, loss=4.2527265548706055
I0305 19:58:13.315964 139708944332544 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6348618268966675, loss=4.2245049476623535
I0305 19:58:48.217386 139708935939840 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6455333828926086, loss=4.195932388305664
I0305 19:59:23.084292 139708944332544 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6303609013557434, loss=4.108180046081543
I0305 19:59:58.010573 139708935939840 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6974364519119263, loss=4.125591278076172
I0305 20:00:32.921615 139708944332544 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5745992660522461, loss=4.019906997680664
I0305 20:01:07.798614 139708935939840 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.697944700717926, loss=3.988227128982544
I0305 20:01:42.700800 139708944332544 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7341029047966003, loss=4.020081520080566
I0305 20:02:17.598729 139708935939840 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6194031238555908, loss=3.9138498306274414
I0305 20:02:52.530305 139708944332544 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5899437069892883, loss=3.930199384689331
I0305 20:03:27.442924 139708935939840 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5501380562782288, loss=3.8676397800445557
I0305 20:04:02.349343 139708944332544 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5942625403404236, loss=3.9428770542144775
I0305 20:04:37.247852 139708935939840 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5941837430000305, loss=3.886653184890747
I0305 20:05:12.163608 139708944332544 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5521784424781799, loss=3.780045509338379
I0305 20:05:47.063928 139708935939840 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5801642537117004, loss=3.8541009426116943
I0305 20:06:21.963332 139708944332544 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5503385066986084, loss=3.740445375442505
I0305 20:06:56.885489 139708935939840 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5157902240753174, loss=3.7606775760650635
I0305 20:07:31.827306 139708944332544 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6012564897537231, loss=3.693239688873291
I0305 20:07:38.454339 139852506842304 spec.py:321] Evaluating on the training split.
I0305 20:07:41.110270 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:10:27.205711 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 20:10:29.844141 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:13:13.328537 139852506842304 spec.py:349] Evaluating on the test split.
I0305 20:13:15.968584 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:15:46.547611 139852506842304 submission_runner.py:469] Time since start: 3709.65s, 	Step: 4820, 	{'train/accuracy': 0.5459850430488586, 'train/loss': 2.7041351795196533, 'train/bleu': 24.537025090133007, 'validation/accuracy': 0.5497985482215881, 'validation/loss': 2.6535212993621826, 'validation/bleu': 20.789592351663433, 'validation/num_examples': 3000, 'test/accuracy': 0.5503765344619751, 'test/loss': 2.6854612827301025, 'test/bleu': 19.508493993622434, 'test/num_examples': 3003, 'score': 1706.1331958770752, 'total_duration': 3709.652266740799, 'accumulated_submission_time': 1706.1331958770752, 'accumulated_eval_time': 2003.1521430015564, 'accumulated_logging_time': 0.03605842590332031}
I0305 20:15:46.556642 139708935939840 logging_writer.py:48] [4820] accumulated_eval_time=2003.15, accumulated_logging_time=0.0360584, accumulated_submission_time=1706.13, global_step=4820, preemption_count=0, score=1706.13, test/accuracy=0.550377, test/bleu=19.5085, test/loss=2.68546, test/num_examples=3003, total_duration=3709.65, train/accuracy=0.545985, train/bleu=24.537, train/loss=2.70414, validation/accuracy=0.549799, validation/bleu=20.7896, validation/loss=2.65352, validation/num_examples=3000
I0305 20:16:14.725707 139708944332544 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5524070262908936, loss=3.7401318550109863
I0305 20:16:49.521205 139708935939840 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5397441387176514, loss=3.670475721359253
I0305 20:17:24.395972 139708944332544 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7345094084739685, loss=3.6580259799957275
I0305 20:17:59.297542 139708935939840 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.4438818693161011, loss=3.6334898471832275
I0305 20:18:34.186957 139708944332544 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.4766848087310791, loss=3.726593017578125
I0305 20:19:09.046560 139708935939840 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.45095211267471313, loss=3.613217830657959
I0305 20:19:43.928672 139708944332544 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5511067509651184, loss=3.680271625518799
I0305 20:20:18.833013 139708935939840 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.4512489438056946, loss=3.6045637130737305
I0305 20:20:53.727863 139708944332544 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.5152019262313843, loss=3.533064365386963
I0305 20:21:28.632881 139708935939840 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.49622493982315063, loss=3.555025339126587
I0305 20:22:03.520916 139708944332544 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4725337028503418, loss=3.5141472816467285
I0305 20:22:38.413866 139708935939840 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4023038148880005, loss=3.461676836013794
I0305 20:23:13.319449 139708944332544 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4745039939880371, loss=3.5230205059051514
I0305 20:23:48.195731 139708935939840 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.49745410680770874, loss=3.5743110179901123
I0305 20:24:23.096843 139708776544000 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4466586709022522, loss=3.6012940406799316
I0305 20:24:57.967970 139708768151296 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.430327832698822, loss=3.519501209259033
I0305 20:25:32.851132 139708776544000 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.48809295892715454, loss=3.4914705753326416
I0305 20:26:07.704021 139708768151296 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4745369255542755, loss=3.516721248626709
I0305 20:26:42.581246 139708776544000 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4204328656196594, loss=3.5129923820495605
I0305 20:27:17.478534 139708768151296 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4045640826225281, loss=3.4500677585601807
I0305 20:27:52.345759 139708776544000 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.41773903369903564, loss=3.5431134700775146
I0305 20:28:27.218400 139708768151296 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.36634182929992676, loss=3.4087939262390137
I0305 20:29:02.122896 139708776544000 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4092232584953308, loss=3.4531362056732178
I0305 20:29:36.984870 139708768151296 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.4085257351398468, loss=3.324448347091675
I0305 20:29:46.760048 139852506842304 spec.py:321] Evaluating on the training split.
I0305 20:29:49.406344 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:32:25.471542 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 20:32:28.109692 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:35:09.070951 139852506842304 spec.py:349] Evaluating on the test split.
I0305 20:35:11.714408 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:37:42.985027 139852506842304 submission_runner.py:469] Time since start: 5026.09s, 	Step: 7229, 	{'train/accuracy': 0.5858600735664368, 'train/loss': 2.3224937915802, 'train/bleu': 27.6802262408556, 'validation/accuracy': 0.5911799073219299, 'validation/loss': 2.2754979133605957, 'validation/bleu': 23.32863177280011, 'validation/num_examples': 3000, 'test/accuracy': 0.5931178331375122, 'test/loss': 2.268697500228882, 'test/bleu': 22.097597695085067, 'test/num_examples': 3003, 'score': 2546.1659734249115, 'total_duration': 5026.089684963226, 'accumulated_submission_time': 2546.1659734249115, 'accumulated_eval_time': 2479.3770694732666, 'accumulated_logging_time': 0.0534365177154541}
I0305 20:37:42.994227 139708776544000 logging_writer.py:48] [7229] accumulated_eval_time=2479.38, accumulated_logging_time=0.0534365, accumulated_submission_time=2546.17, global_step=7229, preemption_count=0, score=2546.17, test/accuracy=0.593118, test/bleu=22.0976, test/loss=2.2687, test/num_examples=3003, total_duration=5026.09, train/accuracy=0.58586, train/bleu=27.6802, train/loss=2.32249, validation/accuracy=0.59118, validation/bleu=23.3286, validation/loss=2.2755, validation/num_examples=3000
I0305 20:38:08.008036 139708768151296 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.439005047082901, loss=3.3959555625915527
I0305 20:38:42.768490 139708776544000 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.38505831360816956, loss=3.52604079246521
I0305 20:39:17.589181 139708768151296 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4177456796169281, loss=3.3850693702697754
I0305 20:39:52.452426 139708776544000 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.40126293897628784, loss=3.477825880050659
I0305 20:40:27.309310 139708768151296 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.31990623474121094, loss=3.355193614959717
I0305 20:41:02.174360 139708776544000 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.37684664130210876, loss=3.415907382965088
I0305 20:41:37.049870 139708768151296 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.3752114474773407, loss=3.4385757446289062
I0305 20:42:11.912192 139708776544000 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3515909016132355, loss=3.332973003387451
I0305 20:42:46.782008 139708768151296 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.33063021302223206, loss=3.4627485275268555
I0305 20:43:21.657381 139708776544000 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.33187487721443176, loss=3.4046082496643066
I0305 20:43:56.510511 139708768151296 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3837997615337372, loss=3.3376424312591553
I0305 20:44:31.376258 139708776544000 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3481845557689667, loss=3.3831634521484375
I0305 20:45:06.228513 139708768151296 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.34090426564216614, loss=3.3286986351013184
I0305 20:45:41.082771 139708776544000 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3127562701702118, loss=3.2829177379608154
I0305 20:46:15.936561 139708768151296 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3563091456890106, loss=3.3663675785064697
I0305 20:46:50.795445 139708776544000 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.34666427969932556, loss=3.365790605545044
I0305 20:47:25.670465 139708768151296 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3508322238922119, loss=3.2984554767608643
I0305 20:48:00.535804 139708776544000 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3146514594554901, loss=3.391014337539673
I0305 20:48:35.422098 139708768151296 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.33982425928115845, loss=3.2967379093170166
I0305 20:49:10.307135 139708776544000 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.2990380525588989, loss=3.3278768062591553
I0305 20:49:45.174353 139708768151296 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.2868722379207611, loss=3.330490827560425
I0305 20:50:20.018392 139708776544000 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.29468193650245667, loss=3.351050615310669
I0305 20:50:54.897141 139708768151296 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.34280019998550415, loss=3.2757956981658936
I0305 20:51:29.739212 139708776544000 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.28343120217323303, loss=3.3259294033050537
I0305 20:51:42.987145 139852506842304 spec.py:321] Evaluating on the training split.
I0305 20:51:45.653676 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:54:38.623758 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 20:54:41.255486 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:57:18.300426 139852506842304 spec.py:349] Evaluating on the test split.
I0305 20:57:20.949399 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 20:59:45.192589 139852506842304 submission_runner.py:469] Time since start: 6348.30s, 	Step: 9639, 	{'train/accuracy': 0.595770001411438, 'train/loss': 2.2054076194763184, 'train/bleu': 28.391961989913796, 'validation/accuracy': 0.608953595161438, 'validation/loss': 2.1008777618408203, 'validation/bleu': 24.876423012297934, 'validation/num_examples': 3000, 'test/accuracy': 0.6161510944366455, 'test/loss': 2.0748791694641113, 'test/bleu': 23.814821639047352, 'test/num_examples': 3003, 'score': 3385.9925627708435, 'total_duration': 6348.297238826752, 'accumulated_submission_time': 3385.9925627708435, 'accumulated_eval_time': 2961.5824570655823, 'accumulated_logging_time': 0.07094693183898926}
I0305 20:59:45.202291 139708768151296 logging_writer.py:48] [9639] accumulated_eval_time=2961.58, accumulated_logging_time=0.0709469, accumulated_submission_time=3385.99, global_step=9639, preemption_count=0, score=3385.99, test/accuracy=0.616151, test/bleu=23.8148, test/loss=2.07488, test/num_examples=3003, total_duration=6348.3, train/accuracy=0.59577, train/bleu=28.392, train/loss=2.20541, validation/accuracy=0.608954, validation/bleu=24.8764, validation/loss=2.10088, validation/num_examples=3000
I0305 21:00:06.722707 139708776544000 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.27059406042099, loss=3.235731363296509
I0305 21:00:41.468812 139708768151296 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.32472914457321167, loss=3.276540517807007
I0305 21:01:16.262165 139708776544000 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.2743571400642395, loss=3.260592460632324
I0305 21:01:51.067889 139708768151296 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.29652631282806396, loss=3.239326238632202
I0305 21:02:25.910419 139708776544000 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2605210542678833, loss=3.3137996196746826
I0305 21:03:00.735590 139708768151296 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.29030266404151917, loss=3.213999032974243
I0305 21:03:35.577190 139708776544000 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.283451646566391, loss=3.298313856124878
I0305 21:04:10.380332 139708768151296 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.33995476365089417, loss=3.29769229888916
I0305 21:04:45.235558 139708776544000 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2712898552417755, loss=3.288607358932495
I0305 21:05:20.097918 139708768151296 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.2634112238883972, loss=3.2951040267944336
I0305 21:05:54.946451 139708776544000 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.24911727011203766, loss=3.254828929901123
I0305 21:06:29.802814 139708768151296 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2835375964641571, loss=3.236159324645996
I0305 21:07:04.642287 139708776544000 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.2723182439804077, loss=3.2272636890411377
I0305 21:07:39.478239 139708768151296 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.25393959879875183, loss=3.2768664360046387
I0305 21:08:14.330017 139708776544000 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2664223909378052, loss=3.2908103466033936
I0305 21:08:49.149262 139708768151296 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.2837105095386505, loss=3.2429616451263428
I0305 21:09:24.012771 139708776544000 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.24704205989837646, loss=3.2345499992370605
I0305 21:09:58.853633 139708768151296 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.25658902525901794, loss=3.1776835918426514
I0305 21:10:33.675334 139708776544000 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2381569743156433, loss=3.151265859603882
I0305 21:11:08.507990 139708768151296 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.23766764998435974, loss=3.141540288925171
I0305 21:11:43.350618 139708776544000 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2655874490737915, loss=3.25224232673645
I0305 21:12:18.185517 139708768151296 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.24769818782806396, loss=3.2363388538360596
I0305 21:12:53.022044 139708776544000 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.27656036615371704, loss=3.1723473072052
I0305 21:13:27.866612 139708768151296 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.24072232842445374, loss=3.2502753734588623
I0305 21:13:45.286424 139852506842304 spec.py:321] Evaluating on the training split.
I0305 21:13:47.933682 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 21:16:47.233367 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 21:16:49.859223 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 21:19:37.845188 139852506842304 spec.py:349] Evaluating on the test split.
I0305 21:19:40.480586 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 21:21:59.289584 139852506842304 submission_runner.py:469] Time since start: 7682.39s, 	Step: 12051, 	{'train/accuracy': 0.6027227640151978, 'train/loss': 2.1442761421203613, 'train/bleu': 29.44077606983374, 'validation/accuracy': 0.6228091716766357, 'validation/loss': 1.9855448007583618, 'validation/bleu': 25.74414990782134, 'validation/num_examples': 3000, 'test/accuracy': 0.6294288039207458, 'test/loss': 1.9414881467819214, 'test/bleu': 24.94115133871372, 'test/num_examples': 3003, 'score': 4225.918093442917, 'total_duration': 7682.394242525101, 'accumulated_submission_time': 4225.918093442917, 'accumulated_eval_time': 3455.5855684280396, 'accumulated_logging_time': 0.08934473991394043}
I0305 21:21:59.299430 139708776544000 logging_writer.py:48] [12051] accumulated_eval_time=3455.59, accumulated_logging_time=0.0893447, accumulated_submission_time=4225.92, global_step=12051, preemption_count=0, score=4225.92, test/accuracy=0.629429, test/bleu=24.9412, test/loss=1.94149, test/num_examples=3003, total_duration=7682.39, train/accuracy=0.602723, train/bleu=29.4408, train/loss=2.14428, validation/accuracy=0.622809, validation/bleu=25.7441, validation/loss=1.98554, validation/num_examples=3000
I0305 21:22:16.617134 139708768151296 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.2446761280298233, loss=3.1722166538238525
I0305 21:22:51.306752 139708776544000 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.26071903109550476, loss=3.19606876373291
I0305 21:23:26.095462 139708768151296 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.22795546054840088, loss=3.1597208976745605
I0305 21:24:00.901745 139708776544000 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.244897723197937, loss=3.159167528152466
I0305 21:24:35.750243 139708768151296 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.23182153701782227, loss=3.16878604888916
I0305 21:25:10.516883 139708776544000 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.26234135031700134, loss=3.158705472946167
I0305 21:25:45.264209 139708768151296 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.22375808656215668, loss=3.154081344604492
I0305 21:26:20.008251 139708776544000 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.23698076605796814, loss=3.1202378273010254
I0305 21:26:54.747865 139708768151296 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.25813665986061096, loss=3.251418113708496
I0305 21:27:29.490550 139708776544000 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.30074551701545715, loss=3.180124044418335
I0305 21:28:04.225045 139708768151296 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.23138752579689026, loss=3.1284642219543457
I0305 21:28:38.982917 139708776544000 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.24320532381534576, loss=3.1604583263397217
I0305 21:29:13.739682 139708768151296 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.27313873171806335, loss=3.1831960678100586
I0305 21:29:48.483828 139708776544000 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.25221309065818787, loss=3.1722524166107178
I0305 21:30:23.255981 139708768151296 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2970356047153473, loss=3.1883089542388916
I0305 21:30:58.014257 139708776544000 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.24841606616973877, loss=3.1507749557495117
I0305 21:31:32.780899 139708768151296 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2657814621925354, loss=3.2240049839019775
I0305 21:32:07.518660 139708776544000 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2563017010688782, loss=3.17687726020813
I0305 21:32:42.266198 139708768151296 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.24880068004131317, loss=3.191720962524414
I0305 21:33:17.025360 139708776544000 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.23730073869228363, loss=3.154592752456665
I0305 21:33:51.770727 139708768151296 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.3061276078224182, loss=3.0993030071258545
I0305 21:34:26.549886 139708776544000 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2752387523651123, loss=3.1126182079315186
I0305 21:35:01.315782 139708768151296 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.256245493888855, loss=3.191006660461426
I0305 21:35:36.062674 139708776544000 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2654823660850525, loss=3.1931731700897217
I0305 21:35:59.354749 139852506842304 spec.py:321] Evaluating on the training split.
I0305 21:36:01.999450 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 21:38:48.747945 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 21:38:51.386536 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 21:41:30.553452 139852506842304 spec.py:349] Evaluating on the test split.
I0305 21:41:33.192528 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 21:44:03.371034 139852506842304 submission_runner.py:469] Time since start: 9006.48s, 	Step: 14468, 	{'train/accuracy': 0.6183366179466248, 'train/loss': 2.016723155975342, 'train/bleu': 29.57251553557313, 'validation/accuracy': 0.6347736716270447, 'validation/loss': 1.8878432512283325, 'validation/bleu': 26.489837441616515, 'validation/num_examples': 3000, 'test/accuracy': 0.6437492966651917, 'test/loss': 1.8373864889144897, 'test/bleu': 25.770261872530057, 'test/num_examples': 3003, 'score': 5065.820095539093, 'total_duration': 9006.475695371628, 'accumulated_submission_time': 5065.820095539093, 'accumulated_eval_time': 3939.6018047332764, 'accumulated_logging_time': 0.1076514720916748}
I0305 21:44:03.381161 139708768151296 logging_writer.py:48] [14468] accumulated_eval_time=3939.6, accumulated_logging_time=0.107651, accumulated_submission_time=5065.82, global_step=14468, preemption_count=0, score=5065.82, test/accuracy=0.643749, test/bleu=25.7703, test/loss=1.83739, test/num_examples=3003, total_duration=9006.48, train/accuracy=0.618337, train/bleu=29.5725, train/loss=2.01672, validation/accuracy=0.634774, validation/bleu=26.4898, validation/loss=1.88784, validation/num_examples=3000
I0305 21:44:14.803291 139708776544000 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.28930535912513733, loss=3.1559319496154785
I0305 21:44:49.446883 139708768151296 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.29675671458244324, loss=3.0999386310577393
I0305 21:45:24.193887 139708776544000 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.26202672719955444, loss=3.1235382556915283
I0305 21:45:58.957065 139708768151296 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2694920301437378, loss=3.1249146461486816
I0305 21:46:33.720249 139708776544000 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3046327531337738, loss=3.16876220703125
I0305 21:47:08.449145 139708768151296 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.24492190778255463, loss=3.0854668617248535
I0305 21:47:43.187297 139708776544000 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2650688588619232, loss=3.0650131702423096
I0305 21:48:17.952660 139708768151296 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.35648858547210693, loss=3.12654447555542
I0305 21:48:52.705538 139708776544000 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.31072813272476196, loss=3.152935028076172
I0305 21:49:27.433810 139708768151296 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3629787266254425, loss=3.1239283084869385
I0305 21:50:02.178357 139708776544000 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.2955119013786316, loss=3.076275587081909
I0305 21:50:36.946409 139708768151296 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2843613624572754, loss=3.0928735733032227
I0305 21:51:11.697522 139708776544000 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.2577246427536011, loss=3.063357353210449
I0305 21:51:46.427733 139708768151296 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2410232126712799, loss=3.081737756729126
I0305 21:52:21.176642 139708776544000 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.28620487451553345, loss=3.1472513675689697
I0305 21:52:55.918370 139708768151296 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2889164090156555, loss=3.0939671993255615
I0305 21:53:30.673006 139708776544000 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.26020318269729614, loss=3.080411434173584
I0305 21:54:05.440244 139708768151296 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.37706390023231506, loss=3.026771306991577
I0305 21:54:40.181152 139708776544000 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.31120073795318604, loss=3.114828586578369
I0305 21:55:14.951727 139708768151296 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.27695775032043457, loss=3.1332767009735107
I0305 21:55:49.745730 139708776544000 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.2903125584125519, loss=3.081127643585205
I0305 21:56:24.503889 139708768151296 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.34635481238365173, loss=3.0768790245056152
I0305 21:56:59.265506 139708776544000 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.25529927015304565, loss=3.1233413219451904
I0305 21:57:33.990646 139708768151296 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.30082836747169495, loss=3.1296029090881348
I0305 21:58:03.529536 139852506842304 spec.py:321] Evaluating on the training split.
I0305 21:58:06.171802 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:00:53.784652 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 22:00:56.431810 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:03:27.407159 139852506842304 spec.py:349] Evaluating on the test split.
I0305 22:03:30.043373 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:05:59.509504 139852506842304 submission_runner.py:469] Time since start: 10322.61s, 	Step: 16886, 	{'train/accuracy': 0.6191582679748535, 'train/loss': 2.0026276111602783, 'train/bleu': 30.418952942439557, 'validation/accuracy': 0.6432403326034546, 'validation/loss': 1.8318157196044922, 'validation/bleu': 27.093489321429118, 'validation/num_examples': 3000, 'test/accuracy': 0.6507009863853455, 'test/loss': 1.784277081489563, 'test/bleu': 26.60813606557522, 'test/num_examples': 3003, 'score': 5905.813939809799, 'total_duration': 10322.614156723022, 'accumulated_submission_time': 5905.813939809799, 'accumulated_eval_time': 4415.581715583801, 'accumulated_logging_time': 0.12589430809020996}
I0305 22:05:59.519732 139708776544000 logging_writer.py:48] [16886] accumulated_eval_time=4415.58, accumulated_logging_time=0.125894, accumulated_submission_time=5905.81, global_step=16886, preemption_count=0, score=5905.81, test/accuracy=0.650701, test/bleu=26.6081, test/loss=1.78428, test/num_examples=3003, total_duration=10322.6, train/accuracy=0.619158, train/bleu=30.419, train/loss=2.00263, validation/accuracy=0.64324, validation/bleu=27.0935, validation/loss=1.83182, validation/num_examples=3000
I0305 22:06:04.723310 139708768151296 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.26801154017448425, loss=3.063532829284668
I0305 22:06:39.319331 139708776544000 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.42821744084358215, loss=3.1369338035583496
I0305 22:07:14.007766 139708768151296 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3572482466697693, loss=3.201382637023926
I0305 22:07:48.729578 139708776544000 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.39218640327453613, loss=3.068699598312378
I0305 22:08:23.450480 139708768151296 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.28707292675971985, loss=3.0541622638702393
I0305 22:08:58.177740 139708776544000 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.322689950466156, loss=3.048647403717041
I0305 22:09:32.917511 139708768151296 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3510623276233673, loss=3.0206165313720703
I0305 22:10:07.692550 139708776544000 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.29797378182411194, loss=3.079094648361206
I0305 22:10:42.455720 139708768151296 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.31021037697792053, loss=3.132577896118164
I0305 22:11:17.211320 139708776544000 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.31464844942092896, loss=3.021282434463501
I0305 22:11:51.966506 139708768151296 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3090085983276367, loss=3.0862233638763428
I0305 22:12:26.721946 139708776544000 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.33449751138687134, loss=3.082409143447876
I0305 22:13:01.466825 139708768151296 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3273877203464508, loss=3.09100341796875
I0305 22:13:36.223963 139708776544000 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3134090006351471, loss=2.974151372909546
I0305 22:14:10.985991 139708768151296 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.35041266679763794, loss=3.0653562545776367
I0305 22:14:45.715194 139708776544000 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.29464343190193176, loss=3.029719114303589
I0305 22:15:20.490860 139708768151296 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3054300546646118, loss=2.990067720413208
I0305 22:15:55.234136 139708776544000 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3162393271923065, loss=3.0541398525238037
I0305 22:16:29.997813 139708768151296 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3024933934211731, loss=3.0234642028808594
I0305 22:17:04.732062 139708776544000 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.31572067737579346, loss=3.0281929969787598
I0305 22:17:39.783093 139708768151296 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.39746949076652527, loss=3.0396182537078857
I0305 22:18:14.748318 139708776544000 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3263586759567261, loss=3.0555293560028076
I0305 22:18:49.705564 139708768151296 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3985517621040344, loss=3.064546823501587
I0305 22:19:24.672247 139708776544000 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.32237234711647034, loss=3.0573079586029053
I0305 22:19:59.643723 139708768151296 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.38886818289756775, loss=2.9797768592834473
I0305 22:19:59.653966 139852506842304 spec.py:321] Evaluating on the training split.
I0305 22:20:02.313417 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:24:50.307070 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 22:24:52.958269 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:29:40.661449 139852506842304 spec.py:349] Evaluating on the test split.
I0305 22:29:43.304678 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:34:30.425565 139852506842304 submission_runner.py:469] Time since start: 12033.53s, 	Step: 19301, 	{'train/accuracy': 0.6381615400314331, 'train/loss': 1.8695921897888184, 'train/bleu': 30.90519965594945, 'validation/accuracy': 0.6470966339111328, 'validation/loss': 1.797232747077942, 'validation/bleu': 26.589907519100716, 'validation/num_examples': 3000, 'test/accuracy': 0.6560769081115723, 'test/loss': 1.7378911972045898, 'test/bleu': 26.640809227861872, 'test/num_examples': 3003, 'score': 6745.794331073761, 'total_duration': 12033.53022146225, 'accumulated_submission_time': 6745.794331073761, 'accumulated_eval_time': 5286.353251218796, 'accumulated_logging_time': 0.14536213874816895}
I0305 22:34:30.438309 139708776544000 logging_writer.py:48] [19301] accumulated_eval_time=5286.35, accumulated_logging_time=0.145362, accumulated_submission_time=6745.79, global_step=19301, preemption_count=0, score=6745.79, test/accuracy=0.656077, test/bleu=26.6408, test/loss=1.73789, test/num_examples=3003, total_duration=12033.5, train/accuracy=0.638162, train/bleu=30.9052, train/loss=1.86959, validation/accuracy=0.647097, validation/bleu=26.5899, validation/loss=1.79723, validation/num_examples=3000
I0305 22:35:05.218014 139708768151296 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.29631364345550537, loss=3.085545063018799
I0305 22:35:40.120424 139708776544000 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.2932561933994293, loss=3.0234062671661377
I0305 22:36:15.060214 139708768151296 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.27901825308799744, loss=3.0072526931762695
I0305 22:36:49.990178 139708776544000 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3897765874862671, loss=3.0899274349212646
I0305 22:37:24.962238 139708768151296 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3096916079521179, loss=3.048258066177368
I0305 22:37:59.938385 139708776544000 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3078854978084564, loss=3.0068066120147705
I0305 22:38:34.900838 139708768151296 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.31878411769866943, loss=3.047929286956787
I0305 22:39:09.870095 139708776544000 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3418503999710083, loss=3.0671141147613525
I0305 22:39:44.855673 139708768151296 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.29675546288490295, loss=2.982443332672119
I0305 22:40:19.848757 139708776544000 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.4026651084423065, loss=3.045257806777954
I0305 22:40:54.825814 139708768151296 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.3256756365299225, loss=3.0258235931396484
I0305 22:41:29.778672 139708776544000 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3332289457321167, loss=3.0761492252349854
I0305 22:42:04.759401 139708768151296 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.34704676270484924, loss=3.1142914295196533
I0305 22:42:39.719120 139708776544000 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.33727142214775085, loss=2.9982388019561768
I0305 22:43:14.699246 139708768151296 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.38633671402931213, loss=3.007314443588257
I0305 22:43:49.662899 139708776544000 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.30505019426345825, loss=3.042656183242798
I0305 22:44:24.608021 139708768151296 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.381212443113327, loss=3.0754098892211914
I0305 22:44:59.580025 139708776544000 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.3747978210449219, loss=3.0068633556365967
I0305 22:45:34.546679 139708768151296 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.3550969958305359, loss=2.9836161136627197
I0305 22:46:09.485023 139708776544000 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.3548673391342163, loss=2.9921793937683105
I0305 22:46:44.428036 139708768151296 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3567177355289459, loss=3.0389199256896973
I0305 22:47:19.367017 139708776544000 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.40646490454673767, loss=3.0706593990325928
I0305 22:47:54.319658 139708768151296 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.338189959526062, loss=3.0097908973693848
I0305 22:48:29.268491 139708776544000 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.3759823143482208, loss=3.031667947769165
I0305 22:48:30.679737 139852506842304 spec.py:321] Evaluating on the training split.
I0305 22:48:33.339969 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:51:24.389949 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 22:51:27.038863 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:54:02.288780 139852506842304 spec.py:349] Evaluating on the test split.
I0305 22:54:04.938503 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 22:56:36.502558 139852506842304 submission_runner.py:469] Time since start: 13359.61s, 	Step: 21705, 	{'train/accuracy': 0.6303253173828125, 'train/loss': 1.922417402267456, 'train/bleu': 30.23589656929473, 'validation/accuracy': 0.6509900689125061, 'validation/loss': 1.7742375135421753, 'validation/bleu': 27.89847240870862, 'validation/num_examples': 3000, 'test/accuracy': 0.6612212061882019, 'test/loss': 1.7151057720184326, 'test/bleu': 26.940625120694804, 'test/num_examples': 3003, 'score': 7585.881723642349, 'total_duration': 13359.607218503952, 'accumulated_submission_time': 7585.881723642349, 'accumulated_eval_time': 5772.176019191742, 'accumulated_logging_time': 0.16701459884643555}
I0305 22:56:36.513784 139708768151296 logging_writer.py:48] [21705] accumulated_eval_time=5772.18, accumulated_logging_time=0.167015, accumulated_submission_time=7585.88, global_step=21705, preemption_count=0, score=7585.88, test/accuracy=0.661221, test/bleu=26.9406, test/loss=1.71511, test/num_examples=3003, total_duration=13359.6, train/accuracy=0.630325, train/bleu=30.2359, train/loss=1.92242, validation/accuracy=0.65099, validation/bleu=27.8985, validation/loss=1.77424, validation/num_examples=3000
I0305 22:57:09.880465 139708776544000 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.36474090814590454, loss=3.069828987121582
I0305 22:57:44.742189 139708768151296 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3873659074306488, loss=2.979341745376587
I0305 22:58:19.674905 139708776544000 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.34743160009384155, loss=2.977045774459839
I0305 22:58:54.613105 139708768151296 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.3738497793674469, loss=2.9747745990753174
I0305 22:59:29.542635 139708776544000 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.4389307498931885, loss=3.084263563156128
I0305 23:00:04.504302 139708768151296 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3444647789001465, loss=2.9725217819213867
I0305 23:00:39.432880 139708776544000 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.3481120765209198, loss=2.938015937805176
I0305 23:01:14.358426 139708768151296 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3364025056362152, loss=3.043919563293457
I0305 23:01:49.289736 139708776544000 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.36140215396881104, loss=2.951498031616211
I0305 23:02:24.220026 139708768151296 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.33503562211990356, loss=3.033259153366089
I0305 23:02:59.158306 139708776544000 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.3567573130130768, loss=3.044008493423462
I0305 23:03:34.132736 139708768151296 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5228123664855957, loss=3.059222936630249
I0305 23:04:09.073854 139708776544000 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.450636088848114, loss=2.9631025791168213
I0305 23:04:43.989228 139708768151296 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.3526018559932709, loss=3.048189640045166
I0305 23:05:18.919164 139708776544000 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3384966552257538, loss=2.9990549087524414
I0305 23:05:53.855122 139708768151296 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.34832963347435, loss=3.0023417472839355
I0305 23:06:28.774329 139708776544000 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5097131133079529, loss=3.048173189163208
I0305 23:07:03.688087 139708768151296 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.4201250374317169, loss=2.9578640460968018
I0305 23:07:38.609328 139708776544000 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3828105926513672, loss=2.93203067779541
I0305 23:08:13.535348 139708768151296 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3369131088256836, loss=3.080775022506714
I0305 23:08:48.456446 139708776544000 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.3930806517601013, loss=3.037442207336426
I0305 23:09:23.431771 139708768151296 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.39882585406303406, loss=3.014537811279297
I0305 23:09:58.343287 139708776544000 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.42156773805618286, loss=2.9869894981384277
I0305 23:10:33.291462 139708768151296 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3229622542858124, loss=2.9924774169921875
I0305 23:10:36.783951 139852506842304 spec.py:321] Evaluating on the training split.
I0305 23:10:39.446485 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 23:14:20.626989 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 23:14:23.255541 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 23:17:33.781378 139852506842304 spec.py:349] Evaluating on the test split.
I0305 23:17:36.415771 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 23:20:36.015718 139852506842304 submission_runner.py:469] Time since start: 14799.12s, 	Step: 24111, 	{'train/accuracy': 0.6299898624420166, 'train/loss': 1.9242160320281982, 'train/bleu': 30.559869686208867, 'validation/accuracy': 0.6520777344703674, 'validation/loss': 1.7576435804367065, 'validation/bleu': 27.915332295007094, 'validation/num_examples': 3000, 'test/accuracy': 0.6603058576583862, 'test/loss': 1.6992652416229248, 'test/bleu': 26.817969834344407, 'test/num_examples': 3003, 'score': 8426.003172159195, 'total_duration': 14799.120358228683, 'accumulated_submission_time': 8426.003172159195, 'accumulated_eval_time': 6371.407712459564, 'accumulated_logging_time': 0.18689775466918945}
I0305 23:20:36.027273 139708776544000 logging_writer.py:48] [24111] accumulated_eval_time=6371.41, accumulated_logging_time=0.186898, accumulated_submission_time=8426, global_step=24111, preemption_count=0, score=8426, test/accuracy=0.660306, test/bleu=26.818, test/loss=1.69927, test/num_examples=3003, total_duration=14799.1, train/accuracy=0.62999, train/bleu=30.5599, train/loss=1.92422, validation/accuracy=0.652078, validation/bleu=27.9153, validation/loss=1.75764, validation/num_examples=3000
I0305 23:21:07.338585 139708768151296 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3628274202346802, loss=2.9713380336761475
I0305 23:21:42.188936 139708776544000 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3611801862716675, loss=2.979313611984253
I0305 23:22:17.108083 139708768151296 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.5792950987815857, loss=3.0457682609558105
I0305 23:22:52.047318 139708776544000 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.36239421367645264, loss=3.000699281692505
I0305 23:23:26.920309 139708768151296 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.361771821975708, loss=3.001295804977417
I0305 23:24:01.832849 139708776544000 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.39710450172424316, loss=3.021876573562622
I0305 23:24:36.791234 139708768151296 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3583256006240845, loss=3.064440965652466
I0305 23:25:11.750310 139708776544000 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.42625123262405396, loss=3.0087645053863525
I0305 23:25:46.675101 139708768151296 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.3506030738353729, loss=3.026282548904419
I0305 23:26:21.649564 139708776544000 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.31865984201431274, loss=3.0392119884490967
I0305 23:26:56.608911 139708768151296 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.3209364712238312, loss=3.02193021774292
I0305 23:27:31.561562 139708776544000 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4625520408153534, loss=3.0576837062835693
I0305 23:28:06.518639 139708768151296 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3742980360984802, loss=3.1061813831329346
I0305 23:28:41.491489 139708776544000 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.33603721857070923, loss=2.9814350605010986
I0305 23:29:16.463574 139708768151296 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.3509305715560913, loss=2.9987120628356934
I0305 23:29:51.423448 139708776544000 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3147156238555908, loss=2.9594452381134033
I0305 23:30:26.376860 139708768151296 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.36106181144714355, loss=2.944751739501953
I0305 23:31:01.326853 139708776544000 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3682000935077667, loss=3.0196495056152344
I0305 23:31:36.266287 139708768151296 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4212476909160614, loss=3.073230743408203
I0305 23:32:11.253478 139708776544000 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3592469394207001, loss=3.031888008117676
I0305 23:32:46.230406 139708768151296 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.4119064509868622, loss=2.9993810653686523
I0305 23:33:21.215665 139708776544000 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.3490677773952484, loss=2.980163335800171
I0305 23:33:56.184573 139708768151296 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.34690871834754944, loss=3.0492513179779053
I0305 23:34:31.144035 139708776544000 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3376871645450592, loss=3.0785539150238037
I0305 23:34:36.045122 139852506842304 spec.py:321] Evaluating on the training split.
I0305 23:34:38.705418 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 23:37:21.770394 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 23:37:24.402903 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 23:39:59.371814 139852506842304 spec.py:349] Evaluating on the test split.
I0305 23:40:02.014365 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 23:42:36.289921 139852506842304 submission_runner.py:469] Time since start: 16119.39s, 	Step: 26515, 	{'train/accuracy': 0.6331630349159241, 'train/loss': 1.889734148979187, 'train/bleu': 30.84656444180887, 'validation/accuracy': 0.6529058814048767, 'validation/loss': 1.7470378875732422, 'validation/bleu': 27.998916500556494, 'validation/num_examples': 3000, 'test/accuracy': 0.6653690338134766, 'test/loss': 1.6885408163070679, 'test/bleu': 27.20259209013743, 'test/num_examples': 3003, 'score': 9265.873390197754, 'total_duration': 16119.394581079483, 'accumulated_submission_time': 9265.873390197754, 'accumulated_eval_time': 6851.652458429337, 'accumulated_logging_time': 0.2075519561767578}
I0305 23:42:36.302165 139708768151296 logging_writer.py:48] [26515] accumulated_eval_time=6851.65, accumulated_logging_time=0.207552, accumulated_submission_time=9265.87, global_step=26515, preemption_count=0, score=9265.87, test/accuracy=0.665369, test/bleu=27.2026, test/loss=1.68854, test/num_examples=3003, total_duration=16119.4, train/accuracy=0.633163, train/bleu=30.8466, train/loss=1.88973, validation/accuracy=0.652906, validation/bleu=27.9989, validation/loss=1.74704, validation/num_examples=3000
I0305 23:43:06.209901 139708776544000 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.3389546573162079, loss=3.0020313262939453
I0305 23:43:41.045766 139708768151296 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3896924555301666, loss=3.0257821083068848
I0305 23:44:15.969318 139708776544000 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.3970724046230316, loss=2.975334405899048
I0305 23:44:50.926003 139708768151296 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.34054654836654663, loss=2.9233357906341553
I0305 23:45:25.886431 139708776544000 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4479472041130066, loss=2.9931740760803223
I0305 23:46:00.824328 139708768151296 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.36365267634391785, loss=2.9120140075683594
I0305 23:46:35.761041 139708776544000 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3516521155834198, loss=3.067876100540161
I0305 23:47:10.739567 139708768151296 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.4100551903247833, loss=3.0072309970855713
I0305 23:47:45.700356 139708776544000 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3543526828289032, loss=3.02607798576355
I0305 23:48:20.673678 139708768151296 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.4161251485347748, loss=2.9737212657928467
I0305 23:48:55.639132 139708776544000 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.36254456639289856, loss=3.0032687187194824
I0305 23:49:30.627159 139708768151296 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.35206159949302673, loss=2.9430506229400635
I0305 23:50:05.592429 139708776544000 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.33546778559684753, loss=2.9842069149017334
I0305 23:50:40.594518 139708768151296 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.3416599929332733, loss=2.979353666305542
I0305 23:51:15.539527 139708776544000 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.37125906348228455, loss=3.020228385925293
I0305 23:51:50.487990 139708768151296 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.35549667477607727, loss=2.8982808589935303
I0305 23:52:25.400276 139708776544000 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.3347203731536865, loss=2.9653875827789307
I0305 23:53:00.379776 139708768151296 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.3780960142612457, loss=3.0524373054504395
I0305 23:53:35.330471 139708776544000 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.4145340919494629, loss=2.92922043800354
I0305 23:54:10.284246 139708768151296 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3332129120826721, loss=3.010017156600952
I0305 23:54:45.201570 139708776544000 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.4697818160057068, loss=2.9471447467803955
I0305 23:55:20.169558 139708768151296 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.41429224610328674, loss=2.9920766353607178
I0305 23:55:55.139178 139708776544000 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3260795772075653, loss=2.9585964679718018
I0305 23:56:30.073421 139708768151296 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.42917972803115845, loss=2.8911406993865967
I0305 23:56:36.351616 139852506842304 spec.py:321] Evaluating on the training split.
I0305 23:56:39.006356 139852506842304 workload.py:181] Translating evaluation dataset.
I0305 23:59:25.796545 139852506842304 spec.py:333] Evaluating on the validation split.
I0305 23:59:28.447245 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:02:02.080258 139852506842304 spec.py:349] Evaluating on the test split.
I0306 00:02:04.717333 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:04:36.298760 139852506842304 submission_runner.py:469] Time since start: 17439.40s, 	Step: 28919, 	{'train/accuracy': 0.6342933177947998, 'train/loss': 1.8857753276824951, 'train/bleu': 30.73051204934578, 'validation/accuracy': 0.6558598875999451, 'validation/loss': 1.7290669679641724, 'validation/bleu': 27.94156082790439, 'validation/num_examples': 3000, 'test/accuracy': 0.6678252816200256, 'test/loss': 1.667887806892395, 'test/bleu': 27.628518314499786, 'test/num_examples': 3003, 'score': 10105.777422904968, 'total_duration': 17439.403420209885, 'accumulated_submission_time': 10105.777422904968, 'accumulated_eval_time': 7331.599550008774, 'accumulated_logging_time': 0.22798991203308105}
I0306 00:04:36.310916 139708776544000 logging_writer.py:48] [28919] accumulated_eval_time=7331.6, accumulated_logging_time=0.22799, accumulated_submission_time=10105.8, global_step=28919, preemption_count=0, score=10105.8, test/accuracy=0.667825, test/bleu=27.6285, test/loss=1.66789, test/num_examples=3003, total_duration=17439.4, train/accuracy=0.634293, train/bleu=30.7305, train/loss=1.88578, validation/accuracy=0.65586, validation/bleu=27.9416, validation/loss=1.72907, validation/num_examples=3000
I0306 00:05:04.852725 139708768151296 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.3784797787666321, loss=2.9857430458068848
I0306 00:05:39.715074 139708776544000 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.32843565940856934, loss=2.8962290287017822
I0306 00:06:14.675558 139708768151296 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.3721539378166199, loss=2.9868340492248535
I0306 00:06:49.622152 139708776544000 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.4222613573074341, loss=3.0373973846435547
I0306 00:07:24.563816 139708768151296 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.4199813902378082, loss=2.989980936050415
I0306 00:07:59.524121 139708776544000 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.31090569496154785, loss=2.9081170558929443
I0306 00:08:34.468811 139708768151296 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3527388274669647, loss=2.9874775409698486
I0306 00:09:09.430548 139708776544000 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.36204349994659424, loss=2.9133238792419434
I0306 00:09:44.388003 139708768151296 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.2246531248092651, loss=3.055985689163208
I0306 00:10:19.346432 139708776544000 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.42757222056388855, loss=3.0256235599517822
I0306 00:10:54.323663 139708768151296 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.3634554147720337, loss=2.8792810440063477
I0306 00:11:29.293780 139708776544000 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3776867687702179, loss=2.9456682205200195
I0306 00:12:04.248658 139708768151296 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.32330554723739624, loss=2.9320802688598633
I0306 00:12:39.191031 139708776544000 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.33391568064689636, loss=2.9482529163360596
I0306 00:13:14.136996 139708768151296 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.37421467900276184, loss=2.9968552589416504
I0306 00:13:49.125579 139708776544000 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.33142295479774475, loss=2.9762485027313232
I0306 00:14:24.106325 139708768151296 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.30789098143577576, loss=2.9463226795196533
I0306 00:14:59.059632 139708776544000 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.35213369131088257, loss=2.963343381881714
I0306 00:15:34.008703 139708768151296 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.36065858602523804, loss=2.950510263442993
I0306 00:16:08.954584 139708776544000 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3989364206790924, loss=2.9863903522491455
I0306 00:16:43.931461 139708768151296 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.37945815920829773, loss=2.976562976837158
I0306 00:17:18.923705 139708776544000 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3435571789741516, loss=2.969360589981079
I0306 00:17:53.907279 139708768151296 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.3432242274284363, loss=2.928382396697998
I0306 00:18:28.875808 139708776544000 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3829149901866913, loss=2.9625704288482666
I0306 00:18:36.569336 139852506842304 spec.py:321] Evaluating on the training split.
I0306 00:18:39.228306 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:22:10.728123 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 00:22:13.361988 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:25:21.223981 139852506842304 spec.py:349] Evaluating on the test split.
I0306 00:25:23.858895 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:28:27.547692 139852506842304 submission_runner.py:469] Time since start: 18870.65s, 	Step: 31323, 	{'train/accuracy': 0.6364214420318604, 'train/loss': 1.8872747421264648, 'train/bleu': 30.734913848436165, 'validation/accuracy': 0.6571947932243347, 'validation/loss': 1.7246627807617188, 'validation/bleu': 28.082402193328384, 'validation/num_examples': 3000, 'test/accuracy': 0.6681265234947205, 'test/loss': 1.6692999601364136, 'test/bleu': 27.544509536495223, 'test/num_examples': 3003, 'score': 10945.888687372208, 'total_duration': 18870.652339458466, 'accumulated_submission_time': 10945.888687372208, 'accumulated_eval_time': 7922.577842235565, 'accumulated_logging_time': 0.2482743263244629}
I0306 00:28:27.560950 139708768151296 logging_writer.py:48] [31323] accumulated_eval_time=7922.58, accumulated_logging_time=0.248274, accumulated_submission_time=10945.9, global_step=31323, preemption_count=0, score=10945.9, test/accuracy=0.668127, test/bleu=27.5445, test/loss=1.6693, test/num_examples=3003, total_duration=18870.7, train/accuracy=0.636421, train/bleu=30.7349, train/loss=1.88727, validation/accuracy=0.657195, validation/bleu=28.0824, validation/loss=1.72466, validation/num_examples=3000
I0306 00:28:54.600759 139708776544000 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.3378150463104248, loss=2.9624650478363037
I0306 00:29:29.283595 139708768151296 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3956097960472107, loss=2.9472198486328125
I0306 00:30:04.052576 139708776544000 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.3534921109676361, loss=2.9783263206481934
I0306 00:30:38.848014 139708768151296 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.3436020314693451, loss=2.8879024982452393
I0306 00:31:13.644728 139708776544000 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3335241973400116, loss=2.939622163772583
I0306 00:31:48.449790 139708768151296 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3542908728122711, loss=3.0220601558685303
I0306 00:32:23.264038 139708776544000 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.4504421055316925, loss=2.9935877323150635
I0306 00:32:58.060288 139708768151296 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.4471071660518646, loss=2.95135760307312
I0306 00:33:32.839285 139708776544000 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.31250908970832825, loss=2.8929660320281982
I0306 00:34:07.587163 139708768151296 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3791602849960327, loss=2.9393346309661865
I0306 00:34:42.351407 139708776544000 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.37391817569732666, loss=2.9969379901885986
I0306 00:35:17.114735 139708768151296 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3733026385307312, loss=2.984346389770508
I0306 00:35:51.835314 139708776544000 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3562217950820923, loss=2.9269235134124756
I0306 00:36:26.595734 139708768151296 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.36134326457977295, loss=2.961747646331787
I0306 00:37:01.383700 139708776544000 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.37933987379074097, loss=2.9491636753082275
I0306 00:37:36.139036 139708768151296 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.36122915148735046, loss=2.956908941268921
I0306 00:38:10.865735 139708776544000 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3372844159603119, loss=2.9730188846588135
I0306 00:38:45.604424 139708768151296 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.3721916377544403, loss=2.9906444549560547
I0306 00:39:20.365718 139708776544000 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.37405386567115784, loss=2.9719326496124268
I0306 00:39:55.122018 139708768151296 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.35264769196510315, loss=2.9364352226257324
I0306 00:40:29.862619 139708776544000 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.35454949736595154, loss=2.9120497703552246
I0306 00:41:04.617460 139708768151296 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.4119013249874115, loss=2.9301979541778564
I0306 00:41:39.353222 139708776544000 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.34642937779426575, loss=2.927677631378174
I0306 00:42:14.111765 139708768151296 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.31085875630378723, loss=2.9646787643432617
I0306 00:42:27.658979 139852506842304 spec.py:321] Evaluating on the training split.
I0306 00:42:30.307883 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:45:32.680335 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 00:45:35.321721 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:48:08.858727 139852506842304 spec.py:349] Evaluating on the test split.
I0306 00:48:11.497052 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 00:50:50.766731 139852506842304 submission_runner.py:469] Time since start: 20213.87s, 	Step: 33740, 	{'train/accuracy': 0.6405691504478455, 'train/loss': 1.8434520959854126, 'train/bleu': 30.678251681709757, 'validation/accuracy': 0.6594937443733215, 'validation/loss': 1.7051464319229126, 'validation/bleu': 27.97360696459214, 'validation/num_examples': 3000, 'test/accuracy': 0.6690302491188049, 'test/loss': 1.6490671634674072, 'test/bleu': 27.722226741015714, 'test/num_examples': 3003, 'score': 11785.839684009552, 'total_duration': 20213.871381998062, 'accumulated_submission_time': 11785.839684009552, 'accumulated_eval_time': 8425.685530424118, 'accumulated_logging_time': 0.26983189582824707}
I0306 00:50:50.778604 139708776544000 logging_writer.py:48] [33740] accumulated_eval_time=8425.69, accumulated_logging_time=0.269832, accumulated_submission_time=11785.8, global_step=33740, preemption_count=0, score=11785.8, test/accuracy=0.66903, test/bleu=27.7222, test/loss=1.64907, test/num_examples=3003, total_duration=20213.9, train/accuracy=0.640569, train/bleu=30.6783, train/loss=1.84345, validation/accuracy=0.659494, validation/bleu=27.9736, validation/loss=1.70515, validation/num_examples=3000
I0306 00:51:11.875449 139708768151296 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.32239702343940735, loss=2.929661273956299
I0306 00:51:46.464869 139708776544000 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.3535339832305908, loss=2.9244487285614014
I0306 00:52:21.135110 139708768151296 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3354693651199341, loss=3.0096826553344727
I0306 00:52:55.858726 139708776544000 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.32885804772377014, loss=2.958836555480957
I0306 00:53:30.594146 139708768151296 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.3804459571838379, loss=2.9378151893615723
I0306 00:54:05.311408 139708776544000 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.33299022912979126, loss=2.994236469268799
I0306 00:54:40.057252 139708768151296 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.3764481842517853, loss=2.9300737380981445
I0306 00:55:14.796957 139708776544000 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.36393579840660095, loss=2.975217819213867
I0306 00:55:49.526896 139708768151296 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.3984583020210266, loss=2.924459218978882
I0306 00:56:24.252868 139708776544000 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3552614450454712, loss=2.9099783897399902
I0306 00:56:58.997657 139708768151296 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.3461935520172119, loss=2.982945442199707
I0306 00:57:33.739031 139708776544000 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3422786295413971, loss=2.933769464492798
I0306 00:58:08.467405 139708768151296 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.35609346628189087, loss=2.962672233581543
I0306 00:58:43.187580 139708776544000 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.33182477951049805, loss=2.91664457321167
I0306 00:59:17.934259 139708768151296 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.37191876769065857, loss=2.996103525161743
I0306 00:59:52.665280 139708776544000 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.34385332465171814, loss=3.020195960998535
I0306 01:00:27.400105 139708768151296 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.36412280797958374, loss=2.8979032039642334
I0306 01:01:02.150648 139708776544000 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.42428600788116455, loss=2.9506118297576904
I0306 01:01:36.885634 139708768151296 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3469432592391968, loss=2.8850951194763184
I0306 01:02:11.615271 139708776544000 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.37093356251716614, loss=2.96250057220459
I0306 01:02:46.373466 139708768151296 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.36115843057632446, loss=2.980912923812866
I0306 01:03:21.118193 139708776544000 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.35584115982055664, loss=2.9847638607025146
I0306 01:03:55.863018 139708768151296 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.328823059797287, loss=2.9512064456939697
I0306 01:04:30.588959 139708776544000 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.353793740272522, loss=2.954610586166382
I0306 01:04:51.077567 139852506842304 spec.py:321] Evaluating on the training split.
I0306 01:04:53.730834 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:07:27.045015 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 01:07:29.676017 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:09:59.828488 139852506842304 spec.py:349] Evaluating on the test split.
I0306 01:10:02.462351 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:12:26.325987 139852506842304 submission_runner.py:469] Time since start: 21509.43s, 	Step: 36160, 	{'train/accuracy': 0.6369484663009644, 'train/loss': 1.86934232711792, 'train/bleu': 31.591586046169475, 'validation/accuracy': 0.6588139533996582, 'validation/loss': 1.7098349332809448, 'validation/bleu': 28.07531150460195, 'validation/num_examples': 3000, 'test/accuracy': 0.6717761754989624, 'test/loss': 1.6449332237243652, 'test/bleu': 27.815180626523215, 'test/num_examples': 3003, 'score': 12625.991487503052, 'total_duration': 21509.43064880371, 'accumulated_submission_time': 12625.991487503052, 'accumulated_eval_time': 8880.933901071548, 'accumulated_logging_time': 0.28970766067504883}
I0306 01:12:26.338112 139708768151296 logging_writer.py:48] [36160] accumulated_eval_time=8880.93, accumulated_logging_time=0.289708, accumulated_submission_time=12626, global_step=36160, preemption_count=0, score=12626, test/accuracy=0.671776, test/bleu=27.8152, test/loss=1.64493, test/num_examples=3003, total_duration=21509.4, train/accuracy=0.636948, train/bleu=31.5916, train/loss=1.86934, validation/accuracy=0.658814, validation/bleu=28.0753, validation/loss=1.70983, validation/num_examples=3000
I0306 01:12:40.511127 139708776544000 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.38062766194343567, loss=2.9599900245666504
I0306 01:13:15.098476 139708768151296 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.3626650869846344, loss=2.9860024452209473
I0306 01:13:49.747563 139708776544000 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.33557116985321045, loss=2.9382293224334717
I0306 01:14:24.475846 139708768151296 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.322894424200058, loss=2.9688665866851807
I0306 01:14:59.204322 139708776544000 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.37471669912338257, loss=2.9752042293548584
I0306 01:15:33.938554 139708768151296 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.32006072998046875, loss=2.9151198863983154
I0306 01:16:08.633224 139708776544000 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3623324930667877, loss=2.9305179119110107
I0306 01:16:43.336623 139708768151296 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3605705499649048, loss=2.9603638648986816
I0306 01:17:18.074462 139708776544000 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.400814950466156, loss=2.953997850418091
I0306 01:17:52.818378 139708768151296 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.34078776836395264, loss=2.965404748916626
I0306 01:18:27.557688 139708776544000 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.32154446840286255, loss=3.0002713203430176
I0306 01:19:02.300133 139708768151296 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.33667656779289246, loss=2.9617462158203125
I0306 01:19:37.019114 139708776544000 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3435545265674591, loss=2.9624087810516357
I0306 01:20:11.768155 139708768151296 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.37905749678611755, loss=3.0019447803497314
I0306 01:20:46.517693 139708776544000 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.330205500125885, loss=3.006046772003174
I0306 01:21:21.305994 139708768151296 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.42460352182388306, loss=2.9816389083862305
I0306 01:21:56.117426 139708776544000 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3342740535736084, loss=2.9019572734832764
I0306 01:22:30.921766 139708768151296 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.3494289219379425, loss=2.9695675373077393
I0306 01:23:05.739385 139708776544000 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3399384617805481, loss=3.0428414344787598
I0306 01:23:40.554243 139708768151296 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3969007432460785, loss=3.0018270015716553
I0306 01:24:15.353082 139708776544000 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.36348938941955566, loss=2.9522030353546143
I0306 01:24:50.170899 139708768151296 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.40382128953933716, loss=2.968308925628662
I0306 01:25:24.967991 139708776544000 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.3731367588043213, loss=2.9322307109832764
I0306 01:25:59.769482 139708768151296 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.37955573201179504, loss=2.937675952911377
I0306 01:26:26.565757 139852506842304 spec.py:321] Evaluating on the training split.
I0306 01:26:29.213404 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:29:25.173386 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 01:29:27.809826 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:32:19.293433 139852506842304 spec.py:349] Evaluating on the test split.
I0306 01:32:21.929417 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:35:04.467116 139852506842304 submission_runner.py:469] Time since start: 22867.57s, 	Step: 38578, 	{'train/accuracy': 0.6456181406974792, 'train/loss': 1.8031083345413208, 'train/bleu': 31.690782831963073, 'validation/accuracy': 0.6602600812911987, 'validation/loss': 1.6966221332550049, 'validation/bleu': 28.18649346345213, 'validation/num_examples': 3000, 'test/accuracy': 0.6722743511199951, 'test/loss': 1.6358877420425415, 'test/bleu': 27.8037680103288, 'test/num_examples': 3003, 'score': 13466.073394536972, 'total_duration': 22867.5717792511, 'accumulated_submission_time': 13466.073394536972, 'accumulated_eval_time': 9398.835209608078, 'accumulated_logging_time': 0.309859037399292}
I0306 01:35:04.479591 139708776544000 logging_writer.py:48] [38578] accumulated_eval_time=9398.84, accumulated_logging_time=0.309859, accumulated_submission_time=13466.1, global_step=38578, preemption_count=0, score=13466.1, test/accuracy=0.672274, test/bleu=27.8038, test/loss=1.63589, test/num_examples=3003, total_duration=22867.6, train/accuracy=0.645618, train/bleu=31.6908, train/loss=1.80311, validation/accuracy=0.66026, validation/bleu=28.1865, validation/loss=1.69662, validation/num_examples=3000
I0306 01:35:12.429766 139708768151296 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.3905281126499176, loss=2.87874698638916
I0306 01:35:47.082997 139708776544000 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.34282106161117554, loss=2.9407496452331543
I0306 01:36:21.831980 139708768151296 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3991890847682953, loss=3.0084657669067383
I0306 01:36:56.588663 139708776544000 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.336626261472702, loss=2.941195487976074
I0306 01:37:31.392400 139708768151296 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.40534237027168274, loss=2.8855137825012207
I0306 01:38:06.212515 139708776544000 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.3462131917476654, loss=2.9573962688446045
I0306 01:38:40.995326 139708768151296 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3570197522640228, loss=2.8994815349578857
I0306 01:39:15.827000 139708776544000 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.3747510313987732, loss=2.9290127754211426
I0306 01:39:50.627919 139708768151296 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.3565705418586731, loss=2.9461798667907715
I0306 01:40:25.454351 139708776544000 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.39673706889152527, loss=2.988910436630249
I0306 01:41:00.274981 139708768151296 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3731028139591217, loss=2.9694409370422363
I0306 01:41:35.091483 139708776544000 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.33728912472724915, loss=2.9249892234802246
I0306 01:42:09.929187 139708768151296 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.3322599232196808, loss=2.8643572330474854
I0306 01:42:44.735174 139708776544000 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.33340340852737427, loss=2.8859550952911377
I0306 01:43:19.558124 139708768151296 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.35923337936401367, loss=2.9143989086151123
I0306 01:43:54.381754 139708776544000 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.39602482318878174, loss=3.0023179054260254
I0306 01:44:29.204118 139708768151296 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3681894838809967, loss=2.9190869331359863
I0306 01:45:04.057842 139708776544000 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.33588510751724243, loss=2.9621098041534424
I0306 01:45:38.892182 139708768151296 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.3366234302520752, loss=2.9365246295928955
I0306 01:46:13.786666 139708776544000 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.39694347977638245, loss=2.954037666320801
I0306 01:46:48.696066 139708768151296 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.4649447500705719, loss=2.97196888923645
I0306 01:47:23.571763 139708776544000 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.33045023679733276, loss=3.0043485164642334
I0306 01:47:58.386373 139708768151296 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.38149598240852356, loss=2.9130115509033203
I0306 01:48:33.214966 139708776544000 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.35168230533599854, loss=2.9054195880889893
I0306 01:49:04.575055 139852506842304 spec.py:321] Evaluating on the training split.
I0306 01:49:07.222332 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:52:10.121316 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 01:52:12.752034 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:54:53.105708 139852506842304 spec.py:349] Evaluating on the test split.
I0306 01:54:55.742843 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 01:57:25.249832 139852506842304 submission_runner.py:469] Time since start: 24208.35s, 	Step: 40991, 	{'train/accuracy': 0.6403080821037292, 'train/loss': 1.847672462463379, 'train/bleu': 30.99196682391661, 'validation/accuracy': 0.6626455783843994, 'validation/loss': 1.6861871480941772, 'validation/bleu': 28.170658651857863, 'validation/num_examples': 3000, 'test/accuracy': 0.6758313179016113, 'test/loss': 1.6197723150253296, 'test/bleu': 28.082646092400754, 'test/num_examples': 3003, 'score': 14306.01717877388, 'total_duration': 24208.354489564896, 'accumulated_submission_time': 14306.01717877388, 'accumulated_eval_time': 9899.509933710098, 'accumulated_logging_time': 0.33040642738342285}
I0306 01:57:25.262629 139708768151296 logging_writer.py:48] [40991] accumulated_eval_time=9899.51, accumulated_logging_time=0.330406, accumulated_submission_time=14306, global_step=40991, preemption_count=0, score=14306, test/accuracy=0.675831, test/bleu=28.0826, test/loss=1.61977, test/num_examples=3003, total_duration=24208.4, train/accuracy=0.640308, train/bleu=30.992, train/loss=1.84767, validation/accuracy=0.662646, validation/bleu=28.1707, validation/loss=1.68619, validation/num_examples=3000
I0306 01:57:28.725216 139708776544000 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.3350197374820709, loss=2.86328387260437
I0306 01:58:03.390611 139708768151296 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.40745648741722107, loss=2.9677412509918213
I0306 01:58:38.123374 139708776544000 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.37576499581336975, loss=2.8717422485351562
I0306 01:59:12.925269 139708768151296 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.4205625355243683, loss=2.905160427093506
I0306 01:59:47.695417 139708776544000 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.4045848250389099, loss=2.8961708545684814
I0306 02:00:22.499628 139708768151296 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3781622350215912, loss=2.9235851764678955
I0306 02:00:57.305736 139708776544000 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.38743719458580017, loss=2.987318277359009
I0306 02:01:32.093343 139708768151296 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3779323995113373, loss=2.9175431728363037
I0306 02:02:06.918834 139708776544000 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.34800946712493896, loss=2.9424002170562744
I0306 02:02:41.760101 139708768151296 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3000921607017517, loss=2.873933792114258
I0306 02:03:16.601228 139708776544000 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.34537366032600403, loss=2.906541109085083
I0306 02:03:51.441395 139708768151296 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.39559224247932434, loss=2.9452109336853027
I0306 02:04:26.255108 139708776544000 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3818318843841553, loss=2.9633963108062744
I0306 02:05:01.070792 139708768151296 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3480643630027771, loss=2.8961594104766846
I0306 02:05:35.885786 139708776544000 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.356891006231308, loss=2.972881317138672
I0306 02:06:10.702160 139708768151296 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.36934399604797363, loss=2.9528536796569824
I0306 02:06:45.508141 139708776544000 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.362830251455307, loss=2.9822428226470947
I0306 02:07:20.315266 139708768151296 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.37087488174438477, loss=2.9178619384765625
I0306 02:07:55.127823 139708776544000 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.3096885681152344, loss=2.9199466705322266
I0306 02:08:29.941919 139708768151296 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.34589776396751404, loss=2.94222354888916
I0306 02:09:04.736379 139708776544000 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3424019515514374, loss=2.9648492336273193
I0306 02:09:39.564585 139708768151296 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.34723925590515137, loss=2.8983044624328613
I0306 02:10:14.388720 139708776544000 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.35719287395477295, loss=2.8908259868621826
I0306 02:10:49.226488 139708768151296 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.351428747177124, loss=2.9611334800720215
I0306 02:11:24.042576 139708776544000 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3424336910247803, loss=2.908703088760376
I0306 02:11:25.445265 139852506842304 spec.py:321] Evaluating on the training split.
I0306 02:11:28.090850 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 02:14:31.576041 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 02:14:34.210030 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 02:17:18.449222 139852506842304 spec.py:349] Evaluating on the test split.
I0306 02:17:21.079752 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 02:19:53.305444 139852506842304 submission_runner.py:469] Time since start: 25556.41s, 	Step: 43405, 	{'train/accuracy': 0.6395072340965271, 'train/loss': 1.8533098697662354, 'train/bleu': 31.874181130620048, 'validation/accuracy': 0.6642523407936096, 'validation/loss': 1.6788249015808105, 'validation/bleu': 28.610164621569854, 'validation/num_examples': 3000, 'test/accuracy': 0.6759124398231506, 'test/loss': 1.613234043121338, 'test/bleu': 28.34741367238577, 'test/num_examples': 3003, 'score': 15146.052419185638, 'total_duration': 25556.410094738007, 'accumulated_submission_time': 15146.052419185638, 'accumulated_eval_time': 10407.3700568676, 'accumulated_logging_time': 0.3514692783355713}
I0306 02:19:53.320791 139708768151296 logging_writer.py:48] [43405] accumulated_eval_time=10407.4, accumulated_logging_time=0.351469, accumulated_submission_time=15146.1, global_step=43405, preemption_count=0, score=15146.1, test/accuracy=0.675912, test/bleu=28.3474, test/loss=1.61323, test/num_examples=3003, total_duration=25556.4, train/accuracy=0.639507, train/bleu=31.8742, train/loss=1.85331, validation/accuracy=0.664252, validation/bleu=28.6102, validation/loss=1.67882, validation/num_examples=3000
I0306 02:20:26.584286 139708776544000 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7042669057846069, loss=2.9741408824920654
I0306 02:21:01.291079 139708768151296 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.3557458221912384, loss=2.8497633934020996
I0306 02:21:36.079312 139708776544000 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.38534015417099, loss=2.968291759490967
I0306 02:22:10.865667 139708768151296 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.33880117535591125, loss=2.8910775184631348
I0306 02:22:45.676427 139708776544000 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3891963064670563, loss=2.941046714782715
I0306 02:23:20.489372 139708768151296 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3567280173301697, loss=2.9189062118530273
I0306 02:23:55.303376 139708776544000 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.37618809938430786, loss=2.9617385864257812
I0306 02:24:30.103017 139708768151296 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.3101072907447815, loss=2.9467461109161377
I0306 02:25:04.909205 139708776544000 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.329426109790802, loss=2.9188034534454346
I0306 02:25:39.732289 139708768151296 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3783388137817383, loss=2.8832428455352783
I0306 02:26:14.558398 139708776544000 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.38258057832717896, loss=2.8771634101867676
I0306 02:26:49.385558 139708768151296 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.3272521197795868, loss=2.888188362121582
I0306 02:27:24.190228 139708776544000 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.38461199402809143, loss=2.8920974731445312
I0306 02:27:59.031905 139708768151296 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.3455027937889099, loss=2.984683036804199
I0306 02:28:33.830186 139708776544000 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.35797974467277527, loss=3.0224788188934326
I0306 02:29:08.625170 139708768151296 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.37399157881736755, loss=2.8529417514801025
I0306 02:29:43.421928 139708776544000 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.39305877685546875, loss=2.9259893894195557
I0306 02:30:18.228072 139708768151296 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.42320716381073, loss=2.949510097503662
I0306 02:30:53.055446 139708776544000 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.33157673478126526, loss=2.8626205921173096
I0306 02:31:27.883255 139708768151296 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.35822615027427673, loss=2.882913827896118
I0306 02:32:02.708911 139708776544000 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.32027584314346313, loss=2.9327235221862793
I0306 02:32:37.545849 139708768151296 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3315254747867584, loss=2.8370869159698486
I0306 02:33:12.362086 139708776544000 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.36750441789627075, loss=2.9056332111358643
I0306 02:33:47.168352 139708768151296 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.37599167227745056, loss=2.969191074371338
I0306 02:33:53.437880 139852506842304 spec.py:321] Evaluating on the training split.
I0306 02:33:56.080432 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 02:36:40.094487 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 02:36:42.724932 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 02:39:15.258297 139852506842304 spec.py:349] Evaluating on the test split.
I0306 02:39:17.902066 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 02:41:43.412728 139852506842304 submission_runner.py:469] Time since start: 26866.52s, 	Step: 45819, 	{'train/accuracy': 0.6451601386070251, 'train/loss': 1.8110008239746094, 'train/bleu': 31.79004667094335, 'validation/accuracy': 0.6654388904571533, 'validation/loss': 1.6698273420333862, 'validation/bleu': 28.9114861017733, 'validation/num_examples': 3000, 'test/accuracy': 0.6774881482124329, 'test/loss': 1.6059889793395996, 'test/bleu': 28.60628702110906, 'test/num_examples': 3003, 'score': 15986.023608207703, 'total_duration': 26866.517375707626, 'accumulated_submission_time': 15986.023608207703, 'accumulated_eval_time': 10877.34483742714, 'accumulated_logging_time': 0.37587738037109375}
I0306 02:41:43.427353 139708776544000 logging_writer.py:48] [45819] accumulated_eval_time=10877.3, accumulated_logging_time=0.375877, accumulated_submission_time=15986, global_step=45819, preemption_count=0, score=15986, test/accuracy=0.677488, test/bleu=28.6063, test/loss=1.60599, test/num_examples=3003, total_duration=26866.5, train/accuracy=0.64516, train/bleu=31.79, train/loss=1.811, validation/accuracy=0.665439, validation/bleu=28.9115, validation/loss=1.66983, validation/num_examples=3000
I0306 02:42:11.824795 139708768151296 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3757331669330597, loss=2.9232921600341797
I0306 02:42:46.540763 139708776544000 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3916768431663513, loss=3.041879892349243
I0306 02:43:21.333861 139708768151296 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.4136902093887329, loss=2.8571207523345947
I0306 02:43:56.112567 139708776544000 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.3772886097431183, loss=3.029709577560425
I0306 02:44:30.923384 139708768151296 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.4270177185535431, loss=2.941939353942871
I0306 02:45:05.714979 139708776544000 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3617255687713623, loss=2.9657340049743652
I0306 02:45:40.526251 139708768151296 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.38886165618896484, loss=2.9192006587982178
I0306 02:46:15.335185 139708776544000 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.35651734471321106, loss=2.87803053855896
I0306 02:46:50.157577 139708768151296 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.34380701184272766, loss=2.8994832038879395
I0306 02:47:25.007016 139708776544000 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.35106441378593445, loss=2.9197983741760254
I0306 02:47:59.816448 139708768151296 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.3521071970462799, loss=2.873112678527832
I0306 02:48:34.633329 139708776544000 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3354242742061615, loss=2.9495656490325928
I0306 02:49:09.436414 139708768151296 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.3204799294471741, loss=2.911113739013672
I0306 02:49:44.237722 139708776544000 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.3267489969730377, loss=2.849297523498535
I0306 02:50:19.046976 139708768151296 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.46488073468208313, loss=2.913684606552124
I0306 02:50:53.869781 139708776544000 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3746377229690552, loss=3.0004518032073975
I0306 02:51:28.668385 139708768151296 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.324179083108902, loss=2.8875842094421387
I0306 02:52:03.536005 139708776544000 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.3308754563331604, loss=2.885315179824829
I0306 02:52:38.318940 139708768151296 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.339610755443573, loss=2.9857075214385986
I0306 02:53:13.130455 139708776544000 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3342207372188568, loss=2.96768856048584
I0306 02:53:47.934747 139708768151296 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.33355939388275146, loss=2.9450976848602295
I0306 02:54:22.739033 139708776544000 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.3441393971443176, loss=2.902230978012085
I0306 02:54:57.551376 139708768151296 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.33964574337005615, loss=2.8977484703063965
I0306 02:55:32.339803 139708776544000 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3630277216434479, loss=2.904672384262085
I0306 02:55:43.470119 139852506842304 spec.py:321] Evaluating on the training split.
I0306 02:55:46.116657 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 02:58:38.770097 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 02:58:41.404540 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:01:12.295892 139852506842304 spec.py:349] Evaluating on the test split.
I0306 03:01:14.927481 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:03:43.208568 139852506842304 submission_runner.py:469] Time since start: 28186.31s, 	Step: 48233, 	{'train/accuracy': 0.640514075756073, 'train/loss': 1.8414708375930786, 'train/bleu': 31.60429646668812, 'validation/accuracy': 0.6651793718338013, 'validation/loss': 1.6687612533569336, 'validation/bleu': 28.65014278320386, 'validation/num_examples': 3000, 'test/accuracy': 0.6791217923164368, 'test/loss': 1.6001701354980469, 'test/bleu': 28.465046636333845, 'test/num_examples': 3003, 'score': 16825.920708179474, 'total_duration': 28186.313233852386, 'accumulated_submission_time': 16825.920708179474, 'accumulated_eval_time': 11357.083236694336, 'accumulated_logging_time': 0.39872312545776367}
I0306 03:03:43.221924 139708768151296 logging_writer.py:48] [48233] accumulated_eval_time=11357.1, accumulated_logging_time=0.398723, accumulated_submission_time=16825.9, global_step=48233, preemption_count=0, score=16825.9, test/accuracy=0.679122, test/bleu=28.465, test/loss=1.60017, test/num_examples=3003, total_duration=28186.3, train/accuracy=0.640514, train/bleu=31.6043, train/loss=1.84147, validation/accuracy=0.665179, validation/bleu=28.6501, validation/loss=1.66876, validation/num_examples=3000
I0306 03:04:06.803284 139708776544000 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.3457602262496948, loss=2.8254153728485107
I0306 03:04:41.484611 139708768151296 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.3956175744533539, loss=3.079411745071411
I0306 03:05:16.253763 139708776544000 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.3793805241584778, loss=2.9059689044952393
I0306 03:05:51.098174 139708768151296 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.4207139015197754, loss=2.9054079055786133
I0306 03:06:25.936981 139708776544000 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3433205187320709, loss=2.916309356689453
I0306 03:07:00.736684 139708768151296 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.33832934498786926, loss=2.9181673526763916
I0306 03:07:35.569100 139708776544000 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.4347470998764038, loss=2.906437635421753
I0306 03:08:10.388559 139708768151296 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.38063278794288635, loss=2.977119207382202
I0306 03:08:45.206284 139708776544000 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3867408037185669, loss=2.9445724487304688
I0306 03:09:20.009784 139708768151296 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3352920413017273, loss=2.903071403503418
I0306 03:09:54.831013 139708776544000 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3554852306842804, loss=2.845322608947754
I0306 03:10:29.645249 139708768151296 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3271274268627167, loss=2.934497594833374
I0306 03:11:04.465406 139708776544000 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3337186276912689, loss=2.8601040840148926
I0306 03:11:39.291910 139708768151296 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.35202792286872864, loss=2.8811869621276855
I0306 03:12:14.112091 139708776544000 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.33723384141921997, loss=2.891420602798462
I0306 03:12:48.950978 139708768151296 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.37937334179878235, loss=2.9221863746643066
I0306 03:13:23.792397 139708776544000 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.36015838384628296, loss=2.913585901260376
I0306 03:13:58.624758 139708768151296 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.35870954394340515, loss=2.9045166969299316
I0306 03:14:33.453007 139708776544000 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.37588849663734436, loss=2.9598379135131836
I0306 03:15:08.282466 139708768151296 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3556934595108032, loss=2.91099214553833
I0306 03:15:43.084469 139708776544000 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.3226061165332794, loss=2.894798755645752
I0306 03:16:17.919346 139708768151296 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.3468528985977173, loss=2.882927894592285
I0306 03:16:52.759156 139708776544000 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.37980443239212036, loss=2.880605936050415
I0306 03:17:27.572926 139708768151296 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3659888505935669, loss=2.9838244915008545
I0306 03:17:43.242613 139852506842304 spec.py:321] Evaluating on the training split.
I0306 03:17:45.895893 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:21:02.201171 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 03:21:04.834732 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:23:45.747161 139852506842304 spec.py:349] Evaluating on the test split.
I0306 03:23:48.390630 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:26:30.735217 139852506842304 submission_runner.py:469] Time since start: 29553.84s, 	Step: 50646, 	{'train/accuracy': 0.6634311676025391, 'train/loss': 1.6947810649871826, 'train/bleu': 32.96347261408606, 'validation/accuracy': 0.6677131652832031, 'validation/loss': 1.6553740501403809, 'validation/bleu': 28.811225517164814, 'validation/num_examples': 3000, 'test/accuracy': 0.6791217923164368, 'test/loss': 1.5924015045166016, 'test/bleu': 28.445858758700652, 'test/num_examples': 3003, 'score': 17665.795039653778, 'total_duration': 29553.83986735344, 'accumulated_submission_time': 17665.795039653778, 'accumulated_eval_time': 11884.57578086853, 'accumulated_logging_time': 0.41984105110168457}
I0306 03:26:30.748722 139708776544000 logging_writer.py:48] [50646] accumulated_eval_time=11884.6, accumulated_logging_time=0.419841, accumulated_submission_time=17665.8, global_step=50646, preemption_count=0, score=17665.8, test/accuracy=0.679122, test/bleu=28.4459, test/loss=1.5924, test/num_examples=3003, total_duration=29553.8, train/accuracy=0.663431, train/bleu=32.9635, train/loss=1.69478, validation/accuracy=0.667713, validation/bleu=28.8112, validation/loss=1.65537, validation/num_examples=3000
I0306 03:26:49.774657 139708768151296 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.3350556194782257, loss=2.959622621536255
I0306 03:27:24.453121 139708776544000 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3352375030517578, loss=2.873406171798706
I0306 03:27:59.240161 139708768151296 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.38087281584739685, loss=2.8797240257263184
I0306 03:28:34.029798 139708776544000 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3646281957626343, loss=2.934540271759033
I0306 03:29:08.825929 139708768151296 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.4429416060447693, loss=2.945587635040283
I0306 03:29:43.660239 139708776544000 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3633762001991272, loss=2.897407293319702
I0306 03:30:18.458013 139708768151296 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.3345698118209839, loss=2.903118133544922
I0306 03:30:53.290900 139708776544000 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.3292419910430908, loss=2.9180054664611816
I0306 03:31:28.089906 139708768151296 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.35851427912712097, loss=2.926267385482788
I0306 03:32:02.916185 139708776544000 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.37357547879219055, loss=2.946397304534912
I0306 03:32:37.708951 139708768151296 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.34421777725219727, loss=2.9636034965515137
I0306 03:33:12.549137 139708776544000 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3436962068080902, loss=2.9122283458709717
I0306 03:33:47.382945 139708768151296 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3260297179222107, loss=2.8781135082244873
I0306 03:34:22.194045 139708776544000 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3335173428058624, loss=2.8955602645874023
I0306 03:34:57.006002 139708768151296 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.340838223695755, loss=2.7961347103118896
I0306 03:35:31.829462 139708776544000 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.37798547744750977, loss=2.897303819656372
I0306 03:36:06.672516 139708768151296 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.31082504987716675, loss=2.9407687187194824
I0306 03:36:41.492773 139708776544000 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.31952953338623047, loss=2.918226718902588
I0306 03:37:16.334375 139708768151296 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3265254497528076, loss=2.9346425533294678
I0306 03:37:51.155112 139708776544000 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.34397822618484497, loss=2.964142084121704
I0306 03:38:26.018172 139708768151296 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3579803705215454, loss=2.8956165313720703
I0306 03:39:00.840185 139708776544000 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3668482303619385, loss=2.883151054382324
I0306 03:39:35.640082 139708768151296 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3413514196872711, loss=2.9145987033843994
I0306 03:40:10.436554 139708776544000 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.3627276122570038, loss=2.8806326389312744
I0306 03:40:30.983278 139852506842304 spec.py:321] Evaluating on the training split.
I0306 03:40:33.628973 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:44:09.644191 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 03:44:12.276759 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:46:47.955978 139852506842304 spec.py:349] Evaluating on the test split.
I0306 03:46:50.598021 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 03:49:19.481601 139852506842304 submission_runner.py:469] Time since start: 30922.59s, 	Step: 53060, 	{'train/accuracy': 0.6471867561340332, 'train/loss': 1.7907124757766724, 'train/bleu': 31.524997127136846, 'validation/accuracy': 0.6661063432693481, 'validation/loss': 1.6556403636932373, 'validation/bleu': 28.577419952640984, 'validation/num_examples': 3000, 'test/accuracy': 0.6812304258346558, 'test/loss': 1.5828485488891602, 'test/bleu': 28.539773456855333, 'test/num_examples': 3003, 'score': 18505.883224487305, 'total_duration': 30922.586253881454, 'accumulated_submission_time': 18505.883224487305, 'accumulated_eval_time': 12413.074045658112, 'accumulated_logging_time': 0.44127511978149414}
I0306 03:49:19.495468 139708768151296 logging_writer.py:48] [53060] accumulated_eval_time=12413.1, accumulated_logging_time=0.441275, accumulated_submission_time=18505.9, global_step=53060, preemption_count=0, score=18505.9, test/accuracy=0.68123, test/bleu=28.5398, test/loss=1.58285, test/num_examples=3003, total_duration=30922.6, train/accuracy=0.647187, train/bleu=31.525, train/loss=1.79071, validation/accuracy=0.666106, validation/bleu=28.5774, validation/loss=1.65564, validation/num_examples=3000
I0306 03:49:33.673978 139708776544000 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.3923453688621521, loss=2.953869104385376
I0306 03:50:08.321051 139708768151296 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.3435121178627014, loss=2.960846424102783
I0306 03:50:43.054392 139708776544000 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.32127153873443604, loss=2.846557140350342
I0306 03:51:17.830673 139708768151296 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.34925517439842224, loss=2.8658382892608643
I0306 03:51:52.634542 139708776544000 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.36070379614830017, loss=2.9235854148864746
I0306 03:52:27.417221 139708768151296 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.3452093303203583, loss=2.9549803733825684
I0306 03:53:02.226734 139708776544000 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3489585220813751, loss=2.854795217514038
I0306 03:53:37.011684 139708768151296 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.35435208678245544, loss=2.8891348838806152
I0306 03:54:11.819667 139708776544000 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.3984583914279938, loss=2.87491774559021
I0306 03:54:46.636027 139708768151296 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3201953172683716, loss=2.826716899871826
I0306 03:55:21.471452 139708776544000 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.33849793672561646, loss=2.8733620643615723
I0306 03:55:56.283923 139708768151296 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.3344199061393738, loss=2.943565607070923
I0306 03:56:31.106651 139708776544000 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3170901834964752, loss=2.954559564590454
I0306 03:57:05.944613 139708768151296 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.32638442516326904, loss=2.83935284614563
I0306 03:57:40.752814 139708776544000 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.32440516352653503, loss=2.887342691421509
I0306 03:58:15.535602 139708768151296 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3350539207458496, loss=2.8408255577087402
I0306 03:58:50.310098 139708776544000 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.4071899950504303, loss=2.880662202835083
I0306 03:59:25.132241 139708768151296 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3617594838142395, loss=2.892515182495117
I0306 03:59:59.944316 139708776544000 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.32193753123283386, loss=2.8755781650543213
I0306 04:00:34.760965 139708768151296 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.340295672416687, loss=2.922985553741455
I0306 04:01:09.597710 139708776544000 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.3529622554779053, loss=2.8459184169769287
I0306 04:01:44.363172 139708768151296 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.414550244808197, loss=2.90036678314209
I0306 04:02:19.222584 139708776544000 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3253209590911865, loss=2.8264782428741455
I0306 04:02:54.065802 139708768151296 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.4047059416770935, loss=2.828138828277588
I0306 04:03:19.492859 139852506842304 spec.py:321] Evaluating on the training split.
I0306 04:03:22.146536 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:06:36.723073 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 04:06:39.351370 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:09:30.068208 139852506842304 spec.py:349] Evaluating on the test split.
I0306 04:09:32.703173 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:12:07.841277 139852506842304 submission_runner.py:469] Time since start: 32290.95s, 	Step: 55474, 	{'train/accuracy': 0.6478999257087708, 'train/loss': 1.7947591543197632, 'train/bleu': 31.104204555931076, 'validation/accuracy': 0.6690356731414795, 'validation/loss': 1.6471420526504517, 'validation/bleu': 29.00787699840739, 'validation/num_examples': 3000, 'test/accuracy': 0.6815664768218994, 'test/loss': 1.574040174484253, 'test/bleu': 28.57670685535395, 'test/num_examples': 3003, 'score': 19345.73685193062, 'total_duration': 32290.945919275284, 'accumulated_submission_time': 19345.73685193062, 'accumulated_eval_time': 12941.422396421432, 'accumulated_logging_time': 0.46326398849487305}
I0306 04:12:07.855293 139708776544000 logging_writer.py:48] [55474] accumulated_eval_time=12941.4, accumulated_logging_time=0.463264, accumulated_submission_time=19345.7, global_step=55474, preemption_count=0, score=19345.7, test/accuracy=0.681566, test/bleu=28.5767, test/loss=1.57404, test/num_examples=3003, total_duration=32290.9, train/accuracy=0.6479, train/bleu=31.1042, train/loss=1.79476, validation/accuracy=0.669036, validation/bleu=29.0079, validation/loss=1.64714, validation/num_examples=3000
I0306 04:12:17.197554 139708768151296 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.38214197754859924, loss=2.934370279312134
I0306 04:12:51.873880 139708776544000 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3376571536064148, loss=2.8921923637390137
I0306 04:13:26.640445 139708768151296 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3345727324485779, loss=2.964212656021118
I0306 04:14:01.445223 139708776544000 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.39536479115486145, loss=2.8682236671447754
I0306 04:14:36.234203 139708768151296 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.33848801255226135, loss=2.90490984916687
I0306 04:15:11.027367 139708776544000 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.35762259364128113, loss=2.812680721282959
I0306 04:15:45.856531 139708768151296 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3813701570034027, loss=2.8986709117889404
I0306 04:16:20.665551 139708776544000 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.36133915185928345, loss=2.86024808883667
I0306 04:16:55.493850 139708768151296 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.35496583580970764, loss=2.869786262512207
I0306 04:17:30.330483 139708776544000 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.35010725259780884, loss=2.8933491706848145
I0306 04:18:05.142850 139708768151296 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.34675538539886475, loss=2.8913474082946777
I0306 04:18:39.963542 139708776544000 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.35447707772254944, loss=2.842280387878418
I0306 04:19:14.759173 139708768151296 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.3307941257953644, loss=2.867105484008789
I0306 04:19:49.567235 139708776544000 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.34481483697891235, loss=2.8780357837677
I0306 04:20:24.401602 139708768151296 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.4364378750324249, loss=2.8936426639556885
I0306 04:20:59.246932 139708776544000 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3680420517921448, loss=2.8316025733947754
I0306 04:21:34.055819 139708768151296 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.35272523760795593, loss=2.9107377529144287
I0306 04:22:08.861339 139708776544000 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3394260108470917, loss=2.918363571166992
I0306 04:22:43.660141 139708768151296 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.33066967129707336, loss=2.816422462463379
I0306 04:23:18.465203 139708776544000 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3563816547393799, loss=2.8266754150390625
I0306 04:23:53.305855 139708768151296 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3659830093383789, loss=2.956271171569824
I0306 04:24:28.114014 139708776544000 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3602551221847534, loss=2.8536365032196045
I0306 04:25:02.914989 139708768151296 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.4119327962398529, loss=2.9081199169158936
I0306 04:25:37.745756 139708776544000 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.4103376865386963, loss=2.8495686054229736
I0306 04:26:08.063984 139852506842304 spec.py:321] Evaluating on the training split.
I0306 04:26:10.716400 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:29:21.497483 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 04:29:24.121749 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:31:57.625273 139852506842304 spec.py:349] Evaluating on the test split.
I0306 04:32:00.260983 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:34:33.831225 139852506842304 submission_runner.py:469] Time since start: 33636.94s, 	Step: 57888, 	{'train/accuracy': 0.6565040349960327, 'train/loss': 1.7403749227523804, 'train/bleu': 32.182956096505784, 'validation/accuracy': 0.6694188714027405, 'validation/loss': 1.6355479955673218, 'validation/bleu': 28.777596005391874, 'validation/num_examples': 3000, 'test/accuracy': 0.6819719672203064, 'test/loss': 1.5685808658599854, 'test/bleu': 28.657735946323896, 'test/num_examples': 3003, 'score': 20185.798642635345, 'total_duration': 33636.93588399887, 'accumulated_submission_time': 20185.798642635345, 'accumulated_eval_time': 13447.189587593079, 'accumulated_logging_time': 0.48529982566833496}
I0306 04:34:33.846690 139708768151296 logging_writer.py:48] [57888] accumulated_eval_time=13447.2, accumulated_logging_time=0.4853, accumulated_submission_time=20185.8, global_step=57888, preemption_count=0, score=20185.8, test/accuracy=0.681972, test/bleu=28.6577, test/loss=1.56858, test/num_examples=3003, total_duration=33636.9, train/accuracy=0.656504, train/bleu=32.183, train/loss=1.74037, validation/accuracy=0.669419, validation/bleu=28.7776, validation/loss=1.63555, validation/num_examples=3000
I0306 04:34:38.360381 139708776544000 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.3656238913536072, loss=2.7991340160369873
I0306 04:35:13.008655 139708768151296 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.35677096247673035, loss=2.847841501235962
I0306 04:35:47.745522 139708776544000 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.35329464077949524, loss=2.9862587451934814
I0306 04:36:22.504965 139708768151296 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3432871997356415, loss=2.8306009769439697
I0306 04:36:57.314982 139708776544000 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.32439425587654114, loss=2.8231427669525146
I0306 04:37:32.085156 139708768151296 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.3462330102920532, loss=2.901479959487915
I0306 04:38:06.898640 139708776544000 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.3521372675895691, loss=2.84002947807312
I0306 04:38:41.747644 139708768151296 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.34684649109840393, loss=2.8454723358154297
I0306 04:39:16.538740 139708776544000 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.33245334029197693, loss=2.8556604385375977
I0306 04:39:51.359996 139708768151296 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.32958635687828064, loss=2.881012439727783
I0306 04:40:26.172365 139708776544000 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3596416711807251, loss=2.944857358932495
I0306 04:41:00.996457 139708768151296 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3527882695198059, loss=2.889826536178589
I0306 04:41:35.800284 139708776544000 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.37178871035575867, loss=2.9251317977905273
I0306 04:42:10.633454 139708768151296 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.36805370450019836, loss=2.9096338748931885
I0306 04:42:45.444509 139708776544000 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.355431467294693, loss=2.870049476623535
I0306 04:43:20.212519 139708768151296 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3680480122566223, loss=2.8256757259368896
I0306 04:43:55.031258 139708776544000 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3211716413497925, loss=2.9022698402404785
I0306 04:44:29.859831 139708768151296 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3783941864967346, loss=2.962519884109497
I0306 04:45:04.682421 139708776544000 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.35408860445022583, loss=2.936528444290161
I0306 04:45:39.489258 139708768151296 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.4131745398044586, loss=2.904069423675537
I0306 04:46:14.313933 139708776544000 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.36361172795295715, loss=2.8476741313934326
I0306 04:46:49.138354 139708768151296 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.37810441851615906, loss=2.9128968715667725
I0306 04:47:23.949268 139708776544000 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.41711878776550293, loss=2.8571083545684814
I0306 04:47:58.753798 139708768151296 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.33486127853393555, loss=2.894699811935425
I0306 04:48:33.565989 139708776544000 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.38312631845474243, loss=2.9162981510162354
I0306 04:48:33.919134 139852506842304 spec.py:321] Evaluating on the training split.
I0306 04:48:36.565268 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:51:24.915711 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 04:51:27.550245 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:54:08.050895 139852506842304 spec.py:349] Evaluating on the test split.
I0306 04:54:10.688295 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 04:56:50.472928 139852506842304 submission_runner.py:469] Time since start: 34973.58s, 	Step: 60302, 	{'train/accuracy': 0.6491498947143555, 'train/loss': 1.7927241325378418, 'train/bleu': 32.08753312460676, 'validation/accuracy': 0.6715200543403625, 'validation/loss': 1.6254328489303589, 'validation/bleu': 29.2614829180244, 'validation/num_examples': 3000, 'test/accuracy': 0.6852856278419495, 'test/loss': 1.5561716556549072, 'test/bleu': 29.075932837787565, 'test/num_examples': 3003, 'score': 21025.7261698246, 'total_duration': 34973.57757258415, 'accumulated_submission_time': 21025.7261698246, 'accumulated_eval_time': 13943.74331855774, 'accumulated_logging_time': 0.5088150501251221}
I0306 04:56:50.487333 139708768151296 logging_writer.py:48] [60302] accumulated_eval_time=13943.7, accumulated_logging_time=0.508815, accumulated_submission_time=21025.7, global_step=60302, preemption_count=0, score=21025.7, test/accuracy=0.685286, test/bleu=29.0759, test/loss=1.55617, test/num_examples=3003, total_duration=34973.6, train/accuracy=0.64915, train/bleu=32.0875, train/loss=1.79272, validation/accuracy=0.67152, validation/bleu=29.2615, validation/loss=1.62543, validation/num_examples=3000
I0306 04:57:24.731820 139708776544000 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.32119205594062805, loss=2.844714641571045
I0306 04:57:59.435756 139708768151296 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3546062707901001, loss=2.877915382385254
I0306 04:58:34.184645 139708776544000 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.3557984530925751, loss=2.905911922454834
I0306 04:59:08.954413 139708768151296 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3573567867279053, loss=2.8601012229919434
I0306 04:59:43.736918 139708776544000 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.3731616139411926, loss=2.8589868545532227
I0306 05:00:18.543791 139708768151296 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.33875444531440735, loss=2.8737640380859375
I0306 05:00:53.370528 139708776544000 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3459615111351013, loss=2.864978551864624
I0306 05:01:28.180547 139708768151296 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.33289268612861633, loss=2.8815696239471436
I0306 05:02:02.997354 139708776544000 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3241172730922699, loss=2.819762706756592
I0306 05:02:37.775525 139708768151296 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.32181018590927124, loss=2.8962059020996094
I0306 05:03:12.608690 139708776544000 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.3424404561519623, loss=2.8446388244628906
I0306 05:03:47.376147 139708768151296 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.32697877287864685, loss=2.8527681827545166
I0306 05:04:22.192951 139708776544000 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.36520230770111084, loss=2.877481460571289
I0306 05:04:56.991183 139708768151296 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3377227783203125, loss=2.842007875442505
I0306 05:05:31.777443 139708776544000 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3274522125720978, loss=2.7492189407348633
I0306 05:06:06.596788 139708768151296 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3314093351364136, loss=2.871757745742798
I0306 05:06:41.406997 139708776544000 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.38587889075279236, loss=2.823838233947754
I0306 05:07:16.217656 139708768151296 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.3387924134731293, loss=2.859128475189209
I0306 05:07:51.012456 139708776544000 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.3666301369667053, loss=2.880984306335449
I0306 05:08:25.829696 139708768151296 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.34612351655960083, loss=2.8459436893463135
I0306 05:09:00.619158 139708776544000 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3559151291847229, loss=2.8607006072998047
I0306 05:09:35.412136 139708768151296 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3594608008861542, loss=2.9279391765594482
I0306 05:10:10.208600 139708776544000 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3609476685523987, loss=2.9012601375579834
I0306 05:10:45.034649 139708768151296 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.33761411905288696, loss=2.8623805046081543
I0306 05:10:50.620080 139852506842304 spec.py:321] Evaluating on the training split.
I0306 05:10:53.262721 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 05:14:40.386918 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 05:14:43.014380 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 05:17:28.645277 139852506842304 spec.py:349] Evaluating on the test split.
I0306 05:17:31.278812 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 05:20:19.293346 139852506842304 submission_runner.py:469] Time since start: 36382.40s, 	Step: 62717, 	{'train/accuracy': 0.6520125865936279, 'train/loss': 1.7698822021484375, 'train/bleu': 31.919736707299073, 'validation/accuracy': 0.6724346876144409, 'validation/loss': 1.6232401132583618, 'validation/bleu': 29.31364298681988, 'validation/num_examples': 3000, 'test/accuracy': 0.6854941248893738, 'test/loss': 1.5551882982254028, 'test/bleu': 28.546150413983344, 'test/num_examples': 3003, 'score': 21865.714310646057, 'total_duration': 36382.397992134094, 'accumulated_submission_time': 21865.714310646057, 'accumulated_eval_time': 14512.416515827179, 'accumulated_logging_time': 0.5313296318054199}
I0306 05:20:19.308054 139708776544000 logging_writer.py:48] [62717] accumulated_eval_time=14512.4, accumulated_logging_time=0.53133, accumulated_submission_time=21865.7, global_step=62717, preemption_count=0, score=21865.7, test/accuracy=0.685494, test/bleu=28.5462, test/loss=1.55519, test/num_examples=3003, total_duration=36382.4, train/accuracy=0.652013, train/bleu=31.9197, train/loss=1.76988, validation/accuracy=0.672435, validation/bleu=29.3136, validation/loss=1.62324, validation/num_examples=3000
I0306 05:20:48.407547 139708768151296 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.3455252945423126, loss=2.8962411880493164
I0306 05:21:23.132019 139708776544000 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.35211309790611267, loss=2.8469624519348145
I0306 05:21:57.892158 139708768151296 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3401343524456024, loss=2.8265745639801025
I0306 05:22:32.727854 139708776544000 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.34410589933395386, loss=2.8665990829467773
I0306 05:23:07.579130 139708768151296 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.35100558400154114, loss=2.8227641582489014
I0306 05:23:42.374435 139708776544000 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.4022301733493805, loss=2.881117343902588
I0306 05:24:17.188708 139708768151296 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.35162076354026794, loss=2.823573112487793
I0306 05:24:52.004515 139708776544000 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3674849271774292, loss=2.806467056274414
I0306 05:25:26.783612 139708768151296 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.6184937357902527, loss=2.9153904914855957
I0306 05:26:01.576556 139708776544000 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.35767194628715515, loss=2.794306516647339
I0306 05:26:36.389197 139708768151296 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3607042729854584, loss=2.884110927581787
I0306 05:27:11.198267 139708776544000 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.40404507517814636, loss=2.9006998538970947
I0306 05:27:46.012491 139708768151296 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.34404104948043823, loss=2.829352378845215
I0306 05:28:20.792380 139708776544000 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.33957523107528687, loss=2.8012802600860596
I0306 05:28:55.610070 139708768151296 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.33549004793167114, loss=2.8065948486328125
I0306 05:29:30.427905 139708776544000 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3729992210865021, loss=2.8892526626586914
I0306 05:30:05.238269 139708768151296 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3251029849052429, loss=2.8250386714935303
I0306 05:30:40.064207 139708776544000 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.3526952266693115, loss=2.9382569789886475
I0306 05:31:14.847676 139708768151296 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3419208228588104, loss=2.8421216011047363
I0306 05:31:49.662524 139708776544000 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3455306887626648, loss=2.8523569107055664
I0306 05:32:24.490600 139708768151296 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.353355348110199, loss=2.8174960613250732
I0306 05:32:59.291329 139708776544000 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.366810142993927, loss=2.868535280227661
I0306 05:33:34.076741 139708768151296 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.37829428911209106, loss=2.8112409114837646
I0306 05:34:08.914706 139708776544000 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.334437757730484, loss=2.8699264526367188
I0306 05:34:19.371523 139852506842304 spec.py:321] Evaluating on the training split.
I0306 05:34:22.018973 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 05:37:11.469862 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 05:37:14.116643 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 05:39:45.682226 139852506842304 spec.py:349] Evaluating on the test split.
I0306 05:39:48.319547 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 05:42:08.850700 139852506842304 submission_runner.py:469] Time since start: 37691.96s, 	Step: 65131, 	{'train/accuracy': 0.6570355296134949, 'train/loss': 1.7334505319595337, 'train/bleu': 32.422927077129856, 'validation/accuracy': 0.6735841631889343, 'validation/loss': 1.6154221296310425, 'validation/bleu': 29.122842781581575, 'validation/num_examples': 3000, 'test/accuracy': 0.6868960857391357, 'test/loss': 1.5414601564407349, 'test/bleu': 29.048349351694977, 'test/num_examples': 3003, 'score': 22705.63618540764, 'total_duration': 37691.95536565781, 'accumulated_submission_time': 22705.63618540764, 'accumulated_eval_time': 14981.89564538002, 'accumulated_logging_time': 0.5537524223327637}
I0306 05:42:08.866449 139708768151296 logging_writer.py:48] [65131] accumulated_eval_time=14981.9, accumulated_logging_time=0.553752, accumulated_submission_time=22705.6, global_step=65131, preemption_count=0, score=22705.6, test/accuracy=0.686896, test/bleu=29.0483, test/loss=1.54146, test/num_examples=3003, total_duration=37692, train/accuracy=0.657036, train/bleu=32.4229, train/loss=1.73345, validation/accuracy=0.673584, validation/bleu=29.1228, validation/loss=1.61542, validation/num_examples=3000
I0306 05:42:33.124970 139708776544000 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3718489706516266, loss=2.8945088386535645
I0306 05:43:07.828075 139708768151296 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.3338521420955658, loss=2.8873300552368164
I0306 05:43:42.621625 139708776544000 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3373551070690155, loss=2.8700225353240967
I0306 05:44:17.437736 139708768151296 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.35642504692077637, loss=2.881999969482422
I0306 05:44:52.241661 139708776544000 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.35512059926986694, loss=2.8251559734344482
I0306 05:45:27.052460 139708768151296 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.3891570270061493, loss=2.9003992080688477
I0306 05:46:01.882303 139708776544000 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.38651400804519653, loss=2.881998300552368
I0306 05:46:36.717012 139708768151296 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.36187782883644104, loss=2.8350789546966553
I0306 05:47:11.548170 139708776544000 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.33882004022598267, loss=2.7981936931610107
I0306 05:47:46.349595 139708768151296 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3210640549659729, loss=2.8277297019958496
I0306 05:48:21.182115 139708776544000 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.39378055930137634, loss=2.845649003982544
I0306 05:48:56.008981 139708768151296 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.33273786306381226, loss=2.861898183822632
I0306 05:49:30.818078 139708776544000 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.32539889216423035, loss=2.8111953735351562
I0306 05:50:05.606691 139708768151296 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3231584131717682, loss=2.828803777694702
I0306 05:50:40.413454 139708776544000 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.3388082981109619, loss=2.8733057975769043
I0306 05:51:15.220026 139708768151296 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.35132265090942383, loss=2.847790002822876
I0306 05:51:50.049388 139708776544000 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3453379273414612, loss=2.9046249389648438
I0306 05:52:24.866762 139708768151296 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.369564026594162, loss=2.825911521911621
I0306 05:52:59.682903 139708776544000 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3792460262775421, loss=2.835545539855957
I0306 05:53:34.492417 139708768151296 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3387790322303772, loss=2.8529131412506104
I0306 05:54:09.317606 139708776544000 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.34658321738243103, loss=2.7812206745147705
I0306 05:54:44.149683 139708768151296 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.33059191703796387, loss=2.887197256088257
I0306 05:55:19.012856 139708776544000 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3602887690067291, loss=2.901503801345825
I0306 05:55:53.998069 139708768151296 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.38518452644348145, loss=2.851123094558716
I0306 05:56:08.976122 139852506842304 spec.py:321] Evaluating on the training split.
I0306 05:56:11.628569 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 05:59:04.451049 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 05:59:07.087466 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:01:35.844240 139852506842304 spec.py:349] Evaluating on the test split.
I0306 06:01:38.480678 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:04:05.937628 139852506842304 submission_runner.py:469] Time since start: 39009.04s, 	Step: 67544, 	{'train/accuracy': 0.6577116847038269, 'train/loss': 1.7310400009155273, 'train/bleu': 32.39182963686128, 'validation/accuracy': 0.675326943397522, 'validation/loss': 1.6035248041152954, 'validation/bleu': 29.40856387984765, 'validation/num_examples': 3000, 'test/accuracy': 0.6904646158218384, 'test/loss': 1.5232166051864624, 'test/bleu': 29.572496398948413, 'test/num_examples': 3003, 'score': 23545.597474098206, 'total_duration': 39009.042259931564, 'accumulated_submission_time': 23545.597474098206, 'accumulated_eval_time': 15458.857070207596, 'accumulated_logging_time': 0.5776288509368896}
I0306 06:04:05.953089 139708776544000 logging_writer.py:48] [67544] accumulated_eval_time=15458.9, accumulated_logging_time=0.577629, accumulated_submission_time=23545.6, global_step=67544, preemption_count=0, score=23545.6, test/accuracy=0.690465, test/bleu=29.5725, test/loss=1.52322, test/num_examples=3003, total_duration=39009, train/accuracy=0.657712, train/bleu=32.3918, train/loss=1.73104, validation/accuracy=0.675327, validation/bleu=29.4086, validation/loss=1.60352, validation/num_examples=3000
I0306 06:04:25.688492 139708768151296 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.3567633032798767, loss=2.9216456413269043
I0306 06:05:00.365381 139708776544000 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3420132100582123, loss=2.867689371109009
I0306 06:05:35.100365 139708768151296 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.34790775179862976, loss=2.7917628288269043
I0306 06:06:09.868369 139708776544000 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.356953889131546, loss=2.8573904037475586
I0306 06:06:44.650107 139708768151296 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3578079342842102, loss=2.9173738956451416
I0306 06:07:19.471764 139708776544000 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.376983106136322, loss=2.8316993713378906
I0306 06:07:54.259724 139708768151296 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.384852796792984, loss=2.8108694553375244
I0306 06:08:29.058422 139708776544000 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3782191872596741, loss=2.8573601245880127
I0306 06:09:03.875614 139708768151296 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3357545733451843, loss=2.868741750717163
I0306 06:09:38.668124 139708776544000 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.35133615136146545, loss=2.857099771499634
I0306 06:10:13.467362 139708768151296 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.3432044982910156, loss=2.795135974884033
I0306 06:10:48.301392 139708776544000 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.3491228520870209, loss=2.809814929962158
I0306 06:11:23.102427 139708768151296 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.34525585174560547, loss=2.7824819087982178
I0306 06:11:57.918360 139708776544000 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.39304307103157043, loss=2.837693214416504
I0306 06:12:32.734522 139708768151296 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.34763985872268677, loss=2.8474247455596924
I0306 06:13:07.571877 139708776544000 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.4260786771774292, loss=2.8919997215270996
I0306 06:13:42.412773 139708768151296 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.33443689346313477, loss=2.7602691650390625
I0306 06:14:17.227620 139708776544000 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3592921197414398, loss=2.9210007190704346
I0306 06:14:52.039620 139708768151296 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.34787946939468384, loss=2.8155033588409424
I0306 06:15:26.862970 139708776544000 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.3620649576187134, loss=2.834482192993164
I0306 06:16:01.678847 139708768151296 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3560910224914551, loss=2.851228952407837
I0306 06:16:36.585773 139708776544000 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3440920412540436, loss=2.860517740249634
I0306 06:17:11.406759 139708768151296 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3435618281364441, loss=2.810523748397827
I0306 06:17:46.245975 139708776544000 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.36553192138671875, loss=2.8355932235717773
I0306 06:18:06.116561 139852506842304 spec.py:321] Evaluating on the training split.
I0306 06:18:08.760812 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:21:23.245898 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 06:21:25.883780 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:24:04.013020 139852506842304 spec.py:349] Evaluating on the test split.
I0306 06:24:06.658023 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:26:34.804041 139852506842304 submission_runner.py:469] Time since start: 40357.91s, 	Step: 69958, 	{'train/accuracy': 0.6651302576065063, 'train/loss': 1.6812773942947388, 'train/bleu': 33.31203898848172, 'validation/accuracy': 0.6760562062263489, 'validation/loss': 1.5955429077148438, 'validation/bleu': 29.334678551980925, 'validation/num_examples': 3000, 'test/accuracy': 0.6904529929161072, 'test/loss': 1.523226022720337, 'test/bleu': 29.406920001466755, 'test/num_examples': 3003, 'score': 24385.615493535995, 'total_duration': 40357.90869808197, 'accumulated_submission_time': 24385.615493535995, 'accumulated_eval_time': 15967.544497013092, 'accumulated_logging_time': 0.6013941764831543}
I0306 06:26:34.820385 139708768151296 logging_writer.py:48] [69958] accumulated_eval_time=15967.5, accumulated_logging_time=0.601394, accumulated_submission_time=24385.6, global_step=69958, preemption_count=0, score=24385.6, test/accuracy=0.690453, test/bleu=29.4069, test/loss=1.52323, test/num_examples=3003, total_duration=40357.9, train/accuracy=0.66513, train/bleu=33.312, train/loss=1.68128, validation/accuracy=0.676056, validation/bleu=29.3347, validation/loss=1.59554, validation/num_examples=3000
I0306 06:26:49.740336 139708776544000 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3453405201435089, loss=2.8274598121643066
I0306 06:27:24.426797 139708768151296 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3726601302623749, loss=2.8937714099884033
I0306 06:27:59.195014 139708776544000 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.35229647159576416, loss=2.8223671913146973
I0306 06:28:33.982858 139708768151296 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3747880458831787, loss=2.827467203140259
I0306 06:29:08.802987 139708776544000 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.3491131365299225, loss=2.88726806640625
I0306 06:29:43.606745 139708768151296 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.34787532687187195, loss=2.8527276515960693
I0306 06:30:18.445257 139708776544000 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.3818033039569855, loss=2.8161048889160156
I0306 06:30:53.279403 139708768151296 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.34764209389686584, loss=2.7594637870788574
I0306 06:31:28.162126 139708776544000 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.3641519844532013, loss=2.804666042327881
I0306 06:32:02.984155 139708768151296 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.33456316590309143, loss=2.8081564903259277
I0306 06:32:37.835783 139708776544000 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.36533376574516296, loss=2.82283878326416
I0306 06:33:12.632762 139708768151296 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.35557129979133606, loss=2.8944461345672607
I0306 06:33:47.453294 139708776544000 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.35192641615867615, loss=2.849351167678833
I0306 06:34:22.301915 139708768151296 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.3764631450176239, loss=2.8751027584075928
I0306 06:34:57.151026 139708776544000 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3439199924468994, loss=2.859905242919922
I0306 06:35:31.968526 139708768151296 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.36063194274902344, loss=2.798264741897583
I0306 06:36:06.827716 139708776544000 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.3627855181694031, loss=2.9062061309814453
I0306 06:36:41.652025 139708768151296 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.40679922699928284, loss=2.846285104751587
I0306 06:37:16.488117 139708776544000 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.39914172887802124, loss=2.8667259216308594
I0306 06:37:51.292539 139708768151296 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.363054096698761, loss=2.808852195739746
I0306 06:38:26.150907 139708776544000 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3538297414779663, loss=2.8158719539642334
I0306 06:39:00.951640 139708768151296 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3448878228664398, loss=2.756505250930786
I0306 06:39:35.788427 139708776544000 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.35464829206466675, loss=2.8149218559265137
I0306 06:40:10.604985 139708768151296 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3997538685798645, loss=2.8550872802734375
I0306 06:40:34.811071 139852506842304 spec.py:321] Evaluating on the training split.
I0306 06:40:37.460709 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:43:21.607482 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 06:43:24.247840 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:45:58.606167 139852506842304 spec.py:349] Evaluating on the test split.
I0306 06:46:01.264156 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 06:48:22.681765 139852506842304 submission_runner.py:469] Time since start: 41665.79s, 	Step: 72370, 	{'train/accuracy': 0.6614954471588135, 'train/loss': 1.7066394090652466, 'train/bleu': 32.613614038973076, 'validation/accuracy': 0.6786146759986877, 'validation/loss': 1.5909779071807861, 'validation/bleu': 29.503945444659728, 'validation/num_examples': 3000, 'test/accuracy': 0.6916927695274353, 'test/loss': 1.5103695392608643, 'test/bleu': 29.64421645028372, 'test/num_examples': 3003, 'score': 25225.462176561356, 'total_duration': 41665.78641748428, 'accumulated_submission_time': 25225.462176561356, 'accumulated_eval_time': 16435.415130615234, 'accumulated_logging_time': 0.6256895065307617}
I0306 06:48:22.700602 139708776544000 logging_writer.py:48] [72370] accumulated_eval_time=16435.4, accumulated_logging_time=0.62569, accumulated_submission_time=25225.5, global_step=72370, preemption_count=0, score=25225.5, test/accuracy=0.691693, test/bleu=29.6442, test/loss=1.51037, test/num_examples=3003, total_duration=41665.8, train/accuracy=0.661495, train/bleu=32.6136, train/loss=1.70664, validation/accuracy=0.678615, validation/bleu=29.5039, validation/loss=1.59098, validation/num_examples=3000
I0306 06:48:33.484483 139708768151296 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3358202874660492, loss=2.8436119556427
I0306 06:49:08.312000 139708776544000 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3502023220062256, loss=2.84869122505188
I0306 06:49:43.210365 139708768151296 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.37046608328819275, loss=2.833667278289795
I0306 06:50:18.166067 139708776544000 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.3639330267906189, loss=2.7620222568511963
I0306 06:50:53.106471 139708768151296 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.36720192432403564, loss=2.8106822967529297
I0306 06:51:28.013430 139708776544000 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3840997517108917, loss=2.875493288040161
I0306 06:52:02.967218 139708768151296 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3468814194202423, loss=2.8386476039886475
I0306 06:52:37.891957 139708776544000 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.38646528124809265, loss=2.8048784732818604
I0306 06:53:12.831917 139708768151296 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.43379178643226624, loss=2.837721347808838
I0306 06:53:47.784556 139708776544000 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.3578364849090576, loss=2.8301098346710205
I0306 06:54:22.722215 139708768151296 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3732720911502838, loss=2.825523614883423
I0306 06:54:57.690908 139708776544000 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.37351855635643005, loss=2.8276302814483643
I0306 06:55:32.634899 139708768151296 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.38255438208580017, loss=2.8376548290252686
I0306 06:56:07.597682 139708776544000 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3764553666114807, loss=2.8262369632720947
I0306 06:56:42.565924 139708768151296 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3464011549949646, loss=2.8052830696105957
I0306 06:57:17.511147 139708776544000 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3849351108074188, loss=2.7957990169525146
I0306 06:57:52.493278 139708768151296 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3655690848827362, loss=2.7537569999694824
I0306 06:58:27.447966 139708776544000 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3620710074901581, loss=2.803403854370117
I0306 06:59:02.398687 139708768151296 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.34261617064476013, loss=2.8402256965637207
I0306 06:59:37.327960 139708776544000 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.35584917664527893, loss=2.8273160457611084
I0306 07:00:12.291161 139708768151296 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3688449263572693, loss=2.7400782108306885
I0306 07:00:47.244851 139708776544000 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3872762620449066, loss=2.803406000137329
I0306 07:01:22.209547 139708768151296 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.3726919889450073, loss=2.81683349609375
I0306 07:01:57.155075 139708776544000 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.35640931129455566, loss=2.8037588596343994
I0306 07:02:22.986046 139852506842304 spec.py:321] Evaluating on the training split.
I0306 07:02:25.645797 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:05:23.026360 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 07:05:25.678930 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:07:59.405530 139852506842304 spec.py:349] Evaluating on the test split.
I0306 07:08:02.052512 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:10:35.384583 139852506842304 submission_runner.py:469] Time since start: 42998.49s, 	Step: 74775, 	{'train/accuracy': 0.657471776008606, 'train/loss': 1.7333810329437256, 'train/bleu': 33.01413835298889, 'validation/accuracy': 0.6799001693725586, 'validation/loss': 1.5785874128341675, 'validation/bleu': 29.793749124454397, 'validation/num_examples': 3000, 'test/accuracy': 0.694206953048706, 'test/loss': 1.5045973062515259, 'test/bleu': 29.818785440213592, 'test/num_examples': 3003, 'score': 26065.604117393494, 'total_duration': 42998.489241838455, 'accumulated_submission_time': 26065.604117393494, 'accumulated_eval_time': 16927.813620090485, 'accumulated_logging_time': 0.6527254581451416}
I0306 07:10:35.401966 139708768151296 logging_writer.py:48] [74775] accumulated_eval_time=16927.8, accumulated_logging_time=0.652725, accumulated_submission_time=26065.6, global_step=74775, preemption_count=0, score=26065.6, test/accuracy=0.694207, test/bleu=29.8188, test/loss=1.5046, test/num_examples=3003, total_duration=42998.5, train/accuracy=0.657472, train/bleu=33.0141, train/loss=1.73338, validation/accuracy=0.6799, validation/bleu=29.7937, validation/loss=1.57859, validation/num_examples=3000
I0306 07:10:44.448987 139708776544000 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.37021371722221375, loss=2.7994399070739746
I0306 07:11:19.250153 139708768151296 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3568388521671295, loss=2.7632832527160645
I0306 07:11:54.113462 139708776544000 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3801717758178711, loss=2.7986254692077637
I0306 07:12:29.004884 139708768151296 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.391851007938385, loss=2.8764448165893555
I0306 07:13:03.951186 139708776544000 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3501697778701782, loss=2.7920243740081787
I0306 07:13:38.874922 139708768151296 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.3922082185745239, loss=2.8569979667663574
I0306 07:14:13.790305 139708776544000 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.3925720751285553, loss=2.83318829536438
I0306 07:14:48.741044 139708768151296 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3866223990917206, loss=2.8058388233184814
I0306 07:15:23.662410 139708776544000 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.37229859828948975, loss=2.810044288635254
I0306 07:15:58.606721 139708768151296 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.4144819378852844, loss=2.884209156036377
I0306 07:16:33.537176 139708776544000 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3642209470272064, loss=2.7546803951263428
I0306 07:17:08.489247 139708768151296 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3874482214450836, loss=2.8403871059417725
I0306 07:17:43.414464 139708776544000 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3697282671928406, loss=2.8271329402923584
I0306 07:18:18.335196 139708768151296 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.3932479918003082, loss=2.819322109222412
I0306 07:18:53.266302 139708776544000 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.38715413212776184, loss=2.751215934753418
I0306 07:19:28.199338 139708768151296 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.4195331633090973, loss=2.7801175117492676
I0306 07:20:03.114317 139708776544000 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.4251539409160614, loss=2.8302698135375977
I0306 07:20:38.023887 139708768151296 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.3727697730064392, loss=2.784909725189209
I0306 07:21:12.971154 139708776544000 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3675341010093689, loss=2.7658541202545166
I0306 07:21:47.942838 139708768151296 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3710643947124481, loss=2.7842965126037598
I0306 07:22:22.857693 139708776544000 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3784446120262146, loss=2.7548749446868896
I0306 07:22:57.769544 139708768151296 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.36290910840034485, loss=2.7813456058502197
I0306 07:23:32.731530 139708776544000 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.3763769865036011, loss=2.7501518726348877
I0306 07:24:07.666704 139708768151296 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.38515281677246094, loss=2.7886269092559814
I0306 07:24:35.598509 139852506842304 spec.py:321] Evaluating on the training split.
I0306 07:24:38.248135 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:28:22.158557 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 07:28:24.792276 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:30:57.052099 139852506842304 spec.py:349] Evaluating on the test split.
I0306 07:30:59.697009 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:33:30.719935 139852506842304 submission_runner.py:469] Time since start: 44373.82s, 	Step: 77181, 	{'train/accuracy': 0.6651853919029236, 'train/loss': 1.6744420528411865, 'train/bleu': 33.52294921083123, 'validation/accuracy': 0.6806788444519043, 'validation/loss': 1.5769414901733398, 'validation/bleu': 29.514648316043534, 'validation/num_examples': 3000, 'test/accuracy': 0.6961881518363953, 'test/loss': 1.4972673654556274, 'test/bleu': 29.991848523644617, 'test/num_examples': 3003, 'score': 26905.657557964325, 'total_duration': 44373.8246011734, 'accumulated_submission_time': 26905.657557964325, 'accumulated_eval_time': 17462.93500161171, 'accumulated_logging_time': 0.6782655715942383}
I0306 07:33:30.737390 139708776544000 logging_writer.py:48] [77181] accumulated_eval_time=17462.9, accumulated_logging_time=0.678266, accumulated_submission_time=26905.7, global_step=77181, preemption_count=0, score=26905.7, test/accuracy=0.696188, test/bleu=29.9918, test/loss=1.49727, test/num_examples=3003, total_duration=44373.8, train/accuracy=0.665185, train/bleu=33.5229, train/loss=1.67444, validation/accuracy=0.680679, validation/bleu=29.5146, validation/loss=1.57694, validation/num_examples=3000
I0306 07:33:37.692356 139708768151296 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.3724314570426941, loss=2.842866897583008
I0306 07:34:12.468349 139708776544000 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.3678015470504761, loss=2.7227439880371094
I0306 07:34:47.307564 139708768151296 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.3695537745952606, loss=2.8404412269592285
I0306 07:35:22.226048 139708776544000 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.42681339383125305, loss=2.764181613922119
I0306 07:35:57.139613 139708768151296 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.3817341923713684, loss=2.757312536239624
I0306 07:36:32.060246 139708776544000 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.4130890667438507, loss=2.7722604274749756
I0306 07:37:07.027461 139708768151296 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3821277320384979, loss=2.7993338108062744
I0306 07:37:41.946489 139708776544000 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.366921603679657, loss=2.7613472938537598
I0306 07:38:16.863361 139708768151296 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.36621880531311035, loss=2.8197274208068848
I0306 07:38:51.807808 139708776544000 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3883609175682068, loss=2.8322691917419434
I0306 07:39:26.764930 139708768151296 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.42748165130615234, loss=2.7894248962402344
I0306 07:40:01.710701 139708776544000 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.37951162457466125, loss=2.7490885257720947
I0306 07:40:36.663330 139708768151296 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.3651977479457855, loss=2.7861714363098145
I0306 07:41:11.624245 139708776544000 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3650106191635132, loss=2.78621244430542
I0306 07:41:46.573816 139708768151296 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.4046878516674042, loss=2.8340978622436523
I0306 07:42:21.516502 139708776544000 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.3903564214706421, loss=2.8582265377044678
I0306 07:42:56.426999 139708768151296 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.38937732577323914, loss=2.7968804836273193
I0306 07:43:31.345017 139708776544000 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.41424068808555603, loss=2.823638677597046
I0306 07:44:06.284868 139708768151296 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3847953975200653, loss=2.781524896621704
I0306 07:44:41.224561 139708776544000 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.3988510072231293, loss=2.774336099624634
I0306 07:45:16.167685 139708768151296 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.3940567672252655, loss=2.8401472568511963
I0306 07:45:51.121271 139708776544000 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.36107537150382996, loss=2.780456304550171
I0306 07:46:26.059974 139708768151296 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3794059753417969, loss=2.761763572692871
I0306 07:47:01.001005 139708776544000 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3859929144382477, loss=2.719020128250122
I0306 07:47:30.724085 139852506842304 spec.py:321] Evaluating on the training split.
I0306 07:47:33.384537 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:50:33.772357 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 07:50:36.413238 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:53:14.463909 139852506842304 spec.py:349] Evaluating on the test split.
I0306 07:53:17.105577 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 07:55:48.412794 139852506842304 submission_runner.py:469] Time since start: 45711.52s, 	Step: 79586, 	{'train/accuracy': 0.6659746766090393, 'train/loss': 1.6808340549468994, 'train/bleu': 33.02125358446005, 'validation/accuracy': 0.6822361946105957, 'validation/loss': 1.5652544498443604, 'validation/bleu': 30.104736884059864, 'validation/num_examples': 3000, 'test/accuracy': 0.6975321769714355, 'test/loss': 1.4865301847457886, 'test/bleu': 30.256953859068997, 'test/num_examples': 3003, 'score': 27745.499103546143, 'total_duration': 45711.51742267609, 'accumulated_submission_time': 27745.499103546143, 'accumulated_eval_time': 17960.623639583588, 'accumulated_logging_time': 0.7037298679351807}
I0306 07:55:48.433839 139708768151296 logging_writer.py:48] [79586] accumulated_eval_time=17960.6, accumulated_logging_time=0.70373, accumulated_submission_time=27745.5, global_step=79586, preemption_count=0, score=27745.5, test/accuracy=0.697532, test/bleu=30.257, test/loss=1.48653, test/num_examples=3003, total_duration=45711.5, train/accuracy=0.665975, train/bleu=33.0213, train/loss=1.68083, validation/accuracy=0.682236, validation/bleu=30.1047, validation/loss=1.56525, validation/num_examples=3000
I0306 07:55:53.658755 139708776544000 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3797798752784729, loss=2.7901973724365234
I0306 07:56:28.420965 139708768151296 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.4225738048553467, loss=2.7626254558563232
I0306 07:57:03.297738 139708776544000 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3808344900608063, loss=2.817511558532715
I0306 07:57:38.241629 139708768151296 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.3880767226219177, loss=2.7708017826080322
I0306 07:58:13.147232 139708776544000 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.37865474820137024, loss=2.693443536758423
I0306 07:58:48.067768 139708768151296 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.4001484811306, loss=2.8134419918060303
I0306 07:59:22.975487 139708776544000 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3949488699436188, loss=2.8117430210113525
I0306 07:59:57.920774 139708768151296 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.36156684160232544, loss=2.8170089721679688
I0306 08:00:32.844720 139708776544000 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3923373818397522, loss=2.7321817874908447
I0306 08:01:07.775113 139708768151296 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3985271751880646, loss=2.75712251663208
I0306 08:01:42.701500 139708776544000 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.4138789772987366, loss=2.806283950805664
I0306 08:02:17.669288 139708768151296 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.41804906725883484, loss=2.8064768314361572
I0306 08:02:52.622279 139708776544000 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.38049229979515076, loss=2.7605717182159424
I0306 08:03:27.567470 139708768151296 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.4069340229034424, loss=2.802811622619629
I0306 08:04:02.485982 139708776544000 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.414193719625473, loss=2.747472047805786
I0306 08:04:37.394175 139708768151296 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.49611806869506836, loss=2.775773286819458
I0306 08:05:12.334519 139708776544000 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.4057875871658325, loss=2.8055219650268555
I0306 08:05:47.258000 139708768151296 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3824898600578308, loss=2.7841453552246094
I0306 08:06:22.205904 139708776544000 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.39873436093330383, loss=2.733931064605713
I0306 08:06:57.147260 139708768151296 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.40233954787254333, loss=2.7293126583099365
I0306 08:07:31.987864 139708776544000 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.37691348791122437, loss=2.7324130535125732
I0306 08:08:06.834867 139708768151296 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.41005879640579224, loss=2.790221929550171
I0306 08:08:41.685637 139708776544000 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.4097898602485657, loss=2.8587374687194824
I0306 08:09:16.524419 139708768151296 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.3787161409854889, loss=2.750375986099243
I0306 08:09:48.563991 139852506842304 spec.py:321] Evaluating on the training split.
I0306 08:09:51.214543 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 08:13:19.219952 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 08:13:21.858573 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 08:16:07.305158 139852506842304 spec.py:349] Evaluating on the test split.
I0306 08:16:09.953630 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 08:18:39.207876 139852506842304 submission_runner.py:469] Time since start: 47082.31s, 	Step: 81993, 	{'train/accuracy': 0.6884478330612183, 'train/loss': 1.5379515886306763, 'train/bleu': 34.86726419077571, 'validation/accuracy': 0.6845104098320007, 'validation/loss': 1.5560353994369507, 'validation/bleu': 29.847521220219416, 'validation/num_examples': 3000, 'test/accuracy': 0.6979956030845642, 'test/loss': 1.4797499179840088, 'test/bleu': 30.151530260802794, 'test/num_examples': 3003, 'score': 28585.481604099274, 'total_duration': 47082.3125333786, 'accumulated_submission_time': 28585.481604099274, 'accumulated_eval_time': 18491.26747250557, 'accumulated_logging_time': 0.7338624000549316}
I0306 08:18:39.226593 139708776544000 logging_writer.py:48] [81993] accumulated_eval_time=18491.3, accumulated_logging_time=0.733862, accumulated_submission_time=28585.5, global_step=81993, preemption_count=0, score=28585.5, test/accuracy=0.697996, test/bleu=30.1515, test/loss=1.47975, test/num_examples=3003, total_duration=47082.3, train/accuracy=0.688448, train/bleu=34.8673, train/loss=1.53795, validation/accuracy=0.68451, validation/bleu=29.8475, validation/loss=1.55604, validation/num_examples=3000
I0306 08:18:42.023840 139708768151296 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.394747257232666, loss=2.82486629486084
I0306 08:19:16.702475 139708776544000 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.4393407702445984, loss=2.7577590942382812
I0306 08:19:51.462458 139708768151296 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.4154849946498871, loss=2.753769874572754
I0306 08:20:26.279969 139708776544000 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.43072745203971863, loss=2.76638126373291
I0306 08:21:01.094635 139708768151296 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4030853807926178, loss=2.803440809249878
I0306 08:21:35.936817 139708776544000 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.40684500336647034, loss=2.8119726181030273
I0306 08:22:10.732623 139708768151296 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.38626793026924133, loss=2.7436132431030273
I0306 08:22:45.551776 139708776544000 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.4273698925971985, loss=2.8221476078033447
I0306 08:23:20.371296 139708768151296 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.3912411332130432, loss=2.715747833251953
I0306 08:23:55.193036 139708776544000 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.4011874794960022, loss=2.7351276874542236
I0306 08:24:30.017136 139708768151296 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.41131505370140076, loss=2.812671661376953
I0306 08:25:04.829684 139708776544000 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.40205514430999756, loss=2.7315685749053955
I0306 08:25:39.641053 139708768151296 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.3911597430706024, loss=2.7422404289245605
I0306 08:26:14.477899 139708776544000 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.407174676656723, loss=2.7791314125061035
I0306 08:26:49.316760 139708768151296 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.39824482798576355, loss=2.750143051147461
I0306 08:27:24.117157 139708776544000 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.41915038228034973, loss=2.8099753856658936
I0306 08:27:58.940647 139708768151296 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.38623979687690735, loss=2.7567930221557617
I0306 08:28:33.784377 139708776544000 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.4074481427669525, loss=2.793367624282837
I0306 08:29:08.600280 139708768151296 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.3937821686267853, loss=2.6637818813323975
I0306 08:29:43.409799 139708776544000 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.40387415885925293, loss=2.7331478595733643
I0306 08:30:18.209605 139708768151296 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.4001654088497162, loss=2.713608980178833
I0306 08:30:53.045773 139708776544000 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.3900371491909027, loss=2.7052056789398193
I0306 08:31:27.857539 139708768151296 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3811257779598236, loss=2.735468626022339
I0306 08:32:02.658248 139708776544000 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3922523856163025, loss=2.7379472255706787
I0306 08:32:37.489416 139708768151296 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4024583101272583, loss=2.776263475418091
I0306 08:32:39.239389 139852506842304 spec.py:321] Evaluating on the training split.
I0306 08:32:41.902271 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 08:36:03.841722 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 08:36:06.483013 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 08:39:14.557937 139852506842304 spec.py:349] Evaluating on the test split.
I0306 08:39:17.201952 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 08:42:06.782155 139852506842304 submission_runner.py:469] Time since start: 48489.89s, 	Step: 84406, 	{'train/accuracy': 0.674456775188446, 'train/loss': 1.6333317756652832, 'train/bleu': 33.751387262258156, 'validation/accuracy': 0.6854497790336609, 'validation/loss': 1.5468295812606812, 'validation/bleu': 30.037420662788662, 'validation/num_examples': 3000, 'test/accuracy': 0.7006256580352783, 'test/loss': 1.4670251607894897, 'test/bleu': 30.272939407489115, 'test/num_examples': 3003, 'score': 29425.350321292877, 'total_duration': 48489.88679552078, 'accumulated_submission_time': 29425.350321292877, 'accumulated_eval_time': 19058.810165643692, 'accumulated_logging_time': 0.7605640888214111}
I0306 08:42:06.800952 139708776544000 logging_writer.py:48] [84406] accumulated_eval_time=19058.8, accumulated_logging_time=0.760564, accumulated_submission_time=29425.4, global_step=84406, preemption_count=0, score=29425.4, test/accuracy=0.700626, test/bleu=30.2729, test/loss=1.46703, test/num_examples=3003, total_duration=48489.9, train/accuracy=0.674457, train/bleu=33.7514, train/loss=1.63333, validation/accuracy=0.68545, validation/bleu=30.0374, validation/loss=1.54683, validation/num_examples=3000
I0306 08:42:39.714362 139708768151296 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.4175092875957489, loss=2.830045223236084
I0306 08:43:14.470568 139708776544000 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.4209659695625305, loss=2.7293360233306885
I0306 08:43:49.271645 139708768151296 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.420335978269577, loss=2.737921714782715
I0306 08:44:24.080535 139708776544000 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.42331647872924805, loss=2.6915125846862793
I0306 08:44:58.914661 139708768151296 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.40198031067848206, loss=2.7553703784942627
I0306 08:45:33.713825 139708776544000 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.39050477743148804, loss=2.7777206897735596
I0306 08:46:08.558473 139708768151296 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.433991014957428, loss=2.754957914352417
I0306 08:46:43.405027 139708776544000 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.4137124717235565, loss=2.739482879638672
I0306 08:47:18.210812 139708768151296 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.405857652425766, loss=2.7453696727752686
I0306 08:47:53.019660 139708776544000 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3908626437187195, loss=2.756906509399414
I0306 08:48:27.826185 139708768151296 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.40701597929000854, loss=2.7812538146972656
I0306 08:49:02.661460 139708776544000 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.4021793305873871, loss=2.7644124031066895
I0306 08:49:37.494318 139708768151296 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.40899285674095154, loss=2.7617971897125244
I0306 08:50:12.306454 139708776544000 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.46328282356262207, loss=2.819680690765381
I0306 08:50:47.130030 139708768151296 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.41654977202415466, loss=2.794145107269287
I0306 08:51:21.934031 139708776544000 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.422416090965271, loss=2.782269239425659
I0306 08:51:56.737823 139708768151296 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.4219197630882263, loss=2.7567298412323
I0306 08:52:31.557217 139708776544000 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.41643255949020386, loss=2.726504325866699
I0306 08:53:06.388096 139708768151296 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.4126727879047394, loss=2.761784315109253
I0306 08:53:41.199866 139708776544000 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4103628396987915, loss=2.6461803913116455
I0306 08:54:16.001083 139708768151296 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.43499523401260376, loss=2.7365784645080566
I0306 08:54:50.784408 139708776544000 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.41082340478897095, loss=2.747750759124756
I0306 08:55:25.580015 139708768151296 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3945629894733429, loss=2.7125489711761475
I0306 08:56:00.364762 139708776544000 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.4363635778427124, loss=2.7695817947387695
I0306 08:56:06.961760 139852506842304 spec.py:321] Evaluating on the training split.
I0306 08:56:09.609868 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 08:59:44.116036 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 08:59:46.756656 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:02:27.981190 139852506842304 spec.py:349] Evaluating on the test split.
I0306 09:02:30.622640 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:05:08.895939 139852506842304 submission_runner.py:469] Time since start: 49872.00s, 	Step: 86820, 	{'train/accuracy': 0.6733415126800537, 'train/loss': 1.6406230926513672, 'train/bleu': 33.85104683343075, 'validation/accuracy': 0.6854868531227112, 'validation/loss': 1.5395935773849487, 'validation/bleu': 30.299796071298964, 'validation/num_examples': 3000, 'test/accuracy': 0.7007762789726257, 'test/loss': 1.4608391523361206, 'test/bleu': 30.31391819971247, 'test/num_examples': 3003, 'score': 30265.368695020676, 'total_duration': 49872.0005903244, 'accumulated_submission_time': 30265.368695020676, 'accumulated_eval_time': 19600.744283914566, 'accumulated_logging_time': 0.7873983383178711}
I0306 09:05:08.913773 139708768151296 logging_writer.py:48] [86820] accumulated_eval_time=19600.7, accumulated_logging_time=0.787398, accumulated_submission_time=30265.4, global_step=86820, preemption_count=0, score=30265.4, test/accuracy=0.700776, test/bleu=30.3139, test/loss=1.46084, test/num_examples=3003, total_duration=49872, train/accuracy=0.673342, train/bleu=33.851, train/loss=1.64062, validation/accuracy=0.685487, validation/bleu=30.2998, validation/loss=1.53959, validation/num_examples=3000
I0306 09:05:36.945843 139708776544000 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.43471771478652954, loss=2.6544766426086426
I0306 09:06:11.570520 139708768151296 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.44891786575317383, loss=2.7276313304901123
I0306 09:06:46.271402 139708776544000 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.41699275374412537, loss=2.7983624935150146
I0306 09:07:20.988686 139708768151296 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.4307592213153839, loss=2.779719352722168
I0306 09:07:55.733814 139708776544000 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.46084025502204895, loss=2.772043228149414
I0306 09:08:30.445930 139708768151296 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.42665770649909973, loss=2.7735345363616943
I0306 09:09:05.169775 139708776544000 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4241844117641449, loss=2.676429033279419
I0306 09:09:39.905994 139708768151296 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.44298067688941956, loss=2.7476143836975098
I0306 09:10:14.650216 139708776544000 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.4378513693809509, loss=2.7501046657562256
I0306 09:10:49.383233 139708768151296 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.42747315764427185, loss=2.682345151901245
I0306 09:11:24.201714 139708776544000 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.4448202848434448, loss=2.7916054725646973
I0306 09:11:59.027848 139708768151296 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.40740951895713806, loss=2.645888090133667
I0306 09:12:33.855930 139708776544000 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.4210824966430664, loss=2.78649640083313
I0306 09:13:08.661587 139708768151296 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.4273136854171753, loss=2.72992205619812
I0306 09:13:43.475319 139708776544000 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.4407912790775299, loss=2.712001085281372
I0306 09:14:18.323724 139708768151296 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.44784966111183167, loss=2.7403111457824707
I0306 09:14:53.147226 139708776544000 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.42358216643333435, loss=2.7838828563690186
I0306 09:15:27.986972 139708768151296 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.43368732929229736, loss=2.7591447830200195
I0306 09:16:02.793684 139708776544000 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.4121105968952179, loss=2.672126293182373
I0306 09:16:37.620511 139708768151296 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4431198835372925, loss=2.7164454460144043
I0306 09:17:12.433768 139708776544000 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4369964003562927, loss=2.7552454471588135
I0306 09:17:47.256400 139708768151296 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.44911447167396545, loss=2.7806591987609863
I0306 09:18:22.103561 139708776544000 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4478842616081238, loss=2.712805986404419
I0306 09:18:56.936939 139708768151296 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.45662155747413635, loss=2.709416389465332
I0306 09:19:09.118964 139852506842304 spec.py:321] Evaluating on the training split.
I0306 09:19:11.771649 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:23:05.637440 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 09:23:08.268074 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:25:48.283661 139852506842304 spec.py:349] Evaluating on the test split.
I0306 09:25:50.920525 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:28:14.863118 139852506842304 submission_runner.py:469] Time since start: 51257.97s, 	Step: 89236, 	{'train/accuracy': 0.6885882616043091, 'train/loss': 1.550801396369934, 'train/bleu': 34.46831958993319, 'validation/accuracy': 0.6874892115592957, 'validation/loss': 1.5362476110458374, 'validation/bleu': 30.437435623657887, 'validation/num_examples': 3000, 'test/accuracy': 0.7021550536155701, 'test/loss': 1.4629102945327759, 'test/bleu': 30.25459864119037, 'test/num_examples': 3003, 'score': 31105.43193101883, 'total_duration': 51257.967772483826, 'accumulated_submission_time': 31105.43193101883, 'accumulated_eval_time': 20146.488379716873, 'accumulated_logging_time': 0.8136112689971924}
I0306 09:28:14.881042 139708776544000 logging_writer.py:48] [89236] accumulated_eval_time=20146.5, accumulated_logging_time=0.813611, accumulated_submission_time=31105.4, global_step=89236, preemption_count=0, score=31105.4, test/accuracy=0.702155, test/bleu=30.2546, test/loss=1.46291, test/num_examples=3003, total_duration=51258, train/accuracy=0.688588, train/bleu=34.4683, train/loss=1.5508, validation/accuracy=0.687489, validation/bleu=30.4374, validation/loss=1.53625, validation/num_examples=3000
I0306 09:28:37.412660 139708768151296 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.44010129570961, loss=2.7633392810821533
I0306 09:29:12.115738 139708776544000 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.43333712220191956, loss=2.6750895977020264
I0306 09:29:46.904031 139708768151296 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.46243372559547424, loss=2.7325949668884277
I0306 09:30:21.762138 139708776544000 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.43293124437332153, loss=2.7035655975341797
I0306 09:30:56.604490 139708768151296 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4598294496536255, loss=2.7535715103149414
I0306 09:31:31.416796 139708776544000 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.4731456935405731, loss=2.8202362060546875
I0306 09:32:06.238787 139708768151296 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.46209102869033813, loss=2.7397756576538086
I0306 09:32:41.055058 139708776544000 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.44293561577796936, loss=2.770472764968872
I0306 09:33:15.888978 139708768151296 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.45165979862213135, loss=2.8046438694000244
I0306 09:33:50.724613 139708776544000 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.44804608821868896, loss=2.7720065116882324
I0306 09:34:25.546869 139708768151296 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.4431604743003845, loss=2.667262554168701
I0306 09:35:00.389099 139708776544000 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.47255370020866394, loss=2.7101268768310547
I0306 09:35:35.215761 139708768151296 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4534605145454407, loss=2.746283769607544
I0306 09:36:10.057578 139708776544000 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.4609454572200775, loss=2.689422607421875
I0306 09:36:44.873824 139708768151296 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.4512479305267334, loss=2.7457973957061768
I0306 09:37:19.728697 139708776544000 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.45227929949760437, loss=2.634260654449463
I0306 09:37:54.542455 139708768151296 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.481899231672287, loss=2.7955117225646973
I0306 09:38:29.362384 139708776544000 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.4637163281440735, loss=2.720616102218628
I0306 09:39:04.180667 139708768151296 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.44636744260787964, loss=2.668696880340576
I0306 09:39:38.994777 139708776544000 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.43569403886795044, loss=2.673903226852417
I0306 09:40:13.811914 139708768151296 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.4562171995639801, loss=2.7335448265075684
I0306 09:40:48.658805 139708776544000 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.46345773339271545, loss=2.7658333778381348
I0306 09:41:23.495913 139708768151296 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.44174495339393616, loss=2.6446738243103027
I0306 09:41:58.321482 139708776544000 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4797208607196808, loss=2.7198452949523926
I0306 09:42:15.032106 139852506842304 spec.py:321] Evaluating on the training split.
I0306 09:42:17.683838 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:45:10.720138 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 09:45:13.366623 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:47:49.738348 139852506842304 spec.py:349] Evaluating on the test split.
I0306 09:47:52.382470 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 09:50:21.303852 139852506842304 submission_runner.py:469] Time since start: 52584.41s, 	Step: 91649, 	{'train/accuracy': 0.6805351972579956, 'train/loss': 1.5954099893569946, 'train/bleu': 34.06174556810005, 'validation/accuracy': 0.6886510252952576, 'validation/loss': 1.5314713716506958, 'validation/bleu': 30.390031041974947, 'validation/num_examples': 3000, 'test/accuracy': 0.7036148905754089, 'test/loss': 1.4470577239990234, 'test/bleu': 30.427888908006608, 'test/num_examples': 3003, 'score': 31945.44021844864, 'total_duration': 52584.40848445892, 'accumulated_submission_time': 31945.44021844864, 'accumulated_eval_time': 20632.76004385948, 'accumulated_logging_time': 0.8397274017333984}
I0306 09:50:21.324768 139708768151296 logging_writer.py:48] [91649] accumulated_eval_time=20632.8, accumulated_logging_time=0.839727, accumulated_submission_time=31945.4, global_step=91649, preemption_count=0, score=31945.4, test/accuracy=0.703615, test/bleu=30.4279, test/loss=1.44706, test/num_examples=3003, total_duration=52584.4, train/accuracy=0.680535, train/bleu=34.0617, train/loss=1.59541, validation/accuracy=0.688651, validation/bleu=30.39, validation/loss=1.53147, validation/num_examples=3000
I0306 09:50:39.347732 139708776544000 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.4738360643386841, loss=2.7653298377990723
I0306 09:51:14.034153 139708768151296 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.44370198249816895, loss=2.6674811840057373
I0306 09:51:48.805520 139708776544000 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4587479531764984, loss=2.7382876873016357
I0306 09:52:23.591200 139708768151296 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4717116355895996, loss=2.7359120845794678
I0306 09:52:58.418860 139708776544000 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.49419328570365906, loss=2.6986520290374756
I0306 09:53:33.225475 139708768151296 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.45968735218048096, loss=2.7681925296783447
I0306 09:54:08.046808 139708776544000 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.49703317880630493, loss=2.685250997543335
I0306 09:54:42.875064 139708768151296 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.47807934880256653, loss=2.636608362197876
I0306 09:55:17.692542 139708776544000 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.46173232793807983, loss=2.6404013633728027
I0306 09:55:52.526266 139708768151296 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.48004084825515747, loss=2.7240824699401855
I0306 09:56:27.372339 139708776544000 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.46670863032341003, loss=2.760655403137207
I0306 09:57:02.199848 139708768151296 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.46870702505111694, loss=2.7376015186309814
I0306 09:57:37.031895 139708776544000 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.5022990703582764, loss=2.726152181625366
I0306 09:58:11.838127 139708768151296 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.47087520360946655, loss=2.7216994762420654
I0306 09:58:46.658585 139708776544000 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.4874950051307678, loss=2.675323724746704
I0306 09:59:21.484616 139708768151296 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.47333183884620667, loss=2.697547435760498
I0306 09:59:56.321700 139708776544000 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4871755540370941, loss=2.7361066341400146
I0306 10:00:31.166315 139708768151296 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.46519356966018677, loss=2.742091178894043
I0306 10:01:05.988546 139708776544000 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.0450657606124878, loss=2.601266384124756
I0306 10:01:40.818577 139708768151296 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.4750041663646698, loss=2.7479522228240967
I0306 10:02:15.664803 139708776544000 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.47498610615730286, loss=2.700878143310547
I0306 10:02:50.479980 139708768151296 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.47010868787765503, loss=2.6678919792175293
I0306 10:03:25.310014 139708776544000 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.4698738157749176, loss=2.7315878868103027
I0306 10:04:00.126826 139708768151296 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.45912352204322815, loss=2.7006068229675293
I0306 10:04:21.382650 139852506842304 spec.py:321] Evaluating on the training split.
I0306 10:04:24.040819 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:07:23.374346 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 10:07:26.013964 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:10:00.405623 139852506842304 spec.py:349] Evaluating on the test split.
I0306 10:10:03.052293 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:12:31.158429 139852506842304 submission_runner.py:469] Time since start: 53914.26s, 	Step: 94062, 	{'train/accuracy': 0.6818845868110657, 'train/loss': 1.589035153388977, 'train/bleu': 33.87692341164172, 'validation/accuracy': 0.6899241209030151, 'validation/loss': 1.5212359428405762, 'validation/bleu': 30.746544761793338, 'validation/num_examples': 3000, 'test/accuracy': 0.7048546075820923, 'test/loss': 1.4391686916351318, 'test/bleu': 30.68938342621374, 'test/num_examples': 3003, 'score': 32785.350706100464, 'total_duration': 53914.26307940483, 'accumulated_submission_time': 32785.350706100464, 'accumulated_eval_time': 21122.535757303238, 'accumulated_logging_time': 0.8692505359649658}
I0306 10:12:31.177197 139708776544000 logging_writer.py:48] [94062] accumulated_eval_time=21122.5, accumulated_logging_time=0.869251, accumulated_submission_time=32785.4, global_step=94062, preemption_count=0, score=32785.4, test/accuracy=0.704855, test/bleu=30.6894, test/loss=1.43917, test/num_examples=3003, total_duration=53914.3, train/accuracy=0.681885, train/bleu=33.8769, train/loss=1.58904, validation/accuracy=0.689924, validation/bleu=30.7465, validation/loss=1.52124, validation/num_examples=3000
I0306 10:12:44.700250 139708768151296 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.4752349555492401, loss=2.7216122150421143
I0306 10:13:19.388968 139708776544000 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.487251877784729, loss=2.687962293624878
I0306 10:13:54.154722 139708768151296 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.4917183816432953, loss=2.749579668045044
I0306 10:14:28.971428 139708776544000 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4795641303062439, loss=2.7549943923950195
I0306 10:15:03.791067 139708768151296 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4700489938259125, loss=2.700265407562256
I0306 10:15:38.600332 139708776544000 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.4828689992427826, loss=2.7455391883850098
I0306 10:16:13.426042 139708768151296 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.5104812383651733, loss=2.7399799823760986
I0306 10:16:48.222832 139708776544000 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.4756414294242859, loss=2.7089734077453613
I0306 10:17:23.053006 139708768151296 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4867359697818756, loss=2.6288225650787354
I0306 10:17:57.876338 139708776544000 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.48041030764579773, loss=2.6488993167877197
I0306 10:18:32.737374 139708768151296 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.46420004963874817, loss=2.7885396480560303
I0306 10:19:07.575484 139708776544000 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.5003296732902527, loss=2.657334327697754
I0306 10:19:42.418664 139708768151296 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.47505298256874084, loss=2.7196993827819824
I0306 10:20:17.255195 139708776544000 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.4894874095916748, loss=2.6886465549468994
I0306 10:20:52.094883 139708768151296 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.4872245788574219, loss=2.68790602684021
I0306 10:21:26.930654 139708776544000 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.49410954117774963, loss=2.682741403579712
I0306 10:22:01.743489 139708768151296 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.48980095982551575, loss=2.654630661010742
I0306 10:22:36.582790 139708776544000 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.48236867785453796, loss=2.6374831199645996
I0306 10:23:11.389761 139708768151296 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.5019971132278442, loss=2.689174175262451
I0306 10:23:46.218486 139708776544000 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.48713213205337524, loss=2.6676433086395264
I0306 10:24:21.040700 139708768151296 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.500723123550415, loss=2.6048080921173096
I0306 10:24:55.905886 139708776544000 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.481266587972641, loss=2.677567720413208
I0306 10:25:30.856804 139708768151296 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.4811565577983856, loss=2.7098543643951416
I0306 10:26:05.730451 139708776544000 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5129779577255249, loss=2.676802396774292
I0306 10:26:31.502744 139852506842304 spec.py:321] Evaluating on the training split.
I0306 10:26:34.147844 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:29:53.025172 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 10:29:55.664212 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:32:28.113802 139852506842304 spec.py:349] Evaluating on the test split.
I0306 10:32:30.765769 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:35:11.837109 139852506842304 submission_runner.py:469] Time since start: 55274.94s, 	Step: 96475, 	{'train/accuracy': 0.688685953617096, 'train/loss': 1.5498021841049194, 'train/bleu': 34.59552767526004, 'validation/accuracy': 0.6897263526916504, 'validation/loss': 1.5207273960113525, 'validation/bleu': 30.196592470561175, 'validation/num_examples': 3000, 'test/accuracy': 0.703649640083313, 'test/loss': 1.4389573335647583, 'test/bleu': 30.592569004955397, 'test/num_examples': 3003, 'score': 33625.52310419083, 'total_duration': 55274.94172143936, 'accumulated_submission_time': 33625.52310419083, 'accumulated_eval_time': 21642.870022773743, 'accumulated_logging_time': 0.897902250289917}
I0306 10:35:11.858330 139708768151296 logging_writer.py:48] [96475] accumulated_eval_time=21642.9, accumulated_logging_time=0.897902, accumulated_submission_time=33625.5, global_step=96475, preemption_count=0, score=33625.5, test/accuracy=0.70365, test/bleu=30.5926, test/loss=1.43896, test/num_examples=3003, total_duration=55274.9, train/accuracy=0.688686, train/bleu=34.5955, train/loss=1.5498, validation/accuracy=0.689726, validation/bleu=30.1966, validation/loss=1.52073, validation/num_examples=3000
I0306 10:35:20.877014 139708776544000 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.5150019526481628, loss=2.6717443466186523
I0306 10:35:55.587291 139708768151296 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.5090110301971436, loss=2.743206739425659
I0306 10:36:30.368029 139708776544000 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.495756596326828, loss=2.6773641109466553
I0306 10:37:05.177739 139708768151296 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5285532474517822, loss=2.704892635345459
I0306 10:37:39.979229 139708776544000 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.5148031115531921, loss=2.659411668777466
I0306 10:38:14.808542 139708768151296 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5172581076622009, loss=2.6977756023406982
I0306 10:38:49.658667 139708776544000 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.5132367014884949, loss=2.636265754699707
I0306 10:39:24.468546 139708768151296 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.5067253708839417, loss=2.6733670234680176
I0306 10:39:59.314120 139708776544000 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5420856475830078, loss=2.722853660583496
I0306 10:40:34.149982 139708768151296 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5096868276596069, loss=2.7522637844085693
I0306 10:41:08.979722 139708776544000 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.5231978893280029, loss=2.6477303504943848
I0306 10:41:43.785130 139708768151296 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.5275887250900269, loss=2.756575345993042
I0306 10:42:18.626673 139708776544000 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.5214325785636902, loss=2.675969123840332
I0306 10:42:53.458407 139708768151296 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.5074036717414856, loss=2.6934635639190674
I0306 10:43:28.290489 139708776544000 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.5157877206802368, loss=2.644268274307251
I0306 10:44:03.088597 139708768151296 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5316381454467773, loss=2.6966958045959473
I0306 10:44:37.912711 139708776544000 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5045790076255798, loss=2.6538898944854736
I0306 10:45:12.728260 139708768151296 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.5406539440155029, loss=2.629262685775757
I0306 10:45:47.551604 139708776544000 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5134485363960266, loss=2.641439437866211
I0306 10:46:22.438729 139708768151296 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.4998661279678345, loss=2.58797287940979
I0306 10:46:57.258702 139708776544000 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5050820708274841, loss=2.6389760971069336
I0306 10:47:32.084801 139708768151296 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5126727819442749, loss=2.657438278198242
I0306 10:48:06.919489 139708776544000 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5187334418296814, loss=2.694380521774292
I0306 10:48:41.749819 139708768151296 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.521299421787262, loss=2.6022934913635254
I0306 10:49:12.046853 139852506842304 spec.py:321] Evaluating on the training split.
I0306 10:49:14.707790 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:52:47.650870 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 10:52:50.277125 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:55:35.526168 139852506842304 spec.py:349] Evaluating on the test split.
I0306 10:55:38.159791 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 10:58:19.398658 139852506842304 submission_runner.py:469] Time since start: 56662.50s, 	Step: 98888, 	{'train/accuracy': 0.6855859756469727, 'train/loss': 1.570130705833435, 'train/bleu': 34.784487084954826, 'validation/accuracy': 0.6906657218933105, 'validation/loss': 1.512605905532837, 'validation/bleu': 30.67338966187723, 'validation/num_examples': 3000, 'test/accuracy': 0.7056192755699158, 'test/loss': 1.4315801858901978, 'test/bleu': 30.89607037139512, 'test/num_examples': 3003, 'score': 34465.56069326401, 'total_duration': 56662.50332283974, 'accumulated_submission_time': 34465.56069326401, 'accumulated_eval_time': 22190.221782445908, 'accumulated_logging_time': 0.9279346466064453}
I0306 10:58:19.417589 139708776544000 logging_writer.py:48] [98888] accumulated_eval_time=22190.2, accumulated_logging_time=0.927935, accumulated_submission_time=34465.6, global_step=98888, preemption_count=0, score=34465.6, test/accuracy=0.705619, test/bleu=30.8961, test/loss=1.43158, test/num_examples=3003, total_duration=56662.5, train/accuracy=0.685586, train/bleu=34.7845, train/loss=1.57013, validation/accuracy=0.690666, validation/bleu=30.6734, validation/loss=1.51261, validation/num_examples=3000
I0306 10:58:23.901146 139708768151296 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5203046798706055, loss=2.660621166229248
I0306 10:58:58.552239 139708776544000 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5197851657867432, loss=2.6270833015441895
I0306 10:59:33.314184 139708768151296 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5305691361427307, loss=2.568598985671997
I0306 11:00:08.093887 139708776544000 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5389984846115112, loss=2.6708028316497803
I0306 11:00:42.921900 139708768151296 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.518045961856842, loss=2.670492172241211
I0306 11:01:17.744826 139708776544000 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5329251885414124, loss=2.750368595123291
I0306 11:01:52.564049 139708768151296 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5690666437149048, loss=2.763805389404297
I0306 11:02:27.391917 139708776544000 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.579816460609436, loss=2.7271177768707275
I0306 11:03:02.210518 139708768151296 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5052688717842102, loss=2.629286050796509
I0306 11:03:37.030821 139708776544000 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5304633975028992, loss=2.595947265625
I0306 11:04:11.885027 139708768151296 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5404717922210693, loss=2.6722638607025146
I0306 11:04:46.687155 139708776544000 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5249759554862976, loss=2.6764025688171387
I0306 11:05:21.488558 139708768151296 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5436956882476807, loss=2.7348392009735107
I0306 11:05:56.325962 139708776544000 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5324360728263855, loss=2.630760669708252
I0306 11:06:31.159837 139708768151296 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5498784184455872, loss=2.59659481048584
I0306 11:07:05.931565 139708776544000 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5259018540382385, loss=2.5925726890563965
I0306 11:07:40.691727 139708768151296 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5575202107429504, loss=2.654277801513672
I0306 11:08:15.436619 139708776544000 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5773035287857056, loss=2.6316981315612793
I0306 11:08:50.185365 139708768151296 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5648401975631714, loss=2.606600761413574
I0306 11:09:24.933625 139708776544000 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5503460168838501, loss=2.645045518875122
I0306 11:09:59.689330 139708768151296 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5354205369949341, loss=2.6242339611053467
I0306 11:10:34.423101 139708776544000 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5909941792488098, loss=2.6774961948394775
I0306 11:11:09.170161 139708768151296 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5290181040763855, loss=2.6470842361450195
I0306 11:11:43.903018 139708776544000 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5811224579811096, loss=2.6780552864074707
I0306 11:12:18.639411 139708768151296 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.5270664095878601, loss=2.5997323989868164
I0306 11:12:19.695554 139852506842304 spec.py:321] Evaluating on the training split.
I0306 11:12:22.344460 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 11:15:41.810493 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 11:15:44.454808 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 11:18:27.311469 139852506842304 spec.py:349] Evaluating on the test split.
I0306 11:18:29.945812 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 11:21:00.807546 139852506842304 submission_runner.py:469] Time since start: 58023.91s, 	Step: 101304, 	{'train/accuracy': 0.7007566094398499, 'train/loss': 1.4874826669692993, 'train/bleu': 36.06102217404117, 'validation/accuracy': 0.6931253671646118, 'validation/loss': 1.5058908462524414, 'validation/bleu': 30.69361796346096, 'validation/num_examples': 3000, 'test/accuracy': 0.7080639600753784, 'test/loss': 1.4224376678466797, 'test/bleu': 30.92272888077162, 'test/num_examples': 3003, 'score': 35305.692708969116, 'total_duration': 58023.91221046448, 'accumulated_submission_time': 35305.692708969116, 'accumulated_eval_time': 22711.333723783493, 'accumulated_logging_time': 0.9560112953186035}
I0306 11:21:00.826919 139708776544000 logging_writer.py:48] [101304] accumulated_eval_time=22711.3, accumulated_logging_time=0.956011, accumulated_submission_time=35305.7, global_step=101304, preemption_count=0, score=35305.7, test/accuracy=0.708064, test/bleu=30.9227, test/loss=1.42244, test/num_examples=3003, total_duration=58023.9, train/accuracy=0.700757, train/bleu=36.061, train/loss=1.48748, validation/accuracy=0.693125, validation/bleu=30.6936, validation/loss=1.50589, validation/num_examples=3000
I0306 11:21:34.383979 139708768151296 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5502636432647705, loss=2.66359281539917
I0306 11:22:09.085396 139708776544000 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5338436365127563, loss=2.6934359073638916
I0306 11:22:43.789345 139708768151296 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5863165855407715, loss=2.689337968826294
I0306 11:23:18.531335 139708776544000 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5770448446273804, loss=2.654346227645874
I0306 11:23:53.284159 139708768151296 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5492189526557922, loss=2.661147356033325
I0306 11:24:28.023759 139708776544000 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5746826529502869, loss=2.631175994873047
I0306 11:25:02.762357 139708768151296 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5483940243721008, loss=2.7354536056518555
I0306 11:25:37.518142 139708776544000 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5463685989379883, loss=2.6874454021453857
I0306 11:26:12.262278 139708768151296 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5818182826042175, loss=2.639808177947998
I0306 11:26:46.983883 139708776544000 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5743618607521057, loss=2.679719924926758
I0306 11:27:21.734863 139708768151296 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5667271614074707, loss=2.6727547645568848
I0306 11:27:56.494864 139708776544000 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5536430478096008, loss=2.5932259559631348
I0306 11:28:31.242021 139708768151296 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5567597150802612, loss=2.643343687057495
I0306 11:29:05.977607 139708776544000 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5599417090415955, loss=2.6506006717681885
I0306 11:29:40.744016 139708768151296 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.591414213180542, loss=2.6474947929382324
I0306 11:30:15.494109 139708776544000 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5582817792892456, loss=2.6770477294921875
I0306 11:30:50.229518 139708768151296 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.582200825214386, loss=2.7273175716400146
I0306 11:31:24.990074 139708776544000 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5551080703735352, loss=2.62601375579834
I0306 11:31:59.734882 139708768151296 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5812883377075195, loss=2.6338911056518555
I0306 11:32:34.468097 139708776544000 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.6133714914321899, loss=2.6076223850250244
I0306 11:33:09.197667 139708768151296 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5480564832687378, loss=2.6271860599517822
I0306 11:33:43.930557 139708776544000 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5811177492141724, loss=2.6879079341888428
I0306 11:34:18.688093 139708768151296 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5815076231956482, loss=2.6917481422424316
I0306 11:34:53.453402 139708776544000 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.6068824529647827, loss=2.6716690063476562
I0306 11:35:01.087494 139852506842304 spec.py:321] Evaluating on the training split.
I0306 11:35:03.741419 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 11:38:42.558146 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 11:38:45.201979 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 11:41:31.162663 139852506842304 spec.py:349] Evaluating on the test split.
I0306 11:41:33.797642 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 11:43:59.113019 139852506842304 submission_runner.py:469] Time since start: 59402.22s, 	Step: 103723, 	{'train/accuracy': 0.6987356543540955, 'train/loss': 1.4958245754241943, 'train/bleu': 35.160060051405345, 'validation/accuracy': 0.6947197914123535, 'validation/loss': 1.5008476972579956, 'validation/bleu': 30.791894049455433, 'validation/num_examples': 3000, 'test/accuracy': 0.7088981866836548, 'test/loss': 1.4161386489868164, 'test/bleu': 31.152641072411846, 'test/num_examples': 3003, 'score': 36145.80793952942, 'total_duration': 59402.21764707565, 'accumulated_submission_time': 36145.80793952942, 'accumulated_eval_time': 23249.359160900116, 'accumulated_logging_time': 0.9842977523803711}
I0306 11:43:59.135150 139708768151296 logging_writer.py:48] [103723] accumulated_eval_time=23249.4, accumulated_logging_time=0.984298, accumulated_submission_time=36145.8, global_step=103723, preemption_count=0, score=36145.8, test/accuracy=0.708898, test/bleu=31.1526, test/loss=1.41614, test/num_examples=3003, total_duration=59402.2, train/accuracy=0.698736, train/bleu=35.1601, train/loss=1.49582, validation/accuracy=0.69472, validation/bleu=30.7919, validation/loss=1.50085, validation/num_examples=3000
I0306 11:44:26.097794 139708776544000 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.6112990975379944, loss=2.7427432537078857
I0306 11:45:00.792036 139708768151296 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5915130376815796, loss=2.6474552154541016
I0306 11:45:35.494656 139708776544000 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5896910429000854, loss=2.6624338626861572
I0306 11:46:10.219695 139708768151296 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5748299360275269, loss=2.657261371612549
I0306 11:46:44.955723 139708776544000 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.5982386469841003, loss=2.6358892917633057
I0306 11:47:19.694391 139708768151296 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.6197093725204468, loss=2.6228280067443848
I0306 11:47:54.425885 139708776544000 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.6141263246536255, loss=2.6086137294769287
I0306 11:48:29.160368 139708768151296 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.597660481929779, loss=2.6967713832855225
I0306 11:49:03.896190 139708776544000 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5843870639801025, loss=2.596794605255127
I0306 11:49:38.606688 139708768151296 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.6190032958984375, loss=2.645235776901245
I0306 11:50:13.349514 139708776544000 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.5674382448196411, loss=2.6574959754943848
I0306 11:50:48.079663 139708768151296 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5637333393096924, loss=2.5689697265625
I0306 11:51:22.794621 139708776544000 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5802536010742188, loss=2.5915565490722656
I0306 11:51:57.547845 139708768151296 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5875579714775085, loss=2.572326183319092
I0306 11:52:32.292600 139708776544000 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.603705883026123, loss=2.6200027465820312
I0306 11:53:07.013875 139708768151296 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.6033619046211243, loss=2.6638643741607666
I0306 11:53:41.745798 139708776544000 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5854952335357666, loss=2.6065635681152344
I0306 11:54:16.497652 139708768151296 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.5990486741065979, loss=2.653651237487793
I0306 11:54:51.242566 139708776544000 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5760824084281921, loss=2.5752193927764893
I0306 11:55:25.981399 139708768151296 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5905922055244446, loss=2.6251139640808105
I0306 11:56:00.715531 139708776544000 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5803797245025635, loss=2.574500799179077
I0306 11:56:35.467413 139708768151296 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.5914109349250793, loss=2.6648993492126465
I0306 11:57:10.203624 139708776544000 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5912556648254395, loss=2.6672892570495605
I0306 11:57:44.949380 139708768151296 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.6022276878356934, loss=2.5933244228363037
I0306 11:57:59.199251 139852506842304 spec.py:321] Evaluating on the training split.
I0306 11:58:01.856709 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 12:01:14.681824 139852506842304 spec.py:333] Evaluating on the validation split.
I0306 12:01:17.326151 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 12:03:49.247843 139852506842304 spec.py:349] Evaluating on the test split.
I0306 12:03:51.888793 139852506842304 workload.py:181] Translating evaluation dataset.
I0306 12:06:11.704923 139852506842304 submission_runner.py:469] Time since start: 60734.81s, 	Step: 106142, 	{'train/accuracy': 0.6960100531578064, 'train/loss': 1.511432409286499, 'train/bleu': 35.66017449228269, 'validation/accuracy': 0.6939411163330078, 'validation/loss': 1.4963687658309937, 'validation/bleu': 30.933116003000794, 'validation/num_examples': 3000, 'test/accuracy': 0.7096049189567566, 'test/loss': 1.413786768913269, 'test/bleu': 31.19889851098314, 'test/num_examples': 3003, 'score': 36985.72701525688, 'total_duration': 60734.80958008766, 'accumulated_submission_time': 36985.72701525688, 'accumulated_eval_time': 23741.86478495598, 'accumulated_logging_time': 1.0150835514068604}
I0306 12:06:11.724836 139708776544000 logging_writer.py:48] [106142] accumulated_eval_time=23741.9, accumulated_logging_time=1.01508, accumulated_submission_time=36985.7, global_step=106142, preemption_count=0, score=36985.7, test/accuracy=0.709605, test/bleu=31.1989, test/loss=1.41379, test/num_examples=3003, total_duration=60734.8, train/accuracy=0.69601, train/bleu=35.6602, train/loss=1.51143, validation/accuracy=0.693941, validation/bleu=30.9331, validation/loss=1.49637, validation/num_examples=3000
I0306 12:06:11.745511 139708768151296 logging_writer.py:48] [106142] global_step=106142, preemption_count=0, score=36985.7
I0306 12:06:11.773283 139852506842304 submission_runner.py:646] Tuning trial 1/5
I0306 12:06:11.773463 139852506842304 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0306 12:06:11.775411 139852506842304 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007001033518463373, 'train/loss': 11.072443962097168, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.089398384094238, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.093862533569336, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.007185459136963, 'total_duration': 945.7834956645966, 'accumulated_submission_time': 26.007185459136963, 'accumulated_eval_time': 919.7761988639832, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2412, {'train/accuracy': 0.42491084337234497, 'train/loss': 3.8637702465057373, 'train/bleu': 15.61919232914508, 'validation/accuracy': 0.4137764871120453, 'validation/loss': 3.9510624408721924, 'validation/bleu': 10.762993728862007, 'validation/num_examples': 3000, 'test/accuracy': 0.400359183549881, 'test/loss': 4.126694679260254, 'test/bleu': 9.021694400708748, 'test/num_examples': 3003, 'score': 866.0806493759155, 'total_duration': 2381.325071334839, 'accumulated_submission_time': 866.0806493759155, 'accumulated_eval_time': 1515.0589277744293, 'accumulated_logging_time': 0.01842665672302246, 'global_step': 2412, 'preemption_count': 0}), (4820, {'train/accuracy': 0.5459850430488586, 'train/loss': 2.7041351795196533, 'train/bleu': 24.537025090133007, 'validation/accuracy': 0.5497985482215881, 'validation/loss': 2.6535212993621826, 'validation/bleu': 20.789592351663433, 'validation/num_examples': 3000, 'test/accuracy': 0.5503765344619751, 'test/loss': 2.6854612827301025, 'test/bleu': 19.508493993622434, 'test/num_examples': 3003, 'score': 1706.1331958770752, 'total_duration': 3709.652266740799, 'accumulated_submission_time': 1706.1331958770752, 'accumulated_eval_time': 2003.1521430015564, 'accumulated_logging_time': 0.03605842590332031, 'global_step': 4820, 'preemption_count': 0}), (7229, {'train/accuracy': 0.5858600735664368, 'train/loss': 2.3224937915802, 'train/bleu': 27.6802262408556, 'validation/accuracy': 0.5911799073219299, 'validation/loss': 2.2754979133605957, 'validation/bleu': 23.32863177280011, 'validation/num_examples': 3000, 'test/accuracy': 0.5931178331375122, 'test/loss': 2.268697500228882, 'test/bleu': 22.097597695085067, 'test/num_examples': 3003, 'score': 2546.1659734249115, 'total_duration': 5026.089684963226, 'accumulated_submission_time': 2546.1659734249115, 'accumulated_eval_time': 2479.3770694732666, 'accumulated_logging_time': 0.0534365177154541, 'global_step': 7229, 'preemption_count': 0}), (9639, {'train/accuracy': 0.595770001411438, 'train/loss': 2.2054076194763184, 'train/bleu': 28.391961989913796, 'validation/accuracy': 0.608953595161438, 'validation/loss': 2.1008777618408203, 'validation/bleu': 24.876423012297934, 'validation/num_examples': 3000, 'test/accuracy': 0.6161510944366455, 'test/loss': 2.0748791694641113, 'test/bleu': 23.814821639047352, 'test/num_examples': 3003, 'score': 3385.9925627708435, 'total_duration': 6348.297238826752, 'accumulated_submission_time': 3385.9925627708435, 'accumulated_eval_time': 2961.5824570655823, 'accumulated_logging_time': 0.07094693183898926, 'global_step': 9639, 'preemption_count': 0}), (12051, {'train/accuracy': 0.6027227640151978, 'train/loss': 2.1442761421203613, 'train/bleu': 29.44077606983374, 'validation/accuracy': 0.6228091716766357, 'validation/loss': 1.9855448007583618, 'validation/bleu': 25.74414990782134, 'validation/num_examples': 3000, 'test/accuracy': 0.6294288039207458, 'test/loss': 1.9414881467819214, 'test/bleu': 24.94115133871372, 'test/num_examples': 3003, 'score': 4225.918093442917, 'total_duration': 7682.394242525101, 'accumulated_submission_time': 4225.918093442917, 'accumulated_eval_time': 3455.5855684280396, 'accumulated_logging_time': 0.08934473991394043, 'global_step': 12051, 'preemption_count': 0}), (14468, {'train/accuracy': 0.6183366179466248, 'train/loss': 2.016723155975342, 'train/bleu': 29.57251553557313, 'validation/accuracy': 0.6347736716270447, 'validation/loss': 1.8878432512283325, 'validation/bleu': 26.489837441616515, 'validation/num_examples': 3000, 'test/accuracy': 0.6437492966651917, 'test/loss': 1.8373864889144897, 'test/bleu': 25.770261872530057, 'test/num_examples': 3003, 'score': 5065.820095539093, 'total_duration': 9006.475695371628, 'accumulated_submission_time': 5065.820095539093, 'accumulated_eval_time': 3939.6018047332764, 'accumulated_logging_time': 0.1076514720916748, 'global_step': 14468, 'preemption_count': 0}), (16886, {'train/accuracy': 0.6191582679748535, 'train/loss': 2.0026276111602783, 'train/bleu': 30.418952942439557, 'validation/accuracy': 0.6432403326034546, 'validation/loss': 1.8318157196044922, 'validation/bleu': 27.093489321429118, 'validation/num_examples': 3000, 'test/accuracy': 0.6507009863853455, 'test/loss': 1.784277081489563, 'test/bleu': 26.60813606557522, 'test/num_examples': 3003, 'score': 5905.813939809799, 'total_duration': 10322.614156723022, 'accumulated_submission_time': 5905.813939809799, 'accumulated_eval_time': 4415.581715583801, 'accumulated_logging_time': 0.12589430809020996, 'global_step': 16886, 'preemption_count': 0}), (19301, {'train/accuracy': 0.6381615400314331, 'train/loss': 1.8695921897888184, 'train/bleu': 30.90519965594945, 'validation/accuracy': 0.6470966339111328, 'validation/loss': 1.797232747077942, 'validation/bleu': 26.589907519100716, 'validation/num_examples': 3000, 'test/accuracy': 0.6560769081115723, 'test/loss': 1.7378911972045898, 'test/bleu': 26.640809227861872, 'test/num_examples': 3003, 'score': 6745.794331073761, 'total_duration': 12033.53022146225, 'accumulated_submission_time': 6745.794331073761, 'accumulated_eval_time': 5286.353251218796, 'accumulated_logging_time': 0.14536213874816895, 'global_step': 19301, 'preemption_count': 0}), (21705, {'train/accuracy': 0.6303253173828125, 'train/loss': 1.922417402267456, 'train/bleu': 30.23589656929473, 'validation/accuracy': 0.6509900689125061, 'validation/loss': 1.7742375135421753, 'validation/bleu': 27.89847240870862, 'validation/num_examples': 3000, 'test/accuracy': 0.6612212061882019, 'test/loss': 1.7151057720184326, 'test/bleu': 26.940625120694804, 'test/num_examples': 3003, 'score': 7585.881723642349, 'total_duration': 13359.607218503952, 'accumulated_submission_time': 7585.881723642349, 'accumulated_eval_time': 5772.176019191742, 'accumulated_logging_time': 0.16701459884643555, 'global_step': 21705, 'preemption_count': 0}), (24111, {'train/accuracy': 0.6299898624420166, 'train/loss': 1.9242160320281982, 'train/bleu': 30.559869686208867, 'validation/accuracy': 0.6520777344703674, 'validation/loss': 1.7576435804367065, 'validation/bleu': 27.915332295007094, 'validation/num_examples': 3000, 'test/accuracy': 0.6603058576583862, 'test/loss': 1.6992652416229248, 'test/bleu': 26.817969834344407, 'test/num_examples': 3003, 'score': 8426.003172159195, 'total_duration': 14799.120358228683, 'accumulated_submission_time': 8426.003172159195, 'accumulated_eval_time': 6371.407712459564, 'accumulated_logging_time': 0.18689775466918945, 'global_step': 24111, 'preemption_count': 0}), (26515, {'train/accuracy': 0.6331630349159241, 'train/loss': 1.889734148979187, 'train/bleu': 30.84656444180887, 'validation/accuracy': 0.6529058814048767, 'validation/loss': 1.7470378875732422, 'validation/bleu': 27.998916500556494, 'validation/num_examples': 3000, 'test/accuracy': 0.6653690338134766, 'test/loss': 1.6885408163070679, 'test/bleu': 27.20259209013743, 'test/num_examples': 3003, 'score': 9265.873390197754, 'total_duration': 16119.394581079483, 'accumulated_submission_time': 9265.873390197754, 'accumulated_eval_time': 6851.652458429337, 'accumulated_logging_time': 0.2075519561767578, 'global_step': 26515, 'preemption_count': 0}), (28919, {'train/accuracy': 0.6342933177947998, 'train/loss': 1.8857753276824951, 'train/bleu': 30.73051204934578, 'validation/accuracy': 0.6558598875999451, 'validation/loss': 1.7290669679641724, 'validation/bleu': 27.94156082790439, 'validation/num_examples': 3000, 'test/accuracy': 0.6678252816200256, 'test/loss': 1.667887806892395, 'test/bleu': 27.628518314499786, 'test/num_examples': 3003, 'score': 10105.777422904968, 'total_duration': 17439.403420209885, 'accumulated_submission_time': 10105.777422904968, 'accumulated_eval_time': 7331.599550008774, 'accumulated_logging_time': 0.22798991203308105, 'global_step': 28919, 'preemption_count': 0}), (31323, {'train/accuracy': 0.6364214420318604, 'train/loss': 1.8872747421264648, 'train/bleu': 30.734913848436165, 'validation/accuracy': 0.6571947932243347, 'validation/loss': 1.7246627807617188, 'validation/bleu': 28.082402193328384, 'validation/num_examples': 3000, 'test/accuracy': 0.6681265234947205, 'test/loss': 1.6692999601364136, 'test/bleu': 27.544509536495223, 'test/num_examples': 3003, 'score': 10945.888687372208, 'total_duration': 18870.652339458466, 'accumulated_submission_time': 10945.888687372208, 'accumulated_eval_time': 7922.577842235565, 'accumulated_logging_time': 0.2482743263244629, 'global_step': 31323, 'preemption_count': 0}), (33740, {'train/accuracy': 0.6405691504478455, 'train/loss': 1.8434520959854126, 'train/bleu': 30.678251681709757, 'validation/accuracy': 0.6594937443733215, 'validation/loss': 1.7051464319229126, 'validation/bleu': 27.97360696459214, 'validation/num_examples': 3000, 'test/accuracy': 0.6690302491188049, 'test/loss': 1.6490671634674072, 'test/bleu': 27.722226741015714, 'test/num_examples': 3003, 'score': 11785.839684009552, 'total_duration': 20213.871381998062, 'accumulated_submission_time': 11785.839684009552, 'accumulated_eval_time': 8425.685530424118, 'accumulated_logging_time': 0.26983189582824707, 'global_step': 33740, 'preemption_count': 0}), (36160, {'train/accuracy': 0.6369484663009644, 'train/loss': 1.86934232711792, 'train/bleu': 31.591586046169475, 'validation/accuracy': 0.6588139533996582, 'validation/loss': 1.7098349332809448, 'validation/bleu': 28.07531150460195, 'validation/num_examples': 3000, 'test/accuracy': 0.6717761754989624, 'test/loss': 1.6449332237243652, 'test/bleu': 27.815180626523215, 'test/num_examples': 3003, 'score': 12625.991487503052, 'total_duration': 21509.43064880371, 'accumulated_submission_time': 12625.991487503052, 'accumulated_eval_time': 8880.933901071548, 'accumulated_logging_time': 0.28970766067504883, 'global_step': 36160, 'preemption_count': 0}), (38578, {'train/accuracy': 0.6456181406974792, 'train/loss': 1.8031083345413208, 'train/bleu': 31.690782831963073, 'validation/accuracy': 0.6602600812911987, 'validation/loss': 1.6966221332550049, 'validation/bleu': 28.18649346345213, 'validation/num_examples': 3000, 'test/accuracy': 0.6722743511199951, 'test/loss': 1.6358877420425415, 'test/bleu': 27.8037680103288, 'test/num_examples': 3003, 'score': 13466.073394536972, 'total_duration': 22867.5717792511, 'accumulated_submission_time': 13466.073394536972, 'accumulated_eval_time': 9398.835209608078, 'accumulated_logging_time': 0.309859037399292, 'global_step': 38578, 'preemption_count': 0}), (40991, {'train/accuracy': 0.6403080821037292, 'train/loss': 1.847672462463379, 'train/bleu': 30.99196682391661, 'validation/accuracy': 0.6626455783843994, 'validation/loss': 1.6861871480941772, 'validation/bleu': 28.170658651857863, 'validation/num_examples': 3000, 'test/accuracy': 0.6758313179016113, 'test/loss': 1.6197723150253296, 'test/bleu': 28.082646092400754, 'test/num_examples': 3003, 'score': 14306.01717877388, 'total_duration': 24208.354489564896, 'accumulated_submission_time': 14306.01717877388, 'accumulated_eval_time': 9899.509933710098, 'accumulated_logging_time': 0.33040642738342285, 'global_step': 40991, 'preemption_count': 0}), (43405, {'train/accuracy': 0.6395072340965271, 'train/loss': 1.8533098697662354, 'train/bleu': 31.874181130620048, 'validation/accuracy': 0.6642523407936096, 'validation/loss': 1.6788249015808105, 'validation/bleu': 28.610164621569854, 'validation/num_examples': 3000, 'test/accuracy': 0.6759124398231506, 'test/loss': 1.613234043121338, 'test/bleu': 28.34741367238577, 'test/num_examples': 3003, 'score': 15146.052419185638, 'total_duration': 25556.410094738007, 'accumulated_submission_time': 15146.052419185638, 'accumulated_eval_time': 10407.3700568676, 'accumulated_logging_time': 0.3514692783355713, 'global_step': 43405, 'preemption_count': 0}), (45819, {'train/accuracy': 0.6451601386070251, 'train/loss': 1.8110008239746094, 'train/bleu': 31.79004667094335, 'validation/accuracy': 0.6654388904571533, 'validation/loss': 1.6698273420333862, 'validation/bleu': 28.9114861017733, 'validation/num_examples': 3000, 'test/accuracy': 0.6774881482124329, 'test/loss': 1.6059889793395996, 'test/bleu': 28.60628702110906, 'test/num_examples': 3003, 'score': 15986.023608207703, 'total_duration': 26866.517375707626, 'accumulated_submission_time': 15986.023608207703, 'accumulated_eval_time': 10877.34483742714, 'accumulated_logging_time': 0.37587738037109375, 'global_step': 45819, 'preemption_count': 0}), (48233, {'train/accuracy': 0.640514075756073, 'train/loss': 1.8414708375930786, 'train/bleu': 31.60429646668812, 'validation/accuracy': 0.6651793718338013, 'validation/loss': 1.6687612533569336, 'validation/bleu': 28.65014278320386, 'validation/num_examples': 3000, 'test/accuracy': 0.6791217923164368, 'test/loss': 1.6001701354980469, 'test/bleu': 28.465046636333845, 'test/num_examples': 3003, 'score': 16825.920708179474, 'total_duration': 28186.313233852386, 'accumulated_submission_time': 16825.920708179474, 'accumulated_eval_time': 11357.083236694336, 'accumulated_logging_time': 0.39872312545776367, 'global_step': 48233, 'preemption_count': 0}), (50646, {'train/accuracy': 0.6634311676025391, 'train/loss': 1.6947810649871826, 'train/bleu': 32.96347261408606, 'validation/accuracy': 0.6677131652832031, 'validation/loss': 1.6553740501403809, 'validation/bleu': 28.811225517164814, 'validation/num_examples': 3000, 'test/accuracy': 0.6791217923164368, 'test/loss': 1.5924015045166016, 'test/bleu': 28.445858758700652, 'test/num_examples': 3003, 'score': 17665.795039653778, 'total_duration': 29553.83986735344, 'accumulated_submission_time': 17665.795039653778, 'accumulated_eval_time': 11884.57578086853, 'accumulated_logging_time': 0.41984105110168457, 'global_step': 50646, 'preemption_count': 0}), (53060, {'train/accuracy': 0.6471867561340332, 'train/loss': 1.7907124757766724, 'train/bleu': 31.524997127136846, 'validation/accuracy': 0.6661063432693481, 'validation/loss': 1.6556403636932373, 'validation/bleu': 28.577419952640984, 'validation/num_examples': 3000, 'test/accuracy': 0.6812304258346558, 'test/loss': 1.5828485488891602, 'test/bleu': 28.539773456855333, 'test/num_examples': 3003, 'score': 18505.883224487305, 'total_duration': 30922.586253881454, 'accumulated_submission_time': 18505.883224487305, 'accumulated_eval_time': 12413.074045658112, 'accumulated_logging_time': 0.44127511978149414, 'global_step': 53060, 'preemption_count': 0}), (55474, {'train/accuracy': 0.6478999257087708, 'train/loss': 1.7947591543197632, 'train/bleu': 31.104204555931076, 'validation/accuracy': 0.6690356731414795, 'validation/loss': 1.6471420526504517, 'validation/bleu': 29.00787699840739, 'validation/num_examples': 3000, 'test/accuracy': 0.6815664768218994, 'test/loss': 1.574040174484253, 'test/bleu': 28.57670685535395, 'test/num_examples': 3003, 'score': 19345.73685193062, 'total_duration': 32290.945919275284, 'accumulated_submission_time': 19345.73685193062, 'accumulated_eval_time': 12941.422396421432, 'accumulated_logging_time': 0.46326398849487305, 'global_step': 55474, 'preemption_count': 0}), (57888, {'train/accuracy': 0.6565040349960327, 'train/loss': 1.7403749227523804, 'train/bleu': 32.182956096505784, 'validation/accuracy': 0.6694188714027405, 'validation/loss': 1.6355479955673218, 'validation/bleu': 28.777596005391874, 'validation/num_examples': 3000, 'test/accuracy': 0.6819719672203064, 'test/loss': 1.5685808658599854, 'test/bleu': 28.657735946323896, 'test/num_examples': 3003, 'score': 20185.798642635345, 'total_duration': 33636.93588399887, 'accumulated_submission_time': 20185.798642635345, 'accumulated_eval_time': 13447.189587593079, 'accumulated_logging_time': 0.48529982566833496, 'global_step': 57888, 'preemption_count': 0}), (60302, {'train/accuracy': 0.6491498947143555, 'train/loss': 1.7927241325378418, 'train/bleu': 32.08753312460676, 'validation/accuracy': 0.6715200543403625, 'validation/loss': 1.6254328489303589, 'validation/bleu': 29.2614829180244, 'validation/num_examples': 3000, 'test/accuracy': 0.6852856278419495, 'test/loss': 1.5561716556549072, 'test/bleu': 29.075932837787565, 'test/num_examples': 3003, 'score': 21025.7261698246, 'total_duration': 34973.57757258415, 'accumulated_submission_time': 21025.7261698246, 'accumulated_eval_time': 13943.74331855774, 'accumulated_logging_time': 0.5088150501251221, 'global_step': 60302, 'preemption_count': 0}), (62717, {'train/accuracy': 0.6520125865936279, 'train/loss': 1.7698822021484375, 'train/bleu': 31.919736707299073, 'validation/accuracy': 0.6724346876144409, 'validation/loss': 1.6232401132583618, 'validation/bleu': 29.31364298681988, 'validation/num_examples': 3000, 'test/accuracy': 0.6854941248893738, 'test/loss': 1.5551882982254028, 'test/bleu': 28.546150413983344, 'test/num_examples': 3003, 'score': 21865.714310646057, 'total_duration': 36382.397992134094, 'accumulated_submission_time': 21865.714310646057, 'accumulated_eval_time': 14512.416515827179, 'accumulated_logging_time': 0.5313296318054199, 'global_step': 62717, 'preemption_count': 0}), (65131, {'train/accuracy': 0.6570355296134949, 'train/loss': 1.7334505319595337, 'train/bleu': 32.422927077129856, 'validation/accuracy': 0.6735841631889343, 'validation/loss': 1.6154221296310425, 'validation/bleu': 29.122842781581575, 'validation/num_examples': 3000, 'test/accuracy': 0.6868960857391357, 'test/loss': 1.5414601564407349, 'test/bleu': 29.048349351694977, 'test/num_examples': 3003, 'score': 22705.63618540764, 'total_duration': 37691.95536565781, 'accumulated_submission_time': 22705.63618540764, 'accumulated_eval_time': 14981.89564538002, 'accumulated_logging_time': 0.5537524223327637, 'global_step': 65131, 'preemption_count': 0}), (67544, {'train/accuracy': 0.6577116847038269, 'train/loss': 1.7310400009155273, 'train/bleu': 32.39182963686128, 'validation/accuracy': 0.675326943397522, 'validation/loss': 1.6035248041152954, 'validation/bleu': 29.40856387984765, 'validation/num_examples': 3000, 'test/accuracy': 0.6904646158218384, 'test/loss': 1.5232166051864624, 'test/bleu': 29.572496398948413, 'test/num_examples': 3003, 'score': 23545.597474098206, 'total_duration': 39009.042259931564, 'accumulated_submission_time': 23545.597474098206, 'accumulated_eval_time': 15458.857070207596, 'accumulated_logging_time': 0.5776288509368896, 'global_step': 67544, 'preemption_count': 0}), (69958, {'train/accuracy': 0.6651302576065063, 'train/loss': 1.6812773942947388, 'train/bleu': 33.31203898848172, 'validation/accuracy': 0.6760562062263489, 'validation/loss': 1.5955429077148438, 'validation/bleu': 29.334678551980925, 'validation/num_examples': 3000, 'test/accuracy': 0.6904529929161072, 'test/loss': 1.523226022720337, 'test/bleu': 29.406920001466755, 'test/num_examples': 3003, 'score': 24385.615493535995, 'total_duration': 40357.90869808197, 'accumulated_submission_time': 24385.615493535995, 'accumulated_eval_time': 15967.544497013092, 'accumulated_logging_time': 0.6013941764831543, 'global_step': 69958, 'preemption_count': 0}), (72370, {'train/accuracy': 0.6614954471588135, 'train/loss': 1.7066394090652466, 'train/bleu': 32.613614038973076, 'validation/accuracy': 0.6786146759986877, 'validation/loss': 1.5909779071807861, 'validation/bleu': 29.503945444659728, 'validation/num_examples': 3000, 'test/accuracy': 0.6916927695274353, 'test/loss': 1.5103695392608643, 'test/bleu': 29.64421645028372, 'test/num_examples': 3003, 'score': 25225.462176561356, 'total_duration': 41665.78641748428, 'accumulated_submission_time': 25225.462176561356, 'accumulated_eval_time': 16435.415130615234, 'accumulated_logging_time': 0.6256895065307617, 'global_step': 72370, 'preemption_count': 0}), (74775, {'train/accuracy': 0.657471776008606, 'train/loss': 1.7333810329437256, 'train/bleu': 33.01413835298889, 'validation/accuracy': 0.6799001693725586, 'validation/loss': 1.5785874128341675, 'validation/bleu': 29.793749124454397, 'validation/num_examples': 3000, 'test/accuracy': 0.694206953048706, 'test/loss': 1.5045973062515259, 'test/bleu': 29.818785440213592, 'test/num_examples': 3003, 'score': 26065.604117393494, 'total_duration': 42998.489241838455, 'accumulated_submission_time': 26065.604117393494, 'accumulated_eval_time': 16927.813620090485, 'accumulated_logging_time': 0.6527254581451416, 'global_step': 74775, 'preemption_count': 0}), (77181, {'train/accuracy': 0.6651853919029236, 'train/loss': 1.6744420528411865, 'train/bleu': 33.52294921083123, 'validation/accuracy': 0.6806788444519043, 'validation/loss': 1.5769414901733398, 'validation/bleu': 29.514648316043534, 'validation/num_examples': 3000, 'test/accuracy': 0.6961881518363953, 'test/loss': 1.4972673654556274, 'test/bleu': 29.991848523644617, 'test/num_examples': 3003, 'score': 26905.657557964325, 'total_duration': 44373.8246011734, 'accumulated_submission_time': 26905.657557964325, 'accumulated_eval_time': 17462.93500161171, 'accumulated_logging_time': 0.6782655715942383, 'global_step': 77181, 'preemption_count': 0}), (79586, {'train/accuracy': 0.6659746766090393, 'train/loss': 1.6808340549468994, 'train/bleu': 33.02125358446005, 'validation/accuracy': 0.6822361946105957, 'validation/loss': 1.5652544498443604, 'validation/bleu': 30.104736884059864, 'validation/num_examples': 3000, 'test/accuracy': 0.6975321769714355, 'test/loss': 1.4865301847457886, 'test/bleu': 30.256953859068997, 'test/num_examples': 3003, 'score': 27745.499103546143, 'total_duration': 45711.51742267609, 'accumulated_submission_time': 27745.499103546143, 'accumulated_eval_time': 17960.623639583588, 'accumulated_logging_time': 0.7037298679351807, 'global_step': 79586, 'preemption_count': 0}), (81993, {'train/accuracy': 0.6884478330612183, 'train/loss': 1.5379515886306763, 'train/bleu': 34.86726419077571, 'validation/accuracy': 0.6845104098320007, 'validation/loss': 1.5560353994369507, 'validation/bleu': 29.847521220219416, 'validation/num_examples': 3000, 'test/accuracy': 0.6979956030845642, 'test/loss': 1.4797499179840088, 'test/bleu': 30.151530260802794, 'test/num_examples': 3003, 'score': 28585.481604099274, 'total_duration': 47082.3125333786, 'accumulated_submission_time': 28585.481604099274, 'accumulated_eval_time': 18491.26747250557, 'accumulated_logging_time': 0.7338624000549316, 'global_step': 81993, 'preemption_count': 0}), (84406, {'train/accuracy': 0.674456775188446, 'train/loss': 1.6333317756652832, 'train/bleu': 33.751387262258156, 'validation/accuracy': 0.6854497790336609, 'validation/loss': 1.5468295812606812, 'validation/bleu': 30.037420662788662, 'validation/num_examples': 3000, 'test/accuracy': 0.7006256580352783, 'test/loss': 1.4670251607894897, 'test/bleu': 30.272939407489115, 'test/num_examples': 3003, 'score': 29425.350321292877, 'total_duration': 48489.88679552078, 'accumulated_submission_time': 29425.350321292877, 'accumulated_eval_time': 19058.810165643692, 'accumulated_logging_time': 0.7605640888214111, 'global_step': 84406, 'preemption_count': 0}), (86820, {'train/accuracy': 0.6733415126800537, 'train/loss': 1.6406230926513672, 'train/bleu': 33.85104683343075, 'validation/accuracy': 0.6854868531227112, 'validation/loss': 1.5395935773849487, 'validation/bleu': 30.299796071298964, 'validation/num_examples': 3000, 'test/accuracy': 0.7007762789726257, 'test/loss': 1.4608391523361206, 'test/bleu': 30.31391819971247, 'test/num_examples': 3003, 'score': 30265.368695020676, 'total_duration': 49872.0005903244, 'accumulated_submission_time': 30265.368695020676, 'accumulated_eval_time': 19600.744283914566, 'accumulated_logging_time': 0.7873983383178711, 'global_step': 86820, 'preemption_count': 0}), (89236, {'train/accuracy': 0.6885882616043091, 'train/loss': 1.550801396369934, 'train/bleu': 34.46831958993319, 'validation/accuracy': 0.6874892115592957, 'validation/loss': 1.5362476110458374, 'validation/bleu': 30.437435623657887, 'validation/num_examples': 3000, 'test/accuracy': 0.7021550536155701, 'test/loss': 1.4629102945327759, 'test/bleu': 30.25459864119037, 'test/num_examples': 3003, 'score': 31105.43193101883, 'total_duration': 51257.967772483826, 'accumulated_submission_time': 31105.43193101883, 'accumulated_eval_time': 20146.488379716873, 'accumulated_logging_time': 0.8136112689971924, 'global_step': 89236, 'preemption_count': 0}), (91649, {'train/accuracy': 0.6805351972579956, 'train/loss': 1.5954099893569946, 'train/bleu': 34.06174556810005, 'validation/accuracy': 0.6886510252952576, 'validation/loss': 1.5314713716506958, 'validation/bleu': 30.390031041974947, 'validation/num_examples': 3000, 'test/accuracy': 0.7036148905754089, 'test/loss': 1.4470577239990234, 'test/bleu': 30.427888908006608, 'test/num_examples': 3003, 'score': 31945.44021844864, 'total_duration': 52584.40848445892, 'accumulated_submission_time': 31945.44021844864, 'accumulated_eval_time': 20632.76004385948, 'accumulated_logging_time': 0.8397274017333984, 'global_step': 91649, 'preemption_count': 0}), (94062, {'train/accuracy': 0.6818845868110657, 'train/loss': 1.589035153388977, 'train/bleu': 33.87692341164172, 'validation/accuracy': 0.6899241209030151, 'validation/loss': 1.5212359428405762, 'validation/bleu': 30.746544761793338, 'validation/num_examples': 3000, 'test/accuracy': 0.7048546075820923, 'test/loss': 1.4391686916351318, 'test/bleu': 30.68938342621374, 'test/num_examples': 3003, 'score': 32785.350706100464, 'total_duration': 53914.26307940483, 'accumulated_submission_time': 32785.350706100464, 'accumulated_eval_time': 21122.535757303238, 'accumulated_logging_time': 0.8692505359649658, 'global_step': 94062, 'preemption_count': 0}), (96475, {'train/accuracy': 0.688685953617096, 'train/loss': 1.5498021841049194, 'train/bleu': 34.59552767526004, 'validation/accuracy': 0.6897263526916504, 'validation/loss': 1.5207273960113525, 'validation/bleu': 30.196592470561175, 'validation/num_examples': 3000, 'test/accuracy': 0.703649640083313, 'test/loss': 1.4389573335647583, 'test/bleu': 30.592569004955397, 'test/num_examples': 3003, 'score': 33625.52310419083, 'total_duration': 55274.94172143936, 'accumulated_submission_time': 33625.52310419083, 'accumulated_eval_time': 21642.870022773743, 'accumulated_logging_time': 0.897902250289917, 'global_step': 96475, 'preemption_count': 0}), (98888, {'train/accuracy': 0.6855859756469727, 'train/loss': 1.570130705833435, 'train/bleu': 34.784487084954826, 'validation/accuracy': 0.6906657218933105, 'validation/loss': 1.512605905532837, 'validation/bleu': 30.67338966187723, 'validation/num_examples': 3000, 'test/accuracy': 0.7056192755699158, 'test/loss': 1.4315801858901978, 'test/bleu': 30.89607037139512, 'test/num_examples': 3003, 'score': 34465.56069326401, 'total_duration': 56662.50332283974, 'accumulated_submission_time': 34465.56069326401, 'accumulated_eval_time': 22190.221782445908, 'accumulated_logging_time': 0.9279346466064453, 'global_step': 98888, 'preemption_count': 0}), (101304, {'train/accuracy': 0.7007566094398499, 'train/loss': 1.4874826669692993, 'train/bleu': 36.06102217404117, 'validation/accuracy': 0.6931253671646118, 'validation/loss': 1.5058908462524414, 'validation/bleu': 30.69361796346096, 'validation/num_examples': 3000, 'test/accuracy': 0.7080639600753784, 'test/loss': 1.4224376678466797, 'test/bleu': 30.92272888077162, 'test/num_examples': 3003, 'score': 35305.692708969116, 'total_duration': 58023.91221046448, 'accumulated_submission_time': 35305.692708969116, 'accumulated_eval_time': 22711.333723783493, 'accumulated_logging_time': 0.9560112953186035, 'global_step': 101304, 'preemption_count': 0}), (103723, {'train/accuracy': 0.6987356543540955, 'train/loss': 1.4958245754241943, 'train/bleu': 35.160060051405345, 'validation/accuracy': 0.6947197914123535, 'validation/loss': 1.5008476972579956, 'validation/bleu': 30.791894049455433, 'validation/num_examples': 3000, 'test/accuracy': 0.7088981866836548, 'test/loss': 1.4161386489868164, 'test/bleu': 31.152641072411846, 'test/num_examples': 3003, 'score': 36145.80793952942, 'total_duration': 59402.21764707565, 'accumulated_submission_time': 36145.80793952942, 'accumulated_eval_time': 23249.359160900116, 'accumulated_logging_time': 0.9842977523803711, 'global_step': 103723, 'preemption_count': 0}), (106142, {'train/accuracy': 0.6960100531578064, 'train/loss': 1.511432409286499, 'train/bleu': 35.66017449228269, 'validation/accuracy': 0.6939411163330078, 'validation/loss': 1.4963687658309937, 'validation/bleu': 30.933116003000794, 'validation/num_examples': 3000, 'test/accuracy': 0.7096049189567566, 'test/loss': 1.413786768913269, 'test/bleu': 31.19889851098314, 'test/num_examples': 3003, 'score': 36985.72701525688, 'total_duration': 60734.80958008766, 'accumulated_submission_time': 36985.72701525688, 'accumulated_eval_time': 23741.86478495598, 'accumulated_logging_time': 1.0150835514068604, 'global_step': 106142, 'preemption_count': 0})], 'global_step': 106142}
I0306 12:06:11.775536 139852506842304 submission_runner.py:649] Timing: 36985.72701525688
I0306 12:06:11.775573 139852506842304 submission_runner.py:651] Total number of evals: 45
I0306 12:06:11.775603 139852506842304 submission_runner.py:652] ====================
I0306 12:06:11.775725 139852506842304 submission_runner.py:750] Final wmt score: 0
