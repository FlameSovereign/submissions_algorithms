python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=640306864 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-05.log
2025-03-05 19:12:06.259430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201926.283644       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201926.290914       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:13.348521 139885236581568 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax.
I0305 19:12:14.168174 139885236581568 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:14.171178 139885236581568 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:14.172909 139885236581568 submission_runner.py:606] Using RNG seed 640306864
I0305 19:12:14.779106 139885236581568 submission_runner.py:615] --- Tuning run 5/5 ---
I0305 19:12:14.779296 139885236581568 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_5.
I0305 19:12:14.779479 139885236581568 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_5/hparams.json.
I0305 19:12:15.007876 139885236581568 submission_runner.py:218] Initializing dataset.
I0305 19:12:15.160050 139885236581568 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:15.167349 139885236581568 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:15.246332 139885236581568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:16.607089 139885236581568 submission_runner.py:229] Initializing model.
I0305 19:12:55.940274 139885236581568 submission_runner.py:272] Initializing optimizer.
I0305 19:12:56.729248 139885236581568 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:56.729473 139885236581568 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:56.730433 139885236581568 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_5 with prefix checkpoint_
I0305 19:12:56.730524 139885236581568 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_5/meta_data_0.json.
I0305 19:12:56.730703 139885236581568 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:56.730762 139885236581568 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:56.920438 139885236581568 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/wmt_jax/trial_5/flags_0.json.
I0305 19:12:56.953252 139885236581568 submission_runner.py:337] Starting training loop.
I0305 19:13:33.173426 139748941084416 logging_writer.py:48] [0] global_step=0, grad_norm=5.238467693328857, loss=11.085379600524902
I0305 19:13:33.241320 139885236581568 spec.py:321] Evaluating on the training split.
I0305 19:13:33.243897 139885236581568 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:33.248700 139885236581568 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:33.281598 139885236581568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:39.611766 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 19:18:44.044829 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 19:18:44.079866 139885236581568 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:18:44.085731 139885236581568 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:18:44.116169 139885236581568 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:18:49.384878 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 19:23:48.003150 139885236581568 spec.py:349] Evaluating on the test split.
I0305 19:23:48.005598 139885236581568 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:23:48.008605 139885236581568 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:23:48.039001 139885236581568 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:23:50.791186 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 19:28:49.278620 139885236581568 submission_runner.py:469] Time since start: 952.33s, 	Step: 1, 	{'train/accuracy': 0.0005968984332866967, 'train/loss': 11.112054824829102, 'train/bleu': 1.3999307104715225e-05, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.112346649169922, 'validation/bleu': 3.0022342653337152e-06, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.119194984436035, 'test/bleu': 5.469450157135306e-06, 'test/num_examples': 3003, 'score': 36.287920236587524, 'total_duration': 952.3253033161163, 'accumulated_submission_time': 36.287920236587524, 'accumulated_eval_time': 916.0372471809387, 'accumulated_logging_time': 0}
I0305 19:28:49.285423 139742623868672 logging_writer.py:48] [1] accumulated_eval_time=916.037, accumulated_logging_time=0, accumulated_submission_time=36.2879, global_step=1, preemption_count=0, score=36.2879, test/accuracy=0.000718341, test/bleu=5.46945e-06, test/loss=11.1192, test/num_examples=3003, total_duration=952.325, train/accuracy=0.000596898, train/bleu=1.39993e-05, train/loss=11.1121, validation/accuracy=0.000482041, validation/bleu=3.00223e-06, validation/loss=11.1123, validation/num_examples=3000
I0305 19:29:24.493098 139742615475968 logging_writer.py:48] [100] global_step=100, grad_norm=0.18708957731723785, loss=8.133382797241211
I0305 19:29:59.664939 139742623868672 logging_writer.py:48] [200] global_step=200, grad_norm=0.330698698759079, loss=7.390553951263428
I0305 19:30:34.863873 139742615475968 logging_writer.py:48] [300] global_step=300, grad_norm=0.3501361012458801, loss=6.790492534637451
I0305 19:31:10.070312 139742623868672 logging_writer.py:48] [400] global_step=400, grad_norm=0.42086169123649597, loss=6.243640422821045
I0305 19:31:45.297024 139742615475968 logging_writer.py:48] [500] global_step=500, grad_norm=0.5400691628456116, loss=5.7539825439453125
I0305 19:32:20.513408 139742623868672 logging_writer.py:48] [600] global_step=600, grad_norm=0.6512781381607056, loss=5.513459205627441
I0305 19:32:55.772753 139742615475968 logging_writer.py:48] [700] global_step=700, grad_norm=0.583473801612854, loss=5.190279960632324
I0305 19:33:31.000761 139742623868672 logging_writer.py:48] [800] global_step=800, grad_norm=0.5678462982177734, loss=4.974395751953125
I0305 19:34:06.266599 139742615475968 logging_writer.py:48] [900] global_step=900, grad_norm=0.5393055081367493, loss=4.739295959472656
I0305 19:34:41.490363 139742623868672 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5428556203842163, loss=4.404098987579346
I0305 19:35:16.719224 139742615475968 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.46825307607650757, loss=4.224225044250488
I0305 19:35:51.996288 139742623868672 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5644243955612183, loss=3.931105613708496
I0305 19:36:27.226545 139742615475968 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.47259002923965454, loss=3.7630746364593506
I0305 19:37:02.434891 139742623868672 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7233316898345947, loss=3.8329906463623047
I0305 19:37:37.690956 139742615475968 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.48783329129219055, loss=3.5413658618927
I0305 19:38:12.954265 139742623868672 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.579649806022644, loss=3.4380061626434326
I0305 19:38:48.207777 139742615475968 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5012421607971191, loss=3.405329704284668
I0305 19:39:23.462813 139742623868672 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5079811215400696, loss=3.3219759464263916
I0305 19:39:58.691597 139742615475968 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.3995463252067566, loss=3.2351417541503906
I0305 19:40:33.919040 139742623868672 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.35087457299232483, loss=3.1069908142089844
I0305 19:41:09.126574 139742615475968 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.4115995168685913, loss=3.086005926132202
I0305 19:41:44.379862 139742623868672 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4046269357204437, loss=3.0510122776031494
I0305 19:42:19.640447 139742615475968 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.3434140086174011, loss=3.0486087799072266
I0305 19:42:49.574981 139885236581568 spec.py:321] Evaluating on the training split.
I0305 19:42:52.205896 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 19:45:53.478296 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 19:45:56.090045 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 19:48:49.427197 139885236581568 spec.py:349] Evaluating on the test split.
I0305 19:48:52.018669 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 19:51:25.961373 139885236581568 submission_runner.py:469] Time since start: 2309.01s, 	Step: 2386, 	{'train/accuracy': 0.5155846476554871, 'train/loss': 2.830928087234497, 'train/bleu': 22.62922485475278, 'validation/accuracy': 0.5207648277282715, 'validation/loss': 2.7797136306762695, 'validation/bleu': 18.609285283978583, 'validation/num_examples': 3000, 'test/accuracy': 0.5181555151939392, 'test/loss': 2.8277132511138916, 'test/bleu': 17.034169741275207, 'test/num_examples': 3003, 'score': 876.4308915138245, 'total_duration': 2309.0080399513245, 'accumulated_submission_time': 876.4308915138245, 'accumulated_eval_time': 1432.4235861301422, 'accumulated_logging_time': 0.01533961296081543}
I0305 19:51:25.969746 139742171580160 logging_writer.py:48] [2386] accumulated_eval_time=1432.42, accumulated_logging_time=0.0153396, accumulated_submission_time=876.431, global_step=2386, preemption_count=0, score=876.431, test/accuracy=0.518156, test/bleu=17.0342, test/loss=2.82771, test/num_examples=3003, total_duration=2309.01, train/accuracy=0.515585, train/bleu=22.6292, train/loss=2.83093, validation/accuracy=0.520765, validation/bleu=18.6093, validation/loss=2.77971, validation/num_examples=3000
I0305 19:51:31.232667 139742163187456 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3214395344257355, loss=3.0202786922454834
I0305 19:52:06.329712 139742171580160 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.29268988966941833, loss=2.841719627380371
I0305 19:52:41.512404 139742163187456 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.28131136298179626, loss=2.867050886154175
I0305 19:53:16.804511 139742171580160 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.25787580013275146, loss=2.829270839691162
I0305 19:53:52.048974 139742163187456 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.27197590470314026, loss=2.7546374797821045
I0305 19:54:27.308187 139742171580160 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.30755582451820374, loss=2.760549545288086
I0305 19:55:02.536120 139742163187456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.21546266973018646, loss=2.7211990356445312
I0305 19:55:37.784320 139742171580160 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.21779398620128632, loss=2.6216859817504883
I0305 19:56:13.027085 139742163187456 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.2268783450126648, loss=2.619176149368286
I0305 19:56:48.296396 139742171580160 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.21732158958911896, loss=2.553957223892212
I0305 19:57:23.531356 139742163187456 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.18816536664962769, loss=2.5823581218719482
I0305 19:57:58.766355 139742171580160 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.20544782280921936, loss=2.557995319366455
I0305 19:58:34.034972 139742163187456 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.19169390201568604, loss=2.5625619888305664
I0305 19:59:09.275353 139742171580160 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.18179470300674438, loss=2.4367544651031494
I0305 19:59:44.499792 139742163187456 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.19136308133602142, loss=2.4179646968841553
I0305 20:00:19.745957 139742171580160 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.19960059225559235, loss=2.4426686763763428
I0305 20:00:54.995124 139742163187456 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.17165514826774597, loss=2.448286771774292
I0305 20:01:30.265671 139742171580160 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.21021124720573425, loss=2.3535892963409424
I0305 20:02:05.506932 139742163187456 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.20713773369789124, loss=2.410313844680786
I0305 20:02:40.721535 139742171580160 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.16572050750255585, loss=2.478703737258911
I0305 20:03:15.988637 139742163187456 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.1542172133922577, loss=2.3603522777557373
I0305 20:03:51.271965 139742171580160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.16809099912643433, loss=2.3426601886749268
I0305 20:04:26.499193 139742163187456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.1629137098789215, loss=2.335857629776001
I0305 20:05:01.762443 139742171580160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.14983868598937988, loss=2.362137794494629
I0305 20:05:26.095677 139885236581568 spec.py:321] Evaluating on the training split.
I0305 20:05:28.715212 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:08:03.632184 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 20:08:06.234418 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:10:36.178244 139885236581568 spec.py:349] Evaluating on the test split.
I0305 20:10:38.773421 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:13:03.210009 139885236581568 submission_runner.py:469] Time since start: 3606.26s, 	Step: 4770, 	{'train/accuracy': 0.5809102058410645, 'train/loss': 2.2332332134246826, 'train/bleu': 26.847404537341863, 'validation/accuracy': 0.5918596982955933, 'validation/loss': 2.131425142288208, 'validation/bleu': 23.74768278772634, 'validation/num_examples': 3000, 'test/accuracy': 0.5961418151855469, 'test/loss': 2.1029398441314697, 'test/bleu': 22.01586094768627, 'test/num_examples': 3003, 'score': 1716.410752773285, 'total_duration': 3606.2566981315613, 'accumulated_submission_time': 1716.410752773285, 'accumulated_eval_time': 1889.5378682613373, 'accumulated_logging_time': 0.032012224197387695}
I0305 20:13:03.219406 139742163187456 logging_writer.py:48] [4770] accumulated_eval_time=1889.54, accumulated_logging_time=0.0320122, accumulated_submission_time=1716.41, global_step=4770, preemption_count=0, score=1716.41, test/accuracy=0.596142, test/bleu=22.0159, test/loss=2.10294, test/num_examples=3003, total_duration=3606.26, train/accuracy=0.58091, train/bleu=26.8474, train/loss=2.23323, validation/accuracy=0.59186, validation/bleu=23.7477, validation/loss=2.13143, validation/num_examples=3000
I0305 20:13:14.085990 139742171580160 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.16587044298648834, loss=2.2142298221588135
I0305 20:13:49.176294 139742163187456 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.15724529325962067, loss=2.2290031909942627
I0305 20:14:24.376832 139742171580160 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1510888785123825, loss=2.241664409637451
I0305 20:14:59.609380 139742163187456 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.15962979197502136, loss=2.2839667797088623
I0305 20:15:34.840004 139742171580160 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.14250926673412323, loss=2.271406650543213
I0305 20:16:10.081158 139742163187456 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.14042475819587708, loss=2.266806125640869
I0305 20:16:45.319402 139742171580160 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.14304177463054657, loss=2.2288715839385986
I0305 20:17:20.568834 139742163187456 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.14732487499713898, loss=2.2816638946533203
I0305 20:17:55.780970 139742171580160 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.152761310338974, loss=2.2124972343444824
I0305 20:18:31.040339 139742163187456 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.16072845458984375, loss=2.199129343032837
I0305 20:19:06.329456 139742171580160 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.17346864938735962, loss=2.176744222640991
I0305 20:19:41.560249 139742163187456 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.15883491933345795, loss=2.2071478366851807
I0305 20:20:16.844996 139742171580160 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.1677244007587433, loss=2.171565532684326
I0305 20:20:52.078321 139742163187456 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.15468357503414154, loss=2.2673981189727783
I0305 20:21:27.319685 139742171580160 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.15211188793182373, loss=2.248894214630127
I0305 20:22:02.549010 139742096045824 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.14243273437023163, loss=2.2809603214263916
I0305 20:22:37.752087 139742087653120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.19803482294082642, loss=2.140575408935547
I0305 20:23:12.962114 139742096045824 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.26318037509918213, loss=2.081834316253662
I0305 20:23:48.147874 139742087653120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.15827085077762604, loss=2.1186683177948
I0305 20:24:23.333402 139742096045824 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.15189266204833984, loss=2.258988618850708
I0305 20:24:58.513961 139742087653120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.17503412067890167, loss=2.1878013610839844
I0305 20:25:33.691738 139742096045824 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.31042203307151794, loss=2.1844944953918457
I0305 20:26:08.863853 139742087653120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.16291296482086182, loss=2.1932296752929688
I0305 20:26:44.047748 139742096045824 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.16552719473838806, loss=2.14436936378479
I0305 20:27:03.431286 139885236581568 spec.py:321] Evaluating on the training split.
I0305 20:27:06.052527 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:29:57.970915 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 20:30:00.565844 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:32:39.545542 139885236581568 spec.py:349] Evaluating on the test split.
I0305 20:32:42.141919 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:35:05.854038 139885236581568 submission_runner.py:469] Time since start: 4928.90s, 	Step: 7156, 	{'train/accuracy': 0.6053150296211243, 'train/loss': 2.010892629623413, 'train/bleu': 29.075266441995126, 'validation/accuracy': 0.6186809539794922, 'validation/loss': 1.9145147800445557, 'validation/bleu': 25.12629937778364, 'validation/num_examples': 3000, 'test/accuracy': 0.6237632036209106, 'test/loss': 1.8658379316329956, 'test/bleu': 24.159114578400022, 'test/num_examples': 3003, 'score': 2556.4836115837097, 'total_duration': 4928.900734901428, 'accumulated_submission_time': 2556.4836115837097, 'accumulated_eval_time': 2371.9605751037598, 'accumulated_logging_time': 0.04958701133728027}
I0305 20:35:05.862859 139742087653120 logging_writer.py:48] [7156] accumulated_eval_time=2371.96, accumulated_logging_time=0.049587, accumulated_submission_time=2556.48, global_step=7156, preemption_count=0, score=2556.48, test/accuracy=0.623763, test/bleu=24.1591, test/loss=1.86584, test/num_examples=3003, total_duration=4928.9, train/accuracy=0.605315, train/bleu=29.0753, train/loss=2.01089, validation/accuracy=0.618681, validation/bleu=25.1263, validation/loss=1.91451, validation/num_examples=3000
I0305 20:35:21.591340 139742096045824 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.14157183468341827, loss=2.0891079902648926
I0305 20:35:56.600917 139742087653120 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.2133646309375763, loss=2.168715476989746
I0305 20:36:31.739226 139742096045824 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.15474534034729004, loss=2.1959004402160645
I0305 20:37:06.899793 139742087653120 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.16733179986476898, loss=2.1030216217041016
I0305 20:37:42.071631 139742096045824 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.14474830031394958, loss=2.0269646644592285
I0305 20:38:17.244439 139742087653120 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.15735206007957458, loss=2.101504325866699
I0305 20:38:52.415995 139742096045824 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1398938000202179, loss=2.0939788818359375
I0305 20:39:27.575700 139742087653120 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.14524869620800018, loss=2.093244791030884
I0305 20:40:02.764074 139742096045824 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.15708781778812408, loss=2.1623952388763428
I0305 20:40:37.975712 139742087653120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.15121515095233917, loss=2.0861685276031494
I0305 20:41:13.181049 139742096045824 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.14881087839603424, loss=2.08133602142334
I0305 20:41:48.339310 139742087653120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.17334988713264465, loss=2.043067216873169
I0305 20:42:23.527418 139742096045824 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.15570494532585144, loss=2.1109635829925537
I0305 20:42:58.667752 139742087653120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1545702964067459, loss=2.069512367248535
I0305 20:43:33.888227 139742096045824 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16044718027114868, loss=1.9669419527053833
I0305 20:44:09.076272 139742087653120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.16785281896591187, loss=2.0841920375823975
I0305 20:44:44.240925 139742096045824 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1957549899816513, loss=1.9657057523727417
I0305 20:45:19.435651 139742087653120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.14836250245571136, loss=2.01997447013855
I0305 20:45:54.610619 139742096045824 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.14256855845451355, loss=2.1067793369293213
I0305 20:46:29.808100 139742087653120 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.14991533756256104, loss=2.027614116668701
I0305 20:47:04.964360 139742096045824 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.15773771703243256, loss=2.0456364154815674
I0305 20:47:40.131609 139742087653120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.17624661326408386, loss=2.0203864574432373
I0305 20:48:15.313555 139742096045824 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.1451103389263153, loss=1.9824562072753906
I0305 20:48:50.474519 139742087653120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.14328862726688385, loss=2.0165839195251465
I0305 20:49:05.947007 139885236581568 spec.py:321] Evaluating on the training split.
I0305 20:49:08.576678 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:51:54.233136 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 20:51:56.838363 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:54:30.316404 139885236581568 spec.py:349] Evaluating on the test split.
I0305 20:54:32.919881 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 20:56:53.958022 139885236581568 submission_runner.py:469] Time since start: 6237.00s, 	Step: 9545, 	{'train/accuracy': 0.6147077083587646, 'train/loss': 1.9319205284118652, 'train/bleu': 29.794039277321136, 'validation/accuracy': 0.6319308876991272, 'validation/loss': 1.8102539777755737, 'validation/bleu': 26.293252720394303, 'validation/num_examples': 3000, 'test/accuracy': 0.640447199344635, 'test/loss': 1.7457208633422852, 'test/bleu': 25.006314914270256, 'test/num_examples': 3003, 'score': 3396.4345319271088, 'total_duration': 6237.004716396332, 'accumulated_submission_time': 3396.4345319271088, 'accumulated_eval_time': 2839.97154211998, 'accumulated_logging_time': 0.06656622886657715}
I0305 20:56:53.966944 139742096045824 logging_writer.py:48] [9545] accumulated_eval_time=2839.97, accumulated_logging_time=0.0665662, accumulated_submission_time=3396.43, global_step=9545, preemption_count=0, score=3396.43, test/accuracy=0.640447, test/bleu=25.0063, test/loss=1.74572, test/num_examples=3003, total_duration=6237, train/accuracy=0.614708, train/bleu=29.794, train/loss=1.93192, validation/accuracy=0.631931, validation/bleu=26.2933, validation/loss=1.81025, validation/num_examples=3000
I0305 20:57:13.536937 139742087653120 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.1529245525598526, loss=1.9269243478775024
I0305 20:57:48.597621 139742096045824 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.15135717391967773, loss=1.964537501335144
I0305 20:58:23.754843 139742087653120 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.17941313982009888, loss=2.0134572982788086
I0305 20:58:58.974348 139742096045824 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.16553720831871033, loss=2.0230417251586914
I0305 20:59:34.144189 139742087653120 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.1542801558971405, loss=2.037341594696045
I0305 21:00:09.319041 139742096045824 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.18684299290180206, loss=1.9803595542907715
I0305 21:00:44.516162 139742087653120 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.19150783121585846, loss=2.060096263885498
I0305 21:01:19.713143 139742096045824 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.15605278313159943, loss=1.96053946018219
I0305 21:01:54.860355 139742087653120 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17080548405647278, loss=2.0142688751220703
I0305 21:02:30.080383 139742096045824 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.19639573991298676, loss=2.01879620552063
I0305 21:03:05.298769 139742087653120 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.16959422826766968, loss=2.0441296100616455
I0305 21:03:40.494572 139742096045824 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.19883641600608826, loss=2.002761125564575
I0305 21:04:15.656028 139742087653120 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.16068290174007416, loss=2.0227932929992676
I0305 21:04:50.833692 139742096045824 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1877623349428177, loss=2.044790267944336
I0305 21:05:26.017423 139742087653120 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.17851124703884125, loss=1.9421511888504028
I0305 21:06:01.164067 139742096045824 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.15506196022033691, loss=1.9542839527130127
I0305 21:06:36.329704 139742087653120 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.22204536199569702, loss=1.9804881811141968
I0305 21:07:11.532505 139742096045824 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.17294608056545258, loss=1.964072823524475
I0305 21:07:46.687616 139742087653120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1639297902584076, loss=1.9797247648239136
I0305 21:08:21.868367 139742096045824 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.15022055804729462, loss=1.9620510339736938
I0305 21:08:57.088193 139742087653120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1801251918077469, loss=2.0030150413513184
I0305 21:09:32.305016 139742096045824 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2391868382692337, loss=1.9503117799758911
I0305 21:10:07.486006 139742087653120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.21069292724132538, loss=1.9699902534484863
I0305 21:10:42.666919 139742096045824 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.18407829105854034, loss=1.9451669454574585
I0305 21:10:54.277404 139885236581568 spec.py:321] Evaluating on the training split.
I0305 21:10:56.897157 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 21:13:56.936088 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 21:13:59.525055 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 21:16:32.334941 139885236581568 spec.py:349] Evaluating on the test split.
I0305 21:16:34.936674 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 21:19:02.833127 139885236581568 submission_runner.py:469] Time since start: 7565.88s, 	Step: 11934, 	{'train/accuracy': 0.619958221912384, 'train/loss': 1.8879899978637695, 'train/bleu': 30.050480550476113, 'validation/accuracy': 0.6389884948730469, 'validation/loss': 1.7453337907791138, 'validation/bleu': 26.689153439416398, 'validation/num_examples': 3000, 'test/accuracy': 0.6483373641967773, 'test/loss': 1.681549310684204, 'test/bleu': 25.503032994386196, 'test/num_examples': 3003, 'score': 4236.611150741577, 'total_duration': 7565.879806756973, 'accumulated_submission_time': 4236.611150741577, 'accumulated_eval_time': 3328.52721619606, 'accumulated_logging_time': 0.08384037017822266}
I0305 21:19:02.842292 139742087653120 logging_writer.py:48] [11934] accumulated_eval_time=3328.53, accumulated_logging_time=0.0838404, accumulated_submission_time=4236.61, global_step=11934, preemption_count=0, score=4236.61, test/accuracy=0.648337, test/bleu=25.503, test/loss=1.68155, test/num_examples=3003, total_duration=7565.88, train/accuracy=0.619958, train/bleu=30.0505, train/loss=1.88799, validation/accuracy=0.638988, validation/bleu=26.6892, validation/loss=1.74533, validation/num_examples=3000
I0305 21:19:26.263155 139742096045824 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.15691639482975006, loss=1.9536004066467285
I0305 21:20:01.290640 139742087653120 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.2128685563802719, loss=1.9539624452590942
I0305 21:20:36.480429 139742096045824 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.17976778745651245, loss=1.949419379234314
I0305 21:21:11.752844 139742087653120 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1617906093597412, loss=1.93572199344635
I0305 21:21:46.954749 139742096045824 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.21947044134140015, loss=1.9318948984146118
I0305 21:22:22.133863 139742087653120 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.15622830390930176, loss=1.969681739807129
I0305 21:22:57.321633 139742096045824 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.15715749561786652, loss=1.9399769306182861
I0305 21:23:32.547237 139742087653120 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1731640100479126, loss=1.958558201789856
I0305 21:24:07.792366 139742096045824 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.24622048437595367, loss=1.9514600038528442
I0305 21:24:43.027008 139742087653120 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.17322340607643127, loss=1.970230221748352
I0305 21:25:18.237574 139742096045824 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.19055864214897156, loss=1.9488011598587036
I0305 21:25:53.447212 139742087653120 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.16963917016983032, loss=1.9387907981872559
I0305 21:26:28.642276 139742096045824 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.17146477103233337, loss=1.975495457649231
I0305 21:27:03.852782 139742087653120 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.19101014733314514, loss=1.869402527809143
I0305 21:27:39.119582 139742096045824 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1810137778520584, loss=1.970151662826538
I0305 21:28:14.362807 139742087653120 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16858337819576263, loss=1.9994966983795166
I0305 21:28:49.588042 139742096045824 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3158819377422333, loss=1.9557130336761475
I0305 21:29:24.809524 139742087653120 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.16183817386627197, loss=1.949413776397705
I0305 21:30:00.029195 139742096045824 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.1738528609275818, loss=1.9274814128875732
I0305 21:30:35.239297 139742087653120 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.17118613421916962, loss=1.8798396587371826
I0305 21:31:10.446679 139742096045824 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.22945083677768707, loss=1.9809595346450806
I0305 21:31:45.672026 139742087653120 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.260768860578537, loss=1.9810787439346313
I0305 21:32:20.881211 139742096045824 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.17381201684474945, loss=1.9721851348876953
I0305 21:32:56.081272 139742087653120 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.16446909308433533, loss=1.917296051979065
I0305 21:33:03.117550 139885236581568 spec.py:321] Evaluating on the training split.
I0305 21:33:05.731610 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 21:36:41.457618 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 21:36:44.050345 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 21:39:17.734545 139885236581568 spec.py:349] Evaluating on the test split.
I0305 21:39:20.331492 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 21:41:34.818627 139885236581568 submission_runner.py:469] Time since start: 8917.87s, 	Step: 14321, 	{'train/accuracy': 0.6277436017990112, 'train/loss': 1.8285695314407349, 'train/bleu': 30.518571804021665, 'validation/accuracy': 0.6443403363227844, 'validation/loss': 1.7072266340255737, 'validation/bleu': 27.121178480830874, 'validation/num_examples': 3000, 'test/accuracy': 0.6547097563743591, 'test/loss': 1.6385301351547241, 'test/bleu': 26.32388591131773, 'test/num_examples': 3003, 'score': 5076.754111766815, 'total_duration': 8917.865320682526, 'accumulated_submission_time': 5076.754111766815, 'accumulated_eval_time': 3840.2282617092133, 'accumulated_logging_time': 0.10098695755004883}
I0305 21:41:34.828929 139742096045824 logging_writer.py:48] [14321] accumulated_eval_time=3840.23, accumulated_logging_time=0.100987, accumulated_submission_time=5076.75, global_step=14321, preemption_count=0, score=5076.75, test/accuracy=0.65471, test/bleu=26.3239, test/loss=1.63853, test/num_examples=3003, total_duration=8917.87, train/accuracy=0.627744, train/bleu=30.5186, train/loss=1.82857, validation/accuracy=0.64434, validation/bleu=27.1212, validation/loss=1.70723, validation/num_examples=3000
I0305 21:42:02.852386 139742087653120 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2657431960105896, loss=1.9318376779556274
I0305 21:42:38.025179 139742096045824 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1642424315214157, loss=1.8774657249450684
I0305 21:43:13.239497 139742087653120 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.3070043921470642, loss=1.9165314435958862
I0305 21:43:48.462774 139742096045824 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.21912571787834167, loss=1.8904293775558472
I0305 21:44:23.706701 139742087653120 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.20238956809043884, loss=1.956705927848816
I0305 21:44:58.906767 139742096045824 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.17893391847610474, loss=1.928874135017395
I0305 21:45:34.101198 139742087653120 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18070945143699646, loss=1.856581687927246
I0305 21:46:09.284478 139742096045824 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2246134728193283, loss=1.902639389038086
I0305 21:46:44.482226 139742087653120 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.15984328091144562, loss=1.8972724676132202
I0305 21:47:19.713206 139742096045824 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.18411314487457275, loss=1.842300534248352
I0305 21:47:54.926341 139742087653120 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.18098407983779907, loss=1.8662514686584473
I0305 21:48:30.125017 139742096045824 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.19865018129348755, loss=2.0589144229888916
I0305 21:49:05.324496 139742087653120 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.1678134799003601, loss=1.8869634866714478
I0305 21:49:40.521927 139742096045824 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.15140469372272491, loss=1.8897408246994019
I0305 21:50:15.724295 139742087653120 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.16470076143741608, loss=1.8459056615829468
I0305 21:50:50.941349 139742096045824 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.18613971769809723, loss=1.8891557455062866
I0305 21:51:26.160984 139742087653120 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2006085366010666, loss=1.865946888923645
I0305 21:52:01.418105 139742096045824 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1667279452085495, loss=1.9226274490356445
I0305 21:52:36.636759 139742087653120 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.21846221387386322, loss=1.8849095106124878
I0305 21:53:11.834637 139742096045824 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2351292371749878, loss=1.8708418607711792
I0305 21:53:47.059755 139742087653120 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.21626536548137665, loss=1.8998991250991821
I0305 21:54:22.276169 139742096045824 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.1580209583044052, loss=1.8849256038665771
I0305 21:54:57.468494 139742087653120 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.16534043848514557, loss=1.8937653303146362
I0305 21:55:32.670592 139742096045824 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.16155728697776794, loss=1.8702583312988281
I0305 21:55:35.140933 139885236581568 spec.py:321] Evaluating on the training split.
I0305 21:55:37.763308 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 21:59:08.525207 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 21:59:11.119710 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:01:55.075004 139885236581568 spec.py:349] Evaluating on the test split.
I0305 22:01:57.676562 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:04:28.214393 139885236581568 submission_runner.py:469] Time since start: 10291.26s, 	Step: 16708, 	{'train/accuracy': 0.6294390559196472, 'train/loss': 1.8149199485778809, 'train/bleu': 30.73203820463991, 'validation/accuracy': 0.6504956483840942, 'validation/loss': 1.677403211593628, 'validation/bleu': 27.360159098301146, 'validation/num_examples': 3000, 'test/accuracy': 0.6586953997612, 'test/loss': 1.607671856880188, 'test/bleu': 26.478083320604117, 'test/num_examples': 3003, 'score': 5916.937549591064, 'total_duration': 10291.261077404022, 'accumulated_submission_time': 5916.937549591064, 'accumulated_eval_time': 4373.3016719818115, 'accumulated_logging_time': 0.11936521530151367}
I0305 22:04:28.223657 139742087653120 logging_writer.py:48] [16708] accumulated_eval_time=4373.3, accumulated_logging_time=0.119365, accumulated_submission_time=5916.94, global_step=16708, preemption_count=0, score=5916.94, test/accuracy=0.658695, test/bleu=26.4781, test/loss=1.60767, test/num_examples=3003, total_duration=10291.3, train/accuracy=0.629439, train/bleu=30.732, train/loss=1.81492, validation/accuracy=0.650496, validation/bleu=27.3602, validation/loss=1.6774, validation/num_examples=3000
I0305 22:05:00.806258 139742096045824 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.209503173828125, loss=1.9099093675613403
I0305 22:05:35.930793 139742087653120 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.18095442652702332, loss=1.7583612203598022
I0305 22:06:11.146841 139742096045824 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.16315945982933044, loss=1.925002098083496
I0305 22:06:46.354227 139742087653120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.1771623194217682, loss=1.9014703035354614
I0305 22:07:21.594614 139742096045824 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.17506317794322968, loss=1.8199517726898193
I0305 22:07:56.838216 139742087653120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.19962148368358612, loss=1.7851980924606323
I0305 22:08:32.052757 139742096045824 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.17358127236366272, loss=1.9175679683685303
I0305 22:09:07.273369 139742087653120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.36726289987564087, loss=1.8313510417938232
I0305 22:09:42.460006 139742096045824 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.18302515149116516, loss=1.8562451601028442
I0305 22:10:17.690740 139742087653120 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.23484036326408386, loss=1.8229420185089111
I0305 22:10:52.924562 139742096045824 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.295566588640213, loss=1.8194007873535156
I0305 22:11:28.138072 139742087653120 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.18772687017917633, loss=1.8621264696121216
I0305 22:12:03.352978 139742096045824 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.16691112518310547, loss=1.9547688961029053
I0305 22:12:38.584729 139742087653120 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.18234165012836456, loss=1.8336251974105835
I0305 22:13:13.855531 139742096045824 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.18902495503425598, loss=1.8525652885437012
I0305 22:13:49.093390 139742087653120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.15301509201526642, loss=1.8228280544281006
I0305 22:14:24.324970 139742096045824 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2024146169424057, loss=2.0060245990753174
I0305 22:14:59.545988 139742087653120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2521313726902008, loss=1.8900684118270874
I0305 22:15:34.734009 139742096045824 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1900434046983719, loss=1.8396223783493042
I0305 22:16:09.992000 139742087653120 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.1872357279062271, loss=1.913786768913269
I0305 22:16:45.187536 139742096045824 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.18393804132938385, loss=1.8843812942504883
I0305 22:17:20.735579 139742087653120 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.2133682370185852, loss=1.8108576536178589
I0305 22:17:56.081558 139742096045824 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.22066637873649597, loss=1.9481642246246338
I0305 22:18:28.320037 139885236581568 spec.py:321] Evaluating on the training split.
I0305 22:18:30.942577 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:21:21.712407 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 22:21:24.319685 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:23:59.959440 139885236581568 spec.py:349] Evaluating on the test split.
I0305 22:24:02.567922 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:26:30.273505 139885236581568 submission_runner.py:469] Time since start: 11613.32s, 	Step: 19092, 	{'train/accuracy': 0.6501817107200623, 'train/loss': 1.6559077501296997, 'train/bleu': 31.553598452572924, 'validation/accuracy': 0.6511631011962891, 'validation/loss': 1.6547967195510864, 'validation/bleu': 27.32445786247743, 'validation/num_examples': 3000, 'test/accuracy': 0.662009060382843, 'test/loss': 1.5799468755722046, 'test/bleu': 26.7706046642871, 'test/num_examples': 3003, 'score': 6756.903774738312, 'total_duration': 11613.320173025131, 'accumulated_submission_time': 6756.903774738312, 'accumulated_eval_time': 4855.255084991455, 'accumulated_logging_time': 0.1366136074066162}
I0305 22:26:30.284543 139742087653120 logging_writer.py:48] [19092] accumulated_eval_time=4855.26, accumulated_logging_time=0.136614, accumulated_submission_time=6756.9, global_step=19092, preemption_count=0, score=6756.9, test/accuracy=0.662009, test/bleu=26.7706, test/loss=1.57995, test/num_examples=3003, total_duration=11613.3, train/accuracy=0.650182, train/bleu=31.5536, train/loss=1.65591, validation/accuracy=0.651163, validation/bleu=27.3245, validation/loss=1.6548, validation/num_examples=3000
I0305 22:26:33.455153 139742096045824 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.20947930216789246, loss=1.8642258644104004
I0305 22:27:08.700841 139742087653120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.1785580962896347, loss=1.8686811923980713
I0305 22:27:44.057065 139742096045824 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.16734446585178375, loss=1.7605957984924316
I0305 22:28:19.429979 139742087653120 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.19086481630802155, loss=1.9350703954696655
I0305 22:28:54.862790 139742096045824 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.28267568349838257, loss=1.9038854837417603
I0305 22:29:30.227598 139742087653120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.19588039815425873, loss=1.8912838697433472
I0305 22:30:05.608011 139742096045824 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.1645219922065735, loss=1.945554494857788
I0305 22:30:40.997444 139742087653120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17752821743488312, loss=1.7509067058563232
I0305 22:31:16.371815 139742096045824 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.2417905032634735, loss=1.8587249517440796
I0305 22:31:51.723275 139742087653120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.17167873680591583, loss=1.8576138019561768
I0305 22:32:27.102057 139742096045824 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.16665807366371155, loss=1.7467223405838013
I0305 22:33:02.484853 139742087653120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.16964468359947205, loss=1.811069369316101
I0305 22:33:37.848748 139742096045824 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.18336902558803558, loss=1.858689785003662
I0305 22:34:13.217705 139742087653120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.16818192601203918, loss=1.7945631742477417
I0305 22:34:48.590929 139742096045824 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.17069882154464722, loss=1.8771851062774658
I0305 22:35:23.971648 139742087653120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.19345812499523163, loss=1.8506488800048828
I0305 22:35:59.319464 139742096045824 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.17414632439613342, loss=1.8338834047317505
I0305 22:36:34.732152 139742087653120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.20402845740318298, loss=1.8725509643554688
I0305 22:37:10.129250 139742096045824 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.17348076403141022, loss=1.8007073402404785
I0305 22:37:45.501381 139742087653120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.18977773189544678, loss=1.8299527168273926
I0305 22:38:20.897067 139742096045824 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.17470195889472961, loss=1.858737826347351
I0305 22:38:56.271270 139742087653120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.23572757840156555, loss=1.839243769645691
I0305 22:39:31.613061 139742096045824 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2633742690086365, loss=1.8284958600997925
I0305 22:40:07.008148 139742087653120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.25026607513427734, loss=1.789144515991211
I0305 22:40:30.328993 139885236581568 spec.py:321] Evaluating on the training split.
I0305 22:40:32.962465 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:44:23.180430 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 22:44:25.781482 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:47:04.067369 139885236581568 spec.py:349] Evaluating on the test split.
I0305 22:47:06.658242 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 22:49:31.359143 139885236581568 submission_runner.py:469] Time since start: 12994.41s, 	Step: 21467, 	{'train/accuracy': 0.6384540796279907, 'train/loss': 1.7438496351242065, 'train/bleu': 31.142454487942356, 'validation/accuracy': 0.6537092328071594, 'validation/loss': 1.6332379579544067, 'validation/bleu': 27.787133537813673, 'validation/num_examples': 3000, 'test/accuracy': 0.6646854281425476, 'test/loss': 1.5604816675186157, 'test/bleu': 26.834439389606466, 'test/num_examples': 3003, 'score': 7596.819486618042, 'total_duration': 12994.405839920044, 'accumulated_submission_time': 7596.819486618042, 'accumulated_eval_time': 5396.285189151764, 'accumulated_logging_time': 0.155808687210083}
I0305 22:49:31.370316 139742096045824 logging_writer.py:48] [21467] accumulated_eval_time=5396.29, accumulated_logging_time=0.155809, accumulated_submission_time=7596.82, global_step=21467, preemption_count=0, score=7596.82, test/accuracy=0.664685, test/bleu=26.8344, test/loss=1.56048, test/num_examples=3003, total_duration=12994.4, train/accuracy=0.638454, train/bleu=31.1425, train/loss=1.74385, validation/accuracy=0.653709, validation/bleu=27.7871, validation/loss=1.63324, validation/num_examples=3000
I0305 22:49:43.330976 139742087653120 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.20223097503185272, loss=1.8433839082717896
I0305 22:50:18.524557 139742096045824 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.2101885825395584, loss=1.915811538696289
I0305 22:50:53.873284 139742087653120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.19173720479011536, loss=1.833072304725647
I0305 22:51:29.193369 139742096045824 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.2575576901435852, loss=1.8396797180175781
I0305 22:52:04.519420 139742087653120 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.18165311217308044, loss=1.7964167594909668
I0305 22:52:39.855803 139742096045824 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.1715845763683319, loss=1.8078107833862305
I0305 22:53:15.180640 139742087653120 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.1644110083580017, loss=1.8300743103027344
I0305 22:53:50.493966 139742096045824 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.15924082696437836, loss=1.9057798385620117
I0305 22:54:25.881080 139742087653120 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.18248799443244934, loss=1.8424299955368042
I0305 22:55:01.189526 139742096045824 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.21055591106414795, loss=1.8249541521072388
I0305 22:55:36.504328 139742087653120 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.26869580149650574, loss=1.8617807626724243
I0305 22:56:11.881921 139742096045824 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.2472865879535675, loss=1.7594298124313354
I0305 22:56:47.267078 139742087653120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.17796939611434937, loss=1.7680447101593018
I0305 22:57:22.644600 139742096045824 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.21649615466594696, loss=1.8506277799606323
I0305 22:57:57.974540 139742087653120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.18069419264793396, loss=1.8091377019882202
I0305 22:58:33.350638 139742096045824 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.30147871375083923, loss=1.7691426277160645
I0305 22:59:08.699902 139742087653120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.20312069356441498, loss=1.8351528644561768
I0305 22:59:44.100702 139742096045824 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.19513657689094543, loss=1.869442343711853
I0305 23:00:19.432680 139742087653120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.22288382053375244, loss=1.8042354583740234
I0305 23:00:54.751841 139742096045824 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.23115558922290802, loss=1.8842389583587646
I0305 23:01:30.111124 139742087653120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.940879762172699, loss=1.8112443685531616
I0305 23:02:05.468513 139742096045824 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.1723264902830124, loss=1.7380045652389526
I0305 23:02:40.855478 139742087653120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.23009493947029114, loss=2.008991241455078
I0305 23:03:16.192939 139742096045824 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.19545632600784302, loss=1.8157737255096436
I0305 23:03:31.395450 139885236581568 spec.py:321] Evaluating on the training split.
I0305 23:03:34.022805 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:07:52.441214 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 23:07:55.044959 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:10:49.284734 139885236581568 spec.py:349] Evaluating on the test split.
I0305 23:10:51.900209 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:13:20.089507 139885236581568 submission_runner.py:469] Time since start: 14423.14s, 	Step: 23844, 	{'train/accuracy': 0.6358504295349121, 'train/loss': 1.7706661224365234, 'train/bleu': 30.908002248377738, 'validation/accuracy': 0.6564284563064575, 'validation/loss': 1.621553659439087, 'validation/bleu': 27.775573021909228, 'validation/num_examples': 3000, 'test/accuracy': 0.6653458476066589, 'test/loss': 1.5477651357650757, 'test/bleu': 27.030670050558573, 'test/num_examples': 3003, 'score': 8436.71908211708, 'total_duration': 14423.136200666428, 'accumulated_submission_time': 8436.71908211708, 'accumulated_eval_time': 5984.979205608368, 'accumulated_logging_time': 0.17611479759216309}
I0305 23:13:20.100837 139742087653120 logging_writer.py:48] [23844] accumulated_eval_time=5984.98, accumulated_logging_time=0.176115, accumulated_submission_time=8436.72, global_step=23844, preemption_count=0, score=8436.72, test/accuracy=0.665346, test/bleu=27.0307, test/loss=1.54777, test/num_examples=3003, total_duration=14423.1, train/accuracy=0.63585, train/bleu=30.908, train/loss=1.77067, validation/accuracy=0.656428, validation/bleu=27.7756, validation/loss=1.62155, validation/num_examples=3000
I0305 23:13:40.121388 139742096045824 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.7662875056266785, loss=1.8056057691574097
I0305 23:14:15.351199 139742087653120 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.23532873392105103, loss=1.9097341299057007
I0305 23:14:50.667601 139742096045824 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.1706278771162033, loss=1.8112545013427734
I0305 23:15:26.050355 139742087653120 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.23999767005443573, loss=1.8294081687927246
I0305 23:16:01.358041 139742096045824 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.17713184654712677, loss=1.8047699928283691
I0305 23:16:36.729807 139742087653120 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.1685798168182373, loss=1.7640619277954102
I0305 23:17:12.067644 139742096045824 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.16968639194965363, loss=1.8372434377670288
I0305 23:17:47.425505 139742087653120 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.21416227519512177, loss=1.8204165697097778
I0305 23:18:22.780439 139742096045824 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.22122347354888916, loss=1.7623488903045654
I0305 23:18:58.169705 139742087653120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.17649289965629578, loss=1.8174296617507935
I0305 23:19:33.528486 139742096045824 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.16208404302597046, loss=1.761073112487793
I0305 23:20:08.937249 139742087653120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2284451574087143, loss=1.8561341762542725
I0305 23:20:44.279680 139742096045824 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.17768347263336182, loss=1.841646671295166
I0305 23:21:19.537517 139742087653120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.20144949853420258, loss=1.7949695587158203
I0305 23:21:54.772052 139742096045824 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.1796426773071289, loss=1.8163976669311523
I0305 23:22:30.057898 139742087653120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2145524024963379, loss=1.9100834131240845
I0305 23:23:05.311933 139742096045824 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.17098286747932434, loss=1.8038370609283447
I0305 23:23:40.568862 139742087653120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.28347137570381165, loss=1.9292815923690796
I0305 23:24:15.850134 139742096045824 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.19152994453907013, loss=1.7211809158325195
I0305 23:24:51.140802 139742087653120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.19534771144390106, loss=1.7562527656555176
I0305 23:25:26.426998 139742096045824 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.17061971127986908, loss=1.7644782066345215
I0305 23:26:01.677316 139742087653120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.1872754544019699, loss=1.8795651197433472
I0305 23:26:36.935339 139742096045824 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.21161748468875885, loss=1.8543540239334106
I0305 23:27:12.202943 139742087653120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.21860133111476898, loss=1.785383701324463
I0305 23:27:20.320491 139885236581568 spec.py:321] Evaluating on the training split.
I0305 23:27:22.943719 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:30:57.727716 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 23:31:00.327265 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:33:53.614812 139885236581568 spec.py:349] Evaluating on the test split.
I0305 23:33:56.234770 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:36:34.346606 139885236581568 submission_runner.py:469] Time since start: 15817.39s, 	Step: 26224, 	{'train/accuracy': 0.6429998278617859, 'train/loss': 1.7062196731567383, 'train/bleu': 31.46378502208229, 'validation/accuracy': 0.6591352820396423, 'validation/loss': 1.6072542667388916, 'validation/bleu': 28.28424042137784, 'validation/num_examples': 3000, 'test/accuracy': 0.6689491271972656, 'test/loss': 1.5348715782165527, 'test/bleu': 27.385319889382068, 'test/num_examples': 3003, 'score': 9276.811330795288, 'total_duration': 15817.393292665482, 'accumulated_submission_time': 9276.811330795288, 'accumulated_eval_time': 6539.005263328552, 'accumulated_logging_time': 0.19555974006652832}
I0305 23:36:34.359111 139742096045824 logging_writer.py:48] [26224] accumulated_eval_time=6539.01, accumulated_logging_time=0.19556, accumulated_submission_time=9276.81, global_step=26224, preemption_count=0, score=9276.81, test/accuracy=0.668949, test/bleu=27.3853, test/loss=1.53487, test/num_examples=3003, total_duration=15817.4, train/accuracy=0.643, train/bleu=31.4638, train/loss=1.70622, validation/accuracy=0.659135, validation/bleu=28.2842, validation/loss=1.60725, validation/num_examples=3000
I0305 23:37:01.324578 139742087653120 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.17909680306911469, loss=1.828431248664856
I0305 23:37:36.468172 139742096045824 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.21494896709918976, loss=1.7839120626449585
I0305 23:38:11.681896 139742087653120 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.21145717799663544, loss=1.8269469738006592
I0305 23:38:46.922185 139742096045824 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.24123698472976685, loss=1.8078057765960693
I0305 23:39:22.161296 139742087653120 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.18185269832611084, loss=1.8469479084014893
I0305 23:39:57.410958 139742096045824 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.20757488906383514, loss=1.8453702926635742
I0305 23:40:32.634174 139742087653120 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.18168509006500244, loss=1.8434182405471802
I0305 23:41:07.912405 139742096045824 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.23775100708007812, loss=1.8580248355865479
I0305 23:41:43.165709 139742087653120 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.17719051241874695, loss=1.8449798822402954
I0305 23:42:18.476360 139742096045824 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.23659716546535492, loss=1.8532299995422363
I0305 23:42:53.761376 139742087653120 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.247490793466568, loss=1.8227163553237915
I0305 23:43:29.021219 139742096045824 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.16721203923225403, loss=1.7740888595581055
I0305 23:44:04.277827 139742087653120 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.19943377375602722, loss=1.7745769023895264
I0305 23:44:39.537653 139742096045824 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.1827816367149353, loss=1.847294569015503
I0305 23:45:14.787387 139742087653120 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.2154282033443451, loss=1.780547022819519
I0305 23:45:50.027002 139742096045824 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2101455181837082, loss=1.80106520652771
I0305 23:46:25.384116 139742087653120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.22528310120105743, loss=1.813555359840393
I0305 23:47:00.614451 139742096045824 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.20819778740406036, loss=1.8142300844192505
I0305 23:47:35.894659 139742087653120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.1776696741580963, loss=1.8589799404144287
I0305 23:48:11.171784 139742096045824 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.21079595386981964, loss=1.795782446861267
I0305 23:48:46.430140 139742087653120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.1871669590473175, loss=1.8325190544128418
I0305 23:49:21.687795 139742096045824 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.1957201510667801, loss=1.8103930950164795
I0305 23:49:56.933046 139742087653120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.20617008209228516, loss=1.7728381156921387
I0305 23:50:32.162659 139742096045824 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.22034600377082825, loss=1.7081882953643799
I0305 23:50:34.634102 139885236581568 spec.py:321] Evaluating on the training split.
I0305 23:50:37.282229 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:54:27.636441 139885236581568 spec.py:333] Evaluating on the validation split.
I0305 23:54:30.243437 139885236581568 workload.py:181] Translating evaluation dataset.
I0305 23:57:40.423338 139885236581568 spec.py:349] Evaluating on the test split.
I0305 23:57:43.022789 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 00:00:58.553414 139885236581568 submission_runner.py:469] Time since start: 17281.60s, 	Step: 28608, 	{'train/accuracy': 0.6398133635520935, 'train/loss': 1.7360574007034302, 'train/bleu': 31.340299294472487, 'validation/accuracy': 0.6612612009048462, 'validation/loss': 1.5949608087539673, 'validation/bleu': 28.12563769336987, 'validation/num_examples': 3000, 'test/accuracy': 0.6713590621948242, 'test/loss': 1.5191816091537476, 'test/bleu': 27.61001260264253, 'test/num_examples': 3003, 'score': 10116.96026802063, 'total_duration': 17281.600105047226, 'accumulated_submission_time': 10116.96026802063, 'accumulated_eval_time': 7162.924523353577, 'accumulated_logging_time': 0.21575093269348145}
I0306 00:00:58.565641 139742087653120 logging_writer.py:48] [28608] accumulated_eval_time=7162.92, accumulated_logging_time=0.215751, accumulated_submission_time=10117, global_step=28608, preemption_count=0, score=10117, test/accuracy=0.671359, test/bleu=27.61, test/loss=1.51918, test/num_examples=3003, total_duration=17281.6, train/accuracy=0.639813, train/bleu=31.3403, train/loss=1.73606, validation/accuracy=0.661261, validation/bleu=28.1256, validation/loss=1.59496, validation/num_examples=3000
I0306 00:01:31.155841 139742096045824 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.18744802474975586, loss=1.8032739162445068
I0306 00:02:06.357450 139742087653120 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1766965538263321, loss=1.839665174484253
I0306 00:02:41.602272 139742096045824 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.19239012897014618, loss=1.8630006313323975
I0306 00:03:16.861701 139742087653120 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.2400219887495041, loss=1.7744524478912354
I0306 00:03:52.112240 139742096045824 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.26326999068260193, loss=1.7678314447402954
I0306 00:04:27.387532 139742087653120 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.1864611953496933, loss=1.819481372833252
I0306 00:05:02.644688 139742096045824 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.18171828985214233, loss=1.80470609664917
I0306 00:05:37.920078 139742087653120 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.23375310003757477, loss=1.7830525636672974
I0306 00:06:13.151074 139742096045824 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.23237930238246918, loss=1.7352116107940674
I0306 00:06:48.458125 139742087653120 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.18429706990718842, loss=1.7918251752853394
I0306 00:07:23.736930 139742096045824 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.19120226800441742, loss=1.7195930480957031
I0306 00:07:59.016007 139742087653120 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.1967390924692154, loss=1.8728888034820557
I0306 00:08:34.284254 139742096045824 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.1986604630947113, loss=1.828992247581482
I0306 00:09:09.552843 139742087653120 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.18688325583934784, loss=1.7702608108520508
I0306 00:09:44.789970 139742096045824 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19757279753684998, loss=1.888379693031311
I0306 00:10:20.033920 139742087653120 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.24431848526000977, loss=1.7696207761764526
I0306 00:10:55.279285 139742096045824 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.1857355237007141, loss=1.8219656944274902
I0306 00:11:30.533719 139742087653120 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.17911221086978912, loss=1.8104321956634521
I0306 00:12:05.858900 139742096045824 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.1784147471189499, loss=1.778950810432434
I0306 00:12:41.099853 139742087653120 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.19933544099330902, loss=1.763817548751831
I0306 00:13:16.363127 139742096045824 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.23774327337741852, loss=1.8019561767578125
I0306 00:13:51.639622 139742087653120 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.19005945324897766, loss=1.8691208362579346
I0306 00:14:26.874902 139742096045824 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.19535839557647705, loss=1.8197739124298096
I0306 00:14:58.615269 139885236581568 spec.py:321] Evaluating on the training split.
I0306 00:15:01.241703 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 00:18:00.128356 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 00:18:02.731047 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 00:20:38.363712 139885236581568 spec.py:349] Evaluating on the test split.
I0306 00:20:40.969739 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 00:23:08.130243 139885236581568 submission_runner.py:469] Time since start: 18611.18s, 	Step: 30991, 	{'train/accuracy': 0.6429835557937622, 'train/loss': 1.722961664199829, 'train/bleu': 31.29162549797011, 'validation/accuracy': 0.6604331135749817, 'validation/loss': 1.5888334512710571, 'validation/bleu': 27.961954303677967, 'validation/num_examples': 3000, 'test/accuracy': 0.6739196181297302, 'test/loss': 1.5085657835006714, 'test/bleu': 27.348063364341442, 'test/num_examples': 3003, 'score': 10956.880996704102, 'total_duration': 18611.176918268204, 'accumulated_submission_time': 10956.880996704102, 'accumulated_eval_time': 7652.439444065094, 'accumulated_logging_time': 0.23578929901123047}
I0306 00:23:08.143140 139742087653120 logging_writer.py:48] [30991] accumulated_eval_time=7652.44, accumulated_logging_time=0.235789, accumulated_submission_time=10956.9, global_step=30991, preemption_count=0, score=10956.9, test/accuracy=0.67392, test/bleu=27.3481, test/loss=1.50857, test/num_examples=3003, total_duration=18611.2, train/accuracy=0.642984, train/bleu=31.2916, train/loss=1.72296, validation/accuracy=0.660433, validation/bleu=27.962, validation/loss=1.58883, validation/num_examples=3000
I0306 00:23:11.682046 139742096045824 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.17899847030639648, loss=1.804756999015808
I0306 00:23:46.736322 139742087653120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.20714136958122253, loss=1.7789769172668457
I0306 00:24:21.890338 139742096045824 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.19597867131233215, loss=1.7475078105926514
I0306 00:24:57.126669 139742087653120 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.18365412950515747, loss=1.835325002670288
I0306 00:25:32.417336 139742096045824 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.20660686492919922, loss=1.7246875762939453
I0306 00:26:07.799905 139742087653120 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.20354163646697998, loss=1.7515374422073364
I0306 00:26:43.142690 139742096045824 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.2553774416446686, loss=1.7074068784713745
I0306 00:27:18.544608 139742087653120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.1889723390340805, loss=1.8346500396728516
I0306 00:27:53.922242 139742096045824 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.21151654422283173, loss=1.7187321186065674
I0306 00:28:29.251714 139742087653120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.21820521354675293, loss=1.7883098125457764
I0306 00:29:04.583724 139742096045824 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.18724633753299713, loss=1.7967851161956787
I0306 00:29:39.903700 139742087653120 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.191792830824852, loss=1.789176106452942
I0306 00:30:15.205857 139742096045824 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.18915601074695587, loss=1.8484035730361938
I0306 00:30:50.524658 139742087653120 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.17137543857097626, loss=1.790932536125183
I0306 00:31:25.843276 139742096045824 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.21821148693561554, loss=1.7868975400924683
I0306 00:32:01.132932 139742087653120 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.18638616800308228, loss=1.7269538640975952
I0306 00:32:36.451226 139742096045824 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.20811878144741058, loss=1.7053303718566895
I0306 00:33:11.749595 139742087653120 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.27426138520240784, loss=1.7911415100097656
I0306 00:33:47.044291 139742096045824 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.18227146565914154, loss=1.742042064666748
I0306 00:34:22.299141 139742087653120 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.17884549498558044, loss=1.7896826267242432
I0306 00:34:57.565116 139742096045824 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.22659029066562653, loss=1.7675299644470215
I0306 00:35:32.800958 139742087653120 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.17288103699684143, loss=1.7221919298171997
I0306 00:36:08.026361 139742096045824 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.2098320722579956, loss=1.8377386331558228
I0306 00:36:43.264254 139742087653120 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.1896878331899643, loss=1.7932385206222534
I0306 00:37:08.282380 139885236581568 spec.py:321] Evaluating on the training split.
I0306 00:37:10.902673 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 00:41:37.588546 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 00:41:40.178889 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 00:44:25.449423 139885236581568 spec.py:349] Evaluating on the test split.
I0306 00:44:28.047040 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 00:47:04.598270 139885236581568 submission_runner.py:469] Time since start: 20047.64s, 	Step: 33372, 	{'train/accuracy': 0.641374409198761, 'train/loss': 1.7096197605133057, 'train/bleu': 31.407106989497983, 'validation/accuracy': 0.6623612642288208, 'validation/loss': 1.5775400400161743, 'validation/bleu': 28.113268729657044, 'validation/num_examples': 3000, 'test/accuracy': 0.6737342476844788, 'test/loss': 1.5002679824829102, 'test/bleu': 27.498789061251816, 'test/num_examples': 3003, 'score': 11796.889864444733, 'total_duration': 20047.64496088028, 'accumulated_submission_time': 11796.889864444733, 'accumulated_eval_time': 8248.755282878876, 'accumulated_logging_time': 0.2577850818634033}
I0306 00:47:04.609916 139742096045824 logging_writer.py:48] [33372] accumulated_eval_time=8248.76, accumulated_logging_time=0.257785, accumulated_submission_time=11796.9, global_step=33372, preemption_count=0, score=11796.9, test/accuracy=0.673734, test/bleu=27.4988, test/loss=1.50027, test/num_examples=3003, total_duration=20047.6, train/accuracy=0.641374, train/bleu=31.4071, train/loss=1.70962, validation/accuracy=0.662361, validation/bleu=28.1133, validation/loss=1.57754, validation/num_examples=3000
I0306 00:47:14.790644 139742087653120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.18696674704551697, loss=1.7082122564315796
I0306 00:47:49.836359 139742096045824 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3496386408805847, loss=1.8040755987167358
I0306 00:48:25.058387 139742087653120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.19209153950214386, loss=1.8041176795959473
I0306 00:49:00.269313 139742096045824 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.22935304045677185, loss=1.717340350151062
I0306 00:49:35.503704 139742087653120 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.196447491645813, loss=1.7075728178024292
I0306 00:50:10.731100 139742096045824 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.1927061527967453, loss=1.6895931959152222
I0306 00:50:45.992715 139742087653120 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.21771077811717987, loss=1.7924529314041138
I0306 00:51:21.246378 139742096045824 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.20912915468215942, loss=1.8164604902267456
I0306 00:51:56.485998 139742087653120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.2496374249458313, loss=1.8065167665481567
I0306 00:52:31.707931 139742096045824 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.23759756982326508, loss=1.8982343673706055
I0306 00:53:06.943802 139742087653120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.18018294870853424, loss=1.7860608100891113
I0306 00:53:42.159628 139742096045824 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.183877632021904, loss=1.7315499782562256
I0306 00:54:17.390657 139742087653120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.18845756351947784, loss=1.7083258628845215
I0306 00:54:52.648904 139742096045824 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.186926007270813, loss=1.8368809223175049
I0306 00:55:27.836088 139742087653120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.21819236874580383, loss=1.8165940046310425
I0306 00:56:03.074855 139742096045824 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.23499226570129395, loss=1.7536990642547607
I0306 00:56:38.315688 139742087653120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.22467654943466187, loss=1.7383911609649658
I0306 00:57:13.543518 139742096045824 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.21830672025680542, loss=1.714473843574524
I0306 00:57:48.777576 139742087653120 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.19079627096652985, loss=1.8554083108901978
I0306 00:58:24.012053 139742096045824 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.1777345836162567, loss=1.7980624437332153
I0306 00:58:59.247258 139742087653120 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.2014731764793396, loss=1.7547461986541748
I0306 00:59:34.475630 139742096045824 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.18700425326824188, loss=1.7485796213150024
I0306 01:00:09.715835 139742087653120 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.18635866045951843, loss=1.811251163482666
I0306 01:00:45.002362 139742096045824 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.1757546067237854, loss=1.8000651597976685
I0306 01:01:04.735331 139885236581568 spec.py:321] Evaluating on the training split.
I0306 01:01:07.359891 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:04:31.674529 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 01:04:34.274482 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:07:05.523696 139885236581568 spec.py:349] Evaluating on the test split.
I0306 01:07:08.124797 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:09:26.745013 139885236581568 submission_runner.py:469] Time since start: 21389.79s, 	Step: 35757, 	{'train/accuracy': 0.6438221335411072, 'train/loss': 1.7084859609603882, 'train/bleu': 31.09415555574639, 'validation/accuracy': 0.6628680229187012, 'validation/loss': 1.5695996284484863, 'validation/bleu': 28.262267697305713, 'validation/num_examples': 3000, 'test/accuracy': 0.6755648255348206, 'test/loss': 1.493857502937317, 'test/bleu': 27.756730013202453, 'test/num_examples': 3003, 'score': 12636.887555599213, 'total_duration': 21389.79168343544, 'accumulated_submission_time': 12636.887555599213, 'accumulated_eval_time': 8750.76489830017, 'accumulated_logging_time': 0.2774543762207031}
I0306 01:09:26.758819 139742087653120 logging_writer.py:48] [35757] accumulated_eval_time=8750.76, accumulated_logging_time=0.277454, accumulated_submission_time=12636.9, global_step=35757, preemption_count=0, score=12636.9, test/accuracy=0.675565, test/bleu=27.7567, test/loss=1.49386, test/num_examples=3003, total_duration=21389.8, train/accuracy=0.643822, train/bleu=31.0942, train/loss=1.70849, validation/accuracy=0.662868, validation/bleu=28.2623, validation/loss=1.5696, validation/num_examples=3000
I0306 01:09:42.173011 139742096045824 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.2276521474123001, loss=1.729565143585205
I0306 01:10:17.247884 139742087653120 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.2694418728351593, loss=1.8752861022949219
I0306 01:10:52.486432 139742096045824 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.1928698718547821, loss=1.8094685077667236
I0306 01:11:27.686310 139742087653120 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.18078620731830597, loss=1.8029412031173706
I0306 01:12:02.898497 139742096045824 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.19076263904571533, loss=1.7676033973693848
I0306 01:12:38.120320 139742087653120 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.2037363499403, loss=1.856870174407959
I0306 01:13:13.383461 139742096045824 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.18149076402187347, loss=1.7392244338989258
I0306 01:13:48.619794 139742087653120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.22338825464248657, loss=1.7968966960906982
I0306 01:14:23.846387 139742096045824 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.17733892798423767, loss=1.8621958494186401
I0306 01:14:59.096945 139742087653120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.22670455276966095, loss=1.779557228088379
I0306 01:15:34.294074 139742096045824 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.19854292273521423, loss=1.7005620002746582
I0306 01:16:09.503266 139742087653120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2267952859401703, loss=1.7884684801101685
I0306 01:16:44.706729 139742096045824 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.18590641021728516, loss=1.7705631256103516
I0306 01:17:19.947124 139742087653120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.2614789605140686, loss=1.7297132015228271
I0306 01:17:55.192254 139742096045824 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2208235263824463, loss=1.7627297639846802
I0306 01:18:30.428713 139742087653120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.24398908019065857, loss=1.6798559427261353
I0306 01:19:05.659390 139742096045824 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.21593508124351501, loss=1.7872916460037231
I0306 01:19:40.908057 139742087653120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.17996875941753387, loss=1.7063902616500854
I0306 01:20:16.125262 139742096045824 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.1850704401731491, loss=1.7593718767166138
I0306 01:20:51.351620 139742087653120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.20583432912826538, loss=1.775946855545044
I0306 01:21:26.594470 139742096045824 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.20565026998519897, loss=1.75555419921875
I0306 01:22:01.767107 139742087653120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.18834881484508514, loss=1.8020045757293701
I0306 01:22:36.984227 139742096045824 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.19798225164413452, loss=1.83881413936615
I0306 01:23:12.184673 139742087653120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.18422122299671173, loss=1.8105080127716064
I0306 01:23:26.951662 139885236581568 spec.py:321] Evaluating on the training split.
I0306 01:23:29.569622 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:27:36.112156 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 01:27:38.700950 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:30:21.630043 139885236581568 spec.py:349] Evaluating on the test split.
I0306 01:30:24.224906 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:32:48.228513 139885236581568 submission_runner.py:469] Time since start: 22791.28s, 	Step: 38143, 	{'train/accuracy': 0.6565806865692139, 'train/loss': 1.6036137342453003, 'train/bleu': 32.481665664560836, 'validation/accuracy': 0.6639928221702576, 'validation/loss': 1.5630806684494019, 'validation/bleu': 28.53714031591542, 'validation/num_examples': 3000, 'test/accuracy': 0.6750202775001526, 'test/loss': 1.4866255521774292, 'test/bleu': 27.636506933172203, 'test/num_examples': 3003, 'score': 13476.949308872223, 'total_duration': 22791.275188684464, 'accumulated_submission_time': 13476.949308872223, 'accumulated_eval_time': 9312.041680574417, 'accumulated_logging_time': 0.3001992702484131}
I0306 01:32:48.241058 139742096045824 logging_writer.py:48] [38143] accumulated_eval_time=9312.04, accumulated_logging_time=0.300199, accumulated_submission_time=13476.9, global_step=38143, preemption_count=0, score=13476.9, test/accuracy=0.67502, test/bleu=27.6365, test/loss=1.48663, test/num_examples=3003, total_duration=22791.3, train/accuracy=0.656581, train/bleu=32.4817, train/loss=1.60361, validation/accuracy=0.663993, validation/bleu=28.5371, validation/loss=1.56308, validation/num_examples=3000
I0306 01:33:08.531629 139742087653120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1968204379081726, loss=1.7007957696914673
I0306 01:33:43.590421 139742096045824 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.23639625310897827, loss=1.7213441133499146
I0306 01:34:18.743628 139742087653120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.19459842145442963, loss=1.7213187217712402
I0306 01:34:53.981245 139742096045824 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.19445501267910004, loss=1.6699377298355103
I0306 01:35:29.238680 139742087653120 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.23959532380104065, loss=1.6594905853271484
I0306 01:36:04.482091 139742096045824 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.18739938735961914, loss=1.7526675462722778
I0306 01:36:39.690947 139742087653120 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.1862560659646988, loss=1.8143545389175415
I0306 01:37:14.904948 139742096045824 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.23214077949523926, loss=1.7392702102661133
I0306 01:37:50.124585 139742087653120 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.17739452421665192, loss=1.7539297342300415
I0306 01:38:25.411341 139742096045824 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.2144368588924408, loss=1.8088151216506958
I0306 01:39:00.632617 139742087653120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.21059122681617737, loss=1.7740885019302368
I0306 01:39:35.833093 139742096045824 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.19934821128845215, loss=1.7889444828033447
I0306 01:40:11.055168 139742087653120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1789441555738449, loss=1.736124038696289
I0306 01:40:46.284795 139742096045824 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1954946517944336, loss=1.6993215084075928
I0306 01:41:21.504079 139742087653120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.29920023679733276, loss=1.7960691452026367
I0306 01:41:56.753035 139742096045824 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.18108302354812622, loss=1.7464475631713867
I0306 01:42:31.984842 139742087653120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.18780764937400818, loss=1.7244899272918701
I0306 01:43:07.218284 139742096045824 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.17719119787216187, loss=1.746511697769165
I0306 01:43:42.440055 139742087653120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.19242402911186218, loss=1.7220087051391602
I0306 01:44:17.648158 139742096045824 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.24443639814853668, loss=1.7454450130462646
I0306 01:44:52.871063 139742087653120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.23468171060085297, loss=1.720847725868225
I0306 01:45:28.088464 139742096045824 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.5040644407272339, loss=1.9408254623413086
I0306 01:46:03.331088 139742087653120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.1987730711698532, loss=1.7704988718032837
I0306 01:46:38.566844 139742096045824 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.18383370339870453, loss=1.6927409172058105
I0306 01:46:48.430583 139885236581568 spec.py:321] Evaluating on the training split.
I0306 01:46:51.041252 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:49:58.421244 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 01:50:01.017627 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:52:31.638247 139885236581568 spec.py:349] Evaluating on the test split.
I0306 01:52:34.229354 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 01:55:08.474552 139885236581568 submission_runner.py:469] Time since start: 24131.52s, 	Step: 40529, 	{'train/accuracy': 0.6460224390029907, 'train/loss': 1.6900311708450317, 'train/bleu': 31.691792158137854, 'validation/accuracy': 0.6660074591636658, 'validation/loss': 1.554399013519287, 'validation/bleu': 28.36518742532591, 'validation/num_examples': 3000, 'test/accuracy': 0.6779515743255615, 'test/loss': 1.4710854291915894, 'test/bleu': 27.866400599934593, 'test/num_examples': 3003, 'score': 14317.008560657501, 'total_duration': 24131.521239042282, 'accumulated_submission_time': 14317.008560657501, 'accumulated_eval_time': 9812.085590839386, 'accumulated_logging_time': 0.3209075927734375}
I0306 01:55:08.486739 139742087653120 logging_writer.py:48] [40529] accumulated_eval_time=9812.09, accumulated_logging_time=0.320908, accumulated_submission_time=14317, global_step=40529, preemption_count=0, score=14317, test/accuracy=0.677952, test/bleu=27.8664, test/loss=1.47109, test/num_examples=3003, total_duration=24131.5, train/accuracy=0.646022, train/bleu=31.6918, train/loss=1.69003, validation/accuracy=0.666007, validation/bleu=28.3652, validation/loss=1.5544, validation/num_examples=3000
I0306 01:55:33.744752 139742096045824 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.18498557806015015, loss=1.7633532285690308
I0306 01:56:08.884401 139742087653120 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.17757733166217804, loss=1.8376601934432983
I0306 01:56:44.099466 139742096045824 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.21821443736553192, loss=1.693751335144043
I0306 01:57:19.324732 139742087653120 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.21035437285900116, loss=1.8351329565048218
I0306 01:57:54.580099 139742096045824 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.18194086849689484, loss=1.728820562362671
I0306 01:58:29.808889 139742087653120 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.20262406766414642, loss=1.811730146408081
I0306 01:59:05.064019 139742096045824 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.20206768810749054, loss=1.6822831630706787
I0306 01:59:40.318487 139742087653120 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.18146531283855438, loss=1.7563387155532837
I0306 02:00:15.515857 139742096045824 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.18711671233177185, loss=1.7262983322143555
I0306 02:00:50.762892 139742087653120 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.22717176377773285, loss=1.690988540649414
I0306 02:01:26.034404 139742096045824 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2156498283147812, loss=1.7494679689407349
I0306 02:02:01.274681 139742087653120 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.19711244106292725, loss=1.8179218769073486
I0306 02:02:36.488914 139742096045824 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.20590682327747345, loss=1.722213864326477
I0306 02:03:11.723605 139742087653120 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.21351252496242523, loss=1.7090072631835938
I0306 02:03:46.936366 139742096045824 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.19256709516048431, loss=1.8128998279571533
I0306 02:04:22.154270 139742087653120 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.18869422376155853, loss=1.7096126079559326
I0306 02:04:57.387274 139742096045824 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1978720724582672, loss=1.7656337022781372
I0306 02:05:32.585392 139742087653120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.19004082679748535, loss=1.7632452249526978
I0306 02:06:07.819070 139742096045824 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.18782974779605865, loss=1.743852972984314
I0306 02:06:43.061735 139742087653120 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.25155875086784363, loss=1.7572777271270752
I0306 02:07:18.301141 139742096045824 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.22348807752132416, loss=1.7797510623931885
I0306 02:07:53.521293 139742087653120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2497980296611786, loss=1.7789616584777832
I0306 02:08:28.754411 139742096045824 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.20593883097171783, loss=1.713216781616211
I0306 02:09:04.015509 139742087653120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.22542746365070343, loss=1.8447130918502808
I0306 02:09:08.605720 139885236581568 spec.py:321] Evaluating on the training split.
I0306 02:09:11.221245 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 02:12:42.120456 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 02:12:44.718626 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 02:15:09.923566 139885236581568 spec.py:349] Evaluating on the test split.
I0306 02:15:12.519659 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 02:17:25.585971 139885236581568 submission_runner.py:469] Time since start: 25468.63s, 	Step: 42914, 	{'train/accuracy': 0.6463142037391663, 'train/loss': 1.6980946063995361, 'train/bleu': 31.02852420549763, 'validation/accuracy': 0.6663906574249268, 'validation/loss': 1.5459277629852295, 'validation/bleu': 28.991434465037198, 'validation/num_examples': 3000, 'test/accuracy': 0.6786583065986633, 'test/loss': 1.464038372039795, 'test/bleu': 28.160435770490817, 'test/num_examples': 3003, 'score': 15156.995676517487, 'total_duration': 25468.632650136948, 'accumulated_submission_time': 15156.995676517487, 'accumulated_eval_time': 10309.065776109695, 'accumulated_logging_time': 0.34190893173217773}
I0306 02:17:25.598082 139742096045824 logging_writer.py:48] [42914] accumulated_eval_time=10309.1, accumulated_logging_time=0.341909, accumulated_submission_time=15157, global_step=42914, preemption_count=0, score=15157, test/accuracy=0.678658, test/bleu=28.1604, test/loss=1.46404, test/num_examples=3003, total_duration=25468.6, train/accuracy=0.646314, train/bleu=31.0285, train/loss=1.69809, validation/accuracy=0.666391, validation/bleu=28.9914, validation/loss=1.54593, validation/num_examples=3000
I0306 02:17:56.021869 139742087653120 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.20869788527488708, loss=1.7384991645812988
I0306 02:18:31.130373 139742096045824 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2459300458431244, loss=1.753093957901001
I0306 02:19:06.332511 139742087653120 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.19482547044754028, loss=1.764626145362854
I0306 02:19:41.517440 139742096045824 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.20688578486442566, loss=1.8242790699005127
I0306 02:20:16.728222 139742087653120 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.22198989987373352, loss=1.785664439201355
I0306 02:20:51.925466 139742096045824 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.23435142636299133, loss=1.6735650300979614
I0306 02:21:27.107750 139742087653120 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.18984439969062805, loss=1.69984769821167
I0306 02:22:02.309685 139742096045824 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.2710399329662323, loss=1.7652004957199097
I0306 02:22:37.486010 139742087653120 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.19987408816814423, loss=1.6589186191558838
I0306 02:23:12.713379 139742096045824 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19872495532035828, loss=1.820819616317749
I0306 02:23:47.841784 139742087653120 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.20463570952415466, loss=1.7674710750579834
I0306 02:24:23.000876 139742096045824 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.19652560353279114, loss=1.739553451538086
I0306 02:24:58.108396 139742087653120 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.20319661498069763, loss=1.7581958770751953
I0306 02:25:33.255051 139742096045824 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.17651695013046265, loss=1.7025154829025269
I0306 02:26:08.418241 139742087653120 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2003246247768402, loss=1.706500768661499
I0306 02:26:43.553087 139742096045824 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.21071070432662964, loss=1.7393395900726318
I0306 02:27:18.690570 139742087653120 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.22419235110282898, loss=1.7824788093566895
I0306 02:27:53.904299 139742096045824 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.19859755039215088, loss=1.74679696559906
I0306 02:28:29.058736 139742087653120 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.20262522995471954, loss=1.8282146453857422
I0306 02:29:04.180400 139742096045824 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2330842763185501, loss=1.8374381065368652
I0306 02:29:39.312948 139742087653120 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.18063080310821533, loss=1.7295297384262085
I0306 02:30:14.463627 139742096045824 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.18825258314609528, loss=1.7772098779678345
I0306 02:30:49.622097 139742087653120 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.18525993824005127, loss=1.7662323713302612
I0306 02:31:24.779336 139742096045824 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.2384347915649414, loss=1.7519252300262451
I0306 02:31:25.836856 139885236581568 spec.py:321] Evaluating on the training split.
I0306 02:31:28.452245 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 02:35:09.143610 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 02:35:11.746688 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 02:37:53.143344 139885236581568 spec.py:349] Evaluating on the test split.
I0306 02:37:55.740647 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 02:40:17.461964 139885236581568 submission_runner.py:469] Time since start: 26840.51s, 	Step: 45304, 	{'train/accuracy': 0.6534265875816345, 'train/loss': 1.633876085281372, 'train/bleu': 32.34557604502924, 'validation/accuracy': 0.667181670665741, 'validation/loss': 1.544896125793457, 'validation/bleu': 28.757719735774693, 'validation/num_examples': 3000, 'test/accuracy': 0.6800834536552429, 'test/loss': 1.4621938467025757, 'test/bleu': 27.960087803744564, 'test/num_examples': 3003, 'score': 15997.106632471085, 'total_duration': 26840.508650541306, 'accumulated_submission_time': 15997.106632471085, 'accumulated_eval_time': 10840.690826654434, 'accumulated_logging_time': 0.3619399070739746}
I0306 02:40:17.474694 139742087653120 logging_writer.py:48] [45304] accumulated_eval_time=10840.7, accumulated_logging_time=0.36194, accumulated_submission_time=15997.1, global_step=45304, preemption_count=0, score=15997.1, test/accuracy=0.680083, test/bleu=27.9601, test/loss=1.46219, test/num_examples=3003, total_duration=26840.5, train/accuracy=0.653427, train/bleu=32.3456, train/loss=1.63388, validation/accuracy=0.667182, validation/bleu=28.7577, validation/loss=1.5449, validation/num_examples=3000
I0306 02:40:51.369122 139742096045824 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.19611984491348267, loss=1.7655318975448608
I0306 02:41:26.427385 139742087653120 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.20487897098064423, loss=1.774404764175415
I0306 02:42:01.538990 139742096045824 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.18638460338115692, loss=1.7413477897644043
I0306 02:42:36.674543 139742087653120 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.18809880316257477, loss=1.6883478164672852
I0306 02:43:11.796326 139742096045824 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.1974714696407318, loss=1.8127048015594482
I0306 02:43:46.911370 139742087653120 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.20166705548763275, loss=1.7291836738586426
I0306 02:44:22.035855 139742096045824 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.2171247899532318, loss=1.7069133520126343
I0306 02:44:57.177783 139742087653120 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.18588897585868835, loss=1.6475616693496704
I0306 02:45:32.302876 139742096045824 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.19348712265491486, loss=1.786002278327942
I0306 02:46:07.409895 139742087653120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.1971762329339981, loss=1.7021217346191406
I0306 02:46:42.574630 139742096045824 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.19570156931877136, loss=1.7377972602844238
I0306 02:47:17.703183 139742087653120 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.17842267453670502, loss=1.8217915296554565
I0306 02:47:52.821897 139742096045824 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.18812906742095947, loss=1.730445384979248
I0306 02:48:27.947031 139742087653120 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.1879051923751831, loss=1.6741690635681152
I0306 02:49:03.082407 139742096045824 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.20717528462409973, loss=1.7621382474899292
I0306 02:49:38.278229 139742087653120 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.18819500505924225, loss=1.8032872676849365
I0306 02:50:13.439396 139742096045824 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.2081197053194046, loss=1.753529667854309
I0306 02:50:48.547346 139742087653120 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2082279622554779, loss=1.7110484838485718
I0306 02:51:23.636948 139742096045824 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.19911523163318634, loss=1.7512415647506714
I0306 02:51:58.749820 139742087653120 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.2455974817276, loss=1.759385347366333
I0306 02:52:33.890644 139742096045824 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.2137686312198639, loss=1.7369556427001953
I0306 02:53:09.022872 139742087653120 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.17748543620109558, loss=1.7717598676681519
I0306 02:53:44.162812 139742096045824 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.17918352782726288, loss=1.7827584743499756
I0306 02:54:17.538573 139885236581568 spec.py:321] Evaluating on the training split.
I0306 02:54:20.162256 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 02:57:31.605195 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 02:57:34.190772 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:00:18.264724 139885236581568 spec.py:349] Evaluating on the test split.
I0306 03:00:20.863231 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:02:47.223083 139885236581568 submission_runner.py:469] Time since start: 28190.27s, 	Step: 47696, 	{'train/accuracy': 0.648568868637085, 'train/loss': 1.6702409982681274, 'train/bleu': 31.505368858483, 'validation/accuracy': 0.6689121127128601, 'validation/loss': 1.5327070951461792, 'validation/bleu': 28.53076402120324, 'validation/num_examples': 3000, 'test/accuracy': 0.6815896034240723, 'test/loss': 1.4474530220031738, 'test/bleu': 28.03009694102109, 'test/num_examples': 3003, 'score': 16837.045126199722, 'total_duration': 28190.26974964142, 'accumulated_submission_time': 16837.045126199722, 'accumulated_eval_time': 11350.37526512146, 'accumulated_logging_time': 0.3831977844238281}
I0306 03:02:47.235662 139742087653120 logging_writer.py:48] [47696] accumulated_eval_time=11350.4, accumulated_logging_time=0.383198, accumulated_submission_time=16837, global_step=47696, preemption_count=0, score=16837, test/accuracy=0.68159, test/bleu=28.0301, test/loss=1.44745, test/num_examples=3003, total_duration=28190.3, train/accuracy=0.648569, train/bleu=31.5054, train/loss=1.67024, validation/accuracy=0.668912, validation/bleu=28.5308, validation/loss=1.53271, validation/num_examples=3000
I0306 03:02:48.989218 139742096045824 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.2093583047389984, loss=1.6613537073135376
I0306 03:03:23.911871 139742087653120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.19240020215511322, loss=1.7923868894577026
I0306 03:03:58.954692 139742096045824 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.21273887157440186, loss=1.7057039737701416
I0306 03:04:34.141555 139742087653120 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.1883450150489807, loss=1.7359200716018677
I0306 03:05:09.256784 139742096045824 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.20781362056732178, loss=1.726464033126831
I0306 03:05:44.383776 139742087653120 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.18091419339179993, loss=1.718768835067749
I0306 03:06:19.479784 139742096045824 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.19507357478141785, loss=1.784193992614746
I0306 03:06:54.600134 139742087653120 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.0734843015670776, loss=1.7266767024993896
I0306 03:07:29.741033 139742096045824 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.18788032233715057, loss=1.7692298889160156
I0306 03:08:04.893332 139742087653120 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.1986517757177353, loss=1.6727043390274048
I0306 03:08:40.050196 139742096045824 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.18966315686702728, loss=1.733891248703003
I0306 03:09:15.186627 139742087653120 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.2105051428079605, loss=1.726802945137024
I0306 03:09:50.321054 139742096045824 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2583959102630615, loss=1.6981592178344727
I0306 03:10:25.471315 139742087653120 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.20746511220932007, loss=1.6598260402679443
I0306 03:11:00.574302 139742096045824 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.19019784033298492, loss=1.6981910467147827
I0306 03:11:35.708527 139742087653120 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.224748894572258, loss=1.7417371273040771
I0306 03:12:10.812698 139742096045824 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.23217947781085968, loss=1.650620937347412
I0306 03:12:45.957530 139742087653120 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2380479872226715, loss=1.717968225479126
I0306 03:13:21.087330 139742096045824 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.19726115465164185, loss=1.8434525728225708
I0306 03:13:56.243780 139742087653120 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.18770642578601837, loss=1.639038324356079
I0306 03:14:31.393550 139742096045824 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1855912059545517, loss=1.6846816539764404
I0306 03:15:06.531726 139742087653120 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.20481806993484497, loss=1.7402535676956177
I0306 03:15:41.685509 139742096045824 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.20216618478298187, loss=1.694489598274231
I0306 03:16:16.830748 139742087653120 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.20843161642551422, loss=1.7763748168945312
I0306 03:16:47.380863 139885236581568 spec.py:321] Evaluating on the training split.
I0306 03:16:50.000211 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:20:31.824141 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 03:20:34.423473 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:24:02.738531 139885236581568 spec.py:349] Evaluating on the test split.
I0306 03:24:05.333684 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:26:55.528436 139885236581568 submission_runner.py:469] Time since start: 29638.58s, 	Step: 50088, 	{'train/accuracy': 0.646837055683136, 'train/loss': 1.6821136474609375, 'train/bleu': 31.885168824700628, 'validation/accuracy': 0.6696289777755737, 'validation/loss': 1.5294376611709595, 'validation/bleu': 28.675530194392284, 'validation/num_examples': 3000, 'test/accuracy': 0.6807090640068054, 'test/loss': 1.4460885524749756, 'test/bleu': 28.14150435242082, 'test/num_examples': 3003, 'score': 17677.062327861786, 'total_duration': 29638.5751247406, 'accumulated_submission_time': 17677.062327861786, 'accumulated_eval_time': 11958.522782325745, 'accumulated_logging_time': 0.4037659168243408}
I0306 03:26:55.541091 139742096045824 logging_writer.py:48] [50088] accumulated_eval_time=11958.5, accumulated_logging_time=0.403766, accumulated_submission_time=17677.1, global_step=50088, preemption_count=0, score=17677.1, test/accuracy=0.680709, test/bleu=28.1415, test/loss=1.44609, test/num_examples=3003, total_duration=29638.6, train/accuracy=0.646837, train/bleu=31.8852, train/loss=1.68211, validation/accuracy=0.669629, validation/bleu=28.6755, validation/loss=1.52944, validation/num_examples=3000
I0306 03:27:00.086385 139742087653120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.22822126746177673, loss=1.6433981657028198
I0306 03:27:35.025800 139742096045824 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2045888453722, loss=1.7552375793457031
I0306 03:28:10.182706 139742087653120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.1834782361984253, loss=1.6444979906082153
I0306 03:28:45.368066 139742096045824 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.23719342052936554, loss=1.6645853519439697
I0306 03:29:20.555254 139742087653120 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.2865588366985321, loss=1.711159110069275
I0306 03:29:55.782550 139742096045824 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.20920105278491974, loss=1.7949786186218262
I0306 03:30:31.009768 139742087653120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.18557018041610718, loss=1.7413387298583984
I0306 03:31:06.194500 139742096045824 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.1838834285736084, loss=1.6399592161178589
I0306 03:31:41.408438 139742087653120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.18735937774181366, loss=1.7432894706726074
I0306 03:32:16.600704 139742096045824 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.19801409542560577, loss=1.6702710390090942
I0306 03:32:51.788789 139742087653120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.2404514104127884, loss=1.7167612314224243
I0306 03:33:27.006190 139742096045824 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.1876426637172699, loss=1.6628050804138184
I0306 03:34:02.217107 139742087653120 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2013365924358368, loss=1.760353684425354
I0306 03:34:37.446533 139742096045824 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.2059199959039688, loss=1.7443679571151733
I0306 03:35:12.633864 139742087653120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.20310205221176147, loss=1.6603738069534302
I0306 03:35:47.854084 139742096045824 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.20310035347938538, loss=1.7443331480026245
I0306 03:36:23.027371 139742087653120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.20995782315731049, loss=1.706501841545105
I0306 03:36:58.243893 139742096045824 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.22277000546455383, loss=1.7508373260498047
I0306 03:37:33.439605 139742087653120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.1906881481409073, loss=1.7031985521316528
I0306 03:38:08.651797 139742096045824 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.1946631819009781, loss=1.700814127922058
I0306 03:38:43.829276 139742087653120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.19013535976409912, loss=1.741140604019165
I0306 03:39:19.027539 139742096045824 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.21954670548439026, loss=1.8007643222808838
I0306 03:39:54.221606 139742087653120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.18115800619125366, loss=1.677897334098816
I0306 03:40:29.409886 139742096045824 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.20318862795829773, loss=1.7132608890533447
I0306 03:40:55.805867 139885236581568 spec.py:321] Evaluating on the training split.
I0306 03:40:58.421917 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:45:18.077701 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 03:45:20.678835 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:48:17.278752 139885236581568 spec.py:349] Evaluating on the test split.
I0306 03:48:19.874163 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 03:51:52.042251 139885236581568 submission_runner.py:469] Time since start: 31135.09s, 	Step: 52476, 	{'train/accuracy': 0.6543849110603333, 'train/loss': 1.6304444074630737, 'train/bleu': 32.57838162859172, 'validation/accuracy': 0.6717425584793091, 'validation/loss': 1.5152212381362915, 'validation/bleu': 28.740153040064893, 'validation/num_examples': 3000, 'test/accuracy': 0.6849148273468018, 'test/loss': 1.4301799535751343, 'test/bleu': 28.293649156891888, 'test/num_examples': 3003, 'score': 18517.200508356094, 'total_duration': 31135.088946819305, 'accumulated_submission_time': 18517.200508356094, 'accumulated_eval_time': 12614.759129524231, 'accumulated_logging_time': 0.4241955280303955}
I0306 03:51:52.055773 139742087653120 logging_writer.py:48] [52476] accumulated_eval_time=12614.8, accumulated_logging_time=0.424196, accumulated_submission_time=18517.2, global_step=52476, preemption_count=0, score=18517.2, test/accuracy=0.684915, test/bleu=28.2936, test/loss=1.43018, test/num_examples=3003, total_duration=31135.1, train/accuracy=0.654385, train/bleu=32.5784, train/loss=1.63044, validation/accuracy=0.671743, validation/bleu=28.7402, validation/loss=1.51522, validation/num_examples=3000
I0306 03:52:00.778118 139742096045824 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.20160235464572906, loss=1.7569727897644043
I0306 03:52:35.781756 139742087653120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.1973446160554886, loss=1.6973119974136353
I0306 03:53:10.931975 139742096045824 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2120429426431656, loss=1.6947402954101562
I0306 03:53:46.157718 139742087653120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.21495604515075684, loss=1.7205842733383179
I0306 03:54:21.335439 139742096045824 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.18405479192733765, loss=1.7167580127716064
I0306 03:54:56.562711 139742087653120 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.2059255987405777, loss=1.7257307767868042
I0306 03:55:31.725892 139742096045824 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.20412328839302063, loss=1.7473424673080444
I0306 03:56:06.924924 139742087653120 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.18382583558559418, loss=1.700446605682373
I0306 03:56:42.126694 139742096045824 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.20059986412525177, loss=1.6654096841812134
I0306 03:57:17.351918 139742087653120 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.19018730521202087, loss=1.742875337600708
I0306 03:57:52.540444 139742096045824 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.19068315625190735, loss=1.6287730932235718
I0306 03:58:27.766959 139742087653120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.17274796962738037, loss=1.7132526636123657
I0306 03:59:02.981801 139742096045824 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.20705246925354004, loss=1.6818785667419434
I0306 03:59:38.190455 139742087653120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.22669966518878937, loss=1.69590163230896
I0306 04:00:13.405195 139742096045824 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.2089918553829193, loss=1.7387707233428955
I0306 04:00:48.606862 139742087653120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.21455997228622437, loss=1.7148791551589966
I0306 04:01:23.781424 139742096045824 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.17838479578495026, loss=1.7222012281417847
I0306 04:01:59.002210 139742087653120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.22737661004066467, loss=1.8162826299667358
I0306 04:02:34.177554 139742096045824 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.21224546432495117, loss=1.7884643077850342
I0306 04:03:09.355722 139742087653120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.1782611459493637, loss=1.759246826171875
I0306 04:03:44.569140 139742096045824 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.17369964718818665, loss=1.7429742813110352
I0306 04:04:19.777622 139742087653120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.19394756853580475, loss=1.6956840753555298
I0306 04:04:54.969754 139742096045824 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.20565277338027954, loss=1.7392714023590088
I0306 04:05:30.160337 139742087653120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2083643227815628, loss=1.7476050853729248
I0306 04:05:52.318956 139885236581568 spec.py:321] Evaluating on the training split.
I0306 04:05:54.941610 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 04:10:18.397592 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 04:10:20.996618 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 04:13:05.814582 139885236581568 spec.py:349] Evaluating on the test split.
I0306 04:13:08.410787 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 04:15:49.126167 139885236581568 submission_runner.py:469] Time since start: 32572.17s, 	Step: 54864, 	{'train/accuracy': 0.6557061076164246, 'train/loss': 1.6275218725204468, 'train/bleu': 32.064179227481695, 'validation/accuracy': 0.6722493171691895, 'validation/loss': 1.509812593460083, 'validation/bleu': 29.27205322842188, 'validation/num_examples': 3000, 'test/accuracy': 0.6857143044471741, 'test/loss': 1.4192802906036377, 'test/bleu': 28.59075865788106, 'test/num_examples': 3003, 'score': 19357.332693338394, 'total_duration': 32572.17284846306, 'accumulated_submission_time': 19357.332693338394, 'accumulated_eval_time': 13211.566282749176, 'accumulated_logging_time': 0.44776129722595215}
I0306 04:15:49.139980 139742096045824 logging_writer.py:48] [54864] accumulated_eval_time=13211.6, accumulated_logging_time=0.447761, accumulated_submission_time=19357.3, global_step=54864, preemption_count=0, score=19357.3, test/accuracy=0.685714, test/bleu=28.5908, test/loss=1.41928, test/num_examples=3003, total_duration=32572.2, train/accuracy=0.655706, train/bleu=32.0642, train/loss=1.62752, validation/accuracy=0.672249, validation/bleu=29.2721, validation/loss=1.50981, validation/num_examples=3000
I0306 04:16:02.096082 139742087653120 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.18705134093761444, loss=1.7572816610336304
I0306 04:16:37.212679 139742096045824 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.19036367535591125, loss=1.7335764169692993
I0306 04:17:12.415046 139742087653120 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.19391511380672455, loss=1.6965175867080688
I0306 04:17:47.630191 139742096045824 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1842387467622757, loss=1.679020881652832
I0306 04:18:22.857543 139742087653120 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1858341246843338, loss=1.6664079427719116
I0306 04:18:58.089552 139742096045824 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.19980306923389435, loss=1.7204700708389282
I0306 04:19:33.306578 139742087653120 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.24191319942474365, loss=1.6831756830215454
I0306 04:20:08.539155 139742096045824 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.18323421478271484, loss=1.631558895111084
I0306 04:20:43.800929 139742087653120 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.19236712157726288, loss=1.6374497413635254
I0306 04:21:19.001484 139742096045824 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.200638085603714, loss=1.760339379310608
I0306 04:21:54.222368 139742087653120 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.20786035060882568, loss=1.737431287765503
I0306 04:22:29.441289 139742096045824 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2112889140844345, loss=1.6290850639343262
I0306 04:23:04.651467 139742087653120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.20109353959560394, loss=1.6500438451766968
I0306 04:23:39.874354 139742096045824 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.19370849430561066, loss=1.6868021488189697
I0306 04:24:15.152923 139742087653120 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.18627868592739105, loss=1.6683541536331177
I0306 04:24:50.378937 139742096045824 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.19924096763134003, loss=1.6273953914642334
I0306 04:25:25.600792 139742087653120 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.17694084346294403, loss=1.6118937730789185
I0306 04:26:00.832060 139742096045824 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.19400261342525482, loss=1.708986520767212
I0306 04:26:36.060832 139742087653120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.20285648107528687, loss=1.7768970727920532
I0306 04:27:11.285758 139742096045824 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.18347664177417755, loss=1.6476953029632568
I0306 04:27:46.601742 139742087653120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1815308779478073, loss=1.661138653755188
I0306 04:28:21.781360 139742096045824 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.18797779083251953, loss=1.72332763671875
I0306 04:28:57.011310 139742087653120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.17807282507419586, loss=1.761388897895813
I0306 04:29:32.256094 139742096045824 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2231895923614502, loss=1.6800148487091064
I0306 04:29:49.165670 139885236581568 spec.py:321] Evaluating on the training split.
I0306 04:29:51.779660 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 04:33:24.394914 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 04:33:27.004059 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 04:36:03.047064 139885236581568 spec.py:349] Evaluating on the test split.
I0306 04:36:05.648942 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 04:38:42.089154 139885236581568 submission_runner.py:469] Time since start: 33945.14s, 	Step: 57249, 	{'train/accuracy': 0.6651856303215027, 'train/loss': 1.5562095642089844, 'train/bleu': 32.501149336854596, 'validation/accuracy': 0.6733987927436829, 'validation/loss': 1.5057837963104248, 'validation/bleu': 29.38013745118279, 'validation/num_examples': 3000, 'test/accuracy': 0.6871857047080994, 'test/loss': 1.4133574962615967, 'test/bleu': 28.52058064367549, 'test/num_examples': 3003, 'score': 20197.22725534439, 'total_duration': 33945.13581442833, 'accumulated_submission_time': 20197.22725534439, 'accumulated_eval_time': 13744.489685058594, 'accumulated_logging_time': 0.46982407569885254}
I0306 04:38:42.105601 139742087653120 logging_writer.py:48] [57249] accumulated_eval_time=13744.5, accumulated_logging_time=0.469824, accumulated_submission_time=20197.2, global_step=57249, preemption_count=0, score=20197.2, test/accuracy=0.687186, test/bleu=28.5206, test/loss=1.41336, test/num_examples=3003, total_duration=33945.1, train/accuracy=0.665186, train/bleu=32.5011, train/loss=1.55621, validation/accuracy=0.673399, validation/bleu=29.3801, validation/loss=1.50578, validation/num_examples=3000
I0306 04:39:00.285118 139742096045824 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.20457719266414642, loss=1.7241958379745483
I0306 04:39:35.329594 139742087653120 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19599993526935577, loss=1.7011874914169312
I0306 04:40:10.492415 139742096045824 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.195280060172081, loss=1.6990623474121094
I0306 04:40:45.673995 139742087653120 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19457629323005676, loss=1.7489358186721802
I0306 04:41:20.855925 139742096045824 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2041560560464859, loss=1.6896523237228394
I0306 04:41:56.081023 139742087653120 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.19101130962371826, loss=1.7090084552764893
I0306 04:42:31.277922 139742096045824 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.18611523509025574, loss=1.6351343393325806
I0306 04:43:06.486959 139742087653120 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.1871914267539978, loss=1.803593397140503
I0306 04:43:41.721256 139742096045824 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.19509226083755493, loss=1.7289643287658691
I0306 04:44:16.955334 139742087653120 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19671553373336792, loss=1.7583736181259155
I0306 04:44:52.186351 139742096045824 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.1961866319179535, loss=1.7071360349655151
I0306 04:45:27.387284 139742087653120 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.19468529522418976, loss=1.7648144960403442
I0306 04:46:02.617941 139742096045824 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.19838213920593262, loss=1.661805272102356
I0306 04:46:37.830778 139742087653120 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2117272913455963, loss=1.681860327720642
I0306 04:47:13.032032 139742096045824 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.18217065930366516, loss=1.5823601484298706
I0306 04:47:48.242380 139742087653120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.18374687433242798, loss=1.6913182735443115
I0306 04:48:23.448383 139742096045824 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.1911167949438095, loss=1.6560192108154297
I0306 04:48:58.669551 139742087653120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.20104719698429108, loss=1.7168068885803223
I0306 04:49:33.855424 139742096045824 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19639472663402557, loss=1.7279634475708008
I0306 04:50:09.098810 139742087653120 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.18385034799575806, loss=1.695709228515625
I0306 04:50:44.304937 139742096045824 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.20664522051811218, loss=1.652235507965088
I0306 04:51:19.499937 139742087653120 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.20477637648582458, loss=1.6555067300796509
I0306 04:51:54.723399 139742096045824 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.19886650145053864, loss=1.689843773841858
I0306 04:52:29.938231 139742087653120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2416374236345291, loss=1.711607575416565
I0306 04:52:42.282862 139885236581568 spec.py:321] Evaluating on the training split.
I0306 04:52:44.902455 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 04:57:19.004580 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 04:57:21.603511 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:01:43.610367 139885236581568 spec.py:349] Evaluating on the test split.
I0306 05:01:46.216318 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:06:18.147898 139885236581568 submission_runner.py:469] Time since start: 35601.19s, 	Step: 59636, 	{'train/accuracy': 0.6592624187469482, 'train/loss': 1.6009031534194946, 'train/bleu': 32.88387624002771, 'validation/accuracy': 0.6752033233642578, 'validation/loss': 1.490800142288208, 'validation/bleu': 29.430445295752012, 'validation/num_examples': 3000, 'test/accuracy': 0.6879040598869324, 'test/loss': 1.404262661933899, 'test/bleu': 29.035741080313024, 'test/num_examples': 3003, 'score': 21037.277321577072, 'total_duration': 35601.19456291199, 'accumulated_submission_time': 21037.277321577072, 'accumulated_eval_time': 14560.35464167595, 'accumulated_logging_time': 0.4947798252105713}
I0306 05:06:18.166434 139742096045824 logging_writer.py:48] [59636] accumulated_eval_time=14560.4, accumulated_logging_time=0.49478, accumulated_submission_time=21037.3, global_step=59636, preemption_count=0, score=21037.3, test/accuracy=0.687904, test/bleu=29.0357, test/loss=1.40426, test/num_examples=3003, total_duration=35601.2, train/accuracy=0.659262, train/bleu=32.8839, train/loss=1.6009, validation/accuracy=0.675203, validation/bleu=29.4304, validation/loss=1.4908, validation/num_examples=3000
I0306 05:06:40.955637 139742087653120 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.1945241093635559, loss=1.6857675313949585
I0306 05:07:16.081130 139742096045824 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.18694555759429932, loss=1.6730976104736328
I0306 05:07:51.277872 139742087653120 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.195624440908432, loss=1.7176238298416138
I0306 05:08:26.467684 139742096045824 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.21459536254405975, loss=1.6449322700500488
I0306 05:09:01.680482 139742087653120 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.19687499105930328, loss=1.732816219329834
I0306 05:09:36.898515 139742096045824 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.20926588773727417, loss=1.6693392992019653
I0306 05:10:12.083563 139742087653120 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.19626984000205994, loss=1.6401357650756836
I0306 05:10:47.317572 139742096045824 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.19642207026481628, loss=1.714417815208435
I0306 05:11:22.532136 139742087653120 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.19050024449825287, loss=1.7231812477111816
I0306 05:11:57.755090 139742096045824 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.19471798837184906, loss=1.6908361911773682
I0306 05:12:32.947157 139742087653120 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.1947328746318817, loss=1.669411540031433
I0306 05:13:08.167064 139742096045824 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.19463956356048584, loss=1.6559935808181763
I0306 05:13:43.370818 139742087653120 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.1948312371969223, loss=1.6899287700653076
I0306 05:14:18.599462 139742096045824 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.1959313452243805, loss=1.6874873638153076
I0306 05:14:53.815421 139742087653120 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.19824613630771637, loss=1.665406584739685
I0306 05:15:29.056295 139742096045824 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.20084163546562195, loss=1.6563135385513306
I0306 05:16:04.266280 139742087653120 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.21787306666374207, loss=1.8422691822052002
I0306 05:16:39.548427 139742096045824 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.20816132426261902, loss=1.7085511684417725
I0306 05:17:14.784357 139742087653120 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.1949504315853119, loss=1.5993330478668213
I0306 05:17:50.043543 139742096045824 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.22694264352321625, loss=1.6467618942260742
I0306 05:18:25.290482 139742087653120 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.22047413885593414, loss=1.7116690874099731
I0306 05:19:00.534554 139742096045824 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.20314756035804749, loss=1.6482144594192505
I0306 05:19:35.719160 139742087653120 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.21006761491298676, loss=1.6453701257705688
I0306 05:20:10.937260 139742096045824 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.18666738271713257, loss=1.620528221130371
I0306 05:20:18.325633 139885236581568 spec.py:321] Evaluating on the training split.
I0306 05:20:20.944820 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:24:30.127581 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 05:24:32.733719 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:27:36.049711 139885236581568 spec.py:349] Evaluating on the test split.
I0306 05:27:38.639453 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:30:09.693787 139885236581568 submission_runner.py:469] Time since start: 37032.74s, 	Step: 62022, 	{'train/accuracy': 0.6542908549308777, 'train/loss': 1.6235066652297974, 'train/bleu': 32.23385560021843, 'validation/accuracy': 0.6766617894172668, 'validation/loss': 1.4837257862091064, 'validation/bleu': 29.228771543920587, 'validation/num_examples': 3000, 'test/accuracy': 0.6891553997993469, 'test/loss': 1.3970434665679932, 'test/bleu': 28.92918005614701, 'test/num_examples': 3003, 'score': 21877.30684351921, 'total_duration': 37032.74046874046, 'accumulated_submission_time': 21877.30684351921, 'accumulated_eval_time': 15151.722733974457, 'accumulated_logging_time': 0.5221579074859619}
I0306 05:30:09.708862 139742087653120 logging_writer.py:48] [62022] accumulated_eval_time=15151.7, accumulated_logging_time=0.522158, accumulated_submission_time=21877.3, global_step=62022, preemption_count=0, score=21877.3, test/accuracy=0.689155, test/bleu=28.9292, test/loss=1.39704, test/num_examples=3003, total_duration=37032.7, train/accuracy=0.654291, train/bleu=32.2339, train/loss=1.62351, validation/accuracy=0.676662, validation/bleu=29.2288, validation/loss=1.48373, validation/num_examples=3000
I0306 05:30:37.373534 139742096045824 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.1938280612230301, loss=1.6202259063720703
I0306 05:31:12.497927 139742087653120 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.19010823965072632, loss=1.6955643892288208
I0306 05:31:47.689843 139742096045824 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.1871103048324585, loss=1.6961867809295654
I0306 05:32:22.883451 139742087653120 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.22384977340698242, loss=1.7565652132034302
I0306 05:32:58.102653 139742096045824 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.20772123336791992, loss=1.6821483373641968
I0306 05:33:33.331006 139742087653120 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.21177344024181366, loss=1.648163080215454
I0306 05:34:08.527894 139742096045824 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.20679868757724762, loss=1.735569715499878
I0306 05:34:43.672914 139742087653120 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.22287291288375854, loss=1.685124158859253
I0306 05:35:18.779790 139742096045824 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.19231954216957092, loss=1.6117660999298096
I0306 05:35:53.915719 139742087653120 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.17531977593898773, loss=1.5824068784713745
I0306 05:36:29.014339 139742096045824 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.20379100739955902, loss=1.690333366394043
I0306 05:37:04.124776 139742087653120 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.22318856418132782, loss=1.626987338066101
I0306 05:37:39.228409 139742096045824 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.20763728022575378, loss=1.654850721359253
I0306 05:38:14.363482 139742087653120 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.22170452773571014, loss=1.714357614517212
I0306 05:38:49.479031 139742096045824 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.1979474127292633, loss=1.6212612390518188
I0306 05:39:24.567995 139742087653120 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.19724881649017334, loss=1.6516300439834595
I0306 05:39:59.701829 139742096045824 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.19145210087299347, loss=1.6745389699935913
I0306 05:40:34.807135 139742087653120 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.2135918289422989, loss=1.6520344018936157
I0306 05:41:09.938173 139742096045824 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.21424932777881622, loss=1.7218666076660156
I0306 05:41:45.044055 139742087653120 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.25813940167427063, loss=1.6524131298065186
I0306 05:42:20.189203 139742096045824 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.20347505807876587, loss=1.646358847618103
I0306 05:42:55.296328 139742087653120 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.1897232085466385, loss=1.667822003364563
I0306 05:43:30.434307 139742096045824 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.18577836453914642, loss=1.7072579860687256
I0306 05:44:05.537350 139742087653120 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.18873775005340576, loss=1.6657248735427856
I0306 05:44:09.757538 139885236581568 spec.py:321] Evaluating on the training split.
I0306 05:44:12.380759 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:48:07.550809 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 05:48:10.147771 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:50:51.109286 139885236581568 spec.py:349] Evaluating on the test split.
I0306 05:50:53.731808 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 05:53:46.246853 139885236581568 submission_runner.py:469] Time since start: 38449.29s, 	Step: 64413, 	{'train/accuracy': 0.6656155586242676, 'train/loss': 1.5586925745010376, 'train/bleu': 33.114841645828605, 'validation/accuracy': 0.6765629649162292, 'validation/loss': 1.4768515825271606, 'validation/bleu': 29.282869187823604, 'validation/num_examples': 3000, 'test/accuracy': 0.6907658576965332, 'test/loss': 1.3847627639770508, 'test/bleu': 29.362154066471096, 'test/num_examples': 3003, 'score': 22717.22613310814, 'total_duration': 38449.293536901474, 'accumulated_submission_time': 22717.22613310814, 'accumulated_eval_time': 15728.211986780167, 'accumulated_logging_time': 0.5455820560455322}
I0306 05:53:46.261517 139742096045824 logging_writer.py:48] [64413] accumulated_eval_time=15728.2, accumulated_logging_time=0.545582, accumulated_submission_time=22717.2, global_step=64413, preemption_count=0, score=22717.2, test/accuracy=0.690766, test/bleu=29.3622, test/loss=1.38476, test/num_examples=3003, total_duration=38449.3, train/accuracy=0.665616, train/bleu=33.1148, train/loss=1.55869, validation/accuracy=0.676563, validation/bleu=29.2829, validation/loss=1.47685, validation/num_examples=3000
I0306 05:54:16.961865 139742087653120 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.21311455965042114, loss=1.7017899751663208
I0306 05:54:51.965209 139742096045824 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.18772991001605988, loss=1.6399149894714355
I0306 05:55:27.054191 139742087653120 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19336187839508057, loss=1.6235523223876953
I0306 05:56:02.194830 139742096045824 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.18523888289928436, loss=1.6781611442565918
I0306 05:56:37.305107 139742087653120 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.20659436285495758, loss=1.675827980041504
I0306 05:57:12.387843 139742096045824 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2222275286912918, loss=1.720127820968628
I0306 05:57:47.476835 139742087653120 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.19213654100894928, loss=1.6963387727737427
I0306 05:58:22.579082 139742096045824 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.20705725252628326, loss=1.598456859588623
I0306 05:58:57.723069 139742087653120 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.19584432244300842, loss=1.6101174354553223
I0306 05:59:32.825473 139742096045824 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.20291586220264435, loss=1.6042373180389404
I0306 06:00:07.939359 139742087653120 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.20705828070640564, loss=1.6721446514129639
I0306 06:00:43.048280 139742096045824 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.18774104118347168, loss=1.625140905380249
I0306 06:01:18.135113 139742087653120 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20706085860729218, loss=1.6803734302520752
I0306 06:01:53.266205 139742096045824 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.18905647099018097, loss=1.6513266563415527
I0306 06:02:28.451553 139742087653120 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.18523335456848145, loss=1.6827338933944702
I0306 06:03:03.596764 139742096045824 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2040221393108368, loss=1.6191908121109009
I0306 06:03:38.701399 139742087653120 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.18493375182151794, loss=1.6053799390792847
I0306 06:04:13.812569 139742096045824 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.20069178938865662, loss=1.6428837776184082
I0306 06:04:48.938751 139742087653120 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.19670966267585754, loss=1.6537688970565796
I0306 06:05:24.033643 139742096045824 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.21268849074840546, loss=1.6361230611801147
I0306 06:05:59.180861 139742087653120 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2068059742450714, loss=1.6855971813201904
I0306 06:06:34.290531 139742096045824 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2043086290359497, loss=1.6683958768844604
I0306 06:07:09.438978 139742087653120 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.18777655065059662, loss=1.6767936944961548
I0306 06:07:44.568967 139742096045824 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.1978255957365036, loss=1.6146445274353027
I0306 06:07:46.333870 139885236581568 spec.py:321] Evaluating on the training split.
I0306 06:07:48.951739 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 06:11:42.545544 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 06:11:45.142266 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 06:15:02.602381 139885236581568 spec.py:349] Evaluating on the test split.
I0306 06:15:05.187215 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 06:18:19.239554 139885236581568 submission_runner.py:469] Time since start: 39922.29s, 	Step: 66806, 	{'train/accuracy': 0.6609426736831665, 'train/loss': 1.5866572856903076, 'train/bleu': 32.313436372213715, 'validation/accuracy': 0.6784416437149048, 'validation/loss': 1.473099946975708, 'validation/bleu': 29.348781815296835, 'validation/num_examples': 3000, 'test/accuracy': 0.6939867734909058, 'test/loss': 1.3743016719818115, 'test/bleu': 29.25840474652485, 'test/num_examples': 3003, 'score': 23557.170120239258, 'total_duration': 39922.286219120026, 'accumulated_submission_time': 23557.170120239258, 'accumulated_eval_time': 16361.117591619492, 'accumulated_logging_time': 0.5681564807891846}
I0306 06:18:19.255047 139742087653120 logging_writer.py:48] [66806] accumulated_eval_time=16361.1, accumulated_logging_time=0.568156, accumulated_submission_time=23557.2, global_step=66806, preemption_count=0, score=23557.2, test/accuracy=0.693987, test/bleu=29.2584, test/loss=1.3743, test/num_examples=3003, total_duration=39922.3, train/accuracy=0.660943, train/bleu=32.3134, train/loss=1.58666, validation/accuracy=0.678442, validation/bleu=29.3488, validation/loss=1.4731, validation/num_examples=3000
I0306 06:18:52.425341 139742096045824 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2551100254058838, loss=1.672270655632019
I0306 06:19:27.499242 139742087653120 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.20052826404571533, loss=1.6585861444473267
I0306 06:20:02.634056 139742096045824 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.1915992945432663, loss=1.6769508123397827
I0306 06:20:37.789658 139742087653120 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.1957572102546692, loss=1.6425297260284424
I0306 06:21:12.918535 139742096045824 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.21135951578617096, loss=1.7080386877059937
I0306 06:21:48.067216 139742087653120 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.18579484522342682, loss=1.6450661420822144
I0306 06:22:23.210921 139742096045824 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.2026481181383133, loss=1.626283884048462
I0306 06:22:58.324675 139742087653120 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.19119970500469208, loss=1.6451648473739624
I0306 06:23:33.461105 139742096045824 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.1934690773487091, loss=1.7228918075561523
I0306 06:24:08.582963 139742087653120 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.19418174028396606, loss=1.6496456861495972
I0306 06:24:43.734847 139742096045824 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.20756103098392487, loss=1.6862386465072632
I0306 06:25:18.889988 139742087653120 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.19491827487945557, loss=1.7394893169403076
I0306 06:25:54.049745 139742096045824 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.19281433522701263, loss=1.7072542905807495
I0306 06:26:29.180372 139742087653120 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.2064296007156372, loss=1.6027259826660156
I0306 06:27:04.294066 139742096045824 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.21250104904174805, loss=1.711349368095398
I0306 06:27:39.411787 139742087653120 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.22671052813529968, loss=1.6918960809707642
I0306 06:28:14.528385 139742096045824 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.19832421839237213, loss=1.6285120248794556
I0306 06:28:49.633656 139742087653120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.21939781308174133, loss=1.7036622762680054
I0306 06:29:24.749294 139742096045824 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.20568804442882538, loss=1.7038384675979614
I0306 06:29:59.838321 139742087653120 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.1895829141139984, loss=1.596680760383606
I0306 06:30:34.959450 139742096045824 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.22171466052532196, loss=1.693960428237915
I0306 06:31:10.063523 139742087653120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.35565468668937683, loss=1.6791980266571045
I0306 06:31:45.296742 139742096045824 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.1983669400215149, loss=1.5829318761825562
I0306 06:32:19.397150 139885236581568 spec.py:321] Evaluating on the training split.
I0306 06:32:22.016980 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 06:36:16.044369 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 06:36:18.634208 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 06:39:01.228804 139885236581568 spec.py:349] Evaluating on the test split.
I0306 06:39:03.834266 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 06:41:36.176408 139885236581568 submission_runner.py:469] Time since start: 41319.22s, 	Step: 69198, 	{'train/accuracy': 0.6814278960227966, 'train/loss': 1.4526829719543457, 'train/bleu': 34.33669969487311, 'validation/accuracy': 0.6807777285575867, 'validation/loss': 1.4597761631011963, 'validation/bleu': 29.752712343561274, 'validation/num_examples': 3000, 'test/accuracy': 0.6924806237220764, 'test/loss': 1.3710615634918213, 'test/bleu': 29.376223629152825, 'test/num_examples': 3003, 'score': 24397.180092811584, 'total_duration': 41319.22310304642, 'accumulated_submission_time': 24397.180092811584, 'accumulated_eval_time': 16917.89681839943, 'accumulated_logging_time': 0.593174934387207}
I0306 06:41:36.191166 139742087653120 logging_writer.py:48] [69198] accumulated_eval_time=16917.9, accumulated_logging_time=0.593175, accumulated_submission_time=24397.2, global_step=69198, preemption_count=0, score=24397.2, test/accuracy=0.692481, test/bleu=29.3762, test/loss=1.37106, test/num_examples=3003, total_duration=41319.2, train/accuracy=0.681428, train/bleu=34.3367, train/loss=1.45268, validation/accuracy=0.680778, validation/bleu=29.7527, validation/loss=1.45978, validation/num_examples=3000
I0306 06:41:37.253519 139742096045824 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.19779770076274872, loss=1.6468479633331299
I0306 06:42:12.282752 139742087653120 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.19890040159225464, loss=1.6135953664779663
I0306 06:42:47.463440 139742096045824 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2346155196428299, loss=1.6283856630325317
I0306 06:43:22.700344 139742087653120 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.21413004398345947, loss=1.6163188219070435
I0306 06:43:57.911624 139742096045824 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.19001315534114838, loss=1.6157265901565552
I0306 06:44:33.095497 139742087653120 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.20346029102802277, loss=1.6621583700180054
I0306 06:45:08.265863 139742096045824 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.19639082252979279, loss=1.5955803394317627
I0306 06:45:43.485240 139742087653120 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.21597696840763092, loss=1.617092490196228
I0306 06:46:18.674763 139742096045824 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.19495904445648193, loss=1.6082786321640015
I0306 06:46:53.890317 139742087653120 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.20664328336715698, loss=1.6472340822219849
I0306 06:47:29.096936 139742096045824 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.21641701459884644, loss=1.573159098625183
I0306 06:48:04.300065 139742087653120 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.19050724804401398, loss=1.6253632307052612
I0306 06:48:39.497668 139742096045824 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2026398777961731, loss=1.6411006450653076
I0306 06:49:14.705035 139742087653120 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.21244782209396362, loss=1.6547143459320068
I0306 06:49:49.916414 139742096045824 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.18943512439727783, loss=1.5748708248138428
I0306 06:50:25.157931 139742087653120 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.20634901523590088, loss=1.679720401763916
I0306 06:51:00.385927 139742096045824 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.19435541331768036, loss=1.6182224750518799
I0306 06:51:35.591492 139742087653120 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.2074926346540451, loss=1.5766440629959106
I0306 06:52:10.810897 139742096045824 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20307928323745728, loss=1.6716090440750122
I0306 06:52:46.048517 139742087653120 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.19165591895580292, loss=1.617113709449768
I0306 06:53:21.243716 139742096045824 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.2113654613494873, loss=1.6368476152420044
I0306 06:53:56.470201 139742087653120 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.20987507700920105, loss=1.5561623573303223
I0306 06:54:31.716658 139742096045824 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.22140558063983917, loss=1.6462513208389282
I0306 06:55:06.945802 139742087653120 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.1927594542503357, loss=1.6166934967041016
I0306 06:55:36.522074 139885236581568 spec.py:321] Evaluating on the training split.
I0306 06:55:39.151369 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:00:18.487994 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 07:00:21.101284 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:04:43.392269 139885236581568 spec.py:349] Evaluating on the test split.
I0306 07:04:45.996011 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:09:27.759566 139885236581568 submission_runner.py:469] Time since start: 42990.81s, 	Step: 71585, 	{'train/accuracy': 0.6640447378158569, 'train/loss': 1.5616756677627563, 'train/bleu': 32.98032791086242, 'validation/accuracy': 0.6825822591781616, 'validation/loss': 1.4531196355819702, 'validation/bleu': 29.481937311735184, 'validation/num_examples': 3000, 'test/accuracy': 0.6946471929550171, 'test/loss': 1.362155556678772, 'test/bleu': 29.272507172363646, 'test/num_examples': 3003, 'score': 25237.385013103485, 'total_duration': 42990.806248903275, 'accumulated_submission_time': 25237.385013103485, 'accumulated_eval_time': 17749.134249210358, 'accumulated_logging_time': 0.6159002780914307}
I0306 07:09:27.774909 139742096045824 logging_writer.py:48] [71585] accumulated_eval_time=17749.1, accumulated_logging_time=0.6159, accumulated_submission_time=25237.4, global_step=71585, preemption_count=0, score=25237.4, test/accuracy=0.694647, test/bleu=29.2725, test/loss=1.36216, test/num_examples=3003, total_duration=42990.8, train/accuracy=0.664045, train/bleu=32.9803, train/loss=1.56168, validation/accuracy=0.682582, validation/bleu=29.4819, validation/loss=1.45312, validation/num_examples=3000
I0306 07:09:33.395648 139742087653120 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.20582590997219086, loss=1.6118042469024658
I0306 07:10:08.412918 139742096045824 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.24102938175201416, loss=1.6628696918487549
I0306 07:10:43.826261 139742087653120 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.19802570343017578, loss=1.609157919883728
I0306 07:11:19.146824 139742096045824 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2161196768283844, loss=1.6131352186203003
I0306 07:11:54.509191 139742087653120 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.21087290346622467, loss=1.6742223501205444
I0306 07:12:29.926874 139742096045824 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.19491596519947052, loss=1.6886765956878662
I0306 07:13:05.300456 139742087653120 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.19703638553619385, loss=1.716132640838623
I0306 07:13:40.688575 139742096045824 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.1858053356409073, loss=1.5496337413787842
I0306 07:14:16.054352 139742087653120 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.20070184767246246, loss=1.5971647500991821
I0306 07:14:51.424925 139742096045824 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.1939113736152649, loss=1.680285096168518
I0306 07:15:26.800416 139742087653120 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.19800961017608643, loss=1.7284724712371826
I0306 07:16:02.164762 139742096045824 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.19584600627422333, loss=1.5652064085006714
I0306 07:16:37.524344 139742087653120 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.1949249655008316, loss=1.5861839056015015
I0306 07:17:12.881717 139742096045824 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.19928473234176636, loss=1.551289439201355
I0306 07:17:48.217603 139742087653120 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.20766068994998932, loss=1.5872831344604492
I0306 07:18:23.593472 139742096045824 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.19236597418785095, loss=1.5743030309677124
I0306 07:18:58.910488 139742087653120 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.20961128175258636, loss=1.6258213520050049
I0306 07:19:34.245970 139742096045824 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.20375579595565796, loss=1.5802326202392578
I0306 07:20:09.647368 139742087653120 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.20280572772026062, loss=1.6455403566360474
I0306 07:20:44.997295 139742096045824 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.23760081827640533, loss=1.7329049110412598
I0306 07:21:20.353292 139742087653120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.20306701958179474, loss=1.6432509422302246
I0306 07:21:55.722176 139742096045824 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.19604313373565674, loss=1.5350223779678345
I0306 07:22:31.108044 139742087653120 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.1965254545211792, loss=1.572532296180725
I0306 07:23:06.454832 139742096045824 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.20439651608467102, loss=1.6675260066986084
I0306 07:23:28.048188 139885236581568 spec.py:321] Evaluating on the training split.
I0306 07:23:30.672658 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:27:42.153562 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 07:27:44.761880 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:30:28.706162 139885236581568 spec.py:349] Evaluating on the test split.
I0306 07:30:31.306391 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:33:20.485231 139885236581568 submission_runner.py:469] Time since start: 44423.53s, 	Step: 73962, 	{'train/accuracy': 0.6669295430183411, 'train/loss': 1.5572843551635742, 'train/bleu': 33.073376780926715, 'validation/accuracy': 0.6822485327720642, 'validation/loss': 1.4546315670013428, 'validation/bleu': 30.058969972125116, 'validation/num_examples': 3000, 'test/accuracy': 0.6963387727737427, 'test/loss': 1.3564364910125732, 'test/bleu': 29.551286101779482, 'test/num_examples': 3003, 'score': 26077.530081510544, 'total_duration': 44423.53189110756, 'accumulated_submission_time': 26077.530081510544, 'accumulated_eval_time': 18341.571209430695, 'accumulated_logging_time': 0.6391153335571289}
I0306 07:33:20.506236 139742087653120 logging_writer.py:48] [73962] accumulated_eval_time=18341.6, accumulated_logging_time=0.639115, accumulated_submission_time=26077.5, global_step=73962, preemption_count=0, score=26077.5, test/accuracy=0.696339, test/bleu=29.5513, test/loss=1.35644, test/num_examples=3003, total_duration=44423.5, train/accuracy=0.66693, train/bleu=33.0734, train/loss=1.55728, validation/accuracy=0.682249, validation/bleu=30.059, validation/loss=1.45463, validation/num_examples=3000
I0306 07:33:34.210974 139742096045824 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.20262522995471954, loss=1.565836787223816
I0306 07:34:09.439021 139742087653120 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.19867292046546936, loss=1.6419991254806519
I0306 07:34:44.768632 139742096045824 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.20462608337402344, loss=1.549010157585144
I0306 07:35:20.112873 139742087653120 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.20335949957370758, loss=1.5375454425811768
I0306 07:35:55.458883 139742096045824 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.21764548122882843, loss=1.6584473848342896
I0306 07:36:30.805817 139742087653120 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.20100514590740204, loss=1.5664207935333252
I0306 07:37:06.220329 139742096045824 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.19893690943717957, loss=1.6887801885604858
I0306 07:37:41.599999 139742087653120 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.20880208909511566, loss=1.6467068195343018
I0306 07:38:16.937230 139742096045824 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.1953175961971283, loss=1.6251052618026733
I0306 07:38:52.291159 139742087653120 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.19352978467941284, loss=1.6067204475402832
I0306 07:39:27.647119 139742096045824 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.21022355556488037, loss=1.6292781829833984
I0306 07:40:03.023340 139742087653120 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.20752672851085663, loss=1.5373868942260742
I0306 07:40:38.441467 139742096045824 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.20841315388679504, loss=1.6320260763168335
I0306 07:41:13.826536 139742087653120 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.19544321298599243, loss=1.5914299488067627
I0306 07:41:49.214064 139742096045824 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.20596784353256226, loss=1.6077603101730347
I0306 07:42:24.606564 139742087653120 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.2190435528755188, loss=1.6254909038543701
I0306 07:43:00.003518 139742096045824 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.20805682241916656, loss=1.5640116930007935
I0306 07:43:35.381031 139742087653120 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.20965074002742767, loss=1.6376231908798218
I0306 07:44:10.731194 139742096045824 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.20433562994003296, loss=1.5932151079177856
I0306 07:44:46.100853 139742087653120 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.20842550694942474, loss=1.6291836500167847
I0306 07:45:21.447671 139742096045824 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.2106485366821289, loss=1.6341240406036377
I0306 07:45:56.798774 139742087653120 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.19100092351436615, loss=1.6858068704605103
I0306 07:46:32.170273 139742096045824 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.21231909096240997, loss=1.661736249923706
I0306 07:47:07.529430 139742087653120 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.20271088182926178, loss=1.596673607826233
I0306 07:47:20.629760 139885236581568 spec.py:321] Evaluating on the training split.
I0306 07:47:23.263926 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:50:59.277631 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 07:51:01.897334 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:54:16.226200 139885236581568 spec.py:349] Evaluating on the test split.
I0306 07:54:18.834969 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 07:57:23.145883 139885236581568 submission_runner.py:469] Time since start: 45866.19s, 	Step: 76338, 	{'train/accuracy': 0.6771021485328674, 'train/loss': 1.4857722520828247, 'train/bleu': 34.567156465688754, 'validation/accuracy': 0.683880090713501, 'validation/loss': 1.4409430027008057, 'validation/bleu': 30.026824441141656, 'validation/num_examples': 3000, 'test/accuracy': 0.6990499496459961, 'test/loss': 1.342016577720642, 'test/bleu': 29.673529429214604, 'test/num_examples': 3003, 'score': 26917.524755239487, 'total_duration': 45866.19248723984, 'accumulated_submission_time': 26917.524755239487, 'accumulated_eval_time': 18944.087207317352, 'accumulated_logging_time': 0.6687526702880859}
I0306 07:57:23.162924 139742096045824 logging_writer.py:48] [76338] accumulated_eval_time=18944.1, accumulated_logging_time=0.668753, accumulated_submission_time=26917.5, global_step=76338, preemption_count=0, score=26917.5, test/accuracy=0.69905, test/bleu=29.6735, test/loss=1.34202, test/num_examples=3003, total_duration=45866.2, train/accuracy=0.677102, train/bleu=34.5672, train/loss=1.48577, validation/accuracy=0.68388, validation/bleu=30.0268, validation/loss=1.44094, validation/num_examples=3000
I0306 07:57:45.341042 139742087653120 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.19647088646888733, loss=1.5922771692276
I0306 07:58:20.599141 139742096045824 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.1981247067451477, loss=1.5699769258499146
I0306 07:58:55.966365 139742087653120 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.19999302923679352, loss=1.5861358642578125
I0306 07:59:31.276180 139742096045824 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2022624909877777, loss=1.6477833986282349
I0306 08:00:06.606949 139742087653120 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.20445390045642853, loss=1.6757715940475464
I0306 08:00:41.960875 139742096045824 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.19756712019443512, loss=1.6468353271484375
I0306 08:01:17.377154 139742087653120 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.22677887976169586, loss=1.5457936525344849
I0306 08:01:52.766940 139742096045824 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.19870610535144806, loss=1.5566104650497437
I0306 08:02:28.152596 139742087653120 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.19649212062358856, loss=1.6368619203567505
I0306 08:03:03.575395 139742096045824 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21137958765029907, loss=1.621751308441162
I0306 08:03:38.933958 139742087653120 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.18197205662727356, loss=1.4989341497421265
I0306 08:04:14.307633 139742096045824 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.19705148041248322, loss=1.5672025680541992
I0306 08:04:49.664628 139742087653120 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.21157631278038025, loss=1.5949605703353882
I0306 08:05:25.000765 139742096045824 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.20029865205287933, loss=1.5493998527526855
I0306 08:06:00.341952 139742087653120 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.9805100560188293, loss=1.6361955404281616
I0306 08:06:35.685096 139742096045824 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.20607870817184448, loss=1.5319463014602661
I0306 08:07:11.032953 139742087653120 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.19784241914749146, loss=1.6491413116455078
I0306 08:07:46.367986 139742096045824 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.19897697865962982, loss=1.5542736053466797
I0306 08:08:21.717702 139742087653120 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.2070886492729187, loss=1.6452316045761108
I0306 08:08:57.075674 139742096045824 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2040812224149704, loss=1.5508055686950684
I0306 08:09:32.395466 139742087653120 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.21297572553157806, loss=1.5503588914871216
I0306 08:10:07.772434 139742096045824 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.20986336469650269, loss=1.6132744550704956
I0306 08:10:43.107089 139742087653120 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.20932340621948242, loss=1.6552602052688599
I0306 08:11:18.505575 139742096045824 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2053707093000412, loss=1.639235496520996
I0306 08:11:23.466486 139885236581568 spec.py:321] Evaluating on the training split.
I0306 08:11:26.100342 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 08:15:46.975217 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 08:15:49.578558 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 08:18:48.924451 139885236581568 spec.py:349] Evaluating on the test split.
I0306 08:18:51.536585 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 08:21:36.483513 139885236581568 submission_runner.py:469] Time since start: 47319.53s, 	Step: 78715, 	{'train/accuracy': 0.6721581220626831, 'train/loss': 1.5164607763290405, 'train/bleu': 33.62383509081055, 'validation/accuracy': 0.6857587695121765, 'validation/loss': 1.4350206851959229, 'validation/bleu': 30.052879559697878, 'validation/num_examples': 3000, 'test/accuracy': 0.7004286646842957, 'test/loss': 1.3303409814834595, 'test/bleu': 30.177913169933635, 'test/num_examples': 3003, 'score': 27757.699819803238, 'total_duration': 47319.530191898346, 'accumulated_submission_time': 27757.699819803238, 'accumulated_eval_time': 19557.104168653488, 'accumulated_logging_time': 0.6937887668609619}
I0306 08:21:36.502104 139742087653120 logging_writer.py:48] [78715] accumulated_eval_time=19557.1, accumulated_logging_time=0.693789, accumulated_submission_time=27757.7, global_step=78715, preemption_count=0, score=27757.7, test/accuracy=0.700429, test/bleu=30.1779, test/loss=1.33034, test/num_examples=3003, total_duration=47319.5, train/accuracy=0.672158, train/bleu=33.6238, train/loss=1.51646, validation/accuracy=0.685759, validation/bleu=30.0529, validation/loss=1.43502, validation/num_examples=3000
I0306 08:22:06.788493 139742096045824 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.19377949833869934, loss=1.5174026489257812
I0306 08:22:42.061750 139742087653120 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.19720250368118286, loss=1.548362374305725
I0306 08:23:17.425960 139742096045824 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.20853906869888306, loss=1.5633230209350586
I0306 08:23:52.794909 139742087653120 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.20368625223636627, loss=1.549411654472351
I0306 08:24:28.195807 139742096045824 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2179115116596222, loss=1.573001742362976
I0306 08:25:03.578484 139742087653120 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20223617553710938, loss=1.5507882833480835
I0306 08:25:38.989017 139742096045824 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.19889932870864868, loss=1.5751762390136719
I0306 08:26:14.360824 139742087653120 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.19880487024784088, loss=1.630411982536316
I0306 08:26:49.722429 139742096045824 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.24046461284160614, loss=1.6378782987594604
I0306 08:27:25.073042 139742087653120 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.21576540172100067, loss=1.5891687870025635
I0306 08:28:00.430162 139742096045824 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.20716483891010284, loss=1.6516499519348145
I0306 08:28:35.796428 139742087653120 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.19870589673519135, loss=1.5363733768463135
I0306 08:29:11.144829 139742096045824 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.19794845581054688, loss=1.572912573814392
I0306 08:29:46.537670 139742087653120 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.19658267498016357, loss=1.5896751880645752
I0306 08:30:21.935865 139742096045824 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.2013736218214035, loss=1.608627438545227
I0306 08:30:57.346760 139742087653120 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.20058482885360718, loss=1.5646438598632812
I0306 08:31:32.732017 139742096045824 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.2066698670387268, loss=1.5939280986785889
I0306 08:32:08.087595 139742087653120 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.1964515596628189, loss=1.4799925088882446
I0306 08:32:43.485420 139742096045824 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.20062172412872314, loss=1.5974658727645874
I0306 08:33:18.891746 139742087653120 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20970897376537323, loss=1.6523202657699585
I0306 08:33:54.264065 139742096045824 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.20242449641227722, loss=1.6296621561050415
I0306 08:34:29.658377 139742087653120 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2118324488401413, loss=1.6415590047836304
I0306 08:35:05.041337 139742096045824 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.22312726080417633, loss=1.5563455820083618
I0306 08:35:36.561572 139885236581568 spec.py:321] Evaluating on the training split.
I0306 08:35:39.192883 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 08:39:26.989238 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 08:39:29.589510 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 08:42:09.251971 139885236581568 spec.py:349] Evaluating on the test split.
I0306 08:42:11.857054 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 08:44:41.948418 139885236581568 submission_runner.py:469] Time since start: 48705.00s, 	Step: 81090, 	{'train/accuracy': 0.6715818643569946, 'train/loss': 1.5212568044662476, 'train/bleu': 33.612299009602246, 'validation/accuracy': 0.6869206428527832, 'validation/loss': 1.4230678081512451, 'validation/bleu': 29.950158803195844, 'validation/num_examples': 3000, 'test/accuracy': 0.7011702060699463, 'test/loss': 1.3279190063476562, 'test/bleu': 29.91503150060462, 'test/num_examples': 3003, 'score': 28597.628089427948, 'total_duration': 48704.99511170387, 'accumulated_submission_time': 28597.628089427948, 'accumulated_eval_time': 20102.49096941948, 'accumulated_logging_time': 0.7207818031311035}
I0306 08:44:41.966220 139742087653120 logging_writer.py:48] [81090] accumulated_eval_time=20102.5, accumulated_logging_time=0.720782, accumulated_submission_time=28597.6, global_step=81090, preemption_count=0, score=28597.6, test/accuracy=0.70117, test/bleu=29.915, test/loss=1.32792, test/num_examples=3003, total_duration=48705, train/accuracy=0.671582, train/bleu=33.6123, train/loss=1.52126, validation/accuracy=0.686921, validation/bleu=29.9502, validation/loss=1.42307, validation/num_examples=3000
I0306 08:44:45.824089 139742096045824 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.210588276386261, loss=1.5934669971466064
I0306 08:45:20.961630 139742087653120 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.21039049327373505, loss=1.5949686765670776
I0306 08:45:56.273184 139742096045824 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.21239544451236725, loss=1.5896086692810059
I0306 08:46:31.584779 139742087653120 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.20114454627037048, loss=1.5714287757873535
I0306 08:47:06.920564 139742096045824 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2128957211971283, loss=1.5811772346496582
I0306 08:47:42.171983 139742087653120 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.1957237422466278, loss=1.5891592502593994
I0306 08:48:17.366620 139742096045824 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.21025684475898743, loss=1.5770982503890991
I0306 08:48:52.585316 139742087653120 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.21637441217899323, loss=1.6495323181152344
I0306 08:49:27.778807 139742096045824 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.19859713315963745, loss=1.5555528402328491
I0306 08:50:02.980891 139742087653120 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.20571784675121307, loss=1.6253058910369873
I0306 08:50:38.172689 139742096045824 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2125576287508011, loss=1.5319137573242188
I0306 08:51:13.343808 139742087653120 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.19165238738059998, loss=1.5692659616470337
I0306 08:51:48.516618 139742096045824 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.21368810534477234, loss=1.5357227325439453
I0306 08:52:23.681939 139742087653120 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.20070885121822357, loss=1.6431820392608643
I0306 08:52:58.844563 139742096045824 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.20552721619606018, loss=1.6245273351669312
I0306 08:53:34.073857 139742087653120 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.21286273002624512, loss=1.6229530572891235
I0306 08:54:09.202161 139742096045824 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.2064688801765442, loss=1.6015788316726685
I0306 08:54:44.327262 139742087653120 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.20696291327476501, loss=1.5662479400634766
I0306 08:55:19.492212 139742096045824 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.20989693701267242, loss=1.6109349727630615
I0306 08:55:54.682888 139742087653120 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.21287937462329865, loss=1.5215020179748535
I0306 08:56:29.835868 139742096045824 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.20635998249053955, loss=1.5961410999298096
I0306 08:57:05.044467 139742087653120 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.20624412596225739, loss=1.5756943225860596
I0306 08:57:40.195441 139742096045824 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2028716802597046, loss=1.5336350202560425
I0306 08:58:15.347955 139742087653120 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.22130407392978668, loss=1.6323766708374023
I0306 08:58:42.067340 139885236581568 spec.py:321] Evaluating on the training split.
I0306 08:58:44.697581 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:03:16.544445 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 09:03:19.151851 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:07:18.867667 139885236581568 spec.py:349] Evaluating on the test split.
I0306 09:07:21.454716 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:11:29.963212 139885236581568 submission_runner.py:469] Time since start: 50313.01s, 	Step: 83477, 	{'train/accuracy': 0.6751096844673157, 'train/loss': 1.4865987300872803, 'train/bleu': 33.96741543358203, 'validation/accuracy': 0.6875756978988647, 'validation/loss': 1.4165263175964355, 'validation/bleu': 30.19484164008841, 'validation/num_examples': 3000, 'test/accuracy': 0.7027691006660461, 'test/loss': 1.316909909248352, 'test/bleu': 29.700478638202743, 'test/num_examples': 3003, 'score': 29437.60350561142, 'total_duration': 50313.009873867035, 'accumulated_submission_time': 29437.60350561142, 'accumulated_eval_time': 20870.386761665344, 'accumulated_logging_time': 0.746668815612793}
I0306 09:11:29.979745 139742096045824 logging_writer.py:48] [83477] accumulated_eval_time=20870.4, accumulated_logging_time=0.746669, accumulated_submission_time=29437.6, global_step=83477, preemption_count=0, score=29437.6, test/accuracy=0.702769, test/bleu=29.7005, test/loss=1.31691, test/num_examples=3003, total_duration=50313, train/accuracy=0.67511, train/bleu=33.9674, train/loss=1.4866, validation/accuracy=0.687576, validation/bleu=30.1948, validation/loss=1.41653, validation/num_examples=3000
I0306 09:11:38.331935 139742087653120 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.21640321612358093, loss=1.563905119895935
I0306 09:12:13.284967 139742096045824 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.22970300912857056, loss=1.5703792572021484
I0306 09:12:48.407860 139742087653120 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.19947992265224457, loss=1.5699100494384766
I0306 09:13:23.533091 139742096045824 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.20537711679935455, loss=1.584059715270996
I0306 09:13:58.652292 139742087653120 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.22279679775238037, loss=1.535913109779358
I0306 09:14:33.786045 139742096045824 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.1964816004037857, loss=1.4492547512054443
I0306 09:15:08.943358 139742087653120 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.21968305110931396, loss=1.6364611387252808
I0306 09:15:44.080663 139742096045824 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.1998376101255417, loss=1.5481526851654053
I0306 09:16:19.219187 139742087653120 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.2086673229932785, loss=1.5841506719589233
I0306 09:16:54.356648 139742096045824 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.20685917139053345, loss=1.5745686292648315
I0306 09:17:29.490155 139742087653120 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.206020787358284, loss=1.6292325258255005
I0306 09:18:04.622996 139742096045824 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.2196345180273056, loss=1.5363284349441528
I0306 09:18:39.768315 139742087653120 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.19433657824993134, loss=1.533549427986145
I0306 09:19:14.964849 139742096045824 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.21340583264827728, loss=1.5869030952453613
I0306 09:19:50.113332 139742087653120 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.20426052808761597, loss=1.5713781118392944
I0306 09:20:25.292989 139742096045824 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.20783360302448273, loss=1.534190058708191
I0306 09:21:00.424618 139742087653120 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.20964625477790833, loss=1.5625346899032593
I0306 09:21:35.560455 139742096045824 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.31584155559539795, loss=1.55237877368927
I0306 09:22:10.663214 139742087653120 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.20201417803764343, loss=1.5213396549224854
I0306 09:22:45.814817 139742096045824 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.1965261995792389, loss=1.5517432689666748
I0306 09:23:20.937012 139742087653120 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.21499940752983093, loss=1.5549168586730957
I0306 09:23:56.058806 139742096045824 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.20586620271205902, loss=1.564512014389038
I0306 09:24:31.200771 139742087653120 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.21429751813411713, loss=1.5149081945419312
I0306 09:25:06.343732 139742096045824 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2116394191980362, loss=1.654662847518921
I0306 09:25:30.252929 139885236581568 spec.py:321] Evaluating on the training split.
I0306 09:25:32.880885 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:30:09.275385 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 09:30:11.887490 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:33:12.950015 139885236581568 spec.py:349] Evaluating on the test split.
I0306 09:33:15.549811 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:36:02.470746 139885236581568 submission_runner.py:469] Time since start: 51785.52s, 	Step: 85869, 	{'train/accuracy': 0.6745820641517639, 'train/loss': 1.4965497255325317, 'train/bleu': 33.503225221490595, 'validation/accuracy': 0.6878229379653931, 'validation/loss': 1.4148178100585938, 'validation/bleu': 30.183429615415964, 'validation/num_examples': 3000, 'test/accuracy': 0.7030587196350098, 'test/loss': 1.3134899139404297, 'test/bleu': 30.227746099986422, 'test/num_examples': 3003, 'score': 30277.74830722809, 'total_duration': 51785.51743197441, 'accumulated_submission_time': 30277.74830722809, 'accumulated_eval_time': 21502.60453248024, 'accumulated_logging_time': 0.7721519470214844}
I0306 09:36:02.488015 139742087653120 logging_writer.py:48] [85869] accumulated_eval_time=21502.6, accumulated_logging_time=0.772152, accumulated_submission_time=30277.7, global_step=85869, preemption_count=0, score=30277.7, test/accuracy=0.703059, test/bleu=30.2277, test/loss=1.31349, test/num_examples=3003, total_duration=51785.5, train/accuracy=0.674582, train/bleu=33.5032, train/loss=1.49655, validation/accuracy=0.687823, validation/bleu=30.1834, validation/loss=1.41482, validation/num_examples=3000
I0306 09:36:13.661717 139742096045824 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.20441997051239014, loss=1.5117863416671753
I0306 09:36:48.616618 139742087653120 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.21805395185947418, loss=1.6678427457809448
I0306 09:37:23.695929 139742096045824 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.20137904584407806, loss=1.5682597160339355
I0306 09:37:58.779131 139742087653120 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.2180389016866684, loss=1.5720239877700806
I0306 09:38:33.879885 139742096045824 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.21086305379867554, loss=1.6257458925247192
I0306 09:39:08.994584 139742087653120 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.20224903523921967, loss=1.4843846559524536
I0306 09:39:44.118802 139742096045824 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.22267934679985046, loss=1.5467256307601929
I0306 09:40:19.237846 139742087653120 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.20323200523853302, loss=1.6448484659194946
I0306 09:40:54.373160 139742096045824 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.20891208946704865, loss=1.521690011024475
I0306 09:41:29.471123 139742087653120 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.20824991166591644, loss=1.560679316520691
I0306 09:42:04.611122 139742096045824 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.20791824162006378, loss=1.5472760200500488
I0306 09:42:39.756603 139742087653120 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.20468229055404663, loss=1.5053578615188599
I0306 09:43:14.860491 139742096045824 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.21413710713386536, loss=1.5954822301864624
I0306 09:43:50.000579 139742087653120 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.20525021851062775, loss=1.5411834716796875
I0306 09:44:25.099414 139742096045824 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.21520252525806427, loss=1.5650526285171509
I0306 09:45:00.235116 139742087653120 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.21008901298046112, loss=1.4708178043365479
I0306 09:45:35.350848 139742096045824 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.2236604392528534, loss=1.649959921836853
I0306 09:46:10.464363 139742087653120 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.2328542023897171, loss=1.544524908065796
I0306 09:46:45.585953 139742096045824 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.21742141246795654, loss=1.6111935377120972
I0306 09:47:20.703097 139742087653120 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.2165151983499527, loss=1.55050528049469
I0306 09:47:55.894097 139742096045824 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.20384156703948975, loss=1.6306016445159912
I0306 09:48:31.105732 139742087653120 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.2082495242357254, loss=1.5750330686569214
I0306 09:49:06.320898 139742096045824 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.20898739993572235, loss=1.5207983255386353
I0306 09:49:41.501263 139742087653120 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21132978796958923, loss=1.609840750694275
I0306 09:50:02.611023 139885236581568 spec.py:321] Evaluating on the training split.
I0306 09:50:05.235754 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:54:24.854458 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 09:54:27.458556 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 09:57:41.051764 139885236581568 spec.py:349] Evaluating on the test split.
I0306 09:57:43.643893 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 10:01:04.930541 139885236581568 submission_runner.py:469] Time since start: 53287.98s, 	Step: 88261, 	{'train/accuracy': 0.6885298490524292, 'train/loss': 1.408444881439209, 'train/bleu': 35.039764819947074, 'validation/accuracy': 0.6905297636985779, 'validation/loss': 1.4080631732940674, 'validation/bleu': 30.26102191038166, 'validation/num_examples': 3000, 'test/accuracy': 0.7043332457542419, 'test/loss': 1.3062477111816406, 'test/bleu': 30.292233355091927, 'test/num_examples': 3003, 'score': 31117.744438171387, 'total_duration': 53287.977234601974, 'accumulated_submission_time': 31117.744438171387, 'accumulated_eval_time': 22164.92399907112, 'accumulated_logging_time': 0.7976181507110596}
I0306 10:01:04.947726 139742096045824 logging_writer.py:48] [88261] accumulated_eval_time=22164.9, accumulated_logging_time=0.797618, accumulated_submission_time=31117.7, global_step=88261, preemption_count=0, score=31117.7, test/accuracy=0.704333, test/bleu=30.2922, test/loss=1.30625, test/num_examples=3003, total_duration=53288, train/accuracy=0.68853, train/bleu=35.0398, train/loss=1.40844, validation/accuracy=0.69053, validation/bleu=30.261, validation/loss=1.40806, validation/num_examples=3000
I0306 10:01:18.918652 139742087653120 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.2138247787952423, loss=1.5388000011444092
I0306 10:01:54.028028 139742096045824 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.2269977629184723, loss=1.5637456178665161
I0306 10:02:29.243304 139742087653120 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21568377315998077, loss=1.5954811573028564
I0306 10:03:04.437598 139742096045824 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.21007126569747925, loss=1.555030107498169
I0306 10:03:39.656064 139742087653120 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21319609880447388, loss=1.513119101524353
I0306 10:04:14.897271 139742096045824 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.2116401642560959, loss=1.5026800632476807
I0306 10:04:50.108647 139742087653120 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.201925590634346, loss=1.5876617431640625
I0306 10:05:25.343317 139742096045824 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.20984376966953278, loss=1.5270636081695557
I0306 10:06:00.585115 139742087653120 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.223414346575737, loss=1.5902414321899414
I0306 10:06:35.788978 139742096045824 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2050732672214508, loss=1.5623027086257935
I0306 10:07:11.013144 139742087653120 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.2100749909877777, loss=1.5598249435424805
I0306 10:07:46.252371 139742096045824 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.21946363151073456, loss=1.5712260007858276
I0306 10:08:21.477908 139742087653120 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.20121154189109802, loss=1.5130845308303833
I0306 10:08:56.715060 139742096045824 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.2134920209646225, loss=1.524855375289917
I0306 10:09:31.962092 139742087653120 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.22419819235801697, loss=1.668021559715271
I0306 10:10:07.186173 139742096045824 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.2213107943534851, loss=1.5451867580413818
I0306 10:10:42.390708 139742087653120 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.21522870659828186, loss=1.4993494749069214
I0306 10:11:17.603907 139742096045824 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.21053755283355713, loss=1.533591628074646
I0306 10:11:52.831144 139742087653120 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.2533288300037384, loss=1.5739805698394775
I0306 10:12:28.045528 139742096045824 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2109731286764145, loss=1.5527431964874268
I0306 10:13:03.288208 139742087653120 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.2160356044769287, loss=1.5953389406204224
I0306 10:13:38.526055 139742096045824 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.2280666083097458, loss=1.4849098920822144
I0306 10:14:13.751631 139742087653120 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.2064037173986435, loss=1.5167837142944336
I0306 10:14:48.976189 139742096045824 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.22208383679389954, loss=1.592568278312683
I0306 10:15:05.180892 139885236581568 spec.py:321] Evaluating on the training split.
I0306 10:15:07.805311 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 10:19:45.086550 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 10:19:47.697714 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 10:23:06.692322 139885236581568 spec.py:349] Evaluating on the test split.
I0306 10:23:09.282232 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 10:26:22.564975 139885236581568 submission_runner.py:469] Time since start: 54805.61s, 	Step: 90647, 	{'train/accuracy': 0.681182861328125, 'train/loss': 1.4545726776123047, 'train/bleu': 34.419142203223515, 'validation/accuracy': 0.691407322883606, 'validation/loss': 1.3958710432052612, 'validation/bleu': 30.34154127941769, 'validation/num_examples': 3000, 'test/accuracy': 0.7075425982475281, 'test/loss': 1.2912956476211548, 'test/bleu': 30.57466208625405, 'test/num_examples': 3003, 'score': 31957.84832262993, 'total_duration': 54805.611669540405, 'accumulated_submission_time': 31957.84832262993, 'accumulated_eval_time': 22842.308034658432, 'accumulated_logging_time': 0.8229062557220459}
I0306 10:26:22.582036 139742087653120 logging_writer.py:48] [90647] accumulated_eval_time=22842.3, accumulated_logging_time=0.822906, accumulated_submission_time=31957.8, global_step=90647, preemption_count=0, score=31957.8, test/accuracy=0.707543, test/bleu=30.5747, test/loss=1.2913, test/num_examples=3003, total_duration=54805.6, train/accuracy=0.681183, train/bleu=34.4191, train/loss=1.45457, validation/accuracy=0.691407, validation/bleu=30.3415, validation/loss=1.39587, validation/num_examples=3000
I0306 10:26:41.433495 139742096045824 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20640455186367035, loss=1.5632050037384033
I0306 10:27:16.470596 139742087653120 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.22503069043159485, loss=1.5859322547912598
I0306 10:27:51.664082 139742096045824 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.22487454116344452, loss=1.5613983869552612
I0306 10:28:26.867157 139742087653120 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.21586215496063232, loss=1.5495857000350952
I0306 10:29:02.036963 139742096045824 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.2307322919368744, loss=1.525120735168457
I0306 10:29:37.251898 139742087653120 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.20787696540355682, loss=1.4591188430786133
I0306 10:30:12.448007 139742096045824 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.2169732004404068, loss=1.4846121072769165
I0306 10:30:47.641843 139742087653120 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.20977795124053955, loss=1.6133822202682495
I0306 10:31:22.812390 139742096045824 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.2110908180475235, loss=1.5554159879684448
I0306 10:31:57.982104 139742087653120 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.2077520191669464, loss=1.4949196577072144
I0306 10:32:33.174510 139742096045824 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.22065851092338562, loss=1.5744866132736206
I0306 10:33:08.391184 139742087653120 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.21566051244735718, loss=1.4785137176513672
I0306 10:33:43.578001 139742096045824 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.23869800567626953, loss=1.542103886604309
I0306 10:34:18.769504 139742087653120 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.21167945861816406, loss=1.4684667587280273
I0306 10:34:53.966305 139742096045824 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.2577553689479828, loss=1.4725103378295898
I0306 10:35:29.172916 139742087653120 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.221875861287117, loss=1.5373926162719727
I0306 10:36:04.366688 139742096045824 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.23143631219863892, loss=1.5554897785186768
I0306 10:36:39.554797 139742087653120 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.22302335500717163, loss=1.4818687438964844
I0306 10:37:14.755889 139742096045824 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.21416646242141724, loss=1.5928866863250732
I0306 10:37:49.966099 139742087653120 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.21207360923290253, loss=1.4802409410476685
I0306 10:38:25.182372 139742096045824 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.22127018868923187, loss=1.4677174091339111
I0306 10:39:00.420347 139742087653120 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.21690097451210022, loss=1.532177448272705
I0306 10:39:35.669067 139742096045824 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.21062929928302765, loss=1.5593316555023193
I0306 10:40:10.885011 139742087653120 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.21044699847698212, loss=1.5236287117004395
I0306 10:40:22.848486 139885236581568 spec.py:321] Evaluating on the training split.
I0306 10:40:25.470954 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 10:44:48.933353 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 10:44:51.527518 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 10:47:41.124066 139885236581568 spec.py:349] Evaluating on the test split.
I0306 10:47:43.721413 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 10:50:32.170528 139885236581568 submission_runner.py:469] Time since start: 56255.22s, 	Step: 93035, 	{'train/accuracy': 0.6825011968612671, 'train/loss': 1.457434892654419, 'train/bleu': 33.80254110481923, 'validation/accuracy': 0.691815197467804, 'validation/loss': 1.3937342166900635, 'validation/bleu': 30.65905531582299, 'validation/num_examples': 3000, 'test/accuracy': 0.7074035406112671, 'test/loss': 1.2903655767440796, 'test/bleu': 30.41611768441175, 'test/num_examples': 3003, 'score': 32797.98564243317, 'total_duration': 56255.21722102165, 'accumulated_submission_time': 32797.98564243317, 'accumulated_eval_time': 23451.630036592484, 'accumulated_logging_time': 0.8482229709625244}
I0306 10:50:32.188231 139742096045824 logging_writer.py:48] [93035] accumulated_eval_time=23451.6, accumulated_logging_time=0.848223, accumulated_submission_time=32798, global_step=93035, preemption_count=0, score=32798, test/accuracy=0.707404, test/bleu=30.4161, test/loss=1.29037, test/num_examples=3003, total_duration=56255.2, train/accuracy=0.682501, train/bleu=33.8025, train/loss=1.45743, validation/accuracy=0.691815, validation/bleu=30.6591, validation/loss=1.39373, validation/num_examples=3000
I0306 10:50:55.269857 139742087653120 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.21650013327598572, loss=1.5152029991149902
I0306 10:51:30.370775 139742096045824 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.20943433046340942, loss=1.5456122159957886
I0306 10:52:05.593151 139742087653120 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.21910308301448822, loss=1.640124797821045
I0306 10:52:40.818399 139742096045824 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.22391977906227112, loss=1.5818288326263428
I0306 10:53:16.027377 139742087653120 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.23001894354820251, loss=1.6024584770202637
I0306 10:53:51.222290 139742096045824 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.21990936994552612, loss=1.615233063697815
I0306 10:54:26.456165 139742087653120 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.28705164790153503, loss=1.565132975578308
I0306 10:55:01.658482 139742096045824 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2323559820652008, loss=1.5492990016937256
I0306 10:55:36.858409 139742087653120 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.220929354429245, loss=1.5797204971313477
I0306 10:56:12.049825 139742096045824 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.21096286177635193, loss=1.4937797784805298
I0306 10:56:47.290754 139742087653120 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.3492910861968994, loss=1.569256067276001
I0306 10:57:22.514360 139742096045824 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.2169049233198166, loss=1.505714774131775
I0306 10:57:57.723384 139742087653120 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.21590304374694824, loss=1.505164384841919
I0306 10:58:32.920598 139742096045824 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.21494026482105255, loss=1.5401452779769897
I0306 10:59:08.095777 139742087653120 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21261927485466003, loss=1.513203740119934
I0306 10:59:43.310282 139742096045824 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.22149735689163208, loss=1.536024570465088
I0306 11:00:18.524005 139742087653120 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.21553850173950195, loss=1.5207451581954956
I0306 11:00:53.715804 139742096045824 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.2092919498682022, loss=1.4955745935440063
I0306 11:01:28.925581 139742087653120 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.22565004229545593, loss=1.5457490682601929
I0306 11:02:04.149262 139742096045824 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.20790298283100128, loss=1.4745012521743774
I0306 11:02:39.349308 139742087653120 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22355352342128754, loss=1.5041539669036865
I0306 11:03:14.613173 139742096045824 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.21190792322158813, loss=1.3813329935073853
I0306 11:03:49.797916 139742087653120 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.22865557670593262, loss=1.565281629562378
I0306 11:04:25.074563 139742096045824 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2216300517320633, loss=1.5021556615829468
I0306 11:04:32.475824 139885236581568 spec.py:321] Evaluating on the training split.
I0306 11:04:35.095038 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 11:08:55.632522 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 11:08:58.241281 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 11:12:11.282498 139885236581568 spec.py:349] Evaluating on the test split.
I0306 11:12:13.874884 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 11:15:20.057564 139885236581568 submission_runner.py:469] Time since start: 57743.10s, 	Step: 95422, 	{'train/accuracy': 0.6870148777961731, 'train/loss': 1.427930474281311, 'train/bleu': 34.64504173620339, 'validation/accuracy': 0.6932613253593445, 'validation/loss': 1.385765552520752, 'validation/bleu': 30.749760392242234, 'validation/num_examples': 3000, 'test/accuracy': 0.7098366618156433, 'test/loss': 1.2805626392364502, 'test/bleu': 30.710215511305474, 'test/num_examples': 3003, 'score': 33638.14417767525, 'total_duration': 57743.10424041748, 'accumulated_submission_time': 33638.14417767525, 'accumulated_eval_time': 24099.21170926094, 'accumulated_logging_time': 0.8736398220062256}
I0306 11:15:20.075646 139742087653120 logging_writer.py:48] [95422] accumulated_eval_time=24099.2, accumulated_logging_time=0.87364, accumulated_submission_time=33638.1, global_step=95422, preemption_count=0, score=33638.1, test/accuracy=0.709837, test/bleu=30.7102, test/loss=1.28056, test/num_examples=3003, total_duration=57743.1, train/accuracy=0.687015, train/bleu=34.645, train/loss=1.42793, validation/accuracy=0.693261, validation/bleu=30.7498, validation/loss=1.38577, validation/num_examples=3000
I0306 11:15:47.731880 139742096045824 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.2277163416147232, loss=1.4724417924880981
I0306 11:16:22.844648 139742087653120 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22532503306865692, loss=1.5050345659255981
I0306 11:16:58.027603 139742096045824 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.22395184636116028, loss=1.4583641290664673
I0306 11:17:33.217482 139742087653120 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.2178964763879776, loss=1.4881389141082764
I0306 11:18:08.402576 139742096045824 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.22432708740234375, loss=1.5563658475875854
I0306 11:18:43.608917 139742087653120 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.22461526095867157, loss=1.5389587879180908
I0306 11:19:18.767841 139742096045824 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.21019648015499115, loss=1.4883750677108765
I0306 11:19:53.974898 139742087653120 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.2185758799314499, loss=1.4784104824066162
I0306 11:20:29.183186 139742096045824 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.21744075417518616, loss=1.5553606748580933
I0306 11:21:04.374397 139742087653120 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.22317147254943848, loss=1.4835044145584106
I0306 11:21:39.557080 139742096045824 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.21654018759727478, loss=1.4453743696212769
I0306 11:22:14.750755 139742087653120 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.2154024988412857, loss=1.5447839498519897
I0306 11:22:49.928033 139742096045824 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.2263585478067398, loss=1.5468380451202393
I0306 11:23:25.156581 139742087653120 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.22080472111701965, loss=1.4781818389892578
I0306 11:24:00.346157 139742096045824 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.21858571469783783, loss=1.5027703046798706
I0306 11:24:35.556239 139742087653120 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.21659262478351593, loss=1.4973540306091309
I0306 11:25:10.758136 139742096045824 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.21809807419776917, loss=1.4935358762741089
I0306 11:25:46.003744 139742087653120 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.22274552285671234, loss=1.5079659223556519
I0306 11:26:21.255119 139742096045824 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.24148032069206238, loss=1.542278528213501
I0306 11:26:56.481469 139742087653120 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.21735769510269165, loss=1.4894229173660278
I0306 11:27:31.725707 139742096045824 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.2326899915933609, loss=1.487794280052185
I0306 11:28:06.918642 139742087653120 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.2540697455406189, loss=1.5721259117126465
I0306 11:28:42.164072 139742096045824 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.21535225212574005, loss=1.5159225463867188
I0306 11:29:17.385608 139742087653120 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.21996431052684784, loss=1.4834821224212646
I0306 11:29:20.206453 139885236581568 spec.py:321] Evaluating on the training split.
I0306 11:29:22.830495 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 11:33:51.786773 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 11:33:54.383775 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 11:36:39.268790 139885236581568 spec.py:349] Evaluating on the test split.
I0306 11:36:41.876327 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 11:39:07.030837 139885236581568 submission_runner.py:469] Time since start: 59170.08s, 	Step: 97809, 	{'train/accuracy': 0.6842559576034546, 'train/loss': 1.431403636932373, 'train/bleu': 34.47885990244001, 'validation/accuracy': 0.6928781867027283, 'validation/loss': 1.3808809518814087, 'validation/bleu': 30.620935746207273, 'validation/num_examples': 3000, 'test/accuracy': 0.7091530561447144, 'test/loss': 1.2769479751586914, 'test/bleu': 30.599210739555136, 'test/num_examples': 3003, 'score': 34478.14409327507, 'total_duration': 59170.0775141716, 'accumulated_submission_time': 34478.14409327507, 'accumulated_eval_time': 24686.036029338837, 'accumulated_logging_time': 0.9009041786193848}
I0306 11:39:07.050066 139742096045824 logging_writer.py:48] [97809] accumulated_eval_time=24686, accumulated_logging_time=0.900904, accumulated_submission_time=34478.1, global_step=97809, preemption_count=0, score=34478.1, test/accuracy=0.709153, test/bleu=30.5992, test/loss=1.27695, test/num_examples=3003, total_duration=59170.1, train/accuracy=0.684256, train/bleu=34.4789, train/loss=1.4314, validation/accuracy=0.692878, validation/bleu=30.6209, validation/loss=1.38088, validation/num_examples=3000
I0306 11:39:39.290546 139742087653120 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.22507373988628387, loss=1.4766440391540527
I0306 11:40:14.473713 139742096045824 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.23551538586616516, loss=1.5333882570266724
I0306 11:40:49.701279 139742087653120 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.23204202950000763, loss=1.5357348918914795
I0306 11:41:24.878729 139742096045824 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.21668227016925812, loss=1.5350937843322754
I0306 11:42:00.080943 139742087653120 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.22619131207466125, loss=1.5292500257492065
I0306 11:42:35.298075 139742096045824 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.2191718965768814, loss=1.521240472793579
I0306 11:43:10.489774 139742087653120 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.22673122584819794, loss=1.5066051483154297
I0306 11:43:45.705885 139742096045824 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.23181894421577454, loss=1.4626960754394531
I0306 11:44:20.889808 139742087653120 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.22792035341262817, loss=1.4619041681289673
I0306 11:44:56.129138 139742096045824 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.21105952560901642, loss=1.4482715129852295
I0306 11:45:31.364843 139742087653120 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.21969707310199738, loss=1.472509503364563
I0306 11:46:06.588756 139742096045824 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2133084386587143, loss=1.5219697952270508
I0306 11:46:41.805112 139742087653120 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.2105640470981598, loss=1.471593976020813
I0306 11:47:17.026521 139742096045824 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.22501763701438904, loss=1.470596194267273
I0306 11:47:52.260679 139742087653120 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.2289707362651825, loss=1.5351042747497559
I0306 11:48:27.489018 139742096045824 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.22044479846954346, loss=1.4457200765609741
I0306 11:49:02.744441 139742087653120 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.21614469587802887, loss=1.4988079071044922
I0306 11:49:37.931879 139742096045824 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.22506675124168396, loss=1.5215436220169067
I0306 11:50:13.144378 139742087653120 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22318226099014282, loss=1.5366740226745605
I0306 11:50:48.354691 139742096045824 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.22247496247291565, loss=1.4530975818634033
I0306 11:51:23.571506 139742087653120 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.23073728382587433, loss=1.5139987468719482
I0306 11:51:58.767064 139742096045824 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.22826167941093445, loss=1.4805275201797485
I0306 11:52:33.982327 139742087653120 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.2442874163389206, loss=1.5785006284713745
I0306 11:53:07.078355 139885236581568 spec.py:321] Evaluating on the training split.
I0306 11:53:09.702189 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 11:57:34.325686 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 11:57:36.938694 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:01:05.786063 139885236581568 spec.py:349] Evaluating on the test split.
I0306 12:01:08.382098 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:04:04.618293 139885236581568 submission_runner.py:469] Time since start: 60667.66s, 	Step: 100195, 	{'train/accuracy': 0.683368444442749, 'train/loss': 1.443142294883728, 'train/bleu': 34.883796549817305, 'validation/accuracy': 0.6939164400100708, 'validation/loss': 1.378912091255188, 'validation/bleu': 30.696563969855934, 'validation/num_examples': 3000, 'test/accuracy': 0.710334837436676, 'test/loss': 1.2714380025863647, 'test/bleu': 30.603064640650743, 'test/num_examples': 3003, 'score': 35318.042961359024, 'total_duration': 60667.664976358414, 'accumulated_submission_time': 35318.042961359024, 'accumulated_eval_time': 25343.57590985298, 'accumulated_logging_time': 0.9284610748291016}
I0306 12:04:04.636523 139742096045824 logging_writer.py:48] [100195] accumulated_eval_time=25343.6, accumulated_logging_time=0.928461, accumulated_submission_time=35318, global_step=100195, preemption_count=0, score=35318, test/accuracy=0.710335, test/bleu=30.6031, test/loss=1.27144, test/num_examples=3003, total_duration=60667.7, train/accuracy=0.683368, train/bleu=34.8838, train/loss=1.44314, validation/accuracy=0.693916, validation/bleu=30.6966, validation/loss=1.37891, validation/num_examples=3000
I0306 12:04:06.737573 139742087653120 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.2188343107700348, loss=1.4920368194580078
I0306 12:04:41.729909 139742096045824 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.2091626524925232, loss=1.4343369007110596
I0306 12:05:16.836613 139742087653120 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.22287829220294952, loss=1.4764001369476318
I0306 12:05:52.075401 139742096045824 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.2219482809305191, loss=1.4002097845077515
I0306 12:06:27.264975 139742087653120 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21677421033382416, loss=1.4464912414550781
I0306 12:07:02.441501 139742096045824 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.21873219311237335, loss=1.4115766286849976
I0306 12:07:37.630693 139742087653120 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.2256099134683609, loss=1.4023301601409912
I0306 12:08:12.838380 139742096045824 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.22489698231220245, loss=1.4961341619491577
I0306 12:08:48.037865 139742087653120 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.2258523851633072, loss=1.5380579233169556
I0306 12:09:23.242483 139742096045824 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.22652779519557953, loss=1.4375914335250854
I0306 12:09:58.438206 139742087653120 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.22368855774402618, loss=1.4796990156173706
I0306 12:10:33.666025 139742096045824 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.2167121022939682, loss=1.4857103824615479
I0306 12:11:08.885088 139742087653120 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.22922855615615845, loss=1.5182666778564453
I0306 12:11:44.090535 139742096045824 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.2265530675649643, loss=1.4893757104873657
I0306 12:12:19.293642 139742087653120 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.21975409984588623, loss=1.560579776763916
I0306 12:12:54.500950 139742096045824 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.22707664966583252, loss=1.5024105310440063
I0306 12:13:29.737181 139742087653120 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.221445694565773, loss=1.4543503522872925
I0306 12:14:04.940126 139742096045824 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.23243039846420288, loss=1.567468523979187
I0306 12:14:40.138300 139742087653120 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.2294999361038208, loss=1.5616304874420166
I0306 12:15:15.350264 139742096045824 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.23309779167175293, loss=1.568682312965393
I0306 12:15:50.512657 139742087653120 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.21655304729938507, loss=1.4540711641311646
I0306 12:16:25.653209 139742096045824 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22909784317016602, loss=1.5262051820755005
I0306 12:17:00.846664 139742087653120 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.22350622713565826, loss=1.467118501663208
I0306 12:17:36.017083 139742096045824 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.21861252188682556, loss=1.5057958364486694
I0306 12:18:04.888395 139885236581568 spec.py:321] Evaluating on the training split.
I0306 12:18:07.515401 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:22:36.784124 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 12:22:39.379621 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:25:56.930667 139885236581568 spec.py:349] Evaluating on the test split.
I0306 12:25:59.535211 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:29:36.494617 139885236581568 submission_runner.py:469] Time since start: 62199.54s, 	Step: 102583, 	{'train/accuracy': 0.6922838687896729, 'train/loss': 1.393486499786377, 'train/bleu': 35.181573749581695, 'validation/accuracy': 0.6957951188087463, 'validation/loss': 1.3708521127700806, 'validation/bleu': 30.801268478507772, 'validation/num_examples': 3000, 'test/accuracy': 0.7120496034622192, 'test/loss': 1.2633037567138672, 'test/bleu': 30.9098532590266, 'test/num_examples': 3003, 'score': 36158.16145992279, 'total_duration': 62199.5412940979, 'accumulated_submission_time': 36158.16145992279, 'accumulated_eval_time': 26035.182079553604, 'accumulated_logging_time': 0.9557516574859619}
I0306 12:29:36.513254 139742087653120 logging_writer.py:48] [102583] accumulated_eval_time=26035.2, accumulated_logging_time=0.955752, accumulated_submission_time=36158.2, global_step=102583, preemption_count=0, score=36158.2, test/accuracy=0.71205, test/bleu=30.9099, test/loss=1.2633, test/num_examples=3003, total_duration=62199.5, train/accuracy=0.692284, train/bleu=35.1816, train/loss=1.39349, validation/accuracy=0.695795, validation/bleu=30.8013, validation/loss=1.37085, validation/num_examples=3000
I0306 12:29:42.819120 139742096045824 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.21867729723453522, loss=1.4754267930984497
I0306 12:30:17.842748 139742087653120 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23056696355342865, loss=1.5235401391983032
I0306 12:30:52.994986 139742096045824 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.22685223817825317, loss=1.5587537288665771
I0306 12:31:28.173218 139742087653120 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.2721862196922302, loss=1.5132184028625488
I0306 12:32:03.375684 139742096045824 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.24321754276752472, loss=1.4704337120056152
I0306 12:32:38.591156 139742087653120 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.21884647011756897, loss=1.5000025033950806
I0306 12:33:13.783401 139742096045824 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.23274220526218414, loss=1.4400016069412231
I0306 12:33:48.984082 139742087653120 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.23407651484012604, loss=1.465296983718872
I0306 12:34:24.187986 139742096045824 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.22753766179084778, loss=1.5115940570831299
I0306 12:34:59.413350 139742087653120 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.23032799363136292, loss=1.4547450542449951
I0306 12:35:34.581717 139742096045824 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.22065488994121552, loss=1.513533592224121
I0306 12:36:09.836354 139742087653120 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.23727962374687195, loss=1.554038405418396
I0306 12:36:45.028766 139742096045824 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22985023260116577, loss=1.4757323265075684
I0306 12:37:20.251775 139742087653120 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.2367127537727356, loss=1.460072636604309
I0306 12:37:55.439788 139742096045824 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.2355411946773529, loss=1.4581718444824219
I0306 12:38:30.654436 139742087653120 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.2283894568681717, loss=1.5352250337600708
I0306 12:39:05.853163 139742096045824 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.21953901648521423, loss=1.450490117073059
I0306 12:39:41.054977 139742087653120 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.22831344604492188, loss=1.5510883331298828
I0306 12:40:16.257358 139742096045824 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.21667858958244324, loss=1.4902231693267822
I0306 12:40:51.464424 139742087653120 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.21913954615592957, loss=1.4463317394256592
I0306 12:41:26.689383 139742096045824 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.2244875282049179, loss=1.4724467992782593
I0306 12:42:01.883465 139742087653120 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.23627673089504242, loss=1.4168967008590698
I0306 12:42:37.060712 139742096045824 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.22610358893871307, loss=1.482340693473816
I0306 12:43:12.286475 139742087653120 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.22661903500556946, loss=1.4544486999511719
I0306 12:43:36.604880 139885236581568 spec.py:321] Evaluating on the training split.
I0306 12:43:39.231709 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:47:57.920203 139885236581568 spec.py:333] Evaluating on the validation split.
I0306 12:48:00.525044 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:50:58.449342 139885236581568 spec.py:349] Evaluating on the test split.
I0306 12:51:01.046558 139885236581568 workload.py:181] Translating evaluation dataset.
I0306 12:53:20.979270 139885236581568 submission_runner.py:469] Time since start: 63624.03s, 	Step: 104970, 	{'train/accuracy': 0.6866263151168823, 'train/loss': 1.4214521646499634, 'train/bleu': 34.839753745918955, 'validation/accuracy': 0.695758044719696, 'validation/loss': 1.3688535690307617, 'validation/bleu': 30.853491187933724, 'validation/num_examples': 3000, 'test/accuracy': 0.7108678221702576, 'test/loss': 1.2609308958053589, 'test/bleu': 30.945976805840736, 'test/num_examples': 3003, 'score': 36998.12640976906, 'total_duration': 63624.02596259117, 'accumulated_submission_time': 36998.12640976906, 'accumulated_eval_time': 26619.55642414093, 'accumulated_logging_time': 0.9824159145355225}
I0306 12:53:20.997920 139742096045824 logging_writer.py:48] [104970] accumulated_eval_time=26619.6, accumulated_logging_time=0.982416, accumulated_submission_time=36998.1, global_step=104970, preemption_count=0, score=36998.1, test/accuracy=0.710868, test/bleu=30.946, test/loss=1.26093, test/num_examples=3003, total_duration=63624, train/accuracy=0.686626, train/bleu=34.8398, train/loss=1.42145, validation/accuracy=0.695758, validation/bleu=30.8535, validation/loss=1.36885, validation/num_examples=3000
I0306 12:53:21.017810 139742087653120 logging_writer.py:48] [104970] global_step=104970, preemption_count=0, score=36998.1
I0306 12:53:21.042438 139885236581568 submission_runner.py:646] Tuning trial 5/5
I0306 12:53:21.042540 139885236581568 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0306 12:53:21.044344 139885236581568 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005968984332866967, 'train/loss': 11.112054824829102, 'train/bleu': 1.3999307104715225e-05, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.112346649169922, 'validation/bleu': 3.0022342653337152e-06, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.119194984436035, 'test/bleu': 5.469450157135306e-06, 'test/num_examples': 3003, 'score': 36.287920236587524, 'total_duration': 952.3253033161163, 'accumulated_submission_time': 36.287920236587524, 'accumulated_eval_time': 916.0372471809387, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2386, {'train/accuracy': 0.5155846476554871, 'train/loss': 2.830928087234497, 'train/bleu': 22.62922485475278, 'validation/accuracy': 0.5207648277282715, 'validation/loss': 2.7797136306762695, 'validation/bleu': 18.609285283978583, 'validation/num_examples': 3000, 'test/accuracy': 0.5181555151939392, 'test/loss': 2.8277132511138916, 'test/bleu': 17.034169741275207, 'test/num_examples': 3003, 'score': 876.4308915138245, 'total_duration': 2309.0080399513245, 'accumulated_submission_time': 876.4308915138245, 'accumulated_eval_time': 1432.4235861301422, 'accumulated_logging_time': 0.01533961296081543, 'global_step': 2386, 'preemption_count': 0}), (4770, {'train/accuracy': 0.5809102058410645, 'train/loss': 2.2332332134246826, 'train/bleu': 26.847404537341863, 'validation/accuracy': 0.5918596982955933, 'validation/loss': 2.131425142288208, 'validation/bleu': 23.74768278772634, 'validation/num_examples': 3000, 'test/accuracy': 0.5961418151855469, 'test/loss': 2.1029398441314697, 'test/bleu': 22.01586094768627, 'test/num_examples': 3003, 'score': 1716.410752773285, 'total_duration': 3606.2566981315613, 'accumulated_submission_time': 1716.410752773285, 'accumulated_eval_time': 1889.5378682613373, 'accumulated_logging_time': 0.032012224197387695, 'global_step': 4770, 'preemption_count': 0}), (7156, {'train/accuracy': 0.6053150296211243, 'train/loss': 2.010892629623413, 'train/bleu': 29.075266441995126, 'validation/accuracy': 0.6186809539794922, 'validation/loss': 1.9145147800445557, 'validation/bleu': 25.12629937778364, 'validation/num_examples': 3000, 'test/accuracy': 0.6237632036209106, 'test/loss': 1.8658379316329956, 'test/bleu': 24.159114578400022, 'test/num_examples': 3003, 'score': 2556.4836115837097, 'total_duration': 4928.900734901428, 'accumulated_submission_time': 2556.4836115837097, 'accumulated_eval_time': 2371.9605751037598, 'accumulated_logging_time': 0.04958701133728027, 'global_step': 7156, 'preemption_count': 0}), (9545, {'train/accuracy': 0.6147077083587646, 'train/loss': 1.9319205284118652, 'train/bleu': 29.794039277321136, 'validation/accuracy': 0.6319308876991272, 'validation/loss': 1.8102539777755737, 'validation/bleu': 26.293252720394303, 'validation/num_examples': 3000, 'test/accuracy': 0.640447199344635, 'test/loss': 1.7457208633422852, 'test/bleu': 25.006314914270256, 'test/num_examples': 3003, 'score': 3396.4345319271088, 'total_duration': 6237.004716396332, 'accumulated_submission_time': 3396.4345319271088, 'accumulated_eval_time': 2839.97154211998, 'accumulated_logging_time': 0.06656622886657715, 'global_step': 9545, 'preemption_count': 0}), (11934, {'train/accuracy': 0.619958221912384, 'train/loss': 1.8879899978637695, 'train/bleu': 30.050480550476113, 'validation/accuracy': 0.6389884948730469, 'validation/loss': 1.7453337907791138, 'validation/bleu': 26.689153439416398, 'validation/num_examples': 3000, 'test/accuracy': 0.6483373641967773, 'test/loss': 1.681549310684204, 'test/bleu': 25.503032994386196, 'test/num_examples': 3003, 'score': 4236.611150741577, 'total_duration': 7565.879806756973, 'accumulated_submission_time': 4236.611150741577, 'accumulated_eval_time': 3328.52721619606, 'accumulated_logging_time': 0.08384037017822266, 'global_step': 11934, 'preemption_count': 0}), (14321, {'train/accuracy': 0.6277436017990112, 'train/loss': 1.8285695314407349, 'train/bleu': 30.518571804021665, 'validation/accuracy': 0.6443403363227844, 'validation/loss': 1.7072266340255737, 'validation/bleu': 27.121178480830874, 'validation/num_examples': 3000, 'test/accuracy': 0.6547097563743591, 'test/loss': 1.6385301351547241, 'test/bleu': 26.32388591131773, 'test/num_examples': 3003, 'score': 5076.754111766815, 'total_duration': 8917.865320682526, 'accumulated_submission_time': 5076.754111766815, 'accumulated_eval_time': 3840.2282617092133, 'accumulated_logging_time': 0.10098695755004883, 'global_step': 14321, 'preemption_count': 0}), (16708, {'train/accuracy': 0.6294390559196472, 'train/loss': 1.8149199485778809, 'train/bleu': 30.73203820463991, 'validation/accuracy': 0.6504956483840942, 'validation/loss': 1.677403211593628, 'validation/bleu': 27.360159098301146, 'validation/num_examples': 3000, 'test/accuracy': 0.6586953997612, 'test/loss': 1.607671856880188, 'test/bleu': 26.478083320604117, 'test/num_examples': 3003, 'score': 5916.937549591064, 'total_duration': 10291.261077404022, 'accumulated_submission_time': 5916.937549591064, 'accumulated_eval_time': 4373.3016719818115, 'accumulated_logging_time': 0.11936521530151367, 'global_step': 16708, 'preemption_count': 0}), (19092, {'train/accuracy': 0.6501817107200623, 'train/loss': 1.6559077501296997, 'train/bleu': 31.553598452572924, 'validation/accuracy': 0.6511631011962891, 'validation/loss': 1.6547967195510864, 'validation/bleu': 27.32445786247743, 'validation/num_examples': 3000, 'test/accuracy': 0.662009060382843, 'test/loss': 1.5799468755722046, 'test/bleu': 26.7706046642871, 'test/num_examples': 3003, 'score': 6756.903774738312, 'total_duration': 11613.320173025131, 'accumulated_submission_time': 6756.903774738312, 'accumulated_eval_time': 4855.255084991455, 'accumulated_logging_time': 0.1366136074066162, 'global_step': 19092, 'preemption_count': 0}), (21467, {'train/accuracy': 0.6384540796279907, 'train/loss': 1.7438496351242065, 'train/bleu': 31.142454487942356, 'validation/accuracy': 0.6537092328071594, 'validation/loss': 1.6332379579544067, 'validation/bleu': 27.787133537813673, 'validation/num_examples': 3000, 'test/accuracy': 0.6646854281425476, 'test/loss': 1.5604816675186157, 'test/bleu': 26.834439389606466, 'test/num_examples': 3003, 'score': 7596.819486618042, 'total_duration': 12994.405839920044, 'accumulated_submission_time': 7596.819486618042, 'accumulated_eval_time': 5396.285189151764, 'accumulated_logging_time': 0.155808687210083, 'global_step': 21467, 'preemption_count': 0}), (23844, {'train/accuracy': 0.6358504295349121, 'train/loss': 1.7706661224365234, 'train/bleu': 30.908002248377738, 'validation/accuracy': 0.6564284563064575, 'validation/loss': 1.621553659439087, 'validation/bleu': 27.775573021909228, 'validation/num_examples': 3000, 'test/accuracy': 0.6653458476066589, 'test/loss': 1.5477651357650757, 'test/bleu': 27.030670050558573, 'test/num_examples': 3003, 'score': 8436.71908211708, 'total_duration': 14423.136200666428, 'accumulated_submission_time': 8436.71908211708, 'accumulated_eval_time': 5984.979205608368, 'accumulated_logging_time': 0.17611479759216309, 'global_step': 23844, 'preemption_count': 0}), (26224, {'train/accuracy': 0.6429998278617859, 'train/loss': 1.7062196731567383, 'train/bleu': 31.46378502208229, 'validation/accuracy': 0.6591352820396423, 'validation/loss': 1.6072542667388916, 'validation/bleu': 28.28424042137784, 'validation/num_examples': 3000, 'test/accuracy': 0.6689491271972656, 'test/loss': 1.5348715782165527, 'test/bleu': 27.385319889382068, 'test/num_examples': 3003, 'score': 9276.811330795288, 'total_duration': 15817.393292665482, 'accumulated_submission_time': 9276.811330795288, 'accumulated_eval_time': 6539.005263328552, 'accumulated_logging_time': 0.19555974006652832, 'global_step': 26224, 'preemption_count': 0}), (28608, {'train/accuracy': 0.6398133635520935, 'train/loss': 1.7360574007034302, 'train/bleu': 31.340299294472487, 'validation/accuracy': 0.6612612009048462, 'validation/loss': 1.5949608087539673, 'validation/bleu': 28.12563769336987, 'validation/num_examples': 3000, 'test/accuracy': 0.6713590621948242, 'test/loss': 1.5191816091537476, 'test/bleu': 27.61001260264253, 'test/num_examples': 3003, 'score': 10116.96026802063, 'total_duration': 17281.600105047226, 'accumulated_submission_time': 10116.96026802063, 'accumulated_eval_time': 7162.924523353577, 'accumulated_logging_time': 0.21575093269348145, 'global_step': 28608, 'preemption_count': 0}), (30991, {'train/accuracy': 0.6429835557937622, 'train/loss': 1.722961664199829, 'train/bleu': 31.29162549797011, 'validation/accuracy': 0.6604331135749817, 'validation/loss': 1.5888334512710571, 'validation/bleu': 27.961954303677967, 'validation/num_examples': 3000, 'test/accuracy': 0.6739196181297302, 'test/loss': 1.5085657835006714, 'test/bleu': 27.348063364341442, 'test/num_examples': 3003, 'score': 10956.880996704102, 'total_duration': 18611.176918268204, 'accumulated_submission_time': 10956.880996704102, 'accumulated_eval_time': 7652.439444065094, 'accumulated_logging_time': 0.23578929901123047, 'global_step': 30991, 'preemption_count': 0}), (33372, {'train/accuracy': 0.641374409198761, 'train/loss': 1.7096197605133057, 'train/bleu': 31.407106989497983, 'validation/accuracy': 0.6623612642288208, 'validation/loss': 1.5775400400161743, 'validation/bleu': 28.113268729657044, 'validation/num_examples': 3000, 'test/accuracy': 0.6737342476844788, 'test/loss': 1.5002679824829102, 'test/bleu': 27.498789061251816, 'test/num_examples': 3003, 'score': 11796.889864444733, 'total_duration': 20047.64496088028, 'accumulated_submission_time': 11796.889864444733, 'accumulated_eval_time': 8248.755282878876, 'accumulated_logging_time': 0.2577850818634033, 'global_step': 33372, 'preemption_count': 0}), (35757, {'train/accuracy': 0.6438221335411072, 'train/loss': 1.7084859609603882, 'train/bleu': 31.09415555574639, 'validation/accuracy': 0.6628680229187012, 'validation/loss': 1.5695996284484863, 'validation/bleu': 28.262267697305713, 'validation/num_examples': 3000, 'test/accuracy': 0.6755648255348206, 'test/loss': 1.493857502937317, 'test/bleu': 27.756730013202453, 'test/num_examples': 3003, 'score': 12636.887555599213, 'total_duration': 21389.79168343544, 'accumulated_submission_time': 12636.887555599213, 'accumulated_eval_time': 8750.76489830017, 'accumulated_logging_time': 0.2774543762207031, 'global_step': 35757, 'preemption_count': 0}), (38143, {'train/accuracy': 0.6565806865692139, 'train/loss': 1.6036137342453003, 'train/bleu': 32.481665664560836, 'validation/accuracy': 0.6639928221702576, 'validation/loss': 1.5630806684494019, 'validation/bleu': 28.53714031591542, 'validation/num_examples': 3000, 'test/accuracy': 0.6750202775001526, 'test/loss': 1.4866255521774292, 'test/bleu': 27.636506933172203, 'test/num_examples': 3003, 'score': 13476.949308872223, 'total_duration': 22791.275188684464, 'accumulated_submission_time': 13476.949308872223, 'accumulated_eval_time': 9312.041680574417, 'accumulated_logging_time': 0.3001992702484131, 'global_step': 38143, 'preemption_count': 0}), (40529, {'train/accuracy': 0.6460224390029907, 'train/loss': 1.6900311708450317, 'train/bleu': 31.691792158137854, 'validation/accuracy': 0.6660074591636658, 'validation/loss': 1.554399013519287, 'validation/bleu': 28.36518742532591, 'validation/num_examples': 3000, 'test/accuracy': 0.6779515743255615, 'test/loss': 1.4710854291915894, 'test/bleu': 27.866400599934593, 'test/num_examples': 3003, 'score': 14317.008560657501, 'total_duration': 24131.521239042282, 'accumulated_submission_time': 14317.008560657501, 'accumulated_eval_time': 9812.085590839386, 'accumulated_logging_time': 0.3209075927734375, 'global_step': 40529, 'preemption_count': 0}), (42914, {'train/accuracy': 0.6463142037391663, 'train/loss': 1.6980946063995361, 'train/bleu': 31.02852420549763, 'validation/accuracy': 0.6663906574249268, 'validation/loss': 1.5459277629852295, 'validation/bleu': 28.991434465037198, 'validation/num_examples': 3000, 'test/accuracy': 0.6786583065986633, 'test/loss': 1.464038372039795, 'test/bleu': 28.160435770490817, 'test/num_examples': 3003, 'score': 15156.995676517487, 'total_duration': 25468.632650136948, 'accumulated_submission_time': 15156.995676517487, 'accumulated_eval_time': 10309.065776109695, 'accumulated_logging_time': 0.34190893173217773, 'global_step': 42914, 'preemption_count': 0}), (45304, {'train/accuracy': 0.6534265875816345, 'train/loss': 1.633876085281372, 'train/bleu': 32.34557604502924, 'validation/accuracy': 0.667181670665741, 'validation/loss': 1.544896125793457, 'validation/bleu': 28.757719735774693, 'validation/num_examples': 3000, 'test/accuracy': 0.6800834536552429, 'test/loss': 1.4621938467025757, 'test/bleu': 27.960087803744564, 'test/num_examples': 3003, 'score': 15997.106632471085, 'total_duration': 26840.508650541306, 'accumulated_submission_time': 15997.106632471085, 'accumulated_eval_time': 10840.690826654434, 'accumulated_logging_time': 0.3619399070739746, 'global_step': 45304, 'preemption_count': 0}), (47696, {'train/accuracy': 0.648568868637085, 'train/loss': 1.6702409982681274, 'train/bleu': 31.505368858483, 'validation/accuracy': 0.6689121127128601, 'validation/loss': 1.5327070951461792, 'validation/bleu': 28.53076402120324, 'validation/num_examples': 3000, 'test/accuracy': 0.6815896034240723, 'test/loss': 1.4474530220031738, 'test/bleu': 28.03009694102109, 'test/num_examples': 3003, 'score': 16837.045126199722, 'total_duration': 28190.26974964142, 'accumulated_submission_time': 16837.045126199722, 'accumulated_eval_time': 11350.37526512146, 'accumulated_logging_time': 0.3831977844238281, 'global_step': 47696, 'preemption_count': 0}), (50088, {'train/accuracy': 0.646837055683136, 'train/loss': 1.6821136474609375, 'train/bleu': 31.885168824700628, 'validation/accuracy': 0.6696289777755737, 'validation/loss': 1.5294376611709595, 'validation/bleu': 28.675530194392284, 'validation/num_examples': 3000, 'test/accuracy': 0.6807090640068054, 'test/loss': 1.4460885524749756, 'test/bleu': 28.14150435242082, 'test/num_examples': 3003, 'score': 17677.062327861786, 'total_duration': 29638.5751247406, 'accumulated_submission_time': 17677.062327861786, 'accumulated_eval_time': 11958.522782325745, 'accumulated_logging_time': 0.4037659168243408, 'global_step': 50088, 'preemption_count': 0}), (52476, {'train/accuracy': 0.6543849110603333, 'train/loss': 1.6304444074630737, 'train/bleu': 32.57838162859172, 'validation/accuracy': 0.6717425584793091, 'validation/loss': 1.5152212381362915, 'validation/bleu': 28.740153040064893, 'validation/num_examples': 3000, 'test/accuracy': 0.6849148273468018, 'test/loss': 1.4301799535751343, 'test/bleu': 28.293649156891888, 'test/num_examples': 3003, 'score': 18517.200508356094, 'total_duration': 31135.088946819305, 'accumulated_submission_time': 18517.200508356094, 'accumulated_eval_time': 12614.759129524231, 'accumulated_logging_time': 0.4241955280303955, 'global_step': 52476, 'preemption_count': 0}), (54864, {'train/accuracy': 0.6557061076164246, 'train/loss': 1.6275218725204468, 'train/bleu': 32.064179227481695, 'validation/accuracy': 0.6722493171691895, 'validation/loss': 1.509812593460083, 'validation/bleu': 29.27205322842188, 'validation/num_examples': 3000, 'test/accuracy': 0.6857143044471741, 'test/loss': 1.4192802906036377, 'test/bleu': 28.59075865788106, 'test/num_examples': 3003, 'score': 19357.332693338394, 'total_duration': 32572.17284846306, 'accumulated_submission_time': 19357.332693338394, 'accumulated_eval_time': 13211.566282749176, 'accumulated_logging_time': 0.44776129722595215, 'global_step': 54864, 'preemption_count': 0}), (57249, {'train/accuracy': 0.6651856303215027, 'train/loss': 1.5562095642089844, 'train/bleu': 32.501149336854596, 'validation/accuracy': 0.6733987927436829, 'validation/loss': 1.5057837963104248, 'validation/bleu': 29.38013745118279, 'validation/num_examples': 3000, 'test/accuracy': 0.6871857047080994, 'test/loss': 1.4133574962615967, 'test/bleu': 28.52058064367549, 'test/num_examples': 3003, 'score': 20197.22725534439, 'total_duration': 33945.13581442833, 'accumulated_submission_time': 20197.22725534439, 'accumulated_eval_time': 13744.489685058594, 'accumulated_logging_time': 0.46982407569885254, 'global_step': 57249, 'preemption_count': 0}), (59636, {'train/accuracy': 0.6592624187469482, 'train/loss': 1.6009031534194946, 'train/bleu': 32.88387624002771, 'validation/accuracy': 0.6752033233642578, 'validation/loss': 1.490800142288208, 'validation/bleu': 29.430445295752012, 'validation/num_examples': 3000, 'test/accuracy': 0.6879040598869324, 'test/loss': 1.404262661933899, 'test/bleu': 29.035741080313024, 'test/num_examples': 3003, 'score': 21037.277321577072, 'total_duration': 35601.19456291199, 'accumulated_submission_time': 21037.277321577072, 'accumulated_eval_time': 14560.35464167595, 'accumulated_logging_time': 0.4947798252105713, 'global_step': 59636, 'preemption_count': 0}), (62022, {'train/accuracy': 0.6542908549308777, 'train/loss': 1.6235066652297974, 'train/bleu': 32.23385560021843, 'validation/accuracy': 0.6766617894172668, 'validation/loss': 1.4837257862091064, 'validation/bleu': 29.228771543920587, 'validation/num_examples': 3000, 'test/accuracy': 0.6891553997993469, 'test/loss': 1.3970434665679932, 'test/bleu': 28.92918005614701, 'test/num_examples': 3003, 'score': 21877.30684351921, 'total_duration': 37032.74046874046, 'accumulated_submission_time': 21877.30684351921, 'accumulated_eval_time': 15151.722733974457, 'accumulated_logging_time': 0.5221579074859619, 'global_step': 62022, 'preemption_count': 0}), (64413, {'train/accuracy': 0.6656155586242676, 'train/loss': 1.5586925745010376, 'train/bleu': 33.114841645828605, 'validation/accuracy': 0.6765629649162292, 'validation/loss': 1.4768515825271606, 'validation/bleu': 29.282869187823604, 'validation/num_examples': 3000, 'test/accuracy': 0.6907658576965332, 'test/loss': 1.3847627639770508, 'test/bleu': 29.362154066471096, 'test/num_examples': 3003, 'score': 22717.22613310814, 'total_duration': 38449.293536901474, 'accumulated_submission_time': 22717.22613310814, 'accumulated_eval_time': 15728.211986780167, 'accumulated_logging_time': 0.5455820560455322, 'global_step': 64413, 'preemption_count': 0}), (66806, {'train/accuracy': 0.6609426736831665, 'train/loss': 1.5866572856903076, 'train/bleu': 32.313436372213715, 'validation/accuracy': 0.6784416437149048, 'validation/loss': 1.473099946975708, 'validation/bleu': 29.348781815296835, 'validation/num_examples': 3000, 'test/accuracy': 0.6939867734909058, 'test/loss': 1.3743016719818115, 'test/bleu': 29.25840474652485, 'test/num_examples': 3003, 'score': 23557.170120239258, 'total_duration': 39922.286219120026, 'accumulated_submission_time': 23557.170120239258, 'accumulated_eval_time': 16361.117591619492, 'accumulated_logging_time': 0.5681564807891846, 'global_step': 66806, 'preemption_count': 0}), (69198, {'train/accuracy': 0.6814278960227966, 'train/loss': 1.4526829719543457, 'train/bleu': 34.33669969487311, 'validation/accuracy': 0.6807777285575867, 'validation/loss': 1.4597761631011963, 'validation/bleu': 29.752712343561274, 'validation/num_examples': 3000, 'test/accuracy': 0.6924806237220764, 'test/loss': 1.3710615634918213, 'test/bleu': 29.376223629152825, 'test/num_examples': 3003, 'score': 24397.180092811584, 'total_duration': 41319.22310304642, 'accumulated_submission_time': 24397.180092811584, 'accumulated_eval_time': 16917.89681839943, 'accumulated_logging_time': 0.593174934387207, 'global_step': 69198, 'preemption_count': 0}), (71585, {'train/accuracy': 0.6640447378158569, 'train/loss': 1.5616756677627563, 'train/bleu': 32.98032791086242, 'validation/accuracy': 0.6825822591781616, 'validation/loss': 1.4531196355819702, 'validation/bleu': 29.481937311735184, 'validation/num_examples': 3000, 'test/accuracy': 0.6946471929550171, 'test/loss': 1.362155556678772, 'test/bleu': 29.272507172363646, 'test/num_examples': 3003, 'score': 25237.385013103485, 'total_duration': 42990.806248903275, 'accumulated_submission_time': 25237.385013103485, 'accumulated_eval_time': 17749.134249210358, 'accumulated_logging_time': 0.6159002780914307, 'global_step': 71585, 'preemption_count': 0}), (73962, {'train/accuracy': 0.6669295430183411, 'train/loss': 1.5572843551635742, 'train/bleu': 33.073376780926715, 'validation/accuracy': 0.6822485327720642, 'validation/loss': 1.4546315670013428, 'validation/bleu': 30.058969972125116, 'validation/num_examples': 3000, 'test/accuracy': 0.6963387727737427, 'test/loss': 1.3564364910125732, 'test/bleu': 29.551286101779482, 'test/num_examples': 3003, 'score': 26077.530081510544, 'total_duration': 44423.53189110756, 'accumulated_submission_time': 26077.530081510544, 'accumulated_eval_time': 18341.571209430695, 'accumulated_logging_time': 0.6391153335571289, 'global_step': 73962, 'preemption_count': 0}), (76338, {'train/accuracy': 0.6771021485328674, 'train/loss': 1.4857722520828247, 'train/bleu': 34.567156465688754, 'validation/accuracy': 0.683880090713501, 'validation/loss': 1.4409430027008057, 'validation/bleu': 30.026824441141656, 'validation/num_examples': 3000, 'test/accuracy': 0.6990499496459961, 'test/loss': 1.342016577720642, 'test/bleu': 29.673529429214604, 'test/num_examples': 3003, 'score': 26917.524755239487, 'total_duration': 45866.19248723984, 'accumulated_submission_time': 26917.524755239487, 'accumulated_eval_time': 18944.087207317352, 'accumulated_logging_time': 0.6687526702880859, 'global_step': 76338, 'preemption_count': 0}), (78715, {'train/accuracy': 0.6721581220626831, 'train/loss': 1.5164607763290405, 'train/bleu': 33.62383509081055, 'validation/accuracy': 0.6857587695121765, 'validation/loss': 1.4350206851959229, 'validation/bleu': 30.052879559697878, 'validation/num_examples': 3000, 'test/accuracy': 0.7004286646842957, 'test/loss': 1.3303409814834595, 'test/bleu': 30.177913169933635, 'test/num_examples': 3003, 'score': 27757.699819803238, 'total_duration': 47319.530191898346, 'accumulated_submission_time': 27757.699819803238, 'accumulated_eval_time': 19557.104168653488, 'accumulated_logging_time': 0.6937887668609619, 'global_step': 78715, 'preemption_count': 0}), (81090, {'train/accuracy': 0.6715818643569946, 'train/loss': 1.5212568044662476, 'train/bleu': 33.612299009602246, 'validation/accuracy': 0.6869206428527832, 'validation/loss': 1.4230678081512451, 'validation/bleu': 29.950158803195844, 'validation/num_examples': 3000, 'test/accuracy': 0.7011702060699463, 'test/loss': 1.3279190063476562, 'test/bleu': 29.91503150060462, 'test/num_examples': 3003, 'score': 28597.628089427948, 'total_duration': 48704.99511170387, 'accumulated_submission_time': 28597.628089427948, 'accumulated_eval_time': 20102.49096941948, 'accumulated_logging_time': 0.7207818031311035, 'global_step': 81090, 'preemption_count': 0}), (83477, {'train/accuracy': 0.6751096844673157, 'train/loss': 1.4865987300872803, 'train/bleu': 33.96741543358203, 'validation/accuracy': 0.6875756978988647, 'validation/loss': 1.4165263175964355, 'validation/bleu': 30.19484164008841, 'validation/num_examples': 3000, 'test/accuracy': 0.7027691006660461, 'test/loss': 1.316909909248352, 'test/bleu': 29.700478638202743, 'test/num_examples': 3003, 'score': 29437.60350561142, 'total_duration': 50313.009873867035, 'accumulated_submission_time': 29437.60350561142, 'accumulated_eval_time': 20870.386761665344, 'accumulated_logging_time': 0.746668815612793, 'global_step': 83477, 'preemption_count': 0}), (85869, {'train/accuracy': 0.6745820641517639, 'train/loss': 1.4965497255325317, 'train/bleu': 33.503225221490595, 'validation/accuracy': 0.6878229379653931, 'validation/loss': 1.4148178100585938, 'validation/bleu': 30.183429615415964, 'validation/num_examples': 3000, 'test/accuracy': 0.7030587196350098, 'test/loss': 1.3134899139404297, 'test/bleu': 30.227746099986422, 'test/num_examples': 3003, 'score': 30277.74830722809, 'total_duration': 51785.51743197441, 'accumulated_submission_time': 30277.74830722809, 'accumulated_eval_time': 21502.60453248024, 'accumulated_logging_time': 0.7721519470214844, 'global_step': 85869, 'preemption_count': 0}), (88261, {'train/accuracy': 0.6885298490524292, 'train/loss': 1.408444881439209, 'train/bleu': 35.039764819947074, 'validation/accuracy': 0.6905297636985779, 'validation/loss': 1.4080631732940674, 'validation/bleu': 30.26102191038166, 'validation/num_examples': 3000, 'test/accuracy': 0.7043332457542419, 'test/loss': 1.3062477111816406, 'test/bleu': 30.292233355091927, 'test/num_examples': 3003, 'score': 31117.744438171387, 'total_duration': 53287.977234601974, 'accumulated_submission_time': 31117.744438171387, 'accumulated_eval_time': 22164.92399907112, 'accumulated_logging_time': 0.7976181507110596, 'global_step': 88261, 'preemption_count': 0}), (90647, {'train/accuracy': 0.681182861328125, 'train/loss': 1.4545726776123047, 'train/bleu': 34.419142203223515, 'validation/accuracy': 0.691407322883606, 'validation/loss': 1.3958710432052612, 'validation/bleu': 30.34154127941769, 'validation/num_examples': 3000, 'test/accuracy': 0.7075425982475281, 'test/loss': 1.2912956476211548, 'test/bleu': 30.57466208625405, 'test/num_examples': 3003, 'score': 31957.84832262993, 'total_duration': 54805.611669540405, 'accumulated_submission_time': 31957.84832262993, 'accumulated_eval_time': 22842.308034658432, 'accumulated_logging_time': 0.8229062557220459, 'global_step': 90647, 'preemption_count': 0}), (93035, {'train/accuracy': 0.6825011968612671, 'train/loss': 1.457434892654419, 'train/bleu': 33.80254110481923, 'validation/accuracy': 0.691815197467804, 'validation/loss': 1.3937342166900635, 'validation/bleu': 30.65905531582299, 'validation/num_examples': 3000, 'test/accuracy': 0.7074035406112671, 'test/loss': 1.2903655767440796, 'test/bleu': 30.41611768441175, 'test/num_examples': 3003, 'score': 32797.98564243317, 'total_duration': 56255.21722102165, 'accumulated_submission_time': 32797.98564243317, 'accumulated_eval_time': 23451.630036592484, 'accumulated_logging_time': 0.8482229709625244, 'global_step': 93035, 'preemption_count': 0}), (95422, {'train/accuracy': 0.6870148777961731, 'train/loss': 1.427930474281311, 'train/bleu': 34.64504173620339, 'validation/accuracy': 0.6932613253593445, 'validation/loss': 1.385765552520752, 'validation/bleu': 30.749760392242234, 'validation/num_examples': 3000, 'test/accuracy': 0.7098366618156433, 'test/loss': 1.2805626392364502, 'test/bleu': 30.710215511305474, 'test/num_examples': 3003, 'score': 33638.14417767525, 'total_duration': 57743.10424041748, 'accumulated_submission_time': 33638.14417767525, 'accumulated_eval_time': 24099.21170926094, 'accumulated_logging_time': 0.8736398220062256, 'global_step': 95422, 'preemption_count': 0}), (97809, {'train/accuracy': 0.6842559576034546, 'train/loss': 1.431403636932373, 'train/bleu': 34.47885990244001, 'validation/accuracy': 0.6928781867027283, 'validation/loss': 1.3808809518814087, 'validation/bleu': 30.620935746207273, 'validation/num_examples': 3000, 'test/accuracy': 0.7091530561447144, 'test/loss': 1.2769479751586914, 'test/bleu': 30.599210739555136, 'test/num_examples': 3003, 'score': 34478.14409327507, 'total_duration': 59170.0775141716, 'accumulated_submission_time': 34478.14409327507, 'accumulated_eval_time': 24686.036029338837, 'accumulated_logging_time': 0.9009041786193848, 'global_step': 97809, 'preemption_count': 0}), (100195, {'train/accuracy': 0.683368444442749, 'train/loss': 1.443142294883728, 'train/bleu': 34.883796549817305, 'validation/accuracy': 0.6939164400100708, 'validation/loss': 1.378912091255188, 'validation/bleu': 30.696563969855934, 'validation/num_examples': 3000, 'test/accuracy': 0.710334837436676, 'test/loss': 1.2714380025863647, 'test/bleu': 30.603064640650743, 'test/num_examples': 3003, 'score': 35318.042961359024, 'total_duration': 60667.664976358414, 'accumulated_submission_time': 35318.042961359024, 'accumulated_eval_time': 25343.57590985298, 'accumulated_logging_time': 0.9284610748291016, 'global_step': 100195, 'preemption_count': 0}), (102583, {'train/accuracy': 0.6922838687896729, 'train/loss': 1.393486499786377, 'train/bleu': 35.181573749581695, 'validation/accuracy': 0.6957951188087463, 'validation/loss': 1.3708521127700806, 'validation/bleu': 30.801268478507772, 'validation/num_examples': 3000, 'test/accuracy': 0.7120496034622192, 'test/loss': 1.2633037567138672, 'test/bleu': 30.9098532590266, 'test/num_examples': 3003, 'score': 36158.16145992279, 'total_duration': 62199.5412940979, 'accumulated_submission_time': 36158.16145992279, 'accumulated_eval_time': 26035.182079553604, 'accumulated_logging_time': 0.9557516574859619, 'global_step': 102583, 'preemption_count': 0}), (104970, {'train/accuracy': 0.6866263151168823, 'train/loss': 1.4214521646499634, 'train/bleu': 34.839753745918955, 'validation/accuracy': 0.695758044719696, 'validation/loss': 1.3688535690307617, 'validation/bleu': 30.853491187933724, 'validation/num_examples': 3000, 'test/accuracy': 0.7108678221702576, 'test/loss': 1.2609308958053589, 'test/bleu': 30.945976805840736, 'test/num_examples': 3003, 'score': 36998.12640976906, 'total_duration': 63624.02596259117, 'accumulated_submission_time': 36998.12640976906, 'accumulated_eval_time': 26619.55642414093, 'accumulated_logging_time': 0.9824159145355225, 'global_step': 104970, 'preemption_count': 0})], 'global_step': 104970}
I0306 12:53:21.044451 139885236581568 submission_runner.py:649] Timing: 36998.12640976906
I0306 12:53:21.044520 139885236581568 submission_runner.py:651] Total number of evals: 45
I0306 12:53:21.044556 139885236581568 submission_runner.py:652] ====================
I0306 12:53:21.044664 139885236581568 submission_runner.py:750] Final wmt score: 4
