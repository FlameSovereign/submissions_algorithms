python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-755500724 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-12-35.log
2025-03-05 19:12:36.566506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201956.589694       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201956.596969       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:42.918546 140127519114432 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax.
I0305 19:12:43.870671 140127519114432 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:43.873833 140127519114432 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:43.875497 140127519114432 submission_runner.py:606] Using RNG seed -755500724
I0305 19:12:44.470865 140127519114432 submission_runner.py:615] --- Tuning run 1/5 ---
I0305 19:12:44.471067 140127519114432 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_1.
I0305 19:12:44.471323 140127519114432 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_1/hparams.json.
I0305 19:12:44.714348 140127519114432 submission_runner.py:218] Initializing dataset.
I0305 19:12:44.930683 140127519114432 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:44.951098 140127519114432 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:12:45.176287 140127519114432 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:12:45.225715 140127519114432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:45.254816 140127519114432 submission_runner.py:229] Initializing model.
I0305 19:12:53.001361 140127519114432 submission_runner.py:272] Initializing optimizer.
I0305 19:12:53.423290 140127519114432 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:53.423522 140127519114432 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:53.424392 140127519114432 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_1 with prefix checkpoint_
I0305 19:12:53.424515 140127519114432 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_1/meta_data_0.json.
I0305 19:12:53.424699 140127519114432 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:53.424754 140127519114432 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:53.583684 140127519114432 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_1/flags_0.json.
I0305 19:12:53.616185 140127519114432 submission_runner.py:337] Starting training loop.
I0305 19:13:04.328310 140083327899392 logging_writer.py:48] [0] global_step=0, grad_norm=2.731511354446411, loss=0.7783588767051697
I0305 19:13:04.378518 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:13:04.382076 140127519114432 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:04.385426 140127519114432 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:04.445251 140127519114432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:21.538854 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:14:21.541680 140127519114432 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:21.545611 140127519114432 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:21.611204 140127519114432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:24.932665 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:15:24.935188 140127519114432 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:24.938956 140127519114432 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:24.998829 140127519114432 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:28.489260 140127519114432 submission_runner.py:469] Time since start: 214.87s, 	Step: 1, 	{'train/accuracy': 0.4533552825450897, 'train/loss': 0.7773282527923584, 'train/mean_average_precision': 0.021668608978261672, 'validation/accuracy': 0.448541522026062, 'validation/loss': 0.780256986618042, 'validation/mean_average_precision': 0.027700106460599398, 'validation/num_examples': 43793, 'test/accuracy': 0.4468294084072113, 'test/loss': 0.780686616897583, 'test/mean_average_precision': 0.027520667620448164, 'test/num_examples': 43793, 'score': 10.762229442596436, 'total_duration': 214.87293481826782, 'accumulated_submission_time': 10.762229442596436, 'accumulated_eval_time': 204.11060094833374, 'accumulated_logging_time': 0}
I0305 19:16:28.496417 139985432622848 logging_writer.py:48] [1] accumulated_eval_time=204.111, accumulated_logging_time=0, accumulated_submission_time=10.7622, global_step=1, preemption_count=0, score=10.7622, test/accuracy=0.446829, test/loss=0.780687, test/mean_average_precision=0.0275207, test/num_examples=43793, total_duration=214.873, train/accuracy=0.453355, train/loss=0.777328, train/mean_average_precision=0.0216686, validation/accuracy=0.448542, validation/loss=0.780257, validation/mean_average_precision=0.0277001, validation/num_examples=43793
I0305 19:16:49.370093 139985441015552 logging_writer.py:48] [100] global_step=100, grad_norm=0.5235795378684998, loss=0.4625709354877472
I0305 19:17:10.511978 139985432622848 logging_writer.py:48] [200] global_step=200, grad_norm=0.3503882586956024, loss=0.31425198912620544
I0305 19:17:31.523327 139985441015552 logging_writer.py:48] [300] global_step=300, grad_norm=0.21532729268074036, loss=0.1911919265985489
I0305 19:17:52.183011 139985432622848 logging_writer.py:48] [400] global_step=400, grad_norm=0.11477448791265488, loss=0.12225153297185898
I0305 19:18:13.078398 139985441015552 logging_writer.py:48] [500] global_step=500, grad_norm=0.06575263291597366, loss=0.07898204773664474
I0305 19:18:34.223242 139985432622848 logging_writer.py:48] [600] global_step=600, grad_norm=0.0370466373860836, loss=0.06935741007328033
I0305 19:18:55.723051 139986028422912 logging_writer.py:48] [700] global_step=700, grad_norm=0.08443669229745865, loss=0.06525074690580368
I0305 19:19:16.682348 139986020030208 logging_writer.py:48] [800] global_step=800, grad_norm=0.07073423266410828, loss=0.05000036582350731
I0305 19:19:37.877667 139986028422912 logging_writer.py:48] [900] global_step=900, grad_norm=0.14709323644638062, loss=0.057075511664152145
I0305 19:19:59.253898 139986020030208 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.402805358171463, loss=0.04960888624191284
I0305 19:20:20.498389 139986028422912 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.12094300985336304, loss=0.048080481588840485
I0305 19:20:28.675735 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:21:40.896982 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:21:42.799969 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:21:44.710565 140127519114432 submission_runner.py:469] Time since start: 531.09s, 	Step: 1140, 	{'train/accuracy': 0.9868268966674805, 'train/loss': 0.05152887850999832, 'train/mean_average_precision': 0.06714007559876531, 'validation/accuracy': 0.9842774868011475, 'validation/loss': 0.06125123053789139, 'validation/mean_average_precision': 0.06714002430027763, 'validation/num_examples': 43793, 'test/accuracy': 0.983289897441864, 'test/loss': 0.06449496001005173, 'test/mean_average_precision': 0.0674123032600958, 'test/num_examples': 43793, 'score': 250.90052938461304, 'total_duration': 531.0943324565887, 'accumulated_submission_time': 250.90052938461304, 'accumulated_eval_time': 280.14541029930115, 'accumulated_logging_time': 0.016854286193847656}
I0305 19:21:44.718969 139986020030208 logging_writer.py:48] [1140] accumulated_eval_time=280.145, accumulated_logging_time=0.0168543, accumulated_submission_time=250.901, global_step=1140, preemption_count=0, score=250.901, test/accuracy=0.98329, test/loss=0.064495, test/mean_average_precision=0.0674123, test/num_examples=43793, total_duration=531.094, train/accuracy=0.986827, train/loss=0.0515289, train/mean_average_precision=0.0671401, validation/accuracy=0.984277, validation/loss=0.0612512, validation/mean_average_precision=0.06714, validation/num_examples=43793
I0305 19:21:57.609038 139986028422912 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.08634598553180695, loss=0.04883170500397682
I0305 19:22:18.972743 139986020030208 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.2051830142736435, loss=0.051287032663822174
I0305 19:22:40.224481 139986028422912 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.08278001844882965, loss=0.04925477132201195
I0305 19:23:01.432002 139986020030208 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.08940228074789047, loss=0.04596870765089989
I0305 19:23:22.709197 139986028422912 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.19624122977256775, loss=0.05035960674285889
I0305 19:23:43.806609 139986020030208 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.10215422511100769, loss=0.05188322439789772
I0305 19:24:04.985583 139986028422912 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.045740753412246704, loss=0.04105771705508232
I0305 19:24:25.879227 139986020030208 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.17882460355758667, loss=0.051780153065919876
I0305 19:24:46.930630 139986028422912 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.06626540422439575, loss=0.04777638986706734
I0305 19:25:08.061879 139986020030208 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.14393600821495056, loss=0.052135590463876724
I0305 19:25:28.733175 139986028422912 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.07714071869850159, loss=0.04274296015501022
I0305 19:25:44.904212 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:26:58.136057 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:27:00.056752 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:27:02.006659 140127519114432 submission_runner.py:469] Time since start: 848.39s, 	Step: 2278, 	{'train/accuracy': 0.9878330230712891, 'train/loss': 0.04347018525004387, 'train/mean_average_precision': 0.1442274770458427, 'validation/accuracy': 0.9850097894668579, 'validation/loss': 0.052855100482702255, 'validation/mean_average_precision': 0.14119393698497126, 'validation/num_examples': 43793, 'test/accuracy': 0.984050989151001, 'test/loss': 0.05584289878606796, 'test/mean_average_precision': 0.14297948004448258, 'test/num_examples': 43793, 'score': 491.04615354537964, 'total_duration': 848.390426158905, 'accumulated_submission_time': 491.04615354537964, 'accumulated_eval_time': 357.2478201389313, 'accumulated_logging_time': 0.03429007530212402}
I0305 19:27:02.015386 139986020030208 logging_writer.py:48] [2278] accumulated_eval_time=357.248, accumulated_logging_time=0.0342901, accumulated_submission_time=491.046, global_step=2278, preemption_count=0, score=491.046, test/accuracy=0.984051, test/loss=0.0558429, test/mean_average_precision=0.142979, test/num_examples=43793, total_duration=848.39, train/accuracy=0.987833, train/loss=0.0434702, train/mean_average_precision=0.144227, validation/accuracy=0.98501, validation/loss=0.0528551, validation/mean_average_precision=0.141194, validation/num_examples=43793
I0305 19:27:06.800545 139986028422912 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04310239478945732, loss=0.043708231300115585
I0305 19:27:27.220716 139986020030208 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.27191171050071716, loss=0.04582697153091431
I0305 19:27:47.804263 139986028422912 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.11327113956212997, loss=0.04720358923077583
I0305 19:28:08.537488 139986020030208 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.13768061995506287, loss=0.043776463717222214
I0305 19:28:29.317114 139986028422912 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.059979021549224854, loss=0.05018201097846031
I0305 19:28:49.943076 139986020030208 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.08173353970050812, loss=0.0409129299223423
I0305 19:29:10.670033 139986028422912 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.11552882939577103, loss=0.04597514495253563
I0305 19:29:31.272635 139986020030208 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.07116283476352692, loss=0.042854007333517075
I0305 19:29:51.929349 139986028422912 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.0510345920920372, loss=0.040419042110443115
I0305 19:30:12.449172 139986020030208 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.03402271494269371, loss=0.041688330471515656
I0305 19:30:32.984332 139986028422912 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.06704747676849365, loss=0.04417836293578148
I0305 19:30:53.306126 139986020030208 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.05297783017158508, loss=0.04349825158715248
I0305 19:31:02.162487 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:32:14.298655 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:32:16.211237 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:32:18.093601 140127519114432 submission_runner.py:469] Time since start: 1164.48s, 	Step: 3444, 	{'train/accuracy': 0.9882678985595703, 'train/loss': 0.04083511233329773, 'train/mean_average_precision': 0.19176204744822845, 'validation/accuracy': 0.9854303598403931, 'validation/loss': 0.05036444589495659, 'validation/mean_average_precision': 0.17322428590639102, 'validation/num_examples': 43793, 'test/accuracy': 0.9845210313796997, 'test/loss': 0.05312596634030342, 'test/mean_average_precision': 0.1704825687669865, 'test/num_examples': 43793, 'score': 731.1531708240509, 'total_duration': 1164.4773681163788, 'accumulated_submission_time': 731.1531708240509, 'accumulated_eval_time': 433.17888736724854, 'accumulated_logging_time': 0.05198025703430176}
I0305 19:32:18.102237 139986028422912 logging_writer.py:48] [3444] accumulated_eval_time=433.179, accumulated_logging_time=0.0519803, accumulated_submission_time=731.153, global_step=3444, preemption_count=0, score=731.153, test/accuracy=0.984521, test/loss=0.053126, test/mean_average_precision=0.170483, test/num_examples=43793, total_duration=1164.48, train/accuracy=0.988268, train/loss=0.0408351, train/mean_average_precision=0.191762, validation/accuracy=0.98543, validation/loss=0.0503644, validation/mean_average_precision=0.173224, validation/num_examples=43793
I0305 19:32:29.923380 139986020030208 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.038641735911369324, loss=0.041903093457221985
I0305 19:32:50.241657 139986028422912 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.032721225172281265, loss=0.040887150913476944
I0305 19:33:10.309365 139986020030208 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.052619364112615585, loss=0.04329197481274605
I0305 19:33:30.271754 139986028422912 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.07219188660383224, loss=0.039456047117710114
I0305 19:33:50.778698 139986020030208 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.062102627009153366, loss=0.048309195786714554
I0305 19:34:11.241151 139986028422912 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.07712748646736145, loss=0.04287048429250717
I0305 19:34:31.848280 139986020030208 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.03520575165748596, loss=0.041877225041389465
I0305 19:34:52.720816 139986028422912 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.05267477408051491, loss=0.043072834610939026
I0305 19:35:13.073295 139986020030208 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05156286433339119, loss=0.047044698148965836
I0305 19:35:33.362460 139986028422912 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.06083308905363083, loss=0.042801517993211746
I0305 19:35:53.653074 139986020030208 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.04103660210967064, loss=0.040642689913511276
I0305 19:36:14.132349 139986028422912 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.04346265643835068, loss=0.043273914605379105
I0305 19:36:18.212983 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:37:30.530480 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:37:32.443780 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:37:34.312191 140127519114432 submission_runner.py:469] Time since start: 1480.70s, 	Step: 4621, 	{'train/accuracy': 0.9884931445121765, 'train/loss': 0.03955801576375961, 'train/mean_average_precision': 0.20979450904212815, 'validation/accuracy': 0.9857173562049866, 'validation/loss': 0.04866034537553787, 'validation/mean_average_precision': 0.18585047100630053, 'validation/num_examples': 43793, 'test/accuracy': 0.9848702549934387, 'test/loss': 0.05123370513319969, 'test/mean_average_precision': 0.18984949184540242, 'test/num_examples': 43793, 'score': 971.2248680591583, 'total_duration': 1480.6959273815155, 'accumulated_submission_time': 971.2248680591583, 'accumulated_eval_time': 509.2780182361603, 'accumulated_logging_time': 0.07073760032653809}
I0305 19:37:34.320916 139986020030208 logging_writer.py:48] [4621] accumulated_eval_time=509.278, accumulated_logging_time=0.0707376, accumulated_submission_time=971.225, global_step=4621, preemption_count=0, score=971.225, test/accuracy=0.98487, test/loss=0.0512337, test/mean_average_precision=0.189849, test/num_examples=43793, total_duration=1480.7, train/accuracy=0.988493, train/loss=0.039558, train/mean_average_precision=0.209795, validation/accuracy=0.985717, validation/loss=0.0486603, validation/mean_average_precision=0.18585, validation/num_examples=43793
I0305 19:37:50.758317 139986028422912 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.08538047224283218, loss=0.04408583045005798
I0305 19:38:11.863558 139986020030208 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.02213343232870102, loss=0.03825582563877106
I0305 19:38:32.699767 139986028422912 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.03658395633101463, loss=0.04172615334391594
I0305 19:38:53.298097 139986020030208 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.07404015213251114, loss=0.04503970220685005
I0305 19:39:13.818969 139986028422912 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.04286978393793106, loss=0.03932175040245056
I0305 19:39:34.246974 139986020030208 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03636255860328674, loss=0.04447990283370018
I0305 19:39:55.006001 139986028422912 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.0317380391061306, loss=0.04660341888666153
I0305 19:40:15.821635 139986020030208 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.027640560641884804, loss=0.03930040821433067
I0305 19:40:36.687555 139986028422912 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.05949179455637932, loss=0.039735957980155945
I0305 19:40:57.477186 139986020030208 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.04351994767785072, loss=0.042828213423490524
I0305 19:41:18.100876 139986028422912 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.027101488783955574, loss=0.043364517390728
I0305 19:41:34.348080 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:42:44.393314 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:42:46.439255 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:42:48.439805 140127519114432 submission_runner.py:469] Time since start: 1794.82s, 	Step: 5780, 	{'train/accuracy': 0.9887377023696899, 'train/loss': 0.038263872265815735, 'train/mean_average_precision': 0.25242484485758127, 'validation/accuracy': 0.9857810735702515, 'validation/loss': 0.04816998168826103, 'validation/mean_average_precision': 0.2077936551110407, 'validation/num_examples': 43793, 'test/accuracy': 0.984925389289856, 'test/loss': 0.050946492701768875, 'test/mean_average_precision': 0.205052731479209, 'test/num_examples': 43793, 'score': 1211.2083296775818, 'total_duration': 1794.8235669136047, 'accumulated_submission_time': 1211.2083296775818, 'accumulated_eval_time': 583.3696949481964, 'accumulated_logging_time': 0.09019660949707031}
I0305 19:42:48.449105 139986020030208 logging_writer.py:48] [5780] accumulated_eval_time=583.37, accumulated_logging_time=0.0901966, accumulated_submission_time=1211.21, global_step=5780, preemption_count=0, score=1211.21, test/accuracy=0.984925, test/loss=0.0509465, test/mean_average_precision=0.205053, test/num_examples=43793, total_duration=1794.82, train/accuracy=0.988738, train/loss=0.0382639, train/mean_average_precision=0.252425, validation/accuracy=0.985781, validation/loss=0.04817, validation/mean_average_precision=0.207794, validation/num_examples=43793
I0305 19:42:52.593115 139986028422912 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04016803950071335, loss=0.04050635173916817
I0305 19:43:12.899557 139986020030208 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.02151586301624775, loss=0.03920164331793785
I0305 19:43:33.723763 139986028422912 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.02680942229926586, loss=0.04157065227627754
I0305 19:43:54.585326 139986020030208 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.023406311869621277, loss=0.04330487176775932
I0305 19:44:15.244453 139986028422912 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.03226041421294212, loss=0.042108409106731415
I0305 19:44:35.274965 139986020030208 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.07316027581691742, loss=0.04152593016624451
I0305 19:44:56.264475 139986028422912 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.041572343558073044, loss=0.040346063673496246
I0305 19:45:17.284599 139986020030208 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.04270704835653305, loss=0.04538899287581444
I0305 19:45:38.444436 139986028422912 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.028310533612966537, loss=0.04043989256024361
I0305 19:45:59.645959 139986020030208 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.020791975781321526, loss=0.03971807658672333
I0305 19:46:20.998173 139986028422912 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.021184589713811874, loss=0.03427880257368088
I0305 19:46:41.870502 139986020030208 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.026844633743166924, loss=0.03940417617559433
I0305 19:46:48.448798 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:47:58.395748 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:48:00.355513 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:48:02.285289 140127519114432 submission_runner.py:469] Time since start: 2108.67s, 	Step: 6933, 	{'train/accuracy': 0.9890442490577698, 'train/loss': 0.037319764494895935, 'train/mean_average_precision': 0.2524225965293105, 'validation/accuracy': 0.9861577749252319, 'validation/loss': 0.04681247100234032, 'validation/mean_average_precision': 0.21435507753131103, 'validation/num_examples': 43793, 'test/accuracy': 0.985277533531189, 'test/loss': 0.04920097440481186, 'test/mean_average_precision': 0.21803274330582847, 'test/num_examples': 43793, 'score': 1451.166642665863, 'total_duration': 2108.6690390110016, 'accumulated_submission_time': 1451.166642665863, 'accumulated_eval_time': 657.2061231136322, 'accumulated_logging_time': 0.10887551307678223}
I0305 19:48:02.295843 139986028422912 logging_writer.py:48] [6933] accumulated_eval_time=657.206, accumulated_logging_time=0.108876, accumulated_submission_time=1451.17, global_step=6933, preemption_count=0, score=1451.17, test/accuracy=0.985278, test/loss=0.049201, test/mean_average_precision=0.218033, test/num_examples=43793, total_duration=2108.67, train/accuracy=0.989044, train/loss=0.0373198, train/mean_average_precision=0.252423, validation/accuracy=0.986158, validation/loss=0.0468125, validation/mean_average_precision=0.214355, validation/num_examples=43793
I0305 19:48:16.406142 139986020030208 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.03623601421713829, loss=0.038294076919555664
I0305 19:48:37.180049 139986028422912 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.05762750282883644, loss=0.04334650933742523
I0305 19:48:57.872248 139986020030208 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.031105443835258484, loss=0.0406753346323967
I0305 19:49:18.738839 139986028422912 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.030713031068444252, loss=0.042693641036748886
I0305 19:49:39.514567 139986020030208 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.027978001162409782, loss=0.04009387642145157
I0305 19:50:00.578188 139986028422912 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.03462092578411102, loss=0.03967753425240517
I0305 19:50:21.199360 139986020030208 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.020560791715979576, loss=0.039330072700977325
I0305 19:50:41.369692 139986028422912 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.05403054133057594, loss=0.04373938590288162
I0305 19:51:01.460621 139986020030208 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.024195369333028793, loss=0.04431892931461334
I0305 19:51:21.889771 139986028422912 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.030720200389623642, loss=0.040438245981931686
I0305 19:51:42.458570 139986020030208 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.021494856104254723, loss=0.03969382122159004
I0305 19:52:02.404096 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:53:13.589229 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:53:15.547386 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:53:17.502516 140127519114432 submission_runner.py:469] Time since start: 2423.89s, 	Step: 8098, 	{'train/accuracy': 0.9895936846733093, 'train/loss': 0.03538041189312935, 'train/mean_average_precision': 0.2910443609191573, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.04580126702785492, 'validation/mean_average_precision': 0.23256951116331034, 'validation/num_examples': 43793, 'test/accuracy': 0.9854413866996765, 'test/loss': 0.048324596136808395, 'test/mean_average_precision': 0.22333050090121923, 'test/num_examples': 43793, 'score': 1691.2323768138885, 'total_duration': 2423.8861384391785, 'accumulated_submission_time': 1691.2323768138885, 'accumulated_eval_time': 732.3043532371521, 'accumulated_logging_time': 0.1295475959777832}
I0305 19:53:17.512022 139986028422912 logging_writer.py:48] [8098] accumulated_eval_time=732.304, accumulated_logging_time=0.129548, accumulated_submission_time=1691.23, global_step=8098, preemption_count=0, score=1691.23, test/accuracy=0.985441, test/loss=0.0483246, test/mean_average_precision=0.223331, test/num_examples=43793, total_duration=2423.89, train/accuracy=0.989594, train/loss=0.0353804, train/mean_average_precision=0.291044, validation/accuracy=0.986323, validation/loss=0.0458013, validation/mean_average_precision=0.23257, validation/num_examples=43793
I0305 19:53:18.173840 139986020030208 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.029409252107143402, loss=0.04084252938628197
I0305 19:53:38.899188 139986028422912 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03072047047317028, loss=0.03650893643498421
I0305 19:53:59.912282 139986020030208 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.019107233732938766, loss=0.03905260190367699
I0305 19:54:20.780986 139986028422912 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.03468416631221771, loss=0.0395047552883625
I0305 19:54:41.044449 139986020030208 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.025958053767681122, loss=0.03997126594185829
I0305 19:55:02.057233 139986028422912 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03542663902044296, loss=0.03750674054026604
I0305 19:55:22.997178 139986020030208 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.031529877334833145, loss=0.03734321519732475
I0305 19:55:43.921563 139986028422912 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.033159878104925156, loss=0.04018345847725868
I0305 19:56:05.043960 139986020030208 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.02862224541604519, loss=0.038532234728336334
I0305 19:56:26.107495 139986028422912 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03771292418241501, loss=0.039866428822278976
I0305 19:56:46.587772 139986020030208 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.031279098242521286, loss=0.03914372995495796
I0305 19:57:07.205581 139986028422912 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.026511475443840027, loss=0.03478659689426422
I0305 19:57:17.697918 140127519114432 spec.py:321] Evaluating on the training split.
I0305 19:58:31.576894 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 19:58:33.544751 140127519114432 spec.py:349] Evaluating on the test split.
I0305 19:58:35.444340 140127519114432 submission_runner.py:469] Time since start: 2741.83s, 	Step: 9252, 	{'train/accuracy': 0.9897444248199463, 'train/loss': 0.034677185118198395, 'train/mean_average_precision': 0.30095830325318246, 'validation/accuracy': 0.9864492416381836, 'validation/loss': 0.04539342597126961, 'validation/mean_average_precision': 0.23260019348985556, 'validation/num_examples': 43793, 'test/accuracy': 0.9855824708938599, 'test/loss': 0.04789038747549057, 'test/mean_average_precision': 0.22779992872190927, 'test/num_examples': 43793, 'score': 1931.3781192302704, 'total_duration': 2741.8279597759247, 'accumulated_submission_time': 1931.3781192302704, 'accumulated_eval_time': 810.0505945682526, 'accumulated_logging_time': 0.1483473777770996}
I0305 19:58:35.453675 139986020030208 logging_writer.py:48] [9252] accumulated_eval_time=810.051, accumulated_logging_time=0.148347, accumulated_submission_time=1931.38, global_step=9252, preemption_count=0, score=1931.38, test/accuracy=0.985582, test/loss=0.0478904, test/mean_average_precision=0.2278, test/num_examples=43793, total_duration=2741.83, train/accuracy=0.989744, train/loss=0.0346772, train/mean_average_precision=0.300958, validation/accuracy=0.986449, validation/loss=0.0453934, validation/mean_average_precision=0.2326, validation/num_examples=43793
I0305 19:58:45.824516 139986028422912 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.028662240132689476, loss=0.04092097282409668
I0305 19:59:06.423062 139986020030208 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03074519895017147, loss=0.03750966489315033
I0305 19:59:26.450846 139986028422912 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.022160189226269722, loss=0.03395318612456322
I0305 19:59:47.076735 139986020030208 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.019741453230381012, loss=0.037342824041843414
I0305 20:00:07.783734 139986028422912 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.030261240899562836, loss=0.04006509482860565
I0305 20:00:28.290124 139986020030208 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.028830472379922867, loss=0.042696163058280945
I0305 20:00:48.902504 139986028422912 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.04429440200328827, loss=0.035359740257263184
I0305 20:01:09.583651 139986020030208 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.05100322514772415, loss=0.03692666441202164
I0305 20:01:30.289817 139986028422912 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.02928425371646881, loss=0.035190265625715256
I0305 20:01:50.967477 139986020030208 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.037755269557237625, loss=0.04038214311003685
I0305 20:02:11.304211 139986028422912 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.03573222458362579, loss=0.03852110356092453
I0305 20:02:31.357574 139986020030208 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.02511896938085556, loss=0.0331374816596508
I0305 20:02:35.570138 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:03:46.640887 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:03:48.591880 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:03:50.527093 140127519114432 submission_runner.py:469] Time since start: 3056.91s, 	Step: 10422, 	{'train/accuracy': 0.9899512529373169, 'train/loss': 0.03357997164130211, 'train/mean_average_precision': 0.33839680296499514, 'validation/accuracy': 0.9864715933799744, 'validation/loss': 0.045309219509363174, 'validation/mean_average_precision': 0.23814975856455584, 'validation/num_examples': 43793, 'test/accuracy': 0.9857105016708374, 'test/loss': 0.04773522913455963, 'test/mean_average_precision': 0.2397938027003563, 'test/num_examples': 43793, 'score': 2171.4511201381683, 'total_duration': 3056.9107139110565, 'accumulated_submission_time': 2171.4511201381683, 'accumulated_eval_time': 885.0073554515839, 'accumulated_logging_time': 0.16663169860839844}
I0305 20:03:50.537944 139986028422912 logging_writer.py:48] [10422] accumulated_eval_time=885.007, accumulated_logging_time=0.166632, accumulated_submission_time=2171.45, global_step=10422, preemption_count=0, score=2171.45, test/accuracy=0.985711, test/loss=0.0477352, test/mean_average_precision=0.239794, test/num_examples=43793, total_duration=3056.91, train/accuracy=0.989951, train/loss=0.03358, train/mean_average_precision=0.338397, validation/accuracy=0.986472, validation/loss=0.0453092, validation/mean_average_precision=0.23815, validation/num_examples=43793
I0305 20:04:06.807683 139986020030208 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.03861057385802269, loss=0.04002189263701439
I0305 20:04:27.418123 139986028422912 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.02914944663643837, loss=0.037355005741119385
I0305 20:04:48.120170 139986020030208 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.03231668099761009, loss=0.03794395551085472
I0305 20:05:08.646672 139986028422912 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.030440598726272583, loss=0.04254616051912308
I0305 20:05:29.355095 139986020030208 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.024656249210238457, loss=0.035938188433647156
I0305 20:05:50.125437 139986028422912 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.036900900304317474, loss=0.03571997210383415
I0305 20:06:10.655548 139986020030208 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.042794954031705856, loss=0.034943513572216034
I0305 20:06:31.482707 139986028422912 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.030451860278844833, loss=0.035295531153678894
I0305 20:06:51.967526 139986020030208 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.03437790647149086, loss=0.03543606400489807
I0305 20:07:11.717816 139986028422912 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.039276331663131714, loss=0.03594902530312538
I0305 20:07:31.498284 139986020030208 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03009854257106781, loss=0.035817377269268036
I0305 20:07:50.574567 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:09:01.501863 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:09:03.459815 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:09:05.357169 140127519114432 submission_runner.py:469] Time since start: 3371.74s, 	Step: 11597, 	{'train/accuracy': 0.9900012612342834, 'train/loss': 0.03328365087509155, 'train/mean_average_precision': 0.35223851977347453, 'validation/accuracy': 0.9865710139274597, 'validation/loss': 0.045056186616420746, 'validation/mean_average_precision': 0.24086969721731705, 'validation/num_examples': 43793, 'test/accuracy': 0.9857231378555298, 'test/loss': 0.04776609688997269, 'test/mean_average_precision': 0.242269357819331, 'test/num_examples': 43793, 'score': 2411.444804430008, 'total_duration': 3371.7409329414368, 'accumulated_submission_time': 2411.444804430008, 'accumulated_eval_time': 959.7899127006531, 'accumulated_logging_time': 0.18727874755859375}
I0305 20:09:05.367060 139984967476992 logging_writer.py:48] [11597] accumulated_eval_time=959.79, accumulated_logging_time=0.187279, accumulated_submission_time=2411.44, global_step=11597, preemption_count=0, score=2411.44, test/accuracy=0.985723, test/loss=0.0477661, test/mean_average_precision=0.242269, test/num_examples=43793, total_duration=3371.74, train/accuracy=0.990001, train/loss=0.0332837, train/mean_average_precision=0.352239, validation/accuracy=0.986571, validation/loss=0.0450562, validation/mean_average_precision=0.24087, validation/num_examples=43793
I0305 20:09:06.204027 139984959084288 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.04177619144320488, loss=0.03949739411473274
I0305 20:09:26.937675 139984967476992 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.024408265948295593, loss=0.03522928059101105
I0305 20:09:47.874178 139984959084288 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.05123087391257286, loss=0.03846254199743271
I0305 20:10:08.396399 139984967476992 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.048892028629779816, loss=0.033524829894304276
I0305 20:10:29.124772 139984959084288 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.03330390155315399, loss=0.032952647656202316
I0305 20:10:50.064049 139984967476992 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03513060882687569, loss=0.038693711161613464
I0305 20:11:10.949121 139984959084288 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.040025558322668076, loss=0.03785274177789688
I0305 20:11:31.219772 139984967476992 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.052049532532691956, loss=0.036324504762887955
I0305 20:11:51.323543 139984959084288 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.051529522985219955, loss=0.034915704280138016
I0305 20:12:11.828898 139984967476992 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.04950648173689842, loss=0.038130152970552444
I0305 20:12:32.915053 139984959084288 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.04411735385656357, loss=0.03477035462856293
I0305 20:12:53.722113 139984967476992 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.08010612428188324, loss=0.036548953503370285
I0305 20:13:05.538356 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:14:17.078563 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:14:18.982753 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:14:20.882086 140127519114432 submission_runner.py:469] Time since start: 3687.27s, 	Step: 12758, 	{'train/accuracy': 0.9904212355613708, 'train/loss': 0.031753815710544586, 'train/mean_average_precision': 0.36642807074573713, 'validation/accuracy': 0.9866436719894409, 'validation/loss': 0.04474074766039848, 'validation/mean_average_precision': 0.2493593571368245, 'validation/num_examples': 43793, 'test/accuracy': 0.9857656955718994, 'test/loss': 0.047649361193180084, 'test/mean_average_precision': 0.23719579188298698, 'test/num_examples': 43793, 'score': 2651.5743041038513, 'total_duration': 3687.265855550766, 'accumulated_submission_time': 2651.5743041038513, 'accumulated_eval_time': 1035.1336002349854, 'accumulated_logging_time': 0.20632481575012207}
I0305 20:14:20.891977 139984959084288 logging_writer.py:48] [12758] accumulated_eval_time=1035.13, accumulated_logging_time=0.206325, accumulated_submission_time=2651.57, global_step=12758, preemption_count=0, score=2651.57, test/accuracy=0.985766, test/loss=0.0476494, test/mean_average_precision=0.237196, test/num_examples=43793, total_duration=3687.27, train/accuracy=0.990421, train/loss=0.0317538, train/mean_average_precision=0.366428, validation/accuracy=0.986644, validation/loss=0.0447407, validation/mean_average_precision=0.249359, validation/num_examples=43793
I0305 20:14:29.835084 139984967476992 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.040993887931108475, loss=0.037085577845573425
I0305 20:14:50.550036 139984959084288 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.03990209102630615, loss=0.035047490149736404
I0305 20:15:11.551328 139984967476992 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.06013636663556099, loss=0.03342929482460022
I0305 20:15:32.565885 139984959084288 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.038111209869384766, loss=0.034450024366378784
I0305 20:15:53.327770 139984967476992 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.041559621691703796, loss=0.03373689949512482
I0305 20:16:13.817973 139984959084288 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.045344799757003784, loss=0.034838516265153885
I0305 20:16:34.501645 139984967476992 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.04742361232638359, loss=0.03370319679379463
I0305 20:16:55.610745 139984959084288 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.043787598609924316, loss=0.03447176143527031
I0305 20:17:15.886159 139984967476992 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.03914198279380798, loss=0.03784801810979843
I0305 20:17:36.568696 139984959084288 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.041841693222522736, loss=0.0346553772687912
I0305 20:17:57.445903 139984967476992 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.06402350962162018, loss=0.03530054911971092
I0305 20:18:18.374577 139984959084288 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.07322847843170166, loss=0.037800755351781845
I0305 20:18:21.053253 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:19:31.363909 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:19:33.333974 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:19:35.213999 140127519114432 submission_runner.py:469] Time since start: 4001.60s, 	Step: 13914, 	{'train/accuracy': 0.9902939796447754, 'train/loss': 0.03219129517674446, 'train/mean_average_precision': 0.36750967032591414, 'validation/accuracy': 0.9867833256721497, 'validation/loss': 0.044609151780605316, 'validation/mean_average_precision': 0.25819032223946725, 'validation/num_examples': 43793, 'test/accuracy': 0.9859147667884827, 'test/loss': 0.04720975086092949, 'test/mean_average_precision': 0.25917715213760234, 'test/num_examples': 43793, 'score': 2891.692756175995, 'total_duration': 4001.597636461258, 'accumulated_submission_time': 2891.692756175995, 'accumulated_eval_time': 1109.2941818237305, 'accumulated_logging_time': 0.2255704402923584}
I0305 20:19:35.223856 139984967476992 logging_writer.py:48] [13914] accumulated_eval_time=1109.29, accumulated_logging_time=0.22557, accumulated_submission_time=2891.69, global_step=13914, preemption_count=0, score=2891.69, test/accuracy=0.985915, test/loss=0.0472098, test/mean_average_precision=0.259177, test/num_examples=43793, total_duration=4001.6, train/accuracy=0.990294, train/loss=0.0321913, train/mean_average_precision=0.36751, validation/accuracy=0.986783, validation/loss=0.0446092, validation/mean_average_precision=0.25819, validation/num_examples=43793
I0305 20:19:53.206078 139984959084288 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.051618997007608414, loss=0.03418353572487831
I0305 20:20:13.960303 139984967476992 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.05938130244612694, loss=0.03717351332306862
I0305 20:20:34.672916 139984959084288 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.06022999808192253, loss=0.03581191599369049
I0305 20:20:55.524015 139984967476992 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.05717925727367401, loss=0.03375890478491783
I0305 20:21:16.388614 139984959084288 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.04509321600198746, loss=0.034594420343637466
I0305 20:21:37.464880 139984967476992 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0538456104695797, loss=0.03621269389986992
I0305 20:21:58.813095 139984959084288 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.05128585919737816, loss=0.03610342741012573
I0305 20:22:19.608412 139984967476992 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.05504904314875603, loss=0.03905867040157318
I0305 20:22:40.142419 139984959084288 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.05709141120314598, loss=0.037609685212373734
I0305 20:23:00.264499 139984967476992 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.1440783590078354, loss=0.036566950380802155
I0305 20:23:20.420609 139984959084288 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.058251578360795975, loss=0.03390898182988167
I0305 20:23:35.304035 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:24:46.231363 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:24:48.200107 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:24:50.119188 140127519114432 submission_runner.py:469] Time since start: 4316.50s, 	Step: 15073, 	{'train/accuracy': 0.9906931519508362, 'train/loss': 0.030546216294169426, 'train/mean_average_precision': 0.4021203702152252, 'validation/accuracy': 0.9867151379585266, 'validation/loss': 0.04461422190070152, 'validation/mean_average_precision': 0.25678019209070935, 'validation/num_examples': 43793, 'test/accuracy': 0.9858583807945251, 'test/loss': 0.047217756509780884, 'test/mean_average_precision': 0.24882538075651361, 'test/num_examples': 43793, 'score': 3131.734242916107, 'total_duration': 4316.502843618393, 'accumulated_submission_time': 3131.734242916107, 'accumulated_eval_time': 1184.109183549881, 'accumulated_logging_time': 0.24453067779541016}
I0305 20:24:50.130430 139984967476992 logging_writer.py:48] [15073] accumulated_eval_time=1184.11, accumulated_logging_time=0.244531, accumulated_submission_time=3131.73, global_step=15073, preemption_count=0, score=3131.73, test/accuracy=0.985858, test/loss=0.0472178, test/mean_average_precision=0.248825, test/num_examples=43793, total_duration=4316.5, train/accuracy=0.990693, train/loss=0.0305462, train/mean_average_precision=0.40212, validation/accuracy=0.986715, validation/loss=0.0446142, validation/mean_average_precision=0.25678, validation/num_examples=43793
I0305 20:24:56.142441 139984959084288 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.047489188611507416, loss=0.0321447029709816
I0305 20:25:17.209012 139984967476992 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.04363967850804329, loss=0.03125700354576111
I0305 20:25:38.231848 139984959084288 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.04928925260901451, loss=0.03413541987538338
I0305 20:25:59.747297 139984967476992 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.082605741918087, loss=0.034953948110342026
I0305 20:26:21.018203 139984959084288 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.045649461448192596, loss=0.0341874398291111
I0305 20:26:42.290953 139984967476992 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.046904295682907104, loss=0.03261725232005119
I0305 20:27:03.729959 139984959084288 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.07341937720775604, loss=0.03314513713121414
I0305 20:27:25.131920 139984967476992 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.055902592837810516, loss=0.03843237832188606
I0305 20:27:46.098300 139984959084288 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.05589435622096062, loss=0.03571956232190132
I0305 20:28:06.830626 139984967476992 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.07754074037075043, loss=0.03747229650616646
I0305 20:28:27.716439 139984959084288 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.06511469930410385, loss=0.03409883752465248
I0305 20:28:48.706398 139984967476992 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0877927839756012, loss=0.03289341181516647
I0305 20:28:50.156387 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:29:59.532933 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:30:01.460515 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:30:03.323359 140127519114432 submission_runner.py:469] Time since start: 4629.71s, 	Step: 16208, 	{'train/accuracy': 0.9906432032585144, 'train/loss': 0.031065478920936584, 'train/mean_average_precision': 0.39303610163280056, 'validation/accuracy': 0.9867350459098816, 'validation/loss': 0.04435378313064575, 'validation/mean_average_precision': 0.2675796817471025, 'validation/num_examples': 43793, 'test/accuracy': 0.9859114289283752, 'test/loss': 0.047142524272203445, 'test/mean_average_precision': 0.2543949258199262, 'test/num_examples': 43793, 'score': 3371.720043897629, 'total_duration': 4629.707128286362, 'accumulated_submission_time': 3371.720043897629, 'accumulated_eval_time': 1257.2761113643646, 'accumulated_logging_time': 0.26519107818603516}
I0305 20:30:03.333780 139984959084288 logging_writer.py:48] [16208] accumulated_eval_time=1257.28, accumulated_logging_time=0.265191, accumulated_submission_time=3371.72, global_step=16208, preemption_count=0, score=3371.72, test/accuracy=0.985911, test/loss=0.0471425, test/mean_average_precision=0.254395, test/num_examples=43793, total_duration=4629.71, train/accuracy=0.990643, train/loss=0.0310655, train/mean_average_precision=0.393036, validation/accuracy=0.986735, validation/loss=0.0443538, validation/mean_average_precision=0.26758, validation/num_examples=43793
I0305 20:30:22.595818 139984967476992 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.0599592961370945, loss=0.0337858572602272
I0305 20:30:43.523584 139984959084288 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.05531013011932373, loss=0.031798891723155975
I0305 20:31:04.498594 139984967476992 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.05598040670156479, loss=0.03306712955236435
I0305 20:31:25.586269 139984959084288 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.04972497373819351, loss=0.03527725115418434
I0305 20:31:46.460733 139984967476992 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.1020665243268013, loss=0.03902380168437958
I0305 20:32:07.443392 139984959084288 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.05016752704977989, loss=0.03595222532749176
I0305 20:32:28.685269 139984967476992 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.05373485013842583, loss=0.033881060779094696
I0305 20:32:49.992486 139984959084288 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.06707173585891724, loss=0.029530053958296776
I0305 20:33:11.103665 139984967476992 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.05926283821463585, loss=0.03354499116539955
I0305 20:33:32.012693 139984959084288 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.05449385941028595, loss=0.033973634243011475
I0305 20:33:53.083331 139984967476992 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.06740698963403702, loss=0.034800779074430466
I0305 20:34:03.510562 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:35:12.392855 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:35:14.354869 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:35:16.259814 140127519114432 submission_runner.py:469] Time since start: 4942.64s, 	Step: 17351, 	{'train/accuracy': 0.9909688830375671, 'train/loss': 0.029408328235149384, 'train/mean_average_precision': 0.43705568752259816, 'validation/accuracy': 0.9868064522743225, 'validation/loss': 0.0445091649889946, 'validation/mean_average_precision': 0.2664873621912444, 'validation/num_examples': 43793, 'test/accuracy': 0.9859514236450195, 'test/loss': 0.04732939973473549, 'test/mean_average_precision': 0.2582068606871591, 'test/num_examples': 43793, 'score': 3611.853848695755, 'total_duration': 4942.64346575737, 'accumulated_submission_time': 3611.853848695755, 'accumulated_eval_time': 1330.0252051353455, 'accumulated_logging_time': 0.28485870361328125}
I0305 20:35:16.271128 139984959084288 logging_writer.py:48] [17351] accumulated_eval_time=1330.03, accumulated_logging_time=0.284859, accumulated_submission_time=3611.85, global_step=17351, preemption_count=0, score=3611.85, test/accuracy=0.985951, test/loss=0.0473294, test/mean_average_precision=0.258207, test/num_examples=43793, total_duration=4942.64, train/accuracy=0.990969, train/loss=0.0294083, train/mean_average_precision=0.437056, validation/accuracy=0.986806, validation/loss=0.0445092, validation/mean_average_precision=0.266487, validation/num_examples=43793
I0305 20:35:27.083045 139984967476992 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.060618337243795395, loss=0.03251657634973526
I0305 20:35:48.185832 139984959084288 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.06354464590549469, loss=0.03356369212269783
I0305 20:36:09.172342 139984967476992 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.08655887097120285, loss=0.034681133925914764
I0305 20:36:30.027442 139984959084288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.055446114391088486, loss=0.034909747540950775
I0305 20:36:50.956202 139984967476992 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.06396965682506561, loss=0.033438414335250854
I0305 20:37:11.831147 139984959084288 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.07506430149078369, loss=0.03324831277132034
I0305 20:37:32.774969 139984967476992 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.07090641558170319, loss=0.032735057175159454
I0305 20:37:53.073276 139984959084288 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.11404499411582947, loss=0.03384485840797424
I0305 20:38:13.833749 139984967476992 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.10332520306110382, loss=0.03309465944766998
I0305 20:38:34.534712 139984959084288 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.06623248755931854, loss=0.0355452299118042
I0305 20:38:55.307941 139984967476992 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.05823342502117157, loss=0.035108987241983414
I0305 20:39:16.250960 139984959084288 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0602974034845829, loss=0.03392703831195831
I0305 20:39:16.456083 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:40:28.797661 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:40:30.895668 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:40:32.889281 140127519114432 submission_runner.py:469] Time since start: 5259.27s, 	Step: 18502, 	{'train/accuracy': 0.990720272064209, 'train/loss': 0.030478470027446747, 'train/mean_average_precision': 0.4041112308421484, 'validation/accuracy': 0.9868020415306091, 'validation/loss': 0.044579531997442245, 'validation/mean_average_precision': 0.2559727922854657, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.04730149358510971, 'test/mean_average_precision': 0.25856812127632606, 'test/num_examples': 43793, 'score': 3851.997809410095, 'total_duration': 5259.272873401642, 'accumulated_submission_time': 3851.997809410095, 'accumulated_eval_time': 1406.4581809043884, 'accumulated_logging_time': 0.30603814125061035}
I0305 20:40:32.901612 139984967476992 logging_writer.py:48] [18502] accumulated_eval_time=1406.46, accumulated_logging_time=0.306038, accumulated_submission_time=3852, global_step=18502, preemption_count=0, score=3852, test/accuracy=0.985882, test/loss=0.0473015, test/mean_average_precision=0.258568, test/num_examples=43793, total_duration=5259.27, train/accuracy=0.99072, train/loss=0.0304785, train/mean_average_precision=0.404111, validation/accuracy=0.986802, validation/loss=0.0445795, validation/mean_average_precision=0.255973, validation/num_examples=43793
I0305 20:40:53.048854 139984959084288 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.10020619630813599, loss=0.032696809619665146
I0305 20:41:13.334589 139984967476992 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.06264534592628479, loss=0.03360406309366226
I0305 20:41:33.527784 139984959084288 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.07120591402053833, loss=0.03518598899245262
I0305 20:41:54.083265 139984967476992 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.07081805169582367, loss=0.03678255155682564
I0305 20:42:15.124500 139984959084288 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.07654641568660736, loss=0.03326226770877838
I0305 20:42:35.995141 139984967476992 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.07483962178230286, loss=0.03057851269841194
I0305 20:42:56.981459 139984959084288 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.06858465075492859, loss=0.03509167209267616
I0305 20:43:18.070292 139984967476992 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.07252354919910431, loss=0.03408629819750786
I0305 20:43:38.967988 139984959084288 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07184719294309616, loss=0.032646987587213516
I0305 20:43:59.870665 139984967476992 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05457758903503418, loss=0.0304912980645895
I0305 20:44:20.880433 139984959084288 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.06825201958417892, loss=0.035634882748126984
I0305 20:44:32.918564 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:45:45.488832 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:45:47.438539 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:45:49.321346 140127519114432 submission_runner.py:469] Time since start: 5575.70s, 	Step: 19659, 	{'train/accuracy': 0.9911894798278809, 'train/loss': 0.028543828055262566, 'train/mean_average_precision': 0.4433005545019544, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.04503616318106651, 'validation/mean_average_precision': 0.2606181092650766, 'validation/num_examples': 43793, 'test/accuracy': 0.9858912229537964, 'test/loss': 0.04783489555120468, 'test/mean_average_precision': 0.25853764723846423, 'test/num_examples': 43793, 'score': 4091.9764292240143, 'total_duration': 5575.704982995987, 'accumulated_submission_time': 4091.9764292240143, 'accumulated_eval_time': 1482.8607847690582, 'accumulated_logging_time': 0.32793474197387695}
I0305 20:45:49.331956 139984967476992 logging_writer.py:48] [19659] accumulated_eval_time=1482.86, accumulated_logging_time=0.327935, accumulated_submission_time=4091.98, global_step=19659, preemption_count=0, score=4091.98, test/accuracy=0.985891, test/loss=0.0478349, test/mean_average_precision=0.258538, test/num_examples=43793, total_duration=5575.7, train/accuracy=0.991189, train/loss=0.0285438, train/mean_average_precision=0.443301, validation/accuracy=0.986664, validation/loss=0.0450362, validation/mean_average_precision=0.260618, validation/num_examples=43793
I0305 20:45:58.149272 139984959084288 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.06214243546128273, loss=0.03707285225391388
I0305 20:46:19.048196 139984967476992 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.06900248676538467, loss=0.03285304084420204
I0305 20:46:39.947580 139984959084288 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.08065832406282425, loss=0.035764992237091064
I0305 20:47:00.731280 139984967476992 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.061188943684101105, loss=0.032887741923332214
I0305 20:47:21.707197 139984959084288 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.06896330416202545, loss=0.029632633551955223
I0305 20:47:42.464059 139984967476992 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.10471130907535553, loss=0.031051868572831154
I0305 20:48:03.289443 139984959084288 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.09017942100763321, loss=0.035225819796323776
I0305 20:48:23.585506 139984967476992 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.08115766942501068, loss=0.03286617621779442
I0305 20:48:43.934173 139984959084288 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.08497646450996399, loss=0.03249640390276909
I0305 20:49:04.868745 139984967476992 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.1338881105184555, loss=0.03416870906949043
I0305 20:49:25.668817 139984959084288 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.08567844331264496, loss=0.03448490798473358
I0305 20:49:46.419604 139984967476992 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.07595668733119965, loss=0.03160836920142174
I0305 20:49:49.364416 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:50:59.093175 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:51:01.044836 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:51:02.907246 140127519114432 submission_runner.py:469] Time since start: 5889.29s, 	Step: 20815, 	{'train/accuracy': 0.991061806678772, 'train/loss': 0.029439395293593407, 'train/mean_average_precision': 0.42460744742915035, 'validation/accuracy': 0.9866542816162109, 'validation/loss': 0.04446346312761307, 'validation/mean_average_precision': 0.2615110946879504, 'validation/num_examples': 43793, 'test/accuracy': 0.9858777523040771, 'test/loss': 0.046956129372119904, 'test/mean_average_precision': 0.25924808490262014, 'test/num_examples': 43793, 'score': 4331.9673771858215, 'total_duration': 5889.291003704071, 'accumulated_submission_time': 4331.9673771858215, 'accumulated_eval_time': 1556.4035575389862, 'accumulated_logging_time': 0.3504455089569092}
I0305 20:51:02.918518 139984959084288 logging_writer.py:48] [20815] accumulated_eval_time=1556.4, accumulated_logging_time=0.350446, accumulated_submission_time=4331.97, global_step=20815, preemption_count=0, score=4331.97, test/accuracy=0.985878, test/loss=0.0469561, test/mean_average_precision=0.259248, test/num_examples=43793, total_duration=5889.29, train/accuracy=0.991062, train/loss=0.0294394, train/mean_average_precision=0.424607, validation/accuracy=0.986654, validation/loss=0.0444635, validation/mean_average_precision=0.261511, validation/num_examples=43793
I0305 20:51:20.671380 139984967476992 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.06720759719610214, loss=0.034293096512556076
I0305 20:51:41.768632 139984959084288 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.09351716935634613, loss=0.03359612077474594
I0305 20:52:02.694255 139984967476992 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.08753266930580139, loss=0.03350335732102394
I0305 20:52:23.178516 139984959084288 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.08747339993715286, loss=0.03302973881363869
I0305 20:52:43.303245 139984967476992 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.06251145899295807, loss=0.030996762216091156
I0305 20:53:03.883323 139984959084288 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.06896688044071198, loss=0.032473124563694
I0305 20:53:24.691526 139984967476992 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.07652685046195984, loss=0.03129220008850098
I0305 20:53:45.359467 139984959084288 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.09994751214981079, loss=0.03494628146290779
I0305 20:54:06.349848 139984967476992 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.0739433541893959, loss=0.03337095305323601
I0305 20:54:27.265840 139984959084288 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.07462430000305176, loss=0.0332067497074604
I0305 20:54:48.090041 139984967476992 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.07981470227241516, loss=0.035718441009521484
I0305 20:55:03.092247 140127519114432 spec.py:321] Evaluating on the training split.
I0305 20:56:12.989599 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 20:56:14.880851 140127519114432 spec.py:349] Evaluating on the test split.
I0305 20:56:16.776371 140127519114432 submission_runner.py:469] Time since start: 6203.16s, 	Step: 21973, 	{'train/accuracy': 0.9916960597038269, 'train/loss': 0.027112770825624466, 'train/mean_average_precision': 0.4854680093744215, 'validation/accuracy': 0.9869307279586792, 'validation/loss': 0.04433503374457359, 'validation/mean_average_precision': 0.26973960357351057, 'validation/num_examples': 43793, 'test/accuracy': 0.9859400391578674, 'test/loss': 0.04721763730049133, 'test/mean_average_precision': 0.25739433278435203, 'test/num_examples': 43793, 'score': 4572.100345849991, 'total_duration': 6203.16002869606, 'accumulated_submission_time': 4572.100345849991, 'accumulated_eval_time': 1630.0875265598297, 'accumulated_logging_time': 0.3714566230773926}
I0305 20:56:16.787155 139984959084288 logging_writer.py:48] [21973] accumulated_eval_time=1630.09, accumulated_logging_time=0.371457, accumulated_submission_time=4572.1, global_step=21973, preemption_count=0, score=4572.1, test/accuracy=0.98594, test/loss=0.0472176, test/mean_average_precision=0.257394, test/num_examples=43793, total_duration=6203.16, train/accuracy=0.991696, train/loss=0.0271128, train/mean_average_precision=0.485468, validation/accuracy=0.986931, validation/loss=0.044335, validation/mean_average_precision=0.26974, validation/num_examples=43793
I0305 20:56:22.665622 139984967476992 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.10367690026760101, loss=0.030910653993487358
I0305 20:56:43.244182 139984959084288 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.08399800956249237, loss=0.0343388132750988
I0305 20:57:04.385080 139984967476992 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.08719279617071152, loss=0.03440802916884422
I0305 20:57:25.551501 139984959084288 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.07347308844327927, loss=0.037800781428813934
I0305 20:57:46.003054 139984967476992 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.08616126328706741, loss=0.035633329302072525
I0305 20:58:06.568310 139984959084288 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.07085422426462173, loss=0.03337681293487549
I0305 20:58:27.932012 139984967476992 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.09895159304141998, loss=0.033694203943014145
I0305 20:58:48.913426 139984959084288 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.07700344920158386, loss=0.03208331763744354
I0305 20:59:09.962580 139984967476992 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.07830740511417389, loss=0.030026648193597794
I0305 20:59:30.896250 139984959084288 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.07546961307525635, loss=0.031176680698990822
I0305 20:59:51.296001 139984967476992 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.07941626757383347, loss=0.03350546211004257
I0305 21:00:11.385844 139984959084288 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.1092655137181282, loss=0.03330780193209648
I0305 21:00:16.932973 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:01:30.041731 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:01:32.065408 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:01:33.935212 140127519114432 submission_runner.py:469] Time since start: 6520.32s, 	Step: 23128, 	{'train/accuracy': 0.9912074208259583, 'train/loss': 0.02878197282552719, 'train/mean_average_precision': 0.4538782150338502, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.04452703893184662, 'validation/mean_average_precision': 0.26088366105260874, 'validation/num_examples': 43793, 'test/accuracy': 0.9858996272087097, 'test/loss': 0.04724275320768356, 'test/mean_average_precision': 0.2626329188820472, 'test/num_examples': 43793, 'score': 4812.2043607234955, 'total_duration': 6520.318869590759, 'accumulated_submission_time': 4812.2043607234955, 'accumulated_eval_time': 1707.0896158218384, 'accumulated_logging_time': 0.3926372528076172}
I0305 21:01:33.947041 139984967476992 logging_writer.py:48] [23128] accumulated_eval_time=1707.09, accumulated_logging_time=0.392637, accumulated_submission_time=4812.2, global_step=23128, preemption_count=0, score=4812.2, test/accuracy=0.9859, test/loss=0.0472428, test/mean_average_precision=0.262633, test/num_examples=43793, total_duration=6520.32, train/accuracy=0.991207, train/loss=0.028782, train/mean_average_precision=0.453878, validation/accuracy=0.986778, validation/loss=0.044527, validation/mean_average_precision=0.260884, validation/num_examples=43793
I0305 21:01:49.028511 139984959084288 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.08738040924072266, loss=0.03091072291135788
I0305 21:02:09.949863 139984967476992 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.0849466547369957, loss=0.03302370756864548
I0305 21:02:31.203143 139984959084288 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.0792461559176445, loss=0.03401848301291466
I0305 21:02:52.529577 139984967476992 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0798041820526123, loss=0.031118251383304596
I0305 21:03:13.529684 139984959084288 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.0847458764910698, loss=0.031124768778681755
I0305 21:03:34.896185 139984967476992 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.08955547213554382, loss=0.033037394285202026
I0305 21:03:56.124715 139984959084288 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.11053363233804703, loss=0.033945854753255844
I0305 21:04:16.885725 139984967476992 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.07831951975822449, loss=0.032044392079114914
I0305 21:04:38.084156 139984959084288 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.11385811865329742, loss=0.03578711301088333
I0305 21:04:58.949905 139984967476992 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.09841691702604294, loss=0.0324707068502903
I0305 21:05:19.808586 139984959084288 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.0809805616736412, loss=0.03137952461838722
I0305 21:05:34.010731 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:06:41.456955 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:06:43.402975 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:06:45.276295 140127519114432 submission_runner.py:469] Time since start: 6831.66s, 	Step: 24269, 	{'train/accuracy': 0.9919978380203247, 'train/loss': 0.02638961747288704, 'train/mean_average_precision': 0.5011742525555465, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04433467984199524, 'validation/mean_average_precision': 0.27027066765630403, 'validation/num_examples': 43793, 'test/accuracy': 0.9860007166862488, 'test/loss': 0.04699287191033363, 'test/mean_average_precision': 0.26516535518521295, 'test/num_examples': 43793, 'score': 5052.228254318237, 'total_duration': 6831.660053730011, 'accumulated_submission_time': 5052.228254318237, 'accumulated_eval_time': 1778.355131149292, 'accumulated_logging_time': 0.4137258529663086}
I0305 21:06:45.287455 139984967476992 logging_writer.py:48] [24269] accumulated_eval_time=1778.36, accumulated_logging_time=0.413726, accumulated_submission_time=5052.23, global_step=24269, preemption_count=0, score=5052.23, test/accuracy=0.986001, test/loss=0.0469929, test/mean_average_precision=0.265165, test/num_examples=43793, total_duration=6831.66, train/accuracy=0.991998, train/loss=0.0263896, train/mean_average_precision=0.501174, validation/accuracy=0.986755, validation/loss=0.0443347, validation/mean_average_precision=0.270271, validation/num_examples=43793
I0305 21:06:51.947454 139984959084288 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.12435746937990189, loss=0.030570652335882187
I0305 21:07:12.396085 139984967476992 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.09147514402866364, loss=0.02988358587026596
I0305 21:07:33.098564 139984959084288 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.09207923710346222, loss=0.03088628128170967
I0305 21:07:54.154273 139984967476992 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.08018070459365845, loss=0.03341035172343254
I0305 21:08:14.882244 139984959084288 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.08166275173425674, loss=0.028895976021885872
I0305 21:08:36.018537 139984967476992 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.07017078250646591, loss=0.031364474445581436
I0305 21:08:57.197901 139984959084288 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.13176676630973816, loss=0.03044162690639496
I0305 21:09:18.286171 139984967476992 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.08521062135696411, loss=0.03191952407360077
I0305 21:09:39.197352 139984959084288 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.08899839222431183, loss=0.031077709048986435
I0305 21:10:00.165931 139984967476992 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.07664546370506287, loss=0.03030621074140072
I0305 21:10:21.372935 139984959084288 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.083124078810215, loss=0.03011804260313511
I0305 21:10:42.376265 139984967476992 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.1018052026629448, loss=0.03205883502960205
I0305 21:10:45.405936 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:12:00.157532 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:12:02.064228 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:12:03.930184 140127519114432 submission_runner.py:469] Time since start: 7150.31s, 	Step: 25415, 	{'train/accuracy': 0.9915741086006165, 'train/loss': 0.027419302612543106, 'train/mean_average_precision': 0.46642887108285436, 'validation/accuracy': 0.98691326379776, 'validation/loss': 0.04444209858775139, 'validation/mean_average_precision': 0.2751171298447432, 'validation/num_examples': 43793, 'test/accuracy': 0.9860453605651855, 'test/loss': 0.047338008880615234, 'test/mean_average_precision': 0.2656707282055037, 'test/num_examples': 43793, 'score': 5292.303888320923, 'total_duration': 7150.31390953064, 'accumulated_submission_time': 5292.303888320923, 'accumulated_eval_time': 1856.8792922496796, 'accumulated_logging_time': 0.43389081954956055}
I0305 21:12:03.943508 139984959084288 logging_writer.py:48] [25415] accumulated_eval_time=1856.88, accumulated_logging_time=0.433891, accumulated_submission_time=5292.3, global_step=25415, preemption_count=0, score=5292.3, test/accuracy=0.986045, test/loss=0.047338, test/mean_average_precision=0.265671, test/num_examples=43793, total_duration=7150.31, train/accuracy=0.991574, train/loss=0.0274193, train/mean_average_precision=0.466429, validation/accuracy=0.986913, validation/loss=0.0444421, validation/mean_average_precision=0.275117, validation/num_examples=43793
I0305 21:12:22.373715 139984967476992 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.11308545619249344, loss=0.03585370257496834
I0305 21:12:43.346752 139984959084288 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.09940246492624283, loss=0.03203247860074043
I0305 21:13:04.614867 139984967476992 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.08854971826076508, loss=0.031979747116565704
I0305 21:13:25.829650 139984959084288 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.09374605119228363, loss=0.032241806387901306
I0305 21:13:46.454661 139984967476992 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.08052597939968109, loss=0.0296075027436018
I0305 21:14:06.870196 139984959084288 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.09772548079490662, loss=0.03130199387669563
I0305 21:14:27.218324 139984967476992 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.06963962316513062, loss=0.030317582190036774
I0305 21:14:48.278235 139984959084288 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.096788190305233, loss=0.034199394285678864
I0305 21:15:09.534922 139984967476992 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.10121773183345795, loss=0.030174756422638893
I0305 21:15:30.895333 139984959084288 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.10016264766454697, loss=0.0318123959004879
I0305 21:15:52.494905 139984967476992 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.09404350817203522, loss=0.032030630856752396
I0305 21:16:03.990535 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:17:15.561647 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:17:17.512253 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:17:19.390373 140127519114432 submission_runner.py:469] Time since start: 7465.77s, 	Step: 26554, 	{'train/accuracy': 0.9919848442077637, 'train/loss': 0.026217423379421234, 'train/mean_average_precision': 0.5102128645234757, 'validation/accuracy': 0.9868884682655334, 'validation/loss': 0.044460564851760864, 'validation/mean_average_precision': 0.26955390640106985, 'validation/num_examples': 43793, 'test/accuracy': 0.9860537648200989, 'test/loss': 0.04739837720990181, 'test/mean_average_precision': 0.256179155282119, 'test/num_examples': 43793, 'score': 5532.311073303223, 'total_duration': 7465.774094820023, 'accumulated_submission_time': 5532.311073303223, 'accumulated_eval_time': 1932.279062986374, 'accumulated_logging_time': 0.45673060417175293}
I0305 21:17:19.402420 139984959084288 logging_writer.py:48] [26554] accumulated_eval_time=1932.28, accumulated_logging_time=0.456731, accumulated_submission_time=5532.31, global_step=26554, preemption_count=0, score=5532.31, test/accuracy=0.986054, test/loss=0.0473984, test/mean_average_precision=0.256179, test/num_examples=43793, total_duration=7465.77, train/accuracy=0.991985, train/loss=0.0262174, train/mean_average_precision=0.510213, validation/accuracy=0.986888, validation/loss=0.0444606, validation/mean_average_precision=0.269554, validation/num_examples=43793
I0305 21:17:29.404315 139984967476992 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.1142229288816452, loss=0.03197978809475899
I0305 21:17:50.405585 139984959084288 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.10126189142465591, loss=0.028677141293883324
I0305 21:18:11.323879 139984967476992 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.12491261214017868, loss=0.03225548937916756
I0305 21:18:32.336735 139984959084288 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.09549669176340103, loss=0.028898311778903008
I0305 21:18:53.498111 139984967476992 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.07730064541101456, loss=0.030017336830496788
I0305 21:19:14.483294 139984959084288 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.10233232378959656, loss=0.03117028996348381
I0305 21:19:35.454096 139984967476992 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.13592544198036194, loss=0.03238474950194359
I0305 21:19:56.574758 139984959084288 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.09594256430864334, loss=0.03005487285554409
I0305 21:20:17.517849 139984967476992 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.11122153699398041, loss=0.03116258978843689
I0305 21:20:38.561238 139984959084288 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.08975665271282196, loss=0.0288102850317955
I0305 21:20:59.894673 139984967476992 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.10019462555646896, loss=0.030591437593102455
I0305 21:21:19.519478 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:22:31.366072 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:22:33.352921 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:22:35.299590 140127519114432 submission_runner.py:469] Time since start: 7781.68s, 	Step: 27695, 	{'train/accuracy': 0.9918943047523499, 'train/loss': 0.026157131418585777, 'train/mean_average_precision': 0.5005074876569646, 'validation/accuracy': 0.9869453310966492, 'validation/loss': 0.044807810336351395, 'validation/mean_average_precision': 0.2779499341489269, 'validation/num_examples': 43793, 'test/accuracy': 0.9860626459121704, 'test/loss': 0.047654103487730026, 'test/mean_average_precision': 0.267730828385846, 'test/num_examples': 43793, 'score': 5772.38810634613, 'total_duration': 7781.683296918869, 'accumulated_submission_time': 5772.38810634613, 'accumulated_eval_time': 2008.0590863227844, 'accumulated_logging_time': 0.4780416488647461}
I0305 21:22:35.311320 139984959084288 logging_writer.py:48] [27695] accumulated_eval_time=2008.06, accumulated_logging_time=0.478042, accumulated_submission_time=5772.39, global_step=27695, preemption_count=0, score=5772.39, test/accuracy=0.986063, test/loss=0.0476541, test/mean_average_precision=0.267731, test/num_examples=43793, total_duration=7781.68, train/accuracy=0.991894, train/loss=0.0261571, train/mean_average_precision=0.500507, validation/accuracy=0.986945, validation/loss=0.0448078, validation/mean_average_precision=0.27795, validation/num_examples=43793
I0305 21:22:36.796044 139984967476992 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.08474370837211609, loss=0.028938977047801018
I0305 21:22:57.960607 139984959084288 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.09361519664525986, loss=0.0300309881567955
I0305 21:23:18.700614 139984967476992 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.0890415832400322, loss=0.030709000304341316
I0305 21:23:39.466231 139984959084288 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.09764277189970016, loss=0.029032671824097633
I0305 21:24:00.697335 139984967476992 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.11589280515909195, loss=0.03079410083591938
I0305 21:24:21.571967 139984959084288 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.0937637984752655, loss=0.029481492936611176
I0305 21:24:42.328454 139984967476992 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.13131046295166016, loss=0.03142251819372177
I0305 21:25:03.390780 139984959084288 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.09489802271127701, loss=0.029124919325113297
I0305 21:25:24.360628 139984967476992 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.07960960268974304, loss=0.028442196547985077
I0305 21:25:45.317349 139984959084288 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.11522570252418518, loss=0.02949102222919464
I0305 21:26:06.550745 139984967476992 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.08962517976760864, loss=0.029253970831632614
I0305 21:26:27.962525 139984959084288 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.10919632017612457, loss=0.029786694794893265
I0305 21:26:35.415017 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:27:45.585490 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:27:47.513376 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:27:49.440959 140127519114432 submission_runner.py:469] Time since start: 8095.82s, 	Step: 28836, 	{'train/accuracy': 0.9921720623970032, 'train/loss': 0.02557559125125408, 'train/mean_average_precision': 0.5163926469386997, 'validation/accuracy': 0.9868645071983337, 'validation/loss': 0.04469155892729759, 'validation/mean_average_precision': 0.27169798276187945, 'validation/num_examples': 43793, 'test/accuracy': 0.9860491752624512, 'test/loss': 0.047639425843954086, 'test/mean_average_precision': 0.26144788353039583, 'test/num_examples': 43793, 'score': 6012.268154382706, 'total_duration': 8095.824626684189, 'accumulated_submission_time': 6012.268154382706, 'accumulated_eval_time': 2082.0848817825317, 'accumulated_logging_time': 0.6821310520172119}
I0305 21:27:49.452841 139984967476992 logging_writer.py:48] [28836] accumulated_eval_time=2082.08, accumulated_logging_time=0.682131, accumulated_submission_time=6012.27, global_step=28836, preemption_count=0, score=6012.27, test/accuracy=0.986049, test/loss=0.0476394, test/mean_average_precision=0.261448, test/num_examples=43793, total_duration=8095.82, train/accuracy=0.992172, train/loss=0.0255756, train/mean_average_precision=0.516393, validation/accuracy=0.986865, validation/loss=0.0446916, validation/mean_average_precision=0.271698, validation/num_examples=43793
I0305 21:28:03.207398 139984959084288 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.1025252714753151, loss=0.029144419357180595
I0305 21:28:24.509236 139984967476992 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.08662662655115128, loss=0.027556682005524635
I0305 21:28:45.020509 139984959084288 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.1686941683292389, loss=0.03158435598015785
I0305 21:29:05.886275 139984967476992 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.12465303391218185, loss=0.02952418103814125
I0305 21:29:27.195901 139984959084288 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1067354753613472, loss=0.029533814638853073
I0305 21:29:48.832881 139984967476992 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.09915462881326675, loss=0.03079535998404026
I0305 21:30:10.728788 139984959084288 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.12294408679008484, loss=0.03157996013760567
I0305 21:30:31.941501 139984967476992 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.09649582207202911, loss=0.03023895062506199
I0305 21:30:53.266396 139984959084288 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.10821377485990524, loss=0.031043877825140953
I0305 21:31:14.313184 139984967476992 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.11817409098148346, loss=0.02963896282017231
I0305 21:31:35.483232 139984959084288 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.09290861338376999, loss=0.029954345896840096
I0305 21:31:49.563499 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:33:00.359492 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:33:02.410329 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:33:04.446345 140127519114432 submission_runner.py:469] Time since start: 8410.83s, 	Step: 29968, 	{'train/accuracy': 0.9922459125518799, 'train/loss': 0.024941151961684227, 'train/mean_average_precision': 0.5425853272372012, 'validation/accuracy': 0.9869444966316223, 'validation/loss': 0.04503217339515686, 'validation/mean_average_precision': 0.27545535377543634, 'validation/num_examples': 43793, 'test/accuracy': 0.98606938123703, 'test/loss': 0.04790951684117317, 'test/mean_average_precision': 0.2620476067839077, 'test/num_examples': 43793, 'score': 6252.336189508438, 'total_duration': 8410.829968690872, 'accumulated_submission_time': 6252.336189508438, 'accumulated_eval_time': 2156.9675409793854, 'accumulated_logging_time': 0.7037742137908936}
I0305 21:33:04.459084 139984967476992 logging_writer.py:48] [29968] accumulated_eval_time=2156.97, accumulated_logging_time=0.703774, accumulated_submission_time=6252.34, global_step=29968, preemption_count=0, score=6252.34, test/accuracy=0.986069, test/loss=0.0479095, test/mean_average_precision=0.262048, test/num_examples=43793, total_duration=8410.83, train/accuracy=0.992246, train/loss=0.0249412, train/mean_average_precision=0.542585, validation/accuracy=0.986944, validation/loss=0.0450322, validation/mean_average_precision=0.275455, validation/num_examples=43793
I0305 21:33:11.293250 139984959084288 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.10907942801713943, loss=0.030665507540106773
I0305 21:33:32.186348 139984967476992 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.12188176065683365, loss=0.03331947326660156
I0305 21:33:53.235893 139984959084288 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.10316342860460281, loss=0.02931472659111023
I0305 21:34:14.347766 139984967476992 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.10173314809799194, loss=0.02717498131096363
I0305 21:34:35.556759 139984959084288 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.10549787431955338, loss=0.029200181365013123
I0305 21:34:56.634940 139984967476992 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.1132730096578598, loss=0.02930987812578678
I0305 21:35:17.581356 139984959084288 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.1006198525428772, loss=0.02988845854997635
I0305 21:35:38.520614 139984967476992 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.10983310639858246, loss=0.03175070136785507
I0305 21:35:59.391666 139984959084288 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.10475903004407883, loss=0.028422195464372635
I0305 21:36:20.554139 139984967476992 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.1185300350189209, loss=0.02764618583023548
I0305 21:36:41.523180 139984959084288 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.10740804672241211, loss=0.02814336307346821
I0305 21:37:02.640495 139984967476992 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.12559780478477478, loss=0.02979450114071369
I0305 21:37:04.497853 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:38:16.826833 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:38:18.763770 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:38:20.692317 140127519114432 submission_runner.py:469] Time since start: 8727.08s, 	Step: 31110, 	{'train/accuracy': 0.9921659231185913, 'train/loss': 0.02535690739750862, 'train/mean_average_precision': 0.5160200076079325, 'validation/accuracy': 0.9869173169136047, 'validation/loss': 0.044967345893383026, 'validation/mean_average_precision': 0.27727260717811875, 'validation/num_examples': 43793, 'test/accuracy': 0.9860209226608276, 'test/loss': 0.04813773185014725, 'test/mean_average_precision': 0.2654290630703528, 'test/num_examples': 43793, 'score': 6492.335484266281, 'total_duration': 8727.075937509537, 'accumulated_submission_time': 6492.335484266281, 'accumulated_eval_time': 2233.1618144512177, 'accumulated_logging_time': 0.7262639999389648}
I0305 21:38:20.704912 139984959084288 logging_writer.py:48] [31110] accumulated_eval_time=2233.16, accumulated_logging_time=0.726264, accumulated_submission_time=6492.34, global_step=31110, preemption_count=0, score=6492.34, test/accuracy=0.986021, test/loss=0.0481377, test/mean_average_precision=0.265429, test/num_examples=43793, total_duration=8727.08, train/accuracy=0.992166, train/loss=0.0253569, train/mean_average_precision=0.51602, validation/accuracy=0.986917, validation/loss=0.0449673, validation/mean_average_precision=0.277273, validation/num_examples=43793
I0305 21:38:39.718764 139984967476992 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.13480789959430695, loss=0.03074164316058159
I0305 21:39:00.699401 139984959084288 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.1401122510433197, loss=0.02933604270219803
I0305 21:39:21.556613 139984967476992 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.11699662357568741, loss=0.030236657708883286
I0305 21:39:42.655659 139984959084288 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.09755158424377441, loss=0.027760909870266914
I0305 21:40:03.460590 139984967476992 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1267295479774475, loss=0.02811730094254017
I0305 21:40:24.378314 139984959084288 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.1232338398694992, loss=0.029038911685347557
I0305 21:40:45.368757 139984967476992 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.10222011059522629, loss=0.029277442023158073
I0305 21:41:06.306863 139984959084288 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.10851306468248367, loss=0.027453942224383354
I0305 21:41:27.212739 139984967476992 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.10517312586307526, loss=0.025349969044327736
I0305 21:41:47.427776 139984959084288 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.1201273575425148, loss=0.030225815251469612
I0305 21:42:07.758769 139984967476992 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.10191123187541962, loss=0.02621540054678917
I0305 21:42:20.797138 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:43:26.627079 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:43:28.546814 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:43:30.407556 140127519114432 submission_runner.py:469] Time since start: 9036.79s, 	Step: 32265, 	{'train/accuracy': 0.9928392767906189, 'train/loss': 0.023082315921783447, 'train/mean_average_precision': 0.5608201979317712, 'validation/accuracy': 0.9868758916854858, 'validation/loss': 0.0453580878674984, 'validation/mean_average_precision': 0.27498988713130557, 'validation/num_examples': 43793, 'test/accuracy': 0.9859960675239563, 'test/loss': 0.048615049570798874, 'test/mean_average_precision': 0.2650522464492782, 'test/num_examples': 43793, 'score': 6732.385160684586, 'total_duration': 9036.791203737259, 'accumulated_submission_time': 6732.385160684586, 'accumulated_eval_time': 2302.772072315216, 'accumulated_logging_time': 0.7477900981903076}
I0305 21:43:30.421070 139984959084288 logging_writer.py:48] [32265] accumulated_eval_time=2302.77, accumulated_logging_time=0.74779, accumulated_submission_time=6732.39, global_step=32265, preemption_count=0, score=6732.39, test/accuracy=0.985996, test/loss=0.048615, test/mean_average_precision=0.265052, test/num_examples=43793, total_duration=9036.79, train/accuracy=0.992839, train/loss=0.0230823, train/mean_average_precision=0.56082, validation/accuracy=0.986876, validation/loss=0.0453581, validation/mean_average_precision=0.27499, validation/num_examples=43793
I0305 21:43:38.036286 139984967476992 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.10893891751766205, loss=0.027328694239258766
I0305 21:43:59.112899 139984959084288 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.12441789358854294, loss=0.026739533990621567
I0305 21:44:20.248760 139984967476992 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.11958831548690796, loss=0.028010142967104912
I0305 21:44:41.486785 139984959084288 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.1193864718079567, loss=0.028652900829911232
I0305 21:45:02.998703 139984967476992 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.11527805775403976, loss=0.028064008802175522
I0305 21:45:24.176373 139984959084288 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.11807389557361603, loss=0.026329277083277702
I0305 21:45:45.501385 139984967476992 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.09811969846487045, loss=0.02615984156727791
I0305 21:46:06.548463 139984959084288 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.12488578259944916, loss=0.029624324291944504
I0305 21:46:27.639145 139984967476992 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.11132247745990753, loss=0.028370091691613197
I0305 21:46:48.970497 139984959084288 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.11275535821914673, loss=0.028196468949317932
I0305 21:47:10.435978 139984967476992 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.1088927686214447, loss=0.025925016030669212
I0305 21:47:30.563482 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:48:44.155164 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:48:46.126631 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:48:48.092410 140127519114432 submission_runner.py:469] Time since start: 9354.48s, 	Step: 33399, 	{'train/accuracy': 0.9926450252532959, 'train/loss': 0.02364608645439148, 'train/mean_average_precision': 0.5588305319227906, 'validation/accuracy': 0.9868202805519104, 'validation/loss': 0.04571348428726196, 'validation/mean_average_precision': 0.2768562846424662, 'validation/num_examples': 43793, 'test/accuracy': 0.9859421849250793, 'test/loss': 0.04902222752571106, 'test/mean_average_precision': 0.25715969436367253, 'test/num_examples': 43793, 'score': 6972.485014915466, 'total_duration': 9354.476155757904, 'accumulated_submission_time': 6972.485014915466, 'accumulated_eval_time': 2380.3009345531464, 'accumulated_logging_time': 0.7705228328704834}
I0305 21:48:48.105347 139984959084288 logging_writer.py:48] [33399] accumulated_eval_time=2380.3, accumulated_logging_time=0.770523, accumulated_submission_time=6972.49, global_step=33399, preemption_count=0, score=6972.49, test/accuracy=0.985942, test/loss=0.0490222, test/mean_average_precision=0.25716, test/num_examples=43793, total_duration=9354.48, train/accuracy=0.992645, train/loss=0.0236461, train/mean_average_precision=0.558831, validation/accuracy=0.98682, validation/loss=0.0457135, validation/mean_average_precision=0.276856, validation/num_examples=43793
I0305 21:48:48.563670 139984967476992 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.11923681199550629, loss=0.028280548751354218
I0305 21:49:09.668819 139984959084288 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.14224211871623993, loss=0.02919757552444935
I0305 21:49:30.894515 139984967476992 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.12522761523723602, loss=0.028693342581391335
I0305 21:49:51.872334 139984959084288 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.09926388412714005, loss=0.026423363015055656
I0305 21:50:12.952754 139984967476992 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.13655228912830353, loss=0.027778206393122673
I0305 21:50:34.202733 139984959084288 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.12258061021566391, loss=0.028338801115751266
I0305 21:50:55.075218 139984967476992 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.10840947926044464, loss=0.028038300573825836
I0305 21:51:16.023625 139984959084288 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.12372537702322006, loss=0.028552604839205742
I0305 21:51:36.581702 139984967476992 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.15010955929756165, loss=0.029818424955010414
I0305 21:51:57.516678 139984959084288 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.1188177689909935, loss=0.026220504194498062
I0305 21:52:18.779511 139984967476992 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.1189311295747757, loss=0.02739856205880642
I0305 21:52:39.911774 139984959084288 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.12424178421497345, loss=0.029009554535150528
I0305 21:52:48.117814 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:53:58.749394 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:54:00.688211 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:54:02.588260 140127519114432 submission_runner.py:469] Time since start: 9668.97s, 	Step: 34541, 	{'train/accuracy': 0.9934738874435425, 'train/loss': 0.02116295136511326, 'train/mean_average_precision': 0.6161199942723665, 'validation/accuracy': 0.9868738651275635, 'validation/loss': 0.046113163232803345, 'validation/mean_average_precision': 0.2743765303671988, 'validation/num_examples': 43793, 'test/accuracy': 0.9860247373580933, 'test/loss': 0.04923577234148979, 'test/mean_average_precision': 0.26699488233620927, 'test/num_examples': 43793, 'score': 7212.458361625671, 'total_duration': 9668.971990823746, 'accumulated_submission_time': 7212.458361625671, 'accumulated_eval_time': 2454.7712993621826, 'accumulated_logging_time': 0.7925794124603271}
I0305 21:54:02.601480 139984967476992 logging_writer.py:48] [34541] accumulated_eval_time=2454.77, accumulated_logging_time=0.792579, accumulated_submission_time=7212.46, global_step=34541, preemption_count=0, score=7212.46, test/accuracy=0.986025, test/loss=0.0492358, test/mean_average_precision=0.266995, test/num_examples=43793, total_duration=9668.97, train/accuracy=0.993474, train/loss=0.021163, train/mean_average_precision=0.61612, validation/accuracy=0.986874, validation/loss=0.0461132, validation/mean_average_precision=0.274377, validation/num_examples=43793
I0305 21:54:15.487665 139984959084288 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.13280533254146576, loss=0.027562223374843597
I0305 21:54:36.524282 139984967476992 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.12663374841213226, loss=0.026403669267892838
I0305 21:54:57.487366 139984959084288 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.2103201448917389, loss=0.029439568519592285
I0305 21:55:18.806602 139984967476992 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1409144550561905, loss=0.02607790380716324
I0305 21:55:39.602525 139984959084288 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.12878002226352692, loss=0.02619456872344017
I0305 21:56:00.206176 139984967476992 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.15390723943710327, loss=0.027065301313996315
I0305 21:56:21.155024 139984959084288 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.12442109733819962, loss=0.02680102176964283
I0305 21:56:42.246512 139984967476992 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.11451275646686554, loss=0.025854745879769325
I0305 21:57:03.188635 139984959084288 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.12276531755924225, loss=0.024026647210121155
I0305 21:57:24.296551 139984967476992 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.1433284878730774, loss=0.027236493304371834
I0305 21:57:45.246518 139984959084288 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.140702486038208, loss=0.02634999342262745
I0305 21:58:02.709850 140127519114432 spec.py:321] Evaluating on the training split.
I0305 21:59:13.019518 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 21:59:14.964550 140127519114432 spec.py:349] Evaluating on the test split.
I0305 21:59:16.823648 140127519114432 submission_runner.py:469] Time since start: 9983.21s, 	Step: 35686, 	{'train/accuracy': 0.9930078983306885, 'train/loss': 0.022653525695204735, 'train/mean_average_precision': 0.5737253428167677, 'validation/accuracy': 0.9867066144943237, 'validation/loss': 0.046449627727270126, 'validation/mean_average_precision': 0.2684280875258214, 'validation/num_examples': 43793, 'test/accuracy': 0.9857656955718994, 'test/loss': 0.04978271201252937, 'test/mean_average_precision': 0.25721039436428267, 'test/num_examples': 43793, 'score': 7452.527245044708, 'total_duration': 9983.207334041595, 'accumulated_submission_time': 7452.527245044708, 'accumulated_eval_time': 2528.8849728107452, 'accumulated_logging_time': 0.8157484531402588}
I0305 21:59:16.836459 139984967476992 logging_writer.py:48] [35686] accumulated_eval_time=2528.88, accumulated_logging_time=0.815748, accumulated_submission_time=7452.53, global_step=35686, preemption_count=0, score=7452.53, test/accuracy=0.985766, test/loss=0.0497827, test/mean_average_precision=0.25721, test/num_examples=43793, total_duration=9983.21, train/accuracy=0.993008, train/loss=0.0226535, train/mean_average_precision=0.573725, validation/accuracy=0.986707, validation/loss=0.0464496, validation/mean_average_precision=0.268428, validation/num_examples=43793
I0305 21:59:20.111032 139984959084288 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.12961505353450775, loss=0.02575714886188507
I0305 21:59:40.936965 139984967476992 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.1258147805929184, loss=0.026235459372401237
I0305 22:00:01.895148 139984959084288 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1365399956703186, loss=0.02557270973920822
I0305 22:00:23.107214 139984967476992 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.15299314260482788, loss=0.02797812595963478
I0305 22:00:44.110655 139984959084288 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.13390497863292694, loss=0.0252092182636261
I0305 22:01:04.985608 139984967476992 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.13213570415973663, loss=0.02728535421192646
I0305 22:01:25.743651 139984959084288 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.14651133120059967, loss=0.026508742943406105
I0305 22:01:46.814979 139984967476992 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.13923704624176025, loss=0.026465630158782005
I0305 22:02:08.068253 139984959084288 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.14741064608097076, loss=0.02462671510875225
I0305 22:02:28.885007 139984967476992 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.14109806716442108, loss=0.025085201486945152
I0305 22:02:49.663506 139984959084288 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.15304216742515564, loss=0.025427846238017082
I0305 22:03:10.842918 139984967476992 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.15498961508274078, loss=0.028796322643756866
I0305 22:03:16.900107 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:04:26.070980 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:04:28.017868 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:04:29.936650 140127519114432 submission_runner.py:469] Time since start: 10296.32s, 	Step: 36830, 	{'train/accuracy': 0.9942060708999634, 'train/loss': 0.019067538902163506, 'train/mean_average_precision': 0.6528371482783737, 'validation/accuracy': 0.9868312478065491, 'validation/loss': 0.0466800257563591, 'validation/mean_average_precision': 0.2752563377830114, 'validation/num_examples': 43793, 'test/accuracy': 0.9858655333518982, 'test/loss': 0.050164829939603806, 'test/mean_average_precision': 0.2595261167050926, 'test/num_examples': 43793, 'score': 7692.550749540329, 'total_duration': 10296.32037472725, 'accumulated_submission_time': 7692.550749540329, 'accumulated_eval_time': 2601.9214272499084, 'accumulated_logging_time': 0.8375959396362305}
I0305 22:04:29.950260 139984959084288 logging_writer.py:48] [36830] accumulated_eval_time=2601.92, accumulated_logging_time=0.837596, accumulated_submission_time=7692.55, global_step=36830, preemption_count=0, score=7692.55, test/accuracy=0.985866, test/loss=0.0501648, test/mean_average_precision=0.259526, test/num_examples=43793, total_duration=10296.3, train/accuracy=0.994206, train/loss=0.0190675, train/mean_average_precision=0.652837, validation/accuracy=0.986831, validation/loss=0.04668, validation/mean_average_precision=0.275256, validation/num_examples=43793
I0305 22:04:44.906412 139984967476992 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.16519111394882202, loss=0.02725963480770588
I0305 22:05:05.868731 139984959084288 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.1381232887506485, loss=0.024884236976504326
I0305 22:05:26.830304 139984967476992 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.1506451666355133, loss=0.025578180328011513
I0305 22:05:47.670262 139984959084288 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.159592866897583, loss=0.026875820010900497
I0305 22:06:08.638905 139984967476992 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.1401587724685669, loss=0.026427576318383217
I0305 22:06:28.860565 139984959084288 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.17900340259075165, loss=0.025686590000987053
I0305 22:06:49.062793 139984967476992 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.17614588141441345, loss=0.02792288549244404
I0305 22:07:09.358956 139984959084288 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.1694726049900055, loss=0.02717476524412632
I0305 22:07:29.550636 139984967476992 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.170579731464386, loss=0.02724798582494259
I0305 22:07:49.641096 139984959084288 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.16141512989997864, loss=0.026534251868724823
I0305 22:08:10.051836 139984967476992 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.1532689779996872, loss=0.024888379499316216
I0305 22:08:29.994011 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:09:38.859846 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:09:40.789386 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:09:42.706541 140127519114432 submission_runner.py:469] Time since start: 10609.09s, 	Step: 37998, 	{'train/accuracy': 0.993248462677002, 'train/loss': 0.021436164155602455, 'train/mean_average_precision': 0.5860496611746411, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.04749639704823494, 'validation/mean_average_precision': 0.26897217019881975, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.050679318606853485, 'test/mean_average_precision': 0.26110510388091945, 'test/num_examples': 43793, 'score': 7932.5567219257355, 'total_duration': 10609.09031033516, 'accumulated_submission_time': 7932.5567219257355, 'accumulated_eval_time': 2674.633913755417, 'accumulated_logging_time': 0.8605086803436279}
I0305 22:09:42.720168 139984959084288 logging_writer.py:48] [37998] accumulated_eval_time=2674.63, accumulated_logging_time=0.860509, accumulated_submission_time=7932.56, global_step=37998, preemption_count=0, score=7932.56, test/accuracy=0.985836, test/loss=0.0506793, test/mean_average_precision=0.261105, test/num_examples=43793, total_duration=10609.1, train/accuracy=0.993248, train/loss=0.0214362, train/mean_average_precision=0.58605, validation/accuracy=0.986678, validation/loss=0.0474964, validation/mean_average_precision=0.268972, validation/num_examples=43793
I0305 22:09:43.362128 139984967476992 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.15650740265846252, loss=0.02530149184167385
I0305 22:10:04.412793 139984959084288 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.17042231559753418, loss=0.026387354359030724
I0305 22:10:25.264567 139984967476992 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1332176774740219, loss=0.024560295045375824
I0305 22:10:46.596547 139984959084288 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.1394548863172531, loss=0.026426995173096657
I0305 22:11:07.690405 139984967476992 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.16291609406471252, loss=0.026366204023361206
I0305 22:11:28.723196 139984959084288 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.16573114693164825, loss=0.026854362338781357
I0305 22:11:49.561445 139984967476992 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.15038596093654633, loss=0.022973017767071724
I0305 22:12:10.606090 139984959084288 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.15348611772060394, loss=0.026031937450170517
I0305 22:12:31.648872 139984967476992 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.15842866897583008, loss=0.0257191713899374
I0305 22:12:52.587378 139984959084288 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.1469903439283371, loss=0.024322450160980225
I0305 22:13:13.604771 139984967476992 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.14862293004989624, loss=0.026664767414331436
I0305 22:13:34.454184 139984959084288 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.15322794020175934, loss=0.022936657071113586
I0305 22:13:42.849654 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:14:54.563978 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:14:56.485240 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:14:58.334248 140127519114432 submission_runner.py:469] Time since start: 10924.72s, 	Step: 39142, 	{'train/accuracy': 0.9945171475410461, 'train/loss': 0.017819788306951523, 'train/mean_average_precision': 0.6816268024683101, 'validation/accuracy': 0.9867541193962097, 'validation/loss': 0.047857433557510376, 'validation/mean_average_precision': 0.2713309631417209, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.05174333229660988, 'test/mean_average_precision': 0.2573115381451356, 'test/num_examples': 43793, 'score': 8172.640521526337, 'total_duration': 10924.717975616455, 'accumulated_submission_time': 8172.640521526337, 'accumulated_eval_time': 2750.118423938751, 'accumulated_logging_time': 0.884434700012207}
I0305 22:14:58.347175 139984967476992 logging_writer.py:48] [39142] accumulated_eval_time=2750.12, accumulated_logging_time=0.884435, accumulated_submission_time=8172.64, global_step=39142, preemption_count=0, score=8172.64, test/accuracy=0.98585, test/loss=0.0517433, test/mean_average_precision=0.257312, test/num_examples=43793, total_duration=10924.7, train/accuracy=0.994517, train/loss=0.0178198, train/mean_average_precision=0.681627, validation/accuracy=0.986754, validation/loss=0.0478574, validation/mean_average_precision=0.271331, validation/num_examples=43793
I0305 22:15:10.739913 139984959084288 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.14669732749462128, loss=0.02330745942890644
I0305 22:15:31.600620 139984967476992 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.14705327153205872, loss=0.022777969017624855
I0305 22:15:52.708570 139984959084288 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1639508306980133, loss=0.02566443383693695
I0305 22:16:13.676566 139984967476992 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1572895348072052, loss=0.023876037448644638
I0305 22:16:34.452713 139984959084288 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.15636128187179565, loss=0.024031994864344597
I0305 22:16:55.446875 139984967476992 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.16581249237060547, loss=0.02439684234559536
I0305 22:17:16.059051 139984959084288 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.18350152671337128, loss=0.025342540815472603
I0305 22:17:37.058334 139984967476992 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.1714532971382141, loss=0.02596636489033699
I0305 22:17:58.261462 139984959084288 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.16324947774410248, loss=0.021084340289235115
I0305 22:18:19.380577 139984967476992 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.15682914853096008, loss=0.026559481397271156
I0305 22:18:40.346410 139984959084288 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.15799087285995483, loss=0.023063192144036293
I0305 22:18:58.365332 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:20:06.559547 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:20:08.520164 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:20:10.423507 140127519114432 submission_runner.py:469] Time since start: 11236.81s, 	Step: 40287, 	{'train/accuracy': 0.99378901720047, 'train/loss': 0.019771583378314972, 'train/mean_average_precision': 0.6244414242355489, 'validation/accuracy': 0.9865612983703613, 'validation/loss': 0.04878024384379387, 'validation/mean_average_precision': 0.26757761482970654, 'validation/num_examples': 43793, 'test/accuracy': 0.9857273697853088, 'test/loss': 0.05246104300022125, 'test/mean_average_precision': 0.25543275062509974, 'test/num_examples': 43793, 'score': 8412.61855840683, 'total_duration': 11236.807277679443, 'accumulated_submission_time': 8412.61855840683, 'accumulated_eval_time': 2822.176558494568, 'accumulated_logging_time': 0.9071769714355469}
I0305 22:20:10.437328 139984967476992 logging_writer.py:48] [40287] accumulated_eval_time=2822.18, accumulated_logging_time=0.907177, accumulated_submission_time=8412.62, global_step=40287, preemption_count=0, score=8412.62, test/accuracy=0.985727, test/loss=0.052461, test/mean_average_precision=0.255433, test/num_examples=43793, total_duration=11236.8, train/accuracy=0.993789, train/loss=0.0197716, train/mean_average_precision=0.624441, validation/accuracy=0.986561, validation/loss=0.0487802, validation/mean_average_precision=0.267578, validation/num_examples=43793
I0305 22:20:13.342129 139984959084288 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.1651170551776886, loss=0.023619748651981354
I0305 22:20:34.106469 139984967476992 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.16365914046764374, loss=0.024342041462659836
I0305 22:20:55.249203 139984959084288 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.177473247051239, loss=0.025201842188835144
I0305 22:21:16.307646 139984967476992 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.17251569032669067, loss=0.023692725226283073
I0305 22:21:37.233026 139984959084288 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.2249494343996048, loss=0.02333476021885872
I0305 22:21:58.266995 139984967476992 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.17219142615795135, loss=0.022989388555288315
I0305 22:22:19.238316 139984959084288 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.15884417295455933, loss=0.02202794887125492
I0305 22:22:40.160761 139984967476992 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.1742187887430191, loss=0.024168584495782852
I0305 22:23:01.243385 139984959084288 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.17914508283138275, loss=0.022390039637684822
I0305 22:23:22.469094 139984967476992 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.16982989013195038, loss=0.02258387953042984
I0305 22:23:43.534911 139984959084288 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.16289600729942322, loss=0.024057339876890182
I0305 22:24:04.744817 139984967476992 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.1999799907207489, loss=0.02621784619987011
I0305 22:24:10.566931 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:25:25.074520 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:25:27.155321 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:25:29.200658 140127519114432 submission_runner.py:469] Time since start: 11555.58s, 	Step: 41428, 	{'train/accuracy': 0.9951443076133728, 'train/loss': 0.016101926565170288, 'train/mean_average_precision': 0.7086035571904423, 'validation/accuracy': 0.9865174293518066, 'validation/loss': 0.04976053908467293, 'validation/mean_average_precision': 0.26367705348263387, 'validation/num_examples': 43793, 'test/accuracy': 0.9856621026992798, 'test/loss': 0.05334242433309555, 'test/mean_average_precision': 0.253602397323489, 'test/num_examples': 43793, 'score': 8652.705162286758, 'total_duration': 11555.584413528442, 'accumulated_submission_time': 8652.705162286758, 'accumulated_eval_time': 2900.8102271556854, 'accumulated_logging_time': 0.9302163124084473}
I0305 22:25:29.215691 139984959084288 logging_writer.py:48] [41428] accumulated_eval_time=2900.81, accumulated_logging_time=0.930216, accumulated_submission_time=8652.71, global_step=41428, preemption_count=0, score=8652.71, test/accuracy=0.985662, test/loss=0.0533424, test/mean_average_precision=0.253602, test/num_examples=43793, total_duration=11555.6, train/accuracy=0.995144, train/loss=0.0161019, train/mean_average_precision=0.708604, validation/accuracy=0.986517, validation/loss=0.0497605, validation/mean_average_precision=0.263677, validation/num_examples=43793
I0305 22:25:44.129982 139984967476992 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.19436299800872803, loss=0.025277499109506607
I0305 22:26:04.314800 139984959084288 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.16650386154651642, loss=0.021373042836785316
I0305 22:26:24.652661 139984967476992 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.164774551987648, loss=0.021908296272158623
I0305 22:26:45.368191 139984959084288 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.18895873427391052, loss=0.02390982396900654
I0305 22:27:06.456930 139984967476992 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.18156211078166962, loss=0.023078206926584244
I0305 22:27:27.710453 139984959084288 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1818101406097412, loss=0.023776983842253685
I0305 22:27:48.984358 139984967476992 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.19300882518291473, loss=0.02345912903547287
I0305 22:28:10.344786 139984959084288 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.15953832864761353, loss=0.02271990105509758
I0305 22:28:31.340370 139984967476992 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.22433343529701233, loss=0.025224901735782623
I0305 22:28:52.509019 139984959084288 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.16394205391407013, loss=0.02311895042657852
I0305 22:29:13.701667 139984967476992 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.18117058277130127, loss=0.02374032884836197
I0305 22:29:29.389748 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:30:40.628988 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:30:42.653855 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:30:44.602842 140127519114432 submission_runner.py:469] Time since start: 11870.99s, 	Step: 42575, 	{'train/accuracy': 0.9942857027053833, 'train/loss': 0.01802401803433895, 'train/mean_average_precision': 0.6771790066359358, 'validation/accuracy': 0.9865283966064453, 'validation/loss': 0.05046654865145683, 'validation/mean_average_precision': 0.2637375602827192, 'validation/num_examples': 43793, 'test/accuracy': 0.9857020974159241, 'test/loss': 0.05431224778294563, 'test/mean_average_precision': 0.2537504972470193, 'test/num_examples': 43793, 'score': 8892.839651107788, 'total_duration': 11870.986486673355, 'accumulated_submission_time': 8892.839651107788, 'accumulated_eval_time': 2976.023154258728, 'accumulated_logging_time': 0.9544816017150879}
I0305 22:30:44.617509 139984959084288 logging_writer.py:48] [42575] accumulated_eval_time=2976.02, accumulated_logging_time=0.954482, accumulated_submission_time=8892.84, global_step=42575, preemption_count=0, score=8892.84, test/accuracy=0.985702, test/loss=0.0543122, test/mean_average_precision=0.25375, test/num_examples=43793, total_duration=11871, train/accuracy=0.994286, train/loss=0.018024, train/mean_average_precision=0.677179, validation/accuracy=0.986528, validation/loss=0.0504665, validation/mean_average_precision=0.263738, validation/num_examples=43793
I0305 22:30:50.107476 139984967476992 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.1872059851884842, loss=0.02198689989745617
I0305 22:31:11.282012 139984959084288 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.1702372282743454, loss=0.022210052236914635
I0305 22:31:32.796855 139984967476992 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.1941104382276535, loss=0.02352290041744709
I0305 22:31:54.302435 139984959084288 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.1659001260995865, loss=0.02201947383582592
I0305 22:32:15.954079 139984967476992 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.1831788867712021, loss=0.022247180342674255
I0305 22:32:37.307880 139984959084288 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.1797148734331131, loss=0.02230837196111679
I0305 22:32:58.766109 139984967476992 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.19279706478118896, loss=0.023365117609500885
I0305 22:33:19.199097 139984959084288 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.1981852948665619, loss=0.022059539332985878
I0305 22:33:39.461167 139984967476992 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.1755387783050537, loss=0.021396277472376823
I0305 22:34:01.005870 139984959084288 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.1679062843322754, loss=0.021190544590353966
I0305 22:34:22.142906 139984967476992 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.1927003562450409, loss=0.02293398231267929
I0305 22:34:43.242405 139984959084288 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.1873481124639511, loss=0.019941389560699463
I0305 22:34:44.713822 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:35:58.969341 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:36:00.900782 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:36:02.816821 140127519114432 submission_runner.py:469] Time since start: 12189.20s, 	Step: 43708, 	{'train/accuracy': 0.995294451713562, 'train/loss': 0.015386600978672504, 'train/mean_average_precision': 0.7130191115419166, 'validation/accuracy': 0.9864017367362976, 'validation/loss': 0.05124441906809807, 'validation/mean_average_precision': 0.25821619531950285, 'validation/num_examples': 43793, 'test/accuracy': 0.9855639338493347, 'test/loss': 0.055325426161289215, 'test/mean_average_precision': 0.2517867768552215, 'test/num_examples': 43793, 'score': 9132.895245552063, 'total_duration': 12189.200589179993, 'accumulated_submission_time': 9132.895245552063, 'accumulated_eval_time': 3054.126108646393, 'accumulated_logging_time': 0.9808549880981445}
I0305 22:36:02.831865 139984967476992 logging_writer.py:48] [43708] accumulated_eval_time=3054.13, accumulated_logging_time=0.980855, accumulated_submission_time=9132.9, global_step=43708, preemption_count=0, score=9132.9, test/accuracy=0.985564, test/loss=0.0553254, test/mean_average_precision=0.251787, test/num_examples=43793, total_duration=12189.2, train/accuracy=0.995294, train/loss=0.0153866, train/mean_average_precision=0.713019, validation/accuracy=0.986402, validation/loss=0.0512444, validation/mean_average_precision=0.258216, validation/num_examples=43793
I0305 22:36:21.712346 139984959084288 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.1756235510110855, loss=0.021184153854846954
I0305 22:36:42.282494 139984967476992 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.16874931752681732, loss=0.020388085395097733
I0305 22:37:03.441521 139984959084288 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.18391340970993042, loss=0.02250601537525654
I0305 22:37:24.708789 139984967476992 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.20659182965755463, loss=0.021140791475772858
I0305 22:37:45.744731 139984959084288 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.19732096791267395, loss=0.02079470269382
I0305 22:38:07.194228 139984967476992 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.25884100794792175, loss=0.02389107458293438
I0305 22:38:28.651936 139984959084288 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.1991555243730545, loss=0.020044799894094467
I0305 22:38:49.983317 139984967476992 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.1881958246231079, loss=0.021922852843999863
I0305 22:39:11.225472 139984959084288 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.1988581418991089, loss=0.022595735266804695
I0305 22:39:32.354893 139984967476992 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1829911321401596, loss=0.02028866857290268
I0305 22:39:53.538598 139984959084288 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.18059836328029633, loss=0.02188269793987274
I0305 22:40:02.991590 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:41:11.296368 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:41:13.225569 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:41:15.117553 140127519114432 submission_runner.py:469] Time since start: 12501.50s, 	Step: 44847, 	{'train/accuracy': 0.9947444796562195, 'train/loss': 0.016666201874613762, 'train/mean_average_precision': 0.6781362395343611, 'validation/accuracy': 0.9863173365592957, 'validation/loss': 0.051825739443302155, 'validation/mean_average_precision': 0.25895351871184746, 'validation/num_examples': 43793, 'test/accuracy': 0.985499918460846, 'test/loss': 0.0556478388607502, 'test/mean_average_precision': 0.25222121488484445, 'test/num_examples': 43793, 'score': 9373.012095928192, 'total_duration': 12501.501206874847, 'accumulated_submission_time': 9373.012095928192, 'accumulated_eval_time': 3126.251916408539, 'accumulated_logging_time': 1.0073065757751465}
I0305 22:41:15.131139 139984967476992 logging_writer.py:48] [44847] accumulated_eval_time=3126.25, accumulated_logging_time=1.00731, accumulated_submission_time=9373.01, global_step=44847, preemption_count=0, score=9373.01, test/accuracy=0.9855, test/loss=0.0556478, test/mean_average_precision=0.252221, test/num_examples=43793, total_duration=12501.5, train/accuracy=0.994744, train/loss=0.0166662, train/mean_average_precision=0.678136, validation/accuracy=0.986317, validation/loss=0.0518257, validation/mean_average_precision=0.258954, validation/num_examples=43793
I0305 22:41:26.491718 139984959084288 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.21154287457466125, loss=0.022204279899597168
I0305 22:41:46.920782 139984967476992 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2637728154659271, loss=0.023978041484951973
I0305 22:42:07.243660 139984959084288 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.18585015833377838, loss=0.022375157102942467
I0305 22:42:27.459782 139984967476992 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.19746749103069305, loss=0.022816581651568413
I0305 22:42:47.706107 139984959084288 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.21036459505558014, loss=0.021435391157865524
I0305 22:43:08.471332 139984967476992 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.1850101798772812, loss=0.02051052451133728
I0305 22:43:29.745225 139984959084288 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.17575056850910187, loss=0.018873808905482292
I0305 22:43:51.062562 139984967476992 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.19050517678260803, loss=0.01969214528799057
I0305 22:44:12.141848 139984959084288 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.21325188875198364, loss=0.023263772949576378
I0305 22:44:33.242560 139984967476992 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.20768772065639496, loss=0.020322397351264954
I0305 22:44:54.393095 139984959084288 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2172565758228302, loss=0.021311819553375244
I0305 22:45:15.243881 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:46:31.121312 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:46:33.252464 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:46:35.287888 140127519114432 submission_runner.py:469] Time since start: 12821.67s, 	Step: 46000, 	{'train/accuracy': 0.9954876899719238, 'train/loss': 0.014756465330719948, 'train/mean_average_precision': 0.7255885965050961, 'validation/accuracy': 0.9863587021827698, 'validation/loss': 0.052331190556287766, 'validation/mean_average_precision': 0.26217852946610726, 'validation/num_examples': 43793, 'test/accuracy': 0.9854245185852051, 'test/loss': 0.05650071054697037, 'test/mean_average_precision': 0.2469669993461553, 'test/num_examples': 43793, 'score': 9613.08573937416, 'total_duration': 12821.671533346176, 'accumulated_submission_time': 9613.08573937416, 'accumulated_eval_time': 3206.2957735061646, 'accumulated_logging_time': 1.0298819541931152}
I0305 22:46:35.303138 139984967476992 logging_writer.py:48] [46000] accumulated_eval_time=3206.3, accumulated_logging_time=1.02988, accumulated_submission_time=9613.09, global_step=46000, preemption_count=0, score=9613.09, test/accuracy=0.985425, test/loss=0.0565007, test/mean_average_precision=0.246967, test/num_examples=43793, total_duration=12821.7, train/accuracy=0.995488, train/loss=0.0147565, train/mean_average_precision=0.725589, validation/accuracy=0.986359, validation/loss=0.0523312, validation/mean_average_precision=0.262179, validation/num_examples=43793
I0305 22:46:35.524607 139984959084288 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.18495438992977142, loss=0.01988827995955944
I0305 22:46:55.975373 139984967476992 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.17681428790092468, loss=0.017424384132027626
I0305 22:47:16.565769 139984959084288 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.19800961017608643, loss=0.02137739770114422
I0305 22:47:37.412394 139984967476992 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.17580386996269226, loss=0.020881276577711105
I0305 22:47:58.228808 139984959084288 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.21820171177387238, loss=0.02173769474029541
I0305 22:48:19.290698 139984967476992 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.18836018443107605, loss=0.019924221560359
I0305 22:48:40.194805 139984959084288 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.17972266674041748, loss=0.020480869337916374
I0305 22:49:01.386569 139984967476992 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.1735658049583435, loss=0.020151792094111443
I0305 22:49:22.477160 139984959084288 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.19243942201137543, loss=0.019788654521107674
I0305 22:49:43.358450 139984967476992 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.1963239312171936, loss=0.018791019916534424
I0305 22:50:04.306179 139984959084288 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.19468651711940765, loss=0.019342759624123573
I0305 22:50:25.423310 139984967476992 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.21079154312610626, loss=0.019806422293186188
I0305 22:50:35.453212 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:51:46.415022 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:51:48.370629 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:51:50.298086 140127519114432 submission_runner.py:469] Time since start: 13136.68s, 	Step: 47148, 	{'train/accuracy': 0.9951308369636536, 'train/loss': 0.015495703555643559, 'train/mean_average_precision': 0.7276559388006254, 'validation/accuracy': 0.9862426519393921, 'validation/loss': 0.05307828262448311, 'validation/mean_average_precision': 0.2546053847921652, 'validation/num_examples': 43793, 'test/accuracy': 0.9853609204292297, 'test/loss': 0.05728951841592789, 'test/mean_average_precision': 0.24394026416339362, 'test/num_examples': 43793, 'score': 9853.194214105606, 'total_duration': 13136.681802988052, 'accumulated_submission_time': 9853.194214105606, 'accumulated_eval_time': 3281.1405560970306, 'accumulated_logging_time': 1.05521559715271}
I0305 22:51:50.312218 139984959084288 logging_writer.py:48] [47148] accumulated_eval_time=3281.14, accumulated_logging_time=1.05522, accumulated_submission_time=9853.19, global_step=47148, preemption_count=0, score=9853.19, test/accuracy=0.985361, test/loss=0.0572895, test/mean_average_precision=0.24394, test/num_examples=43793, total_duration=13136.7, train/accuracy=0.995131, train/loss=0.0154957, train/mean_average_precision=0.727656, validation/accuracy=0.986243, validation/loss=0.0530783, validation/mean_average_precision=0.254605, validation/num_examples=43793
I0305 22:52:01.434351 139984967476992 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.18758757412433624, loss=0.01985211856663227
I0305 22:52:22.577698 139984959084288 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.20876583456993103, loss=0.02089865133166313
I0305 22:52:43.819877 139984967476992 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.18957164883613586, loss=0.020224567502737045
I0305 22:53:04.712444 139984959084288 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2086724489927292, loss=0.020035937428474426
I0305 22:53:25.556697 139984967476992 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.20435556769371033, loss=0.019379446282982826
I0305 22:53:46.177719 139984959084288 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19754022359848022, loss=0.01984461396932602
I0305 22:54:06.804953 139984967476992 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.19934915006160736, loss=0.019973954185843468
I0305 22:54:27.919928 139984959084288 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.21035771071910858, loss=0.02130497805774212
I0305 22:54:48.944502 139984967476992 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.1868048906326294, loss=0.020752165466547012
I0305 22:55:09.864615 139984959084288 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.21440739929676056, loss=0.019397633150219917
I0305 22:55:31.107007 139984967476992 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.18904995918273926, loss=0.018868213519454002
I0305 22:55:50.324955 140127519114432 spec.py:321] Evaluating on the training split.
I0305 22:56:58.025566 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 22:56:59.984839 140127519114432 spec.py:349] Evaluating on the test split.
I0305 22:57:01.874712 140127519114432 submission_runner.py:469] Time since start: 13448.26s, 	Step: 48292, 	{'train/accuracy': 0.995542585849762, 'train/loss': 0.014487281441688538, 'train/mean_average_precision': 0.7343132507814407, 'validation/accuracy': 0.9862040877342224, 'validation/loss': 0.05333206057548523, 'validation/mean_average_precision': 0.2578803043521914, 'validation/num_examples': 43793, 'test/accuracy': 0.985332727432251, 'test/loss': 0.05723022669553757, 'test/mean_average_precision': 0.2495107846766161, 'test/num_examples': 43793, 'score': 10093.16878080368, 'total_duration': 13448.258479595184, 'accumulated_submission_time': 10093.16878080368, 'accumulated_eval_time': 3352.690266609192, 'accumulated_logging_time': 1.0784709453582764}
I0305 22:57:01.889304 139984959084288 logging_writer.py:48] [48292] accumulated_eval_time=3352.69, accumulated_logging_time=1.07847, accumulated_submission_time=10093.2, global_step=48292, preemption_count=0, score=10093.2, test/accuracy=0.985333, test/loss=0.0572302, test/mean_average_precision=0.249511, test/num_examples=43793, total_duration=13448.3, train/accuracy=0.995543, train/loss=0.0144873, train/mean_average_precision=0.734313, validation/accuracy=0.986204, validation/loss=0.0533321, validation/mean_average_precision=0.25788, validation/num_examples=43793
I0305 22:57:03.788337 139984967476992 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.18649306893348694, loss=0.019743062555789948
I0305 22:57:24.937346 139984959084288 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.19734272360801697, loss=0.019803402945399284
I0305 22:57:46.019180 139984967476992 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.2028253823518753, loss=0.01993643492460251
I0305 22:58:07.085060 139984959084288 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.208943709731102, loss=0.02042798511683941
I0305 22:58:28.155498 139984967476992 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.19467772543430328, loss=0.019052626565098763
I0305 22:58:49.519779 139984959084288 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.19632971286773682, loss=0.021040435880422592
I0305 22:59:10.604647 139984967476992 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.20783422887325287, loss=0.02035740204155445
I0305 22:59:31.584002 139984959084288 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.21050333976745605, loss=0.019520562142133713
I0305 22:59:52.713549 139984967476992 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.19369980692863464, loss=0.019024448469281197
I0305 23:00:14.103163 139984959084288 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.24561454355716705, loss=0.02095143124461174
I0305 23:00:35.358522 139984967476992 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.22696062922477722, loss=0.0190927404910326
I0305 23:00:56.190940 139984959084288 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2002343237400055, loss=0.021516047418117523
I0305 23:01:02.022297 140127519114432 spec.py:321] Evaluating on the training split.
I0305 23:02:15.949630 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 23:02:17.998112 140127519114432 spec.py:349] Evaluating on the test split.
I0305 23:02:20.018727 140127519114432 submission_runner.py:469] Time since start: 13766.40s, 	Step: 49429, 	{'train/accuracy': 0.9961375594139099, 'train/loss': 0.013219335116446018, 'train/mean_average_precision': 0.7466779718940627, 'validation/accuracy': 0.9862807989120483, 'validation/loss': 0.053458862006664276, 'validation/mean_average_precision': 0.2571797655138536, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05773406848311424, 'test/mean_average_precision': 0.24522332916190204, 'test/num_examples': 43793, 'score': 10333.258596420288, 'total_duration': 13766.4024746418, 'accumulated_submission_time': 10333.258596420288, 'accumulated_eval_time': 3430.686639547348, 'accumulated_logging_time': 1.1023828983306885}
I0305 23:02:20.033685 139984967476992 logging_writer.py:48] [49429] accumulated_eval_time=3430.69, accumulated_logging_time=1.10238, accumulated_submission_time=10333.3, global_step=49429, preemption_count=0, score=10333.3, test/accuracy=0.985395, test/loss=0.0577341, test/mean_average_precision=0.245223, test/num_examples=43793, total_duration=13766.4, train/accuracy=0.996138, train/loss=0.0132193, train/mean_average_precision=0.746678, validation/accuracy=0.986281, validation/loss=0.0534589, validation/mean_average_precision=0.25718, validation/num_examples=43793
I0305 23:02:35.092592 139984959084288 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.18296237289905548, loss=0.01943552866578102
I0305 23:02:56.455358 139984967476992 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.21355624496936798, loss=0.020850328728556633
I0305 23:03:18.086230 139984959084288 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.18608902394771576, loss=0.020574312657117844
I0305 23:03:39.423805 139984967476992 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.18206727504730225, loss=0.01961437799036503
I0305 23:04:00.774089 139984959084288 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1861790269613266, loss=0.02035003900527954
I0305 23:04:22.078342 139984967476992 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.22649869322776794, loss=0.02186748944222927
I0305 23:04:43.525772 139984959084288 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.21315898001194, loss=0.019762713462114334
I0305 23:05:04.631126 139984967476992 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.19261683523654938, loss=0.02081364206969738
I0305 23:05:25.720116 139984959084288 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.21768435835838318, loss=0.02164478600025177
I0305 23:05:47.029238 139984967476992 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.19835783541202545, loss=0.01946355402469635
I0305 23:06:07.899989 139984959084288 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.17756542563438416, loss=0.017671778798103333
I0305 23:06:20.164394 140127519114432 spec.py:321] Evaluating on the training split.
I0305 23:07:30.384898 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 23:07:32.307265 140127519114432 spec.py:349] Evaluating on the test split.
I0305 23:07:34.251382 140127519114432 submission_runner.py:469] Time since start: 14080.64s, 	Step: 50560, 	{'train/accuracy': 0.9956316947937012, 'train/loss': 0.014292573556303978, 'train/mean_average_precision': 0.7298499271560117, 'validation/accuracy': 0.9862515330314636, 'validation/loss': 0.05354971066117287, 'validation/mean_average_precision': 0.259421452879091, 'validation/num_examples': 43793, 'test/accuracy': 0.9853963255882263, 'test/loss': 0.057525504380464554, 'test/mean_average_precision': 0.24618376805988074, 'test/num_examples': 43793, 'score': 10573.349601268768, 'total_duration': 14080.635037660599, 'accumulated_submission_time': 10573.349601268768, 'accumulated_eval_time': 3504.773469686508, 'accumulated_logging_time': 1.1270251274108887}
I0305 23:07:34.266436 139984967476992 logging_writer.py:48] [50560] accumulated_eval_time=3504.77, accumulated_logging_time=1.12703, accumulated_submission_time=10573.3, global_step=50560, preemption_count=0, score=10573.3, test/accuracy=0.985396, test/loss=0.0575255, test/mean_average_precision=0.246184, test/num_examples=43793, total_duration=14080.6, train/accuracy=0.995632, train/loss=0.0142926, train/mean_average_precision=0.72985, validation/accuracy=0.986252, validation/loss=0.0535497, validation/mean_average_precision=0.259421, validation/num_examples=43793
I0305 23:07:43.108241 139984959084288 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.22960302233695984, loss=0.020758451893925667
I0305 23:08:04.417782 139984967476992 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.23481744527816772, loss=0.021275021135807037
I0305 23:08:25.508929 139984959084288 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2034350335597992, loss=0.01990623213350773
I0305 23:08:46.239665 139984967476992 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.17725539207458496, loss=0.0196550190448761
I0305 23:09:07.368678 139984959084288 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.1921505331993103, loss=0.019753072410821915
I0305 23:09:28.646574 139984967476992 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.23357194662094116, loss=0.02137603797018528
I0305 23:09:49.973153 139984959084288 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.20681758224964142, loss=0.020032690837979317
I0305 23:10:11.766742 139984967476992 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2142176777124405, loss=0.021043693646788597
I0305 23:10:32.906832 139984959084288 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.23533129692077637, loss=0.02044464461505413
I0305 23:10:54.514713 139984967476992 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2153642773628235, loss=0.02156107686460018
I0305 23:11:15.976152 139984959084288 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.21251799166202545, loss=0.02096867933869362
I0305 23:11:34.422925 140127519114432 spec.py:321] Evaluating on the training split.
I0305 23:12:45.600452 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 23:12:47.549768 140127519114432 spec.py:349] Evaluating on the test split.
I0305 23:12:49.437428 140127519114432 submission_runner.py:469] Time since start: 14395.82s, 	Step: 51687, 	{'train/accuracy': 0.9961116313934326, 'train/loss': 0.013209393247961998, 'train/mean_average_precision': 0.7614514545555104, 'validation/accuracy': 0.9863120317459106, 'validation/loss': 0.05350922793149948, 'validation/mean_average_precision': 0.2586384420729237, 'validation/num_examples': 43793, 'test/accuracy': 0.9854363203048706, 'test/loss': 0.05763426423072815, 'test/mean_average_precision': 0.2462864037886279, 'test/num_examples': 43793, 'score': 10813.460890769958, 'total_duration': 14395.821115970612, 'accumulated_submission_time': 10813.460890769958, 'accumulated_eval_time': 3579.787850379944, 'accumulated_logging_time': 1.1514432430267334}
I0305 23:12:49.452858 139984967476992 logging_writer.py:48] [51687] accumulated_eval_time=3579.79, accumulated_logging_time=1.15144, accumulated_submission_time=10813.5, global_step=51687, preemption_count=0, score=10813.5, test/accuracy=0.985436, test/loss=0.0576343, test/mean_average_precision=0.246286, test/num_examples=43793, total_duration=14395.8, train/accuracy=0.996112, train/loss=0.0132094, train/mean_average_precision=0.761451, validation/accuracy=0.986312, validation/loss=0.0535092, validation/mean_average_precision=0.258638, validation/num_examples=43793
I0305 23:12:52.439710 139984959084288 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.20043984055519104, loss=0.021686220541596413
I0305 23:13:13.840255 139984967476992 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2056742161512375, loss=0.019924797117710114
I0305 23:13:35.211755 139984959084288 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2156882882118225, loss=0.01906431093811989
I0305 23:13:56.140698 139984967476992 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.18842379748821259, loss=0.020035797730088234
I0305 23:14:17.066425 139984959084288 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.22039516270160675, loss=0.020399268716573715
I0305 23:14:38.044220 139984967476992 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.20493198931217194, loss=0.021080633625388145
I0305 23:14:59.304670 139984959084288 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1859711855649948, loss=0.01861352100968361
I0305 23:15:19.796601 139984967476992 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.19829058647155762, loss=0.01954951509833336
I0305 23:15:40.271430 139984959084288 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.20650269091129303, loss=0.019864192232489586
I0305 23:16:01.498787 139984967476992 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.20750081539154053, loss=0.020240793004631996
I0305 23:16:22.794891 139984959084288 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.20080974698066711, loss=0.020855071023106575
I0305 23:16:43.939806 139984967476992 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.20185475051403046, loss=0.02036534994840622
I0305 23:16:49.491897 140127519114432 spec.py:321] Evaluating on the training split.
I0305 23:17:58.003995 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 23:17:59.991294 140127519114432 spec.py:349] Evaluating on the test split.
I0305 23:18:01.963089 140127519114432 submission_runner.py:469] Time since start: 14708.35s, 	Step: 52827, 	{'train/accuracy': 0.9961345195770264, 'train/loss': 0.013243354856967926, 'train/mean_average_precision': 0.7565695031180117, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.25847527463974207, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.24614469571155714, 'test/num_examples': 43793, 'score': 11053.459902524948, 'total_duration': 14708.34685754776, 'accumulated_submission_time': 11053.459902524948, 'accumulated_eval_time': 3652.258996486664, 'accumulated_logging_time': 1.1765832901000977}
I0305 23:18:01.979809 139984959084288 logging_writer.py:48] [52827] accumulated_eval_time=3652.26, accumulated_logging_time=1.17658, accumulated_submission_time=11053.5, global_step=52827, preemption_count=0, score=11053.5, test/accuracy=0.985437, test/loss=0.0576213, test/mean_average_precision=0.246145, test/num_examples=43793, total_duration=14708.3, train/accuracy=0.996135, train/loss=0.0132434, train/mean_average_precision=0.75657, validation/accuracy=0.986314, validation/loss=0.0535067, validation/mean_average_precision=0.258475, validation/num_examples=43793
I0305 23:18:17.385121 139984967476992 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.1946008950471878, loss=0.019056720659136772
I0305 23:18:38.234244 139984959084288 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.18970784544944763, loss=0.01928560622036457
I0305 23:18:59.765243 139984967476992 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.18864718079566956, loss=0.0197744257748127
I0305 23:19:20.599586 139984959084288 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.2284199744462967, loss=0.021721649914979935
I0305 23:19:41.555307 139984967476992 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.20509891211986542, loss=0.018396344035863876
I0305 23:20:02.740299 139984959084288 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.24119256436824799, loss=0.020709609612822533
I0305 23:20:23.764153 139984967476992 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20724569261074066, loss=0.02055441215634346
I0305 23:20:44.477086 139984959084288 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2071559727191925, loss=0.02083583176136017
I0305 23:21:05.562130 139984967476992 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1776142418384552, loss=0.019678223878145218
I0305 23:21:26.428747 139984959084288 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.17854636907577515, loss=0.01955718733370304
I0305 23:21:47.700727 139984967476992 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.24855491518974304, loss=0.020568910986185074
I0305 23:22:02.075681 140127519114432 spec.py:321] Evaluating on the training split.
I0305 23:23:13.500285 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 23:23:15.467968 140127519114432 spec.py:349] Evaluating on the test split.
I0305 23:23:17.366565 140127519114432 submission_runner.py:469] Time since start: 15023.75s, 	Step: 53968, 	{'train/accuracy': 0.9959548711776733, 'train/loss': 0.01355651579797268, 'train/mean_average_precision': 0.7565405275635215, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.2585767907216017, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.24601231627926345, 'test/num_examples': 43793, 'score': 11293.516966342926, 'total_duration': 15023.750236988068, 'accumulated_submission_time': 11293.516966342926, 'accumulated_eval_time': 3727.549740791321, 'accumulated_logging_time': 1.2030248641967773}
I0305 23:23:17.381783 139984959084288 logging_writer.py:48] [53968] accumulated_eval_time=3727.55, accumulated_logging_time=1.20302, accumulated_submission_time=11293.5, global_step=53968, preemption_count=0, score=11293.5, test/accuracy=0.985437, test/loss=0.0576213, test/mean_average_precision=0.246012, test/num_examples=43793, total_duration=15023.8, train/accuracy=0.995955, train/loss=0.0135565, train/mean_average_precision=0.756541, validation/accuracy=0.986314, validation/loss=0.0535067, validation/mean_average_precision=0.258577, validation/num_examples=43793
I0305 23:23:24.558299 139984967476992 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.17593546211719513, loss=0.018777308985590935
I0305 23:23:46.156231 139984959084288 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.18850664794445038, loss=0.017857912927865982
I0305 23:24:07.443462 139984967476992 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.20559318363666534, loss=0.019899997860193253
I0305 23:24:29.104432 139984959084288 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.20436663925647736, loss=0.020052112638950348
I0305 23:24:50.898751 139984967476992 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.1903160810470581, loss=0.020178722217679024
I0305 23:25:12.907923 139984959084288 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.19982314109802246, loss=0.020605888217687607
I0305 23:25:34.885963 139984967476992 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.22752094268798828, loss=0.021167665719985962
I0305 23:25:56.084867 139984959084288 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.1904442310333252, loss=0.019429780542850494
I0305 23:26:17.142524 139984967476992 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1875263750553131, loss=0.0183719452470541
I0305 23:26:38.340807 139984959084288 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.19225741922855377, loss=0.02026675082743168
I0305 23:26:59.462002 139984967476992 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.19641520082950592, loss=0.020014895126223564
I0305 23:27:17.516403 140127519114432 spec.py:321] Evaluating on the training split.
I0305 23:28:30.646056 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 23:28:32.606531 140127519114432 spec.py:349] Evaluating on the test split.
I0305 23:28:34.466471 140127519114432 submission_runner.py:469] Time since start: 15340.85s, 	Step: 55087, 	{'train/accuracy': 0.9959933757781982, 'train/loss': 0.013456234708428383, 'train/mean_average_precision': 0.7487577703781363, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.2584347517802553, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.24609285532321998, 'test/num_examples': 43793, 'score': 11533.608405351639, 'total_duration': 15340.850167274475, 'accumulated_submission_time': 11533.608405351639, 'accumulated_eval_time': 3804.499694108963, 'accumulated_logging_time': 1.2273650169372559}
I0305 23:28:34.482576 139984959084288 logging_writer.py:48] [55087] accumulated_eval_time=3804.5, accumulated_logging_time=1.22737, accumulated_submission_time=11533.6, global_step=55087, preemption_count=0, score=11533.6, test/accuracy=0.985437, test/loss=0.0576213, test/mean_average_precision=0.246093, test/num_examples=43793, total_duration=15340.9, train/accuracy=0.995993, train/loss=0.0134562, train/mean_average_precision=0.748758, validation/accuracy=0.986314, validation/loss=0.0535067, validation/mean_average_precision=0.258435, validation/num_examples=43793
I0305 23:28:37.460645 139984967476992 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2074228823184967, loss=0.018391715362668037
I0305 23:28:58.265383 139984959084288 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.184441938996315, loss=0.01795930787920952
I0305 23:29:18.543039 139984967476992 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.22301898896694183, loss=0.02181304804980755
I0305 23:29:39.408115 139984959084288 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.20901693403720856, loss=0.018703488633036613
I0305 23:30:00.292399 139984967476992 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.21344274282455444, loss=0.019967902451753616
I0305 23:30:21.156857 139984959084288 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.18965327739715576, loss=0.01951196789741516
I0305 23:30:41.973565 139984967476992 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.20321562886238098, loss=0.017381099984049797
I0305 23:31:03.081783 139984959084288 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.1873190999031067, loss=0.019810212776064873
I0305 23:31:24.368480 139984967476992 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2040751576423645, loss=0.019915111362934113
I0305 23:31:45.377620 139984959084288 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.21760153770446777, loss=0.020176975056529045
I0305 23:32:06.886924 139984967476992 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.22806131839752197, loss=0.019227564334869385
I0305 23:32:27.614199 139984959084288 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.26111921668052673, loss=0.022488074377179146
I0305 23:32:34.531961 140127519114432 spec.py:321] Evaluating on the training split.
I0305 23:33:43.519076 140127519114432 spec.py:333] Evaluating on the validation split.
I0305 23:33:45.480294 140127519114432 spec.py:349] Evaluating on the test split.
I0305 23:33:47.421673 140127519114432 submission_runner.py:469] Time since start: 15653.81s, 	Step: 56235, 	{'train/accuracy': 0.9959317445755005, 'train/loss': 0.013582339510321617, 'train/mean_average_precision': 0.7446769976771346, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.2584587896799103, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.246060350537443, 'test/num_examples': 43793, 'score': 11773.617523431778, 'total_duration': 15653.805370807648, 'accumulated_submission_time': 11773.617523431778, 'accumulated_eval_time': 3877.38929104805, 'accumulated_logging_time': 1.253816843032837}
I0305 23:33:47.436931 139984967476992 logging_writer.py:48] [56235] accumulated_eval_time=3877.39, accumulated_logging_time=1.25382, accumulated_submission_time=11773.6, global_step=56235, preemption_count=0, score=11773.6, test/accuracy=0.985437, test/loss=0.0576213, test/mean_average_precision=0.24606, test/num_examples=43793, total_duration=15653.8, train/accuracy=0.995932, train/loss=0.0135823, train/mean_average_precision=0.744677, validation/accuracy=0.986314, validation/loss=0.0535067, validation/mean_average_precision=0.258459, validation/num_examples=43793
I0305 23:34:01.572226 139984959084288 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.211870014667511, loss=0.021467721089720726
I0305 23:34:22.977833 139984967476992 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.1935005933046341, loss=0.020031170919537544
I0305 23:34:44.129924 139984959084288 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.18170562386512756, loss=0.020484106615185738
I0305 23:35:05.601988 139984967476992 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.17753739655017853, loss=0.018472738564014435
I0305 23:35:27.022790 139984959084288 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.22688131034374237, loss=0.021821076050400734
I0305 23:35:48.157265 139984967476992 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.2054850161075592, loss=0.01993631385266781
I0305 23:36:09.355012 139984959084288 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.21357373893260956, loss=0.020841091871261597
I0305 23:36:30.562536 139984967476992 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21329255402088165, loss=0.019281046465039253
I0305 23:36:51.221081 139984959084288 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.21111921966075897, loss=0.02004166878759861
I0305 23:37:11.628600 139984967476992 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1829158514738083, loss=0.01925797574222088
I0305 23:37:32.626736 139984959084288 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.21074169874191284, loss=0.019457167014479637
I0305 23:37:47.527917 139984967476992 logging_writer.py:48] [57371] global_step=57371, preemption_count=0, score=12013.7
I0305 23:37:47.666368 140127519114432 submission_runner.py:646] Tuning trial 1/5
I0305 23:37:47.666543 140127519114432 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0305 23:37:47.671338 140127519114432 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4533552825450897, 'train/loss': 0.7773282527923584, 'train/mean_average_precision': 0.021668608978261672, 'validation/accuracy': 0.448541522026062, 'validation/loss': 0.780256986618042, 'validation/mean_average_precision': 0.027700106460599398, 'validation/num_examples': 43793, 'test/accuracy': 0.4468294084072113, 'test/loss': 0.780686616897583, 'test/mean_average_precision': 0.027520667620448164, 'test/num_examples': 43793, 'score': 10.762229442596436, 'total_duration': 214.87293481826782, 'accumulated_submission_time': 10.762229442596436, 'accumulated_eval_time': 204.11060094833374, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1140, {'train/accuracy': 0.9868268966674805, 'train/loss': 0.05152887850999832, 'train/mean_average_precision': 0.06714007559876531, 'validation/accuracy': 0.9842774868011475, 'validation/loss': 0.06125123053789139, 'validation/mean_average_precision': 0.06714002430027763, 'validation/num_examples': 43793, 'test/accuracy': 0.983289897441864, 'test/loss': 0.06449496001005173, 'test/mean_average_precision': 0.0674123032600958, 'test/num_examples': 43793, 'score': 250.90052938461304, 'total_duration': 531.0943324565887, 'accumulated_submission_time': 250.90052938461304, 'accumulated_eval_time': 280.14541029930115, 'accumulated_logging_time': 0.016854286193847656, 'global_step': 1140, 'preemption_count': 0}), (2278, {'train/accuracy': 0.9878330230712891, 'train/loss': 0.04347018525004387, 'train/mean_average_precision': 0.1442274770458427, 'validation/accuracy': 0.9850097894668579, 'validation/loss': 0.052855100482702255, 'validation/mean_average_precision': 0.14119393698497126, 'validation/num_examples': 43793, 'test/accuracy': 0.984050989151001, 'test/loss': 0.05584289878606796, 'test/mean_average_precision': 0.14297948004448258, 'test/num_examples': 43793, 'score': 491.04615354537964, 'total_duration': 848.390426158905, 'accumulated_submission_time': 491.04615354537964, 'accumulated_eval_time': 357.2478201389313, 'accumulated_logging_time': 0.03429007530212402, 'global_step': 2278, 'preemption_count': 0}), (3444, {'train/accuracy': 0.9882678985595703, 'train/loss': 0.04083511233329773, 'train/mean_average_precision': 0.19176204744822845, 'validation/accuracy': 0.9854303598403931, 'validation/loss': 0.05036444589495659, 'validation/mean_average_precision': 0.17322428590639102, 'validation/num_examples': 43793, 'test/accuracy': 0.9845210313796997, 'test/loss': 0.05312596634030342, 'test/mean_average_precision': 0.1704825687669865, 'test/num_examples': 43793, 'score': 731.1531708240509, 'total_duration': 1164.4773681163788, 'accumulated_submission_time': 731.1531708240509, 'accumulated_eval_time': 433.17888736724854, 'accumulated_logging_time': 0.05198025703430176, 'global_step': 3444, 'preemption_count': 0}), (4621, {'train/accuracy': 0.9884931445121765, 'train/loss': 0.03955801576375961, 'train/mean_average_precision': 0.20979450904212815, 'validation/accuracy': 0.9857173562049866, 'validation/loss': 0.04866034537553787, 'validation/mean_average_precision': 0.18585047100630053, 'validation/num_examples': 43793, 'test/accuracy': 0.9848702549934387, 'test/loss': 0.05123370513319969, 'test/mean_average_precision': 0.18984949184540242, 'test/num_examples': 43793, 'score': 971.2248680591583, 'total_duration': 1480.6959273815155, 'accumulated_submission_time': 971.2248680591583, 'accumulated_eval_time': 509.2780182361603, 'accumulated_logging_time': 0.07073760032653809, 'global_step': 4621, 'preemption_count': 0}), (5780, {'train/accuracy': 0.9887377023696899, 'train/loss': 0.038263872265815735, 'train/mean_average_precision': 0.25242484485758127, 'validation/accuracy': 0.9857810735702515, 'validation/loss': 0.04816998168826103, 'validation/mean_average_precision': 0.2077936551110407, 'validation/num_examples': 43793, 'test/accuracy': 0.984925389289856, 'test/loss': 0.050946492701768875, 'test/mean_average_precision': 0.205052731479209, 'test/num_examples': 43793, 'score': 1211.2083296775818, 'total_duration': 1794.8235669136047, 'accumulated_submission_time': 1211.2083296775818, 'accumulated_eval_time': 583.3696949481964, 'accumulated_logging_time': 0.09019660949707031, 'global_step': 5780, 'preemption_count': 0}), (6933, {'train/accuracy': 0.9890442490577698, 'train/loss': 0.037319764494895935, 'train/mean_average_precision': 0.2524225965293105, 'validation/accuracy': 0.9861577749252319, 'validation/loss': 0.04681247100234032, 'validation/mean_average_precision': 0.21435507753131103, 'validation/num_examples': 43793, 'test/accuracy': 0.985277533531189, 'test/loss': 0.04920097440481186, 'test/mean_average_precision': 0.21803274330582847, 'test/num_examples': 43793, 'score': 1451.166642665863, 'total_duration': 2108.6690390110016, 'accumulated_submission_time': 1451.166642665863, 'accumulated_eval_time': 657.2061231136322, 'accumulated_logging_time': 0.10887551307678223, 'global_step': 6933, 'preemption_count': 0}), (8098, {'train/accuracy': 0.9895936846733093, 'train/loss': 0.03538041189312935, 'train/mean_average_precision': 0.2910443609191573, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.04580126702785492, 'validation/mean_average_precision': 0.23256951116331034, 'validation/num_examples': 43793, 'test/accuracy': 0.9854413866996765, 'test/loss': 0.048324596136808395, 'test/mean_average_precision': 0.22333050090121923, 'test/num_examples': 43793, 'score': 1691.2323768138885, 'total_duration': 2423.8861384391785, 'accumulated_submission_time': 1691.2323768138885, 'accumulated_eval_time': 732.3043532371521, 'accumulated_logging_time': 0.1295475959777832, 'global_step': 8098, 'preemption_count': 0}), (9252, {'train/accuracy': 0.9897444248199463, 'train/loss': 0.034677185118198395, 'train/mean_average_precision': 0.30095830325318246, 'validation/accuracy': 0.9864492416381836, 'validation/loss': 0.04539342597126961, 'validation/mean_average_precision': 0.23260019348985556, 'validation/num_examples': 43793, 'test/accuracy': 0.9855824708938599, 'test/loss': 0.04789038747549057, 'test/mean_average_precision': 0.22779992872190927, 'test/num_examples': 43793, 'score': 1931.3781192302704, 'total_duration': 2741.8279597759247, 'accumulated_submission_time': 1931.3781192302704, 'accumulated_eval_time': 810.0505945682526, 'accumulated_logging_time': 0.1483473777770996, 'global_step': 9252, 'preemption_count': 0}), (10422, {'train/accuracy': 0.9899512529373169, 'train/loss': 0.03357997164130211, 'train/mean_average_precision': 0.33839680296499514, 'validation/accuracy': 0.9864715933799744, 'validation/loss': 0.045309219509363174, 'validation/mean_average_precision': 0.23814975856455584, 'validation/num_examples': 43793, 'test/accuracy': 0.9857105016708374, 'test/loss': 0.04773522913455963, 'test/mean_average_precision': 0.2397938027003563, 'test/num_examples': 43793, 'score': 2171.4511201381683, 'total_duration': 3056.9107139110565, 'accumulated_submission_time': 2171.4511201381683, 'accumulated_eval_time': 885.0073554515839, 'accumulated_logging_time': 0.16663169860839844, 'global_step': 10422, 'preemption_count': 0}), (11597, {'train/accuracy': 0.9900012612342834, 'train/loss': 0.03328365087509155, 'train/mean_average_precision': 0.35223851977347453, 'validation/accuracy': 0.9865710139274597, 'validation/loss': 0.045056186616420746, 'validation/mean_average_precision': 0.24086969721731705, 'validation/num_examples': 43793, 'test/accuracy': 0.9857231378555298, 'test/loss': 0.04776609688997269, 'test/mean_average_precision': 0.242269357819331, 'test/num_examples': 43793, 'score': 2411.444804430008, 'total_duration': 3371.7409329414368, 'accumulated_submission_time': 2411.444804430008, 'accumulated_eval_time': 959.7899127006531, 'accumulated_logging_time': 0.18727874755859375, 'global_step': 11597, 'preemption_count': 0}), (12758, {'train/accuracy': 0.9904212355613708, 'train/loss': 0.031753815710544586, 'train/mean_average_precision': 0.36642807074573713, 'validation/accuracy': 0.9866436719894409, 'validation/loss': 0.04474074766039848, 'validation/mean_average_precision': 0.2493593571368245, 'validation/num_examples': 43793, 'test/accuracy': 0.9857656955718994, 'test/loss': 0.047649361193180084, 'test/mean_average_precision': 0.23719579188298698, 'test/num_examples': 43793, 'score': 2651.5743041038513, 'total_duration': 3687.265855550766, 'accumulated_submission_time': 2651.5743041038513, 'accumulated_eval_time': 1035.1336002349854, 'accumulated_logging_time': 0.20632481575012207, 'global_step': 12758, 'preemption_count': 0}), (13914, {'train/accuracy': 0.9902939796447754, 'train/loss': 0.03219129517674446, 'train/mean_average_precision': 0.36750967032591414, 'validation/accuracy': 0.9867833256721497, 'validation/loss': 0.044609151780605316, 'validation/mean_average_precision': 0.25819032223946725, 'validation/num_examples': 43793, 'test/accuracy': 0.9859147667884827, 'test/loss': 0.04720975086092949, 'test/mean_average_precision': 0.25917715213760234, 'test/num_examples': 43793, 'score': 2891.692756175995, 'total_duration': 4001.597636461258, 'accumulated_submission_time': 2891.692756175995, 'accumulated_eval_time': 1109.2941818237305, 'accumulated_logging_time': 0.2255704402923584, 'global_step': 13914, 'preemption_count': 0}), (15073, {'train/accuracy': 0.9906931519508362, 'train/loss': 0.030546216294169426, 'train/mean_average_precision': 0.4021203702152252, 'validation/accuracy': 0.9867151379585266, 'validation/loss': 0.04461422190070152, 'validation/mean_average_precision': 0.25678019209070935, 'validation/num_examples': 43793, 'test/accuracy': 0.9858583807945251, 'test/loss': 0.047217756509780884, 'test/mean_average_precision': 0.24882538075651361, 'test/num_examples': 43793, 'score': 3131.734242916107, 'total_duration': 4316.502843618393, 'accumulated_submission_time': 3131.734242916107, 'accumulated_eval_time': 1184.109183549881, 'accumulated_logging_time': 0.24453067779541016, 'global_step': 15073, 'preemption_count': 0}), (16208, {'train/accuracy': 0.9906432032585144, 'train/loss': 0.031065478920936584, 'train/mean_average_precision': 0.39303610163280056, 'validation/accuracy': 0.9867350459098816, 'validation/loss': 0.04435378313064575, 'validation/mean_average_precision': 0.2675796817471025, 'validation/num_examples': 43793, 'test/accuracy': 0.9859114289283752, 'test/loss': 0.047142524272203445, 'test/mean_average_precision': 0.2543949258199262, 'test/num_examples': 43793, 'score': 3371.720043897629, 'total_duration': 4629.707128286362, 'accumulated_submission_time': 3371.720043897629, 'accumulated_eval_time': 1257.2761113643646, 'accumulated_logging_time': 0.26519107818603516, 'global_step': 16208, 'preemption_count': 0}), (17351, {'train/accuracy': 0.9909688830375671, 'train/loss': 0.029408328235149384, 'train/mean_average_precision': 0.43705568752259816, 'validation/accuracy': 0.9868064522743225, 'validation/loss': 0.0445091649889946, 'validation/mean_average_precision': 0.2664873621912444, 'validation/num_examples': 43793, 'test/accuracy': 0.9859514236450195, 'test/loss': 0.04732939973473549, 'test/mean_average_precision': 0.2582068606871591, 'test/num_examples': 43793, 'score': 3611.853848695755, 'total_duration': 4942.64346575737, 'accumulated_submission_time': 3611.853848695755, 'accumulated_eval_time': 1330.0252051353455, 'accumulated_logging_time': 0.28485870361328125, 'global_step': 17351, 'preemption_count': 0}), (18502, {'train/accuracy': 0.990720272064209, 'train/loss': 0.030478470027446747, 'train/mean_average_precision': 0.4041112308421484, 'validation/accuracy': 0.9868020415306091, 'validation/loss': 0.044579531997442245, 'validation/mean_average_precision': 0.2559727922854657, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.04730149358510971, 'test/mean_average_precision': 0.25856812127632606, 'test/num_examples': 43793, 'score': 3851.997809410095, 'total_duration': 5259.272873401642, 'accumulated_submission_time': 3851.997809410095, 'accumulated_eval_time': 1406.4581809043884, 'accumulated_logging_time': 0.30603814125061035, 'global_step': 18502, 'preemption_count': 0}), (19659, {'train/accuracy': 0.9911894798278809, 'train/loss': 0.028543828055262566, 'train/mean_average_precision': 0.4433005545019544, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.04503616318106651, 'validation/mean_average_precision': 0.2606181092650766, 'validation/num_examples': 43793, 'test/accuracy': 0.9858912229537964, 'test/loss': 0.04783489555120468, 'test/mean_average_precision': 0.25853764723846423, 'test/num_examples': 43793, 'score': 4091.9764292240143, 'total_duration': 5575.704982995987, 'accumulated_submission_time': 4091.9764292240143, 'accumulated_eval_time': 1482.8607847690582, 'accumulated_logging_time': 0.32793474197387695, 'global_step': 19659, 'preemption_count': 0}), (20815, {'train/accuracy': 0.991061806678772, 'train/loss': 0.029439395293593407, 'train/mean_average_precision': 0.42460744742915035, 'validation/accuracy': 0.9866542816162109, 'validation/loss': 0.04446346312761307, 'validation/mean_average_precision': 0.2615110946879504, 'validation/num_examples': 43793, 'test/accuracy': 0.9858777523040771, 'test/loss': 0.046956129372119904, 'test/mean_average_precision': 0.25924808490262014, 'test/num_examples': 43793, 'score': 4331.9673771858215, 'total_duration': 5889.291003704071, 'accumulated_submission_time': 4331.9673771858215, 'accumulated_eval_time': 1556.4035575389862, 'accumulated_logging_time': 0.3504455089569092, 'global_step': 20815, 'preemption_count': 0}), (21973, {'train/accuracy': 0.9916960597038269, 'train/loss': 0.027112770825624466, 'train/mean_average_precision': 0.4854680093744215, 'validation/accuracy': 0.9869307279586792, 'validation/loss': 0.04433503374457359, 'validation/mean_average_precision': 0.26973960357351057, 'validation/num_examples': 43793, 'test/accuracy': 0.9859400391578674, 'test/loss': 0.04721763730049133, 'test/mean_average_precision': 0.25739433278435203, 'test/num_examples': 43793, 'score': 4572.100345849991, 'total_duration': 6203.16002869606, 'accumulated_submission_time': 4572.100345849991, 'accumulated_eval_time': 1630.0875265598297, 'accumulated_logging_time': 0.3714566230773926, 'global_step': 21973, 'preemption_count': 0}), (23128, {'train/accuracy': 0.9912074208259583, 'train/loss': 0.02878197282552719, 'train/mean_average_precision': 0.4538782150338502, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.04452703893184662, 'validation/mean_average_precision': 0.26088366105260874, 'validation/num_examples': 43793, 'test/accuracy': 0.9858996272087097, 'test/loss': 0.04724275320768356, 'test/mean_average_precision': 0.2626329188820472, 'test/num_examples': 43793, 'score': 4812.2043607234955, 'total_duration': 6520.318869590759, 'accumulated_submission_time': 4812.2043607234955, 'accumulated_eval_time': 1707.0896158218384, 'accumulated_logging_time': 0.3926372528076172, 'global_step': 23128, 'preemption_count': 0}), (24269, {'train/accuracy': 0.9919978380203247, 'train/loss': 0.02638961747288704, 'train/mean_average_precision': 0.5011742525555465, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04433467984199524, 'validation/mean_average_precision': 0.27027066765630403, 'validation/num_examples': 43793, 'test/accuracy': 0.9860007166862488, 'test/loss': 0.04699287191033363, 'test/mean_average_precision': 0.26516535518521295, 'test/num_examples': 43793, 'score': 5052.228254318237, 'total_duration': 6831.660053730011, 'accumulated_submission_time': 5052.228254318237, 'accumulated_eval_time': 1778.355131149292, 'accumulated_logging_time': 0.4137258529663086, 'global_step': 24269, 'preemption_count': 0}), (25415, {'train/accuracy': 0.9915741086006165, 'train/loss': 0.027419302612543106, 'train/mean_average_precision': 0.46642887108285436, 'validation/accuracy': 0.98691326379776, 'validation/loss': 0.04444209858775139, 'validation/mean_average_precision': 0.2751171298447432, 'validation/num_examples': 43793, 'test/accuracy': 0.9860453605651855, 'test/loss': 0.047338008880615234, 'test/mean_average_precision': 0.2656707282055037, 'test/num_examples': 43793, 'score': 5292.303888320923, 'total_duration': 7150.31390953064, 'accumulated_submission_time': 5292.303888320923, 'accumulated_eval_time': 1856.8792922496796, 'accumulated_logging_time': 0.43389081954956055, 'global_step': 25415, 'preemption_count': 0}), (26554, {'train/accuracy': 0.9919848442077637, 'train/loss': 0.026217423379421234, 'train/mean_average_precision': 0.5102128645234757, 'validation/accuracy': 0.9868884682655334, 'validation/loss': 0.044460564851760864, 'validation/mean_average_precision': 0.26955390640106985, 'validation/num_examples': 43793, 'test/accuracy': 0.9860537648200989, 'test/loss': 0.04739837720990181, 'test/mean_average_precision': 0.256179155282119, 'test/num_examples': 43793, 'score': 5532.311073303223, 'total_duration': 7465.774094820023, 'accumulated_submission_time': 5532.311073303223, 'accumulated_eval_time': 1932.279062986374, 'accumulated_logging_time': 0.45673060417175293, 'global_step': 26554, 'preemption_count': 0}), (27695, {'train/accuracy': 0.9918943047523499, 'train/loss': 0.026157131418585777, 'train/mean_average_precision': 0.5005074876569646, 'validation/accuracy': 0.9869453310966492, 'validation/loss': 0.044807810336351395, 'validation/mean_average_precision': 0.2779499341489269, 'validation/num_examples': 43793, 'test/accuracy': 0.9860626459121704, 'test/loss': 0.047654103487730026, 'test/mean_average_precision': 0.267730828385846, 'test/num_examples': 43793, 'score': 5772.38810634613, 'total_duration': 7781.683296918869, 'accumulated_submission_time': 5772.38810634613, 'accumulated_eval_time': 2008.0590863227844, 'accumulated_logging_time': 0.4780416488647461, 'global_step': 27695, 'preemption_count': 0}), (28836, {'train/accuracy': 0.9921720623970032, 'train/loss': 0.02557559125125408, 'train/mean_average_precision': 0.5163926469386997, 'validation/accuracy': 0.9868645071983337, 'validation/loss': 0.04469155892729759, 'validation/mean_average_precision': 0.27169798276187945, 'validation/num_examples': 43793, 'test/accuracy': 0.9860491752624512, 'test/loss': 0.047639425843954086, 'test/mean_average_precision': 0.26144788353039583, 'test/num_examples': 43793, 'score': 6012.268154382706, 'total_duration': 8095.824626684189, 'accumulated_submission_time': 6012.268154382706, 'accumulated_eval_time': 2082.0848817825317, 'accumulated_logging_time': 0.6821310520172119, 'global_step': 28836, 'preemption_count': 0}), (29968, {'train/accuracy': 0.9922459125518799, 'train/loss': 0.024941151961684227, 'train/mean_average_precision': 0.5425853272372012, 'validation/accuracy': 0.9869444966316223, 'validation/loss': 0.04503217339515686, 'validation/mean_average_precision': 0.27545535377543634, 'validation/num_examples': 43793, 'test/accuracy': 0.98606938123703, 'test/loss': 0.04790951684117317, 'test/mean_average_precision': 0.2620476067839077, 'test/num_examples': 43793, 'score': 6252.336189508438, 'total_duration': 8410.829968690872, 'accumulated_submission_time': 6252.336189508438, 'accumulated_eval_time': 2156.9675409793854, 'accumulated_logging_time': 0.7037742137908936, 'global_step': 29968, 'preemption_count': 0}), (31110, {'train/accuracy': 0.9921659231185913, 'train/loss': 0.02535690739750862, 'train/mean_average_precision': 0.5160200076079325, 'validation/accuracy': 0.9869173169136047, 'validation/loss': 0.044967345893383026, 'validation/mean_average_precision': 0.27727260717811875, 'validation/num_examples': 43793, 'test/accuracy': 0.9860209226608276, 'test/loss': 0.04813773185014725, 'test/mean_average_precision': 0.2654290630703528, 'test/num_examples': 43793, 'score': 6492.335484266281, 'total_duration': 8727.075937509537, 'accumulated_submission_time': 6492.335484266281, 'accumulated_eval_time': 2233.1618144512177, 'accumulated_logging_time': 0.7262639999389648, 'global_step': 31110, 'preemption_count': 0}), (32265, {'train/accuracy': 0.9928392767906189, 'train/loss': 0.023082315921783447, 'train/mean_average_precision': 0.5608201979317712, 'validation/accuracy': 0.9868758916854858, 'validation/loss': 0.0453580878674984, 'validation/mean_average_precision': 0.27498988713130557, 'validation/num_examples': 43793, 'test/accuracy': 0.9859960675239563, 'test/loss': 0.048615049570798874, 'test/mean_average_precision': 0.2650522464492782, 'test/num_examples': 43793, 'score': 6732.385160684586, 'total_duration': 9036.791203737259, 'accumulated_submission_time': 6732.385160684586, 'accumulated_eval_time': 2302.772072315216, 'accumulated_logging_time': 0.7477900981903076, 'global_step': 32265, 'preemption_count': 0}), (33399, {'train/accuracy': 0.9926450252532959, 'train/loss': 0.02364608645439148, 'train/mean_average_precision': 0.5588305319227906, 'validation/accuracy': 0.9868202805519104, 'validation/loss': 0.04571348428726196, 'validation/mean_average_precision': 0.2768562846424662, 'validation/num_examples': 43793, 'test/accuracy': 0.9859421849250793, 'test/loss': 0.04902222752571106, 'test/mean_average_precision': 0.25715969436367253, 'test/num_examples': 43793, 'score': 6972.485014915466, 'total_duration': 9354.476155757904, 'accumulated_submission_time': 6972.485014915466, 'accumulated_eval_time': 2380.3009345531464, 'accumulated_logging_time': 0.7705228328704834, 'global_step': 33399, 'preemption_count': 0}), (34541, {'train/accuracy': 0.9934738874435425, 'train/loss': 0.02116295136511326, 'train/mean_average_precision': 0.6161199942723665, 'validation/accuracy': 0.9868738651275635, 'validation/loss': 0.046113163232803345, 'validation/mean_average_precision': 0.2743765303671988, 'validation/num_examples': 43793, 'test/accuracy': 0.9860247373580933, 'test/loss': 0.04923577234148979, 'test/mean_average_precision': 0.26699488233620927, 'test/num_examples': 43793, 'score': 7212.458361625671, 'total_duration': 9668.971990823746, 'accumulated_submission_time': 7212.458361625671, 'accumulated_eval_time': 2454.7712993621826, 'accumulated_logging_time': 0.7925794124603271, 'global_step': 34541, 'preemption_count': 0}), (35686, {'train/accuracy': 0.9930078983306885, 'train/loss': 0.022653525695204735, 'train/mean_average_precision': 0.5737253428167677, 'validation/accuracy': 0.9867066144943237, 'validation/loss': 0.046449627727270126, 'validation/mean_average_precision': 0.2684280875258214, 'validation/num_examples': 43793, 'test/accuracy': 0.9857656955718994, 'test/loss': 0.04978271201252937, 'test/mean_average_precision': 0.25721039436428267, 'test/num_examples': 43793, 'score': 7452.527245044708, 'total_duration': 9983.207334041595, 'accumulated_submission_time': 7452.527245044708, 'accumulated_eval_time': 2528.8849728107452, 'accumulated_logging_time': 0.8157484531402588, 'global_step': 35686, 'preemption_count': 0}), (36830, {'train/accuracy': 0.9942060708999634, 'train/loss': 0.019067538902163506, 'train/mean_average_precision': 0.6528371482783737, 'validation/accuracy': 0.9868312478065491, 'validation/loss': 0.0466800257563591, 'validation/mean_average_precision': 0.2752563377830114, 'validation/num_examples': 43793, 'test/accuracy': 0.9858655333518982, 'test/loss': 0.050164829939603806, 'test/mean_average_precision': 0.2595261167050926, 'test/num_examples': 43793, 'score': 7692.550749540329, 'total_duration': 10296.32037472725, 'accumulated_submission_time': 7692.550749540329, 'accumulated_eval_time': 2601.9214272499084, 'accumulated_logging_time': 0.8375959396362305, 'global_step': 36830, 'preemption_count': 0}), (37998, {'train/accuracy': 0.993248462677002, 'train/loss': 0.021436164155602455, 'train/mean_average_precision': 0.5860496611746411, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.04749639704823494, 'validation/mean_average_precision': 0.26897217019881975, 'validation/num_examples': 43793, 'test/accuracy': 0.9858364462852478, 'test/loss': 0.050679318606853485, 'test/mean_average_precision': 0.26110510388091945, 'test/num_examples': 43793, 'score': 7932.5567219257355, 'total_duration': 10609.09031033516, 'accumulated_submission_time': 7932.5567219257355, 'accumulated_eval_time': 2674.633913755417, 'accumulated_logging_time': 0.8605086803436279, 'global_step': 37998, 'preemption_count': 0}), (39142, {'train/accuracy': 0.9945171475410461, 'train/loss': 0.017819788306951523, 'train/mean_average_precision': 0.6816268024683101, 'validation/accuracy': 0.9867541193962097, 'validation/loss': 0.047857433557510376, 'validation/mean_average_precision': 0.2713309631417209, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.05174333229660988, 'test/mean_average_precision': 0.2573115381451356, 'test/num_examples': 43793, 'score': 8172.640521526337, 'total_duration': 10924.717975616455, 'accumulated_submission_time': 8172.640521526337, 'accumulated_eval_time': 2750.118423938751, 'accumulated_logging_time': 0.884434700012207, 'global_step': 39142, 'preemption_count': 0}), (40287, {'train/accuracy': 0.99378901720047, 'train/loss': 0.019771583378314972, 'train/mean_average_precision': 0.6244414242355489, 'validation/accuracy': 0.9865612983703613, 'validation/loss': 0.04878024384379387, 'validation/mean_average_precision': 0.26757761482970654, 'validation/num_examples': 43793, 'test/accuracy': 0.9857273697853088, 'test/loss': 0.05246104300022125, 'test/mean_average_precision': 0.25543275062509974, 'test/num_examples': 43793, 'score': 8412.61855840683, 'total_duration': 11236.807277679443, 'accumulated_submission_time': 8412.61855840683, 'accumulated_eval_time': 2822.176558494568, 'accumulated_logging_time': 0.9071769714355469, 'global_step': 40287, 'preemption_count': 0}), (41428, {'train/accuracy': 0.9951443076133728, 'train/loss': 0.016101926565170288, 'train/mean_average_precision': 0.7086035571904423, 'validation/accuracy': 0.9865174293518066, 'validation/loss': 0.04976053908467293, 'validation/mean_average_precision': 0.26367705348263387, 'validation/num_examples': 43793, 'test/accuracy': 0.9856621026992798, 'test/loss': 0.05334242433309555, 'test/mean_average_precision': 0.253602397323489, 'test/num_examples': 43793, 'score': 8652.705162286758, 'total_duration': 11555.584413528442, 'accumulated_submission_time': 8652.705162286758, 'accumulated_eval_time': 2900.8102271556854, 'accumulated_logging_time': 0.9302163124084473, 'global_step': 41428, 'preemption_count': 0}), (42575, {'train/accuracy': 0.9942857027053833, 'train/loss': 0.01802401803433895, 'train/mean_average_precision': 0.6771790066359358, 'validation/accuracy': 0.9865283966064453, 'validation/loss': 0.05046654865145683, 'validation/mean_average_precision': 0.2637375602827192, 'validation/num_examples': 43793, 'test/accuracy': 0.9857020974159241, 'test/loss': 0.05431224778294563, 'test/mean_average_precision': 0.2537504972470193, 'test/num_examples': 43793, 'score': 8892.839651107788, 'total_duration': 11870.986486673355, 'accumulated_submission_time': 8892.839651107788, 'accumulated_eval_time': 2976.023154258728, 'accumulated_logging_time': 0.9544816017150879, 'global_step': 42575, 'preemption_count': 0}), (43708, {'train/accuracy': 0.995294451713562, 'train/loss': 0.015386600978672504, 'train/mean_average_precision': 0.7130191115419166, 'validation/accuracy': 0.9864017367362976, 'validation/loss': 0.05124441906809807, 'validation/mean_average_precision': 0.25821619531950285, 'validation/num_examples': 43793, 'test/accuracy': 0.9855639338493347, 'test/loss': 0.055325426161289215, 'test/mean_average_precision': 0.2517867768552215, 'test/num_examples': 43793, 'score': 9132.895245552063, 'total_duration': 12189.200589179993, 'accumulated_submission_time': 9132.895245552063, 'accumulated_eval_time': 3054.126108646393, 'accumulated_logging_time': 0.9808549880981445, 'global_step': 43708, 'preemption_count': 0}), (44847, {'train/accuracy': 0.9947444796562195, 'train/loss': 0.016666201874613762, 'train/mean_average_precision': 0.6781362395343611, 'validation/accuracy': 0.9863173365592957, 'validation/loss': 0.051825739443302155, 'validation/mean_average_precision': 0.25895351871184746, 'validation/num_examples': 43793, 'test/accuracy': 0.985499918460846, 'test/loss': 0.0556478388607502, 'test/mean_average_precision': 0.25222121488484445, 'test/num_examples': 43793, 'score': 9373.012095928192, 'total_duration': 12501.501206874847, 'accumulated_submission_time': 9373.012095928192, 'accumulated_eval_time': 3126.251916408539, 'accumulated_logging_time': 1.0073065757751465, 'global_step': 44847, 'preemption_count': 0}), (46000, {'train/accuracy': 0.9954876899719238, 'train/loss': 0.014756465330719948, 'train/mean_average_precision': 0.7255885965050961, 'validation/accuracy': 0.9863587021827698, 'validation/loss': 0.052331190556287766, 'validation/mean_average_precision': 0.26217852946610726, 'validation/num_examples': 43793, 'test/accuracy': 0.9854245185852051, 'test/loss': 0.05650071054697037, 'test/mean_average_precision': 0.2469669993461553, 'test/num_examples': 43793, 'score': 9613.08573937416, 'total_duration': 12821.671533346176, 'accumulated_submission_time': 9613.08573937416, 'accumulated_eval_time': 3206.2957735061646, 'accumulated_logging_time': 1.0298819541931152, 'global_step': 46000, 'preemption_count': 0}), (47148, {'train/accuracy': 0.9951308369636536, 'train/loss': 0.015495703555643559, 'train/mean_average_precision': 0.7276559388006254, 'validation/accuracy': 0.9862426519393921, 'validation/loss': 0.05307828262448311, 'validation/mean_average_precision': 0.2546053847921652, 'validation/num_examples': 43793, 'test/accuracy': 0.9853609204292297, 'test/loss': 0.05728951841592789, 'test/mean_average_precision': 0.24394026416339362, 'test/num_examples': 43793, 'score': 9853.194214105606, 'total_duration': 13136.681802988052, 'accumulated_submission_time': 9853.194214105606, 'accumulated_eval_time': 3281.1405560970306, 'accumulated_logging_time': 1.05521559715271, 'global_step': 47148, 'preemption_count': 0}), (48292, {'train/accuracy': 0.995542585849762, 'train/loss': 0.014487281441688538, 'train/mean_average_precision': 0.7343132507814407, 'validation/accuracy': 0.9862040877342224, 'validation/loss': 0.05333206057548523, 'validation/mean_average_precision': 0.2578803043521914, 'validation/num_examples': 43793, 'test/accuracy': 0.985332727432251, 'test/loss': 0.05723022669553757, 'test/mean_average_precision': 0.2495107846766161, 'test/num_examples': 43793, 'score': 10093.16878080368, 'total_duration': 13448.258479595184, 'accumulated_submission_time': 10093.16878080368, 'accumulated_eval_time': 3352.690266609192, 'accumulated_logging_time': 1.0784709453582764, 'global_step': 48292, 'preemption_count': 0}), (49429, {'train/accuracy': 0.9961375594139099, 'train/loss': 0.013219335116446018, 'train/mean_average_precision': 0.7466779718940627, 'validation/accuracy': 0.9862807989120483, 'validation/loss': 0.053458862006664276, 'validation/mean_average_precision': 0.2571797655138536, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05773406848311424, 'test/mean_average_precision': 0.24522332916190204, 'test/num_examples': 43793, 'score': 10333.258596420288, 'total_duration': 13766.4024746418, 'accumulated_submission_time': 10333.258596420288, 'accumulated_eval_time': 3430.686639547348, 'accumulated_logging_time': 1.1023828983306885, 'global_step': 49429, 'preemption_count': 0}), (50560, {'train/accuracy': 0.9956316947937012, 'train/loss': 0.014292573556303978, 'train/mean_average_precision': 0.7298499271560117, 'validation/accuracy': 0.9862515330314636, 'validation/loss': 0.05354971066117287, 'validation/mean_average_precision': 0.259421452879091, 'validation/num_examples': 43793, 'test/accuracy': 0.9853963255882263, 'test/loss': 0.057525504380464554, 'test/mean_average_precision': 0.24618376805988074, 'test/num_examples': 43793, 'score': 10573.349601268768, 'total_duration': 14080.635037660599, 'accumulated_submission_time': 10573.349601268768, 'accumulated_eval_time': 3504.773469686508, 'accumulated_logging_time': 1.1270251274108887, 'global_step': 50560, 'preemption_count': 0}), (51687, {'train/accuracy': 0.9961116313934326, 'train/loss': 0.013209393247961998, 'train/mean_average_precision': 0.7614514545555104, 'validation/accuracy': 0.9863120317459106, 'validation/loss': 0.05350922793149948, 'validation/mean_average_precision': 0.2586384420729237, 'validation/num_examples': 43793, 'test/accuracy': 0.9854363203048706, 'test/loss': 0.05763426423072815, 'test/mean_average_precision': 0.2462864037886279, 'test/num_examples': 43793, 'score': 10813.460890769958, 'total_duration': 14395.821115970612, 'accumulated_submission_time': 10813.460890769958, 'accumulated_eval_time': 3579.787850379944, 'accumulated_logging_time': 1.1514432430267334, 'global_step': 51687, 'preemption_count': 0}), (52827, {'train/accuracy': 0.9961345195770264, 'train/loss': 0.013243354856967926, 'train/mean_average_precision': 0.7565695031180117, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.25847527463974207, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.24614469571155714, 'test/num_examples': 43793, 'score': 11053.459902524948, 'total_duration': 14708.34685754776, 'accumulated_submission_time': 11053.459902524948, 'accumulated_eval_time': 3652.258996486664, 'accumulated_logging_time': 1.1765832901000977, 'global_step': 52827, 'preemption_count': 0}), (53968, {'train/accuracy': 0.9959548711776733, 'train/loss': 0.01355651579797268, 'train/mean_average_precision': 0.7565405275635215, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.2585767907216017, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.24601231627926345, 'test/num_examples': 43793, 'score': 11293.516966342926, 'total_duration': 15023.750236988068, 'accumulated_submission_time': 11293.516966342926, 'accumulated_eval_time': 3727.549740791321, 'accumulated_logging_time': 1.2030248641967773, 'global_step': 53968, 'preemption_count': 0}), (55087, {'train/accuracy': 0.9959933757781982, 'train/loss': 0.013456234708428383, 'train/mean_average_precision': 0.7487577703781363, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.2584347517802553, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.24609285532321998, 'test/num_examples': 43793, 'score': 11533.608405351639, 'total_duration': 15340.850167274475, 'accumulated_submission_time': 11533.608405351639, 'accumulated_eval_time': 3804.499694108963, 'accumulated_logging_time': 1.2273650169372559, 'global_step': 55087, 'preemption_count': 0}), (56235, {'train/accuracy': 0.9959317445755005, 'train/loss': 0.013582339510321617, 'train/mean_average_precision': 0.7446769976771346, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053506653755903244, 'validation/mean_average_precision': 0.2584587896799103, 'validation/num_examples': 43793, 'test/accuracy': 0.9854371547698975, 'test/loss': 0.05762132629752159, 'test/mean_average_precision': 0.246060350537443, 'test/num_examples': 43793, 'score': 11773.617523431778, 'total_duration': 15653.805370807648, 'accumulated_submission_time': 11773.617523431778, 'accumulated_eval_time': 3877.38929104805, 'accumulated_logging_time': 1.253816843032837, 'global_step': 56235, 'preemption_count': 0})], 'global_step': 57371}
I0305 23:37:47.671478 140127519114432 submission_runner.py:649] Timing: 12013.658044815063
I0305 23:37:47.671525 140127519114432 submission_runner.py:651] Total number of evals: 50
I0305 23:37:47.671558 140127519114432 submission_runner.py:652] ====================
I0305 23:37:47.671939 140127519114432 submission_runner.py:750] Final ogbg score: 0
