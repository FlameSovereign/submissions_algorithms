python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-1976057248 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-12-36.log
2025-03-05 19:12:37.398250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201957.421573       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201957.428446       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:43.603279 139980287489216 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax.
I0305 19:12:44.526005 139980287489216 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:44.528983 139980287489216 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:44.530639 139980287489216 submission_runner.py:606] Using RNG seed -1976057248
I0305 19:12:45.128740 139980287489216 submission_runner.py:615] --- Tuning run 5/5 ---
I0305 19:12:45.128935 139980287489216 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_5.
I0305 19:12:45.129123 139980287489216 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_5/hparams.json.
I0305 19:12:45.363439 139980287489216 submission_runner.py:218] Initializing dataset.
I0305 19:12:45.584611 139980287489216 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:45.624378 139980287489216 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:12:45.866726 139980287489216 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:12:45.922815 139980287489216 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:45.955195 139980287489216 submission_runner.py:229] Initializing model.
I0305 19:12:54.335939 139980287489216 submission_runner.py:272] Initializing optimizer.
I0305 19:12:54.755026 139980287489216 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:54.755265 139980287489216 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:54.756049 139980287489216 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_5 with prefix checkpoint_
I0305 19:12:54.756157 139980287489216 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_5/meta_data_0.json.
I0305 19:12:54.756313 139980287489216 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:54.756360 139980287489216 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:54.911036 139980287489216 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_5/flags_0.json.
I0305 19:12:54.943622 139980287489216 submission_runner.py:337] Starting training loop.
I0305 19:13:10.388492 139844107429632 logging_writer.py:48] [0] global_step=0, grad_norm=2.1365857124328613, loss=0.7726348042488098
I0305 19:13:10.442656 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:13:10.446442 139980287489216 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:10.450054 139980287489216 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:10.512221 139980287489216 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:27.834938 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:14:27.837627 139980287489216 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:27.841330 139980287489216 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:27.900624 139980287489216 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:31.646073 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:15:31.648691 139980287489216 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:31.652629 139980287489216 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:31.714430 139980287489216 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:34.541363 139980287489216 submission_runner.py:469] Time since start: 219.60s, 	Step: 1, 	{'train/accuracy': 0.4823930859565735, 'train/loss': 0.7742568254470825, 'train/mean_average_precision': 0.022662643726394538, 'validation/accuracy': 0.4782155752182007, 'validation/loss': 0.7708846926689148, 'validation/mean_average_precision': 0.026394293036276037, 'validation/num_examples': 43793, 'test/accuracy': 0.4787319600582123, 'test/loss': 0.7687624096870422, 'test/mean_average_precision': 0.027653068525673407, 'test/num_examples': 43793, 'score': 15.498920679092407, 'total_duration': 219.59759664535522, 'accumulated_submission_time': 15.498920679092407, 'accumulated_eval_time': 204.09856963157654, 'accumulated_logging_time': 0}
I0305 19:16:34.549286 139838055929600 logging_writer.py:48] [1] accumulated_eval_time=204.099, accumulated_logging_time=0, accumulated_submission_time=15.4989, global_step=1, preemption_count=0, score=15.4989, test/accuracy=0.478732, test/loss=0.768762, test/mean_average_precision=0.0276531, test/num_examples=43793, total_duration=219.598, train/accuracy=0.482393, train/loss=0.774257, train/mean_average_precision=0.0226626, validation/accuracy=0.478216, validation/loss=0.770885, validation/mean_average_precision=0.0263943, validation/num_examples=43793
I0305 19:16:54.877234 139838064322304 logging_writer.py:48] [100] global_step=100, grad_norm=0.21322692930698395, loss=0.2008543610572815
I0305 19:17:15.342153 139838055929600 logging_writer.py:48] [200] global_step=200, grad_norm=0.04715937376022339, loss=0.07898511737585068
I0305 19:17:36.589928 139838064322304 logging_writer.py:48] [300] global_step=300, grad_norm=0.016069242730736732, loss=0.06067035719752312
I0305 19:17:57.513551 139838055929600 logging_writer.py:48] [400] global_step=400, grad_norm=0.021049950271844864, loss=0.05715945363044739
I0305 19:18:18.609237 139838064322304 logging_writer.py:48] [500] global_step=500, grad_norm=0.04186427593231201, loss=0.048589903861284256
I0305 19:18:39.661304 139838055929600 logging_writer.py:48] [600] global_step=600, grad_norm=0.049924932420253754, loss=0.05676765367388725
I0305 19:19:00.672054 139838799984384 logging_writer.py:48] [700] global_step=700, grad_norm=0.038161542266607285, loss=0.05299311876296997
I0305 19:19:21.871352 139838791591680 logging_writer.py:48] [800] global_step=800, grad_norm=0.022461971268057823, loss=0.04757174849510193
I0305 19:19:42.839703 139838799984384 logging_writer.py:48] [900] global_step=900, grad_norm=0.015395820140838623, loss=0.05220533162355423
I0305 19:20:03.873679 139838791591680 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03569713234901428, loss=0.04969387128949165
I0305 19:20:24.898499 139838799984384 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.020074034109711647, loss=0.04528839886188507
I0305 19:20:34.610379 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:21:46.527092 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:21:48.485553 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:21:50.375777 139980287489216 submission_runner.py:469] Time since start: 535.43s, 	Step: 1147, 	{'train/accuracy': 0.9871066808700562, 'train/loss': 0.04772581160068512, 'train/mean_average_precision': 0.0886581654715948, 'validation/accuracy': 0.9844759702682495, 'validation/loss': 0.05728433281183243, 'validation/mean_average_precision': 0.08778112135965022, 'validation/num_examples': 43793, 'test/accuracy': 0.9834529161453247, 'test/loss': 0.06067773327231407, 'test/mean_average_precision': 0.08643403545101526, 'test/num_examples': 43793, 'score': 255.5209014415741, 'total_duration': 535.432097196579, 'accumulated_submission_time': 255.5209014415741, 'accumulated_eval_time': 279.86393094062805, 'accumulated_logging_time': 0.018268108367919922}
I0305 19:21:50.385017 139838791591680 logging_writer.py:48] [1147] accumulated_eval_time=279.864, accumulated_logging_time=0.0182681, accumulated_submission_time=255.521, global_step=1147, preemption_count=0, score=255.521, test/accuracy=0.983453, test/loss=0.0606777, test/mean_average_precision=0.086434, test/num_examples=43793, total_duration=535.432, train/accuracy=0.987107, train/loss=0.0477258, train/mean_average_precision=0.0886582, validation/accuracy=0.984476, validation/loss=0.0572843, validation/mean_average_precision=0.0877811, validation/num_examples=43793
I0305 19:22:01.911388 139838799984384 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.021721171215176582, loss=0.04799809679389
I0305 19:22:23.367037 139838791591680 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.013276844285428524, loss=0.04469655081629753
I0305 19:22:44.395789 139838799984384 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.01809155009686947, loss=0.0463048592209816
I0305 19:23:06.002907 139838791591680 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.03855542093515396, loss=0.044647641479969025
I0305 19:23:26.860258 139838799984384 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0288751982152462, loss=0.051575012505054474
I0305 19:23:47.386420 139838791591680 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.018207069486379623, loss=0.045727554708719254
I0305 19:24:08.506272 139838799984384 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.013715595006942749, loss=0.043964434415102005
I0305 19:24:29.646597 139838791591680 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.009668241254985332, loss=0.04051464423537254
I0305 19:24:50.471934 139838799984384 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.025972167029976845, loss=0.04606049880385399
I0305 19:25:11.476934 139838791591680 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.016749441623687744, loss=0.050128936767578125
I0305 19:25:32.285089 139838799984384 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.019106276333332062, loss=0.0503055565059185
I0305 19:25:50.543430 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:27:03.001480 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:27:05.082105 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:27:06.953508 139980287489216 submission_runner.py:469] Time since start: 852.01s, 	Step: 2289, 	{'train/accuracy': 0.9878824949264526, 'train/loss': 0.043100789189338684, 'train/mean_average_precision': 0.13921025311069274, 'validation/accuracy': 0.9849789142608643, 'validation/loss': 0.05254128947854042, 'validation/mean_average_precision': 0.13491266992168494, 'validation/num_examples': 43793, 'test/accuracy': 0.9840438365936279, 'test/loss': 0.05559955909848213, 'test/mean_average_precision': 0.13477756215243067, 'test/num_examples': 43793, 'score': 495.6405735015869, 'total_duration': 852.009829044342, 'accumulated_submission_time': 495.6405735015869, 'accumulated_eval_time': 356.27398443222046, 'accumulated_logging_time': 0.0369412899017334}
I0305 19:27:06.962419 139838791591680 logging_writer.py:48] [2289] accumulated_eval_time=356.274, accumulated_logging_time=0.0369413, accumulated_submission_time=495.641, global_step=2289, preemption_count=0, score=495.641, test/accuracy=0.984044, test/loss=0.0555996, test/mean_average_precision=0.134778, test/num_examples=43793, total_duration=852.01, train/accuracy=0.987882, train/loss=0.0431008, train/mean_average_precision=0.13921, validation/accuracy=0.984979, validation/loss=0.0525413, validation/mean_average_precision=0.134913, validation/num_examples=43793
I0305 19:27:09.450220 139838799984384 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.015832798555493355, loss=0.05035153776407242
I0305 19:27:30.010045 139838791591680 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.017731063067913055, loss=0.040578149259090424
I0305 19:27:50.757531 139838799984384 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.030318288132548332, loss=0.04337277263402939
I0305 19:28:11.571742 139838791591680 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.01602545939385891, loss=0.04447007179260254
I0305 19:28:32.416835 139838799984384 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.015674563124775887, loss=0.0443737767636776
I0305 19:28:52.986114 139838791591680 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.01489794161170721, loss=0.04088955745100975
I0305 19:29:13.148229 139838799984384 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.015441721305251122, loss=0.03656991198658943
I0305 19:29:33.643546 139838791591680 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.011848962865769863, loss=0.04016665369272232
I0305 19:29:54.112195 139838799984384 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.017909124493598938, loss=0.04327132925391197
I0305 19:30:14.712109 139838791591680 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.013584122061729431, loss=0.037431128323078156
I0305 19:30:35.315280 139838799984384 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.019324885681271553, loss=0.04468473047018051
I0305 19:30:55.912103 139838791591680 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.009010988287627697, loss=0.038653258234262466
I0305 19:31:06.956478 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:32:18.973441 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:32:20.924274 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:32:22.791649 139980287489216 submission_runner.py:469] Time since start: 1167.85s, 	Step: 3453, 	{'train/accuracy': 0.9883587956428528, 'train/loss': 0.040597304701805115, 'train/mean_average_precision': 0.19470280643648571, 'validation/accuracy': 0.9854685068130493, 'validation/loss': 0.050178900361061096, 'validation/mean_average_precision': 0.1675313546680088, 'validation/num_examples': 43793, 'test/accuracy': 0.9845846891403198, 'test/loss': 0.0531773678958416, 'test/mean_average_precision': 0.1673708622509024, 'test/num_examples': 43793, 'score': 735.594518661499, 'total_duration': 1167.8479692935944, 'accumulated_submission_time': 735.594518661499, 'accumulated_eval_time': 432.1091272830963, 'accumulated_logging_time': 0.0549769401550293}
I0305 19:32:22.802069 139838799984384 logging_writer.py:48] [3453] accumulated_eval_time=432.109, accumulated_logging_time=0.0549769, accumulated_submission_time=735.595, global_step=3453, preemption_count=0, score=735.595, test/accuracy=0.984585, test/loss=0.0531774, test/mean_average_precision=0.167371, test/num_examples=43793, total_duration=1167.85, train/accuracy=0.988359, train/loss=0.0405973, train/mean_average_precision=0.194703, validation/accuracy=0.985469, validation/loss=0.0501789, validation/mean_average_precision=0.167531, validation/num_examples=43793
I0305 19:32:32.819661 139838791591680 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.010062969289720058, loss=0.04480276629328728
I0305 19:32:53.478584 139838799984384 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.0146655747666955, loss=0.03838074579834938
I0305 19:33:14.530387 139838791591680 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.015238557010889053, loss=0.04504641517996788
I0305 19:33:35.663929 139838799984384 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.014391647651791573, loss=0.04010912775993347
I0305 19:33:56.621584 139838791591680 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.01957513950765133, loss=0.04219880700111389
I0305 19:34:17.398365 139838799984384 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.011692801490426064, loss=0.041548728942871094
I0305 19:34:38.276964 139838791591680 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.012751827947795391, loss=0.039234887808561325
I0305 19:34:59.338118 139838799984384 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.015763364732265472, loss=0.0419887900352478
I0305 19:35:19.929546 139838791591680 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.011461921967566013, loss=0.038060225546360016
I0305 19:35:40.657227 139838799984384 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.011989480815827847, loss=0.04326450452208519
I0305 19:36:01.380719 139838791591680 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.012951691634953022, loss=0.0390038788318634
I0305 19:36:22.189886 139838799984384 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.012066371738910675, loss=0.03732205554842949
I0305 19:36:22.806569 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:37:34.001434 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:37:35.998172 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:37:37.883273 139980287489216 submission_runner.py:469] Time since start: 1482.94s, 	Step: 4604, 	{'train/accuracy': 0.988811194896698, 'train/loss': 0.0386018343269825, 'train/mean_average_precision': 0.21960476074494423, 'validation/accuracy': 0.9857392907142639, 'validation/loss': 0.04850190877914429, 'validation/mean_average_precision': 0.19084514469652927, 'validation/num_examples': 43793, 'test/accuracy': 0.984883725643158, 'test/loss': 0.05119326338171959, 'test/mean_average_precision': 0.19736195165367312, 'test/num_examples': 43793, 'score': 975.5567357540131, 'total_duration': 1482.9395978450775, 'accumulated_submission_time': 975.5567357540131, 'accumulated_eval_time': 507.18579053878784, 'accumulated_logging_time': 0.07667827606201172}
I0305 19:37:37.892886 139838467335936 logging_writer.py:48] [4604] accumulated_eval_time=507.186, accumulated_logging_time=0.0766783, accumulated_submission_time=975.557, global_step=4604, preemption_count=0, score=975.557, test/accuracy=0.984884, test/loss=0.0511933, test/mean_average_precision=0.197362, test/num_examples=43793, total_duration=1482.94, train/accuracy=0.988811, train/loss=0.0386018, train/mean_average_precision=0.219605, validation/accuracy=0.985739, validation/loss=0.0485019, validation/mean_average_precision=0.190845, validation/num_examples=43793
I0305 19:37:57.719690 139838458943232 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.017560718581080437, loss=0.03850836679339409
I0305 19:38:18.613556 139838467335936 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.011530224233865738, loss=0.037065550684928894
I0305 19:38:38.726596 139838458943232 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.012967895716428757, loss=0.03648984804749489
I0305 19:38:59.122210 139838467335936 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.011636527255177498, loss=0.03505075350403786
I0305 19:39:19.618487 139838458943232 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.017007971182465553, loss=0.040633682161569595
I0305 19:39:40.101092 139838467335936 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.012119919992983341, loss=0.03404198959469795
I0305 19:40:00.617445 139838458943232 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.013703501783311367, loss=0.039946917444467545
I0305 19:40:21.123800 139838467335936 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0200625192373991, loss=0.04229935631155968
I0305 19:40:41.711589 139838458943232 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.01245865598320961, loss=0.03975367546081543
I0305 19:41:02.420326 139838467335936 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.012550165876746178, loss=0.03912249207496643
I0305 19:41:22.948155 139838458943232 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.022451480850577354, loss=0.03818691149353981
I0305 19:41:37.924825 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:42:48.534338 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:42:51.431714 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:42:53.325369 139980287489216 submission_runner.py:469] Time since start: 1798.38s, 	Step: 5773, 	{'train/accuracy': 0.989326000213623, 'train/loss': 0.03614448010921478, 'train/mean_average_precision': 0.27729914324643856, 'validation/accuracy': 0.9861488342285156, 'validation/loss': 0.04666164889931679, 'validation/mean_average_precision': 0.21222980407134523, 'validation/num_examples': 43793, 'test/accuracy': 0.9853798747062683, 'test/loss': 0.04919525608420372, 'test/mean_average_precision': 0.2150066512935459, 'test/num_examples': 43793, 'score': 1215.5502200126648, 'total_duration': 1798.381689786911, 'accumulated_submission_time': 1215.5502200126648, 'accumulated_eval_time': 582.5862905979156, 'accumulated_logging_time': 0.09579896926879883}
I0305 19:42:53.334690 139838467335936 logging_writer.py:48] [5773] accumulated_eval_time=582.586, accumulated_logging_time=0.095799, accumulated_submission_time=1215.55, global_step=5773, preemption_count=0, score=1215.55, test/accuracy=0.98538, test/loss=0.0491953, test/mean_average_precision=0.215007, test/num_examples=43793, total_duration=1798.38, train/accuracy=0.989326, train/loss=0.0361445, train/mean_average_precision=0.277299, validation/accuracy=0.986149, validation/loss=0.0466616, validation/mean_average_precision=0.21223, validation/num_examples=43793
I0305 19:42:59.151580 139838458943232 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01725255884230137, loss=0.037814605981111526
I0305 19:43:19.857535 139838467335936 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.010808306746184826, loss=0.0344989113509655
I0305 19:43:40.231715 139838458943232 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.014740471728146076, loss=0.03964228928089142
I0305 19:44:00.779064 139838467335936 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01414227019995451, loss=0.035162605345249176
I0305 19:44:21.634077 139838458943232 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.01552711520344019, loss=0.03651886805891991
I0305 19:44:42.320012 139838467335936 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.016031034290790558, loss=0.037468187510967255
I0305 19:45:02.523409 139838458943232 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.013475464656949043, loss=0.03665795177221298
I0305 19:45:23.254949 139838467335936 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.014199493452906609, loss=0.03435772284865379
I0305 19:45:43.930104 139838458943232 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.022759070619940758, loss=0.033308304846286774
I0305 19:46:04.634385 139838467335936 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.014298690482974052, loss=0.03858504816889763
I0305 19:46:25.285650 139838458943232 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.016112245619297028, loss=0.03772039711475372
I0305 19:46:46.184041 139838467335936 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.014180228114128113, loss=0.03716566041111946
I0305 19:46:53.459690 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:48:05.370448 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:48:07.425334 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:48:09.423475 139980287489216 submission_runner.py:469] Time since start: 2114.48s, 	Step: 6936, 	{'train/accuracy': 0.9893827438354492, 'train/loss': 0.03575415536761284, 'train/mean_average_precision': 0.28229193757016047, 'validation/accuracy': 0.986353874206543, 'validation/loss': 0.04590628296136856, 'validation/mean_average_precision': 0.22778890821231373, 'validation/num_examples': 43793, 'test/accuracy': 0.9855024218559265, 'test/loss': 0.048700619488954544, 'test/mean_average_precision': 0.23033977054415639, 'test/num_examples': 43793, 'score': 1455.6363513469696, 'total_duration': 2114.4797990322113, 'accumulated_submission_time': 1455.6363513469696, 'accumulated_eval_time': 658.5500316619873, 'accumulated_logging_time': 0.11391949653625488}
I0305 19:48:09.432757 139838458943232 logging_writer.py:48] [6936] accumulated_eval_time=658.55, accumulated_logging_time=0.113919, accumulated_submission_time=1455.64, global_step=6936, preemption_count=0, score=1455.64, test/accuracy=0.985502, test/loss=0.0487006, test/mean_average_precision=0.23034, test/num_examples=43793, total_duration=2114.48, train/accuracy=0.989383, train/loss=0.0357542, train/mean_average_precision=0.282292, validation/accuracy=0.986354, validation/loss=0.0459063, validation/mean_average_precision=0.227789, validation/num_examples=43793
I0305 19:48:22.428578 139838467335936 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.014623542316257954, loss=0.035459328442811966
I0305 19:48:42.934149 139838458943232 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.019791100174188614, loss=0.033250294625759125
I0305 19:49:03.885426 139838467335936 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.01384367048740387, loss=0.03401120379567146
I0305 19:49:24.874549 139838458943232 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.016872964799404144, loss=0.033966097980737686
I0305 19:49:45.994182 139838467335936 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.013390286825597286, loss=0.036557186394929886
I0305 19:50:07.240531 139838458943232 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.023161688819527626, loss=0.03819737210869789
I0305 19:50:28.610442 139838467335936 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01566203683614731, loss=0.03761759400367737
I0305 19:50:49.846229 139838458943232 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.023868117481470108, loss=0.03963862359523773
I0305 19:51:11.308976 139838467335936 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.017152298241853714, loss=0.03576970845460892
I0305 19:51:32.532528 139838458943232 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.014971786178648472, loss=0.03317812830209732
I0305 19:51:53.429011 139838467335936 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.014940844848752022, loss=0.035059139132499695
I0305 19:52:09.451364 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:53:19.437398 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:53:21.394341 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:53:23.316356 139980287489216 submission_runner.py:469] Time since start: 2428.37s, 	Step: 8077, 	{'train/accuracy': 0.9897826910018921, 'train/loss': 0.034086674451828, 'train/mean_average_precision': 0.3168013288495426, 'validation/accuracy': 0.9863818883895874, 'validation/loss': 0.04561801999807358, 'validation/mean_average_precision': 0.23272723777937865, 'validation/num_examples': 43793, 'test/accuracy': 0.9855462312698364, 'test/loss': 0.04821566492319107, 'test/mean_average_precision': 0.24145918121242319, 'test/num_examples': 43793, 'score': 1695.6167976856232, 'total_duration': 2428.372677564621, 'accumulated_submission_time': 1695.6167976856232, 'accumulated_eval_time': 732.4149825572968, 'accumulated_logging_time': 0.13286447525024414}
I0305 19:53:23.326093 139838458943232 logging_writer.py:48] [8077] accumulated_eval_time=732.415, accumulated_logging_time=0.132864, accumulated_submission_time=1695.62, global_step=8077, preemption_count=0, score=1695.62, test/accuracy=0.985546, test/loss=0.0482157, test/mean_average_precision=0.241459, test/num_examples=43793, total_duration=2428.37, train/accuracy=0.989783, train/loss=0.0340867, train/mean_average_precision=0.316801, validation/accuracy=0.986382, validation/loss=0.045618, validation/mean_average_precision=0.232727, validation/num_examples=43793
I0305 19:53:28.512354 139838467335936 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.017268730327486992, loss=0.037754323333501816
I0305 19:53:49.794764 139838458943232 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.01861863024532795, loss=0.036633167415857315
I0305 19:54:10.749654 139838467335936 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.0208126213401556, loss=0.03566212207078934
I0305 19:54:31.406013 139838458943232 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.01670004427433014, loss=0.0325864814221859
I0305 19:54:52.661383 139838467335936 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01867792382836342, loss=0.03401263430714607
I0305 19:55:13.565556 139838458943232 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.022445274516940117, loss=0.03690139204263687
I0305 19:55:34.790642 139838467335936 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.017554765567183495, loss=0.03857269883155823
I0305 19:55:55.727308 139838458943232 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.024334903806447983, loss=0.03553419187664986
I0305 19:56:16.636426 139838467335936 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.013855133205652237, loss=0.03367218002676964
I0305 19:56:37.819271 139838458943232 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.01701473817229271, loss=0.034813281148672104
I0305 19:56:58.764384 139838467335936 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.02107647992670536, loss=0.030491139739751816
I0305 19:57:19.833401 139838458943232 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.01991748809814453, loss=0.03269954398274422
I0305 19:57:23.481603 139980287489216 spec.py:321] Evaluating on the training split.
I0305 19:58:33.897514 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 19:58:35.858594 139980287489216 spec.py:349] Evaluating on the test split.
I0305 19:58:37.809576 139980287489216 submission_runner.py:469] Time since start: 2742.87s, 	Step: 9218, 	{'train/accuracy': 0.9900190234184265, 'train/loss': 0.0335635207593441, 'train/mean_average_precision': 0.32155172925309805, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.045186348259449005, 'validation/mean_average_precision': 0.24295020854180135, 'validation/num_examples': 43793, 'test/accuracy': 0.9854788780212402, 'test/loss': 0.0479326993227005, 'test/mean_average_precision': 0.24655869994132765, 'test/num_examples': 43793, 'score': 1935.730122089386, 'total_duration': 2742.865765094757, 'accumulated_submission_time': 1935.730122089386, 'accumulated_eval_time': 806.7427773475647, 'accumulated_logging_time': 0.15198278427124023}
I0305 19:58:37.819765 139838467335936 logging_writer.py:48] [9218] accumulated_eval_time=806.743, accumulated_logging_time=0.151983, accumulated_submission_time=1935.73, global_step=9218, preemption_count=0, score=1935.73, test/accuracy=0.985479, test/loss=0.0479327, test/mean_average_precision=0.246559, test/num_examples=43793, total_duration=2742.87, train/accuracy=0.990019, train/loss=0.0335635, train/mean_average_precision=0.321552, validation/accuracy=0.986314, validation/loss=0.0451863, validation/mean_average_precision=0.24295, validation/num_examples=43793
I0305 19:58:55.335770 139838458943232 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.01565602235496044, loss=0.030559813603758812
I0305 19:59:15.969513 139838467335936 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.029898300766944885, loss=0.044480420649051666
I0305 19:59:36.816630 139838458943232 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.018744977191090584, loss=0.03281238675117493
I0305 19:59:57.312517 139838467335936 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.019434085115790367, loss=0.0317353792488575
I0305 20:00:17.556068 139838458943232 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.022176390513777733, loss=0.033026888966560364
I0305 20:00:37.872981 139838467335936 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.01960856094956398, loss=0.034556277096271515
I0305 20:00:59.117451 139838458943232 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.022132206708192825, loss=0.0351485013961792
I0305 20:01:19.609845 139838467335936 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.02352663315832615, loss=0.03322472423315048
I0305 20:01:40.616409 139838458943232 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.022847171872854233, loss=0.03464550897479057
I0305 20:02:01.598557 139838467335936 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.023705922067165375, loss=0.033029522746801376
I0305 20:02:22.566509 139838458943232 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.01974152773618698, loss=0.035843461751937866
I0305 20:02:37.903263 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:03:47.181610 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:03:49.126365 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:03:51.033809 139980287489216 submission_runner.py:469] Time since start: 3056.09s, 	Step: 10377, 	{'train/accuracy': 0.9903536438941956, 'train/loss': 0.03204602375626564, 'train/mean_average_precision': 0.37004586894340397, 'validation/accuracy': 0.9866331219673157, 'validation/loss': 0.04455218091607094, 'validation/mean_average_precision': 0.2500235444010267, 'validation/num_examples': 43793, 'test/accuracy': 0.98580402135849, 'test/loss': 0.04715646058320999, 'test/mean_average_precision': 0.2544811834442035, 'test/num_examples': 43793, 'score': 2175.775504589081, 'total_duration': 3056.0900371074677, 'accumulated_submission_time': 2175.775504589081, 'accumulated_eval_time': 879.8731844425201, 'accumulated_logging_time': 0.17185139656066895}
I0305 20:03:51.043605 139838467335936 logging_writer.py:48] [10377] accumulated_eval_time=879.873, accumulated_logging_time=0.171851, accumulated_submission_time=2175.78, global_step=10377, preemption_count=0, score=2175.78, test/accuracy=0.985804, test/loss=0.0471565, test/mean_average_precision=0.254481, test/num_examples=43793, total_duration=3056.09, train/accuracy=0.990354, train/loss=0.032046, train/mean_average_precision=0.370046, validation/accuracy=0.986633, validation/loss=0.0445522, validation/mean_average_precision=0.250024, validation/num_examples=43793
I0305 20:03:56.096472 139838458943232 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.022242141887545586, loss=0.03128896281123161
I0305 20:04:17.155590 139838467335936 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.024551717564463615, loss=0.03401075676083565
I0305 20:04:37.913459 139838458943232 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.027118492871522903, loss=0.035562943667173386
I0305 20:04:58.194810 139838467335936 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.021178139373660088, loss=0.031150642782449722
I0305 20:05:18.321533 139838458943232 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.030086318030953407, loss=0.03281509503722191
I0305 20:05:38.521246 139838467335936 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.027132390066981316, loss=0.03365224972367287
I0305 20:05:59.188538 139838458943232 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.02168707363307476, loss=0.030641762539744377
I0305 20:06:19.808514 139838467335936 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.021490834653377533, loss=0.03211052715778351
I0305 20:06:40.482089 139838458943232 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.02326980233192444, loss=0.03366004303097725
I0305 20:07:01.121762 139838467335936 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.023197941482067108, loss=0.03290242701768875
I0305 20:07:21.862000 139838458943232 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.04376926273107529, loss=0.03054710291326046
I0305 20:07:42.817873 139838467335936 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.025332774966955185, loss=0.0356375090777874
I0305 20:07:51.238932 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:09:03.495596 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:09:05.478556 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:09:07.343929 139980287489216 submission_runner.py:469] Time since start: 3372.40s, 	Step: 11541, 	{'train/accuracy': 0.9903134703636169, 'train/loss': 0.03206717595458031, 'train/mean_average_precision': 0.37290988475972814, 'validation/accuracy': 0.9866952300071716, 'validation/loss': 0.04431198909878731, 'validation/mean_average_precision': 0.2560908872310378, 'validation/num_examples': 43793, 'test/accuracy': 0.9858347773551941, 'test/loss': 0.04703463613986969, 'test/mean_average_precision': 0.25570046878743635, 'test/num_examples': 43793, 'score': 2415.9324645996094, 'total_duration': 3372.400179862976, 'accumulated_submission_time': 2415.9324645996094, 'accumulated_eval_time': 955.97807097435, 'accumulated_logging_time': 0.19110631942749023}
I0305 20:09:07.353662 139838458943232 logging_writer.py:48] [11541] accumulated_eval_time=955.978, accumulated_logging_time=0.191106, accumulated_submission_time=2415.93, global_step=11541, preemption_count=0, score=2415.93, test/accuracy=0.985835, test/loss=0.0470346, test/mean_average_precision=0.2557, test/num_examples=43793, total_duration=3372.4, train/accuracy=0.990313, train/loss=0.0320672, train/mean_average_precision=0.37291, validation/accuracy=0.986695, validation/loss=0.044312, validation/mean_average_precision=0.256091, validation/num_examples=43793
I0305 20:09:19.283807 139838467335936 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.02435142733156681, loss=0.033773958683013916
I0305 20:09:39.354016 139838458943232 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.02565361000597477, loss=0.02991262450814247
I0305 20:09:59.479315 139838467335936 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.02314569242298603, loss=0.031719088554382324
I0305 20:10:19.621505 139838458943232 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.03069852851331234, loss=0.037208400666713715
I0305 20:10:39.889311 139838467335936 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.024019090458750725, loss=0.03057079203426838
I0305 20:11:00.172687 139838458943232 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03993582725524902, loss=0.03760650381445885
I0305 20:11:20.779157 139838467335936 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.030705006793141365, loss=0.037966471165418625
I0305 20:11:41.697939 139838458943232 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.023855624720454216, loss=0.03289022296667099
I0305 20:12:02.452527 139838467335936 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.023435072973370552, loss=0.03440316021442413
I0305 20:12:23.066973 139838458943232 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.027765708044171333, loss=0.030769076198339462
I0305 20:12:43.992667 139838467335936 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.032695259898900986, loss=0.03167042136192322
I0305 20:13:04.971648 139838458943232 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.023893440142273903, loss=0.02984725870192051
I0305 20:13:07.432994 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:14:18.599506 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:14:20.540273 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:14:22.409614 139980287489216 submission_runner.py:469] Time since start: 3687.47s, 	Step: 12713, 	{'train/accuracy': 0.9906961917877197, 'train/loss': 0.030567592009902, 'train/mean_average_precision': 0.4085416505627893, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.0444832481443882, 'validation/mean_average_precision': 0.25944469720586666, 'validation/num_examples': 43793, 'test/accuracy': 0.9858844876289368, 'test/loss': 0.047163087874650955, 'test/mean_average_precision': 0.25761031148278557, 'test/num_examples': 43793, 'score': 2655.975471019745, 'total_duration': 3687.4658617973328, 'accumulated_submission_time': 2655.975471019745, 'accumulated_eval_time': 1030.9545719623566, 'accumulated_logging_time': 0.20967650413513184}
I0305 20:14:22.419568 139838467335936 logging_writer.py:48] [12713] accumulated_eval_time=1030.95, accumulated_logging_time=0.209677, accumulated_submission_time=2655.98, global_step=12713, preemption_count=0, score=2655.98, test/accuracy=0.985884, test/loss=0.0471631, test/mean_average_precision=0.25761, test/num_examples=43793, total_duration=3687.47, train/accuracy=0.990696, train/loss=0.0305676, train/mean_average_precision=0.408542, validation/accuracy=0.986664, validation/loss=0.0444832, validation/mean_average_precision=0.259445, validation/num_examples=43793
I0305 20:14:40.764853 139838458943232 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03314729034900665, loss=0.034432973712682724
I0305 20:15:01.756647 139838467335936 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.026218600571155548, loss=0.032248467206954956
I0305 20:15:22.663368 139838458943232 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.03186434879899025, loss=0.0322703942656517
I0305 20:15:43.515256 139838467335936 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.027214156463742256, loss=0.034609079360961914
I0305 20:16:03.881744 139838458943232 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.028217995539307594, loss=0.030417127534747124
I0305 20:16:24.149765 139838467335936 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.030241956934332848, loss=0.029651999473571777
I0305 20:16:44.776444 139838458943232 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03414594754576683, loss=0.037754740566015244
I0305 20:17:05.597651 139838467335936 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.035503022372722626, loss=0.03413262218236923
I0305 20:17:26.216043 139838458943232 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.031701769679784775, loss=0.0301691684871912
I0305 20:17:47.111769 139838467335936 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.03833832964301109, loss=0.03349069505929947
I0305 20:18:07.913105 139838458943232 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.038767535239458084, loss=0.030842121690511703
I0305 20:18:22.456286 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:19:34.737437 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:19:36.670205 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:19:38.545721 139980287489216 submission_runner.py:469] Time since start: 4003.60s, 	Step: 13870, 	{'train/accuracy': 0.9906468391418457, 'train/loss': 0.030958540737628937, 'train/mean_average_precision': 0.3856650859959224, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.04421554505825043, 'validation/mean_average_precision': 0.2571836699555103, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.046645570546388626, 'test/mean_average_precision': 0.2602141208304837, 'test/num_examples': 43793, 'score': 2895.973425388336, 'total_duration': 4003.602040052414, 'accumulated_submission_time': 2895.973425388336, 'accumulated_eval_time': 1107.043963432312, 'accumulated_logging_time': 0.22904181480407715}
I0305 20:19:38.556758 139838467335936 logging_writer.py:48] [13870] accumulated_eval_time=1107.04, accumulated_logging_time=0.229042, accumulated_submission_time=2895.97, global_step=13870, preemption_count=0, score=2895.97, test/accuracy=0.985925, test/loss=0.0466456, test/mean_average_precision=0.260214, test/num_examples=43793, total_duration=4003.6, train/accuracy=0.990647, train/loss=0.0309585, train/mean_average_precision=0.385665, validation/accuracy=0.986664, validation/loss=0.0442155, validation/mean_average_precision=0.257184, validation/num_examples=43793
I0305 20:19:44.966752 139838458943232 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03201713040471077, loss=0.02918846160173416
I0305 20:20:05.581310 139838467335936 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.028794683516025543, loss=0.03138231486082077
I0305 20:20:26.402292 139838458943232 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03293892741203308, loss=0.03158273175358772
I0305 20:20:46.914509 139838467335936 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.031940802931785583, loss=0.03302512690424919
I0305 20:21:07.280535 139838458943232 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.029526468366384506, loss=0.029862456023693085
I0305 20:21:27.921423 139838467335936 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.03527648374438286, loss=0.030823320150375366
I0305 20:21:48.719630 139838458943232 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.03331661969423294, loss=0.02863321267068386
I0305 20:22:09.748893 139838467335936 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.02875939942896366, loss=0.029090581461787224
I0305 20:22:30.878492 139838458943232 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.04716203734278679, loss=0.03150973096489906
I0305 20:22:51.671746 139838467335936 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.029628071933984756, loss=0.029418710619211197
I0305 20:23:12.421610 139838458943232 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.0443173423409462, loss=0.03401735797524452
I0305 20:23:33.466385 139838467335936 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.04627295956015587, loss=0.03367726877331734
I0305 20:23:38.548434 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:24:48.290334 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:24:50.404493 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:24:52.448401 139980287489216 submission_runner.py:469] Time since start: 4317.50s, 	Step: 15025, 	{'train/accuracy': 0.9910566210746765, 'train/loss': 0.029271526262164116, 'train/mean_average_precision': 0.42564648404610733, 'validation/accuracy': 0.986661970615387, 'validation/loss': 0.04430257901549339, 'validation/mean_average_precision': 0.26422095372942794, 'validation/num_examples': 43793, 'test/accuracy': 0.9858958125114441, 'test/loss': 0.0468885637819767, 'test/mean_average_precision': 0.2597977737904241, 'test/num_examples': 43793, 'score': 3135.9273018836975, 'total_duration': 4317.504724979401, 'accumulated_submission_time': 3135.9273018836975, 'accumulated_eval_time': 1180.9438874721527, 'accumulated_logging_time': 0.2491745948791504}
I0305 20:24:52.459555 139838458943232 logging_writer.py:48] [15025] accumulated_eval_time=1180.94, accumulated_logging_time=0.249175, accumulated_submission_time=3135.93, global_step=15025, preemption_count=0, score=3135.93, test/accuracy=0.985896, test/loss=0.0468886, test/mean_average_precision=0.259798, test/num_examples=43793, total_duration=4317.5, train/accuracy=0.991057, train/loss=0.0292715, train/mean_average_precision=0.425646, validation/accuracy=0.986662, validation/loss=0.0443026, validation/mean_average_precision=0.264221, validation/num_examples=43793
I0305 20:25:08.011935 139838467335936 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.03510811924934387, loss=0.03220639005303383
I0305 20:25:28.660310 139838458943232 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.03180330991744995, loss=0.02885468304157257
I0305 20:25:49.480124 139838467335936 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.042320918291807175, loss=0.031699907034635544
I0305 20:26:10.558904 139838458943232 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.03523885831236839, loss=0.030010974034667015
I0305 20:26:31.547067 139838467335936 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.040104907006025314, loss=0.031999848783016205
I0305 20:26:52.163050 139838458943232 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.03576478734612465, loss=0.030311234295368195
I0305 20:27:13.256539 139838467335936 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.03534189611673355, loss=0.033346645534038544
I0305 20:27:34.427419 139838458943232 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04605501890182495, loss=0.029963906854391098
I0305 20:27:55.420020 139838467335936 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.037797365337610245, loss=0.0313289500772953
I0305 20:28:16.428645 139838458943232 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.036513399332761765, loss=0.03180496022105217
I0305 20:28:37.332114 139838467335936 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.04360948130488396, loss=0.03201133385300636
I0305 20:28:52.454589 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:30:04.241744 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:30:06.192326 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:30:08.064184 139980287489216 submission_runner.py:469] Time since start: 4633.12s, 	Step: 16173, 	{'train/accuracy': 0.9909566044807434, 'train/loss': 0.029773570597171783, 'train/mean_average_precision': 0.41131781100886333, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.04397893324494362, 'validation/mean_average_precision': 0.26776311751215204, 'validation/num_examples': 43793, 'test/accuracy': 0.9860718846321106, 'test/loss': 0.04668596386909485, 'test/mean_average_precision': 0.26253016266681867, 'test/num_examples': 43793, 'score': 3375.8815655708313, 'total_duration': 4633.120508432388, 'accumulated_submission_time': 3375.8815655708313, 'accumulated_eval_time': 1256.55344247818, 'accumulated_logging_time': 0.27141714096069336}
I0305 20:30:08.076316 139838458943232 logging_writer.py:48] [16173] accumulated_eval_time=1256.55, accumulated_logging_time=0.271417, accumulated_submission_time=3375.88, global_step=16173, preemption_count=0, score=3375.88, test/accuracy=0.986072, test/loss=0.046686, test/mean_average_precision=0.26253, test/num_examples=43793, total_duration=4633.12, train/accuracy=0.990957, train/loss=0.0297736, train/mean_average_precision=0.411318, validation/accuracy=0.986856, validation/loss=0.0439789, validation/mean_average_precision=0.267763, validation/num_examples=43793
I0305 20:30:13.878430 139838467335936 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.03350929170846939, loss=0.03185122832655907
I0305 20:30:35.012952 139838458943232 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.03229318931698799, loss=0.02872445434331894
I0305 20:30:55.969379 139838467335936 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.03955554589629173, loss=0.031805455684661865
I0305 20:31:16.919061 139838458943232 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.04070728272199631, loss=0.03436417877674103
I0305 20:31:37.865327 139838467335936 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.039713937789201736, loss=0.028816604986786842
I0305 20:31:58.765352 139838458943232 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.039564356207847595, loss=0.02982567995786667
I0305 20:32:19.650304 139838467335936 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.037884172052145004, loss=0.02782316505908966
I0305 20:32:40.742432 139838458943232 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.047176800668239594, loss=0.03388106822967529
I0305 20:33:01.471901 139838467335936 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.035576231777668, loss=0.03053838387131691
I0305 20:33:22.601397 139838458943232 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04086821898818016, loss=0.031434252858161926
I0305 20:33:43.547076 139838467335936 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.04353387653827667, loss=0.03220577538013458
I0305 20:34:04.746409 139838458943232 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.04545191302895546, loss=0.03266250714659691
I0305 20:34:08.086287 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:35:22.302371 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:35:24.388100 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:35:26.389808 139980287489216 submission_runner.py:469] Time since start: 4951.45s, 	Step: 17317, 	{'train/accuracy': 0.9914838075637817, 'train/loss': 0.027793170884251595, 'train/mean_average_precision': 0.4619491998492642, 'validation/accuracy': 0.9867228865623474, 'validation/loss': 0.044263266026973724, 'validation/mean_average_precision': 0.2693320215532364, 'validation/num_examples': 43793, 'test/accuracy': 0.9859695434570312, 'test/loss': 0.04712387174367905, 'test/mean_average_precision': 0.2587577291596019, 'test/num_examples': 43793, 'score': 3615.8517298698425, 'total_duration': 4951.44611787796, 'accumulated_submission_time': 3615.8517298698425, 'accumulated_eval_time': 1334.8569085597992, 'accumulated_logging_time': 0.2925727367401123}
I0305 20:35:26.400712 139838467335936 logging_writer.py:48] [17317] accumulated_eval_time=1334.86, accumulated_logging_time=0.292573, accumulated_submission_time=3615.85, global_step=17317, preemption_count=0, score=3615.85, test/accuracy=0.98597, test/loss=0.0471239, test/mean_average_precision=0.258758, test/num_examples=43793, total_duration=4951.45, train/accuracy=0.991484, train/loss=0.0277932, train/mean_average_precision=0.461949, validation/accuracy=0.986723, validation/loss=0.0442633, validation/mean_average_precision=0.269332, validation/num_examples=43793
I0305 20:35:43.662986 139838458943232 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.04303354397416115, loss=0.030682027339935303
I0305 20:36:04.512428 139838467335936 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.03813677281141281, loss=0.029437215998768806
I0305 20:36:25.494892 139838458943232 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.04653245210647583, loss=0.03237826004624367
I0305 20:36:47.133583 139838467335936 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.043127138167619705, loss=0.02612038515508175
I0305 20:37:08.398986 139838458943232 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.05211694911122322, loss=0.027580922469496727
I0305 20:37:29.415473 139838467335936 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.038822486996650696, loss=0.03065485693514347
I0305 20:37:50.603391 139838458943232 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.04854893311858177, loss=0.03061712346971035
I0305 20:38:11.447854 139838467335936 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.03678496554493904, loss=0.02934166230261326
I0305 20:38:32.312968 139838458943232 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04408504068851471, loss=0.03412269428372383
I0305 20:38:53.065951 139838467335936 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.04162034019827843, loss=0.027352817356586456
I0305 20:39:14.079239 139838458943232 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.048704057931900024, loss=0.03221040591597557
I0305 20:39:26.414613 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:40:40.172442 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:40:42.287649 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:40:44.235090 139980287489216 submission_runner.py:469] Time since start: 5269.29s, 	Step: 18460, 	{'train/accuracy': 0.9911417961120605, 'train/loss': 0.02884545922279358, 'train/mean_average_precision': 0.4377091849105926, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.044104378670454025, 'validation/mean_average_precision': 0.27214622281942813, 'validation/num_examples': 43793, 'test/accuracy': 0.9860533475875854, 'test/loss': 0.04682868719100952, 'test/mean_average_precision': 0.262396743178888, 'test/num_examples': 43793, 'score': 3855.8258833885193, 'total_duration': 5269.291365623474, 'accumulated_submission_time': 3855.8258833885193, 'accumulated_eval_time': 1412.677294254303, 'accumulated_logging_time': 0.3132791519165039}
I0305 20:40:44.246019 139838467335936 logging_writer.py:48] [18460] accumulated_eval_time=1412.68, accumulated_logging_time=0.313279, accumulated_submission_time=3855.83, global_step=18460, preemption_count=0, score=3855.83, test/accuracy=0.986053, test/loss=0.0468287, test/mean_average_precision=0.262397, test/num_examples=43793, total_duration=5269.29, train/accuracy=0.991142, train/loss=0.0288455, train/mean_average_precision=0.437709, validation/accuracy=0.986831, validation/loss=0.0441044, validation/mean_average_precision=0.272146, validation/num_examples=43793
I0305 20:40:52.835572 139838458943232 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.04531199112534523, loss=0.028964195400476456
I0305 20:41:13.710783 139838467335936 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.04351834952831268, loss=0.027872977778315544
I0305 20:41:34.792201 139838458943232 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.04977826029062271, loss=0.03338274359703064
I0305 20:41:55.569218 139838467335936 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0579107366502285, loss=0.03368538245558739
I0305 20:42:16.737086 139838458943232 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.0506194569170475, loss=0.030054008588194847
I0305 20:42:38.031319 139838467335936 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.054030004888772964, loss=0.03294176608324051
I0305 20:42:59.201278 139838458943232 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.04256604239344597, loss=0.028118150308728218
I0305 20:43:20.330600 139838467335936 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.04564353823661804, loss=0.02717725560069084
I0305 20:43:41.670226 139838458943232 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.055425796657800674, loss=0.03210774064064026
I0305 20:44:02.954680 139838467335936 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.04765962064266205, loss=0.03110179677605629
I0305 20:44:23.706683 139838458943232 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.06620103865861893, loss=0.030485577881336212
I0305 20:44:44.258708 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:45:56.247303 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:45:58.173184 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:46:00.054113 139980287489216 submission_runner.py:469] Time since start: 5585.11s, 	Step: 19598, 	{'train/accuracy': 0.9915357232093811, 'train/loss': 0.027552323415875435, 'train/mean_average_precision': 0.4671392850410019, 'validation/accuracy': 0.9869339466094971, 'validation/loss': 0.04378799721598625, 'validation/mean_average_precision': 0.27159048028357524, 'validation/num_examples': 43793, 'test/accuracy': 0.9861809611320496, 'test/loss': 0.046339575201272964, 'test/mean_average_precision': 0.26829704705419727, 'test/num_examples': 43793, 'score': 4095.7946321964264, 'total_duration': 5585.110351085663, 'accumulated_submission_time': 4095.7946321964264, 'accumulated_eval_time': 1488.4725723266602, 'accumulated_logging_time': 0.3339731693267822}
I0305 20:46:00.065294 139838467335936 logging_writer.py:48] [19598] accumulated_eval_time=1488.47, accumulated_logging_time=0.333973, accumulated_submission_time=4095.79, global_step=19598, preemption_count=0, score=4095.79, test/accuracy=0.986181, test/loss=0.0463396, test/mean_average_precision=0.268297, test/num_examples=43793, total_duration=5585.11, train/accuracy=0.991536, train/loss=0.0275523, train/mean_average_precision=0.467139, validation/accuracy=0.986934, validation/loss=0.043788, validation/mean_average_precision=0.27159, validation/num_examples=43793
I0305 20:46:00.715286 139838458943232 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.050460223108530045, loss=0.031599465757608414
I0305 20:46:21.599735 139838467335936 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.055393561720848083, loss=0.02945350483059883
I0305 20:46:42.732229 139838458943232 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.05177458003163338, loss=0.029559819027781487
I0305 20:47:03.731827 139838467335936 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.05460117384791374, loss=0.03025742433965206
I0305 20:47:25.133767 139838458943232 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.04873298108577728, loss=0.031877096742391586
I0305 20:47:46.437937 139838467335936 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04559045657515526, loss=0.032429590821266174
I0305 20:48:07.288545 139838458943232 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.05001070722937584, loss=0.029916951432824135
I0305 20:48:27.916531 139838467335936 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.058918967843055725, loss=0.03341400995850563
I0305 20:48:48.850080 139838458943232 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.05318281799554825, loss=0.03147394582629204
I0305 20:49:09.744545 139838467335936 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.04540703445672989, loss=0.027907628566026688
I0305 20:49:30.667384 139838458943232 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04968118295073509, loss=0.029051266610622406
I0305 20:49:51.510815 139838467335936 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.04932556301355362, loss=0.03171601891517639
I0305 20:50:00.077451 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:51:12.725277 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:51:14.668409 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:51:16.509635 139980287489216 submission_runner.py:469] Time since start: 5901.57s, 	Step: 20743, 	{'train/accuracy': 0.9913951754570007, 'train/loss': 0.02799431048333645, 'train/mean_average_precision': 0.44902251236465013, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04400802403688431, 'validation/mean_average_precision': 0.2780012465228094, 'validation/num_examples': 43793, 'test/accuracy': 0.9861177802085876, 'test/loss': 0.046820566058158875, 'test/mean_average_precision': 0.2690552973915572, 'test/num_examples': 43793, 'score': 4335.765934467316, 'total_duration': 5901.565896034241, 'accumulated_submission_time': 4335.765934467316, 'accumulated_eval_time': 1564.9046449661255, 'accumulated_logging_time': 0.35477566719055176}
I0305 20:51:16.521456 139838458943232 logging_writer.py:48] [20743] accumulated_eval_time=1564.9, accumulated_logging_time=0.354776, accumulated_submission_time=4335.77, global_step=20743, preemption_count=0, score=4335.77, test/accuracy=0.986118, test/loss=0.0468206, test/mean_average_precision=0.269055, test/num_examples=43793, total_duration=5901.57, train/accuracy=0.991395, train/loss=0.0279943, train/mean_average_precision=0.449023, validation/accuracy=0.986971, validation/loss=0.044008, validation/mean_average_precision=0.278001, validation/num_examples=43793
I0305 20:51:28.223089 139838467335936 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.05223485827445984, loss=0.03062845766544342
I0305 20:51:48.671077 139838458943232 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.0474800169467926, loss=0.028081435710191727
I0305 20:52:09.689917 139838467335936 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.059673506766557693, loss=0.026989787817001343
I0305 20:52:30.628833 139838458943232 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.0656571015715599, loss=0.03103959746658802
I0305 20:52:51.358210 139838467335936 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.06750250607728958, loss=0.029184913262724876
I0305 20:53:12.174212 139838458943232 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.0566161572933197, loss=0.032259855419397354
I0305 20:53:33.022554 139838467335936 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.062928706407547, loss=0.031346820294857025
I0305 20:53:53.852494 139838458943232 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.06067539006471634, loss=0.030625980347394943
I0305 20:54:14.735104 139838467335936 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.05225985497236252, loss=0.029670435935258865
I0305 20:54:35.481847 139838458943232 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.056031931191682816, loss=0.032214563339948654
I0305 20:54:56.525019 139838467335936 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.0488753505051136, loss=0.031193584203720093
I0305 20:55:16.597057 139980287489216 spec.py:321] Evaluating on the training split.
I0305 20:56:28.136364 139980287489216 spec.py:333] Evaluating on the validation split.
I0305 20:56:30.112203 139980287489216 spec.py:349] Evaluating on the test split.
I0305 20:56:32.056540 139980287489216 submission_runner.py:469] Time since start: 6217.11s, 	Step: 21897, 	{'train/accuracy': 0.9917647838592529, 'train/loss': 0.026797432452440262, 'train/mean_average_precision': 0.48910692151807733, 'validation/accuracy': 0.9868702292442322, 'validation/loss': 0.04401025548577309, 'validation/mean_average_precision': 0.28174934072078295, 'validation/num_examples': 43793, 'test/accuracy': 0.9860954880714417, 'test/loss': 0.04672539234161377, 'test/mean_average_precision': 0.27289647236142517, 'test/num_examples': 43793, 'score': 4575.802174806595, 'total_duration': 6217.112738609314, 'accumulated_submission_time': 4575.802174806595, 'accumulated_eval_time': 1640.3639612197876, 'accumulated_logging_time': 0.37573790550231934}
I0305 20:56:32.069060 139838458943232 logging_writer.py:48] [21897] accumulated_eval_time=1640.36, accumulated_logging_time=0.375738, accumulated_submission_time=4575.8, global_step=21897, preemption_count=0, score=4575.8, test/accuracy=0.986095, test/loss=0.0467254, test/mean_average_precision=0.272896, test/num_examples=43793, total_duration=6217.11, train/accuracy=0.991765, train/loss=0.0267974, train/mean_average_precision=0.489107, validation/accuracy=0.98687, validation/loss=0.0440103, validation/mean_average_precision=0.281749, validation/num_examples=43793
I0305 20:56:32.084200 139838467335936 logging_writer.py:48] [21897] global_step=21897, preemption_count=0, score=4575.8
I0305 20:56:32.234490 139980287489216 submission_runner.py:646] Tuning trial 5/5
I0305 20:56:32.234680 139980287489216 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0305 20:56:32.235764 139980287489216 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4823930859565735, 'train/loss': 0.7742568254470825, 'train/mean_average_precision': 0.022662643726394538, 'validation/accuracy': 0.4782155752182007, 'validation/loss': 0.7708846926689148, 'validation/mean_average_precision': 0.026394293036276037, 'validation/num_examples': 43793, 'test/accuracy': 0.4787319600582123, 'test/loss': 0.7687624096870422, 'test/mean_average_precision': 0.027653068525673407, 'test/num_examples': 43793, 'score': 15.498920679092407, 'total_duration': 219.59759664535522, 'accumulated_submission_time': 15.498920679092407, 'accumulated_eval_time': 204.09856963157654, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1147, {'train/accuracy': 0.9871066808700562, 'train/loss': 0.04772581160068512, 'train/mean_average_precision': 0.0886581654715948, 'validation/accuracy': 0.9844759702682495, 'validation/loss': 0.05728433281183243, 'validation/mean_average_precision': 0.08778112135965022, 'validation/num_examples': 43793, 'test/accuracy': 0.9834529161453247, 'test/loss': 0.06067773327231407, 'test/mean_average_precision': 0.08643403545101526, 'test/num_examples': 43793, 'score': 255.5209014415741, 'total_duration': 535.432097196579, 'accumulated_submission_time': 255.5209014415741, 'accumulated_eval_time': 279.86393094062805, 'accumulated_logging_time': 0.018268108367919922, 'global_step': 1147, 'preemption_count': 0}), (2289, {'train/accuracy': 0.9878824949264526, 'train/loss': 0.043100789189338684, 'train/mean_average_precision': 0.13921025311069274, 'validation/accuracy': 0.9849789142608643, 'validation/loss': 0.05254128947854042, 'validation/mean_average_precision': 0.13491266992168494, 'validation/num_examples': 43793, 'test/accuracy': 0.9840438365936279, 'test/loss': 0.05559955909848213, 'test/mean_average_precision': 0.13477756215243067, 'test/num_examples': 43793, 'score': 495.6405735015869, 'total_duration': 852.009829044342, 'accumulated_submission_time': 495.6405735015869, 'accumulated_eval_time': 356.27398443222046, 'accumulated_logging_time': 0.0369412899017334, 'global_step': 2289, 'preemption_count': 0}), (3453, {'train/accuracy': 0.9883587956428528, 'train/loss': 0.040597304701805115, 'train/mean_average_precision': 0.19470280643648571, 'validation/accuracy': 0.9854685068130493, 'validation/loss': 0.050178900361061096, 'validation/mean_average_precision': 0.1675313546680088, 'validation/num_examples': 43793, 'test/accuracy': 0.9845846891403198, 'test/loss': 0.0531773678958416, 'test/mean_average_precision': 0.1673708622509024, 'test/num_examples': 43793, 'score': 735.594518661499, 'total_duration': 1167.8479692935944, 'accumulated_submission_time': 735.594518661499, 'accumulated_eval_time': 432.1091272830963, 'accumulated_logging_time': 0.0549769401550293, 'global_step': 3453, 'preemption_count': 0}), (4604, {'train/accuracy': 0.988811194896698, 'train/loss': 0.0386018343269825, 'train/mean_average_precision': 0.21960476074494423, 'validation/accuracy': 0.9857392907142639, 'validation/loss': 0.04850190877914429, 'validation/mean_average_precision': 0.19084514469652927, 'validation/num_examples': 43793, 'test/accuracy': 0.984883725643158, 'test/loss': 0.05119326338171959, 'test/mean_average_precision': 0.19736195165367312, 'test/num_examples': 43793, 'score': 975.5567357540131, 'total_duration': 1482.9395978450775, 'accumulated_submission_time': 975.5567357540131, 'accumulated_eval_time': 507.18579053878784, 'accumulated_logging_time': 0.07667827606201172, 'global_step': 4604, 'preemption_count': 0}), (5773, {'train/accuracy': 0.989326000213623, 'train/loss': 0.03614448010921478, 'train/mean_average_precision': 0.27729914324643856, 'validation/accuracy': 0.9861488342285156, 'validation/loss': 0.04666164889931679, 'validation/mean_average_precision': 0.21222980407134523, 'validation/num_examples': 43793, 'test/accuracy': 0.9853798747062683, 'test/loss': 0.04919525608420372, 'test/mean_average_precision': 0.2150066512935459, 'test/num_examples': 43793, 'score': 1215.5502200126648, 'total_duration': 1798.381689786911, 'accumulated_submission_time': 1215.5502200126648, 'accumulated_eval_time': 582.5862905979156, 'accumulated_logging_time': 0.09579896926879883, 'global_step': 5773, 'preemption_count': 0}), (6936, {'train/accuracy': 0.9893827438354492, 'train/loss': 0.03575415536761284, 'train/mean_average_precision': 0.28229193757016047, 'validation/accuracy': 0.986353874206543, 'validation/loss': 0.04590628296136856, 'validation/mean_average_precision': 0.22778890821231373, 'validation/num_examples': 43793, 'test/accuracy': 0.9855024218559265, 'test/loss': 0.048700619488954544, 'test/mean_average_precision': 0.23033977054415639, 'test/num_examples': 43793, 'score': 1455.6363513469696, 'total_duration': 2114.4797990322113, 'accumulated_submission_time': 1455.6363513469696, 'accumulated_eval_time': 658.5500316619873, 'accumulated_logging_time': 0.11391949653625488, 'global_step': 6936, 'preemption_count': 0}), (8077, {'train/accuracy': 0.9897826910018921, 'train/loss': 0.034086674451828, 'train/mean_average_precision': 0.3168013288495426, 'validation/accuracy': 0.9863818883895874, 'validation/loss': 0.04561801999807358, 'validation/mean_average_precision': 0.23272723777937865, 'validation/num_examples': 43793, 'test/accuracy': 0.9855462312698364, 'test/loss': 0.04821566492319107, 'test/mean_average_precision': 0.24145918121242319, 'test/num_examples': 43793, 'score': 1695.6167976856232, 'total_duration': 2428.372677564621, 'accumulated_submission_time': 1695.6167976856232, 'accumulated_eval_time': 732.4149825572968, 'accumulated_logging_time': 0.13286447525024414, 'global_step': 8077, 'preemption_count': 0}), (9218, {'train/accuracy': 0.9900190234184265, 'train/loss': 0.0335635207593441, 'train/mean_average_precision': 0.32155172925309805, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.045186348259449005, 'validation/mean_average_precision': 0.24295020854180135, 'validation/num_examples': 43793, 'test/accuracy': 0.9854788780212402, 'test/loss': 0.0479326993227005, 'test/mean_average_precision': 0.24655869994132765, 'test/num_examples': 43793, 'score': 1935.730122089386, 'total_duration': 2742.865765094757, 'accumulated_submission_time': 1935.730122089386, 'accumulated_eval_time': 806.7427773475647, 'accumulated_logging_time': 0.15198278427124023, 'global_step': 9218, 'preemption_count': 0}), (10377, {'train/accuracy': 0.9903536438941956, 'train/loss': 0.03204602375626564, 'train/mean_average_precision': 0.37004586894340397, 'validation/accuracy': 0.9866331219673157, 'validation/loss': 0.04455218091607094, 'validation/mean_average_precision': 0.2500235444010267, 'validation/num_examples': 43793, 'test/accuracy': 0.98580402135849, 'test/loss': 0.04715646058320999, 'test/mean_average_precision': 0.2544811834442035, 'test/num_examples': 43793, 'score': 2175.775504589081, 'total_duration': 3056.0900371074677, 'accumulated_submission_time': 2175.775504589081, 'accumulated_eval_time': 879.8731844425201, 'accumulated_logging_time': 0.17185139656066895, 'global_step': 10377, 'preemption_count': 0}), (11541, {'train/accuracy': 0.9903134703636169, 'train/loss': 0.03206717595458031, 'train/mean_average_precision': 0.37290988475972814, 'validation/accuracy': 0.9866952300071716, 'validation/loss': 0.04431198909878731, 'validation/mean_average_precision': 0.2560908872310378, 'validation/num_examples': 43793, 'test/accuracy': 0.9858347773551941, 'test/loss': 0.04703463613986969, 'test/mean_average_precision': 0.25570046878743635, 'test/num_examples': 43793, 'score': 2415.9324645996094, 'total_duration': 3372.400179862976, 'accumulated_submission_time': 2415.9324645996094, 'accumulated_eval_time': 955.97807097435, 'accumulated_logging_time': 0.19110631942749023, 'global_step': 11541, 'preemption_count': 0}), (12713, {'train/accuracy': 0.9906961917877197, 'train/loss': 0.030567592009902, 'train/mean_average_precision': 0.4085416505627893, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.0444832481443882, 'validation/mean_average_precision': 0.25944469720586666, 'validation/num_examples': 43793, 'test/accuracy': 0.9858844876289368, 'test/loss': 0.047163087874650955, 'test/mean_average_precision': 0.25761031148278557, 'test/num_examples': 43793, 'score': 2655.975471019745, 'total_duration': 3687.4658617973328, 'accumulated_submission_time': 2655.975471019745, 'accumulated_eval_time': 1030.9545719623566, 'accumulated_logging_time': 0.20967650413513184, 'global_step': 12713, 'preemption_count': 0}), (13870, {'train/accuracy': 0.9906468391418457, 'train/loss': 0.030958540737628937, 'train/mean_average_precision': 0.3856650859959224, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.04421554505825043, 'validation/mean_average_precision': 0.2571836699555103, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.046645570546388626, 'test/mean_average_precision': 0.2602141208304837, 'test/num_examples': 43793, 'score': 2895.973425388336, 'total_duration': 4003.602040052414, 'accumulated_submission_time': 2895.973425388336, 'accumulated_eval_time': 1107.043963432312, 'accumulated_logging_time': 0.22904181480407715, 'global_step': 13870, 'preemption_count': 0}), (15025, {'train/accuracy': 0.9910566210746765, 'train/loss': 0.029271526262164116, 'train/mean_average_precision': 0.42564648404610733, 'validation/accuracy': 0.986661970615387, 'validation/loss': 0.04430257901549339, 'validation/mean_average_precision': 0.26422095372942794, 'validation/num_examples': 43793, 'test/accuracy': 0.9858958125114441, 'test/loss': 0.0468885637819767, 'test/mean_average_precision': 0.2597977737904241, 'test/num_examples': 43793, 'score': 3135.9273018836975, 'total_duration': 4317.504724979401, 'accumulated_submission_time': 3135.9273018836975, 'accumulated_eval_time': 1180.9438874721527, 'accumulated_logging_time': 0.2491745948791504, 'global_step': 15025, 'preemption_count': 0}), (16173, {'train/accuracy': 0.9909566044807434, 'train/loss': 0.029773570597171783, 'train/mean_average_precision': 0.41131781100886333, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.04397893324494362, 'validation/mean_average_precision': 0.26776311751215204, 'validation/num_examples': 43793, 'test/accuracy': 0.9860718846321106, 'test/loss': 0.04668596386909485, 'test/mean_average_precision': 0.26253016266681867, 'test/num_examples': 43793, 'score': 3375.8815655708313, 'total_duration': 4633.120508432388, 'accumulated_submission_time': 3375.8815655708313, 'accumulated_eval_time': 1256.55344247818, 'accumulated_logging_time': 0.27141714096069336, 'global_step': 16173, 'preemption_count': 0}), (17317, {'train/accuracy': 0.9914838075637817, 'train/loss': 0.027793170884251595, 'train/mean_average_precision': 0.4619491998492642, 'validation/accuracy': 0.9867228865623474, 'validation/loss': 0.044263266026973724, 'validation/mean_average_precision': 0.2693320215532364, 'validation/num_examples': 43793, 'test/accuracy': 0.9859695434570312, 'test/loss': 0.04712387174367905, 'test/mean_average_precision': 0.2587577291596019, 'test/num_examples': 43793, 'score': 3615.8517298698425, 'total_duration': 4951.44611787796, 'accumulated_submission_time': 3615.8517298698425, 'accumulated_eval_time': 1334.8569085597992, 'accumulated_logging_time': 0.2925727367401123, 'global_step': 17317, 'preemption_count': 0}), (18460, {'train/accuracy': 0.9911417961120605, 'train/loss': 0.02884545922279358, 'train/mean_average_precision': 0.4377091849105926, 'validation/accuracy': 0.9868308305740356, 'validation/loss': 0.044104378670454025, 'validation/mean_average_precision': 0.27214622281942813, 'validation/num_examples': 43793, 'test/accuracy': 0.9860533475875854, 'test/loss': 0.04682868719100952, 'test/mean_average_precision': 0.262396743178888, 'test/num_examples': 43793, 'score': 3855.8258833885193, 'total_duration': 5269.291365623474, 'accumulated_submission_time': 3855.8258833885193, 'accumulated_eval_time': 1412.677294254303, 'accumulated_logging_time': 0.3132791519165039, 'global_step': 18460, 'preemption_count': 0}), (19598, {'train/accuracy': 0.9915357232093811, 'train/loss': 0.027552323415875435, 'train/mean_average_precision': 0.4671392850410019, 'validation/accuracy': 0.9869339466094971, 'validation/loss': 0.04378799721598625, 'validation/mean_average_precision': 0.27159048028357524, 'validation/num_examples': 43793, 'test/accuracy': 0.9861809611320496, 'test/loss': 0.046339575201272964, 'test/mean_average_precision': 0.26829704705419727, 'test/num_examples': 43793, 'score': 4095.7946321964264, 'total_duration': 5585.110351085663, 'accumulated_submission_time': 4095.7946321964264, 'accumulated_eval_time': 1488.4725723266602, 'accumulated_logging_time': 0.3339731693267822, 'global_step': 19598, 'preemption_count': 0}), (20743, {'train/accuracy': 0.9913951754570007, 'train/loss': 0.02799431048333645, 'train/mean_average_precision': 0.44902251236465013, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04400802403688431, 'validation/mean_average_precision': 0.2780012465228094, 'validation/num_examples': 43793, 'test/accuracy': 0.9861177802085876, 'test/loss': 0.046820566058158875, 'test/mean_average_precision': 0.2690552973915572, 'test/num_examples': 43793, 'score': 4335.765934467316, 'total_duration': 5901.565896034241, 'accumulated_submission_time': 4335.765934467316, 'accumulated_eval_time': 1564.9046449661255, 'accumulated_logging_time': 0.35477566719055176, 'global_step': 20743, 'preemption_count': 0}), (21897, {'train/accuracy': 0.9917647838592529, 'train/loss': 0.026797432452440262, 'train/mean_average_precision': 0.48910692151807733, 'validation/accuracy': 0.9868702292442322, 'validation/loss': 0.04401025548577309, 'validation/mean_average_precision': 0.28174934072078295, 'validation/num_examples': 43793, 'test/accuracy': 0.9860954880714417, 'test/loss': 0.04672539234161377, 'test/mean_average_precision': 0.27289647236142517, 'test/num_examples': 43793, 'score': 4575.802174806595, 'total_duration': 6217.112738609314, 'accumulated_submission_time': 4575.802174806595, 'accumulated_eval_time': 1640.3639612197876, 'accumulated_logging_time': 0.37573790550231934, 'global_step': 21897, 'preemption_count': 0})], 'global_step': 21897}
I0305 20:56:32.235854 139980287489216 submission_runner.py:649] Timing: 4575.802174806595
I0305 20:56:32.235891 139980287489216 submission_runner.py:651] Total number of evals: 20
I0305 20:56:32.235924 139980287489216 submission_runner.py:652] ====================
I0305 20:56:32.236102 139980287489216 submission_runner.py:750] Final ogbg score: 4
