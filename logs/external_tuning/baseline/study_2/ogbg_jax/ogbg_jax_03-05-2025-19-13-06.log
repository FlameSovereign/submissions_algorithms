python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-454642995 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-13-06.log
2025-03-05 19:13:07.654117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201987.678333       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201987.685187       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:13:14.080186 139874152301760 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax.
I0305 19:13:15.132548 139874152301760 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:13:15.135646 139874152301760 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:13:15.137362 139874152301760 submission_runner.py:606] Using RNG seed -454642995
I0305 19:13:15.781165 139874152301760 submission_runner.py:615] --- Tuning run 2/5 ---
I0305 19:13:15.781342 139874152301760 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_2.
I0305 19:13:15.781520 139874152301760 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_2/hparams.json.
I0305 19:13:16.013083 139874152301760 submission_runner.py:218] Initializing dataset.
I0305 19:13:16.268294 139874152301760 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:16.348254 139874152301760 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:13:16.594708 139874152301760 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:13:16.644307 139874152301760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:16.676219 139874152301760 submission_runner.py:229] Initializing model.
I0305 19:13:24.644807 139874152301760 submission_runner.py:272] Initializing optimizer.
I0305 19:13:25.055943 139874152301760 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:25.056165 139874152301760 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:25.056981 139874152301760 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_2 with prefix checkpoint_
I0305 19:13:25.057088 139874152301760 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_2/meta_data_0.json.
I0305 19:13:25.057270 139874152301760 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:25.057321 139874152301760 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:25.214516 139874152301760 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_2/flags_0.json.
I0305 19:13:25.247586 139874152301760 submission_runner.py:337] Starting training loop.
I0305 19:13:36.287535 139830612702976 logging_writer.py:48] [0] global_step=0, grad_norm=2.763181447982788, loss=0.7432482838630676
I0305 19:13:36.340625 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:13:36.344259 139874152301760 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:36.347791 139874152301760 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:36.409312 139874152301760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:52.237905 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:14:52.240573 139874152301760 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:52.244316 139874152301760 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:52.303034 139874152301760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:55.163823 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:15:55.166389 139874152301760 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:55.170090 139874152301760 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:55.232437 139874152301760 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:59.359290 139874152301760 submission_runner.py:469] Time since start: 214.11s, 	Step: 1, 	{'train/accuracy': 0.49877113103866577, 'train/loss': 0.7439650893211365, 'train/mean_average_precision': 0.022749856777801804, 'validation/accuracy': 0.5072119235992432, 'validation/loss': 0.7433469295501709, 'validation/mean_average_precision': 0.027750354548743285, 'validation/num_examples': 43793, 'test/accuracy': 0.5080327987670898, 'test/loss': 0.7442464828491211, 'test/mean_average_precision': 0.029261753404640253, 'test/num_examples': 43793, 'score': 11.092934608459473, 'total_duration': 214.11165142059326, 'accumulated_submission_time': 11.092934608459473, 'accumulated_eval_time': 203.01861023902893, 'accumulated_logging_time': 0}
I0305 19:16:59.366273 139732323165952 logging_writer.py:48] [1] accumulated_eval_time=203.019, accumulated_logging_time=0, accumulated_submission_time=11.0929, global_step=1, preemption_count=0, score=11.0929, test/accuracy=0.508033, test/loss=0.744246, test/mean_average_precision=0.0292618, test/num_examples=43793, total_duration=214.112, train/accuracy=0.498771, train/loss=0.743965, train/mean_average_precision=0.0227499, validation/accuracy=0.507212, validation/loss=0.743347, validation/mean_average_precision=0.0277504, validation/num_examples=43793
I0305 19:17:20.228948 139732331558656 logging_writer.py:48] [100] global_step=100, grad_norm=0.40727654099464417, loss=0.35940176248550415
I0305 19:17:41.358402 139732323165952 logging_writer.py:48] [200] global_step=200, grad_norm=0.30244678258895874, loss=0.24502794444561005
I0305 19:18:02.291161 139732331558656 logging_writer.py:48] [300] global_step=300, grad_norm=0.17108727991580963, loss=0.14973345398902893
I0305 19:18:23.278914 139732323165952 logging_writer.py:48] [400] global_step=400, grad_norm=0.10429789870977402, loss=0.09835752844810486
I0305 19:18:44.149123 139732331558656 logging_writer.py:48] [500] global_step=500, grad_norm=0.05525553226470947, loss=0.07123182713985443
I0305 19:19:04.932075 139732323165952 logging_writer.py:48] [600] global_step=600, grad_norm=0.04089004173874855, loss=0.06605139374732971
I0305 19:19:25.833572 139732339951360 logging_writer.py:48] [700] global_step=700, grad_norm=0.0373120941221714, loss=0.06057591363787651
I0305 19:19:46.801054 139732348344064 logging_writer.py:48] [800] global_step=800, grad_norm=0.056641120463609695, loss=0.052818022668361664
I0305 19:20:08.007995 139732339951360 logging_writer.py:48] [900] global_step=900, grad_norm=0.04947706311941147, loss=0.052858904004096985
I0305 19:20:29.339664 139732348344064 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.05174492672085762, loss=0.05456463247537613
I0305 19:20:50.363411 139732339951360 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.011700501665472984, loss=0.054922960698604584
I0305 19:20:59.444014 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:22:12.756748 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:22:14.891948 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:22:16.959862 139874152301760 submission_runner.py:469] Time since start: 531.71s, 	Step: 1145, 	{'train/accuracy': 0.9864374995231628, 'train/loss': 0.05516144260764122, 'train/mean_average_precision': 0.04591063021475647, 'validation/accuracy': 0.983965277671814, 'validation/loss': 0.06401167064905167, 'validation/mean_average_precision': 0.04511029391143287, 'validation/num_examples': 43793, 'test/accuracy': 0.9829360842704773, 'test/loss': 0.06705603748559952, 'test/mean_average_precision': 0.047387303631053146, 'test/num_examples': 43793, 'score': 251.12976384162903, 'total_duration': 531.7122213840485, 'accumulated_submission_time': 251.12976384162903, 'accumulated_eval_time': 280.5344157218933, 'accumulated_logging_time': 0.016844749450683594}
I0305 19:22:16.968968 139732348344064 logging_writer.py:48] [1145] accumulated_eval_time=280.534, accumulated_logging_time=0.0168447, accumulated_submission_time=251.13, global_step=1145, preemption_count=0, score=251.13, test/accuracy=0.982936, test/loss=0.067056, test/mean_average_precision=0.0473873, test/num_examples=43793, total_duration=531.712, train/accuracy=0.986437, train/loss=0.0551614, train/mean_average_precision=0.0459106, validation/accuracy=0.983965, validation/loss=0.0640117, validation/mean_average_precision=0.0451103, validation/num_examples=43793
I0305 19:22:28.332242 139732339951360 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.03133239224553108, loss=0.056905362755060196
I0305 19:22:48.950040 139732348344064 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.025942692533135414, loss=0.05429902672767639
I0305 19:23:10.157284 139732767979264 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.08754968643188477, loss=0.0527491495013237
I0305 19:23:31.183391 139732759586560 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04831192269921303, loss=0.053685057908296585
I0305 19:23:52.274927 139732767979264 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.026425238698720932, loss=0.045451246201992035
I0305 19:24:13.332990 139732759586560 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03845806047320366, loss=0.0473724827170372
I0305 19:24:34.416895 139732767979264 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03027176856994629, loss=0.04763060435652733
I0305 19:24:55.271738 139732759586560 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.026011863723397255, loss=0.05021589249372482
I0305 19:25:16.052615 139732767979264 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.028222206979990005, loss=0.049097687005996704
I0305 19:25:37.405099 139732759586560 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.023269202560186386, loss=0.051367923617362976
I0305 19:25:58.514997 139732767979264 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.02754111774265766, loss=0.049652907997369766
I0305 19:26:17.085000 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:27:29.806186 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:27:31.742312 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:27:33.771037 139874152301760 submission_runner.py:469] Time since start: 848.52s, 	Step: 2289, 	{'train/accuracy': 0.9877419471740723, 'train/loss': 0.04516187682747841, 'train/mean_average_precision': 0.12057029700064636, 'validation/accuracy': 0.9848746061325073, 'validation/loss': 0.054856833070516586, 'validation/mean_average_precision': 0.11576847013670194, 'validation/num_examples': 43793, 'test/accuracy': 0.9838648438453674, 'test/loss': 0.05775333195924759, 'test/mean_average_precision': 0.11517125875675659, 'test/num_examples': 43793, 'score': 491.20559000968933, 'total_duration': 848.5234010219574, 'accumulated_submission_time': 491.20559000968933, 'accumulated_eval_time': 357.22040486335754, 'accumulated_logging_time': 0.03621363639831543}
I0305 19:27:33.780350 139732759586560 logging_writer.py:48] [2289] accumulated_eval_time=357.22, accumulated_logging_time=0.0362136, accumulated_submission_time=491.206, global_step=2289, preemption_count=0, score=491.206, test/accuracy=0.983865, test/loss=0.0577533, test/mean_average_precision=0.115171, test/num_examples=43793, total_duration=848.523, train/accuracy=0.987742, train/loss=0.0451619, train/mean_average_precision=0.12057, validation/accuracy=0.984875, validation/loss=0.0548568, validation/mean_average_precision=0.115768, validation/num_examples=43793
I0305 19:27:36.229356 139732767979264 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.017783543094992638, loss=0.045953188091516495
I0305 19:27:56.734545 139732759586560 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.022691311314702034, loss=0.04370223730802536
I0305 19:28:17.132151 139732767979264 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.023276858031749725, loss=0.04856828972697258
I0305 19:28:37.740836 139732759586560 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.031190449371933937, loss=0.0467255674302578
I0305 19:28:59.258313 139732767979264 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.01885930262506008, loss=0.042955778539180756
I0305 19:29:20.243226 139732759586560 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.03454819694161415, loss=0.04651724547147751
I0305 19:29:41.364127 139732767979264 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.023421594873070717, loss=0.0501076877117157
I0305 19:30:02.597184 139732759586560 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.026893222704529762, loss=0.04651615396142006
I0305 19:30:23.736351 139732767979264 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03377155959606171, loss=0.04734236001968384
I0305 19:30:44.795928 139732759586560 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.018474681302905083, loss=0.043819595128297806
I0305 19:31:06.365914 139732767979264 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02059691958129406, loss=0.046063899993896484
I0305 19:31:27.374523 139732759586560 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.015984181314706802, loss=0.040539272129535675
I0305 19:31:33.871681 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:32:47.934245 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:32:49.918853 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:32:51.814507 139874152301760 submission_runner.py:469] Time since start: 1166.57s, 	Step: 3432, 	{'train/accuracy': 0.9879274964332581, 'train/loss': 0.04282098636031151, 'train/mean_average_precision': 0.16973557183398788, 'validation/accuracy': 0.985166072845459, 'validation/loss': 0.05215247720479965, 'validation/mean_average_precision': 0.15336056071848927, 'validation/num_examples': 43793, 'test/accuracy': 0.9842060208320618, 'test/loss': 0.054896775633096695, 'test/mean_average_precision': 0.15374219588800284, 'test/num_examples': 43793, 'score': 731.2589046955109, 'total_duration': 1166.5668742656708, 'accumulated_submission_time': 731.2589046955109, 'accumulated_eval_time': 435.16318368911743, 'accumulated_logging_time': 0.05500626564025879}
I0305 19:32:51.824471 139732767979264 logging_writer.py:48] [3432] accumulated_eval_time=435.163, accumulated_logging_time=0.0550063, accumulated_submission_time=731.259, global_step=3432, preemption_count=0, score=731.259, test/accuracy=0.984206, test/loss=0.0548968, test/mean_average_precision=0.153742, test/num_examples=43793, total_duration=1166.57, train/accuracy=0.987927, train/loss=0.042821, train/mean_average_precision=0.169736, validation/accuracy=0.985166, validation/loss=0.0521525, validation/mean_average_precision=0.153361, validation/num_examples=43793
I0305 19:33:06.257812 139732759586560 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.019004758447408676, loss=0.04881972447037697
I0305 19:33:27.425242 139732767979264 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.030019322410225868, loss=0.04628504067659378
I0305 19:33:48.663055 139732759586560 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.03343839943408966, loss=0.04726475477218628
I0305 19:34:09.683253 139732767979264 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.023565253242850304, loss=0.046020183712244034
I0305 19:34:31.040849 139732759586560 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.025573143735527992, loss=0.04330127686262131
I0305 19:34:52.222754 139732767979264 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.01944589801132679, loss=0.04810474440455437
I0305 19:35:13.404202 139732759586560 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.013825906440615654, loss=0.04528732970356941
I0305 19:35:34.501596 139732767979264 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.02019931934773922, loss=0.04239632189273834
I0305 19:35:55.677761 139732759586560 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.025383610278367996, loss=0.04483989626169205
I0305 19:36:16.946333 139732767979264 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.01328726764768362, loss=0.048475481569767
I0305 19:36:38.406184 139732759586560 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.026197127997875214, loss=0.04601099714636803
I0305 19:36:51.976950 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:38:04.440402 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:38:06.395700 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:38:08.278242 139874152301760 submission_runner.py:469] Time since start: 1483.03s, 	Step: 4565, 	{'train/accuracy': 0.9884599447250366, 'train/loss': 0.04024913161993027, 'train/mean_average_precision': 0.2074693941699252, 'validation/accuracy': 0.9856154322624207, 'validation/loss': 0.049436211585998535, 'validation/mean_average_precision': 0.18266864093542523, 'validation/num_examples': 43793, 'test/accuracy': 0.9846979379653931, 'test/loss': 0.052152447402477264, 'test/mean_average_precision': 0.18193304438090765, 'test/num_examples': 43793, 'score': 971.3696434497833, 'total_duration': 1483.0306069850922, 'accumulated_submission_time': 971.3696434497833, 'accumulated_eval_time': 511.46442675590515, 'accumulated_logging_time': 0.07589077949523926}
I0305 19:38:08.287194 139732767979264 logging_writer.py:48] [4565] accumulated_eval_time=511.464, accumulated_logging_time=0.0758908, accumulated_submission_time=971.37, global_step=4565, preemption_count=0, score=971.37, test/accuracy=0.984698, test/loss=0.0521524, test/mean_average_precision=0.181933, test/num_examples=43793, total_duration=1483.03, train/accuracy=0.98846, train/loss=0.0402491, train/mean_average_precision=0.207469, validation/accuracy=0.985615, validation/loss=0.0494362, validation/mean_average_precision=0.182669, validation/num_examples=43793
I0305 19:38:15.734236 139732759586560 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.021469194442033768, loss=0.046473726630210876
I0305 19:38:36.886855 139732767979264 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0345563068985939, loss=0.046678151935338974
I0305 19:38:58.136198 139732759586560 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.015981538221240044, loss=0.0442960262298584
I0305 19:39:19.576469 139732767979264 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.029596436768770218, loss=0.05090094357728958
I0305 19:39:40.844090 139732759586560 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.012397216632962227, loss=0.04473884031176567
I0305 19:40:02.370676 139732767979264 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.018764175474643707, loss=0.04352859780192375
I0305 19:40:23.678705 139732759586560 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.02002629078924656, loss=0.04131055250763893
I0305 19:40:44.216597 139732767979264 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.01684335060417652, loss=0.0435640774667263
I0305 19:41:04.806127 139732759586560 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.021522577852010727, loss=0.04804045334458351
I0305 19:41:26.003961 139732767979264 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.03388279303908348, loss=0.042540036141872406
I0305 19:41:46.881215 139732759586560 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.0368114672601223, loss=0.04926387593150139
I0305 19:42:07.807750 139732767979264 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.024021804332733154, loss=0.044950831681489944
I0305 19:42:08.425496 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:43:22.058658 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:43:23.989928 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:43:25.922589 139874152301760 submission_runner.py:469] Time since start: 1800.67s, 	Step: 5704, 	{'train/accuracy': 0.9889758229255676, 'train/loss': 0.03821399807929993, 'train/mean_average_precision': 0.25583006029625704, 'validation/accuracy': 0.9858922958374023, 'validation/loss': 0.047857388854026794, 'validation/mean_average_precision': 0.19973735105812732, 'validation/num_examples': 43793, 'test/accuracy': 0.9850130081176758, 'test/loss': 0.05038413405418396, 'test/mean_average_precision': 0.20289022672155488, 'test/num_examples': 43793, 'score': 1211.4673175811768, 'total_duration': 1800.6749556064606, 'accumulated_submission_time': 1211.4673175811768, 'accumulated_eval_time': 588.9614708423615, 'accumulated_logging_time': 0.09637761116027832}
I0305 19:43:25.932221 139732759586560 logging_writer.py:48] [5704] accumulated_eval_time=588.961, accumulated_logging_time=0.0963776, accumulated_submission_time=1211.47, global_step=5704, preemption_count=0, score=1211.47, test/accuracy=0.985013, test/loss=0.0503841, test/mean_average_precision=0.20289, test/num_examples=43793, total_duration=1800.67, train/accuracy=0.988976, train/loss=0.038214, train/mean_average_precision=0.25583, validation/accuracy=0.985892, validation/loss=0.0478574, validation/mean_average_precision=0.199737, validation/num_examples=43793
I0305 19:43:46.009205 139732767979264 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.016385182738304138, loss=0.04071085527539253
I0305 19:44:06.787187 139732759586560 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.019390519708395004, loss=0.046652160584926605
I0305 19:44:27.485105 139732767979264 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01526735071092844, loss=0.04551822319626808
I0305 19:44:48.589101 139732759586560 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.017685526981949806, loss=0.04455059394240379
I0305 19:45:09.571996 139732767979264 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.021550176665186882, loss=0.04398419335484505
I0305 19:45:30.353961 139732759586560 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.018905969336628914, loss=0.03812291473150253
I0305 19:45:50.418624 139732767979264 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.015679648146033287, loss=0.045052528381347656
I0305 19:46:10.916432 139732759586560 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.015397759154438972, loss=0.044751789420843124
I0305 19:46:31.658558 139732767979264 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.01400636974722147, loss=0.038800474256277084
I0305 19:46:52.211738 139732759586560 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.014396960847079754, loss=0.041855961084365845
I0305 19:47:12.815660 139732767979264 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.01595260575413704, loss=0.04321718215942383
I0305 19:47:25.992915 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:48:37.000985 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:48:38.948266 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:48:40.854157 139874152301760 submission_runner.py:469] Time since start: 2115.61s, 	Step: 6865, 	{'train/accuracy': 0.988731324672699, 'train/loss': 0.03798190504312515, 'train/mean_average_precision': 0.27071761958956864, 'validation/accuracy': 0.9858971834182739, 'validation/loss': 0.047891873866319656, 'validation/mean_average_precision': 0.22091041719475196, 'validation/num_examples': 43793, 'test/accuracy': 0.9850416779518127, 'test/loss': 0.05059099942445755, 'test/mean_average_precision': 0.22152432496286573, 'test/num_examples': 43793, 'score': 1451.4891369342804, 'total_duration': 2115.606386899948, 'accumulated_submission_time': 1451.4891369342804, 'accumulated_eval_time': 663.8225286006927, 'accumulated_logging_time': 0.11505937576293945}
I0305 19:48:40.862932 139732759586560 logging_writer.py:48] [6865] accumulated_eval_time=663.823, accumulated_logging_time=0.115059, accumulated_submission_time=1451.49, global_step=6865, preemption_count=0, score=1451.49, test/accuracy=0.985042, test/loss=0.050591, test/mean_average_precision=0.221524, test/num_examples=43793, total_duration=2115.61, train/accuracy=0.988731, train/loss=0.0379819, train/mean_average_precision=0.270718, validation/accuracy=0.985897, validation/loss=0.0478919, validation/mean_average_precision=0.22091, validation/num_examples=43793
I0305 19:48:48.396519 139732767979264 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.023248249664902687, loss=0.045693282037973404
I0305 19:49:09.357742 139732759586560 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.018206913024187088, loss=0.04044792056083679
I0305 19:49:30.058670 139732767979264 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.01677144132554531, loss=0.04333589971065521
I0305 19:49:50.542453 139732759586560 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.02655758149921894, loss=0.04110528528690338
I0305 19:50:11.191847 139732767979264 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.014379242435097694, loss=0.03648952767252922
I0305 19:50:31.963236 139732759586560 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.014992930926382542, loss=0.0403435193002224
I0305 19:50:52.683914 139732767979264 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.019304044544696808, loss=0.039558276534080505
I0305 19:51:13.550351 139732759586560 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01969345659017563, loss=0.03984512388706207
I0305 19:51:34.197643 139732767979264 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.017476119101047516, loss=0.03945459797978401
I0305 19:51:55.006393 139732759586560 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.024520475417375565, loss=0.042851321399211884
I0305 19:52:15.618768 139732767979264 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.024163058027625084, loss=0.04333551600575447
I0305 19:52:36.493338 139732759586560 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0205541905015707, loss=0.03875225782394409
I0305 19:52:40.880694 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:53:53.120819 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:53:55.059234 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:53:57.010079 139874152301760 submission_runner.py:469] Time since start: 2431.76s, 	Step: 8022, 	{'train/accuracy': 0.9893003702163696, 'train/loss': 0.035843394696712494, 'train/mean_average_precision': 0.31398676502207606, 'validation/accuracy': 0.9860972762107849, 'validation/loss': 0.04667762294411659, 'validation/mean_average_precision': 0.2330377733332588, 'validation/num_examples': 43793, 'test/accuracy': 0.9852880835533142, 'test/loss': 0.049434591084718704, 'test/mean_average_precision': 0.23080995411207608, 'test/num_examples': 43793, 'score': 1691.4678633213043, 'total_duration': 2431.7623097896576, 'accumulated_submission_time': 1691.4678633213043, 'accumulated_eval_time': 739.9517350196838, 'accumulated_logging_time': 0.13390016555786133}
I0305 19:53:57.020889 139732767979264 logging_writer.py:48] [8022] accumulated_eval_time=739.952, accumulated_logging_time=0.1339, accumulated_submission_time=1691.47, global_step=8022, preemption_count=0, score=1691.47, test/accuracy=0.985288, test/loss=0.0494346, test/mean_average_precision=0.23081, test/num_examples=43793, total_duration=2431.76, train/accuracy=0.9893, train/loss=0.0358434, train/mean_average_precision=0.313987, validation/accuracy=0.986097, validation/loss=0.0466776, validation/mean_average_precision=0.233038, validation/num_examples=43793
I0305 19:54:13.282547 139732759586560 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.01518236007541418, loss=0.04063642397522926
I0305 19:54:33.731291 139732767979264 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.02439396269619465, loss=0.04579029977321625
I0305 19:54:54.009696 139732759586560 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.015375281684100628, loss=0.04141015559434891
I0305 19:55:14.524039 139732767979264 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.018274057656526566, loss=0.03920840099453926
I0305 19:55:35.268910 139732759586560 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.016068855300545692, loss=0.038450341671705246
I0305 19:55:55.529461 139732767979264 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.018546858802437782, loss=0.04404028132557869
I0305 19:56:15.921852 139732759586560 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.030553003773093224, loss=0.03759779781103134
I0305 19:56:36.725242 139732767979264 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.013573208823800087, loss=0.03688795492053032
I0305 19:56:57.541991 139732759586560 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.020729122683405876, loss=0.04187735542654991
I0305 19:57:18.177150 139732767979264 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.017354432493448257, loss=0.039595477283000946
I0305 19:57:38.659509 139732759586560 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.017482606694102287, loss=0.039349135011434555
I0305 19:57:57.015107 139874152301760 spec.py:321] Evaluating on the training split.
I0305 19:59:08.527025 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 19:59:10.592154 139874152301760 spec.py:349] Evaluating on the test split.
I0305 19:59:12.479511 139874152301760 submission_runner.py:469] Time since start: 2747.23s, 	Step: 9190, 	{'train/accuracy': 0.9897857904434204, 'train/loss': 0.034656308591365814, 'train/mean_average_precision': 0.3368483868153639, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.04576916992664337, 'validation/mean_average_precision': 0.2444831890665955, 'validation/num_examples': 43793, 'test/accuracy': 0.9854721426963806, 'test/loss': 0.0485151968896389, 'test/mean_average_precision': 0.23839677236397414, 'test/num_examples': 43793, 'score': 1931.4174807071686, 'total_duration': 2747.2317550182343, 'accumulated_submission_time': 1931.4174807071686, 'accumulated_eval_time': 815.4159688949585, 'accumulated_logging_time': 0.1585092544555664}
I0305 19:59:12.488893 139732767979264 logging_writer.py:48] [9190] accumulated_eval_time=815.416, accumulated_logging_time=0.158509, accumulated_submission_time=1931.42, global_step=9190, preemption_count=0, score=1931.42, test/accuracy=0.985472, test/loss=0.0485152, test/mean_average_precision=0.238397, test/num_examples=43793, total_duration=2747.23, train/accuracy=0.989786, train/loss=0.0346563, train/mean_average_precision=0.336848, validation/accuracy=0.986351, validation/loss=0.0457692, validation/mean_average_precision=0.244483, validation/num_examples=43793
I0305 19:59:14.757625 139732759586560 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.018500085920095444, loss=0.041301652789115906
I0305 19:59:35.505843 139732767979264 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.019712653011083603, loss=0.04210779443383217
I0305 19:59:56.120197 139732759586560 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.012560797855257988, loss=0.03580985218286514
I0305 20:00:16.855904 139732767979264 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.014828277751803398, loss=0.038101501762866974
I0305 20:00:37.407613 139732759586560 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.016823729500174522, loss=0.03869521617889404
I0305 20:00:58.356620 139732767979264 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.01725391112267971, loss=0.03649476170539856
I0305 20:01:18.839848 139732759586560 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.019253438338637352, loss=0.04203854128718376
I0305 20:01:38.891026 139732767979264 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.015453377738595009, loss=0.03938237205147743
I0305 20:01:59.197427 139732759586560 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.020819205790758133, loss=0.036595169454813004
I0305 20:02:19.969234 139732767979264 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.016201801598072052, loss=0.0376184917986393
I0305 20:02:40.691478 139732759586560 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.023326631635427475, loss=0.040589287877082825
I0305 20:03:01.461335 139732767979264 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.016770586371421814, loss=0.04132289066910744
I0305 20:03:12.582129 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:04:23.685443 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:04:25.649972 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:04:27.547959 139874152301760 submission_runner.py:469] Time since start: 3062.30s, 	Step: 10354, 	{'train/accuracy': 0.990146815776825, 'train/loss': 0.03284255787730217, 'train/mean_average_precision': 0.37508642111157514, 'validation/accuracy': 0.9864902496337891, 'validation/loss': 0.045455023646354675, 'validation/mean_average_precision': 0.2446381251014512, 'validation/num_examples': 43793, 'test/accuracy': 0.985659122467041, 'test/loss': 0.04795210435986519, 'test/mean_average_precision': 0.2450861195018509, 'test/num_examples': 43793, 'score': 2171.470983982086, 'total_duration': 3062.3003227710724, 'accumulated_submission_time': 2171.470983982086, 'accumulated_eval_time': 890.38174700737, 'accumulated_logging_time': 0.1770155429840088}
I0305 20:04:27.559061 139732759586560 logging_writer.py:48] [10354] accumulated_eval_time=890.382, accumulated_logging_time=0.177016, accumulated_submission_time=2171.47, global_step=10354, preemption_count=0, score=2171.47, test/accuracy=0.985659, test/loss=0.0479521, test/mean_average_precision=0.245086, test/num_examples=43793, total_duration=3062.3, train/accuracy=0.990147, train/loss=0.0328426, train/mean_average_precision=0.375086, validation/accuracy=0.98649, validation/loss=0.045455, validation/mean_average_precision=0.244638, validation/num_examples=43793
I0305 20:04:37.332829 139732767979264 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.014528350904583931, loss=0.037445615977048874
I0305 20:04:58.171992 139732759586560 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.01696852780878544, loss=0.03788921609520912
I0305 20:05:18.787360 139732767979264 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.01601449027657509, loss=0.03984711691737175
I0305 20:05:39.558962 139732759586560 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.02168373577296734, loss=0.04268781840801239
I0305 20:06:00.378581 139732767979264 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.026755794882774353, loss=0.036143869161605835
I0305 20:06:21.122148 139732759586560 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.01866935007274151, loss=0.038425806909799576
I0305 20:06:41.937095 139732767979264 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.020825577899813652, loss=0.03945881128311157
I0305 20:07:02.869826 139732759586560 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.025211021304130554, loss=0.036829713732004166
I0305 20:07:23.442214 139732767979264 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.017418140545487404, loss=0.03748410567641258
I0305 20:07:44.597581 139732759586560 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.022967955097556114, loss=0.03732890635728836
I0305 20:08:05.453930 139732767979264 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.024992628023028374, loss=0.040924232453107834
I0305 20:08:26.282894 139732759586560 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.01904967986047268, loss=0.037240516394376755
I0305 20:08:27.570296 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:09:40.219276 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:09:42.186402 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:09:44.201313 139874152301760 submission_runner.py:469] Time since start: 3378.95s, 	Step: 11507, 	{'train/accuracy': 0.9903236031532288, 'train/loss': 0.03239036351442337, 'train/mean_average_precision': 0.3871162408295008, 'validation/accuracy': 0.9866433143615723, 'validation/loss': 0.045177657157182693, 'validation/mean_average_precision': 0.25284161528217025, 'validation/num_examples': 43793, 'test/accuracy': 0.9857555627822876, 'test/loss': 0.04797894507646561, 'test/mean_average_precision': 0.2481245184895599, 'test/num_examples': 43793, 'score': 2411.435134410858, 'total_duration': 3378.9535393714905, 'accumulated_submission_time': 2411.435134410858, 'accumulated_eval_time': 967.0125823020935, 'accumulated_logging_time': 0.19714641571044922}
I0305 20:09:44.211317 139732767979264 logging_writer.py:48] [11507] accumulated_eval_time=967.013, accumulated_logging_time=0.197146, accumulated_submission_time=2411.44, global_step=11507, preemption_count=0, score=2411.44, test/accuracy=0.985756, test/loss=0.0479789, test/mean_average_precision=0.248125, test/num_examples=43793, total_duration=3378.95, train/accuracy=0.990324, train/loss=0.0323904, train/mean_average_precision=0.387116, validation/accuracy=0.986643, validation/loss=0.0451777, validation/mean_average_precision=0.252842, validation/num_examples=43793
I0305 20:10:02.890600 139732759586560 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.017458781599998474, loss=0.037524789571762085
I0305 20:10:22.603205 139732767979264 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.016275446861982346, loss=0.037298716604709625
I0305 20:10:42.348091 139732759586560 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.02327623777091503, loss=0.03912472724914551
I0305 20:11:02.226801 139732767979264 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.02794468402862549, loss=0.03839419037103653
I0305 20:11:23.049244 139732759586560 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.02217843383550644, loss=0.037513963878154755
I0305 20:11:43.942072 139732767979264 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.022778721526265144, loss=0.034024082124233246
I0305 20:12:05.020090 139732759586560 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.021054720506072044, loss=0.038264721632003784
I0305 20:12:26.262221 139732767979264 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.0203609187155962, loss=0.03725747764110565
I0305 20:12:47.282516 139732759586560 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.01705816201865673, loss=0.040822550654411316
I0305 20:13:07.995734 139732767979264 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.02695538103580475, loss=0.036306001245975494
I0305 20:13:28.558183 139732759586560 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.03330878168344498, loss=0.041328296065330505
I0305 20:13:44.264217 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:14:54.942246 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:14:57.005064 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:14:59.076029 139874152301760 submission_runner.py:469] Time since start: 3693.83s, 	Step: 12678, 	{'train/accuracy': 0.9907436966896057, 'train/loss': 0.030696703121066093, 'train/mean_average_precision': 0.4229015274387266, 'validation/accuracy': 0.9866238236427307, 'validation/loss': 0.04529475048184395, 'validation/mean_average_precision': 0.25419988198231624, 'validation/num_examples': 43793, 'test/accuracy': 0.985772430896759, 'test/loss': 0.048084307461977005, 'test/mean_average_precision': 0.2524693550631113, 'test/num_examples': 43793, 'score': 2651.4471337795258, 'total_duration': 3693.828275680542, 'accumulated_submission_time': 2651.4471337795258, 'accumulated_eval_time': 1041.8242259025574, 'accumulated_logging_time': 0.21685123443603516}
I0305 20:14:59.086291 139732767979264 logging_writer.py:48] [12678] accumulated_eval_time=1041.82, accumulated_logging_time=0.216851, accumulated_submission_time=2651.45, global_step=12678, preemption_count=0, score=2651.45, test/accuracy=0.985772, test/loss=0.0480843, test/mean_average_precision=0.252469, test/num_examples=43793, total_duration=3693.83, train/accuracy=0.990744, train/loss=0.0306967, train/mean_average_precision=0.422902, validation/accuracy=0.986624, validation/loss=0.0452948, validation/mean_average_precision=0.2542, validation/num_examples=43793
I0305 20:15:03.665504 139732759586560 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.024980904534459114, loss=0.03916170820593834
I0305 20:15:23.665361 139732767979264 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.018065160140395164, loss=0.03858773410320282
I0305 20:15:44.249766 139732759586560 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.01706637069582939, loss=0.03363679721951485
I0305 20:16:04.930366 139732767979264 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.022775869816541672, loss=0.03770923614501953
I0305 20:16:25.601974 139732759586560 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.026230035349726677, loss=0.03533391281962395
I0305 20:16:46.599897 139732767979264 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.01757495291531086, loss=0.03663003072142601
I0305 20:17:07.450614 139732759586560 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.02566683106124401, loss=0.039329953491687775
I0305 20:17:28.542350 139732767979264 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02009901963174343, loss=0.038555290549993515
I0305 20:17:49.655461 139732759586560 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.029103057458996773, loss=0.03733672946691513
I0305 20:18:09.912587 139732767979264 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.017497535794973373, loss=0.037520237267017365
I0305 20:18:30.466995 139732759586560 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.017805881798267365, loss=0.03682780638337135
I0305 20:18:51.139062 139732767979264 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.02047715336084366, loss=0.036202993243932724
I0305 20:18:59.258915 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:20:09.003891 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:20:10.945175 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:20:12.981917 139874152301760 submission_runner.py:469] Time since start: 4007.73s, 	Step: 13841, 	{'train/accuracy': 0.99073725938797, 'train/loss': 0.031154727563261986, 'train/mean_average_precision': 0.41898705233073286, 'validation/accuracy': 0.9866591095924377, 'validation/loss': 0.04519756883382797, 'validation/mean_average_precision': 0.25840477633142106, 'validation/num_examples': 43793, 'test/accuracy': 0.9856797456741333, 'test/loss': 0.048298295587301254, 'test/mean_average_precision': 0.2470126916673839, 'test/num_examples': 43793, 'score': 2891.5813200473785, 'total_duration': 4007.7341310977936, 'accumulated_submission_time': 2891.5813200473785, 'accumulated_eval_time': 1115.5470321178436, 'accumulated_logging_time': 0.23698925971984863}
I0305 20:20:12.992535 139732759586560 logging_writer.py:48] [13841] accumulated_eval_time=1115.55, accumulated_logging_time=0.236989, accumulated_submission_time=2891.58, global_step=13841, preemption_count=0, score=2891.58, test/accuracy=0.98568, test/loss=0.0482983, test/mean_average_precision=0.247013, test/num_examples=43793, total_duration=4007.73, train/accuracy=0.990737, train/loss=0.0311547, train/mean_average_precision=0.418987, validation/accuracy=0.986659, validation/loss=0.0451976, validation/mean_average_precision=0.258405, validation/num_examples=43793
I0305 20:20:24.921188 139732767979264 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.02356950379908085, loss=0.03679380565881729
I0305 20:20:45.102057 139732759586560 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.02215416543185711, loss=0.03643546998500824
I0305 20:21:05.937688 139732767979264 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.017711207270622253, loss=0.0339578241109848
I0305 20:21:26.654304 139732759586560 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.02779795043170452, loss=0.03828156366944313
I0305 20:21:47.481267 139732767979264 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.023065101355314255, loss=0.03580133244395256
I0305 20:22:08.245599 139732759586560 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.024144595488905907, loss=0.03562290966510773
I0305 20:22:28.935255 139732767979264 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.018622171133756638, loss=0.03451993688941002
I0305 20:22:49.542777 139732759586560 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.019946757704019547, loss=0.036925189197063446
I0305 20:23:10.012258 139732767979264 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.018717262893915176, loss=0.036315083503723145
I0305 20:23:30.296726 139732759586560 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.024394217878580093, loss=0.03909241408109665
I0305 20:23:51.006366 139732767979264 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.02402876503765583, loss=0.03769083321094513
I0305 20:24:11.715628 139732759586560 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.019351372495293617, loss=0.03703146427869797
I0305 20:24:13.180789 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:25:25.291193 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:25:27.237138 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:25:29.133251 139874152301760 submission_runner.py:469] Time since start: 4323.89s, 	Step: 15008, 	{'train/accuracy': 0.9914278984069824, 'train/loss': 0.028476286679506302, 'train/mean_average_precision': 0.4560264047999383, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.04524127021431923, 'validation/mean_average_precision': 0.25725007743941913, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.04817013069987297, 'test/mean_average_precision': 0.25109018127640476, 'test/num_examples': 43793, 'score': 3131.730295419693, 'total_duration': 4323.885477781296, 'accumulated_submission_time': 3131.730295419693, 'accumulated_eval_time': 1191.4993033409119, 'accumulated_logging_time': 0.2574193477630615}
I0305 20:25:29.143050 139732767979264 logging_writer.py:48] [15008] accumulated_eval_time=1191.5, accumulated_logging_time=0.257419, accumulated_submission_time=3131.73, global_step=15008, preemption_count=0, score=3131.73, test/accuracy=0.985859, test/loss=0.0481701, test/mean_average_precision=0.25109, test/num_examples=43793, total_duration=4323.89, train/accuracy=0.991428, train/loss=0.0284763, train/mean_average_precision=0.456026, validation/accuracy=0.986692, validation/loss=0.0452413, validation/mean_average_precision=0.25725, validation/num_examples=43793
I0305 20:25:48.219000 139732759586560 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.01944827474653721, loss=0.03579573333263397
I0305 20:26:08.822820 139732767979264 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.028482098132371902, loss=0.03544990345835686
I0305 20:26:29.600894 139732759586560 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.024667076766490936, loss=0.036175500601530075
I0305 20:26:50.373881 139732767979264 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.02160937339067459, loss=0.03720184788107872
I0305 20:27:11.230109 139732759586560 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.02462686598300934, loss=0.034035757184028625
I0305 20:27:31.781943 139732767979264 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.021869728341698647, loss=0.035429175943136215
I0305 20:27:52.257923 139732759586560 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.022180091589689255, loss=0.03526829183101654
I0305 20:28:13.046439 139732767979264 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.022977814078330994, loss=0.0361797958612442
I0305 20:28:33.690495 139732759586560 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.025731686502695084, loss=0.03732234239578247
I0305 20:28:54.513657 139732767979264 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.023123525083065033, loss=0.03503507375717163
I0305 20:29:15.307034 139732759586560 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.02536727488040924, loss=0.038208961486816406
I0305 20:29:29.138504 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:30:40.495473 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:30:42.472183 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:30:44.422126 139874152301760 submission_runner.py:469] Time since start: 4639.17s, 	Step: 16167, 	{'train/accuracy': 0.9910731315612793, 'train/loss': 0.029393423348665237, 'train/mean_average_precision': 0.4634485163445269, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04545791074633598, 'validation/mean_average_precision': 0.25572840462593255, 'validation/num_examples': 43793, 'test/accuracy': 0.9857509732246399, 'test/loss': 0.048647813498973846, 'test/mean_average_precision': 0.25169142053904886, 'test/num_examples': 43793, 'score': 3371.6855449676514, 'total_duration': 4639.17437338829, 'accumulated_submission_time': 3371.6855449676514, 'accumulated_eval_time': 1266.7827770709991, 'accumulated_logging_time': 0.27634644508361816}
I0305 20:30:44.432543 139732767979264 logging_writer.py:48] [16167] accumulated_eval_time=1266.78, accumulated_logging_time=0.276346, accumulated_submission_time=3371.69, global_step=16167, preemption_count=0, score=3371.69, test/accuracy=0.985751, test/loss=0.0486478, test/mean_average_precision=0.251691, test/num_examples=43793, total_duration=4639.17, train/accuracy=0.991073, train/loss=0.0293934, train/mean_average_precision=0.463449, validation/accuracy=0.986696, validation/loss=0.0454579, validation/mean_average_precision=0.255728, validation/num_examples=43793
I0305 20:30:51.442603 139732759586560 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.024213742464780807, loss=0.032674551010131836
I0305 20:31:11.985057 139732767979264 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.03047802299261093, loss=0.033579036593437195
I0305 20:31:32.782028 139732759586560 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.026659991592168808, loss=0.03542259708046913
I0305 20:31:53.591025 139732767979264 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.020268328487873077, loss=0.03443353623151779
I0305 20:32:14.232025 139732759586560 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.022913362830877304, loss=0.03385675325989723
I0305 20:32:34.941905 139732767979264 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.03158704563975334, loss=0.035803504288196564
I0305 20:32:55.794689 139732759586560 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.030277123674750328, loss=0.036692604422569275
I0305 20:33:16.874089 139732767979264 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.025091366842389107, loss=0.03396550938487053
I0305 20:33:37.743577 139732759586560 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.023977842181921005, loss=0.031723447144031525
I0305 20:33:58.594995 139732767979264 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.027146490290760994, loss=0.036817096173763275
I0305 20:34:18.992154 139732759586560 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.02746463380753994, loss=0.03271682560443878
I0305 20:34:39.246633 139732767979264 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.026878029108047485, loss=0.03663657605648041
I0305 20:34:44.429381 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:35:53.793163 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:35:55.738548 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:35:57.693470 139874152301760 submission_runner.py:469] Time since start: 4952.45s, 	Step: 17327, 	{'train/accuracy': 0.9917005300521851, 'train/loss': 0.02687568962574005, 'train/mean_average_precision': 0.5139073470529663, 'validation/accuracy': 0.986731767654419, 'validation/loss': 0.04569559171795845, 'validation/mean_average_precision': 0.2583347806244402, 'validation/num_examples': 43793, 'test/accuracy': 0.985765278339386, 'test/loss': 0.048895251005887985, 'test/mean_average_precision': 0.2535455374173522, 'test/num_examples': 43793, 'score': 3611.642764568329, 'total_duration': 4952.445822954178, 'accumulated_submission_time': 3611.642764568329, 'accumulated_eval_time': 1340.0468039512634, 'accumulated_logging_time': 0.296067476272583}
I0305 20:35:57.704869 139732759586560 logging_writer.py:48] [17327] accumulated_eval_time=1340.05, accumulated_logging_time=0.296067, accumulated_submission_time=3611.64, global_step=17327, preemption_count=0, score=3611.64, test/accuracy=0.985765, test/loss=0.0488953, test/mean_average_precision=0.253546, test/num_examples=43793, total_duration=4952.45, train/accuracy=0.991701, train/loss=0.0268757, train/mean_average_precision=0.513907, validation/accuracy=0.986732, validation/loss=0.0456956, validation/mean_average_precision=0.258335, validation/num_examples=43793
I0305 20:36:13.237098 139732767979264 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.025124764069914818, loss=0.03731701150536537
I0305 20:36:34.132664 139732759586560 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.025812841951847076, loss=0.033980678766965866
I0305 20:36:54.696166 139732767979264 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.032730553299188614, loss=0.036324359476566315
I0305 20:37:15.010367 139732759586560 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.019892292097210884, loss=0.03210316225886345
I0305 20:37:35.450759 139732767979264 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.026082202792167664, loss=0.03651367127895355
I0305 20:37:55.699189 139732759586560 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.02551589533686638, loss=0.03662954270839691
I0305 20:38:15.854166 139732767979264 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.03012334555387497, loss=0.03738000988960266
I0305 20:38:36.259882 139732759586560 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.027589907869696617, loss=0.034836966544389725
I0305 20:38:56.300372 139732767979264 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.026809515431523323, loss=0.03591196984052658
I0305 20:39:17.180775 139732759586560 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.027594847604632378, loss=0.0348736010491848
I0305 20:39:37.748349 139732767979264 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.022683676332235336, loss=0.03431526944041252
I0305 20:39:57.839772 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:41:10.026558 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:41:12.010839 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:41:13.926933 139874152301760 submission_runner.py:469] Time since start: 5268.68s, 	Step: 18496, 	{'train/accuracy': 0.991524338722229, 'train/loss': 0.027586758136749268, 'train/mean_average_precision': 0.4999564340516715, 'validation/accuracy': 0.986860454082489, 'validation/loss': 0.045700524002313614, 'validation/mean_average_precision': 0.2613823137622827, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.04874904453754425, 'test/mean_average_precision': 0.25528535449662565, 'test/num_examples': 43793, 'score': 3851.7410027980804, 'total_duration': 5268.67920255661, 'accumulated_submission_time': 3851.7410027980804, 'accumulated_eval_time': 1416.1338195800781, 'accumulated_logging_time': 0.31731724739074707}
I0305 20:41:13.939020 139732759586560 logging_writer.py:48] [18496] accumulated_eval_time=1416.13, accumulated_logging_time=0.317317, accumulated_submission_time=3851.74, global_step=18496, preemption_count=0, score=3851.74, test/accuracy=0.985906, test/loss=0.048749, test/mean_average_precision=0.255285, test/num_examples=43793, total_duration=5268.68, train/accuracy=0.991524, train/loss=0.0275868, train/mean_average_precision=0.499956, validation/accuracy=0.98686, validation/loss=0.0457005, validation/mean_average_precision=0.261382, validation/num_examples=43793
I0305 20:41:15.024064 139732767979264 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.024373017251491547, loss=0.03199794515967369
I0305 20:41:35.588987 139732759586560 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.031570758670568466, loss=0.03427401930093765
I0305 20:41:56.382098 139732767979264 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.028179239481687546, loss=0.035360969603061676
I0305 20:42:17.163309 139732759586560 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.02829701639711857, loss=0.03547538444399834
I0305 20:42:38.423131 139732767979264 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.027931300923228264, loss=0.034218646585941315
I0305 20:42:59.659470 139732759586560 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.03272305056452751, loss=0.03317955508828163
I0305 20:43:20.618950 139732767979264 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.023637844249606133, loss=0.03237668797373772
I0305 20:43:41.598701 139732759586560 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.026409650221467018, loss=0.03533181548118591
I0305 20:44:02.699142 139732767979264 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.029790198430418968, loss=0.037353094667196274
I0305 20:44:23.353559 139732759586560 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.028479021042585373, loss=0.033859942108392715
I0305 20:44:44.530648 139732767979264 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.03177053481340408, loss=0.033395178616046906
I0305 20:45:05.231830 139732759586560 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.032159291207790375, loss=0.03412390127778053
I0305 20:45:14.065787 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:46:25.907982 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:46:28.002153 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:46:30.077043 139874152301760 submission_runner.py:469] Time since start: 5584.83s, 	Step: 19645, 	{'train/accuracy': 0.9923588037490845, 'train/loss': 0.02485167421400547, 'train/mean_average_precision': 0.549003910825518, 'validation/accuracy': 0.9868689775466919, 'validation/loss': 0.045794595032930374, 'validation/mean_average_precision': 0.2584218284647551, 'validation/num_examples': 43793, 'test/accuracy': 0.9859084486961365, 'test/loss': 0.04903876781463623, 'test/mean_average_precision': 0.2585274268765164, 'test/num_examples': 43793, 'score': 4091.8255486488342, 'total_duration': 5584.82938170433, 'accumulated_submission_time': 4091.8255486488342, 'accumulated_eval_time': 1492.1450102329254, 'accumulated_logging_time': 0.33849430084228516}
I0305 20:46:30.087724 139732767979264 logging_writer.py:48] [19645] accumulated_eval_time=1492.15, accumulated_logging_time=0.338494, accumulated_submission_time=4091.83, global_step=19645, preemption_count=0, score=4091.83, test/accuracy=0.985908, test/loss=0.0490388, test/mean_average_precision=0.258527, test/num_examples=43793, total_duration=5584.83, train/accuracy=0.992359, train/loss=0.0248517, train/mean_average_precision=0.549004, validation/accuracy=0.986869, validation/loss=0.0457946, validation/mean_average_precision=0.258422, validation/num_examples=43793
I0305 20:46:41.357416 139732759586560 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.025340156629681587, loss=0.034368183463811874
I0305 20:47:01.626255 139732767979264 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.034135252237319946, loss=0.03360086306929588
I0305 20:47:22.200547 139732759586560 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.023663770407438278, loss=0.03301229700446129
I0305 20:47:42.880678 139732767979264 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.03019215166568756, loss=0.03272112458944321
I0305 20:48:03.672576 139732759586560 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.024557337164878845, loss=0.031034395098686218
I0305 20:48:24.528723 139732767979264 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.029359476640820503, loss=0.033230092376470566
I0305 20:48:45.025909 139732759586560 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.029244355857372284, loss=0.03529266640543938
I0305 20:49:05.643238 139732767979264 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.027198586612939835, loss=0.03262203559279442
I0305 20:49:26.820769 139732759586560 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.02678713947534561, loss=0.03180158510804176
I0305 20:49:47.679828 139732767979264 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.03269445523619652, loss=0.03674303740262985
I0305 20:50:08.656968 139732759586560 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.03335366025567055, loss=0.03365734592080116
I0305 20:50:29.584144 139732767979264 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.03072807565331459, loss=0.03446033224463463
I0305 20:50:30.212975 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:51:40.838868 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:51:42.792264 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:51:44.719308 139874152301760 submission_runner.py:469] Time since start: 5899.47s, 	Step: 20804, 	{'train/accuracy': 0.9921115040779114, 'train/loss': 0.025916822254657745, 'train/mean_average_precision': 0.5165288718174297, 'validation/accuracy': 0.986763060092926, 'validation/loss': 0.046055614948272705, 'validation/mean_average_precision': 0.2582366919475289, 'validation/num_examples': 43793, 'test/accuracy': 0.9858600497245789, 'test/loss': 0.04925216734409332, 'test/mean_average_precision': 0.2554506800584729, 'test/num_examples': 43793, 'score': 4331.910178184509, 'total_duration': 5899.47155380249, 'accumulated_submission_time': 4331.910178184509, 'accumulated_eval_time': 1566.6511771678925, 'accumulated_logging_time': 0.35932350158691406}
I0305 20:51:44.730261 139732759586560 logging_writer.py:48] [20804] accumulated_eval_time=1566.65, accumulated_logging_time=0.359324, accumulated_submission_time=4331.91, global_step=20804, preemption_count=0, score=4331.91, test/accuracy=0.98586, test/loss=0.0492522, test/mean_average_precision=0.255451, test/num_examples=43793, total_duration=5899.47, train/accuracy=0.992112, train/loss=0.0259168, train/mean_average_precision=0.516529, validation/accuracy=0.986763, validation/loss=0.0460556, validation/mean_average_precision=0.258237, validation/num_examples=43793
I0305 20:52:05.117163 139732767979264 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.029127156361937523, loss=0.031355444341897964
I0305 20:52:25.890488 139732759586560 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0282954853028059, loss=0.03176400437951088
I0305 20:52:46.450531 139732767979264 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.03311906009912491, loss=0.033781133592128754
I0305 20:53:07.471397 139732759586560 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.028396081179380417, loss=0.03279697149991989
I0305 20:53:28.139598 139732767979264 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.045119721442461014, loss=0.033144403249025345
I0305 20:53:49.076548 139732759586560 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.036513179540634155, loss=0.03405527397990227
I0305 20:54:10.099109 139732767979264 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.03499500826001167, loss=0.03291963413357735
I0305 20:54:31.191156 139732759586560 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.032696325331926346, loss=0.03401995450258255
I0305 20:54:52.212000 139732767979264 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.029536249116063118, loss=0.03274456411600113
I0305 20:55:12.910251 139732759586560 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.03170836344361305, loss=0.0333709642291069
I0305 20:55:33.394598 139732767979264 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.029181065037846565, loss=0.03384261950850487
I0305 20:55:44.779551 139874152301760 spec.py:321] Evaluating on the training split.
I0305 20:56:55.520089 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 20:56:57.445556 139874152301760 spec.py:349] Evaluating on the test split.
I0305 20:56:59.347062 139874152301760 submission_runner.py:469] Time since start: 6214.10s, 	Step: 21956, 	{'train/accuracy': 0.9928665161132812, 'train/loss': 0.023163488134741783, 'train/mean_average_precision': 0.5913190726050988, 'validation/accuracy': 0.9867261052131653, 'validation/loss': 0.04660394787788391, 'validation/mean_average_precision': 0.2571444896194497, 'validation/num_examples': 43793, 'test/accuracy': 0.9857956171035767, 'test/loss': 0.04987436532974243, 'test/mean_average_precision': 0.24984429184119222, 'test/num_examples': 43793, 'score': 4571.919264316559, 'total_duration': 6214.099309921265, 'accumulated_submission_time': 4571.919264316559, 'accumulated_eval_time': 1641.2185254096985, 'accumulated_logging_time': 0.3810579776763916}
I0305 20:56:59.358835 139732759586560 logging_writer.py:48] [21956] accumulated_eval_time=1641.22, accumulated_logging_time=0.381058, accumulated_submission_time=4571.92, global_step=21956, preemption_count=0, score=4571.92, test/accuracy=0.985796, test/loss=0.0498744, test/mean_average_precision=0.249844, test/num_examples=43793, total_duration=6214.1, train/accuracy=0.992867, train/loss=0.0231635, train/mean_average_precision=0.591319, validation/accuracy=0.986726, validation/loss=0.0466039, validation/mean_average_precision=0.257144, validation/num_examples=43793
I0305 20:57:08.880196 139732767979264 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.03515950217843056, loss=0.03382427245378494
I0305 20:57:30.047129 139732759586560 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.029371505603194237, loss=0.032395001500844955
I0305 20:57:50.982955 139732767979264 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.027064433321356773, loss=0.02990325354039669
I0305 20:58:11.784221 139732759586560 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.03425140678882599, loss=0.03542814776301384
I0305 20:58:32.300509 139732767979264 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.03586125746369362, loss=0.03034970536828041
I0305 20:58:53.393234 139732759586560 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.036069106310606, loss=0.03191223740577698
I0305 20:59:14.685981 139732767979264 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.03194915130734444, loss=0.03401270508766174
I0305 20:59:35.811355 139732759586560 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.032016392797231674, loss=0.031920332461595535
I0305 20:59:57.164474 139732767979264 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.032655902206897736, loss=0.03425033763051033
I0305 21:00:18.354962 139732759586560 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.03510916233062744, loss=0.031030859798192978
I0305 21:00:39.032844 139732767979264 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.03236214071512222, loss=0.030359117314219475
I0305 21:00:59.376759 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:02:09.240666 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:02:11.269419 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:02:13.157628 139874152301760 submission_runner.py:469] Time since start: 6527.91s, 	Step: 23099, 	{'train/accuracy': 0.9921954870223999, 'train/loss': 0.025153161957859993, 'train/mean_average_precision': 0.5458033140146548, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.04703688994050026, 'validation/mean_average_precision': 0.2576230886367076, 'validation/num_examples': 43793, 'test/accuracy': 0.9858444333076477, 'test/loss': 0.05042051896452904, 'test/mean_average_precision': 0.25767971063007483, 'test/num_examples': 43793, 'score': 4811.897761106491, 'total_duration': 6527.909942388535, 'accumulated_submission_time': 4811.897761106491, 'accumulated_eval_time': 1714.9992928504944, 'accumulated_logging_time': 0.4030427932739258}
I0305 21:02:13.168570 139732759586560 logging_writer.py:48] [23099] accumulated_eval_time=1715, accumulated_logging_time=0.403043, accumulated_submission_time=4811.9, global_step=23099, preemption_count=0, score=4811.9, test/accuracy=0.985844, test/loss=0.0504205, test/mean_average_precision=0.25768, test/num_examples=43793, total_duration=6527.91, train/accuracy=0.992195, train/loss=0.0251532, train/mean_average_precision=0.545803, validation/accuracy=0.986742, validation/loss=0.0470369, validation/mean_average_precision=0.257623, validation/num_examples=43793
I0305 21:02:13.601504 139732767979264 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.031109528616070747, loss=0.032412879168987274
I0305 21:02:34.615217 139732759586560 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.031614530831575394, loss=0.033617984503507614
I0305 21:02:55.483952 139732767979264 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.03803262114524841, loss=0.03225530683994293
I0305 21:03:16.907209 139732759586560 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.036713920533657074, loss=0.030962638556957245
I0305 21:03:37.784303 139732767979264 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.052283402532339096, loss=0.034575480967760086
I0305 21:03:58.830856 139732759586560 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.03735131025314331, loss=0.031562332063913345
I0305 21:04:20.050534 139732767979264 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.032140351831912994, loss=0.030411705374717712
I0305 21:04:41.101300 139732759586560 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.045442912727594376, loss=0.03165987879037857
I0305 21:05:01.872445 139732767979264 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.03594008460640907, loss=0.02984442375600338
I0305 21:05:22.598741 139732759586560 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.037719763815402985, loss=0.032259251922369
I0305 21:05:43.664297 139732767979264 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.03836408630013466, loss=0.030396508052945137
I0305 21:06:05.110457 139732759586560 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.030746812000870705, loss=0.029294071719050407
I0305 21:06:13.214241 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:07:23.852009 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:07:25.789128 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:07:27.671473 139874152301760 submission_runner.py:469] Time since start: 6842.42s, 	Step: 24239, 	{'train/accuracy': 0.9931254982948303, 'train/loss': 0.02246207371354103, 'train/mean_average_precision': 0.5974875951937973, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.047493305057287216, 'validation/mean_average_precision': 0.25046716428373667, 'validation/num_examples': 43793, 'test/accuracy': 0.9857892990112305, 'test/loss': 0.0508321151137352, 'test/mean_average_precision': 0.24394047755672874, 'test/num_examples': 43793, 'score': 5051.900489091873, 'total_duration': 6842.423726081848, 'accumulated_submission_time': 5051.900489091873, 'accumulated_eval_time': 1789.456362247467, 'accumulated_logging_time': 0.4244542121887207}
I0305 21:07:27.682495 139732767979264 logging_writer.py:48] [24239] accumulated_eval_time=1789.46, accumulated_logging_time=0.424454, accumulated_submission_time=5051.9, global_step=24239, preemption_count=0, score=5051.9, test/accuracy=0.985789, test/loss=0.0508321, test/mean_average_precision=0.24394, test/num_examples=43793, total_duration=6842.42, train/accuracy=0.993125, train/loss=0.0224621, train/mean_average_precision=0.597488, validation/accuracy=0.986672, validation/loss=0.0474933, validation/mean_average_precision=0.250467, validation/num_examples=43793
I0305 21:07:40.433249 139732759586560 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03386931121349335, loss=0.028768539428710938
I0305 21:08:01.366306 139732767979264 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.034875404089689255, loss=0.03140599653124809
I0305 21:08:22.450399 139732759586560 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.03599384427070618, loss=0.03110111877322197
I0305 21:08:43.498105 139732767979264 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.03448479622602463, loss=0.0323241651058197
I0305 21:09:04.649262 139732759586560 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.036293670535087585, loss=0.030726777389645576
I0305 21:09:25.791908 139732767979264 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.03486298397183418, loss=0.030095135793089867
I0305 21:09:46.990585 139732759586560 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.04569076746702194, loss=0.033278562128543854
I0305 21:10:07.869111 139732767979264 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.04171120747923851, loss=0.0319380909204483
I0305 21:10:28.716731 139732759586560 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.03760495409369469, loss=0.030013905838131905
I0305 21:10:49.862514 139732767979264 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.03937852755188942, loss=0.0328773595392704
I0305 21:11:10.669215 139732759586560 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.03281306102871895, loss=0.029245911166071892
I0305 21:11:27.691028 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:12:38.820365 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:12:40.800525 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:12:42.853436 139874152301760 submission_runner.py:469] Time since start: 7157.61s, 	Step: 25385, 	{'train/accuracy': 0.9928680658340454, 'train/loss': 0.02308470569550991, 'train/mean_average_precision': 0.5870958621481228, 'validation/accuracy': 0.9867252707481384, 'validation/loss': 0.04742967709898949, 'validation/mean_average_precision': 0.2565169749292392, 'validation/num_examples': 43793, 'test/accuracy': 0.9857997894287109, 'test/loss': 0.05089139938354492, 'test/mean_average_precision': 0.2469513090827234, 'test/num_examples': 43793, 'score': 5291.868155002594, 'total_duration': 7157.60574388504, 'accumulated_submission_time': 5291.868155002594, 'accumulated_eval_time': 1864.6186635494232, 'accumulated_logging_time': 0.4451620578765869}
I0305 21:12:42.865136 139732767979264 logging_writer.py:48] [25385] accumulated_eval_time=1864.62, accumulated_logging_time=0.445162, accumulated_submission_time=5291.87, global_step=25385, preemption_count=0, score=5291.87, test/accuracy=0.9858, test/loss=0.0508914, test/mean_average_precision=0.246951, test/num_examples=43793, total_duration=7157.61, train/accuracy=0.992868, train/loss=0.0230847, train/mean_average_precision=0.587096, validation/accuracy=0.986725, validation/loss=0.0474297, validation/mean_average_precision=0.256517, validation/num_examples=43793
I0305 21:12:46.106999 139732759586560 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.03358934447169304, loss=0.03118104673922062
I0305 21:13:06.569016 139732767979264 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.032961126416921616, loss=0.029597211629152298
I0305 21:13:27.279976 139732759586560 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.03856590762734413, loss=0.031589437276124954
I0305 21:13:47.950973 139732767979264 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.03925537317991257, loss=0.03010503388941288
I0305 21:14:08.756769 139732759586560 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.039017777889966965, loss=0.03146810084581375
I0305 21:14:29.493504 139732767979264 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.03540244698524475, loss=0.029915882274508476
I0305 21:14:50.278240 139732759586560 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.036095745861530304, loss=0.030667744576931
I0305 21:15:10.986049 139732767979264 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.039373625069856644, loss=0.030715269967913628
I0305 21:15:31.874276 139732759586560 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.03494696691632271, loss=0.02903822436928749
I0305 21:15:52.627174 139732767979264 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.04410042613744736, loss=0.031639888882637024
I0305 21:16:13.416202 139732759586560 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.039802320301532745, loss=0.031906820833683014
I0305 21:16:34.325831 139732767979264 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.04555968940258026, loss=0.032099660485982895
I0305 21:16:42.965645 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:17:51.087582 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:17:53.098315 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:17:54.981897 139874152301760 submission_runner.py:469] Time since start: 7469.73s, 	Step: 26542, 	{'train/accuracy': 0.9937341213226318, 'train/loss': 0.020803652703762054, 'train/mean_average_precision': 0.6316388064937534, 'validation/accuracy': 0.9866083860397339, 'validation/loss': 0.04819588363170624, 'validation/mean_average_precision': 0.2502100090846716, 'validation/num_examples': 43793, 'test/accuracy': 0.9856246113777161, 'test/loss': 0.051725100725889206, 'test/mean_average_precision': 0.24612282365579394, 'test/num_examples': 43793, 'score': 5531.930026292801, 'total_duration': 7469.73424744606, 'accumulated_submission_time': 5531.930026292801, 'accumulated_eval_time': 1936.6348514556885, 'accumulated_logging_time': 0.46584033966064453}
I0305 21:17:54.993716 139732759586560 logging_writer.py:48] [26542] accumulated_eval_time=1936.63, accumulated_logging_time=0.46584, accumulated_submission_time=5531.93, global_step=26542, preemption_count=0, score=5531.93, test/accuracy=0.985625, test/loss=0.0517251, test/mean_average_precision=0.246123, test/num_examples=43793, total_duration=7469.73, train/accuracy=0.993734, train/loss=0.0208037, train/mean_average_precision=0.631639, validation/accuracy=0.986608, validation/loss=0.0481959, validation/mean_average_precision=0.25021, validation/num_examples=43793
I0305 21:18:07.383084 139732767979264 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.04026417434215546, loss=0.030937442556023598
I0305 21:18:28.281257 139732759586560 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.04137277975678444, loss=0.028834713622927666
I0305 21:18:49.357857 139732767979264 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.039423976093530655, loss=0.03001207485795021
I0305 21:19:10.484007 139732759586560 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.03940873593091965, loss=0.030562374740839005
I0305 21:19:31.305564 139732767979264 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.04493539780378342, loss=0.030164409428834915
I0305 21:19:52.234994 139732759586560 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.034789927303791046, loss=0.02872706577181816
I0305 21:20:13.098456 139732767979264 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.045883018523454666, loss=0.031214790418744087
I0305 21:20:34.330007 139732759586560 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.04011561721563339, loss=0.031948063522577286
I0305 21:20:55.206642 139732767979264 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.03764478489756584, loss=0.030056165531277657
I0305 21:21:16.165982 139732759586560 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.044240113347768784, loss=0.02834600768983364
I0305 21:21:37.189555 139732767979264 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.03962448611855507, loss=0.03173164278268814
I0305 21:21:55.077646 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:23:07.266377 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:23:09.251557 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:23:11.149384 139874152301760 submission_runner.py:469] Time since start: 7785.90s, 	Step: 27686, 	{'train/accuracy': 0.993607223033905, 'train/loss': 0.02094670757651329, 'train/mean_average_precision': 0.6301575330527193, 'validation/accuracy': 0.9864906668663025, 'validation/loss': 0.048356007784605026, 'validation/mean_average_precision': 0.25252388986337104, 'validation/num_examples': 43793, 'test/accuracy': 0.9856148958206177, 'test/loss': 0.05162663385272026, 'test/mean_average_precision': 0.24578366870256033, 'test/num_examples': 43793, 'score': 5771.970009326935, 'total_duration': 7785.901625394821, 'accumulated_submission_time': 5771.970009326935, 'accumulated_eval_time': 2012.7064208984375, 'accumulated_logging_time': 0.4870107173919678}
I0305 21:23:11.161017 139732759586560 logging_writer.py:48] [27686] accumulated_eval_time=2012.71, accumulated_logging_time=0.487011, accumulated_submission_time=5771.97, global_step=27686, preemption_count=0, score=5771.97, test/accuracy=0.985615, test/loss=0.0516266, test/mean_average_precision=0.245784, test/num_examples=43793, total_duration=7785.9, train/accuracy=0.993607, train/loss=0.0209467, train/mean_average_precision=0.630158, validation/accuracy=0.986491, validation/loss=0.048356, validation/mean_average_precision=0.252524, validation/num_examples=43793
I0305 21:23:14.517580 139732767979264 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.04679745435714722, loss=0.029747268185019493
I0305 21:23:35.592568 139732759586560 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.03673652932047844, loss=0.03009837493300438
I0305 21:23:56.586309 139732767979264 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04629260674118996, loss=0.0321056954562664
I0305 21:24:17.355838 139732759586560 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.04123780131340027, loss=0.030612751841545105
I0305 21:24:38.200537 139732767979264 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.045318931341171265, loss=0.031054053455591202
I0305 21:24:58.973759 139732759586560 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.04416032135486603, loss=0.02864794060587883
I0305 21:25:20.015592 139732767979264 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.04980846494436264, loss=0.03071538358926773
I0305 21:25:41.759908 139732759586560 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.04691285640001297, loss=0.03222562372684479
I0305 21:26:03.811479 139732767979264 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.045476220548152924, loss=0.02974390611052513
I0305 21:26:24.960172 139732759586560 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.039026472717523575, loss=0.029727259650826454
I0305 21:26:45.954517 139732767979264 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.047356683760881424, loss=0.03195595368742943
I0305 21:27:06.979862 139732759586560 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.04877087101340294, loss=0.030220188200473785
I0305 21:27:11.344015 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:28:18.794098 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:28:20.774152 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:28:22.700013 139874152301760 submission_runner.py:469] Time since start: 8097.45s, 	Step: 28822, 	{'train/accuracy': 0.9936826825141907, 'train/loss': 0.020257622003555298, 'train/mean_average_precision': 0.658814244789802, 'validation/accuracy': 0.9865779280662537, 'validation/loss': 0.04961155727505684, 'validation/mean_average_precision': 0.2514140937941614, 'validation/num_examples': 43793, 'test/accuracy': 0.9856418371200562, 'test/loss': 0.05323880538344383, 'test/mean_average_precision': 0.2417591569772401, 'test/num_examples': 43793, 'score': 6011.909794092178, 'total_duration': 8097.452238082886, 'accumulated_submission_time': 6011.909794092178, 'accumulated_eval_time': 2084.0622293949127, 'accumulated_logging_time': 0.7099466323852539}
I0305 21:28:22.712229 139732767979264 logging_writer.py:48] [28822] accumulated_eval_time=2084.06, accumulated_logging_time=0.709947, accumulated_submission_time=6011.91, global_step=28822, preemption_count=0, score=6011.91, test/accuracy=0.985642, test/loss=0.0532388, test/mean_average_precision=0.241759, test/num_examples=43793, total_duration=8097.45, train/accuracy=0.993683, train/loss=0.0202576, train/mean_average_precision=0.658814, validation/accuracy=0.986578, validation/loss=0.0496116, validation/mean_average_precision=0.251414, validation/num_examples=43793
I0305 21:28:39.337425 139732759586560 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.03917029872536659, loss=0.0272352397441864
I0305 21:29:00.283129 139732767979264 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.04295286908745766, loss=0.029316581785678864
I0305 21:29:21.318435 139732759586560 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.04366972669959068, loss=0.028406424447894096
I0305 21:29:42.830090 139732767979264 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04633326455950737, loss=0.029481390491127968
I0305 21:30:04.206599 139732759586560 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.046963319182395935, loss=0.029198987409472466
I0305 21:30:25.645011 139732767979264 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.05070634186267853, loss=0.029535893350839615
I0305 21:30:46.815604 139732759586560 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.0464327447116375, loss=0.03021886758506298
I0305 21:31:08.034145 139732767979264 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.04656864330172539, loss=0.029962003231048584
I0305 21:31:29.086287 139732759586560 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.04789658635854721, loss=0.027349449694156647
I0305 21:31:49.479202 139732767979264 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.05127790570259094, loss=0.029454056173563004
I0305 21:32:10.162334 139732759586560 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.045318108052015305, loss=0.029385767877101898
I0305 21:32:22.773182 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:33:33.458327 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:33:35.375127 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:33:37.311218 139874152301760 submission_runner.py:469] Time since start: 8412.06s, 	Step: 29962, 	{'train/accuracy': 0.993719756603241, 'train/loss': 0.020009413361549377, 'train/mean_average_precision': 0.6417948322585378, 'validation/accuracy': 0.9865661859512329, 'validation/loss': 0.050407394766807556, 'validation/mean_average_precision': 0.24902321630328184, 'validation/num_examples': 43793, 'test/accuracy': 0.9856254458427429, 'test/loss': 0.05380432680249214, 'test/mean_average_precision': 0.23985349993736801, 'test/num_examples': 43793, 'score': 6251.930826425552, 'total_duration': 8412.063458919525, 'accumulated_submission_time': 6251.930826425552, 'accumulated_eval_time': 2158.6000969409943, 'accumulated_logging_time': 0.7314238548278809}
I0305 21:33:37.323008 139732767979264 logging_writer.py:48] [29962] accumulated_eval_time=2158.6, accumulated_logging_time=0.731424, accumulated_submission_time=6251.93, global_step=29962, preemption_count=0, score=6251.93, test/accuracy=0.985625, test/loss=0.0538043, test/mean_average_precision=0.239853, test/num_examples=43793, total_duration=8412.06, train/accuracy=0.99372, train/loss=0.0200094, train/mean_average_precision=0.641795, validation/accuracy=0.986566, validation/loss=0.0504074, validation/mean_average_precision=0.249023, validation/num_examples=43793
I0305 21:33:45.246498 139732759586560 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.04832528531551361, loss=0.028631970286369324
I0305 21:34:06.172055 139732767979264 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.04366477578878403, loss=0.02762196958065033
I0305 21:34:27.082144 139732759586560 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.04668682813644409, loss=0.030165771022439003
I0305 21:34:47.882224 139732767979264 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.05155450850725174, loss=0.027819614857435226
I0305 21:35:09.068953 139732759586560 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.051236655563116074, loss=0.02875109761953354
I0305 21:35:30.109083 139732767979264 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.05000864714384079, loss=0.02777983620762825
I0305 21:35:50.800151 139732759586560 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.042590055614709854, loss=0.026313265785574913
I0305 21:36:11.368167 139732767979264 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.04889326170086861, loss=0.028223160654306412
I0305 21:36:32.147565 139732759586560 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05464662238955498, loss=0.02699722722172737
I0305 21:36:52.892762 139732767979264 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05009341612458229, loss=0.029544411227107048
I0305 21:37:13.926946 139732759586560 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.04995051398873329, loss=0.028132272884249687
I0305 21:37:34.894699 139732767979264 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.053767379373311996, loss=0.031055329367518425
I0305 21:37:37.431311 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:38:48.155890 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:38:50.144406 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:38:52.031064 139874152301760 submission_runner.py:469] Time since start: 8726.78s, 	Step: 31113, 	{'train/accuracy': 0.993526816368103, 'train/loss': 0.020579291507601738, 'train/mean_average_precision': 0.6352587161937853, 'validation/accuracy': 0.9863713383674622, 'validation/loss': 0.050849247723817825, 'validation/mean_average_precision': 0.24517835116222927, 'validation/num_examples': 43793, 'test/accuracy': 0.9855453968048096, 'test/loss': 0.0541442409157753, 'test/mean_average_precision': 0.23467304273784842, 'test/num_examples': 43793, 'score': 6491.998294830322, 'total_duration': 8726.783337831497, 'accumulated_submission_time': 6491.998294830322, 'accumulated_eval_time': 2233.19970870018, 'accumulated_logging_time': 0.7522673606872559}
I0305 21:38:52.043226 139732759586560 logging_writer.py:48] [31113] accumulated_eval_time=2233.2, accumulated_logging_time=0.752267, accumulated_submission_time=6492, global_step=31113, preemption_count=0, score=6492, test/accuracy=0.985545, test/loss=0.0541442, test/mean_average_precision=0.234673, test/num_examples=43793, total_duration=8726.78, train/accuracy=0.993527, train/loss=0.0205793, train/mean_average_precision=0.635259, validation/accuracy=0.986371, validation/loss=0.0508492, validation/mean_average_precision=0.245178, validation/num_examples=43793
I0305 21:39:10.636103 139732767979264 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.05378307029604912, loss=0.029517531394958496
I0305 21:39:31.776598 139732759586560 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.0505877248942852, loss=0.027278589084744453
I0305 21:39:52.860699 139732767979264 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.04913095012307167, loss=0.02876199595630169
I0305 21:40:13.648824 139732759586560 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.04681456461548805, loss=0.027368269860744476
I0305 21:40:34.589624 139732767979264 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.04716464877128601, loss=0.028529711067676544
I0305 21:40:55.892675 139732759586560 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.05685507506132126, loss=0.03010144643485546
I0305 21:41:17.073886 139732767979264 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.048708170652389526, loss=0.027761653065681458
I0305 21:41:37.699622 139732759586560 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.04779834300279617, loss=0.027978943660855293
I0305 21:41:58.797034 139732767979264 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05186225846409798, loss=0.02537977695465088
I0305 21:42:19.838923 139732759586560 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.06043018773198128, loss=0.029377400875091553
I0305 21:42:40.465711 139732767979264 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.049658555537462234, loss=0.02599995769560337
I0305 21:42:52.173645 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:44:01.239027 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:44:03.342592 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:44:05.409802 139874152301760 submission_runner.py:469] Time since start: 9040.16s, 	Step: 32258, 	{'train/accuracy': 0.9946621060371399, 'train/loss': 0.017532046884298325, 'train/mean_average_precision': 0.6939108241643204, 'validation/accuracy': 0.9864147305488586, 'validation/loss': 0.0510583259165287, 'validation/mean_average_precision': 0.24764756606908744, 'validation/num_examples': 43793, 'test/accuracy': 0.985542893409729, 'test/loss': 0.054522089660167694, 'test/mean_average_precision': 0.23828597622566788, 'test/num_examples': 43793, 'score': 6732.089219331741, 'total_duration': 9040.162019729614, 'accumulated_submission_time': 6732.089219331741, 'accumulated_eval_time': 2306.4356710910797, 'accumulated_logging_time': 0.773418664932251}
I0305 21:44:05.422847 139732759586560 logging_writer.py:48] [32258] accumulated_eval_time=2306.44, accumulated_logging_time=0.773419, accumulated_submission_time=6732.09, global_step=32258, preemption_count=0, score=6732.09, test/accuracy=0.985543, test/loss=0.0545221, test/mean_average_precision=0.238286, test/num_examples=43793, total_duration=9040.16, train/accuracy=0.994662, train/loss=0.017532, train/mean_average_precision=0.693911, validation/accuracy=0.986415, validation/loss=0.0510583, validation/mean_average_precision=0.247648, validation/num_examples=43793
I0305 21:44:14.245979 139732767979264 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.050069354474544525, loss=0.02836478129029274
I0305 21:44:34.780630 139732759586560 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.06268086284399033, loss=0.026732908561825752
I0305 21:44:55.794633 139732767979264 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.052114006131887436, loss=0.025770721957087517
I0305 21:45:16.827244 139732759586560 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.046353280544281006, loss=0.026864901185035706
I0305 21:45:37.737741 139732767979264 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.05308718979358673, loss=0.027597913518548012
I0305 21:45:58.638533 139732759586560 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.049229610711336136, loss=0.026833144947886467
I0305 21:46:19.756955 139732767979264 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.051052577793598175, loss=0.027680734172463417
I0305 21:46:40.697654 139732759586560 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.051196612417697906, loss=0.02748190239071846
I0305 21:47:01.409205 139732767979264 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.058905117213726044, loss=0.02783266454935074
I0305 21:47:21.840433 139732759586560 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.046668168157339096, loss=0.02481817826628685
I0305 21:47:42.382450 139732767979264 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.051164913922548294, loss=0.02664278820157051
I0305 21:48:03.194354 139732759586560 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.044358544051647186, loss=0.026303553953766823
I0305 21:48:05.486564 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:49:16.116115 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:49:18.126419 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:49:20.101469 139874152301760 submission_runner.py:469] Time since start: 9354.85s, 	Step: 33412, 	{'train/accuracy': 0.9946038722991943, 'train/loss': 0.017565829679369926, 'train/mean_average_precision': 0.691784880147996, 'validation/accuracy': 0.9862726926803589, 'validation/loss': 0.05233044922351837, 'validation/mean_average_precision': 0.24262168195289693, 'validation/num_examples': 43793, 'test/accuracy': 0.9854300022125244, 'test/loss': 0.05581118166446686, 'test/mean_average_precision': 0.23631146350958138, 'test/num_examples': 43793, 'score': 6972.114375114441, 'total_duration': 9354.853789806366, 'accumulated_submission_time': 6972.114375114441, 'accumulated_eval_time': 2381.0504817962646, 'accumulated_logging_time': 0.7963719367980957}
I0305 21:49:20.113817 139732767979264 logging_writer.py:48] [33412] accumulated_eval_time=2381.05, accumulated_logging_time=0.796372, accumulated_submission_time=6972.11, global_step=33412, preemption_count=0, score=6972.11, test/accuracy=0.98543, test/loss=0.0558112, test/mean_average_precision=0.236311, test/num_examples=43793, total_duration=9354.85, train/accuracy=0.994604, train/loss=0.0175658, train/mean_average_precision=0.691785, validation/accuracy=0.986273, validation/loss=0.0523304, validation/mean_average_precision=0.242622, validation/num_examples=43793
I0305 21:49:39.039711 139732759586560 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05612793564796448, loss=0.028018593788146973
I0305 21:50:00.153172 139732767979264 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.048564810305833817, loss=0.029195211827754974
I0305 21:50:21.127231 139732759586560 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.053068291395902634, loss=0.026983775198459625
I0305 21:50:42.073957 139732767979264 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05102862790226936, loss=0.027745746076107025
I0305 21:51:03.243761 139732759586560 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.04967107996344566, loss=0.027270056307315826
I0305 21:51:24.023658 139732767979264 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.061081547290086746, loss=0.02709406428039074
I0305 21:51:44.913803 139732759586560 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.053844522684812546, loss=0.026628587394952774
I0305 21:52:05.400209 139732767979264 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.053539570420980453, loss=0.026375433430075645
I0305 21:52:25.885169 139732759586560 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.05986351519823074, loss=0.0264985803514719
I0305 21:52:46.364616 139732767979264 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.050980452448129654, loss=0.025446930900216103
I0305 21:53:07.284039 139732759586560 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.05293106660246849, loss=0.026552552357316017
I0305 21:53:20.195491 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:54:31.738767 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:54:33.655066 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:54:35.534727 139874152301760 submission_runner.py:469] Time since start: 9670.29s, 	Step: 34562, 	{'train/accuracy': 0.9952947497367859, 'train/loss': 0.015384063124656677, 'train/mean_average_precision': 0.7525746115424387, 'validation/accuracy': 0.9862949848175049, 'validation/loss': 0.053698163479566574, 'validation/mean_average_precision': 0.23808560064300088, 'validation/num_examples': 43793, 'test/accuracy': 0.9854207038879395, 'test/loss': 0.057549938559532166, 'test/mean_average_precision': 0.22971863826004962, 'test/num_examples': 43793, 'score': 7212.15430855751, 'total_duration': 9670.287094831467, 'accumulated_submission_time': 7212.15430855751, 'accumulated_eval_time': 2456.3896741867065, 'accumulated_logging_time': 0.8182563781738281}
I0305 21:54:35.547188 139732767979264 logging_writer.py:48] [34562] accumulated_eval_time=2456.39, accumulated_logging_time=0.818256, accumulated_submission_time=7212.15, global_step=34562, preemption_count=0, score=7212.15, test/accuracy=0.985421, test/loss=0.0575499, test/mean_average_precision=0.229719, test/num_examples=43793, total_duration=9670.29, train/accuracy=0.995295, train/loss=0.0153841, train/mean_average_precision=0.752575, validation/accuracy=0.986295, validation/loss=0.0536982, validation/mean_average_precision=0.238086, validation/num_examples=43793
I0305 21:54:43.637762 139732759586560 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.05139314755797386, loss=0.025172650814056396
I0305 21:55:04.234624 139732767979264 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.048261672258377075, loss=0.026323562487959862
I0305 21:55:25.213206 139732759586560 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06252223253250122, loss=0.02920382283627987
I0305 21:55:46.318304 139732767979264 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.056794069707393646, loss=0.026369521394371986
I0305 21:56:07.481077 139732759586560 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.05179666727781296, loss=0.02645597793161869
I0305 21:56:28.389916 139732767979264 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.05044180154800415, loss=0.025561587885022163
I0305 21:56:49.240202 139732759586560 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05580554157495499, loss=0.026036644354462624
I0305 21:57:10.295314 139732767979264 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06845180690288544, loss=0.026117905974388123
I0305 21:57:31.264158 139732759586560 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.05347301810979843, loss=0.024168923497200012
I0305 21:57:51.948002 139732767979264 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.057698264718055725, loss=0.026686709374189377
I0305 21:58:12.689525 139732759586560 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05028083920478821, loss=0.02517693117260933
I0305 21:58:33.690054 139732767979264 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.06439219415187836, loss=0.027167465537786484
I0305 21:58:35.611427 139874152301760 spec.py:321] Evaluating on the training split.
I0305 21:59:44.546038 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 21:59:46.685367 139874152301760 spec.py:349] Evaluating on the test split.
I0305 21:59:48.598207 139874152301760 submission_runner.py:469] Time since start: 9983.35s, 	Step: 35710, 	{'train/accuracy': 0.994560182094574, 'train/loss': 0.017042359337210655, 'train/mean_average_precision': 0.7098460528371133, 'validation/accuracy': 0.9863213896751404, 'validation/loss': 0.054151590913534164, 'validation/mean_average_precision': 0.24075044495033687, 'validation/num_examples': 43793, 'test/accuracy': 0.9853811264038086, 'test/loss': 0.05825928598642349, 'test/mean_average_precision': 0.23167227900498352, 'test/num_examples': 43793, 'score': 7452.1787214279175, 'total_duration': 9983.350497484207, 'accumulated_submission_time': 7452.1787214279175, 'accumulated_eval_time': 2529.3763267993927, 'accumulated_logging_time': 0.8398258686065674}
I0305 21:59:48.610701 139732759586560 logging_writer.py:48] [35710] accumulated_eval_time=2529.38, accumulated_logging_time=0.839826, accumulated_submission_time=7452.18, global_step=35710, preemption_count=0, score=7452.18, test/accuracy=0.985381, test/loss=0.0582593, test/mean_average_precision=0.231672, test/num_examples=43793, total_duration=9983.35, train/accuracy=0.99456, train/loss=0.0170424, train/mean_average_precision=0.709846, validation/accuracy=0.986321, validation/loss=0.0541516, validation/mean_average_precision=0.24075, validation/num_examples=43793
I0305 22:00:07.715906 139732767979264 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.05642514303326607, loss=0.02601674571633339
I0305 22:00:28.581969 139732759586560 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05469268187880516, loss=0.025209909304976463
I0305 22:00:49.624709 139732767979264 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.05874699726700783, loss=0.026464488357305527
I0305 22:01:10.549415 139732759586560 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.05531911924481392, loss=0.025040609762072563
I0305 22:01:31.501256 139732767979264 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.0508965440094471, loss=0.02475079335272312
I0305 22:01:52.354440 139732759586560 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.050610724836587906, loss=0.024380918592214584
I0305 22:02:13.464804 139732767979264 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.05761699751019478, loss=0.026861270889639854
I0305 22:02:34.469796 139732759586560 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.04528024420142174, loss=0.023798592388629913
I0305 22:02:55.453467 139732767979264 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.05453803017735481, loss=0.02397090382874012
I0305 22:03:15.984065 139732759586560 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.05699007585644722, loss=0.026415133848786354
I0305 22:03:36.418972 139732767979264 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.06445256620645523, loss=0.025283370167016983
I0305 22:03:48.639147 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:05:02.760945 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:05:04.697969 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:05:06.565789 139874152301760 submission_runner.py:469] Time since start: 10301.32s, 	Step: 36860, 	{'train/accuracy': 0.9963377118110657, 'train/loss': 0.013156083412468433, 'train/mean_average_precision': 0.7923306539296511, 'validation/accuracy': 0.9862077236175537, 'validation/loss': 0.054792370647192, 'validation/mean_average_precision': 0.24084438166872574, 'validation/num_examples': 43793, 'test/accuracy': 0.9852867722511292, 'test/loss': 0.05871647968888283, 'test/mean_average_precision': 0.2333666808963135, 'test/num_examples': 43793, 'score': 7692.168416023254, 'total_duration': 10301.318064689636, 'accumulated_submission_time': 7692.168416023254, 'accumulated_eval_time': 2607.302830219269, 'accumulated_logging_time': 0.8612604141235352}
I0305 22:05:06.578525 139732759586560 logging_writer.py:48] [36860] accumulated_eval_time=2607.3, accumulated_logging_time=0.86126, accumulated_submission_time=7692.17, global_step=36860, preemption_count=0, score=7692.17, test/accuracy=0.985287, test/loss=0.0587165, test/mean_average_precision=0.233367, test/num_examples=43793, total_duration=10301.3, train/accuracy=0.996338, train/loss=0.0131561, train/mean_average_precision=0.792331, validation/accuracy=0.986208, validation/loss=0.0547924, validation/mean_average_precision=0.240844, validation/num_examples=43793
I0305 22:05:15.240724 139732767979264 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.05646880343556404, loss=0.02449227124452591
I0305 22:05:36.168926 139732759586560 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.0515628308057785, loss=0.024661919102072716
I0305 22:05:56.928590 139732767979264 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06004372984170914, loss=0.026892663910984993
I0305 22:06:17.835414 139732759586560 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.051760233938694, loss=0.02488979883491993
I0305 22:06:38.980643 139732767979264 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.059201810508966446, loss=0.02512192539870739
I0305 22:06:59.954802 139732759586560 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.05626964941620827, loss=0.025351697579026222
I0305 22:07:21.033414 139732767979264 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.05318238213658333, loss=0.024188729003071785
I0305 22:07:42.244330 139732759586560 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.0511154942214489, loss=0.02490418218076229
I0305 22:08:03.503405 139732767979264 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.055090561509132385, loss=0.02483876794576645
I0305 22:08:24.673253 139732759586560 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.05409301817417145, loss=0.025776702910661697
I0305 22:08:45.734324 139732767979264 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.05997716635465622, loss=0.02505050227046013
I0305 22:09:06.655389 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:10:16.897608 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:10:18.905253 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:10:20.789456 139874152301760 submission_runner.py:469] Time since start: 10615.54s, 	Step: 38000, 	{'train/accuracy': 0.994249165058136, 'train/loss': 0.017675340175628662, 'train/mean_average_precision': 0.6903032068773789, 'validation/accuracy': 0.9861496686935425, 'validation/loss': 0.05637277290225029, 'validation/mean_average_precision': 0.23400285314993474, 'validation/num_examples': 43793, 'test/accuracy': 0.985261082649231, 'test/loss': 0.06050729379057884, 'test/mean_average_precision': 0.22420483867792856, 'test/num_examples': 43793, 'score': 7932.2036011219025, 'total_duration': 10615.541823625565, 'accumulated_submission_time': 7932.2036011219025, 'accumulated_eval_time': 2681.436852455139, 'accumulated_logging_time': 0.8832120895385742}
I0305 22:10:20.802654 139732759586560 logging_writer.py:48] [38000] accumulated_eval_time=2681.44, accumulated_logging_time=0.883212, accumulated_submission_time=7932.2, global_step=38000, preemption_count=0, score=7932.2, test/accuracy=0.985261, test/loss=0.0605073, test/mean_average_precision=0.224205, test/num_examples=43793, total_duration=10615.5, train/accuracy=0.994249, train/loss=0.0176753, train/mean_average_precision=0.690303, validation/accuracy=0.98615, validation/loss=0.0563728, validation/mean_average_precision=0.234003, validation/num_examples=43793
I0305 22:10:21.021199 139732767979264 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.05913688987493515, loss=0.023823218420147896
I0305 22:10:42.160252 139732759586560 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.05257831886410713, loss=0.02464108355343342
I0305 22:11:03.292616 139732767979264 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.04387751966714859, loss=0.02223333716392517
I0305 22:11:24.291035 139732759586560 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.062330618500709534, loss=0.025792840868234634
I0305 22:11:45.338919 139732767979264 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.059933144599199295, loss=0.024161145091056824
I0305 22:12:06.489009 139732759586560 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.05716476961970329, loss=0.02425273135304451
I0305 22:12:27.237367 139732767979264 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.05930950492620468, loss=0.026110755279660225
I0305 22:12:48.339764 139732759586560 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.0614306665956974, loss=0.024394743144512177
I0305 22:13:09.291944 139732767979264 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.054082322865724564, loss=0.023624863475561142
I0305 22:13:30.533155 139732759586560 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.05170782282948494, loss=0.02446035109460354
I0305 22:13:51.567804 139732767979264 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.05894920229911804, loss=0.024650586768984795
I0305 22:14:12.695074 139732759586560 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.05592428892850876, loss=0.023918770253658295
I0305 22:14:20.810643 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:15:33.502772 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:15:35.454227 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:15:37.402580 139874152301760 submission_runner.py:469] Time since start: 10932.15s, 	Step: 39139, 	{'train/accuracy': 0.9967915415763855, 'train/loss': 0.01196231972426176, 'train/mean_average_precision': 0.8155705073548373, 'validation/accuracy': 0.98621666431427, 'validation/loss': 0.056888166815042496, 'validation/mean_average_precision': 0.23239354318376004, 'validation/num_examples': 43793, 'test/accuracy': 0.9853238463401794, 'test/loss': 0.060974471271038055, 'test/mean_average_precision': 0.22815414317868796, 'test/num_examples': 43793, 'score': 8172.170121431351, 'total_duration': 10932.154947280884, 'accumulated_submission_time': 8172.170121431351, 'accumulated_eval_time': 2758.028742313385, 'accumulated_logging_time': 0.9058797359466553}
I0305 22:15:37.416169 139732767979264 logging_writer.py:48] [39139] accumulated_eval_time=2758.03, accumulated_logging_time=0.90588, accumulated_submission_time=8172.17, global_step=39139, preemption_count=0, score=8172.17, test/accuracy=0.985324, test/loss=0.0609745, test/mean_average_precision=0.228154, test/num_examples=43793, total_duration=10932.2, train/accuracy=0.996792, train/loss=0.0119623, train/mean_average_precision=0.815571, validation/accuracy=0.986217, validation/loss=0.0568882, validation/mean_average_precision=0.232394, validation/num_examples=43793
I0305 22:15:50.483895 139732759586560 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.056528642773628235, loss=0.024878382682800293
I0305 22:16:11.540360 139732767979264 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.05296138674020767, loss=0.02243175357580185
I0305 22:16:32.738933 139732759586560 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.04972311481833458, loss=0.02339959889650345
I0305 22:16:53.695319 139732767979264 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.053817566484212875, loss=0.024876929819583893
I0305 22:17:15.200446 139732759586560 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.058794744312763214, loss=0.02443697676062584
I0305 22:17:36.318302 139732767979264 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.05560539290308952, loss=0.024139342829585075
I0305 22:17:57.582526 139732759586560 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.05232216417789459, loss=0.024952180683612823
I0305 22:18:18.746223 139732767979264 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.05724543705582619, loss=0.02570585533976555
I0305 22:18:40.317262 139732759586560 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.06038423255085945, loss=0.024522358551621437
I0305 22:19:01.788983 139732767979264 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.05418221279978752, loss=0.022321410477161407
I0305 22:19:23.480435 139732759586560 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.04887412488460541, loss=0.02252109721302986
I0305 22:19:37.480539 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:20:45.216697 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:20:47.339807 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:20:49.409090 139874152301760 submission_runner.py:469] Time since start: 11244.16s, 	Step: 40268, 	{'train/accuracy': 0.9949260950088501, 'train/loss': 0.015661848708987236, 'train/mean_average_precision': 0.753983069275038, 'validation/accuracy': 0.9861054420471191, 'validation/loss': 0.05771198868751526, 'validation/mean_average_precision': 0.23610322844222537, 'validation/num_examples': 43793, 'test/accuracy': 0.985156238079071, 'test/loss': 0.062026187777519226, 'test/mean_average_precision': 0.22584390609876207, 'test/num_examples': 43793, 'score': 8412.192198753357, 'total_duration': 11244.161386728287, 'accumulated_submission_time': 8412.192198753357, 'accumulated_eval_time': 2829.957175016403, 'accumulated_logging_time': 0.9296557903289795}
I0305 22:20:49.424689 139732767979264 logging_writer.py:48] [40268] accumulated_eval_time=2829.96, accumulated_logging_time=0.929656, accumulated_submission_time=8412.19, global_step=40268, preemption_count=0, score=8412.19, test/accuracy=0.985156, test/loss=0.0620262, test/mean_average_precision=0.225844, test/num_examples=43793, total_duration=11244.2, train/accuracy=0.994926, train/loss=0.0156618, train/mean_average_precision=0.753983, validation/accuracy=0.986105, validation/loss=0.057712, validation/mean_average_precision=0.236103, validation/num_examples=43793
I0305 22:20:56.251378 139732759586560 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.049217939376831055, loss=0.022690424695611
I0305 22:21:17.235597 139732767979264 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.05116761475801468, loss=0.023748623207211494
I0305 22:21:38.399495 139732759586560 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0527685172855854, loss=0.023312564939260483
I0305 22:21:59.324213 139732767979264 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.056069813668727875, loss=0.023668935522437096
I0305 22:22:20.094545 139732759586560 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.05668347701430321, loss=0.026595592498779297
I0305 22:22:41.232647 139732767979264 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.047043174505233765, loss=0.023588057607412338
I0305 22:23:02.264284 139732759586560 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.049954600632190704, loss=0.023747731000185013
I0305 22:23:23.231037 139732767979264 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.054460156708955765, loss=0.02306315489113331
I0305 22:23:44.597776 139732759586560 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.05971590429544449, loss=0.02384863793849945
I0305 22:24:05.810844 139732767979264 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.048277661204338074, loss=0.023499947041273117
I0305 22:24:27.132250 139732759586560 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.050651915371418, loss=0.022572116926312447
I0305 22:24:48.393324 139732767979264 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.05146852880716324, loss=0.022996336221694946
I0305 22:24:49.468055 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:26:01.989542 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:26:03.967299 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:26:05.877997 139874152301760 submission_runner.py:469] Time since start: 11560.63s, 	Step: 41406, 	{'train/accuracy': 0.9973022937774658, 'train/loss': 0.010876358486711979, 'train/mean_average_precision': 0.8285463677012882, 'validation/accuracy': 0.9860429167747498, 'validation/loss': 0.05826247110962868, 'validation/mean_average_precision': 0.22849953487184563, 'validation/num_examples': 43793, 'test/accuracy': 0.9851676225662231, 'test/loss': 0.06246663257479668, 'test/mean_average_precision': 0.22317851136181902, 'test/num_examples': 43793, 'score': 8652.19577908516, 'total_duration': 11560.630364894867, 'accumulated_submission_time': 8652.19577908516, 'accumulated_eval_time': 2906.367070198059, 'accumulated_logging_time': 0.9549150466918945}
I0305 22:26:05.891210 139732759586560 logging_writer.py:48] [41406] accumulated_eval_time=2906.37, accumulated_logging_time=0.954915, accumulated_submission_time=8652.2, global_step=41406, preemption_count=0, score=8652.2, test/accuracy=0.985168, test/loss=0.0624666, test/mean_average_precision=0.223179, test/num_examples=43793, total_duration=11560.6, train/accuracy=0.997302, train/loss=0.0108764, train/mean_average_precision=0.828546, validation/accuracy=0.986043, validation/loss=0.0582625, validation/mean_average_precision=0.2285, validation/num_examples=43793
I0305 22:26:25.746316 139732767979264 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.055030107498168945, loss=0.022301483899354935
I0305 22:26:46.708253 139732759586560 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.05254775285720825, loss=0.02545931190252304
I0305 22:27:07.622299 139732767979264 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.05033745989203453, loss=0.0234700795263052
I0305 22:27:28.728434 139732759586560 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.04973404109477997, loss=0.022843200713396072
I0305 22:27:49.550773 139732767979264 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.05668967589735985, loss=0.02446080930531025
I0305 22:28:10.524475 139732759586560 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.04661406949162483, loss=0.021167203783988953
I0305 22:28:31.188518 139732767979264 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.04878658428788185, loss=0.022615335881710052
I0305 22:28:52.119146 139732759586560 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.058792825788259506, loss=0.02374436892569065
I0305 22:29:12.972919 139732767979264 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.045952145010232925, loss=0.022596534341573715
I0305 22:29:33.971738 139732759586560 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.047878045588731766, loss=0.022926459088921547
I0305 22:29:55.115856 139732767979264 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.05732673034071922, loss=0.025113578885793686
I0305 22:30:05.984055 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:31:15.772356 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:31:17.719730 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:31:19.807363 139874152301760 submission_runner.py:469] Time since start: 11874.56s, 	Step: 42553, 	{'train/accuracy': 0.9956129193305969, 'train/loss': 0.013847298920154572, 'train/mean_average_precision': 0.7825633837318187, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.059115588665008545, 'validation/mean_average_precision': 0.23044694181131412, 'validation/num_examples': 43793, 'test/accuracy': 0.9850820899009705, 'test/loss': 0.06356512010097504, 'test/mean_average_precision': 0.22224576390463255, 'test/num_examples': 43793, 'score': 8892.249380111694, 'total_duration': 11874.559579610825, 'accumulated_submission_time': 8892.249380111694, 'accumulated_eval_time': 2980.1901791095734, 'accumulated_logging_time': 0.9772748947143555}
I0305 22:31:19.822374 139732759586560 logging_writer.py:48] [42553] accumulated_eval_time=2980.19, accumulated_logging_time=0.977275, accumulated_submission_time=8892.25, global_step=42553, preemption_count=0, score=8892.25, test/accuracy=0.985082, test/loss=0.0635651, test/mean_average_precision=0.222246, test/num_examples=43793, total_duration=11874.6, train/accuracy=0.995613, train/loss=0.0138473, train/mean_average_precision=0.782563, validation/accuracy=0.986042, validation/loss=0.0591156, validation/mean_average_precision=0.230447, validation/num_examples=43793
I0305 22:31:29.916724 139732767979264 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.056697744876146317, loss=0.022591590881347656
I0305 22:31:50.741398 139732759586560 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.05142732337117195, loss=0.022996697574853897
I0305 22:32:11.749341 139732767979264 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.060844141989946365, loss=0.023221848532557487
I0305 22:32:32.804089 139732759586560 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.047488339245319366, loss=0.022828131914138794
I0305 22:32:53.506453 139732767979264 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.04834830388426781, loss=0.0222900602966547
I0305 22:33:14.515952 139732759586560 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.05412499979138374, loss=0.023768607527017593
I0305 22:33:35.468353 139732767979264 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.04523606225848198, loss=0.022306393831968307
I0305 22:33:56.662339 139732759586560 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.04764784127473831, loss=0.022176600992679596
I0305 22:34:17.843609 139732767979264 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.05085427686572075, loss=0.02388266660273075
I0305 22:34:38.806036 139732759586560 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.04906585067510605, loss=0.0234679002314806
I0305 22:34:59.749878 139732767979264 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.042217910289764404, loss=0.022340286523103714
I0305 22:35:19.982525 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:36:32.516911 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:36:34.452821 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:36:36.396453 139874152301760 submission_runner.py:469] Time since start: 12191.15s, 	Step: 43697, 	{'train/accuracy': 0.9974721670150757, 'train/loss': 0.010402362793684006, 'train/mean_average_precision': 0.8359061808925774, 'validation/accuracy': 0.9860473871231079, 'validation/loss': 0.06011923775076866, 'validation/mean_average_precision': 0.23138874299048715, 'validation/num_examples': 43793, 'test/accuracy': 0.985093891620636, 'test/loss': 0.0643911063671112, 'test/mean_average_precision': 0.22238335071195395, 'test/num_examples': 43793, 'score': 9132.370262622833, 'total_duration': 12191.148692131042, 'accumulated_submission_time': 9132.370262622833, 'accumulated_eval_time': 3056.6039323806763, 'accumulated_logging_time': 1.002333641052246}
I0305 22:36:36.410140 139732759586560 logging_writer.py:48] [43697] accumulated_eval_time=3056.6, accumulated_logging_time=1.00233, accumulated_submission_time=9132.37, global_step=43697, preemption_count=0, score=9132.37, test/accuracy=0.985094, test/loss=0.0643911, test/mean_average_precision=0.222383, test/num_examples=43793, total_duration=12191.1, train/accuracy=0.997472, train/loss=0.0104024, train/mean_average_precision=0.835906, validation/accuracy=0.986047, validation/loss=0.0601192, validation/mean_average_precision=0.231389, validation/num_examples=43793
I0305 22:36:37.238450 139732767979264 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.05412496253848076, loss=0.022619260475039482
I0305 22:36:58.015347 139732759586560 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.0533364862203598, loss=0.02316291816532612
I0305 22:37:18.950168 139732767979264 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.05251336842775345, loss=0.02332267537713051
I0305 22:37:40.029185 139732759586560 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.050476428121328354, loss=0.022542936727404594
I0305 22:38:00.660744 139732767979264 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.04659903421998024, loss=0.021312348544597626
I0305 22:38:21.663506 139732759586560 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.048951905220746994, loss=0.023275386542081833
I0305 22:38:42.746270 139732767979264 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.080544114112854, loss=0.023485001176595688
I0305 22:39:03.979636 139732759586560 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.04653489589691162, loss=0.02297966741025448
I0305 22:39:25.273378 139732767979264 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.04977580904960632, loss=0.024250604212284088
I0305 22:39:46.310276 139732759586560 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.05454783886671066, loss=0.0225980281829834
I0305 22:40:07.252045 139732767979264 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.048884134739637375, loss=0.022905463352799416
I0305 22:40:28.551012 139732759586560 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.048580072820186615, loss=0.022623248398303986
I0305 22:40:36.423184 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:41:47.174134 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:41:49.126167 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:41:51.034841 139874152301760 submission_runner.py:469] Time since start: 12505.79s, 	Step: 44839, 	{'train/accuracy': 0.9960214495658875, 'train/loss': 0.012837124988436699, 'train/mean_average_precision': 0.807660260154525, 'validation/accuracy': 0.9859750866889954, 'validation/loss': 0.060532186180353165, 'validation/mean_average_precision': 0.22518951313756239, 'validation/num_examples': 43793, 'test/accuracy': 0.9851128458976746, 'test/loss': 0.06476683169603348, 'test/mean_average_precision': 0.22001704352598167, 'test/num_examples': 43793, 'score': 9372.344907522202, 'total_duration': 12505.787116289139, 'accumulated_submission_time': 9372.344907522202, 'accumulated_eval_time': 3131.215449333191, 'accumulated_logging_time': 1.02547287940979}
I0305 22:41:51.049237 139732767979264 logging_writer.py:48] [44839] accumulated_eval_time=3131.22, accumulated_logging_time=1.02547, accumulated_submission_time=9372.34, global_step=44839, preemption_count=0, score=9372.34, test/accuracy=0.985113, test/loss=0.0647668, test/mean_average_precision=0.220017, test/num_examples=43793, total_duration=12505.8, train/accuracy=0.996021, train/loss=0.0128371, train/mean_average_precision=0.80766, validation/accuracy=0.985975, validation/loss=0.0605322, validation/mean_average_precision=0.22519, validation/num_examples=43793
I0305 22:42:04.151305 139732759586560 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.045379068702459335, loss=0.022611763328313828
I0305 22:42:24.986382 139732767979264 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0586850568652153, loss=0.022960562258958817
I0305 22:42:46.095475 139732759586560 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.060740042477846146, loss=0.022342629730701447
I0305 22:43:07.261027 139732767979264 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.04499800130724907, loss=0.022105546668171883
I0305 22:43:28.235450 139732759586560 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.046681154519319534, loss=0.022039147093892097
I0305 22:43:49.273543 139732767979264 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.04744996502995491, loss=0.022044820711016655
I0305 22:44:10.416513 139732759586560 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.051473468542099, loss=0.02274656854569912
I0305 22:44:31.351574 139732767979264 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.0421312190592289, loss=0.02236892841756344
I0305 22:44:52.303339 139732759586560 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.04868133366107941, loss=0.022646497935056686
I0305 22:45:13.044144 139732767979264 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.045294735580682755, loss=0.02215440571308136
I0305 22:45:34.137440 139732759586560 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.051833998411893845, loss=0.02322722040116787
I0305 22:45:51.163521 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:47:00.382063 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:47:02.405598 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:47:04.323331 139874152301760 submission_runner.py:469] Time since start: 12819.08s, 	Step: 45982, 	{'train/accuracy': 0.9971339106559753, 'train/loss': 0.010748808272182941, 'train/mean_average_precision': 0.8484449479518312, 'validation/accuracy': 0.9860027432441711, 'validation/loss': 0.06084206700325012, 'validation/mean_average_precision': 0.22730050509141708, 'validation/num_examples': 43793, 'test/accuracy': 0.9850223064422607, 'test/loss': 0.06530077755451202, 'test/mean_average_precision': 0.21729932089808773, 'test/num_examples': 43793, 'score': 9612.418736934662, 'total_duration': 12819.075697898865, 'accumulated_submission_time': 9612.418736934662, 'accumulated_eval_time': 3204.3752167224884, 'accumulated_logging_time': 1.049227237701416}
I0305 22:47:04.338349 139732767979264 logging_writer.py:48] [45982] accumulated_eval_time=3204.38, accumulated_logging_time=1.04923, accumulated_submission_time=9612.42, global_step=45982, preemption_count=0, score=9612.42, test/accuracy=0.985022, test/loss=0.0653008, test/mean_average_precision=0.217299, test/num_examples=43793, total_duration=12819.1, train/accuracy=0.997134, train/loss=0.0107488, train/mean_average_precision=0.848445, validation/accuracy=0.986003, validation/loss=0.0608421, validation/mean_average_precision=0.227301, validation/num_examples=43793
I0305 22:47:08.378691 139732759586560 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.04585610330104828, loss=0.02236280031502247
I0305 22:47:29.240372 139732767979264 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.04970230162143707, loss=0.023417191579937935
I0305 22:47:50.083669 139732759586560 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.04365032911300659, loss=0.021924331784248352
I0305 22:48:11.149613 139732767979264 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.04184561222791672, loss=0.02268236316740513
I0305 22:48:32.404483 139732759586560 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.04463569074869156, loss=0.022624852135777473
I0305 22:48:53.322504 139732767979264 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.04539492353796959, loss=0.02154000848531723
I0305 22:49:14.199091 139732759586560 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.04271446540951729, loss=0.022987328469753265
I0305 22:49:35.329019 139732767979264 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.05228138342499733, loss=0.022431254386901855
I0305 22:49:56.527070 139732759586560 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.04137527942657471, loss=0.022034890949726105
I0305 22:50:17.595244 139732767979264 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.04658735170960426, loss=0.023081645369529724
I0305 22:50:38.906826 139732759586560 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.05226217955350876, loss=0.0218687504529953
I0305 22:50:59.989124 139732767979264 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.0442395843565464, loss=0.021880725398659706
I0305 22:51:04.542439 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:52:16.492689 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:52:18.448054 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:52:20.355868 139874152301760 submission_runner.py:469] Time since start: 13135.11s, 	Step: 47123, 	{'train/accuracy': 0.9955915808677673, 'train/loss': 0.013439957983791828, 'train/mean_average_precision': 0.7982546224283423, 'validation/accuracy': 0.9859146475791931, 'validation/loss': 0.06146799400448799, 'validation/mean_average_precision': 0.22559409645419728, 'validation/num_examples': 43793, 'test/accuracy': 0.9849928021430969, 'test/loss': 0.06581880897283554, 'test/mean_average_precision': 0.21860881178712743, 'test/num_examples': 43793, 'score': 9852.579642295837, 'total_duration': 13135.108164548874, 'accumulated_submission_time': 9852.579642295837, 'accumulated_eval_time': 3280.188529729843, 'accumulated_logging_time': 1.07326340675354}
I0305 22:52:20.369946 139732759586560 logging_writer.py:48] [47123] accumulated_eval_time=3280.19, accumulated_logging_time=1.07326, accumulated_submission_time=9852.58, global_step=47123, preemption_count=0, score=9852.58, test/accuracy=0.984993, test/loss=0.0658188, test/mean_average_precision=0.218609, test/num_examples=43793, total_duration=13135.1, train/accuracy=0.995592, train/loss=0.01344, train/mean_average_precision=0.798255, validation/accuracy=0.985915, validation/loss=0.061468, validation/mean_average_precision=0.225594, validation/num_examples=43793
I0305 22:52:37.122036 139732767979264 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.04947119951248169, loss=0.0231170691549778
I0305 22:52:58.345112 139732759586560 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.048514533787965775, loss=0.02221231535077095
I0305 22:53:19.351627 139732767979264 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.04785865172743797, loss=0.02396584488451481
I0305 22:53:40.237646 139732759586560 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.04037689045071602, loss=0.02171473577618599
I0305 22:54:01.376130 139732767979264 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.04555894434452057, loss=0.023076828569173813
I0305 22:54:22.359508 139732759586560 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.04387287423014641, loss=0.021940767765045166
I0305 22:54:43.187579 139732767979264 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.04721793904900551, loss=0.021124279126524925
I0305 22:55:04.426414 139732759586560 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.04129393398761749, loss=0.021027518436312675
I0305 22:55:25.543609 139732767979264 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.04223655164241791, loss=0.021825863048434258
I0305 22:55:46.714819 139732759586560 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.0445760041475296, loss=0.023514840751886368
I0305 22:56:07.631017 139732767979264 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.041192617267370224, loss=0.021807603538036346
I0305 22:56:20.539076 139874152301760 spec.py:321] Evaluating on the training split.
I0305 22:57:31.265239 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 22:57:33.286728 139874152301760 spec.py:349] Evaluating on the test split.
I0305 22:57:35.222523 139874152301760 submission_runner.py:469] Time since start: 13449.97s, 	Step: 48263, 	{'train/accuracy': 0.9966636300086975, 'train/loss': 0.01138642430305481, 'train/mean_average_precision': 0.8308602084599331, 'validation/accuracy': 0.9859268069267273, 'validation/loss': 0.061555761843919754, 'validation/mean_average_precision': 0.22400767463290686, 'validation/num_examples': 43793, 'test/accuracy': 0.9850496649742126, 'test/loss': 0.06599121540784836, 'test/mean_average_precision': 0.2167627788501794, 'test/num_examples': 43793, 'score': 10092.707715034485, 'total_duration': 13449.974822282791, 'accumulated_submission_time': 10092.707715034485, 'accumulated_eval_time': 3354.871862411499, 'accumulated_logging_time': 1.0978589057922363}
I0305 22:57:35.238444 139732759586560 logging_writer.py:48] [48263] accumulated_eval_time=3354.87, accumulated_logging_time=1.09786, accumulated_submission_time=10092.7, global_step=48263, preemption_count=0, score=10092.7, test/accuracy=0.98505, test/loss=0.0659912, test/mean_average_precision=0.216763, test/num_examples=43793, total_duration=13450, train/accuracy=0.996664, train/loss=0.0113864, train/mean_average_precision=0.83086, validation/accuracy=0.985927, validation/loss=0.0615558, validation/mean_average_precision=0.224008, validation/num_examples=43793
I0305 22:57:43.091569 139732767979264 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.04475010931491852, loss=0.021577944979071617
I0305 22:58:04.426551 139732759586560 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.04256022348999977, loss=0.02242792211472988
I0305 22:58:25.571294 139732767979264 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.04489181190729141, loss=0.02070263773202896
I0305 22:58:46.316667 139732759586560 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.04266804829239845, loss=0.02192140370607376
I0305 22:59:07.272798 139732767979264 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.0463477298617363, loss=0.02272319234907627
I0305 22:59:28.581690 139732759586560 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.04478517919778824, loss=0.02228596992790699
I0305 22:59:50.140693 139732767979264 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.04214635118842125, loss=0.021284151822328568
I0305 23:00:11.234067 139732759586560 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.04769035801291466, loss=0.021290386095643044
I0305 23:00:32.593359 139732767979264 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.0400770865380764, loss=0.022426405921578407
I0305 23:00:53.760228 139732759586560 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.043568868190050125, loss=0.021008651703596115
I0305 23:01:14.412550 139732767979264 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.05135894566774368, loss=0.024188145995140076
I0305 23:01:34.962098 139732759586560 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.05010470002889633, loss=0.022254161536693573
I0305 23:01:35.374704 139874152301760 spec.py:321] Evaluating on the training split.
I0305 23:02:49.485544 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 23:02:51.420664 139874152301760 spec.py:349] Evaluating on the test split.
I0305 23:02:53.326750 139874152301760 submission_runner.py:469] Time since start: 13768.08s, 	Step: 49403, 	{'train/accuracy': 0.9980002641677856, 'train/loss': 0.009391127154231071, 'train/mean_average_precision': 0.8595822128628009, 'validation/accuracy': 0.9859235882759094, 'validation/loss': 0.06142592802643776, 'validation/mean_average_precision': 0.2256212279317991, 'validation/num_examples': 43793, 'test/accuracy': 0.9849632978439331, 'test/loss': 0.06578873842954636, 'test/mean_average_precision': 0.21828367403432294, 'test/num_examples': 43793, 'score': 10332.806250333786, 'total_duration': 13768.079024553299, 'accumulated_submission_time': 10332.806250333786, 'accumulated_eval_time': 3432.8237850666046, 'accumulated_logging_time': 1.1231625080108643}
I0305 23:02:53.341983 139732767979264 logging_writer.py:48] [49403] accumulated_eval_time=3432.82, accumulated_logging_time=1.12316, accumulated_submission_time=10332.8, global_step=49403, preemption_count=0, score=10332.8, test/accuracy=0.984963, test/loss=0.0657887, test/mean_average_precision=0.218284, test/num_examples=43793, total_duration=13768.1, train/accuracy=0.998, train/loss=0.00939113, train/mean_average_precision=0.859582, validation/accuracy=0.985924, validation/loss=0.0614259, validation/mean_average_precision=0.225621, validation/num_examples=43793
I0305 23:03:13.974972 139732759586560 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.05489052087068558, loss=0.023446116596460342
I0305 23:03:35.288661 139732767979264 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.047301240265369415, loss=0.02185419574379921
I0305 23:03:56.213446 139732759586560 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.050711460411548615, loss=0.023787250742316246
I0305 23:04:16.905700 139732767979264 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.04407963156700134, loss=0.020519454032182693
I0305 23:04:37.923108 139732759586560 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.06656073033809662, loss=0.02226375788450241
I0305 23:04:58.816311 139732767979264 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.048875004053115845, loss=0.021132715046405792
I0305 23:05:19.632873 139732759586560 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.051386650651693344, loss=0.021977465599775314
I0305 23:05:40.132334 139732767979264 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.047561582177877426, loss=0.02190050110220909
I0305 23:06:00.912977 139732759586560 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.04990319162607193, loss=0.021550917997956276
I0305 23:06:21.641291 139732767979264 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.0647478774189949, loss=0.023409826681017876
I0305 23:06:42.402161 139732759586560 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.050311051309108734, loss=0.021863149479031563
I0305 23:06:53.465160 139874152301760 spec.py:321] Evaluating on the training split.
I0305 23:08:03.404034 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 23:08:05.337638 139874152301760 spec.py:349] Evaluating on the test split.
I0305 23:08:07.196267 139874152301760 submission_runner.py:469] Time since start: 14081.95s, 	Step: 50553, 	{'train/accuracy': 0.9966402649879456, 'train/loss': 0.011622194200754166, 'train/mean_average_precision': 0.8261209586689353, 'validation/accuracy': 0.9858484864234924, 'validation/loss': 0.06109374761581421, 'validation/mean_average_precision': 0.22330623289516918, 'validation/num_examples': 43793, 'test/accuracy': 0.9849397540092468, 'test/loss': 0.06542015075683594, 'test/mean_average_precision': 0.21834268129552697, 'test/num_examples': 43793, 'score': 10572.889156579971, 'total_duration': 14081.948567152023, 'accumulated_submission_time': 10572.889156579971, 'accumulated_eval_time': 3506.5547783374786, 'accumulated_logging_time': 1.147759199142456}
I0305 23:08:07.212236 139732767979264 logging_writer.py:48] [50553] accumulated_eval_time=3506.55, accumulated_logging_time=1.14776, accumulated_submission_time=10572.9, global_step=50553, preemption_count=0, score=10572.9, test/accuracy=0.98494, test/loss=0.0654202, test/mean_average_precision=0.218343, test/num_examples=43793, total_duration=14081.9, train/accuracy=0.99664, train/loss=0.0116222, train/mean_average_precision=0.826121, validation/accuracy=0.985848, validation/loss=0.0610937, validation/mean_average_precision=0.223306, validation/num_examples=43793
I0305 23:08:17.328406 139732759586560 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.07340562343597412, loss=0.023257115855813026
I0305 23:08:38.761428 139732767979264 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.07714750617742538, loss=0.023255275562405586
I0305 23:09:00.029854 139732759586560 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.05092620477080345, loss=0.023205097764730453
I0305 23:09:21.194063 139732767979264 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.06006363779306412, loss=0.023795921355485916
I0305 23:09:42.614010 139732759586560 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.07517959177494049, loss=0.024203112348914146
I0305 23:10:03.938402 139732767979264 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.07782666385173798, loss=0.02464948408305645
I0305 23:10:25.038678 139732759586560 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.07513908296823502, loss=0.0225794967263937
I0305 23:10:46.008619 139732767979264 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.09770520031452179, loss=0.02433963119983673
I0305 23:11:07.102545 139732759586560 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.08098147809505463, loss=0.02237541601061821
I0305 23:11:28.332749 139732767979264 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.06721701472997665, loss=0.021463636308908463
I0305 23:11:49.352950 139732759586560 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.061748404055833817, loss=0.022438788786530495
I0305 23:12:07.215708 139874152301760 spec.py:321] Evaluating on the training split.
I0305 23:13:18.194227 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 23:13:20.178297 139874152301760 spec.py:349] Evaluating on the test split.
I0305 23:13:22.082607 139874152301760 submission_runner.py:469] Time since start: 14396.83s, 	Step: 51689, 	{'train/accuracy': 0.9978092908859253, 'train/loss': 0.009726175107061863, 'train/mean_average_precision': 0.8564151721030591, 'validation/accuracy': 0.9859117865562439, 'validation/loss': 0.06114786118268967, 'validation/mean_average_precision': 0.22473642219369588, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.0655323714017868, 'test/mean_average_precision': 0.21932679277631054, 'test/num_examples': 43793, 'score': 10812.849580049515, 'total_duration': 14396.834939956665, 'accumulated_submission_time': 10812.849580049515, 'accumulated_eval_time': 3581.421598672867, 'accumulated_logging_time': 1.1749012470245361}
I0305 23:13:22.097969 139732767979264 logging_writer.py:48] [51689] accumulated_eval_time=3581.42, accumulated_logging_time=1.1749, accumulated_submission_time=10812.8, global_step=51689, preemption_count=0, score=10812.8, test/accuracy=0.984976, test/loss=0.0655324, test/mean_average_precision=0.219327, test/num_examples=43793, total_duration=14396.8, train/accuracy=0.997809, train/loss=0.00972618, train/mean_average_precision=0.856415, validation/accuracy=0.985912, validation/loss=0.0611479, validation/mean_average_precision=0.224736, validation/num_examples=43793
I0305 23:13:24.643783 139732759586560 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.06465615332126617, loss=0.022851016372442245
I0305 23:13:45.538855 139732767979264 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.0823647603392601, loss=0.02351756952702999
I0305 23:14:06.882244 139732759586560 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.08276256173849106, loss=0.02287774719297886
I0305 23:14:27.785101 139732767979264 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.08652607351541519, loss=0.023850739002227783
I0305 23:14:48.541154 139732759586560 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.07710622251033783, loss=0.02296883426606655
I0305 23:15:09.608610 139732767979264 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.06426724791526794, loss=0.022784439846873283
I0305 23:15:30.985894 139732759586560 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.06384768337011337, loss=0.023192405700683594
I0305 23:15:52.334929 139732767979264 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.06702695041894913, loss=0.022357221692800522
I0305 23:16:13.102744 139732759586560 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.091438889503479, loss=0.024460867047309875
I0305 23:16:33.968672 139732767979264 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.07139093428850174, loss=0.022416897118091583
I0305 23:16:55.302161 139732759586560 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.0787753239274025, loss=0.023182254284620285
I0305 23:17:16.530142 139732767979264 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.06851525604724884, loss=0.022285185754299164
I0305 23:17:22.094320 139874152301760 spec.py:321] Evaluating on the training split.
I0305 23:18:36.648592 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 23:18:38.734100 139874152301760 spec.py:349] Evaluating on the test split.
I0305 23:18:40.642208 139874152301760 submission_runner.py:469] Time since start: 14715.39s, 	Step: 52828, 	{'train/accuracy': 0.9976444244384766, 'train/loss': 0.009950489737093449, 'train/mean_average_precision': 0.8626955169129665, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.061140526086091995, 'validation/mean_average_precision': 0.2245727177855169, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265673995018, 'test/mean_average_precision': 0.2196943871380077, 'test/num_examples': 43793, 'score': 11052.803470373154, 'total_duration': 14715.394503831863, 'accumulated_submission_time': 11052.803470373154, 'accumulated_eval_time': 3659.969368457794, 'accumulated_logging_time': 1.2009425163269043}
I0305 23:18:40.656978 139732759586560 logging_writer.py:48] [52828] accumulated_eval_time=3659.97, accumulated_logging_time=1.20094, accumulated_submission_time=11052.8, global_step=52828, preemption_count=0, score=11052.8, test/accuracy=0.984979, test/loss=0.0655266, test/mean_average_precision=0.219694, test/num_examples=43793, total_duration=14715.4, train/accuracy=0.997644, train/loss=0.00995049, train/mean_average_precision=0.862696, validation/accuracy=0.98592, validation/loss=0.0611405, validation/mean_average_precision=0.224573, validation/num_examples=43793
I0305 23:18:56.090440 139732767979264 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.060034845024347305, loss=0.022221116349101067
I0305 23:19:17.443510 139732759586560 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.05867084115743637, loss=0.022583529353141785
I0305 23:19:38.441510 139732767979264 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.06678099185228348, loss=0.02210024744272232
I0305 23:19:59.572690 139732759586560 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09529896080493927, loss=0.023534586653113365
I0305 23:20:20.868486 139732767979264 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.07112714648246765, loss=0.022860223427414894
I0305 23:20:42.256886 139732759586560 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.07472892105579376, loss=0.02230069227516651
I0305 23:21:03.757510 139732767979264 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.06680461764335632, loss=0.02160531096160412
I0305 23:21:25.099249 139732759586560 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.06804085522890091, loss=0.022655043751001358
I0305 23:21:46.441279 139732767979264 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.05051063373684883, loss=0.02081582508981228
I0305 23:22:07.246848 139732759586560 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.0743197575211525, loss=0.023946046829223633
I0305 23:22:28.049228 139732767979264 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.07714632898569107, loss=0.021790502592921257
I0305 23:22:40.676186 139874152301760 spec.py:321] Evaluating on the training split.
I0305 23:23:47.735227 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 23:23:49.708250 139874152301760 spec.py:349] Evaluating on the test split.
I0305 23:23:51.763881 139874152301760 submission_runner.py:469] Time since start: 15026.52s, 	Step: 53961, 	{'train/accuracy': 0.9971904158592224, 'train/loss': 0.010524259880185127, 'train/mean_average_precision': 0.8435058116368522, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.0611405223608017, 'validation/mean_average_precision': 0.22451027018643, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265748500824, 'test/mean_average_precision': 0.21958940289531087, 'test/num_examples': 43793, 'score': 11292.784072637558, 'total_duration': 15026.516201496124, 'accumulated_submission_time': 11292.784072637558, 'accumulated_eval_time': 3731.056984901428, 'accumulated_logging_time': 1.2245934009552002}
I0305 23:23:51.780184 139732759586560 logging_writer.py:48] [53961] accumulated_eval_time=3731.06, accumulated_logging_time=1.22459, accumulated_submission_time=11292.8, global_step=53961, preemption_count=0, score=11292.8, test/accuracy=0.984979, test/loss=0.0655266, test/mean_average_precision=0.219589, test/num_examples=43793, total_duration=15026.5, train/accuracy=0.99719, train/loss=0.0105243, train/mean_average_precision=0.843506, validation/accuracy=0.98592, validation/loss=0.0611405, validation/mean_average_precision=0.22451, validation/num_examples=43793
I0305 23:24:00.171901 139732767979264 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.06662940233945847, loss=0.02267506718635559
I0305 23:24:21.137555 139732759586560 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.07678429782390594, loss=0.021883992478251457
I0305 23:24:42.262476 139732767979264 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.06779690086841583, loss=0.022406945005059242
I0305 23:25:03.506544 139732759586560 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.06955210119485855, loss=0.021427514031529427
I0305 23:25:24.543269 139732767979264 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.06478086113929749, loss=0.02400840073823929
I0305 23:25:45.552546 139732759586560 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.05987682566046715, loss=0.02234063483774662
I0305 23:26:06.439018 139732767979264 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.084245465695858, loss=0.023005286231637
I0305 23:26:27.252939 139732759586560 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.06892542541027069, loss=0.02306235022842884
I0305 23:26:48.057133 139732767979264 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09585608541965485, loss=0.02504260092973709
I0305 23:27:08.955562 139732759586560 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.07121328264474869, loss=0.023014221340417862
I0305 23:27:29.914015 139732767979264 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.06471222639083862, loss=0.024328842759132385
I0305 23:27:50.870773 139732759586560 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.06266968697309494, loss=0.02127675525844097
I0305 23:27:51.890580 139874152301760 spec.py:321] Evaluating on the training split.
I0305 23:29:04.260221 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 23:29:06.234806 139874152301760 spec.py:349] Evaluating on the test split.
I0305 23:29:08.142552 139874152301760 submission_runner.py:469] Time since start: 15342.89s, 	Step: 55106, 	{'train/accuracy': 0.997311532497406, 'train/loss': 0.0104548130184412, 'train/mean_average_precision': 0.847944794044512, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.0611405149102211, 'validation/mean_average_precision': 0.2244286208671341, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265673995018, 'test/mean_average_precision': 0.21954968550268894, 'test/num_examples': 43793, 'score': 11532.855032682419, 'total_duration': 15342.894850969315, 'accumulated_submission_time': 11532.855032682419, 'accumulated_eval_time': 3807.308841228485, 'accumulated_logging_time': 1.251119613647461}
I0305 23:29:08.157830 139732767979264 logging_writer.py:48] [55106] accumulated_eval_time=3807.31, accumulated_logging_time=1.25112, accumulated_submission_time=11532.9, global_step=55106, preemption_count=0, score=11532.9, test/accuracy=0.984979, test/loss=0.0655266, test/mean_average_precision=0.21955, test/num_examples=43793, total_duration=15342.9, train/accuracy=0.997312, train/loss=0.0104548, train/mean_average_precision=0.847945, validation/accuracy=0.98592, validation/loss=0.0611405, validation/mean_average_precision=0.224429, validation/num_examples=43793
I0305 23:29:28.084641 139732759586560 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.07375034689903259, loss=0.02280004322528839
I0305 23:29:48.946701 139732767979264 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.09034838527441025, loss=0.02322428487241268
I0305 23:30:10.240410 139732759586560 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.07590895891189575, loss=0.022931044921278954
I0305 23:30:31.764014 139732767979264 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.09963139146566391, loss=0.02375965379178524
I0305 23:30:53.152275 139732759586560 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.06413429975509644, loss=0.021497074514627457
I0305 23:31:14.653543 139732767979264 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.06106051802635193, loss=0.021311473101377487
I0305 23:31:35.769792 139732759586560 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.05684042349457741, loss=0.02218211628496647
I0305 23:31:57.360306 139732767979264 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.08949386328458786, loss=0.022104181349277496
I0305 23:32:18.497736 139732759586560 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.09024131298065186, loss=0.02192637138068676
I0305 23:32:39.648222 139732767979264 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.06886111199855804, loss=0.022890599444508553
I0305 23:33:01.073854 139732759586560 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.07362782210111618, loss=0.022535832598805428
I0305 23:33:08.257823 139874152301760 spec.py:321] Evaluating on the training split.
I0305 23:34:20.763672 139874152301760 spec.py:333] Evaluating on the validation split.
I0305 23:34:22.744731 139874152301760 spec.py:349] Evaluating on the test split.
I0305 23:34:24.671571 139874152301760 submission_runner.py:469] Time since start: 15659.42s, 	Step: 56235, 	{'train/accuracy': 0.9974057078361511, 'train/loss': 0.010266638360917568, 'train/mean_average_precision': 0.8402096272803246, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.0611405223608017, 'validation/mean_average_precision': 0.2244589653244849, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265748500824, 'test/mean_average_precision': 0.21951629967269648, 'test/num_examples': 43793, 'score': 11772.915124177933, 'total_duration': 15659.423861026764, 'accumulated_submission_time': 11772.915124177933, 'accumulated_eval_time': 3883.7224667072296, 'accumulated_logging_time': 1.2764782905578613}
I0305 23:34:24.687757 139732767979264 logging_writer.py:48] [56235] accumulated_eval_time=3883.72, accumulated_logging_time=1.27648, accumulated_submission_time=11772.9, global_step=56235, preemption_count=0, score=11772.9, test/accuracy=0.984979, test/loss=0.0655266, test/mean_average_precision=0.219516, test/num_examples=43793, total_duration=15659.4, train/accuracy=0.997406, train/loss=0.0102666, train/mean_average_precision=0.84021, validation/accuracy=0.98592, validation/loss=0.0611405, validation/mean_average_precision=0.224459, validation/num_examples=43793
I0305 23:34:38.829741 139732759586560 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.06552982330322266, loss=0.02320278435945511
I0305 23:34:59.898030 139732767979264 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.06060685217380524, loss=0.022181913256645203
I0305 23:35:20.262813 139732759586560 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.053862906992435455, loss=0.02191932126879692
I0305 23:35:41.034730 139732767979264 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.0774678885936737, loss=0.022236481308937073
I0305 23:36:02.138466 139732759586560 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.08923111855983734, loss=0.02317906729876995
I0305 23:36:22.831926 139732767979264 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.06536830216646194, loss=0.023283889517188072
I0305 23:36:43.738984 139732759586560 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.08293066918849945, loss=0.022746777161955833
I0305 23:37:04.811011 139732767979264 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.0769653245806694, loss=0.023958617821335793
I0305 23:37:26.071899 139732759586560 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.0580977126955986, loss=0.021440444514155388
I0305 23:37:47.062745 139732767979264 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.06079481542110443, loss=0.022175276651978493
I0305 23:38:08.072426 139732759586560 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.07593280076980591, loss=0.023024406284093857
I0305 23:38:24.884488 139732767979264 logging_writer.py:48] [57381] global_step=57381, preemption_count=0, score=12013.1
I0305 23:38:25.027333 139874152301760 submission_runner.py:646] Tuning trial 2/5
I0305 23:38:25.027530 139874152301760 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0305 23:38:25.029291 139874152301760 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.49877113103866577, 'train/loss': 0.7439650893211365, 'train/mean_average_precision': 0.022749856777801804, 'validation/accuracy': 0.5072119235992432, 'validation/loss': 0.7433469295501709, 'validation/mean_average_precision': 0.027750354548743285, 'validation/num_examples': 43793, 'test/accuracy': 0.5080327987670898, 'test/loss': 0.7442464828491211, 'test/mean_average_precision': 0.029261753404640253, 'test/num_examples': 43793, 'score': 11.092934608459473, 'total_duration': 214.11165142059326, 'accumulated_submission_time': 11.092934608459473, 'accumulated_eval_time': 203.01861023902893, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1145, {'train/accuracy': 0.9864374995231628, 'train/loss': 0.05516144260764122, 'train/mean_average_precision': 0.04591063021475647, 'validation/accuracy': 0.983965277671814, 'validation/loss': 0.06401167064905167, 'validation/mean_average_precision': 0.04511029391143287, 'validation/num_examples': 43793, 'test/accuracy': 0.9829360842704773, 'test/loss': 0.06705603748559952, 'test/mean_average_precision': 0.047387303631053146, 'test/num_examples': 43793, 'score': 251.12976384162903, 'total_duration': 531.7122213840485, 'accumulated_submission_time': 251.12976384162903, 'accumulated_eval_time': 280.5344157218933, 'accumulated_logging_time': 0.016844749450683594, 'global_step': 1145, 'preemption_count': 0}), (2289, {'train/accuracy': 0.9877419471740723, 'train/loss': 0.04516187682747841, 'train/mean_average_precision': 0.12057029700064636, 'validation/accuracy': 0.9848746061325073, 'validation/loss': 0.054856833070516586, 'validation/mean_average_precision': 0.11576847013670194, 'validation/num_examples': 43793, 'test/accuracy': 0.9838648438453674, 'test/loss': 0.05775333195924759, 'test/mean_average_precision': 0.11517125875675659, 'test/num_examples': 43793, 'score': 491.20559000968933, 'total_duration': 848.5234010219574, 'accumulated_submission_time': 491.20559000968933, 'accumulated_eval_time': 357.22040486335754, 'accumulated_logging_time': 0.03621363639831543, 'global_step': 2289, 'preemption_count': 0}), (3432, {'train/accuracy': 0.9879274964332581, 'train/loss': 0.04282098636031151, 'train/mean_average_precision': 0.16973557183398788, 'validation/accuracy': 0.985166072845459, 'validation/loss': 0.05215247720479965, 'validation/mean_average_precision': 0.15336056071848927, 'validation/num_examples': 43793, 'test/accuracy': 0.9842060208320618, 'test/loss': 0.054896775633096695, 'test/mean_average_precision': 0.15374219588800284, 'test/num_examples': 43793, 'score': 731.2589046955109, 'total_duration': 1166.5668742656708, 'accumulated_submission_time': 731.2589046955109, 'accumulated_eval_time': 435.16318368911743, 'accumulated_logging_time': 0.05500626564025879, 'global_step': 3432, 'preemption_count': 0}), (4565, {'train/accuracy': 0.9884599447250366, 'train/loss': 0.04024913161993027, 'train/mean_average_precision': 0.2074693941699252, 'validation/accuracy': 0.9856154322624207, 'validation/loss': 0.049436211585998535, 'validation/mean_average_precision': 0.18266864093542523, 'validation/num_examples': 43793, 'test/accuracy': 0.9846979379653931, 'test/loss': 0.052152447402477264, 'test/mean_average_precision': 0.18193304438090765, 'test/num_examples': 43793, 'score': 971.3696434497833, 'total_duration': 1483.0306069850922, 'accumulated_submission_time': 971.3696434497833, 'accumulated_eval_time': 511.46442675590515, 'accumulated_logging_time': 0.07589077949523926, 'global_step': 4565, 'preemption_count': 0}), (5704, {'train/accuracy': 0.9889758229255676, 'train/loss': 0.03821399807929993, 'train/mean_average_precision': 0.25583006029625704, 'validation/accuracy': 0.9858922958374023, 'validation/loss': 0.047857388854026794, 'validation/mean_average_precision': 0.19973735105812732, 'validation/num_examples': 43793, 'test/accuracy': 0.9850130081176758, 'test/loss': 0.05038413405418396, 'test/mean_average_precision': 0.20289022672155488, 'test/num_examples': 43793, 'score': 1211.4673175811768, 'total_duration': 1800.6749556064606, 'accumulated_submission_time': 1211.4673175811768, 'accumulated_eval_time': 588.9614708423615, 'accumulated_logging_time': 0.09637761116027832, 'global_step': 5704, 'preemption_count': 0}), (6865, {'train/accuracy': 0.988731324672699, 'train/loss': 0.03798190504312515, 'train/mean_average_precision': 0.27071761958956864, 'validation/accuracy': 0.9858971834182739, 'validation/loss': 0.047891873866319656, 'validation/mean_average_precision': 0.22091041719475196, 'validation/num_examples': 43793, 'test/accuracy': 0.9850416779518127, 'test/loss': 0.05059099942445755, 'test/mean_average_precision': 0.22152432496286573, 'test/num_examples': 43793, 'score': 1451.4891369342804, 'total_duration': 2115.606386899948, 'accumulated_submission_time': 1451.4891369342804, 'accumulated_eval_time': 663.8225286006927, 'accumulated_logging_time': 0.11505937576293945, 'global_step': 6865, 'preemption_count': 0}), (8022, {'train/accuracy': 0.9893003702163696, 'train/loss': 0.035843394696712494, 'train/mean_average_precision': 0.31398676502207606, 'validation/accuracy': 0.9860972762107849, 'validation/loss': 0.04667762294411659, 'validation/mean_average_precision': 0.2330377733332588, 'validation/num_examples': 43793, 'test/accuracy': 0.9852880835533142, 'test/loss': 0.049434591084718704, 'test/mean_average_precision': 0.23080995411207608, 'test/num_examples': 43793, 'score': 1691.4678633213043, 'total_duration': 2431.7623097896576, 'accumulated_submission_time': 1691.4678633213043, 'accumulated_eval_time': 739.9517350196838, 'accumulated_logging_time': 0.13390016555786133, 'global_step': 8022, 'preemption_count': 0}), (9190, {'train/accuracy': 0.9897857904434204, 'train/loss': 0.034656308591365814, 'train/mean_average_precision': 0.3368483868153639, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.04576916992664337, 'validation/mean_average_precision': 0.2444831890665955, 'validation/num_examples': 43793, 'test/accuracy': 0.9854721426963806, 'test/loss': 0.0485151968896389, 'test/mean_average_precision': 0.23839677236397414, 'test/num_examples': 43793, 'score': 1931.4174807071686, 'total_duration': 2747.2317550182343, 'accumulated_submission_time': 1931.4174807071686, 'accumulated_eval_time': 815.4159688949585, 'accumulated_logging_time': 0.1585092544555664, 'global_step': 9190, 'preemption_count': 0}), (10354, {'train/accuracy': 0.990146815776825, 'train/loss': 0.03284255787730217, 'train/mean_average_precision': 0.37508642111157514, 'validation/accuracy': 0.9864902496337891, 'validation/loss': 0.045455023646354675, 'validation/mean_average_precision': 0.2446381251014512, 'validation/num_examples': 43793, 'test/accuracy': 0.985659122467041, 'test/loss': 0.04795210435986519, 'test/mean_average_precision': 0.2450861195018509, 'test/num_examples': 43793, 'score': 2171.470983982086, 'total_duration': 3062.3003227710724, 'accumulated_submission_time': 2171.470983982086, 'accumulated_eval_time': 890.38174700737, 'accumulated_logging_time': 0.1770155429840088, 'global_step': 10354, 'preemption_count': 0}), (11507, {'train/accuracy': 0.9903236031532288, 'train/loss': 0.03239036351442337, 'train/mean_average_precision': 0.3871162408295008, 'validation/accuracy': 0.9866433143615723, 'validation/loss': 0.045177657157182693, 'validation/mean_average_precision': 0.25284161528217025, 'validation/num_examples': 43793, 'test/accuracy': 0.9857555627822876, 'test/loss': 0.04797894507646561, 'test/mean_average_precision': 0.2481245184895599, 'test/num_examples': 43793, 'score': 2411.435134410858, 'total_duration': 3378.9535393714905, 'accumulated_submission_time': 2411.435134410858, 'accumulated_eval_time': 967.0125823020935, 'accumulated_logging_time': 0.19714641571044922, 'global_step': 11507, 'preemption_count': 0}), (12678, {'train/accuracy': 0.9907436966896057, 'train/loss': 0.030696703121066093, 'train/mean_average_precision': 0.4229015274387266, 'validation/accuracy': 0.9866238236427307, 'validation/loss': 0.04529475048184395, 'validation/mean_average_precision': 0.25419988198231624, 'validation/num_examples': 43793, 'test/accuracy': 0.985772430896759, 'test/loss': 0.048084307461977005, 'test/mean_average_precision': 0.2524693550631113, 'test/num_examples': 43793, 'score': 2651.4471337795258, 'total_duration': 3693.828275680542, 'accumulated_submission_time': 2651.4471337795258, 'accumulated_eval_time': 1041.8242259025574, 'accumulated_logging_time': 0.21685123443603516, 'global_step': 12678, 'preemption_count': 0}), (13841, {'train/accuracy': 0.99073725938797, 'train/loss': 0.031154727563261986, 'train/mean_average_precision': 0.41898705233073286, 'validation/accuracy': 0.9866591095924377, 'validation/loss': 0.04519756883382797, 'validation/mean_average_precision': 0.25840477633142106, 'validation/num_examples': 43793, 'test/accuracy': 0.9856797456741333, 'test/loss': 0.048298295587301254, 'test/mean_average_precision': 0.2470126916673839, 'test/num_examples': 43793, 'score': 2891.5813200473785, 'total_duration': 4007.7341310977936, 'accumulated_submission_time': 2891.5813200473785, 'accumulated_eval_time': 1115.5470321178436, 'accumulated_logging_time': 0.23698925971984863, 'global_step': 13841, 'preemption_count': 0}), (15008, {'train/accuracy': 0.9914278984069824, 'train/loss': 0.028476286679506302, 'train/mean_average_precision': 0.4560264047999383, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.04524127021431923, 'validation/mean_average_precision': 0.25725007743941913, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.04817013069987297, 'test/mean_average_precision': 0.25109018127640476, 'test/num_examples': 43793, 'score': 3131.730295419693, 'total_duration': 4323.885477781296, 'accumulated_submission_time': 3131.730295419693, 'accumulated_eval_time': 1191.4993033409119, 'accumulated_logging_time': 0.2574193477630615, 'global_step': 15008, 'preemption_count': 0}), (16167, {'train/accuracy': 0.9910731315612793, 'train/loss': 0.029393423348665237, 'train/mean_average_precision': 0.4634485163445269, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04545791074633598, 'validation/mean_average_precision': 0.25572840462593255, 'validation/num_examples': 43793, 'test/accuracy': 0.9857509732246399, 'test/loss': 0.048647813498973846, 'test/mean_average_precision': 0.25169142053904886, 'test/num_examples': 43793, 'score': 3371.6855449676514, 'total_duration': 4639.17437338829, 'accumulated_submission_time': 3371.6855449676514, 'accumulated_eval_time': 1266.7827770709991, 'accumulated_logging_time': 0.27634644508361816, 'global_step': 16167, 'preemption_count': 0}), (17327, {'train/accuracy': 0.9917005300521851, 'train/loss': 0.02687568962574005, 'train/mean_average_precision': 0.5139073470529663, 'validation/accuracy': 0.986731767654419, 'validation/loss': 0.04569559171795845, 'validation/mean_average_precision': 0.2583347806244402, 'validation/num_examples': 43793, 'test/accuracy': 0.985765278339386, 'test/loss': 0.048895251005887985, 'test/mean_average_precision': 0.2535455374173522, 'test/num_examples': 43793, 'score': 3611.642764568329, 'total_duration': 4952.445822954178, 'accumulated_submission_time': 3611.642764568329, 'accumulated_eval_time': 1340.0468039512634, 'accumulated_logging_time': 0.296067476272583, 'global_step': 17327, 'preemption_count': 0}), (18496, {'train/accuracy': 0.991524338722229, 'train/loss': 0.027586758136749268, 'train/mean_average_precision': 0.4999564340516715, 'validation/accuracy': 0.986860454082489, 'validation/loss': 0.045700524002313614, 'validation/mean_average_precision': 0.2613823137622827, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.04874904453754425, 'test/mean_average_precision': 0.25528535449662565, 'test/num_examples': 43793, 'score': 3851.7410027980804, 'total_duration': 5268.67920255661, 'accumulated_submission_time': 3851.7410027980804, 'accumulated_eval_time': 1416.1338195800781, 'accumulated_logging_time': 0.31731724739074707, 'global_step': 18496, 'preemption_count': 0}), (19645, {'train/accuracy': 0.9923588037490845, 'train/loss': 0.02485167421400547, 'train/mean_average_precision': 0.549003910825518, 'validation/accuracy': 0.9868689775466919, 'validation/loss': 0.045794595032930374, 'validation/mean_average_precision': 0.2584218284647551, 'validation/num_examples': 43793, 'test/accuracy': 0.9859084486961365, 'test/loss': 0.04903876781463623, 'test/mean_average_precision': 0.2585274268765164, 'test/num_examples': 43793, 'score': 4091.8255486488342, 'total_duration': 5584.82938170433, 'accumulated_submission_time': 4091.8255486488342, 'accumulated_eval_time': 1492.1450102329254, 'accumulated_logging_time': 0.33849430084228516, 'global_step': 19645, 'preemption_count': 0}), (20804, {'train/accuracy': 0.9921115040779114, 'train/loss': 0.025916822254657745, 'train/mean_average_precision': 0.5165288718174297, 'validation/accuracy': 0.986763060092926, 'validation/loss': 0.046055614948272705, 'validation/mean_average_precision': 0.2582366919475289, 'validation/num_examples': 43793, 'test/accuracy': 0.9858600497245789, 'test/loss': 0.04925216734409332, 'test/mean_average_precision': 0.2554506800584729, 'test/num_examples': 43793, 'score': 4331.910178184509, 'total_duration': 5899.47155380249, 'accumulated_submission_time': 4331.910178184509, 'accumulated_eval_time': 1566.6511771678925, 'accumulated_logging_time': 0.35932350158691406, 'global_step': 20804, 'preemption_count': 0}), (21956, {'train/accuracy': 0.9928665161132812, 'train/loss': 0.023163488134741783, 'train/mean_average_precision': 0.5913190726050988, 'validation/accuracy': 0.9867261052131653, 'validation/loss': 0.04660394787788391, 'validation/mean_average_precision': 0.2571444896194497, 'validation/num_examples': 43793, 'test/accuracy': 0.9857956171035767, 'test/loss': 0.04987436532974243, 'test/mean_average_precision': 0.24984429184119222, 'test/num_examples': 43793, 'score': 4571.919264316559, 'total_duration': 6214.099309921265, 'accumulated_submission_time': 4571.919264316559, 'accumulated_eval_time': 1641.2185254096985, 'accumulated_logging_time': 0.3810579776763916, 'global_step': 21956, 'preemption_count': 0}), (23099, {'train/accuracy': 0.9921954870223999, 'train/loss': 0.025153161957859993, 'train/mean_average_precision': 0.5458033140146548, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.04703688994050026, 'validation/mean_average_precision': 0.2576230886367076, 'validation/num_examples': 43793, 'test/accuracy': 0.9858444333076477, 'test/loss': 0.05042051896452904, 'test/mean_average_precision': 0.25767971063007483, 'test/num_examples': 43793, 'score': 4811.897761106491, 'total_duration': 6527.909942388535, 'accumulated_submission_time': 4811.897761106491, 'accumulated_eval_time': 1714.9992928504944, 'accumulated_logging_time': 0.4030427932739258, 'global_step': 23099, 'preemption_count': 0}), (24239, {'train/accuracy': 0.9931254982948303, 'train/loss': 0.02246207371354103, 'train/mean_average_precision': 0.5974875951937973, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.047493305057287216, 'validation/mean_average_precision': 0.25046716428373667, 'validation/num_examples': 43793, 'test/accuracy': 0.9857892990112305, 'test/loss': 0.0508321151137352, 'test/mean_average_precision': 0.24394047755672874, 'test/num_examples': 43793, 'score': 5051.900489091873, 'total_duration': 6842.423726081848, 'accumulated_submission_time': 5051.900489091873, 'accumulated_eval_time': 1789.456362247467, 'accumulated_logging_time': 0.4244542121887207, 'global_step': 24239, 'preemption_count': 0}), (25385, {'train/accuracy': 0.9928680658340454, 'train/loss': 0.02308470569550991, 'train/mean_average_precision': 0.5870958621481228, 'validation/accuracy': 0.9867252707481384, 'validation/loss': 0.04742967709898949, 'validation/mean_average_precision': 0.2565169749292392, 'validation/num_examples': 43793, 'test/accuracy': 0.9857997894287109, 'test/loss': 0.05089139938354492, 'test/mean_average_precision': 0.2469513090827234, 'test/num_examples': 43793, 'score': 5291.868155002594, 'total_duration': 7157.60574388504, 'accumulated_submission_time': 5291.868155002594, 'accumulated_eval_time': 1864.6186635494232, 'accumulated_logging_time': 0.4451620578765869, 'global_step': 25385, 'preemption_count': 0}), (26542, {'train/accuracy': 0.9937341213226318, 'train/loss': 0.020803652703762054, 'train/mean_average_precision': 0.6316388064937534, 'validation/accuracy': 0.9866083860397339, 'validation/loss': 0.04819588363170624, 'validation/mean_average_precision': 0.2502100090846716, 'validation/num_examples': 43793, 'test/accuracy': 0.9856246113777161, 'test/loss': 0.051725100725889206, 'test/mean_average_precision': 0.24612282365579394, 'test/num_examples': 43793, 'score': 5531.930026292801, 'total_duration': 7469.73424744606, 'accumulated_submission_time': 5531.930026292801, 'accumulated_eval_time': 1936.6348514556885, 'accumulated_logging_time': 0.46584033966064453, 'global_step': 26542, 'preemption_count': 0}), (27686, {'train/accuracy': 0.993607223033905, 'train/loss': 0.02094670757651329, 'train/mean_average_precision': 0.6301575330527193, 'validation/accuracy': 0.9864906668663025, 'validation/loss': 0.048356007784605026, 'validation/mean_average_precision': 0.25252388986337104, 'validation/num_examples': 43793, 'test/accuracy': 0.9856148958206177, 'test/loss': 0.05162663385272026, 'test/mean_average_precision': 0.24578366870256033, 'test/num_examples': 43793, 'score': 5771.970009326935, 'total_duration': 7785.901625394821, 'accumulated_submission_time': 5771.970009326935, 'accumulated_eval_time': 2012.7064208984375, 'accumulated_logging_time': 0.4870107173919678, 'global_step': 27686, 'preemption_count': 0}), (28822, {'train/accuracy': 0.9936826825141907, 'train/loss': 0.020257622003555298, 'train/mean_average_precision': 0.658814244789802, 'validation/accuracy': 0.9865779280662537, 'validation/loss': 0.04961155727505684, 'validation/mean_average_precision': 0.2514140937941614, 'validation/num_examples': 43793, 'test/accuracy': 0.9856418371200562, 'test/loss': 0.05323880538344383, 'test/mean_average_precision': 0.2417591569772401, 'test/num_examples': 43793, 'score': 6011.909794092178, 'total_duration': 8097.452238082886, 'accumulated_submission_time': 6011.909794092178, 'accumulated_eval_time': 2084.0622293949127, 'accumulated_logging_time': 0.7099466323852539, 'global_step': 28822, 'preemption_count': 0}), (29962, {'train/accuracy': 0.993719756603241, 'train/loss': 0.020009413361549377, 'train/mean_average_precision': 0.6417948322585378, 'validation/accuracy': 0.9865661859512329, 'validation/loss': 0.050407394766807556, 'validation/mean_average_precision': 0.24902321630328184, 'validation/num_examples': 43793, 'test/accuracy': 0.9856254458427429, 'test/loss': 0.05380432680249214, 'test/mean_average_precision': 0.23985349993736801, 'test/num_examples': 43793, 'score': 6251.930826425552, 'total_duration': 8412.063458919525, 'accumulated_submission_time': 6251.930826425552, 'accumulated_eval_time': 2158.6000969409943, 'accumulated_logging_time': 0.7314238548278809, 'global_step': 29962, 'preemption_count': 0}), (31113, {'train/accuracy': 0.993526816368103, 'train/loss': 0.020579291507601738, 'train/mean_average_precision': 0.6352587161937853, 'validation/accuracy': 0.9863713383674622, 'validation/loss': 0.050849247723817825, 'validation/mean_average_precision': 0.24517835116222927, 'validation/num_examples': 43793, 'test/accuracy': 0.9855453968048096, 'test/loss': 0.0541442409157753, 'test/mean_average_precision': 0.23467304273784842, 'test/num_examples': 43793, 'score': 6491.998294830322, 'total_duration': 8726.783337831497, 'accumulated_submission_time': 6491.998294830322, 'accumulated_eval_time': 2233.19970870018, 'accumulated_logging_time': 0.7522673606872559, 'global_step': 31113, 'preemption_count': 0}), (32258, {'train/accuracy': 0.9946621060371399, 'train/loss': 0.017532046884298325, 'train/mean_average_precision': 0.6939108241643204, 'validation/accuracy': 0.9864147305488586, 'validation/loss': 0.0510583259165287, 'validation/mean_average_precision': 0.24764756606908744, 'validation/num_examples': 43793, 'test/accuracy': 0.985542893409729, 'test/loss': 0.054522089660167694, 'test/mean_average_precision': 0.23828597622566788, 'test/num_examples': 43793, 'score': 6732.089219331741, 'total_duration': 9040.162019729614, 'accumulated_submission_time': 6732.089219331741, 'accumulated_eval_time': 2306.4356710910797, 'accumulated_logging_time': 0.773418664932251, 'global_step': 32258, 'preemption_count': 0}), (33412, {'train/accuracy': 0.9946038722991943, 'train/loss': 0.017565829679369926, 'train/mean_average_precision': 0.691784880147996, 'validation/accuracy': 0.9862726926803589, 'validation/loss': 0.05233044922351837, 'validation/mean_average_precision': 0.24262168195289693, 'validation/num_examples': 43793, 'test/accuracy': 0.9854300022125244, 'test/loss': 0.05581118166446686, 'test/mean_average_precision': 0.23631146350958138, 'test/num_examples': 43793, 'score': 6972.114375114441, 'total_duration': 9354.853789806366, 'accumulated_submission_time': 6972.114375114441, 'accumulated_eval_time': 2381.0504817962646, 'accumulated_logging_time': 0.7963719367980957, 'global_step': 33412, 'preemption_count': 0}), (34562, {'train/accuracy': 0.9952947497367859, 'train/loss': 0.015384063124656677, 'train/mean_average_precision': 0.7525746115424387, 'validation/accuracy': 0.9862949848175049, 'validation/loss': 0.053698163479566574, 'validation/mean_average_precision': 0.23808560064300088, 'validation/num_examples': 43793, 'test/accuracy': 0.9854207038879395, 'test/loss': 0.057549938559532166, 'test/mean_average_precision': 0.22971863826004962, 'test/num_examples': 43793, 'score': 7212.15430855751, 'total_duration': 9670.287094831467, 'accumulated_submission_time': 7212.15430855751, 'accumulated_eval_time': 2456.3896741867065, 'accumulated_logging_time': 0.8182563781738281, 'global_step': 34562, 'preemption_count': 0}), (35710, {'train/accuracy': 0.994560182094574, 'train/loss': 0.017042359337210655, 'train/mean_average_precision': 0.7098460528371133, 'validation/accuracy': 0.9863213896751404, 'validation/loss': 0.054151590913534164, 'validation/mean_average_precision': 0.24075044495033687, 'validation/num_examples': 43793, 'test/accuracy': 0.9853811264038086, 'test/loss': 0.05825928598642349, 'test/mean_average_precision': 0.23167227900498352, 'test/num_examples': 43793, 'score': 7452.1787214279175, 'total_duration': 9983.350497484207, 'accumulated_submission_time': 7452.1787214279175, 'accumulated_eval_time': 2529.3763267993927, 'accumulated_logging_time': 0.8398258686065674, 'global_step': 35710, 'preemption_count': 0}), (36860, {'train/accuracy': 0.9963377118110657, 'train/loss': 0.013156083412468433, 'train/mean_average_precision': 0.7923306539296511, 'validation/accuracy': 0.9862077236175537, 'validation/loss': 0.054792370647192, 'validation/mean_average_precision': 0.24084438166872574, 'validation/num_examples': 43793, 'test/accuracy': 0.9852867722511292, 'test/loss': 0.05871647968888283, 'test/mean_average_precision': 0.2333666808963135, 'test/num_examples': 43793, 'score': 7692.168416023254, 'total_duration': 10301.318064689636, 'accumulated_submission_time': 7692.168416023254, 'accumulated_eval_time': 2607.302830219269, 'accumulated_logging_time': 0.8612604141235352, 'global_step': 36860, 'preemption_count': 0}), (38000, {'train/accuracy': 0.994249165058136, 'train/loss': 0.017675340175628662, 'train/mean_average_precision': 0.6903032068773789, 'validation/accuracy': 0.9861496686935425, 'validation/loss': 0.05637277290225029, 'validation/mean_average_precision': 0.23400285314993474, 'validation/num_examples': 43793, 'test/accuracy': 0.985261082649231, 'test/loss': 0.06050729379057884, 'test/mean_average_precision': 0.22420483867792856, 'test/num_examples': 43793, 'score': 7932.2036011219025, 'total_duration': 10615.541823625565, 'accumulated_submission_time': 7932.2036011219025, 'accumulated_eval_time': 2681.436852455139, 'accumulated_logging_time': 0.8832120895385742, 'global_step': 38000, 'preemption_count': 0}), (39139, {'train/accuracy': 0.9967915415763855, 'train/loss': 0.01196231972426176, 'train/mean_average_precision': 0.8155705073548373, 'validation/accuracy': 0.98621666431427, 'validation/loss': 0.056888166815042496, 'validation/mean_average_precision': 0.23239354318376004, 'validation/num_examples': 43793, 'test/accuracy': 0.9853238463401794, 'test/loss': 0.060974471271038055, 'test/mean_average_precision': 0.22815414317868796, 'test/num_examples': 43793, 'score': 8172.170121431351, 'total_duration': 10932.154947280884, 'accumulated_submission_time': 8172.170121431351, 'accumulated_eval_time': 2758.028742313385, 'accumulated_logging_time': 0.9058797359466553, 'global_step': 39139, 'preemption_count': 0}), (40268, {'train/accuracy': 0.9949260950088501, 'train/loss': 0.015661848708987236, 'train/mean_average_precision': 0.753983069275038, 'validation/accuracy': 0.9861054420471191, 'validation/loss': 0.05771198868751526, 'validation/mean_average_precision': 0.23610322844222537, 'validation/num_examples': 43793, 'test/accuracy': 0.985156238079071, 'test/loss': 0.062026187777519226, 'test/mean_average_precision': 0.22584390609876207, 'test/num_examples': 43793, 'score': 8412.192198753357, 'total_duration': 11244.161386728287, 'accumulated_submission_time': 8412.192198753357, 'accumulated_eval_time': 2829.957175016403, 'accumulated_logging_time': 0.9296557903289795, 'global_step': 40268, 'preemption_count': 0}), (41406, {'train/accuracy': 0.9973022937774658, 'train/loss': 0.010876358486711979, 'train/mean_average_precision': 0.8285463677012882, 'validation/accuracy': 0.9860429167747498, 'validation/loss': 0.05826247110962868, 'validation/mean_average_precision': 0.22849953487184563, 'validation/num_examples': 43793, 'test/accuracy': 0.9851676225662231, 'test/loss': 0.06246663257479668, 'test/mean_average_precision': 0.22317851136181902, 'test/num_examples': 43793, 'score': 8652.19577908516, 'total_duration': 11560.630364894867, 'accumulated_submission_time': 8652.19577908516, 'accumulated_eval_time': 2906.367070198059, 'accumulated_logging_time': 0.9549150466918945, 'global_step': 41406, 'preemption_count': 0}), (42553, {'train/accuracy': 0.9956129193305969, 'train/loss': 0.013847298920154572, 'train/mean_average_precision': 0.7825633837318187, 'validation/accuracy': 0.9860416650772095, 'validation/loss': 0.059115588665008545, 'validation/mean_average_precision': 0.23044694181131412, 'validation/num_examples': 43793, 'test/accuracy': 0.9850820899009705, 'test/loss': 0.06356512010097504, 'test/mean_average_precision': 0.22224576390463255, 'test/num_examples': 43793, 'score': 8892.249380111694, 'total_duration': 11874.559579610825, 'accumulated_submission_time': 8892.249380111694, 'accumulated_eval_time': 2980.1901791095734, 'accumulated_logging_time': 0.9772748947143555, 'global_step': 42553, 'preemption_count': 0}), (43697, {'train/accuracy': 0.9974721670150757, 'train/loss': 0.010402362793684006, 'train/mean_average_precision': 0.8359061808925774, 'validation/accuracy': 0.9860473871231079, 'validation/loss': 0.06011923775076866, 'validation/mean_average_precision': 0.23138874299048715, 'validation/num_examples': 43793, 'test/accuracy': 0.985093891620636, 'test/loss': 0.0643911063671112, 'test/mean_average_precision': 0.22238335071195395, 'test/num_examples': 43793, 'score': 9132.370262622833, 'total_duration': 12191.148692131042, 'accumulated_submission_time': 9132.370262622833, 'accumulated_eval_time': 3056.6039323806763, 'accumulated_logging_time': 1.002333641052246, 'global_step': 43697, 'preemption_count': 0}), (44839, {'train/accuracy': 0.9960214495658875, 'train/loss': 0.012837124988436699, 'train/mean_average_precision': 0.807660260154525, 'validation/accuracy': 0.9859750866889954, 'validation/loss': 0.060532186180353165, 'validation/mean_average_precision': 0.22518951313756239, 'validation/num_examples': 43793, 'test/accuracy': 0.9851128458976746, 'test/loss': 0.06476683169603348, 'test/mean_average_precision': 0.22001704352598167, 'test/num_examples': 43793, 'score': 9372.344907522202, 'total_duration': 12505.787116289139, 'accumulated_submission_time': 9372.344907522202, 'accumulated_eval_time': 3131.215449333191, 'accumulated_logging_time': 1.02547287940979, 'global_step': 44839, 'preemption_count': 0}), (45982, {'train/accuracy': 0.9971339106559753, 'train/loss': 0.010748808272182941, 'train/mean_average_precision': 0.8484449479518312, 'validation/accuracy': 0.9860027432441711, 'validation/loss': 0.06084206700325012, 'validation/mean_average_precision': 0.22730050509141708, 'validation/num_examples': 43793, 'test/accuracy': 0.9850223064422607, 'test/loss': 0.06530077755451202, 'test/mean_average_precision': 0.21729932089808773, 'test/num_examples': 43793, 'score': 9612.418736934662, 'total_duration': 12819.075697898865, 'accumulated_submission_time': 9612.418736934662, 'accumulated_eval_time': 3204.3752167224884, 'accumulated_logging_time': 1.049227237701416, 'global_step': 45982, 'preemption_count': 0}), (47123, {'train/accuracy': 0.9955915808677673, 'train/loss': 0.013439957983791828, 'train/mean_average_precision': 0.7982546224283423, 'validation/accuracy': 0.9859146475791931, 'validation/loss': 0.06146799400448799, 'validation/mean_average_precision': 0.22559409645419728, 'validation/num_examples': 43793, 'test/accuracy': 0.9849928021430969, 'test/loss': 0.06581880897283554, 'test/mean_average_precision': 0.21860881178712743, 'test/num_examples': 43793, 'score': 9852.579642295837, 'total_duration': 13135.108164548874, 'accumulated_submission_time': 9852.579642295837, 'accumulated_eval_time': 3280.188529729843, 'accumulated_logging_time': 1.07326340675354, 'global_step': 47123, 'preemption_count': 0}), (48263, {'train/accuracy': 0.9966636300086975, 'train/loss': 0.01138642430305481, 'train/mean_average_precision': 0.8308602084599331, 'validation/accuracy': 0.9859268069267273, 'validation/loss': 0.061555761843919754, 'validation/mean_average_precision': 0.22400767463290686, 'validation/num_examples': 43793, 'test/accuracy': 0.9850496649742126, 'test/loss': 0.06599121540784836, 'test/mean_average_precision': 0.2167627788501794, 'test/num_examples': 43793, 'score': 10092.707715034485, 'total_duration': 13449.974822282791, 'accumulated_submission_time': 10092.707715034485, 'accumulated_eval_time': 3354.871862411499, 'accumulated_logging_time': 1.0978589057922363, 'global_step': 48263, 'preemption_count': 0}), (49403, {'train/accuracy': 0.9980002641677856, 'train/loss': 0.009391127154231071, 'train/mean_average_precision': 0.8595822128628009, 'validation/accuracy': 0.9859235882759094, 'validation/loss': 0.06142592802643776, 'validation/mean_average_precision': 0.2256212279317991, 'validation/num_examples': 43793, 'test/accuracy': 0.9849632978439331, 'test/loss': 0.06578873842954636, 'test/mean_average_precision': 0.21828367403432294, 'test/num_examples': 43793, 'score': 10332.806250333786, 'total_duration': 13768.079024553299, 'accumulated_submission_time': 10332.806250333786, 'accumulated_eval_time': 3432.8237850666046, 'accumulated_logging_time': 1.1231625080108643, 'global_step': 49403, 'preemption_count': 0}), (50553, {'train/accuracy': 0.9966402649879456, 'train/loss': 0.011622194200754166, 'train/mean_average_precision': 0.8261209586689353, 'validation/accuracy': 0.9858484864234924, 'validation/loss': 0.06109374761581421, 'validation/mean_average_precision': 0.22330623289516918, 'validation/num_examples': 43793, 'test/accuracy': 0.9849397540092468, 'test/loss': 0.06542015075683594, 'test/mean_average_precision': 0.21834268129552697, 'test/num_examples': 43793, 'score': 10572.889156579971, 'total_duration': 14081.948567152023, 'accumulated_submission_time': 10572.889156579971, 'accumulated_eval_time': 3506.5547783374786, 'accumulated_logging_time': 1.147759199142456, 'global_step': 50553, 'preemption_count': 0}), (51689, {'train/accuracy': 0.9978092908859253, 'train/loss': 0.009726175107061863, 'train/mean_average_precision': 0.8564151721030591, 'validation/accuracy': 0.9859117865562439, 'validation/loss': 0.06114786118268967, 'validation/mean_average_precision': 0.22473642219369588, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.0655323714017868, 'test/mean_average_precision': 0.21932679277631054, 'test/num_examples': 43793, 'score': 10812.849580049515, 'total_duration': 14396.834939956665, 'accumulated_submission_time': 10812.849580049515, 'accumulated_eval_time': 3581.421598672867, 'accumulated_logging_time': 1.1749012470245361, 'global_step': 51689, 'preemption_count': 0}), (52828, {'train/accuracy': 0.9976444244384766, 'train/loss': 0.009950489737093449, 'train/mean_average_precision': 0.8626955169129665, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.061140526086091995, 'validation/mean_average_precision': 0.2245727177855169, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265673995018, 'test/mean_average_precision': 0.2196943871380077, 'test/num_examples': 43793, 'score': 11052.803470373154, 'total_duration': 14715.394503831863, 'accumulated_submission_time': 11052.803470373154, 'accumulated_eval_time': 3659.969368457794, 'accumulated_logging_time': 1.2009425163269043, 'global_step': 52828, 'preemption_count': 0}), (53961, {'train/accuracy': 0.9971904158592224, 'train/loss': 0.010524259880185127, 'train/mean_average_precision': 0.8435058116368522, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.0611405223608017, 'validation/mean_average_precision': 0.22451027018643, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265748500824, 'test/mean_average_precision': 0.21958940289531087, 'test/num_examples': 43793, 'score': 11292.784072637558, 'total_duration': 15026.516201496124, 'accumulated_submission_time': 11292.784072637558, 'accumulated_eval_time': 3731.056984901428, 'accumulated_logging_time': 1.2245934009552002, 'global_step': 53961, 'preemption_count': 0}), (55106, {'train/accuracy': 0.997311532497406, 'train/loss': 0.0104548130184412, 'train/mean_average_precision': 0.847944794044512, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.0611405149102211, 'validation/mean_average_precision': 0.2244286208671341, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265673995018, 'test/mean_average_precision': 0.21954968550268894, 'test/num_examples': 43793, 'score': 11532.855032682419, 'total_duration': 15342.894850969315, 'accumulated_submission_time': 11532.855032682419, 'accumulated_eval_time': 3807.308841228485, 'accumulated_logging_time': 1.251119613647461, 'global_step': 55106, 'preemption_count': 0}), (56235, {'train/accuracy': 0.9974057078361511, 'train/loss': 0.010266638360917568, 'train/mean_average_precision': 0.8402096272803246, 'validation/accuracy': 0.9859198927879333, 'validation/loss': 0.0611405223608017, 'validation/mean_average_precision': 0.2244589653244849, 'validation/num_examples': 43793, 'test/accuracy': 0.9849793314933777, 'test/loss': 0.0655265748500824, 'test/mean_average_precision': 0.21951629967269648, 'test/num_examples': 43793, 'score': 11772.915124177933, 'total_duration': 15659.423861026764, 'accumulated_submission_time': 11772.915124177933, 'accumulated_eval_time': 3883.7224667072296, 'accumulated_logging_time': 1.2764782905578613, 'global_step': 56235, 'preemption_count': 0})], 'global_step': 57381}
I0305 23:38:25.029392 139874152301760 submission_runner.py:649] Timing: 12013.060721158981
I0305 23:38:25.029430 139874152301760 submission_runner.py:651] Total number of evals: 50
I0305 23:38:25.029462 139874152301760 submission_runner.py:652] ====================
I0305 23:38:25.029837 139874152301760 submission_runner.py:750] Final ogbg score: 1
