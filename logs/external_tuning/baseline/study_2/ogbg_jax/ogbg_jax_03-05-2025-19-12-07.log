python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_2 --overwrite=True --save_checkpoints=False --rng_seed=605492072 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-12-07.log
2025-03-05 19:12:08.328051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201928.350447       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201928.357343       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:14.690288 140323414672576 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax.
I0305 19:12:15.702146 140323414672576 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:15.705634 140323414672576 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:15.707353 140323414672576 submission_runner.py:606] Using RNG seed 605492072
I0305 19:12:16.262887 140323414672576 submission_runner.py:615] --- Tuning run 3/5 ---
I0305 19:12:16.263064 140323414672576 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_3.
I0305 19:12:16.263244 140323414672576 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_3/hparams.json.
I0305 19:12:16.486728 140323414672576 submission_runner.py:218] Initializing dataset.
I0305 19:12:16.740180 140323414672576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:16.814642 140323414672576 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:12:17.041311 140323414672576 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:12:17.091341 140323414672576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:17.119452 140323414672576 submission_runner.py:229] Initializing model.
I0305 19:12:24.776951 140323414672576 submission_runner.py:272] Initializing optimizer.
I0305 19:12:25.193976 140323414672576 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:25.194198 140323414672576 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:25.194997 140323414672576 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_3 with prefix checkpoint_
I0305 19:12:25.195108 140323414672576 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_3/meta_data_0.json.
I0305 19:12:25.195272 140323414672576 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:25.195323 140323414672576 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:25.350290 140323414672576 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_2/ogbg_jax/trial_3/flags_0.json.
I0305 19:12:25.382937 140323414672576 submission_runner.py:337] Starting training loop.
I0305 19:12:36.265406 140187235116800 logging_writer.py:48] [0] global_step=0, grad_norm=3.1380105018615723, loss=0.7763809561729431
I0305 19:12:36.316246 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:12:36.319956 140323414672576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:36.323786 140323414672576 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:12:36.383592 140323414672576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:52.553667 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:13:52.556351 140323414672576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:52.560169 140323414672576 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:52.620348 140323414672576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:56.713186 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:14:56.715761 140323414672576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:56.719360 140323414672576 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:56.779645 140323414672576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:00.105605 140323414672576 submission_runner.py:469] Time since start: 214.72s, 	Step: 1, 	{'train/accuracy': 0.466335266828537, 'train/loss': 0.7765662670135498, 'train/mean_average_precision': 0.024501512820790904, 'validation/accuracy': 0.4627687335014343, 'validation/loss': 0.7771925926208496, 'validation/mean_average_precision': 0.027871177223361375, 'validation/num_examples': 43793, 'test/accuracy': 0.4637924134731293, 'test/loss': 0.7764967679977417, 'test/mean_average_precision': 0.02989974297768301, 'test/num_examples': 43793, 'score': 10.93319296836853, 'total_duration': 214.72251796722412, 'accumulated_submission_time': 10.93319296836853, 'accumulated_eval_time': 203.7892165184021, 'accumulated_logging_time': 0}
I0305 19:16:00.112493 140181337741056 logging_writer.py:48] [1] accumulated_eval_time=203.789, accumulated_logging_time=0, accumulated_submission_time=10.9332, global_step=1, preemption_count=0, score=10.9332, test/accuracy=0.463792, test/loss=0.776497, test/mean_average_precision=0.0298997, test/num_examples=43793, total_duration=214.723, train/accuracy=0.466335, train/loss=0.776566, train/mean_average_precision=0.0245015, validation/accuracy=0.462769, validation/loss=0.777193, validation/mean_average_precision=0.0278712, validation/num_examples=43793
I0305 19:16:20.851408 140181346133760 logging_writer.py:48] [100] global_step=100, grad_norm=0.5334748029708862, loss=0.43826761841773987
I0305 19:16:41.646982 140181337741056 logging_writer.py:48] [200] global_step=200, grad_norm=0.35068830847740173, loss=0.30447378754615784
I0305 19:17:01.983597 140181346133760 logging_writer.py:48] [300] global_step=300, grad_norm=0.21747735142707825, loss=0.19654133915901184
I0305 19:17:23.048037 140181337741056 logging_writer.py:48] [400] global_step=400, grad_norm=0.12228982150554657, loss=0.12095267325639725
I0305 19:17:43.887500 140181346133760 logging_writer.py:48] [500] global_step=500, grad_norm=0.06703991442918777, loss=0.08611123263835907
I0305 19:18:05.152067 140181337741056 logging_writer.py:48] [600] global_step=600, grad_norm=0.03949841856956482, loss=0.06882278621196747
I0305 19:18:26.056082 140182120523520 logging_writer.py:48] [700] global_step=700, grad_norm=0.029229572042822838, loss=0.0620642714202404
I0305 19:18:46.870625 140182112130816 logging_writer.py:48] [800] global_step=800, grad_norm=0.05666498839855194, loss=0.06297816336154938
I0305 19:19:07.644175 140182120523520 logging_writer.py:48] [900] global_step=900, grad_norm=0.32043197751045227, loss=0.05914629250764847
I0305 19:19:28.993123 140182112130816 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.14584806561470032, loss=0.05470704659819603
I0305 19:19:49.533508 140182120523520 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.2262255847454071, loss=0.04630908742547035
I0305 19:20:00.110226 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:21:10.230768 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:21:12.326860 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:21:14.352557 140323414672576 submission_runner.py:469] Time since start: 528.97s, 	Step: 1153, 	{'train/accuracy': 0.9870119690895081, 'train/loss': 0.05065571144223213, 'train/mean_average_precision': 0.06195725116692419, 'validation/accuracy': 0.984386682510376, 'validation/loss': 0.05967370420694351, 'validation/mean_average_precision': 0.06065736560898678, 'validation/num_examples': 43793, 'test/accuracy': 0.9834019541740417, 'test/loss': 0.062791608273983, 'test/mean_average_precision': 0.06295777856325767, 'test/num_examples': 43793, 'score': 250.89254188537598, 'total_duration': 528.9694299697876, 'accumulated_submission_time': 250.89254188537598, 'accumulated_eval_time': 278.03136944770813, 'accumulated_logging_time': 0.016626358032226562}
I0305 19:21:14.361647 140182112130816 logging_writer.py:48] [1153] accumulated_eval_time=278.031, accumulated_logging_time=0.0166264, accumulated_submission_time=250.893, global_step=1153, preemption_count=0, score=250.893, test/accuracy=0.983402, test/loss=0.0627916, test/mean_average_precision=0.0629578, test/num_examples=43793, total_duration=528.969, train/accuracy=0.987012, train/loss=0.0506557, train/mean_average_precision=0.0619573, validation/accuracy=0.984387, validation/loss=0.0596737, validation/mean_average_precision=0.0606574, validation/num_examples=43793
I0305 19:21:24.148145 140182120523520 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.12977665662765503, loss=0.05153520032763481
I0305 19:21:44.977855 140182112130816 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.11934155225753784, loss=0.04274039715528488
I0305 19:22:05.522746 140182120523520 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.09691674262285233, loss=0.04846619442105293
I0305 19:22:26.158015 140182112130816 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04505914822220802, loss=0.0454912930727005
I0305 19:22:46.790017 140182120523520 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.17596592009067535, loss=0.05442548170685768
I0305 19:23:07.613212 140182112130816 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.21057945489883423, loss=0.05110885947942734
I0305 19:23:28.606216 140182120523520 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.21437911689281464, loss=0.0416841097176075
I0305 19:23:49.461246 140182112130816 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.11109674721956253, loss=0.04422731697559357
I0305 19:24:10.561898 140182120523520 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.06946706026792526, loss=0.04261742904782295
I0305 19:24:31.410608 140182112130816 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.10118865966796875, loss=0.045795783400535583
I0305 19:24:52.317089 140182120523520 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.08008864521980286, loss=0.04514739662408829
I0305 19:25:13.328816 140182112130816 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.05282939225435257, loss=0.0402514673769474
I0305 19:25:14.389884 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:26:25.719469 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:26:27.828810 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:26:29.951029 140323414672576 submission_runner.py:469] Time since start: 844.57s, 	Step: 2306, 	{'train/accuracy': 0.9877615571022034, 'train/loss': 0.04390899837017059, 'train/mean_average_precision': 0.13969970405515886, 'validation/accuracy': 0.9848441481590271, 'validation/loss': 0.05365576595067978, 'validation/mean_average_precision': 0.14120361715364035, 'validation/num_examples': 43793, 'test/accuracy': 0.9839457273483276, 'test/loss': 0.056595172733068466, 'test/mean_average_precision': 0.1427101074687839, 'test/num_examples': 43793, 'score': 490.8798429965973, 'total_duration': 844.5680403709412, 'accumulated_submission_time': 490.8798429965973, 'accumulated_eval_time': 353.5924632549286, 'accumulated_logging_time': 0.035361289978027344}
I0305 19:26:29.960232 140182120523520 logging_writer.py:48] [2306] accumulated_eval_time=353.592, accumulated_logging_time=0.0353613, accumulated_submission_time=490.88, global_step=2306, preemption_count=0, score=490.88, test/accuracy=0.983946, test/loss=0.0565952, test/mean_average_precision=0.14271, test/num_examples=43793, total_duration=844.568, train/accuracy=0.987762, train/loss=0.043909, train/mean_average_precision=0.1397, validation/accuracy=0.984844, validation/loss=0.0536558, validation/mean_average_precision=0.141204, validation/num_examples=43793
I0305 19:26:49.716639 140182112130816 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.048326876014471054, loss=0.04435759410262108
I0305 19:27:10.573167 140182120523520 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.07929795235395432, loss=0.039934709668159485
I0305 19:27:31.594634 140182112130816 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.11594206094741821, loss=0.042025789618492126
I0305 19:27:52.502532 140182120523520 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.12137645483016968, loss=0.03939130902290344
I0305 19:28:13.268872 140182112130816 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.11386578530073166, loss=0.040848419070243835
I0305 19:28:34.326921 140182120523520 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.05188192427158356, loss=0.04099350422620773
I0305 19:28:55.372108 140182112130816 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.07869800180196762, loss=0.04565318301320076
I0305 19:29:16.025254 140182120523520 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.058228589594364166, loss=0.04236894100904465
I0305 19:29:36.941709 140182112130816 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.050520189106464386, loss=0.0417652502655983
I0305 19:29:57.983374 140182120523520 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.05468563362956047, loss=0.04069865122437477
I0305 19:30:18.954528 140182112130816 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.06691454350948334, loss=0.03644648566842079
I0305 19:30:30.063396 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:31:42.257336 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:31:44.216500 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:31:46.090984 140323414672576 submission_runner.py:469] Time since start: 1160.71s, 	Step: 3455, 	{'train/accuracy': 0.9884949922561646, 'train/loss': 0.040114063769578934, 'train/mean_average_precision': 0.19413902669517072, 'validation/accuracy': 0.9854822754859924, 'validation/loss': 0.049768220633268356, 'validation/mean_average_precision': 0.1734602633881552, 'validation/num_examples': 43793, 'test/accuracy': 0.9846010804176331, 'test/loss': 0.0524454228579998, 'test/mean_average_precision': 0.1725589042006732, 'test/num_examples': 43793, 'score': 730.9433691501617, 'total_duration': 1160.7079865932465, 'accumulated_submission_time': 730.9433691501617, 'accumulated_eval_time': 429.6199929714203, 'accumulated_logging_time': 0.05483841896057129}
I0305 19:31:46.099865 140182120523520 logging_writer.py:48] [3455] accumulated_eval_time=429.62, accumulated_logging_time=0.0548384, accumulated_submission_time=730.943, global_step=3455, preemption_count=0, score=730.943, test/accuracy=0.984601, test/loss=0.0524454, test/mean_average_precision=0.172559, test/num_examples=43793, total_duration=1160.71, train/accuracy=0.988495, train/loss=0.0401141, train/mean_average_precision=0.194139, validation/accuracy=0.985482, validation/loss=0.0497682, validation/mean_average_precision=0.17346, validation/num_examples=43793
I0305 19:31:55.827260 140182112130816 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.03652465343475342, loss=0.042909204959869385
I0305 19:32:16.628303 140182120523520 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.11530105769634247, loss=0.041826654225587845
I0305 19:32:37.421649 140182112130816 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.08121750503778458, loss=0.03777886554598808
I0305 19:32:58.394062 140182120523520 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.06752458214759827, loss=0.03569373860955238
I0305 19:33:19.721218 140182112130816 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.07890619337558746, loss=0.04511350020766258
I0305 19:33:40.371380 140182120523520 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.09968128800392151, loss=0.046768564730882645
I0305 19:34:01.375133 140182112130816 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.06617274880409241, loss=0.04098111018538475
I0305 19:34:22.232110 140182120523520 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.06388857960700989, loss=0.038440193980932236
I0305 19:34:42.888416 140182112130816 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0806741937994957, loss=0.04419955611228943
I0305 19:35:03.620503 140182120523520 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.09376567602157593, loss=0.044771209359169006
I0305 19:35:24.251579 140182112130816 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.05123021453619003, loss=0.03847673535346985
I0305 19:35:44.657376 140182120523520 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0404425673186779, loss=0.03790127858519554
I0305 19:35:46.270269 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:36:58.363821 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:37:00.279652 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:37:02.139958 140323414672576 submission_runner.py:469] Time since start: 1476.76s, 	Step: 4609, 	{'train/accuracy': 0.9884700775146484, 'train/loss': 0.0396701879799366, 'train/mean_average_precision': 0.21797261006167074, 'validation/accuracy': 0.9855736494064331, 'validation/loss': 0.048748794943094254, 'validation/mean_average_precision': 0.1861019223225609, 'validation/num_examples': 43793, 'test/accuracy': 0.9846512079238892, 'test/loss': 0.0512719489634037, 'test/mean_average_precision': 0.18644269774130875, 'test/num_examples': 43793, 'score': 971.0745029449463, 'total_duration': 1476.7569706439972, 'accumulated_submission_time': 971.0745029449463, 'accumulated_eval_time': 505.4896352291107, 'accumulated_logging_time': 0.07304954528808594}
I0305 19:37:02.149028 140181729392384 logging_writer.py:48] [4609] accumulated_eval_time=505.49, accumulated_logging_time=0.0730495, accumulated_submission_time=971.075, global_step=4609, preemption_count=0, score=971.075, test/accuracy=0.984651, test/loss=0.0512719, test/mean_average_precision=0.186443, test/num_examples=43793, total_duration=1476.76, train/accuracy=0.98847, train/loss=0.0396702, train/mean_average_precision=0.217973, validation/accuracy=0.985574, validation/loss=0.0487488, validation/mean_average_precision=0.186102, validation/num_examples=43793
I0305 19:37:20.745918 140181720999680 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.07100874930620193, loss=0.03591105341911316
I0305 19:37:41.192796 140181729392384 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04485078901052475, loss=0.03534424677491188
I0305 19:38:01.735570 140181720999680 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.035339392721652985, loss=0.03951563686132431
I0305 19:38:22.337058 140181729392384 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.04932758957147598, loss=0.037266816943883896
I0305 19:38:42.990575 140181720999680 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.07239445298910141, loss=0.040182359516620636
I0305 19:39:03.301770 140181729392384 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.038380514830350876, loss=0.04141872376203537
I0305 19:39:23.024984 140181720999680 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.03899897634983063, loss=0.03697843849658966
I0305 19:39:42.811010 140181729392384 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.03118681162595749, loss=0.03925027325749397
I0305 19:40:02.641546 140181720999680 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.07360716164112091, loss=0.04298621416091919
I0305 19:40:22.554717 140181729392384 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.034568071365356445, loss=0.039059825241565704
I0305 19:40:43.084884 140181720999680 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.08690709620714188, loss=0.03953322395682335
I0305 19:41:02.301346 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:42:12.696385 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:42:14.671094 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:42:16.532514 140323414672576 submission_runner.py:469] Time since start: 1791.15s, 	Step: 5794, 	{'train/accuracy': 0.9890326261520386, 'train/loss': 0.037212952971458435, 'train/mean_average_precision': 0.24388577357169772, 'validation/accuracy': 0.9859738945960999, 'validation/loss': 0.0472651869058609, 'validation/mean_average_precision': 0.20724059249750948, 'validation/num_examples': 43793, 'test/accuracy': 0.9850492477416992, 'test/loss': 0.05001422017812729, 'test/mean_average_precision': 0.20401994511518742, 'test/num_examples': 43793, 'score': 1211.1871082782745, 'total_duration': 1791.1494870185852, 'accumulated_submission_time': 1211.1871082782745, 'accumulated_eval_time': 579.7207245826721, 'accumulated_logging_time': 0.09314227104187012}
I0305 19:42:16.542116 140181729392384 logging_writer.py:48] [5794] accumulated_eval_time=579.721, accumulated_logging_time=0.0931423, accumulated_submission_time=1211.19, global_step=5794, preemption_count=0, score=1211.19, test/accuracy=0.985049, test/loss=0.0500142, test/mean_average_precision=0.20402, test/num_examples=43793, total_duration=1791.15, train/accuracy=0.989033, train/loss=0.037213, train/mean_average_precision=0.243886, validation/accuracy=0.985974, validation/loss=0.0472652, validation/mean_average_precision=0.207241, validation/num_examples=43793
I0305 19:42:17.991440 140181720999680 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.026339540258049965, loss=0.037611350417137146
I0305 19:42:38.580135 140181729392384 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.03293217346072197, loss=0.03929785266518593
I0305 19:42:59.006974 140181720999680 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.031791381537914276, loss=0.03661134093999863
I0305 19:43:19.426902 140181729392384 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.02184315025806427, loss=0.03922315686941147
I0305 19:43:40.018124 140181720999680 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.03913978859782219, loss=0.03929993882775307
I0305 19:44:00.298591 140181729392384 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.030519962310791016, loss=0.036265552043914795
I0305 19:44:20.722210 140181720999680 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.04665561765432358, loss=0.03989550471305847
I0305 19:44:41.110140 140181729392384 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.034629229456186295, loss=0.03768017515540123
I0305 19:45:01.441966 140181720999680 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.03740176185965538, loss=0.03873840719461441
I0305 19:45:21.486392 140181729392384 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.03731873631477356, loss=0.03580924868583679
I0305 19:45:42.052883 140181720999680 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.026425274088978767, loss=0.03720055893063545
I0305 19:46:02.577927 140181729392384 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.031231360509991646, loss=0.03525394946336746
I0305 19:46:16.610026 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:47:27.390873 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:47:29.335224 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:47:31.238169 140323414672576 submission_runner.py:469] Time since start: 2105.86s, 	Step: 6969, 	{'train/accuracy': 0.9891321063041687, 'train/loss': 0.03671769052743912, 'train/mean_average_precision': 0.2629625421933687, 'validation/accuracy': 0.9861281514167786, 'validation/loss': 0.04645803943276405, 'validation/mean_average_precision': 0.22041973268118908, 'validation/num_examples': 43793, 'test/accuracy': 0.9852808713912964, 'test/loss': 0.04892151057720184, 'test/mean_average_precision': 0.22360193969464554, 'test/num_examples': 43793, 'score': 1451.2160964012146, 'total_duration': 2105.8550441265106, 'accumulated_submission_time': 1451.2160964012146, 'accumulated_eval_time': 654.3486821651459, 'accumulated_logging_time': 0.11270856857299805}
I0305 19:47:31.247441 140181720999680 logging_writer.py:48] [6969] accumulated_eval_time=654.349, accumulated_logging_time=0.112709, accumulated_submission_time=1451.22, global_step=6969, preemption_count=0, score=1451.22, test/accuracy=0.985281, test/loss=0.0489215, test/mean_average_precision=0.223602, test/num_examples=43793, total_duration=2105.86, train/accuracy=0.989132, train/loss=0.0367177, train/mean_average_precision=0.262963, validation/accuracy=0.986128, validation/loss=0.046458, validation/mean_average_precision=0.22042, validation/num_examples=43793
I0305 19:47:37.740746 140181729392384 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.02899181842803955, loss=0.04086262732744217
I0305 19:47:58.313507 140181720999680 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.02772386744618416, loss=0.03963868319988251
I0305 19:48:18.650594 140181729392384 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.04357143118977547, loss=0.03676721826195717
I0305 19:48:39.099849 140181720999680 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03491170331835747, loss=0.040135517716407776
I0305 19:48:59.776357 140181729392384 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.03035445138812065, loss=0.03547195345163345
I0305 19:49:20.203196 140181720999680 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.02725512534379959, loss=0.040365077555179596
I0305 19:49:40.416266 140181729392384 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.030683893710374832, loss=0.03696557506918907
I0305 19:50:01.081079 140181720999680 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.04323531687259674, loss=0.03721216320991516
I0305 19:50:21.839653 140181729392384 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.050220947712659836, loss=0.03607563301920891
I0305 19:50:42.306475 140181720999680 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.03395391255617142, loss=0.033692024648189545
I0305 19:51:02.620876 140181729392384 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03856207802891731, loss=0.031977102160453796
I0305 19:51:23.208385 140181720999680 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.029198354110121727, loss=0.03650050237774849
I0305 19:51:31.434472 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:52:42.130856 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:52:44.069998 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:52:45.945201 140323414672576 submission_runner.py:469] Time since start: 2420.56s, 	Step: 8141, 	{'train/accuracy': 0.9894486665725708, 'train/loss': 0.035610850900411606, 'train/mean_average_precision': 0.28654313611546, 'validation/accuracy': 0.9859938025474548, 'validation/loss': 0.0464618057012558, 'validation/mean_average_precision': 0.22155698046841207, 'validation/num_examples': 43793, 'test/accuracy': 0.9852935075759888, 'test/loss': 0.048900753259658813, 'test/mean_average_precision': 0.22726390799865198, 'test/num_examples': 43793, 'score': 1691.3623287677765, 'total_duration': 2420.5620770454407, 'accumulated_submission_time': 1691.3623287677765, 'accumulated_eval_time': 728.8592274188995, 'accumulated_logging_time': 0.13066744804382324}
I0305 19:52:45.955264 140181729392384 logging_writer.py:48] [8141] accumulated_eval_time=728.859, accumulated_logging_time=0.130667, accumulated_submission_time=1691.36, global_step=8141, preemption_count=0, score=1691.36, test/accuracy=0.985294, test/loss=0.0489008, test/mean_average_precision=0.227264, test/num_examples=43793, total_duration=2420.56, train/accuracy=0.989449, train/loss=0.0356109, train/mean_average_precision=0.286543, validation/accuracy=0.985994, validation/loss=0.0464618, validation/mean_average_precision=0.221557, validation/num_examples=43793
I0305 19:52:58.320047 140181720999680 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.0351903922855854, loss=0.03254029527306557
I0305 19:53:18.897045 140181729392384 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.028816677629947662, loss=0.0357823483645916
I0305 19:53:39.374598 140181720999680 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.027340054512023926, loss=0.03431979939341545
I0305 19:53:59.706619 140181729392384 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.025884300470352173, loss=0.035223040729761124
I0305 19:54:20.299354 140181720999680 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03699834644794464, loss=0.04051388055086136
I0305 19:54:40.675056 140181729392384 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.041826777160167694, loss=0.03837958350777626
I0305 19:55:01.144083 140181720999680 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.06060086190700531, loss=0.03633884713053703
I0305 19:55:21.689031 140181729392384 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.05298915505409241, loss=0.03714987263083458
I0305 19:55:42.500901 140181720999680 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03810976818203926, loss=0.030465926975011826
I0305 19:56:03.163548 140181729392384 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.03312813863158226, loss=0.036355312913656235
I0305 19:56:23.801895 140181720999680 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.03429684415459633, loss=0.03564590960741043
I0305 19:56:43.868863 140181729392384 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.026119207963347435, loss=0.035912394523620605
I0305 19:56:46.052131 140323414672576 spec.py:321] Evaluating on the training split.
I0305 19:57:56.888074 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 19:57:58.816927 140323414672576 spec.py:349] Evaluating on the test split.
I0305 19:58:00.678234 140323414672576 submission_runner.py:469] Time since start: 2735.30s, 	Step: 9312, 	{'train/accuracy': 0.989785373210907, 'train/loss': 0.03430156037211418, 'train/mean_average_precision': 0.3101843536906001, 'validation/accuracy': 0.9863859415054321, 'validation/loss': 0.04521529749035835, 'validation/mean_average_precision': 0.2363399842471766, 'validation/num_examples': 43793, 'test/accuracy': 0.9855361580848694, 'test/loss': 0.04785566404461861, 'test/mean_average_precision': 0.2399642554209753, 'test/num_examples': 43793, 'score': 1931.4180495738983, 'total_duration': 2735.2951188087463, 'accumulated_submission_time': 1931.4180495738983, 'accumulated_eval_time': 803.4851558208466, 'accumulated_logging_time': 0.15070128440856934}
I0305 19:58:00.688339 140181720999680 logging_writer.py:48] [9312] accumulated_eval_time=803.485, accumulated_logging_time=0.150701, accumulated_submission_time=1931.42, global_step=9312, preemption_count=0, score=1931.42, test/accuracy=0.985536, test/loss=0.0478557, test/mean_average_precision=0.239964, test/num_examples=43793, total_duration=2735.3, train/accuracy=0.989785, train/loss=0.0343016, train/mean_average_precision=0.310184, validation/accuracy=0.986386, validation/loss=0.0452153, validation/mean_average_precision=0.23634, validation/num_examples=43793
I0305 19:58:18.904272 140181729392384 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03193080052733421, loss=0.03527558594942093
I0305 19:58:39.462935 140181720999680 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.03264201804995537, loss=0.03555135801434517
I0305 19:58:59.943768 140181729392384 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0638418197631836, loss=0.03420021012425423
I0305 19:59:20.401962 140181720999680 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.036958858370780945, loss=0.0328250490128994
I0305 19:59:41.262220 140181729392384 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.032105546444654465, loss=0.0339810848236084
I0305 20:00:01.933761 140181720999680 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03963841125369072, loss=0.03501836210489273
I0305 20:00:21.868692 140181729392384 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.04151596128940582, loss=0.03281538188457489
I0305 20:00:41.764285 140181720999680 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.030396487563848495, loss=0.03061438724398613
I0305 20:01:01.760388 140181729392384 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.044024862349033356, loss=0.03377246484160423
I0305 20:01:22.229264 140181720999680 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.033808138221502304, loss=0.03266296535730362
I0305 20:01:42.750403 140181729392384 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.058774203062057495, loss=0.036801815032958984
I0305 20:02:00.870898 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:03:12.495049 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:03:14.421095 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:03:16.310340 140323414672576 submission_runner.py:469] Time since start: 3050.93s, 	Step: 10488, 	{'train/accuracy': 0.9901410937309265, 'train/loss': 0.033130716532468796, 'train/mean_average_precision': 0.3284355740962277, 'validation/accuracy': 0.986511766910553, 'validation/loss': 0.044880785048007965, 'validation/mean_average_precision': 0.2456731069626298, 'validation/num_examples': 43793, 'test/accuracy': 0.9856662750244141, 'test/loss': 0.04759758338332176, 'test/mean_average_precision': 0.24573916602012844, 'test/num_examples': 43793, 'score': 2171.5629694461823, 'total_duration': 3050.9273416996, 'accumulated_submission_time': 2171.5629694461823, 'accumulated_eval_time': 878.9245419502258, 'accumulated_logging_time': 0.1699838638305664}
I0305 20:03:16.320613 140181720999680 logging_writer.py:48] [10488] accumulated_eval_time=878.925, accumulated_logging_time=0.169984, accumulated_submission_time=2171.56, global_step=10488, preemption_count=0, score=2171.56, test/accuracy=0.985666, test/loss=0.0475976, test/mean_average_precision=0.245739, test/num_examples=43793, total_duration=3050.93, train/accuracy=0.990141, train/loss=0.0331307, train/mean_average_precision=0.328436, validation/accuracy=0.986512, validation/loss=0.0448808, validation/mean_average_precision=0.245673, validation/num_examples=43793
I0305 20:03:19.028895 140181729392384 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.03309899568557739, loss=0.03442710265517235
I0305 20:03:39.580030 140181720999680 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.031000325456261635, loss=0.03377707302570343
I0305 20:04:00.539416 140181729392384 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.031331997364759445, loss=0.03319019824266434
I0305 20:04:21.270920 140181720999680 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.03832588717341423, loss=0.030832238495349884
I0305 20:04:41.792211 140181729392384 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.040348052978515625, loss=0.03406684845685959
I0305 20:05:02.199352 140181720999680 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03633984923362732, loss=0.03474365547299385
I0305 20:05:23.085576 140181729392384 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03693227469921112, loss=0.03356396034359932
I0305 20:05:43.790337 140181720999680 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.028990386053919792, loss=0.02948603220283985
I0305 20:06:04.241674 140181729392384 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.032559629529714584, loss=0.033403199166059494
I0305 20:06:24.723792 140181720999680 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.03013484925031662, loss=0.030440974980592728
I0305 20:06:45.160548 140181729392384 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03736186772584915, loss=0.032164592295885086
I0305 20:07:05.562549 140181720999680 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03947073966264725, loss=0.02993692085146904
I0305 20:07:16.515628 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:08:27.216108 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:08:29.106324 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:08:30.954995 140323414672576 submission_runner.py:469] Time since start: 3365.57s, 	Step: 11654, 	{'train/accuracy': 0.9901654124259949, 'train/loss': 0.032487694174051285, 'train/mean_average_precision': 0.35544792824294813, 'validation/accuracy': 0.986464262008667, 'validation/loss': 0.045079831033945084, 'validation/mean_average_precision': 0.24950404915177132, 'validation/num_examples': 43793, 'test/accuracy': 0.98563551902771, 'test/loss': 0.04782219976186752, 'test/mean_average_precision': 0.2430625370524352, 'test/num_examples': 43793, 'score': 2411.7179279327393, 'total_duration': 3365.5718653202057, 'accumulated_submission_time': 2411.7179279327393, 'accumulated_eval_time': 953.3637182712555, 'accumulated_logging_time': 0.19066262245178223}
I0305 20:08:30.964323 140181048514304 logging_writer.py:48] [11654] accumulated_eval_time=953.364, accumulated_logging_time=0.190663, accumulated_submission_time=2411.72, global_step=11654, preemption_count=0, score=2411.72, test/accuracy=0.985636, test/loss=0.0478222, test/mean_average_precision=0.243063, test/num_examples=43793, total_duration=3365.57, train/accuracy=0.990165, train/loss=0.0324877, train/mean_average_precision=0.355448, validation/accuracy=0.986464, validation/loss=0.0450798, validation/mean_average_precision=0.249504, validation/num_examples=43793
I0305 20:08:40.610129 140181040121600 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.050134360790252686, loss=0.03181818500161171
I0305 20:09:01.324930 140181048514304 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.06096610426902771, loss=0.03280584514141083
I0305 20:09:22.088672 140181040121600 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.05926532298326492, loss=0.03321515768766403
I0305 20:09:42.512793 140181048514304 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.05715488642454147, loss=0.03164393827319145
I0305 20:10:03.186786 140181040121600 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03931634873151779, loss=0.030821025371551514
I0305 20:10:23.759489 140181048514304 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.04553147032856941, loss=0.03172053024172783
I0305 20:10:44.640192 140181040121600 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.042811069637537, loss=0.03427210822701454
I0305 20:11:05.453805 140181048514304 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.05635108798742294, loss=0.03166801482439041
I0305 20:11:26.296608 140181040121600 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.037236206233501434, loss=0.025503084063529968
I0305 20:11:46.993215 140181048514304 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.050885822623968124, loss=0.0314229354262352
I0305 20:12:08.135217 140181040121600 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.054167307913303375, loss=0.029795322567224503
I0305 20:12:28.748894 140181048514304 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.05047735199332237, loss=0.03239969164133072
I0305 20:12:31.053197 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:13:43.406537 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:13:45.339400 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:13:47.415231 140323414672576 submission_runner.py:469] Time since start: 3682.03s, 	Step: 12812, 	{'train/accuracy': 0.9905313849449158, 'train/loss': 0.03124281018972397, 'train/mean_average_precision': 0.37896046556666024, 'validation/accuracy': 0.9866449236869812, 'validation/loss': 0.04479825496673584, 'validation/mean_average_precision': 0.25468768827299426, 'validation/num_examples': 43793, 'test/accuracy': 0.9858250617980957, 'test/loss': 0.04755214601755142, 'test/mean_average_precision': 0.2539833355432807, 'test/num_examples': 43793, 'score': 2651.768351793289, 'total_duration': 3682.032088279724, 'accumulated_submission_time': 2651.768351793289, 'accumulated_eval_time': 1029.725546836853, 'accumulated_logging_time': 0.20875144004821777}
I0305 20:13:47.426225 140181040121600 logging_writer.py:48] [12812] accumulated_eval_time=1029.73, accumulated_logging_time=0.208751, accumulated_submission_time=2651.77, global_step=12812, preemption_count=0, score=2651.77, test/accuracy=0.985825, test/loss=0.0475521, test/mean_average_precision=0.253983, test/num_examples=43793, total_duration=3682.03, train/accuracy=0.990531, train/loss=0.0312428, train/mean_average_precision=0.37896, validation/accuracy=0.986645, validation/loss=0.0447983, validation/mean_average_precision=0.254688, validation/num_examples=43793
I0305 20:14:05.672258 140181048514304 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.059766750782728195, loss=0.03278839960694313
I0305 20:14:26.445340 140181040121600 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.04668574035167694, loss=0.029437929391860962
I0305 20:14:47.426993 140181048514304 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.04500817134976387, loss=0.031279344111680984
I0305 20:15:08.165108 140181040121600 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.044457726180553436, loss=0.02763572707772255
I0305 20:15:28.868196 140181048514304 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.06997497379779816, loss=0.030040457844734192
I0305 20:15:49.151914 140181040121600 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.07614048570394516, loss=0.031905222684144974
I0305 20:16:09.765789 140181048514304 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.05406608060002327, loss=0.028650257736444473
I0305 20:16:30.262984 140181040121600 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.06359989941120148, loss=0.033379536122083664
I0305 20:16:50.636736 140181048514304 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.0708765983581543, loss=0.0313422754406929
I0305 20:17:11.138758 140181040121600 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.053364407271146774, loss=0.033594388514757156
I0305 20:17:31.478982 140181048514304 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.07371288537979126, loss=0.0306796133518219
I0305 20:17:47.495073 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:18:59.048120 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:19:00.936246 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:19:02.798140 140323414672576 submission_runner.py:469] Time since start: 3997.42s, 	Step: 13979, 	{'train/accuracy': 0.9903814792633057, 'train/loss': 0.031676072627305984, 'train/mean_average_precision': 0.3674400428025558, 'validation/accuracy': 0.9864833354949951, 'validation/loss': 0.04480401799082756, 'validation/mean_average_precision': 0.2539983989492452, 'validation/num_examples': 43793, 'test/accuracy': 0.9857164025306702, 'test/loss': 0.047417737543582916, 'test/mean_average_precision': 0.2506572946455104, 'test/num_examples': 43793, 'score': 2891.79754114151, 'total_duration': 3997.4150631427765, 'accumulated_submission_time': 2891.79754114151, 'accumulated_eval_time': 1105.0284843444824, 'accumulated_logging_time': 0.22975921630859375}
I0305 20:19:02.808048 140181040121600 logging_writer.py:48] [13979] accumulated_eval_time=1105.03, accumulated_logging_time=0.229759, accumulated_submission_time=2891.8, global_step=13979, preemption_count=0, score=2891.8, test/accuracy=0.985716, test/loss=0.0474177, test/mean_average_precision=0.250657, test/num_examples=43793, total_duration=3997.42, train/accuracy=0.990381, train/loss=0.0316761, train/mean_average_precision=0.36744, validation/accuracy=0.986483, validation/loss=0.044804, validation/mean_average_precision=0.253998, validation/num_examples=43793
I0305 20:19:07.235630 140181048514304 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.12351503223180771, loss=0.030879592522978783
I0305 20:19:27.519685 140181040121600 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.09579956531524658, loss=0.03379568085074425
I0305 20:19:47.662703 140181048514304 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.07801283150911331, loss=0.03550003841519356
I0305 20:20:08.125192 140181040121600 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.05611340329051018, loss=0.03271814435720444
I0305 20:20:28.480751 140181048514304 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.07032071053981781, loss=0.030989592894911766
I0305 20:20:49.094548 140181040121600 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.06700349599123001, loss=0.030763331800699234
I0305 20:21:09.186997 140181048514304 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.04765934497117996, loss=0.031483232975006104
I0305 20:21:29.684670 140181040121600 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.09797204285860062, loss=0.035302650183439255
I0305 20:21:49.821963 140181048514304 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.06852663308382034, loss=0.028930576518177986
I0305 20:22:10.090589 140181040121600 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.05457107722759247, loss=0.026741936802864075
I0305 20:22:30.441199 140181048514304 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0752476155757904, loss=0.030999060720205307
I0305 20:22:51.004712 140181040121600 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.06341579556465149, loss=0.0319051519036293
I0305 20:23:02.808601 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:24:15.036069 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:24:17.025630 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:24:18.909066 140323414672576 submission_runner.py:469] Time since start: 4313.53s, 	Step: 15159, 	{'train/accuracy': 0.9907143712043762, 'train/loss': 0.030279938131570816, 'train/mean_average_precision': 0.40031920960122586, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.044796157628297806, 'validation/mean_average_precision': 0.2591027981005202, 'validation/num_examples': 43793, 'test/accuracy': 0.9858903884887695, 'test/loss': 0.047630976885557175, 'test/mean_average_precision': 0.25436957244714936, 'test/num_examples': 43793, 'score': 3131.7593042850494, 'total_duration': 4313.525934457779, 'accumulated_submission_time': 3131.7593042850494, 'accumulated_eval_time': 1181.128756761551, 'accumulated_logging_time': 0.2487773895263672}
I0305 20:24:18.918703 140181048514304 logging_writer.py:48] [15159] accumulated_eval_time=1181.13, accumulated_logging_time=0.248777, accumulated_submission_time=3131.76, global_step=15159, preemption_count=0, score=3131.76, test/accuracy=0.98589, test/loss=0.047631, test/mean_average_precision=0.25437, test/num_examples=43793, total_duration=4313.53, train/accuracy=0.990714, train/loss=0.0302799, train/mean_average_precision=0.400319, validation/accuracy=0.986795, validation/loss=0.0447962, validation/mean_average_precision=0.259103, validation/num_examples=43793
I0305 20:24:27.471207 140181040121600 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0854000598192215, loss=0.030197670683264732
I0305 20:24:47.842222 140181048514304 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.06341175734996796, loss=0.031058408319950104
I0305 20:25:07.986337 140181040121600 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.06278552860021591, loss=0.02752559445798397
I0305 20:25:28.664138 140181048514304 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.07152193784713745, loss=0.03263149783015251
I0305 20:25:49.002180 140181040121600 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.06572504341602325, loss=0.03250443562865257
I0305 20:26:09.212224 140181048514304 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.08567553013563156, loss=0.028207341209053993
I0305 20:26:29.471305 140181040121600 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.08292371034622192, loss=0.031000064685940742
I0305 20:26:50.267649 140181048514304 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.10683345049619675, loss=0.03153778240084648
I0305 20:27:10.729210 140181040121600 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.09077460318803787, loss=0.027786673977971077
I0305 20:27:31.332828 140181048514304 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.08524888753890991, loss=0.029236674308776855
I0305 20:27:51.588671 140181040121600 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.09108076244592667, loss=0.03246178478002548
I0305 20:28:12.096329 140181048514304 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.08458057790994644, loss=0.029202954843640327
I0305 20:28:18.998836 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:29:28.093658 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:29:30.202392 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:29:32.077659 140323414672576 submission_runner.py:469] Time since start: 4626.69s, 	Step: 16335, 	{'train/accuracy': 0.9908336400985718, 'train/loss': 0.030252518132328987, 'train/mean_average_precision': 0.3931483787548169, 'validation/accuracy': 0.9867780804634094, 'validation/loss': 0.04409050941467285, 'validation/mean_average_precision': 0.2683983880921344, 'validation/num_examples': 43793, 'test/accuracy': 0.9858853220939636, 'test/loss': 0.04689682647585869, 'test/mean_average_precision': 0.2582523247154778, 'test/num_examples': 43793, 'score': 3371.7986681461334, 'total_duration': 4626.694617271423, 'accumulated_submission_time': 3371.7986681461334, 'accumulated_eval_time': 1254.2074756622314, 'accumulated_logging_time': 0.2676990032196045}
I0305 20:29:32.087958 140181040121600 logging_writer.py:48] [16335] accumulated_eval_time=1254.21, accumulated_logging_time=0.267699, accumulated_submission_time=3371.8, global_step=16335, preemption_count=0, score=3371.8, test/accuracy=0.985885, test/loss=0.0468968, test/mean_average_precision=0.258252, test/num_examples=43793, total_duration=4626.69, train/accuracy=0.990834, train/loss=0.0302525, train/mean_average_precision=0.393148, validation/accuracy=0.986778, validation/loss=0.0440905, validation/mean_average_precision=0.268398, validation/num_examples=43793
I0305 20:29:45.654741 140181048514304 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.07563971728086472, loss=0.034528207033872604
I0305 20:30:06.056516 140181040121600 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.06681480258703232, loss=0.02677091397345066
I0305 20:30:26.178118 140181048514304 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.07387150079011917, loss=0.027446549385786057
I0305 20:30:46.307108 140181040121600 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.09829054027795792, loss=0.03374052792787552
I0305 20:31:06.678823 140181048514304 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.07734715193510056, loss=0.031063534319400787
I0305 20:31:26.991781 140181040121600 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.07507321238517761, loss=0.035339612513780594
I0305 20:31:47.236160 140181048514304 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.06337078660726547, loss=0.029687901958823204
I0305 20:32:08.031756 140181040121600 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.09438181668519974, loss=0.032237034291028976
I0305 20:32:28.277525 140181048514304 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.10494644939899445, loss=0.03131365776062012
I0305 20:32:48.591627 140181040121600 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.09249594062566757, loss=0.03127382695674896
I0305 20:33:08.933362 140181048514304 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.07844770699739456, loss=0.02768804505467415
I0305 20:33:29.328312 140181040121600 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.07730886340141296, loss=0.02978391945362091
I0305 20:33:32.216285 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:34:42.713553 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:34:44.647683 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:34:46.558321 140323414672576 submission_runner.py:469] Time since start: 4941.18s, 	Step: 17515, 	{'train/accuracy': 0.9908801317214966, 'train/loss': 0.02966691553592682, 'train/mean_average_precision': 0.41545911713610517, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.04448854178190231, 'validation/mean_average_precision': 0.271140390211123, 'validation/num_examples': 43793, 'test/accuracy': 0.9859564900398254, 'test/loss': 0.04732132330536842, 'test/mean_average_precision': 0.25312751116094817, 'test/num_examples': 43793, 'score': 3611.886740922928, 'total_duration': 4941.175326347351, 'accumulated_submission_time': 3611.886740922928, 'accumulated_eval_time': 1328.5494711399078, 'accumulated_logging_time': 0.28993844985961914}
I0305 20:34:46.569040 140181048514304 logging_writer.py:48] [17515] accumulated_eval_time=1328.55, accumulated_logging_time=0.289938, accumulated_submission_time=3611.89, global_step=17515, preemption_count=0, score=3611.89, test/accuracy=0.985956, test/loss=0.0473213, test/mean_average_precision=0.253128, test/num_examples=43793, total_duration=4941.18, train/accuracy=0.99088, train/loss=0.0296669, train/mean_average_precision=0.415459, validation/accuracy=0.986765, validation/loss=0.0444885, validation/mean_average_precision=0.27114, validation/num_examples=43793
I0305 20:35:04.327657 140181040121600 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.07675676047801971, loss=0.0320485420525074
I0305 20:35:24.833672 140181048514304 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.07281799614429474, loss=0.029257752001285553
I0305 20:35:45.326303 140181040121600 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.0725824385881424, loss=0.031679846346378326
I0305 20:36:05.473480 140181048514304 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.08456416428089142, loss=0.030193991959095
I0305 20:36:26.071647 140181040121600 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.07253194600343704, loss=0.028861207887530327
I0305 20:36:46.459873 140181048514304 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.09858860075473785, loss=0.03217080980539322
I0305 20:37:06.874609 140181040121600 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.10690413415431976, loss=0.032887477427721024
I0305 20:37:27.288544 140181048514304 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.0753159299492836, loss=0.031341079622507095
I0305 20:37:47.977777 140181040121600 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.06399939209222794, loss=0.02829127199947834
I0305 20:38:08.311830 140181048514304 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.1164054200053215, loss=0.0295416247099638
I0305 20:38:28.960315 140181040121600 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.09120840579271317, loss=0.029883278533816338
I0305 20:38:46.695412 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:39:57.639394 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:39:59.523845 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:40:01.406657 140323414672576 submission_runner.py:469] Time since start: 5256.02s, 	Step: 18687, 	{'train/accuracy': 0.9906529188156128, 'train/loss': 0.030500255525112152, 'train/mean_average_precision': 0.4209120021722421, 'validation/accuracy': 0.9865559935569763, 'validation/loss': 0.045080482959747314, 'validation/mean_average_precision': 0.2668583827734413, 'validation/num_examples': 43793, 'test/accuracy': 0.9857783317565918, 'test/loss': 0.047503095120191574, 'test/mean_average_precision': 0.25781563102755245, 'test/num_examples': 43793, 'score': 3851.973825931549, 'total_duration': 5256.023555994034, 'accumulated_submission_time': 3851.973825931549, 'accumulated_eval_time': 1403.2605533599854, 'accumulated_logging_time': 0.3101847171783447}
I0305 20:40:01.417139 140181048514304 logging_writer.py:48] [18687] accumulated_eval_time=1403.26, accumulated_logging_time=0.310185, accumulated_submission_time=3851.97, global_step=18687, preemption_count=0, score=3851.97, test/accuracy=0.985778, test/loss=0.0475031, test/mean_average_precision=0.257816, test/num_examples=43793, total_duration=5256.02, train/accuracy=0.990653, train/loss=0.0305003, train/mean_average_precision=0.420912, validation/accuracy=0.986556, validation/loss=0.0450805, validation/mean_average_precision=0.266858, validation/num_examples=43793
I0305 20:40:04.295086 140181040121600 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.0861407220363617, loss=0.02881453186273575
I0305 20:40:24.700855 140181048514304 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.06581872701644897, loss=0.02813737280666828
I0305 20:40:45.017381 140181040121600 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.08378975838422775, loss=0.031539976596832275
I0305 20:41:05.238819 140181048514304 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.09265881031751633, loss=0.031160499900579453
I0305 20:41:25.461871 140181040121600 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.135234996676445, loss=0.02627711556851864
I0305 20:41:45.973437 140181048514304 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.08647987991571426, loss=0.03167925029993057
I0305 20:42:06.309519 140181040121600 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.09390067309141159, loss=0.030551495030522346
I0305 20:42:26.809935 140181048514304 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.08056122064590454, loss=0.030469447374343872
I0305 20:42:47.296261 140181040121600 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.1171286553144455, loss=0.03568504378199577
I0305 20:43:07.443327 140181048514304 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.09585690498352051, loss=0.031947556883096695
I0305 20:43:27.788580 140181040121600 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.0849497839808464, loss=0.02766994945704937
I0305 20:43:48.100685 140181048514304 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.09350298345088959, loss=0.027627766132354736
I0305 20:44:01.549921 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:45:11.279672 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:45:14.978147 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:45:16.857847 140323414672576 submission_runner.py:469] Time since start: 5571.47s, 	Step: 19867, 	{'train/accuracy': 0.9909771084785461, 'train/loss': 0.02932092733681202, 'train/mean_average_precision': 0.42404470792376164, 'validation/accuracy': 0.9867427349090576, 'validation/loss': 0.04458590969443321, 'validation/mean_average_precision': 0.2653829366622306, 'validation/num_examples': 43793, 'test/accuracy': 0.9859800934791565, 'test/loss': 0.047199707478284836, 'test/mean_average_precision': 0.2569790034379748, 'test/num_examples': 43793, 'score': 4092.066060781479, 'total_duration': 5571.474860429764, 'accumulated_submission_time': 4092.066060781479, 'accumulated_eval_time': 1478.5684304237366, 'accumulated_logging_time': 0.3311624526977539}
I0305 20:45:16.868457 140181040121600 logging_writer.py:48] [19867] accumulated_eval_time=1478.57, accumulated_logging_time=0.331162, accumulated_submission_time=4092.07, global_step=19867, preemption_count=0, score=4092.07, test/accuracy=0.98598, test/loss=0.0471997, test/mean_average_precision=0.256979, test/num_examples=43793, total_duration=5571.47, train/accuracy=0.990977, train/loss=0.0293209, train/mean_average_precision=0.424045, validation/accuracy=0.986743, validation/loss=0.0445859, validation/mean_average_precision=0.265383, validation/num_examples=43793
I0305 20:45:23.793555 140181048514304 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.07888513058423996, loss=0.029748111963272095
I0305 20:45:44.398477 140181040121600 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.085373654961586, loss=0.029831955209374428
I0305 20:46:04.989127 140181048514304 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.08844093233346939, loss=0.02822621911764145
I0305 20:46:25.460621 140181040121600 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.11434097588062286, loss=0.03209352865815163
I0305 20:46:45.857641 140181048514304 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.09924878180027008, loss=0.030244382098317146
I0305 20:47:06.512407 140181040121600 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.08517497032880783, loss=0.02973034791648388
I0305 20:47:27.487734 140181048514304 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.1046881452202797, loss=0.02930270880460739
I0305 20:47:48.103757 140181040121600 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.08939680457115173, loss=0.028321541845798492
I0305 20:48:08.960317 140181048514304 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.10186868906021118, loss=0.028832480311393738
I0305 20:48:30.071478 140181040121600 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.12952710688114166, loss=0.03228660672903061
I0305 20:48:50.890547 140181048514304 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.09204988181591034, loss=0.030706092715263367
I0305 20:49:11.636879 140181040121600 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.13489410281181335, loss=0.02779354527592659
I0305 20:49:16.884093 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:50:25.018995 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:50:26.955029 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:50:28.868851 140323414672576 submission_runner.py:469] Time since start: 5883.49s, 	Step: 21026, 	{'train/accuracy': 0.9913184642791748, 'train/loss': 0.02832372486591339, 'train/mean_average_precision': 0.43958617184370263, 'validation/accuracy': 0.9868292212486267, 'validation/loss': 0.04461655020713806, 'validation/mean_average_precision': 0.2678854960720369, 'validation/num_examples': 43793, 'test/accuracy': 0.985869288444519, 'test/loss': 0.04747440293431282, 'test/mean_average_precision': 0.25587157651797093, 'test/num_examples': 43793, 'score': 4332.043648481369, 'total_duration': 5883.485830783844, 'accumulated_submission_time': 4332.043648481369, 'accumulated_eval_time': 1550.5531079769135, 'accumulated_logging_time': 0.351360559463501}
I0305 20:50:28.879761 140181048514304 logging_writer.py:48] [21026] accumulated_eval_time=1550.55, accumulated_logging_time=0.351361, accumulated_submission_time=4332.04, global_step=21026, preemption_count=0, score=4332.04, test/accuracy=0.985869, test/loss=0.0474744, test/mean_average_precision=0.255872, test/num_examples=43793, total_duration=5883.49, train/accuracy=0.991318, train/loss=0.0283237, train/mean_average_precision=0.439586, validation/accuracy=0.986829, validation/loss=0.0446166, validation/mean_average_precision=0.267885, validation/num_examples=43793
I0305 20:50:44.837898 140181040121600 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.11258536577224731, loss=0.03088480979204178
I0305 20:51:05.860891 140181048514304 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.08638577908277512, loss=0.030444705858826637
I0305 20:51:26.978404 140181040121600 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.11855494976043701, loss=0.02705206535756588
I0305 20:51:47.894709 140181048514304 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.07361764460802078, loss=0.028751881793141365
I0305 20:52:08.796988 140181040121600 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.12285729497671127, loss=0.02658364549279213
I0305 20:52:29.547977 140181048514304 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.08839365839958191, loss=0.028440507128834724
I0305 20:52:50.270201 140181040121600 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.08735454827547073, loss=0.027516307309269905
I0305 20:53:11.169712 140181048514304 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.0831415206193924, loss=0.02662327140569687
I0305 20:53:32.495115 140181040121600 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.09177994728088379, loss=0.03016122803092003
I0305 20:53:53.377583 140181048514304 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.13567186892032623, loss=0.03074929490685463
I0305 20:54:13.915457 140181040121600 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.08805544674396515, loss=0.029195979237556458
I0305 20:54:29.020130 140323414672576 spec.py:321] Evaluating on the training split.
I0305 20:55:42.062692 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 20:55:44.065192 140323414672576 spec.py:349] Evaluating on the test split.
I0305 20:55:45.998299 140323414672576 submission_runner.py:469] Time since start: 6200.62s, 	Step: 22174, 	{'train/accuracy': 0.9914518594741821, 'train/loss': 0.027702966704964638, 'train/mean_average_precision': 0.4644357967232483, 'validation/accuracy': 0.9867590069770813, 'validation/loss': 0.04439372196793556, 'validation/mean_average_precision': 0.269447145805152, 'validation/num_examples': 43793, 'test/accuracy': 0.9858987927436829, 'test/loss': 0.04731141775846481, 'test/mean_average_precision': 0.2557575138847978, 'test/num_examples': 43793, 'score': 4572.144749879837, 'total_duration': 6200.615307807922, 'accumulated_submission_time': 4572.144749879837, 'accumulated_eval_time': 1627.531227350235, 'accumulated_logging_time': 0.37154221534729004}
I0305 20:55:46.011061 140181048514304 logging_writer.py:48] [22174] accumulated_eval_time=1627.53, accumulated_logging_time=0.371542, accumulated_submission_time=4572.14, global_step=22174, preemption_count=0, score=4572.14, test/accuracy=0.985899, test/loss=0.0473114, test/mean_average_precision=0.255758, test/num_examples=43793, total_duration=6200.62, train/accuracy=0.991452, train/loss=0.027703, train/mean_average_precision=0.464436, validation/accuracy=0.986759, validation/loss=0.0443937, validation/mean_average_precision=0.269447, validation/num_examples=43793
I0305 20:55:51.700432 140181040121600 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.11043142527341843, loss=0.027712056413292885
I0305 20:56:12.876044 140181048514304 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.10225134342908859, loss=0.028322795405983925
I0305 20:56:33.206300 140181040121600 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.12284096330404282, loss=0.03151484951376915
I0305 20:56:53.368186 140181048514304 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.08924402296543121, loss=0.03280717507004738
I0305 20:57:13.693667 140181040121600 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.11608230322599411, loss=0.027156636118888855
I0305 20:57:34.395605 140181048514304 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.1263940930366516, loss=0.02583995647728443
I0305 20:57:55.218456 140181040121600 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.09326503425836563, loss=0.026322994381189346
I0305 20:58:15.845229 140181048514304 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.11109266430139542, loss=0.029458599165081978
I0305 20:58:36.496928 140181040121600 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.10305072367191315, loss=0.027914758771657944
I0305 20:58:56.549636 140181048514304 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.09095337241888046, loss=0.026298677548766136
I0305 20:59:17.174165 140181040121600 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.09908235818147659, loss=0.028973592445254326
I0305 20:59:37.696874 140181048514304 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.11353036761283875, loss=0.02711992710828781
I0305 20:59:46.092599 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:00:55.158561 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:00:57.088784 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:00:59.015781 140323414672576 submission_runner.py:469] Time since start: 6513.63s, 	Step: 23342, 	{'train/accuracy': 0.9915069341659546, 'train/loss': 0.027634425088763237, 'train/mean_average_precision': 0.4606353431102186, 'validation/accuracy': 0.9868580102920532, 'validation/loss': 0.044646430760622025, 'validation/mean_average_precision': 0.2732313752861011, 'validation/num_examples': 43793, 'test/accuracy': 0.986052930355072, 'test/loss': 0.047586437314748764, 'test/mean_average_precision': 0.26334678308197046, 'test/num_examples': 43793, 'score': 4812.187657356262, 'total_duration': 6513.632674217224, 'accumulated_submission_time': 4812.187657356262, 'accumulated_eval_time': 1700.454246520996, 'accumulated_logging_time': 0.3936655521392822}
I0305 21:00:59.027713 140181040121600 logging_writer.py:48] [23342] accumulated_eval_time=1700.45, accumulated_logging_time=0.393666, accumulated_submission_time=4812.19, global_step=23342, preemption_count=0, score=4812.19, test/accuracy=0.986053, test/loss=0.0475864, test/mean_average_precision=0.263347, test/num_examples=43793, total_duration=6513.63, train/accuracy=0.991507, train/loss=0.0276344, train/mean_average_precision=0.460635, validation/accuracy=0.986858, validation/loss=0.0446464, validation/mean_average_precision=0.273231, validation/num_examples=43793
I0305 21:01:11.136464 140181048514304 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.08653901517391205, loss=0.02491455152630806
I0305 21:01:31.412100 140181040121600 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.10604150593280792, loss=0.030263284221291542
I0305 21:01:51.888968 140181048514304 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.1389853060245514, loss=0.027883106842637062
I0305 21:02:12.496690 140181040121600 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.09133294969797134, loss=0.028496690094470978
I0305 21:02:33.031885 140181048514304 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.09519326686859131, loss=0.027056220918893814
I0305 21:02:53.608910 140181040121600 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.11353722959756851, loss=0.027508754283189774
I0305 21:03:13.623137 140181048514304 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.11598831415176392, loss=0.031130362302064896
I0305 21:03:33.996154 140181040121600 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.10370956361293793, loss=0.029248489066958427
I0305 21:03:54.449099 140181048514304 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.0974697396159172, loss=0.02468743547797203
I0305 21:04:14.749135 140181040121600 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.1504928171634674, loss=0.030366968363523483
I0305 21:04:35.127444 140181048514304 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.1772981584072113, loss=0.03203557804226875
I0305 21:04:56.040275 140181040121600 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.09747529029846191, loss=0.029549218714237213
I0305 21:04:59.123223 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:06:10.420503 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:06:12.376352 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:06:14.269213 140323414672576 submission_runner.py:469] Time since start: 6828.89s, 	Step: 24516, 	{'train/accuracy': 0.9916749000549316, 'train/loss': 0.026768837124109268, 'train/mean_average_precision': 0.4847076427153393, 'validation/accuracy': 0.9868206977844238, 'validation/loss': 0.0445321761071682, 'validation/mean_average_precision': 0.27062057079293783, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.047279488295316696, 'test/mean_average_precision': 0.2592041691558263, 'test/num_examples': 43793, 'score': 5052.244487285614, 'total_duration': 6828.886110067368, 'accumulated_submission_time': 5052.244487285614, 'accumulated_eval_time': 1775.6000895500183, 'accumulated_logging_time': 0.4145171642303467}
I0305 21:06:14.280247 140181048514304 logging_writer.py:48] [24516] accumulated_eval_time=1775.6, accumulated_logging_time=0.414517, accumulated_submission_time=5052.24, global_step=24516, preemption_count=0, score=5052.24, test/accuracy=0.985962, test/loss=0.0472795, test/mean_average_precision=0.259204, test/num_examples=43793, total_duration=6828.89, train/accuracy=0.991675, train/loss=0.0267688, train/mean_average_precision=0.484708, validation/accuracy=0.986821, validation/loss=0.0445322, validation/mean_average_precision=0.270621, validation/num_examples=43793
I0305 21:06:31.656782 140181040121600 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.11110691726207733, loss=0.027962014079093933
I0305 21:06:52.126009 140181048514304 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.10952194035053253, loss=0.02952682040631771
I0305 21:07:12.693631 140181040121600 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.11886793375015259, loss=0.025927487760782242
I0305 21:07:33.545573 140181048514304 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.1742376834154129, loss=0.028113311156630516
I0305 21:07:54.216830 140181040121600 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.11485126614570618, loss=0.02627500146627426
I0305 21:08:14.853414 140181048514304 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.10550770163536072, loss=0.028009064495563507
I0305 21:08:35.502743 140181040121600 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.08828357607126236, loss=0.027229707688093185
I0305 21:08:56.446602 140181048514304 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.12193563580513, loss=0.026115085929632187
I0305 21:09:16.874374 140181040121600 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.10385402292013168, loss=0.027183981612324715
I0305 21:09:37.070961 140181048514304 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.13346323370933533, loss=0.02543615736067295
I0305 21:09:57.069988 140181040121600 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.11724559217691422, loss=0.026860902085900307
I0305 21:10:14.317874 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:11:26.243325 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:11:28.180361 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:11:30.094691 140323414672576 submission_runner.py:469] Time since start: 7144.71s, 	Step: 25686, 	{'train/accuracy': 0.9917594790458679, 'train/loss': 0.0264980960637331, 'train/mean_average_precision': 0.48456089799359725, 'validation/accuracy': 0.9867764711380005, 'validation/loss': 0.04483954980969429, 'validation/mean_average_precision': 0.2776095949448317, 'validation/num_examples': 43793, 'test/accuracy': 0.9859291315078735, 'test/loss': 0.0478808656334877, 'test/mean_average_precision': 0.2572342924180867, 'test/num_examples': 43793, 'score': 5292.241194009781, 'total_duration': 7144.711585044861, 'accumulated_submission_time': 5292.241194009781, 'accumulated_eval_time': 1851.376755952835, 'accumulated_logging_time': 0.4351999759674072}
I0305 21:11:30.105973 140181048514304 logging_writer.py:48] [25686] accumulated_eval_time=1851.38, accumulated_logging_time=0.4352, accumulated_submission_time=5292.24, global_step=25686, preemption_count=0, score=5292.24, test/accuracy=0.985929, test/loss=0.0478809, test/mean_average_precision=0.257234, test/num_examples=43793, total_duration=7144.71, train/accuracy=0.991759, train/loss=0.0264981, train/mean_average_precision=0.484561, validation/accuracy=0.986776, validation/loss=0.0448395, validation/mean_average_precision=0.27761, validation/num_examples=43793
I0305 21:11:33.106699 140181040121600 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.12814249098300934, loss=0.02482025697827339
I0305 21:11:53.328248 140181048514304 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.12187591195106506, loss=0.026676760986447334
I0305 21:12:13.741002 140181040121600 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.10036807507276535, loss=0.02694741077721119
I0305 21:12:34.261008 140181048514304 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.17189858853816986, loss=0.027581224218010902
I0305 21:12:54.840703 140181040121600 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.09396956115961075, loss=0.02508338913321495
I0305 21:13:15.336297 140181048514304 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.12486904114484787, loss=0.025461148470640182
I0305 21:13:35.624186 140181040121600 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.1078520268201828, loss=0.02397983893752098
I0305 21:13:56.048230 140181048514304 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.10162501782178879, loss=0.02890680730342865
I0305 21:14:16.667987 140181040121600 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.11866781115531921, loss=0.027331950142979622
I0305 21:14:37.195168 140181048514304 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.09815290570259094, loss=0.026271624490618706
I0305 21:14:57.569397 140181040121600 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.10330644994974136, loss=0.026927119120955467
I0305 21:15:18.185720 140181048514304 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.12832602858543396, loss=0.025706924498081207
I0305 21:15:30.106079 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:16:40.284259 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:16:42.254350 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:16:44.185626 140323414672576 submission_runner.py:469] Time since start: 7458.80s, 	Step: 26859, 	{'train/accuracy': 0.9919121861457825, 'train/loss': 0.025988435372710228, 'train/mean_average_precision': 0.5066883529462435, 'validation/accuracy': 0.986818253993988, 'validation/loss': 0.04498967155814171, 'validation/mean_average_precision': 0.2742437594418257, 'validation/num_examples': 43793, 'test/accuracy': 0.9859526753425598, 'test/loss': 0.04789399728178978, 'test/mean_average_precision': 0.260470862895351, 'test/num_examples': 43793, 'score': 5532.201447963715, 'total_duration': 7458.802631378174, 'accumulated_submission_time': 5532.201447963715, 'accumulated_eval_time': 1925.456248998642, 'accumulated_logging_time': 0.455519437789917}
I0305 21:16:44.197744 140181040121600 logging_writer.py:48] [26859] accumulated_eval_time=1925.46, accumulated_logging_time=0.455519, accumulated_submission_time=5532.2, global_step=26859, preemption_count=0, score=5532.2, test/accuracy=0.985953, test/loss=0.047894, test/mean_average_precision=0.260471, test/num_examples=43793, total_duration=7458.8, train/accuracy=0.991912, train/loss=0.0259884, train/mean_average_precision=0.506688, validation/accuracy=0.986818, validation/loss=0.0449897, validation/mean_average_precision=0.274244, validation/num_examples=43793
I0305 21:16:52.965872 140181048514304 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.11586609482765198, loss=0.025362877175211906
I0305 21:17:13.172412 140181040121600 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.10361301153898239, loss=0.025371210649609566
I0305 21:17:33.705384 140181048514304 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.11949086934328079, loss=0.02783568762242794
I0305 21:17:54.304701 140181040121600 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.0878632515668869, loss=0.024650350213050842
I0305 21:18:15.082043 140181048514304 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.13427890837192535, loss=0.02687637321650982
I0305 21:18:35.740523 140181040121600 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.12401182949542999, loss=0.029601380228996277
I0305 21:18:56.483434 140181048514304 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.15159863233566284, loss=0.02794540300965309
I0305 21:19:17.029699 140181040121600 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.14911893010139465, loss=0.024604467675089836
I0305 21:19:37.762701 140181048514304 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.16115523874759674, loss=0.0286251287907362
I0305 21:19:58.254057 140181040121600 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.11893725395202637, loss=0.02679051272571087
I0305 21:20:18.820411 140181048514304 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.11482684314250946, loss=0.025320371612906456
I0305 21:20:39.257596 140181040121600 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.14290361106395721, loss=0.028071794658899307
I0305 21:20:44.354521 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:21:54.696875 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:21:56.710262 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:21:58.584428 140323414672576 submission_runner.py:469] Time since start: 7773.20s, 	Step: 28026, 	{'train/accuracy': 0.992068350315094, 'train/loss': 0.025575853884220123, 'train/mean_average_precision': 0.5093305501186075, 'validation/accuracy': 0.98676997423172, 'validation/loss': 0.04546408727765083, 'validation/mean_average_precision': 0.2774210883788067, 'validation/num_examples': 43793, 'test/accuracy': 0.9859409332275391, 'test/loss': 0.04856036975979805, 'test/mean_average_precision': 0.2589473862411662, 'test/num_examples': 43793, 'score': 5772.314005613327, 'total_duration': 7773.201402425766, 'accumulated_submission_time': 5772.314005613327, 'accumulated_eval_time': 1999.6860909461975, 'accumulated_logging_time': 0.4820897579193115}
I0305 21:21:58.596265 140181048514304 logging_writer.py:48] [28026] accumulated_eval_time=1999.69, accumulated_logging_time=0.48209, accumulated_submission_time=5772.31, global_step=28026, preemption_count=0, score=5772.31, test/accuracy=0.985941, test/loss=0.0485604, test/mean_average_precision=0.258947, test/num_examples=43793, total_duration=7773.2, train/accuracy=0.992068, train/loss=0.0255759, train/mean_average_precision=0.509331, validation/accuracy=0.98677, validation/loss=0.0454641, validation/mean_average_precision=0.277421, validation/num_examples=43793
I0305 21:22:14.216145 140181040121600 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.12110839784145355, loss=0.024481719359755516
I0305 21:22:35.034248 140181048514304 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.11755968630313873, loss=0.027046823874115944
I0305 21:22:55.821819 140181040121600 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.13173280656337738, loss=0.025532681494951248
I0305 21:23:16.517679 140181048514304 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.11517921835184097, loss=0.024735765531659126
I0305 21:23:37.454670 140181040121600 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.10831745713949203, loss=0.022324591875076294
I0305 21:23:58.192784 140181048514304 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.13078221678733826, loss=0.02702973410487175
I0305 21:24:19.173438 140181040121600 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.14327771961688995, loss=0.027441533282399178
I0305 21:24:39.936054 140181048514304 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1386079490184784, loss=0.029437914490699768
I0305 21:25:00.614833 140181040121600 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.11202681809663773, loss=0.022263849154114723
I0305 21:25:21.308317 140181048514304 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.12523867189884186, loss=0.025985682383179665
I0305 21:25:42.026609 140181040121600 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.15149171650409698, loss=0.027842465788125992
I0305 21:25:58.634271 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:27:09.501524 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:27:11.413700 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:27:13.305732 140323414672576 submission_runner.py:469] Time since start: 8087.92s, 	Step: 29181, 	{'train/accuracy': 0.9922259449958801, 'train/loss': 0.02485993504524231, 'train/mean_average_precision': 0.5260727822862852, 'validation/accuracy': 0.9867967367172241, 'validation/loss': 0.0454312264919281, 'validation/mean_average_precision': 0.278488574127944, 'validation/num_examples': 43793, 'test/accuracy': 0.985890805721283, 'test/loss': 0.04876728728413582, 'test/mean_average_precision': 0.2578138540729559, 'test/num_examples': 43793, 'score': 6012.096572875977, 'total_duration': 8087.9226224422455, 'accumulated_submission_time': 6012.096572875977, 'accumulated_eval_time': 2074.357400894165, 'accumulated_logging_time': 0.7150609493255615}
I0305 21:27:13.318574 140181048514304 logging_writer.py:48] [29181] accumulated_eval_time=2074.36, accumulated_logging_time=0.715061, accumulated_submission_time=6012.1, global_step=29181, preemption_count=0, score=6012.1, test/accuracy=0.985891, test/loss=0.0487673, test/mean_average_precision=0.257814, test/num_examples=43793, total_duration=8087.92, train/accuracy=0.992226, train/loss=0.0248599, train/mean_average_precision=0.526073, validation/accuracy=0.986797, validation/loss=0.0454312, validation/mean_average_precision=0.278489, validation/num_examples=43793
I0305 21:27:17.379308 140181040121600 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.1020277589559555, loss=0.024256326258182526
I0305 21:27:38.212961 140181048514304 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.12523753941059113, loss=0.026474198326468468
I0305 21:27:58.996147 140181040121600 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.11605209112167358, loss=0.025051867589354515
I0305 21:28:19.782344 140181048514304 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1251719892024994, loss=0.025983115658164024
I0305 21:28:40.685132 140181040121600 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.11874367296695709, loss=0.025111235678195953
I0305 21:29:01.516048 140181048514304 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.1313404142856598, loss=0.024851463735103607
I0305 21:29:22.173671 140181040121600 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.1448565572500229, loss=0.02693723328411579
I0305 21:29:42.880445 140181048514304 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.13553360104560852, loss=0.024892043322324753
I0305 21:30:03.382807 140181040121600 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.12159750610589981, loss=0.021975981071591377
I0305 21:30:24.270494 140181048514304 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.12398026883602142, loss=0.022045686841011047
I0305 21:30:44.905589 140181040121600 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.14129209518432617, loss=0.022381139919161797
I0305 21:31:05.722382 140181048514304 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.1514550894498825, loss=0.024802325293421745
I0305 21:31:13.420705 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:32:24.639731 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:32:26.598350 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:32:28.504281 140323414672576 submission_runner.py:469] Time since start: 8403.12s, 	Step: 30338, 	{'train/accuracy': 0.9925255179405212, 'train/loss': 0.024014722555875778, 'train/mean_average_precision': 0.5361810671808445, 'validation/accuracy': 0.98674476146698, 'validation/loss': 0.045607633888721466, 'validation/mean_average_precision': 0.2803041059396133, 'validation/num_examples': 43793, 'test/accuracy': 0.9858090877532959, 'test/loss': 0.0487041100859642, 'test/mean_average_precision': 0.26535465098545596, 'test/num_examples': 43793, 'score': 6252.160385608673, 'total_duration': 8403.121178627014, 'accumulated_submission_time': 6252.160385608673, 'accumulated_eval_time': 2149.440814971924, 'accumulated_logging_time': 0.7369751930236816}
I0305 21:32:28.516187 140181040121600 logging_writer.py:48] [30338] accumulated_eval_time=2149.44, accumulated_logging_time=0.736975, accumulated_submission_time=6252.16, global_step=30338, preemption_count=0, score=6252.16, test/accuracy=0.985809, test/loss=0.0487041, test/mean_average_precision=0.265355, test/num_examples=43793, total_duration=8403.12, train/accuracy=0.992526, train/loss=0.0240147, train/mean_average_precision=0.536181, validation/accuracy=0.986745, validation/loss=0.0456076, validation/mean_average_precision=0.280304, validation/num_examples=43793
I0305 21:32:41.563453 140181048514304 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.153214693069458, loss=0.025815343484282494
I0305 21:33:02.170016 140181040121600 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.11964225023984909, loss=0.022709637880325317
I0305 21:33:22.774161 140181048514304 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.14710262417793274, loss=0.023951929062604904
I0305 21:33:43.390953 140181040121600 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.1382860690355301, loss=0.025162184610962868
I0305 21:34:04.077883 140181048514304 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.15409931540489197, loss=0.02170042134821415
I0305 21:34:24.738029 140181040121600 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.13586097955703735, loss=0.02509407326579094
I0305 21:34:45.458623 140181048514304 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.12876921892166138, loss=0.022685980424284935
I0305 21:35:05.844488 140181040121600 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.18510447442531586, loss=0.024563923478126526
I0305 21:35:26.745483 140181048514304 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.14255313575267792, loss=0.022870412096381187
I0305 21:35:47.437792 140181040121600 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.11279034614562988, loss=0.020541880279779434
I0305 21:36:08.232009 140181048514304 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.1617053598165512, loss=0.024668864905834198
I0305 21:36:28.636978 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:37:38.811812 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:37:40.768062 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:37:42.693930 140323414672576 submission_runner.py:469] Time since start: 8717.31s, 	Step: 31500, 	{'train/accuracy': 0.9926242232322693, 'train/loss': 0.023406900465488434, 'train/mean_average_precision': 0.5624632319165195, 'validation/accuracy': 0.9867528676986694, 'validation/loss': 0.04611608386039734, 'validation/mean_average_precision': 0.27335544341539186, 'validation/num_examples': 43793, 'test/accuracy': 0.9859278798103333, 'test/loss': 0.04926510527729988, 'test/mean_average_precision': 0.25906426707756247, 'test/num_examples': 43793, 'score': 6492.241488456726, 'total_duration': 8717.310858488083, 'accumulated_submission_time': 6492.241488456726, 'accumulated_eval_time': 2223.497634410858, 'accumulated_logging_time': 0.7595162391662598}
I0305 21:37:42.706008 140181040121600 logging_writer.py:48] [31500] accumulated_eval_time=2223.5, accumulated_logging_time=0.759516, accumulated_submission_time=6492.24, global_step=31500, preemption_count=0, score=6492.24, test/accuracy=0.985928, test/loss=0.0492651, test/mean_average_precision=0.259064, test/num_examples=43793, total_duration=8717.31, train/accuracy=0.992624, train/loss=0.0234069, train/mean_average_precision=0.562463, validation/accuracy=0.986753, validation/loss=0.0461161, validation/mean_average_precision=0.273355, validation/num_examples=43793
I0305 21:37:42.937709 140181048514304 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.15876974165439606, loss=0.02412940375506878
I0305 21:38:03.411074 140181040121600 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.17803096771240234, loss=0.023073043674230576
I0305 21:38:24.285902 140181048514304 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.14724178612232208, loss=0.023511618375778198
I0305 21:38:44.748491 140181040121600 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.13814130425453186, loss=0.024151401594281197
I0305 21:39:05.243770 140181048514304 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.13094156980514526, loss=0.023034727200865746
I0305 21:39:25.850240 140181040121600 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.16537758708000183, loss=0.02372264489531517
I0305 21:39:46.373879 140181048514304 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.1358480602502823, loss=0.023128123953938484
I0305 21:40:07.339705 140181040121600 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.14756125211715698, loss=0.021908778697252274
I0305 21:40:27.839351 140181048514304 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.15119819343090057, loss=0.023037487640976906
I0305 21:40:48.583941 140181040121600 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.13322722911834717, loss=0.02237790636718273
I0305 21:41:09.087553 140181048514304 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.1470024734735489, loss=0.023767409846186638
I0305 21:41:29.747530 140181040121600 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.15805630385875702, loss=0.02483229897916317
I0305 21:41:42.774091 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:42:54.142078 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:42:56.082929 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:42:57.987082 140323414672576 submission_runner.py:469] Time since start: 9032.60s, 	Step: 32665, 	{'train/accuracy': 0.9927356243133545, 'train/loss': 0.02317107655107975, 'train/mean_average_precision': 0.559708597296965, 'validation/accuracy': 0.9866887331008911, 'validation/loss': 0.046457622200250626, 'validation/mean_average_precision': 0.28008448878891623, 'validation/num_examples': 43793, 'test/accuracy': 0.9857892990112305, 'test/loss': 0.049622468650341034, 'test/mean_average_precision': 0.2622657203536141, 'test/num_examples': 43793, 'score': 6732.265762090683, 'total_duration': 9032.604087114334, 'accumulated_submission_time': 6732.265762090683, 'accumulated_eval_time': 2298.710569381714, 'accumulated_logging_time': 0.7862045764923096}
I0305 21:42:57.999657 140181048514304 logging_writer.py:48] [32665] accumulated_eval_time=2298.71, accumulated_logging_time=0.786205, accumulated_submission_time=6732.27, global_step=32665, preemption_count=0, score=6732.27, test/accuracy=0.985789, test/loss=0.0496225, test/mean_average_precision=0.262266, test/num_examples=43793, total_duration=9032.6, train/accuracy=0.992736, train/loss=0.0231711, train/mean_average_precision=0.559709, validation/accuracy=0.986689, validation/loss=0.0464576, validation/mean_average_precision=0.280084, validation/num_examples=43793
I0305 21:43:05.386942 140181040121600 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.15624500811100006, loss=0.021497957408428192
I0305 21:43:26.174954 140181048514304 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.16086821258068085, loss=0.022459398955106735
I0305 21:43:47.862589 140181040121600 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.1579064577817917, loss=0.021987179294228554
I0305 21:44:09.633524 140181048514304 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.15836314857006073, loss=0.025812089443206787
I0305 21:44:30.420323 140181040121600 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.15034474432468414, loss=0.022102229297161102
I0305 21:44:51.380532 140181048514304 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.16963855922222137, loss=0.025163080543279648
I0305 21:45:11.973238 140181040121600 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.16721254587173462, loss=0.02252795547246933
I0305 21:45:32.550997 140181048514304 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.16561956703662872, loss=0.024808375164866447
I0305 21:45:53.094559 140181040121600 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.15000420808792114, loss=0.022318724542856216
I0305 21:46:13.604006 140181048514304 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.16137449443340302, loss=0.02513839863240719
I0305 21:46:34.636046 140181040121600 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.1417069435119629, loss=0.020405659452080727
I0305 21:46:55.325945 140181048514304 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.17616082727909088, loss=0.025367524474859238
I0305 21:46:58.186790 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:48:07.669311 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:48:09.593894 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:48:11.463996 140323414672576 submission_runner.py:469] Time since start: 9346.08s, 	Step: 33815, 	{'train/accuracy': 0.9931453466415405, 'train/loss': 0.021778862923383713, 'train/mean_average_precision': 0.6030676853342777, 'validation/accuracy': 0.9865401983261108, 'validation/loss': 0.046925950795412064, 'validation/mean_average_precision': 0.2754254203488731, 'validation/num_examples': 43793, 'test/accuracy': 0.9856321811676025, 'test/loss': 0.0501764751970768, 'test/mean_average_precision': 0.2627487030694094, 'test/num_examples': 43793, 'score': 6972.411457061768, 'total_duration': 9346.08096075058, 'accumulated_submission_time': 6972.411457061768, 'accumulated_eval_time': 2371.987680912018, 'accumulated_logging_time': 0.8078811168670654}
I0305 21:48:11.476319 140181040121600 logging_writer.py:48] [33815] accumulated_eval_time=2371.99, accumulated_logging_time=0.807881, accumulated_submission_time=6972.41, global_step=33815, preemption_count=0, score=6972.41, test/accuracy=0.985632, test/loss=0.0501765, test/mean_average_precision=0.262749, test/num_examples=43793, total_duration=9346.08, train/accuracy=0.993145, train/loss=0.0217789, train/mean_average_precision=0.603068, validation/accuracy=0.98654, validation/loss=0.046926, validation/mean_average_precision=0.275425, validation/num_examples=43793
I0305 21:48:29.249227 140181048514304 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.20549531280994415, loss=0.023771941661834717
I0305 21:48:49.856577 140181040121600 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1414813995361328, loss=0.02279643341898918
I0305 21:49:10.514039 140181048514304 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.1454157680273056, loss=0.019527466967701912
I0305 21:49:31.266735 140181040121600 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.14788666367530823, loss=0.021370144560933113
I0305 21:49:51.887960 140181048514304 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.1722465306520462, loss=0.020798655226826668
I0305 21:50:12.732096 140181040121600 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.1520053744316101, loss=0.021074803546071053
I0305 21:50:33.407790 140181048514304 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.17995278537273407, loss=0.02310512214899063
I0305 21:50:54.188684 140181040121600 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.18140380084514618, loss=0.02517748810350895
I0305 21:51:14.548835 140181048514304 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.13786259293556213, loss=0.01979987323284149
I0305 21:51:35.138198 140181040121600 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.17647352814674377, loss=0.021470727398991585
I0305 21:51:55.813391 140181048514304 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1936938613653183, loss=0.02477811649441719
I0305 21:52:11.557759 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:53:24.156575 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:53:26.236050 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:53:28.126473 140323414672576 submission_runner.py:469] Time since start: 9662.74s, 	Step: 34976, 	{'train/accuracy': 0.9932435154914856, 'train/loss': 0.021446095779538155, 'train/mean_average_precision': 0.5987051117749832, 'validation/accuracy': 0.9867520928382874, 'validation/loss': 0.04754088073968887, 'validation/mean_average_precision': 0.28283858959263486, 'validation/num_examples': 43793, 'test/accuracy': 0.9858726859092712, 'test/loss': 0.05084618553519249, 'test/mean_average_precision': 0.2569959978016241, 'test/num_examples': 43793, 'score': 7212.45259308815, 'total_duration': 9662.743321180344, 'accumulated_submission_time': 7212.45259308815, 'accumulated_eval_time': 2448.556200504303, 'accumulated_logging_time': 0.8298265933990479}
I0305 21:53:28.138845 140181040121600 logging_writer.py:48] [34976] accumulated_eval_time=2448.56, accumulated_logging_time=0.829827, accumulated_submission_time=7212.45, global_step=34976, preemption_count=0, score=7212.45, test/accuracy=0.985873, test/loss=0.0508462, test/mean_average_precision=0.256996, test/num_examples=43793, total_duration=9662.74, train/accuracy=0.993244, train/loss=0.0214461, train/mean_average_precision=0.598705, validation/accuracy=0.986752, validation/loss=0.0475409, validation/mean_average_precision=0.282839, validation/num_examples=43793
I0305 21:53:33.269034 140181048514304 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.174692764878273, loss=0.021022412925958633
I0305 21:53:53.810245 140181040121600 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.18419857323169708, loss=0.022958680987358093
I0305 21:54:14.468343 140181048514304 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.17696091532707214, loss=0.019846925511956215
I0305 21:54:35.074580 140181040121600 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.20394524931907654, loss=0.022069087252020836
I0305 21:54:56.136119 140181048514304 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.16646736860275269, loss=0.020394587889313698
I0305 21:55:16.733587 140181040121600 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.18310122191905975, loss=0.02392873354256153
I0305 21:55:37.409685 140181048514304 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.19050660729408264, loss=0.019894758239388466
I0305 21:55:58.074919 140181040121600 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.1649906486272812, loss=0.0205604899674654
I0305 21:56:18.700585 140181048514304 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.19202487170696259, loss=0.02229236252605915
I0305 21:56:39.242924 140181040121600 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.17264041304588318, loss=0.0203756894916296
I0305 21:57:00.030888 140181048514304 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.18514031171798706, loss=0.021679271012544632
I0305 21:57:20.731494 140181040121600 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.15209300816059113, loss=0.01954405941069126
I0305 21:57:28.269475 140323414672576 spec.py:321] Evaluating on the training split.
I0305 21:58:35.433091 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 21:58:37.390686 140323414672576 spec.py:349] Evaluating on the test split.
I0305 21:58:39.271499 140323414672576 submission_runner.py:469] Time since start: 9973.89s, 	Step: 36137, 	{'train/accuracy': 0.9931691884994507, 'train/loss': 0.02131800353527069, 'train/mean_average_precision': 0.5922817980373578, 'validation/accuracy': 0.9866794347763062, 'validation/loss': 0.04855526238679886, 'validation/mean_average_precision': 0.2767632809486688, 'validation/num_examples': 43793, 'test/accuracy': 0.9856637716293335, 'test/loss': 0.05191114917397499, 'test/mean_average_precision': 0.2584998156215234, 'test/num_examples': 43793, 'score': 7452.54484128952, 'total_duration': 9973.888405323029, 'accumulated_submission_time': 7452.54484128952, 'accumulated_eval_time': 2519.558069705963, 'accumulated_logging_time': 0.8511898517608643}
I0305 21:58:39.284500 140181048514304 logging_writer.py:48] [36137] accumulated_eval_time=2519.56, accumulated_logging_time=0.85119, accumulated_submission_time=7452.54, global_step=36137, preemption_count=0, score=7452.54, test/accuracy=0.985664, test/loss=0.0519111, test/mean_average_precision=0.2585, test/num_examples=43793, total_duration=9973.89, train/accuracy=0.993169, train/loss=0.021318, train/mean_average_precision=0.592282, validation/accuracy=0.986679, validation/loss=0.0485553, validation/mean_average_precision=0.276763, validation/num_examples=43793
I0305 21:58:52.619011 140181040121600 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.17682354152202606, loss=0.020048312842845917
I0305 21:59:13.257694 140181048514304 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.194456085562706, loss=0.022046387195587158
I0305 21:59:34.165995 140181040121600 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.1970241516828537, loss=0.02178383618593216
I0305 21:59:54.845104 140181048514304 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.18876640498638153, loss=0.01906931959092617
I0305 22:00:15.832279 140181040121600 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.1918700635433197, loss=0.02108032815158367
I0305 22:00:36.465725 140181048514304 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.17389652132987976, loss=0.0193302184343338
I0305 22:00:57.124506 140181040121600 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.18836885690689087, loss=0.01977033168077469
I0305 22:01:18.002475 140181048514304 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2069890797138214, loss=0.0198493804782629
I0305 22:01:39.202601 140181040121600 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.1883898824453354, loss=0.018991176038980484
I0305 22:02:00.055141 140181048514304 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.17305341362953186, loss=0.020635927096009254
I0305 22:02:20.659290 140181040121600 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.17103490233421326, loss=0.020641406998038292
I0305 22:02:39.297231 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:03:50.309572 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:03:52.297893 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:03:54.199696 140323414672576 submission_runner.py:469] Time since start: 10288.82s, 	Step: 37291, 	{'train/accuracy': 0.9934427738189697, 'train/loss': 0.020633360370993614, 'train/mean_average_precision': 0.6194492156065936, 'validation/accuracy': 0.9864614605903625, 'validation/loss': 0.04939074069261551, 'validation/mean_average_precision': 0.26909185397109614, 'validation/num_examples': 43793, 'test/accuracy': 0.9855445623397827, 'test/loss': 0.0527832917869091, 'test/mean_average_precision': 0.25639599212378517, 'test/num_examples': 43793, 'score': 7692.518120765686, 'total_duration': 10288.816576480865, 'accumulated_submission_time': 7692.518120765686, 'accumulated_eval_time': 2594.4603536129, 'accumulated_logging_time': 0.8734204769134521}
I0305 22:03:54.213439 140181048514304 logging_writer.py:48] [37291] accumulated_eval_time=2594.46, accumulated_logging_time=0.87342, accumulated_submission_time=7692.52, global_step=37291, preemption_count=0, score=7692.52, test/accuracy=0.985545, test/loss=0.0527833, test/mean_average_precision=0.256396, test/num_examples=43793, total_duration=10288.8, train/accuracy=0.993443, train/loss=0.0206334, train/mean_average_precision=0.619449, validation/accuracy=0.986461, validation/loss=0.0493907, validation/mean_average_precision=0.269092, validation/num_examples=43793
I0305 22:03:56.296399 140181040121600 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.20714379847049713, loss=0.023366723209619522
I0305 22:04:16.832214 140181048514304 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.18813925981521606, loss=0.021931113675236702
I0305 22:04:37.346839 140181040121600 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.19163532555103302, loss=0.018351037055253983
I0305 22:04:57.917926 140181048514304 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.16049665212631226, loss=0.018990851938724518
I0305 22:05:18.572572 140181040121600 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.1798197329044342, loss=0.018661102280020714
I0305 22:05:39.363886 140181048514304 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.18427938222885132, loss=0.018429482355713844
I0305 22:05:59.894241 140181040121600 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.24121515452861786, loss=0.02185969054698944
I0305 22:06:20.325968 140181048514304 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2216099500656128, loss=0.019797807559370995
I0305 22:06:40.946834 140181040121600 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.2050703912973404, loss=0.018779441714286804
I0305 22:07:01.681965 140181048514304 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.18799152970314026, loss=0.019396623596549034
I0305 22:07:22.379593 140181040121600 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.20445342361927032, loss=0.01887408271431923
I0305 22:07:43.028516 140181048514304 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.20774073898792267, loss=0.018388740718364716
I0305 22:07:54.293414 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:09:04.180707 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:09:06.103112 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:09:07.998307 140323414672576 submission_runner.py:469] Time since start: 10602.62s, 	Step: 38455, 	{'train/accuracy': 0.994021475315094, 'train/loss': 0.018586156889796257, 'train/mean_average_precision': 0.6616864014678188, 'validation/accuracy': 0.9865787625312805, 'validation/loss': 0.0503096804022789, 'validation/mean_average_precision': 0.27201386812874095, 'validation/num_examples': 43793, 'test/accuracy': 0.9857387542724609, 'test/loss': 0.05387747660279274, 'test/mean_average_precision': 0.25571954700427096, 'test/num_examples': 43793, 'score': 7932.557765960693, 'total_duration': 10602.61527967453, 'accumulated_submission_time': 7932.557765960693, 'accumulated_eval_time': 2668.1651582717896, 'accumulated_logging_time': 0.8964812755584717}
I0305 22:09:08.011745 140181040121600 logging_writer.py:48] [38455] accumulated_eval_time=2668.17, accumulated_logging_time=0.896481, accumulated_submission_time=7932.56, global_step=38455, preemption_count=0, score=7932.56, test/accuracy=0.985739, test/loss=0.0538775, test/mean_average_precision=0.25572, test/num_examples=43793, total_duration=10602.6, train/accuracy=0.994021, train/loss=0.0185862, train/mean_average_precision=0.661686, validation/accuracy=0.986579, validation/loss=0.0503097, validation/mean_average_precision=0.272014, validation/num_examples=43793
I0305 22:09:17.413031 140181048514304 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.22961212694644928, loss=0.016531789675354958
I0305 22:09:38.076194 140181040121600 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.18577350676059723, loss=0.01742088608443737
I0305 22:09:58.671806 140181048514304 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.23609520494937897, loss=0.01984143815934658
I0305 22:10:19.391307 140181040121600 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.2655463218688965, loss=0.01834181696176529
I0305 22:10:39.736087 140181048514304 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.1943332552909851, loss=0.01871560513973236
I0305 22:11:00.412310 140181040121600 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.24591752886772156, loss=0.01845090463757515
I0305 22:11:20.827301 140181048514304 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.19963690638542175, loss=0.01801973208785057
I0305 22:11:41.561397 140181040121600 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.21236099302768707, loss=0.02162020467221737
I0305 22:12:02.039823 140181048514304 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.20541532337665558, loss=0.018634101375937462
I0305 22:12:22.717730 140181040121600 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.20024870336055756, loss=0.017644425854086876
I0305 22:12:43.426298 140181048514304 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.21440286934375763, loss=0.018921367824077606
I0305 22:13:03.749840 140181040121600 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.221734881401062, loss=0.017596380785107613
I0305 22:13:08.028314 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:14:19.404994 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:14:21.525437 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:14:23.490694 140323414672576 submission_runner.py:469] Time since start: 10918.11s, 	Step: 39622, 	{'train/accuracy': 0.9940230250358582, 'train/loss': 0.01866920292377472, 'train/mean_average_precision': 0.6602977210903629, 'validation/accuracy': 0.9864342212677002, 'validation/loss': 0.05094640702009201, 'validation/mean_average_precision': 0.2754222485648608, 'validation/num_examples': 43793, 'test/accuracy': 0.9854232668876648, 'test/loss': 0.05489158630371094, 'test/mean_average_precision': 0.25126696776085883, 'test/num_examples': 43793, 'score': 8172.530004501343, 'total_duration': 10918.107706069946, 'accumulated_submission_time': 8172.530004501343, 'accumulated_eval_time': 2743.6274876594543, 'accumulated_logging_time': 0.9257495403289795}
I0305 22:14:23.503876 140181048514304 logging_writer.py:48] [39622] accumulated_eval_time=2743.63, accumulated_logging_time=0.92575, accumulated_submission_time=8172.53, global_step=39622, preemption_count=0, score=8172.53, test/accuracy=0.985423, test/loss=0.0548916, test/mean_average_precision=0.251267, test/num_examples=43793, total_duration=10918.1, train/accuracy=0.994023, train/loss=0.0186692, train/mean_average_precision=0.660298, validation/accuracy=0.986434, validation/loss=0.0509464, validation/mean_average_precision=0.275422, validation/num_examples=43793
I0305 22:14:39.894278 140181040121600 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2435312420129776, loss=0.019561180844902992
I0305 22:15:00.393444 140181048514304 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1981222927570343, loss=0.014245990663766861
I0305 22:15:21.047929 140181040121600 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.23017306625843048, loss=0.018479589372873306
I0305 22:15:41.778438 140181048514304 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.24125099182128906, loss=0.017480146139860153
I0305 22:16:02.712945 140181040121600 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2014332264661789, loss=0.01573488488793373
I0305 22:16:23.454237 140181048514304 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.19858697056770325, loss=0.018084164708852768
I0305 22:16:44.070423 140181040121600 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.20279636979103088, loss=0.016384022310376167
I0305 22:17:04.618870 140181048514304 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.20944559574127197, loss=0.017951391637325287
I0305 22:17:25.373186 140181040121600 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.2203768640756607, loss=0.016987396404147148
I0305 22:17:46.011164 140181048514304 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.20739078521728516, loss=0.01644071750342846
I0305 22:18:06.648159 140181040121600 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.21588295698165894, loss=0.01476366352289915
I0305 22:18:23.583796 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:19:34.011997 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:19:35.990703 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:19:38.074029 140323414672576 submission_runner.py:469] Time since start: 11232.69s, 	Step: 40783, 	{'train/accuracy': 0.9946579337120056, 'train/loss': 0.016698967665433884, 'train/mean_average_precision': 0.7004277545733029, 'validation/accuracy': 0.9862154126167297, 'validation/loss': 0.052649084478616714, 'validation/mean_average_precision': 0.2705861960885959, 'validation/num_examples': 43793, 'test/accuracy': 0.9854004979133606, 'test/loss': 0.056571584194898605, 'test/mean_average_precision': 0.25075871372373465, 'test/num_examples': 43793, 'score': 8412.570796728134, 'total_duration': 11232.69090127945, 'accumulated_submission_time': 8412.570796728134, 'accumulated_eval_time': 2818.1175332069397, 'accumulated_logging_time': 0.9481105804443359}
I0305 22:19:38.088387 140181048514304 logging_writer.py:48] [40783] accumulated_eval_time=2818.12, accumulated_logging_time=0.948111, accumulated_submission_time=8412.57, global_step=40783, preemption_count=0, score=8412.57, test/accuracy=0.9854, test/loss=0.0565716, test/mean_average_precision=0.250759, test/num_examples=43793, total_duration=11232.7, train/accuracy=0.994658, train/loss=0.016699, train/mean_average_precision=0.700428, validation/accuracy=0.986215, validation/loss=0.0526491, validation/mean_average_precision=0.270586, validation/num_examples=43793
I0305 22:19:41.748030 140181040121600 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.20754103362560272, loss=0.015965979546308517
I0305 22:20:02.368821 140181048514304 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.22097834944725037, loss=0.017141874879598618
I0305 22:20:22.999356 140181040121600 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.23451876640319824, loss=0.017066558822989464
I0305 22:20:43.787079 140181048514304 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.20229169726371765, loss=0.01607968658208847
I0305 22:21:04.331043 140181040121600 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.22595582902431488, loss=0.016438988968729973
I0305 22:21:24.951479 140181048514304 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.21700984239578247, loss=0.015292673371732235
I0305 22:21:45.770106 140181040121600 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.21977294981479645, loss=0.01679304987192154
I0305 22:22:06.285109 140181048514304 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.22274428606033325, loss=0.015838610008358955
I0305 22:22:26.735500 140181040121600 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2461760938167572, loss=0.015603614039719105
I0305 22:22:47.438719 140181048514304 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.25217849016189575, loss=0.01770411990582943
I0305 22:23:08.217666 140181040121600 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.22740478813648224, loss=0.01492855604737997
I0305 22:23:28.707490 140181048514304 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.21826227009296417, loss=0.014463715255260468
I0305 22:23:38.175082 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:24:53.036079 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:24:54.998730 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:24:56.929772 140323414672576 submission_runner.py:469] Time since start: 11551.55s, 	Step: 41947, 	{'train/accuracy': 0.9945183992385864, 'train/loss': 0.01701134443283081, 'train/mean_average_precision': 0.6892549028018587, 'validation/accuracy': 0.9862669706344604, 'validation/loss': 0.05340876057744026, 'validation/mean_average_precision': 0.2703639747734576, 'validation/num_examples': 43793, 'test/accuracy': 0.9853967428207397, 'test/loss': 0.05744592472910881, 'test/mean_average_precision': 0.24805050184159175, 'test/num_examples': 43793, 'score': 8652.619043588638, 'total_duration': 11551.546646595001, 'accumulated_submission_time': 8652.619043588638, 'accumulated_eval_time': 2896.8720409870148, 'accumulated_logging_time': 0.9718317985534668}
I0305 22:24:56.943021 140181040121600 logging_writer.py:48] [41947] accumulated_eval_time=2896.87, accumulated_logging_time=0.971832, accumulated_submission_time=8652.62, global_step=41947, preemption_count=0, score=8652.62, test/accuracy=0.985397, test/loss=0.0574459, test/mean_average_precision=0.248051, test/num_examples=43793, total_duration=11551.5, train/accuracy=0.994518, train/loss=0.0170113, train/mean_average_precision=0.689255, validation/accuracy=0.986267, validation/loss=0.0534088, validation/mean_average_precision=0.270364, validation/num_examples=43793
I0305 22:25:08.079148 140181048514304 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.22327759861946106, loss=0.015297459438443184
I0305 22:25:28.544815 140181040121600 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.24492323398590088, loss=0.012944843620061874
I0305 22:25:49.501485 140181048514304 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.24382656812667847, loss=0.015550509095191956
I0305 22:26:10.146496 140181040121600 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.2377525418996811, loss=0.014910134486854076
I0305 22:26:30.584623 140181048514304 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2539493143558502, loss=0.014381973072886467
I0305 22:26:51.125750 140181040121600 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2500419318675995, loss=0.01536557637155056
I0305 22:27:11.852260 140181048514304 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.26183539628982544, loss=0.017430316656827927
I0305 22:27:32.292066 140181040121600 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.23859988152980804, loss=0.01530885137617588
I0305 22:27:52.800610 140181048514304 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.22356976568698883, loss=0.014260736294090748
I0305 22:28:13.238767 140181040121600 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.23543819785118103, loss=0.013356726616621017
I0305 22:28:33.839218 140181048514304 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.24907082319259644, loss=0.015556924045085907
I0305 22:28:54.257908 140181040121600 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.26095694303512573, loss=0.01391951646655798
I0305 22:28:57.119040 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:30:04.813020 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:30:06.763849 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:30:08.667401 140323414672576 submission_runner.py:469] Time since start: 11863.28s, 	Step: 43115, 	{'train/accuracy': 0.9949214458465576, 'train/loss': 0.01557965762913227, 'train/mean_average_precision': 0.7149288577241606, 'validation/accuracy': 0.9862214922904968, 'validation/loss': 0.05527900904417038, 'validation/mean_average_precision': 0.2693493371293331, 'validation/num_examples': 43793, 'test/accuracy': 0.985284686088562, 'test/loss': 0.05919085070490837, 'test/mean_average_precision': 0.24818950969380496, 'test/num_examples': 43793, 'score': 8892.75588274002, 'total_duration': 11863.284273386002, 'accumulated_submission_time': 8892.75588274002, 'accumulated_eval_time': 2968.4202122688293, 'accumulated_logging_time': 0.9942829608917236}
I0305 22:30:08.680808 140181048514304 logging_writer.py:48] [43115] accumulated_eval_time=2968.42, accumulated_logging_time=0.994283, accumulated_submission_time=8892.76, global_step=43115, preemption_count=0, score=8892.76, test/accuracy=0.985285, test/loss=0.0591909, test/mean_average_precision=0.24819, test/num_examples=43793, total_duration=11863.3, train/accuracy=0.994921, train/loss=0.0155797, train/mean_average_precision=0.714929, validation/accuracy=0.986221, validation/loss=0.055279, validation/mean_average_precision=0.269349, validation/num_examples=43793
I0305 22:30:26.321537 140181040121600 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.2542487680912018, loss=0.015829317271709442
I0305 22:30:46.942344 140181048514304 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.2711554169654846, loss=0.01487017422914505
I0305 22:31:07.776799 140181040121600 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.2480693906545639, loss=0.013768903911113739
I0305 22:31:28.333738 140181048514304 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3084452152252197, loss=0.016578221693634987
I0305 22:31:48.824300 140181040121600 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.26195037364959717, loss=0.013055160641670227
I0305 22:32:09.267967 140181048514304 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.28774845600128174, loss=0.013374421745538712
I0305 22:32:29.678330 140181040121600 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.28010043501853943, loss=0.015160681679844856
I0305 22:32:50.020719 140181048514304 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.2720446288585663, loss=0.016293788328766823
I0305 22:33:10.598037 140181040121600 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2754843533039093, loss=0.015937568619847298
I0305 22:33:30.952661 140181048514304 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.24934236705303192, loss=0.012420992366969585
I0305 22:33:51.656164 140181040121600 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2742459774017334, loss=0.014204157516360283
I0305 22:34:08.742613 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:35:19.153730 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:35:21.153411 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:35:23.086517 140323414672576 submission_runner.py:469] Time since start: 12177.70s, 	Step: 44284, 	{'train/accuracy': 0.9942291975021362, 'train/loss': 0.017625173553824425, 'train/mean_average_precision': 0.6696050000635414, 'validation/accuracy': 0.9860242605209351, 'validation/loss': 0.056895118206739426, 'validation/mean_average_precision': 0.2644042083742525, 'validation/num_examples': 43793, 'test/accuracy': 0.9852294921875, 'test/loss': 0.06100516393780708, 'test/mean_average_precision': 0.24425035979295948, 'test/num_examples': 43793, 'score': 9132.779817581177, 'total_duration': 12177.703366994858, 'accumulated_submission_time': 9132.779817581177, 'accumulated_eval_time': 3042.7639050483704, 'accumulated_logging_time': 1.0175633430480957}
I0305 22:35:23.100244 140181048514304 logging_writer.py:48] [44284] accumulated_eval_time=3042.76, accumulated_logging_time=1.01756, accumulated_submission_time=9132.78, global_step=44284, preemption_count=0, score=9132.78, test/accuracy=0.985229, test/loss=0.0610052, test/mean_average_precision=0.24425, test/num_examples=43793, total_duration=12177.7, train/accuracy=0.994229, train/loss=0.0176252, train/mean_average_precision=0.669605, validation/accuracy=0.986024, validation/loss=0.0568951, validation/mean_average_precision=0.264404, validation/num_examples=43793
I0305 22:35:26.566054 140181040121600 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2873942255973816, loss=0.016026701778173447
I0305 22:35:47.143886 140181048514304 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2698107659816742, loss=0.013995878398418427
I0305 22:36:07.937902 140181040121600 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2517004609107971, loss=0.015039194375276566
I0305 22:36:28.563337 140181048514304 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.24873043596744537, loss=0.012722575105726719
I0305 22:36:49.159587 140181040121600 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2349286824464798, loss=0.012964524328708649
I0305 22:37:09.757692 140181048514304 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.25300711393356323, loss=0.013153903186321259
I0305 22:37:30.411602 140181040121600 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2525216042995453, loss=0.01261873822659254
I0305 22:37:51.054158 140181048514304 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2838246822357178, loss=0.013664829544723034
I0305 22:38:11.584883 140181040121600 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.26395416259765625, loss=0.01326566282659769
I0305 22:38:32.158653 140181048514304 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.28249362111091614, loss=0.011941560544073582
I0305 22:38:52.919211 140181040121600 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.2621665596961975, loss=0.014346403069794178
I0305 22:39:13.633840 140181048514304 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.2310827374458313, loss=0.012565592303872108
I0305 22:39:23.176998 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:40:35.110192 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:40:37.081307 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:40:38.983295 140323414672576 submission_runner.py:469] Time since start: 12493.60s, 	Step: 45447, 	{'train/accuracy': 0.9956562519073486, 'train/loss': 0.013483216054737568, 'train/mean_average_precision': 0.7672508280689583, 'validation/accuracy': 0.9859856963157654, 'validation/loss': 0.05761616304516792, 'validation/mean_average_precision': 0.26346808498597984, 'validation/num_examples': 43793, 'test/accuracy': 0.985129714012146, 'test/loss': 0.061833687126636505, 'test/mean_average_precision': 0.24306629866618418, 'test/num_examples': 43793, 'score': 9372.818446397781, 'total_duration': 12493.600263834, 'accumulated_submission_time': 9372.818446397781, 'accumulated_eval_time': 3118.5701117515564, 'accumulated_logging_time': 1.040273904800415}
I0305 22:40:38.997286 140181040121600 logging_writer.py:48] [45447] accumulated_eval_time=3118.57, accumulated_logging_time=1.04027, accumulated_submission_time=9372.82, global_step=45447, preemption_count=0, score=9372.82, test/accuracy=0.98513, test/loss=0.0618337, test/mean_average_precision=0.243066, test/num_examples=43793, total_duration=12493.6, train/accuracy=0.995656, train/loss=0.0134832, train/mean_average_precision=0.767251, validation/accuracy=0.985986, validation/loss=0.0576162, validation/mean_average_precision=0.263468, validation/num_examples=43793
I0305 22:40:50.039551 140181048514304 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.23115505278110504, loss=0.012142975814640522
I0305 22:41:10.394684 140181040121600 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.28124523162841797, loss=0.012983364053070545
I0305 22:41:31.071903 140181048514304 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.30169185996055603, loss=0.014684543944895267
I0305 22:41:51.457956 140181040121600 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.25625497102737427, loss=0.013898498378694057
I0305 22:42:12.358785 140181048514304 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2632588744163513, loss=0.01360231451690197
I0305 22:42:32.803450 140181040121600 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.24760307371616364, loss=0.011547574773430824
I0305 22:42:53.229821 140181048514304 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2854613959789276, loss=0.01403187494724989
I0305 22:43:13.719965 140181040121600 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.28363940119743347, loss=0.012243181467056274
I0305 22:43:34.214179 140181048514304 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2731764316558838, loss=0.010830453597009182
I0305 22:43:54.737464 140181040121600 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3087390065193176, loss=0.01369237620383501
I0305 22:44:15.093238 140181048514304 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.2722267806529999, loss=0.01177457720041275
I0305 22:44:35.888998 140181040121600 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2961445152759552, loss=0.012398279272019863
I0305 22:44:39.021926 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:45:49.961850 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:45:51.962791 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:45:53.894021 140323414672576 submission_runner.py:469] Time since start: 12808.51s, 	Step: 46616, 	{'train/accuracy': 0.9953954219818115, 'train/loss': 0.014111501164734364, 'train/mean_average_precision': 0.746546922507489, 'validation/accuracy': 0.9858711957931519, 'validation/loss': 0.05887119099497795, 'validation/mean_average_precision': 0.2642048072528853, 'validation/num_examples': 43793, 'test/accuracy': 0.9849578142166138, 'test/loss': 0.06313353031873703, 'test/mean_average_precision': 0.24235539957882893, 'test/num_examples': 43793, 'score': 9612.8030667305, 'total_duration': 12808.510948896408, 'accumulated_submission_time': 9612.8030667305, 'accumulated_eval_time': 3193.4420721530914, 'accumulated_logging_time': 1.0644874572753906}
I0305 22:45:53.908259 140181048514304 logging_writer.py:48] [46616] accumulated_eval_time=3193.44, accumulated_logging_time=1.06449, accumulated_submission_time=9612.8, global_step=46616, preemption_count=0, score=9612.8, test/accuracy=0.984958, test/loss=0.0631335, test/mean_average_precision=0.242355, test/num_examples=43793, total_duration=12808.5, train/accuracy=0.995395, train/loss=0.0141115, train/mean_average_precision=0.746547, validation/accuracy=0.985871, validation/loss=0.0588712, validation/mean_average_precision=0.264205, validation/num_examples=43793
I0305 22:46:11.785943 140181040121600 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.29949289560317993, loss=0.013204354792833328
I0305 22:46:32.556314 140181048514304 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.24852916598320007, loss=0.01137569360435009
I0305 22:46:52.962986 140181040121600 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.28573110699653625, loss=0.013052091933786869
I0305 22:47:13.415155 140181048514304 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.26776671409606934, loss=0.013569002039730549
I0305 22:47:34.135432 140181040121600 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.26450714468955994, loss=0.013038337230682373
I0305 22:47:54.651168 140181048514304 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.29164639115333557, loss=0.01437618862837553
I0305 22:48:15.365066 140181040121600 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.25617536902427673, loss=0.011412306688725948
I0305 22:48:35.936488 140181048514304 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.25971293449401855, loss=0.012057574465870857
I0305 22:48:56.694120 140181040121600 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2720942199230194, loss=0.01201997697353363
I0305 22:49:17.461124 140181048514304 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2671445310115814, loss=0.012276439927518368
I0305 22:49:37.921612 140181040121600 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.23753714561462402, loss=0.011659793555736542
I0305 22:49:53.914895 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:51:06.325140 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:51:08.295126 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:51:10.324163 140323414672576 submission_runner.py:469] Time since start: 13124.94s, 	Step: 47780, 	{'train/accuracy': 0.9962916374206543, 'train/loss': 0.011886448599398136, 'train/mean_average_precision': 0.7915379062583754, 'validation/accuracy': 0.9857478141784668, 'validation/loss': 0.05954267457127571, 'validation/mean_average_precision': 0.2575658515725678, 'validation/num_examples': 43793, 'test/accuracy': 0.9848373532295227, 'test/loss': 0.06399857252836227, 'test/mean_average_precision': 0.24035700200587892, 'test/num_examples': 43793, 'score': 9852.771946907043, 'total_duration': 13124.94112110138, 'accumulated_submission_time': 9852.771946907043, 'accumulated_eval_time': 3269.851238965988, 'accumulated_logging_time': 1.088731050491333}
I0305 22:51:10.339028 140181048514304 logging_writer.py:48] [47780] accumulated_eval_time=3269.85, accumulated_logging_time=1.08873, accumulated_submission_time=9852.77, global_step=47780, preemption_count=0, score=9852.77, test/accuracy=0.984837, test/loss=0.0639986, test/mean_average_precision=0.240357, test/num_examples=43793, total_duration=13124.9, train/accuracy=0.996292, train/loss=0.0118864, train/mean_average_precision=0.791538, validation/accuracy=0.985748, validation/loss=0.0595427, validation/mean_average_precision=0.257566, validation/num_examples=43793
I0305 22:51:14.580107 140181040121600 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.26646000146865845, loss=0.011251203715801239
I0305 22:51:34.825301 140181048514304 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2576814889907837, loss=0.012135986238718033
I0305 22:51:55.459094 140181040121600 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2478446364402771, loss=0.010923909954726696
I0305 22:52:16.282624 140181048514304 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.23726792633533478, loss=0.010766004212200642
I0305 22:52:36.988122 140181040121600 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.28188660740852356, loss=0.011682122014462948
I0305 22:52:57.986596 140181048514304 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2561075687408447, loss=0.011192185804247856
I0305 22:53:18.831307 140181040121600 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.29409825801849365, loss=0.012753644026815891
I0305 22:53:39.492531 140181048514304 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.26072433590888977, loss=0.010424411855638027
I0305 22:54:00.114777 140181040121600 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.29297053813934326, loss=0.011115871369838715
I0305 22:54:20.995234 140181048514304 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.26947665214538574, loss=0.013255526311695576
I0305 22:54:41.626318 140181040121600 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.26851406693458557, loss=0.01233687438070774
I0305 22:55:02.296364 140181048514304 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.29969456791877747, loss=0.012018510140478611
I0305 22:55:10.387471 140323414672576 spec.py:321] Evaluating on the training split.
I0305 22:56:17.611018 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 22:56:19.583058 140323414672576 spec.py:349] Evaluating on the test split.
I0305 22:56:21.629549 140323414672576 submission_runner.py:469] Time since start: 13436.25s, 	Step: 48940, 	{'train/accuracy': 0.9955523610115051, 'train/loss': 0.013553488999605179, 'train/mean_average_precision': 0.7524136932291805, 'validation/accuracy': 0.9858115315437317, 'validation/loss': 0.06012628972530365, 'validation/mean_average_precision': 0.2592776481451712, 'validation/num_examples': 43793, 'test/accuracy': 0.9849308729171753, 'test/loss': 0.06443016231060028, 'test/mean_average_precision': 0.24031841545506663, 'test/num_examples': 43793, 'score': 10092.78107380867, 'total_duration': 13436.246411085129, 'accumulated_submission_time': 10092.78107380867, 'accumulated_eval_time': 3341.093116044998, 'accumulated_logging_time': 1.1130468845367432}
I0305 22:56:21.644542 140181040121600 logging_writer.py:48] [48940] accumulated_eval_time=3341.09, accumulated_logging_time=1.11305, accumulated_submission_time=10092.8, global_step=48940, preemption_count=0, score=10092.8, test/accuracy=0.984931, test/loss=0.0644302, test/mean_average_precision=0.240318, test/num_examples=43793, total_duration=13436.2, train/accuracy=0.995552, train/loss=0.0135535, train/mean_average_precision=0.752414, validation/accuracy=0.985812, validation/loss=0.0601263, validation/mean_average_precision=0.259278, validation/num_examples=43793
I0305 22:56:33.902632 140181048514304 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.2719002068042755, loss=0.012240109033882618
I0305 22:56:54.317694 140181040121600 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.2832491397857666, loss=0.012877872213721275
I0305 22:57:14.610689 140181048514304 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2783105671405792, loss=0.01147957518696785
I0305 22:57:35.214272 140181040121600 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.2680484354496002, loss=0.011149940080940723
I0305 22:57:55.972914 140181048514304 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.30223146080970764, loss=0.012479362078011036
I0305 22:58:16.655181 140181040121600 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2852358818054199, loss=0.011303874664008617
I0305 22:58:37.667460 140181048514304 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.25814008712768555, loss=0.011330872774124146
I0305 22:58:58.629303 140181040121600 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3048016428947449, loss=0.011887401342391968
I0305 22:59:19.593735 140181048514304 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.2727547585964203, loss=0.011207245290279388
I0305 22:59:40.480300 140181040121600 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.2855831980705261, loss=0.011822307482361794
I0305 23:00:01.115911 140181048514304 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2876865267753601, loss=0.012470434419810772
I0305 23:00:21.560673 140181040121600 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.27903038263320923, loss=0.011586346663534641
I0305 23:00:21.771949 140323414672576 spec.py:321] Evaluating on the training split.
I0305 23:01:34.941316 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 23:01:36.943299 140323414672576 spec.py:349] Evaluating on the test split.
I0305 23:01:38.902045 140323414672576 submission_runner.py:469] Time since start: 13753.52s, 	Step: 50102, 	{'train/accuracy': 0.9965635538101196, 'train/loss': 0.01125608291476965, 'train/mean_average_precision': 0.7973499884337357, 'validation/accuracy': 0.9858399629592896, 'validation/loss': 0.06023498624563217, 'validation/mean_average_precision': 0.2625930456204962, 'validation/num_examples': 43793, 'test/accuracy': 0.9849430918693542, 'test/loss': 0.06458736956119537, 'test/mean_average_precision': 0.24086352112886147, 'test/num_examples': 43793, 'score': 10332.87071609497, 'total_duration': 13753.519015789032, 'accumulated_submission_time': 10332.87071609497, 'accumulated_eval_time': 3418.2231209278107, 'accumulated_logging_time': 1.1377451419830322}
I0305 23:01:38.916814 140181048514304 logging_writer.py:48] [50102] accumulated_eval_time=3418.22, accumulated_logging_time=1.13775, accumulated_submission_time=10332.9, global_step=50102, preemption_count=0, score=10332.9, test/accuracy=0.984943, test/loss=0.0645874, test/mean_average_precision=0.240864, test/num_examples=43793, total_duration=13753.5, train/accuracy=0.996564, train/loss=0.0112561, train/mean_average_precision=0.79735, validation/accuracy=0.98584, validation/loss=0.060235, validation/mean_average_precision=0.262593, validation/num_examples=43793
I0305 23:01:59.661242 140181040121600 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.25047844648361206, loss=0.010343152098357677
I0305 23:02:20.675065 140181048514304 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.28680479526519775, loss=0.0145647544413805
I0305 23:02:41.525928 140181040121600 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.3000316917896271, loss=0.013238448649644852
I0305 23:03:02.425880 140181048514304 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.2965540587902069, loss=0.012433702126145363
I0305 23:03:23.254789 140181040121600 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.30084869265556335, loss=0.012109325267374516
I0305 23:03:43.945232 140181048514304 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.28839150071144104, loss=0.009371167048811913
I0305 23:04:04.285461 140181040121600 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.30814090371131897, loss=0.011002106592059135
I0305 23:04:24.864566 140181048514304 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.26894912123680115, loss=0.010833135806024075
I0305 23:04:45.492578 140181040121600 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3155088424682617, loss=0.012036959640681744
I0305 23:05:06.345521 140181048514304 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.24972952902317047, loss=0.010053252801299095
I0305 23:05:27.187384 140181040121600 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.2901431620121002, loss=0.011969801038503647
I0305 23:05:38.919410 140323414672576 spec.py:321] Evaluating on the training split.
I0305 23:06:50.174639 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 23:06:52.122430 140323414672576 spec.py:349] Evaluating on the test split.
I0305 23:06:54.035689 140323414672576 submission_runner.py:469] Time since start: 14068.65s, 	Step: 51257, 	{'train/accuracy': 0.9961840510368347, 'train/loss': 0.012022496201097965, 'train/mean_average_precision': 0.7901851216467538, 'validation/accuracy': 0.9857761859893799, 'validation/loss': 0.06021349877119064, 'validation/mean_average_precision': 0.26051661924330416, 'validation/num_examples': 43793, 'test/accuracy': 0.9849784970283508, 'test/loss': 0.06450852006673813, 'test/mean_average_precision': 0.24150207427599157, 'test/num_examples': 43793, 'score': 10572.832387447357, 'total_duration': 14068.652569293976, 'accumulated_submission_time': 10572.832387447357, 'accumulated_eval_time': 3493.33921957016, 'accumulated_logging_time': 1.1626899242401123}
I0305 23:06:54.050505 140181048514304 logging_writer.py:48] [51257] accumulated_eval_time=3493.34, accumulated_logging_time=1.16269, accumulated_submission_time=10572.8, global_step=51257, preemption_count=0, score=10572.8, test/accuracy=0.984978, test/loss=0.0645085, test/mean_average_precision=0.241502, test/num_examples=43793, total_duration=14068.7, train/accuracy=0.996184, train/loss=0.0120225, train/mean_average_precision=0.790185, validation/accuracy=0.985776, validation/loss=0.0602135, validation/mean_average_precision=0.260517, validation/num_examples=43793
I0305 23:07:02.983669 140181040121600 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.28935253620147705, loss=0.012981588020920753
I0305 23:07:23.728111 140181048514304 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.29126250743865967, loss=0.010974623262882233
I0305 23:07:44.352003 140181040121600 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.26997777819633484, loss=0.010080820880830288
I0305 23:08:04.832727 140181048514304 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.26115670800209045, loss=0.01217571459710598
I0305 23:08:25.490398 140181040121600 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.28286442160606384, loss=0.010638059116899967
I0305 23:08:46.106923 140181048514304 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3008701801300049, loss=0.013211368583142757
I0305 23:09:06.922207 140181040121600 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2971245050430298, loss=0.01138797216117382
I0305 23:09:27.343899 140181048514304 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.26884424686431885, loss=0.01207889523357153
I0305 23:09:47.959638 140181040121600 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.28812554478645325, loss=0.011880943551659584
I0305 23:10:08.689254 140181048514304 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.28167724609375, loss=0.011650723405182362
I0305 23:10:29.490344 140181040121600 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.26655998826026917, loss=0.011821655556559563
I0305 23:10:49.965285 140181048514304 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.26326417922973633, loss=0.010535596869885921
I0305 23:10:54.044643 140323414672576 spec.py:321] Evaluating on the training split.
I0305 23:12:02.252790 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 23:12:04.213184 140323414672576 spec.py:349] Evaluating on the test split.
I0305 23:12:06.116569 140323414672576 submission_runner.py:469] Time since start: 14380.73s, 	Step: 52421, 	{'train/accuracy': 0.9964466691017151, 'train/loss': 0.011446944437921047, 'train/mean_average_precision': 0.7970200857495544, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026484817266464, 'validation/mean_average_precision': 0.2594645982827487, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24159476956803602, 'test/num_examples': 43793, 'score': 10812.784964561462, 'total_duration': 14380.733457565308, 'accumulated_submission_time': 10812.784964561462, 'accumulated_eval_time': 3565.4109737873077, 'accumulated_logging_time': 1.1869869232177734}
I0305 23:12:06.131165 140181040121600 logging_writer.py:48] [52421] accumulated_eval_time=3565.41, accumulated_logging_time=1.18699, accumulated_submission_time=10812.8, global_step=52421, preemption_count=0, score=10812.8, test/accuracy=0.984977, test/loss=0.0645697, test/mean_average_precision=0.241595, test/num_examples=43793, total_duration=14380.7, train/accuracy=0.996447, train/loss=0.0114469, train/mean_average_precision=0.79702, validation/accuracy=0.985823, validation/loss=0.0602648, validation/mean_average_precision=0.259465, validation/num_examples=43793
I0305 23:12:22.615704 140181048514304 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.25956791639328003, loss=0.011401137337088585
I0305 23:12:43.094407 140181040121600 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.26837849617004395, loss=0.012865704484283924
I0305 23:13:03.964068 140181048514304 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2887813448905945, loss=0.01188505720347166
I0305 23:13:24.732350 140181040121600 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3028259575366974, loss=0.012982743792235851
I0305 23:13:45.449350 140181048514304 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.2583436071872711, loss=0.011386997066438198
I0305 23:14:06.165569 140181040121600 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.2602589428424835, loss=0.011208638548851013
I0305 23:14:26.897462 140181048514304 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2851794958114624, loss=0.01146427821367979
I0305 23:14:47.561444 140181040121600 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.26469796895980835, loss=0.010899826884269714
I0305 23:15:08.164841 140181048514304 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2744050920009613, loss=0.011170821264386177
I0305 23:15:28.877909 140181040121600 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.2943834066390991, loss=0.012521321885287762
I0305 23:15:49.758325 140181048514304 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.2667115330696106, loss=0.012765349820256233
I0305 23:16:06.168975 140323414672576 spec.py:321] Evaluating on the training split.
I0305 23:17:21.378547 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 23:17:23.327016 140323414672576 spec.py:349] Evaluating on the test split.
I0305 23:17:25.267863 140323414672576 submission_runner.py:469] Time since start: 14699.88s, 	Step: 53580, 	{'train/accuracy': 0.9964762926101685, 'train/loss': 0.011372756212949753, 'train/mean_average_precision': 0.7938330895636596, 'validation/accuracy': 0.9858235120773315, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.2595437817929092, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24158163143192424, 'test/num_examples': 43793, 'score': 11052.78490781784, 'total_duration': 14699.884835481644, 'accumulated_submission_time': 11052.78490781784, 'accumulated_eval_time': 3644.5097727775574, 'accumulated_logging_time': 1.2112836837768555}
I0305 23:17:25.282649 140181040121600 logging_writer.py:48] [53580] accumulated_eval_time=3644.51, accumulated_logging_time=1.21128, accumulated_submission_time=11052.8, global_step=53580, preemption_count=0, score=11052.8, test/accuracy=0.984977, test/loss=0.0645697, test/mean_average_precision=0.241582, test/num_examples=43793, total_duration=14699.9, train/accuracy=0.996476, train/loss=0.0113728, train/mean_average_precision=0.793833, validation/accuracy=0.985824, validation/loss=0.0602649, validation/mean_average_precision=0.259544, validation/num_examples=43793
I0305 23:17:29.692757 140181048514304 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.26219573616981506, loss=0.011567236855626106
I0305 23:17:50.476346 140181040121600 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2596491873264313, loss=0.011490615084767342
I0305 23:18:11.264881 140181048514304 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.274786114692688, loss=0.011172818019986153
I0305 23:18:31.963578 140181040121600 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.337816059589386, loss=0.014374802820384502
I0305 23:18:52.693265 140181048514304 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.28040534257888794, loss=0.012805220670998096
I0305 23:19:13.359204 140181040121600 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3332003355026245, loss=0.011109412647783756
I0305 23:19:34.099390 140181048514304 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.29100000858306885, loss=0.012615877203643322
I0305 23:19:55.053593 140181040121600 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.25383681058883667, loss=0.011468378826975822
I0305 23:20:15.773103 140181048514304 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.254673033952713, loss=0.011326432228088379
I0305 23:20:36.031087 140181040121600 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3138374984264374, loss=0.013398637063801289
I0305 23:20:56.853163 140181048514304 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.26159265637397766, loss=0.00927223451435566
I0305 23:21:17.648507 140181040121600 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.28773587942123413, loss=0.01181851327419281
I0305 23:21:25.324429 140323414672576 spec.py:321] Evaluating on the training split.
I0305 23:22:34.555999 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 23:22:36.549980 140323414672576 spec.py:349] Evaluating on the test split.
I0305 23:22:38.509423 140323414672576 submission_runner.py:469] Time since start: 15013.13s, 	Step: 54738, 	{'train/accuracy': 0.9961983561515808, 'train/loss': 0.01190643198788166, 'train/mean_average_precision': 0.7919387023430402, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.25964303035746966, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24176569701953918, 'test/num_examples': 43793, 'score': 11292.788722038269, 'total_duration': 15013.126317024231, 'accumulated_submission_time': 11292.788722038269, 'accumulated_eval_time': 3717.6946024894714, 'accumulated_logging_time': 1.235180377960205}
I0305 23:22:38.524862 140181048514304 logging_writer.py:48] [54738] accumulated_eval_time=3717.69, accumulated_logging_time=1.23518, accumulated_submission_time=11292.8, global_step=54738, preemption_count=0, score=11292.8, test/accuracy=0.984977, test/loss=0.0645697, test/mean_average_precision=0.241766, test/num_examples=43793, total_duration=15013.1, train/accuracy=0.996198, train/loss=0.0119064, train/mean_average_precision=0.791939, validation/accuracy=0.985823, validation/loss=0.0602649, validation/mean_average_precision=0.259643, validation/num_examples=43793
I0305 23:22:51.658124 140181040121600 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.26046156883239746, loss=0.01094160508364439
I0305 23:23:12.231652 140181048514304 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.27300405502319336, loss=0.011487331241369247
I0305 23:23:33.079687 140181040121600 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2801308333873749, loss=0.0109514519572258
I0305 23:23:53.771264 140181048514304 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2568112015724182, loss=0.01176523882895708
I0305 23:24:14.630207 140181040121600 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2668275833129883, loss=0.01129485946148634
I0305 23:24:35.240711 140181048514304 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3529178500175476, loss=0.012580104172229767
I0305 23:24:55.843867 140181040121600 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.2528322637081146, loss=0.010207458399236202
I0305 23:25:16.576215 140181048514304 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.3207952082157135, loss=0.012194366194307804
I0305 23:25:37.169178 140181040121600 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2663242518901825, loss=0.010656068101525307
I0305 23:25:57.881023 140181048514304 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.2638300061225891, loss=0.011605381034314632
I0305 23:26:18.534770 140181040121600 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.26343080401420593, loss=0.011860507540404797
I0305 23:26:38.671415 140323414672576 spec.py:321] Evaluating on the training split.
I0305 23:27:52.352993 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 23:27:54.395460 140323414672576 spec.py:349] Evaluating on the test split.
I0305 23:27:56.323511 140323414672576 submission_runner.py:469] Time since start: 15330.94s, 	Step: 55897, 	{'train/accuracy': 0.9962978363037109, 'train/loss': 0.011785424314439297, 'train/mean_average_precision': 0.7809687030648927, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.2595002275369526, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24182635362297308, 'test/num_examples': 43793, 'score': 11532.894360542297, 'total_duration': 15330.940398931503, 'accumulated_submission_time': 11532.894360542297, 'accumulated_eval_time': 3795.3465275764465, 'accumulated_logging_time': 1.261643648147583}
I0305 23:27:56.338723 140181048514304 logging_writer.py:48] [55897] accumulated_eval_time=3795.35, accumulated_logging_time=1.26164, accumulated_submission_time=11532.9, global_step=55897, preemption_count=0, score=11532.9, test/accuracy=0.984977, test/loss=0.0645697, test/mean_average_precision=0.241826, test/num_examples=43793, total_duration=15330.9, train/accuracy=0.996298, train/loss=0.0117854, train/mean_average_precision=0.780969, validation/accuracy=0.985823, validation/loss=0.0602649, validation/mean_average_precision=0.2595, validation/num_examples=43793
I0305 23:27:57.207123 140181040121600 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.27418068051338196, loss=0.011586830951273441
I0305 23:28:17.852464 140181048514304 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2987515926361084, loss=0.011799318715929985
I0305 23:28:38.600153 140181040121600 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2975339889526367, loss=0.011499623768031597
I0305 23:28:59.358606 140181048514304 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.27475985884666443, loss=0.01399227324873209
I0305 23:29:20.372072 140181040121600 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.2744196653366089, loss=0.011772219091653824
I0305 23:29:41.025733 140181048514304 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.26576364040374756, loss=0.0116807222366333
I0305 23:30:01.753282 140181040121600 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.24219226837158203, loss=0.010906576178967953
I0305 23:30:22.728559 140181048514304 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2740066349506378, loss=0.01253442745655775
I0305 23:30:43.672201 140181040121600 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.249949112534523, loss=0.01030102651566267
I0305 23:31:04.482108 140181048514304 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.26762285828590393, loss=0.011756885796785355
I0305 23:31:24.949482 140181040121600 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.25751397013664246, loss=0.01043320819735527
I0305 23:31:45.572480 140181048514304 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.27831393480300903, loss=0.012530929408967495
I0305 23:31:56.466161 140323414672576 spec.py:321] Evaluating on the training split.
I0305 23:33:06.965105 140323414672576 spec.py:333] Evaluating on the validation split.
I0305 23:33:08.958869 140323414672576 spec.py:349] Evaluating on the test split.
I0305 23:33:10.885395 140323414672576 submission_runner.py:469] Time since start: 15645.50s, 	Step: 57054, 	{'train/accuracy': 0.9963226318359375, 'train/loss': 0.011711775325238705, 'train/mean_average_precision': 0.7877331391676641, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.2594412977473483, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.2416251711275031, 'test/num_examples': 43793, 'score': 11772.982586860657, 'total_duration': 15645.50228214264, 'accumulated_submission_time': 11772.982586860657, 'accumulated_eval_time': 3869.7655885219574, 'accumulated_logging_time': 1.2866084575653076}
I0305 23:33:10.900407 140181040121600 logging_writer.py:48] [57054] accumulated_eval_time=3869.77, accumulated_logging_time=1.28661, accumulated_submission_time=11773, global_step=57054, preemption_count=0, score=11773, test/accuracy=0.984977, test/loss=0.0645697, test/mean_average_precision=0.241625, test/num_examples=43793, total_duration=15645.5, train/accuracy=0.996323, train/loss=0.0117118, train/mean_average_precision=0.787733, validation/accuracy=0.985823, validation/loss=0.0602649, validation/mean_average_precision=0.259441, validation/num_examples=43793
I0305 23:33:20.716828 140181048514304 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.27361106872558594, loss=0.011990467086434364
I0305 23:33:41.450660 140181040121600 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2512030601501465, loss=0.012071124278008938
I0305 23:34:02.074296 140181048514304 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3013911843299866, loss=0.013503501191735268
I0305 23:34:22.705513 140181040121600 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3047277629375458, loss=0.012478578835725784
I0305 23:34:43.681937 140181048514304 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.29347583651542664, loss=0.011782962828874588
I0305 23:35:04.278746 140181040121600 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.31452545523643494, loss=0.011615839786827564
I0305 23:35:24.996802 140181048514304 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.26203611493110657, loss=0.010864555835723877
I0305 23:35:45.695283 140181040121600 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.23455677926540375, loss=0.009579537436366081
I0305 23:36:05.995009 140181048514304 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2565506100654602, loss=0.011237241327762604
I0305 23:36:26.669434 140181040121600 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.28872767090797424, loss=0.010993892326951027
I0305 23:36:47.515688 140181048514304 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.28917205333709717, loss=0.011025206185877323
I0305 23:37:08.004007 140181040121600 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.25361278653144836, loss=0.01184895634651184
I0305 23:37:10.920936 140181048514304 logging_writer.py:48] [58215] global_step=58215, preemption_count=0, score=12013
I0305 23:37:11.071473 140323414672576 submission_runner.py:646] Tuning trial 3/5
I0305 23:37:11.071663 140323414672576 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0305 23:37:11.073260 140323414672576 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.466335266828537, 'train/loss': 0.7765662670135498, 'train/mean_average_precision': 0.024501512820790904, 'validation/accuracy': 0.4627687335014343, 'validation/loss': 0.7771925926208496, 'validation/mean_average_precision': 0.027871177223361375, 'validation/num_examples': 43793, 'test/accuracy': 0.4637924134731293, 'test/loss': 0.7764967679977417, 'test/mean_average_precision': 0.02989974297768301, 'test/num_examples': 43793, 'score': 10.93319296836853, 'total_duration': 214.72251796722412, 'accumulated_submission_time': 10.93319296836853, 'accumulated_eval_time': 203.7892165184021, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1153, {'train/accuracy': 0.9870119690895081, 'train/loss': 0.05065571144223213, 'train/mean_average_precision': 0.06195725116692419, 'validation/accuracy': 0.984386682510376, 'validation/loss': 0.05967370420694351, 'validation/mean_average_precision': 0.06065736560898678, 'validation/num_examples': 43793, 'test/accuracy': 0.9834019541740417, 'test/loss': 0.062791608273983, 'test/mean_average_precision': 0.06295777856325767, 'test/num_examples': 43793, 'score': 250.89254188537598, 'total_duration': 528.9694299697876, 'accumulated_submission_time': 250.89254188537598, 'accumulated_eval_time': 278.03136944770813, 'accumulated_logging_time': 0.016626358032226562, 'global_step': 1153, 'preemption_count': 0}), (2306, {'train/accuracy': 0.9877615571022034, 'train/loss': 0.04390899837017059, 'train/mean_average_precision': 0.13969970405515886, 'validation/accuracy': 0.9848441481590271, 'validation/loss': 0.05365576595067978, 'validation/mean_average_precision': 0.14120361715364035, 'validation/num_examples': 43793, 'test/accuracy': 0.9839457273483276, 'test/loss': 0.056595172733068466, 'test/mean_average_precision': 0.1427101074687839, 'test/num_examples': 43793, 'score': 490.8798429965973, 'total_duration': 844.5680403709412, 'accumulated_submission_time': 490.8798429965973, 'accumulated_eval_time': 353.5924632549286, 'accumulated_logging_time': 0.035361289978027344, 'global_step': 2306, 'preemption_count': 0}), (3455, {'train/accuracy': 0.9884949922561646, 'train/loss': 0.040114063769578934, 'train/mean_average_precision': 0.19413902669517072, 'validation/accuracy': 0.9854822754859924, 'validation/loss': 0.049768220633268356, 'validation/mean_average_precision': 0.1734602633881552, 'validation/num_examples': 43793, 'test/accuracy': 0.9846010804176331, 'test/loss': 0.0524454228579998, 'test/mean_average_precision': 0.1725589042006732, 'test/num_examples': 43793, 'score': 730.9433691501617, 'total_duration': 1160.7079865932465, 'accumulated_submission_time': 730.9433691501617, 'accumulated_eval_time': 429.6199929714203, 'accumulated_logging_time': 0.05483841896057129, 'global_step': 3455, 'preemption_count': 0}), (4609, {'train/accuracy': 0.9884700775146484, 'train/loss': 0.0396701879799366, 'train/mean_average_precision': 0.21797261006167074, 'validation/accuracy': 0.9855736494064331, 'validation/loss': 0.048748794943094254, 'validation/mean_average_precision': 0.1861019223225609, 'validation/num_examples': 43793, 'test/accuracy': 0.9846512079238892, 'test/loss': 0.0512719489634037, 'test/mean_average_precision': 0.18644269774130875, 'test/num_examples': 43793, 'score': 971.0745029449463, 'total_duration': 1476.7569706439972, 'accumulated_submission_time': 971.0745029449463, 'accumulated_eval_time': 505.4896352291107, 'accumulated_logging_time': 0.07304954528808594, 'global_step': 4609, 'preemption_count': 0}), (5794, {'train/accuracy': 0.9890326261520386, 'train/loss': 0.037212952971458435, 'train/mean_average_precision': 0.24388577357169772, 'validation/accuracy': 0.9859738945960999, 'validation/loss': 0.0472651869058609, 'validation/mean_average_precision': 0.20724059249750948, 'validation/num_examples': 43793, 'test/accuracy': 0.9850492477416992, 'test/loss': 0.05001422017812729, 'test/mean_average_precision': 0.20401994511518742, 'test/num_examples': 43793, 'score': 1211.1871082782745, 'total_duration': 1791.1494870185852, 'accumulated_submission_time': 1211.1871082782745, 'accumulated_eval_time': 579.7207245826721, 'accumulated_logging_time': 0.09314227104187012, 'global_step': 5794, 'preemption_count': 0}), (6969, {'train/accuracy': 0.9891321063041687, 'train/loss': 0.03671769052743912, 'train/mean_average_precision': 0.2629625421933687, 'validation/accuracy': 0.9861281514167786, 'validation/loss': 0.04645803943276405, 'validation/mean_average_precision': 0.22041973268118908, 'validation/num_examples': 43793, 'test/accuracy': 0.9852808713912964, 'test/loss': 0.04892151057720184, 'test/mean_average_precision': 0.22360193969464554, 'test/num_examples': 43793, 'score': 1451.2160964012146, 'total_duration': 2105.8550441265106, 'accumulated_submission_time': 1451.2160964012146, 'accumulated_eval_time': 654.3486821651459, 'accumulated_logging_time': 0.11270856857299805, 'global_step': 6969, 'preemption_count': 0}), (8141, {'train/accuracy': 0.9894486665725708, 'train/loss': 0.035610850900411606, 'train/mean_average_precision': 0.28654313611546, 'validation/accuracy': 0.9859938025474548, 'validation/loss': 0.0464618057012558, 'validation/mean_average_precision': 0.22155698046841207, 'validation/num_examples': 43793, 'test/accuracy': 0.9852935075759888, 'test/loss': 0.048900753259658813, 'test/mean_average_precision': 0.22726390799865198, 'test/num_examples': 43793, 'score': 1691.3623287677765, 'total_duration': 2420.5620770454407, 'accumulated_submission_time': 1691.3623287677765, 'accumulated_eval_time': 728.8592274188995, 'accumulated_logging_time': 0.13066744804382324, 'global_step': 8141, 'preemption_count': 0}), (9312, {'train/accuracy': 0.989785373210907, 'train/loss': 0.03430156037211418, 'train/mean_average_precision': 0.3101843536906001, 'validation/accuracy': 0.9863859415054321, 'validation/loss': 0.04521529749035835, 'validation/mean_average_precision': 0.2363399842471766, 'validation/num_examples': 43793, 'test/accuracy': 0.9855361580848694, 'test/loss': 0.04785566404461861, 'test/mean_average_precision': 0.2399642554209753, 'test/num_examples': 43793, 'score': 1931.4180495738983, 'total_duration': 2735.2951188087463, 'accumulated_submission_time': 1931.4180495738983, 'accumulated_eval_time': 803.4851558208466, 'accumulated_logging_time': 0.15070128440856934, 'global_step': 9312, 'preemption_count': 0}), (10488, {'train/accuracy': 0.9901410937309265, 'train/loss': 0.033130716532468796, 'train/mean_average_precision': 0.3284355740962277, 'validation/accuracy': 0.986511766910553, 'validation/loss': 0.044880785048007965, 'validation/mean_average_precision': 0.2456731069626298, 'validation/num_examples': 43793, 'test/accuracy': 0.9856662750244141, 'test/loss': 0.04759758338332176, 'test/mean_average_precision': 0.24573916602012844, 'test/num_examples': 43793, 'score': 2171.5629694461823, 'total_duration': 3050.9273416996, 'accumulated_submission_time': 2171.5629694461823, 'accumulated_eval_time': 878.9245419502258, 'accumulated_logging_time': 0.1699838638305664, 'global_step': 10488, 'preemption_count': 0}), (11654, {'train/accuracy': 0.9901654124259949, 'train/loss': 0.032487694174051285, 'train/mean_average_precision': 0.35544792824294813, 'validation/accuracy': 0.986464262008667, 'validation/loss': 0.045079831033945084, 'validation/mean_average_precision': 0.24950404915177132, 'validation/num_examples': 43793, 'test/accuracy': 0.98563551902771, 'test/loss': 0.04782219976186752, 'test/mean_average_precision': 0.2430625370524352, 'test/num_examples': 43793, 'score': 2411.7179279327393, 'total_duration': 3365.5718653202057, 'accumulated_submission_time': 2411.7179279327393, 'accumulated_eval_time': 953.3637182712555, 'accumulated_logging_time': 0.19066262245178223, 'global_step': 11654, 'preemption_count': 0}), (12812, {'train/accuracy': 0.9905313849449158, 'train/loss': 0.03124281018972397, 'train/mean_average_precision': 0.37896046556666024, 'validation/accuracy': 0.9866449236869812, 'validation/loss': 0.04479825496673584, 'validation/mean_average_precision': 0.25468768827299426, 'validation/num_examples': 43793, 'test/accuracy': 0.9858250617980957, 'test/loss': 0.04755214601755142, 'test/mean_average_precision': 0.2539833355432807, 'test/num_examples': 43793, 'score': 2651.768351793289, 'total_duration': 3682.032088279724, 'accumulated_submission_time': 2651.768351793289, 'accumulated_eval_time': 1029.725546836853, 'accumulated_logging_time': 0.20875144004821777, 'global_step': 12812, 'preemption_count': 0}), (13979, {'train/accuracy': 0.9903814792633057, 'train/loss': 0.031676072627305984, 'train/mean_average_precision': 0.3674400428025558, 'validation/accuracy': 0.9864833354949951, 'validation/loss': 0.04480401799082756, 'validation/mean_average_precision': 0.2539983989492452, 'validation/num_examples': 43793, 'test/accuracy': 0.9857164025306702, 'test/loss': 0.047417737543582916, 'test/mean_average_precision': 0.2506572946455104, 'test/num_examples': 43793, 'score': 2891.79754114151, 'total_duration': 3997.4150631427765, 'accumulated_submission_time': 2891.79754114151, 'accumulated_eval_time': 1105.0284843444824, 'accumulated_logging_time': 0.22975921630859375, 'global_step': 13979, 'preemption_count': 0}), (15159, {'train/accuracy': 0.9907143712043762, 'train/loss': 0.030279938131570816, 'train/mean_average_precision': 0.40031920960122586, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.044796157628297806, 'validation/mean_average_precision': 0.2591027981005202, 'validation/num_examples': 43793, 'test/accuracy': 0.9858903884887695, 'test/loss': 0.047630976885557175, 'test/mean_average_precision': 0.25436957244714936, 'test/num_examples': 43793, 'score': 3131.7593042850494, 'total_duration': 4313.525934457779, 'accumulated_submission_time': 3131.7593042850494, 'accumulated_eval_time': 1181.128756761551, 'accumulated_logging_time': 0.2487773895263672, 'global_step': 15159, 'preemption_count': 0}), (16335, {'train/accuracy': 0.9908336400985718, 'train/loss': 0.030252518132328987, 'train/mean_average_precision': 0.3931483787548169, 'validation/accuracy': 0.9867780804634094, 'validation/loss': 0.04409050941467285, 'validation/mean_average_precision': 0.2683983880921344, 'validation/num_examples': 43793, 'test/accuracy': 0.9858853220939636, 'test/loss': 0.04689682647585869, 'test/mean_average_precision': 0.2582523247154778, 'test/num_examples': 43793, 'score': 3371.7986681461334, 'total_duration': 4626.694617271423, 'accumulated_submission_time': 3371.7986681461334, 'accumulated_eval_time': 1254.2074756622314, 'accumulated_logging_time': 0.2676990032196045, 'global_step': 16335, 'preemption_count': 0}), (17515, {'train/accuracy': 0.9908801317214966, 'train/loss': 0.02966691553592682, 'train/mean_average_precision': 0.41545911713610517, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.04448854178190231, 'validation/mean_average_precision': 0.271140390211123, 'validation/num_examples': 43793, 'test/accuracy': 0.9859564900398254, 'test/loss': 0.04732132330536842, 'test/mean_average_precision': 0.25312751116094817, 'test/num_examples': 43793, 'score': 3611.886740922928, 'total_duration': 4941.175326347351, 'accumulated_submission_time': 3611.886740922928, 'accumulated_eval_time': 1328.5494711399078, 'accumulated_logging_time': 0.28993844985961914, 'global_step': 17515, 'preemption_count': 0}), (18687, {'train/accuracy': 0.9906529188156128, 'train/loss': 0.030500255525112152, 'train/mean_average_precision': 0.4209120021722421, 'validation/accuracy': 0.9865559935569763, 'validation/loss': 0.045080482959747314, 'validation/mean_average_precision': 0.2668583827734413, 'validation/num_examples': 43793, 'test/accuracy': 0.9857783317565918, 'test/loss': 0.047503095120191574, 'test/mean_average_precision': 0.25781563102755245, 'test/num_examples': 43793, 'score': 3851.973825931549, 'total_duration': 5256.023555994034, 'accumulated_submission_time': 3851.973825931549, 'accumulated_eval_time': 1403.2605533599854, 'accumulated_logging_time': 0.3101847171783447, 'global_step': 18687, 'preemption_count': 0}), (19867, {'train/accuracy': 0.9909771084785461, 'train/loss': 0.02932092733681202, 'train/mean_average_precision': 0.42404470792376164, 'validation/accuracy': 0.9867427349090576, 'validation/loss': 0.04458590969443321, 'validation/mean_average_precision': 0.2653829366622306, 'validation/num_examples': 43793, 'test/accuracy': 0.9859800934791565, 'test/loss': 0.047199707478284836, 'test/mean_average_precision': 0.2569790034379748, 'test/num_examples': 43793, 'score': 4092.066060781479, 'total_duration': 5571.474860429764, 'accumulated_submission_time': 4092.066060781479, 'accumulated_eval_time': 1478.5684304237366, 'accumulated_logging_time': 0.3311624526977539, 'global_step': 19867, 'preemption_count': 0}), (21026, {'train/accuracy': 0.9913184642791748, 'train/loss': 0.02832372486591339, 'train/mean_average_precision': 0.43958617184370263, 'validation/accuracy': 0.9868292212486267, 'validation/loss': 0.04461655020713806, 'validation/mean_average_precision': 0.2678854960720369, 'validation/num_examples': 43793, 'test/accuracy': 0.985869288444519, 'test/loss': 0.04747440293431282, 'test/mean_average_precision': 0.25587157651797093, 'test/num_examples': 43793, 'score': 4332.043648481369, 'total_duration': 5883.485830783844, 'accumulated_submission_time': 4332.043648481369, 'accumulated_eval_time': 1550.5531079769135, 'accumulated_logging_time': 0.351360559463501, 'global_step': 21026, 'preemption_count': 0}), (22174, {'train/accuracy': 0.9914518594741821, 'train/loss': 0.027702966704964638, 'train/mean_average_precision': 0.4644357967232483, 'validation/accuracy': 0.9867590069770813, 'validation/loss': 0.04439372196793556, 'validation/mean_average_precision': 0.269447145805152, 'validation/num_examples': 43793, 'test/accuracy': 0.9858987927436829, 'test/loss': 0.04731141775846481, 'test/mean_average_precision': 0.2557575138847978, 'test/num_examples': 43793, 'score': 4572.144749879837, 'total_duration': 6200.615307807922, 'accumulated_submission_time': 4572.144749879837, 'accumulated_eval_time': 1627.531227350235, 'accumulated_logging_time': 0.37154221534729004, 'global_step': 22174, 'preemption_count': 0}), (23342, {'train/accuracy': 0.9915069341659546, 'train/loss': 0.027634425088763237, 'train/mean_average_precision': 0.4606353431102186, 'validation/accuracy': 0.9868580102920532, 'validation/loss': 0.044646430760622025, 'validation/mean_average_precision': 0.2732313752861011, 'validation/num_examples': 43793, 'test/accuracy': 0.986052930355072, 'test/loss': 0.047586437314748764, 'test/mean_average_precision': 0.26334678308197046, 'test/num_examples': 43793, 'score': 4812.187657356262, 'total_duration': 6513.632674217224, 'accumulated_submission_time': 4812.187657356262, 'accumulated_eval_time': 1700.454246520996, 'accumulated_logging_time': 0.3936655521392822, 'global_step': 23342, 'preemption_count': 0}), (24516, {'train/accuracy': 0.9916749000549316, 'train/loss': 0.026768837124109268, 'train/mean_average_precision': 0.4847076427153393, 'validation/accuracy': 0.9868206977844238, 'validation/loss': 0.0445321761071682, 'validation/mean_average_precision': 0.27062057079293783, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.047279488295316696, 'test/mean_average_precision': 0.2592041691558263, 'test/num_examples': 43793, 'score': 5052.244487285614, 'total_duration': 6828.886110067368, 'accumulated_submission_time': 5052.244487285614, 'accumulated_eval_time': 1775.6000895500183, 'accumulated_logging_time': 0.4145171642303467, 'global_step': 24516, 'preemption_count': 0}), (25686, {'train/accuracy': 0.9917594790458679, 'train/loss': 0.0264980960637331, 'train/mean_average_precision': 0.48456089799359725, 'validation/accuracy': 0.9867764711380005, 'validation/loss': 0.04483954980969429, 'validation/mean_average_precision': 0.2776095949448317, 'validation/num_examples': 43793, 'test/accuracy': 0.9859291315078735, 'test/loss': 0.0478808656334877, 'test/mean_average_precision': 0.2572342924180867, 'test/num_examples': 43793, 'score': 5292.241194009781, 'total_duration': 7144.711585044861, 'accumulated_submission_time': 5292.241194009781, 'accumulated_eval_time': 1851.376755952835, 'accumulated_logging_time': 0.4351999759674072, 'global_step': 25686, 'preemption_count': 0}), (26859, {'train/accuracy': 0.9919121861457825, 'train/loss': 0.025988435372710228, 'train/mean_average_precision': 0.5066883529462435, 'validation/accuracy': 0.986818253993988, 'validation/loss': 0.04498967155814171, 'validation/mean_average_precision': 0.2742437594418257, 'validation/num_examples': 43793, 'test/accuracy': 0.9859526753425598, 'test/loss': 0.04789399728178978, 'test/mean_average_precision': 0.260470862895351, 'test/num_examples': 43793, 'score': 5532.201447963715, 'total_duration': 7458.802631378174, 'accumulated_submission_time': 5532.201447963715, 'accumulated_eval_time': 1925.456248998642, 'accumulated_logging_time': 0.455519437789917, 'global_step': 26859, 'preemption_count': 0}), (28026, {'train/accuracy': 0.992068350315094, 'train/loss': 0.025575853884220123, 'train/mean_average_precision': 0.5093305501186075, 'validation/accuracy': 0.98676997423172, 'validation/loss': 0.04546408727765083, 'validation/mean_average_precision': 0.2774210883788067, 'validation/num_examples': 43793, 'test/accuracy': 0.9859409332275391, 'test/loss': 0.04856036975979805, 'test/mean_average_precision': 0.2589473862411662, 'test/num_examples': 43793, 'score': 5772.314005613327, 'total_duration': 7773.201402425766, 'accumulated_submission_time': 5772.314005613327, 'accumulated_eval_time': 1999.6860909461975, 'accumulated_logging_time': 0.4820897579193115, 'global_step': 28026, 'preemption_count': 0}), (29181, {'train/accuracy': 0.9922259449958801, 'train/loss': 0.02485993504524231, 'train/mean_average_precision': 0.5260727822862852, 'validation/accuracy': 0.9867967367172241, 'validation/loss': 0.0454312264919281, 'validation/mean_average_precision': 0.278488574127944, 'validation/num_examples': 43793, 'test/accuracy': 0.985890805721283, 'test/loss': 0.04876728728413582, 'test/mean_average_precision': 0.2578138540729559, 'test/num_examples': 43793, 'score': 6012.096572875977, 'total_duration': 8087.9226224422455, 'accumulated_submission_time': 6012.096572875977, 'accumulated_eval_time': 2074.357400894165, 'accumulated_logging_time': 0.7150609493255615, 'global_step': 29181, 'preemption_count': 0}), (30338, {'train/accuracy': 0.9925255179405212, 'train/loss': 0.024014722555875778, 'train/mean_average_precision': 0.5361810671808445, 'validation/accuracy': 0.98674476146698, 'validation/loss': 0.045607633888721466, 'validation/mean_average_precision': 0.2803041059396133, 'validation/num_examples': 43793, 'test/accuracy': 0.9858090877532959, 'test/loss': 0.0487041100859642, 'test/mean_average_precision': 0.26535465098545596, 'test/num_examples': 43793, 'score': 6252.160385608673, 'total_duration': 8403.121178627014, 'accumulated_submission_time': 6252.160385608673, 'accumulated_eval_time': 2149.440814971924, 'accumulated_logging_time': 0.7369751930236816, 'global_step': 30338, 'preemption_count': 0}), (31500, {'train/accuracy': 0.9926242232322693, 'train/loss': 0.023406900465488434, 'train/mean_average_precision': 0.5624632319165195, 'validation/accuracy': 0.9867528676986694, 'validation/loss': 0.04611608386039734, 'validation/mean_average_precision': 0.27335544341539186, 'validation/num_examples': 43793, 'test/accuracy': 0.9859278798103333, 'test/loss': 0.04926510527729988, 'test/mean_average_precision': 0.25906426707756247, 'test/num_examples': 43793, 'score': 6492.241488456726, 'total_duration': 8717.310858488083, 'accumulated_submission_time': 6492.241488456726, 'accumulated_eval_time': 2223.497634410858, 'accumulated_logging_time': 0.7595162391662598, 'global_step': 31500, 'preemption_count': 0}), (32665, {'train/accuracy': 0.9927356243133545, 'train/loss': 0.02317107655107975, 'train/mean_average_precision': 0.559708597296965, 'validation/accuracy': 0.9866887331008911, 'validation/loss': 0.046457622200250626, 'validation/mean_average_precision': 0.28008448878891623, 'validation/num_examples': 43793, 'test/accuracy': 0.9857892990112305, 'test/loss': 0.049622468650341034, 'test/mean_average_precision': 0.2622657203536141, 'test/num_examples': 43793, 'score': 6732.265762090683, 'total_duration': 9032.604087114334, 'accumulated_submission_time': 6732.265762090683, 'accumulated_eval_time': 2298.710569381714, 'accumulated_logging_time': 0.7862045764923096, 'global_step': 32665, 'preemption_count': 0}), (33815, {'train/accuracy': 0.9931453466415405, 'train/loss': 0.021778862923383713, 'train/mean_average_precision': 0.6030676853342777, 'validation/accuracy': 0.9865401983261108, 'validation/loss': 0.046925950795412064, 'validation/mean_average_precision': 0.2754254203488731, 'validation/num_examples': 43793, 'test/accuracy': 0.9856321811676025, 'test/loss': 0.0501764751970768, 'test/mean_average_precision': 0.2627487030694094, 'test/num_examples': 43793, 'score': 6972.411457061768, 'total_duration': 9346.08096075058, 'accumulated_submission_time': 6972.411457061768, 'accumulated_eval_time': 2371.987680912018, 'accumulated_logging_time': 0.8078811168670654, 'global_step': 33815, 'preemption_count': 0}), (34976, {'train/accuracy': 0.9932435154914856, 'train/loss': 0.021446095779538155, 'train/mean_average_precision': 0.5987051117749832, 'validation/accuracy': 0.9867520928382874, 'validation/loss': 0.04754088073968887, 'validation/mean_average_precision': 0.28283858959263486, 'validation/num_examples': 43793, 'test/accuracy': 0.9858726859092712, 'test/loss': 0.05084618553519249, 'test/mean_average_precision': 0.2569959978016241, 'test/num_examples': 43793, 'score': 7212.45259308815, 'total_duration': 9662.743321180344, 'accumulated_submission_time': 7212.45259308815, 'accumulated_eval_time': 2448.556200504303, 'accumulated_logging_time': 0.8298265933990479, 'global_step': 34976, 'preemption_count': 0}), (36137, {'train/accuracy': 0.9931691884994507, 'train/loss': 0.02131800353527069, 'train/mean_average_precision': 0.5922817980373578, 'validation/accuracy': 0.9866794347763062, 'validation/loss': 0.04855526238679886, 'validation/mean_average_precision': 0.2767632809486688, 'validation/num_examples': 43793, 'test/accuracy': 0.9856637716293335, 'test/loss': 0.05191114917397499, 'test/mean_average_precision': 0.2584998156215234, 'test/num_examples': 43793, 'score': 7452.54484128952, 'total_duration': 9973.888405323029, 'accumulated_submission_time': 7452.54484128952, 'accumulated_eval_time': 2519.558069705963, 'accumulated_logging_time': 0.8511898517608643, 'global_step': 36137, 'preemption_count': 0}), (37291, {'train/accuracy': 0.9934427738189697, 'train/loss': 0.020633360370993614, 'train/mean_average_precision': 0.6194492156065936, 'validation/accuracy': 0.9864614605903625, 'validation/loss': 0.04939074069261551, 'validation/mean_average_precision': 0.26909185397109614, 'validation/num_examples': 43793, 'test/accuracy': 0.9855445623397827, 'test/loss': 0.0527832917869091, 'test/mean_average_precision': 0.25639599212378517, 'test/num_examples': 43793, 'score': 7692.518120765686, 'total_duration': 10288.816576480865, 'accumulated_submission_time': 7692.518120765686, 'accumulated_eval_time': 2594.4603536129, 'accumulated_logging_time': 0.8734204769134521, 'global_step': 37291, 'preemption_count': 0}), (38455, {'train/accuracy': 0.994021475315094, 'train/loss': 0.018586156889796257, 'train/mean_average_precision': 0.6616864014678188, 'validation/accuracy': 0.9865787625312805, 'validation/loss': 0.0503096804022789, 'validation/mean_average_precision': 0.27201386812874095, 'validation/num_examples': 43793, 'test/accuracy': 0.9857387542724609, 'test/loss': 0.05387747660279274, 'test/mean_average_precision': 0.25571954700427096, 'test/num_examples': 43793, 'score': 7932.557765960693, 'total_duration': 10602.61527967453, 'accumulated_submission_time': 7932.557765960693, 'accumulated_eval_time': 2668.1651582717896, 'accumulated_logging_time': 0.8964812755584717, 'global_step': 38455, 'preemption_count': 0}), (39622, {'train/accuracy': 0.9940230250358582, 'train/loss': 0.01866920292377472, 'train/mean_average_precision': 0.6602977210903629, 'validation/accuracy': 0.9864342212677002, 'validation/loss': 0.05094640702009201, 'validation/mean_average_precision': 0.2754222485648608, 'validation/num_examples': 43793, 'test/accuracy': 0.9854232668876648, 'test/loss': 0.05489158630371094, 'test/mean_average_precision': 0.25126696776085883, 'test/num_examples': 43793, 'score': 8172.530004501343, 'total_duration': 10918.107706069946, 'accumulated_submission_time': 8172.530004501343, 'accumulated_eval_time': 2743.6274876594543, 'accumulated_logging_time': 0.9257495403289795, 'global_step': 39622, 'preemption_count': 0}), (40783, {'train/accuracy': 0.9946579337120056, 'train/loss': 0.016698967665433884, 'train/mean_average_precision': 0.7004277545733029, 'validation/accuracy': 0.9862154126167297, 'validation/loss': 0.052649084478616714, 'validation/mean_average_precision': 0.2705861960885959, 'validation/num_examples': 43793, 'test/accuracy': 0.9854004979133606, 'test/loss': 0.056571584194898605, 'test/mean_average_precision': 0.25075871372373465, 'test/num_examples': 43793, 'score': 8412.570796728134, 'total_duration': 11232.69090127945, 'accumulated_submission_time': 8412.570796728134, 'accumulated_eval_time': 2818.1175332069397, 'accumulated_logging_time': 0.9481105804443359, 'global_step': 40783, 'preemption_count': 0}), (41947, {'train/accuracy': 0.9945183992385864, 'train/loss': 0.01701134443283081, 'train/mean_average_precision': 0.6892549028018587, 'validation/accuracy': 0.9862669706344604, 'validation/loss': 0.05340876057744026, 'validation/mean_average_precision': 0.2703639747734576, 'validation/num_examples': 43793, 'test/accuracy': 0.9853967428207397, 'test/loss': 0.05744592472910881, 'test/mean_average_precision': 0.24805050184159175, 'test/num_examples': 43793, 'score': 8652.619043588638, 'total_duration': 11551.546646595001, 'accumulated_submission_time': 8652.619043588638, 'accumulated_eval_time': 2896.8720409870148, 'accumulated_logging_time': 0.9718317985534668, 'global_step': 41947, 'preemption_count': 0}), (43115, {'train/accuracy': 0.9949214458465576, 'train/loss': 0.01557965762913227, 'train/mean_average_precision': 0.7149288577241606, 'validation/accuracy': 0.9862214922904968, 'validation/loss': 0.05527900904417038, 'validation/mean_average_precision': 0.2693493371293331, 'validation/num_examples': 43793, 'test/accuracy': 0.985284686088562, 'test/loss': 0.05919085070490837, 'test/mean_average_precision': 0.24818950969380496, 'test/num_examples': 43793, 'score': 8892.75588274002, 'total_duration': 11863.284273386002, 'accumulated_submission_time': 8892.75588274002, 'accumulated_eval_time': 2968.4202122688293, 'accumulated_logging_time': 0.9942829608917236, 'global_step': 43115, 'preemption_count': 0}), (44284, {'train/accuracy': 0.9942291975021362, 'train/loss': 0.017625173553824425, 'train/mean_average_precision': 0.6696050000635414, 'validation/accuracy': 0.9860242605209351, 'validation/loss': 0.056895118206739426, 'validation/mean_average_precision': 0.2644042083742525, 'validation/num_examples': 43793, 'test/accuracy': 0.9852294921875, 'test/loss': 0.06100516393780708, 'test/mean_average_precision': 0.24425035979295948, 'test/num_examples': 43793, 'score': 9132.779817581177, 'total_duration': 12177.703366994858, 'accumulated_submission_time': 9132.779817581177, 'accumulated_eval_time': 3042.7639050483704, 'accumulated_logging_time': 1.0175633430480957, 'global_step': 44284, 'preemption_count': 0}), (45447, {'train/accuracy': 0.9956562519073486, 'train/loss': 0.013483216054737568, 'train/mean_average_precision': 0.7672508280689583, 'validation/accuracy': 0.9859856963157654, 'validation/loss': 0.05761616304516792, 'validation/mean_average_precision': 0.26346808498597984, 'validation/num_examples': 43793, 'test/accuracy': 0.985129714012146, 'test/loss': 0.061833687126636505, 'test/mean_average_precision': 0.24306629866618418, 'test/num_examples': 43793, 'score': 9372.818446397781, 'total_duration': 12493.600263834, 'accumulated_submission_time': 9372.818446397781, 'accumulated_eval_time': 3118.5701117515564, 'accumulated_logging_time': 1.040273904800415, 'global_step': 45447, 'preemption_count': 0}), (46616, {'train/accuracy': 0.9953954219818115, 'train/loss': 0.014111501164734364, 'train/mean_average_precision': 0.746546922507489, 'validation/accuracy': 0.9858711957931519, 'validation/loss': 0.05887119099497795, 'validation/mean_average_precision': 0.2642048072528853, 'validation/num_examples': 43793, 'test/accuracy': 0.9849578142166138, 'test/loss': 0.06313353031873703, 'test/mean_average_precision': 0.24235539957882893, 'test/num_examples': 43793, 'score': 9612.8030667305, 'total_duration': 12808.510948896408, 'accumulated_submission_time': 9612.8030667305, 'accumulated_eval_time': 3193.4420721530914, 'accumulated_logging_time': 1.0644874572753906, 'global_step': 46616, 'preemption_count': 0}), (47780, {'train/accuracy': 0.9962916374206543, 'train/loss': 0.011886448599398136, 'train/mean_average_precision': 0.7915379062583754, 'validation/accuracy': 0.9857478141784668, 'validation/loss': 0.05954267457127571, 'validation/mean_average_precision': 0.2575658515725678, 'validation/num_examples': 43793, 'test/accuracy': 0.9848373532295227, 'test/loss': 0.06399857252836227, 'test/mean_average_precision': 0.24035700200587892, 'test/num_examples': 43793, 'score': 9852.771946907043, 'total_duration': 13124.94112110138, 'accumulated_submission_time': 9852.771946907043, 'accumulated_eval_time': 3269.851238965988, 'accumulated_logging_time': 1.088731050491333, 'global_step': 47780, 'preemption_count': 0}), (48940, {'train/accuracy': 0.9955523610115051, 'train/loss': 0.013553488999605179, 'train/mean_average_precision': 0.7524136932291805, 'validation/accuracy': 0.9858115315437317, 'validation/loss': 0.06012628972530365, 'validation/mean_average_precision': 0.2592776481451712, 'validation/num_examples': 43793, 'test/accuracy': 0.9849308729171753, 'test/loss': 0.06443016231060028, 'test/mean_average_precision': 0.24031841545506663, 'test/num_examples': 43793, 'score': 10092.78107380867, 'total_duration': 13436.246411085129, 'accumulated_submission_time': 10092.78107380867, 'accumulated_eval_time': 3341.093116044998, 'accumulated_logging_time': 1.1130468845367432, 'global_step': 48940, 'preemption_count': 0}), (50102, {'train/accuracy': 0.9965635538101196, 'train/loss': 0.01125608291476965, 'train/mean_average_precision': 0.7973499884337357, 'validation/accuracy': 0.9858399629592896, 'validation/loss': 0.06023498624563217, 'validation/mean_average_precision': 0.2625930456204962, 'validation/num_examples': 43793, 'test/accuracy': 0.9849430918693542, 'test/loss': 0.06458736956119537, 'test/mean_average_precision': 0.24086352112886147, 'test/num_examples': 43793, 'score': 10332.87071609497, 'total_duration': 13753.519015789032, 'accumulated_submission_time': 10332.87071609497, 'accumulated_eval_time': 3418.2231209278107, 'accumulated_logging_time': 1.1377451419830322, 'global_step': 50102, 'preemption_count': 0}), (51257, {'train/accuracy': 0.9961840510368347, 'train/loss': 0.012022496201097965, 'train/mean_average_precision': 0.7901851216467538, 'validation/accuracy': 0.9857761859893799, 'validation/loss': 0.06021349877119064, 'validation/mean_average_precision': 0.26051661924330416, 'validation/num_examples': 43793, 'test/accuracy': 0.9849784970283508, 'test/loss': 0.06450852006673813, 'test/mean_average_precision': 0.24150207427599157, 'test/num_examples': 43793, 'score': 10572.832387447357, 'total_duration': 14068.652569293976, 'accumulated_submission_time': 10572.832387447357, 'accumulated_eval_time': 3493.33921957016, 'accumulated_logging_time': 1.1626899242401123, 'global_step': 51257, 'preemption_count': 0}), (52421, {'train/accuracy': 0.9964466691017151, 'train/loss': 0.011446944437921047, 'train/mean_average_precision': 0.7970200857495544, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026484817266464, 'validation/mean_average_precision': 0.2594645982827487, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24159476956803602, 'test/num_examples': 43793, 'score': 10812.784964561462, 'total_duration': 14380.733457565308, 'accumulated_submission_time': 10812.784964561462, 'accumulated_eval_time': 3565.4109737873077, 'accumulated_logging_time': 1.1869869232177734, 'global_step': 52421, 'preemption_count': 0}), (53580, {'train/accuracy': 0.9964762926101685, 'train/loss': 0.011372756212949753, 'train/mean_average_precision': 0.7938330895636596, 'validation/accuracy': 0.9858235120773315, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.2595437817929092, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24158163143192424, 'test/num_examples': 43793, 'score': 11052.78490781784, 'total_duration': 14699.884835481644, 'accumulated_submission_time': 11052.78490781784, 'accumulated_eval_time': 3644.5097727775574, 'accumulated_logging_time': 1.2112836837768555, 'global_step': 53580, 'preemption_count': 0}), (54738, {'train/accuracy': 0.9961983561515808, 'train/loss': 0.01190643198788166, 'train/mean_average_precision': 0.7919387023430402, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.25964303035746966, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24176569701953918, 'test/num_examples': 43793, 'score': 11292.788722038269, 'total_duration': 15013.126317024231, 'accumulated_submission_time': 11292.788722038269, 'accumulated_eval_time': 3717.6946024894714, 'accumulated_logging_time': 1.235180377960205, 'global_step': 54738, 'preemption_count': 0}), (55897, {'train/accuracy': 0.9962978363037109, 'train/loss': 0.011785424314439297, 'train/mean_average_precision': 0.7809687030648927, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.2595002275369526, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.24182635362297308, 'test/num_examples': 43793, 'score': 11532.894360542297, 'total_duration': 15330.940398931503, 'accumulated_submission_time': 11532.894360542297, 'accumulated_eval_time': 3795.3465275764465, 'accumulated_logging_time': 1.261643648147583, 'global_step': 55897, 'preemption_count': 0}), (57054, {'train/accuracy': 0.9963226318359375, 'train/loss': 0.011711775325238705, 'train/mean_average_precision': 0.7877331391676641, 'validation/accuracy': 0.9858232736587524, 'validation/loss': 0.06026485562324524, 'validation/mean_average_precision': 0.2594412977473483, 'validation/num_examples': 43793, 'test/accuracy': 0.9849771857261658, 'test/loss': 0.06456968933343887, 'test/mean_average_precision': 0.2416251711275031, 'test/num_examples': 43793, 'score': 11772.982586860657, 'total_duration': 15645.50228214264, 'accumulated_submission_time': 11772.982586860657, 'accumulated_eval_time': 3869.7655885219574, 'accumulated_logging_time': 1.2866084575653076, 'global_step': 57054, 'preemption_count': 0})], 'global_step': 58215}
I0305 23:37:11.073368 140323414672576 submission_runner.py:649] Timing: 12012.95390367508
I0305 23:37:11.073406 140323414672576 submission_runner.py:651] Total number of evals: 50
I0305 23:37:11.073433 140323414672576 submission_runner.py:652] ====================
I0305 23:37:11.073774 140323414672576 submission_runner.py:750] Final ogbg score: 2
