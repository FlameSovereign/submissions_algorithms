python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-2046206386 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-02-08-29.log
2025-03-07 02:08:47.118773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741313327.507208       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741313327.681546       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 02:09:33.538559 140452179379392 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax.
I0307 02:09:36.379652 140452179379392 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 02:09:36.382739 140452179379392 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 02:09:36.415634 140452179379392 submission_runner.py:606] Using RNG seed -2046206386
I0307 02:09:41.953282 140452179379392 submission_runner.py:615] --- Tuning run 5/5 ---
I0307 02:09:41.953503 140452179379392 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_5.
I0307 02:09:41.953703 140452179379392 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_5/hparams.json.
I0307 02:09:42.193787 140452179379392 submission_runner.py:218] Initializing dataset.
I0307 02:09:43.607286 140452179379392 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:09:43.925068 140452179379392 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:09:44.291781 140452179379392 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:09:46.001829 140452179379392 submission_runner.py:229] Initializing model.
I0307 02:10:09.685954 140452179379392 submission_runner.py:272] Initializing optimizer.
I0307 02:10:10.810949 140452179379392 submission_runner.py:279] Initializing metrics bundle.
I0307 02:10:10.811181 140452179379392 submission_runner.py:301] Initializing checkpoint and logger.
I0307 02:10:10.812301 140452179379392 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0307 02:10:10.812408 140452179379392 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_5/meta_data_0.json.
I0307 02:10:11.364624 140452179379392 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_5/flags_0.json.
I0307 02:10:11.713881 140452179379392 submission_runner.py:337] Starting training loop.
I0307 02:11:09.773048 140315798750976 logging_writer.py:48] [0] global_step=0, grad_norm=0.6854109168052673, loss=6.933595180511475
I0307 02:11:10.049550 140452179379392 spec.py:321] Evaluating on the training split.
I0307 02:11:10.519344 140452179379392 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:11:10.543671 140452179379392 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:11:10.588173 140452179379392 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:11:30.005751 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 02:11:30.584512 140452179379392 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:11:30.653550 140452179379392 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:11:30.879705 140452179379392 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:12:16.403868 140452179379392 spec.py:349] Evaluating on the test split.
I0307 02:12:16.909983 140452179379392 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:12:16.961889 140452179379392 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 02:12:17.002262 140452179379392 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:13:29.974779 140452179379392 submission_runner.py:469] Time since start: 198.26s, 	Step: 1, 	{'train/accuracy': 0.001235650503076613, 'train/loss': 6.912410736083984, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.912078380584717, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.912538051605225, 'test/num_examples': 10000, 'score': 58.335450649261475, 'total_duration': 198.26079630851746, 'accumulated_submission_time': 58.335450649261475, 'accumulated_eval_time': 139.9251353740692, 'accumulated_logging_time': 0}
I0307 02:13:30.016304 140296817981184 logging_writer.py:48] [1] accumulated_eval_time=139.925, accumulated_logging_time=0, accumulated_submission_time=58.3355, global_step=1, preemption_count=0, score=58.3355, test/accuracy=0.0011, test/loss=6.91254, test/num_examples=10000, total_duration=198.261, train/accuracy=0.00123565, train/loss=6.91241, validation/accuracy=0.001, validation/loss=6.91208, validation/num_examples=50000
I0307 02:14:06.854700 140296809588480 logging_writer.py:48] [100] global_step=100, grad_norm=0.6618762016296387, loss=6.822850704193115
I0307 02:14:43.404173 140296817981184 logging_writer.py:48] [200] global_step=200, grad_norm=0.7929983735084534, loss=6.561201095581055
I0307 02:15:20.749877 140296809588480 logging_writer.py:48] [300] global_step=300, grad_norm=0.9680806994438171, loss=6.324309349060059
I0307 02:15:58.531709 140296817981184 logging_writer.py:48] [400] global_step=400, grad_norm=1.7239048480987549, loss=6.0405426025390625
I0307 02:16:36.660790 140296809588480 logging_writer.py:48] [500] global_step=500, grad_norm=2.6388697624206543, loss=5.810656547546387
I0307 02:17:14.531573 140296817981184 logging_writer.py:48] [600] global_step=600, grad_norm=2.207753896713257, loss=5.629804611206055
I0307 02:17:52.395102 140296809588480 logging_writer.py:48] [700] global_step=700, grad_norm=4.298214435577393, loss=5.465376853942871
I0307 02:18:30.284324 140296817981184 logging_writer.py:48] [800] global_step=800, grad_norm=2.8993442058563232, loss=5.405009746551514
I0307 02:19:08.554976 140296809588480 logging_writer.py:48] [900] global_step=900, grad_norm=3.7301747798919678, loss=5.263114929199219
I0307 02:19:46.991266 140296817981184 logging_writer.py:48] [1000] global_step=1000, grad_norm=4.959299087524414, loss=5.026666641235352
I0307 02:20:24.871632 140296809588480 logging_writer.py:48] [1100] global_step=1100, grad_norm=5.725847244262695, loss=4.845191478729248
I0307 02:21:02.636827 140296817981184 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.688513994216919, loss=4.826401710510254
I0307 02:21:38.961277 140296809588480 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.072371959686279, loss=4.920809745788574
I0307 02:22:00.054338 140452179379392 spec.py:321] Evaluating on the training split.
I0307 02:22:12.319098 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 02:22:31.344577 140452179379392 spec.py:349] Evaluating on the test split.
I0307 02:22:33.350469 140452179379392 submission_runner.py:469] Time since start: 741.64s, 	Step: 1356, 	{'train/accuracy': 0.15836256742477417, 'train/loss': 4.316074848175049, 'validation/accuracy': 0.13161998987197876, 'validation/loss': 4.559778213500977, 'validation/num_examples': 50000, 'test/accuracy': 0.10210000723600388, 'test/loss': 4.961103916168213, 'test/num_examples': 10000, 'score': 568.1441419124603, 'total_duration': 741.6363072395325, 'accumulated_submission_time': 568.1441419124603, 'accumulated_eval_time': 173.22100186347961, 'accumulated_logging_time': 0.06844806671142578}
I0307 02:22:33.411623 140296826373888 logging_writer.py:48] [1356] accumulated_eval_time=173.221, accumulated_logging_time=0.0684481, accumulated_submission_time=568.144, global_step=1356, preemption_count=0, score=568.144, test/accuracy=0.1021, test/loss=4.9611, test/num_examples=10000, total_duration=741.636, train/accuracy=0.158363, train/loss=4.31607, validation/accuracy=0.13162, validation/loss=4.55978, validation/num_examples=50000
I0307 02:22:49.901933 140296834766592 logging_writer.py:48] [1400] global_step=1400, grad_norm=6.3178935050964355, loss=4.569613933563232
I0307 02:23:26.937938 140296826373888 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.8094658851623535, loss=4.539663791656494
I0307 02:24:04.408181 140296834766592 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.5739943981170654, loss=4.3822174072265625
I0307 02:24:42.231478 140296826373888 logging_writer.py:48] [1700] global_step=1700, grad_norm=5.989958763122559, loss=4.388649940490723
I0307 02:25:20.339385 140296834766592 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.111408710479736, loss=4.296319007873535
I0307 02:25:58.219192 140296826373888 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.6066107749938965, loss=4.198940753936768
I0307 02:26:35.855583 140296834766592 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.113505840301514, loss=4.113855361938477
I0307 02:27:13.921728 140296826373888 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.6996378898620605, loss=3.9502933025360107
I0307 02:27:51.704888 140296834766592 logging_writer.py:48] [2200] global_step=2200, grad_norm=9.237263679504395, loss=3.8741860389709473
I0307 02:28:30.119113 140296826373888 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.17912483215332, loss=3.798567295074463
I0307 02:29:08.177424 140296834766592 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.245246410369873, loss=3.8217949867248535
I0307 02:29:45.400928 140296826373888 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.9565932750701904, loss=3.726609230041504
I0307 02:30:23.458296 140296834766592 logging_writer.py:48] [2600] global_step=2600, grad_norm=5.461706161499023, loss=3.7432374954223633
I0307 02:31:01.766771 140296826373888 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.5847537517547607, loss=3.6081137657165527
I0307 02:31:03.693595 140452179379392 spec.py:321] Evaluating on the training split.
I0307 02:31:15.467822 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 02:31:37.537606 140452179379392 spec.py:349] Evaluating on the test split.
I0307 02:31:39.358862 140452179379392 submission_runner.py:469] Time since start: 1287.64s, 	Step: 2706, 	{'train/accuracy': 0.3202327787876129, 'train/loss': 3.1716320514678955, 'validation/accuracy': 0.27947998046875, 'validation/loss': 3.450259208679199, 'validation/num_examples': 50000, 'test/accuracy': 0.21220001578330994, 'test/loss': 4.076381683349609, 'test/num_examples': 10000, 'score': 1078.233915567398, 'total_duration': 1287.6449422836304, 'accumulated_submission_time': 1078.233915567398, 'accumulated_eval_time': 208.88624215126038, 'accumulated_logging_time': 0.13827133178710938}
I0307 02:31:39.381483 140296834766592 logging_writer.py:48] [2706] accumulated_eval_time=208.886, accumulated_logging_time=0.138271, accumulated_submission_time=1078.23, global_step=2706, preemption_count=0, score=1078.23, test/accuracy=0.2122, test/loss=4.07638, test/num_examples=10000, total_duration=1287.64, train/accuracy=0.320233, train/loss=3.17163, validation/accuracy=0.27948, validation/loss=3.45026, validation/num_examples=50000
I0307 02:32:15.318939 140296826373888 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.288297176361084, loss=3.596238613128662
I0307 02:32:54.016808 140296834766592 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.276756286621094, loss=3.537492513656616
I0307 02:33:32.463063 140296826373888 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.265041351318359, loss=3.582559585571289
I0307 02:34:10.969013 140296834766592 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.7098474502563477, loss=3.39113450050354
I0307 02:34:49.358106 140296826373888 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.02812385559082, loss=3.3235621452331543
I0307 02:35:27.834227 140296834766592 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.8257813453674316, loss=3.2666573524475098
I0307 02:36:06.244955 140296826373888 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.454107284545898, loss=3.293987989425659
I0307 02:36:44.781782 140296834766592 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.6005496978759766, loss=3.0349936485290527
I0307 02:37:23.340091 140296826373888 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.543440580368042, loss=3.1979501247406006
I0307 02:38:01.791345 140296834766592 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.826819896697998, loss=3.0524590015411377
I0307 02:38:39.920458 140296826373888 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.288912057876587, loss=3.1326730251312256
I0307 02:39:18.457148 140296834766592 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.898606061935425, loss=2.9669172763824463
I0307 02:39:57.063568 140296826373888 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.0568835735321045, loss=3.0540568828582764
I0307 02:40:09.717365 140452179379392 spec.py:321] Evaluating on the training split.
I0307 02:40:20.984646 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 02:40:43.991368 140452179379392 spec.py:349] Evaluating on the test split.
I0307 02:40:45.819180 140452179379392 submission_runner.py:469] Time since start: 1834.11s, 	Step: 4034, 	{'train/accuracy': 0.4220742881298065, 'train/loss': 2.564941167831421, 'validation/accuracy': 0.3745799958705902, 'validation/loss': 2.85689640045166, 'validation/num_examples': 50000, 'test/accuracy': 0.2808000147342682, 'test/loss': 3.550607204437256, 'test/num_examples': 10000, 'score': 1588.4089176654816, 'total_duration': 1834.1051959991455, 'accumulated_submission_time': 1588.4089176654816, 'accumulated_eval_time': 244.98796010017395, 'accumulated_logging_time': 0.16879653930664062}
I0307 02:40:45.857582 140296834766592 logging_writer.py:48] [4034] accumulated_eval_time=244.988, accumulated_logging_time=0.168797, accumulated_submission_time=1588.41, global_step=4034, preemption_count=0, score=1588.41, test/accuracy=0.2808, test/loss=3.55061, test/num_examples=10000, total_duration=1834.11, train/accuracy=0.422074, train/loss=2.56494, validation/accuracy=0.37458, validation/loss=2.8569, validation/num_examples=50000
I0307 02:41:11.091064 140296826373888 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.108501672744751, loss=2.893944501876831
I0307 02:41:49.244743 140296834766592 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.7842977046966553, loss=2.8998260498046875
I0307 02:42:27.341754 140296826373888 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.1182456016540527, loss=2.8427000045776367
I0307 02:43:05.665392 140296834766592 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.3738925457000732, loss=2.8281052112579346
I0307 02:43:43.916534 140296826373888 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.733114719390869, loss=2.7369272708892822
I0307 02:44:21.968795 140296834766592 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.6650586128234863, loss=2.789504289627075
I0307 02:45:02.668906 140296826373888 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.623500108718872, loss=2.8389599323272705
I0307 02:45:41.596883 140296834766592 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.026841163635254, loss=2.816244602203369
I0307 02:46:19.256235 140296826373888 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.0043537616729736, loss=2.764087677001953
I0307 02:46:58.788106 140296834766592 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.776341199874878, loss=2.8275604248046875
I0307 02:47:37.703561 140296826373888 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.591049313545227, loss=2.6085779666900635
I0307 02:48:16.242001 140296834766592 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.6041855812072754, loss=2.5748772621154785
I0307 02:48:55.410788 140296826373888 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.928739309310913, loss=2.5117554664611816
I0307 02:49:15.839343 140452179379392 spec.py:321] Evaluating on the training split.
I0307 02:49:28.445965 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 02:49:50.324458 140452179379392 spec.py:349] Evaluating on the test split.
I0307 02:49:52.132653 140452179379392 submission_runner.py:469] Time since start: 2380.42s, 	Step: 5354, 	{'train/accuracy': 0.5254504084587097, 'train/loss': 2.0259480476379395, 'validation/accuracy': 0.4670799970626831, 'validation/loss': 2.3427321910858154, 'validation/num_examples': 50000, 'test/accuracy': 0.35920003056526184, 'test/loss': 3.03536057472229, 'test/num_examples': 10000, 'score': 2098.2202339172363, 'total_duration': 2380.4187211990356, 'accumulated_submission_time': 2098.2202339172363, 'accumulated_eval_time': 281.28122782707214, 'accumulated_logging_time': 0.22119140625}
I0307 02:49:52.196617 140296834766592 logging_writer.py:48] [5354] accumulated_eval_time=281.281, accumulated_logging_time=0.221191, accumulated_submission_time=2098.22, global_step=5354, preemption_count=0, score=2098.22, test/accuracy=0.3592, test/loss=3.03536, test/num_examples=10000, total_duration=2380.42, train/accuracy=0.52545, train/loss=2.02595, validation/accuracy=0.46708, validation/loss=2.34273, validation/num_examples=50000
I0307 02:50:10.201374 140296826373888 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.559685230255127, loss=2.6176962852478027
I0307 02:50:48.929010 140296834766592 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.403184175491333, loss=2.511580228805542
I0307 02:51:27.564946 140296826373888 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.532548189163208, loss=2.5257880687713623
I0307 02:52:06.116546 140296834766592 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.0196495056152344, loss=2.4528985023498535
I0307 02:52:44.615335 140296826373888 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.614147663116455, loss=2.5189263820648193
I0307 02:53:22.860520 140296834766592 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.6128804683685303, loss=2.5124828815460205
I0307 02:54:01.167516 140296826373888 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.0380008220672607, loss=2.5335428714752197
I0307 02:54:39.381542 140296834766592 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.5382821559906006, loss=2.4524035453796387
I0307 02:55:17.919605 140296826373888 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.4105470180511475, loss=2.341925859451294
I0307 02:55:56.577922 140296834766592 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.171222686767578, loss=2.2747750282287598
I0307 02:56:35.064179 140296826373888 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.0835063457489014, loss=2.4399607181549072
I0307 02:57:13.490963 140296834766592 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.021458387374878, loss=2.4144537448883057
I0307 02:57:52.142488 140296826373888 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.048008441925049, loss=2.4355335235595703
I0307 02:58:22.170630 140452179379392 spec.py:321] Evaluating on the training split.
I0307 02:58:33.358101 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 02:58:53.728249 140452179379392 spec.py:349] Evaluating on the test split.
I0307 02:58:55.591468 140452179379392 submission_runner.py:469] Time since start: 2923.88s, 	Step: 6679, 	{'train/accuracy': 0.5712491869926453, 'train/loss': 1.7849279642105103, 'validation/accuracy': 0.5128600001335144, 'validation/loss': 2.0951907634735107, 'validation/num_examples': 50000, 'test/accuracy': 0.3945000171661377, 'test/loss': 2.8610944747924805, 'test/num_examples': 10000, 'score': 2608.0634620189667, 'total_duration': 2923.8775482177734, 'accumulated_submission_time': 2608.0634620189667, 'accumulated_eval_time': 314.70203375816345, 'accumulated_logging_time': 0.292834997177124}
I0307 02:58:55.625013 140296834766592 logging_writer.py:48] [6679] accumulated_eval_time=314.702, accumulated_logging_time=0.292835, accumulated_submission_time=2608.06, global_step=6679, preemption_count=0, score=2608.06, test/accuracy=0.3945, test/loss=2.86109, test/num_examples=10000, total_duration=2923.88, train/accuracy=0.571249, train/loss=1.78493, validation/accuracy=0.51286, validation/loss=2.09519, validation/num_examples=50000
I0307 02:59:04.130709 140296826373888 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.0248496532440186, loss=2.3205907344818115
I0307 02:59:42.802717 140296834766592 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.792428731918335, loss=2.3311076164245605
I0307 03:00:21.474549 140296826373888 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.1003148555755615, loss=2.2741894721984863
I0307 03:01:00.464623 140296834766592 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.7655152082443237, loss=2.4402167797088623
I0307 03:01:39.323287 140296826373888 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.9541314840316772, loss=2.272590160369873
I0307 03:02:17.699676 140296834766592 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.9746062755584717, loss=2.3728437423706055
I0307 03:02:55.988122 140296826373888 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.6787041425704956, loss=2.2922544479370117
I0307 03:03:34.575916 140296834766592 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.5590530633926392, loss=2.371626377105713
I0307 03:04:13.064911 140296826373888 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.329024076461792, loss=2.181502103805542
I0307 03:04:51.791951 140296834766592 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.1414055824279785, loss=2.357100009918213
I0307 03:05:30.089506 140296826373888 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.1089134216308594, loss=2.328110694885254
I0307 03:06:08.593807 140296834766592 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.803825855255127, loss=2.2552297115325928
I0307 03:06:47.281149 140296826373888 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.745216727256775, loss=2.183259963989258
I0307 03:07:25.549760 140296834766592 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.5850138664245605, loss=2.22868275642395
I0307 03:07:25.979788 140452179379392 spec.py:321] Evaluating on the training split.
I0307 03:07:36.848154 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 03:07:59.651157 140452179379392 spec.py:349] Evaluating on the test split.
I0307 03:08:01.421647 140452179379392 submission_runner.py:469] Time since start: 3469.71s, 	Step: 8002, 	{'train/accuracy': 0.5975366830825806, 'train/loss': 1.6618019342422485, 'validation/accuracy': 0.535860002040863, 'validation/loss': 1.997391700744629, 'validation/num_examples': 50000, 'test/accuracy': 0.4132000207901001, 'test/loss': 2.7312655448913574, 'test/num_examples': 10000, 'score': 3118.2666482925415, 'total_duration': 3469.707709789276, 'accumulated_submission_time': 3118.2666482925415, 'accumulated_eval_time': 350.1438443660736, 'accumulated_logging_time': 0.34224510192871094}
I0307 03:08:01.452334 140296826373888 logging_writer.py:48] [8002] accumulated_eval_time=350.144, accumulated_logging_time=0.342245, accumulated_submission_time=3118.27, global_step=8002, preemption_count=0, score=3118.27, test/accuracy=0.4132, test/loss=2.73127, test/num_examples=10000, total_duration=3469.71, train/accuracy=0.597537, train/loss=1.6618, validation/accuracy=0.53586, validation/loss=1.99739, validation/num_examples=50000
I0307 03:08:39.112333 140296834766592 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.58083176612854, loss=2.1293745040893555
I0307 03:09:17.564734 140296826373888 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.780202031135559, loss=2.165491819381714
I0307 03:09:56.976033 140296834766592 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.9914031028747559, loss=2.155425548553467
I0307 03:10:35.340616 140296826373888 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.4076346158981323, loss=2.254647731781006
I0307 03:11:14.297008 140296834766592 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.5996205806732178, loss=2.127840042114258
I0307 03:11:53.451332 140296826373888 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.6362671852111816, loss=2.1911702156066895
I0307 03:12:32.206069 140296834766592 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.3152624368667603, loss=2.1431479454040527
I0307 03:13:11.731282 140296826373888 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.9570249319076538, loss=2.2056643962860107
I0307 03:13:50.033058 140296834766592 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.0030784606933594, loss=2.1411967277526855
I0307 03:14:28.629030 140296826373888 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.8289462327957153, loss=2.148282766342163
I0307 03:15:07.012310 140296834766592 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.4731674194335938, loss=2.1834301948547363
I0307 03:15:45.304265 140296826373888 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.117669105529785, loss=2.190981388092041
I0307 03:16:23.675039 140296834766592 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.7712078094482422, loss=2.1866090297698975
I0307 03:16:31.732192 140452179379392 spec.py:321] Evaluating on the training split.
I0307 03:16:42.986048 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 03:17:03.397164 140452179379392 spec.py:349] Evaluating on the test split.
I0307 03:17:05.244477 140452179379392 submission_runner.py:469] Time since start: 4013.53s, 	Step: 9322, 	{'train/accuracy': 0.6185826063156128, 'train/loss': 1.5447943210601807, 'validation/accuracy': 0.5566799640655518, 'validation/loss': 1.8915765285491943, 'validation/num_examples': 50000, 'test/accuracy': 0.43790000677108765, 'test/loss': 2.6292927265167236, 'test/num_examples': 10000, 'score': 3628.397201538086, 'total_duration': 4013.5305309295654, 'accumulated_submission_time': 3628.397201538086, 'accumulated_eval_time': 383.65606713294983, 'accumulated_logging_time': 0.3810913562774658}
I0307 03:17:05.270030 140296826373888 logging_writer.py:48] [9322] accumulated_eval_time=383.656, accumulated_logging_time=0.381091, accumulated_submission_time=3628.4, global_step=9322, preemption_count=0, score=3628.4, test/accuracy=0.4379, test/loss=2.62929, test/num_examples=10000, total_duration=4013.53, train/accuracy=0.618583, train/loss=1.54479, validation/accuracy=0.55668, validation/loss=1.89158, validation/num_examples=50000
I0307 03:17:35.589360 140296834766592 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.5059770345687866, loss=2.091911554336548
I0307 03:18:14.295882 140296826373888 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.6578667163848877, loss=2.0930538177490234
I0307 03:18:52.704246 140296834766592 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.529111623764038, loss=2.0800459384918213
I0307 03:19:31.269501 140296826373888 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.149444103240967, loss=2.1783390045166016
I0307 03:20:09.634860 140296834766592 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.7846053838729858, loss=2.0465402603149414
I0307 03:20:48.565018 140296826373888 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.400656819343567, loss=2.129884719848633
I0307 03:21:27.289232 140296834766592 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.6263071298599243, loss=2.1018588542938232
I0307 03:22:06.767946 140296826373888 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.1610794067382812, loss=1.9713060855865479
I0307 03:22:45.988151 140296834766592 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.9716640710830688, loss=2.049572706222534
I0307 03:23:24.528991 140296826373888 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.3426581621170044, loss=2.0352566242218018
I0307 03:24:02.796288 140296834766592 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.827283501625061, loss=2.0861876010894775
I0307 03:24:41.420091 140296826373888 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.514137625694275, loss=2.183289051055908
I0307 03:25:19.917494 140296834766592 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.682120680809021, loss=2.1229095458984375
I0307 03:25:35.253340 140452179379392 spec.py:321] Evaluating on the training split.
I0307 03:25:52.128460 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 03:26:12.082787 140452179379392 spec.py:349] Evaluating on the test split.
I0307 03:26:13.923558 140452179379392 submission_runner.py:469] Time since start: 4562.21s, 	Step: 10641, 	{'train/accuracy': 0.6275510191917419, 'train/loss': 1.5131089687347412, 'validation/accuracy': 0.5677599906921387, 'validation/loss': 1.8413199186325073, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.5470688343048096, 'test/num_examples': 10000, 'score': 4138.229695558548, 'total_duration': 4562.209621667862, 'accumulated_submission_time': 4138.229695558548, 'accumulated_eval_time': 422.3262388706207, 'accumulated_logging_time': 0.41465187072753906}
I0307 03:26:14.002538 140296826373888 logging_writer.py:48] [10641] accumulated_eval_time=422.326, accumulated_logging_time=0.414652, accumulated_submission_time=4138.23, global_step=10641, preemption_count=0, score=4138.23, test/accuracy=0.4445, test/loss=2.54707, test/num_examples=10000, total_duration=4562.21, train/accuracy=0.627551, train/loss=1.51311, validation/accuracy=0.56776, validation/loss=1.84132, validation/num_examples=50000
I0307 03:26:37.144505 140296834766592 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.3710570335388184, loss=2.0039637088775635
I0307 03:27:15.491655 140296826373888 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.100008010864258, loss=2.0194125175476074
I0307 03:27:53.825631 140296834766592 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.6978442668914795, loss=1.9489235877990723
I0307 03:28:32.012105 140296826373888 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.6274890899658203, loss=1.9715747833251953
I0307 03:29:10.515369 140296834766592 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.557970404624939, loss=2.071120262145996
I0307 03:29:48.985859 140296826373888 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.3396183252334595, loss=1.9606053829193115
I0307 03:30:27.963981 140296834766592 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.5716890096664429, loss=2.0332274436950684
I0307 03:31:06.300051 140296826373888 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.5443205833435059, loss=2.0053112506866455
I0307 03:31:44.891995 140296834766592 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.7059558629989624, loss=2.0058047771453857
I0307 03:32:23.223930 140296826373888 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.5851221084594727, loss=1.9022330045700073
I0307 03:33:01.474122 140296834766592 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.4546312093734741, loss=2.0134365558624268
I0307 03:33:39.772536 140296826373888 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.9659981727600098, loss=1.9013381004333496
I0307 03:34:18.413418 140296834766592 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.391540288925171, loss=1.9289777278900146
I0307 03:34:43.950179 140452179379392 spec.py:321] Evaluating on the training split.
I0307 03:34:56.602944 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 03:35:16.302188 140452179379392 spec.py:349] Evaluating on the test split.
I0307 03:35:18.159980 140452179379392 submission_runner.py:469] Time since start: 5106.45s, 	Step: 11967, 	{'train/accuracy': 0.6413424611091614, 'train/loss': 1.4538108110427856, 'validation/accuracy': 0.5785199999809265, 'validation/loss': 1.7881860733032227, 'validation/num_examples': 50000, 'test/accuracy': 0.4458000063896179, 'test/loss': 2.533802032470703, 'test/num_examples': 10000, 'score': 4648.0253529548645, 'total_duration': 5106.446059465408, 'accumulated_submission_time': 4648.0253529548645, 'accumulated_eval_time': 456.53600883483887, 'accumulated_logging_time': 0.5018694400787354}
I0307 03:35:18.231471 140296826373888 logging_writer.py:48] [11967] accumulated_eval_time=456.536, accumulated_logging_time=0.501869, accumulated_submission_time=4648.03, global_step=11967, preemption_count=0, score=4648.03, test/accuracy=0.4458, test/loss=2.5338, test/num_examples=10000, total_duration=5106.45, train/accuracy=0.641342, train/loss=1.45381, validation/accuracy=0.57852, validation/loss=1.78819, validation/num_examples=50000
I0307 03:35:31.477864 140296834766592 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.7680028676986694, loss=1.9724395275115967
I0307 03:36:10.455300 140296826373888 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.5689265727996826, loss=1.9795501232147217
I0307 03:36:53.944091 140296834766592 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6345374584197998, loss=2.0231094360351562
I0307 03:37:52.879706 140296826373888 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.3503713607788086, loss=2.0533556938171387
I0307 03:38:32.325134 140296834766592 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.6491068601608276, loss=1.914433240890503
I0307 03:39:11.196097 140296826373888 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.3567497730255127, loss=1.8436133861541748
I0307 03:39:50.268956 140296834766592 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.7728335857391357, loss=1.9214675426483154
I0307 03:40:29.015852 140296826373888 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4051302671432495, loss=1.9794151782989502
I0307 03:41:07.571291 140296834766592 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.7703526020050049, loss=1.8604905605316162
I0307 03:41:46.551416 140296826373888 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.5316872596740723, loss=1.8655264377593994
I0307 03:42:25.282412 140296834766592 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.6424567699432373, loss=1.949800729751587
I0307 03:43:04.409299 140296826373888 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.6293491125106812, loss=1.9042142629623413
I0307 03:43:43.438579 140296834766592 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4527853727340698, loss=1.8865716457366943
I0307 03:43:48.403362 140452179379392 spec.py:321] Evaluating on the training split.
I0307 03:44:04.838246 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 03:44:25.752095 140452179379392 spec.py:349] Evaluating on the test split.
I0307 03:44:27.590787 140452179379392 submission_runner.py:469] Time since start: 5655.88s, 	Step: 13214, 	{'train/accuracy': 0.6535594463348389, 'train/loss': 1.3951847553253174, 'validation/accuracy': 0.5795400142669678, 'validation/loss': 1.775004506111145, 'validation/num_examples': 50000, 'test/accuracy': 0.4613000154495239, 'test/loss': 2.5031328201293945, 'test/num_examples': 10000, 'score': 5158.053788900375, 'total_duration': 5655.876765489578, 'accumulated_submission_time': 5158.053788900375, 'accumulated_eval_time': 495.7233021259308, 'accumulated_logging_time': 0.5809483528137207}
I0307 03:44:27.665580 140296826373888 logging_writer.py:48] [13214] accumulated_eval_time=495.723, accumulated_logging_time=0.580948, accumulated_submission_time=5158.05, global_step=13214, preemption_count=0, score=5158.05, test/accuracy=0.4613, test/loss=2.50313, test/num_examples=10000, total_duration=5655.88, train/accuracy=0.653559, train/loss=1.39518, validation/accuracy=0.57954, validation/loss=1.775, validation/num_examples=50000
I0307 03:45:01.128461 140296834766592 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.3060380220413208, loss=1.9204421043395996
I0307 03:45:39.631117 140296826373888 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.3493856191635132, loss=2.0093741416931152
I0307 03:46:20.005611 140296834766592 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.664412498474121, loss=1.9701356887817383
I0307 03:46:58.492051 140296826373888 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.643601894378662, loss=1.842543601989746
I0307 03:47:37.217998 140296834766592 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.62327241897583, loss=1.8795051574707031
I0307 03:48:16.054656 140296826373888 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.5132805109024048, loss=1.9708893299102783
I0307 03:48:54.540746 140296834766592 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.3239701986312866, loss=1.8402583599090576
I0307 03:49:33.345273 140296826373888 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.3563778400421143, loss=1.8888529539108276
I0307 03:50:12.014235 140296834766592 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.6866427659988403, loss=1.8129394054412842
I0307 03:50:50.637513 140296826373888 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.8186638355255127, loss=2.029693365097046
I0307 03:51:29.382678 140296834766592 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.3864823579788208, loss=1.9653335809707642
I0307 03:52:08.247340 140296826373888 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.4320812225341797, loss=1.8796806335449219
I0307 03:52:46.578303 140296834766592 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.9449796676635742, loss=2.0610599517822266
I0307 03:52:57.673263 140452179379392 spec.py:321] Evaluating on the training split.
I0307 03:53:18.044911 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 03:53:39.377341 140452179379392 spec.py:349] Evaluating on the test split.
I0307 03:53:41.228352 140452179379392 submission_runner.py:469] Time since start: 6209.51s, 	Step: 14530, 	{'train/accuracy': 0.6630261540412903, 'train/loss': 1.3424205780029297, 'validation/accuracy': 0.5957199931144714, 'validation/loss': 1.698004126548767, 'validation/num_examples': 50000, 'test/accuracy': 0.4620000123977661, 'test/loss': 2.4951677322387695, 'test/num_examples': 10000, 'score': 5667.884410381317, 'total_duration': 6209.514318704605, 'accumulated_submission_time': 5667.884410381317, 'accumulated_eval_time': 539.2782566547394, 'accumulated_logging_time': 0.6986403465270996}
I0307 03:53:41.376818 140296826373888 logging_writer.py:48] [14530] accumulated_eval_time=539.278, accumulated_logging_time=0.69864, accumulated_submission_time=5667.88, global_step=14530, preemption_count=0, score=5667.88, test/accuracy=0.462, test/loss=2.49517, test/num_examples=10000, total_duration=6209.51, train/accuracy=0.663026, train/loss=1.34242, validation/accuracy=0.59572, validation/loss=1.698, validation/num_examples=50000
I0307 03:54:08.989567 140296834766592 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.824791669845581, loss=1.9239675998687744
I0307 03:54:47.459345 140296826373888 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.7231786251068115, loss=1.9006695747375488
I0307 03:55:26.072655 140296834766592 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.9624780416488647, loss=1.7699908018112183
I0307 03:56:04.452553 140296826373888 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.6848602294921875, loss=1.8465931415557861
I0307 03:56:42.927133 140296834766592 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.4797492027282715, loss=1.9462604522705078
I0307 03:57:21.986857 140296834766592 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.4140137434005737, loss=2.0168066024780273
I0307 03:58:00.729296 140296826373888 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.5389944314956665, loss=2.066981077194214
I0307 03:58:39.344474 140296834766592 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4998064041137695, loss=1.916304349899292
I0307 03:59:18.137030 140296826373888 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.4613745212554932, loss=1.9731248617172241
I0307 03:59:56.815291 140296834766592 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.5574874877929688, loss=1.9162737131118774
I0307 04:00:35.150536 140296826373888 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.5089725255966187, loss=1.8497711420059204
I0307 04:01:13.684783 140296834766592 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.4422190189361572, loss=1.8332507610321045
I0307 04:01:52.436608 140296826373888 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.4904264211654663, loss=1.9329674243927002
I0307 04:02:11.362901 140452179379392 spec.py:321] Evaluating on the training split.
I0307 04:02:29.226945 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 04:02:53.409430 140452179379392 spec.py:349] Evaluating on the test split.
I0307 04:02:55.398195 140452179379392 submission_runner.py:469] Time since start: 6763.68s, 	Step: 15850, 	{'train/accuracy': 0.6654974222183228, 'train/loss': 1.334952473640442, 'validation/accuracy': 0.5925599932670593, 'validation/loss': 1.700812816619873, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.4369471073150635, 'test/num_examples': 10000, 'score': 6177.715204238892, 'total_duration': 6763.684148788452, 'accumulated_submission_time': 6177.715204238892, 'accumulated_eval_time': 583.313392162323, 'accumulated_logging_time': 0.8645684719085693}
I0307 04:02:55.522290 140296834766592 logging_writer.py:48] [15850] accumulated_eval_time=583.313, accumulated_logging_time=0.864568, accumulated_submission_time=6177.72, global_step=15850, preemption_count=0, score=6177.72, test/accuracy=0.4716, test/loss=2.43695, test/num_examples=10000, total_duration=6763.68, train/accuracy=0.665497, train/loss=1.33495, validation/accuracy=0.59256, validation/loss=1.70081, validation/num_examples=50000
I0307 04:03:15.201503 140296826373888 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.3024756908416748, loss=1.8216665983200073
I0307 04:03:53.976490 140296834766592 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.7330131530761719, loss=1.8578928709030151
I0307 04:04:32.476550 140296826373888 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.576969027519226, loss=1.8281104564666748
I0307 04:05:11.270301 140296834766592 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.6919018030166626, loss=1.9604227542877197
I0307 04:05:50.431875 140296826373888 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.5278408527374268, loss=1.774709701538086
I0307 04:06:28.823505 140296834766592 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.7645277976989746, loss=1.9350829124450684
I0307 04:07:07.983144 140296826373888 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.3959547281265259, loss=1.8368093967437744
I0307 04:07:46.257035 140296834766592 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.415118932723999, loss=1.74944007396698
I0307 04:08:25.110643 140296826373888 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.7607566118240356, loss=1.8453373908996582
I0307 04:09:03.930694 140296834766592 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.9449825286865234, loss=1.8636889457702637
I0307 04:09:42.624605 140296826373888 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.5666520595550537, loss=1.9141372442245483
I0307 04:10:21.388874 140296834766592 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.6628947257995605, loss=1.8295183181762695
I0307 04:10:59.633727 140296826373888 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.573281168937683, loss=1.8242627382278442
I0307 04:11:25.625350 140452179379392 spec.py:321] Evaluating on the training split.
I0307 04:11:42.250634 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 04:12:04.708356 140452179379392 spec.py:349] Evaluating on the test split.
I0307 04:12:06.545706 140452179379392 submission_runner.py:469] Time since start: 7314.83s, 	Step: 17169, 	{'train/accuracy': 0.6803850531578064, 'train/loss': 1.269286036491394, 'validation/accuracy': 0.6047199964523315, 'validation/loss': 1.64718759059906, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.3867063522338867, 'test/num_examples': 10000, 'score': 6687.653282165527, 'total_duration': 7314.831635951996, 'accumulated_submission_time': 6687.653282165527, 'accumulated_eval_time': 624.2335669994354, 'accumulated_logging_time': 1.0213801860809326}
I0307 04:12:06.646634 140296834766592 logging_writer.py:48] [17169] accumulated_eval_time=624.234, accumulated_logging_time=1.02138, accumulated_submission_time=6687.65, global_step=17169, preemption_count=0, score=6687.65, test/accuracy=0.4782, test/loss=2.38671, test/num_examples=10000, total_duration=7314.83, train/accuracy=0.680385, train/loss=1.26929, validation/accuracy=0.60472, validation/loss=1.64719, validation/num_examples=50000
I0307 04:12:19.067594 140296826373888 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.7623590230941772, loss=2.0051896572113037
I0307 04:12:57.421206 140296834766592 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.9802442789077759, loss=1.9127358198165894
I0307 04:13:35.964408 140296826373888 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.6794579029083252, loss=1.9104652404785156
I0307 04:14:14.760639 140296834766592 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.7184377908706665, loss=1.8513096570968628
I0307 04:14:54.126779 140296826373888 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.649444580078125, loss=1.9414756298065186
I0307 04:15:33.105875 140296834766592 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.0495667457580566, loss=1.8024678230285645
I0307 04:16:13.361804 140296826373888 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.3991321325302124, loss=1.8315246105194092
I0307 04:16:51.897098 140296834766592 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.5772264003753662, loss=1.890000343322754
I0307 04:17:30.353357 140296826373888 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.8938415050506592, loss=1.8082892894744873
I0307 04:18:08.582689 140296834766592 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.4507701396942139, loss=1.7546956539154053
I0307 04:18:47.211687 140296826373888 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.6177982091903687, loss=1.9742196798324585
I0307 04:19:26.223621 140296834766592 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.2342026233673096, loss=1.7916992902755737
I0307 04:20:04.996730 140296826373888 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.6089409589767456, loss=1.7876242399215698
I0307 04:20:36.595129 140452179379392 spec.py:321] Evaluating on the training split.
I0307 04:20:53.412630 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 04:21:15.341529 140452179379392 spec.py:349] Evaluating on the test split.
I0307 04:21:17.191166 140452179379392 submission_runner.py:469] Time since start: 7865.48s, 	Step: 18483, 	{'train/accuracy': 0.663504421710968, 'train/loss': 1.3375612497329712, 'validation/accuracy': 0.5932799577713013, 'validation/loss': 1.7013968229293823, 'validation/num_examples': 50000, 'test/accuracy': 0.46560001373291016, 'test/loss': 2.4728827476501465, 'test/num_examples': 10000, 'score': 7197.45038151741, 'total_duration': 7865.477244615555, 'accumulated_submission_time': 7197.45038151741, 'accumulated_eval_time': 664.8295745849609, 'accumulated_logging_time': 1.144235610961914}
I0307 04:21:17.216018 140296834766592 logging_writer.py:48] [18483] accumulated_eval_time=664.83, accumulated_logging_time=1.14424, accumulated_submission_time=7197.45, global_step=18483, preemption_count=0, score=7197.45, test/accuracy=0.4656, test/loss=2.47288, test/num_examples=10000, total_duration=7865.48, train/accuracy=0.663504, train/loss=1.33756, validation/accuracy=0.59328, validation/loss=1.7014, validation/num_examples=50000
I0307 04:21:24.114793 140296826373888 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.7479078769683838, loss=1.7588351964950562
I0307 04:22:02.988327 140296834766592 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.936179757118225, loss=1.7434684038162231
I0307 04:22:42.099690 140296826373888 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.974018931388855, loss=1.9153438806533813
I0307 04:23:20.954110 140296834766592 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.008662700653076, loss=1.8704677820205688
I0307 04:23:59.457763 140296826373888 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.656995415687561, loss=1.8469183444976807
I0307 04:24:38.355861 140296834766592 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.803293228149414, loss=1.9111034870147705
I0307 04:25:17.123825 140296826373888 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.8364205360412598, loss=1.8367680311203003
I0307 04:25:56.831432 140296834766592 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.706007719039917, loss=1.8448654413223267
I0307 04:26:36.033320 140296826373888 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.4992519617080688, loss=1.797830581665039
I0307 04:27:14.259164 140296834766592 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.6230112314224243, loss=1.8578656911849976
I0307 04:27:52.929325 140296826373888 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.6598502397537231, loss=1.8058316707611084
I0307 04:28:31.705591 140296834766592 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.7823735475540161, loss=1.7483508586883545
I0307 04:29:10.624574 140296826373888 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.8184359073638916, loss=1.708686351776123
I0307 04:29:47.415225 140452179379392 spec.py:321] Evaluating on the training split.
I0307 04:30:01.684450 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 04:30:22.941385 140452179379392 spec.py:349] Evaluating on the test split.
I0307 04:30:24.769604 140452179379392 submission_runner.py:469] Time since start: 8413.05s, 	Step: 19797, 	{'train/accuracy': 0.6736088991165161, 'train/loss': 1.3136613368988037, 'validation/accuracy': 0.6010800004005432, 'validation/loss': 1.6835243701934814, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.4559056758880615, 'test/num_examples': 10000, 'score': 7707.508712053299, 'total_duration': 8413.045548200607, 'accumulated_submission_time': 7707.508712053299, 'accumulated_eval_time': 702.173793554306, 'accumulated_logging_time': 1.1776418685913086}
I0307 04:30:24.789421 140296834766592 logging_writer.py:48] [19797] accumulated_eval_time=702.174, accumulated_logging_time=1.17764, accumulated_submission_time=7707.51, global_step=19797, preemption_count=0, score=7707.51, test/accuracy=0.4697, test/loss=2.45591, test/num_examples=10000, total_duration=8413.05, train/accuracy=0.673609, train/loss=1.31366, validation/accuracy=0.60108, validation/loss=1.68352, validation/num_examples=50000
I0307 04:30:26.371206 140296826373888 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6135295629501343, loss=1.6629668474197388
I0307 04:31:05.236155 140296834766592 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.5378655195236206, loss=1.757593035697937
I0307 04:31:43.719403 140296826373888 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.9217549562454224, loss=1.810875654220581
I0307 04:32:22.894078 140296834766592 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.2388551235198975, loss=1.7423291206359863
I0307 04:33:01.708530 140296826373888 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.91819167137146, loss=1.8328577280044556
I0307 04:33:40.110676 140296834766592 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.6197388172149658, loss=1.7897710800170898
I0307 04:34:18.878541 140296826373888 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.5062068700790405, loss=1.794134497642517
I0307 04:34:57.799374 140296834766592 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.5722895860671997, loss=1.7004716396331787
I0307 04:35:36.680956 140296826373888 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6146541833877563, loss=1.7815752029418945
I0307 04:36:15.285639 140296834766592 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.7097214460372925, loss=1.8859142065048218
I0307 04:36:53.894923 140296826373888 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.6851621866226196, loss=1.8125488758087158
I0307 04:37:32.546071 140296834766592 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.586584448814392, loss=1.7370139360427856
I0307 04:38:11.351798 140296826373888 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.7105594873428345, loss=1.7358734607696533
I0307 04:38:50.440879 140296834766592 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.8668384552001953, loss=1.8366262912750244
I0307 04:38:55.009802 140452179379392 spec.py:321] Evaluating on the training split.
I0307 04:39:11.882435 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 04:39:32.488980 140452179379392 spec.py:349] Evaluating on the test split.
I0307 04:39:34.340112 140452179379392 submission_runner.py:469] Time since start: 8962.63s, 	Step: 21113, 	{'train/accuracy': 0.683613657951355, 'train/loss': 1.255070447921753, 'validation/accuracy': 0.608680009841919, 'validation/loss': 1.6251435279846191, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.3887507915496826, 'test/num_examples': 10000, 'score': 8217.593552827835, 'total_duration': 8962.626177310944, 'accumulated_submission_time': 8217.593552827835, 'accumulated_eval_time': 741.5040574073792, 'accumulated_logging_time': 1.2053511142730713}
I0307 04:39:34.430381 140296826373888 logging_writer.py:48] [21113] accumulated_eval_time=741.504, accumulated_logging_time=1.20535, accumulated_submission_time=8217.59, global_step=21113, preemption_count=0, score=8217.59, test/accuracy=0.4838, test/loss=2.38875, test/num_examples=10000, total_duration=8962.63, train/accuracy=0.683614, train/loss=1.25507, validation/accuracy=0.60868, validation/loss=1.62514, validation/num_examples=50000
I0307 04:40:08.384512 140296834766592 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.6673887968063354, loss=1.869810700416565
I0307 04:40:47.199672 140296826373888 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.687956690788269, loss=1.888334035873413
I0307 04:41:25.614883 140296834766592 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.597253680229187, loss=1.7762049436569214
I0307 04:42:04.743899 140296826373888 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.8605434894561768, loss=1.954042673110962
I0307 04:42:44.278444 140296834766592 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.4925850629806519, loss=1.8128641843795776
I0307 04:43:23.282293 140296826373888 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.5705702304840088, loss=1.789083480834961
I0307 04:44:01.892827 140296834766592 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.6907585859298706, loss=1.8113499879837036
I0307 04:44:40.780005 140296826373888 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.5017945766448975, loss=1.6700489521026611
I0307 04:45:19.140120 140296834766592 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.713584065437317, loss=1.8287721872329712
I0307 04:45:58.151500 140296826373888 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.6483300924301147, loss=1.6775591373443604
I0307 04:46:37.067796 140296834766592 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.629407525062561, loss=1.8257147073745728
I0307 04:47:15.718525 140296826373888 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.5301119089126587, loss=1.729978084564209
I0307 04:47:54.606850 140296834766592 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.7171893119812012, loss=1.7694072723388672
I0307 04:48:04.552912 140452179379392 spec.py:321] Evaluating on the training split.
I0307 04:48:18.199820 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 04:48:39.856304 140452179379392 spec.py:349] Evaluating on the test split.
I0307 04:48:41.725984 140452179379392 submission_runner.py:469] Time since start: 9509.99s, 	Step: 22427, 	{'train/accuracy': 0.6825374364852905, 'train/loss': 1.243003010749817, 'validation/accuracy': 0.6122199892997742, 'validation/loss': 1.6221086978912354, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.3713154792785645, 'test/num_examples': 10000, 'score': 8727.54271864891, 'total_duration': 9509.988741397858, 'accumulated_submission_time': 8727.54271864891, 'accumulated_eval_time': 778.6537783145905, 'accumulated_logging_time': 1.337156057357788}
I0307 04:48:41.749940 140296826373888 logging_writer.py:48] [22427] accumulated_eval_time=778.654, accumulated_logging_time=1.33716, accumulated_submission_time=8727.54, global_step=22427, preemption_count=0, score=8727.54, test/accuracy=0.4838, test/loss=2.37132, test/num_examples=10000, total_duration=9509.99, train/accuracy=0.682537, train/loss=1.243, validation/accuracy=0.61222, validation/loss=1.62211, validation/num_examples=50000
I0307 04:49:10.126492 140296834766592 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.5440778732299805, loss=1.7159239053726196
I0307 04:49:49.066951 140296826373888 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.6659072637557983, loss=1.6738224029541016
I0307 04:50:27.837812 140296834766592 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.699834942817688, loss=1.722848653793335
I0307 04:51:06.709852 140296826373888 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.6343125104904175, loss=1.8913929462432861
I0307 04:51:45.577699 140296834766592 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.9179606437683105, loss=1.8388376235961914
I0307 04:52:24.517181 140296826373888 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.7973637580871582, loss=1.8189996480941772
I0307 04:53:03.357212 140296834766592 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.194629430770874, loss=1.781232237815857
I0307 04:53:41.917689 140296826373888 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.7111624479293823, loss=1.924288272857666
I0307 04:54:22.421666 140296834766592 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.776210069656372, loss=1.9201138019561768
I0307 04:55:01.716138 140296826373888 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.6709372997283936, loss=1.9372713565826416
I0307 04:55:40.376242 140296834766592 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.653555154800415, loss=1.8227510452270508
I0307 04:56:19.087991 140296826373888 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.6722359657287598, loss=1.6833868026733398
I0307 04:56:58.021337 140296834766592 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.4599124193191528, loss=1.7092616558074951
I0307 04:57:11.984397 140452179379392 spec.py:321] Evaluating on the training split.
I0307 04:57:30.137649 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 04:57:51.546085 140452179379392 spec.py:349] Evaluating on the test split.
I0307 04:57:53.344146 140452179379392 submission_runner.py:469] Time since start: 10061.63s, 	Step: 23737, 	{'train/accuracy': 0.6640226244926453, 'train/loss': 1.3183033466339111, 'validation/accuracy': 0.6015399694442749, 'validation/loss': 1.6845316886901855, 'validation/num_examples': 50000, 'test/accuracy': 0.4718000292778015, 'test/loss': 2.4605982303619385, 'test/num_examples': 10000, 'score': 9237.619418621063, 'total_duration': 10061.630222797394, 'accumulated_submission_time': 9237.619418621063, 'accumulated_eval_time': 820.0134935379028, 'accumulated_logging_time': 1.3854548931121826}
I0307 04:57:53.375533 140296826373888 logging_writer.py:48] [23737] accumulated_eval_time=820.013, accumulated_logging_time=1.38545, accumulated_submission_time=9237.62, global_step=23737, preemption_count=0, score=9237.62, test/accuracy=0.4718, test/loss=2.4606, test/num_examples=10000, total_duration=10061.6, train/accuracy=0.664023, train/loss=1.3183, validation/accuracy=0.60154, validation/loss=1.68453, validation/num_examples=50000
I0307 04:58:18.854263 140296834766592 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.9623332023620605, loss=1.7625224590301514
I0307 04:58:57.477565 140296826373888 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.9303338527679443, loss=1.953676700592041
I0307 04:59:36.622849 140296834766592 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.5348047018051147, loss=1.8696980476379395
I0307 05:00:15.950133 140296826373888 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.6527018547058105, loss=1.7330011129379272
I0307 05:00:54.583635 140296834766592 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.7663547992706299, loss=1.6990611553192139
I0307 05:01:33.151760 140296826373888 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.8813365697860718, loss=1.842305302619934
I0307 05:02:12.013607 140296834766592 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.7506698369979858, loss=1.7822601795196533
I0307 05:02:50.806035 140296826373888 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.6713573932647705, loss=1.873166799545288
I0307 05:03:29.462531 140296834766592 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.7834107875823975, loss=1.8322805166244507
I0307 05:04:08.413056 140296826373888 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.7279900312423706, loss=1.7324364185333252
I0307 05:04:47.321790 140296834766592 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.6349581480026245, loss=1.5977590084075928
I0307 05:05:26.141083 140296826373888 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.760980248451233, loss=1.695141315460205
I0307 05:06:05.093458 140296834766592 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.7021738290786743, loss=1.7910611629486084
I0307 05:06:23.710148 140452179379392 spec.py:321] Evaluating on the training split.
I0307 05:06:38.243119 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 05:07:00.951423 140452179379392 spec.py:349] Evaluating on the test split.
I0307 05:07:02.740302 140452179379392 submission_runner.py:469] Time since start: 10611.03s, 	Step: 25048, 	{'train/accuracy': 0.6875, 'train/loss': 1.2308251857757568, 'validation/accuracy': 0.614359974861145, 'validation/loss': 1.6169533729553223, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.381502389907837, 'test/num_examples': 10000, 'score': 9747.735475063324, 'total_duration': 10611.026360988617, 'accumulated_submission_time': 9747.735475063324, 'accumulated_eval_time': 859.0435960292816, 'accumulated_logging_time': 1.5049934387207031}
I0307 05:07:02.806291 140296826373888 logging_writer.py:48] [25048] accumulated_eval_time=859.044, accumulated_logging_time=1.50499, accumulated_submission_time=9747.74, global_step=25048, preemption_count=0, score=9747.74, test/accuracy=0.4862, test/loss=2.3815, test/num_examples=10000, total_duration=10611, train/accuracy=0.6875, train/loss=1.23083, validation/accuracy=0.61436, validation/loss=1.61695, validation/num_examples=50000
I0307 05:07:23.621818 140296834766592 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.7103841304779053, loss=1.8469278812408447
I0307 05:08:02.318368 140296826373888 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.6586942672729492, loss=1.6484066247940063
I0307 05:08:41.110023 140296834766592 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.899556040763855, loss=1.991640567779541
I0307 05:09:19.785420 140296826373888 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.7580426931381226, loss=1.8103824853897095
I0307 05:09:58.399704 140296834766592 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.6094603538513184, loss=1.8744080066680908
I0307 05:10:37.128152 140296826373888 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.832069993019104, loss=1.812138319015503
I0307 05:11:16.064112 140296834766592 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.5481232404708862, loss=1.7395302057266235
I0307 05:11:54.881138 140296826373888 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.8881287574768066, loss=1.6915713548660278
I0307 05:12:33.389424 140296834766592 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.8796496391296387, loss=1.741519570350647
I0307 05:13:11.984644 140296826373888 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.9006534814834595, loss=1.8517584800720215
I0307 05:13:50.831687 140296834766592 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.030940055847168, loss=1.800527572631836
I0307 05:14:29.116331 140296826373888 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.7075037956237793, loss=1.717320442199707
I0307 05:15:08.312538 140296834766592 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.7409368753433228, loss=1.718420386314392
I0307 05:15:33.040079 140452179379392 spec.py:321] Evaluating on the training split.
I0307 05:15:49.242108 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 05:16:09.889247 140452179379392 spec.py:349] Evaluating on the test split.
I0307 05:16:11.745947 140452179379392 submission_runner.py:469] Time since start: 11160.03s, 	Step: 26365, 	{'train/accuracy': 0.6862643361091614, 'train/loss': 1.2357330322265625, 'validation/accuracy': 0.6197599768638611, 'validation/loss': 1.5840685367584229, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.346871852874756, 'test/num_examples': 10000, 'score': 10257.817177295685, 'total_duration': 11160.032019376755, 'accumulated_submission_time': 10257.817177295685, 'accumulated_eval_time': 897.7494268417358, 'accumulated_logging_time': 1.596116304397583}
I0307 05:16:11.834817 140296826373888 logging_writer.py:48] [26365] accumulated_eval_time=897.749, accumulated_logging_time=1.59612, accumulated_submission_time=10257.8, global_step=26365, preemption_count=0, score=10257.8, test/accuracy=0.492, test/loss=2.34687, test/num_examples=10000, total_duration=11160, train/accuracy=0.686264, train/loss=1.23573, validation/accuracy=0.61976, validation/loss=1.58407, validation/num_examples=50000
I0307 05:16:26.145966 140296834766592 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.658355712890625, loss=1.8677083253860474
I0307 05:17:04.925345 140296826373888 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.7934530973434448, loss=1.6944351196289062
I0307 05:17:43.811082 140296834766592 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6385724544525146, loss=1.8139503002166748
I0307 05:18:22.239346 140296826373888 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.521520733833313, loss=1.7230010032653809
I0307 05:19:01.021558 140296834766592 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.9057008028030396, loss=1.7758134603500366
I0307 05:19:40.110883 140296826373888 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.821291208267212, loss=1.7338995933532715
I0307 05:20:18.817471 140296834766592 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.7254163026809692, loss=1.739703893661499
I0307 05:20:57.496582 140296826373888 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.741424560546875, loss=1.8165439367294312
I0307 05:21:36.189431 140296834766592 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.7545299530029297, loss=1.711851716041565
I0307 05:22:14.982239 140296826373888 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6194665431976318, loss=1.6850860118865967
I0307 05:22:53.803306 140296834766592 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.7573376893997192, loss=1.8055551052093506
I0307 05:23:32.699881 140296826373888 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.890770673751831, loss=1.7940778732299805
I0307 05:24:11.890018 140296834766592 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.832618236541748, loss=1.6394611597061157
I0307 05:24:42.080863 140452179379392 spec.py:321] Evaluating on the training split.
I0307 05:25:00.390680 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 05:25:23.297019 140452179379392 spec.py:349] Evaluating on the test split.
I0307 05:25:25.132750 140452179379392 submission_runner.py:469] Time since start: 11713.42s, 	Step: 27678, 	{'train/accuracy': 0.6795479655265808, 'train/loss': 1.259886622428894, 'validation/accuracy': 0.6112200021743774, 'validation/loss': 1.614196538925171, 'validation/num_examples': 50000, 'test/accuracy': 0.48750001192092896, 'test/loss': 2.3494980335235596, 'test/num_examples': 10000, 'score': 10767.557693481445, 'total_duration': 11713.418690443039, 'accumulated_submission_time': 10767.557693481445, 'accumulated_eval_time': 940.801164150238, 'accumulated_logging_time': 2.0594067573547363}
I0307 05:25:25.272119 140296826373888 logging_writer.py:48] [27678] accumulated_eval_time=940.801, accumulated_logging_time=2.05941, accumulated_submission_time=10767.6, global_step=27678, preemption_count=0, score=10767.6, test/accuracy=0.4875, test/loss=2.3495, test/num_examples=10000, total_duration=11713.4, train/accuracy=0.679548, train/loss=1.25989, validation/accuracy=0.61122, validation/loss=1.6142, validation/num_examples=50000
I0307 05:25:34.318319 140296834766592 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.6232620477676392, loss=1.7836817502975464
I0307 05:26:12.846239 140296826373888 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.9063433408737183, loss=1.7939670085906982
I0307 05:26:51.000085 140296834766592 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.7206629514694214, loss=1.7138324975967407
I0307 05:27:29.577846 140296826373888 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.5089858770370483, loss=1.723214864730835
I0307 05:28:09.023104 140296834766592 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.6172823905944824, loss=1.7873057126998901
I0307 05:28:48.280499 140296826373888 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.7474720478057861, loss=1.7503533363342285
I0307 05:29:26.829905 140296834766592 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.8024864196777344, loss=1.7030754089355469
I0307 05:30:05.340164 140296826373888 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.9490227699279785, loss=1.7676806449890137
I0307 05:30:43.537928 140296834766592 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.7075313329696655, loss=1.8073232173919678
I0307 05:31:22.351250 140296826373888 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.6711243391036987, loss=1.7001080513000488
I0307 05:32:01.159114 140296834766592 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.7926205396652222, loss=1.722025990486145
I0307 05:32:40.593228 140296826373888 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.869173288345337, loss=1.7781496047973633
I0307 05:33:19.351820 140296834766592 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.5978952646255493, loss=1.6599385738372803
I0307 05:33:55.153037 140452179379392 spec.py:321] Evaluating on the training split.
I0307 05:34:10.631687 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 05:34:32.007832 140452179379392 spec.py:349] Evaluating on the test split.
I0307 05:34:33.995258 140452179379392 submission_runner.py:469] Time since start: 12262.28s, 	Step: 28993, 	{'train/accuracy': 0.6900709271430969, 'train/loss': 1.2107036113739014, 'validation/accuracy': 0.6157400012016296, 'validation/loss': 1.582605004310608, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.3410840034484863, 'test/num_examples': 10000, 'score': 11277.26110458374, 'total_duration': 12262.281204938889, 'accumulated_submission_time': 11277.26110458374, 'accumulated_eval_time': 979.6432375907898, 'accumulated_logging_time': 2.244678497314453}
I0307 05:34:34.110690 140296826373888 logging_writer.py:48] [28993] accumulated_eval_time=979.643, accumulated_logging_time=2.24468, accumulated_submission_time=11277.3, global_step=28993, preemption_count=0, score=11277.3, test/accuracy=0.4833, test/loss=2.34108, test/num_examples=10000, total_duration=12262.3, train/accuracy=0.690071, train/loss=1.2107, validation/accuracy=0.61574, validation/loss=1.58261, validation/num_examples=50000
I0307 05:34:37.330994 140296834766592 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.5709240436553955, loss=1.75250244140625
I0307 05:35:16.050928 140296826373888 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.0071938037872314, loss=1.700239658355713
I0307 05:35:54.398282 140296834766592 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.5991226434707642, loss=1.7060601711273193
I0307 05:36:33.102838 140296826373888 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.6228505373001099, loss=1.7332775592803955
I0307 05:37:11.675681 140296834766592 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.6408575773239136, loss=1.747119665145874
I0307 05:37:50.264408 140296826373888 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.6722599267959595, loss=1.5835847854614258
I0307 05:38:28.843185 140296834766592 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.6556780338287354, loss=1.6465214490890503
I0307 05:39:07.479969 140296826373888 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.7146974802017212, loss=1.7335673570632935
I0307 05:39:46.402682 140296834766592 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7330918312072754, loss=1.7257189750671387
I0307 05:40:25.219711 140296826373888 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.8641581535339355, loss=1.6487377882003784
I0307 05:41:03.547811 140296834766592 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.7319566011428833, loss=1.8182239532470703
I0307 05:41:42.670227 140296826373888 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.5949056148529053, loss=1.82488214969635
I0307 05:42:21.463708 140296834766592 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.7173258066177368, loss=1.840451955795288
I0307 05:43:00.058568 140296826373888 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.6774215698242188, loss=1.8675034046173096
I0307 05:43:04.291926 140452179379392 spec.py:321] Evaluating on the training split.
I0307 05:43:20.030955 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 05:43:43.536598 140452179379392 spec.py:349] Evaluating on the test split.
I0307 05:43:45.395325 140452179379392 submission_runner.py:469] Time since start: 12813.68s, 	Step: 30312, 	{'train/accuracy': 0.6761399507522583, 'train/loss': 1.2695378065109253, 'validation/accuracy': 0.6057800054550171, 'validation/loss': 1.6360821723937988, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.408809185028076, 'test/num_examples': 10000, 'score': 11787.274441719055, 'total_duration': 12813.681232213974, 'accumulated_submission_time': 11787.274441719055, 'accumulated_eval_time': 1020.7464399337769, 'accumulated_logging_time': 2.3946149349212646}
I0307 05:43:45.516483 140296834766592 logging_writer.py:48] [30312] accumulated_eval_time=1020.75, accumulated_logging_time=2.39461, accumulated_submission_time=11787.3, global_step=30312, preemption_count=0, score=11787.3, test/accuracy=0.4754, test/loss=2.40881, test/num_examples=10000, total_duration=12813.7, train/accuracy=0.67614, train/loss=1.26954, validation/accuracy=0.60578, validation/loss=1.63608, validation/num_examples=50000
I0307 05:44:20.432001 140296826373888 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.8948239088058472, loss=1.7633103132247925
I0307 05:44:59.153864 140296834766592 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.8580256700515747, loss=1.7313182353973389
I0307 05:45:37.656612 140296826373888 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.8122164011001587, loss=1.774193286895752
I0307 05:46:16.283174 140296834766592 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.624839186668396, loss=1.7814136743545532
I0307 05:46:55.105075 140296826373888 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.876586675643921, loss=1.6985865831375122
I0307 05:47:33.683145 140296834766592 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.7432260513305664, loss=1.7928731441497803
I0307 05:48:12.618031 140296826373888 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.5982460975646973, loss=1.6793938875198364
I0307 05:48:52.894577 140296834766592 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.8512386083602905, loss=1.782126784324646
I0307 05:49:31.607435 140296826373888 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.618263840675354, loss=1.63284170627594
I0307 05:50:10.860310 140296834766592 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.8631930351257324, loss=1.7197599411010742
I0307 05:50:49.567743 140296826373888 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.1150851249694824, loss=1.842761754989624
I0307 05:51:28.068074 140296834766592 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.674595832824707, loss=1.7359280586242676
I0307 05:52:07.199463 140296826373888 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.6519744396209717, loss=1.698328971862793
I0307 05:52:15.575195 140452179379392 spec.py:321] Evaluating on the training split.
I0307 05:52:31.238821 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 05:52:49.765047 140452179379392 spec.py:349] Evaluating on the test split.
I0307 05:52:51.614667 140452179379392 submission_runner.py:469] Time since start: 13359.90s, 	Step: 31623, 	{'train/accuracy': 0.6651785373687744, 'train/loss': 1.3193256855010986, 'validation/accuracy': 0.600879967212677, 'validation/loss': 1.6765329837799072, 'validation/num_examples': 50000, 'test/accuracy': 0.4807000160217285, 'test/loss': 2.4096968173980713, 'test/num_examples': 10000, 'score': 12297.177497386932, 'total_duration': 13359.900551080704, 'accumulated_submission_time': 12297.177497386932, 'accumulated_eval_time': 1056.7856845855713, 'accumulated_logging_time': 2.5392894744873047}
I0307 05:52:51.713030 140296834766592 logging_writer.py:48] [31623] accumulated_eval_time=1056.79, accumulated_logging_time=2.53929, accumulated_submission_time=12297.2, global_step=31623, preemption_count=0, score=12297.2, test/accuracy=0.4807, test/loss=2.4097, test/num_examples=10000, total_duration=13359.9, train/accuracy=0.665179, train/loss=1.31933, validation/accuracy=0.60088, validation/loss=1.67653, validation/num_examples=50000
I0307 05:53:22.105813 140296826373888 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.5940574407577515, loss=1.776050090789795
I0307 05:54:00.779153 140296834766592 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.733288288116455, loss=1.6063423156738281
I0307 05:54:39.789597 140296826373888 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.6460245847702026, loss=1.7103853225708008
I0307 05:55:18.188412 140296834766592 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.5942904949188232, loss=1.635185956954956
I0307 05:55:57.086923 140296826373888 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.7842158079147339, loss=1.6900396347045898
I0307 05:56:35.962531 140296834766592 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.9093579053878784, loss=1.6470266580581665
I0307 05:57:14.732882 140296826373888 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.7771632671356201, loss=1.5602545738220215
I0307 05:57:53.840131 140296834766592 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.519850492477417, loss=1.6972612142562866
I0307 05:58:32.433793 140296826373888 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.7706611156463623, loss=1.712720513343811
I0307 05:59:11.727919 140296834766592 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.0167734622955322, loss=1.837099552154541
I0307 05:59:50.668479 140296826373888 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.7535861730575562, loss=1.835902452468872
I0307 06:00:29.407676 140296834766592 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.879850149154663, loss=1.6600637435913086
I0307 06:01:08.256266 140296826373888 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.6009200811386108, loss=1.7686426639556885
I0307 06:01:21.732935 140452179379392 spec.py:321] Evaluating on the training split.
I0307 06:01:34.626782 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 06:02:02.270901 140452179379392 spec.py:349] Evaluating on the test split.
I0307 06:02:04.051790 140452179379392 submission_runner.py:469] Time since start: 13912.34s, 	Step: 32936, 	{'train/accuracy': 0.6986008882522583, 'train/loss': 1.179189682006836, 'validation/accuracy': 0.6294199824333191, 'validation/loss': 1.5444709062576294, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.267225980758667, 'test/num_examples': 10000, 'score': 12806.887058734894, 'total_duration': 13912.337742328644, 'accumulated_submission_time': 12806.887058734894, 'accumulated_eval_time': 1099.1043810844421, 'accumulated_logging_time': 2.818448543548584}
I0307 06:02:04.156149 140296834766592 logging_writer.py:48] [32936] accumulated_eval_time=1099.1, accumulated_logging_time=2.81845, accumulated_submission_time=12806.9, global_step=32936, preemption_count=0, score=12806.9, test/accuracy=0.5031, test/loss=2.26723, test/num_examples=10000, total_duration=13912.3, train/accuracy=0.698601, train/loss=1.17919, validation/accuracy=0.62942, validation/loss=1.54447, validation/num_examples=50000
I0307 06:02:29.617333 140296826373888 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.8841588497161865, loss=1.6148288249969482
I0307 06:03:08.153243 140296834766592 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.9324413537979126, loss=1.653412103652954
I0307 06:03:47.399997 140296826373888 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.6331932544708252, loss=1.573012351989746
I0307 06:04:25.725666 140296834766592 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.6164131164550781, loss=1.636816143989563
I0307 06:05:04.436416 140296826373888 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.668541669845581, loss=1.7162593603134155
I0307 06:05:43.044616 140296834766592 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.7554256916046143, loss=1.7011029720306396
I0307 06:06:22.110731 140296826373888 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.7646158933639526, loss=1.5371137857437134
I0307 06:07:00.448547 140296834766592 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.9242900609970093, loss=1.657529354095459
I0307 06:07:39.556476 140296826373888 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.8715335130691528, loss=1.6006052494049072
I0307 06:08:18.095663 140296834766592 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.6769208908081055, loss=1.7727974653244019
I0307 06:08:56.674209 140296826373888 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.7099612951278687, loss=1.66869056224823
I0307 06:09:35.688122 140296834766592 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.740573525428772, loss=1.7021141052246094
I0307 06:10:14.549057 140296826373888 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.2316808700561523, loss=1.9057295322418213
I0307 06:10:34.132857 140452179379392 spec.py:321] Evaluating on the training split.
I0307 06:10:47.858608 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 06:11:09.715452 140452179379392 spec.py:349] Evaluating on the test split.
I0307 06:11:11.586487 140452179379392 submission_runner.py:469] Time since start: 14459.87s, 	Step: 34251, 	{'train/accuracy': 0.7006337642669678, 'train/loss': 1.170133113861084, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.5177743434906006, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.249415636062622, 'test/num_examples': 10000, 'score': 13316.700063467026, 'total_duration': 14459.872414112091, 'accumulated_submission_time': 13316.700063467026, 'accumulated_eval_time': 1136.5578255653381, 'accumulated_logging_time': 2.9557945728302}
I0307 06:11:11.699744 140296834766592 logging_writer.py:48] [34251] accumulated_eval_time=1136.56, accumulated_logging_time=2.95579, accumulated_submission_time=13316.7, global_step=34251, preemption_count=0, score=13316.7, test/accuracy=0.5084, test/loss=2.24942, test/num_examples=10000, total_duration=14459.9, train/accuracy=0.700634, train/loss=1.17013, validation/accuracy=0.63068, validation/loss=1.51777, validation/num_examples=50000
I0307 06:11:30.944575 140296826373888 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.7832509279251099, loss=1.6876367330551147
I0307 06:12:09.873702 140296834766592 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.8343665599822998, loss=1.8059887886047363
I0307 06:12:48.714333 140296826373888 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.7812448740005493, loss=1.7276408672332764
I0307 06:13:27.113284 140296834766592 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.8268390893936157, loss=1.632733941078186
I0307 06:14:05.653208 140296826373888 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.5706615447998047, loss=1.655531644821167
I0307 06:14:44.331802 140296834766592 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.6586096286773682, loss=1.7162237167358398
I0307 06:15:23.092305 140296826373888 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.6148115396499634, loss=1.696321725845337
I0307 06:16:02.036957 140296834766592 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.9118996858596802, loss=1.717452049255371
I0307 06:16:41.505132 140296826373888 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.782660961151123, loss=1.6598787307739258
I0307 06:17:19.733570 140296834766592 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.9360069036483765, loss=1.6520020961761475
I0307 06:18:04.015136 140296826373888 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.77671480178833, loss=1.6909053325653076
I0307 06:18:56.950407 140296834766592 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.713303804397583, loss=1.5959577560424805
I0307 06:19:36.571911 140296826373888 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.2901957035064697, loss=1.7229056358337402
I0307 06:19:41.949836 140452179379392 spec.py:321] Evaluating on the training split.
I0307 06:19:53.688284 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 06:20:15.804755 140452179379392 spec.py:349] Evaluating on the test split.
I0307 06:20:17.645067 140452179379392 submission_runner.py:469] Time since start: 15005.93s, 	Step: 35515, 	{'train/accuracy': 0.7026865482330322, 'train/loss': 1.16473388671875, 'validation/accuracy': 0.6310999989509583, 'validation/loss': 1.533669352531433, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.2530882358551025, 'test/num_examples': 10000, 'score': 13826.795982599258, 'total_duration': 15005.930996418, 'accumulated_submission_time': 13826.795982599258, 'accumulated_eval_time': 1172.252869606018, 'accumulated_logging_time': 3.0915355682373047}
I0307 06:20:17.752573 140296834766592 logging_writer.py:48] [35515] accumulated_eval_time=1172.25, accumulated_logging_time=3.09154, accumulated_submission_time=13826.8, global_step=35515, preemption_count=0, score=13826.8, test/accuracy=0.5083, test/loss=2.25309, test/num_examples=10000, total_duration=15005.9, train/accuracy=0.702687, train/loss=1.16473, validation/accuracy=0.6311, validation/loss=1.53367, validation/num_examples=50000
I0307 06:20:50.848262 140296826373888 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.6914253234863281, loss=1.5861766338348389
I0307 06:21:29.807361 140296834766592 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.7173879146575928, loss=1.6473095417022705
I0307 06:22:08.425436 140296826373888 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.8582327365875244, loss=1.6655009984970093
I0307 06:22:46.946223 140296834766592 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.5772007703781128, loss=1.5868992805480957
I0307 06:23:25.494355 140296826373888 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.6747207641601562, loss=1.7382142543792725
I0307 06:24:04.352326 140296834766592 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.8156843185424805, loss=1.6448349952697754
I0307 06:24:43.586401 140296826373888 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.6588400602340698, loss=1.5852389335632324
I0307 06:25:22.603258 140296834766592 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.7378493547439575, loss=1.6107361316680908
I0307 06:26:00.999044 140296826373888 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.909332513809204, loss=1.6355311870574951
I0307 06:26:39.780401 140296834766592 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.9059981107711792, loss=1.7212445735931396
I0307 06:27:18.330124 140296826373888 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.096086025238037, loss=1.7127552032470703
I0307 06:27:57.222329 140296834766592 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.683912754058838, loss=1.6069968938827515
I0307 06:28:35.896695 140296826373888 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.8149113655090332, loss=1.714880347251892
I0307 06:28:47.985861 140452179379392 spec.py:321] Evaluating on the training split.
I0307 06:28:59.452172 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 06:29:23.633592 140452179379392 spec.py:349] Evaluating on the test split.
I0307 06:29:25.483525 140452179379392 submission_runner.py:469] Time since start: 15553.77s, 	Step: 36832, 	{'train/accuracy': 0.687898576259613, 'train/loss': 1.2164710760116577, 'validation/accuracy': 0.6231399774551392, 'validation/loss': 1.5651354789733887, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.3190078735351562, 'test/num_examples': 10000, 'score': 14336.873234272003, 'total_duration': 15553.769481658936, 'accumulated_submission_time': 14336.873234272003, 'accumulated_eval_time': 1209.7503817081451, 'accumulated_logging_time': 3.2222983837127686}
I0307 06:29:25.566549 140296834766592 logging_writer.py:48] [36832] accumulated_eval_time=1209.75, accumulated_logging_time=3.2223, accumulated_submission_time=14336.9, global_step=36832, preemption_count=0, score=14336.9, test/accuracy=0.4949, test/loss=2.31901, test/num_examples=10000, total_duration=15553.8, train/accuracy=0.687899, train/loss=1.21647, validation/accuracy=0.62314, validation/loss=1.56514, validation/num_examples=50000
I0307 06:29:52.658962 140296826373888 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.5823122262954712, loss=1.4855343103408813
I0307 06:30:31.494172 140296834766592 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.711923360824585, loss=1.7440712451934814
I0307 06:31:10.295418 140296826373888 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.7953077554702759, loss=1.5854089260101318
I0307 06:31:48.935659 140296834766592 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.015062093734741, loss=1.7625269889831543
I0307 06:32:27.620545 140296826373888 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.8016424179077148, loss=1.648383617401123
I0307 06:33:06.826625 140296834766592 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.9631030559539795, loss=1.7361119985580444
I0307 06:33:45.345379 140296826373888 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.7828774452209473, loss=1.633517861366272
I0307 06:34:24.479760 140296834766592 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.7442214488983154, loss=1.5729799270629883
I0307 06:35:02.823551 140296826373888 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.8283883333206177, loss=1.6743428707122803
I0307 06:35:41.537266 140296834766592 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.6112391948699951, loss=1.6233580112457275
I0307 06:36:20.311779 140296826373888 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.663164496421814, loss=1.6693733930587769
I0307 06:36:59.704045 140296834766592 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.6793304681777954, loss=1.7304644584655762
I0307 06:37:38.516052 140296826373888 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.7225096225738525, loss=1.731698989868164
I0307 06:37:55.884225 140452179379392 spec.py:321] Evaluating on the training split.
I0307 06:38:07.427045 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 06:38:29.673573 140452179379392 spec.py:349] Evaluating on the test split.
I0307 06:38:31.510509 140452179379392 submission_runner.py:469] Time since start: 16099.80s, 	Step: 38145, 	{'train/accuracy': 0.6995575428009033, 'train/loss': 1.167974829673767, 'validation/accuracy': 0.6320599913597107, 'validation/loss': 1.5250407457351685, 'validation/num_examples': 50000, 'test/accuracy': 0.5016000270843506, 'test/loss': 2.303520679473877, 'test/num_examples': 10000, 'score': 14847.014067173004, 'total_duration': 16099.796370506287, 'accumulated_submission_time': 14847.014067173004, 'accumulated_eval_time': 1245.3764162063599, 'accumulated_logging_time': 3.3506453037261963}
I0307 06:38:31.576952 140296834766592 logging_writer.py:48] [38145] accumulated_eval_time=1245.38, accumulated_logging_time=3.35065, accumulated_submission_time=14847, global_step=38145, preemption_count=0, score=14847, test/accuracy=0.5016, test/loss=2.30352, test/num_examples=10000, total_duration=16099.8, train/accuracy=0.699558, train/loss=1.16797, validation/accuracy=0.63206, validation/loss=1.52504, validation/num_examples=50000
I0307 06:38:53.463397 140296826373888 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.644419550895691, loss=1.5480046272277832
I0307 06:39:32.175933 140296834766592 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.8086283206939697, loss=1.6851001977920532
I0307 06:40:10.595197 140296826373888 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7238126993179321, loss=1.6087005138397217
I0307 06:40:48.895986 140296834766592 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.009829044342041, loss=1.7040776014328003
I0307 06:41:28.109351 140296826373888 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.096931219100952, loss=1.6241865158081055
I0307 06:42:07.285939 140296834766592 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.7915763854980469, loss=1.6748241186141968
I0307 06:42:46.472365 140296826373888 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.6849029064178467, loss=1.6744930744171143
I0307 06:43:25.338833 140296834766592 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.8698033094406128, loss=1.6025313138961792
I0307 06:44:04.094196 140296826373888 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.842841386795044, loss=1.5845636129379272
I0307 06:44:43.030939 140296834766592 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.7442669868469238, loss=1.5730559825897217
I0307 06:45:21.588703 140296826373888 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.688936710357666, loss=1.65921950340271
I0307 06:46:00.289343 140296834766592 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7528114318847656, loss=1.6813948154449463
I0307 06:46:38.869083 140296826373888 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.6537199020385742, loss=1.5310777425765991
I0307 06:47:01.744886 140452179379392 spec.py:321] Evaluating on the training split.
I0307 06:47:12.692962 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 06:47:34.566623 140452179379392 spec.py:349] Evaluating on the test split.
I0307 06:47:36.568962 140452179379392 submission_runner.py:469] Time since start: 16644.85s, 	Step: 39460, 	{'train/accuracy': 0.7017298936843872, 'train/loss': 1.1635953187942505, 'validation/accuracy': 0.6313799619674683, 'validation/loss': 1.5133898258209229, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.2279975414276123, 'test/num_examples': 10000, 'score': 15357.014038801193, 'total_duration': 16644.854928970337, 'accumulated_submission_time': 15357.014038801193, 'accumulated_eval_time': 1280.200347661972, 'accumulated_logging_time': 3.4531116485595703}
I0307 06:47:36.662837 140296834766592 logging_writer.py:48] [39460] accumulated_eval_time=1280.2, accumulated_logging_time=3.45311, accumulated_submission_time=15357, global_step=39460, preemption_count=0, score=15357, test/accuracy=0.5032, test/loss=2.228, test/num_examples=10000, total_duration=16644.9, train/accuracy=0.70173, train/loss=1.1636, validation/accuracy=0.63138, validation/loss=1.51339, validation/num_examples=50000
I0307 06:47:52.445230 140296826373888 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.699751853942871, loss=1.6414427757263184
I0307 06:48:31.026868 140296834766592 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.5649009943008423, loss=1.6317566633224487
I0307 06:49:10.143779 140296826373888 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.7122695446014404, loss=1.5971702337265015
I0307 06:49:48.763335 140296834766592 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.7985583543777466, loss=1.5992224216461182
I0307 06:50:27.371076 140296826373888 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.7217758893966675, loss=1.6478004455566406
I0307 06:51:05.702374 140296834766592 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.7141467332839966, loss=1.7222094535827637
I0307 06:51:45.053018 140296826373888 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.713477373123169, loss=1.6984891891479492
I0307 06:52:24.156459 140296834766592 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.7534682750701904, loss=1.7535958290100098
I0307 06:53:02.665520 140296826373888 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.6898324489593506, loss=1.7393927574157715
I0307 06:53:41.382309 140296834766592 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.8296927213668823, loss=1.6901798248291016
I0307 06:54:20.087933 140296826373888 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.8389108180999756, loss=1.846177339553833
I0307 06:54:58.943550 140296834766592 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.9712854623794556, loss=1.721590518951416
I0307 06:55:37.770594 140296826373888 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.757856011390686, loss=1.6383641958236694
I0307 06:56:06.781944 140452179379392 spec.py:321] Evaluating on the training split.
I0307 06:56:17.604028 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 06:56:42.262259 140452179379392 spec.py:349] Evaluating on the test split.
I0307 06:56:44.099209 140452179379392 submission_runner.py:469] Time since start: 17192.39s, 	Step: 40776, 	{'train/accuracy': 0.7074697017669678, 'train/loss': 1.1410713195800781, 'validation/accuracy': 0.6385999917984009, 'validation/loss': 1.4968065023422241, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.2737174034118652, 'test/num_examples': 10000, 'score': 15866.974277973175, 'total_duration': 17192.385166168213, 'accumulated_submission_time': 15866.974277973175, 'accumulated_eval_time': 1317.5174612998962, 'accumulated_logging_time': 3.570793628692627}
I0307 06:56:44.230834 140296834766592 logging_writer.py:48] [40776] accumulated_eval_time=1317.52, accumulated_logging_time=3.57079, accumulated_submission_time=15867, global_step=40776, preemption_count=0, score=15867, test/accuracy=0.5081, test/loss=2.27372, test/num_examples=10000, total_duration=17192.4, train/accuracy=0.70747, train/loss=1.14107, validation/accuracy=0.6386, validation/loss=1.49681, validation/num_examples=50000
I0307 06:56:54.021485 140296826373888 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7723227739334106, loss=1.7036606073379517
I0307 06:57:32.318853 140296834766592 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.8523346185684204, loss=1.7292921543121338
I0307 06:58:10.728391 140296826373888 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.9280253648757935, loss=1.7150064706802368
I0307 06:58:49.763050 140296834766592 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7885993719100952, loss=1.680491328239441
I0307 06:59:28.490601 140296826373888 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.9422427415847778, loss=1.6841814517974854
I0307 07:00:07.703791 140296834766592 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.7844523191452026, loss=1.752579689025879
I0307 07:00:46.585029 140296826373888 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.7769818305969238, loss=1.6071455478668213
I0307 07:01:25.428170 140296834766592 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.8436412811279297, loss=1.7151063680648804
I0307 07:02:04.255921 140296826373888 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.846656084060669, loss=1.6626574993133545
I0307 07:02:42.786755 140296834766592 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.737436294555664, loss=1.7115123271942139
I0307 07:03:21.621215 140296826373888 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.545328140258789, loss=1.5881774425506592
I0307 07:04:00.101352 140296834766592 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8676272630691528, loss=1.804750680923462
I0307 07:04:39.083826 140296826373888 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.9133409261703491, loss=1.5170633792877197
I0307 07:05:14.476114 140452179379392 spec.py:321] Evaluating on the training split.
I0307 07:05:25.296121 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 07:05:46.639176 140452179379392 spec.py:349] Evaluating on the test split.
I0307 07:05:48.479850 140452179379392 submission_runner.py:469] Time since start: 17736.77s, 	Step: 42092, 	{'train/accuracy': 0.7047791481018066, 'train/loss': 1.147672414779663, 'validation/accuracy': 0.6356199979782104, 'validation/loss': 1.5010168552398682, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.2536654472351074, 'test/num_examples': 10000, 'score': 16377.061843633652, 'total_duration': 17736.765770435333, 'accumulated_submission_time': 16377.061843633652, 'accumulated_eval_time': 1351.5210099220276, 'accumulated_logging_time': 3.725583553314209}
I0307 07:05:48.603514 140296834766592 logging_writer.py:48] [42092] accumulated_eval_time=1351.52, accumulated_logging_time=3.72558, accumulated_submission_time=16377.1, global_step=42092, preemption_count=0, score=16377.1, test/accuracy=0.5103, test/loss=2.25367, test/num_examples=10000, total_duration=17736.8, train/accuracy=0.704779, train/loss=1.14767, validation/accuracy=0.63562, validation/loss=1.50102, validation/num_examples=50000
I0307 07:05:52.290259 140296826373888 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.715619444847107, loss=1.591083288192749
I0307 07:06:30.944856 140296834766592 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.964762568473816, loss=1.695416808128357
I0307 07:07:09.682837 140296826373888 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.7733662128448486, loss=1.6145778894424438
I0307 07:07:48.165843 140296834766592 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.6666314601898193, loss=1.6703141927719116
I0307 07:08:27.023479 140296826373888 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.7216418981552124, loss=1.672827959060669
I0307 07:09:06.873471 140296834766592 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.9423614740371704, loss=1.6756900548934937
I0307 07:09:45.956386 140296826373888 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.6957664489746094, loss=1.6452441215515137
I0307 07:10:24.938982 140296834766592 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.7362840175628662, loss=1.5861107110977173
I0307 07:11:03.662480 140296826373888 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.6635088920593262, loss=1.6893744468688965
I0307 07:11:42.547622 140296834766592 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.8852667808532715, loss=1.692481279373169
I0307 07:12:21.371245 140296826373888 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.9066370725631714, loss=1.7057971954345703
I0307 07:13:00.234360 140296834766592 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.057197332382202, loss=1.673366665840149
I0307 07:13:39.325943 140296826373888 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.7578630447387695, loss=1.5275564193725586
I0307 07:14:17.388044 140296834766592 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.7045186758041382, loss=1.751408576965332
I0307 07:14:18.497218 140452179379392 spec.py:321] Evaluating on the training split.
I0307 07:14:30.782566 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 07:14:52.007372 140452179379392 spec.py:349] Evaluating on the test split.
I0307 07:14:53.855052 140452179379392 submission_runner.py:469] Time since start: 18282.14s, 	Step: 43404, 	{'train/accuracy': 0.6994379758834839, 'train/loss': 1.1790796518325806, 'validation/accuracy': 0.6329599618911743, 'validation/loss': 1.5107394456863403, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2469840049743652, 'test/num_examples': 10000, 'score': 16886.80089879036, 'total_duration': 18282.140962839127, 'accumulated_submission_time': 16886.80089879036, 'accumulated_eval_time': 1386.87864279747, 'accumulated_logging_time': 3.8720855712890625}
I0307 07:14:53.917280 140296826373888 logging_writer.py:48] [43404] accumulated_eval_time=1386.88, accumulated_logging_time=3.87209, accumulated_submission_time=16886.8, global_step=43404, preemption_count=0, score=16886.8, test/accuracy=0.5119, test/loss=2.24698, test/num_examples=10000, total_duration=18282.1, train/accuracy=0.699438, train/loss=1.17908, validation/accuracy=0.63296, validation/loss=1.51074, validation/num_examples=50000
I0307 07:15:31.485651 140296834766592 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.833661437034607, loss=1.64375901222229
I0307 07:16:10.283584 140296826373888 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.7594629526138306, loss=1.5922452211380005
I0307 07:16:49.005881 140296834766592 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.94952392578125, loss=1.650455117225647
I0307 07:17:28.301867 140296826373888 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.6813945770263672, loss=1.6315306425094604
I0307 07:18:07.161080 140296834766592 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.6575499773025513, loss=1.5892647504806519
I0307 07:18:45.697873 140296826373888 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.8870277404785156, loss=1.6492576599121094
I0307 07:19:24.291182 140296834766592 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.7932978868484497, loss=1.5642046928405762
I0307 07:20:03.254805 140296826373888 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.7603689432144165, loss=1.5850416421890259
I0307 07:20:42.036291 140296834766592 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.8055397272109985, loss=1.5450471639633179
I0307 07:21:20.590378 140296826373888 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.7510285377502441, loss=1.6518687009811401
I0307 07:21:59.754335 140296834766592 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.8814942836761475, loss=1.5769933462142944
I0307 07:22:38.394120 140296826373888 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8355268239974976, loss=1.6699469089508057
I0307 07:23:17.580746 140296834766592 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.7393056154251099, loss=1.629044771194458
I0307 07:23:24.168177 140452179379392 spec.py:321] Evaluating on the training split.
I0307 07:23:36.594881 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 07:23:56.577881 140452179379392 spec.py:349] Evaluating on the test split.
I0307 07:23:58.356481 140452179379392 submission_runner.py:469] Time since start: 18826.64s, 	Step: 44718, 	{'train/accuracy': 0.703125, 'train/loss': 1.159155249595642, 'validation/accuracy': 0.6333799958229065, 'validation/loss': 1.5013694763183594, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2602763175964355, 'test/num_examples': 10000, 'score': 17396.885651111603, 'total_duration': 18826.64243364334, 'accumulated_submission_time': 17396.885651111603, 'accumulated_eval_time': 1421.0667879581451, 'accumulated_logging_time': 3.9691731929779053}
I0307 07:23:58.443455 140296826373888 logging_writer.py:48] [44718] accumulated_eval_time=1421.07, accumulated_logging_time=3.96917, accumulated_submission_time=17396.9, global_step=44718, preemption_count=0, score=17396.9, test/accuracy=0.5049, test/loss=2.26028, test/num_examples=10000, total_duration=18826.6, train/accuracy=0.703125, train/loss=1.15916, validation/accuracy=0.63338, validation/loss=1.50137, validation/num_examples=50000
I0307 07:24:30.648020 140296834766592 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.6928564310073853, loss=1.5765817165374756
I0307 07:25:09.727242 140296826373888 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.1825923919677734, loss=1.7012859582901
I0307 07:25:49.967565 140296834766592 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.6859490871429443, loss=1.568839192390442
I0307 07:26:29.811682 140296826373888 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.171447992324829, loss=1.6194409132003784
I0307 07:27:08.699136 140296834766592 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.7798839807510376, loss=1.6084842681884766
I0307 07:27:47.348982 140296826373888 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8391904830932617, loss=1.749983549118042
I0307 07:28:26.123193 140296834766592 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.869968295097351, loss=1.749845027923584
I0307 07:29:04.934894 140296826373888 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7553380727767944, loss=1.6698001623153687
I0307 07:29:43.706214 140296834766592 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.8956693410873413, loss=1.7740520238876343
I0307 07:30:22.383974 140296826373888 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.9868510961532593, loss=1.6646347045898438
I0307 07:31:01.050091 140296834766592 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.89385187625885, loss=1.6129579544067383
I0307 07:31:39.483269 140296826373888 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.9014707803726196, loss=1.769430160522461
I0307 07:32:18.221504 140296834766592 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7964222431182861, loss=1.5723419189453125
I0307 07:32:28.731873 140452179379392 spec.py:321] Evaluating on the training split.
I0307 07:32:41.107399 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 07:33:04.624046 140452179379392 spec.py:349] Evaluating on the test split.
I0307 07:33:06.409077 140452179379392 submission_runner.py:469] Time since start: 19374.69s, 	Step: 46028, 	{'train/accuracy': 0.712332546710968, 'train/loss': 1.1168365478515625, 'validation/accuracy': 0.6403200030326843, 'validation/loss': 1.4838718175888062, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.2062296867370605, 'test/num_examples': 10000, 'score': 17907.0224776268, 'total_duration': 19374.694994211197, 'accumulated_submission_time': 17907.0224776268, 'accumulated_eval_time': 1458.7437987327576, 'accumulated_logging_time': 4.0757880210876465}
I0307 07:33:06.517112 140296826373888 logging_writer.py:48] [46028] accumulated_eval_time=1458.74, accumulated_logging_time=4.07579, accumulated_submission_time=17907, global_step=46028, preemption_count=0, score=17907, test/accuracy=0.5141, test/loss=2.20623, test/num_examples=10000, total_duration=19374.7, train/accuracy=0.712333, train/loss=1.11684, validation/accuracy=0.64032, validation/loss=1.48387, validation/num_examples=50000
I0307 07:33:34.917251 140296834766592 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.8102909326553345, loss=1.6524219512939453
I0307 07:34:14.261665 140296826373888 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.7007514238357544, loss=1.6880404949188232
I0307 07:34:54.201117 140296834766592 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6603175401687622, loss=1.5850176811218262
I0307 07:35:33.340547 140296826373888 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.005772590637207, loss=1.6214289665222168
I0307 07:36:12.111725 140296834766592 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.9048880338668823, loss=1.6388109922409058
I0307 07:36:51.002896 140296826373888 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.6858197450637817, loss=1.6545155048370361
I0307 07:37:29.540690 140296834766592 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.7339043617248535, loss=1.613406777381897
I0307 07:38:08.261569 140296826373888 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.902785062789917, loss=1.663215160369873
I0307 07:38:47.267156 140296834766592 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.0452733039855957, loss=1.5909737348556519
I0307 07:39:25.745808 140296826373888 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.892026424407959, loss=1.6159348487854004
I0307 07:40:04.356962 140296834766592 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.729763150215149, loss=1.4920681715011597
I0307 07:40:43.185732 140296826373888 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.716613531112671, loss=1.5765244960784912
I0307 07:41:22.011543 140296834766592 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.7384989261627197, loss=1.6844825744628906
I0307 07:41:36.478287 140452179379392 spec.py:321] Evaluating on the training split.
I0307 07:41:48.951382 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 07:42:07.341362 140452179379392 spec.py:349] Evaluating on the test split.
I0307 07:42:09.127355 140452179379392 submission_runner.py:469] Time since start: 19917.41s, 	Step: 47339, 	{'train/accuracy': 0.7066525816917419, 'train/loss': 1.1429005861282349, 'validation/accuracy': 0.6362999677658081, 'validation/loss': 1.5029296875, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2452163696289062, 'test/num_examples': 10000, 'score': 18416.830828666687, 'total_duration': 19917.413342237473, 'accumulated_submission_time': 18416.830828666687, 'accumulated_eval_time': 1491.3927488327026, 'accumulated_logging_time': 4.203917503356934}
I0307 07:42:09.287109 140296826373888 logging_writer.py:48] [47339] accumulated_eval_time=1491.39, accumulated_logging_time=4.20392, accumulated_submission_time=18416.8, global_step=47339, preemption_count=0, score=18416.8, test/accuracy=0.5063, test/loss=2.24522, test/num_examples=10000, total_duration=19917.4, train/accuracy=0.706653, train/loss=1.1429, validation/accuracy=0.6363, validation/loss=1.50293, validation/num_examples=50000
I0307 07:42:33.125709 140296834766592 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.767492651939392, loss=1.631356954574585
I0307 07:43:11.996457 140296826373888 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7701890468597412, loss=1.7042747735977173
I0307 07:43:51.616327 140296834766592 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.7752493619918823, loss=1.6113286018371582
I0307 07:44:30.387052 140296826373888 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.940322756767273, loss=1.7185784578323364
I0307 07:45:09.297698 140296834766592 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.874190330505371, loss=1.6238571405410767
I0307 07:45:48.019476 140296826373888 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.8154107332229614, loss=1.550413727760315
I0307 07:46:26.863095 140296834766592 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.6395885944366455, loss=1.6242619752883911
I0307 07:47:05.576472 140296826373888 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.8645305633544922, loss=1.5963917970657349
I0307 07:47:44.275599 140296834766592 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.9464489221572876, loss=1.611167073249817
I0307 07:48:22.859327 140296826373888 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.7927255630493164, loss=1.617995262145996
I0307 07:49:01.373102 140296834766592 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.6256439685821533, loss=1.562659502029419
I0307 07:49:40.096717 140296826373888 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.774022102355957, loss=1.5385750532150269
I0307 07:50:18.923422 140296834766592 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8229377269744873, loss=1.6321218013763428
I0307 07:50:39.271622 140452179379392 spec.py:321] Evaluating on the training split.
I0307 07:50:51.622004 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 07:51:12.801569 140452179379392 spec.py:349] Evaluating on the test split.
I0307 07:51:14.578985 140452179379392 submission_runner.py:469] Time since start: 20462.86s, 	Step: 48653, 	{'train/accuracy': 0.7029057741165161, 'train/loss': 1.1506329774856567, 'validation/accuracy': 0.6337199807167053, 'validation/loss': 1.5094141960144043, 'validation/num_examples': 50000, 'test/accuracy': 0.5008000135421753, 'test/loss': 2.2742104530334473, 'test/num_examples': 10000, 'score': 18926.651203632355, 'total_duration': 20462.864921569824, 'accumulated_submission_time': 18926.651203632355, 'accumulated_eval_time': 1526.6999325752258, 'accumulated_logging_time': 4.396802186965942}
I0307 07:51:14.665052 140296826373888 logging_writer.py:48] [48653] accumulated_eval_time=1526.7, accumulated_logging_time=4.3968, accumulated_submission_time=18926.7, global_step=48653, preemption_count=0, score=18926.7, test/accuracy=0.5008, test/loss=2.27421, test/num_examples=10000, total_duration=20462.9, train/accuracy=0.702906, train/loss=1.15063, validation/accuracy=0.63372, validation/loss=1.50941, validation/num_examples=50000
I0307 07:51:33.244634 140296834766592 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.8708082437515259, loss=1.530562162399292
I0307 07:52:12.142369 140296826373888 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.7091530561447144, loss=1.6192831993103027
I0307 07:52:50.642507 140296834766592 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7887192964553833, loss=1.5123378038406372
I0307 07:53:29.512608 140296826373888 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.8833703994750977, loss=1.6968283653259277
I0307 07:54:08.345535 140296834766592 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.04544734954834, loss=1.7044862508773804
I0307 07:54:46.787567 140296826373888 logging_writer.py:48] [49200] global_step=49200, grad_norm=2.1056463718414307, loss=1.767744779586792
I0307 07:55:25.472287 140296834766592 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.9348973035812378, loss=1.6371150016784668
I0307 07:56:04.246169 140296826373888 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.761401891708374, loss=1.6418691873550415
I0307 07:56:42.809587 140296834766592 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7781710624694824, loss=1.6467370986938477
I0307 07:57:21.739217 140296826373888 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.8755230903625488, loss=1.708102822303772
I0307 07:58:01.075868 140296834766592 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.7953664064407349, loss=1.5341663360595703
I0307 07:58:40.233766 140296826373888 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.0049543380737305, loss=1.8062037229537964
I0307 07:59:18.563413 140296834766592 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9339399337768555, loss=1.657110571861267
I0307 07:59:44.703287 140452179379392 spec.py:321] Evaluating on the training split.
I0307 07:59:57.244060 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 08:00:15.659806 140452179379392 spec.py:349] Evaluating on the test split.
I0307 08:00:17.450549 140452179379392 submission_runner.py:469] Time since start: 21005.74s, 	Step: 49968, 	{'train/accuracy': 0.7259646058082581, 'train/loss': 1.0541446208953857, 'validation/accuracy': 0.6535800099372864, 'validation/loss': 1.4165523052215576, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.1341519355773926, 'test/num_examples': 10000, 'score': 19436.52112555504, 'total_duration': 21005.736488103867, 'accumulated_submission_time': 19436.52112555504, 'accumulated_eval_time': 1559.4470360279083, 'accumulated_logging_time': 4.521208047866821}
I0307 08:00:17.535957 140296826373888 logging_writer.py:48] [49968] accumulated_eval_time=1559.45, accumulated_logging_time=4.52121, accumulated_submission_time=19436.5, global_step=49968, preemption_count=0, score=19436.5, test/accuracy=0.5304, test/loss=2.13415, test/num_examples=10000, total_duration=21005.7, train/accuracy=0.725965, train/loss=1.05414, validation/accuracy=0.65358, validation/loss=1.41655, validation/num_examples=50000
I0307 08:00:30.240938 140296834766592 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.6805049180984497, loss=1.6049010753631592
I0307 08:01:09.337760 140296826373888 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.794992208480835, loss=1.6692906618118286
I0307 08:01:48.187086 140296834766592 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.9145983457565308, loss=1.6627767086029053
I0307 08:02:26.794412 140296826373888 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.8076393604278564, loss=1.6886160373687744
I0307 08:03:05.453296 140296834766592 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.7999911308288574, loss=1.6394500732421875
I0307 08:03:44.125559 140296826373888 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.8616435527801514, loss=1.7089686393737793
I0307 08:04:22.849240 140296834766592 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.0903055667877197, loss=1.540328860282898
I0307 08:05:01.771720 140296826373888 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.7484554052352905, loss=1.6671857833862305
I0307 08:05:40.320866 140296834766592 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.8062996864318848, loss=1.6666673421859741
I0307 08:06:19.482266 140296826373888 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.8857011795043945, loss=1.624541997909546
I0307 08:06:58.048053 140296834766592 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.8531140089035034, loss=1.595387578010559
I0307 08:07:36.881587 140296826373888 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.9821760654449463, loss=1.5939066410064697
I0307 08:08:15.722321 140296834766592 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7261136770248413, loss=1.592343807220459
I0307 08:08:47.554300 140452179379392 spec.py:321] Evaluating on the training split.
I0307 08:08:59.931611 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 08:09:23.671663 140452179379392 spec.py:349] Evaluating on the test split.
I0307 08:09:25.455945 140452179379392 submission_runner.py:469] Time since start: 21553.74s, 	Step: 51282, 	{'train/accuracy': 0.72074294090271, 'train/loss': 1.0786815881729126, 'validation/accuracy': 0.6505599617958069, 'validation/loss': 1.441593885421753, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.1758639812469482, 'test/num_examples': 10000, 'score': 19946.383144140244, 'total_duration': 21553.74187040329, 'accumulated_submission_time': 19946.383144140244, 'accumulated_eval_time': 1597.3484959602356, 'accumulated_logging_time': 4.6324474811553955}
I0307 08:09:25.564255 140296826373888 logging_writer.py:48] [51282] accumulated_eval_time=1597.35, accumulated_logging_time=4.63245, accumulated_submission_time=19946.4, global_step=51282, preemption_count=0, score=19946.4, test/accuracy=0.5243, test/loss=2.17586, test/num_examples=10000, total_duration=21553.7, train/accuracy=0.720743, train/loss=1.07868, validation/accuracy=0.65056, validation/loss=1.44159, validation/num_examples=50000
I0307 08:09:32.996799 140296834766592 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.9814618825912476, loss=1.652169108390808
I0307 08:10:11.616837 140296826373888 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.9330298900604248, loss=1.6468050479888916
I0307 08:10:50.214577 140296834766592 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.897790789604187, loss=1.6963859796524048
I0307 08:11:28.949817 140296826373888 logging_writer.py:48] [51600] global_step=51600, grad_norm=2.088550090789795, loss=1.6907209157943726
I0307 08:12:07.774185 140296834766592 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8365087509155273, loss=1.6667678356170654
I0307 08:12:46.663761 140296826373888 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.9683061838150024, loss=1.752798080444336
I0307 08:13:25.296145 140296834766592 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.0544466972351074, loss=1.6523995399475098
I0307 08:14:03.483582 140296826373888 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.9574118852615356, loss=1.6747593879699707
I0307 08:14:42.281808 140296834766592 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.9187004566192627, loss=1.6387203931808472
I0307 08:15:20.356629 140296826373888 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.870943307876587, loss=1.5580769777297974
I0307 08:15:59.449234 140296834766592 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.264145851135254, loss=1.6368781328201294
I0307 08:16:38.203507 140296826373888 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.8449246883392334, loss=1.601780652999878
I0307 08:17:16.762689 140296834766592 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8880003690719604, loss=1.4492623805999756
I0307 08:17:55.687967 140296826373888 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.8873811960220337, loss=1.6893349885940552
I0307 08:17:55.754277 140452179379392 spec.py:321] Evaluating on the training split.
I0307 08:18:08.435228 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 08:18:28.122303 140452179379392 spec.py:349] Evaluating on the test split.
I0307 08:18:29.893482 140452179379392 submission_runner.py:469] Time since start: 22098.18s, 	Step: 52601, 	{'train/accuracy': 0.7106186151504517, 'train/loss': 1.1249427795410156, 'validation/accuracy': 0.6447399854660034, 'validation/loss': 1.472260594367981, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.229464292526245, 'test/num_examples': 10000, 'score': 20456.412138223648, 'total_duration': 22098.179429531097, 'accumulated_submission_time': 20456.412138223648, 'accumulated_eval_time': 1631.487529039383, 'accumulated_logging_time': 4.767784118652344}
I0307 08:18:30.016362 140296834766592 logging_writer.py:48] [52601] accumulated_eval_time=1631.49, accumulated_logging_time=4.76778, accumulated_submission_time=20456.4, global_step=52601, preemption_count=0, score=20456.4, test/accuracy=0.5183, test/loss=2.22946, test/num_examples=10000, total_duration=22098.2, train/accuracy=0.710619, train/loss=1.12494, validation/accuracy=0.64474, validation/loss=1.47226, validation/num_examples=50000
I0307 08:19:08.775288 140296826373888 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8739207983016968, loss=1.5274478197097778
I0307 08:19:47.874906 140296834766592 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8208330869674683, loss=1.6718552112579346
I0307 08:20:26.731475 140296826373888 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.9979149103164673, loss=1.6172593832015991
I0307 08:21:05.074360 140296834766592 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.9962644577026367, loss=1.6382761001586914
I0307 08:21:43.692137 140296826373888 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.9715540409088135, loss=1.6635785102844238
I0307 08:22:22.771579 140296834766592 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.7864161729812622, loss=1.6654056310653687
I0307 08:23:01.518937 140296826373888 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.5641968250274658, loss=1.4786003828048706
I0307 08:23:40.531569 140296834766592 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.7521038055419922, loss=1.627251386642456
I0307 08:24:19.120908 140296826373888 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.06396746635437, loss=1.6660484075546265
I0307 08:24:58.039064 140296834766592 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.9647709131240845, loss=1.623788833618164
I0307 08:25:36.809963 140296826373888 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.81238853931427, loss=1.667229413986206
I0307 08:26:16.451994 140296834766592 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.7665982246398926, loss=1.5515367984771729
I0307 08:26:55.399987 140296826373888 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.9753377437591553, loss=1.6225694417953491
I0307 08:27:00.130760 140452179379392 spec.py:321] Evaluating on the training split.
I0307 08:27:13.143216 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 08:27:29.691282 140452179379392 spec.py:349] Evaluating on the test split.
I0307 08:27:31.481223 140452179379392 submission_runner.py:469] Time since start: 22639.77s, 	Step: 53913, 	{'train/accuracy': 0.709980845451355, 'train/loss': 1.1241072416305542, 'validation/accuracy': 0.6460399627685547, 'validation/loss': 1.4633657932281494, 'validation/num_examples': 50000, 'test/accuracy': 0.5040000081062317, 'test/loss': 2.248199224472046, 'test/num_examples': 10000, 'score': 20966.365412950516, 'total_duration': 22639.76716518402, 'accumulated_submission_time': 20966.365412950516, 'accumulated_eval_time': 1662.837821483612, 'accumulated_logging_time': 4.918170690536499}
I0307 08:27:31.562657 140296834766592 logging_writer.py:48] [53913] accumulated_eval_time=1662.84, accumulated_logging_time=4.91817, accumulated_submission_time=20966.4, global_step=53913, preemption_count=0, score=20966.4, test/accuracy=0.504, test/loss=2.2482, test/num_examples=10000, total_duration=22639.8, train/accuracy=0.709981, train/loss=1.12411, validation/accuracy=0.64604, validation/loss=1.46337, validation/num_examples=50000
I0307 08:28:05.708488 140296826373888 logging_writer.py:48] [54000] global_step=54000, grad_norm=2.0237839221954346, loss=1.6707521677017212
I0307 08:28:43.965766 140296834766592 logging_writer.py:48] [54100] global_step=54100, grad_norm=2.108121871948242, loss=1.6906641721725464
I0307 08:29:22.409271 140296826373888 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.88347327709198, loss=1.5975871086120605
I0307 08:30:01.525639 140296834766592 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.9947891235351562, loss=1.6144076585769653
I0307 08:30:40.561399 140296826373888 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.833918571472168, loss=1.6522639989852905
I0307 08:31:19.321782 140296834766592 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.886131763458252, loss=1.5279693603515625
I0307 08:31:58.356787 140296826373888 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.9111526012420654, loss=1.6723806858062744
I0307 08:32:37.076574 140296834766592 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.7521764039993286, loss=1.6619322299957275
I0307 08:33:16.301376 140296826373888 logging_writer.py:48] [54800] global_step=54800, grad_norm=2.0965166091918945, loss=1.629623293876648
I0307 08:33:55.184552 140296834766592 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.029653549194336, loss=1.6953778266906738
I0307 08:34:34.263825 140296826373888 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.7773758172988892, loss=1.5229064226150513
I0307 08:35:13.814379 140296834766592 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.864137053489685, loss=1.5928981304168701
I0307 08:35:52.076140 140296826373888 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.8769340515136719, loss=1.5071725845336914
I0307 08:36:01.765848 140452179379392 spec.py:321] Evaluating on the training split.
I0307 08:36:14.126969 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 08:36:36.888160 140452179379392 spec.py:349] Evaluating on the test split.
I0307 08:36:38.665034 140452179379392 submission_runner.py:469] Time since start: 23186.95s, 	Step: 55226, 	{'train/accuracy': 0.6990792155265808, 'train/loss': 1.1553237438201904, 'validation/accuracy': 0.6372399926185608, 'validation/loss': 1.5018441677093506, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.290228843688965, 'test/num_examples': 10000, 'score': 21476.42167043686, 'total_duration': 23186.950947523117, 'accumulated_submission_time': 21476.42167043686, 'accumulated_eval_time': 1699.7368094921112, 'accumulated_logging_time': 5.015252113342285}
I0307 08:36:38.782968 140296834766592 logging_writer.py:48] [55226] accumulated_eval_time=1699.74, accumulated_logging_time=5.01525, accumulated_submission_time=21476.4, global_step=55226, preemption_count=0, score=21476.4, test/accuracy=0.5041, test/loss=2.29023, test/num_examples=10000, total_duration=23187, train/accuracy=0.699079, train/loss=1.15532, validation/accuracy=0.63724, validation/loss=1.50184, validation/num_examples=50000
I0307 08:37:07.780741 140296826373888 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.790359616279602, loss=1.5673108100891113
I0307 08:37:46.221308 140296834766592 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.9050158262252808, loss=1.629549264907837
I0307 08:38:25.866094 140296826373888 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.9170235395431519, loss=1.6474591493606567
I0307 08:39:04.540223 140296834766592 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.0021772384643555, loss=1.6588788032531738
I0307 08:39:43.411578 140296826373888 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.8132175207138062, loss=1.565279245376587
I0307 08:40:22.012368 140296834766592 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.9339343309402466, loss=1.515928030014038
I0307 08:41:00.912834 140296826373888 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.925107717514038, loss=1.6133208274841309
I0307 08:41:39.625017 140296834766592 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7937322854995728, loss=1.6064308881759644
I0307 08:42:18.246476 140296826373888 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.8913573026657104, loss=1.649101734161377
I0307 08:42:57.480017 140296834766592 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8040591478347778, loss=1.5949395895004272
I0307 08:43:36.914760 140296826373888 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.7432212829589844, loss=1.5439105033874512
I0307 08:44:15.932088 140296834766592 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.9781036376953125, loss=1.6168642044067383
I0307 08:44:54.549790 140296826373888 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8788728713989258, loss=1.493621587753296
I0307 08:45:08.826363 140452179379392 spec.py:321] Evaluating on the training split.
I0307 08:45:21.259815 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 08:45:43.190730 140452179379392 spec.py:349] Evaluating on the test split.
I0307 08:45:44.968900 140452179379392 submission_runner.py:469] Time since start: 23733.25s, 	Step: 56538, 	{'train/accuracy': 0.7137675285339355, 'train/loss': 1.1085933446884155, 'validation/accuracy': 0.6496999859809875, 'validation/loss': 1.4519292116165161, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2400569915771484, 'test/num_examples': 10000, 'score': 21986.30232143402, 'total_duration': 23733.25483226776, 'accumulated_submission_time': 21986.30232143402, 'accumulated_eval_time': 1735.87916970253, 'accumulated_logging_time': 5.163467645645142}
I0307 08:45:45.113608 140296834766592 logging_writer.py:48] [56538] accumulated_eval_time=1735.88, accumulated_logging_time=5.16347, accumulated_submission_time=21986.3, global_step=56538, preemption_count=0, score=21986.3, test/accuracy=0.5121, test/loss=2.24006, test/num_examples=10000, total_duration=23733.3, train/accuracy=0.713768, train/loss=1.10859, validation/accuracy=0.6497, validation/loss=1.45193, validation/num_examples=50000
I0307 08:46:09.582337 140296826373888 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.8456671237945557, loss=1.587329387664795
I0307 08:46:48.007488 140296834766592 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.8606418371200562, loss=1.6829893589019775
I0307 08:47:26.709980 140296826373888 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.8997434377670288, loss=1.6000168323516846
I0307 08:48:05.387057 140296834766592 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8444019556045532, loss=1.488828182220459
I0307 08:48:44.067889 140296826373888 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.925649642944336, loss=1.6326271295547485
I0307 08:49:22.480997 140296834766592 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8502800464630127, loss=1.5509473085403442
I0307 08:50:01.142018 140296826373888 logging_writer.py:48] [57200] global_step=57200, grad_norm=2.05749773979187, loss=1.6001574993133545
I0307 08:50:39.703911 140296834766592 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.0377659797668457, loss=1.6392878293991089
I0307 08:51:17.964056 140296826373888 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9352450370788574, loss=1.5731737613677979
I0307 08:51:57.116679 140296834766592 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8224958181381226, loss=1.5614235401153564
I0307 08:52:36.311771 140296826373888 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.8341788053512573, loss=1.6199631690979004
I0307 08:53:15.310211 140296834766592 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.799194097518921, loss=1.5556743144989014
I0307 08:53:54.203320 140296826373888 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8760466575622559, loss=1.631140947341919
I0307 08:54:15.315016 140452179379392 spec.py:321] Evaluating on the training split.
I0307 08:54:27.797245 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 08:54:49.983063 140452179379392 spec.py:349] Evaluating on the test split.
I0307 08:54:51.762213 140452179379392 submission_runner.py:469] Time since start: 24280.05s, 	Step: 57855, 	{'train/accuracy': 0.7209821343421936, 'train/loss': 1.0657283067703247, 'validation/accuracy': 0.6507399678230286, 'validation/loss': 1.4275306463241577, 'validation/num_examples': 50000, 'test/accuracy': 0.5227000117301941, 'test/loss': 2.159003734588623, 'test/num_examples': 10000, 'score': 22496.30607318878, 'total_duration': 24280.048092842102, 'accumulated_submission_time': 22496.30607318878, 'accumulated_eval_time': 1772.3261406421661, 'accumulated_logging_time': 5.374384641647339}
I0307 08:54:51.893537 140296834766592 logging_writer.py:48] [57855] accumulated_eval_time=1772.33, accumulated_logging_time=5.37438, accumulated_submission_time=22496.3, global_step=57855, preemption_count=0, score=22496.3, test/accuracy=0.5227, test/loss=2.159, test/num_examples=10000, total_duration=24280, train/accuracy=0.720982, train/loss=1.06573, validation/accuracy=0.65074, validation/loss=1.42753, validation/num_examples=50000
I0307 08:55:10.275401 140296826373888 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.809461236000061, loss=1.5021053552627563
I0307 08:55:48.921937 140296834766592 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.8573508262634277, loss=1.6859371662139893
I0307 08:56:27.903638 140296826373888 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9412306547164917, loss=1.5579432249069214
I0307 08:57:07.531240 140296834766592 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.9007656574249268, loss=1.5464837551116943
I0307 08:57:47.885945 140296826373888 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8454817533493042, loss=1.5717569589614868
I0307 08:58:26.533059 140296834766592 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.834938406944275, loss=1.5142658948898315
I0307 08:59:05.162542 140296826373888 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.7565416097640991, loss=1.4158231019973755
I0307 08:59:44.001371 140296834766592 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.846807837486267, loss=1.530786156654358
I0307 09:00:22.512161 140296826373888 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.7809983491897583, loss=1.5688949823379517
I0307 09:01:01.536407 140296834766592 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.8076127767562866, loss=1.5333245992660522
I0307 09:01:40.356389 140296826373888 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.9680507183074951, loss=1.490554690361023
I0307 09:02:19.362647 140296834766592 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9643350839614868, loss=1.5489264726638794
I0307 09:02:58.581416 140296826373888 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.0215132236480713, loss=1.5453580617904663
I0307 09:03:22.049576 140452179379392 spec.py:321] Evaluating on the training split.
I0307 09:03:34.895103 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 09:03:54.047267 140452179379392 spec.py:349] Evaluating on the test split.
I0307 09:03:55.785685 140452179379392 submission_runner.py:469] Time since start: 24824.07s, 	Step: 59161, 	{'train/accuracy': 0.7261240482330322, 'train/loss': 1.0557602643966675, 'validation/accuracy': 0.6517800092697144, 'validation/loss': 1.4288057088851929, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1769392490386963, 'test/num_examples': 10000, 'score': 23006.28792977333, 'total_duration': 24824.071602344513, 'accumulated_submission_time': 23006.28792977333, 'accumulated_eval_time': 1806.062062740326, 'accumulated_logging_time': 5.546006441116333}
I0307 09:03:55.904771 140296834766592 logging_writer.py:48] [59161] accumulated_eval_time=1806.06, accumulated_logging_time=5.54601, accumulated_submission_time=23006.3, global_step=59161, preemption_count=0, score=23006.3, test/accuracy=0.5225, test/loss=2.17694, test/num_examples=10000, total_duration=24824.1, train/accuracy=0.726124, train/loss=1.05576, validation/accuracy=0.65178, validation/loss=1.42881, validation/num_examples=50000
I0307 09:04:11.647491 140296826373888 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.6460492610931396, loss=1.479975938796997
I0307 09:04:50.643765 140296834766592 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8483234643936157, loss=1.4452279806137085
I0307 09:05:29.616370 140296826373888 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.820994257926941, loss=1.5886776447296143
I0307 09:06:08.314661 140296834766592 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.8066565990447998, loss=1.6153444051742554
I0307 09:06:47.087546 140296826373888 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.7244611978530884, loss=1.5440882444381714
I0307 09:07:25.761686 140296834766592 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.8357480764389038, loss=1.6619120836257935
I0307 09:08:04.979701 140296826373888 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.8186224699020386, loss=1.5637074708938599
I0307 09:08:43.523360 140296834766592 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.8350303173065186, loss=1.6185909509658813
I0307 09:09:22.296347 140296826373888 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.8818608522415161, loss=1.5138065814971924
I0307 09:10:01.938623 140296834766592 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.0323188304901123, loss=1.6964912414550781
I0307 09:10:40.455763 140296826373888 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.179563522338867, loss=1.536744475364685
I0307 09:11:19.386018 140296834766592 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.8608111143112183, loss=1.594923496246338
I0307 09:11:58.152872 140296826373888 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8173797130584717, loss=1.4609616994857788
I0307 09:12:25.815373 140452179379392 spec.py:321] Evaluating on the training split.
I0307 09:12:38.311433 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 09:13:03.244095 140452179379392 spec.py:349] Evaluating on the test split.
I0307 09:13:05.005259 140452179379392 submission_runner.py:469] Time since start: 25373.29s, 	Step: 60473, 	{'train/accuracy': 0.7180524468421936, 'train/loss': 1.1005436182022095, 'validation/accuracy': 0.6475399732589722, 'validation/loss': 1.4441503286361694, 'validation/num_examples': 50000, 'test/accuracy': 0.5220000147819519, 'test/loss': 2.1754355430603027, 'test/num_examples': 10000, 'score': 23516.03226876259, 'total_duration': 25373.291135311127, 'accumulated_submission_time': 23516.03226876259, 'accumulated_eval_time': 1845.2517132759094, 'accumulated_logging_time': 5.698989152908325}
I0307 09:13:05.143560 140296834766592 logging_writer.py:48] [60473] accumulated_eval_time=1845.25, accumulated_logging_time=5.69899, accumulated_submission_time=23516, global_step=60473, preemption_count=0, score=23516, test/accuracy=0.522, test/loss=2.17544, test/num_examples=10000, total_duration=25373.3, train/accuracy=0.718052, train/loss=1.10054, validation/accuracy=0.64754, validation/loss=1.44415, validation/num_examples=50000
I0307 09:13:16.046285 140296826373888 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.9905608892440796, loss=1.5984939336776733
I0307 09:13:54.719292 140296834766592 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.9027031660079956, loss=1.5842403173446655
I0307 09:14:33.634041 140296826373888 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.9235212802886963, loss=1.5887938737869263
I0307 09:15:12.320225 140296834766592 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.7659063339233398, loss=1.613237738609314
I0307 09:15:51.372582 140296826373888 logging_writer.py:48] [60900] global_step=60900, grad_norm=2.2076609134674072, loss=1.6426459550857544
I0307 09:16:30.320828 140296834766592 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.828436017036438, loss=1.5739082098007202
I0307 09:17:09.560405 140296826373888 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.8449512720108032, loss=1.5314860343933105
I0307 09:17:48.495924 140296834766592 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.9135602712631226, loss=1.5974434614181519
I0307 09:18:28.113723 140296826373888 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9603418111801147, loss=1.6678009033203125
I0307 09:19:07.636107 140296834766592 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.0695204734802246, loss=1.516121506690979
I0307 09:19:46.321547 140296826373888 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8118607997894287, loss=1.625079870223999
I0307 09:20:24.881167 140296834766592 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.0501019954681396, loss=1.5416334867477417
I0307 09:21:03.490422 140296826373888 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.814658522605896, loss=1.5494208335876465
I0307 09:21:35.317927 140452179379392 spec.py:321] Evaluating on the training split.
I0307 09:21:48.003618 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 09:22:08.575530 140452179379392 spec.py:349] Evaluating on the test split.
I0307 09:22:10.354654 140452179379392 submission_runner.py:469] Time since start: 25918.64s, 	Step: 61783, 	{'train/accuracy': 0.7188097834587097, 'train/loss': 1.0719362497329712, 'validation/accuracy': 0.6545599699020386, 'validation/loss': 1.4217674732208252, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.145974636077881, 'test/num_examples': 10000, 'score': 24026.048491477966, 'total_duration': 25918.64058828354, 'accumulated_submission_time': 24026.048491477966, 'accumulated_eval_time': 1880.2882618904114, 'accumulated_logging_time': 5.862671613693237}
I0307 09:22:10.449346 140296834766592 logging_writer.py:48] [61783] accumulated_eval_time=1880.29, accumulated_logging_time=5.86267, accumulated_submission_time=24026, global_step=61783, preemption_count=0, score=24026, test/accuracy=0.53, test/loss=2.14597, test/num_examples=10000, total_duration=25918.6, train/accuracy=0.71881, train/loss=1.07194, validation/accuracy=0.65456, validation/loss=1.42177, validation/num_examples=50000
I0307 09:22:17.414330 140296826373888 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.8925080299377441, loss=1.5377559661865234
I0307 09:22:55.959570 140296834766592 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.039804458618164, loss=1.5841269493103027
I0307 09:23:34.323715 140296826373888 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.9655121564865112, loss=1.5121257305145264
I0307 09:24:12.880269 140296834766592 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.8945386409759521, loss=1.566685438156128
I0307 09:24:51.730111 140296826373888 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.936855435371399, loss=1.548474669456482
I0307 09:25:30.754588 140296834766592 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.765067219734192, loss=1.5006160736083984
I0307 09:26:09.653887 140296826373888 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.761377215385437, loss=1.5812581777572632
I0307 09:26:48.615483 140296834766592 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.8280936479568481, loss=1.5442227125167847
I0307 09:27:28.474987 140296826373888 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.1013388633728027, loss=1.4966784715652466
I0307 09:28:07.280871 140296834766592 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.9609615802764893, loss=1.6455259323120117
I0307 09:28:46.281563 140296826373888 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.886684775352478, loss=1.712225079536438
I0307 09:29:25.005596 140296834766592 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.8377217054367065, loss=1.5397648811340332
I0307 09:30:03.931166 140296826373888 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.787005066871643, loss=1.504850149154663
I0307 09:30:40.459840 140452179379392 spec.py:321] Evaluating on the training split.
I0307 09:30:53.016143 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 09:31:11.504169 140452179379392 spec.py:349] Evaluating on the test split.
I0307 09:31:13.273812 140452179379392 submission_runner.py:469] Time since start: 26461.56s, 	Step: 63095, 	{'train/accuracy': 0.7278180718421936, 'train/loss': 1.0357601642608643, 'validation/accuracy': 0.6565399765968323, 'validation/loss': 1.404319167137146, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.1122193336486816, 'test/num_examples': 10000, 'score': 24535.905178308487, 'total_duration': 26461.55972480774, 'accumulated_submission_time': 24535.905178308487, 'accumulated_eval_time': 1913.1020357608795, 'accumulated_logging_time': 5.981022357940674}
I0307 09:31:13.370120 140296834766592 logging_writer.py:48] [63095] accumulated_eval_time=1913.1, accumulated_logging_time=5.98102, accumulated_submission_time=24535.9, global_step=63095, preemption_count=0, score=24535.9, test/accuracy=0.529, test/loss=2.11222, test/num_examples=10000, total_duration=26461.6, train/accuracy=0.727818, train/loss=1.03576, validation/accuracy=0.65654, validation/loss=1.40432, validation/num_examples=50000
I0307 09:31:15.872030 140296826373888 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.149712085723877, loss=1.589370846748352
I0307 09:31:54.727848 140296834766592 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.8800442218780518, loss=1.5488307476043701
I0307 09:32:33.580053 140296826373888 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.242389440536499, loss=1.5790188312530518
I0307 09:33:12.107105 140296834766592 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.1279027462005615, loss=1.619428038597107
I0307 09:33:50.395420 140296826373888 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.0047924518585205, loss=1.5438995361328125
I0307 09:34:29.250858 140296834766592 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.930283546447754, loss=1.6663627624511719
I0307 09:35:07.903784 140296826373888 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.0346591472625732, loss=1.4213436841964722
I0307 09:35:47.527369 140296834766592 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.7215951681137085, loss=1.5061800479888916
I0307 09:36:27.072964 140296826373888 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.7838460206985474, loss=1.4342753887176514
I0307 09:37:05.580477 140296834766592 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.995055079460144, loss=1.5741446018218994
I0307 09:37:44.573462 140296826373888 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.045058488845825, loss=1.6437879800796509
I0307 09:38:23.211011 140296834766592 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.0886549949645996, loss=1.4966696500778198
I0307 09:39:02.109391 140296826373888 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.952615737915039, loss=1.596557378768921
I0307 09:39:41.379230 140296834766592 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.766381859779358, loss=1.56671142578125
I0307 09:39:43.415553 140452179379392 spec.py:321] Evaluating on the training split.
I0307 09:39:56.284209 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 09:40:13.774483 140452179379392 spec.py:349] Evaluating on the test split.
I0307 09:40:15.545982 140452179379392 submission_runner.py:469] Time since start: 27003.83s, 	Step: 64406, 	{'train/accuracy': 0.7185905575752258, 'train/loss': 1.093408465385437, 'validation/accuracy': 0.6451799869537354, 'validation/loss': 1.4653292894363403, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.2412753105163574, 'test/num_examples': 10000, 'score': 25045.75627207756, 'total_duration': 27003.83189702034, 'accumulated_submission_time': 25045.75627207756, 'accumulated_eval_time': 1945.2322680950165, 'accumulated_logging_time': 6.138960599899292}
I0307 09:40:15.690333 140296826373888 logging_writer.py:48] [64406] accumulated_eval_time=1945.23, accumulated_logging_time=6.13896, accumulated_submission_time=25045.8, global_step=64406, preemption_count=0, score=25045.8, test/accuracy=0.5161, test/loss=2.24128, test/num_examples=10000, total_duration=27003.8, train/accuracy=0.718591, train/loss=1.09341, validation/accuracy=0.64518, validation/loss=1.46533, validation/num_examples=50000
I0307 09:40:52.847741 140296834766592 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.097001791000366, loss=1.7284717559814453
I0307 09:41:31.849275 140296826373888 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8442291021347046, loss=1.467519998550415
I0307 09:42:10.943572 140296834766592 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.057213544845581, loss=1.5626230239868164
I0307 09:42:49.769724 140296826373888 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.084669351577759, loss=1.521890640258789
I0307 09:43:28.454190 140296834766592 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.451326847076416, loss=1.5493876934051514
I0307 09:44:08.181743 140296826373888 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.1147305965423584, loss=1.6527299880981445
I0307 09:44:48.925260 140296834766592 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.9944244623184204, loss=1.5099960565567017
I0307 09:45:27.279891 140296826373888 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8088377714157104, loss=1.4955265522003174
I0307 09:46:05.884709 140296834766592 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.742010474205017, loss=1.480036973953247
I0307 09:46:44.335266 140296826373888 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.8378928899765015, loss=1.5457086563110352
I0307 09:47:23.320812 140296834766592 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.1200544834136963, loss=1.6726264953613281
I0307 09:48:01.849379 140296826373888 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.224076271057129, loss=1.5750975608825684
I0307 09:48:40.603746 140296834766592 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.018239736557007, loss=1.6241915225982666
I0307 09:48:45.640638 140452179379392 spec.py:321] Evaluating on the training split.
I0307 09:48:57.987905 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 09:49:18.765301 140452179379392 spec.py:349] Evaluating on the test split.
I0307 09:49:20.581202 140452179379392 submission_runner.py:469] Time since start: 27548.87s, 	Step: 65714, 	{'train/accuracy': 0.7205237150192261, 'train/loss': 1.073755145072937, 'validation/accuracy': 0.6462199687957764, 'validation/loss': 1.4506008625030518, 'validation/num_examples': 50000, 'test/accuracy': 0.516800045967102, 'test/loss': 2.1902425289154053, 'test/num_examples': 10000, 'score': 25555.54702115059, 'total_duration': 27548.867127895355, 'accumulated_submission_time': 25555.54702115059, 'accumulated_eval_time': 1980.1726524829865, 'accumulated_logging_time': 6.310142517089844}
I0307 09:49:20.665337 140296826373888 logging_writer.py:48] [65714] accumulated_eval_time=1980.17, accumulated_logging_time=6.31014, accumulated_submission_time=25555.5, global_step=65714, preemption_count=0, score=25555.5, test/accuracy=0.5168, test/loss=2.19024, test/num_examples=10000, total_duration=27548.9, train/accuracy=0.720524, train/loss=1.07376, validation/accuracy=0.64622, validation/loss=1.4506, validation/num_examples=50000
I0307 09:49:54.785143 140296834766592 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.0174880027770996, loss=1.498408555984497
I0307 09:50:33.834397 140296826373888 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.8115739822387695, loss=1.596522569656372
I0307 09:51:12.723653 140296834766592 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.8165314197540283, loss=1.5593749284744263
I0307 09:51:51.393885 140296826373888 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.8787415027618408, loss=1.541602373123169
I0307 09:52:30.112319 140296834766592 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.984329104423523, loss=1.5154671669006348
I0307 09:53:09.235075 140296826373888 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.0410139560699463, loss=1.6007380485534668
I0307 09:53:48.433465 140296834766592 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.0713846683502197, loss=1.6673251390457153
I0307 09:54:27.276561 140296826373888 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.0474936962127686, loss=1.6155885457992554
I0307 09:55:06.280585 140296834766592 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.8715057373046875, loss=1.5390703678131104
I0307 09:55:45.263662 140296826373888 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.1329596042633057, loss=1.580672025680542
I0307 09:56:23.890107 140296834766592 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.834655523300171, loss=1.4729083776474
I0307 09:57:02.805036 140296826373888 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.9290932416915894, loss=1.5718127489089966
I0307 09:57:41.787915 140296834766592 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.0667312145233154, loss=1.558162808418274
I0307 09:57:50.789176 140452179379392 spec.py:321] Evaluating on the training split.
I0307 09:58:03.689132 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 09:58:26.373167 140452179379392 spec.py:349] Evaluating on the test split.
I0307 09:58:28.148497 140452179379392 submission_runner.py:469] Time since start: 28096.43s, 	Step: 67024, 	{'train/accuracy': 0.715262234210968, 'train/loss': 1.1018823385238647, 'validation/accuracy': 0.6509799957275391, 'validation/loss': 1.4456052780151367, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.169757127761841, 'test/num_examples': 10000, 'score': 26065.481178045273, 'total_duration': 28096.434440612793, 'accumulated_submission_time': 26065.481178045273, 'accumulated_eval_time': 2017.5318043231964, 'accumulated_logging_time': 6.453748464584351}
I0307 09:58:28.236352 140296826373888 logging_writer.py:48] [67024] accumulated_eval_time=2017.53, accumulated_logging_time=6.45375, accumulated_submission_time=26065.5, global_step=67024, preemption_count=0, score=26065.5, test/accuracy=0.5258, test/loss=2.16976, test/num_examples=10000, total_duration=28096.4, train/accuracy=0.715262, train/loss=1.10188, validation/accuracy=0.65098, validation/loss=1.44561, validation/num_examples=50000
I0307 09:58:58.123549 140296834766592 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.022326707839966, loss=1.532027006149292
I0307 09:59:36.866536 140296826373888 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.013122797012329, loss=1.6095072031021118
I0307 10:00:15.903146 140296834766592 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.0419883728027344, loss=1.4086037874221802
I0307 10:00:54.749124 140296826373888 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.1663904190063477, loss=1.6252036094665527
I0307 10:01:33.584076 140296834766592 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.9373735189437866, loss=1.652363657951355
I0307 10:02:13.126266 140296826373888 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.7804733514785767, loss=1.544434905052185
I0307 10:02:51.982957 140296834766592 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.9993584156036377, loss=1.5595983266830444
I0307 10:03:30.537577 140296826373888 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.9820277690887451, loss=1.5631844997406006
I0307 10:04:09.208976 140296834766592 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.9645764827728271, loss=1.5147581100463867
I0307 10:04:48.064023 140296826373888 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.113828420639038, loss=1.516298532485962
I0307 10:05:26.902323 140296834766592 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9904440641403198, loss=1.5226407051086426
I0307 10:06:05.525571 140296826373888 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.0893006324768066, loss=1.623983383178711
I0307 10:06:44.410924 140296834766592 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.910589575767517, loss=1.486871600151062
I0307 10:06:58.298257 140452179379392 spec.py:321] Evaluating on the training split.
I0307 10:07:10.745675 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 10:07:26.600150 140452179379392 spec.py:349] Evaluating on the test split.
I0307 10:07:28.378189 140452179379392 submission_runner.py:469] Time since start: 28636.66s, 	Step: 68337, 	{'train/accuracy': 0.7241310477256775, 'train/loss': 1.0647848844528198, 'validation/accuracy': 0.6534199714660645, 'validation/loss': 1.424412727355957, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.1409544944763184, 'test/num_examples': 10000, 'score': 26575.384176254272, 'total_duration': 28636.664094924927, 'accumulated_submission_time': 26575.384176254272, 'accumulated_eval_time': 2047.6115307807922, 'accumulated_logging_time': 6.568617105484009}
I0307 10:07:28.512216 140296826373888 logging_writer.py:48] [68337] accumulated_eval_time=2047.61, accumulated_logging_time=6.56862, accumulated_submission_time=26575.4, global_step=68337, preemption_count=0, score=26575.4, test/accuracy=0.5229, test/loss=2.14095, test/num_examples=10000, total_duration=28636.7, train/accuracy=0.724131, train/loss=1.06478, validation/accuracy=0.65342, validation/loss=1.42441, validation/num_examples=50000
I0307 10:07:53.548162 140296834766592 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.049457550048828, loss=1.5170166492462158
I0307 10:08:32.027954 140296826373888 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.0473439693450928, loss=1.558762788772583
I0307 10:09:10.891100 140296834766592 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.7738062143325806, loss=1.4421907663345337
I0307 10:09:49.801576 140296826373888 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.9941014051437378, loss=1.5758172273635864
I0307 10:10:28.932066 140296834766592 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.839840054512024, loss=1.4445281028747559
I0307 10:11:07.810108 140296826373888 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.293381690979004, loss=1.5722227096557617
I0307 10:11:46.977370 140296834766592 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.7836931943893433, loss=1.5776426792144775
I0307 10:12:25.611209 140296826373888 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.820124864578247, loss=1.500396728515625
I0307 10:13:04.762672 140296834766592 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.9970309734344482, loss=1.4753338098526
I0307 10:13:43.724831 140296826373888 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.9998167753219604, loss=1.5639030933380127
I0307 10:14:22.275153 140296834766592 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.083589792251587, loss=1.5157994031906128
I0307 10:15:01.092839 140296826373888 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.1581904888153076, loss=1.5635838508605957
I0307 10:15:39.818086 140296834766592 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.9400748014450073, loss=1.5444586277008057
I0307 10:15:58.488507 140452179379392 spec.py:321] Evaluating on the training split.
I0307 10:16:11.450973 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 10:16:35.642113 140452179379392 spec.py:349] Evaluating on the test split.
I0307 10:16:37.415133 140452179379392 submission_runner.py:469] Time since start: 29185.70s, 	Step: 69649, 	{'train/accuracy': 0.7201849222183228, 'train/loss': 1.0793545246124268, 'validation/accuracy': 0.6511799693107605, 'validation/loss': 1.4507819414138794, 'validation/num_examples': 50000, 'test/accuracy': 0.5275000333786011, 'test/loss': 2.1988277435302734, 'test/num_examples': 10000, 'score': 27085.194271087646, 'total_duration': 29185.701063632965, 'accumulated_submission_time': 27085.194271087646, 'accumulated_eval_time': 2086.5379796028137, 'accumulated_logging_time': 6.7370710372924805}
I0307 10:16:37.499924 140296826373888 logging_writer.py:48] [69649] accumulated_eval_time=2086.54, accumulated_logging_time=6.73707, accumulated_submission_time=27085.2, global_step=69649, preemption_count=0, score=27085.2, test/accuracy=0.5275, test/loss=2.19883, test/num_examples=10000, total_duration=29185.7, train/accuracy=0.720185, train/loss=1.07935, validation/accuracy=0.65118, validation/loss=1.45078, validation/num_examples=50000
I0307 10:16:58.039925 140296834766592 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.8563662767410278, loss=1.520626425743103
I0307 10:17:37.414517 140296826373888 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.9865751266479492, loss=1.5114495754241943
I0307 10:18:16.397202 140296834766592 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.8821996450424194, loss=1.5473532676696777
I0307 10:18:55.140205 140296826373888 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.8788846731185913, loss=1.6027332544326782
I0307 10:19:34.322915 140296834766592 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.9047964811325073, loss=1.4803377389907837
I0307 10:20:13.534050 140296826373888 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.029454469680786, loss=1.4907190799713135
I0307 10:20:52.514572 140296834766592 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.0328123569488525, loss=1.503364086151123
I0307 10:21:30.949191 140296826373888 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.9396315813064575, loss=1.6236579418182373
I0307 10:22:09.796596 140296834766592 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.9589647054672241, loss=1.5130878686904907
I0307 10:22:47.846474 140296826373888 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.069406032562256, loss=1.6131504774093628
I0307 10:23:26.705341 140296834766592 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.8393959999084473, loss=1.4992594718933105
I0307 10:24:05.784541 140296826373888 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.9883925914764404, loss=1.5479322671890259
I0307 10:24:44.615712 140296834766592 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.9178221225738525, loss=1.4919923543930054
I0307 10:25:07.663793 140452179379392 spec.py:321] Evaluating on the training split.
I0307 10:25:19.890883 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 10:25:38.730392 140452179379392 spec.py:349] Evaluating on the test split.
I0307 10:25:40.502827 140452179379392 submission_runner.py:469] Time since start: 29728.79s, 	Step: 70960, 	{'train/accuracy': 0.7037228941917419, 'train/loss': 1.1489083766937256, 'validation/accuracy': 0.636680006980896, 'validation/loss': 1.494072675704956, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.237518310546875, 'test/num_examples': 10000, 'score': 27595.18388724327, 'total_duration': 29728.788732528687, 'accumulated_submission_time': 27595.18388724327, 'accumulated_eval_time': 2119.3768105506897, 'accumulated_logging_time': 6.865436792373657}
I0307 10:25:40.623460 140296826373888 logging_writer.py:48] [70960] accumulated_eval_time=2119.38, accumulated_logging_time=6.86544, accumulated_submission_time=27595.2, global_step=70960, preemption_count=0, score=27595.2, test/accuracy=0.5158, test/loss=2.23752, test/num_examples=10000, total_duration=29728.8, train/accuracy=0.703723, train/loss=1.14891, validation/accuracy=0.63668, validation/loss=1.49407, validation/num_examples=50000
I0307 10:25:56.575076 140296834766592 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.103347063064575, loss=1.5248098373413086
I0307 10:26:35.151801 140296826373888 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.0045900344848633, loss=1.5010775327682495
I0307 10:27:13.728596 140296834766592 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.320221185684204, loss=1.6180880069732666
I0307 10:27:52.439475 140296826373888 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.010472297668457, loss=1.5829486846923828
I0307 10:28:31.522331 140296834766592 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.2592954635620117, loss=1.5124619007110596
I0307 10:29:10.210336 140296826373888 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.983275294303894, loss=1.6410108804702759
I0307 10:29:48.656359 140296834766592 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.160140037536621, loss=1.6269688606262207
I0307 10:30:27.576922 140296826373888 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.8490259647369385, loss=1.4935413599014282
I0307 10:31:06.296631 140296834766592 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.0872418880462646, loss=1.5408365726470947
I0307 10:31:44.901468 140296826373888 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.982173204421997, loss=1.4165523052215576
I0307 10:32:23.703805 140296834766592 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.0040676593780518, loss=1.5527207851409912
I0307 10:33:02.052503 140296826373888 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.9855495691299438, loss=1.504441738128662
I0307 10:33:40.753082 140296834766592 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.1886773109436035, loss=1.5132334232330322
I0307 10:34:10.649243 140452179379392 spec.py:321] Evaluating on the training split.
I0307 10:34:23.213182 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 10:34:43.581202 140452179379392 spec.py:349] Evaluating on the test split.
I0307 10:34:45.350194 140452179379392 submission_runner.py:469] Time since start: 30273.64s, 	Step: 72278, 	{'train/accuracy': 0.7111567258834839, 'train/loss': 1.1290578842163086, 'validation/accuracy': 0.6398000121116638, 'validation/loss': 1.4834872484207153, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.2210605144500732, 'test/num_examples': 10000, 'score': 28105.055102348328, 'total_duration': 30273.6361413002, 'accumulated_submission_time': 28105.055102348328, 'accumulated_eval_time': 2154.0775978565216, 'accumulated_logging_time': 7.0094029903411865}
I0307 10:34:45.462419 140296826373888 logging_writer.py:48] [72278] accumulated_eval_time=2154.08, accumulated_logging_time=7.0094, accumulated_submission_time=28105.1, global_step=72278, preemption_count=0, score=28105.1, test/accuracy=0.5114, test/loss=2.22106, test/num_examples=10000, total_duration=30273.6, train/accuracy=0.711157, train/loss=1.12906, validation/accuracy=0.6398, validation/loss=1.48349, validation/num_examples=50000
I0307 10:34:54.497726 140296834766592 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.102073907852173, loss=1.6194618940353394
I0307 10:35:32.776935 140296826373888 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.9447784423828125, loss=1.5087177753448486
I0307 10:36:11.796895 140296834766592 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.8416372537612915, loss=1.4918614625930786
I0307 10:36:51.187337 140296826373888 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9041584730148315, loss=1.5533435344696045
I0307 10:37:30.277938 140296834766592 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.9126310348510742, loss=1.461134910583496
I0307 10:38:08.856940 140296826373888 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.077951669692993, loss=1.5177514553070068
I0307 10:38:47.846910 140296834766592 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.9060826301574707, loss=1.6017892360687256
I0307 10:39:26.625026 140296826373888 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.9597194194793701, loss=1.4491811990737915
I0307 10:40:05.729088 140296834766592 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.020059823989868, loss=1.5204564332962036
I0307 10:40:44.683501 140296826373888 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.979333758354187, loss=1.5822362899780273
I0307 10:41:23.512412 140296834766592 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.1335196495056152, loss=1.557937741279602
I0307 10:42:02.489856 140296826373888 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.107572317123413, loss=1.556592583656311
I0307 10:42:41.162853 140296834766592 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.92294180393219, loss=1.3769760131835938
I0307 10:43:15.522902 140452179379392 spec.py:321] Evaluating on the training split.
I0307 10:43:28.285082 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 10:43:53.782617 140452179379392 spec.py:349] Evaluating on the test split.
I0307 10:43:55.552534 140452179379392 submission_runner.py:469] Time since start: 30823.84s, 	Step: 73589, 	{'train/accuracy': 0.7241111397743225, 'train/loss': 1.0535086393356323, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.4209418296813965, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.159533739089966, 'test/num_examples': 10000, 'score': 28614.962498426437, 'total_duration': 30823.838453769684, 'accumulated_submission_time': 28614.962498426437, 'accumulated_eval_time': 2194.1070399284363, 'accumulated_logging_time': 7.146681308746338}
I0307 10:43:55.634467 140296826373888 logging_writer.py:48] [73589] accumulated_eval_time=2194.11, accumulated_logging_time=7.14668, accumulated_submission_time=28615, global_step=73589, preemption_count=0, score=28615, test/accuracy=0.5261, test/loss=2.15953, test/num_examples=10000, total_duration=30823.8, train/accuracy=0.724111, train/loss=1.05351, validation/accuracy=0.652, validation/loss=1.42094, validation/num_examples=50000
I0307 10:44:00.417418 140296834766592 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.9458743333816528, loss=1.5674717426300049
I0307 10:44:39.076126 140296826373888 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.8251594305038452, loss=1.4160513877868652
I0307 10:45:18.032836 140296834766592 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.9281948804855347, loss=1.5838704109191895
I0307 10:45:57.658401 140296826373888 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.0025463104248047, loss=1.5145930051803589
I0307 10:46:36.149019 140296834766592 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.0093345642089844, loss=1.5318694114685059
I0307 10:47:15.110644 140296826373888 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0705456733703613, loss=1.596923589706421
I0307 10:47:53.872402 140296834766592 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.99037766456604, loss=1.570393681526184
I0307 10:48:33.580136 140296826373888 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.9416176080703735, loss=1.4819120168685913
I0307 10:49:12.180406 140296834766592 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.9377135038375854, loss=1.5134376287460327
I0307 10:49:51.277983 140296826373888 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.90003502368927, loss=1.3943732976913452
I0307 10:50:30.020094 140296834766592 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.0203189849853516, loss=1.4861056804656982
I0307 10:51:08.871179 140296826373888 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.1469743251800537, loss=1.4624814987182617
I0307 10:51:47.942171 140296834766592 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.9819515943527222, loss=1.411076307296753
I0307 10:52:25.635617 140452179379392 spec.py:321] Evaluating on the training split.
I0307 10:52:38.067139 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 10:53:00.934266 140452179379392 spec.py:349] Evaluating on the test split.
I0307 10:53:02.713163 140452179379392 submission_runner.py:469] Time since start: 31371.00s, 	Step: 74898, 	{'train/accuracy': 0.7225366830825806, 'train/loss': 1.0740221738815308, 'validation/accuracy': 0.6500399708747864, 'validation/loss': 1.4353623390197754, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.14973783493042, 'test/num_examples': 10000, 'score': 29124.793865203857, 'total_duration': 31370.99911212921, 'accumulated_submission_time': 29124.793865203857, 'accumulated_eval_time': 2231.1844239234924, 'accumulated_logging_time': 7.264568090438843}
I0307 10:53:02.887959 140296826373888 logging_writer.py:48] [74898] accumulated_eval_time=2231.18, accumulated_logging_time=7.26457, accumulated_submission_time=29124.8, global_step=74898, preemption_count=0, score=29124.8, test/accuracy=0.5238, test/loss=2.14974, test/num_examples=10000, total_duration=31371, train/accuracy=0.722537, train/loss=1.07402, validation/accuracy=0.65004, validation/loss=1.43536, validation/num_examples=50000
I0307 10:53:04.059904 140296834766592 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.9230775833129883, loss=1.4818763732910156
I0307 10:53:42.759138 140296826373888 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.0709691047668457, loss=1.5397357940673828
I0307 10:54:22.462291 140296834766592 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.9431335926055908, loss=1.514897346496582
I0307 10:55:01.286200 140296826373888 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.3025388717651367, loss=1.4968297481536865
I0307 10:55:39.882427 140296834766592 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.12428879737854, loss=1.5318048000335693
I0307 10:56:18.764706 140296826373888 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.035534381866455, loss=1.526606798171997
I0307 10:56:57.802792 140296834766592 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.2750723361968994, loss=1.5135135650634766
I0307 10:57:36.477264 140296826373888 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.3631038665771484, loss=1.5650837421417236
I0307 10:58:15.470971 140296834766592 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.0539400577545166, loss=1.5013500452041626
I0307 10:58:54.563713 140296826373888 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.9740816354751587, loss=1.457688570022583
I0307 10:59:33.723245 140296834766592 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.7569478750228882, loss=1.4171991348266602
I0307 11:00:12.607434 140296826373888 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.8760501146316528, loss=1.3455867767333984
I0307 11:00:51.383271 140296834766592 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.9100645780563354, loss=1.465879201889038
I0307 11:01:29.944426 140296826373888 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.259840488433838, loss=1.4989510774612427
I0307 11:01:32.945930 140452179379392 spec.py:321] Evaluating on the training split.
I0307 11:01:45.789198 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 11:02:09.201402 140452179379392 spec.py:349] Evaluating on the test split.
I0307 11:02:11.002360 140452179379392 submission_runner.py:469] Time since start: 31919.29s, 	Step: 76209, 	{'train/accuracy': 0.7336973547935486, 'train/loss': 1.0110986232757568, 'validation/accuracy': 0.6649999618530273, 'validation/loss': 1.3687154054641724, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.089961528778076, 'test/num_examples': 10000, 'score': 29634.678155899048, 'total_duration': 31919.2883040905, 'accumulated_submission_time': 29634.678155899048, 'accumulated_eval_time': 2269.2406833171844, 'accumulated_logging_time': 7.483036279678345}
I0307 11:02:11.101997 140296834766592 logging_writer.py:48] [76209] accumulated_eval_time=2269.24, accumulated_logging_time=7.48304, accumulated_submission_time=29634.7, global_step=76209, preemption_count=0, score=29634.7, test/accuracy=0.5363, test/loss=2.08996, test/num_examples=10000, total_duration=31919.3, train/accuracy=0.733697, train/loss=1.0111, validation/accuracy=0.665, validation/loss=1.36872, validation/num_examples=50000
I0307 11:02:46.610189 140296826373888 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.032792806625366, loss=1.363145351409912
I0307 11:03:26.484368 140296834766592 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.15482234954834, loss=1.5639381408691406
I0307 11:04:05.427530 140296826373888 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.2331600189208984, loss=1.5861968994140625
I0307 11:04:44.418517 140296834766592 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.204399347305298, loss=1.5013856887817383
I0307 11:05:23.590210 140296826373888 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.0613298416137695, loss=1.4526162147521973
I0307 11:06:02.408722 140296834766592 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.03289532661438, loss=1.551276445388794
I0307 11:06:41.579427 140296826373888 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.1368346214294434, loss=1.4927263259887695
I0307 11:07:20.536541 140296834766592 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.0415971279144287, loss=1.5399467945098877
I0307 11:07:59.333880 140296826373888 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.2161571979522705, loss=1.6069684028625488
I0307 11:08:37.943182 140296834766592 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.080406427383423, loss=1.418707013130188
I0307 11:09:16.313303 140296826373888 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.111809730529785, loss=1.4904162883758545
I0307 11:09:54.812332 140296834766592 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.950541377067566, loss=1.4458661079406738
I0307 11:10:33.459123 140296826373888 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.8855934143066406, loss=1.3566899299621582
I0307 11:10:41.181053 140452179379392 spec.py:321] Evaluating on the training split.
I0307 11:10:53.723156 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 11:11:13.926165 140452179379392 spec.py:349] Evaluating on the test split.
I0307 11:11:15.725041 140452179379392 submission_runner.py:469] Time since start: 32464.01s, 	Step: 77521, 	{'train/accuracy': 0.7310666441917419, 'train/loss': 1.0373178720474243, 'validation/accuracy': 0.6620999574661255, 'validation/loss': 1.3757272958755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.099241256713867, 'test/num_examples': 10000, 'score': 30144.583835601807, 'total_duration': 32464.010974168777, 'accumulated_submission_time': 30144.583835601807, 'accumulated_eval_time': 2303.78449344635, 'accumulated_logging_time': 7.621431827545166}
I0307 11:11:15.800873 140296834766592 logging_writer.py:48] [77521] accumulated_eval_time=2303.78, accumulated_logging_time=7.62143, accumulated_submission_time=30144.6, global_step=77521, preemption_count=0, score=30144.6, test/accuracy=0.5335, test/loss=2.09924, test/num_examples=10000, total_duration=32464, train/accuracy=0.731067, train/loss=1.03732, validation/accuracy=0.6621, validation/loss=1.37573, validation/num_examples=50000
I0307 11:11:47.256980 140296826373888 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.1466450691223145, loss=1.5954203605651855
I0307 11:12:26.007958 140296834766592 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.127157688140869, loss=1.6202999353408813
I0307 11:13:04.725643 140296826373888 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.9221798181533813, loss=1.4221992492675781
I0307 11:13:43.184177 140296834766592 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.0855212211608887, loss=1.4977703094482422
I0307 11:14:22.159859 140296826373888 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.043203115463257, loss=1.4993648529052734
I0307 11:15:00.877851 140296834766592 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.0041403770446777, loss=1.53737211227417
I0307 11:15:39.702550 140296826373888 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.1435039043426514, loss=1.4939002990722656
I0307 11:16:18.633682 140296834766592 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.086986780166626, loss=1.3902397155761719
I0307 11:16:57.395324 140296826373888 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.9917062520980835, loss=1.4363505840301514
I0307 11:17:36.149831 140296834766592 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.0205624103546143, loss=1.4553200006484985
I0307 11:18:14.809546 140296826373888 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.041653633117676, loss=1.4509832859039307
I0307 11:18:53.498499 140296834766592 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.2722878456115723, loss=1.5748201608657837
I0307 11:19:36.404912 140296826373888 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.9574099779129028, loss=1.4229228496551514
I0307 11:19:46.015077 140452179379392 spec.py:321] Evaluating on the training split.
I0307 11:20:00.103909 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 11:20:20.483260 140452179379392 spec.py:349] Evaluating on the test split.
I0307 11:20:22.271553 140452179379392 submission_runner.py:469] Time since start: 33010.56s, 	Step: 78819, 	{'train/accuracy': 0.7372449040412903, 'train/loss': 0.999210774898529, 'validation/accuracy': 0.665120005607605, 'validation/loss': 1.364309310913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5375000238418579, 'test/loss': 2.0795700550079346, 'test/num_examples': 10000, 'score': 30654.587359428406, 'total_duration': 33010.55748438835, 'accumulated_submission_time': 30654.587359428406, 'accumulated_eval_time': 2340.040785551071, 'accumulated_logging_time': 7.776157855987549}
I0307 11:20:22.352216 140296834766592 logging_writer.py:48] [78819] accumulated_eval_time=2340.04, accumulated_logging_time=7.77616, accumulated_submission_time=30654.6, global_step=78819, preemption_count=0, score=30654.6, test/accuracy=0.5375, test/loss=2.07957, test/num_examples=10000, total_duration=33010.6, train/accuracy=0.737245, train/loss=0.999211, validation/accuracy=0.66512, validation/loss=1.36431, validation/num_examples=50000
I0307 11:20:54.381838 140296826373888 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.937888503074646, loss=1.4733242988586426
I0307 11:21:33.077069 140296834766592 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.9971792697906494, loss=1.435281753540039
I0307 11:22:11.969796 140296826373888 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.084073305130005, loss=1.567932367324829
I0307 11:22:50.796341 140296834766592 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.945433259010315, loss=1.4009945392608643
I0307 11:23:29.509945 140296826373888 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.13135027885437, loss=1.4329780340194702
I0307 11:24:08.662625 140296834766592 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.079463481903076, loss=1.4810125827789307
I0307 11:24:47.668248 140296826373888 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.120453119277954, loss=1.3939374685287476
I0307 11:25:26.267571 140296834766592 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.018113374710083, loss=1.5104016065597534
I0307 11:26:05.338169 140296826373888 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.0842528343200684, loss=1.4500457048416138
I0307 11:26:44.097944 140296834766592 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.035832166671753, loss=1.4810220003128052
I0307 11:27:22.712166 140296826373888 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.1036391258239746, loss=1.5350677967071533
I0307 11:28:01.202046 140296834766592 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.0404953956604004, loss=1.5288875102996826
I0307 11:28:40.927227 140296826373888 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.1883656978607178, loss=1.6095027923583984
I0307 11:28:52.296540 140452179379392 spec.py:321] Evaluating on the training split.
I0307 11:29:04.819263 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 11:29:28.294329 140452179379392 spec.py:349] Evaluating on the test split.
I0307 11:29:30.072057 140452179379392 submission_runner.py:469] Time since start: 33558.36s, 	Step: 80130, 	{'train/accuracy': 0.7342753410339355, 'train/loss': 1.0147396326065063, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.38283109664917, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.1202588081359863, 'test/num_examples': 10000, 'score': 31164.34555053711, 'total_duration': 33558.357707977295, 'accumulated_submission_time': 31164.34555053711, 'accumulated_eval_time': 2377.8158416748047, 'accumulated_logging_time': 7.9083147048950195}
I0307 11:29:30.170586 140296834766592 logging_writer.py:48] [80130] accumulated_eval_time=2377.82, accumulated_logging_time=7.90831, accumulated_submission_time=31164.3, global_step=80130, preemption_count=0, score=31164.3, test/accuracy=0.5337, test/loss=2.12026, test/num_examples=10000, total_duration=33558.4, train/accuracy=0.734275, train/loss=1.01474, validation/accuracy=0.66152, validation/loss=1.38283, validation/num_examples=50000
I0307 11:29:57.395913 140296826373888 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.981589436531067, loss=1.454259991645813
I0307 11:30:36.320130 140296834766592 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.0680646896362305, loss=1.538857340812683
I0307 11:31:14.924268 140296826373888 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.3179244995117188, loss=1.4749479293823242
I0307 11:31:53.653689 140296834766592 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.070587635040283, loss=1.4463473558425903
I0307 11:32:32.227461 140296826373888 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.0407474040985107, loss=1.5260283946990967
I0307 11:33:11.031616 140296834766592 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.9372962713241577, loss=1.4767571687698364
I0307 11:33:50.040439 140296826373888 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.114927291870117, loss=1.4208896160125732
I0307 11:34:28.770160 140296834766592 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.1085071563720703, loss=1.3874980211257935
I0307 11:35:07.803822 140296826373888 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.0665462017059326, loss=1.534365177154541
I0307 11:35:47.024547 140296834766592 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1173362731933594, loss=1.5615012645721436
I0307 11:36:25.323713 140296826373888 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.0754239559173584, loss=1.5626280307769775
I0307 11:37:03.948179 140296834766592 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.357149839401245, loss=1.6013708114624023
I0307 11:37:43.234940 140296826373888 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.0549330711364746, loss=1.4068764448165894
I0307 11:38:00.256128 140452179379392 spec.py:321] Evaluating on the training split.
I0307 11:38:13.057400 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 11:38:30.207907 140452179379392 spec.py:349] Evaluating on the test split.
I0307 11:38:31.982372 140452179379392 submission_runner.py:469] Time since start: 34100.27s, 	Step: 81445, 	{'train/accuracy': 0.7383609414100647, 'train/loss': 1.0017309188842773, 'validation/accuracy': 0.6639400124549866, 'validation/loss': 1.3660082817077637, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.099245548248291, 'test/num_examples': 10000, 'score': 31674.256913661957, 'total_duration': 34100.26830077171, 'accumulated_submission_time': 31674.256913661957, 'accumulated_eval_time': 2409.5419042110443, 'accumulated_logging_time': 8.043986797332764}
I0307 11:38:32.141845 140296834766592 logging_writer.py:48] [81445] accumulated_eval_time=2409.54, accumulated_logging_time=8.04399, accumulated_submission_time=31674.3, global_step=81445, preemption_count=0, score=31674.3, test/accuracy=0.5361, test/loss=2.09925, test/num_examples=10000, total_duration=34100.3, train/accuracy=0.738361, train/loss=1.00173, validation/accuracy=0.66394, validation/loss=1.36601, validation/num_examples=50000
I0307 11:38:53.761822 140296826373888 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.1338651180267334, loss=1.5097495317459106
I0307 11:39:32.140874 140296834766592 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.1036481857299805, loss=1.5537254810333252
I0307 11:40:10.932157 140296826373888 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.3661623001098633, loss=1.4722883701324463
I0307 11:40:49.841588 140296834766592 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.0712528228759766, loss=1.392120599746704
I0307 11:41:28.958140 140296826373888 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.083019733428955, loss=1.4593533277511597
I0307 11:42:08.269849 140296834766592 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.1497511863708496, loss=1.4175196886062622
I0307 11:42:46.893363 140296826373888 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.0345752239227295, loss=1.4960778951644897
I0307 11:43:26.029742 140296834766592 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.251833200454712, loss=1.4822957515716553
I0307 11:44:04.746554 140296826373888 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.2223587036132812, loss=1.4138333797454834
I0307 11:44:43.403861 140296834766592 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.1054906845092773, loss=1.506016492843628
I0307 11:45:22.248359 140296826373888 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.1498777866363525, loss=1.5309048891067505
I0307 11:46:01.861706 140296834766592 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.103515148162842, loss=1.531351089477539
I0307 11:46:41.020515 140296826373888 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.248753309249878, loss=1.3990874290466309
I0307 11:47:02.041472 140452179379392 spec.py:321] Evaluating on the training split.
I0307 11:47:14.845821 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 11:47:34.737621 140452179379392 spec.py:349] Evaluating on the test split.
I0307 11:47:36.535617 140452179379392 submission_runner.py:469] Time since start: 34644.82s, 	Step: 82755, 	{'train/accuracy': 0.7411710619926453, 'train/loss': 0.9895067811012268, 'validation/accuracy': 0.6690799593925476, 'validation/loss': 1.3471754789352417, 'validation/num_examples': 50000, 'test/accuracy': 0.538100004196167, 'test/loss': 2.07405424118042, 'test/num_examples': 10000, 'score': 32183.98922228813, 'total_duration': 34644.82155919075, 'accumulated_submission_time': 32183.98922228813, 'accumulated_eval_time': 2444.0358991622925, 'accumulated_logging_time': 8.234777212142944}
I0307 11:47:36.719119 140296834766592 logging_writer.py:48] [82755] accumulated_eval_time=2444.04, accumulated_logging_time=8.23478, accumulated_submission_time=32184, global_step=82755, preemption_count=0, score=32184, test/accuracy=0.5381, test/loss=2.07405, test/num_examples=10000, total_duration=34644.8, train/accuracy=0.741171, train/loss=0.989507, validation/accuracy=0.66908, validation/loss=1.34718, validation/num_examples=50000
I0307 11:47:54.594940 140296826373888 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.9471259117126465, loss=1.506101131439209
I0307 11:48:33.157551 140296834766592 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.1932754516601562, loss=1.4309810400009155
I0307 11:49:11.996391 140296826373888 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.3028128147125244, loss=1.3999793529510498
I0307 11:49:50.758679 140296834766592 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.182406187057495, loss=1.5318254232406616
I0307 11:50:29.585495 140296826373888 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.4338998794555664, loss=1.5319031476974487
I0307 11:51:08.646355 140296834766592 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.938618540763855, loss=1.380218505859375
I0307 11:51:47.925131 140296826373888 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.9565606117248535, loss=1.4390956163406372
I0307 11:52:26.322569 140296834766592 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.9681099653244019, loss=1.5016270875930786
I0307 11:53:04.808892 140296826373888 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.219188928604126, loss=1.5391883850097656
I0307 11:53:44.006430 140296834766592 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.884805679321289, loss=1.4930440187454224
I0307 11:54:22.780025 140296826373888 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.1102893352508545, loss=1.4948601722717285
I0307 11:55:02.607413 140296834766592 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.0729613304138184, loss=1.5454655885696411
I0307 11:55:41.732744 140296826373888 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.0591161251068115, loss=1.4421011209487915
I0307 11:56:06.810311 140452179379392 spec.py:321] Evaluating on the training split.
I0307 11:56:19.583657 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 11:56:37.095420 140452179379392 spec.py:349] Evaluating on the test split.
I0307 11:56:38.898966 140452179379392 submission_runner.py:469] Time since start: 35187.18s, 	Step: 84065, 	{'train/accuracy': 0.7470105290412903, 'train/loss': 0.9718596339225769, 'validation/accuracy': 0.6699599623680115, 'validation/loss': 1.34040105342865, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.07735276222229, 'test/num_examples': 10000, 'score': 32693.873821496964, 'total_duration': 35187.1849155426, 'accumulated_submission_time': 32693.873821496964, 'accumulated_eval_time': 2476.124395132065, 'accumulated_logging_time': 8.484883546829224}
I0307 11:56:39.019772 140296834766592 logging_writer.py:48] [84065] accumulated_eval_time=2476.12, accumulated_logging_time=8.48488, accumulated_submission_time=32693.9, global_step=84065, preemption_count=0, score=32693.9, test/accuracy=0.5387, test/loss=2.07735, test/num_examples=10000, total_duration=35187.2, train/accuracy=0.747011, train/loss=0.97186, validation/accuracy=0.66996, validation/loss=1.3404, validation/num_examples=50000
I0307 11:56:53.177273 140296826373888 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.1622314453125, loss=1.48982834815979
I0307 11:57:32.699519 140296834766592 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.2635316848754883, loss=1.4707729816436768
I0307 11:58:12.016129 140296826373888 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.3588662147521973, loss=1.5608181953430176
I0307 11:58:51.115301 140296834766592 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.160688877105713, loss=1.5199368000030518
I0307 11:59:30.238852 140296826373888 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.2742996215820312, loss=1.478109359741211
I0307 12:00:09.415789 140296834766592 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.70771861076355, loss=1.5218853950500488
I0307 12:00:48.830624 140296826373888 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.007826328277588, loss=1.3969333171844482
I0307 12:01:28.191291 140296834766592 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.1501617431640625, loss=1.4447615146636963
I0307 12:02:07.397003 140296826373888 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.2353098392486572, loss=1.5469989776611328
I0307 12:02:46.596456 140296834766592 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.138197183609009, loss=1.3798584938049316
I0307 12:03:26.217319 140296826373888 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.274671792984009, loss=1.5483068227767944
I0307 12:04:05.805929 140296834766592 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.9407092332839966, loss=1.3391478061676025
I0307 12:04:44.756876 140296826373888 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.1179330348968506, loss=1.5557961463928223
I0307 12:05:09.104790 140452179379392 spec.py:321] Evaluating on the training split.
I0307 12:05:21.807363 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 12:05:45.217751 140452179379392 spec.py:349] Evaluating on the test split.
I0307 12:05:46.994992 140452179379392 submission_runner.py:469] Time since start: 35735.28s, 	Step: 85362, 	{'train/accuracy': 0.7373046875, 'train/loss': 1.0042301416397095, 'validation/accuracy': 0.6636999845504761, 'validation/loss': 1.3743995428085327, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.091883420944214, 'test/num_examples': 10000, 'score': 33203.77445316315, 'total_duration': 35735.28090119362, 'accumulated_submission_time': 33203.77445316315, 'accumulated_eval_time': 2514.014396905899, 'accumulated_logging_time': 8.639681339263916}
I0307 12:05:47.116856 140296834766592 logging_writer.py:48] [85362] accumulated_eval_time=2514.01, accumulated_logging_time=8.63968, accumulated_submission_time=33203.8, global_step=85362, preemption_count=0, score=33203.8, test/accuracy=0.5385, test/loss=2.09188, test/num_examples=10000, total_duration=35735.3, train/accuracy=0.737305, train/loss=1.00423, validation/accuracy=0.6637, validation/loss=1.3744, validation/num_examples=50000
I0307 12:06:02.583207 140296826373888 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.028975248336792, loss=1.3750863075256348
I0307 12:06:42.392426 140296834766592 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.1072945594787598, loss=1.3962271213531494
I0307 12:07:22.319173 140296826373888 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.2726452350616455, loss=1.4053428173065186
I0307 12:08:02.427181 140296834766592 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.2776529788970947, loss=1.5302480459213257
I0307 12:08:41.921083 140296826373888 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.0761377811431885, loss=1.4510585069656372
I0307 12:09:21.141373 140296834766592 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.116586685180664, loss=1.3952748775482178
I0307 12:10:00.773203 140296826373888 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.3775007724761963, loss=1.3874486684799194
I0307 12:10:40.789863 140296834766592 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.0859439373016357, loss=1.4790211915969849
I0307 12:11:21.035841 140296826373888 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.182459831237793, loss=1.465221643447876
I0307 12:12:01.776297 140296834766592 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.238943099975586, loss=1.4175968170166016
I0307 12:12:41.882339 140296826373888 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.1396424770355225, loss=1.3979796171188354
I0307 12:13:21.385969 140296834766592 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.311452865600586, loss=1.5424506664276123
I0307 12:14:01.725045 140296826373888 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.159212589263916, loss=1.4464863538742065
I0307 12:14:17.308449 140452179379392 spec.py:321] Evaluating on the training split.
I0307 12:14:30.427667 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 12:14:49.024764 140452179379392 spec.py:349] Evaluating on the test split.
I0307 12:14:50.799453 140452179379392 submission_runner.py:469] Time since start: 36279.09s, 	Step: 86640, 	{'train/accuracy': 0.7416892647743225, 'train/loss': 0.9759896397590637, 'validation/accuracy': 0.6683399677276611, 'validation/loss': 1.354854702949524, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.0962579250335693, 'test/num_examples': 10000, 'score': 33713.77431464195, 'total_duration': 36279.08540081978, 'accumulated_submission_time': 33713.77431464195, 'accumulated_eval_time': 2547.5052325725555, 'accumulated_logging_time': 8.800147533416748}
I0307 12:14:50.919004 140296834766592 logging_writer.py:48] [86640] accumulated_eval_time=2547.51, accumulated_logging_time=8.80015, accumulated_submission_time=33713.8, global_step=86640, preemption_count=0, score=33713.8, test/accuracy=0.5392, test/loss=2.09626, test/num_examples=10000, total_duration=36279.1, train/accuracy=0.741689, train/loss=0.97599, validation/accuracy=0.66834, validation/loss=1.35485, validation/num_examples=50000
I0307 12:15:15.298826 140296826373888 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.2388551235198975, loss=1.3652595281600952
I0307 12:15:55.705217 140296834766592 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.186544418334961, loss=1.4961786270141602
I0307 12:16:36.262833 140296826373888 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.102065324783325, loss=1.4441993236541748
I0307 12:17:16.900564 140296834766592 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.519343376159668, loss=1.4422938823699951
I0307 12:17:57.420716 140296826373888 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.9706101417541504, loss=1.3363707065582275
I0307 12:18:37.618474 140296834766592 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.2761523723602295, loss=1.5572952032089233
I0307 12:19:17.967480 140296826373888 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.1227197647094727, loss=1.5148429870605469
I0307 12:19:58.446782 140296834766592 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.265099287033081, loss=1.364790439605713
I0307 12:20:38.664353 140296826373888 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.236356019973755, loss=1.4194973707199097
I0307 12:21:18.985627 140296834766592 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.0976171493530273, loss=1.4732779264450073
I0307 12:21:58.581003 140296826373888 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.0813848972320557, loss=1.4653534889221191
I0307 12:22:38.229452 140296834766592 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.08502197265625, loss=1.3915551900863647
I0307 12:23:17.940241 140296826373888 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.196122646331787, loss=1.4303572177886963
I0307 12:23:21.143848 140452179379392 spec.py:321] Evaluating on the training split.
I0307 12:23:34.079901 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 12:23:55.108744 140452179379392 spec.py:349] Evaluating on the test split.
I0307 12:23:56.865656 140452179379392 submission_runner.py:469] Time since start: 36825.15s, 	Step: 87909, 	{'train/accuracy': 0.7467913031578064, 'train/loss': 0.9531819820404053, 'validation/accuracy': 0.6702799797058105, 'validation/loss': 1.3429685831069946, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.094825029373169, 'test/num_examples': 10000, 'score': 34223.77736234665, 'total_duration': 36825.15154671669, 'accumulated_submission_time': 34223.77736234665, 'accumulated_eval_time': 2583.226817846298, 'accumulated_logging_time': 8.98764181137085}
I0307 12:23:56.970367 140296834766592 logging_writer.py:48] [87909] accumulated_eval_time=2583.23, accumulated_logging_time=8.98764, accumulated_submission_time=34223.8, global_step=87909, preemption_count=0, score=34223.8, test/accuracy=0.5407, test/loss=2.09483, test/num_examples=10000, total_duration=36825.2, train/accuracy=0.746791, train/loss=0.953182, validation/accuracy=0.67028, validation/loss=1.34297, validation/num_examples=50000
I0307 12:24:33.259333 140296826373888 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.123699903488159, loss=1.4512962102890015
I0307 12:25:13.557770 140296834766592 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3484668731689453, loss=1.4658747911453247
I0307 12:25:53.662030 140296826373888 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.2728171348571777, loss=1.4989676475524902
I0307 12:26:33.576782 140296834766592 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.0506298542022705, loss=1.425981879234314
I0307 12:27:13.675354 140296826373888 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.009406566619873, loss=1.2764389514923096
I0307 12:27:53.606571 140296834766592 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.1426291465759277, loss=1.510771632194519
I0307 12:28:33.754744 140296826373888 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.105111837387085, loss=1.4961999654769897
I0307 12:29:13.714228 140296834766592 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.273768186569214, loss=1.3680782318115234
I0307 12:29:54.305696 140296826373888 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.933923602104187, loss=1.3648271560668945
I0307 12:30:34.384367 140296834766592 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.119356632232666, loss=1.4270106554031372
I0307 12:31:14.751214 140296826373888 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.1983532905578613, loss=1.423025131225586
I0307 12:31:55.078237 140296834766592 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.081289529800415, loss=1.480363368988037
I0307 12:32:27.000643 140452179379392 spec.py:321] Evaluating on the training split.
I0307 12:32:40.003052 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 12:33:00.380762 140452179379392 spec.py:349] Evaluating on the test split.
I0307 12:33:02.155818 140452179379392 submission_runner.py:469] Time since start: 37370.44s, 	Step: 89181, 	{'train/accuracy': 0.7483657598495483, 'train/loss': 0.9555451273918152, 'validation/accuracy': 0.6715399622917175, 'validation/loss': 1.3497025966644287, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0817558765411377, 'test/num_examples': 10000, 'score': 34733.62873029709, 'total_duration': 37370.441772699356, 'accumulated_submission_time': 34733.62873029709, 'accumulated_eval_time': 2618.3818480968475, 'accumulated_logging_time': 9.115766763687134}
I0307 12:33:02.268412 140296826373888 logging_writer.py:48] [89181] accumulated_eval_time=2618.38, accumulated_logging_time=9.11577, accumulated_submission_time=34733.6, global_step=89181, preemption_count=0, score=34733.6, test/accuracy=0.5436, test/loss=2.08176, test/num_examples=10000, total_duration=37370.4, train/accuracy=0.748366, train/loss=0.955545, validation/accuracy=0.67154, validation/loss=1.3497, validation/num_examples=50000
I0307 12:33:10.258671 140296834766592 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.105975389480591, loss=1.5249955654144287
I0307 12:33:50.761508 140296826373888 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.2313613891601562, loss=1.3487589359283447
I0307 12:34:31.584947 140296834766592 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.126579761505127, loss=1.4498493671417236
I0307 12:35:11.801467 140296826373888 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.0761141777038574, loss=1.3163217306137085
I0307 12:35:51.751748 140296834766592 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.2609171867370605, loss=1.4839346408843994
I0307 12:36:32.961456 140296826373888 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.193234443664551, loss=1.5220156908035278
I0307 12:37:14.361056 140296834766592 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.265331745147705, loss=1.4235520362854004
I0307 12:37:55.509078 140296826373888 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.083650827407837, loss=1.450498342514038
I0307 12:38:35.947693 140296834766592 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.0266966819763184, loss=1.3875364065170288
I0307 12:39:17.526981 140296826373888 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.0672881603240967, loss=1.422299861907959
I0307 12:39:57.555018 140296834766592 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.1796813011169434, loss=1.4851337671279907
I0307 12:40:38.038077 140296826373888 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.1519641876220703, loss=1.5592138767242432
I0307 12:41:18.222772 140296834766592 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.2039542198181152, loss=1.4593931436538696
I0307 12:41:32.211267 140452179379392 spec.py:321] Evaluating on the training split.
I0307 12:41:45.282118 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 12:42:06.549886 140452179379392 spec.py:349] Evaluating on the test split.
I0307 12:42:08.328566 140452179379392 submission_runner.py:469] Time since start: 37916.61s, 	Step: 90436, 	{'train/accuracy': 0.7536471486091614, 'train/loss': 0.9456705451011658, 'validation/accuracy': 0.67603999376297, 'validation/loss': 1.330556869506836, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.0763721466064453, 'test/num_examples': 10000, 'score': 35243.37022304535, 'total_duration': 37916.61448478699, 'accumulated_submission_time': 35243.37022304535, 'accumulated_eval_time': 2654.4989569187164, 'accumulated_logging_time': 9.27614450454712}
I0307 12:42:08.424174 140296826373888 logging_writer.py:48] [90436] accumulated_eval_time=2654.5, accumulated_logging_time=9.27614, accumulated_submission_time=35243.4, global_step=90436, preemption_count=0, score=35243.4, test/accuracy=0.5379, test/loss=2.07637, test/num_examples=10000, total_duration=37916.6, train/accuracy=0.753647, train/loss=0.945671, validation/accuracy=0.67604, validation/loss=1.33056, validation/num_examples=50000
I0307 12:42:34.537058 140296834766592 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.044597864151001, loss=1.4201337099075317
I0307 12:43:14.676731 140296826373888 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.293394088745117, loss=1.3806926012039185
I0307 12:43:56.246674 140296834766592 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.209437370300293, loss=1.4578871726989746
I0307 12:44:35.946133 140296826373888 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.1351494789123535, loss=1.3748844861984253
I0307 12:45:16.020076 140296834766592 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.4434850215911865, loss=1.4591057300567627
I0307 12:45:56.632586 140296826373888 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.3120670318603516, loss=1.337881326675415
I0307 12:46:37.565388 140296834766592 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.294924736022949, loss=1.4432907104492188
I0307 12:47:18.395695 140296826373888 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.3012266159057617, loss=1.4472699165344238
I0307 12:47:59.093817 140296834766592 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.527924060821533, loss=1.5290695428848267
I0307 12:48:38.957753 140296826373888 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.0466434955596924, loss=1.3545117378234863
I0307 12:49:18.938884 140296834766592 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.3620541095733643, loss=1.5375806093215942
I0307 12:49:59.473676 140296826373888 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.2973580360412598, loss=1.3675990104675293
I0307 12:50:38.688611 140452179379392 spec.py:321] Evaluating on the training split.
I0307 12:50:51.416351 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 12:51:09.766021 140452179379392 spec.py:349] Evaluating on the test split.
I0307 12:51:11.540393 140452179379392 submission_runner.py:469] Time since start: 38459.83s, 	Step: 91698, 	{'train/accuracy': 0.7416892647743225, 'train/loss': 0.9743335843086243, 'validation/accuracy': 0.6624199748039246, 'validation/loss': 1.3717319965362549, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.1299993991851807, 'test/num_examples': 10000, 'score': 35753.453018665314, 'total_duration': 38459.82631659508, 'accumulated_submission_time': 35753.453018665314, 'accumulated_eval_time': 2687.350551843643, 'accumulated_logging_time': 9.395976305007935}
I0307 12:51:11.714902 140296834766592 logging_writer.py:48] [91698] accumulated_eval_time=2687.35, accumulated_logging_time=9.39598, accumulated_submission_time=35753.5, global_step=91698, preemption_count=0, score=35753.5, test/accuracy=0.5344, test/loss=2.13, test/num_examples=10000, total_duration=38459.8, train/accuracy=0.741689, train/loss=0.974334, validation/accuracy=0.66242, validation/loss=1.37173, validation/num_examples=50000
I0307 12:51:12.945276 140296826373888 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.24849271774292, loss=1.4021334648132324
I0307 12:51:53.148402 140296834766592 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.2926299571990967, loss=1.4277286529541016
I0307 12:52:33.452414 140296826373888 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.1476950645446777, loss=1.3919397592544556
I0307 12:53:13.852877 140296834766592 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.8263142108917236, loss=1.5057281255722046
I0307 12:53:54.056309 140296826373888 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.2086124420166016, loss=1.4073054790496826
I0307 12:54:34.891170 140296834766592 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.3355534076690674, loss=1.4439884424209595
I0307 12:55:15.347927 140296826373888 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.0397069454193115, loss=1.3139417171478271
I0307 12:55:54.991640 140296834766592 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.4320476055145264, loss=1.4263982772827148
I0307 12:56:34.937294 140296826373888 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.600658416748047, loss=1.4975519180297852
I0307 12:57:14.631170 140296834766592 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.2139101028442383, loss=1.3088680505752563
I0307 12:57:54.937656 140296826373888 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.2851145267486572, loss=1.487276554107666
I0307 12:58:35.622713 140296834766592 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.196101665496826, loss=1.476982593536377
I0307 12:59:16.030799 140296826373888 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.207015037536621, loss=1.3667536973953247
I0307 12:59:41.779753 140452179379392 spec.py:321] Evaluating on the training split.
I0307 12:59:54.618868 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 13:00:11.898905 140452179379392 spec.py:349] Evaluating on the test split.
I0307 13:00:13.686563 140452179379392 submission_runner.py:469] Time since start: 39001.97s, 	Step: 92964, 	{'train/accuracy': 0.7483457922935486, 'train/loss': 0.9551435708999634, 'validation/accuracy': 0.6695799827575684, 'validation/loss': 1.3583085536956787, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.0948450565338135, 'test/num_examples': 10000, 'score': 36263.33692955971, 'total_duration': 39001.97222113609, 'accumulated_submission_time': 36263.33692955971, 'accumulated_eval_time': 2719.2569098472595, 'accumulated_logging_time': 9.601212739944458}
I0307 13:00:13.793642 140296834766592 logging_writer.py:48] [92964] accumulated_eval_time=2719.26, accumulated_logging_time=9.60121, accumulated_submission_time=36263.3, global_step=92964, preemption_count=0, score=36263.3, test/accuracy=0.5403, test/loss=2.09485, test/num_examples=10000, total_duration=39002, train/accuracy=0.748346, train/loss=0.955144, validation/accuracy=0.66958, validation/loss=1.35831, validation/num_examples=50000
I0307 13:00:28.569350 140296826373888 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.124727964401245, loss=1.4498369693756104
I0307 13:01:08.735949 140296834766592 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.184068202972412, loss=1.3510513305664062
I0307 13:01:49.362291 140296826373888 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.3469629287719727, loss=1.4767776727676392
I0307 13:02:30.087030 140296834766592 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.152988910675049, loss=1.4918982982635498
I0307 13:03:10.205255 140296826373888 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.2951948642730713, loss=1.3440862894058228
I0307 13:03:50.530771 140296834766592 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.331178903579712, loss=1.4410371780395508
I0307 13:04:30.699098 140296826373888 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.174610137939453, loss=1.3970625400543213
I0307 13:05:11.211917 140296834766592 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.3734405040740967, loss=1.4924798011779785
I0307 13:05:51.534679 140296826373888 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.307293176651001, loss=1.4952170848846436
I0307 13:06:31.474432 140296834766592 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.1604795455932617, loss=1.3309011459350586
I0307 13:07:11.993554 140296826373888 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.3306829929351807, loss=1.4825007915496826
I0307 13:07:52.453054 140296834766592 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.2502806186676025, loss=1.3759318590164185
I0307 13:08:43.826715 140452179379392 spec.py:321] Evaluating on the training split.
I0307 13:08:58.320722 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 13:09:24.346014 140452179379392 spec.py:349] Evaluating on the test split.
I0307 13:09:26.136261 140452179379392 submission_runner.py:469] Time since start: 39554.42s, 	Step: 94145, 	{'train/accuracy': 0.7598851919174194, 'train/loss': 0.9020687341690063, 'validation/accuracy': 0.6735399961471558, 'validation/loss': 1.328059196472168, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.040255069732666, 'test/num_examples': 10000, 'score': 36773.20000553131, 'total_duration': 39554.42208600044, 'accumulated_submission_time': 36773.20000553131, 'accumulated_eval_time': 2761.566164970398, 'accumulated_logging_time': 9.737160444259644}
I0307 13:09:26.297688 140296826373888 logging_writer.py:48] [94145] accumulated_eval_time=2761.57, accumulated_logging_time=9.73716, accumulated_submission_time=36773.2, global_step=94145, preemption_count=0, score=36773.2, test/accuracy=0.5481, test/loss=2.04026, test/num_examples=10000, total_duration=39554.4, train/accuracy=0.759885, train/loss=0.902069, validation/accuracy=0.67354, validation/loss=1.32806, validation/num_examples=50000
I0307 13:09:49.110921 140296834766592 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.2093377113342285, loss=1.4247655868530273
I0307 13:10:29.024755 140296826373888 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.2910428047180176, loss=1.5390512943267822
I0307 13:11:09.395437 140296834766592 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.409510374069214, loss=1.385605812072754
I0307 13:11:49.840657 140296826373888 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.482541561126709, loss=1.380264401435852
I0307 13:12:29.924835 140296834766592 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.224186897277832, loss=1.3978697061538696
I0307 13:13:09.920386 140296826373888 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.393397092819214, loss=1.4458723068237305
I0307 13:13:50.624547 140296834766592 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.329460620880127, loss=1.5124090909957886
I0307 13:14:30.681685 140296826373888 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.2285192012786865, loss=1.2985448837280273
I0307 13:15:10.456507 140296834766592 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.322385787963867, loss=1.4663691520690918
2025-03-07 13:15:34.668165: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:15:50.913752 140296826373888 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.1529245376586914, loss=1.3124386072158813
I0307 13:16:31.380820 140296834766592 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.40138578414917, loss=1.4258708953857422
I0307 13:17:11.224978 140296826373888 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.379662036895752, loss=1.444960594177246
I0307 13:17:51.634063 140296834766592 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.211179256439209, loss=1.3870525360107422
I0307 13:17:56.483759 140452179379392 spec.py:321] Evaluating on the training split.
I0307 13:18:09.332231 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 13:18:28.263839 140452179379392 spec.py:349] Evaluating on the test split.
I0307 13:18:30.038775 140452179379392 submission_runner.py:469] Time since start: 40098.32s, 	Step: 95413, 	{'train/accuracy': 0.7520129084587097, 'train/loss': 0.9332864284515381, 'validation/accuracy': 0.6658799648284912, 'validation/loss': 1.3660082817077637, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.095616340637207, 'test/num_examples': 10000, 'score': 37283.14886236191, 'total_duration': 40098.32472419739, 'accumulated_submission_time': 37283.14886236191, 'accumulated_eval_time': 2795.121016025543, 'accumulated_logging_time': 9.98220181465149}
I0307 13:18:30.154284 140296826373888 logging_writer.py:48] [95413] accumulated_eval_time=2795.12, accumulated_logging_time=9.9822, accumulated_submission_time=37283.1, global_step=95413, preemption_count=0, score=37283.1, test/accuracy=0.537, test/loss=2.09562, test/num_examples=10000, total_duration=40098.3, train/accuracy=0.752013, train/loss=0.933286, validation/accuracy=0.66588, validation/loss=1.36601, validation/num_examples=50000
I0307 13:19:05.207113 140296834766592 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.1927194595336914, loss=1.3636054992675781
I0307 13:19:45.401946 140296826373888 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.2318572998046875, loss=1.3292845487594604
I0307 13:20:25.727435 140296834766592 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.2398266792297363, loss=1.3759299516677856
I0307 13:21:05.365251 140296826373888 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.1527352333068848, loss=1.2805191278457642
I0307 13:21:45.497603 140296834766592 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.335155487060547, loss=1.3141863346099854
I0307 13:22:25.624446 140296826373888 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.5921289920806885, loss=1.4348087310791016
I0307 13:23:05.689862 140296834766592 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.2526469230651855, loss=1.45063316822052
I0307 13:23:46.021295 140296826373888 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.303769588470459, loss=1.3446314334869385
I0307 13:24:25.755096 140296834766592 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.356666326522827, loss=1.5075433254241943
I0307 13:25:05.473905 140296826373888 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.3015897274017334, loss=1.3098113536834717
I0307 13:25:45.581154 140296834766592 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.4003546237945557, loss=1.3333892822265625
I0307 13:26:25.778776 140296826373888 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.4101617336273193, loss=1.3957685232162476
I0307 13:27:00.358353 140452179379392 spec.py:321] Evaluating on the training split.
I0307 13:27:13.011440 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 13:27:30.428527 140452179379392 spec.py:349] Evaluating on the test split.
I0307 13:27:32.458213 140452179379392 submission_runner.py:469] Time since start: 40640.74s, 	Step: 96688, 	{'train/accuracy': 0.7703084945678711, 'train/loss': 0.8618707060813904, 'validation/accuracy': 0.6775599718093872, 'validation/loss': 1.3039822578430176, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.0245487689971924, 'test/num_examples': 10000, 'score': 37793.17760848999, 'total_duration': 40640.74415779114, 'accumulated_submission_time': 37793.17760848999, 'accumulated_eval_time': 2827.2207324504852, 'accumulated_logging_time': 10.1199791431427}
I0307 13:27:32.593186 140296834766592 logging_writer.py:48] [96688] accumulated_eval_time=2827.22, accumulated_logging_time=10.12, accumulated_submission_time=37793.2, global_step=96688, preemption_count=0, score=37793.2, test/accuracy=0.5556, test/loss=2.02455, test/num_examples=10000, total_duration=40640.7, train/accuracy=0.770308, train/loss=0.861871, validation/accuracy=0.67756, validation/loss=1.30398, validation/num_examples=50000
I0307 13:27:37.839143 140296826373888 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.375607967376709, loss=1.4716814756393433
I0307 13:28:16.819060 140296834766592 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.2153573036193848, loss=1.3434628248214722
I0307 13:29:02.740820 140296826373888 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.429550886154175, loss=1.324142336845398
I0307 13:29:50.084555 140296834766592 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.317784547805786, loss=1.4389277696609497
I0307 13:30:28.578515 140296826373888 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.2123751640319824, loss=1.4040720462799072
I0307 13:31:06.737949 140296834766592 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.2877423763275146, loss=1.4069358110427856
I0307 13:31:46.755980 140296826373888 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.2906649112701416, loss=1.3740298748016357
I0307 13:32:27.196856 140296834766592 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.319284200668335, loss=1.3810735940933228
I0307 13:33:07.569953 140296826373888 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.5803287029266357, loss=1.4842396974563599
I0307 13:33:47.264523 140296834766592 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.3585917949676514, loss=1.371814489364624
I0307 13:34:27.317344 140296826373888 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.2742116451263428, loss=1.3757528066635132
I0307 13:35:07.445939 140296834766592 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.1700243949890137, loss=1.3834515810012817
I0307 13:35:47.759953 140296826373888 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.453582763671875, loss=1.437225103378296
I0307 13:36:02.776734 140452179379392 spec.py:321] Evaluating on the training split.
I0307 13:36:15.566920 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 13:36:37.938802 140452179379392 spec.py:349] Evaluating on the test split.
I0307 13:36:39.736405 140452179379392 submission_runner.py:469] Time since start: 41188.02s, 	Step: 97939, 	{'train/accuracy': 0.7665815949440002, 'train/loss': 0.8650805950164795, 'validation/accuracy': 0.677619993686676, 'validation/loss': 1.3222205638885498, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 2.0391316413879395, 'test/num_examples': 10000, 'score': 38303.16890287399, 'total_duration': 41188.022304058075, 'accumulated_submission_time': 38303.16890287399, 'accumulated_eval_time': 2864.180194377899, 'accumulated_logging_time': 10.298583507537842}
I0307 13:36:39.881134 140296834766592 logging_writer.py:48] [97939] accumulated_eval_time=2864.18, accumulated_logging_time=10.2986, accumulated_submission_time=38303.2, global_step=97939, preemption_count=0, score=38303.2, test/accuracy=0.5505, test/loss=2.03913, test/num_examples=10000, total_duration=41188, train/accuracy=0.766582, train/loss=0.865081, validation/accuracy=0.67762, validation/loss=1.32222, validation/num_examples=50000
I0307 13:37:04.361457 140296826373888 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.251455783843994, loss=1.426369309425354
I0307 13:37:44.547097 140296834766592 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.4431583881378174, loss=1.4601296186447144
I0307 13:38:24.873085 140296826373888 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.4191462993621826, loss=1.460516333580017
I0307 13:39:04.660130 140296834766592 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.464846611022949, loss=1.3324165344238281
I0307 13:39:45.049806 140296826373888 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.4432435035705566, loss=1.3365952968597412
I0307 13:40:25.313151 140296834766592 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.3082456588745117, loss=1.3053696155548096
I0307 13:41:05.748221 140296826373888 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.4219930171966553, loss=1.408597469329834
I0307 13:41:45.907542 140296834766592 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.4350838661193848, loss=1.3377765417099
I0307 13:42:26.214586 140296826373888 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.4704506397247314, loss=1.3862851858139038
2025-03-07 13:42:31.672308: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:43:06.699820 140296834766592 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.4424667358398438, loss=1.4583055973052979
I0307 13:43:46.925330 140296826373888 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.1970150470733643, loss=1.3131178617477417
I0307 13:44:26.874709 140296834766592 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.372509479522705, loss=1.3429653644561768
I0307 13:45:06.856444 140296826373888 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.4888012409210205, loss=1.3900123834609985
I0307 13:45:09.962065 140452179379392 spec.py:321] Evaluating on the training split.
I0307 13:45:22.904076 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 13:45:43.260642 140452179379392 spec.py:349] Evaluating on the test split.
I0307 13:45:45.028659 140452179379392 submission_runner.py:469] Time since start: 41733.31s, 	Step: 99209, 	{'train/accuracy': 0.7712053656578064, 'train/loss': 0.8447064757347107, 'validation/accuracy': 0.6776999831199646, 'validation/loss': 1.3116381168365479, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.031226873397827, 'test/num_examples': 10000, 'score': 38813.06130695343, 'total_duration': 41733.31459093094, 'accumulated_submission_time': 38813.06130695343, 'accumulated_eval_time': 2899.246608734131, 'accumulated_logging_time': 10.478414297103882}
I0307 13:45:45.139027 140296834766592 logging_writer.py:48] [99209] accumulated_eval_time=2899.25, accumulated_logging_time=10.4784, accumulated_submission_time=38813.1, global_step=99209, preemption_count=0, score=38813.1, test/accuracy=0.5517, test/loss=2.03123, test/num_examples=10000, total_duration=41733.3, train/accuracy=0.771205, train/loss=0.844706, validation/accuracy=0.6777, validation/loss=1.31164, validation/num_examples=50000
I0307 13:46:21.838765 140296826373888 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.4971842765808105, loss=1.4039039611816406
I0307 13:47:01.905688 140296834766592 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.3884496688842773, loss=1.3488885164260864
I0307 13:47:41.683246 140296826373888 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.4375159740448, loss=1.4356383085250854
I0307 13:48:21.123684 140296834766592 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.373063564300537, loss=1.4125137329101562
I0307 13:49:00.753861 140296826373888 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.5001354217529297, loss=1.3517217636108398
I0307 13:49:40.999840 140296834766592 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.533461332321167, loss=1.345036268234253
I0307 13:50:21.680485 140296826373888 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.2244763374328613, loss=1.3525766134262085
I0307 13:51:01.763283 140296834766592 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.1724016666412354, loss=1.355696678161621
I0307 13:51:41.872158 140296826373888 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.273855209350586, loss=1.304416537284851
I0307 13:52:22.336675 140296834766592 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.5008106231689453, loss=1.3069905042648315
I0307 13:53:02.647380 140296826373888 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.698258638381958, loss=1.389053463935852
I0307 13:53:43.141846 140296834766592 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.4581241607666016, loss=1.400609016418457
I0307 13:54:15.265323 140452179379392 spec.py:321] Evaluating on the training split.
I0307 13:54:28.108751 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 13:54:46.910580 140452179379392 spec.py:349] Evaluating on the test split.
I0307 13:54:48.681841 140452179379392 submission_runner.py:469] Time since start: 42276.97s, 	Step: 100480, 	{'train/accuracy': 0.7671595811843872, 'train/loss': 0.8704963326454163, 'validation/accuracy': 0.6771599650382996, 'validation/loss': 1.308430552482605, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.012143135070801, 'test/num_examples': 10000, 'score': 39323.01236820221, 'total_duration': 42276.967777729034, 'accumulated_submission_time': 39323.01236820221, 'accumulated_eval_time': 2932.6629610061646, 'accumulated_logging_time': 10.611980676651001}
I0307 13:54:48.783874 140296826373888 logging_writer.py:48] [100480] accumulated_eval_time=2932.66, accumulated_logging_time=10.612, accumulated_submission_time=39323, global_step=100480, preemption_count=0, score=39323, test/accuracy=0.5556, test/loss=2.01214, test/num_examples=10000, total_duration=42277, train/accuracy=0.76716, train/loss=0.870496, validation/accuracy=0.67716, validation/loss=1.30843, validation/num_examples=50000
I0307 13:54:57.206374 140296834766592 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.1752939224243164, loss=1.4235057830810547
I0307 13:55:37.279777 140296826373888 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.1662235260009766, loss=1.2141287326812744
I0307 13:56:17.770747 140296834766592 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.36328387260437, loss=1.394701600074768
I0307 13:56:57.995159 140296826373888 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.1915853023529053, loss=1.26934814453125
I0307 13:57:38.332201 140296834766592 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.583223342895508, loss=1.4246070384979248
I0307 13:58:18.479775 140296826373888 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.333871603012085, loss=1.2475615739822388
I0307 13:58:58.637943 140296834766592 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.357563018798828, loss=1.3701865673065186
I0307 13:59:39.560480 140296826373888 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.435595989227295, loss=1.3206322193145752
I0307 14:00:19.977638 140296834766592 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.374013900756836, loss=1.367517113685608
I0307 14:00:58.895100 140296826373888 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.454676628112793, loss=1.3517515659332275
I0307 14:01:38.790572 140296834766592 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.430302381515503, loss=1.4134434461593628
I0307 14:02:19.146341 140296826373888 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.3523809909820557, loss=1.3896653652191162
I0307 14:02:59.487217 140296834766592 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.4227235317230225, loss=1.390169620513916
I0307 14:03:18.940553 140452179379392 spec.py:321] Evaluating on the training split.
I0307 14:03:31.652047 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 14:03:50.261420 140452179379392 spec.py:349] Evaluating on the test split.
I0307 14:03:52.029196 140452179379392 submission_runner.py:469] Time since start: 42820.32s, 	Step: 101749, 	{'train/accuracy': 0.7831632494926453, 'train/loss': 0.800122082233429, 'validation/accuracy': 0.6864199638366699, 'validation/loss': 1.2833116054534912, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 2.0023274421691895, 'test/num_examples': 10000, 'score': 39832.95672917366, 'total_duration': 42820.31513977051, 'accumulated_submission_time': 39832.95672917366, 'accumulated_eval_time': 2965.7514362335205, 'accumulated_logging_time': 10.774179697036743}
I0307 14:03:52.170548 140296826373888 logging_writer.py:48] [101749] accumulated_eval_time=2965.75, accumulated_logging_time=10.7742, accumulated_submission_time=39833, global_step=101749, preemption_count=0, score=39833, test/accuracy=0.5622, test/loss=2.00233, test/num_examples=10000, total_duration=42820.3, train/accuracy=0.783163, train/loss=0.800122, validation/accuracy=0.68642, validation/loss=1.28331, validation/num_examples=50000
I0307 14:04:13.224930 140296834766592 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.2400896549224854, loss=1.3247647285461426
I0307 14:04:53.605980 140296826373888 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.4198269844055176, loss=1.3030750751495361
I0307 14:05:34.239358 140296834766592 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.3484814167022705, loss=1.3738080263137817
I0307 14:06:14.499176 140296826373888 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.436488628387451, loss=1.387547492980957
I0307 14:06:55.470131 140296834766592 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.4553585052490234, loss=1.3828128576278687
I0307 14:07:33.780669 140296826373888 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.507333993911743, loss=1.3412344455718994
I0307 14:08:14.290834 140296834766592 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.2861340045928955, loss=1.3708347082138062
I0307 14:08:55.710995 140296826373888 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.4703917503356934, loss=1.3644368648529053
2025-03-07 14:09:27.860971: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:09:37.641129 140296834766592 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.724820613861084, loss=1.3860461711883545
I0307 14:10:16.952358 140296826373888 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.4020822048187256, loss=1.3365225791931152
I0307 14:10:57.222267 140296834766592 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.483055591583252, loss=1.3731200695037842
I0307 14:11:37.702789 140296826373888 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.394923448562622, loss=1.3202321529388428
I0307 14:12:17.765289 140296834766592 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.396909713745117, loss=1.3535069227218628
I0307 14:12:22.149217 140452179379392 spec.py:321] Evaluating on the training split.
I0307 14:12:34.980576 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 14:12:55.531421 140452179379392 spec.py:349] Evaluating on the test split.
I0307 14:12:57.266724 140452179379392 submission_runner.py:469] Time since start: 43365.55s, 	Step: 103012, 	{'train/accuracy': 0.7931082248687744, 'train/loss': 0.7687116861343384, 'validation/accuracy': 0.6898199915885925, 'validation/loss': 1.250319480895996, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9568206071853638, 'test/num_examples': 10000, 'score': 40342.750413656235, 'total_duration': 43365.55261850357, 'accumulated_submission_time': 40342.750413656235, 'accumulated_eval_time': 3000.8687262535095, 'accumulated_logging_time': 10.950862169265747}
I0307 14:12:57.396201 140296826373888 logging_writer.py:48] [103012] accumulated_eval_time=3000.87, accumulated_logging_time=10.9509, accumulated_submission_time=40342.8, global_step=103012, preemption_count=0, score=40342.8, test/accuracy=0.5697, test/loss=1.95682, test/num_examples=10000, total_duration=43365.6, train/accuracy=0.793108, train/loss=0.768712, validation/accuracy=0.68982, validation/loss=1.25032, validation/num_examples=50000
I0307 14:13:33.860483 140296834766592 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.4096879959106445, loss=1.3829690217971802
I0307 14:14:20.300706 140296826373888 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.4833896160125732, loss=1.4747868776321411
I0307 14:15:04.907155 140296834766592 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.412367582321167, loss=1.4450795650482178
I0307 14:15:45.878381 140296826373888 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.337233781814575, loss=1.304909348487854
I0307 14:16:25.489508 140296834766592 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.4761641025543213, loss=1.334312915802002
I0307 14:17:05.830834 140296826373888 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.560346841812134, loss=1.3876962661743164
I0307 14:17:46.574807 140296834766592 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.501688003540039, loss=1.3312599658966064
I0307 14:18:27.236642 140296826373888 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.247798204421997, loss=1.3558961153030396
I0307 14:19:06.337491 140296834766592 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.6892547607421875, loss=1.3714250326156616
I0307 14:19:45.318435 140296826373888 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.9296483993530273, loss=1.4252464771270752
I0307 14:20:25.920105 140296834766592 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.4784934520721436, loss=1.3813475370407104
I0307 14:21:06.086007 140296826373888 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.7916038036346436, loss=1.4294087886810303
I0307 14:21:27.692081 140452179379392 spec.py:321] Evaluating on the training split.
I0307 14:21:40.563398 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 14:22:04.305246 140452179379392 spec.py:349] Evaluating on the test split.
I0307 14:22:06.065106 140452179379392 submission_runner.py:469] Time since start: 43914.35s, 	Step: 104255, 	{'train/accuracy': 0.7841796875, 'train/loss': 0.788958728313446, 'validation/accuracy': 0.681879997253418, 'validation/loss': 1.297695517539978, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.9892799854278564, 'test/num_examples': 10000, 'score': 40852.842639923096, 'total_duration': 43914.35103082657, 'accumulated_submission_time': 40852.842639923096, 'accumulated_eval_time': 3039.2415664196014, 'accumulated_logging_time': 11.13432502746582}
I0307 14:22:06.179962 140296834766592 logging_writer.py:48] [104255] accumulated_eval_time=3039.24, accumulated_logging_time=11.1343, accumulated_submission_time=40852.8, global_step=104255, preemption_count=0, score=40852.8, test/accuracy=0.5584, test/loss=1.98928, test/num_examples=10000, total_duration=43914.4, train/accuracy=0.78418, train/loss=0.788959, validation/accuracy=0.68188, validation/loss=1.2977, validation/num_examples=50000
I0307 14:22:24.636859 140296826373888 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.3053882122039795, loss=1.3252999782562256
I0307 14:23:04.960780 140296834766592 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.5236732959747314, loss=1.3175952434539795
I0307 14:23:45.454995 140296826373888 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.345837354660034, loss=1.4002901315689087
I0307 14:24:29.807368 140296834766592 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.6324946880340576, loss=1.4322969913482666
I0307 14:25:09.784677 140296826373888 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.5781409740448, loss=1.3526790142059326
I0307 14:25:50.111006 140296834766592 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.395369291305542, loss=1.3032245635986328
I0307 14:26:30.512120 140296826373888 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.4003992080688477, loss=1.385606050491333
I0307 14:27:10.878174 140296834766592 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.6622202396392822, loss=1.4310410022735596
2025-03-07 14:27:38.643938: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:27:51.296441 140296826373888 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.1977579593658447, loss=1.2263857126235962
I0307 14:28:31.606272 140296834766592 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.56520676612854, loss=1.2978986501693726
I0307 14:29:11.960360 140296826373888 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.552537202835083, loss=1.370589256286621
I0307 14:29:54.777261 140296834766592 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.5302820205688477, loss=1.441412329673767
I0307 14:30:36.181646 140452179379392 spec.py:321] Evaluating on the training split.
I0307 14:30:49.873611 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 14:31:09.013674 140452179379392 spec.py:349] Evaluating on the test split.
I0307 14:31:10.768406 140452179379392 submission_runner.py:469] Time since start: 44459.05s, 	Step: 105497, 	{'train/accuracy': 0.7898397445678711, 'train/loss': 0.7826870083808899, 'validation/accuracy': 0.6844399571418762, 'validation/loss': 1.2793796062469482, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 2.001741409301758, 'test/num_examples': 10000, 'score': 41362.66264605522, 'total_duration': 44459.05434703827, 'accumulated_submission_time': 41362.66264605522, 'accumulated_eval_time': 3073.828160047531, 'accumulated_logging_time': 11.28305435180664}
I0307 14:31:10.867653 140296826373888 logging_writer.py:48] [105497] accumulated_eval_time=3073.83, accumulated_logging_time=11.2831, accumulated_submission_time=41362.7, global_step=105497, preemption_count=0, score=41362.7, test/accuracy=0.5552, test/loss=2.00174, test/num_examples=10000, total_duration=44459.1, train/accuracy=0.78984, train/loss=0.782687, validation/accuracy=0.68444, validation/loss=1.27938, validation/num_examples=50000
I0307 14:31:12.369502 140296834766592 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.440053939819336, loss=1.3658682107925415
I0307 14:31:52.643805 140296826373888 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.4230618476867676, loss=1.4110512733459473
I0307 14:32:33.068518 140296834766592 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.449876546859741, loss=1.2963639497756958
I0307 14:33:13.268416 140296826373888 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.7718048095703125, loss=1.345933198928833
I0307 14:33:53.774311 140296834766592 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.452782154083252, loss=1.2851909399032593
I0307 14:34:33.592057 140296826373888 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.6588385105133057, loss=1.5880777835845947
I0307 14:35:13.795707 140296834766592 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.486004590988159, loss=1.2557345628738403
I0307 14:35:54.302911 140296826373888 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.4895713329315186, loss=1.397765874862671
I0307 14:36:39.155365 140296834766592 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.4732584953308105, loss=1.2772005796432495
I0307 14:37:25.970114 140296826373888 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.6803572177886963, loss=1.3782767057418823
I0307 14:38:06.215444 140296834766592 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.425865411758423, loss=1.3531467914581299
I0307 14:38:47.037332 140296826373888 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.4174282550811768, loss=1.3057103157043457
I0307 14:39:27.150339 140296834766592 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.358940601348877, loss=1.3759135007858276
I0307 14:39:40.936209 140452179379392 spec.py:321] Evaluating on the training split.
I0307 14:39:54.823755 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 14:40:13.224932 140452179379392 spec.py:349] Evaluating on the test split.
I0307 14:40:14.997437 140452179379392 submission_runner.py:469] Time since start: 45003.28s, 	Step: 106733, 	{'train/accuracy': 0.8078762292861938, 'train/loss': 0.7024427056312561, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.26376473903656, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.0244131088256836, 'test/num_examples': 10000, 'score': 41872.55877280235, 'total_duration': 45003.28336286545, 'accumulated_submission_time': 41872.55877280235, 'accumulated_eval_time': 3107.889204263687, 'accumulated_logging_time': 11.410769701004028}
I0307 14:40:15.126555 140296826373888 logging_writer.py:48] [106733] accumulated_eval_time=3107.89, accumulated_logging_time=11.4108, accumulated_submission_time=41872.6, global_step=106733, preemption_count=0, score=41872.6, test/accuracy=0.5584, test/loss=2.02441, test/num_examples=10000, total_duration=45003.3, train/accuracy=0.807876, train/loss=0.702443, validation/accuracy=0.68912, validation/loss=1.26376, validation/num_examples=50000
I0307 14:40:42.115651 140296834766592 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.367227792739868, loss=1.3330018520355225
I0307 14:41:21.405971 140296826373888 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.5493924617767334, loss=1.387878656387329
I0307 14:42:03.439340 140296834766592 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.6867854595184326, loss=1.3404228687286377
I0307 14:42:43.441089 140296826373888 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.5652482509613037, loss=1.4909168481826782
I0307 14:43:23.615121 140296834766592 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.5568485260009766, loss=1.4856784343719482
I0307 14:44:04.130646 140296826373888 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.744166851043701, loss=1.2858128547668457
I0307 14:44:44.436538 140296834766592 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.628464460372925, loss=1.415074348449707
I0307 14:45:26.416047 140296826373888 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.583681344985962, loss=1.2671270370483398
I0307 14:46:09.016478 140296834766592 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.630640745162964, loss=1.288068413734436
I0307 14:46:48.494317 140296826373888 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.2354602813720703, loss=1.1788123846054077
I0307 14:47:29.231362 140296834766592 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.3649742603302, loss=1.2928715944290161
I0307 14:48:09.969346 140296826373888 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.5230002403259277, loss=1.3718599081039429
I0307 14:48:45.026327 140452179379392 spec.py:321] Evaluating on the training split.
I0307 14:48:57.754006 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 14:49:19.890994 140452179379392 spec.py:349] Evaluating on the test split.
I0307 14:49:21.684692 140452179379392 submission_runner.py:469] Time since start: 45549.97s, 	Step: 107988, 	{'train/accuracy': 0.8194156289100647, 'train/loss': 0.6624346375465393, 'validation/accuracy': 0.6899399757385254, 'validation/loss': 1.2506000995635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9675315618515015, 'test/num_examples': 10000, 'score': 42382.25957608223, 'total_duration': 45549.97060775757, 'accumulated_submission_time': 42382.25957608223, 'accumulated_eval_time': 3144.547374486923, 'accumulated_logging_time': 11.59263825416565}
I0307 14:49:21.770458 140296834766592 logging_writer.py:48] [107988] accumulated_eval_time=3144.55, accumulated_logging_time=11.5926, accumulated_submission_time=42382.3, global_step=107988, preemption_count=0, score=42382.3, test/accuracy=0.5636, test/loss=1.96753, test/num_examples=10000, total_duration=45550, train/accuracy=0.819416, train/loss=0.662435, validation/accuracy=0.68994, validation/loss=1.2506, validation/num_examples=50000
I0307 14:49:27.097981 140296826373888 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.519805908203125, loss=1.3081238269805908
I0307 14:50:09.001413 140296834766592 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.5222249031066895, loss=1.4590072631835938
I0307 14:50:53.823598 140296826373888 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.5538790225982666, loss=1.3577463626861572
I0307 14:51:38.638085 140296834766592 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.6763954162597656, loss=1.3065434694290161
I0307 14:52:22.824984 140296826373888 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.5385282039642334, loss=1.3143590688705444
I0307 14:53:05.201371 140296834766592 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.589806079864502, loss=1.3067419528961182
I0307 14:53:45.914921 140296826373888 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.429839611053467, loss=1.3643853664398193
I0307 14:54:32.289025 140296834766592 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.7051875591278076, loss=1.386986255645752
I0307 14:55:16.396487 140296826373888 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.504662036895752, loss=1.3239794969558716
2025-03-07 14:55:30.394756: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:55:57.113467 140296834766592 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.5518884658813477, loss=1.4701296091079712
I0307 14:56:36.103831 140296826373888 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.522665023803711, loss=1.3723312616348267
I0307 14:57:16.813760 140296834766592 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.521604299545288, loss=1.3448281288146973
I0307 14:57:52.045900 140452179379392 spec.py:321] Evaluating on the training split.
I0307 14:58:05.043257 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 14:58:25.100782 140452179379392 spec.py:349] Evaluating on the test split.
I0307 14:58:26.910531 140452179379392 submission_runner.py:469] Time since start: 46095.20s, 	Step: 109187, 	{'train/accuracy': 0.7759087681770325, 'train/loss': 0.8255064487457275, 'validation/accuracy': 0.6902399659156799, 'validation/loss': 1.2492961883544922, 'validation/num_examples': 50000, 'test/accuracy': 0.5674000382423401, 'test/loss': 1.953436017036438, 'test/num_examples': 10000, 'score': 42892.34353494644, 'total_duration': 46095.19646310806, 'accumulated_submission_time': 42892.34353494644, 'accumulated_eval_time': 3179.4118280410767, 'accumulated_logging_time': 11.729992866516113}
I0307 14:58:27.025124 140296826373888 logging_writer.py:48] [109187] accumulated_eval_time=3179.41, accumulated_logging_time=11.73, accumulated_submission_time=42892.3, global_step=109187, preemption_count=0, score=42892.3, test/accuracy=0.5674, test/loss=1.95344, test/num_examples=10000, total_duration=46095.2, train/accuracy=0.775909, train/loss=0.825506, validation/accuracy=0.69024, validation/loss=1.2493, validation/num_examples=50000
I0307 14:58:32.590014 140296834766592 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.600945472717285, loss=1.341230869293213
I0307 14:59:18.184171 140296826373888 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.535939931869507, loss=1.3529337644577026
I0307 15:00:06.109833 140296834766592 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.43845272064209, loss=1.248603105545044
I0307 15:00:45.445235 140296826373888 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.5601232051849365, loss=1.1697096824645996
I0307 15:01:23.922515 140296834766592 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.9513957500457764, loss=1.4545832872390747
I0307 15:02:02.674977 140296826373888 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.9487390518188477, loss=1.4112945795059204
I0307 15:02:43.009016 140296834766592 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.6253528594970703, loss=1.4298940896987915
I0307 15:03:23.759009 140296826373888 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.383244752883911, loss=1.1898754835128784
I0307 15:04:04.052635 140296834766592 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.4835596084594727, loss=1.282976746559143
I0307 15:04:43.936380 140296826373888 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.5485239028930664, loss=1.2610529661178589
I0307 15:05:24.188318 140296834766592 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.695713996887207, loss=1.2544529438018799
I0307 15:06:05.130718 140296826373888 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.6198861598968506, loss=1.4021927118301392
I0307 15:06:45.502900 140296834766592 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.7045557498931885, loss=1.2680212259292603
I0307 15:06:57.021681 140452179379392 spec.py:321] Evaluating on the training split.
I0307 15:07:09.751451 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 15:07:32.841576 140452179379392 spec.py:349] Evaluating on the test split.
I0307 15:07:34.613941 140452179379392 submission_runner.py:469] Time since start: 46642.90s, 	Step: 110430, 	{'train/accuracy': 0.7747528553009033, 'train/loss': 0.8336385488510132, 'validation/accuracy': 0.693839967250824, 'validation/loss': 1.2572914361953735, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.9934237003326416, 'test/num_examples': 10000, 'score': 43402.18071317673, 'total_duration': 46642.89986586571, 'accumulated_submission_time': 43402.18071317673, 'accumulated_eval_time': 3217.00390458107, 'accumulated_logging_time': 11.858088970184326}
I0307 15:07:34.695408 140296826373888 logging_writer.py:48] [110430] accumulated_eval_time=3217, accumulated_logging_time=11.8581, accumulated_submission_time=43402.2, global_step=110430, preemption_count=0, score=43402.2, test/accuracy=0.5634, test/loss=1.99342, test/num_examples=10000, total_duration=46642.9, train/accuracy=0.774753, train/loss=0.833639, validation/accuracy=0.69384, validation/loss=1.25729, validation/num_examples=50000
I0307 15:08:14.910177 140296834766592 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.714622974395752, loss=1.3073891401290894
I0307 15:08:55.176637 140296826373888 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.835259199142456, loss=1.2811765670776367
I0307 15:09:37.403967 140296834766592 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.4886181354522705, loss=1.2182320356369019
I0307 15:10:21.174094 140296826373888 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.6715426445007324, loss=1.3880727291107178
I0307 15:11:05.457626 140296834766592 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.9065959453582764, loss=1.4187414646148682
I0307 15:11:47.207262 140296826373888 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.6638343334198, loss=1.3983936309814453
I0307 15:12:41.647149 140296834766592 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.4638447761535645, loss=1.3362149000167847
I0307 15:13:31.951837 140296826373888 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.578052520751953, loss=1.182127594947815
I0307 15:14:19.076117 140296834766592 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.7218503952026367, loss=1.27322518825531
I0307 15:15:01.717688 140296826373888 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.3894803524017334, loss=1.3050892353057861
I0307 15:15:41.602945 140296834766592 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.4970695972442627, loss=1.28378164768219
I0307 15:16:04.652964 140452179379392 spec.py:321] Evaluating on the training split.
I0307 15:16:17.497716 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 15:16:37.191623 140452179379392 spec.py:349] Evaluating on the test split.
I0307 15:16:38.988997 140452179379392 submission_runner.py:469] Time since start: 47187.27s, 	Step: 111558, 	{'train/accuracy': 0.775809109210968, 'train/loss': 0.8351102471351624, 'validation/accuracy': 0.6905800104141235, 'validation/loss': 1.2510921955108643, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.984626054763794, 'test/num_examples': 10000, 'score': 43911.94645190239, 'total_duration': 47187.27496790886, 'accumulated_submission_time': 43911.94645190239, 'accumulated_eval_time': 3251.3397986888885, 'accumulated_logging_time': 12.001140356063843}
I0307 15:16:39.140015 140296826373888 logging_writer.py:48] [111558] accumulated_eval_time=3251.34, accumulated_logging_time=12.0011, accumulated_submission_time=43911.9, global_step=111558, preemption_count=0, score=43911.9, test/accuracy=0.5634, test/loss=1.98463, test/num_examples=10000, total_duration=47187.3, train/accuracy=0.775809, train/loss=0.83511, validation/accuracy=0.69058, validation/loss=1.25109, validation/num_examples=50000
I0307 15:16:56.168582 140296834766592 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.474437952041626, loss=1.289531946182251
I0307 15:17:45.169556 140296826373888 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.6620047092437744, loss=1.2353907823562622
I0307 15:18:35.979385 140296834766592 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.5043885707855225, loss=1.4000587463378906
I0307 15:19:17.120054 140296826373888 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.5699048042297363, loss=1.2879303693771362
I0307 15:19:57.367319 140296834766592 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.5778846740722656, loss=1.3742666244506836
I0307 15:20:37.007079 140296826373888 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.9402430057525635, loss=1.2724688053131104
I0307 15:21:17.242290 140296834766592 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.745910882949829, loss=1.280864953994751
I0307 15:22:03.807067 140296826373888 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.7041537761688232, loss=1.3234522342681885
I0307 15:22:43.400237 140296834766592 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.624028444290161, loss=1.2692407369613647
I0307 15:23:23.875153 140296826373888 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.765475034713745, loss=1.3155977725982666
I0307 15:24:05.734267 140296834766592 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.5027008056640625, loss=1.256662130355835
I0307 15:24:46.164165 140296826373888 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.518270969390869, loss=1.2770159244537354
I0307 15:25:09.336844 140452179379392 spec.py:321] Evaluating on the training split.
I0307 15:25:21.889027 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 15:25:43.220280 140452179379392 spec.py:349] Evaluating on the test split.
I0307 15:25:44.986083 140452179379392 submission_runner.py:469] Time since start: 47733.27s, 	Step: 112758, 	{'train/accuracy': 0.7860730290412903, 'train/loss': 0.7833583354949951, 'validation/accuracy': 0.699180006980896, 'validation/loss': 1.2287652492523193, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.9555494785308838, 'test/num_examples': 10000, 'score': 44421.96342945099, 'total_duration': 47733.27199077606, 'accumulated_submission_time': 44421.96342945099, 'accumulated_eval_time': 3286.9888343811035, 'accumulated_logging_time': 12.194407224655151}
I0307 15:25:45.123549 140296834766592 logging_writer.py:48] [112758] accumulated_eval_time=3286.99, accumulated_logging_time=12.1944, accumulated_submission_time=44422, global_step=112758, preemption_count=0, score=44422, test/accuracy=0.5719, test/loss=1.95555, test/num_examples=10000, total_duration=47733.3, train/accuracy=0.786073, train/loss=0.783358, validation/accuracy=0.69918, validation/loss=1.22877, validation/num_examples=50000
I0307 15:26:02.869343 140296826373888 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.8708505630493164, loss=1.3433610200881958
I0307 15:26:42.812749 140296834766592 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.7136008739471436, loss=1.3124897480010986
I0307 15:27:24.409616 140296826373888 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.5212621688842773, loss=1.3329310417175293
I0307 15:28:11.024755 140296834766592 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.9667015075683594, loss=1.3281853199005127
I0307 15:29:05.032307 140296826373888 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.6822221279144287, loss=1.3990697860717773
I0307 15:29:51.855245 140296834766592 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.6246109008789062, loss=1.2879548072814941
I0307 15:30:44.356830 140296826373888 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.574258327484131, loss=1.2790167331695557
I0307 15:31:28.006505 140296834766592 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.7725610733032227, loss=1.3829423189163208
I0307 15:32:08.526003 140296826373888 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.5212767124176025, loss=1.2358018159866333
I0307 15:32:49.902078 140296834766592 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.8555750846862793, loss=1.2314351797103882
I0307 15:33:44.327646 140296826373888 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.648118495941162, loss=1.293135404586792
I0307 15:34:15.138487 140452179379392 spec.py:321] Evaluating on the training split.
I0307 15:34:28.359660 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 15:34:48.575087 140452179379392 spec.py:349] Evaluating on the test split.
I0307 15:34:50.344986 140452179379392 submission_runner.py:469] Time since start: 48278.63s, 	Step: 113865, 	{'train/accuracy': 0.78812575340271, 'train/loss': 0.786506175994873, 'validation/accuracy': 0.6923199892044067, 'validation/loss': 1.2570250034332275, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.97958505153656, 'test/num_examples': 10000, 'score': 44931.78919625282, 'total_duration': 48278.630932569504, 'accumulated_submission_time': 44931.78919625282, 'accumulated_eval_time': 3322.1951830387115, 'accumulated_logging_time': 12.395719528198242}
I0307 15:34:50.444775 140296834766592 logging_writer.py:48] [113865] accumulated_eval_time=3322.2, accumulated_logging_time=12.3957, accumulated_submission_time=44931.8, global_step=113865, preemption_count=0, score=44931.8, test/accuracy=0.5638, test/loss=1.97959, test/num_examples=10000, total_duration=48278.6, train/accuracy=0.788126, train/loss=0.786506, validation/accuracy=0.69232, validation/loss=1.25703, validation/num_examples=50000
I0307 15:35:04.884614 140296826373888 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.773303747177124, loss=1.1843087673187256
I0307 15:35:45.251622 140296834766592 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.8221731185913086, loss=1.3454827070236206
I0307 15:36:25.761289 140296826373888 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.8511600494384766, loss=1.4052748680114746
I0307 15:37:06.202780 140296834766592 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.686802387237549, loss=1.278889775276184
I0307 15:37:46.216798 140296826373888 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.505669116973877, loss=1.2438534498214722
I0307 15:38:26.301713 140296834766592 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.6633787155151367, loss=1.252142071723938
I0307 15:39:06.165780 140296826373888 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.4940526485443115, loss=1.281253695487976
I0307 15:39:47.702802 140296834766592 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.4546566009521484, loss=1.2908298969268799
I0307 15:40:26.441431 140296826373888 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.6424429416656494, loss=1.3781832456588745
I0307 15:41:14.500921 140296834766592 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.614999771118164, loss=1.3279480934143066
I0307 15:42:12.708674 140296826373888 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.869732141494751, loss=1.2759044170379639
I0307 15:43:04.443723 140296834766592 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.918783664703369, loss=1.3206441402435303
I0307 15:43:20.517925 140452179379392 spec.py:321] Evaluating on the training split.
I0307 15:43:34.408108 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 15:43:50.641823 140452179379392 spec.py:349] Evaluating on the test split.
I0307 15:43:52.442442 140452179379392 submission_runner.py:469] Time since start: 48820.73s, 	Step: 115035, 	{'train/accuracy': 0.7992864847183228, 'train/loss': 0.737346351146698, 'validation/accuracy': 0.6996200084686279, 'validation/loss': 1.2263686656951904, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 1.9519559144973755, 'test/num_examples': 10000, 'score': 45441.70025849342, 'total_duration': 48820.72835946083, 'accumulated_submission_time': 45441.70025849342, 'accumulated_eval_time': 3354.1195068359375, 'accumulated_logging_time': 12.521143674850464}
I0307 15:43:52.567995 140296826373888 logging_writer.py:48] [115035] accumulated_eval_time=3354.12, accumulated_logging_time=12.5211, accumulated_submission_time=45441.7, global_step=115035, preemption_count=0, score=45441.7, test/accuracy=0.5708, test/loss=1.95196, test/num_examples=10000, total_duration=48820.7, train/accuracy=0.799286, train/loss=0.737346, validation/accuracy=0.69962, validation/loss=1.22637, validation/num_examples=50000
I0307 15:44:25.764013 140296834766592 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.7778735160827637, loss=1.3159617185592651
I0307 15:45:17.173402 140296826373888 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.620464563369751, loss=1.263061761856079
I0307 15:46:09.773018 140296834766592 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.830292224884033, loss=1.3142869472503662
I0307 15:46:53.296762 140296826373888 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.5317604541778564, loss=1.2604272365570068
I0307 15:48:01.729033 140296834766592 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.858060121536255, loss=1.262640118598938
I0307 15:48:48.102923 140296826373888 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.815673828125, loss=1.230372428894043
I0307 15:49:44.274374 140296834766592 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.7445619106292725, loss=1.160888671875
I0307 15:50:33.254791 140296826373888 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.767345666885376, loss=1.2198470830917358
I0307 15:51:32.201118 140296834766592 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.546933889389038, loss=1.1728225946426392
I0307 15:52:19.938237 140296826373888 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.9372670650482178, loss=1.3931227922439575
I0307 15:52:22.749388 140452179379392 spec.py:321] Evaluating on the training split.
I0307 15:52:35.773584 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 15:53:01.052518 140452179379392 spec.py:349] Evaluating on the test split.
I0307 15:53:02.807972 140452179379392 submission_runner.py:469] Time since start: 49371.09s, 	Step: 116008, 	{'train/accuracy': 0.8125796914100647, 'train/loss': 0.6921834945678711, 'validation/accuracy': 0.7010200023651123, 'validation/loss': 1.2076854705810547, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 1.918347954750061, 'test/num_examples': 10000, 'score': 45951.7398557663, 'total_duration': 49371.09390282631, 'accumulated_submission_time': 45951.7398557663, 'accumulated_eval_time': 3394.1779069900513, 'accumulated_logging_time': 12.678936243057251}
I0307 15:53:02.924118 140296834766592 logging_writer.py:48] [116008] accumulated_eval_time=3394.18, accumulated_logging_time=12.6789, accumulated_submission_time=45951.7, global_step=116008, preemption_count=0, score=45951.7, test/accuracy=0.5736, test/loss=1.91835, test/num_examples=10000, total_duration=49371.1, train/accuracy=0.81258, train/loss=0.692183, validation/accuracy=0.70102, validation/loss=1.20769, validation/num_examples=50000
I0307 15:53:48.484911 140296826373888 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.924412488937378, loss=1.35197114944458
I0307 15:54:43.167468 140296834766592 logging_writer.py:48] [116200] global_step=116200, grad_norm=3.024052858352661, loss=1.2925658226013184
I0307 15:55:26.226596 140296826373888 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.9106650352478027, loss=1.3627471923828125
I0307 15:56:08.185397 140296834766592 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.7870676517486572, loss=1.3008952140808105
I0307 15:56:51.847559 140296826373888 logging_writer.py:48] [116500] global_step=116500, grad_norm=3.051685333251953, loss=1.3349642753601074
I0307 15:58:04.727263 140296834766592 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.894808053970337, loss=1.2592476606369019
I0307 15:59:28.375546 140296826373888 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.8948163986206055, loss=1.2854276895523071
I0307 16:00:25.216877 140296834766592 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.6116461753845215, loss=1.1995741128921509
I0307 16:01:14.889981 140296826373888 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.9710326194763184, loss=1.287116289138794
I0307 16:01:32.913280 140452179379392 spec.py:321] Evaluating on the training split.
I0307 16:01:46.552110 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 16:02:07.472010 140452179379392 spec.py:349] Evaluating on the test split.
I0307 16:02:09.230504 140452179379392 submission_runner.py:469] Time since start: 49917.52s, 	Step: 116931, 	{'train/accuracy': 0.7839803695678711, 'train/loss': 0.7950624823570251, 'validation/accuracy': 0.6916599869728088, 'validation/loss': 1.2575702667236328, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 1.9666393995285034, 'test/num_examples': 10000, 'score': 46461.60093379021, 'total_duration': 49917.51643538475, 'accumulated_submission_time': 46461.60093379021, 'accumulated_eval_time': 3430.4949519634247, 'accumulated_logging_time': 12.81736159324646}
I0307 16:02:09.330698 140296834766592 logging_writer.py:48] [116931] accumulated_eval_time=3430.49, accumulated_logging_time=12.8174, accumulated_submission_time=46461.6, global_step=116931, preemption_count=0, score=46461.6, test/accuracy=0.573, test/loss=1.96664, test/num_examples=10000, total_duration=49917.5, train/accuracy=0.78398, train/loss=0.795062, validation/accuracy=0.69166, validation/loss=1.25757, validation/num_examples=50000
I0307 16:02:38.187559 140296826373888 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.5418169498443604, loss=1.297136902809143
I0307 16:03:31.758366 140296834766592 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.8769264221191406, loss=1.3141462802886963
I0307 16:04:36.506091 140296826373888 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.8310928344726562, loss=1.2122917175292969
I0307 16:05:28.968043 140296834766592 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.9401252269744873, loss=1.234487771987915
I0307 16:06:12.145840 140296826373888 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.613774299621582, loss=1.2932829856872559
I0307 16:06:51.680220 140296834766592 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.6502411365509033, loss=1.242963433265686
I0307 16:07:47.390186 140296826373888 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.656425714492798, loss=1.3211073875427246
I0307 16:08:41.120517 140296834766592 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.590956449508667, loss=1.3303877115249634
I0307 16:09:27.996989 140296826373888 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.6244473457336426, loss=1.26714026927948
I0307 16:10:26.729079 140296834766592 logging_writer.py:48] [117900] global_step=117900, grad_norm=3.1298065185546875, loss=1.3856127262115479
I0307 16:10:39.763179 140452179379392 spec.py:321] Evaluating on the training split.
I0307 16:10:53.758647 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 16:11:14.777222 140452179379392 spec.py:349] Evaluating on the test split.
I0307 16:11:16.544047 140452179379392 submission_runner.py:469] Time since start: 50464.83s, 	Step: 117917, 	{'train/accuracy': 0.7870495915412903, 'train/loss': 0.7745840549468994, 'validation/accuracy': 0.7005800008773804, 'validation/loss': 1.2236385345458984, 'validation/num_examples': 50000, 'test/accuracy': 0.5693000555038452, 'test/loss': 1.9412338733673096, 'test/num_examples': 10000, 'score': 46971.86617445946, 'total_duration': 50464.829978466034, 'accumulated_submission_time': 46971.86617445946, 'accumulated_eval_time': 3467.2756390571594, 'accumulated_logging_time': 12.97178339958191}
I0307 16:11:16.639508 140296826373888 logging_writer.py:48] [117917] accumulated_eval_time=3467.28, accumulated_logging_time=12.9718, accumulated_submission_time=46971.9, global_step=117917, preemption_count=0, score=46971.9, test/accuracy=0.5693, test/loss=1.94123, test/num_examples=10000, total_duration=50464.8, train/accuracy=0.78705, train/loss=0.774584, validation/accuracy=0.70058, validation/loss=1.22364, validation/num_examples=50000
I0307 16:11:58.146918 140296834766592 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.8510026931762695, loss=1.3146051168441772
I0307 16:12:52.687683 140296826373888 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.8585968017578125, loss=1.1280946731567383
I0307 16:13:39.495979 140296834766592 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.483835220336914, loss=1.1810245513916016
I0307 16:14:32.135382 140296826373888 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.6917130947113037, loss=1.2665884494781494
I0307 16:15:27.310928 140296834766592 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.8448517322540283, loss=1.2645924091339111
I0307 16:16:17.917669 140296826373888 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.3688247203826904, loss=1.1404917240142822
I0307 16:17:02.839124 140296834766592 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.5729403495788574, loss=1.1838713884353638
I0307 16:17:45.661020 140296826373888 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.708812713623047, loss=1.2848138809204102
I0307 16:18:26.332869 140296834766592 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.011201858520508, loss=1.3219107389450073
I0307 16:19:08.812675 140296826373888 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.764803647994995, loss=1.2354143857955933
I0307 16:19:46.740278 140452179379392 spec.py:321] Evaluating on the training split.
I0307 16:19:59.952806 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 16:20:22.379572 140452179379392 spec.py:349] Evaluating on the test split.
I0307 16:20:24.216364 140452179379392 submission_runner.py:469] Time since start: 51012.50s, 	Step: 118980, 	{'train/accuracy': 0.7922512888908386, 'train/loss': 0.7621221542358398, 'validation/accuracy': 0.6893799901008606, 'validation/loss': 1.272264003753662, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.004242181777954, 'test/num_examples': 10000, 'score': 47481.81721043587, 'total_duration': 51012.50205826759, 'accumulated_submission_time': 47481.81721043587, 'accumulated_eval_time': 3504.751314640045, 'accumulated_logging_time': 13.094877481460571}
I0307 16:20:24.364340 140296834766592 logging_writer.py:48] [118980] accumulated_eval_time=3504.75, accumulated_logging_time=13.0949, accumulated_submission_time=47481.8, global_step=118980, preemption_count=0, score=47481.8, test/accuracy=0.563, test/loss=2.00424, test/num_examples=10000, total_duration=51012.5, train/accuracy=0.792251, train/loss=0.762122, validation/accuracy=0.68938, validation/loss=1.27226, validation/num_examples=50000
I0307 16:20:32.615828 140296826373888 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.833786964416504, loss=1.2710134983062744
I0307 16:21:52.977381 140296834766592 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.72517728805542, loss=1.187532901763916
I0307 16:22:45.722894 140296826373888 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.7379982471466064, loss=1.226806879043579
I0307 16:23:31.066184 140296834766592 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.682278871536255, loss=1.2932674884796143
I0307 16:24:29.535342 140296826373888 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.944319725036621, loss=1.1893953084945679
I0307 16:25:25.804778 140296834766592 logging_writer.py:48] [119500] global_step=119500, grad_norm=3.0381860733032227, loss=1.214808702468872
I0307 16:27:36.576953 140296826373888 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.5365726947784424, loss=1.1816420555114746
I0307 16:28:36.471709 140296834766592 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.6768691539764404, loss=1.1846727132797241
I0307 16:28:54.626730 140452179379392 spec.py:321] Evaluating on the training split.
I0307 16:29:07.953395 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 16:29:28.107451 140452179379392 spec.py:349] Evaluating on the test split.
I0307 16:29:29.866315 140452179379392 submission_runner.py:469] Time since start: 51558.15s, 	Step: 119740, 	{'train/accuracy': 0.825215220451355, 'train/loss': 0.6259505748748779, 'validation/accuracy': 0.7038999795913696, 'validation/loss': 1.2092255353927612, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9645354747772217, 'test/num_examples': 10000, 'score': 47991.96129441261, 'total_duration': 51558.15223145485, 'accumulated_submission_time': 47991.96129441261, 'accumulated_eval_time': 3539.9907054901123, 'accumulated_logging_time': 13.275717735290527}
I0307 16:29:29.949143 140296826373888 logging_writer.py:48] [119740] accumulated_eval_time=3539.99, accumulated_logging_time=13.2757, accumulated_submission_time=47992, global_step=119740, preemption_count=0, score=47992, test/accuracy=0.5707, test/loss=1.96454, test/num_examples=10000, total_duration=51558.2, train/accuracy=0.825215, train/loss=0.625951, validation/accuracy=0.7039, validation/loss=1.20923, validation/num_examples=50000
I0307 16:29:59.488797 140296834766592 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.737914800643921, loss=1.287581205368042
I0307 16:31:18.280116 140296826373888 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.8862576484680176, loss=1.2404594421386719
I0307 16:32:26.942543 140296834766592 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.157291889190674, loss=1.328139066696167
2025-03-07 16:34:13.500077: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:34:17.101236 140296826373888 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.5689730644226074, loss=1.1841827630996704
I0307 16:34:56.517309 140296834766592 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.792248487472534, loss=1.235748291015625
I0307 16:35:36.337368 140296826373888 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.780637264251709, loss=1.2591089010238647
I0307 16:36:24.804277 140296834766592 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.7224793434143066, loss=1.20371413230896
I0307 16:37:17.176785 140296826373888 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.76141357421875, loss=1.1878165006637573
I0307 16:38:00.094917 140452179379392 spec.py:321] Evaluating on the training split.
I0307 16:38:13.800163 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 16:38:37.256650 140452179379392 spec.py:349] Evaluating on the test split.
I0307 16:38:39.015995 140452179379392 submission_runner.py:469] Time since start: 52107.30s, 	Step: 120591, 	{'train/accuracy': 0.8002630472183228, 'train/loss': 0.7337987422943115, 'validation/accuracy': 0.703220009803772, 'validation/loss': 1.2129064798355103, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.914723515510559, 'test/num_examples': 10000, 'score': 48501.96317720413, 'total_duration': 52107.30183887482, 'accumulated_submission_time': 48501.96317720413, 'accumulated_eval_time': 3578.911531686783, 'accumulated_logging_time': 13.40663480758667}
I0307 16:38:39.108712 140296834766592 logging_writer.py:48] [120591] accumulated_eval_time=3578.91, accumulated_logging_time=13.4066, accumulated_submission_time=48502, global_step=120591, preemption_count=0, score=48502, test/accuracy=0.5764, test/loss=1.91472, test/num_examples=10000, total_duration=52107.3, train/accuracy=0.800263, train/loss=0.733799, validation/accuracy=0.70322, validation/loss=1.21291, validation/num_examples=50000
I0307 16:38:43.001834 140296826373888 logging_writer.py:48] [120600] global_step=120600, grad_norm=3.1899373531341553, loss=1.2761638164520264
I0307 16:40:03.330214 140296834766592 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.0219202041625977, loss=1.2341411113739014
I0307 16:41:12.035239 140296826373888 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.774709463119507, loss=1.2142574787139893
I0307 16:42:04.616336 140296834766592 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.7215211391448975, loss=1.252102255821228
I0307 16:42:56.928558 140296826373888 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.7664804458618164, loss=1.2728277444839478
I0307 16:43:41.681325 140296834766592 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.8045146465301514, loss=1.245586633682251
I0307 16:45:08.119950 140296826373888 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.7707793712615967, loss=1.2150286436080933
I0307 16:46:08.964100 140296834766592 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.788300037384033, loss=1.2074049711227417
I0307 16:47:10.185181 140452179379392 spec.py:321] Evaluating on the training split.
I0307 16:47:21.820168 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 16:47:43.335130 140452179379392 spec.py:349] Evaluating on the test split.
I0307 16:47:45.143653 140452179379392 submission_runner.py:469] Time since start: 52653.39s, 	Step: 121333, 	{'train/accuracy': 0.8097097873687744, 'train/loss': 0.6910713911056519, 'validation/accuracy': 0.6981799602508545, 'validation/loss': 1.2400017976760864, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 1.9810094833374023, 'test/num_examples': 10000, 'score': 49012.90021824837, 'total_duration': 52653.38587450981, 'accumulated_submission_time': 49012.90021824837, 'accumulated_eval_time': 3613.8261275291443, 'accumulated_logging_time': 13.55582880973816}
I0307 16:47:45.259397 140296826373888 logging_writer.py:48] [121333] accumulated_eval_time=3613.83, accumulated_logging_time=13.5558, accumulated_submission_time=49012.9, global_step=121333, preemption_count=0, score=49012.9, test/accuracy=0.5692, test/loss=1.98101, test/num_examples=10000, total_duration=52653.4, train/accuracy=0.80971, train/loss=0.691071, validation/accuracy=0.69818, validation/loss=1.24, validation/num_examples=50000
I0307 16:48:53.729601 140296834766592 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.767263650894165, loss=1.1872340440750122
I0307 16:49:47.059110 140296826373888 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.77890944480896, loss=1.243241310119629
I0307 16:51:26.474733 140296834766592 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.0338048934936523, loss=1.2553400993347168
I0307 16:53:03.634619 140296826373888 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.736021041870117, loss=1.164790153503418
I0307 16:53:59.216832 140296834766592 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.0720179080963135, loss=1.2277158498764038
I0307 16:54:55.535164 140296826373888 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.809802770614624, loss=1.2500540018081665
I0307 16:56:11.792156 140296834766592 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.2820401191711426, loss=1.3116445541381836
I0307 16:56:15.212159 140452179379392 spec.py:321] Evaluating on the training split.
I0307 16:56:27.353420 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 16:56:47.396568 140452179379392 spec.py:349] Evaluating on the test split.
I0307 16:56:49.177833 140452179379392 submission_runner.py:469] Time since start: 53197.46s, 	Step: 122007, 	{'train/accuracy': 0.8040696382522583, 'train/loss': 0.7202451229095459, 'validation/accuracy': 0.7048999667167664, 'validation/loss': 1.190501093864441, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9123563766479492, 'test/num_examples': 10000, 'score': 49521.98909521103, 'total_duration': 53197.46391201019, 'accumulated_submission_time': 49521.98909521103, 'accumulated_eval_time': 3647.7917675971985, 'accumulated_logging_time': 14.459708452224731}
I0307 16:56:49.220680 140296826373888 logging_writer.py:48] [122007] accumulated_eval_time=3647.79, accumulated_logging_time=14.4597, accumulated_submission_time=49522, global_step=122007, preemption_count=0, score=49522, test/accuracy=0.5732, test/loss=1.91236, test/num_examples=10000, total_duration=53197.5, train/accuracy=0.80407, train/loss=0.720245, validation/accuracy=0.7049, validation/loss=1.1905, validation/num_examples=50000
I0307 16:57:51.341375 140296834766592 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.89127254486084, loss=1.1667319536209106
I0307 16:59:04.950675 140296826373888 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.8846418857574463, loss=1.1782121658325195
I0307 17:00:03.075789 140296834766592 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.7075181007385254, loss=1.2711880207061768
I0307 17:01:19.392390 140296826373888 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.9799861907958984, loss=1.2136662006378174
I0307 17:02:21.735128 140296834766592 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.0276849269866943, loss=1.2760646343231201
I0307 17:03:59.975749 140296826373888 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.818693161010742, loss=1.2027345895767212
I0307 17:05:07.301558 140296834766592 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.7455339431762695, loss=1.2264208793640137
I0307 17:05:19.563907 140452179379392 spec.py:321] Evaluating on the training split.
I0307 17:05:30.761256 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 17:05:51.312530 140452179379392 spec.py:349] Evaluating on the test split.
I0307 17:05:53.097259 140452179379392 submission_runner.py:469] Time since start: 53741.38s, 	Step: 122716, 	{'train/accuracy': 0.8178212642669678, 'train/loss': 0.6545913219451904, 'validation/accuracy': 0.7041400074958801, 'validation/loss': 1.2171365022659302, 'validation/num_examples': 50000, 'test/accuracy': 0.5825000405311584, 'test/loss': 1.9358903169631958, 'test/num_examples': 10000, 'score': 50032.24576711655, 'total_duration': 53741.38331794739, 'accumulated_submission_time': 50032.24576711655, 'accumulated_eval_time': 3681.325066804886, 'accumulated_logging_time': 14.51060152053833}
I0307 17:05:53.175774 140296826373888 logging_writer.py:48] [122716] accumulated_eval_time=3681.33, accumulated_logging_time=14.5106, accumulated_submission_time=50032.2, global_step=122716, preemption_count=0, score=50032.2, test/accuracy=0.5825, test/loss=1.93589, test/num_examples=10000, total_duration=53741.4, train/accuracy=0.817821, train/loss=0.654591, validation/accuracy=0.70414, validation/loss=1.21714, validation/num_examples=50000
I0307 17:06:34.612073 140296834766592 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.8654632568359375, loss=1.1376795768737793
I0307 17:08:01.184851 140296826373888 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.820666790008545, loss=1.2180064916610718
I0307 17:09:20.858936 140296834766592 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.754812717437744, loss=1.1954753398895264
I0307 17:10:26.104309 140296826373888 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.971938133239746, loss=1.215501070022583
I0307 17:11:49.784363 140296834766592 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.004051923751831, loss=1.278213381767273
I0307 17:13:05.056693 140296826373888 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.6720986366271973, loss=1.1157543659210205
I0307 17:14:01.061839 140296834766592 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.0873773097991943, loss=1.2660611867904663
I0307 17:14:23.211672 140452179379392 spec.py:321] Evaluating on the training split.
I0307 17:14:35.720901 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 17:14:57.053839 140452179379392 spec.py:349] Evaluating on the test split.
I0307 17:14:58.823739 140452179379392 submission_runner.py:469] Time since start: 54287.11s, 	Step: 123439, 	{'train/accuracy': 0.8032923936843872, 'train/loss': 0.7089185118675232, 'validation/accuracy': 0.7051799893379211, 'validation/loss': 1.1993591785430908, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.9342819452285767, 'test/num_examples': 10000, 'score': 50542.192996025085, 'total_duration': 54287.10981154442, 'accumulated_submission_time': 50542.192996025085, 'accumulated_eval_time': 3716.937094449997, 'accumulated_logging_time': 14.596786499023438}
I0307 17:14:58.868315 140296826373888 logging_writer.py:48] [123439] accumulated_eval_time=3716.94, accumulated_logging_time=14.5968, accumulated_submission_time=50542.2, global_step=123439, preemption_count=0, score=50542.2, test/accuracy=0.5768, test/loss=1.93428, test/num_examples=10000, total_duration=54287.1, train/accuracy=0.803292, train/loss=0.708919, validation/accuracy=0.70518, validation/loss=1.19936, validation/num_examples=50000
I0307 17:15:27.656053 140296834766592 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.805004119873047, loss=1.1492213010787964
I0307 17:16:36.803846 140296826373888 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.013709545135498, loss=1.2819327116012573
I0307 17:17:59.685821 140296834766592 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.809854507446289, loss=1.2354614734649658
I0307 17:19:19.732910 140296826373888 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.9147355556488037, loss=1.2665088176727295
2025-03-07 17:19:54.007831: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:20:35.184323 140296834766592 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.194385290145874, loss=1.1414116621017456
I0307 17:21:34.330140 140296826373888 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.727485418319702, loss=1.2469252347946167
I0307 17:23:12.606167 140296834766592 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.6283607482910156, loss=1.0693411827087402
I0307 17:23:28.952607 140452179379392 spec.py:321] Evaluating on the training split.
I0307 17:23:40.802607 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 17:24:03.305792 140452179379392 spec.py:349] Evaluating on the test split.
I0307 17:24:05.056023 140452179379392 submission_runner.py:469] Time since start: 54833.34s, 	Step: 124121, 	{'train/accuracy': 0.8209701776504517, 'train/loss': 0.6474238038063049, 'validation/accuracy': 0.7029799818992615, 'validation/loss': 1.2080973386764526, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 1.9225531816482544, 'test/num_examples': 10000, 'score': 51052.16874957085, 'total_duration': 54833.34208202362, 'accumulated_submission_time': 51052.16874957085, 'accumulated_eval_time': 3753.0404579639435, 'accumulated_logging_time': 14.671565294265747}
I0307 17:24:05.129045 140296826373888 logging_writer.py:48] [124121] accumulated_eval_time=3753.04, accumulated_logging_time=14.6716, accumulated_submission_time=51052.2, global_step=124121, preemption_count=0, score=51052.2, test/accuracy=0.5795, test/loss=1.92255, test/num_examples=10000, total_duration=54833.3, train/accuracy=0.82097, train/loss=0.647424, validation/accuracy=0.70298, validation/loss=1.2081, validation/num_examples=50000
I0307 17:24:40.820131 140296834766592 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.378016948699951, loss=1.1919583082199097
I0307 17:25:34.268620 140296826373888 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.8176543712615967, loss=1.226765513420105
I0307 17:26:29.026407 140296834766592 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.9582107067108154, loss=1.2017799615859985
I0307 17:27:34.994162 140296826373888 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.8525424003601074, loss=1.157549500465393
I0307 17:28:56.651187 140296834766592 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.778031587600708, loss=1.2419750690460205
I0307 17:30:13.779776 140296826373888 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.971240997314453, loss=1.2428816556930542
I0307 17:32:01.342186 140296834766592 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0251071453094482, loss=1.188284993171692
I0307 17:32:35.301847 140452179379392 spec.py:321] Evaluating on the training split.
I0307 17:32:47.107274 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 17:33:07.391175 140452179379392 spec.py:349] Evaluating on the test split.
I0307 17:33:09.160830 140452179379392 submission_runner.py:469] Time since start: 55377.45s, 	Step: 124839, 	{'train/accuracy': 0.8093709945678711, 'train/loss': 0.696272075176239, 'validation/accuracy': 0.7047199606895447, 'validation/loss': 1.2032544612884521, 'validation/num_examples': 50000, 'test/accuracy': 0.5853000283241272, 'test/loss': 1.9087883234024048, 'test/num_examples': 10000, 'score': 51562.25228142738, 'total_duration': 55377.44689488411, 'accumulated_submission_time': 51562.25228142738, 'accumulated_eval_time': 3786.8993968963623, 'accumulated_logging_time': 14.752557754516602}
I0307 17:33:09.224327 140296826373888 logging_writer.py:48] [124839] accumulated_eval_time=3786.9, accumulated_logging_time=14.7526, accumulated_submission_time=51562.3, global_step=124839, preemption_count=0, score=51562.3, test/accuracy=0.5853, test/loss=1.90879, test/num_examples=10000, total_duration=55377.4, train/accuracy=0.809371, train/loss=0.696272, validation/accuracy=0.70472, validation/loss=1.20325, validation/num_examples=50000
I0307 17:33:50.304749 140296834766592 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.047999620437622, loss=1.2418913841247559
I0307 17:36:31.791442 140296826373888 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.8635246753692627, loss=1.1235551834106445
I0307 17:37:32.738550 140296834766592 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.713193416595459, loss=1.1260206699371338
I0307 17:38:29.834158 140296826373888 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.9164466857910156, loss=1.224202275276184
I0307 17:40:56.979397 140296834766592 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.927502155303955, loss=1.1548070907592773
I0307 17:41:39.319333 140452179379392 spec.py:321] Evaluating on the training split.
I0307 17:41:51.520727 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 17:42:12.389048 140452179379392 spec.py:349] Evaluating on the test split.
I0307 17:42:14.145581 140452179379392 submission_runner.py:469] Time since start: 55922.43s, 	Step: 125360, 	{'train/accuracy': 0.8184390664100647, 'train/loss': 0.6588167548179626, 'validation/accuracy': 0.6976000070571899, 'validation/loss': 1.2266343832015991, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 1.9314446449279785, 'test/num_examples': 10000, 'score': 52072.27811574936, 'total_duration': 55922.4316444397, 'accumulated_submission_time': 52072.27811574936, 'accumulated_eval_time': 3821.7256004810333, 'accumulated_logging_time': 14.82430624961853}
I0307 17:42:14.194966 140296826373888 logging_writer.py:48] [125360] accumulated_eval_time=3821.73, accumulated_logging_time=14.8243, accumulated_submission_time=52072.3, global_step=125360, preemption_count=0, score=52072.3, test/accuracy=0.5744, test/loss=1.93144, test/num_examples=10000, total_duration=55922.4, train/accuracy=0.818439, train/loss=0.658817, validation/accuracy=0.6976, validation/loss=1.22663, validation/num_examples=50000
I0307 17:42:30.673147 140296834766592 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.9393413066864014, loss=1.1661128997802734
I0307 17:43:42.514036 140296826373888 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.907413959503174, loss=1.1542328596115112
I0307 17:45:28.292559 140296834766592 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.8361074924468994, loss=1.1237446069717407
I0307 17:46:59.615763 140296826373888 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.051870822906494, loss=1.1853289604187012
I0307 17:48:16.166131 140296834766592 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.270899772644043, loss=1.282675862312317
I0307 17:49:49.239215 140296826373888 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.9244959354400635, loss=1.1607515811920166
I0307 17:50:44.420594 140452179379392 spec.py:321] Evaluating on the training split.
I0307 17:50:55.970543 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 17:51:15.925008 140452179379392 spec.py:349] Evaluating on the test split.
I0307 17:51:17.685293 140452179379392 submission_runner.py:469] Time since start: 56465.97s, 	Step: 125935, 	{'train/accuracy': 0.8102877736091614, 'train/loss': 0.6886866688728333, 'validation/accuracy': 0.7069599628448486, 'validation/loss': 1.1878427267074585, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.905565857887268, 'test/num_examples': 10000, 'score': 52582.41988611221, 'total_duration': 56465.97137212753, 'accumulated_submission_time': 52582.41988611221, 'accumulated_eval_time': 3854.9902682304382, 'accumulated_logging_time': 14.893283367156982}
I0307 17:51:17.728444 140296834766592 logging_writer.py:48] [125935] accumulated_eval_time=3854.99, accumulated_logging_time=14.8933, accumulated_submission_time=52582.4, global_step=125935, preemption_count=0, score=52582.4, test/accuracy=0.5771, test/loss=1.90557, test/num_examples=10000, total_duration=56466, train/accuracy=0.810288, train/loss=0.688687, validation/accuracy=0.70696, validation/loss=1.18784, validation/num_examples=50000
I0307 17:51:44.263765 140296826373888 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.8022208213806152, loss=1.1405980587005615
I0307 17:53:41.666247 140296834766592 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.8990302085876465, loss=1.1708946228027344
I0307 17:55:53.342881 140296826373888 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.3529109954833984, loss=1.2282284498214722
I0307 17:57:38.964508 140296834766592 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.981112003326416, loss=1.2860924005508423
I0307 17:58:45.839977 140296826373888 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.950258493423462, loss=1.1345155239105225
I0307 17:59:41.806406 140296834766592 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.8881776332855225, loss=1.2254281044006348
I0307 17:59:48.077134 140452179379392 spec.py:321] Evaluating on the training split.
I0307 18:00:00.120124 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 18:00:22.587033 140452179379392 spec.py:349] Evaluating on the test split.
I0307 18:00:24.343175 140452179379392 submission_runner.py:469] Time since start: 57012.63s, 	Step: 126512, 	{'train/accuracy': 0.8517019748687744, 'train/loss': 0.5410611629486084, 'validation/accuracy': 0.70660001039505, 'validation/loss': 1.1955972909927368, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.932255506515503, 'test/num_examples': 10000, 'score': 53092.69561123848, 'total_duration': 57012.62925815582, 'accumulated_submission_time': 53092.69561123848, 'accumulated_eval_time': 3891.256278991699, 'accumulated_logging_time': 14.943933248519897}
I0307 18:00:24.393802 140296826373888 logging_writer.py:48] [126512] accumulated_eval_time=3891.26, accumulated_logging_time=14.9439, accumulated_submission_time=53092.7, global_step=126512, preemption_count=0, score=53092.7, test/accuracy=0.5768, test/loss=1.93226, test/num_examples=10000, total_duration=57012.6, train/accuracy=0.851702, train/loss=0.541061, validation/accuracy=0.7066, validation/loss=1.1956, validation/num_examples=50000
I0307 18:01:17.626983 140296834766592 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.797776460647583, loss=1.2166551351547241
I0307 18:02:46.286813 140296826373888 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.1822211742401123, loss=1.1863771677017212
I0307 18:04:10.863471 140296834766592 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.090830087661743, loss=1.2406514883041382
I0307 18:06:19.175981 140296826373888 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.428187370300293, loss=1.1792783737182617
I0307 18:08:53.608295 140296834766592 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.9577763080596924, loss=1.2087786197662354
I0307 18:08:54.588032 140452179379392 spec.py:321] Evaluating on the training split.
I0307 18:09:05.797887 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 18:09:25.063241 140452179379392 spec.py:349] Evaluating on the test split.
I0307 18:09:26.864598 140452179379392 submission_runner.py:469] Time since start: 57555.15s, 	Step: 127002, 	{'train/accuracy': 0.8234215378761292, 'train/loss': 0.6400219202041626, 'validation/accuracy': 0.7119399905204773, 'validation/loss': 1.179505705833435, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 1.894606351852417, 'test/num_examples': 10000, 'score': 53602.824206352234, 'total_duration': 57555.15067195892, 'accumulated_submission_time': 53602.824206352234, 'accumulated_eval_time': 3923.532804250717, 'accumulated_logging_time': 15.003150463104248}
I0307 18:09:26.925083 140296826373888 logging_writer.py:48] [127002] accumulated_eval_time=3923.53, accumulated_logging_time=15.0032, accumulated_submission_time=53602.8, global_step=127002, preemption_count=0, score=53602.8, test/accuracy=0.5897, test/loss=1.89461, test/num_examples=10000, total_duration=57555.2, train/accuracy=0.823422, train/loss=0.640022, validation/accuracy=0.71194, validation/loss=1.17951, validation/num_examples=50000
I0307 18:11:14.224149 140296834766592 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.0367796421051025, loss=1.2102811336517334
I0307 18:12:51.251585 140296826373888 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.3077731132507324, loss=1.3079888820648193
I0307 18:13:48.014921 140296834766592 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.332298517227173, loss=1.2484840154647827
I0307 18:16:02.607297 140296826373888 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.8005940914154053, loss=1.1534771919250488
I0307 18:17:57.204304 140452179379392 spec.py:321] Evaluating on the training split.
I0307 18:18:08.928712 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 18:18:33.756937 140452179379392 spec.py:349] Evaluating on the test split.
I0307 18:18:35.517833 140452179379392 submission_runner.py:469] Time since start: 58103.80s, 	Step: 127432, 	{'train/accuracy': 0.813875138759613, 'train/loss': 0.6636583805084229, 'validation/accuracy': 0.7100600004196167, 'validation/loss': 1.1728723049163818, 'validation/num_examples': 50000, 'test/accuracy': 0.5839000344276428, 'test/loss': 1.9206054210662842, 'test/num_examples': 10000, 'score': 54113.0475692749, 'total_duration': 58103.80387854576, 'accumulated_submission_time': 54113.0475692749, 'accumulated_eval_time': 3961.8462760448456, 'accumulated_logging_time': 15.071233987808228}
I0307 18:18:35.566555 140296834766592 logging_writer.py:48] [127432] accumulated_eval_time=3961.85, accumulated_logging_time=15.0712, accumulated_submission_time=54113, global_step=127432, preemption_count=0, score=54113, test/accuracy=0.5839, test/loss=1.92061, test/num_examples=10000, total_duration=58103.8, train/accuracy=0.813875, train/loss=0.663658, validation/accuracy=0.71006, validation/loss=1.17287, validation/num_examples=50000
I0307 18:19:32.723162 140296826373888 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.162341356277466, loss=1.184149980545044
I0307 18:20:54.723269 140296834766592 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.930907726287842, loss=1.211556077003479
I0307 18:22:24.510895 140296826373888 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.0472795963287354, loss=1.1925294399261475
I0307 18:23:47.564296 140296834766592 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.9599263668060303, loss=1.1465462446212769
I0307 18:24:59.497800 140296826373888 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.3268723487854004, loss=1.179182529449463
I0307 18:26:15.733712 140296834766592 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.9377548694610596, loss=1.171767234802246
I0307 18:27:05.822295 140452179379392 spec.py:321] Evaluating on the training split.
I0307 18:27:16.999038 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 18:27:38.846791 140452179379392 spec.py:349] Evaluating on the test split.
I0307 18:27:40.635998 140452179379392 submission_runner.py:469] Time since start: 58648.92s, 	Step: 128059, 	{'train/accuracy': 0.8400430083274841, 'train/loss': 0.5688015222549438, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1803946495056152, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.88873291015625, 'test/num_examples': 10000, 'score': 54623.22213149071, 'total_duration': 58648.92207670212, 'accumulated_submission_time': 54623.22213149071, 'accumulated_eval_time': 3996.659945011139, 'accumulated_logging_time': 15.128484964370728}
I0307 18:27:40.725299 140296826373888 logging_writer.py:48] [128059] accumulated_eval_time=3996.66, accumulated_logging_time=15.1285, accumulated_submission_time=54623.2, global_step=128059, preemption_count=0, score=54623.2, test/accuracy=0.5927, test/loss=1.88873, test/num_examples=10000, total_duration=58648.9, train/accuracy=0.840043, train/loss=0.568802, validation/accuracy=0.71188, validation/loss=1.18039, validation/num_examples=50000
I0307 18:28:52.549406 140296834766592 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.91633677482605, loss=1.1582494974136353
I0307 18:31:19.019423 140296826373888 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.9239609241485596, loss=1.1335090398788452
I0307 18:32:43.211529 140296834766592 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.022310256958008, loss=1.1650028228759766
I0307 18:34:18.001480 140296826373888 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.8460350036621094, loss=1.2250279188156128
I0307 18:35:28.056797 140296834766592 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.7151336669921875, loss=1.0498833656311035
I0307 18:36:10.849717 140452179379392 spec.py:321] Evaluating on the training split.
I0307 18:36:20.968303 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 18:36:39.728223 140452179379392 spec.py:349] Evaluating on the test split.
I0307 18:36:41.503905 140452179379392 submission_runner.py:469] Time since start: 59189.79s, 	Step: 128561, 	{'train/accuracy': 0.8127790093421936, 'train/loss': 0.6717605590820312, 'validation/accuracy': 0.7080599665641785, 'validation/loss': 1.199042797088623, 'validation/num_examples': 50000, 'test/accuracy': 0.5794000029563904, 'test/loss': 1.9341740608215332, 'test/num_examples': 10000, 'score': 55133.282629966736, 'total_duration': 59189.789964199066, 'accumulated_submission_time': 55133.282629966736, 'accumulated_eval_time': 4027.3140909671783, 'accumulated_logging_time': 15.226057052612305}
I0307 18:36:41.577508 140296826373888 logging_writer.py:48] [128561] accumulated_eval_time=4027.31, accumulated_logging_time=15.2261, accumulated_submission_time=55133.3, global_step=128561, preemption_count=0, score=55133.3, test/accuracy=0.5794, test/loss=1.93417, test/num_examples=10000, total_duration=59189.8, train/accuracy=0.812779, train/loss=0.671761, validation/accuracy=0.70806, validation/loss=1.19904, validation/num_examples=50000
I0307 18:37:12.072737 140296834766592 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.9044253826141357, loss=1.1434675455093384
I0307 18:38:57.705125 140296826373888 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.0776827335357666, loss=1.1233398914337158
I0307 18:40:27.091351 140296834766592 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.1977121829986572, loss=1.1472697257995605
I0307 18:42:10.607150 140296826373888 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.1263251304626465, loss=1.2097394466400146
I0307 18:44:12.582198 140296834766592 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.8283629417419434, loss=1.1103370189666748
I0307 18:45:12.651124 140452179379392 spec.py:321] Evaluating on the training split.
I0307 18:45:23.593143 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 18:45:41.518360 140452179379392 spec.py:349] Evaluating on the test split.
I0307 18:45:43.484530 140452179379392 submission_runner.py:469] Time since start: 59731.77s, 	Step: 129062, 	{'train/accuracy': 0.81351637840271, 'train/loss': 0.6617496609687805, 'validation/accuracy': 0.7140600085258484, 'validation/loss': 1.1778055429458618, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.8974639177322388, 'test/num_examples': 10000, 'score': 55644.29246711731, 'total_duration': 59731.770600795746, 'accumulated_submission_time': 55644.29246711731, 'accumulated_eval_time': 4058.1474578380585, 'accumulated_logging_time': 15.307074308395386}
I0307 18:45:43.570817 140296826373888 logging_writer.py:48] [129062] accumulated_eval_time=4058.15, accumulated_logging_time=15.3071, accumulated_submission_time=55644.3, global_step=129062, preemption_count=0, score=55644.3, test/accuracy=0.5845, test/loss=1.89746, test/num_examples=10000, total_duration=59731.8, train/accuracy=0.813516, train/loss=0.66175, validation/accuracy=0.71406, validation/loss=1.17781, validation/num_examples=50000
I0307 18:46:11.604038 140296834766592 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.889949321746826, loss=1.111451268196106
I0307 18:49:59.471425 140296826373888 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.0972185134887695, loss=1.1504619121551514
I0307 18:51:23.004327 140296834766592 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.210088014602661, loss=1.153751015663147
I0307 18:52:44.748887 140296826373888 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.0914204120635986, loss=1.144272804260254
I0307 18:54:14.808897 140452179379392 spec.py:321] Evaluating on the training split.
I0307 18:54:26.054734 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 18:54:45.238314 140452179379392 spec.py:349] Evaluating on the test split.
I0307 18:54:47.011109 140452179379392 submission_runner.py:469] Time since start: 60275.30s, 	Step: 129487, 	{'train/accuracy': 0.8402822017669678, 'train/loss': 0.564418613910675, 'validation/accuracy': 0.714199960231781, 'validation/loss': 1.1659231185913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5874000191688538, 'test/loss': 1.8769652843475342, 'test/num_examples': 10000, 'score': 56155.474486112595, 'total_duration': 60275.29718184471, 'accumulated_submission_time': 56155.474486112595, 'accumulated_eval_time': 4090.349634170532, 'accumulated_logging_time': 15.401219129562378}
I0307 18:54:47.051893 140296834766592 logging_writer.py:48] [129487] accumulated_eval_time=4090.35, accumulated_logging_time=15.4012, accumulated_submission_time=56155.5, global_step=129487, preemption_count=0, score=56155.5, test/accuracy=0.5874, test/loss=1.87697, test/num_examples=10000, total_duration=60275.3, train/accuracy=0.840282, train/loss=0.564419, validation/accuracy=0.7142, validation/loss=1.16592, validation/num_examples=50000
I0307 18:55:25.216985 140296826373888 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.8782825469970703, loss=1.1132972240447998
I0307 19:00:32.442429 140296834766592 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.144735813140869, loss=1.1319003105163574
I0307 19:02:58.318928 140296826373888 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.9287450313568115, loss=1.0715245008468628
I0307 19:03:17.376064 140452179379392 spec.py:321] Evaluating on the training split.
I0307 19:03:29.708483 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 19:03:48.293970 140452179379392 spec.py:349] Evaluating on the test split.
I0307 19:03:50.045395 140452179379392 submission_runner.py:469] Time since start: 60818.33s, 	Step: 129734, 	{'train/accuracy': 0.8283242583274841, 'train/loss': 0.6195014715194702, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1777704954147339, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.9124149084091187, 'test/num_examples': 10000, 'score': 56665.757748126984, 'total_duration': 60818.33146452904, 'accumulated_submission_time': 56665.757748126984, 'accumulated_eval_time': 4123.018921375275, 'accumulated_logging_time': 15.45542860031128}
I0307 19:03:50.095065 140296834766592 logging_writer.py:48] [129734] accumulated_eval_time=4123.02, accumulated_logging_time=15.4554, accumulated_submission_time=56665.8, global_step=129734, preemption_count=0, score=56665.8, test/accuracy=0.5805, test/loss=1.91241, test/num_examples=10000, total_duration=60818.3, train/accuracy=0.828324, train/loss=0.619501, validation/accuracy=0.71188, validation/loss=1.17777, validation/num_examples=50000
I0307 19:07:18.529881 140296826373888 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.046578884124756, loss=1.1845126152038574
I0307 19:08:55.968597 140296834766592 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.125190258026123, loss=1.24299955368042
I0307 19:10:07.301064 140296826373888 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.0552968978881836, loss=1.128225564956665
I0307 19:11:56.240172 140296834766592 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.2883501052856445, loss=1.2023980617523193
I0307 19:12:20.374963 140452179379392 spec.py:321] Evaluating on the training split.
I0307 19:12:32.319482 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 19:12:55.619308 140452179379392 spec.py:349] Evaluating on the test split.
I0307 19:12:57.356692 140452179379392 submission_runner.py:469] Time since start: 61365.64s, 	Step: 130145, 	{'train/accuracy': 0.8169642686843872, 'train/loss': 0.6591873168945312, 'validation/accuracy': 0.7076999545097351, 'validation/loss': 1.1928380727767944, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 1.9340249300003052, 'test/num_examples': 10000, 'score': 57175.97269439697, 'total_duration': 61365.642771959305, 'accumulated_submission_time': 57175.97269439697, 'accumulated_eval_time': 4160.000620365143, 'accumulated_logging_time': 15.52323317527771}
I0307 19:12:57.395770 140296826373888 logging_writer.py:48] [130145] accumulated_eval_time=4160, accumulated_logging_time=15.5232, accumulated_submission_time=57176, global_step=130145, preemption_count=0, score=57176, test/accuracy=0.5784, test/loss=1.93402, test/num_examples=10000, total_duration=61365.6, train/accuracy=0.816964, train/loss=0.659187, validation/accuracy=0.7077, validation/loss=1.19284, validation/num_examples=50000
I0307 19:14:08.611615 140296834766592 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.1515378952026367, loss=1.1168385744094849
I0307 19:15:50.852902 140296826373888 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.1926209926605225, loss=1.2048622369766235
I0307 19:17:36.072184 140296834766592 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.9466683864593506, loss=1.054448127746582
I0307 19:19:23.201538 140296826373888 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.947674036026001, loss=1.1511462926864624
I0307 19:21:09.342656 140296834766592 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.0959715843200684, loss=1.1429343223571777
I0307 19:21:27.461961 140452179379392 spec.py:321] Evaluating on the training split.
I0307 19:21:39.323477 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 19:22:00.155427 140452179379392 spec.py:349] Evaluating on the test split.
I0307 19:22:01.898830 140452179379392 submission_runner.py:469] Time since start: 61910.18s, 	Step: 130618, 	{'train/accuracy': 0.8188974857330322, 'train/loss': 0.6450485587120056, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.1781855821609497, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.9016708135604858, 'test/num_examples': 10000, 'score': 57685.97555708885, 'total_duration': 61910.184910297394, 'accumulated_submission_time': 57685.97555708885, 'accumulated_eval_time': 4194.437458276749, 'accumulated_logging_time': 15.570994138717651}
I0307 19:22:02.004584 140296826373888 logging_writer.py:48] [130618] accumulated_eval_time=4194.44, accumulated_logging_time=15.571, accumulated_submission_time=57686, global_step=130618, preemption_count=0, score=57686, test/accuracy=0.5886, test/loss=1.90167, test/num_examples=10000, total_duration=61910.2, train/accuracy=0.818897, train/loss=0.645049, validation/accuracy=0.71434, validation/loss=1.17819, validation/num_examples=50000
I0307 19:25:38.375613 140296834766592 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.1322665214538574, loss=1.1803064346313477
I0307 19:28:34.914534 140296826373888 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.0041840076446533, loss=1.1587038040161133
I0307 19:30:32.620428 140452179379392 spec.py:321] Evaluating on the training split.
I0307 19:30:43.807556 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 19:31:03.377154 140452179379392 spec.py:349] Evaluating on the test split.
I0307 19:31:05.132253 140452179379392 submission_runner.py:469] Time since start: 62453.42s, 	Step: 130857, 	{'train/accuracy': 0.8402024507522583, 'train/loss': 0.5743464827537537, 'validation/accuracy': 0.7050399780273438, 'validation/loss': 1.2018828392028809, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.9341565370559692, 'test/num_examples': 10000, 'score': 58196.55683040619, 'total_duration': 62453.41831159592, 'accumulated_submission_time': 58196.55683040619, 'accumulated_eval_time': 4226.949236869812, 'accumulated_logging_time': 15.684504508972168}
I0307 19:31:05.222419 140296834766592 logging_writer.py:48] [130857] accumulated_eval_time=4226.95, accumulated_logging_time=15.6845, accumulated_submission_time=58196.6, global_step=130857, preemption_count=0, score=58196.6, test/accuracy=0.5826, test/loss=1.93416, test/num_examples=10000, total_duration=62453.4, train/accuracy=0.840202, train/loss=0.574346, validation/accuracy=0.70504, validation/loss=1.20188, validation/num_examples=50000
I0307 19:32:17.750869 140296826373888 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.1658287048339844, loss=1.2093027830123901
I0307 19:34:53.120637 140296834766592 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.0996415615081787, loss=1.0860928297042847
I0307 19:38:19.639588 140296826373888 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.01521635055542, loss=1.131413459777832
I0307 19:39:37.082079 140452179379392 spec.py:321] Evaluating on the training split.
I0307 19:39:48.604151 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 19:40:09.529381 140452179379392 spec.py:349] Evaluating on the test split.
I0307 19:40:11.281203 140452179379392 submission_runner.py:469] Time since start: 62999.57s, 	Step: 131146, 	{'train/accuracy': 0.8282844424247742, 'train/loss': 0.6296339631080627, 'validation/accuracy': 0.7112799882888794, 'validation/loss': 1.1911766529083252, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.923415184020996, 'test/num_examples': 10000, 'score': 58708.376480817795, 'total_duration': 62999.56728363037, 'accumulated_submission_time': 58708.376480817795, 'accumulated_eval_time': 4261.148334741592, 'accumulated_logging_time': 15.783037424087524}
I0307 19:40:11.331118 140296834766592 logging_writer.py:48] [131146] accumulated_eval_time=4261.15, accumulated_logging_time=15.783, accumulated_submission_time=58708.4, global_step=131146, preemption_count=0, score=58708.4, test/accuracy=0.5866, test/loss=1.92342, test/num_examples=10000, total_duration=62999.6, train/accuracy=0.828284, train/loss=0.629634, validation/accuracy=0.71128, validation/loss=1.19118, validation/num_examples=50000
I0307 19:41:13.418175 140296826373888 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.949894428253174, loss=1.01426362991333
I0307 19:43:01.815391 140296834766592 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.1928141117095947, loss=1.1465871334075928
I0307 19:44:21.745202 140296826373888 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.1706020832061768, loss=1.1953681707382202
I0307 19:45:10.717443 140296834766592 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.0950491428375244, loss=1.0916767120361328
I0307 19:45:59.604550 140296826373888 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.189171075820923, loss=1.2087128162384033
I0307 19:46:48.867341 140296834766592 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.286958694458008, loss=1.2398266792297363
I0307 19:48:44.061134 140452179379392 spec.py:321] Evaluating on the training split.
I0307 19:48:54.775626 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 19:49:13.504149 140452179379392 spec.py:349] Evaluating on the test split.
I0307 19:49:15.319033 140452179379392 submission_runner.py:469] Time since start: 63543.61s, 	Step: 131752, 	{'train/accuracy': 0.8253547549247742, 'train/loss': 0.6296873092651367, 'validation/accuracy': 0.7152999639511108, 'validation/loss': 1.1612296104431152, 'validation/num_examples': 50000, 'test/accuracy': 0.5857000350952148, 'test/loss': 1.8822414875030518, 'test/num_examples': 10000, 'score': 59221.03333187103, 'total_duration': 63543.605088710785, 'accumulated_submission_time': 59221.03333187103, 'accumulated_eval_time': 4292.406179428101, 'accumulated_logging_time': 15.840676307678223}
I0307 19:49:15.368024 140296826373888 logging_writer.py:48] [131752] accumulated_eval_time=4292.41, accumulated_logging_time=15.8407, accumulated_submission_time=59221, global_step=131752, preemption_count=0, score=59221, test/accuracy=0.5857, test/loss=1.88224, test/num_examples=10000, total_duration=63543.6, train/accuracy=0.825355, train/loss=0.629687, validation/accuracy=0.7153, validation/loss=1.16123, validation/num_examples=50000
I0307 19:52:28.185863 140296834766592 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.210381031036377, loss=1.0812572240829468
I0307 19:57:47.287444 140452179379392 spec.py:321] Evaluating on the training split.
I0307 19:57:58.200679 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 19:58:14.383856 140452179379392 spec.py:349] Evaluating on the test split.
I0307 19:58:16.167412 140452179379392 submission_runner.py:469] Time since start: 64084.45s, 	Step: 131876, 	{'train/accuracy': 0.8220663070678711, 'train/loss': 0.6354645490646362, 'validation/accuracy': 0.7161999940872192, 'validation/loss': 1.1676372289657593, 'validation/num_examples': 50000, 'test/accuracy': 0.5860000252723694, 'test/loss': 1.8891657590866089, 'test/num_examples': 10000, 'score': 59732.930132865906, 'total_duration': 64084.45346856117, 'accumulated_submission_time': 59732.930132865906, 'accumulated_eval_time': 4321.286098718643, 'accumulated_logging_time': 15.899031400680542}
I0307 19:58:16.190849 140296826373888 logging_writer.py:48] [131876] accumulated_eval_time=4321.29, accumulated_logging_time=15.899, accumulated_submission_time=59732.9, global_step=131876, preemption_count=0, score=59732.9, test/accuracy=0.586, test/loss=1.88917, test/num_examples=10000, total_duration=64084.5, train/accuracy=0.822066, train/loss=0.635465, validation/accuracy=0.7162, validation/loss=1.16764, validation/num_examples=50000
I0307 19:59:44.789056 140296834766592 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.92085862159729, loss=1.0469732284545898
I0307 20:03:11.742583 140296826373888 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.275526285171509, loss=1.118396520614624
I0307 20:05:02.725609 140296834766592 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.060166597366333, loss=1.1473839282989502
I0307 20:06:47.112604 140452179379392 spec.py:321] Evaluating on the training split.
I0307 20:06:58.503273 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 20:07:18.208612 140452179379392 spec.py:349] Evaluating on the test split.
I0307 20:07:19.950530 140452179379392 submission_runner.py:469] Time since start: 64628.24s, 	Step: 132196, 	{'train/accuracy': 0.8604312539100647, 'train/loss': 0.497049480676651, 'validation/accuracy': 0.7150200009346008, 'validation/loss': 1.166748285293579, 'validation/num_examples': 50000, 'test/accuracy': 0.5807000398635864, 'test/loss': 1.9246114492416382, 'test/num_examples': 10000, 'score': 60243.807784318924, 'total_duration': 64628.23658204079, 'accumulated_submission_time': 60243.807784318924, 'accumulated_eval_time': 4354.123971939087, 'accumulated_logging_time': 15.93018388748169}
I0307 20:07:20.029355 140296826373888 logging_writer.py:48] [132196] accumulated_eval_time=4354.12, accumulated_logging_time=15.9302, accumulated_submission_time=60243.8, global_step=132196, preemption_count=0, score=60243.8, test/accuracy=0.5807, test/loss=1.92461, test/num_examples=10000, total_duration=64628.2, train/accuracy=0.860431, train/loss=0.497049, validation/accuracy=0.71502, validation/loss=1.16675, validation/num_examples=50000
I0307 20:07:22.010069 140296834766592 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.2609658241271973, loss=1.040805459022522
I0307 20:08:17.527837 140296826373888 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.0941169261932373, loss=1.0732126235961914
I0307 20:09:15.351806 140296834766592 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.3034327030181885, loss=1.158676028251648
I0307 20:11:05.473672 140296826373888 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.361083745956421, loss=1.1106183528900146
I0307 20:13:20.531944 140296834766592 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.042412519454956, loss=1.0398950576782227
I0307 20:15:11.097735 140296826373888 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.0434725284576416, loss=1.0902950763702393
I0307 20:15:50.738966 140452179379392 spec.py:321] Evaluating on the training split.
I0307 20:16:01.684527 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 20:16:22.498523 140452179379392 spec.py:349] Evaluating on the test split.
I0307 20:16:24.293205 140452179379392 submission_runner.py:469] Time since start: 65172.58s, 	Step: 132737, 	{'train/accuracy': 0.8304169178009033, 'train/loss': 0.6038551330566406, 'validation/accuracy': 0.7142199873924255, 'validation/loss': 1.1793506145477295, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.9010155200958252, 'test/num_examples': 10000, 'score': 60754.42060017586, 'total_duration': 65172.579268693924, 'accumulated_submission_time': 60754.42060017586, 'accumulated_eval_time': 4387.678166389465, 'accumulated_logging_time': 16.046374797821045}
I0307 20:16:24.353528 140296834766592 logging_writer.py:48] [132737] accumulated_eval_time=4387.68, accumulated_logging_time=16.0464, accumulated_submission_time=60754.4, global_step=132737, preemption_count=0, score=60754.4, test/accuracy=0.5837, test/loss=1.90102, test/num_examples=10000, total_duration=65172.6, train/accuracy=0.830417, train/loss=0.603855, validation/accuracy=0.71422, validation/loss=1.17935, validation/num_examples=50000
I0307 20:20:24.993396 140296826373888 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.360447883605957, loss=1.2401955127716064
I0307 20:23:44.589276 140296834766592 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.3106610774993896, loss=1.2383344173431396
I0307 20:24:55.672487 140452179379392 spec.py:321] Evaluating on the training split.
I0307 20:25:06.861035 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 20:25:28.906093 140452179379392 spec.py:349] Evaluating on the test split.
I0307 20:25:30.727790 140452179379392 submission_runner.py:469] Time since start: 65719.01s, 	Step: 132946, 	{'train/accuracy': 0.8287029266357422, 'train/loss': 0.6000069975852966, 'validation/accuracy': 0.7139999866485596, 'validation/loss': 1.1776269674301147, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.920369267463684, 'test/num_examples': 10000, 'score': 61265.70809793472, 'total_duration': 65719.01386857033, 'accumulated_submission_time': 61265.70809793472, 'accumulated_eval_time': 4422.733438014984, 'accumulated_logging_time': 16.11443066596985}
I0307 20:25:30.752622 140296826373888 logging_writer.py:48] [132946] accumulated_eval_time=4422.73, accumulated_logging_time=16.1144, accumulated_submission_time=61265.7, global_step=132946, preemption_count=0, score=61265.7, test/accuracy=0.5878, test/loss=1.92037, test/num_examples=10000, total_duration=65719, train/accuracy=0.828703, train/loss=0.600007, validation/accuracy=0.714, validation/loss=1.17763, validation/num_examples=50000
I0307 20:26:45.773882 140296834766592 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.1067135334014893, loss=1.1207126379013062
I0307 20:29:36.465479 140296826373888 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.202930212020874, loss=1.062349796295166
I0307 20:32:10.073462 140296834766592 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.4440348148345947, loss=1.151991605758667
I0307 20:33:59.433605 140296826373888 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.1426141262054443, loss=1.1394990682601929
I0307 20:34:01.639237 140452179379392 spec.py:321] Evaluating on the training split.
I0307 20:34:13.041895 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 20:34:33.227765 140452179379392 spec.py:349] Evaluating on the test split.
I0307 20:34:34.990414 140452179379392 submission_runner.py:469] Time since start: 66263.28s, 	Step: 133303, 	{'train/accuracy': 0.826590359210968, 'train/loss': 0.6236283183097839, 'validation/accuracy': 0.7160999774932861, 'validation/loss': 1.1539772748947144, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 1.9088598489761353, 'test/num_examples': 10000, 'score': 61776.545063734055, 'total_duration': 66263.27648663521, 'accumulated_submission_time': 61776.545063734055, 'accumulated_eval_time': 4456.084577560425, 'accumulated_logging_time': 16.148447036743164}
I0307 20:34:35.038488 140296834766592 logging_writer.py:48] [133303] accumulated_eval_time=4456.08, accumulated_logging_time=16.1484, accumulated_submission_time=61776.5, global_step=133303, preemption_count=0, score=61776.5, test/accuracy=0.5867, test/loss=1.90886, test/num_examples=10000, total_duration=66263.3, train/accuracy=0.82659, train/loss=0.623628, validation/accuracy=0.7161, validation/loss=1.15398, validation/num_examples=50000
I0307 20:37:00.465387 140296826373888 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.4089548587799072, loss=1.192801594734192
I0307 20:40:22.972657 140296834766592 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.0944223403930664, loss=1.0508661270141602
I0307 20:41:54.853421 140296826373888 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.413682460784912, loss=1.1712521314620972
I0307 20:43:05.611351 140452179379392 spec.py:321] Evaluating on the training split.
I0307 20:43:17.065630 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 20:43:37.399843 140452179379392 spec.py:349] Evaluating on the test split.
I0307 20:43:39.154067 140452179379392 submission_runner.py:469] Time since start: 66807.44s, 	Step: 133680, 	{'train/accuracy': 0.8625239133834839, 'train/loss': 0.4914766848087311, 'validation/accuracy': 0.7106999754905701, 'validation/loss': 1.1898199319839478, 'validation/num_examples': 50000, 'test/accuracy': 0.5822000503540039, 'test/loss': 1.933200716972351, 'test/num_examples': 10000, 'score': 62287.067898750305, 'total_duration': 66807.44013214111, 'accumulated_submission_time': 62287.067898750305, 'accumulated_eval_time': 4489.6272485256195, 'accumulated_logging_time': 16.20473837852478}
I0307 20:43:39.217496 140296834766592 logging_writer.py:48] [133680] accumulated_eval_time=4489.63, accumulated_logging_time=16.2047, accumulated_submission_time=62287.1, global_step=133680, preemption_count=0, score=62287.1, test/accuracy=0.5822, test/loss=1.9332, test/num_examples=10000, total_duration=66807.4, train/accuracy=0.862524, train/loss=0.491477, validation/accuracy=0.7107, validation/loss=1.18982, validation/num_examples=50000
I0307 20:43:48.819884 140296826373888 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.328000068664551, loss=1.1213306188583374
I0307 20:50:34.842126 140296834766592 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.109077215194702, loss=1.137971043586731
I0307 20:52:10.489449 140452179379392 spec.py:321] Evaluating on the training split.
I0307 20:52:21.971579 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 20:52:42.350767 140452179379392 spec.py:349] Evaluating on the test split.
I0307 20:52:44.131340 140452179379392 submission_runner.py:469] Time since start: 67352.42s, 	Step: 133829, 	{'train/accuracy': 0.8446468114852905, 'train/loss': 0.5512420535087585, 'validation/accuracy': 0.7114399671554565, 'validation/loss': 1.1826568841934204, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9152286052703857, 'test/num_examples': 10000, 'score': 62798.26578402519, 'total_duration': 67352.4173719883, 'accumulated_submission_time': 62798.26578402519, 'accumulated_eval_time': 4523.269063472748, 'accumulated_logging_time': 16.32670831680298}
I0307 20:52:44.156986 140296826373888 logging_writer.py:48] [133829] accumulated_eval_time=4523.27, accumulated_logging_time=16.3267, accumulated_submission_time=62798.3, global_step=133829, preemption_count=0, score=62798.3, test/accuracy=0.5821, test/loss=1.91523, test/num_examples=10000, total_duration=67352.4, train/accuracy=0.844647, train/loss=0.551242, validation/accuracy=0.71144, validation/loss=1.18266, validation/num_examples=50000
I0307 20:54:18.359221 140296834766592 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.3178322315216064, loss=1.1808723211288452
I0307 20:56:42.396512 140296826373888 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.0596206188201904, loss=1.1850193738937378
I0307 20:59:06.883278 140296834766592 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.007624626159668, loss=1.072550654411316
I0307 21:00:56.204838 140296826373888 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.419260025024414, loss=1.134874939918518
I0307 21:01:14.635833 140452179379392 spec.py:321] Evaluating on the training split.
I0307 21:01:26.085974 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 21:01:43.767734 140452179379392 spec.py:349] Evaluating on the test split.
I0307 21:01:45.531267 140452179379392 submission_runner.py:469] Time since start: 67893.82s, 	Step: 134221, 	{'train/accuracy': 0.8385483026504517, 'train/loss': 0.5690528750419617, 'validation/accuracy': 0.7187199592590332, 'validation/loss': 1.158582329750061, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.9015141725540161, 'test/num_examples': 10000, 'score': 63308.693611860275, 'total_duration': 67893.81734800339, 'accumulated_submission_time': 63308.693611860275, 'accumulated_eval_time': 4554.164469718933, 'accumulated_logging_time': 16.360220193862915}
I0307 21:01:45.655416 140296834766592 logging_writer.py:48] [134221] accumulated_eval_time=4554.16, accumulated_logging_time=16.3602, accumulated_submission_time=63308.7, global_step=134221, preemption_count=0, score=63308.7, test/accuracy=0.5902, test/loss=1.90151, test/num_examples=10000, total_duration=67893.8, train/accuracy=0.838548, train/loss=0.569053, validation/accuracy=0.71872, validation/loss=1.15858, validation/num_examples=50000
I0307 21:02:48.925479 140296826373888 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.9621422290802, loss=1.0839654207229614
I0307 21:04:19.551313 140296834766592 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.210387706756592, loss=1.1029804944992065
I0307 21:07:08.354835 140296826373888 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.1912670135498047, loss=1.0314154624938965
I0307 21:10:15.921763 140452179379392 spec.py:321] Evaluating on the training split.
I0307 21:10:26.823570 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 21:10:52.240439 140452179379392 spec.py:349] Evaluating on the test split.
I0307 21:10:53.997384 140452179379392 submission_runner.py:469] Time since start: 68442.28s, 	Step: 134556, 	{'train/accuracy': 0.8328284025192261, 'train/loss': 0.5939211249351501, 'validation/accuracy': 0.719760000705719, 'validation/loss': 1.1464815139770508, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.8816057443618774, 'test/num_examples': 10000, 'score': 63818.90691781044, 'total_duration': 68442.28345990181, 'accumulated_submission_time': 63818.90691781044, 'accumulated_eval_time': 4592.240064620972, 'accumulated_logging_time': 16.50055241584778}
I0307 21:10:54.087992 140296834766592 logging_writer.py:48] [134556] accumulated_eval_time=4592.24, accumulated_logging_time=16.5006, accumulated_submission_time=63818.9, global_step=134556, preemption_count=0, score=63818.9, test/accuracy=0.5918, test/loss=1.88161, test/num_examples=10000, total_duration=68442.3, train/accuracy=0.832828, train/loss=0.593921, validation/accuracy=0.71976, validation/loss=1.14648, validation/num_examples=50000
I0307 21:13:44.115429 140296826373888 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.243157148361206, loss=1.0926908254623413
I0307 21:17:35.257620 140296834766592 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.963785171508789, loss=1.0151699781417847
I0307 21:19:25.243666 140452179379392 spec.py:321] Evaluating on the training split.
I0307 21:19:37.001689 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 21:19:55.601875 140452179379392 spec.py:349] Evaluating on the test split.
I0307 21:19:57.343950 140452179379392 submission_runner.py:469] Time since start: 68985.63s, 	Step: 134777, 	{'train/accuracy': 0.8325294852256775, 'train/loss': 0.6018131971359253, 'validation/accuracy': 0.713919997215271, 'validation/loss': 1.15029776096344, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.8818572759628296, 'test/num_examples': 10000, 'score': 64330.012323856354, 'total_duration': 68985.6300046444, 'accumulated_submission_time': 64330.012323856354, 'accumulated_eval_time': 4624.340298175812, 'accumulated_logging_time': 16.61750292778015}
I0307 21:19:57.371205 140296826373888 logging_writer.py:48] [134777] accumulated_eval_time=4624.34, accumulated_logging_time=16.6175, accumulated_submission_time=64330, global_step=134777, preemption_count=0, score=64330, test/accuracy=0.5914, test/loss=1.88186, test/num_examples=10000, total_duration=68985.6, train/accuracy=0.832529, train/loss=0.601813, validation/accuracy=0.71392, validation/loss=1.1503, validation/num_examples=50000
I0307 21:20:17.325465 140296834766592 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.155656099319458, loss=1.0233920812606812
I0307 21:23:37.052413 140296826373888 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.0733485221862793, loss=1.09803307056427
I0307 21:27:13.262875 140296834766592 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.3013408184051514, loss=1.0108106136322021
I0307 21:28:29.053336 140452179379392 spec.py:321] Evaluating on the training split.
I0307 21:28:40.260117 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 21:29:00.179467 140452179379392 spec.py:349] Evaluating on the test split.
I0307 21:29:01.936429 140452179379392 submission_runner.py:469] Time since start: 69530.22s, 	Step: 135035, 	{'train/accuracy': 0.8338648080825806, 'train/loss': 0.5892207622528076, 'validation/accuracy': 0.7191599607467651, 'validation/loss': 1.1520665884017944, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.893405556678772, 'test/num_examples': 10000, 'score': 64841.657054424286, 'total_duration': 69530.22250390053, 'accumulated_submission_time': 64841.657054424286, 'accumulated_eval_time': 4657.223359823227, 'accumulated_logging_time': 16.654006481170654}
I0307 21:29:02.063421 140296826373888 logging_writer.py:48] [135035] accumulated_eval_time=4657.22, accumulated_logging_time=16.654, accumulated_submission_time=64841.7, global_step=135035, preemption_count=0, score=64841.7, test/accuracy=0.5923, test/loss=1.89341, test/num_examples=10000, total_duration=69530.2, train/accuracy=0.833865, train/loss=0.589221, validation/accuracy=0.71916, validation/loss=1.15207, validation/num_examples=50000
I0307 21:31:13.122284 140296834766592 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.4518072605133057, loss=1.140129804611206
I0307 21:34:46.254646 140296826373888 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.0307061672210693, loss=1.0763201713562012
I0307 21:37:33.425013 140452179379392 spec.py:321] Evaluating on the training split.
I0307 21:37:44.679914 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 21:38:01.231098 140452179379392 spec.py:349] Evaluating on the test split.
I0307 21:38:02.988449 140452179379392 submission_runner.py:469] Time since start: 70071.27s, 	Step: 135280, 	{'train/accuracy': 0.8502271771430969, 'train/loss': 0.5344494581222534, 'validation/accuracy': 0.7087399959564209, 'validation/loss': 1.1960666179656982, 'validation/num_examples': 50000, 'test/accuracy': 0.5786000490188599, 'test/loss': 1.9512779712677002, 'test/num_examples': 10000, 'score': 65352.9531788826, 'total_duration': 70071.27452516556, 'accumulated_submission_time': 65352.9531788826, 'accumulated_eval_time': 4686.78676700592, 'accumulated_logging_time': 16.818994283676147}
I0307 21:38:03.014241 140296834766592 logging_writer.py:48] [135280] accumulated_eval_time=4686.79, accumulated_logging_time=16.819, accumulated_submission_time=65353, global_step=135280, preemption_count=0, score=65353, test/accuracy=0.5786, test/loss=1.95128, test/num_examples=10000, total_duration=70071.3, train/accuracy=0.850227, train/loss=0.534449, validation/accuracy=0.70874, validation/loss=1.19607, validation/num_examples=50000
I0307 21:38:29.221012 140296826373888 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.1467137336730957, loss=1.106257677078247
I0307 21:42:01.685921 140296834766592 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.6791346073150635, loss=1.0703150033950806
I0307 21:45:37.059714 140296826373888 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.3688125610351562, loss=1.102612853050232
I0307 21:46:34.210329 140452179379392 spec.py:321] Evaluating on the training split.
I0307 21:46:45.454260 140452179379392 spec.py:333] Evaluating on the validation split.
I0307 21:47:04.209335 140452179379392 spec.py:349] Evaluating on the test split.
I0307 21:47:06.028768 140452179379392 submission_runner.py:469] Time since start: 70614.31s, 	Step: 135528, 	{'train/accuracy': 0.8508848547935486, 'train/loss': 0.533978283405304, 'validation/accuracy': 0.7188999652862549, 'validation/loss': 1.1550242900848389, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.8896461725234985, 'test/num_examples': 10000, 'score': 65864.11411261559, 'total_duration': 70614.31483054161, 'accumulated_submission_time': 65864.11411261559, 'accumulated_eval_time': 4718.605162143707, 'accumulated_logging_time': 16.852794647216797}
I0307 21:47:06.101557 140296834766592 logging_writer.py:48] [135528] accumulated_eval_time=4718.61, accumulated_logging_time=16.8528, accumulated_submission_time=65864.1, global_step=135528, preemption_count=0, score=65864.1, test/accuracy=0.5952, test/loss=1.88965, test/num_examples=10000, total_duration=70614.3, train/accuracy=0.850885, train/loss=0.533978, validation/accuracy=0.7189, validation/loss=1.15502, validation/num_examples=50000
I0307 21:49:23.331004 140296826373888 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.1308083534240723, loss=1.0449192523956299
I0307 21:52:55.185950 140296834766592 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.339801073074341, loss=1.1656417846679688
I0307 21:55:36.252386 140296826373888 logging_writer.py:48] [135776] global_step=135776, preemption_count=0, score=66374.2
I0307 21:55:38.202421 140452179379392 submission_runner.py:646] Tuning trial 5/5
I0307 21:55:38.226680 140452179379392 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0307 21:55:38.230612 140452179379392 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.001235650503076613, 'train/loss': 6.912410736083984, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.912078380584717, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.912538051605225, 'test/num_examples': 10000, 'score': 58.335450649261475, 'total_duration': 198.26079630851746, 'accumulated_submission_time': 58.335450649261475, 'accumulated_eval_time': 139.9251353740692, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1356, {'train/accuracy': 0.15836256742477417, 'train/loss': 4.316074848175049, 'validation/accuracy': 0.13161998987197876, 'validation/loss': 4.559778213500977, 'validation/num_examples': 50000, 'test/accuracy': 0.10210000723600388, 'test/loss': 4.961103916168213, 'test/num_examples': 10000, 'score': 568.1441419124603, 'total_duration': 741.6363072395325, 'accumulated_submission_time': 568.1441419124603, 'accumulated_eval_time': 173.22100186347961, 'accumulated_logging_time': 0.06844806671142578, 'global_step': 1356, 'preemption_count': 0}), (2706, {'train/accuracy': 0.3202327787876129, 'train/loss': 3.1716320514678955, 'validation/accuracy': 0.27947998046875, 'validation/loss': 3.450259208679199, 'validation/num_examples': 50000, 'test/accuracy': 0.21220001578330994, 'test/loss': 4.076381683349609, 'test/num_examples': 10000, 'score': 1078.233915567398, 'total_duration': 1287.6449422836304, 'accumulated_submission_time': 1078.233915567398, 'accumulated_eval_time': 208.88624215126038, 'accumulated_logging_time': 0.13827133178710938, 'global_step': 2706, 'preemption_count': 0}), (4034, {'train/accuracy': 0.4220742881298065, 'train/loss': 2.564941167831421, 'validation/accuracy': 0.3745799958705902, 'validation/loss': 2.85689640045166, 'validation/num_examples': 50000, 'test/accuracy': 0.2808000147342682, 'test/loss': 3.550607204437256, 'test/num_examples': 10000, 'score': 1588.4089176654816, 'total_duration': 1834.1051959991455, 'accumulated_submission_time': 1588.4089176654816, 'accumulated_eval_time': 244.98796010017395, 'accumulated_logging_time': 0.16879653930664062, 'global_step': 4034, 'preemption_count': 0}), (5354, {'train/accuracy': 0.5254504084587097, 'train/loss': 2.0259480476379395, 'validation/accuracy': 0.4670799970626831, 'validation/loss': 2.3427321910858154, 'validation/num_examples': 50000, 'test/accuracy': 0.35920003056526184, 'test/loss': 3.03536057472229, 'test/num_examples': 10000, 'score': 2098.2202339172363, 'total_duration': 2380.4187211990356, 'accumulated_submission_time': 2098.2202339172363, 'accumulated_eval_time': 281.28122782707214, 'accumulated_logging_time': 0.22119140625, 'global_step': 5354, 'preemption_count': 0}), (6679, {'train/accuracy': 0.5712491869926453, 'train/loss': 1.7849279642105103, 'validation/accuracy': 0.5128600001335144, 'validation/loss': 2.0951907634735107, 'validation/num_examples': 50000, 'test/accuracy': 0.3945000171661377, 'test/loss': 2.8610944747924805, 'test/num_examples': 10000, 'score': 2608.0634620189667, 'total_duration': 2923.8775482177734, 'accumulated_submission_time': 2608.0634620189667, 'accumulated_eval_time': 314.70203375816345, 'accumulated_logging_time': 0.292834997177124, 'global_step': 6679, 'preemption_count': 0}), (8002, {'train/accuracy': 0.5975366830825806, 'train/loss': 1.6618019342422485, 'validation/accuracy': 0.535860002040863, 'validation/loss': 1.997391700744629, 'validation/num_examples': 50000, 'test/accuracy': 0.4132000207901001, 'test/loss': 2.7312655448913574, 'test/num_examples': 10000, 'score': 3118.2666482925415, 'total_duration': 3469.707709789276, 'accumulated_submission_time': 3118.2666482925415, 'accumulated_eval_time': 350.1438443660736, 'accumulated_logging_time': 0.34224510192871094, 'global_step': 8002, 'preemption_count': 0}), (9322, {'train/accuracy': 0.6185826063156128, 'train/loss': 1.5447943210601807, 'validation/accuracy': 0.5566799640655518, 'validation/loss': 1.8915765285491943, 'validation/num_examples': 50000, 'test/accuracy': 0.43790000677108765, 'test/loss': 2.6292927265167236, 'test/num_examples': 10000, 'score': 3628.397201538086, 'total_duration': 4013.5305309295654, 'accumulated_submission_time': 3628.397201538086, 'accumulated_eval_time': 383.65606713294983, 'accumulated_logging_time': 0.3810913562774658, 'global_step': 9322, 'preemption_count': 0}), (10641, {'train/accuracy': 0.6275510191917419, 'train/loss': 1.5131089687347412, 'validation/accuracy': 0.5677599906921387, 'validation/loss': 1.8413199186325073, 'validation/num_examples': 50000, 'test/accuracy': 0.44450002908706665, 'test/loss': 2.5470688343048096, 'test/num_examples': 10000, 'score': 4138.229695558548, 'total_duration': 4562.209621667862, 'accumulated_submission_time': 4138.229695558548, 'accumulated_eval_time': 422.3262388706207, 'accumulated_logging_time': 0.41465187072753906, 'global_step': 10641, 'preemption_count': 0}), (11967, {'train/accuracy': 0.6413424611091614, 'train/loss': 1.4538108110427856, 'validation/accuracy': 0.5785199999809265, 'validation/loss': 1.7881860733032227, 'validation/num_examples': 50000, 'test/accuracy': 0.4458000063896179, 'test/loss': 2.533802032470703, 'test/num_examples': 10000, 'score': 4648.0253529548645, 'total_duration': 5106.446059465408, 'accumulated_submission_time': 4648.0253529548645, 'accumulated_eval_time': 456.53600883483887, 'accumulated_logging_time': 0.5018694400787354, 'global_step': 11967, 'preemption_count': 0}), (13214, {'train/accuracy': 0.6535594463348389, 'train/loss': 1.3951847553253174, 'validation/accuracy': 0.5795400142669678, 'validation/loss': 1.775004506111145, 'validation/num_examples': 50000, 'test/accuracy': 0.4613000154495239, 'test/loss': 2.5031328201293945, 'test/num_examples': 10000, 'score': 5158.053788900375, 'total_duration': 5655.876765489578, 'accumulated_submission_time': 5158.053788900375, 'accumulated_eval_time': 495.7233021259308, 'accumulated_logging_time': 0.5809483528137207, 'global_step': 13214, 'preemption_count': 0}), (14530, {'train/accuracy': 0.6630261540412903, 'train/loss': 1.3424205780029297, 'validation/accuracy': 0.5957199931144714, 'validation/loss': 1.698004126548767, 'validation/num_examples': 50000, 'test/accuracy': 0.4620000123977661, 'test/loss': 2.4951677322387695, 'test/num_examples': 10000, 'score': 5667.884410381317, 'total_duration': 6209.514318704605, 'accumulated_submission_time': 5667.884410381317, 'accumulated_eval_time': 539.2782566547394, 'accumulated_logging_time': 0.6986403465270996, 'global_step': 14530, 'preemption_count': 0}), (15850, {'train/accuracy': 0.6654974222183228, 'train/loss': 1.334952473640442, 'validation/accuracy': 0.5925599932670593, 'validation/loss': 1.700812816619873, 'validation/num_examples': 50000, 'test/accuracy': 0.4716000258922577, 'test/loss': 2.4369471073150635, 'test/num_examples': 10000, 'score': 6177.715204238892, 'total_duration': 6763.684148788452, 'accumulated_submission_time': 6177.715204238892, 'accumulated_eval_time': 583.313392162323, 'accumulated_logging_time': 0.8645684719085693, 'global_step': 15850, 'preemption_count': 0}), (17169, {'train/accuracy': 0.6803850531578064, 'train/loss': 1.269286036491394, 'validation/accuracy': 0.6047199964523315, 'validation/loss': 1.64718759059906, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.3867063522338867, 'test/num_examples': 10000, 'score': 6687.653282165527, 'total_duration': 7314.831635951996, 'accumulated_submission_time': 6687.653282165527, 'accumulated_eval_time': 624.2335669994354, 'accumulated_logging_time': 1.0213801860809326, 'global_step': 17169, 'preemption_count': 0}), (18483, {'train/accuracy': 0.663504421710968, 'train/loss': 1.3375612497329712, 'validation/accuracy': 0.5932799577713013, 'validation/loss': 1.7013968229293823, 'validation/num_examples': 50000, 'test/accuracy': 0.46560001373291016, 'test/loss': 2.4728827476501465, 'test/num_examples': 10000, 'score': 7197.45038151741, 'total_duration': 7865.477244615555, 'accumulated_submission_time': 7197.45038151741, 'accumulated_eval_time': 664.8295745849609, 'accumulated_logging_time': 1.144235610961914, 'global_step': 18483, 'preemption_count': 0}), (19797, {'train/accuracy': 0.6736088991165161, 'train/loss': 1.3136613368988037, 'validation/accuracy': 0.6010800004005432, 'validation/loss': 1.6835243701934814, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.4559056758880615, 'test/num_examples': 10000, 'score': 7707.508712053299, 'total_duration': 8413.045548200607, 'accumulated_submission_time': 7707.508712053299, 'accumulated_eval_time': 702.173793554306, 'accumulated_logging_time': 1.1776418685913086, 'global_step': 19797, 'preemption_count': 0}), (21113, {'train/accuracy': 0.683613657951355, 'train/loss': 1.255070447921753, 'validation/accuracy': 0.608680009841919, 'validation/loss': 1.6251435279846191, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.3887507915496826, 'test/num_examples': 10000, 'score': 8217.593552827835, 'total_duration': 8962.626177310944, 'accumulated_submission_time': 8217.593552827835, 'accumulated_eval_time': 741.5040574073792, 'accumulated_logging_time': 1.2053511142730713, 'global_step': 21113, 'preemption_count': 0}), (22427, {'train/accuracy': 0.6825374364852905, 'train/loss': 1.243003010749817, 'validation/accuracy': 0.6122199892997742, 'validation/loss': 1.6221086978912354, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.3713154792785645, 'test/num_examples': 10000, 'score': 8727.54271864891, 'total_duration': 9509.988741397858, 'accumulated_submission_time': 8727.54271864891, 'accumulated_eval_time': 778.6537783145905, 'accumulated_logging_time': 1.337156057357788, 'global_step': 22427, 'preemption_count': 0}), (23737, {'train/accuracy': 0.6640226244926453, 'train/loss': 1.3183033466339111, 'validation/accuracy': 0.6015399694442749, 'validation/loss': 1.6845316886901855, 'validation/num_examples': 50000, 'test/accuracy': 0.4718000292778015, 'test/loss': 2.4605982303619385, 'test/num_examples': 10000, 'score': 9237.619418621063, 'total_duration': 10061.630222797394, 'accumulated_submission_time': 9237.619418621063, 'accumulated_eval_time': 820.0134935379028, 'accumulated_logging_time': 1.3854548931121826, 'global_step': 23737, 'preemption_count': 0}), (25048, {'train/accuracy': 0.6875, 'train/loss': 1.2308251857757568, 'validation/accuracy': 0.614359974861145, 'validation/loss': 1.6169533729553223, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.381502389907837, 'test/num_examples': 10000, 'score': 9747.735475063324, 'total_duration': 10611.026360988617, 'accumulated_submission_time': 9747.735475063324, 'accumulated_eval_time': 859.0435960292816, 'accumulated_logging_time': 1.5049934387207031, 'global_step': 25048, 'preemption_count': 0}), (26365, {'train/accuracy': 0.6862643361091614, 'train/loss': 1.2357330322265625, 'validation/accuracy': 0.6197599768638611, 'validation/loss': 1.5840685367584229, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.346871852874756, 'test/num_examples': 10000, 'score': 10257.817177295685, 'total_duration': 11160.032019376755, 'accumulated_submission_time': 10257.817177295685, 'accumulated_eval_time': 897.7494268417358, 'accumulated_logging_time': 1.596116304397583, 'global_step': 26365, 'preemption_count': 0}), (27678, {'train/accuracy': 0.6795479655265808, 'train/loss': 1.259886622428894, 'validation/accuracy': 0.6112200021743774, 'validation/loss': 1.614196538925171, 'validation/num_examples': 50000, 'test/accuracy': 0.48750001192092896, 'test/loss': 2.3494980335235596, 'test/num_examples': 10000, 'score': 10767.557693481445, 'total_duration': 11713.418690443039, 'accumulated_submission_time': 10767.557693481445, 'accumulated_eval_time': 940.801164150238, 'accumulated_logging_time': 2.0594067573547363, 'global_step': 27678, 'preemption_count': 0}), (28993, {'train/accuracy': 0.6900709271430969, 'train/loss': 1.2107036113739014, 'validation/accuracy': 0.6157400012016296, 'validation/loss': 1.582605004310608, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.3410840034484863, 'test/num_examples': 10000, 'score': 11277.26110458374, 'total_duration': 12262.281204938889, 'accumulated_submission_time': 11277.26110458374, 'accumulated_eval_time': 979.6432375907898, 'accumulated_logging_time': 2.244678497314453, 'global_step': 28993, 'preemption_count': 0}), (30312, {'train/accuracy': 0.6761399507522583, 'train/loss': 1.2695378065109253, 'validation/accuracy': 0.6057800054550171, 'validation/loss': 1.6360821723937988, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.408809185028076, 'test/num_examples': 10000, 'score': 11787.274441719055, 'total_duration': 12813.681232213974, 'accumulated_submission_time': 11787.274441719055, 'accumulated_eval_time': 1020.7464399337769, 'accumulated_logging_time': 2.3946149349212646, 'global_step': 30312, 'preemption_count': 0}), (31623, {'train/accuracy': 0.6651785373687744, 'train/loss': 1.3193256855010986, 'validation/accuracy': 0.600879967212677, 'validation/loss': 1.6765329837799072, 'validation/num_examples': 50000, 'test/accuracy': 0.4807000160217285, 'test/loss': 2.4096968173980713, 'test/num_examples': 10000, 'score': 12297.177497386932, 'total_duration': 13359.900551080704, 'accumulated_submission_time': 12297.177497386932, 'accumulated_eval_time': 1056.7856845855713, 'accumulated_logging_time': 2.5392894744873047, 'global_step': 31623, 'preemption_count': 0}), (32936, {'train/accuracy': 0.6986008882522583, 'train/loss': 1.179189682006836, 'validation/accuracy': 0.6294199824333191, 'validation/loss': 1.5444709062576294, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.267225980758667, 'test/num_examples': 10000, 'score': 12806.887058734894, 'total_duration': 13912.337742328644, 'accumulated_submission_time': 12806.887058734894, 'accumulated_eval_time': 1099.1043810844421, 'accumulated_logging_time': 2.818448543548584, 'global_step': 32936, 'preemption_count': 0}), (34251, {'train/accuracy': 0.7006337642669678, 'train/loss': 1.170133113861084, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.5177743434906006, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.249415636062622, 'test/num_examples': 10000, 'score': 13316.700063467026, 'total_duration': 14459.872414112091, 'accumulated_submission_time': 13316.700063467026, 'accumulated_eval_time': 1136.5578255653381, 'accumulated_logging_time': 2.9557945728302, 'global_step': 34251, 'preemption_count': 0}), (35515, {'train/accuracy': 0.7026865482330322, 'train/loss': 1.16473388671875, 'validation/accuracy': 0.6310999989509583, 'validation/loss': 1.533669352531433, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.2530882358551025, 'test/num_examples': 10000, 'score': 13826.795982599258, 'total_duration': 15005.930996418, 'accumulated_submission_time': 13826.795982599258, 'accumulated_eval_time': 1172.252869606018, 'accumulated_logging_time': 3.0915355682373047, 'global_step': 35515, 'preemption_count': 0}), (36832, {'train/accuracy': 0.687898576259613, 'train/loss': 1.2164710760116577, 'validation/accuracy': 0.6231399774551392, 'validation/loss': 1.5651354789733887, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.3190078735351562, 'test/num_examples': 10000, 'score': 14336.873234272003, 'total_duration': 15553.769481658936, 'accumulated_submission_time': 14336.873234272003, 'accumulated_eval_time': 1209.7503817081451, 'accumulated_logging_time': 3.2222983837127686, 'global_step': 36832, 'preemption_count': 0}), (38145, {'train/accuracy': 0.6995575428009033, 'train/loss': 1.167974829673767, 'validation/accuracy': 0.6320599913597107, 'validation/loss': 1.5250407457351685, 'validation/num_examples': 50000, 'test/accuracy': 0.5016000270843506, 'test/loss': 2.303520679473877, 'test/num_examples': 10000, 'score': 14847.014067173004, 'total_duration': 16099.796370506287, 'accumulated_submission_time': 14847.014067173004, 'accumulated_eval_time': 1245.3764162063599, 'accumulated_logging_time': 3.3506453037261963, 'global_step': 38145, 'preemption_count': 0}), (39460, {'train/accuracy': 0.7017298936843872, 'train/loss': 1.1635953187942505, 'validation/accuracy': 0.6313799619674683, 'validation/loss': 1.5133898258209229, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.2279975414276123, 'test/num_examples': 10000, 'score': 15357.014038801193, 'total_duration': 16644.854928970337, 'accumulated_submission_time': 15357.014038801193, 'accumulated_eval_time': 1280.200347661972, 'accumulated_logging_time': 3.4531116485595703, 'global_step': 39460, 'preemption_count': 0}), (40776, {'train/accuracy': 0.7074697017669678, 'train/loss': 1.1410713195800781, 'validation/accuracy': 0.6385999917984009, 'validation/loss': 1.4968065023422241, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.2737174034118652, 'test/num_examples': 10000, 'score': 15866.974277973175, 'total_duration': 17192.385166168213, 'accumulated_submission_time': 15866.974277973175, 'accumulated_eval_time': 1317.5174612998962, 'accumulated_logging_time': 3.570793628692627, 'global_step': 40776, 'preemption_count': 0}), (42092, {'train/accuracy': 0.7047791481018066, 'train/loss': 1.147672414779663, 'validation/accuracy': 0.6356199979782104, 'validation/loss': 1.5010168552398682, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.2536654472351074, 'test/num_examples': 10000, 'score': 16377.061843633652, 'total_duration': 17736.765770435333, 'accumulated_submission_time': 16377.061843633652, 'accumulated_eval_time': 1351.5210099220276, 'accumulated_logging_time': 3.725583553314209, 'global_step': 42092, 'preemption_count': 0}), (43404, {'train/accuracy': 0.6994379758834839, 'train/loss': 1.1790796518325806, 'validation/accuracy': 0.6329599618911743, 'validation/loss': 1.5107394456863403, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2469840049743652, 'test/num_examples': 10000, 'score': 16886.80089879036, 'total_duration': 18282.140962839127, 'accumulated_submission_time': 16886.80089879036, 'accumulated_eval_time': 1386.87864279747, 'accumulated_logging_time': 3.8720855712890625, 'global_step': 43404, 'preemption_count': 0}), (44718, {'train/accuracy': 0.703125, 'train/loss': 1.159155249595642, 'validation/accuracy': 0.6333799958229065, 'validation/loss': 1.5013694763183594, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2602763175964355, 'test/num_examples': 10000, 'score': 17396.885651111603, 'total_duration': 18826.64243364334, 'accumulated_submission_time': 17396.885651111603, 'accumulated_eval_time': 1421.0667879581451, 'accumulated_logging_time': 3.9691731929779053, 'global_step': 44718, 'preemption_count': 0}), (46028, {'train/accuracy': 0.712332546710968, 'train/loss': 1.1168365478515625, 'validation/accuracy': 0.6403200030326843, 'validation/loss': 1.4838718175888062, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.2062296867370605, 'test/num_examples': 10000, 'score': 17907.0224776268, 'total_duration': 19374.694994211197, 'accumulated_submission_time': 17907.0224776268, 'accumulated_eval_time': 1458.7437987327576, 'accumulated_logging_time': 4.0757880210876465, 'global_step': 46028, 'preemption_count': 0}), (47339, {'train/accuracy': 0.7066525816917419, 'train/loss': 1.1429005861282349, 'validation/accuracy': 0.6362999677658081, 'validation/loss': 1.5029296875, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2452163696289062, 'test/num_examples': 10000, 'score': 18416.830828666687, 'total_duration': 19917.413342237473, 'accumulated_submission_time': 18416.830828666687, 'accumulated_eval_time': 1491.3927488327026, 'accumulated_logging_time': 4.203917503356934, 'global_step': 47339, 'preemption_count': 0}), (48653, {'train/accuracy': 0.7029057741165161, 'train/loss': 1.1506329774856567, 'validation/accuracy': 0.6337199807167053, 'validation/loss': 1.5094141960144043, 'validation/num_examples': 50000, 'test/accuracy': 0.5008000135421753, 'test/loss': 2.2742104530334473, 'test/num_examples': 10000, 'score': 18926.651203632355, 'total_duration': 20462.864921569824, 'accumulated_submission_time': 18926.651203632355, 'accumulated_eval_time': 1526.6999325752258, 'accumulated_logging_time': 4.396802186965942, 'global_step': 48653, 'preemption_count': 0}), (49968, {'train/accuracy': 0.7259646058082581, 'train/loss': 1.0541446208953857, 'validation/accuracy': 0.6535800099372864, 'validation/loss': 1.4165523052215576, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.1341519355773926, 'test/num_examples': 10000, 'score': 19436.52112555504, 'total_duration': 21005.736488103867, 'accumulated_submission_time': 19436.52112555504, 'accumulated_eval_time': 1559.4470360279083, 'accumulated_logging_time': 4.521208047866821, 'global_step': 49968, 'preemption_count': 0}), (51282, {'train/accuracy': 0.72074294090271, 'train/loss': 1.0786815881729126, 'validation/accuracy': 0.6505599617958069, 'validation/loss': 1.441593885421753, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.1758639812469482, 'test/num_examples': 10000, 'score': 19946.383144140244, 'total_duration': 21553.74187040329, 'accumulated_submission_time': 19946.383144140244, 'accumulated_eval_time': 1597.3484959602356, 'accumulated_logging_time': 4.6324474811553955, 'global_step': 51282, 'preemption_count': 0}), (52601, {'train/accuracy': 0.7106186151504517, 'train/loss': 1.1249427795410156, 'validation/accuracy': 0.6447399854660034, 'validation/loss': 1.472260594367981, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.229464292526245, 'test/num_examples': 10000, 'score': 20456.412138223648, 'total_duration': 22098.179429531097, 'accumulated_submission_time': 20456.412138223648, 'accumulated_eval_time': 1631.487529039383, 'accumulated_logging_time': 4.767784118652344, 'global_step': 52601, 'preemption_count': 0}), (53913, {'train/accuracy': 0.709980845451355, 'train/loss': 1.1241072416305542, 'validation/accuracy': 0.6460399627685547, 'validation/loss': 1.4633657932281494, 'validation/num_examples': 50000, 'test/accuracy': 0.5040000081062317, 'test/loss': 2.248199224472046, 'test/num_examples': 10000, 'score': 20966.365412950516, 'total_duration': 22639.76716518402, 'accumulated_submission_time': 20966.365412950516, 'accumulated_eval_time': 1662.837821483612, 'accumulated_logging_time': 4.918170690536499, 'global_step': 53913, 'preemption_count': 0}), (55226, {'train/accuracy': 0.6990792155265808, 'train/loss': 1.1553237438201904, 'validation/accuracy': 0.6372399926185608, 'validation/loss': 1.5018441677093506, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.290228843688965, 'test/num_examples': 10000, 'score': 21476.42167043686, 'total_duration': 23186.950947523117, 'accumulated_submission_time': 21476.42167043686, 'accumulated_eval_time': 1699.7368094921112, 'accumulated_logging_time': 5.015252113342285, 'global_step': 55226, 'preemption_count': 0}), (56538, {'train/accuracy': 0.7137675285339355, 'train/loss': 1.1085933446884155, 'validation/accuracy': 0.6496999859809875, 'validation/loss': 1.4519292116165161, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2400569915771484, 'test/num_examples': 10000, 'score': 21986.30232143402, 'total_duration': 23733.25483226776, 'accumulated_submission_time': 21986.30232143402, 'accumulated_eval_time': 1735.87916970253, 'accumulated_logging_time': 5.163467645645142, 'global_step': 56538, 'preemption_count': 0}), (57855, {'train/accuracy': 0.7209821343421936, 'train/loss': 1.0657283067703247, 'validation/accuracy': 0.6507399678230286, 'validation/loss': 1.4275306463241577, 'validation/num_examples': 50000, 'test/accuracy': 0.5227000117301941, 'test/loss': 2.159003734588623, 'test/num_examples': 10000, 'score': 22496.30607318878, 'total_duration': 24280.048092842102, 'accumulated_submission_time': 22496.30607318878, 'accumulated_eval_time': 1772.3261406421661, 'accumulated_logging_time': 5.374384641647339, 'global_step': 57855, 'preemption_count': 0}), (59161, {'train/accuracy': 0.7261240482330322, 'train/loss': 1.0557602643966675, 'validation/accuracy': 0.6517800092697144, 'validation/loss': 1.4288057088851929, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1769392490386963, 'test/num_examples': 10000, 'score': 23006.28792977333, 'total_duration': 24824.071602344513, 'accumulated_submission_time': 23006.28792977333, 'accumulated_eval_time': 1806.062062740326, 'accumulated_logging_time': 5.546006441116333, 'global_step': 59161, 'preemption_count': 0}), (60473, {'train/accuracy': 0.7180524468421936, 'train/loss': 1.1005436182022095, 'validation/accuracy': 0.6475399732589722, 'validation/loss': 1.4441503286361694, 'validation/num_examples': 50000, 'test/accuracy': 0.5220000147819519, 'test/loss': 2.1754355430603027, 'test/num_examples': 10000, 'score': 23516.03226876259, 'total_duration': 25373.291135311127, 'accumulated_submission_time': 23516.03226876259, 'accumulated_eval_time': 1845.2517132759094, 'accumulated_logging_time': 5.698989152908325, 'global_step': 60473, 'preemption_count': 0}), (61783, {'train/accuracy': 0.7188097834587097, 'train/loss': 1.0719362497329712, 'validation/accuracy': 0.6545599699020386, 'validation/loss': 1.4217674732208252, 'validation/num_examples': 50000, 'test/accuracy': 0.5300000309944153, 'test/loss': 2.145974636077881, 'test/num_examples': 10000, 'score': 24026.048491477966, 'total_duration': 25918.64058828354, 'accumulated_submission_time': 24026.048491477966, 'accumulated_eval_time': 1880.2882618904114, 'accumulated_logging_time': 5.862671613693237, 'global_step': 61783, 'preemption_count': 0}), (63095, {'train/accuracy': 0.7278180718421936, 'train/loss': 1.0357601642608643, 'validation/accuracy': 0.6565399765968323, 'validation/loss': 1.404319167137146, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.1122193336486816, 'test/num_examples': 10000, 'score': 24535.905178308487, 'total_duration': 26461.55972480774, 'accumulated_submission_time': 24535.905178308487, 'accumulated_eval_time': 1913.1020357608795, 'accumulated_logging_time': 5.981022357940674, 'global_step': 63095, 'preemption_count': 0}), (64406, {'train/accuracy': 0.7185905575752258, 'train/loss': 1.093408465385437, 'validation/accuracy': 0.6451799869537354, 'validation/loss': 1.4653292894363403, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.2412753105163574, 'test/num_examples': 10000, 'score': 25045.75627207756, 'total_duration': 27003.83189702034, 'accumulated_submission_time': 25045.75627207756, 'accumulated_eval_time': 1945.2322680950165, 'accumulated_logging_time': 6.138960599899292, 'global_step': 64406, 'preemption_count': 0}), (65714, {'train/accuracy': 0.7205237150192261, 'train/loss': 1.073755145072937, 'validation/accuracy': 0.6462199687957764, 'validation/loss': 1.4506008625030518, 'validation/num_examples': 50000, 'test/accuracy': 0.516800045967102, 'test/loss': 2.1902425289154053, 'test/num_examples': 10000, 'score': 25555.54702115059, 'total_duration': 27548.867127895355, 'accumulated_submission_time': 25555.54702115059, 'accumulated_eval_time': 1980.1726524829865, 'accumulated_logging_time': 6.310142517089844, 'global_step': 65714, 'preemption_count': 0}), (67024, {'train/accuracy': 0.715262234210968, 'train/loss': 1.1018823385238647, 'validation/accuracy': 0.6509799957275391, 'validation/loss': 1.4456052780151367, 'validation/num_examples': 50000, 'test/accuracy': 0.5258000493049622, 'test/loss': 2.169757127761841, 'test/num_examples': 10000, 'score': 26065.481178045273, 'total_duration': 28096.434440612793, 'accumulated_submission_time': 26065.481178045273, 'accumulated_eval_time': 2017.5318043231964, 'accumulated_logging_time': 6.453748464584351, 'global_step': 67024, 'preemption_count': 0}), (68337, {'train/accuracy': 0.7241310477256775, 'train/loss': 1.0647848844528198, 'validation/accuracy': 0.6534199714660645, 'validation/loss': 1.424412727355957, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.1409544944763184, 'test/num_examples': 10000, 'score': 26575.384176254272, 'total_duration': 28636.664094924927, 'accumulated_submission_time': 26575.384176254272, 'accumulated_eval_time': 2047.6115307807922, 'accumulated_logging_time': 6.568617105484009, 'global_step': 68337, 'preemption_count': 0}), (69649, {'train/accuracy': 0.7201849222183228, 'train/loss': 1.0793545246124268, 'validation/accuracy': 0.6511799693107605, 'validation/loss': 1.4507819414138794, 'validation/num_examples': 50000, 'test/accuracy': 0.5275000333786011, 'test/loss': 2.1988277435302734, 'test/num_examples': 10000, 'score': 27085.194271087646, 'total_duration': 29185.701063632965, 'accumulated_submission_time': 27085.194271087646, 'accumulated_eval_time': 2086.5379796028137, 'accumulated_logging_time': 6.7370710372924805, 'global_step': 69649, 'preemption_count': 0}), (70960, {'train/accuracy': 0.7037228941917419, 'train/loss': 1.1489083766937256, 'validation/accuracy': 0.636680006980896, 'validation/loss': 1.494072675704956, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.237518310546875, 'test/num_examples': 10000, 'score': 27595.18388724327, 'total_duration': 29728.788732528687, 'accumulated_submission_time': 27595.18388724327, 'accumulated_eval_time': 2119.3768105506897, 'accumulated_logging_time': 6.865436792373657, 'global_step': 70960, 'preemption_count': 0}), (72278, {'train/accuracy': 0.7111567258834839, 'train/loss': 1.1290578842163086, 'validation/accuracy': 0.6398000121116638, 'validation/loss': 1.4834872484207153, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.2210605144500732, 'test/num_examples': 10000, 'score': 28105.055102348328, 'total_duration': 30273.6361413002, 'accumulated_submission_time': 28105.055102348328, 'accumulated_eval_time': 2154.0775978565216, 'accumulated_logging_time': 7.0094029903411865, 'global_step': 72278, 'preemption_count': 0}), (73589, {'train/accuracy': 0.7241111397743225, 'train/loss': 1.0535086393356323, 'validation/accuracy': 0.6520000100135803, 'validation/loss': 1.4209418296813965, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.159533739089966, 'test/num_examples': 10000, 'score': 28614.962498426437, 'total_duration': 30823.838453769684, 'accumulated_submission_time': 28614.962498426437, 'accumulated_eval_time': 2194.1070399284363, 'accumulated_logging_time': 7.146681308746338, 'global_step': 73589, 'preemption_count': 0}), (74898, {'train/accuracy': 0.7225366830825806, 'train/loss': 1.0740221738815308, 'validation/accuracy': 0.6500399708747864, 'validation/loss': 1.4353623390197754, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.14973783493042, 'test/num_examples': 10000, 'score': 29124.793865203857, 'total_duration': 31370.99911212921, 'accumulated_submission_time': 29124.793865203857, 'accumulated_eval_time': 2231.1844239234924, 'accumulated_logging_time': 7.264568090438843, 'global_step': 74898, 'preemption_count': 0}), (76209, {'train/accuracy': 0.7336973547935486, 'train/loss': 1.0110986232757568, 'validation/accuracy': 0.6649999618530273, 'validation/loss': 1.3687154054641724, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.089961528778076, 'test/num_examples': 10000, 'score': 29634.678155899048, 'total_duration': 31919.2883040905, 'accumulated_submission_time': 29634.678155899048, 'accumulated_eval_time': 2269.2406833171844, 'accumulated_logging_time': 7.483036279678345, 'global_step': 76209, 'preemption_count': 0}), (77521, {'train/accuracy': 0.7310666441917419, 'train/loss': 1.0373178720474243, 'validation/accuracy': 0.6620999574661255, 'validation/loss': 1.3757272958755493, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.099241256713867, 'test/num_examples': 10000, 'score': 30144.583835601807, 'total_duration': 32464.010974168777, 'accumulated_submission_time': 30144.583835601807, 'accumulated_eval_time': 2303.78449344635, 'accumulated_logging_time': 7.621431827545166, 'global_step': 77521, 'preemption_count': 0}), (78819, {'train/accuracy': 0.7372449040412903, 'train/loss': 0.999210774898529, 'validation/accuracy': 0.665120005607605, 'validation/loss': 1.364309310913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5375000238418579, 'test/loss': 2.0795700550079346, 'test/num_examples': 10000, 'score': 30654.587359428406, 'total_duration': 33010.55748438835, 'accumulated_submission_time': 30654.587359428406, 'accumulated_eval_time': 2340.040785551071, 'accumulated_logging_time': 7.776157855987549, 'global_step': 78819, 'preemption_count': 0}), (80130, {'train/accuracy': 0.7342753410339355, 'train/loss': 1.0147396326065063, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.38283109664917, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.1202588081359863, 'test/num_examples': 10000, 'score': 31164.34555053711, 'total_duration': 33558.357707977295, 'accumulated_submission_time': 31164.34555053711, 'accumulated_eval_time': 2377.8158416748047, 'accumulated_logging_time': 7.9083147048950195, 'global_step': 80130, 'preemption_count': 0}), (81445, {'train/accuracy': 0.7383609414100647, 'train/loss': 1.0017309188842773, 'validation/accuracy': 0.6639400124549866, 'validation/loss': 1.3660082817077637, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.099245548248291, 'test/num_examples': 10000, 'score': 31674.256913661957, 'total_duration': 34100.26830077171, 'accumulated_submission_time': 31674.256913661957, 'accumulated_eval_time': 2409.5419042110443, 'accumulated_logging_time': 8.043986797332764, 'global_step': 81445, 'preemption_count': 0}), (82755, {'train/accuracy': 0.7411710619926453, 'train/loss': 0.9895067811012268, 'validation/accuracy': 0.6690799593925476, 'validation/loss': 1.3471754789352417, 'validation/num_examples': 50000, 'test/accuracy': 0.538100004196167, 'test/loss': 2.07405424118042, 'test/num_examples': 10000, 'score': 32183.98922228813, 'total_duration': 34644.82155919075, 'accumulated_submission_time': 32183.98922228813, 'accumulated_eval_time': 2444.0358991622925, 'accumulated_logging_time': 8.234777212142944, 'global_step': 82755, 'preemption_count': 0}), (84065, {'train/accuracy': 0.7470105290412903, 'train/loss': 0.9718596339225769, 'validation/accuracy': 0.6699599623680115, 'validation/loss': 1.34040105342865, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.07735276222229, 'test/num_examples': 10000, 'score': 32693.873821496964, 'total_duration': 35187.1849155426, 'accumulated_submission_time': 32693.873821496964, 'accumulated_eval_time': 2476.124395132065, 'accumulated_logging_time': 8.484883546829224, 'global_step': 84065, 'preemption_count': 0}), (85362, {'train/accuracy': 0.7373046875, 'train/loss': 1.0042301416397095, 'validation/accuracy': 0.6636999845504761, 'validation/loss': 1.3743995428085327, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.091883420944214, 'test/num_examples': 10000, 'score': 33203.77445316315, 'total_duration': 35735.28090119362, 'accumulated_submission_time': 33203.77445316315, 'accumulated_eval_time': 2514.014396905899, 'accumulated_logging_time': 8.639681339263916, 'global_step': 85362, 'preemption_count': 0}), (86640, {'train/accuracy': 0.7416892647743225, 'train/loss': 0.9759896397590637, 'validation/accuracy': 0.6683399677276611, 'validation/loss': 1.354854702949524, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.0962579250335693, 'test/num_examples': 10000, 'score': 33713.77431464195, 'total_duration': 36279.08540081978, 'accumulated_submission_time': 33713.77431464195, 'accumulated_eval_time': 2547.5052325725555, 'accumulated_logging_time': 8.800147533416748, 'global_step': 86640, 'preemption_count': 0}), (87909, {'train/accuracy': 0.7467913031578064, 'train/loss': 0.9531819820404053, 'validation/accuracy': 0.6702799797058105, 'validation/loss': 1.3429685831069946, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.094825029373169, 'test/num_examples': 10000, 'score': 34223.77736234665, 'total_duration': 36825.15154671669, 'accumulated_submission_time': 34223.77736234665, 'accumulated_eval_time': 2583.226817846298, 'accumulated_logging_time': 8.98764181137085, 'global_step': 87909, 'preemption_count': 0}), (89181, {'train/accuracy': 0.7483657598495483, 'train/loss': 0.9555451273918152, 'validation/accuracy': 0.6715399622917175, 'validation/loss': 1.3497025966644287, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0817558765411377, 'test/num_examples': 10000, 'score': 34733.62873029709, 'total_duration': 37370.441772699356, 'accumulated_submission_time': 34733.62873029709, 'accumulated_eval_time': 2618.3818480968475, 'accumulated_logging_time': 9.115766763687134, 'global_step': 89181, 'preemption_count': 0}), (90436, {'train/accuracy': 0.7536471486091614, 'train/loss': 0.9456705451011658, 'validation/accuracy': 0.67603999376297, 'validation/loss': 1.330556869506836, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.0763721466064453, 'test/num_examples': 10000, 'score': 35243.37022304535, 'total_duration': 37916.61448478699, 'accumulated_submission_time': 35243.37022304535, 'accumulated_eval_time': 2654.4989569187164, 'accumulated_logging_time': 9.27614450454712, 'global_step': 90436, 'preemption_count': 0}), (91698, {'train/accuracy': 0.7416892647743225, 'train/loss': 0.9743335843086243, 'validation/accuracy': 0.6624199748039246, 'validation/loss': 1.3717319965362549, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.1299993991851807, 'test/num_examples': 10000, 'score': 35753.453018665314, 'total_duration': 38459.82631659508, 'accumulated_submission_time': 35753.453018665314, 'accumulated_eval_time': 2687.350551843643, 'accumulated_logging_time': 9.395976305007935, 'global_step': 91698, 'preemption_count': 0}), (92964, {'train/accuracy': 0.7483457922935486, 'train/loss': 0.9551435708999634, 'validation/accuracy': 0.6695799827575684, 'validation/loss': 1.3583085536956787, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.0948450565338135, 'test/num_examples': 10000, 'score': 36263.33692955971, 'total_duration': 39001.97222113609, 'accumulated_submission_time': 36263.33692955971, 'accumulated_eval_time': 2719.2569098472595, 'accumulated_logging_time': 9.601212739944458, 'global_step': 92964, 'preemption_count': 0}), (94145, {'train/accuracy': 0.7598851919174194, 'train/loss': 0.9020687341690063, 'validation/accuracy': 0.6735399961471558, 'validation/loss': 1.328059196472168, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.040255069732666, 'test/num_examples': 10000, 'score': 36773.20000553131, 'total_duration': 39554.42208600044, 'accumulated_submission_time': 36773.20000553131, 'accumulated_eval_time': 2761.566164970398, 'accumulated_logging_time': 9.737160444259644, 'global_step': 94145, 'preemption_count': 0}), (95413, {'train/accuracy': 0.7520129084587097, 'train/loss': 0.9332864284515381, 'validation/accuracy': 0.6658799648284912, 'validation/loss': 1.3660082817077637, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.095616340637207, 'test/num_examples': 10000, 'score': 37283.14886236191, 'total_duration': 40098.32472419739, 'accumulated_submission_time': 37283.14886236191, 'accumulated_eval_time': 2795.121016025543, 'accumulated_logging_time': 9.98220181465149, 'global_step': 95413, 'preemption_count': 0}), (96688, {'train/accuracy': 0.7703084945678711, 'train/loss': 0.8618707060813904, 'validation/accuracy': 0.6775599718093872, 'validation/loss': 1.3039822578430176, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.0245487689971924, 'test/num_examples': 10000, 'score': 37793.17760848999, 'total_duration': 40640.74415779114, 'accumulated_submission_time': 37793.17760848999, 'accumulated_eval_time': 2827.2207324504852, 'accumulated_logging_time': 10.1199791431427, 'global_step': 96688, 'preemption_count': 0}), (97939, {'train/accuracy': 0.7665815949440002, 'train/loss': 0.8650805950164795, 'validation/accuracy': 0.677619993686676, 'validation/loss': 1.3222205638885498, 'validation/num_examples': 50000, 'test/accuracy': 0.5505000352859497, 'test/loss': 2.0391316413879395, 'test/num_examples': 10000, 'score': 38303.16890287399, 'total_duration': 41188.022304058075, 'accumulated_submission_time': 38303.16890287399, 'accumulated_eval_time': 2864.180194377899, 'accumulated_logging_time': 10.298583507537842, 'global_step': 97939, 'preemption_count': 0}), (99209, {'train/accuracy': 0.7712053656578064, 'train/loss': 0.8447064757347107, 'validation/accuracy': 0.6776999831199646, 'validation/loss': 1.3116381168365479, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.031226873397827, 'test/num_examples': 10000, 'score': 38813.06130695343, 'total_duration': 41733.31459093094, 'accumulated_submission_time': 38813.06130695343, 'accumulated_eval_time': 2899.246608734131, 'accumulated_logging_time': 10.478414297103882, 'global_step': 99209, 'preemption_count': 0}), (100480, {'train/accuracy': 0.7671595811843872, 'train/loss': 0.8704963326454163, 'validation/accuracy': 0.6771599650382996, 'validation/loss': 1.308430552482605, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.012143135070801, 'test/num_examples': 10000, 'score': 39323.01236820221, 'total_duration': 42276.967777729034, 'accumulated_submission_time': 39323.01236820221, 'accumulated_eval_time': 2932.6629610061646, 'accumulated_logging_time': 10.611980676651001, 'global_step': 100480, 'preemption_count': 0}), (101749, {'train/accuracy': 0.7831632494926453, 'train/loss': 0.800122082233429, 'validation/accuracy': 0.6864199638366699, 'validation/loss': 1.2833116054534912, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 2.0023274421691895, 'test/num_examples': 10000, 'score': 39832.95672917366, 'total_duration': 42820.31513977051, 'accumulated_submission_time': 39832.95672917366, 'accumulated_eval_time': 2965.7514362335205, 'accumulated_logging_time': 10.774179697036743, 'global_step': 101749, 'preemption_count': 0}), (103012, {'train/accuracy': 0.7931082248687744, 'train/loss': 0.7687116861343384, 'validation/accuracy': 0.6898199915885925, 'validation/loss': 1.250319480895996, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9568206071853638, 'test/num_examples': 10000, 'score': 40342.750413656235, 'total_duration': 43365.55261850357, 'accumulated_submission_time': 40342.750413656235, 'accumulated_eval_time': 3000.8687262535095, 'accumulated_logging_time': 10.950862169265747, 'global_step': 103012, 'preemption_count': 0}), (104255, {'train/accuracy': 0.7841796875, 'train/loss': 0.788958728313446, 'validation/accuracy': 0.681879997253418, 'validation/loss': 1.297695517539978, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.9892799854278564, 'test/num_examples': 10000, 'score': 40852.842639923096, 'total_duration': 43914.35103082657, 'accumulated_submission_time': 40852.842639923096, 'accumulated_eval_time': 3039.2415664196014, 'accumulated_logging_time': 11.13432502746582, 'global_step': 104255, 'preemption_count': 0}), (105497, {'train/accuracy': 0.7898397445678711, 'train/loss': 0.7826870083808899, 'validation/accuracy': 0.6844399571418762, 'validation/loss': 1.2793796062469482, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 2.001741409301758, 'test/num_examples': 10000, 'score': 41362.66264605522, 'total_duration': 44459.05434703827, 'accumulated_submission_time': 41362.66264605522, 'accumulated_eval_time': 3073.828160047531, 'accumulated_logging_time': 11.28305435180664, 'global_step': 105497, 'preemption_count': 0}), (106733, {'train/accuracy': 0.8078762292861938, 'train/loss': 0.7024427056312561, 'validation/accuracy': 0.6891199946403503, 'validation/loss': 1.26376473903656, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.0244131088256836, 'test/num_examples': 10000, 'score': 41872.55877280235, 'total_duration': 45003.28336286545, 'accumulated_submission_time': 41872.55877280235, 'accumulated_eval_time': 3107.889204263687, 'accumulated_logging_time': 11.410769701004028, 'global_step': 106733, 'preemption_count': 0}), (107988, {'train/accuracy': 0.8194156289100647, 'train/loss': 0.6624346375465393, 'validation/accuracy': 0.6899399757385254, 'validation/loss': 1.2506000995635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.9675315618515015, 'test/num_examples': 10000, 'score': 42382.25957608223, 'total_duration': 45549.97060775757, 'accumulated_submission_time': 42382.25957608223, 'accumulated_eval_time': 3144.547374486923, 'accumulated_logging_time': 11.59263825416565, 'global_step': 107988, 'preemption_count': 0}), (109187, {'train/accuracy': 0.7759087681770325, 'train/loss': 0.8255064487457275, 'validation/accuracy': 0.6902399659156799, 'validation/loss': 1.2492961883544922, 'validation/num_examples': 50000, 'test/accuracy': 0.5674000382423401, 'test/loss': 1.953436017036438, 'test/num_examples': 10000, 'score': 42892.34353494644, 'total_duration': 46095.19646310806, 'accumulated_submission_time': 42892.34353494644, 'accumulated_eval_time': 3179.4118280410767, 'accumulated_logging_time': 11.729992866516113, 'global_step': 109187, 'preemption_count': 0}), (110430, {'train/accuracy': 0.7747528553009033, 'train/loss': 0.8336385488510132, 'validation/accuracy': 0.693839967250824, 'validation/loss': 1.2572914361953735, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.9934237003326416, 'test/num_examples': 10000, 'score': 43402.18071317673, 'total_duration': 46642.89986586571, 'accumulated_submission_time': 43402.18071317673, 'accumulated_eval_time': 3217.00390458107, 'accumulated_logging_time': 11.858088970184326, 'global_step': 110430, 'preemption_count': 0}), (111558, {'train/accuracy': 0.775809109210968, 'train/loss': 0.8351102471351624, 'validation/accuracy': 0.6905800104141235, 'validation/loss': 1.2510921955108643, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.984626054763794, 'test/num_examples': 10000, 'score': 43911.94645190239, 'total_duration': 47187.27496790886, 'accumulated_submission_time': 43911.94645190239, 'accumulated_eval_time': 3251.3397986888885, 'accumulated_logging_time': 12.001140356063843, 'global_step': 111558, 'preemption_count': 0}), (112758, {'train/accuracy': 0.7860730290412903, 'train/loss': 0.7833583354949951, 'validation/accuracy': 0.699180006980896, 'validation/loss': 1.2287652492523193, 'validation/num_examples': 50000, 'test/accuracy': 0.5719000101089478, 'test/loss': 1.9555494785308838, 'test/num_examples': 10000, 'score': 44421.96342945099, 'total_duration': 47733.27199077606, 'accumulated_submission_time': 44421.96342945099, 'accumulated_eval_time': 3286.9888343811035, 'accumulated_logging_time': 12.194407224655151, 'global_step': 112758, 'preemption_count': 0}), (113865, {'train/accuracy': 0.78812575340271, 'train/loss': 0.786506175994873, 'validation/accuracy': 0.6923199892044067, 'validation/loss': 1.2570250034332275, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.97958505153656, 'test/num_examples': 10000, 'score': 44931.78919625282, 'total_duration': 48278.630932569504, 'accumulated_submission_time': 44931.78919625282, 'accumulated_eval_time': 3322.1951830387115, 'accumulated_logging_time': 12.395719528198242, 'global_step': 113865, 'preemption_count': 0}), (115035, {'train/accuracy': 0.7992864847183228, 'train/loss': 0.737346351146698, 'validation/accuracy': 0.6996200084686279, 'validation/loss': 1.2263686656951904, 'validation/num_examples': 50000, 'test/accuracy': 0.5708000063896179, 'test/loss': 1.9519559144973755, 'test/num_examples': 10000, 'score': 45441.70025849342, 'total_duration': 48820.72835946083, 'accumulated_submission_time': 45441.70025849342, 'accumulated_eval_time': 3354.1195068359375, 'accumulated_logging_time': 12.521143674850464, 'global_step': 115035, 'preemption_count': 0}), (116008, {'train/accuracy': 0.8125796914100647, 'train/loss': 0.6921834945678711, 'validation/accuracy': 0.7010200023651123, 'validation/loss': 1.2076854705810547, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 1.918347954750061, 'test/num_examples': 10000, 'score': 45951.7398557663, 'total_duration': 49371.09390282631, 'accumulated_submission_time': 45951.7398557663, 'accumulated_eval_time': 3394.1779069900513, 'accumulated_logging_time': 12.678936243057251, 'global_step': 116008, 'preemption_count': 0}), (116931, {'train/accuracy': 0.7839803695678711, 'train/loss': 0.7950624823570251, 'validation/accuracy': 0.6916599869728088, 'validation/loss': 1.2575702667236328, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 1.9666393995285034, 'test/num_examples': 10000, 'score': 46461.60093379021, 'total_duration': 49917.51643538475, 'accumulated_submission_time': 46461.60093379021, 'accumulated_eval_time': 3430.4949519634247, 'accumulated_logging_time': 12.81736159324646, 'global_step': 116931, 'preemption_count': 0}), (117917, {'train/accuracy': 0.7870495915412903, 'train/loss': 0.7745840549468994, 'validation/accuracy': 0.7005800008773804, 'validation/loss': 1.2236385345458984, 'validation/num_examples': 50000, 'test/accuracy': 0.5693000555038452, 'test/loss': 1.9412338733673096, 'test/num_examples': 10000, 'score': 46971.86617445946, 'total_duration': 50464.829978466034, 'accumulated_submission_time': 46971.86617445946, 'accumulated_eval_time': 3467.2756390571594, 'accumulated_logging_time': 12.97178339958191, 'global_step': 117917, 'preemption_count': 0}), (118980, {'train/accuracy': 0.7922512888908386, 'train/loss': 0.7621221542358398, 'validation/accuracy': 0.6893799901008606, 'validation/loss': 1.272264003753662, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.004242181777954, 'test/num_examples': 10000, 'score': 47481.81721043587, 'total_duration': 51012.50205826759, 'accumulated_submission_time': 47481.81721043587, 'accumulated_eval_time': 3504.751314640045, 'accumulated_logging_time': 13.094877481460571, 'global_step': 118980, 'preemption_count': 0}), (119740, {'train/accuracy': 0.825215220451355, 'train/loss': 0.6259505748748779, 'validation/accuracy': 0.7038999795913696, 'validation/loss': 1.2092255353927612, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9645354747772217, 'test/num_examples': 10000, 'score': 47991.96129441261, 'total_duration': 51558.15223145485, 'accumulated_submission_time': 47991.96129441261, 'accumulated_eval_time': 3539.9907054901123, 'accumulated_logging_time': 13.275717735290527, 'global_step': 119740, 'preemption_count': 0}), (120591, {'train/accuracy': 0.8002630472183228, 'train/loss': 0.7337987422943115, 'validation/accuracy': 0.703220009803772, 'validation/loss': 1.2129064798355103, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.914723515510559, 'test/num_examples': 10000, 'score': 48501.96317720413, 'total_duration': 52107.30183887482, 'accumulated_submission_time': 48501.96317720413, 'accumulated_eval_time': 3578.911531686783, 'accumulated_logging_time': 13.40663480758667, 'global_step': 120591, 'preemption_count': 0}), (121333, {'train/accuracy': 0.8097097873687744, 'train/loss': 0.6910713911056519, 'validation/accuracy': 0.6981799602508545, 'validation/loss': 1.2400017976760864, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 1.9810094833374023, 'test/num_examples': 10000, 'score': 49012.90021824837, 'total_duration': 52653.38587450981, 'accumulated_submission_time': 49012.90021824837, 'accumulated_eval_time': 3613.8261275291443, 'accumulated_logging_time': 13.55582880973816, 'global_step': 121333, 'preemption_count': 0}), (122007, {'train/accuracy': 0.8040696382522583, 'train/loss': 0.7202451229095459, 'validation/accuracy': 0.7048999667167664, 'validation/loss': 1.190501093864441, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9123563766479492, 'test/num_examples': 10000, 'score': 49521.98909521103, 'total_duration': 53197.46391201019, 'accumulated_submission_time': 49521.98909521103, 'accumulated_eval_time': 3647.7917675971985, 'accumulated_logging_time': 14.459708452224731, 'global_step': 122007, 'preemption_count': 0}), (122716, {'train/accuracy': 0.8178212642669678, 'train/loss': 0.6545913219451904, 'validation/accuracy': 0.7041400074958801, 'validation/loss': 1.2171365022659302, 'validation/num_examples': 50000, 'test/accuracy': 0.5825000405311584, 'test/loss': 1.9358903169631958, 'test/num_examples': 10000, 'score': 50032.24576711655, 'total_duration': 53741.38331794739, 'accumulated_submission_time': 50032.24576711655, 'accumulated_eval_time': 3681.325066804886, 'accumulated_logging_time': 14.51060152053833, 'global_step': 122716, 'preemption_count': 0}), (123439, {'train/accuracy': 0.8032923936843872, 'train/loss': 0.7089185118675232, 'validation/accuracy': 0.7051799893379211, 'validation/loss': 1.1993591785430908, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.9342819452285767, 'test/num_examples': 10000, 'score': 50542.192996025085, 'total_duration': 54287.10981154442, 'accumulated_submission_time': 50542.192996025085, 'accumulated_eval_time': 3716.937094449997, 'accumulated_logging_time': 14.596786499023438, 'global_step': 123439, 'preemption_count': 0}), (124121, {'train/accuracy': 0.8209701776504517, 'train/loss': 0.6474238038063049, 'validation/accuracy': 0.7029799818992615, 'validation/loss': 1.2080973386764526, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 1.9225531816482544, 'test/num_examples': 10000, 'score': 51052.16874957085, 'total_duration': 54833.34208202362, 'accumulated_submission_time': 51052.16874957085, 'accumulated_eval_time': 3753.0404579639435, 'accumulated_logging_time': 14.671565294265747, 'global_step': 124121, 'preemption_count': 0}), (124839, {'train/accuracy': 0.8093709945678711, 'train/loss': 0.696272075176239, 'validation/accuracy': 0.7047199606895447, 'validation/loss': 1.2032544612884521, 'validation/num_examples': 50000, 'test/accuracy': 0.5853000283241272, 'test/loss': 1.9087883234024048, 'test/num_examples': 10000, 'score': 51562.25228142738, 'total_duration': 55377.44689488411, 'accumulated_submission_time': 51562.25228142738, 'accumulated_eval_time': 3786.8993968963623, 'accumulated_logging_time': 14.752557754516602, 'global_step': 124839, 'preemption_count': 0}), (125360, {'train/accuracy': 0.8184390664100647, 'train/loss': 0.6588167548179626, 'validation/accuracy': 0.6976000070571899, 'validation/loss': 1.2266343832015991, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 1.9314446449279785, 'test/num_examples': 10000, 'score': 52072.27811574936, 'total_duration': 55922.4316444397, 'accumulated_submission_time': 52072.27811574936, 'accumulated_eval_time': 3821.7256004810333, 'accumulated_logging_time': 14.82430624961853, 'global_step': 125360, 'preemption_count': 0}), (125935, {'train/accuracy': 0.8102877736091614, 'train/loss': 0.6886866688728333, 'validation/accuracy': 0.7069599628448486, 'validation/loss': 1.1878427267074585, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.905565857887268, 'test/num_examples': 10000, 'score': 52582.41988611221, 'total_duration': 56465.97137212753, 'accumulated_submission_time': 52582.41988611221, 'accumulated_eval_time': 3854.9902682304382, 'accumulated_logging_time': 14.893283367156982, 'global_step': 125935, 'preemption_count': 0}), (126512, {'train/accuracy': 0.8517019748687744, 'train/loss': 0.5410611629486084, 'validation/accuracy': 0.70660001039505, 'validation/loss': 1.1955972909927368, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.932255506515503, 'test/num_examples': 10000, 'score': 53092.69561123848, 'total_duration': 57012.62925815582, 'accumulated_submission_time': 53092.69561123848, 'accumulated_eval_time': 3891.256278991699, 'accumulated_logging_time': 14.943933248519897, 'global_step': 126512, 'preemption_count': 0}), (127002, {'train/accuracy': 0.8234215378761292, 'train/loss': 0.6400219202041626, 'validation/accuracy': 0.7119399905204773, 'validation/loss': 1.179505705833435, 'validation/num_examples': 50000, 'test/accuracy': 0.5897000432014465, 'test/loss': 1.894606351852417, 'test/num_examples': 10000, 'score': 53602.824206352234, 'total_duration': 57555.15067195892, 'accumulated_submission_time': 53602.824206352234, 'accumulated_eval_time': 3923.532804250717, 'accumulated_logging_time': 15.003150463104248, 'global_step': 127002, 'preemption_count': 0}), (127432, {'train/accuracy': 0.813875138759613, 'train/loss': 0.6636583805084229, 'validation/accuracy': 0.7100600004196167, 'validation/loss': 1.1728723049163818, 'validation/num_examples': 50000, 'test/accuracy': 0.5839000344276428, 'test/loss': 1.9206054210662842, 'test/num_examples': 10000, 'score': 54113.0475692749, 'total_duration': 58103.80387854576, 'accumulated_submission_time': 54113.0475692749, 'accumulated_eval_time': 3961.8462760448456, 'accumulated_logging_time': 15.071233987808228, 'global_step': 127432, 'preemption_count': 0}), (128059, {'train/accuracy': 0.8400430083274841, 'train/loss': 0.5688015222549438, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1803946495056152, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.88873291015625, 'test/num_examples': 10000, 'score': 54623.22213149071, 'total_duration': 58648.92207670212, 'accumulated_submission_time': 54623.22213149071, 'accumulated_eval_time': 3996.659945011139, 'accumulated_logging_time': 15.128484964370728, 'global_step': 128059, 'preemption_count': 0}), (128561, {'train/accuracy': 0.8127790093421936, 'train/loss': 0.6717605590820312, 'validation/accuracy': 0.7080599665641785, 'validation/loss': 1.199042797088623, 'validation/num_examples': 50000, 'test/accuracy': 0.5794000029563904, 'test/loss': 1.9341740608215332, 'test/num_examples': 10000, 'score': 55133.282629966736, 'total_duration': 59189.789964199066, 'accumulated_submission_time': 55133.282629966736, 'accumulated_eval_time': 4027.3140909671783, 'accumulated_logging_time': 15.226057052612305, 'global_step': 128561, 'preemption_count': 0}), (129062, {'train/accuracy': 0.81351637840271, 'train/loss': 0.6617496609687805, 'validation/accuracy': 0.7140600085258484, 'validation/loss': 1.1778055429458618, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.8974639177322388, 'test/num_examples': 10000, 'score': 55644.29246711731, 'total_duration': 59731.770600795746, 'accumulated_submission_time': 55644.29246711731, 'accumulated_eval_time': 4058.1474578380585, 'accumulated_logging_time': 15.307074308395386, 'global_step': 129062, 'preemption_count': 0}), (129487, {'train/accuracy': 0.8402822017669678, 'train/loss': 0.564418613910675, 'validation/accuracy': 0.714199960231781, 'validation/loss': 1.1659231185913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5874000191688538, 'test/loss': 1.8769652843475342, 'test/num_examples': 10000, 'score': 56155.474486112595, 'total_duration': 60275.29718184471, 'accumulated_submission_time': 56155.474486112595, 'accumulated_eval_time': 4090.349634170532, 'accumulated_logging_time': 15.401219129562378, 'global_step': 129487, 'preemption_count': 0}), (129734, {'train/accuracy': 0.8283242583274841, 'train/loss': 0.6195014715194702, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1777704954147339, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.9124149084091187, 'test/num_examples': 10000, 'score': 56665.757748126984, 'total_duration': 60818.33146452904, 'accumulated_submission_time': 56665.757748126984, 'accumulated_eval_time': 4123.018921375275, 'accumulated_logging_time': 15.45542860031128, 'global_step': 129734, 'preemption_count': 0}), (130145, {'train/accuracy': 0.8169642686843872, 'train/loss': 0.6591873168945312, 'validation/accuracy': 0.7076999545097351, 'validation/loss': 1.1928380727767944, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 1.9340249300003052, 'test/num_examples': 10000, 'score': 57175.97269439697, 'total_duration': 61365.642771959305, 'accumulated_submission_time': 57175.97269439697, 'accumulated_eval_time': 4160.000620365143, 'accumulated_logging_time': 15.52323317527771, 'global_step': 130145, 'preemption_count': 0}), (130618, {'train/accuracy': 0.8188974857330322, 'train/loss': 0.6450485587120056, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.1781855821609497, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.9016708135604858, 'test/num_examples': 10000, 'score': 57685.97555708885, 'total_duration': 61910.184910297394, 'accumulated_submission_time': 57685.97555708885, 'accumulated_eval_time': 4194.437458276749, 'accumulated_logging_time': 15.570994138717651, 'global_step': 130618, 'preemption_count': 0}), (130857, {'train/accuracy': 0.8402024507522583, 'train/loss': 0.5743464827537537, 'validation/accuracy': 0.7050399780273438, 'validation/loss': 1.2018828392028809, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.9341565370559692, 'test/num_examples': 10000, 'score': 58196.55683040619, 'total_duration': 62453.41831159592, 'accumulated_submission_time': 58196.55683040619, 'accumulated_eval_time': 4226.949236869812, 'accumulated_logging_time': 15.684504508972168, 'global_step': 130857, 'preemption_count': 0}), (131146, {'train/accuracy': 0.8282844424247742, 'train/loss': 0.6296339631080627, 'validation/accuracy': 0.7112799882888794, 'validation/loss': 1.1911766529083252, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.923415184020996, 'test/num_examples': 10000, 'score': 58708.376480817795, 'total_duration': 62999.56728363037, 'accumulated_submission_time': 58708.376480817795, 'accumulated_eval_time': 4261.148334741592, 'accumulated_logging_time': 15.783037424087524, 'global_step': 131146, 'preemption_count': 0}), (131752, {'train/accuracy': 0.8253547549247742, 'train/loss': 0.6296873092651367, 'validation/accuracy': 0.7152999639511108, 'validation/loss': 1.1612296104431152, 'validation/num_examples': 50000, 'test/accuracy': 0.5857000350952148, 'test/loss': 1.8822414875030518, 'test/num_examples': 10000, 'score': 59221.03333187103, 'total_duration': 63543.605088710785, 'accumulated_submission_time': 59221.03333187103, 'accumulated_eval_time': 4292.406179428101, 'accumulated_logging_time': 15.840676307678223, 'global_step': 131752, 'preemption_count': 0}), (131876, {'train/accuracy': 0.8220663070678711, 'train/loss': 0.6354645490646362, 'validation/accuracy': 0.7161999940872192, 'validation/loss': 1.1676372289657593, 'validation/num_examples': 50000, 'test/accuracy': 0.5860000252723694, 'test/loss': 1.8891657590866089, 'test/num_examples': 10000, 'score': 59732.930132865906, 'total_duration': 64084.45346856117, 'accumulated_submission_time': 59732.930132865906, 'accumulated_eval_time': 4321.286098718643, 'accumulated_logging_time': 15.899031400680542, 'global_step': 131876, 'preemption_count': 0}), (132196, {'train/accuracy': 0.8604312539100647, 'train/loss': 0.497049480676651, 'validation/accuracy': 0.7150200009346008, 'validation/loss': 1.166748285293579, 'validation/num_examples': 50000, 'test/accuracy': 0.5807000398635864, 'test/loss': 1.9246114492416382, 'test/num_examples': 10000, 'score': 60243.807784318924, 'total_duration': 64628.23658204079, 'accumulated_submission_time': 60243.807784318924, 'accumulated_eval_time': 4354.123971939087, 'accumulated_logging_time': 15.93018388748169, 'global_step': 132196, 'preemption_count': 0}), (132737, {'train/accuracy': 0.8304169178009033, 'train/loss': 0.6038551330566406, 'validation/accuracy': 0.7142199873924255, 'validation/loss': 1.1793506145477295, 'validation/num_examples': 50000, 'test/accuracy': 0.5837000012397766, 'test/loss': 1.9010155200958252, 'test/num_examples': 10000, 'score': 60754.42060017586, 'total_duration': 65172.579268693924, 'accumulated_submission_time': 60754.42060017586, 'accumulated_eval_time': 4387.678166389465, 'accumulated_logging_time': 16.046374797821045, 'global_step': 132737, 'preemption_count': 0}), (132946, {'train/accuracy': 0.8287029266357422, 'train/loss': 0.6000069975852966, 'validation/accuracy': 0.7139999866485596, 'validation/loss': 1.1776269674301147, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.920369267463684, 'test/num_examples': 10000, 'score': 61265.70809793472, 'total_duration': 65719.01386857033, 'accumulated_submission_time': 61265.70809793472, 'accumulated_eval_time': 4422.733438014984, 'accumulated_logging_time': 16.11443066596985, 'global_step': 132946, 'preemption_count': 0}), (133303, {'train/accuracy': 0.826590359210968, 'train/loss': 0.6236283183097839, 'validation/accuracy': 0.7160999774932861, 'validation/loss': 1.1539772748947144, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 1.9088598489761353, 'test/num_examples': 10000, 'score': 61776.545063734055, 'total_duration': 66263.27648663521, 'accumulated_submission_time': 61776.545063734055, 'accumulated_eval_time': 4456.084577560425, 'accumulated_logging_time': 16.148447036743164, 'global_step': 133303, 'preemption_count': 0}), (133680, {'train/accuracy': 0.8625239133834839, 'train/loss': 0.4914766848087311, 'validation/accuracy': 0.7106999754905701, 'validation/loss': 1.1898199319839478, 'validation/num_examples': 50000, 'test/accuracy': 0.5822000503540039, 'test/loss': 1.933200716972351, 'test/num_examples': 10000, 'score': 62287.067898750305, 'total_duration': 66807.44013214111, 'accumulated_submission_time': 62287.067898750305, 'accumulated_eval_time': 4489.6272485256195, 'accumulated_logging_time': 16.20473837852478, 'global_step': 133680, 'preemption_count': 0}), (133829, {'train/accuracy': 0.8446468114852905, 'train/loss': 0.5512420535087585, 'validation/accuracy': 0.7114399671554565, 'validation/loss': 1.1826568841934204, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9152286052703857, 'test/num_examples': 10000, 'score': 62798.26578402519, 'total_duration': 67352.4173719883, 'accumulated_submission_time': 62798.26578402519, 'accumulated_eval_time': 4523.269063472748, 'accumulated_logging_time': 16.32670831680298, 'global_step': 133829, 'preemption_count': 0}), (134221, {'train/accuracy': 0.8385483026504517, 'train/loss': 0.5690528750419617, 'validation/accuracy': 0.7187199592590332, 'validation/loss': 1.158582329750061, 'validation/num_examples': 50000, 'test/accuracy': 0.5902000069618225, 'test/loss': 1.9015141725540161, 'test/num_examples': 10000, 'score': 63308.693611860275, 'total_duration': 67893.81734800339, 'accumulated_submission_time': 63308.693611860275, 'accumulated_eval_time': 4554.164469718933, 'accumulated_logging_time': 16.360220193862915, 'global_step': 134221, 'preemption_count': 0}), (134556, {'train/accuracy': 0.8328284025192261, 'train/loss': 0.5939211249351501, 'validation/accuracy': 0.719760000705719, 'validation/loss': 1.1464815139770508, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.8816057443618774, 'test/num_examples': 10000, 'score': 63818.90691781044, 'total_duration': 68442.28345990181, 'accumulated_submission_time': 63818.90691781044, 'accumulated_eval_time': 4592.240064620972, 'accumulated_logging_time': 16.50055241584778, 'global_step': 134556, 'preemption_count': 0}), (134777, {'train/accuracy': 0.8325294852256775, 'train/loss': 0.6018131971359253, 'validation/accuracy': 0.713919997215271, 'validation/loss': 1.15029776096344, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.8818572759628296, 'test/num_examples': 10000, 'score': 64330.012323856354, 'total_duration': 68985.6300046444, 'accumulated_submission_time': 64330.012323856354, 'accumulated_eval_time': 4624.340298175812, 'accumulated_logging_time': 16.61750292778015, 'global_step': 134777, 'preemption_count': 0}), (135035, {'train/accuracy': 0.8338648080825806, 'train/loss': 0.5892207622528076, 'validation/accuracy': 0.7191599607467651, 'validation/loss': 1.1520665884017944, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.893405556678772, 'test/num_examples': 10000, 'score': 64841.657054424286, 'total_duration': 69530.22250390053, 'accumulated_submission_time': 64841.657054424286, 'accumulated_eval_time': 4657.223359823227, 'accumulated_logging_time': 16.654006481170654, 'global_step': 135035, 'preemption_count': 0}), (135280, {'train/accuracy': 0.8502271771430969, 'train/loss': 0.5344494581222534, 'validation/accuracy': 0.7087399959564209, 'validation/loss': 1.1960666179656982, 'validation/num_examples': 50000, 'test/accuracy': 0.5786000490188599, 'test/loss': 1.9512779712677002, 'test/num_examples': 10000, 'score': 65352.9531788826, 'total_duration': 70071.27452516556, 'accumulated_submission_time': 65352.9531788826, 'accumulated_eval_time': 4686.78676700592, 'accumulated_logging_time': 16.818994283676147, 'global_step': 135280, 'preemption_count': 0}), (135528, {'train/accuracy': 0.8508848547935486, 'train/loss': 0.533978283405304, 'validation/accuracy': 0.7188999652862549, 'validation/loss': 1.1550242900848389, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.8896461725234985, 'test/num_examples': 10000, 'score': 65864.11411261559, 'total_duration': 70614.31483054161, 'accumulated_submission_time': 65864.11411261559, 'accumulated_eval_time': 4718.605162143707, 'accumulated_logging_time': 16.852794647216797, 'global_step': 135528, 'preemption_count': 0})], 'global_step': 135776}
I0307 21:55:38.230827 140452179379392 submission_runner.py:649] Timing: 66374.19219136238
I0307 21:55:38.230873 140452179379392 submission_runner.py:651] Total number of evals: 130
I0307 21:55:38.230904 140452179379392 submission_runner.py:652] ====================
I0307 21:55:38.231135 140452179379392 submission_runner.py:750] Final imagenet_resnet score: 4
