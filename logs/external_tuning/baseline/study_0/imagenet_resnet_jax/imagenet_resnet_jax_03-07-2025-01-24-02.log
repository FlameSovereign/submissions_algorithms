python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-830396957 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-01-24-02.log
2025-03-07 01:24:18.547070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741310659.021335       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741310659.196639       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 01:25:06.510090 140226914178240 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax.
I0307 01:25:09.106420 140226914178240 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 01:25:09.109853 140226914178240 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 01:25:09.121915 140226914178240 submission_runner.py:606] Using RNG seed -830396957
I0307 01:25:12.725498 140226914178240 submission_runner.py:615] --- Tuning run 3/5 ---
I0307 01:25:12.725710 140226914178240 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_3.
I0307 01:25:12.725901 140226914178240 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_3/hparams.json.
I0307 01:25:12.984707 140226914178240 submission_runner.py:218] Initializing dataset.
I0307 01:25:14.691591 140226914178240 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:25:15.036523 140226914178240 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:25:15.335102 140226914178240 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:25:17.409215 140226914178240 submission_runner.py:229] Initializing model.
I0307 01:25:41.843893 140226914178240 submission_runner.py:272] Initializing optimizer.
I0307 01:25:43.040556 140226914178240 submission_runner.py:279] Initializing metrics bundle.
I0307 01:25:43.040817 140226914178240 submission_runner.py:301] Initializing checkpoint and logger.
I0307 01:25:43.041892 140226914178240 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0307 01:25:43.041999 140226914178240 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_3/meta_data_0.json.
I0307 01:25:43.439826 140226914178240 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_3/flags_0.json.
I0307 01:25:43.768999 140226914178240 submission_runner.py:337] Starting training loop.
I0307 01:26:39.835849 140089787135744 logging_writer.py:48] [0] global_step=0, grad_norm=0.6742313504219055, loss=6.923664569854736
I0307 01:26:40.225994 140226914178240 spec.py:321] Evaluating on the training split.
I0307 01:26:40.671903 140226914178240 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:26:40.695089 140226914178240 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:26:40.735857 140226914178240 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:26:59.495372 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 01:27:00.029509 140226914178240 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:27:00.086967 140226914178240 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:27:00.280445 140226914178240 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:27:48.650344 140226914178240 spec.py:349] Evaluating on the test split.
I0307 01:27:49.115034 140226914178240 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 01:27:49.178001 140226914178240 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 01:27:49.217677 140226914178240 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 01:28:13.253367 140226914178240 submission_runner.py:469] Time since start: 149.48s, 	Step: 1, 	{'train/accuracy': 0.001195790828205645, 'train/loss': 6.909979343414307, 'validation/accuracy': 0.00107999995816499, 'validation/loss': 6.910284042358398, 'validation/num_examples': 50000, 'test/accuracy': 0.0017000001389533281, 'test/loss': 6.910176753997803, 'test/num_examples': 10000, 'score': 56.4568145275116, 'total_duration': 149.48430800437927, 'accumulated_submission_time': 56.4568145275116, 'accumulated_eval_time': 93.02730703353882, 'accumulated_logging_time': 0}
I0307 01:28:13.263409 140071449630464 logging_writer.py:48] [1] accumulated_eval_time=93.0273, accumulated_logging_time=0, accumulated_submission_time=56.4568, global_step=1, preemption_count=0, score=56.4568, test/accuracy=0.0017, test/loss=6.91018, test/num_examples=10000, total_duration=149.484, train/accuracy=0.00119579, train/loss=6.90998, validation/accuracy=0.00108, validation/loss=6.91028, validation/num_examples=50000
I0307 01:28:49.503484 140071365768960 logging_writer.py:48] [100] global_step=100, grad_norm=0.6724853515625, loss=6.914257526397705
I0307 01:29:25.684195 140071449630464 logging_writer.py:48] [200] global_step=200, grad_norm=0.6802473068237305, loss=6.845773696899414
I0307 01:30:02.373650 140071365768960 logging_writer.py:48] [300] global_step=300, grad_norm=0.7135019302368164, loss=6.771478176116943
I0307 01:30:39.146364 140071449630464 logging_writer.py:48] [400] global_step=400, grad_norm=0.7390916347503662, loss=6.696749687194824
I0307 01:31:16.192803 140071365768960 logging_writer.py:48] [500] global_step=500, grad_norm=0.8132452964782715, loss=6.566973686218262
I0307 01:31:53.170513 140071449630464 logging_writer.py:48] [600] global_step=600, grad_norm=0.8365697264671326, loss=6.464756965637207
I0307 01:32:29.905010 140071365768960 logging_writer.py:48] [700] global_step=700, grad_norm=0.8641921281814575, loss=6.3472580909729
I0307 01:33:07.229261 140071449630464 logging_writer.py:48] [800] global_step=800, grad_norm=0.9321833252906799, loss=6.2939276695251465
I0307 01:33:45.026143 140071365768960 logging_writer.py:48] [900] global_step=900, grad_norm=1.098462462425232, loss=6.088815689086914
I0307 01:34:22.393643 140071449630464 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.6397044658660889, loss=5.981106758117676
I0307 01:35:00.701097 140071365768960 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.505445718765259, loss=5.905866622924805
I0307 01:35:39.022188 140071449630464 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.4848830699920654, loss=5.91071081161499
I0307 01:36:17.193822 140071365768960 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.1743083000183105, loss=5.770169734954834
I0307 01:36:43.321502 140226914178240 spec.py:321] Evaluating on the training split.
I0307 01:36:54.545674 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 01:37:16.588005 140226914178240 spec.py:349] Evaluating on the test split.
I0307 01:37:18.425224 140226914178240 submission_runner.py:469] Time since start: 694.66s, 	Step: 1370, 	{'train/accuracy': 0.0615234375, 'train/loss': 5.465592384338379, 'validation/accuracy': 0.054919999092817307, 'validation/loss': 5.555846691131592, 'validation/num_examples': 50000, 'test/accuracy': 0.03960000351071358, 'test/loss': 5.773700714111328, 'test/num_examples': 10000, 'score': 566.3106379508972, 'total_duration': 694.6561579704285, 'accumulated_submission_time': 566.3106379508972, 'accumulated_eval_time': 128.1309769153595, 'accumulated_logging_time': 0.050501108169555664}
I0307 01:37:18.456255 140071458023168 logging_writer.py:48] [1370] accumulated_eval_time=128.131, accumulated_logging_time=0.0505011, accumulated_submission_time=566.311, global_step=1370, preemption_count=0, score=566.311, test/accuracy=0.0396, test/loss=5.7737, test/num_examples=10000, total_duration=694.656, train/accuracy=0.0615234, train/loss=5.46559, validation/accuracy=0.05492, validation/loss=5.55585, validation/num_examples=50000
I0307 01:37:30.170028 140071466415872 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.1336045265197754, loss=5.652673721313477
I0307 01:38:08.539314 140071458023168 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.7547285556793213, loss=5.585413932800293
I0307 01:38:46.304227 140071466415872 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.903083562850952, loss=5.492928504943848
I0307 01:39:24.406556 140071458023168 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.66519832611084, loss=5.410865306854248
I0307 01:40:02.476482 140071466415872 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.8797571659088135, loss=5.327849864959717
I0307 01:40:39.520711 140071458023168 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.196489095687866, loss=5.085702419281006
I0307 01:41:15.797256 140071466415872 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.8379011154174805, loss=5.188434600830078
I0307 01:41:53.990959 140071458023168 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.439691543579102, loss=5.1141862869262695
I0307 01:42:31.801300 140071466415872 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.677677631378174, loss=5.025809288024902
I0307 01:43:09.930188 140071458023168 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.399587631225586, loss=5.016453266143799
I0307 01:43:47.988758 140071466415872 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.329251289367676, loss=5.00963020324707
I0307 01:44:25.983441 140071458023168 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.9121592044830322, loss=4.947628021240234
I0307 01:45:04.131989 140071466415872 logging_writer.py:48] [2600] global_step=2600, grad_norm=7.903787612915039, loss=4.910783290863037
I0307 01:45:42.454360 140071458023168 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.3409905433654785, loss=4.731098175048828
I0307 01:45:48.503307 140226914178240 spec.py:321] Evaluating on the training split.
I0307 01:45:59.619201 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 01:46:22.064063 140226914178240 spec.py:349] Evaluating on the test split.
I0307 01:46:23.829340 140226914178240 submission_runner.py:469] Time since start: 1240.06s, 	Step: 2717, 	{'train/accuracy': 0.15286192297935486, 'train/loss': 4.414518356323242, 'validation/accuracy': 0.1349799931049347, 'validation/loss': 4.573461532592773, 'validation/num_examples': 50000, 'test/accuracy': 0.09330000728368759, 'test/loss': 5.005978107452393, 'test/num_examples': 10000, 'score': 1076.203177690506, 'total_duration': 1240.0603058338165, 'accumulated_submission_time': 1076.203177690506, 'accumulated_eval_time': 163.4569754600525, 'accumulated_logging_time': 0.08935856819152832}
I0307 01:46:23.862219 140071466415872 logging_writer.py:48] [2717] accumulated_eval_time=163.457, accumulated_logging_time=0.0893586, accumulated_submission_time=1076.2, global_step=2717, preemption_count=0, score=1076.2, test/accuracy=0.0933, test/loss=5.00598, test/num_examples=10000, total_duration=1240.06, train/accuracy=0.152862, train/loss=4.41452, validation/accuracy=0.13498, validation/loss=4.57346, validation/num_examples=50000
I0307 01:46:56.218086 140071458023168 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.35646915435791, loss=4.719437599182129
I0307 01:47:34.231779 140071466415872 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.972301483154297, loss=4.739326477050781
I0307 01:48:12.257389 140071458023168 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.764684200286865, loss=4.602099895477295
I0307 01:48:50.518664 140071466415872 logging_writer.py:48] [3100] global_step=3100, grad_norm=5.25862979888916, loss=4.57559871673584
I0307 01:49:28.562430 140071458023168 logging_writer.py:48] [3200] global_step=3200, grad_norm=6.672007083892822, loss=4.520569324493408
I0307 01:50:07.053431 140071466415872 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.825162410736084, loss=4.500349044799805
I0307 01:50:45.577062 140071458023168 logging_writer.py:48] [3400] global_step=3400, grad_norm=10.643238067626953, loss=4.425295829772949
I0307 01:51:23.677126 140071466415872 logging_writer.py:48] [3500] global_step=3500, grad_norm=11.008298873901367, loss=4.513765335083008
I0307 01:52:01.860344 140071458023168 logging_writer.py:48] [3600] global_step=3600, grad_norm=7.231147766113281, loss=4.273011207580566
I0307 01:52:39.643963 140071466415872 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.9084672927856445, loss=4.353764533996582
I0307 01:53:17.629508 140071458023168 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.218851566314697, loss=4.140528202056885
I0307 01:53:55.647820 140071466415872 logging_writer.py:48] [3900] global_step=3900, grad_norm=8.932010650634766, loss=4.275921821594238
I0307 01:54:34.215230 140071458023168 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.453876972198486, loss=4.130997657775879
I0307 01:54:53.908442 140226914178240 spec.py:321] Evaluating on the training split.
I0307 01:55:04.714801 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 01:55:27.402007 140226914178240 spec.py:349] Evaluating on the test split.
I0307 01:55:29.182339 140226914178240 submission_runner.py:469] Time since start: 1785.41s, 	Step: 4053, 	{'train/accuracy': 0.2474290430545807, 'train/loss': 3.6621642112731934, 'validation/accuracy': 0.21581999957561493, 'validation/loss': 3.8797121047973633, 'validation/num_examples': 50000, 'test/accuracy': 0.16040000319480896, 'test/loss': 4.4392547607421875, 'test/num_examples': 10000, 'score': 1586.102175951004, 'total_duration': 1785.41330575943, 'accumulated_submission_time': 1586.102175951004, 'accumulated_eval_time': 198.7308385372162, 'accumulated_logging_time': 0.1478276252746582}
I0307 01:55:29.190840 140071466415872 logging_writer.py:48] [4053] accumulated_eval_time=198.731, accumulated_logging_time=0.147828, accumulated_submission_time=1586.1, global_step=4053, preemption_count=0, score=1586.1, test/accuracy=0.1604, test/loss=4.43925, test/num_examples=10000, total_duration=1785.41, train/accuracy=0.247429, train/loss=3.66216, validation/accuracy=0.21582, validation/loss=3.87971, validation/num_examples=50000
I0307 01:55:47.536863 140071458023168 logging_writer.py:48] [4100] global_step=4100, grad_norm=8.97616195678711, loss=4.028426647186279
I0307 01:56:25.741521 140071466415872 logging_writer.py:48] [4200] global_step=4200, grad_norm=7.704183578491211, loss=3.926067352294922
I0307 01:57:03.854095 140071458023168 logging_writer.py:48] [4300] global_step=4300, grad_norm=11.466634750366211, loss=4.1448869705200195
I0307 01:57:42.276630 140071466415872 logging_writer.py:48] [4400] global_step=4400, grad_norm=7.181381702423096, loss=4.007061004638672
I0307 01:58:20.354478 140071458023168 logging_writer.py:48] [4500] global_step=4500, grad_norm=7.5422539710998535, loss=3.8685812950134277
I0307 01:58:58.567606 140071466415872 logging_writer.py:48] [4600] global_step=4600, grad_norm=6.290598392486572, loss=3.8938870429992676
I0307 01:59:36.588898 140071458023168 logging_writer.py:48] [4700] global_step=4700, grad_norm=12.102775573730469, loss=3.7912116050720215
I0307 02:00:14.559017 140071466415872 logging_writer.py:48] [4800] global_step=4800, grad_norm=5.781114101409912, loss=3.765162944793701
I0307 02:00:52.783251 140071458023168 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.820491313934326, loss=3.9101412296295166
I0307 02:01:30.629854 140071466415872 logging_writer.py:48] [5000] global_step=5000, grad_norm=5.347495079040527, loss=3.7751917839050293
I0307 02:02:08.633390 140071458023168 logging_writer.py:48] [5100] global_step=5100, grad_norm=8.137264251708984, loss=3.697373151779175
I0307 02:02:46.549605 140071466415872 logging_writer.py:48] [5200] global_step=5200, grad_norm=7.588516712188721, loss=3.7733521461486816
I0307 02:03:24.507116 140071458023168 logging_writer.py:48] [5300] global_step=5300, grad_norm=8.77202033996582, loss=3.665489673614502
I0307 02:03:59.363230 140226914178240 spec.py:321] Evaluating on the training split.
I0307 02:04:10.261969 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 02:04:31.018471 140226914178240 spec.py:349] Evaluating on the test split.
I0307 02:04:32.790337 140226914178240 submission_runner.py:469] Time since start: 2329.02s, 	Step: 5393, 	{'train/accuracy': 0.3272082209587097, 'train/loss': 3.1125736236572266, 'validation/accuracy': 0.28915998339653015, 'validation/loss': 3.350515604019165, 'validation/num_examples': 50000, 'test/accuracy': 0.2079000025987625, 'test/loss': 3.984504461288452, 'test/num_examples': 10000, 'score': 2096.1438636779785, 'total_duration': 2329.0213050842285, 'accumulated_submission_time': 2096.1438636779785, 'accumulated_eval_time': 232.1579167842865, 'accumulated_logging_time': 0.16368651390075684}
I0307 02:04:32.824058 140071466415872 logging_writer.py:48] [5393] accumulated_eval_time=232.158, accumulated_logging_time=0.163687, accumulated_submission_time=2096.14, global_step=5393, preemption_count=0, score=2096.14, test/accuracy=0.2079, test/loss=3.9845, test/num_examples=10000, total_duration=2329.02, train/accuracy=0.327208, train/loss=3.11257, validation/accuracy=0.28916, validation/loss=3.35052, validation/num_examples=50000
I0307 02:04:35.892621 140071458023168 logging_writer.py:48] [5400] global_step=5400, grad_norm=7.029033184051514, loss=3.5649144649505615
I0307 02:05:14.352978 140071466415872 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.430922031402588, loss=3.628263235092163
I0307 02:05:52.472070 140071458023168 logging_writer.py:48] [5600] global_step=5600, grad_norm=6.789072036743164, loss=3.4942665100097656
I0307 02:06:30.343908 140071466415872 logging_writer.py:48] [5700] global_step=5700, grad_norm=11.267528533935547, loss=3.563138961791992
I0307 02:07:08.674666 140071458023168 logging_writer.py:48] [5800] global_step=5800, grad_norm=6.112220287322998, loss=3.51265549659729
I0307 02:07:46.812038 140071466415872 logging_writer.py:48] [5900] global_step=5900, grad_norm=5.530862808227539, loss=3.558572292327881
I0307 02:08:24.788284 140071458023168 logging_writer.py:48] [6000] global_step=6000, grad_norm=7.114993572235107, loss=3.517777919769287
I0307 02:09:03.185745 140071466415872 logging_writer.py:48] [6100] global_step=6100, grad_norm=8.603949546813965, loss=3.4229013919830322
I0307 02:09:41.129561 140071458023168 logging_writer.py:48] [6200] global_step=6200, grad_norm=7.078060150146484, loss=3.154608964920044
I0307 02:10:19.108885 140071466415872 logging_writer.py:48] [6300] global_step=6300, grad_norm=7.239413261413574, loss=3.302567720413208
I0307 02:10:57.415858 140071458023168 logging_writer.py:48] [6400] global_step=6400, grad_norm=7.3185858726501465, loss=3.2293429374694824
I0307 02:11:35.799976 140071466415872 logging_writer.py:48] [6500] global_step=6500, grad_norm=5.894444942474365, loss=3.3128113746643066
I0307 02:12:14.161623 140071458023168 logging_writer.py:48] [6600] global_step=6600, grad_norm=8.22517204284668, loss=3.317622184753418
I0307 02:12:52.393646 140071466415872 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.284214496612549, loss=3.3988919258117676
I0307 02:13:02.800229 140226914178240 spec.py:321] Evaluating on the training split.
I0307 02:13:13.942472 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 02:13:33.652234 140226914178240 spec.py:349] Evaluating on the test split.
I0307 02:13:35.410692 140226914178240 submission_runner.py:469] Time since start: 2871.64s, 	Step: 6728, 	{'train/accuracy': 0.39100366830825806, 'train/loss': 2.7434937953948975, 'validation/accuracy': 0.35189998149871826, 'validation/loss': 2.989835023880005, 'validation/num_examples': 50000, 'test/accuracy': 0.2656000256538391, 'test/loss': 3.6864068508148193, 'test/num_examples': 10000, 'score': 2605.9788506031036, 'total_duration': 2871.6416597366333, 'accumulated_submission_time': 2605.9788506031036, 'accumulated_eval_time': 264.76834535598755, 'accumulated_logging_time': 0.22105741500854492}
I0307 02:13:35.456119 140071458023168 logging_writer.py:48] [6728] accumulated_eval_time=264.768, accumulated_logging_time=0.221057, accumulated_submission_time=2605.98, global_step=6728, preemption_count=0, score=2605.98, test/accuracy=0.2656, test/loss=3.68641, test/num_examples=10000, total_duration=2871.64, train/accuracy=0.391004, train/loss=2.74349, validation/accuracy=0.3519, validation/loss=2.98984, validation/num_examples=50000
I0307 02:14:03.500462 140071466415872 logging_writer.py:48] [6800] global_step=6800, grad_norm=9.031302452087402, loss=3.1922361850738525
I0307 02:14:41.603534 140071458023168 logging_writer.py:48] [6900] global_step=6900, grad_norm=5.528937816619873, loss=3.1160459518432617
I0307 02:15:19.659508 140071466415872 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.323543548583984, loss=3.1452863216400146
I0307 02:15:57.669214 140071458023168 logging_writer.py:48] [7100] global_step=7100, grad_norm=11.069408416748047, loss=3.1076819896698
I0307 02:16:35.936781 140071466415872 logging_writer.py:48] [7200] global_step=7200, grad_norm=10.839272499084473, loss=3.1397194862365723
I0307 02:17:14.475903 140071458023168 logging_writer.py:48] [7300] global_step=7300, grad_norm=10.994474411010742, loss=3.1005139350891113
I0307 02:17:52.528530 140071466415872 logging_writer.py:48] [7400] global_step=7400, grad_norm=11.050776481628418, loss=3.1934783458709717
I0307 02:18:30.766450 140071458023168 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.825355052947998, loss=3.023345947265625
I0307 02:19:09.112826 140071466415872 logging_writer.py:48] [7600] global_step=7600, grad_norm=6.026986122131348, loss=3.016193389892578
I0307 02:19:47.553809 140071458023168 logging_writer.py:48] [7700] global_step=7700, grad_norm=11.673941612243652, loss=2.923431158065796
I0307 02:20:25.710968 140071466415872 logging_writer.py:48] [7800] global_step=7800, grad_norm=6.674468517303467, loss=2.910027265548706
I0307 02:21:03.949608 140071458023168 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.738099575042725, loss=2.9721555709838867
I0307 02:21:42.018092 140071466415872 logging_writer.py:48] [8000] global_step=8000, grad_norm=6.891082286834717, loss=2.927309989929199
I0307 02:22:05.491974 140226914178240 spec.py:321] Evaluating on the training split.
I0307 02:22:18.401415 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 02:22:39.531522 140226914178240 spec.py:349] Evaluating on the test split.
I0307 02:22:41.291758 140226914178240 submission_runner.py:469] Time since start: 3417.52s, 	Step: 8063, 	{'train/accuracy': 0.4448740482330322, 'train/loss': 2.421154737472534, 'validation/accuracy': 0.40591999888420105, 'validation/loss': 2.6750271320343018, 'validation/num_examples': 50000, 'test/accuracy': 0.30560001730918884, 'test/loss': 3.3933873176574707, 'test/num_examples': 10000, 'score': 3115.8826031684875, 'total_duration': 3417.5227279663086, 'accumulated_submission_time': 3115.8826031684875, 'accumulated_eval_time': 300.56809878349304, 'accumulated_logging_time': 0.2833092212677002}
I0307 02:22:41.348134 140071458023168 logging_writer.py:48] [8063] accumulated_eval_time=300.568, accumulated_logging_time=0.283309, accumulated_submission_time=3115.88, global_step=8063, preemption_count=0, score=3115.88, test/accuracy=0.3056, test/loss=3.39339, test/num_examples=10000, total_duration=3417.52, train/accuracy=0.444874, train/loss=2.42115, validation/accuracy=0.40592, validation/loss=2.67503, validation/num_examples=50000
I0307 02:22:55.779448 140071466415872 logging_writer.py:48] [8100] global_step=8100, grad_norm=7.552184104919434, loss=2.8923192024230957
I0307 02:23:34.161868 140071458023168 logging_writer.py:48] [8200] global_step=8200, grad_norm=6.502970218658447, loss=2.9471611976623535
I0307 02:24:12.808050 140071466415872 logging_writer.py:48] [8300] global_step=8300, grad_norm=6.049890995025635, loss=2.8890998363494873
I0307 02:24:50.909488 140071458023168 logging_writer.py:48] [8400] global_step=8400, grad_norm=8.391828536987305, loss=2.845733880996704
I0307 02:25:29.333386 140071466415872 logging_writer.py:48] [8500] global_step=8500, grad_norm=5.6079277992248535, loss=2.749519109725952
I0307 02:26:07.577586 140071458023168 logging_writer.py:48] [8600] global_step=8600, grad_norm=5.204613208770752, loss=2.8411550521850586
I0307 02:26:45.848668 140071466415872 logging_writer.py:48] [8700] global_step=8700, grad_norm=5.736325740814209, loss=2.867339849472046
I0307 02:27:23.909123 140071458023168 logging_writer.py:48] [8800] global_step=8800, grad_norm=6.155250072479248, loss=2.7632365226745605
I0307 02:28:01.897964 140071466415872 logging_writer.py:48] [8900] global_step=8900, grad_norm=6.406046390533447, loss=2.7393674850463867
I0307 02:28:39.829697 140071458023168 logging_writer.py:48] [9000] global_step=9000, grad_norm=5.330280303955078, loss=2.6929991245269775
I0307 02:29:17.672534 140071466415872 logging_writer.py:48] [9100] global_step=9100, grad_norm=5.5952558517456055, loss=2.835724353790283
I0307 02:29:55.704440 140071458023168 logging_writer.py:48] [9200] global_step=9200, grad_norm=5.2685041427612305, loss=2.822065830230713
I0307 02:30:33.642826 140071466415872 logging_writer.py:48] [9300] global_step=9300, grad_norm=6.6727070808410645, loss=2.782146453857422
I0307 02:31:11.608121 140226914178240 spec.py:321] Evaluating on the training split.
I0307 02:31:23.109304 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 02:31:45.936550 140226914178240 spec.py:349] Evaluating on the test split.
I0307 02:31:47.697629 140226914178240 submission_runner.py:469] Time since start: 3963.91s, 	Step: 9400, 	{'train/accuracy': 0.5049226880073547, 'train/loss': 2.1154611110687256, 'validation/accuracy': 0.46025997400283813, 'validation/loss': 2.3665738105773926, 'validation/num_examples': 50000, 'test/accuracy': 0.3505000174045563, 'test/loss': 3.085230588912964, 'test/num_examples': 10000, 'score': 3626.0122554302216, 'total_duration': 3963.9058706760406, 'accumulated_submission_time': 3626.0122554302216, 'accumulated_eval_time': 336.63484930992126, 'accumulated_logging_time': 0.353640079498291}
I0307 02:31:47.820751 140071458023168 logging_writer.py:48] [9400] accumulated_eval_time=336.635, accumulated_logging_time=0.35364, accumulated_submission_time=3626.01, global_step=9400, preemption_count=0, score=3626.01, test/accuracy=0.3505, test/loss=3.08523, test/num_examples=10000, total_duration=3963.91, train/accuracy=0.504923, train/loss=2.11546, validation/accuracy=0.46026, validation/loss=2.36657, validation/num_examples=50000
I0307 02:31:48.224166 140071466415872 logging_writer.py:48] [9400] global_step=9400, grad_norm=5.851225852966309, loss=2.5753259658813477
I0307 02:32:26.492971 140071458023168 logging_writer.py:48] [9500] global_step=9500, grad_norm=7.133732795715332, loss=2.662398338317871
I0307 02:33:04.578895 140071466415872 logging_writer.py:48] [9600] global_step=9600, grad_norm=6.008442401885986, loss=2.7621548175811768
I0307 02:33:42.829215 140071458023168 logging_writer.py:48] [9700] global_step=9700, grad_norm=9.77953052520752, loss=2.7139787673950195
I0307 02:34:21.033497 140071466415872 logging_writer.py:48] [9800] global_step=9800, grad_norm=5.804834365844727, loss=2.6383159160614014
I0307 02:34:59.424304 140071458023168 logging_writer.py:48] [9900] global_step=9900, grad_norm=8.371561050415039, loss=2.6833724975585938
I0307 02:35:37.528455 140071466415872 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.1162428855896, loss=2.6817116737365723
I0307 02:36:15.593825 140071458023168 logging_writer.py:48] [10100] global_step=10100, grad_norm=5.408659934997559, loss=2.608236074447632
I0307 02:36:54.205814 140071466415872 logging_writer.py:48] [10200] global_step=10200, grad_norm=5.386659145355225, loss=2.630488634109497
I0307 02:37:32.585038 140071458023168 logging_writer.py:48] [10300] global_step=10300, grad_norm=7.01328706741333, loss=2.5308690071105957
I0307 02:38:11.020095 140071466415872 logging_writer.py:48] [10400] global_step=10400, grad_norm=7.134242057800293, loss=2.5074551105499268
I0307 02:38:48.842675 140071458023168 logging_writer.py:48] [10500] global_step=10500, grad_norm=5.895739555358887, loss=2.6485824584960938
I0307 02:39:38.342505 140071466415872 logging_writer.py:48] [10600] global_step=10600, grad_norm=5.602097511291504, loss=2.538259983062744
I0307 02:40:17.685442 140226914178240 spec.py:321] Evaluating on the training split.
I0307 02:40:32.009795 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 02:40:55.993760 140226914178240 spec.py:349] Evaluating on the test split.
I0307 02:40:57.728800 140226914178240 submission_runner.py:469] Time since start: 4513.96s, 	Step: 10668, 	{'train/accuracy': 0.5140505433082581, 'train/loss': 2.0655152797698975, 'validation/accuracy': 0.4705599844455719, 'validation/loss': 2.3184568881988525, 'validation/num_examples': 50000, 'test/accuracy': 0.35520002245903015, 'test/loss': 3.0856406688690186, 'test/num_examples': 10000, 'score': 4135.744189023972, 'total_duration': 4513.959764242172, 'accumulated_submission_time': 4135.744189023972, 'accumulated_eval_time': 376.67817425727844, 'accumulated_logging_time': 0.4847090244293213}
I0307 02:40:57.805906 140071458023168 logging_writer.py:48] [10668] accumulated_eval_time=376.678, accumulated_logging_time=0.484709, accumulated_submission_time=4135.74, global_step=10668, preemption_count=0, score=4135.74, test/accuracy=0.3552, test/loss=3.08564, test/num_examples=10000, total_duration=4513.96, train/accuracy=0.514051, train/loss=2.06552, validation/accuracy=0.47056, validation/loss=2.31846, validation/num_examples=50000
I0307 02:41:10.330913 140071466415872 logging_writer.py:48] [10700] global_step=10700, grad_norm=5.026907444000244, loss=2.436762571334839
I0307 02:41:48.516755 140071458023168 logging_writer.py:48] [10800] global_step=10800, grad_norm=7.352560520172119, loss=2.5346033573150635
I0307 02:42:26.840111 140071466415872 logging_writer.py:48] [10900] global_step=10900, grad_norm=5.036489963531494, loss=2.390286445617676
I0307 02:43:05.145046 140071458023168 logging_writer.py:48] [11000] global_step=11000, grad_norm=6.081150054931641, loss=2.5017566680908203
I0307 02:43:43.523411 140071466415872 logging_writer.py:48] [11100] global_step=11100, grad_norm=7.677918434143066, loss=2.375115156173706
I0307 02:44:21.946289 140071458023168 logging_writer.py:48] [11200] global_step=11200, grad_norm=6.56050443649292, loss=2.557115077972412
I0307 02:44:59.977886 140071466415872 logging_writer.py:48] [11300] global_step=11300, grad_norm=8.396387100219727, loss=2.493651866912842
I0307 02:45:38.333963 140071458023168 logging_writer.py:48] [11400] global_step=11400, grad_norm=5.04857063293457, loss=2.2208027839660645
I0307 02:46:16.877194 140071466415872 logging_writer.py:48] [11500] global_step=11500, grad_norm=8.789254188537598, loss=2.383546829223633
I0307 02:46:55.420284 140071458023168 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.261860370635986, loss=2.385777473449707
I0307 02:47:33.876491 140071466415872 logging_writer.py:48] [11700] global_step=11700, grad_norm=6.212775707244873, loss=2.503192663192749
I0307 02:48:12.180035 140071458023168 logging_writer.py:48] [11800] global_step=11800, grad_norm=6.68652868270874, loss=2.4617691040039062
I0307 02:48:50.449031 140071466415872 logging_writer.py:48] [11900] global_step=11900, grad_norm=7.186689376831055, loss=2.3453288078308105
I0307 02:49:27.896622 140226914178240 spec.py:321] Evaluating on the training split.
I0307 02:49:44.594775 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 02:50:07.241903 140226914178240 spec.py:349] Evaluating on the test split.
I0307 02:50:09.012848 140226914178240 submission_runner.py:469] Time since start: 5065.24s, 	Step: 11999, 	{'train/accuracy': 0.5596699714660645, 'train/loss': 1.837249755859375, 'validation/accuracy': 0.5102399587631226, 'validation/loss': 2.1204795837402344, 'validation/num_examples': 50000, 'test/accuracy': 0.38860002160072327, 'test/loss': 2.864274740219116, 'test/num_examples': 10000, 'score': 4645.69776558876, 'total_duration': 5065.243762254715, 'accumulated_submission_time': 4645.69776558876, 'accumulated_eval_time': 417.79432249069214, 'accumulated_logging_time': 0.5691022872924805}
I0307 02:50:09.294803 140071458023168 logging_writer.py:48] [11999] accumulated_eval_time=417.794, accumulated_logging_time=0.569102, accumulated_submission_time=4645.7, global_step=11999, preemption_count=0, score=4645.7, test/accuracy=0.3886, test/loss=2.86427, test/num_examples=10000, total_duration=5065.24, train/accuracy=0.55967, train/loss=1.83725, validation/accuracy=0.51024, validation/loss=2.12048, validation/num_examples=50000
I0307 02:50:10.141228 140071466415872 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.142587184906006, loss=2.577754020690918
I0307 02:50:49.152175 140071458023168 logging_writer.py:48] [12100] global_step=12100, grad_norm=6.12673282623291, loss=2.3440628051757812
I0307 02:51:29.561460 140071466415872 logging_writer.py:48] [12200] global_step=12200, grad_norm=9.177921295166016, loss=2.3779196739196777
I0307 02:52:08.264254 140071458023168 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.6217122077941895, loss=2.358639717102051
I0307 02:52:46.317406 140071466415872 logging_writer.py:48] [12400] global_step=12400, grad_norm=8.279627799987793, loss=2.2801756858825684
I0307 02:53:24.367456 140071458023168 logging_writer.py:48] [12500] global_step=12500, grad_norm=4.832663059234619, loss=2.324850082397461
I0307 02:54:02.953977 140071466415872 logging_writer.py:48] [12600] global_step=12600, grad_norm=11.40740966796875, loss=2.4059457778930664
I0307 02:54:41.505208 140071458023168 logging_writer.py:48] [12700] global_step=12700, grad_norm=7.4290547370910645, loss=2.349794864654541
I0307 02:55:19.892102 140071466415872 logging_writer.py:48] [12800] global_step=12800, grad_norm=7.222967624664307, loss=2.332754611968994
I0307 02:55:58.265512 140071458023168 logging_writer.py:48] [12900] global_step=12900, grad_norm=5.842530727386475, loss=2.2784342765808105
I0307 02:56:36.932847 140071466415872 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.923093795776367, loss=2.324477195739746
I0307 02:57:15.627038 140071458023168 logging_writer.py:48] [13100] global_step=13100, grad_norm=8.485618591308594, loss=2.307605028152466
I0307 02:57:54.032082 140071466415872 logging_writer.py:48] [13200] global_step=13200, grad_norm=7.630488872528076, loss=2.391160488128662
I0307 02:58:32.802397 140071458023168 logging_writer.py:48] [13300] global_step=13300, grad_norm=7.423785209655762, loss=2.339803457260132
I0307 02:58:39.296760 140226914178240 spec.py:321] Evaluating on the training split.
I0307 02:58:55.923881 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 02:59:17.865375 140226914178240 spec.py:349] Evaluating on the test split.
I0307 02:59:19.587471 140226914178240 submission_runner.py:469] Time since start: 5615.82s, 	Step: 13318, 	{'train/accuracy': 0.5774872303009033, 'train/loss': 1.7372361421585083, 'validation/accuracy': 0.5238800048828125, 'validation/loss': 2.025557041168213, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.7279298305511475, 'test/num_examples': 10000, 'score': 5155.5356965065, 'total_duration': 5615.818393707275, 'accumulated_submission_time': 5155.5356965065, 'accumulated_eval_time': 458.0849578380585, 'accumulated_logging_time': 0.8801577091217041}
I0307 02:59:19.757377 140071466415872 logging_writer.py:48] [13318] accumulated_eval_time=458.085, accumulated_logging_time=0.880158, accumulated_submission_time=5155.54, global_step=13318, preemption_count=0, score=5155.54, test/accuracy=0.4104, test/loss=2.72793, test/num_examples=10000, total_duration=5615.82, train/accuracy=0.577487, train/loss=1.73724, validation/accuracy=0.52388, validation/loss=2.02556, validation/num_examples=50000
I0307 02:59:51.758139 140071458023168 logging_writer.py:48] [13400] global_step=13400, grad_norm=6.1374945640563965, loss=2.2774317264556885
I0307 03:00:30.070571 140071466415872 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.731135368347168, loss=2.200225830078125
I0307 03:01:08.678262 140071458023168 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.543050765991211, loss=2.265845775604248
I0307 03:01:47.275208 140071466415872 logging_writer.py:48] [13700] global_step=13700, grad_norm=8.938872337341309, loss=2.301414728164673
I0307 03:02:25.522811 140071458023168 logging_writer.py:48] [13800] global_step=13800, grad_norm=7.50568962097168, loss=2.3954601287841797
I0307 03:03:03.790500 140071466415872 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.0418291091918945, loss=2.189610481262207
I0307 03:03:42.013314 140071458023168 logging_writer.py:48] [14000] global_step=14000, grad_norm=9.290019989013672, loss=2.2210748195648193
I0307 03:04:20.390737 140071466415872 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.2510762214660645, loss=2.3474762439727783
I0307 03:04:58.643225 140071458023168 logging_writer.py:48] [14200] global_step=14200, grad_norm=6.40446138381958, loss=2.380824089050293
I0307 03:05:37.144656 140071466415872 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.709956645965576, loss=2.465921401977539
I0307 03:06:15.193407 140071458023168 logging_writer.py:48] [14400] global_step=14400, grad_norm=6.972991943359375, loss=2.268841505050659
I0307 03:06:53.809958 140071466415872 logging_writer.py:48] [14500] global_step=14500, grad_norm=7.011775493621826, loss=2.285217523574829
I0307 03:07:32.400931 140071458023168 logging_writer.py:48] [14600] global_step=14600, grad_norm=6.701361179351807, loss=2.2158262729644775
I0307 03:07:49.929653 140226914178240 spec.py:321] Evaluating on the training split.
I0307 03:08:06.099619 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 03:08:28.176437 140226914178240 spec.py:349] Evaluating on the test split.
I0307 03:08:29.923409 140226914178240 submission_runner.py:469] Time since start: 6166.15s, 	Step: 14647, 	{'train/accuracy': 0.5940887928009033, 'train/loss': 1.6770062446594238, 'validation/accuracy': 0.5428400039672852, 'validation/loss': 1.9536000490188599, 'validation/num_examples': 50000, 'test/accuracy': 0.41750001907348633, 'test/loss': 2.7149999141693115, 'test/num_examples': 10000, 'score': 5665.5519926548, 'total_duration': 6166.154253959656, 'accumulated_submission_time': 5665.5519926548, 'accumulated_eval_time': 498.07856822013855, 'accumulated_logging_time': 1.0771241188049316}
I0307 03:08:30.134186 140071466415872 logging_writer.py:48] [14647] accumulated_eval_time=498.079, accumulated_logging_time=1.07712, accumulated_submission_time=5665.55, global_step=14647, preemption_count=0, score=5665.55, test/accuracy=0.4175, test/loss=2.715, test/num_examples=10000, total_duration=6166.15, train/accuracy=0.594089, train/loss=1.67701, validation/accuracy=0.54284, validation/loss=1.9536, validation/num_examples=50000
I0307 03:08:50.943413 140071458023168 logging_writer.py:48] [14700] global_step=14700, grad_norm=6.9155120849609375, loss=2.3412747383117676
I0307 03:09:29.646906 140071466415872 logging_writer.py:48] [14800] global_step=14800, grad_norm=7.562277793884277, loss=2.3799781799316406
I0307 03:10:07.555960 140071458023168 logging_writer.py:48] [14900] global_step=14900, grad_norm=7.644420146942139, loss=2.235680341720581
I0307 03:10:45.469037 140071466415872 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.020283222198486, loss=2.1784462928771973
I0307 03:11:23.625569 140071458023168 logging_writer.py:48] [15100] global_step=15100, grad_norm=10.241963386535645, loss=2.248929977416992
I0307 03:12:02.216258 140071466415872 logging_writer.py:48] [15200] global_step=15200, grad_norm=4.507406234741211, loss=2.211948871612549
I0307 03:12:40.593201 140071458023168 logging_writer.py:48] [15300] global_step=15300, grad_norm=7.580501556396484, loss=2.276388645172119
I0307 03:13:18.741165 140071466415872 logging_writer.py:48] [15400] global_step=15400, grad_norm=8.225884437561035, loss=2.253941297531128
I0307 03:13:56.961049 140071458023168 logging_writer.py:48] [15500] global_step=15500, grad_norm=5.375267028808594, loss=2.342226982116699
I0307 03:14:35.491495 140071466415872 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.658909320831299, loss=2.1922388076782227
I0307 03:15:14.432595 140071458023168 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.347294807434082, loss=2.3700497150421143
I0307 03:15:52.799857 140071466415872 logging_writer.py:48] [15800] global_step=15800, grad_norm=6.526965618133545, loss=2.327225923538208
I0307 03:16:31.132600 140071458023168 logging_writer.py:48] [15900] global_step=15900, grad_norm=6.664348602294922, loss=2.3025786876678467
I0307 03:17:00.186738 140226914178240 spec.py:321] Evaluating on the training split.
I0307 03:17:19.815507 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 03:17:44.437776 140226914178240 spec.py:349] Evaluating on the test split.
I0307 03:17:46.200024 140226914178240 submission_runner.py:469] Time since start: 6722.43s, 	Step: 15976, 	{'train/accuracy': 0.5974569320678711, 'train/loss': 1.6548250913619995, 'validation/accuracy': 0.5475999712944031, 'validation/loss': 1.9200537204742432, 'validation/num_examples': 50000, 'test/accuracy': 0.43150001764297485, 'test/loss': 2.635930061340332, 'test/num_examples': 10000, 'score': 6175.442023515701, 'total_duration': 6722.430892705917, 'accumulated_submission_time': 6175.442023515701, 'accumulated_eval_time': 544.0917387008667, 'accumulated_logging_time': 1.3202722072601318}
I0307 03:17:46.498772 140071466415872 logging_writer.py:48] [15976] accumulated_eval_time=544.092, accumulated_logging_time=1.32027, accumulated_submission_time=6175.44, global_step=15976, preemption_count=0, score=6175.44, test/accuracy=0.4315, test/loss=2.63593, test/num_examples=10000, total_duration=6722.43, train/accuracy=0.597457, train/loss=1.65483, validation/accuracy=0.5476, validation/loss=1.92005, validation/num_examples=50000
I0307 03:17:56.275889 140071458023168 logging_writer.py:48] [16000] global_step=16000, grad_norm=5.1384053230285645, loss=2.0936954021453857
I0307 03:18:34.755220 140071466415872 logging_writer.py:48] [16100] global_step=16100, grad_norm=4.273336887359619, loss=2.083641767501831
I0307 03:19:13.149566 140071458023168 logging_writer.py:48] [16200] global_step=16200, grad_norm=4.221654415130615, loss=2.20892596244812
I0307 03:19:51.979632 140071466415872 logging_writer.py:48] [16300] global_step=16300, grad_norm=5.429378986358643, loss=2.2976908683776855
I0307 03:20:30.453457 140071458023168 logging_writer.py:48] [16400] global_step=16400, grad_norm=4.630117416381836, loss=2.1234991550445557
I0307 03:21:08.709432 140071466415872 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.8272809982299805, loss=2.232947587966919
I0307 03:21:46.859033 140071458023168 logging_writer.py:48] [16600] global_step=16600, grad_norm=5.2693634033203125, loss=2.0856151580810547
I0307 03:22:25.095621 140071466415872 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.393387794494629, loss=2.239828586578369
I0307 03:23:03.616931 140071458023168 logging_writer.py:48] [16800] global_step=16800, grad_norm=7.057062149047852, loss=2.212132692337036
I0307 03:23:42.257551 140071466415872 logging_writer.py:48] [16900] global_step=16900, grad_norm=5.9201507568359375, loss=2.2009241580963135
I0307 03:24:20.558949 140071458023168 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.9721271991729736, loss=2.238532066345215
I0307 03:24:59.206390 140071466415872 logging_writer.py:48] [17100] global_step=17100, grad_norm=8.565383911132812, loss=2.2916340827941895
I0307 03:25:37.594294 140071458023168 logging_writer.py:48] [17200] global_step=17200, grad_norm=4.045908451080322, loss=2.0914387702941895
I0307 03:26:15.979747 140071466415872 logging_writer.py:48] [17300] global_step=17300, grad_norm=5.819514274597168, loss=2.175584077835083
I0307 03:26:16.346355 140226914178240 spec.py:321] Evaluating on the training split.
I0307 03:26:31.044573 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 03:26:53.131690 140226914178240 spec.py:349] Evaluating on the test split.
I0307 03:26:54.882640 140226914178240 submission_runner.py:469] Time since start: 7271.11s, 	Step: 17302, 	{'train/accuracy': 0.6120057106018066, 'train/loss': 1.5883805751800537, 'validation/accuracy': 0.5559399724006653, 'validation/loss': 1.879778504371643, 'validation/num_examples': 50000, 'test/accuracy': 0.4369000196456909, 'test/loss': 2.638759136199951, 'test/num_examples': 10000, 'score': 6685.105132341385, 'total_duration': 7271.113498210907, 'accumulated_submission_time': 6685.105132341385, 'accumulated_eval_time': 582.6278820037842, 'accumulated_logging_time': 1.678769826889038}
I0307 03:26:54.939808 140071458023168 logging_writer.py:48] [17302] accumulated_eval_time=582.628, accumulated_logging_time=1.67877, accumulated_submission_time=6685.11, global_step=17302, preemption_count=0, score=6685.11, test/accuracy=0.4369, test/loss=2.63876, test/num_examples=10000, total_duration=7271.11, train/accuracy=0.612006, train/loss=1.58838, validation/accuracy=0.55594, validation/loss=1.87978, validation/num_examples=50000
I0307 03:27:33.236993 140071466415872 logging_writer.py:48] [17400] global_step=17400, grad_norm=5.578124046325684, loss=2.3016881942749023
I0307 03:28:11.832103 140071458023168 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.549263000488281, loss=2.2083475589752197
I0307 03:28:50.187743 140071466415872 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.243374347686768, loss=2.242539644241333
I0307 03:29:28.692671 140071458023168 logging_writer.py:48] [17700] global_step=17700, grad_norm=6.8832573890686035, loss=2.2317142486572266
I0307 03:30:07.306647 140071466415872 logging_writer.py:48] [17800] global_step=17800, grad_norm=6.652030944824219, loss=2.1436946392059326
I0307 03:30:45.805272 140071458023168 logging_writer.py:48] [17900] global_step=17900, grad_norm=6.58793306350708, loss=2.193851947784424
I0307 03:31:24.185459 140071466415872 logging_writer.py:48] [18000] global_step=18000, grad_norm=6.986968994140625, loss=2.1379897594451904
I0307 03:32:02.657360 140071458023168 logging_writer.py:48] [18100] global_step=18100, grad_norm=4.951137065887451, loss=2.103631019592285
I0307 03:32:40.956779 140071466415872 logging_writer.py:48] [18200] global_step=18200, grad_norm=5.87445068359375, loss=2.2530970573425293
I0307 03:33:19.253842 140071458023168 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.174679279327393, loss=2.113118886947632
I0307 03:33:57.935398 140071466415872 logging_writer.py:48] [18400] global_step=18400, grad_norm=5.048953056335449, loss=2.1457407474517822
I0307 03:34:36.128378 140071458023168 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.484411239624023, loss=2.158170223236084
I0307 03:35:14.429975 140071466415872 logging_writer.py:48] [18600] global_step=18600, grad_norm=6.113667011260986, loss=2.168079137802124
I0307 03:35:25.099694 140226914178240 spec.py:321] Evaluating on the training split.
I0307 03:35:38.261841 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 03:35:59.497856 140226914178240 spec.py:349] Evaluating on the test split.
I0307 03:36:01.259414 140226914178240 submission_runner.py:469] Time since start: 7817.49s, 	Step: 18629, 	{'train/accuracy': 0.6077407598495483, 'train/loss': 1.595623254776001, 'validation/accuracy': 0.5612199902534485, 'validation/loss': 1.8499449491500854, 'validation/num_examples': 50000, 'test/accuracy': 0.43060001730918884, 'test/loss': 2.594538927078247, 'test/num_examples': 10000, 'score': 7195.120244264603, 'total_duration': 7817.490381717682, 'accumulated_submission_time': 7195.120244264603, 'accumulated_eval_time': 618.7875738143921, 'accumulated_logging_time': 1.760805368423462}
I0307 03:36:01.343417 140071458023168 logging_writer.py:48] [18629] accumulated_eval_time=618.788, accumulated_logging_time=1.76081, accumulated_submission_time=7195.12, global_step=18629, preemption_count=0, score=7195.12, test/accuracy=0.4306, test/loss=2.59454, test/num_examples=10000, total_duration=7817.49, train/accuracy=0.607741, train/loss=1.59562, validation/accuracy=0.56122, validation/loss=1.84994, validation/num_examples=50000
I0307 03:36:28.881873 140071466415872 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.307682514190674, loss=2.2193191051483154
I0307 03:37:07.342072 140071458023168 logging_writer.py:48] [18800] global_step=18800, grad_norm=5.445573329925537, loss=2.249035358428955
I0307 03:37:46.170600 140071466415872 logging_writer.py:48] [18900] global_step=18900, grad_norm=6.192513942718506, loss=2.123845100402832
I0307 03:38:24.552172 140071458023168 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.303037643432617, loss=2.242152452468872
I0307 03:39:03.183429 140071466415872 logging_writer.py:48] [19100] global_step=19100, grad_norm=6.880370616912842, loss=2.150465965270996
I0307 03:39:41.675026 140071458023168 logging_writer.py:48] [19200] global_step=19200, grad_norm=5.055110454559326, loss=2.1090052127838135
I0307 03:40:20.229950 140071466415872 logging_writer.py:48] [19300] global_step=19300, grad_norm=6.461284160614014, loss=2.1594598293304443
I0307 03:40:58.502217 140071458023168 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.375251770019531, loss=2.2426137924194336
I0307 03:41:37.143982 140071466415872 logging_writer.py:48] [19500] global_step=19500, grad_norm=5.526760578155518, loss=2.160236358642578
I0307 03:42:15.493746 140071458023168 logging_writer.py:48] [19600] global_step=19600, grad_norm=6.102209568023682, loss=2.2130019664764404
I0307 03:42:53.606908 140071466415872 logging_writer.py:48] [19700] global_step=19700, grad_norm=7.0408501625061035, loss=2.1151816844940186
I0307 03:43:32.006525 140071458023168 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.40122652053833, loss=2.1651735305786133
I0307 03:44:10.092991 140071466415872 logging_writer.py:48] [19900] global_step=19900, grad_norm=6.131735801696777, loss=2.1317479610443115
I0307 03:44:31.640101 140226914178240 spec.py:321] Evaluating on the training split.
I0307 03:44:46.642129 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 03:45:07.582948 140226914178240 spec.py:349] Evaluating on the test split.
I0307 03:45:09.331461 140226914178240 submission_runner.py:469] Time since start: 8365.56s, 	Step: 19957, 	{'train/accuracy': 0.6183633208274841, 'train/loss': 1.564002513885498, 'validation/accuracy': 0.5699599981307983, 'validation/loss': 1.8115588426589966, 'validation/num_examples': 50000, 'test/accuracy': 0.4475000202655792, 'test/loss': 2.5293376445770264, 'test/num_examples': 10000, 'score': 7705.289101362228, 'total_duration': 8365.562325716019, 'accumulated_submission_time': 7705.289101362228, 'accumulated_eval_time': 656.4788012504578, 'accumulated_logging_time': 1.8531458377838135}
I0307 03:45:09.426465 140071458023168 logging_writer.py:48] [19957] accumulated_eval_time=656.479, accumulated_logging_time=1.85315, accumulated_submission_time=7705.29, global_step=19957, preemption_count=0, score=7705.29, test/accuracy=0.4475, test/loss=2.52934, test/num_examples=10000, total_duration=8365.56, train/accuracy=0.618363, train/loss=1.564, validation/accuracy=0.56996, validation/loss=1.81156, validation/num_examples=50000
I0307 03:45:26.397228 140071466415872 logging_writer.py:48] [20000] global_step=20000, grad_norm=5.607553005218506, loss=2.00482439994812
I0307 03:46:04.879047 140071458023168 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.639947891235352, loss=2.0888829231262207
I0307 03:46:43.396174 140071466415872 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.982700824737549, loss=2.2944388389587402
I0307 03:47:21.796545 140071458023168 logging_writer.py:48] [20300] global_step=20300, grad_norm=4.724966526031494, loss=2.1533520221710205
I0307 03:48:00.417040 140071466415872 logging_writer.py:48] [20400] global_step=20400, grad_norm=4.217376232147217, loss=2.172794818878174
I0307 03:48:38.897122 140071458023168 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.213912487030029, loss=2.14306378364563
I0307 03:49:17.359393 140071466415872 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.619041919708252, loss=2.151099681854248
I0307 03:49:56.203238 140071458023168 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.273012161254883, loss=1.9896178245544434
I0307 03:50:34.580081 140071466415872 logging_writer.py:48] [20800] global_step=20800, grad_norm=5.409919261932373, loss=2.09647536277771
I0307 03:51:13.135018 140071458023168 logging_writer.py:48] [20900] global_step=20900, grad_norm=7.045781135559082, loss=2.0858733654022217
I0307 03:51:51.170096 140071466415872 logging_writer.py:48] [21000] global_step=21000, grad_norm=4.602990627288818, loss=2.1665713787078857
I0307 03:52:29.351356 140071458023168 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.891890048980713, loss=2.0820136070251465
I0307 03:53:07.711499 140071466415872 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.7961456775665283, loss=2.178986072540283
I0307 03:53:39.581792 140226914178240 spec.py:321] Evaluating on the training split.
I0307 03:53:52.570523 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 03:54:15.246813 140226914178240 spec.py:349] Evaluating on the test split.
I0307 03:54:16.976836 140226914178240 submission_runner.py:469] Time since start: 8913.20s, 	Step: 21284, 	{'train/accuracy': 0.6061463356018066, 'train/loss': 1.610395073890686, 'validation/accuracy': 0.5586599707603455, 'validation/loss': 1.877277135848999, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.6211726665496826, 'test/num_examples': 10000, 'score': 8215.293786525726, 'total_duration': 8913.197173833847, 'accumulated_submission_time': 8215.293786525726, 'accumulated_eval_time': 693.8631854057312, 'accumulated_logging_time': 1.9765114784240723}
I0307 03:54:17.017883 140071458023168 logging_writer.py:48] [21284] accumulated_eval_time=693.863, accumulated_logging_time=1.97651, accumulated_submission_time=8215.29, global_step=21284, preemption_count=0, score=8215.29, test/accuracy=0.4353, test/loss=2.62117, test/num_examples=10000, total_duration=8913.2, train/accuracy=0.606146, train/loss=1.6104, validation/accuracy=0.55866, validation/loss=1.87728, validation/num_examples=50000
I0307 03:54:23.569463 140071466415872 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.885698318481445, loss=2.1281747817993164
I0307 03:55:01.888378 140071458023168 logging_writer.py:48] [21400] global_step=21400, grad_norm=5.683526515960693, loss=2.0332577228546143
I0307 03:55:40.118124 140071466415872 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.002173900604248, loss=1.9877551794052124
I0307 03:56:18.624648 140071458023168 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.873804569244385, loss=2.249371290206909
I0307 03:56:56.972733 140071466415872 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.180107116699219, loss=2.1770176887512207
I0307 03:57:35.434484 140071458023168 logging_writer.py:48] [21800] global_step=21800, grad_norm=4.909606456756592, loss=2.243393898010254
I0307 03:58:13.697428 140071466415872 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.4466118812561035, loss=2.100860834121704
I0307 03:58:52.171285 140071458023168 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.3914265632629395, loss=2.1745824813842773
I0307 03:59:30.727054 140071466415872 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.7923130989074707, loss=2.0266239643096924
I0307 04:00:09.008836 140071458023168 logging_writer.py:48] [22200] global_step=22200, grad_norm=4.526468276977539, loss=2.0718955993652344
I0307 04:00:47.351778 140071466415872 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.801348924636841, loss=2.032489538192749
I0307 04:01:25.499032 140071458023168 logging_writer.py:48] [22400] global_step=22400, grad_norm=5.136545658111572, loss=2.1032822132110596
I0307 04:02:03.269469 140071466415872 logging_writer.py:48] [22500] global_step=22500, grad_norm=6.091230869293213, loss=2.2194089889526367
I0307 04:02:41.931898 140071458023168 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.061919689178467, loss=2.0930938720703125
I0307 04:02:47.320966 140226914178240 spec.py:321] Evaluating on the training split.
I0307 04:03:04.323336 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 04:03:25.338929 140226914178240 spec.py:349] Evaluating on the test split.
I0307 04:03:27.098178 140226914178240 submission_runner.py:469] Time since start: 9463.33s, 	Step: 22615, 	{'train/accuracy': 0.6229273080825806, 'train/loss': 1.5311996936798096, 'validation/accuracy': 0.57669997215271, 'validation/loss': 1.7879992723464966, 'validation/num_examples': 50000, 'test/accuracy': 0.4555000364780426, 'test/loss': 2.546802043914795, 'test/num_examples': 10000, 'score': 8725.43916463852, 'total_duration': 9463.329138278961, 'accumulated_submission_time': 8725.43916463852, 'accumulated_eval_time': 733.6403560638428, 'accumulated_logging_time': 2.053323268890381}
I0307 04:03:27.171828 140071466415872 logging_writer.py:48] [22615] accumulated_eval_time=733.64, accumulated_logging_time=2.05332, accumulated_submission_time=8725.44, global_step=22615, preemption_count=0, score=8725.44, test/accuracy=0.4555, test/loss=2.5468, test/num_examples=10000, total_duration=9463.33, train/accuracy=0.622927, train/loss=1.5312, validation/accuracy=0.5767, validation/loss=1.788, validation/num_examples=50000
I0307 04:04:00.064183 140071458023168 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.8485093116760254, loss=2.038242816925049
I0307 04:04:38.211133 140071466415872 logging_writer.py:48] [22800] global_step=22800, grad_norm=4.149588108062744, loss=2.1969504356384277
I0307 04:05:16.573137 140071458023168 logging_writer.py:48] [22900] global_step=22900, grad_norm=5.302679538726807, loss=2.1390583515167236
I0307 04:05:54.917867 140071466415872 logging_writer.py:48] [23000] global_step=23000, grad_norm=4.634174823760986, loss=2.017322063446045
I0307 04:06:33.322088 140071458023168 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.2110748291015625, loss=2.0274550914764404
I0307 04:07:11.665974 140071466415872 logging_writer.py:48] [23200] global_step=23200, grad_norm=4.841528415679932, loss=2.0135903358459473
I0307 04:07:50.469780 140071458023168 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.797333240509033, loss=2.050853729248047
I0307 04:08:28.895191 140071466415872 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.871821403503418, loss=2.0126256942749023
I0307 04:09:07.146408 140071458023168 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.5719990730285645, loss=2.1124427318573
I0307 04:09:45.460023 140071466415872 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.887228488922119, loss=2.1018776893615723
I0307 04:10:23.705643 140071458023168 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.7314794063568115, loss=2.11344575881958
I0307 04:11:02.289364 140071466415872 logging_writer.py:48] [23800] global_step=23800, grad_norm=4.297492504119873, loss=2.1102046966552734
I0307 04:11:40.800627 140071458023168 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.780561685562134, loss=2.14965558052063
I0307 04:11:57.104717 140226914178240 spec.py:321] Evaluating on the training split.
I0307 04:12:12.446738 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 04:12:34.046518 140226914178240 spec.py:349] Evaluating on the test split.
I0307 04:12:35.847995 140226914178240 submission_runner.py:469] Time since start: 10012.08s, 	Step: 23944, 	{'train/accuracy': 0.6232661008834839, 'train/loss': 1.5279592275619507, 'validation/accuracy': 0.5792399644851685, 'validation/loss': 1.7681270837783813, 'validation/num_examples': 50000, 'test/accuracy': 0.45320001244544983, 'test/loss': 2.5017197132110596, 'test/num_examples': 10000, 'score': 9235.230093240738, 'total_duration': 10012.078787088394, 'accumulated_submission_time': 9235.230093240738, 'accumulated_eval_time': 772.3834238052368, 'accumulated_logging_time': 2.144423484802246}
I0307 04:12:35.935500 140071466415872 logging_writer.py:48] [23944] accumulated_eval_time=772.383, accumulated_logging_time=2.14442, accumulated_submission_time=9235.23, global_step=23944, preemption_count=0, score=9235.23, test/accuracy=0.4532, test/loss=2.50172, test/num_examples=10000, total_duration=10012.1, train/accuracy=0.623266, train/loss=1.52796, validation/accuracy=0.57924, validation/loss=1.76813, validation/num_examples=50000
I0307 04:12:57.965436 140071458023168 logging_writer.py:48] [24000] global_step=24000, grad_norm=4.0290117263793945, loss=2.046354055404663
I0307 04:13:36.293456 140071466415872 logging_writer.py:48] [24100] global_step=24100, grad_norm=4.209035873413086, loss=2.0349936485290527
I0307 04:14:14.745068 140071458023168 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.647907018661499, loss=1.9775723218917847
I0307 04:14:52.653639 140071466415872 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.5991621017456055, loss=2.0898311138153076
I0307 04:15:30.880094 140071458023168 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.734837055206299, loss=2.0954535007476807
I0307 04:16:09.669458 140071466415872 logging_writer.py:48] [24500] global_step=24500, grad_norm=4.649010181427002, loss=2.1104846000671387
I0307 04:16:48.151665 140071458023168 logging_writer.py:48] [24600] global_step=24600, grad_norm=4.873241424560547, loss=2.109236240386963
I0307 04:17:26.829976 140071466415872 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.8016886711120605, loss=1.9781787395477295
I0307 04:18:05.290468 140071458023168 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.4365768432617188, loss=2.057584285736084
I0307 04:18:43.675797 140071466415872 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.315683603286743, loss=2.0034589767456055
I0307 04:19:22.065358 140071458023168 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.0010924339294434, loss=2.0933451652526855
I0307 04:20:00.832918 140071466415872 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.8000195026397705, loss=2.016401529312134
I0307 04:20:39.672702 140071458023168 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.4076833724975586, loss=2.120300531387329
I0307 04:21:05.878483 140226914178240 spec.py:321] Evaluating on the training split.
I0307 04:21:22.006565 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 04:21:45.471515 140226914178240 spec.py:349] Evaluating on the test split.
I0307 04:21:47.221919 140226914178240 submission_runner.py:469] Time since start: 10563.45s, 	Step: 25269, 	{'train/accuracy': 0.6238440275192261, 'train/loss': 1.5286612510681152, 'validation/accuracy': 0.5754799842834473, 'validation/loss': 1.7961288690567017, 'validation/num_examples': 50000, 'test/accuracy': 0.45830002427101135, 'test/loss': 2.50943922996521, 'test/num_examples': 10000, 'score': 9745.036061286926, 'total_duration': 10563.452746152878, 'accumulated_submission_time': 9745.036061286926, 'accumulated_eval_time': 813.726686000824, 'accumulated_logging_time': 2.2400803565979004}
I0307 04:21:47.329544 140071466415872 logging_writer.py:48] [25269] accumulated_eval_time=813.727, accumulated_logging_time=2.24008, accumulated_submission_time=9745.04, global_step=25269, preemption_count=0, score=9745.04, test/accuracy=0.4583, test/loss=2.50944, test/num_examples=10000, total_duration=10563.5, train/accuracy=0.623844, train/loss=1.52866, validation/accuracy=0.57548, validation/loss=1.79613, validation/num_examples=50000
I0307 04:21:59.598908 140071458023168 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.519327402114868, loss=2.087592840194702
I0307 04:22:37.890684 140071466415872 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.7891790866851807, loss=2.155521869659424
I0307 04:23:16.243516 140071458023168 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.2808332443237305, loss=1.9896228313446045
I0307 04:23:54.366105 140071466415872 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.7822341918945312, loss=2.177457571029663
I0307 04:24:32.867705 140071458023168 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.3985133171081543, loss=2.060279130935669
I0307 04:25:11.608781 140071466415872 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.8392174243927, loss=1.9595240354537964
I0307 04:25:50.032025 140071458023168 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.429725408554077, loss=2.03295636177063
I0307 04:26:28.276804 140071466415872 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.810910224914551, loss=2.042463779449463
I0307 04:27:06.891968 140071458023168 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.7850453853607178, loss=2.065544843673706
I0307 04:27:45.479062 140071466415872 logging_writer.py:48] [26200] global_step=26200, grad_norm=4.244165420532227, loss=2.0063791275024414
I0307 04:28:23.928934 140071458023168 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.4968955516815186, loss=2.193167209625244
I0307 04:29:01.926068 140071466415872 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.2283928394317627, loss=1.9903688430786133
I0307 04:29:40.057410 140071458023168 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.2981040477752686, loss=1.9584842920303345
I0307 04:30:17.234826 140226914178240 spec.py:321] Evaluating on the training split.
I0307 04:30:32.599978 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 04:30:54.920271 140226914178240 spec.py:349] Evaluating on the test split.
I0307 04:30:56.659415 140226914178240 submission_runner.py:469] Time since start: 11112.89s, 	Step: 26597, 	{'train/accuracy': 0.6203364133834839, 'train/loss': 1.5316355228424072, 'validation/accuracy': 0.5739200115203857, 'validation/loss': 1.7856478691101074, 'validation/num_examples': 50000, 'test/accuracy': 0.456900030374527, 'test/loss': 2.4875850677490234, 'test/num_examples': 10000, 'score': 10254.811584949493, 'total_duration': 11112.890194416046, 'accumulated_submission_time': 10254.811584949493, 'accumulated_eval_time': 853.15105509758, 'accumulated_logging_time': 2.355114221572876}
I0307 04:30:56.775417 140071466415872 logging_writer.py:48] [26597] accumulated_eval_time=853.151, accumulated_logging_time=2.35511, accumulated_submission_time=10254.8, global_step=26597, preemption_count=0, score=10254.8, test/accuracy=0.4569, test/loss=2.48759, test/num_examples=10000, total_duration=11112.9, train/accuracy=0.620336, train/loss=1.53164, validation/accuracy=0.57392, validation/loss=1.78565, validation/num_examples=50000
I0307 04:30:58.366798 140071458023168 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.7459471225738525, loss=2.1188905239105225
I0307 04:31:36.894721 140071466415872 logging_writer.py:48] [26700] global_step=26700, grad_norm=4.18794059753418, loss=2.0201542377471924
I0307 04:32:15.455914 140071458023168 logging_writer.py:48] [26800] global_step=26800, grad_norm=4.033416748046875, loss=2.039720296859741
I0307 04:32:53.595689 140071466415872 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.6775550842285156, loss=2.027719736099243
I0307 04:33:31.875556 140071458023168 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.0890164375305176, loss=1.9550821781158447
I0307 04:34:10.130860 140071466415872 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.9337356090545654, loss=1.9621062278747559
I0307 04:34:48.395508 140071458023168 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.7015116214752197, loss=2.0909957885742188
I0307 04:35:26.806550 140071466415872 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.3715879917144775, loss=1.9862644672393799
I0307 04:36:05.230558 140071458023168 logging_writer.py:48] [27400] global_step=27400, grad_norm=4.414215564727783, loss=1.897207498550415
I0307 04:36:43.620404 140071466415872 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.87679386138916, loss=1.9077410697937012
I0307 04:37:22.206139 140071458023168 logging_writer.py:48] [27600] global_step=27600, grad_norm=4.788886547088623, loss=2.0028252601623535
I0307 04:38:00.347406 140071466415872 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.3601083755493164, loss=2.106548309326172
I0307 04:38:38.551653 140071458023168 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.6517910957336426, loss=2.0603597164154053
I0307 04:39:17.160077 140071466415872 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.8554766178131104, loss=2.03961181640625
I0307 04:39:26.758353 140226914178240 spec.py:321] Evaluating on the training split.
I0307 04:39:44.402957 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 04:40:05.651969 140226914178240 spec.py:349] Evaluating on the test split.
I0307 04:40:07.428599 140226914178240 submission_runner.py:469] Time since start: 11663.66s, 	Step: 27926, 	{'train/accuracy': 0.6326729655265808, 'train/loss': 1.4903630018234253, 'validation/accuracy': 0.5846199989318848, 'validation/loss': 1.7444636821746826, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.473721504211426, 'test/num_examples': 10000, 'score': 10764.65004658699, 'total_duration': 11663.659442186356, 'accumulated_submission_time': 10764.65004658699, 'accumulated_eval_time': 893.8211438655853, 'accumulated_logging_time': 2.4903788566589355}
I0307 04:40:07.507821 140071458023168 logging_writer.py:48] [27926] accumulated_eval_time=893.821, accumulated_logging_time=2.49038, accumulated_submission_time=10764.7, global_step=27926, preemption_count=0, score=10764.7, test/accuracy=0.4622, test/loss=2.47372, test/num_examples=10000, total_duration=11663.7, train/accuracy=0.632673, train/loss=1.49036, validation/accuracy=0.58462, validation/loss=1.74446, validation/num_examples=50000
I0307 04:40:36.470144 140071466415872 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.2136688232421875, loss=1.923254132270813
I0307 04:41:14.853879 140071458023168 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.038960933685303, loss=2.0110855102539062
I0307 04:41:53.277688 140071466415872 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.983350992202759, loss=2.1152524948120117
I0307 04:42:31.249206 140071458023168 logging_writer.py:48] [28300] global_step=28300, grad_norm=4.014707565307617, loss=1.9211431741714478
I0307 04:43:09.736966 140071466415872 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.9314346313476562, loss=2.0779738426208496
I0307 04:43:48.042151 140071458023168 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.575446128845215, loss=1.9842385053634644
I0307 04:44:26.535524 140071466415872 logging_writer.py:48] [28600] global_step=28600, grad_norm=4.905262470245361, loss=1.9857512712478638
I0307 04:45:04.931183 140071458023168 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.7434120178222656, loss=1.9276790618896484
I0307 04:45:43.109019 140071466415872 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.739882469177246, loss=1.9698175191879272
I0307 04:46:21.669326 140071458023168 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.654102325439453, loss=2.0861334800720215
I0307 04:46:59.889431 140071466415872 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.75174880027771, loss=1.9139463901519775
I0307 04:47:37.880292 140071458023168 logging_writer.py:48] [29100] global_step=29100, grad_norm=4.420015811920166, loss=2.0141992568969727
I0307 04:48:16.450841 140071466415872 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.8305253982543945, loss=1.9626185894012451
I0307 04:48:37.788013 140226914178240 spec.py:321] Evaluating on the training split.
I0307 04:48:52.928376 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 04:49:17.195025 140226914178240 spec.py:349] Evaluating on the test split.
I0307 04:49:18.952468 140226914178240 submission_runner.py:469] Time since start: 12215.18s, 	Step: 29257, 	{'train/accuracy': 0.6325334906578064, 'train/loss': 1.4839988946914673, 'validation/accuracy': 0.5856199860572815, 'validation/loss': 1.7362080812454224, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.4623210430145264, 'test/num_examples': 10000, 'score': 11274.75269126892, 'total_duration': 12215.183251619339, 'accumulated_submission_time': 11274.75269126892, 'accumulated_eval_time': 934.9853844642639, 'accumulated_logging_time': 2.6224234104156494}
I0307 04:49:19.081644 140071458023168 logging_writer.py:48] [29257] accumulated_eval_time=934.985, accumulated_logging_time=2.62242, accumulated_submission_time=11274.8, global_step=29257, preemption_count=0, score=11274.8, test/accuracy=0.4648, test/loss=2.46232, test/num_examples=10000, total_duration=12215.2, train/accuracy=0.632533, train/loss=1.484, validation/accuracy=0.58562, validation/loss=1.73621, validation/num_examples=50000
I0307 04:49:36.227349 140071466415872 logging_writer.py:48] [29300] global_step=29300, grad_norm=4.651507377624512, loss=1.9988888502120972
I0307 04:50:14.472424 140071458023168 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.6395981311798096, loss=1.878101110458374
I0307 04:50:52.815539 140071466415872 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.376927375793457, loss=1.8737813234329224
I0307 04:51:30.937394 140071458023168 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.5990030765533447, loss=1.9973855018615723
I0307 04:52:09.307400 140071466415872 logging_writer.py:48] [29700] global_step=29700, grad_norm=4.632218360900879, loss=1.9206798076629639
I0307 04:52:47.944798 140071458023168 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.595089912414551, loss=1.9013123512268066
I0307 04:53:26.016312 140071466415872 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.520612955093384, loss=1.9728158712387085
I0307 04:54:04.339106 140071458023168 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.8617069721221924, loss=2.0465452671051025
I0307 04:54:42.360945 140071466415872 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.2409508228302, loss=2.0588526725769043
I0307 04:55:20.592766 140071458023168 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.1376214027404785, loss=2.0096662044525146
I0307 04:55:58.914846 140071466415872 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.752915620803833, loss=2.106600046157837
I0307 04:56:37.696104 140071458023168 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.3323752880096436, loss=1.8894691467285156
I0307 04:57:16.074877 140071466415872 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.305454969406128, loss=2.0439438819885254
I0307 04:57:48.972368 140226914178240 spec.py:321] Evaluating on the training split.
I0307 04:58:03.258006 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 04:58:27.517467 140226914178240 spec.py:349] Evaluating on the test split.
I0307 04:58:29.271176 140226914178240 submission_runner.py:469] Time since start: 12765.50s, 	Step: 30587, 	{'train/accuracy': 0.634187638759613, 'train/loss': 1.4777313470840454, 'validation/accuracy': 0.5841599702835083, 'validation/loss': 1.73117995262146, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.472856283187866, 'test/num_examples': 10000, 'score': 11784.496736764908, 'total_duration': 12765.502014160156, 'accumulated_submission_time': 11784.496736764908, 'accumulated_eval_time': 975.2840321063995, 'accumulated_logging_time': 2.7707552909851074}
I0307 04:58:29.371509 140071458023168 logging_writer.py:48] [30587] accumulated_eval_time=975.284, accumulated_logging_time=2.77076, accumulated_submission_time=11784.5, global_step=30587, preemption_count=0, score=11784.5, test/accuracy=0.4604, test/loss=2.47286, test/num_examples=10000, total_duration=12765.5, train/accuracy=0.634188, train/loss=1.47773, validation/accuracy=0.58416, validation/loss=1.73118, validation/num_examples=50000
I0307 04:58:34.815279 140071466415872 logging_writer.py:48] [30600] global_step=30600, grad_norm=4.4485979080200195, loss=1.9822477102279663
I0307 04:59:13.307895 140071458023168 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.5437262058258057, loss=2.0230553150177
I0307 04:59:51.485438 140071466415872 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.959226369857788, loss=2.0229175090789795
I0307 05:00:29.972655 140071458023168 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.725280284881592, loss=1.969081997871399
I0307 05:01:08.623846 140071466415872 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.7202773094177246, loss=1.9343276023864746
I0307 05:01:47.261144 140071458023168 logging_writer.py:48] [31100] global_step=31100, grad_norm=4.559506416320801, loss=1.8426278829574585
I0307 05:02:25.570825 140071466415872 logging_writer.py:48] [31200] global_step=31200, grad_norm=4.065290451049805, loss=1.902831792831421
I0307 05:03:03.954466 140071458023168 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.041893482208252, loss=2.0716333389282227
I0307 05:03:42.584452 140071466415872 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.37198805809021, loss=2.0016376972198486
I0307 05:04:20.983698 140071458023168 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.4313220977783203, loss=1.956963062286377
I0307 05:04:59.463803 140071466415872 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.7934608459472656, loss=2.0793097019195557
I0307 05:05:37.807203 140071458023168 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.3298380374908447, loss=1.8975945711135864
I0307 05:06:16.172544 140071466415872 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.6597824096679688, loss=1.8225232362747192
I0307 05:06:54.340605 140071458023168 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.466538429260254, loss=1.983901023864746
I0307 05:06:59.299815 140226914178240 spec.py:321] Evaluating on the training split.
I0307 05:07:16.583279 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 05:07:37.187756 140226914178240 spec.py:349] Evaluating on the test split.
I0307 05:07:38.928650 140226914178240 submission_runner.py:469] Time since start: 13315.16s, 	Step: 31914, 	{'train/accuracy': 0.6349449753761292, 'train/loss': 1.470556378364563, 'validation/accuracy': 0.5889999866485596, 'validation/loss': 1.704036831855774, 'validation/num_examples': 50000, 'test/accuracy': 0.458700031042099, 'test/loss': 2.4663188457489014, 'test/num_examples': 10000, 'score': 12294.268924713135, 'total_duration': 13315.159407377243, 'accumulated_submission_time': 12294.268924713135, 'accumulated_eval_time': 1014.9126207828522, 'accumulated_logging_time': 2.898590087890625}
I0307 05:07:39.023741 140071466415872 logging_writer.py:48] [31914] accumulated_eval_time=1014.91, accumulated_logging_time=2.89859, accumulated_submission_time=12294.3, global_step=31914, preemption_count=0, score=12294.3, test/accuracy=0.4587, test/loss=2.46632, test/num_examples=10000, total_duration=13315.2, train/accuracy=0.634945, train/loss=1.47056, validation/accuracy=0.589, validation/loss=1.70404, validation/num_examples=50000
I0307 05:08:12.553001 140071458023168 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.7594878673553467, loss=1.9871670007705688
I0307 05:08:50.855846 140071466415872 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.988032102584839, loss=1.9573614597320557
I0307 05:09:29.501402 140071458023168 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.9138784408569336, loss=1.9708046913146973
I0307 05:10:07.926608 140071466415872 logging_writer.py:48] [32300] global_step=32300, grad_norm=4.220608711242676, loss=2.043189287185669
I0307 05:10:46.352764 140071458023168 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.2607784271240234, loss=1.9919160604476929
I0307 05:11:24.752990 140071466415872 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.6836156845092773, loss=1.9134469032287598
I0307 05:12:03.490062 140071458023168 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.2558443546295166, loss=1.8873428106307983
I0307 05:12:42.038653 140071466415872 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.767657518386841, loss=2.013057231903076
I0307 05:13:20.389267 140071458023168 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.263822078704834, loss=1.8677693605422974
I0307 05:13:58.744910 140071466415872 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.750319719314575, loss=1.8366048336029053
I0307 05:14:37.205311 140071458023168 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.999560594558716, loss=1.9538698196411133
I0307 05:15:15.933736 140071466415872 logging_writer.py:48] [33100] global_step=33100, grad_norm=4.951220512390137, loss=2.068711042404175
I0307 05:15:54.428582 140071458023168 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.7011239528656006, loss=1.8450734615325928
I0307 05:16:08.997344 140226914178240 spec.py:321] Evaluating on the training split.
I0307 05:16:20.345827 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 05:16:49.261930 140226914178240 spec.py:349] Evaluating on the test split.
I0307 05:16:51.279821 140226914178240 submission_runner.py:469] Time since start: 13867.51s, 	Step: 33239, 	{'train/accuracy': 0.6412029266357422, 'train/loss': 1.4462796449661255, 'validation/accuracy': 0.5975599884986877, 'validation/loss': 1.6847479343414307, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.4038889408111572, 'test/num_examples': 10000, 'score': 12804.07841539383, 'total_duration': 13867.510663032532, 'accumulated_submission_time': 12804.07841539383, 'accumulated_eval_time': 1057.1949396133423, 'accumulated_logging_time': 3.0320885181427}
I0307 05:16:51.354463 140071466415872 logging_writer.py:48] [33239] accumulated_eval_time=1057.19, accumulated_logging_time=3.03209, accumulated_submission_time=12804.1, global_step=33239, preemption_count=0, score=12804.1, test/accuracy=0.4645, test/loss=2.40389, test/num_examples=10000, total_duration=13867.5, train/accuracy=0.641203, train/loss=1.44628, validation/accuracy=0.59756, validation/loss=1.68475, validation/num_examples=50000
I0307 05:17:15.435232 140071458023168 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.2283220291137695, loss=1.9675109386444092
I0307 05:17:53.766248 140071466415872 logging_writer.py:48] [33400] global_step=33400, grad_norm=4.228402614593506, loss=2.0358827114105225
I0307 05:18:32.328151 140071458023168 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.283250331878662, loss=2.05482816696167
I0307 05:19:10.715363 140071466415872 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.6339151859283447, loss=2.03662371635437
I0307 05:19:49.429826 140071458023168 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.7691233158111572, loss=1.9696707725524902
I0307 05:20:28.103179 140071466415872 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.816631555557251, loss=2.0224790573120117
I0307 05:21:06.450551 140071458023168 logging_writer.py:48] [33900] global_step=33900, grad_norm=4.1635308265686035, loss=2.0404117107391357
I0307 05:21:44.649676 140071466415872 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.212779998779297, loss=1.956723690032959
I0307 05:22:22.916535 140071458023168 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.8228044509887695, loss=1.9083435535430908
I0307 05:23:01.241007 140071466415872 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.925187587738037, loss=1.9362549781799316
I0307 05:23:39.746217 140071458023168 logging_writer.py:48] [34300] global_step=34300, grad_norm=4.014214992523193, loss=2.000805377960205
I0307 05:24:18.046865 140071466415872 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.2843410968780518, loss=2.028043270111084
I0307 05:24:56.802803 140071458023168 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.8847408294677734, loss=1.9097892045974731
I0307 05:25:21.434124 140226914178240 spec.py:321] Evaluating on the training split.
I0307 05:25:33.488551 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 05:25:59.735474 140226914178240 spec.py:349] Evaluating on the test split.
I0307 05:26:01.464851 140226914178240 submission_runner.py:469] Time since start: 14417.70s, 	Step: 34565, 	{'train/accuracy': 0.6437938213348389, 'train/loss': 1.4214361906051636, 'validation/accuracy': 0.597819983959198, 'validation/loss': 1.6649647951126099, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3546218872070312, 'test/num_examples': 10000, 'score': 13314.004073619843, 'total_duration': 14417.695690870285, 'accumulated_submission_time': 13314.004073619843, 'accumulated_eval_time': 1097.2255036830902, 'accumulated_logging_time': 3.135422706604004}
I0307 05:26:01.539562 140071466415872 logging_writer.py:48] [34565] accumulated_eval_time=1097.23, accumulated_logging_time=3.13542, accumulated_submission_time=13314, global_step=34565, preemption_count=0, score=13314, test/accuracy=0.4834, test/loss=2.35462, test/num_examples=10000, total_duration=14417.7, train/accuracy=0.643794, train/loss=1.42144, validation/accuracy=0.59782, validation/loss=1.66496, validation/num_examples=50000
I0307 05:26:15.374082 140071458023168 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.9709560871124268, loss=1.9298083782196045
I0307 05:26:53.457046 140071466415872 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.9908649921417236, loss=2.032395362854004
I0307 05:27:31.929039 140071458023168 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.2548699378967285, loss=1.929836392402649
I0307 05:28:10.312741 140071466415872 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.407640218734741, loss=1.8297343254089355
I0307 05:28:48.717244 140071458023168 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.566204071044922, loss=1.8179023265838623
I0307 05:29:27.149886 140071466415872 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.704761505126953, loss=1.9773926734924316
I0307 05:30:05.787991 140071458023168 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.938371419906616, loss=1.9401174783706665
I0307 05:30:44.452936 140071466415872 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.199308395385742, loss=2.0717411041259766
I0307 05:31:22.554920 140071458023168 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.304219961166382, loss=2.0496697425842285
I0307 05:32:01.044390 140071466415872 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.214226245880127, loss=1.9147121906280518
I0307 05:32:39.705284 140071458023168 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.6199886798858643, loss=1.801164150238037
I0307 05:33:18.121532 140071466415872 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.507688045501709, loss=1.9341850280761719
I0307 05:33:56.669948 140071458023168 logging_writer.py:48] [35800] global_step=35800, grad_norm=4.228662967681885, loss=1.9171351194381714
I0307 05:34:31.808512 140226914178240 spec.py:321] Evaluating on the training split.
I0307 05:34:43.444061 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 05:35:08.228139 140226914178240 spec.py:349] Evaluating on the test split.
I0307 05:35:09.941223 140226914178240 submission_runner.py:469] Time since start: 14966.17s, 	Step: 35893, 	{'train/accuracy': 0.6417410373687744, 'train/loss': 1.4452682733535767, 'validation/accuracy': 0.5956599712371826, 'validation/loss': 1.6790549755096436, 'validation/num_examples': 50000, 'test/accuracy': 0.47140002250671387, 'test/loss': 2.424567699432373, 'test/num_examples': 10000, 'score': 13824.109894275665, 'total_duration': 14966.17204284668, 'accumulated_submission_time': 13824.109894275665, 'accumulated_eval_time': 1135.3580374717712, 'accumulated_logging_time': 3.244441270828247}
I0307 05:35:10.045890 140071466415872 logging_writer.py:48] [35893] accumulated_eval_time=1135.36, accumulated_logging_time=3.24444, accumulated_submission_time=13824.1, global_step=35893, preemption_count=0, score=13824.1, test/accuracy=0.4714, test/loss=2.42457, test/num_examples=10000, total_duration=14966.2, train/accuracy=0.641741, train/loss=1.44527, validation/accuracy=0.59566, validation/loss=1.67905, validation/num_examples=50000
I0307 05:35:13.094487 140071458023168 logging_writer.py:48] [35900] global_step=35900, grad_norm=4.161012649536133, loss=1.8892606496810913
I0307 05:35:51.010637 140071466415872 logging_writer.py:48] [36000] global_step=36000, grad_norm=4.3227219581604, loss=1.9876664876937866
I0307 05:36:29.311799 140071458023168 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.304788589477539, loss=1.9537503719329834
I0307 05:37:07.795138 140071466415872 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.611215114593506, loss=1.975551962852478
I0307 05:37:46.234627 140071458023168 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.817812204360962, loss=1.9543577432632446
I0307 05:38:24.827455 140071466415872 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.6501646041870117, loss=1.997218370437622
I0307 05:39:02.742169 140071458023168 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.7696783542633057, loss=1.9305123090744019
I0307 05:39:40.703142 140071466415872 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.2814362049102783, loss=1.9764602184295654
I0307 05:40:18.950873 140071458023168 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.744089126586914, loss=2.0029945373535156
I0307 05:40:57.518950 140071466415872 logging_writer.py:48] [36800] global_step=36800, grad_norm=4.000727653503418, loss=2.00807523727417
I0307 05:41:35.804121 140071458023168 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.5455029010772705, loss=1.8385508060455322
I0307 05:42:14.756532 140071466415872 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.877364158630371, loss=1.9588181972503662
I0307 05:42:53.536098 140071458023168 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.9021120071411133, loss=1.9420173168182373
I0307 05:43:31.743924 140071466415872 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.3255996704101562, loss=1.936463713645935
I0307 05:43:40.137085 140226914178240 spec.py:321] Evaluating on the training split.
I0307 05:43:52.146523 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 05:44:12.781764 140226914178240 spec.py:349] Evaluating on the test split.
I0307 05:44:14.557885 140226914178240 submission_runner.py:469] Time since start: 15510.79s, 	Step: 37223, 	{'train/accuracy': 0.6357820630073547, 'train/loss': 1.4769904613494873, 'validation/accuracy': 0.5919600129127502, 'validation/loss': 1.7029697895050049, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.4114253520965576, 'test/num_examples': 10000, 'score': 14334.019703149796, 'total_duration': 15510.788739204407, 'accumulated_submission_time': 14334.019703149796, 'accumulated_eval_time': 1169.7786898612976, 'accumulated_logging_time': 3.4008822441101074}
I0307 05:44:14.645264 140071458023168 logging_writer.py:48] [37223] accumulated_eval_time=1169.78, accumulated_logging_time=3.40088, accumulated_submission_time=14334, global_step=37223, preemption_count=0, score=14334, test/accuracy=0.4695, test/loss=2.41143, test/num_examples=10000, total_duration=15510.8, train/accuracy=0.635782, train/loss=1.47699, validation/accuracy=0.59196, validation/loss=1.70297, validation/num_examples=50000
I0307 05:44:44.600306 140071466415872 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.3636972904205322, loss=1.909203290939331
I0307 05:45:22.882655 140071458023168 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.8575356006622314, loss=1.9048644304275513
I0307 05:46:01.396470 140071466415872 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.419431686401367, loss=1.9599008560180664
I0307 05:46:40.076451 140071458023168 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.504260778427124, loss=1.979351282119751
I0307 05:47:18.564355 140071466415872 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.667141914367676, loss=1.9129199981689453
I0307 05:47:57.034459 140071458023168 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.6303467750549316, loss=1.929317831993103
I0307 05:48:35.321291 140071466415872 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.5748181343078613, loss=2.043865919113159
I0307 05:49:13.721023 140071458023168 logging_writer.py:48] [38000] global_step=38000, grad_norm=4.184643268585205, loss=2.0131640434265137
I0307 05:49:52.085103 140071466415872 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.412008285522461, loss=1.8251680135726929
I0307 05:50:30.596493 140071458023168 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.9709863662719727, loss=1.9766716957092285
I0307 05:51:08.934220 140071466415872 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.414395570755005, loss=1.8711727857589722
I0307 05:51:47.399330 140071458023168 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.440300464630127, loss=1.9124951362609863
I0307 05:52:26.268599 140071466415872 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.8502917289733887, loss=1.9533960819244385
I0307 05:52:44.570377 140226914178240 spec.py:321] Evaluating on the training split.
I0307 05:52:56.403827 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 05:53:20.008168 140226914178240 spec.py:349] Evaluating on the test split.
I0307 05:53:22.018296 140226914178240 submission_runner.py:469] Time since start: 16058.25s, 	Step: 38549, 	{'train/accuracy': 0.6342474222183228, 'train/loss': 1.4742919206619263, 'validation/accuracy': 0.5970799922943115, 'validation/loss': 1.6918880939483643, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.454254627227783, 'test/num_examples': 10000, 'score': 14843.79547548294, 'total_duration': 16058.24912905693, 'accumulated_submission_time': 14843.79547548294, 'accumulated_eval_time': 1207.226440668106, 'accumulated_logging_time': 3.509631633758545}
I0307 05:53:22.109060 140071458023168 logging_writer.py:48] [38549] accumulated_eval_time=1207.23, accumulated_logging_time=3.50963, accumulated_submission_time=14843.8, global_step=38549, preemption_count=0, score=14843.8, test/accuracy=0.4713, test/loss=2.45425, test/num_examples=10000, total_duration=16058.2, train/accuracy=0.634247, train/loss=1.47429, validation/accuracy=0.59708, validation/loss=1.69189, validation/num_examples=50000
I0307 05:53:42.226442 140071466415872 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.2657315731048584, loss=1.862770915031433
I0307 05:54:20.514758 140071458023168 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.433662176132202, loss=1.9530588388442993
I0307 05:54:58.850067 140071466415872 logging_writer.py:48] [38800] global_step=38800, grad_norm=4.110607624053955, loss=1.9363361597061157
I0307 05:55:37.465625 140071458023168 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.163949489593506, loss=1.8804208040237427
I0307 05:56:16.186961 140071466415872 logging_writer.py:48] [39000] global_step=39000, grad_norm=4.707708835601807, loss=1.9008777141571045
I0307 05:56:54.671261 140071458023168 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.2666032314300537, loss=1.881957769393921
I0307 05:57:33.065573 140071466415872 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.715398073196411, loss=1.860924482345581
I0307 05:58:11.419292 140071458023168 logging_writer.py:48] [39300] global_step=39300, grad_norm=4.574462413787842, loss=1.884354591369629
I0307 05:58:49.821580 140071466415872 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.518080472946167, loss=2.012120008468628
I0307 05:59:28.119120 140071458023168 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.7210428714752197, loss=2.1410534381866455
I0307 06:00:06.721971 140071466415872 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.9426913261413574, loss=1.9862045049667358
I0307 06:00:45.139529 140071458023168 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.5654475688934326, loss=1.942117691040039
I0307 06:01:23.825753 140071466415872 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.7789437770843506, loss=1.89545476436615
I0307 06:01:52.417205 140226914178240 spec.py:321] Evaluating on the training split.
I0307 06:02:04.450629 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 06:02:23.851423 140226914178240 spec.py:349] Evaluating on the test split.
I0307 06:02:25.648455 140226914178240 submission_runner.py:469] Time since start: 16601.88s, 	Step: 39875, 	{'train/accuracy': 0.6369578838348389, 'train/loss': 1.4644701480865479, 'validation/accuracy': 0.5958399772644043, 'validation/loss': 1.6818727254867554, 'validation/num_examples': 50000, 'test/accuracy': 0.46730002760887146, 'test/loss': 2.422297477722168, 'test/num_examples': 10000, 'score': 15353.945746660233, 'total_duration': 16601.87930560112, 'accumulated_submission_time': 15353.945746660233, 'accumulated_eval_time': 1240.4575395584106, 'accumulated_logging_time': 3.6294965744018555}
I0307 06:02:25.806306 140071458023168 logging_writer.py:48] [39875] accumulated_eval_time=1240.46, accumulated_logging_time=3.6295, accumulated_submission_time=15353.9, global_step=39875, preemption_count=0, score=15353.9, test/accuracy=0.4673, test/loss=2.4223, test/num_examples=10000, total_duration=16601.9, train/accuracy=0.636958, train/loss=1.46447, validation/accuracy=0.59584, validation/loss=1.68187, validation/num_examples=50000
I0307 06:02:35.884624 140071466415872 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.3342366218566895, loss=1.9201366901397705
I0307 06:03:14.179324 140071458023168 logging_writer.py:48] [40000] global_step=40000, grad_norm=5.05027961730957, loss=2.0613505840301514
I0307 06:03:52.495640 140071466415872 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.8806662559509277, loss=2.0152382850646973
I0307 06:04:31.150747 140071458023168 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.554734230041504, loss=1.8707363605499268
I0307 06:05:09.571485 140071466415872 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.456338882446289, loss=1.8800855875015259
I0307 06:05:47.871163 140071458023168 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.3809409141540527, loss=1.9638752937316895
I0307 06:06:26.215957 140071466415872 logging_writer.py:48] [40500] global_step=40500, grad_norm=4.466275215148926, loss=2.063481330871582
I0307 06:07:04.728944 140071458023168 logging_writer.py:48] [40600] global_step=40600, grad_norm=4.558851718902588, loss=1.950242042541504
I0307 06:07:43.059522 140071466415872 logging_writer.py:48] [40700] global_step=40700, grad_norm=4.1384992599487305, loss=1.9785583019256592
I0307 06:08:21.106488 140071458023168 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.424406051635742, loss=1.9111727476119995
I0307 06:08:59.514482 140071466415872 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.902831792831421, loss=1.9484491348266602
I0307 06:09:38.198178 140071458023168 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.993586778640747, loss=1.9581012725830078
I0307 06:10:16.416582 140071466415872 logging_writer.py:48] [41100] global_step=41100, grad_norm=4.0854692459106445, loss=2.004497766494751
I0307 06:10:55.097620 140071458023168 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.5192856788635254, loss=1.856328010559082
I0307 06:10:55.896924 140226914178240 spec.py:321] Evaluating on the training split.
I0307 06:11:07.766520 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 06:11:31.912660 140226914178240 spec.py:349] Evaluating on the test split.
I0307 06:11:33.634168 140226914178240 submission_runner.py:469] Time since start: 17149.87s, 	Step: 41203, 	{'train/accuracy': 0.6549545526504517, 'train/loss': 1.3819515705108643, 'validation/accuracy': 0.6101599931716919, 'validation/loss': 1.6125420331954956, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3199193477630615, 'test/num_examples': 10000, 'score': 15863.868384838104, 'total_duration': 17149.865020275116, 'accumulated_submission_time': 15863.868384838104, 'accumulated_eval_time': 1278.1946349143982, 'accumulated_logging_time': 3.8246634006500244}
I0307 06:11:33.713238 140071466415872 logging_writer.py:48] [41203] accumulated_eval_time=1278.19, accumulated_logging_time=3.82466, accumulated_submission_time=15863.9, global_step=41203, preemption_count=0, score=15863.9, test/accuracy=0.4834, test/loss=2.31992, test/num_examples=10000, total_duration=17149.9, train/accuracy=0.654955, train/loss=1.38195, validation/accuracy=0.61016, validation/loss=1.61254, validation/num_examples=50000
I0307 06:12:11.403772 140071458023168 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.5214552879333496, loss=1.9075647592544556
I0307 06:12:49.673152 140071466415872 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.8387961387634277, loss=2.020092010498047
I0307 06:13:28.254286 140071458023168 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.6422009468078613, loss=1.9265111684799194
I0307 06:14:06.828465 140071466415872 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.680208921432495, loss=1.8501479625701904
I0307 06:14:45.297393 140071458023168 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.8050808906555176, loss=1.9713367223739624
I0307 06:15:23.546575 140071466415872 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.457733154296875, loss=1.847899079322815
I0307 06:16:01.876183 140071458023168 logging_writer.py:48] [41900] global_step=41900, grad_norm=4.058581352233887, loss=1.9750006198883057
I0307 06:16:39.981601 140071466415872 logging_writer.py:48] [42000] global_step=42000, grad_norm=4.0435333251953125, loss=1.8539375066757202
I0307 06:17:18.505938 140071458023168 logging_writer.py:48] [42100] global_step=42100, grad_norm=4.039410591125488, loss=2.0402462482452393
I0307 06:17:56.848902 140071466415872 logging_writer.py:48] [42200] global_step=42200, grad_norm=4.496432781219482, loss=1.857776403427124
I0307 06:18:35.163235 140071458023168 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.371854543685913, loss=1.9337725639343262
I0307 06:19:13.383165 140071466415872 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.205349922180176, loss=1.8658655881881714
I0307 06:19:51.927193 140071458023168 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.6455273628234863, loss=1.978572964668274
I0307 06:20:03.742093 140226914178240 spec.py:321] Evaluating on the training split.
I0307 06:20:15.983506 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 06:20:37.273538 140226914178240 spec.py:349] Evaluating on the test split.
I0307 06:20:39.045343 140226914178240 submission_runner.py:469] Time since start: 17695.28s, 	Step: 42532, 	{'train/accuracy': 0.6523038744926453, 'train/loss': 1.3780807256698608, 'validation/accuracy': 0.6062999963760376, 'validation/loss': 1.6266237497329712, 'validation/num_examples': 50000, 'test/accuracy': 0.4823000133037567, 'test/loss': 2.3270530700683594, 'test/num_examples': 10000, 'score': 16373.743404626846, 'total_duration': 17695.276176214218, 'accumulated_submission_time': 16373.743404626846, 'accumulated_eval_time': 1313.4977173805237, 'accumulated_logging_time': 3.9301509857177734}
I0307 06:20:39.137936 140071466415872 logging_writer.py:48] [42532] accumulated_eval_time=1313.5, accumulated_logging_time=3.93015, accumulated_submission_time=16373.7, global_step=42532, preemption_count=0, score=16373.7, test/accuracy=0.4823, test/loss=2.32705, test/num_examples=10000, total_duration=17695.3, train/accuracy=0.652304, train/loss=1.37808, validation/accuracy=0.6063, validation/loss=1.62662, validation/num_examples=50000
I0307 06:21:05.418891 140071458023168 logging_writer.py:48] [42600] global_step=42600, grad_norm=4.046290397644043, loss=1.9525606632232666
I0307 06:21:43.413982 140071466415872 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.63295578956604, loss=1.9624073505401611
I0307 06:22:21.887633 140071458023168 logging_writer.py:48] [42800] global_step=42800, grad_norm=4.333326816558838, loss=1.9938583374023438
I0307 06:23:00.307177 140071466415872 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.054239273071289, loss=1.8127365112304688
I0307 06:23:38.856198 140071458023168 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.607179641723633, loss=1.8670752048492432
I0307 06:24:17.058279 140071466415872 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.9062342643737793, loss=2.014946222305298
I0307 06:24:55.015610 140071458023168 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.543790102005005, loss=1.8866231441497803
I0307 06:25:33.428975 140071466415872 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.5126659870147705, loss=2.062936305999756
I0307 06:26:11.860837 140071458023168 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.097501516342163, loss=1.852445125579834
I0307 06:26:50.227993 140071466415872 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.6967318058013916, loss=2.0012030601501465
I0307 06:27:28.644779 140071458023168 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.8310015201568604, loss=2.003232002258301
I0307 06:28:07.272733 140071466415872 logging_writer.py:48] [43700] global_step=43700, grad_norm=4.258368015289307, loss=1.9585082530975342
I0307 06:28:45.751243 140071458023168 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.5233354568481445, loss=1.8949424028396606
I0307 06:29:09.184946 140226914178240 spec.py:321] Evaluating on the training split.
I0307 06:29:20.854959 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 06:29:42.801084 140226914178240 spec.py:349] Evaluating on the test split.
I0307 06:29:44.534653 140226914178240 submission_runner.py:469] Time since start: 18240.77s, 	Step: 43862, 	{'train/accuracy': 0.6377351880073547, 'train/loss': 1.4740387201309204, 'validation/accuracy': 0.5946399569511414, 'validation/loss': 1.698374629020691, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.415133237838745, 'test/num_examples': 10000, 'score': 16883.619343280792, 'total_duration': 18240.765509843826, 'accumulated_submission_time': 16883.619343280792, 'accumulated_eval_time': 1348.8472847938538, 'accumulated_logging_time': 4.0624213218688965}
I0307 06:29:44.668021 140071466415872 logging_writer.py:48] [43862] accumulated_eval_time=1348.85, accumulated_logging_time=4.06242, accumulated_submission_time=16883.6, global_step=43862, preemption_count=0, score=16883.6, test/accuracy=0.4745, test/loss=2.41513, test/num_examples=10000, total_duration=18240.8, train/accuracy=0.637735, train/loss=1.47404, validation/accuracy=0.59464, validation/loss=1.69837, validation/num_examples=50000
I0307 06:29:59.774613 140071458023168 logging_writer.py:48] [43900] global_step=43900, grad_norm=4.444866180419922, loss=1.9109947681427002
I0307 06:30:38.212535 140071466415872 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.8324403762817383, loss=1.8040904998779297
I0307 06:31:16.686781 140071458023168 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.830681085586548, loss=1.9612257480621338
I0307 06:31:55.320255 140071466415872 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.516864061355591, loss=1.9735366106033325
I0307 06:32:33.885367 140071458023168 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.6024153232574463, loss=1.9301552772521973
I0307 06:33:12.314100 140071466415872 logging_writer.py:48] [44400] global_step=44400, grad_norm=4.0904951095581055, loss=1.90739905834198
I0307 06:33:50.602875 140071458023168 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.476022958755493, loss=1.8588528633117676
I0307 06:34:28.929290 140071466415872 logging_writer.py:48] [44600] global_step=44600, grad_norm=4.021446704864502, loss=1.822964072227478
I0307 06:35:07.270577 140071458023168 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.7114694118499756, loss=1.7687007188796997
I0307 06:35:45.943558 140071466415872 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.7934377193450928, loss=1.9034695625305176
I0307 06:36:24.244845 140071458023168 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.449549436569214, loss=1.9929842948913574
I0307 06:37:03.447448 140071466415872 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.4455196857452393, loss=1.8913321495056152
I0307 06:37:42.128690 140071458023168 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.511263847351074, loss=1.8838131427764893
I0307 06:38:14.720489 140226914178240 spec.py:321] Evaluating on the training split.
I0307 06:38:26.153897 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 06:38:45.113814 140226914178240 spec.py:349] Evaluating on the test split.
I0307 06:38:46.870218 140226914178240 submission_runner.py:469] Time since start: 18783.10s, 	Step: 45186, 	{'train/accuracy': 0.6428970098495483, 'train/loss': 1.4471725225448608, 'validation/accuracy': 0.5976600050926208, 'validation/loss': 1.676727294921875, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.388176202774048, 'test/num_examples': 10000, 'score': 17393.480707406998, 'total_duration': 18783.10109114647, 'accumulated_submission_time': 17393.480707406998, 'accumulated_eval_time': 1380.9968876838684, 'accumulated_logging_time': 4.258647918701172}
I0307 06:38:46.965330 140071466415872 logging_writer.py:48] [45186] accumulated_eval_time=1381, accumulated_logging_time=4.25865, accumulated_submission_time=17393.5, global_step=45186, preemption_count=0, score=17393.5, test/accuracy=0.4808, test/loss=2.38818, test/num_examples=10000, total_duration=18783.1, train/accuracy=0.642897, train/loss=1.44717, validation/accuracy=0.59766, validation/loss=1.67673, validation/num_examples=50000
I0307 06:38:52.659996 140071458023168 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.004272222518921, loss=1.857751488685608
I0307 06:39:30.907235 140071466415872 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.2263479232788086, loss=1.9152271747589111
I0307 06:40:09.203306 140071458023168 logging_writer.py:48] [45400] global_step=45400, grad_norm=5.127068519592285, loss=2.023094654083252
I0307 06:40:47.700237 140071466415872 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.478592872619629, loss=1.8082466125488281
I0307 06:41:26.220788 140071458023168 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.8638298511505127, loss=1.9317069053649902
I0307 06:42:04.937265 140071466415872 logging_writer.py:48] [45700] global_step=45700, grad_norm=4.279456615447998, loss=1.798724889755249
I0307 06:42:43.288138 140071458023168 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.623599052429199, loss=1.9898250102996826
I0307 06:43:21.957390 140071466415872 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.408000946044922, loss=1.896755337715149
I0307 06:44:00.676135 140071458023168 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.2723498344421387, loss=1.9532679319381714
I0307 06:44:39.371262 140071466415872 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.4870376586914062, loss=1.9181725978851318
I0307 06:45:17.772644 140071458023168 logging_writer.py:48] [46200] global_step=46200, grad_norm=4.578245639801025, loss=1.921963095664978
I0307 06:45:56.230527 140071466415872 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.497341632843018, loss=1.8465754985809326
I0307 06:46:34.724980 140071458023168 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.3334929943084717, loss=1.8416311740875244
I0307 06:47:13.389545 140071466415872 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.9109151363372803, loss=1.7939342260360718
I0307 06:47:17.233198 140226914178240 spec.py:321] Evaluating on the training split.
I0307 06:47:28.983966 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 06:47:53.521248 140226914178240 spec.py:349] Evaluating on the test split.
I0307 06:47:55.242830 140226914178240 submission_runner.py:469] Time since start: 19331.47s, 	Step: 46511, 	{'train/accuracy': 0.6601961255073547, 'train/loss': 1.3506418466567993, 'validation/accuracy': 0.6137599945068359, 'validation/loss': 1.596824288368225, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.330554485321045, 'test/num_examples': 10000, 'score': 17903.584470272064, 'total_duration': 19331.47368788719, 'accumulated_submission_time': 17903.584470272064, 'accumulated_eval_time': 1419.0063767433167, 'accumulated_logging_time': 4.388972520828247}
I0307 06:47:55.342104 140071458023168 logging_writer.py:48] [46511] accumulated_eval_time=1419.01, accumulated_logging_time=4.38897, accumulated_submission_time=17903.6, global_step=46511, preemption_count=0, score=17903.6, test/accuracy=0.4897, test/loss=2.33055, test/num_examples=10000, total_duration=19331.5, train/accuracy=0.660196, train/loss=1.35064, validation/accuracy=0.61376, validation/loss=1.59682, validation/num_examples=50000
I0307 06:48:29.654163 140071466415872 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.852071523666382, loss=1.978967547416687
I0307 06:49:07.998348 140071458023168 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.531919240951538, loss=1.934238314628601
I0307 06:49:46.414267 140071466415872 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.3163342475891113, loss=1.8537757396697998
I0307 06:50:25.150811 140071458023168 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.6788103580474854, loss=1.964176893234253
I0307 06:51:03.886077 140071466415872 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.6827545166015625, loss=1.946455717086792
I0307 06:51:42.389086 140071458023168 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.9549930095672607, loss=1.885844349861145
I0307 06:52:21.131008 140071466415872 logging_writer.py:48] [47200] global_step=47200, grad_norm=4.203520774841309, loss=1.8954863548278809
I0307 06:53:00.401653 140071458023168 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.121704339981079, loss=1.7587063312530518
I0307 06:53:39.064707 140071466415872 logging_writer.py:48] [47400] global_step=47400, grad_norm=4.072512149810791, loss=1.8920642137527466
I0307 06:54:17.551277 140071458023168 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.686638832092285, loss=1.976283073425293
I0307 06:54:56.065350 140071466415872 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.4723000526428223, loss=1.9509024620056152
I0307 06:55:34.793735 140071458023168 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.403584003448486, loss=1.8871207237243652
I0307 06:56:13.214843 140071466415872 logging_writer.py:48] [47800] global_step=47800, grad_norm=4.799356460571289, loss=1.9892057180404663
I0307 06:56:25.492931 140226914178240 spec.py:321] Evaluating on the training split.
I0307 06:56:37.217496 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 06:56:58.490368 140226914178240 spec.py:349] Evaluating on the test split.
I0307 06:57:00.254634 140226914178240 submission_runner.py:469] Time since start: 19876.49s, 	Step: 47833, 	{'train/accuracy': 0.6516262888908386, 'train/loss': 1.397565245628357, 'validation/accuracy': 0.6050400137901306, 'validation/loss': 1.6304758787155151, 'validation/num_examples': 50000, 'test/accuracy': 0.4863000214099884, 'test/loss': 2.3185198307037354, 'test/num_examples': 10000, 'score': 18413.56734395027, 'total_duration': 19876.48544359207, 'accumulated_submission_time': 18413.56734395027, 'accumulated_eval_time': 1453.7678875923157, 'accumulated_logging_time': 4.52807354927063}
I0307 06:57:00.329786 140071458023168 logging_writer.py:48] [47833] accumulated_eval_time=1453.77, accumulated_logging_time=4.52807, accumulated_submission_time=18413.6, global_step=47833, preemption_count=0, score=18413.6, test/accuracy=0.4863, test/loss=2.31852, test/num_examples=10000, total_duration=19876.5, train/accuracy=0.651626, train/loss=1.39757, validation/accuracy=0.60504, validation/loss=1.63048, validation/num_examples=50000
I0307 06:57:26.617353 140071466415872 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.2367823123931885, loss=1.806960940361023
I0307 06:58:04.655395 140071458023168 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.701355218887329, loss=1.9199349880218506
I0307 06:58:42.966869 140071466415872 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.6212990283966064, loss=1.859449028968811
I0307 06:59:21.642670 140071458023168 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.828522205352783, loss=1.8913750648498535
I0307 07:00:00.097453 140071466415872 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.7013771533966064, loss=1.8699997663497925
I0307 07:00:38.673891 140071458023168 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.3194432258605957, loss=1.9197443723678589
I0307 07:01:17.492213 140071466415872 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.6719985008239746, loss=1.8681869506835938
I0307 07:01:55.949675 140071458023168 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.725823402404785, loss=1.9535932540893555
I0307 07:02:34.131709 140071466415872 logging_writer.py:48] [48700] global_step=48700, grad_norm=2.995297908782959, loss=1.901822566986084
I0307 07:03:12.545443 140071458023168 logging_writer.py:48] [48800] global_step=48800, grad_norm=4.0570502281188965, loss=1.9885239601135254
I0307 07:03:51.212078 140071466415872 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.9942877292633057, loss=1.8379757404327393
I0307 07:04:29.372819 140071458023168 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.773892879486084, loss=1.8409422636032104
I0307 07:05:07.773787 140071466415872 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.7264297008514404, loss=1.9358043670654297
I0307 07:05:30.443704 140226914178240 spec.py:321] Evaluating on the training split.
I0307 07:05:42.325463 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 07:06:10.085376 140226914178240 spec.py:349] Evaluating on the test split.
I0307 07:06:11.791524 140226914178240 submission_runner.py:469] Time since start: 20428.02s, 	Step: 49160, 	{'train/accuracy': 0.6392099857330322, 'train/loss': 1.465406060218811, 'validation/accuracy': 0.5956199765205383, 'validation/loss': 1.685867190361023, 'validation/num_examples': 50000, 'test/accuracy': 0.4717000126838684, 'test/loss': 2.435591220855713, 'test/num_examples': 10000, 'score': 18923.516672849655, 'total_duration': 20428.022285938263, 'accumulated_submission_time': 18923.516672849655, 'accumulated_eval_time': 1495.1154696941376, 'accumulated_logging_time': 4.638911485671997}
I0307 07:06:11.878348 140071458023168 logging_writer.py:48] [49160] accumulated_eval_time=1495.12, accumulated_logging_time=4.63891, accumulated_submission_time=18923.5, global_step=49160, preemption_count=0, score=18923.5, test/accuracy=0.4717, test/loss=2.43559, test/num_examples=10000, total_duration=20428, train/accuracy=0.63921, train/loss=1.46541, validation/accuracy=0.59562, validation/loss=1.68587, validation/num_examples=50000
I0307 07:06:27.766817 140071466415872 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.528109312057495, loss=1.8728139400482178
I0307 07:07:06.333210 140071458023168 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.4275577068328857, loss=1.8635785579681396
I0307 07:07:45.015994 140071466415872 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.6592161655426025, loss=1.9524486064910889
I0307 07:08:23.566957 140071458023168 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.9013493061065674, loss=1.9989306926727295
I0307 07:09:02.053581 140071466415872 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.9001636505126953, loss=1.9441955089569092
I0307 07:09:40.573504 140071458023168 logging_writer.py:48] [49700] global_step=49700, grad_norm=4.292397499084473, loss=1.9254868030548096
I0307 07:10:19.436109 140071466415872 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.6897828578948975, loss=1.753219485282898
I0307 07:10:57.888792 140071458023168 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.4967403411865234, loss=2.011721611022949
I0307 07:11:36.344568 140071466415872 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.2827556133270264, loss=1.8559502363204956
I0307 07:12:14.713208 140071458023168 logging_writer.py:48] [50100] global_step=50100, grad_norm=4.424145698547363, loss=1.8876664638519287
I0307 07:12:53.114317 140071466415872 logging_writer.py:48] [50200] global_step=50200, grad_norm=4.203623294830322, loss=1.9168505668640137
I0307 07:13:31.562671 140071458023168 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.9162538051605225, loss=1.8817780017852783
I0307 07:14:09.980011 140071466415872 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.773899555206299, loss=1.8943272829055786
I0307 07:14:42.161987 140226914178240 spec.py:321] Evaluating on the training split.
I0307 07:14:54.004893 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 07:15:19.965859 140226914178240 spec.py:349] Evaluating on the test split.
I0307 07:15:21.680764 140226914178240 submission_runner.py:469] Time since start: 20977.91s, 	Step: 50485, 	{'train/accuracy': 0.6545360088348389, 'train/loss': 1.3919093608856201, 'validation/accuracy': 0.6090799570083618, 'validation/loss': 1.623656153678894, 'validation/num_examples': 50000, 'test/accuracy': 0.48840001225471497, 'test/loss': 2.334744691848755, 'test/num_examples': 10000, 'score': 19433.63016796112, 'total_duration': 20977.911613941193, 'accumulated_submission_time': 19433.63016796112, 'accumulated_eval_time': 1534.6340951919556, 'accumulated_logging_time': 4.766113042831421}
I0307 07:15:21.758081 140071458023168 logging_writer.py:48] [50485] accumulated_eval_time=1534.63, accumulated_logging_time=4.76611, accumulated_submission_time=19433.6, global_step=50485, preemption_count=0, score=19433.6, test/accuracy=0.4884, test/loss=2.33474, test/num_examples=10000, total_duration=20977.9, train/accuracy=0.654536, train/loss=1.39191, validation/accuracy=0.60908, validation/loss=1.62366, validation/num_examples=50000
I0307 07:15:27.996876 140071466415872 logging_writer.py:48] [50500] global_step=50500, grad_norm=4.460274696350098, loss=1.8322088718414307
I0307 07:16:06.647381 140071458023168 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.9604337215423584, loss=1.8594615459442139
I0307 07:16:45.170735 140071466415872 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.806870222091675, loss=1.9343281984329224
I0307 07:17:23.336360 140071458023168 logging_writer.py:48] [50800] global_step=50800, grad_norm=4.2321367263793945, loss=1.9050096273422241
I0307 07:18:01.832998 140071466415872 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.944528341293335, loss=1.9544330835342407
I0307 07:18:40.665623 140071458023168 logging_writer.py:48] [51000] global_step=51000, grad_norm=4.125157833099365, loss=1.7428370714187622
I0307 07:19:19.534800 140071466415872 logging_writer.py:48] [51100] global_step=51100, grad_norm=4.0034637451171875, loss=1.8185726404190063
I0307 07:19:57.728414 140071458023168 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.621002197265625, loss=1.8642314672470093
I0307 07:20:36.217252 140071466415872 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.3717641830444336, loss=1.8186944723129272
I0307 07:21:14.741339 140071458023168 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.2859854698181152, loss=1.822211503982544
I0307 07:21:53.176850 140071466415872 logging_writer.py:48] [51500] global_step=51500, grad_norm=4.146694660186768, loss=1.9184274673461914
I0307 07:22:31.501395 140071458023168 logging_writer.py:48] [51600] global_step=51600, grad_norm=4.521369457244873, loss=2.0143680572509766
I0307 07:23:10.115957 140071466415872 logging_writer.py:48] [51700] global_step=51700, grad_norm=4.913557052612305, loss=1.8564196825027466
I0307 07:23:48.740684 140071458023168 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.473022222518921, loss=1.8247027397155762
I0307 07:23:51.734373 140226914178240 spec.py:321] Evaluating on the training split.
I0307 07:24:03.506787 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 07:24:21.440174 140226914178240 spec.py:349] Evaluating on the test split.
I0307 07:24:23.214244 140226914178240 submission_runner.py:469] Time since start: 21519.45s, 	Step: 51809, 	{'train/accuracy': 0.6561503410339355, 'train/loss': 1.3735103607177734, 'validation/accuracy': 0.6130200028419495, 'validation/loss': 1.6109559535980225, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3277089595794678, 'test/num_examples': 10000, 'score': 19943.454622030258, 'total_duration': 21519.445110559464, 'accumulated_submission_time': 19943.454622030258, 'accumulated_eval_time': 1566.1138343811035, 'accumulated_logging_time': 4.866304636001587}
I0307 07:24:23.296023 140071466415872 logging_writer.py:48] [51809] accumulated_eval_time=1566.11, accumulated_logging_time=4.8663, accumulated_submission_time=19943.5, global_step=51809, preemption_count=0, score=19943.5, test/accuracy=0.4901, test/loss=2.32771, test/num_examples=10000, total_duration=21519.4, train/accuracy=0.65615, train/loss=1.37351, validation/accuracy=0.61302, validation/loss=1.61096, validation/num_examples=50000
I0307 07:24:58.637568 140071458023168 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.5765268802642822, loss=1.9023323059082031
I0307 07:25:37.162528 140071466415872 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.1031229496002197, loss=1.8140556812286377
I0307 07:26:15.348953 140071458023168 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.907083034515381, loss=1.8920612335205078
I0307 07:26:53.566178 140071466415872 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.335714817047119, loss=1.9026306867599487
I0307 07:27:32.448743 140071458023168 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.462031126022339, loss=1.7100070714950562
I0307 07:28:10.711181 140071466415872 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.6842777729034424, loss=1.836291790008545
I0307 07:28:48.926718 140071458023168 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.50943922996521, loss=1.964214563369751
I0307 07:29:27.618842 140071466415872 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.5394649505615234, loss=1.7859911918640137
I0307 07:30:06.118357 140071458023168 logging_writer.py:48] [52700] global_step=52700, grad_norm=4.605322360992432, loss=1.8437696695327759
I0307 07:30:44.732780 140071466415872 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.876711368560791, loss=1.9733408689498901
I0307 07:31:23.291254 140071458023168 logging_writer.py:48] [52900] global_step=52900, grad_norm=4.019626140594482, loss=1.8193591833114624
I0307 07:32:01.500509 140071466415872 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.6364102363586426, loss=1.8507156372070312
I0307 07:32:39.706984 140071458023168 logging_writer.py:48] [53100] global_step=53100, grad_norm=4.565981864929199, loss=2.0319602489471436
I0307 07:32:53.507523 140226914178240 spec.py:321] Evaluating on the training split.
I0307 07:33:05.278981 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 07:33:26.903636 140226914178240 spec.py:349] Evaluating on the test split.
I0307 07:33:28.618674 140226914178240 submission_runner.py:469] Time since start: 22064.85s, 	Step: 53137, 	{'train/accuracy': 0.6599370241165161, 'train/loss': 1.3738901615142822, 'validation/accuracy': 0.6121799945831299, 'validation/loss': 1.6018799543380737, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.2950503826141357, 'test/num_examples': 10000, 'score': 20453.520030498505, 'total_duration': 22064.849479436874, 'accumulated_submission_time': 20453.520030498505, 'accumulated_eval_time': 1601.2248139381409, 'accumulated_logging_time': 4.9659788608551025}
I0307 07:33:28.747253 140071466415872 logging_writer.py:48] [53137] accumulated_eval_time=1601.22, accumulated_logging_time=4.96598, accumulated_submission_time=20453.5, global_step=53137, preemption_count=0, score=20453.5, test/accuracy=0.4916, test/loss=2.29505, test/num_examples=10000, total_duration=22064.8, train/accuracy=0.659937, train/loss=1.37389, validation/accuracy=0.61218, validation/loss=1.60188, validation/num_examples=50000
I0307 07:33:53.433859 140071458023168 logging_writer.py:48] [53200] global_step=53200, grad_norm=4.464007377624512, loss=1.9034686088562012
I0307 07:34:31.907134 140071466415872 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.6224863529205322, loss=2.019350290298462
I0307 07:35:10.025760 140071458023168 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.5436956882476807, loss=1.7408193349838257
I0307 07:35:48.493571 140071466415872 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.7328147888183594, loss=1.786647081375122
I0307 07:36:26.545127 140071458023168 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.343873977661133, loss=1.8151628971099854
I0307 07:37:04.946120 140071466415872 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.673302173614502, loss=1.8478004932403564
I0307 07:37:43.394877 140071458023168 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.671598196029663, loss=1.9379584789276123
I0307 07:38:21.857758 140071466415872 logging_writer.py:48] [53900] global_step=53900, grad_norm=4.283881187438965, loss=1.8197022676467896
I0307 07:38:59.958174 140071458023168 logging_writer.py:48] [54000] global_step=54000, grad_norm=4.024201393127441, loss=1.8173549175262451
I0307 07:39:38.150109 140071466415872 logging_writer.py:48] [54100] global_step=54100, grad_norm=4.431789398193359, loss=1.9815056324005127
I0307 07:40:16.575872 140071458023168 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.3613405227661133, loss=1.867108941078186
I0307 07:40:55.013848 140071466415872 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.8614535331726074, loss=1.8769787549972534
I0307 07:41:33.572277 140071458023168 logging_writer.py:48] [54400] global_step=54400, grad_norm=4.021241188049316, loss=1.7063162326812744
I0307 07:41:58.985048 140226914178240 spec.py:321] Evaluating on the training split.
I0307 07:42:10.700877 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 07:42:31.531552 140226914178240 spec.py:349] Evaluating on the test split.
I0307 07:42:33.349720 140226914178240 submission_runner.py:469] Time since start: 22609.58s, 	Step: 54467, 	{'train/accuracy': 0.6570471525192261, 'train/loss': 1.3796769380569458, 'validation/accuracy': 0.6131399869918823, 'validation/loss': 1.6084030866622925, 'validation/num_examples': 50000, 'test/accuracy': 0.4846000373363495, 'test/loss': 2.328246831893921, 'test/num_examples': 10000, 'score': 20963.606187343597, 'total_duration': 22609.580502033234, 'accumulated_submission_time': 20963.606187343597, 'accumulated_eval_time': 1635.589267730713, 'accumulated_logging_time': 5.118034601211548}
I0307 07:42:33.508426 140071466415872 logging_writer.py:48] [54467] accumulated_eval_time=1635.59, accumulated_logging_time=5.11803, accumulated_submission_time=20963.6, global_step=54467, preemption_count=0, score=20963.6, test/accuracy=0.4846, test/loss=2.32825, test/num_examples=10000, total_duration=22609.6, train/accuracy=0.657047, train/loss=1.37968, validation/accuracy=0.61314, validation/loss=1.6084, validation/num_examples=50000
I0307 07:42:46.759387 140071458023168 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.9553380012512207, loss=1.9887806177139282
I0307 07:43:25.360699 140071466415872 logging_writer.py:48] [54600] global_step=54600, grad_norm=4.723400115966797, loss=1.8456612825393677
I0307 07:44:03.944673 140071458023168 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.5779337882995605, loss=1.9752196073532104
I0307 07:44:42.878098 140071466415872 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.9803364276885986, loss=1.906407356262207
I0307 07:45:21.568373 140071458023168 logging_writer.py:48] [54900] global_step=54900, grad_norm=4.152708053588867, loss=1.9006407260894775
I0307 07:45:59.617117 140071466415872 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.3506808280944824, loss=2.0027003288269043
I0307 07:46:38.118254 140071458023168 logging_writer.py:48] [55100] global_step=55100, grad_norm=4.053505897521973, loss=1.9067057371139526
I0307 07:47:16.335881 140071466415872 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.3904025554656982, loss=1.8849191665649414
I0307 07:47:54.701761 140071458023168 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.8361704349517822, loss=1.882327675819397
I0307 07:48:33.034040 140071466415872 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.283799409866333, loss=1.8406033515930176
I0307 07:49:11.594327 140071458023168 logging_writer.py:48] [55500] global_step=55500, grad_norm=4.126221656799316, loss=1.8847953081130981
I0307 07:49:50.331652 140071466415872 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.654297113418579, loss=1.8755130767822266
I0307 07:50:28.544708 140071458023168 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.669257164001465, loss=1.7413263320922852
I0307 07:51:03.632772 140226914178240 spec.py:321] Evaluating on the training split.
I0307 07:51:15.193018 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 07:51:40.623994 140226914178240 spec.py:349] Evaluating on the test split.
I0307 07:51:42.356217 140226914178240 submission_runner.py:469] Time since start: 23158.59s, 	Step: 55792, 	{'train/accuracy': 0.6514269709587097, 'train/loss': 1.3962053060531616, 'validation/accuracy': 0.6047599911689758, 'validation/loss': 1.643538236618042, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.375528335571289, 'test/num_examples': 10000, 'score': 21473.553010225296, 'total_duration': 23158.5870821476, 'accumulated_submission_time': 21473.553010225296, 'accumulated_eval_time': 1674.3125774860382, 'accumulated_logging_time': 5.3265180587768555}
I0307 07:51:42.551883 140071466415872 logging_writer.py:48] [55792] accumulated_eval_time=1674.31, accumulated_logging_time=5.32652, accumulated_submission_time=21473.6, global_step=55792, preemption_count=0, score=21473.6, test/accuracy=0.4806, test/loss=2.37553, test/num_examples=10000, total_duration=23158.6, train/accuracy=0.651427, train/loss=1.39621, validation/accuracy=0.60476, validation/loss=1.64354, validation/num_examples=50000
I0307 07:51:46.059027 140071458023168 logging_writer.py:48] [55800] global_step=55800, grad_norm=5.00331974029541, loss=1.926939845085144
I0307 07:52:24.501734 140071466415872 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.9456787109375, loss=1.8339827060699463
I0307 07:53:03.011677 140071458023168 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.727823257446289, loss=1.7892998456954956
I0307 07:53:41.933852 140071466415872 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.8388524055480957, loss=1.7943534851074219
I0307 07:54:20.433393 140071458023168 logging_writer.py:48] [56200] global_step=56200, grad_norm=4.5577521324157715, loss=1.7734010219573975
I0307 07:54:58.960947 140071466415872 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.96620512008667, loss=1.7655987739562988
I0307 07:55:37.737211 140071458023168 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.368074417114258, loss=1.9235881567001343
I0307 07:56:16.113312 140071466415872 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.55629301071167, loss=1.7601451873779297
I0307 07:56:53.925530 140071458023168 logging_writer.py:48] [56600] global_step=56600, grad_norm=4.15539026260376, loss=1.8951616287231445
I0307 07:57:31.753123 140071466415872 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.746297597885132, loss=1.9059303998947144
I0307 07:58:09.915881 140071458023168 logging_writer.py:48] [56800] global_step=56800, grad_norm=4.479315280914307, loss=1.71193265914917
I0307 07:58:48.294167 140071466415872 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.9109275341033936, loss=1.7807787656784058
I0307 07:59:26.596987 140071458023168 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.5285565853118896, loss=1.797149896621704
I0307 08:00:05.251383 140071466415872 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.9632489681243896, loss=1.8998186588287354
I0307 08:00:12.523719 140226914178240 spec.py:321] Evaluating on the training split.
I0307 08:00:24.474850 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 08:00:45.518610 140226914178240 spec.py:349] Evaluating on the test split.
I0307 08:00:47.249788 140226914178240 submission_runner.py:469] Time since start: 23703.48s, 	Step: 57120, 	{'train/accuracy': 0.6553332209587097, 'train/loss': 1.3734850883483887, 'validation/accuracy': 0.6106799840927124, 'validation/loss': 1.6087285280227661, 'validation/num_examples': 50000, 'test/accuracy': 0.4814000129699707, 'test/loss': 2.3519668579101562, 'test/num_examples': 10000, 'score': 21983.369214773178, 'total_duration': 23703.480622291565, 'accumulated_submission_time': 21983.369214773178, 'accumulated_eval_time': 1709.038479566574, 'accumulated_logging_time': 5.549750328063965}
I0307 08:00:47.373125 140071458023168 logging_writer.py:48] [57120] accumulated_eval_time=1709.04, accumulated_logging_time=5.54975, accumulated_submission_time=21983.4, global_step=57120, preemption_count=0, score=21983.4, test/accuracy=0.4814, test/loss=2.35197, test/num_examples=10000, total_duration=23703.5, train/accuracy=0.655333, train/loss=1.37349, validation/accuracy=0.61068, validation/loss=1.60873, validation/num_examples=50000
I0307 08:01:18.781119 140071466415872 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.5119752883911133, loss=1.8191202878952026
I0307 08:01:57.440552 140071458023168 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.5952327251434326, loss=1.8679437637329102
I0307 08:02:36.236742 140071466415872 logging_writer.py:48] [57400] global_step=57400, grad_norm=4.1075358390808105, loss=1.9500489234924316
I0307 08:03:14.746622 140071458023168 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.507234811782837, loss=1.8176295757293701
I0307 08:03:53.471212 140071466415872 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.569258451461792, loss=1.8777272701263428
I0307 08:04:31.970816 140071458023168 logging_writer.py:48] [57700] global_step=57700, grad_norm=4.176685333251953, loss=1.7437944412231445
I0307 08:05:10.251955 140071466415872 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.4071104526519775, loss=1.9208050966262817
I0307 08:05:49.011173 140071458023168 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.3367056846618652, loss=1.7965058088302612
I0307 08:06:27.392790 140071466415872 logging_writer.py:48] [58000] global_step=58000, grad_norm=4.120223045349121, loss=1.8571431636810303
I0307 08:07:05.353957 140071458023168 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.7620818614959717, loss=1.9096629619598389
I0307 08:07:43.686932 140071466415872 logging_writer.py:48] [58200] global_step=58200, grad_norm=4.340580463409424, loss=1.8284822702407837
I0307 08:08:22.014462 140071458023168 logging_writer.py:48] [58300] global_step=58300, grad_norm=4.013484477996826, loss=1.905846357345581
I0307 08:09:00.468790 140071466415872 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.892888307571411, loss=1.8450136184692383
I0307 08:09:17.261307 140226914178240 spec.py:321] Evaluating on the training split.
I0307 08:09:29.145697 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 08:09:49.244518 140226914178240 spec.py:349] Evaluating on the test split.
I0307 08:09:50.995151 140226914178240 submission_runner.py:469] Time since start: 24247.23s, 	Step: 58445, 	{'train/accuracy': 0.6597576141357422, 'train/loss': 1.3577125072479248, 'validation/accuracy': 0.6099799871444702, 'validation/loss': 1.612249493598938, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.343747138977051, 'test/num_examples': 10000, 'score': 22493.105979204178, 'total_duration': 24247.226024389267, 'accumulated_submission_time': 22493.105979204178, 'accumulated_eval_time': 1742.772200345993, 'accumulated_logging_time': 5.697880268096924}
I0307 08:09:51.112769 140071458023168 logging_writer.py:48] [58445] accumulated_eval_time=1742.77, accumulated_logging_time=5.69788, accumulated_submission_time=22493.1, global_step=58445, preemption_count=0, score=22493.1, test/accuracy=0.484, test/loss=2.34375, test/num_examples=10000, total_duration=24247.2, train/accuracy=0.659758, train/loss=1.35771, validation/accuracy=0.60998, validation/loss=1.61225, validation/num_examples=50000
I0307 08:10:12.960424 140071466415872 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.849043607711792, loss=1.825014352798462
I0307 08:10:51.567085 140071458023168 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.9917521476745605, loss=1.811365008354187
I0307 08:11:30.355425 140071466415872 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.848055839538574, loss=1.947872519493103
I0307 08:12:08.703590 140071458023168 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.4328436851501465, loss=1.8560689687728882
I0307 08:12:47.140259 140071466415872 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.936608076095581, loss=1.8675477504730225
I0307 08:13:25.567353 140071458023168 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.886137008666992, loss=1.9176530838012695
I0307 08:14:04.076975 140071466415872 logging_writer.py:48] [59100] global_step=59100, grad_norm=4.114808082580566, loss=1.8746417760849
I0307 08:14:42.298045 140071458023168 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.8434858322143555, loss=1.8212789297103882
I0307 08:15:21.082879 140071466415872 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.758197069168091, loss=1.776594638824463
I0307 08:15:59.561516 140071458023168 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.558422803878784, loss=1.8394042253494263
I0307 08:16:37.812117 140071466415872 logging_writer.py:48] [59500] global_step=59500, grad_norm=4.011324882507324, loss=1.7536544799804688
I0307 08:17:16.252872 140071458023168 logging_writer.py:48] [59600] global_step=59600, grad_norm=4.182270526885986, loss=1.9460506439208984
I0307 08:17:54.916405 140071466415872 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.7652759552001953, loss=1.7614940404891968
I0307 08:18:21.134647 140226914178240 spec.py:321] Evaluating on the training split.
I0307 08:18:33.283125 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 08:18:51.516435 140226914178240 spec.py:349] Evaluating on the test split.
I0307 08:18:53.273960 140226914178240 submission_runner.py:469] Time since start: 24789.50s, 	Step: 59768, 	{'train/accuracy': 0.6732102632522583, 'train/loss': 1.3036808967590332, 'validation/accuracy': 0.6162799596786499, 'validation/loss': 1.5882207155227661, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.287404775619507, 'test/num_examples': 10000, 'score': 23002.972229003906, 'total_duration': 24789.504794359207, 'accumulated_submission_time': 23002.972229003906, 'accumulated_eval_time': 1774.911370754242, 'accumulated_logging_time': 5.844316244125366}
I0307 08:18:53.349940 140071458023168 logging_writer.py:48] [59768] accumulated_eval_time=1774.91, accumulated_logging_time=5.84432, accumulated_submission_time=23003, global_step=59768, preemption_count=0, score=23003, test/accuracy=0.4928, test/loss=2.2874, test/num_examples=10000, total_duration=24789.5, train/accuracy=0.67321, train/loss=1.30368, validation/accuracy=0.61628, validation/loss=1.58822, validation/num_examples=50000
I0307 08:19:06.119922 140071466415872 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.183281898498535, loss=1.95087730884552
I0307 08:19:44.628295 140071458023168 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.8341047763824463, loss=1.7343440055847168
I0307 08:20:23.349162 140071466415872 logging_writer.py:48] [60000] global_step=60000, grad_norm=4.188252925872803, loss=1.9937658309936523
I0307 08:21:01.944109 140071458023168 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.751035451889038, loss=1.7689368724822998
I0307 08:21:40.479151 140071466415872 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.7207794189453125, loss=1.8668663501739502
I0307 08:22:19.101855 140071458023168 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.688330888748169, loss=1.8693970441818237
I0307 08:22:57.141952 140071466415872 logging_writer.py:48] [60400] global_step=60400, grad_norm=4.0398993492126465, loss=1.8088220357894897
I0307 08:23:35.576886 140071458023168 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.943180799484253, loss=1.960702896118164
I0307 08:24:14.246565 140071466415872 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.7719383239746094, loss=1.8567856550216675
I0307 08:24:52.792307 140071458023168 logging_writer.py:48] [60700] global_step=60700, grad_norm=4.168474197387695, loss=1.8807231187820435
I0307 08:25:31.087643 140071466415872 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.5980546474456787, loss=1.7823119163513184
I0307 08:26:09.257576 140071458023168 logging_writer.py:48] [60900] global_step=60900, grad_norm=4.029888153076172, loss=1.848265528678894
I0307 08:26:47.940245 140071466415872 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.931464672088623, loss=1.8370543718338013
I0307 08:27:23.442317 140226914178240 spec.py:321] Evaluating on the training split.
I0307 08:27:35.497975 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 08:27:53.606814 140226914178240 spec.py:349] Evaluating on the test split.
I0307 08:27:55.371405 140226914178240 submission_runner.py:469] Time since start: 25331.60s, 	Step: 61092, 	{'train/accuracy': 0.6982023119926453, 'train/loss': 1.1974247694015503, 'validation/accuracy': 0.6164000034332275, 'validation/loss': 1.5848145484924316, 'validation/num_examples': 50000, 'test/accuracy': 0.4921000301837921, 'test/loss': 2.334542989730835, 'test/num_examples': 10000, 'score': 23512.889858961105, 'total_duration': 25331.602241039276, 'accumulated_submission_time': 23512.889858961105, 'accumulated_eval_time': 1806.8402950763702, 'accumulated_logging_time': 5.964541673660278}
I0307 08:27:55.463387 140071458023168 logging_writer.py:48] [61092] accumulated_eval_time=1806.84, accumulated_logging_time=5.96454, accumulated_submission_time=23512.9, global_step=61092, preemption_count=0, score=23512.9, test/accuracy=0.4921, test/loss=2.33454, test/num_examples=10000, total_duration=25331.6, train/accuracy=0.698202, train/loss=1.19742, validation/accuracy=0.6164, validation/loss=1.58481, validation/num_examples=50000
I0307 08:27:59.086363 140071466415872 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.46870493888855, loss=1.8343520164489746
I0307 08:28:37.241777 140071458023168 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.674924373626709, loss=1.8770344257354736
I0307 08:29:15.904404 140071466415872 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.5713531970977783, loss=1.8878154754638672
I0307 08:29:54.477700 140071458023168 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.948729991912842, loss=1.8633992671966553
I0307 08:30:32.878199 140071466415872 logging_writer.py:48] [61500] global_step=61500, grad_norm=4.3314208984375, loss=1.954306721687317
I0307 08:31:11.621948 140071458023168 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.6376144886016846, loss=1.8587446212768555
I0307 08:31:50.464789 140071466415872 logging_writer.py:48] [61700] global_step=61700, grad_norm=4.4563493728637695, loss=1.8590933084487915
I0307 08:32:28.684287 140071458023168 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.7309842109680176, loss=1.82011079788208
I0307 08:33:06.878608 140071466415872 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.8316361904144287, loss=1.8817440271377563
I0307 08:33:45.707252 140071458023168 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.3883423805236816, loss=1.840032696723938
I0307 08:34:24.369766 140071466415872 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.997612237930298, loss=1.8496992588043213
I0307 08:35:03.361041 140071458023168 logging_writer.py:48] [62200] global_step=62200, grad_norm=4.009675025939941, loss=1.8603280782699585
I0307 08:35:42.435583 140071466415872 logging_writer.py:48] [62300] global_step=62300, grad_norm=4.303950309753418, loss=1.7632439136505127
I0307 08:36:21.162042 140071458023168 logging_writer.py:48] [62400] global_step=62400, grad_norm=4.148434638977051, loss=1.885704755783081
I0307 08:36:25.429909 140226914178240 spec.py:321] Evaluating on the training split.
I0307 08:36:36.873413 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 08:36:55.331042 140226914178240 spec.py:349] Evaluating on the test split.
I0307 08:36:57.117204 140226914178240 submission_runner.py:469] Time since start: 25873.35s, 	Step: 62412, 	{'train/accuracy': 0.6979631781578064, 'train/loss': 1.1662652492523193, 'validation/accuracy': 0.6166200041770935, 'validation/loss': 1.592406988143921, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.3402371406555176, 'test/num_examples': 10000, 'score': 24022.653463840485, 'total_duration': 25873.348039388657, 'accumulated_submission_time': 24022.653463840485, 'accumulated_eval_time': 1838.527425289154, 'accumulated_logging_time': 6.126060962677002}
I0307 08:36:57.191138 140071466415872 logging_writer.py:48] [62412] accumulated_eval_time=1838.53, accumulated_logging_time=6.12606, accumulated_submission_time=24022.7, global_step=62412, preemption_count=0, score=24022.7, test/accuracy=0.4891, test/loss=2.34024, test/num_examples=10000, total_duration=25873.3, train/accuracy=0.697963, train/loss=1.16627, validation/accuracy=0.61662, validation/loss=1.59241, validation/num_examples=50000
I0307 08:37:31.657229 140071458023168 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.48926043510437, loss=1.7940113544464111
I0307 08:38:10.281070 140071466415872 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.5602333545684814, loss=1.855670690536499
I0307 08:38:48.979898 140071458023168 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.8198659420013428, loss=1.8402981758117676
I0307 08:39:27.406972 140071466415872 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.7716832160949707, loss=1.914241909980774
I0307 08:40:06.320262 140071458023168 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.6992013454437256, loss=1.7265667915344238
I0307 08:40:45.039711 140071466415872 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.72428822517395, loss=1.854637622833252
I0307 08:41:23.498816 140071458023168 logging_writer.py:48] [63100] global_step=63100, grad_norm=4.549569606781006, loss=1.9276745319366455
I0307 08:42:02.048996 140071466415872 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.521958351135254, loss=1.811292052268982
I0307 08:42:40.760947 140071458023168 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.7517340183258057, loss=1.757675290107727
I0307 08:43:19.529619 140071466415872 logging_writer.py:48] [63400] global_step=63400, grad_norm=4.166162014007568, loss=1.8014979362487793
I0307 08:43:58.415663 140071458023168 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.801016330718994, loss=1.7134783267974854
I0307 08:44:37.560662 140071466415872 logging_writer.py:48] [63600] global_step=63600, grad_norm=5.09067964553833, loss=1.8734198808670044
I0307 08:45:16.311516 140071458023168 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.9936301708221436, loss=1.8373445272445679
I0307 08:45:27.487941 140226914178240 spec.py:321] Evaluating on the training split.
I0307 08:45:39.186161 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 08:46:00.249299 140226914178240 spec.py:349] Evaluating on the test split.
I0307 08:46:02.010193 140226914178240 submission_runner.py:469] Time since start: 26418.24s, 	Step: 63730, 	{'train/accuracy': 0.7108777165412903, 'train/loss': 1.1180082559585571, 'validation/accuracy': 0.6242799758911133, 'validation/loss': 1.555962324142456, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.2857937812805176, 'test/num_examples': 10000, 'score': 24532.78237581253, 'total_duration': 26418.240907907486, 'accumulated_submission_time': 24532.78237581253, 'accumulated_eval_time': 1873.0493907928467, 'accumulated_logging_time': 6.229785919189453}
I0307 08:46:02.225772 140071466415872 logging_writer.py:48] [63730] accumulated_eval_time=1873.05, accumulated_logging_time=6.22979, accumulated_submission_time=24532.8, global_step=63730, preemption_count=0, score=24532.8, test/accuracy=0.4959, test/loss=2.28579, test/num_examples=10000, total_duration=26418.2, train/accuracy=0.710878, train/loss=1.11801, validation/accuracy=0.62428, validation/loss=1.55596, validation/num_examples=50000
I0307 08:46:29.828399 140071458023168 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.0779647827148438, loss=1.7179032564163208
I0307 08:47:08.698285 140071466415872 logging_writer.py:48] [63900] global_step=63900, grad_norm=4.061773300170898, loss=1.849475622177124
I0307 08:47:47.503094 140071458023168 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.89508318901062, loss=1.7596535682678223
I0307 08:48:26.167146 140071466415872 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.352442979812622, loss=1.793076515197754
I0307 08:49:04.639972 140071458023168 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.810107469558716, loss=1.753230094909668
I0307 08:49:43.391083 140071466415872 logging_writer.py:48] [64300] global_step=64300, grad_norm=4.103895664215088, loss=1.8718847036361694
I0307 08:50:21.954463 140071458023168 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.754241466522217, loss=1.8082939386367798
I0307 08:51:00.906257 140071466415872 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.81825852394104, loss=1.8279296159744263
I0307 08:51:39.525814 140071458023168 logging_writer.py:48] [64600] global_step=64600, grad_norm=4.2498555183410645, loss=1.8849706649780273
I0307 08:52:18.344789 140071466415872 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.680495023727417, loss=1.8326587677001953
I0307 08:52:57.353471 140071458023168 logging_writer.py:48] [64800] global_step=64800, grad_norm=4.144643783569336, loss=1.827024221420288
I0307 08:53:36.157616 140071466415872 logging_writer.py:48] [64900] global_step=64900, grad_norm=4.021104335784912, loss=1.7945263385772705
I0307 08:54:15.269570 140071458023168 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.5075559616088867, loss=1.8119831085205078
I0307 08:54:32.197058 140226914178240 spec.py:321] Evaluating on the training split.
I0307 08:54:43.776443 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 08:55:01.290589 140226914178240 spec.py:349] Evaluating on the test split.
I0307 08:55:03.039973 140226914178240 submission_runner.py:469] Time since start: 26959.27s, 	Step: 65045, 	{'train/accuracy': 0.7110570669174194, 'train/loss': 1.1315356492996216, 'validation/accuracy': 0.6221199631690979, 'validation/loss': 1.5635194778442383, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.298199415206909, 'test/num_examples': 10000, 'score': 25042.58123755455, 'total_duration': 26959.270816087723, 'accumulated_submission_time': 25042.58123755455, 'accumulated_eval_time': 1903.8921644687653, 'accumulated_logging_time': 6.475632429122925}
I0307 08:55:03.168387 140071466415872 logging_writer.py:48] [65045] accumulated_eval_time=1903.89, accumulated_logging_time=6.47563, accumulated_submission_time=25042.6, global_step=65045, preemption_count=0, score=25042.6, test/accuracy=0.4938, test/loss=2.2982, test/num_examples=10000, total_duration=26959.3, train/accuracy=0.711057, train/loss=1.13154, validation/accuracy=0.62212, validation/loss=1.56352, validation/num_examples=50000
I0307 08:55:24.636718 140071458023168 logging_writer.py:48] [65100] global_step=65100, grad_norm=4.181463241577148, loss=1.881772518157959
I0307 08:56:03.314338 140071466415872 logging_writer.py:48] [65200] global_step=65200, grad_norm=4.5124192237854, loss=1.781593918800354
I0307 08:56:41.866356 140071458023168 logging_writer.py:48] [65300] global_step=65300, grad_norm=4.923115253448486, loss=1.8328073024749756
I0307 08:57:20.276882 140071466415872 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.4402294158935547, loss=1.7151787281036377
I0307 08:57:58.935450 140071458023168 logging_writer.py:48] [65500] global_step=65500, grad_norm=4.2364091873168945, loss=1.7977995872497559
I0307 08:58:37.763127 140071466415872 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.9167630672454834, loss=1.8036093711853027
I0307 08:59:16.657749 140071458023168 logging_writer.py:48] [65700] global_step=65700, grad_norm=4.076794624328613, loss=1.7666382789611816
I0307 08:59:55.031118 140071466415872 logging_writer.py:48] [65800] global_step=65800, grad_norm=4.021125316619873, loss=1.6495904922485352
I0307 09:00:33.635402 140071458023168 logging_writer.py:48] [65900] global_step=65900, grad_norm=4.6690874099731445, loss=1.7697829008102417
I0307 09:01:12.577091 140071466415872 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.810368537902832, loss=1.8777631521224976
I0307 09:01:51.810797 140071458023168 logging_writer.py:48] [66100] global_step=66100, grad_norm=4.102730751037598, loss=1.864425539970398
I0307 09:02:31.237146 140071466415872 logging_writer.py:48] [66200] global_step=66200, grad_norm=4.131192207336426, loss=1.836043357849121
2025-03-07 09:03:03.678988: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:03:09.877259 140071458023168 logging_writer.py:48] [66300] global_step=66300, grad_norm=4.286993503570557, loss=1.8088421821594238
I0307 09:03:33.208241 140226914178240 spec.py:321] Evaluating on the training split.
I0307 09:03:45.502207 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 09:04:05.805986 140226914178240 spec.py:349] Evaluating on the test split.
I0307 09:04:07.550248 140226914178240 submission_runner.py:469] Time since start: 27503.78s, 	Step: 66361, 	{'train/accuracy': 0.7131098508834839, 'train/loss': 1.1178903579711914, 'validation/accuracy': 0.6258599758148193, 'validation/loss': 1.5511329174041748, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.26155424118042, 'test/num_examples': 10000, 'score': 25552.440224409103, 'total_duration': 27503.78111886978, 'accumulated_submission_time': 25552.440224409103, 'accumulated_eval_time': 1938.2340450286865, 'accumulated_logging_time': 6.642372369766235}
I0307 09:04:07.677872 140071466415872 logging_writer.py:48] [66361] accumulated_eval_time=1938.23, accumulated_logging_time=6.64237, accumulated_submission_time=25552.4, global_step=66361, preemption_count=0, score=25552.4, test/accuracy=0.4984, test/loss=2.26155, test/num_examples=10000, total_duration=27503.8, train/accuracy=0.71311, train/loss=1.11789, validation/accuracy=0.62586, validation/loss=1.55113, validation/num_examples=50000
I0307 09:04:23.145298 140071458023168 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.9696176052093506, loss=1.7931567430496216
I0307 09:05:01.951576 140071466415872 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.843834161758423, loss=1.83194899559021
I0307 09:05:40.609222 140071458023168 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.735643148422241, loss=1.7953227758407593
I0307 09:06:19.436645 140071466415872 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.6229171752929688, loss=1.7975577116012573
I0307 09:06:58.020180 140071458023168 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.2239830493927, loss=1.8602168560028076
I0307 09:07:36.540194 140071466415872 logging_writer.py:48] [66900] global_step=66900, grad_norm=4.645147323608398, loss=1.7610540390014648
I0307 09:08:15.623157 140071458023168 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.9626402854919434, loss=1.8749055862426758
I0307 09:08:54.366796 140071466415872 logging_writer.py:48] [67100] global_step=67100, grad_norm=4.086541652679443, loss=1.7264492511749268
I0307 09:09:33.084831 140071458023168 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.998267650604248, loss=1.9326090812683105
I0307 09:10:11.810378 140071466415872 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.2684197425842285, loss=1.669013500213623
I0307 09:10:49.960115 140071458023168 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.668257474899292, loss=1.9193084239959717
I0307 09:11:28.527342 140071466415872 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.6665244102478027, loss=1.8132671117782593
I0307 09:12:08.456361 140071458023168 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.959033250808716, loss=1.6887391805648804
I0307 09:12:37.828775 140226914178240 spec.py:321] Evaluating on the training split.
I0307 09:12:49.785600 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 09:13:10.845342 140226914178240 spec.py:349] Evaluating on the test split.
I0307 09:13:12.583016 140226914178240 submission_runner.py:469] Time since start: 28048.81s, 	Step: 67676, 	{'train/accuracy': 0.7088448405265808, 'train/loss': 1.1240360736846924, 'validation/accuracy': 0.6275599598884583, 'validation/loss': 1.530971646308899, 'validation/num_examples': 50000, 'test/accuracy': 0.5067999958992004, 'test/loss': 2.2481958866119385, 'test/num_examples': 10000, 'score': 26062.400289297104, 'total_duration': 28048.813851356506, 'accumulated_submission_time': 26062.400289297104, 'accumulated_eval_time': 1972.988124847412, 'accumulated_logging_time': 6.819470643997192}
I0307 09:13:12.712289 140071466415872 logging_writer.py:48] [67676] accumulated_eval_time=1972.99, accumulated_logging_time=6.81947, accumulated_submission_time=26062.4, global_step=67676, preemption_count=0, score=26062.4, test/accuracy=0.5068, test/loss=2.2482, test/num_examples=10000, total_duration=28048.8, train/accuracy=0.708845, train/loss=1.12404, validation/accuracy=0.62756, validation/loss=1.53097, validation/num_examples=50000
I0307 09:13:22.402565 140071458023168 logging_writer.py:48] [67700] global_step=67700, grad_norm=4.03640604019165, loss=1.8249571323394775
I0307 09:14:00.876126 140071466415872 logging_writer.py:48] [67800] global_step=67800, grad_norm=4.009616374969482, loss=1.811539649963379
I0307 09:14:39.908733 140071458023168 logging_writer.py:48] [67900] global_step=67900, grad_norm=4.3224897384643555, loss=1.7899538278579712
I0307 09:15:18.850780 140071466415872 logging_writer.py:48] [68000] global_step=68000, grad_norm=4.234544277191162, loss=1.861460566520691
I0307 09:15:57.576928 140071458023168 logging_writer.py:48] [68100] global_step=68100, grad_norm=4.086433410644531, loss=1.860537052154541
I0307 09:16:36.412663 140071466415872 logging_writer.py:48] [68200] global_step=68200, grad_norm=4.427675247192383, loss=1.7371282577514648
I0307 09:17:15.385952 140071458023168 logging_writer.py:48] [68300] global_step=68300, grad_norm=5.116874694824219, loss=1.7735707759857178
I0307 09:17:54.770880 140071466415872 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.540416955947876, loss=1.7396841049194336
I0307 09:18:37.865142 140071458023168 logging_writer.py:48] [68500] global_step=68500, grad_norm=4.088119983673096, loss=1.809038758277893
I0307 09:19:25.108274 140071466415872 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.5215635299682617, loss=1.849344253540039
I0307 09:20:07.845985 140071458023168 logging_writer.py:48] [68700] global_step=68700, grad_norm=4.159900188446045, loss=1.8390357494354248
I0307 09:20:49.966354 140071466415872 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.8379223346710205, loss=1.8152109384536743
I0307 09:21:28.454464 140071458023168 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.8338980674743652, loss=1.8104677200317383
I0307 09:21:42.897608 140226914178240 spec.py:321] Evaluating on the training split.
I0307 09:21:54.674002 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 09:22:18.850172 140226914178240 spec.py:349] Evaluating on the test split.
I0307 09:22:20.552920 140226914178240 submission_runner.py:469] Time since start: 28596.78s, 	Step: 68938, 	{'train/accuracy': 0.7111766338348389, 'train/loss': 1.121996521949768, 'validation/accuracy': 0.6229400038719177, 'validation/loss': 1.5530850887298584, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2696895599365234, 'test/num_examples': 10000, 'score': 26572.411048173904, 'total_duration': 28596.78377866745, 'accumulated_submission_time': 26572.411048173904, 'accumulated_eval_time': 2010.643295764923, 'accumulated_logging_time': 6.989057779312134}
I0307 09:22:20.655553 140071466415872 logging_writer.py:48] [68938] accumulated_eval_time=2010.64, accumulated_logging_time=6.98906, accumulated_submission_time=26572.4, global_step=68938, preemption_count=0, score=26572.4, test/accuracy=0.5001, test/loss=2.26969, test/num_examples=10000, total_duration=28596.8, train/accuracy=0.711177, train/loss=1.122, validation/accuracy=0.62294, validation/loss=1.55309, validation/num_examples=50000
I0307 09:22:45.007364 140071458023168 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.701669454574585, loss=1.948594093322754
I0307 09:23:23.675678 140071466415872 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.8644189834594727, loss=1.9906933307647705
I0307 09:24:02.687276 140071458023168 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.803670883178711, loss=1.8116300106048584
I0307 09:24:41.522721 140071466415872 logging_writer.py:48] [69300] global_step=69300, grad_norm=4.028481960296631, loss=1.879401445388794
I0307 09:25:20.071134 140071458023168 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.9650652408599854, loss=1.804350733757019
I0307 09:25:58.848996 140071466415872 logging_writer.py:48] [69500] global_step=69500, grad_norm=4.674720287322998, loss=1.8472931385040283
I0307 09:26:37.602541 140071458023168 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.8101806640625, loss=1.8864625692367554
I0307 09:27:16.592902 140071466415872 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.733987331390381, loss=1.7653870582580566
I0307 09:27:56.228065 140071458023168 logging_writer.py:48] [69800] global_step=69800, grad_norm=4.316584587097168, loss=1.8302178382873535
I0307 09:28:35.408737 140071466415872 logging_writer.py:48] [69900] global_step=69900, grad_norm=4.350659370422363, loss=1.7020543813705444
I0307 09:29:14.670124 140071458023168 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.537220001220703, loss=1.8146179914474487
I0307 09:29:54.422791 140071466415872 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.7493669986724854, loss=1.8231734037399292
I0307 09:30:32.636458 140071458023168 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.7926433086395264, loss=1.7726879119873047
I0307 09:30:50.794457 140226914178240 spec.py:321] Evaluating on the training split.
I0307 09:31:02.760858 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 09:31:20.940266 140226914178240 spec.py:349] Evaluating on the test split.
I0307 09:31:22.662523 140226914178240 submission_runner.py:469] Time since start: 29138.89s, 	Step: 70248, 	{'train/accuracy': 0.7157405614852905, 'train/loss': 1.101019024848938, 'validation/accuracy': 0.6300199627876282, 'validation/loss': 1.526586651802063, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.280522346496582, 'test/num_examples': 10000, 'score': 27082.355411291122, 'total_duration': 29138.893358707428, 'accumulated_submission_time': 27082.355411291122, 'accumulated_eval_time': 2042.5111951828003, 'accumulated_logging_time': 7.146895408630371}
I0307 09:31:22.740910 140071466415872 logging_writer.py:48] [70248] accumulated_eval_time=2042.51, accumulated_logging_time=7.1469, accumulated_submission_time=27082.4, global_step=70248, preemption_count=0, score=27082.4, test/accuracy=0.4974, test/loss=2.28052, test/num_examples=10000, total_duration=29138.9, train/accuracy=0.715741, train/loss=1.10102, validation/accuracy=0.63002, validation/loss=1.52659, validation/num_examples=50000
I0307 09:31:43.200925 140071458023168 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.8900763988494873, loss=1.7384731769561768
I0307 09:32:22.059222 140071466415872 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.436098098754883, loss=1.728428602218628
I0307 09:33:00.630054 140071458023168 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.5676157474517822, loss=1.8015847206115723
I0307 09:33:39.397291 140071466415872 logging_writer.py:48] [70600] global_step=70600, grad_norm=4.050103187561035, loss=1.7303245067596436
I0307 09:34:18.004180 140071458023168 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.7348594665527344, loss=1.7956751585006714
I0307 09:34:56.754594 140071466415872 logging_writer.py:48] [70800] global_step=70800, grad_norm=4.053912162780762, loss=1.908935785293579
I0307 09:35:35.489916 140071458023168 logging_writer.py:48] [70900] global_step=70900, grad_norm=4.049159049987793, loss=1.8557047843933105
I0307 09:36:14.241986 140071466415872 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.9471120834350586, loss=1.788952112197876
I0307 09:36:54.532468 140071458023168 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.9109020233154297, loss=1.866711974143982
I0307 09:37:33.451090 140071466415872 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.7141411304473877, loss=1.7577273845672607
I0307 09:38:12.261443 140071458023168 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.7970550060272217, loss=1.8113481998443604
I0307 09:38:51.020434 140071466415872 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.3401196002960205, loss=1.7498670816421509
I0307 09:39:29.619982 140071458023168 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.543483257293701, loss=1.7982200384140015
I0307 09:39:52.875994 140226914178240 spec.py:321] Evaluating on the training split.
I0307 09:40:04.909704 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 09:40:24.479672 140226914178240 spec.py:349] Evaluating on the test split.
I0307 09:40:26.238226 140226914178240 submission_runner.py:469] Time since start: 29682.47s, 	Step: 71561, 	{'train/accuracy': 0.7039022445678711, 'train/loss': 1.1633414030075073, 'validation/accuracy': 0.6174399852752686, 'validation/loss': 1.5731068849563599, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.267239570617676, 'test/num_examples': 10000, 'score': 27592.31870174408, 'total_duration': 29682.469063282013, 'accumulated_submission_time': 27592.31870174408, 'accumulated_eval_time': 2075.8732635974884, 'accumulated_logging_time': 7.256907224655151}
I0307 09:40:26.408267 140071466415872 logging_writer.py:48] [71561] accumulated_eval_time=2075.87, accumulated_logging_time=7.25691, accumulated_submission_time=27592.3, global_step=71561, preemption_count=0, score=27592.3, test/accuracy=0.5014, test/loss=2.26724, test/num_examples=10000, total_duration=29682.5, train/accuracy=0.703902, train/loss=1.16334, validation/accuracy=0.61744, validation/loss=1.57311, validation/num_examples=50000
I0307 09:40:41.777352 140071458023168 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.7747769355773926, loss=1.913177251815796
I0307 09:41:20.381851 140071466415872 logging_writer.py:48] [71700] global_step=71700, grad_norm=4.034587860107422, loss=1.8254369497299194
I0307 09:41:59.347210 140071458023168 logging_writer.py:48] [71800] global_step=71800, grad_norm=4.203151226043701, loss=1.7562114000320435
I0307 09:42:38.144176 140071466415872 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.952463388442993, loss=1.718360185623169
I0307 09:43:17.826975 140071458023168 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.4627485275268555, loss=1.6932569742202759
I0307 09:43:56.743025 140071466415872 logging_writer.py:48] [72100] global_step=72100, grad_norm=4.2729949951171875, loss=1.7748132944107056
I0307 09:44:36.658734 140071458023168 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.963507890701294, loss=1.768010139465332
I0307 09:45:17.084269 140071466415872 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.8096649646759033, loss=1.8261244297027588
I0307 09:45:57.049172 140071458023168 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.5280561447143555, loss=1.597783088684082
I0307 09:46:36.752432 140071466415872 logging_writer.py:48] [72500] global_step=72500, grad_norm=4.431634902954102, loss=1.7492313385009766
I0307 09:47:16.912848 140071458023168 logging_writer.py:48] [72600] global_step=72600, grad_norm=4.985033988952637, loss=1.7631566524505615
I0307 09:47:55.646716 140071466415872 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.9683241844177246, loss=1.7764030694961548
I0307 09:48:34.704250 140071458023168 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.7209229469299316, loss=1.7859175205230713
I0307 09:48:56.477788 140226914178240 spec.py:321] Evaluating on the training split.
I0307 09:49:08.236609 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 09:49:26.380943 140226914178240 spec.py:349] Evaluating on the test split.
I0307 09:49:28.120203 140226914178240 submission_runner.py:469] Time since start: 30224.35s, 	Step: 72857, 	{'train/accuracy': 0.7160195708274841, 'train/loss': 1.0978602170944214, 'validation/accuracy': 0.6301599740982056, 'validation/loss': 1.5278446674346924, 'validation/num_examples': 50000, 'test/accuracy': 0.5060999989509583, 'test/loss': 2.242117166519165, 'test/num_examples': 10000, 'score': 28102.199460029602, 'total_duration': 30224.351070165634, 'accumulated_submission_time': 28102.199460029602, 'accumulated_eval_time': 2107.5155494213104, 'accumulated_logging_time': 7.477160930633545}
I0307 09:49:28.181622 140071466415872 logging_writer.py:48] [72857] accumulated_eval_time=2107.52, accumulated_logging_time=7.47716, accumulated_submission_time=28102.2, global_step=72857, preemption_count=0, score=28102.2, test/accuracy=0.5061, test/loss=2.24212, test/num_examples=10000, total_duration=30224.4, train/accuracy=0.71602, train/loss=1.09786, validation/accuracy=0.63016, validation/loss=1.52784, validation/num_examples=50000
I0307 09:49:45.381094 140071458023168 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.933340072631836, loss=1.7368309497833252
I0307 09:50:24.067402 140071466415872 logging_writer.py:48] [73000] global_step=73000, grad_norm=4.109114646911621, loss=1.847243309020996
I0307 09:51:03.074348 140071458023168 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.9657669067382812, loss=1.8370403051376343
I0307 09:51:42.031981 140071466415872 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.4723777770996094, loss=1.8533387184143066
I0307 09:52:20.878826 140071458023168 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.554816722869873, loss=1.829580545425415
I0307 09:52:59.668494 140071466415872 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.9894073009490967, loss=1.7313412427902222
I0307 09:53:41.718698 140071458023168 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.9893534183502197, loss=1.7903635501861572
I0307 09:54:22.571543 140071466415872 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.5350677967071533, loss=1.6999651193618774
I0307 09:55:07.093658 140071458023168 logging_writer.py:48] [73700] global_step=73700, grad_norm=4.311522483825684, loss=1.691314935684204
I0307 09:55:50.374271 140071466415872 logging_writer.py:48] [73800] global_step=73800, grad_norm=4.382654190063477, loss=1.8011150360107422
I0307 09:56:29.204586 140071458023168 logging_writer.py:48] [73900] global_step=73900, grad_norm=4.31564474105835, loss=1.8652640581130981
I0307 09:57:08.046329 140071466415872 logging_writer.py:48] [74000] global_step=74000, grad_norm=4.270325183868408, loss=1.701613426208496
I0307 09:57:47.115190 140071458023168 logging_writer.py:48] [74100] global_step=74100, grad_norm=4.212345123291016, loss=1.772536277770996
I0307 09:57:58.453679 140226914178240 spec.py:321] Evaluating on the training split.
I0307 09:58:10.312804 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 09:58:30.510199 140226914178240 spec.py:349] Evaluating on the test split.
I0307 09:58:32.269577 140226914178240 submission_runner.py:469] Time since start: 30768.50s, 	Step: 74130, 	{'train/accuracy': 0.7191087007522583, 'train/loss': 1.100938081741333, 'validation/accuracy': 0.6284999847412109, 'validation/loss': 1.5357930660247803, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.248389482498169, 'test/num_examples': 10000, 'score': 28612.27093076706, 'total_duration': 30768.50037097931, 'accumulated_submission_time': 28612.27093076706, 'accumulated_eval_time': 2141.3312528133392, 'accumulated_logging_time': 7.607385158538818}
I0307 09:58:32.424204 140071466415872 logging_writer.py:48] [74130] accumulated_eval_time=2141.33, accumulated_logging_time=7.60739, accumulated_submission_time=28612.3, global_step=74130, preemption_count=0, score=28612.3, test/accuracy=0.5047, test/loss=2.24839, test/num_examples=10000, total_duration=30768.5, train/accuracy=0.719109, train/loss=1.10094, validation/accuracy=0.6285, validation/loss=1.53579, validation/num_examples=50000
I0307 09:59:00.013795 140071458023168 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.7595531940460205, loss=1.812597632408142
I0307 09:59:38.760809 140071466415872 logging_writer.py:48] [74300] global_step=74300, grad_norm=4.249666213989258, loss=1.7726882696151733
I0307 10:00:17.506345 140071458023168 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.7912683486938477, loss=1.6765905618667603
I0307 10:00:56.320814 140071466415872 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.9965381622314453, loss=1.7001619338989258
I0307 10:01:34.737203 140071458023168 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.9608752727508545, loss=1.8340604305267334
I0307 10:02:13.667310 140071466415872 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.9416189193725586, loss=1.6796389818191528
I0307 10:02:53.441438 140071458023168 logging_writer.py:48] [74800] global_step=74800, grad_norm=4.140238285064697, loss=1.7165334224700928
I0307 10:03:32.910846 140071466415872 logging_writer.py:48] [74900] global_step=74900, grad_norm=4.23943567276001, loss=1.872536540031433
I0307 10:04:13.450848 140071458023168 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.3229329586029053, loss=1.7929089069366455
I0307 10:04:53.155835 140071466415872 logging_writer.py:48] [75100] global_step=75100, grad_norm=4.204070568084717, loss=1.7593122720718384
I0307 10:05:30.609698 140071458023168 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.6006383895874023, loss=1.8068104982376099
I0307 10:06:08.693009 140071466415872 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.9818191528320312, loss=1.7824801206588745
I0307 10:06:46.254501 140071458023168 logging_writer.py:48] [75400] global_step=75400, grad_norm=4.180955410003662, loss=1.8367893695831299
I0307 10:07:02.448687 140226914178240 spec.py:321] Evaluating on the training split.
I0307 10:07:14.922087 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 10:07:34.159990 140226914178240 spec.py:349] Evaluating on the test split.
I0307 10:07:35.887803 140226914178240 submission_runner.py:469] Time since start: 31312.12s, 	Step: 75444, 	{'train/accuracy': 0.720703125, 'train/loss': 1.0922532081604004, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.523364543914795, 'validation/num_examples': 50000, 'test/accuracy': 0.5070000290870667, 'test/loss': 2.2272157669067383, 'test/num_examples': 10000, 'score': 29122.12250471115, 'total_duration': 31312.11866259575, 'accumulated_submission_time': 29122.12250471115, 'accumulated_eval_time': 2174.77023601532, 'accumulated_logging_time': 7.795947074890137}
I0307 10:07:35.994337 140071466415872 logging_writer.py:48] [75444] accumulated_eval_time=2174.77, accumulated_logging_time=7.79595, accumulated_submission_time=29122.1, global_step=75444, preemption_count=0, score=29122.1, test/accuracy=0.507, test/loss=2.22722, test/num_examples=10000, total_duration=31312.1, train/accuracy=0.720703, train/loss=1.09225, validation/accuracy=0.63068, validation/loss=1.52336, validation/num_examples=50000
I0307 10:07:58.211136 140071458023168 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.182135581970215, loss=1.7377859354019165
I0307 10:08:36.240316 140071466415872 logging_writer.py:48] [75600] global_step=75600, grad_norm=4.015609264373779, loss=1.7650381326675415
I0307 10:09:13.854350 140071458023168 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.801058053970337, loss=1.7824209928512573
I0307 10:09:51.515273 140071466415872 logging_writer.py:48] [75800] global_step=75800, grad_norm=4.2312469482421875, loss=1.8268011808395386
I0307 10:10:30.538430 140071458023168 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.973217248916626, loss=1.732788324356079
I0307 10:11:10.173124 140071466415872 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.9381794929504395, loss=1.6984584331512451
I0307 10:11:48.949527 140071458023168 logging_writer.py:48] [76100] global_step=76100, grad_norm=4.057263374328613, loss=1.8047608137130737
I0307 10:12:27.185906 140071466415872 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.6156976222991943, loss=1.7151222229003906
I0307 10:13:05.936509 140071458023168 logging_writer.py:48] [76300] global_step=76300, grad_norm=4.536766529083252, loss=1.8651905059814453
2025-03-07 10:13:06.919340: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:13:44.974995 140071466415872 logging_writer.py:48] [76400] global_step=76400, grad_norm=4.040336608886719, loss=1.7088829278945923
I0307 10:14:24.118065 140071458023168 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.9914159774780273, loss=1.7415043115615845
I0307 10:15:09.009574 140071466415872 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.8964216709136963, loss=1.6923131942749023
I0307 10:15:59.795817 140071458023168 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.8002991676330566, loss=1.8172399997711182
I0307 10:16:06.002697 140226914178240 spec.py:321] Evaluating on the training split.
I0307 10:16:18.818862 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 10:16:37.917137 140226914178240 spec.py:349] Evaluating on the test split.
I0307 10:16:39.677465 140226914178240 submission_runner.py:469] Time since start: 31855.91s, 	Step: 76715, 	{'train/accuracy': 0.7245694994926453, 'train/loss': 1.0543209314346313, 'validation/accuracy': 0.6345599889755249, 'validation/loss': 1.503624439239502, 'validation/num_examples': 50000, 'test/accuracy': 0.5113000273704529, 'test/loss': 2.2342586517333984, 'test/num_examples': 10000, 'score': 29631.933977603912, 'total_duration': 31855.908323049545, 'accumulated_submission_time': 29631.933977603912, 'accumulated_eval_time': 2208.444871902466, 'accumulated_logging_time': 7.9680211544036865}
I0307 10:16:39.808576 140071466415872 logging_writer.py:48] [76715] accumulated_eval_time=2208.44, accumulated_logging_time=7.96802, accumulated_submission_time=29631.9, global_step=76715, preemption_count=0, score=29631.9, test/accuracy=0.5113, test/loss=2.23426, test/num_examples=10000, total_duration=31855.9, train/accuracy=0.724569, train/loss=1.05432, validation/accuracy=0.63456, validation/loss=1.50362, validation/num_examples=50000
I0307 10:17:21.816094 140071458023168 logging_writer.py:48] [76800] global_step=76800, grad_norm=4.172516822814941, loss=1.6610840559005737
I0307 10:18:07.679749 140071466415872 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.9982235431671143, loss=1.7869089841842651
I0307 10:18:52.566687 140071458023168 logging_writer.py:48] [77000] global_step=77000, grad_norm=4.143070697784424, loss=1.7445392608642578
I0307 10:19:34.332787 140071466415872 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.8241655826568604, loss=1.777230978012085
I0307 10:20:12.996932 140071458023168 logging_writer.py:48] [77200] global_step=77200, grad_norm=4.147309303283691, loss=1.8541011810302734
I0307 10:20:54.172501 140071466415872 logging_writer.py:48] [77300] global_step=77300, grad_norm=4.5539703369140625, loss=1.7764008045196533
I0307 10:21:34.487827 140071458023168 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.5923871994018555, loss=1.856123447418213
I0307 10:22:15.892455 140071466415872 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.9036011695861816, loss=1.6442832946777344
I0307 10:22:58.476247 140071458023168 logging_writer.py:48] [77600] global_step=77600, grad_norm=4.593154430389404, loss=1.7344385385513306
I0307 10:23:37.257010 140071466415872 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.5631396770477295, loss=1.701163649559021
I0307 10:24:15.213544 140071458023168 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.7789840698242188, loss=1.6919879913330078
I0307 10:24:54.741714 140071466415872 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.873720169067383, loss=1.779237985610962
I0307 10:25:09.792068 140226914178240 spec.py:321] Evaluating on the training split.
I0307 10:25:22.467467 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 10:25:39.100716 140226914178240 spec.py:349] Evaluating on the test split.
I0307 10:25:40.836700 140226914178240 submission_runner.py:469] Time since start: 32397.07s, 	Step: 77939, 	{'train/accuracy': 0.6695033311843872, 'train/loss': 1.3183430433273315, 'validation/accuracy': 0.6229999661445618, 'validation/loss': 1.5599374771118164, 'validation/num_examples': 50000, 'test/accuracy': 0.49470001459121704, 'test/loss': 2.2986526489257812, 'test/num_examples': 10000, 'score': 30141.76208639145, 'total_duration': 32397.067548513412, 'accumulated_submission_time': 30141.76208639145, 'accumulated_eval_time': 2239.4893548488617, 'accumulated_logging_time': 8.129781484603882}
I0307 10:25:40.966697 140071458023168 logging_writer.py:48] [77939] accumulated_eval_time=2239.49, accumulated_logging_time=8.12978, accumulated_submission_time=30141.8, global_step=77939, preemption_count=0, score=30141.8, test/accuracy=0.4947, test/loss=2.29865, test/num_examples=10000, total_duration=32397.1, train/accuracy=0.669503, train/loss=1.31834, validation/accuracy=0.623, validation/loss=1.55994, validation/num_examples=50000
I0307 10:26:04.896164 140071466415872 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.9103755950927734, loss=1.6790038347244263
I0307 10:26:42.884052 140071458023168 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.9451048374176025, loss=1.7288459539413452
I0307 10:27:21.858675 140071466415872 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.9309964179992676, loss=1.7582993507385254
I0307 10:28:07.278078 140071458023168 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.906174659729004, loss=1.9089330434799194
I0307 10:28:51.111413 140071466415872 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.8292226791381836, loss=1.7344969511032104
I0307 10:29:37.609934 140071458023168 logging_writer.py:48] [78500] global_step=78500, grad_norm=4.338980674743652, loss=1.756668210029602
I0307 10:30:28.345379 140071466415872 logging_writer.py:48] [78600] global_step=78600, grad_norm=4.558522701263428, loss=1.900373935699463
I0307 10:31:13.528252 140071458023168 logging_writer.py:48] [78700] global_step=78700, grad_norm=4.030709743499756, loss=1.7432600259780884
I0307 10:31:56.327136 140071466415872 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.973562240600586, loss=1.6708176136016846
I0307 10:32:34.956951 140071458023168 logging_writer.py:48] [78900] global_step=78900, grad_norm=4.041667461395264, loss=1.779225468635559
I0307 10:33:12.802628 140071466415872 logging_writer.py:48] [79000] global_step=79000, grad_norm=4.172207832336426, loss=1.8673421144485474
I0307 10:33:51.322329 140071458023168 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.9286704063415527, loss=1.7138004302978516
I0307 10:34:11.126411 140226914178240 spec.py:321] Evaluating on the training split.
I0307 10:34:24.008808 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 10:34:46.015706 140226914178240 spec.py:349] Evaluating on the test split.
I0307 10:34:47.742661 140226914178240 submission_runner.py:469] Time since start: 32943.97s, 	Step: 79153, 	{'train/accuracy': 0.6783920526504517, 'train/loss': 1.2600232362747192, 'validation/accuracy': 0.6340000033378601, 'validation/loss': 1.5178097486495972, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2697792053222656, 'test/num_examples': 10000, 'score': 30651.762511491776, 'total_duration': 32943.97350382805, 'accumulated_submission_time': 30651.762511491776, 'accumulated_eval_time': 2276.105446100235, 'accumulated_logging_time': 8.292530298233032}
I0307 10:34:47.856005 140071466415872 logging_writer.py:48] [79153] accumulated_eval_time=2276.11, accumulated_logging_time=8.29253, accumulated_submission_time=30651.8, global_step=79153, preemption_count=0, score=30651.8, test/accuracy=0.5041, test/loss=2.26978, test/num_examples=10000, total_duration=32944, train/accuracy=0.678392, train/loss=1.26002, validation/accuracy=0.634, validation/loss=1.51781, validation/num_examples=50000
I0307 10:35:06.341525 140071458023168 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.8983421325683594, loss=1.7185131311416626
I0307 10:35:47.392453 140071466415872 logging_writer.py:48] [79300] global_step=79300, grad_norm=4.532509803771973, loss=1.8076508045196533
I0307 10:36:31.458211 140071458023168 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.936058521270752, loss=1.7820907831192017
I0307 10:37:09.340476 140071466415872 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.7013323307037354, loss=1.8162195682525635
I0307 10:37:50.241922 140071458023168 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.764195203781128, loss=1.766194462776184
I0307 10:38:32.852349 140071466415872 logging_writer.py:48] [79700] global_step=79700, grad_norm=4.3309502601623535, loss=1.6705400943756104
I0307 10:39:15.284942 140071458023168 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.8653266429901123, loss=1.799594521522522
I0307 10:40:01.580080 140071466415872 logging_writer.py:48] [79900] global_step=79900, grad_norm=4.128244876861572, loss=1.8055013418197632
I0307 10:40:45.156530 140071458023168 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.902916669845581, loss=1.7914857864379883
I0307 10:41:25.560825 140071466415872 logging_writer.py:48] [80100] global_step=80100, grad_norm=4.333885669708252, loss=1.6574019193649292
I0307 10:42:05.819828 140071458023168 logging_writer.py:48] [80200] global_step=80200, grad_norm=4.057210922241211, loss=1.8088477849960327
I0307 10:42:57.372441 140071466415872 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.324807167053223, loss=1.7365361452102661
I0307 10:43:18.201836 140226914178240 spec.py:321] Evaluating on the training split.
I0307 10:43:31.181776 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 10:43:52.188280 140226914178240 spec.py:349] Evaluating on the test split.
I0307 10:43:53.920726 140226914178240 submission_runner.py:469] Time since start: 33490.15s, 	Step: 80337, 	{'train/accuracy': 0.6887954473495483, 'train/loss': 1.2155958414077759, 'validation/accuracy': 0.6378799676895142, 'validation/loss': 1.4846913814544678, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.2073123455047607, 'test/num_examples': 10000, 'score': 31161.91561150551, 'total_duration': 33490.15157008171, 'accumulated_submission_time': 31161.91561150551, 'accumulated_eval_time': 2311.8241789340973, 'accumulated_logging_time': 8.477530479431152}
I0307 10:43:54.057251 140071458023168 logging_writer.py:48] [80337] accumulated_eval_time=2311.82, accumulated_logging_time=8.47753, accumulated_submission_time=31161.9, global_step=80337, preemption_count=0, score=31161.9, test/accuracy=0.5103, test/loss=2.20731, test/num_examples=10000, total_duration=33490.2, train/accuracy=0.688795, train/loss=1.2156, validation/accuracy=0.63788, validation/loss=1.48469, validation/num_examples=50000
I0307 10:44:18.462819 140071466415872 logging_writer.py:48] [80400] global_step=80400, grad_norm=4.010464191436768, loss=1.831697702407837
I0307 10:44:56.606468 140071458023168 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.2599992752075195, loss=1.8235079050064087
I0307 10:45:35.012051 140071466415872 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.734454393386841, loss=1.6947298049926758
I0307 10:46:13.746679 140071458023168 logging_writer.py:48] [80700] global_step=80700, grad_norm=5.244091510772705, loss=1.7700146436691284
I0307 10:46:54.125168 140071466415872 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.426852226257324, loss=1.6250640153884888
I0307 10:47:35.365824 140071458023168 logging_writer.py:48] [80900] global_step=80900, grad_norm=3.624516725540161, loss=1.6849445104599
I0307 10:48:31.115424 140071466415872 logging_writer.py:48] [81000] global_step=81000, grad_norm=4.393942832946777, loss=1.719318151473999
I0307 10:49:16.078552 140071458023168 logging_writer.py:48] [81100] global_step=81100, grad_norm=4.0368452072143555, loss=1.6852574348449707
I0307 10:49:57.281483 140071466415872 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.123410701751709, loss=1.7638578414916992
I0307 10:50:41.442029 140071458023168 logging_writer.py:48] [81300] global_step=81300, grad_norm=4.551304817199707, loss=1.7697789669036865
I0307 10:51:23.114437 140071466415872 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.8281524181365967, loss=1.7951910495758057
I0307 10:52:02.788406 140071458023168 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.436904430389404, loss=1.7500888109207153
I0307 10:52:24.350461 140226914178240 spec.py:321] Evaluating on the training split.
I0307 10:52:36.798787 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 10:52:51.791948 140226914178240 spec.py:349] Evaluating on the test split.
I0307 10:52:53.549391 140226914178240 submission_runner.py:469] Time since start: 34029.78s, 	Step: 81553, 	{'train/accuracy': 0.6809231638908386, 'train/loss': 1.261443018913269, 'validation/accuracy': 0.6340599656105042, 'validation/loss': 1.5173825025558472, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.232513427734375, 'test/num_examples': 10000, 'score': 31672.052864313126, 'total_duration': 34029.7802426815, 'accumulated_submission_time': 31672.052864313126, 'accumulated_eval_time': 2341.022961139679, 'accumulated_logging_time': 8.647173643112183}
I0307 10:52:53.652435 140071466415872 logging_writer.py:48] [81553] accumulated_eval_time=2341.02, accumulated_logging_time=8.64717, accumulated_submission_time=31672.1, global_step=81553, preemption_count=0, score=31672.1, test/accuracy=0.5, test/loss=2.23251, test/num_examples=10000, total_duration=34029.8, train/accuracy=0.680923, train/loss=1.26144, validation/accuracy=0.63406, validation/loss=1.51738, validation/num_examples=50000
I0307 10:53:12.032398 140071458023168 logging_writer.py:48] [81600] global_step=81600, grad_norm=4.431302547454834, loss=1.8226597309112549
I0307 10:53:56.555289 140071466415872 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.7989273071289062, loss=1.685544729232788
I0307 10:54:47.590833 140071458023168 logging_writer.py:48] [81800] global_step=81800, grad_norm=4.235367774963379, loss=1.7221428155899048
I0307 10:55:39.405108 140071466415872 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.627838611602783, loss=1.6610932350158691
I0307 10:56:22.671663 140071458023168 logging_writer.py:48] [82000] global_step=82000, grad_norm=4.247567653656006, loss=1.6812705993652344
I0307 10:57:04.984964 140071466415872 logging_writer.py:48] [82100] global_step=82100, grad_norm=4.153600692749023, loss=1.695722222328186
I0307 10:57:49.036875 140071458023168 logging_writer.py:48] [82200] global_step=82200, grad_norm=4.2884440422058105, loss=1.7297674417495728
I0307 10:58:37.521819 140071466415872 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.821662664413452, loss=1.6786998510360718
I0307 10:59:24.343016 140071458023168 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.6985015869140625, loss=1.6939399242401123
I0307 11:00:06.948404 140071466415872 logging_writer.py:48] [82500] global_step=82500, grad_norm=4.047369480133057, loss=1.8061071634292603
2025-03-07 11:00:33.742608: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:00:51.900217 140071458023168 logging_writer.py:48] [82600] global_step=82600, grad_norm=4.328752517700195, loss=1.7106552124023438
I0307 11:01:23.731825 140226914178240 spec.py:321] Evaluating on the training split.
I0307 11:01:36.483132 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 11:01:55.359033 140226914178240 spec.py:349] Evaluating on the test split.
I0307 11:01:57.089568 140226914178240 submission_runner.py:469] Time since start: 34573.32s, 	Step: 82679, 	{'train/accuracy': 0.6881576776504517, 'train/loss': 1.2260911464691162, 'validation/accuracy': 0.636139988899231, 'validation/loss': 1.500401258468628, 'validation/num_examples': 50000, 'test/accuracy': 0.5071000456809998, 'test/loss': 2.20916485786438, 'test/num_examples': 10000, 'score': 32181.98154401779, 'total_duration': 34573.32043719292, 'accumulated_submission_time': 32181.98154401779, 'accumulated_eval_time': 2374.3805763721466, 'accumulated_logging_time': 8.787813425064087}
I0307 11:01:57.183204 140071466415872 logging_writer.py:48] [82679] accumulated_eval_time=2374.38, accumulated_logging_time=8.78781, accumulated_submission_time=32182, global_step=82679, preemption_count=0, score=32182, test/accuracy=0.5071, test/loss=2.20916, test/num_examples=10000, total_duration=34573.3, train/accuracy=0.688158, train/loss=1.22609, validation/accuracy=0.63614, validation/loss=1.5004, validation/num_examples=50000
I0307 11:02:05.563543 140071458023168 logging_writer.py:48] [82700] global_step=82700, grad_norm=4.33513069152832, loss=1.659828543663025
I0307 11:02:50.217806 140071466415872 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.7206039428710938, loss=1.763407826423645
I0307 11:03:40.220724 140071458023168 logging_writer.py:48] [82900] global_step=82900, grad_norm=5.315185546875, loss=1.7866568565368652
I0307 11:04:36.937832 140071466415872 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.596811294555664, loss=1.7586815357208252
I0307 11:05:32.629785 140071458023168 logging_writer.py:48] [83100] global_step=83100, grad_norm=4.170141696929932, loss=1.8184521198272705
I0307 11:06:18.660380 140071466415872 logging_writer.py:48] [83200] global_step=83200, grad_norm=4.521150588989258, loss=1.730057716369629
I0307 11:06:59.325140 140071458023168 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.964172601699829, loss=1.7322440147399902
I0307 11:07:47.347559 140071466415872 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.885759115219116, loss=1.7109816074371338
I0307 11:08:44.977175 140071458023168 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.003136157989502, loss=1.7297613620758057
I0307 11:09:28.632509 140071466415872 logging_writer.py:48] [83600] global_step=83600, grad_norm=4.453915596008301, loss=1.8277705907821655
I0307 11:10:06.904805 140071458023168 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.91961669921875, loss=1.754235863685608
I0307 11:10:27.230421 140226914178240 spec.py:321] Evaluating on the training split.
I0307 11:10:39.903025 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 11:11:06.795582 140226914178240 spec.py:349] Evaluating on the test split.
I0307 11:11:08.480859 140226914178240 submission_runner.py:469] Time since start: 35124.71s, 	Step: 83753, 	{'train/accuracy': 0.7017896771430969, 'train/loss': 1.168305516242981, 'validation/accuracy': 0.6414600014686584, 'validation/loss': 1.4691267013549805, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.1737451553344727, 'test/num_examples': 10000, 'score': 32691.894963741302, 'total_duration': 35124.71170473099, 'accumulated_submission_time': 32691.894963741302, 'accumulated_eval_time': 2415.630859375, 'accumulated_logging_time': 8.907928943634033}
I0307 11:11:08.630617 140071466415872 logging_writer.py:48] [83753] accumulated_eval_time=2415.63, accumulated_logging_time=8.90793, accumulated_submission_time=32691.9, global_step=83753, preemption_count=0, score=32691.9, test/accuracy=0.5191, test/loss=2.17375, test/num_examples=10000, total_duration=35124.7, train/accuracy=0.70179, train/loss=1.16831, validation/accuracy=0.64146, validation/loss=1.46913, validation/num_examples=50000
I0307 11:11:27.419090 140071458023168 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.262551784515381, loss=1.737284541130066
I0307 11:12:05.830204 140071466415872 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.09489107131958, loss=1.8663389682769775
I0307 11:12:44.632156 140071458023168 logging_writer.py:48] [84000] global_step=84000, grad_norm=4.209775447845459, loss=1.8287543058395386
I0307 11:13:23.025232 140071466415872 logging_writer.py:48] [84100] global_step=84100, grad_norm=4.005951404571533, loss=1.790107011795044
I0307 11:14:12.283518 140071458023168 logging_writer.py:48] [84200] global_step=84200, grad_norm=4.101215362548828, loss=1.8369351625442505
I0307 11:15:06.954097 140071466415872 logging_writer.py:48] [84300] global_step=84300, grad_norm=4.030738353729248, loss=1.7910792827606201
I0307 11:15:53.361711 140071458023168 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.7219698429107666, loss=1.7160691022872925
I0307 11:16:38.170878 140071466415872 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.9300551414489746, loss=1.7404801845550537
I0307 11:17:24.026201 140071458023168 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.066725730895996, loss=1.7523329257965088
I0307 11:18:09.824539 140071466415872 logging_writer.py:48] [84700] global_step=84700, grad_norm=5.30338191986084, loss=1.7343193292617798
I0307 11:18:57.728590 140071458023168 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.178673267364502, loss=1.5944792032241821
I0307 11:19:38.521204 140226914178240 spec.py:321] Evaluating on the training split.
I0307 11:19:51.239105 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 11:20:11.762463 140226914178240 spec.py:349] Evaluating on the test split.
I0307 11:20:13.492467 140226914178240 submission_runner.py:469] Time since start: 35669.72s, 	Step: 84881, 	{'train/accuracy': 0.7056760191917419, 'train/loss': 1.1464003324508667, 'validation/accuracy': 0.6401799917221069, 'validation/loss': 1.4737129211425781, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.1885499954223633, 'test/num_examples': 10000, 'score': 33201.62847113609, 'total_duration': 35669.723321676254, 'accumulated_submission_time': 33201.62847113609, 'accumulated_eval_time': 2450.6020019054413, 'accumulated_logging_time': 9.101006984710693}
I0307 11:20:13.667695 140071466415872 logging_writer.py:48] [84881] accumulated_eval_time=2450.6, accumulated_logging_time=9.10101, accumulated_submission_time=33201.6, global_step=84881, preemption_count=0, score=33201.6, test/accuracy=0.5179, test/loss=2.18855, test/num_examples=10000, total_duration=35669.7, train/accuracy=0.705676, train/loss=1.1464, validation/accuracy=0.64018, validation/loss=1.47371, validation/num_examples=50000
I0307 11:20:21.294385 140071458023168 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.044826984405518, loss=1.7384623289108276
I0307 11:20:59.363046 140071466415872 logging_writer.py:48] [85000] global_step=85000, grad_norm=4.662190914154053, loss=1.618929147720337
I0307 11:21:37.537567 140071458023168 logging_writer.py:48] [85100] global_step=85100, grad_norm=4.345492839813232, loss=1.7092243432998657
I0307 11:22:15.549372 140071466415872 logging_writer.py:48] [85200] global_step=85200, grad_norm=4.009572982788086, loss=1.8479294776916504
I0307 11:22:54.062730 140071458023168 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.881882429122925, loss=1.7593889236450195
I0307 11:23:32.426515 140071466415872 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.8723180294036865, loss=1.625954031944275
I0307 11:24:10.867366 140071458023168 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.334918975830078, loss=1.7326381206512451
I0307 11:24:49.335594 140071466415872 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.8942127227783203, loss=1.6780717372894287
I0307 11:25:27.888025 140071458023168 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.03834867477417, loss=1.6917393207550049
I0307 11:26:06.208122 140071466415872 logging_writer.py:48] [85800] global_step=85800, grad_norm=4.010582447052002, loss=1.7206178903579712
I0307 11:26:44.397151 140071458023168 logging_writer.py:48] [85900] global_step=85900, grad_norm=4.724490165710449, loss=1.7486907243728638
I0307 11:27:23.321747 140071466415872 logging_writer.py:48] [86000] global_step=86000, grad_norm=4.004726886749268, loss=1.745086669921875
I0307 11:28:01.544104 140071458023168 logging_writer.py:48] [86100] global_step=86100, grad_norm=4.032356262207031, loss=1.5691570043563843
I0307 11:28:40.051126 140071466415872 logging_writer.py:48] [86200] global_step=86200, grad_norm=4.2659525871276855, loss=1.7354724407196045
I0307 11:28:43.552854 140226914178240 spec.py:321] Evaluating on the training split.
I0307 11:28:55.037938 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 11:29:11.235229 140226914178240 spec.py:349] Evaluating on the test split.
I0307 11:29:12.968291 140226914178240 submission_runner.py:469] Time since start: 36209.20s, 	Step: 86210, 	{'train/accuracy': 0.6952925324440002, 'train/loss': 1.19424569606781, 'validation/accuracy': 0.634119987487793, 'validation/loss': 1.5135719776153564, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.1956052780151367, 'test/num_examples': 10000, 'score': 33711.33899593353, 'total_duration': 36209.19914627075, 'accumulated_submission_time': 33711.33899593353, 'accumulated_eval_time': 2480.017291545868, 'accumulated_logging_time': 9.311240196228027}
I0307 11:29:13.060260 140071458023168 logging_writer.py:48] [86210] accumulated_eval_time=2480.02, accumulated_logging_time=9.31124, accumulated_submission_time=33711.3, global_step=86210, preemption_count=0, score=33711.3, test/accuracy=0.5136, test/loss=2.19561, test/num_examples=10000, total_duration=36209.2, train/accuracy=0.695293, train/loss=1.19425, validation/accuracy=0.63412, validation/loss=1.51357, validation/num_examples=50000
I0307 11:29:48.084430 140071466415872 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.259653568267822, loss=1.5759981870651245
I0307 11:30:26.955765 140071458023168 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.746230125427246, loss=1.7333312034606934
I0307 11:31:05.548784 140071466415872 logging_writer.py:48] [86500] global_step=86500, grad_norm=3.972874879837036, loss=1.7176306247711182
I0307 11:31:44.329467 140071458023168 logging_writer.py:48] [86600] global_step=86600, grad_norm=4.671210765838623, loss=1.703737497329712
I0307 11:32:22.997822 140071466415872 logging_writer.py:48] [86700] global_step=86700, grad_norm=4.003241539001465, loss=1.6584482192993164
I0307 11:33:01.561037 140071458023168 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.229665279388428, loss=1.732207179069519
I0307 11:33:40.450753 140071466415872 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.9724698066711426, loss=1.6795393228530884
I0307 11:34:19.305283 140071458023168 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.844963788986206, loss=1.721494436264038
I0307 11:34:58.114109 140071466415872 logging_writer.py:48] [87100] global_step=87100, grad_norm=4.096199989318848, loss=1.6406357288360596
I0307 11:35:36.821376 140071458023168 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.840243339538574, loss=1.7966933250427246
I0307 11:36:15.505751 140071466415872 logging_writer.py:48] [87300] global_step=87300, grad_norm=5.877434730529785, loss=1.7970830202102661
I0307 11:36:53.562471 140071458023168 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.879255533218384, loss=1.7763938903808594
I0307 11:37:32.054805 140071466415872 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.0804338455200195, loss=1.745241403579712
I0307 11:37:43.264003 140226914178240 spec.py:321] Evaluating on the training split.
I0307 11:37:55.744028 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 11:38:14.923743 140226914178240 spec.py:349] Evaluating on the test split.
I0307 11:38:16.662657 140226914178240 submission_runner.py:469] Time since start: 36752.89s, 	Step: 87530, 	{'train/accuracy': 0.7108577489852905, 'train/loss': 1.123548984527588, 'validation/accuracy': 0.6423599720001221, 'validation/loss': 1.4598665237426758, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.1592886447906494, 'test/num_examples': 10000, 'score': 34221.37405323982, 'total_duration': 36752.89350628853, 'accumulated_submission_time': 34221.37405323982, 'accumulated_eval_time': 2513.4157898426056, 'accumulated_logging_time': 9.43443751335144}
I0307 11:38:16.744226 140071458023168 logging_writer.py:48] [87530] accumulated_eval_time=2513.42, accumulated_logging_time=9.43444, accumulated_submission_time=34221.4, global_step=87530, preemption_count=0, score=34221.4, test/accuracy=0.5231, test/loss=2.15929, test/num_examples=10000, total_duration=36752.9, train/accuracy=0.710858, train/loss=1.12355, validation/accuracy=0.64236, validation/loss=1.45987, validation/num_examples=50000
I0307 11:38:44.038074 140071466415872 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.284176826477051, loss=1.5740911960601807
I0307 11:39:22.836378 140071458023168 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.150836944580078, loss=1.6076743602752686
I0307 11:40:01.907151 140071466415872 logging_writer.py:48] [87800] global_step=87800, grad_norm=4.085058212280273, loss=1.723806619644165
I0307 11:40:40.749946 140071458023168 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.505057334899902, loss=1.679269790649414
I0307 11:41:19.850341 140071466415872 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.570843696594238, loss=1.8130006790161133
I0307 11:41:58.516070 140071458023168 logging_writer.py:48] [88100] global_step=88100, grad_norm=4.604922771453857, loss=1.6811481714248657
I0307 11:42:37.517651 140071466415872 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.817211627960205, loss=1.70076584815979
I0307 11:43:16.352122 140071458023168 logging_writer.py:48] [88300] global_step=88300, grad_norm=4.025012969970703, loss=1.6543039083480835
I0307 11:43:55.077634 140071466415872 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.7638771533966064, loss=1.791418194770813
I0307 11:44:33.433174 140071458023168 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.312980651855469, loss=1.6856632232666016
I0307 11:45:12.957772 140071466415872 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.465145587921143, loss=1.7177677154541016
I0307 11:45:50.902995 140071458023168 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.9759764671325684, loss=1.5249059200286865
I0307 11:46:28.767519 140071466415872 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.237161636352539, loss=1.7721583843231201
I0307 11:46:46.738421 140226914178240 spec.py:321] Evaluating on the training split.
I0307 11:46:59.505873 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 11:47:24.247627 140226914178240 spec.py:349] Evaluating on the test split.
I0307 11:47:25.951915 140226914178240 submission_runner.py:469] Time since start: 37302.18s, 	Step: 88849, 	{'train/accuracy': 0.7043805718421936, 'train/loss': 1.1414951086044312, 'validation/accuracy': 0.6425399780273438, 'validation/loss': 1.4656932353973389, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.1934759616851807, 'test/num_examples': 10000, 'score': 34731.19294667244, 'total_duration': 37302.18275594711, 'accumulated_submission_time': 34731.19294667244, 'accumulated_eval_time': 2552.6291258335114, 'accumulated_logging_time': 9.548306226730347}
I0307 11:47:26.120347 140071458023168 logging_writer.py:48] [88849] accumulated_eval_time=2552.63, accumulated_logging_time=9.54831, accumulated_submission_time=34731.2, global_step=88849, preemption_count=0, score=34731.2, test/accuracy=0.5129, test/loss=2.19348, test/num_examples=10000, total_duration=37302.2, train/accuracy=0.704381, train/loss=1.1415, validation/accuracy=0.64254, validation/loss=1.46569, validation/num_examples=50000
I0307 11:47:46.304174 140071466415872 logging_writer.py:48] [88900] global_step=88900, grad_norm=4.096275329589844, loss=1.6964157819747925
I0307 11:48:24.869061 140071458023168 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.97019100189209, loss=1.7033816576004028
I0307 11:49:03.551285 140071466415872 logging_writer.py:48] [89100] global_step=89100, grad_norm=5.1937336921691895, loss=1.6385666131973267
I0307 11:49:42.448333 140071458023168 logging_writer.py:48] [89200] global_step=89200, grad_norm=4.02689790725708, loss=1.6861605644226074
I0307 11:50:21.250896 140071466415872 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.444624900817871, loss=1.6837995052337646
I0307 11:50:59.894310 140071458023168 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.907644748687744, loss=1.7163710594177246
I0307 11:51:38.776829 140071466415872 logging_writer.py:48] [89500] global_step=89500, grad_norm=4.005084991455078, loss=1.6517932415008545
I0307 11:52:17.610635 140071458023168 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.981814384460449, loss=1.7133997678756714
I0307 11:52:56.248307 140071466415872 logging_writer.py:48] [89700] global_step=89700, grad_norm=4.0918474197387695, loss=1.5434513092041016
I0307 11:53:35.274371 140071458023168 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.281608581542969, loss=1.6166573762893677
I0307 11:54:14.308585 140071466415872 logging_writer.py:48] [89900] global_step=89900, grad_norm=4.267617702484131, loss=1.7228796482086182
I0307 11:54:52.607027 140071458023168 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.796319007873535, loss=1.673661231994629
I0307 11:55:31.190737 140071466415872 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.496256351470947, loss=1.6663329601287842
I0307 11:55:56.264206 140226914178240 spec.py:321] Evaluating on the training split.
I0307 11:56:08.959489 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 11:56:28.052579 140226914178240 spec.py:349] Evaluating on the test split.
I0307 11:56:29.800637 140226914178240 submission_runner.py:469] Time since start: 37846.03s, 	Step: 90169, 	{'train/accuracy': 0.693359375, 'train/loss': 1.197272539138794, 'validation/accuracy': 0.6341399550437927, 'validation/loss': 1.5148682594299316, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.227407217025757, 'test/num_examples': 10000, 'score': 35241.16833615303, 'total_duration': 37846.03147697449, 'accumulated_submission_time': 35241.16833615303, 'accumulated_eval_time': 2586.1653969287872, 'accumulated_logging_time': 9.739075183868408}
I0307 11:56:29.907752 140071458023168 logging_writer.py:48] [90169] accumulated_eval_time=2586.17, accumulated_logging_time=9.73908, accumulated_submission_time=35241.2, global_step=90169, preemption_count=0, score=35241.2, test/accuracy=0.5078, test/loss=2.22741, test/num_examples=10000, total_duration=37846, train/accuracy=0.693359, train/loss=1.19727, validation/accuracy=0.63414, validation/loss=1.51487, validation/num_examples=50000
I0307 11:56:42.372076 140071466415872 logging_writer.py:48] [90200] global_step=90200, grad_norm=4.138689994812012, loss=1.7132325172424316
I0307 11:57:21.143613 140071458023168 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.9650325775146484, loss=1.616483211517334
I0307 11:57:59.677546 140071466415872 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.905977964401245, loss=1.5423582792282104
I0307 11:58:38.246104 140071458023168 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.119723796844482, loss=1.7674224376678467
I0307 11:59:16.785458 140071466415872 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.455718040466309, loss=1.6454800367355347
I0307 11:59:55.712739 140071458023168 logging_writer.py:48] [90700] global_step=90700, grad_norm=4.174607753753662, loss=1.6535183191299438
I0307 12:00:34.384828 140071466415872 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.667759418487549, loss=1.768261194229126
I0307 12:01:13.077620 140071458023168 logging_writer.py:48] [90900] global_step=90900, grad_norm=4.285945415496826, loss=1.7179502248764038
I0307 12:01:51.361444 140071466415872 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.837686061859131, loss=1.728692889213562
I0307 12:02:30.029841 140071458023168 logging_writer.py:48] [91100] global_step=91100, grad_norm=4.279666423797607, loss=1.7425209283828735
I0307 12:03:08.026686 140071466415872 logging_writer.py:48] [91200] global_step=91200, grad_norm=4.0341620445251465, loss=1.620879888534546
I0307 12:03:46.185513 140071458023168 logging_writer.py:48] [91300] global_step=91300, grad_norm=4.748393535614014, loss=1.8115758895874023
I0307 12:04:25.482208 140071466415872 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.463204860687256, loss=1.737208604812622
I0307 12:05:00.095694 140226914178240 spec.py:321] Evaluating on the training split.
I0307 12:05:12.917723 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 12:05:31.838034 140226914178240 spec.py:349] Evaluating on the test split.
I0307 12:05:33.567275 140226914178240 submission_runner.py:469] Time since start: 38389.80s, 	Step: 91490, 	{'train/accuracy': 0.7059151530265808, 'train/loss': 1.1385197639465332, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.459367275238037, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.176414966583252, 'test/num_examples': 10000, 'score': 35751.13984942436, 'total_duration': 38389.79814577103, 'accumulated_submission_time': 35751.13984942436, 'accumulated_eval_time': 2619.636850833893, 'accumulated_logging_time': 9.918402910232544}
I0307 12:05:33.661949 140071458023168 logging_writer.py:48] [91490] accumulated_eval_time=2619.64, accumulated_logging_time=9.9184, accumulated_submission_time=35751.1, global_step=91490, preemption_count=0, score=35751.1, test/accuracy=0.5207, test/loss=2.17641, test/num_examples=10000, total_duration=38389.8, train/accuracy=0.705915, train/loss=1.13852, validation/accuracy=0.64214, validation/loss=1.45937, validation/num_examples=50000
I0307 12:05:38.045341 140071466415872 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.4589924812316895, loss=1.6253465414047241
I0307 12:06:16.694209 140071458023168 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.364107131958008, loss=1.602782964706421
I0307 12:06:55.302063 140071466415872 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.043824195861816, loss=1.7218230962753296
I0307 12:07:34.130278 140071458023168 logging_writer.py:48] [91800] global_step=91800, grad_norm=3.851611852645874, loss=1.673421025276184
I0307 12:08:12.731336 140071466415872 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.546177864074707, loss=1.6029903888702393
I0307 12:08:51.314098 140071458023168 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.935270309448242, loss=1.6878961324691772
I0307 12:09:29.856348 140071466415872 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.420580863952637, loss=1.6832151412963867
I0307 12:10:08.384129 140071458023168 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.5215935707092285, loss=1.729568600654602
I0307 12:10:46.646740 140071466415872 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.6506242752075195, loss=1.6034785509109497
I0307 12:11:26.437169 140071458023168 logging_writer.py:48] [92400] global_step=92400, grad_norm=4.096477508544922, loss=1.7067474126815796
I0307 12:12:04.655631 140071466415872 logging_writer.py:48] [92500] global_step=92500, grad_norm=4.743581295013428, loss=1.6360656023025513
I0307 12:12:43.135297 140071458023168 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.973919153213501, loss=1.6839555501937866
I0307 12:13:21.487481 140071466415872 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.247490882873535, loss=1.703842282295227
I0307 12:13:59.676314 140071458023168 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.26881217956543, loss=1.7590115070343018
I0307 12:14:03.951225 140226914178240 spec.py:321] Evaluating on the training split.
I0307 12:14:16.417804 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 12:14:35.032810 140226914178240 spec.py:349] Evaluating on the test split.
I0307 12:14:36.791765 140226914178240 submission_runner.py:469] Time since start: 38933.02s, 	Step: 92812, 	{'train/accuracy': 0.7111966013908386, 'train/loss': 1.1184314489364624, 'validation/accuracy': 0.648419976234436, 'validation/loss': 1.4376012086868286, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1643314361572266, 'test/num_examples': 10000, 'score': 36261.22060036659, 'total_duration': 38933.02260828018, 'accumulated_submission_time': 36261.22060036659, 'accumulated_eval_time': 2652.4772338867188, 'accumulated_logging_time': 10.078310251235962}
I0307 12:14:36.951953 140071466415872 logging_writer.py:48] [92812] accumulated_eval_time=2652.48, accumulated_logging_time=10.0783, accumulated_submission_time=36261.2, global_step=92812, preemption_count=0, score=36261.2, test/accuracy=0.5205, test/loss=2.16433, test/num_examples=10000, total_duration=38933, train/accuracy=0.711197, train/loss=1.11843, validation/accuracy=0.64842, validation/loss=1.4376, validation/num_examples=50000
I0307 12:15:11.294705 140071458023168 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.351774215698242, loss=1.6641788482666016
I0307 12:15:50.137294 140071466415872 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.258339881896973, loss=1.6895465850830078
I0307 12:16:29.078454 140071458023168 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.30409574508667, loss=1.691606879234314
I0307 12:17:07.587221 140071466415872 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.936023235321045, loss=1.6724845170974731
I0307 12:17:45.984214 140071458023168 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.589956760406494, loss=1.688363790512085
I0307 12:18:25.082481 140071466415872 logging_writer.py:48] [93400] global_step=93400, grad_norm=4.921472072601318, loss=1.6326704025268555
I0307 12:19:04.040085 140071458023168 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.701902389526367, loss=1.7072346210479736
I0307 12:19:42.658172 140071466415872 logging_writer.py:48] [93600] global_step=93600, grad_norm=4.227118015289307, loss=1.627606987953186
I0307 12:20:20.825078 140071458023168 logging_writer.py:48] [93700] global_step=93700, grad_norm=4.48620080947876, loss=1.6545860767364502
I0307 12:20:59.150676 140071466415872 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.277765274047852, loss=1.6564664840698242
I0307 12:21:37.606444 140071458023168 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.369435787200928, loss=1.6834551095962524
I0307 12:22:15.897979 140071466415872 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.207447052001953, loss=1.6409963369369507
I0307 12:22:54.733577 140071458023168 logging_writer.py:48] [94100] global_step=94100, grad_norm=4.6135783195495605, loss=1.6940699815750122
I0307 12:23:07.183228 140226914178240 spec.py:321] Evaluating on the training split.
I0307 12:23:19.007231 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 12:23:36.104678 140226914178240 spec.py:349] Evaluating on the test split.
I0307 12:23:37.849583 140226914178240 submission_runner.py:469] Time since start: 39474.08s, 	Step: 94133, 	{'train/accuracy': 0.711933970451355, 'train/loss': 1.1129710674285889, 'validation/accuracy': 0.6499599814414978, 'validation/loss': 1.4294352531433105, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.1395394802093506, 'test/num_examples': 10000, 'score': 36771.27670288086, 'total_duration': 39474.08042764664, 'accumulated_submission_time': 36771.27670288086, 'accumulated_eval_time': 2683.143434524536, 'accumulated_logging_time': 10.269637107849121}
I0307 12:23:37.983887 140071466415872 logging_writer.py:48] [94133] accumulated_eval_time=2683.14, accumulated_logging_time=10.2696, accumulated_submission_time=36771.3, global_step=94133, preemption_count=0, score=36771.3, test/accuracy=0.5188, test/loss=2.13954, test/num_examples=10000, total_duration=39474.1, train/accuracy=0.711934, train/loss=1.11297, validation/accuracy=0.64996, validation/loss=1.42944, validation/num_examples=50000
I0307 12:24:04.363422 140071458023168 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.103848934173584, loss=1.6086708307266235
I0307 12:24:52.532981 140071466415872 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.198532581329346, loss=1.6809688806533813
I0307 12:25:31.021583 140071458023168 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.743581295013428, loss=1.5972286462783813
I0307 12:26:09.938973 140071466415872 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.527711391448975, loss=1.7504301071166992
I0307 12:26:48.280783 140071458023168 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.7747721672058105, loss=1.7924600839614868
I0307 12:27:27.316798 140071466415872 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.9341113567352295, loss=1.60881507396698
I0307 12:28:06.449543 140071458023168 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.532271385192871, loss=1.5735142230987549
I0307 12:28:45.578889 140071466415872 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.497920989990234, loss=1.778012752532959
I0307 12:29:24.864018 140071458023168 logging_writer.py:48] [95000] global_step=95000, grad_norm=4.56363582611084, loss=1.6817747354507446
2025-03-07 12:29:48.055073: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:30:03.853509 140071466415872 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.050441265106201, loss=1.6454225778579712
I0307 12:30:42.571202 140071458023168 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.257019996643066, loss=1.705751895904541
I0307 12:31:21.087591 140071466415872 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.9468884468078613, loss=1.797559142112732
I0307 12:31:59.700305 140071458023168 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.733711242675781, loss=1.5462442636489868
I0307 12:32:08.172413 140226914178240 spec.py:321] Evaluating on the training split.
I0307 12:32:20.021015 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 12:32:36.953709 140226914178240 spec.py:349] Evaluating on the test split.
I0307 12:32:38.703556 140226914178240 submission_runner.py:469] Time since start: 40014.93s, 	Step: 95423, 	{'train/accuracy': 0.7125916481018066, 'train/loss': 1.1086246967315674, 'validation/accuracy': 0.6494799852371216, 'validation/loss': 1.4302985668182373, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1537413597106934, 'test/num_examples': 10000, 'score': 37281.26604413986, 'total_duration': 40014.93439292908, 'accumulated_submission_time': 37281.26604413986, 'accumulated_eval_time': 2713.674413919449, 'accumulated_logging_time': 10.460416555404663}
I0307 12:32:38.817454 140071466415872 logging_writer.py:48] [95423] accumulated_eval_time=2713.67, accumulated_logging_time=10.4604, accumulated_submission_time=37281.3, global_step=95423, preemption_count=0, score=37281.3, test/accuracy=0.5225, test/loss=2.15374, test/num_examples=10000, total_duration=40014.9, train/accuracy=0.712592, train/loss=1.10862, validation/accuracy=0.64948, validation/loss=1.4303, validation/num_examples=50000
I0307 12:33:09.097909 140071458023168 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.076182842254639, loss=1.603003978729248
I0307 12:33:48.194786 140071466415872 logging_writer.py:48] [95600] global_step=95600, grad_norm=5.0929741859436035, loss=1.691123604774475
I0307 12:34:26.664307 140071458023168 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.951711654663086, loss=1.6991515159606934
I0307 12:35:05.414971 140071466415872 logging_writer.py:48] [95800] global_step=95800, grad_norm=5.123356342315674, loss=1.7899473905563354
I0307 12:35:43.947914 140071458023168 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.392971038818359, loss=1.6595618724822998
I0307 12:36:23.386868 140071466415872 logging_writer.py:48] [96000] global_step=96000, grad_norm=5.164041519165039, loss=1.5835301876068115
I0307 12:37:03.415283 140071458023168 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.822555065155029, loss=1.627298355102539
I0307 12:37:42.882189 140071466415872 logging_writer.py:48] [96200] global_step=96200, grad_norm=5.264449596405029, loss=1.710821270942688
I0307 12:38:21.866896 140071458023168 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.031204700469971, loss=1.5625736713409424
2025-03-07 12:38:28.242824: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:39:00.674183 140071466415872 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.448709964752197, loss=1.6474626064300537
I0307 12:39:39.607671 140071458023168 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.602908134460449, loss=1.7475340366363525
I0307 12:40:18.603812 140071466415872 logging_writer.py:48] [96600] global_step=96600, grad_norm=4.243215084075928, loss=1.7223539352416992
I0307 12:40:57.602905 140071458023168 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.376527786254883, loss=1.5767608880996704
I0307 12:41:08.973427 140226914178240 spec.py:321] Evaluating on the training split.
I0307 12:41:21.118420 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 12:41:36.468517 140226914178240 spec.py:349] Evaluating on the test split.
I0307 12:41:38.202320 140226914178240 submission_runner.py:469] Time since start: 40554.43s, 	Step: 96730, 	{'train/accuracy': 0.7102798223495483, 'train/loss': 1.1264177560806274, 'validation/accuracy': 0.6484999656677246, 'validation/loss': 1.4373013973236084, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.1685001850128174, 'test/num_examples': 10000, 'score': 37791.249816179276, 'total_duration': 40554.43316364288, 'accumulated_submission_time': 37791.249816179276, 'accumulated_eval_time': 2742.9031484127045, 'accumulated_logging_time': 10.597673416137695}
I0307 12:41:38.375295 140071466415872 logging_writer.py:48] [96730] accumulated_eval_time=2742.9, accumulated_logging_time=10.5977, accumulated_submission_time=37791.2, global_step=96730, preemption_count=0, score=37791.2, test/accuracy=0.5202, test/loss=2.1685, test/num_examples=10000, total_duration=40554.4, train/accuracy=0.71028, train/loss=1.12642, validation/accuracy=0.6485, validation/loss=1.4373, validation/num_examples=50000
I0307 12:42:05.775781 140071458023168 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.704611301422119, loss=1.6713426113128662
I0307 12:42:44.702414 140071466415872 logging_writer.py:48] [96900] global_step=96900, grad_norm=4.25838041305542, loss=1.636765718460083
I0307 12:43:23.564996 140071458023168 logging_writer.py:48] [97000] global_step=97000, grad_norm=4.430390357971191, loss=1.6694389581680298
I0307 12:44:02.516782 140071466415872 logging_writer.py:48] [97100] global_step=97100, grad_norm=4.3903727531433105, loss=1.5826570987701416
I0307 12:44:41.557592 140071458023168 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.506098747253418, loss=1.655247449874878
I0307 12:45:20.364627 140071466415872 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.351678848266602, loss=1.7711598873138428
I0307 12:46:00.079022 140071458023168 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.207386016845703, loss=1.6083794832229614
I0307 12:46:39.177125 140071466415872 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.8414955139160156, loss=1.6707701683044434
2025-03-07 12:47:04.168703: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:47:17.658754 140071458023168 logging_writer.py:48] [97600] global_step=97600, grad_norm=3.9679627418518066, loss=1.5939046144485474
I0307 12:47:56.900654 140071466415872 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.395465850830078, loss=1.6398956775665283
I0307 12:48:36.094153 140071458023168 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.957257032394409, loss=1.6312609910964966
I0307 12:49:14.972646 140071466415872 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.613846302032471, loss=1.7379814386367798
I0307 12:49:53.504831 140071458023168 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.109992027282715, loss=1.6328846216201782
I0307 12:50:08.254120 140226914178240 spec.py:321] Evaluating on the training split.
I0307 12:50:19.991662 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 12:50:38.127827 140226914178240 spec.py:349] Evaluating on the test split.
I0307 12:50:39.914233 140226914178240 submission_runner.py:469] Time since start: 41096.15s, 	Step: 98039, 	{'train/accuracy': 0.7233338356018066, 'train/loss': 1.0646077394485474, 'validation/accuracy': 0.6569199562072754, 'validation/loss': 1.3947858810424805, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.0793044567108154, 'test/num_examples': 10000, 'score': 38300.95591688156, 'total_duration': 41096.14505004883, 'accumulated_submission_time': 38300.95591688156, 'accumulated_eval_time': 2774.563080072403, 'accumulated_logging_time': 10.798969268798828}
I0307 12:50:40.018309 140071466415872 logging_writer.py:48] [98039] accumulated_eval_time=2774.56, accumulated_logging_time=10.799, accumulated_submission_time=38301, global_step=98039, preemption_count=0, score=38301, test/accuracy=0.5341, test/loss=2.0793, test/num_examples=10000, total_duration=41096.1, train/accuracy=0.723334, train/loss=1.06461, validation/accuracy=0.65692, validation/loss=1.39479, validation/num_examples=50000
I0307 12:51:03.852759 140071458023168 logging_writer.py:48] [98100] global_step=98100, grad_norm=5.668494701385498, loss=1.7046732902526855
I0307 12:51:42.402345 140071466415872 logging_writer.py:48] [98200] global_step=98200, grad_norm=4.467000961303711, loss=1.6416343450546265
I0307 12:52:21.168868 140071458023168 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.662376880645752, loss=1.7284226417541504
I0307 12:52:59.781942 140071466415872 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.998552083969116, loss=1.6731728315353394
I0307 12:53:38.408789 140071458023168 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.156105995178223, loss=1.5491859912872314
I0307 12:54:17.400776 140071466415872 logging_writer.py:48] [98600] global_step=98600, grad_norm=4.506229877471924, loss=1.6632049083709717
I0307 12:54:56.285536 140071458023168 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.815464019775391, loss=1.6771568059921265
I0307 12:55:35.133340 140071466415872 logging_writer.py:48] [98800] global_step=98800, grad_norm=4.368597030639648, loss=1.6137094497680664
2025-03-07 12:55:40.327827: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:56:13.922365 140071458023168 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.739694118499756, loss=1.694462537765503
I0307 12:56:52.189930 140071466415872 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.524706840515137, loss=1.727992057800293
I0307 12:57:30.887349 140071458023168 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.1009135246276855, loss=1.5424715280532837
I0307 12:58:09.813381 140071466415872 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.917672157287598, loss=1.5893019437789917
I0307 12:58:48.634901 140071458023168 logging_writer.py:48] [99300] global_step=99300, grad_norm=4.317086696624756, loss=1.5828685760498047
I0307 12:59:09.995998 140226914178240 spec.py:321] Evaluating on the training split.
I0307 12:59:21.822787 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 12:59:40.694269 140226914178240 spec.py:349] Evaluating on the test split.
I0307 12:59:42.436275 140226914178240 submission_runner.py:469] Time since start: 41638.67s, 	Step: 99356, 	{'train/accuracy': 0.714863657951355, 'train/loss': 1.1028037071228027, 'validation/accuracy': 0.648639976978302, 'validation/loss': 1.4381848573684692, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.125122547149658, 'test/num_examples': 10000, 'score': 38810.746773958206, 'total_duration': 41638.66712594032, 'accumulated_submission_time': 38810.746773958206, 'accumulated_eval_time': 2807.0032069683075, 'accumulated_logging_time': 10.945482015609741}
I0307 12:59:42.565892 140071466415872 logging_writer.py:48] [99356] accumulated_eval_time=2807, accumulated_logging_time=10.9455, accumulated_submission_time=38810.7, global_step=99356, preemption_count=0, score=38810.7, test/accuracy=0.5254, test/loss=2.12512, test/num_examples=10000, total_duration=41638.7, train/accuracy=0.714864, train/loss=1.1028, validation/accuracy=0.64864, validation/loss=1.43818, validation/num_examples=50000
I0307 12:59:59.894150 140071458023168 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.121458530426025, loss=1.5155857801437378
I0307 13:00:38.755311 140071466415872 logging_writer.py:48] [99500] global_step=99500, grad_norm=4.751328468322754, loss=1.5911550521850586
I0307 13:01:17.823401 140071458023168 logging_writer.py:48] [99600] global_step=99600, grad_norm=4.415095806121826, loss=1.709062099456787
I0307 13:01:56.424218 140071466415872 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.223111629486084, loss=1.61064875125885
I0307 13:02:35.199162 140071458023168 logging_writer.py:48] [99800] global_step=99800, grad_norm=4.628420352935791, loss=1.5160691738128662
I0307 13:03:14.459306 140071466415872 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.633206844329834, loss=1.5806739330291748
I0307 13:03:52.770821 140071458023168 logging_writer.py:48] [100000] global_step=100000, grad_norm=5.0144572257995605, loss=1.6777623891830444
2025-03-07 13:04:17.790428: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:04:31.599049 140071466415872 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.945916652679443, loss=1.7069011926651
I0307 13:05:10.112300 140071458023168 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.262877464294434, loss=1.7165064811706543
I0307 13:05:48.754992 140071466415872 logging_writer.py:48] [100300] global_step=100300, grad_norm=5.003519535064697, loss=1.7553831338882446
I0307 13:06:27.282211 140071458023168 logging_writer.py:48] [100400] global_step=100400, grad_norm=5.015542030334473, loss=1.640555500984192
I0307 13:07:06.375882 140071466415872 logging_writer.py:48] [100500] global_step=100500, grad_norm=4.591083526611328, loss=1.6486551761627197
I0307 13:07:45.615349 140071458023168 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.0241899490356445, loss=1.6267311573028564
I0307 13:08:12.439308 140226914178240 spec.py:321] Evaluating on the training split.
I0307 13:08:24.580663 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 13:08:40.809533 140226914178240 spec.py:349] Evaluating on the test split.
I0307 13:08:42.535875 140226914178240 submission_runner.py:469] Time since start: 42178.77s, 	Step: 100670, 	{'train/accuracy': 0.7261638641357422, 'train/loss': 1.058073878288269, 'validation/accuracy': 0.6642799973487854, 'validation/loss': 1.3675224781036377, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.073992967605591, 'test/num_examples': 10000, 'score': 39320.436344861984, 'total_duration': 42178.76672887802, 'accumulated_submission_time': 39320.436344861984, 'accumulated_eval_time': 2837.099628686905, 'accumulated_logging_time': 11.115626335144043}
I0307 13:08:42.630823 140071466415872 logging_writer.py:48] [100670] accumulated_eval_time=2837.1, accumulated_logging_time=11.1156, accumulated_submission_time=39320.4, global_step=100670, preemption_count=0, score=39320.4, test/accuracy=0.5322, test/loss=2.07399, test/num_examples=10000, total_duration=42178.8, train/accuracy=0.726164, train/loss=1.05807, validation/accuracy=0.66428, validation/loss=1.36752, validation/num_examples=50000
I0307 13:08:54.522966 140071458023168 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.300820827484131, loss=1.6456844806671143
I0307 13:09:32.891167 140071466415872 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.771613597869873, loss=1.672581434249878
I0307 13:10:11.390917 140071458023168 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.680825233459473, loss=1.5611107349395752
I0307 13:10:50.699047 140071466415872 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.262117862701416, loss=1.627916932106018
I0307 13:11:29.534535 140071458023168 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.270132541656494, loss=1.5968527793884277
I0307 13:12:08.844242 140071466415872 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.2533183097839355, loss=1.7416350841522217
I0307 13:12:47.540138 140071458023168 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.647867679595947, loss=1.6992918252944946
2025-03-07 13:12:53.657627: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:13:26.522949 140071466415872 logging_writer.py:48] [101400] global_step=101400, grad_norm=4.694721221923828, loss=1.6061835289001465
I0307 13:14:05.516600 140071458023168 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.8322529792785645, loss=1.7827894687652588
I0307 13:14:44.652633 140071466415872 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.397455215454102, loss=1.6124682426452637
I0307 13:15:23.398536 140071458023168 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.610673427581787, loss=1.647881031036377
I0307 13:16:02.518209 140071466415872 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.601499557495117, loss=1.6148810386657715
I0307 13:16:41.515316 140071458023168 logging_writer.py:48] [101900] global_step=101900, grad_norm=4.234161853790283, loss=1.608253836631775
I0307 13:17:12.539041 140226914178240 spec.py:321] Evaluating on the training split.
I0307 13:17:24.573880 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 13:17:42.422999 140226914178240 spec.py:349] Evaluating on the test split.
I0307 13:17:44.158638 140226914178240 submission_runner.py:469] Time since start: 42720.39s, 	Step: 101980, 	{'train/accuracy': 0.7166374325752258, 'train/loss': 1.0994858741760254, 'validation/accuracy': 0.6571999788284302, 'validation/loss': 1.404793381690979, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.145573854446411, 'test/num_examples': 10000, 'score': 39830.13855481148, 'total_duration': 42720.38946413994, 'accumulated_submission_time': 39830.13855481148, 'accumulated_eval_time': 2868.7190704345703, 'accumulated_logging_time': 11.271819353103638}
I0307 13:17:44.270576 140071466415872 logging_writer.py:48] [101980] accumulated_eval_time=2868.72, accumulated_logging_time=11.2718, accumulated_submission_time=39830.1, global_step=101980, preemption_count=0, score=39830.1, test/accuracy=0.5207, test/loss=2.14557, test/num_examples=10000, total_duration=42720.4, train/accuracy=0.716637, train/loss=1.09949, validation/accuracy=0.6572, validation/loss=1.40479, validation/num_examples=50000
I0307 13:17:52.491415 140071458023168 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.558923721313477, loss=1.571251392364502
I0307 13:18:31.199919 140071466415872 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.841834545135498, loss=1.581932783126831
I0307 13:19:10.151318 140071458023168 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.559170722961426, loss=1.6073863506317139
I0307 13:19:49.207761 140071466415872 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.453749656677246, loss=1.6668195724487305
I0307 13:20:28.333583 140071458023168 logging_writer.py:48] [102400] global_step=102400, grad_norm=5.128813743591309, loss=1.7149195671081543
I0307 13:21:07.322293 140071466415872 logging_writer.py:48] [102500] global_step=102500, grad_norm=5.0687761306762695, loss=1.572907567024231
2025-03-07 13:21:33.424339: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:21:46.421561 140071458023168 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.011831283569336, loss=1.5073602199554443
I0307 13:22:25.657060 140071466415872 logging_writer.py:48] [102700] global_step=102700, grad_norm=5.201783180236816, loss=1.7061594724655151
I0307 13:23:04.670096 140071458023168 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.220565319061279, loss=1.5994071960449219
I0307 13:23:43.765350 140071466415872 logging_writer.py:48] [102900] global_step=102900, grad_norm=5.0887908935546875, loss=1.6621267795562744
I0307 13:24:22.781807 140071458023168 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.329537391662598, loss=1.6407564878463745
I0307 13:25:01.601010 140071466415872 logging_writer.py:48] [103100] global_step=103100, grad_norm=5.275287628173828, loss=1.5528804063796997
I0307 13:25:40.546208 140071458023168 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.840142250061035, loss=1.6201342344284058
I0307 13:26:14.394164 140226914178240 spec.py:321] Evaluating on the training split.
I0307 13:26:26.410578 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 13:26:42.996354 140226914178240 spec.py:349] Evaluating on the test split.
I0307 13:26:44.737766 140226914178240 submission_runner.py:469] Time since start: 43260.97s, 	Step: 103288, 	{'train/accuracy': 0.7274593114852905, 'train/loss': 1.061448097229004, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.3682620525360107, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.0555880069732666, 'test/num_examples': 10000, 'score': 40340.08786869049, 'total_duration': 43260.96857190132, 'accumulated_submission_time': 40340.08786869049, 'accumulated_eval_time': 2899.0624799728394, 'accumulated_logging_time': 11.415260076522827}
I0307 13:26:44.833315 140071466415872 logging_writer.py:48] [103288] accumulated_eval_time=2899.06, accumulated_logging_time=11.4153, accumulated_submission_time=40340.1, global_step=103288, preemption_count=0, score=40340.1, test/accuracy=0.5409, test/loss=2.05559, test/num_examples=10000, total_duration=43261, train/accuracy=0.727459, train/loss=1.06145, validation/accuracy=0.66152, validation/loss=1.36826, validation/num_examples=50000
I0307 13:26:49.923857 140071458023168 logging_writer.py:48] [103300] global_step=103300, grad_norm=4.273534774780273, loss=1.4895254373550415
I0307 13:27:28.805740 140071466415872 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.507474899291992, loss=1.6173515319824219
I0307 13:28:07.915613 140071458023168 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.16649866104126, loss=1.6845747232437134
I0307 13:28:47.271458 140071466415872 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.249650955200195, loss=1.5734261274337769
I0307 13:29:26.535512 140071458023168 logging_writer.py:48] [103700] global_step=103700, grad_norm=5.101719856262207, loss=1.5619641542434692
I0307 13:30:05.300217 140071466415872 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.477749347686768, loss=1.5587822198867798
2025-03-07 13:30:12.162446: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:30:44.366721 140071458023168 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.941815376281738, loss=1.6709835529327393
I0307 13:31:23.438939 140071466415872 logging_writer.py:48] [104000] global_step=104000, grad_norm=5.189363479614258, loss=1.5364341735839844
I0307 13:32:02.470391 140071458023168 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.420756816864014, loss=1.7147225141525269
I0307 13:32:41.432276 140071466415872 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.489955902099609, loss=1.5364007949829102
I0307 13:33:20.243717 140071458023168 logging_writer.py:48] [104300] global_step=104300, grad_norm=5.193309783935547, loss=1.6799668073654175
I0307 13:33:59.379535 140071466415872 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.289063930511475, loss=1.528227686882019
I0307 13:34:38.424843 140071458023168 logging_writer.py:48] [104500] global_step=104500, grad_norm=5.448063850402832, loss=1.643223762512207
I0307 13:35:14.744758 140226914178240 spec.py:321] Evaluating on the training split.
I0307 13:35:26.419712 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 13:35:44.285112 140226914178240 spec.py:349] Evaluating on the test split.
I0307 13:35:46.015638 140226914178240 submission_runner.py:469] Time since start: 43802.25s, 	Step: 104595, 	{'train/accuracy': 0.7285953164100647, 'train/loss': 1.03449547290802, 'validation/accuracy': 0.6672399640083313, 'validation/loss': 1.3548555374145508, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.0678181648254395, 'test/num_examples': 10000, 'score': 40849.828301906586, 'total_duration': 43802.246480464935, 'accumulated_submission_time': 40849.828301906586, 'accumulated_eval_time': 2930.333208322525, 'accumulated_logging_time': 11.538697004318237}
I0307 13:35:46.135589 140071466415872 logging_writer.py:48] [104595] accumulated_eval_time=2930.33, accumulated_logging_time=11.5387, accumulated_submission_time=40849.8, global_step=104595, preemption_count=0, score=40849.8, test/accuracy=0.5439, test/loss=2.06782, test/num_examples=10000, total_duration=43802.2, train/accuracy=0.728595, train/loss=1.0345, validation/accuracy=0.66724, validation/loss=1.35486, validation/num_examples=50000
I0307 13:35:48.584611 140071458023168 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.37912130355835, loss=1.5552204847335815
I0307 13:36:27.814009 140071466415872 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.2825093269348145, loss=1.539474606513977
I0307 13:37:07.060954 140071458023168 logging_writer.py:48] [104800] global_step=104800, grad_norm=5.049644470214844, loss=1.6006536483764648
I0307 13:37:46.110509 140071466415872 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.452179908752441, loss=1.6045596599578857
I0307 13:38:25.474532 140071458023168 logging_writer.py:48] [105000] global_step=105000, grad_norm=5.030517101287842, loss=1.699020504951477
2025-03-07 13:38:52.203830: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:39:04.712158 140071466415872 logging_writer.py:48] [105100] global_step=105100, grad_norm=5.176426410675049, loss=1.609955906867981
I0307 13:39:43.774143 140071458023168 logging_writer.py:48] [105200] global_step=105200, grad_norm=5.291681289672852, loss=1.5341744422912598
I0307 13:40:22.514459 140071466415872 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.1390767097473145, loss=1.5831373929977417
I0307 13:41:01.203138 140071458023168 logging_writer.py:48] [105400] global_step=105400, grad_norm=5.195245742797852, loss=1.6259969472885132
I0307 13:41:40.194016 140071466415872 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.857956409454346, loss=1.5604796409606934
I0307 13:42:18.984505 140071458023168 logging_writer.py:48] [105600] global_step=105600, grad_norm=3.9635846614837646, loss=1.6320054531097412
I0307 13:42:57.837479 140071466415872 logging_writer.py:48] [105700] global_step=105700, grad_norm=5.042269229888916, loss=1.6269605159759521
I0307 13:43:36.644862 140071458023168 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.980602264404297, loss=1.501539945602417
I0307 13:44:15.814158 140071466415872 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.725558757781982, loss=1.6737086772918701
I0307 13:44:16.250379 140226914178240 spec.py:321] Evaluating on the training split.
I0307 13:44:27.752774 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 13:44:42.276567 140226914178240 spec.py:349] Evaluating on the test split.
I0307 13:44:44.013775 140226914178240 submission_runner.py:469] Time since start: 44340.24s, 	Step: 105902, 	{'train/accuracy': 0.7267617583274841, 'train/loss': 1.0453345775604248, 'validation/accuracy': 0.663100004196167, 'validation/loss': 1.3641449213027954, 'validation/num_examples': 50000, 'test/accuracy': 0.5380000472068787, 'test/loss': 2.0845773220062256, 'test/num_examples': 10000, 'score': 41359.74016737938, 'total_duration': 44340.24461388588, 'accumulated_submission_time': 41359.74016737938, 'accumulated_eval_time': 2958.0964431762695, 'accumulated_logging_time': 11.720539093017578}
I0307 13:44:44.182702 140071458023168 logging_writer.py:48] [105902] accumulated_eval_time=2958.1, accumulated_logging_time=11.7205, accumulated_submission_time=41359.7, global_step=105902, preemption_count=0, score=41359.7, test/accuracy=0.538, test/loss=2.08458, test/num_examples=10000, total_duration=44340.2, train/accuracy=0.726762, train/loss=1.04533, validation/accuracy=0.6631, validation/loss=1.36414, validation/num_examples=50000
I0307 13:45:22.790838 140071466415872 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.3002190589904785, loss=1.6147106885910034
I0307 13:46:01.567542 140071458023168 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.790030479431152, loss=1.5724560022354126
I0307 13:46:40.666276 140071466415872 logging_writer.py:48] [106200] global_step=106200, grad_norm=5.2093305587768555, loss=1.5355257987976074
I0307 13:47:19.739645 140071458023168 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.854689598083496, loss=1.5794155597686768
2025-03-07 13:47:27.527035: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:47:58.635449 140071466415872 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.239120006561279, loss=1.552409052848816
I0307 13:48:37.493144 140071458023168 logging_writer.py:48] [106500] global_step=106500, grad_norm=5.688337326049805, loss=1.6363098621368408
I0307 13:49:16.188066 140071466415872 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.789192199707031, loss=1.5295016765594482
I0307 13:49:55.162921 140071458023168 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.632233142852783, loss=1.503572702407837
I0307 13:50:34.352666 140071466415872 logging_writer.py:48] [106800] global_step=106800, grad_norm=6.534543991088867, loss=1.5039498805999756
I0307 13:51:13.417172 140071458023168 logging_writer.py:48] [106900] global_step=106900, grad_norm=5.610132217407227, loss=1.6584999561309814
I0307 13:51:52.319626 140071466415872 logging_writer.py:48] [107000] global_step=107000, grad_norm=4.204514980316162, loss=1.5371339321136475
I0307 13:52:31.505353 140071458023168 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.5932464599609375, loss=1.5956720113754272
I0307 13:53:10.533479 140071466415872 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.555364608764648, loss=1.6908152103424072
I0307 13:53:14.059022 140226914178240 spec.py:321] Evaluating on the training split.
I0307 13:53:25.710842 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 13:53:43.856591 140226914178240 spec.py:349] Evaluating on the test split.
I0307 13:53:45.578275 140226914178240 submission_runner.py:469] Time since start: 44881.81s, 	Step: 107210, 	{'train/accuracy': 0.7234534025192261, 'train/loss': 1.0698288679122925, 'validation/accuracy': 0.6615999937057495, 'validation/loss': 1.3824890851974487, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.093621253967285, 'test/num_examples': 10000, 'score': 41869.41143536568, 'total_duration': 44881.80910515785, 'accumulated_submission_time': 41869.41143536568, 'accumulated_eval_time': 2989.6155257225037, 'accumulated_logging_time': 11.95208215713501}
I0307 13:53:45.679385 140071458023168 logging_writer.py:48] [107210] accumulated_eval_time=2989.62, accumulated_logging_time=11.9521, accumulated_submission_time=41869.4, global_step=107210, preemption_count=0, score=41869.4, test/accuracy=0.5299, test/loss=2.09362, test/num_examples=10000, total_duration=44881.8, train/accuracy=0.723453, train/loss=1.06983, validation/accuracy=0.6616, validation/loss=1.38249, validation/num_examples=50000
I0307 13:54:21.111013 140071466415872 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.7432451248168945, loss=1.6138445138931274
I0307 13:55:00.172888 140071458023168 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.473589897155762, loss=1.6102912425994873
I0307 13:55:39.321091 140071466415872 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.440771579742432, loss=1.656406283378601
2025-03-07 13:56:07.653597: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:56:18.562287 140071458023168 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.558422565460205, loss=1.5956876277923584
I0307 13:56:57.476695 140071466415872 logging_writer.py:48] [107700] global_step=107700, grad_norm=5.84916353225708, loss=1.5773087739944458
I0307 13:57:36.634822 140071458023168 logging_writer.py:48] [107800] global_step=107800, grad_norm=5.3347272872924805, loss=1.529513955116272
I0307 13:58:15.587869 140071466415872 logging_writer.py:48] [107900] global_step=107900, grad_norm=5.708069324493408, loss=1.6216048002243042
I0307 13:58:54.155501 140071458023168 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.896059513092041, loss=1.575071930885315
I0307 13:59:33.088923 140071466415872 logging_writer.py:48] [108100] global_step=108100, grad_norm=5.047513008117676, loss=1.5937879085540771
I0307 14:00:11.862394 140071458023168 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.5030317306518555, loss=1.5115469694137573
I0307 14:00:50.899502 140071466415872 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.319522380828857, loss=1.5066522359848022
I0307 14:01:29.662272 140071458023168 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.847779273986816, loss=1.5937187671661377
I0307 14:02:08.875765 140071466415872 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.993195533752441, loss=1.6042766571044922
I0307 14:02:15.848552 140226914178240 spec.py:321] Evaluating on the training split.
I0307 14:02:27.791411 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 14:02:42.788881 140226914178240 spec.py:349] Evaluating on the test split.
I0307 14:02:44.528453 140226914178240 submission_runner.py:469] Time since start: 45420.76s, 	Step: 108519, 	{'train/accuracy': 0.7308075428009033, 'train/loss': 1.0359797477722168, 'validation/accuracy': 0.6660400032997131, 'validation/loss': 1.3564285039901733, 'validation/num_examples': 50000, 'test/accuracy': 0.5342000126838684, 'test/loss': 2.0638396739959717, 'test/num_examples': 10000, 'score': 42379.39675307274, 'total_duration': 45420.75930452347, 'accumulated_submission_time': 42379.39675307274, 'accumulated_eval_time': 3018.29527759552, 'accumulated_logging_time': 12.095097303390503}
I0307 14:02:44.658843 140071458023168 logging_writer.py:48] [108519] accumulated_eval_time=3018.3, accumulated_logging_time=12.0951, accumulated_submission_time=42379.4, global_step=108519, preemption_count=0, score=42379.4, test/accuracy=0.5342, test/loss=2.06384, test/num_examples=10000, total_duration=45420.8, train/accuracy=0.730808, train/loss=1.03598, validation/accuracy=0.66604, validation/loss=1.35643, validation/num_examples=50000
I0307 14:03:16.800209 140071466415872 logging_writer.py:48] [108600] global_step=108600, grad_norm=5.257679462432861, loss=1.6068191528320312
I0307 14:03:55.429991 140071458023168 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.828425884246826, loss=1.619832158088684
I0307 14:04:34.222747 140071466415872 logging_writer.py:48] [108800] global_step=108800, grad_norm=5.227898120880127, loss=1.563930630683899
2025-03-07 14:04:43.072989: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:05:13.201666 140071458023168 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.772632122039795, loss=1.494299054145813
I0307 14:05:52.081718 140071466415872 logging_writer.py:48] [109000] global_step=109000, grad_norm=5.164729118347168, loss=1.4417556524276733
I0307 14:06:30.724508 140071458023168 logging_writer.py:48] [109100] global_step=109100, grad_norm=4.701422214508057, loss=1.6400198936462402
I0307 14:07:09.363715 140071466415872 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.847278118133545, loss=1.5708754062652588
I0307 14:07:48.410704 140071458023168 logging_writer.py:48] [109300] global_step=109300, grad_norm=5.2670087814331055, loss=1.5142358541488647
I0307 14:08:27.326959 140071466415872 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.627137184143066, loss=1.5318536758422852
I0307 14:09:06.273510 140071458023168 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.875878810882568, loss=1.5644598007202148
I0307 14:09:45.081219 140071466415872 logging_writer.py:48] [109600] global_step=109600, grad_norm=5.213264465332031, loss=1.5837841033935547
I0307 14:10:24.070593 140071458023168 logging_writer.py:48] [109700] global_step=109700, grad_norm=5.804076671600342, loss=1.517221450805664
I0307 14:11:03.113668 140071466415872 logging_writer.py:48] [109800] global_step=109800, grad_norm=5.165113925933838, loss=1.537878394126892
I0307 14:11:14.599096 140226914178240 spec.py:321] Evaluating on the training split.
I0307 14:11:26.382652 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 14:11:42.324133 140226914178240 spec.py:349] Evaluating on the test split.
I0307 14:11:44.075052 140226914178240 submission_runner.py:469] Time since start: 45960.31s, 	Step: 109830, 	{'train/accuracy': 0.7222974896430969, 'train/loss': 1.0619254112243652, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.3880425691604614, 'validation/num_examples': 50000, 'test/accuracy': 0.5437999963760376, 'test/loss': 2.074864149093628, 'test/num_examples': 10000, 'score': 42889.14323568344, 'total_duration': 45960.305901527405, 'accumulated_submission_time': 42889.14323568344, 'accumulated_eval_time': 3047.771082639694, 'accumulated_logging_time': 12.279263019561768}
I0307 14:11:44.224425 140071458023168 logging_writer.py:48] [109830] accumulated_eval_time=3047.77, accumulated_logging_time=12.2793, accumulated_submission_time=42889.1, global_step=109830, preemption_count=0, score=42889.1, test/accuracy=0.5438, test/loss=2.07486, test/num_examples=10000, total_duration=45960.3, train/accuracy=0.722297, train/loss=1.06193, validation/accuracy=0.6581, validation/loss=1.38804, validation/num_examples=50000
I0307 14:12:11.880942 140071466415872 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.576208114624023, loss=1.5861220359802246
I0307 14:12:50.691858 140071458023168 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.509695053100586, loss=1.474919319152832
2025-03-07 14:13:19.139629: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:13:29.550335 140071466415872 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.443297386169434, loss=1.5571671724319458
I0307 14:14:08.674750 140071458023168 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.511031627655029, loss=1.586362361907959
I0307 14:14:47.495177 140071466415872 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.603697776794434, loss=1.5029165744781494
I0307 14:15:26.408669 140071458023168 logging_writer.py:48] [110400] global_step=110400, grad_norm=5.249452114105225, loss=1.4805874824523926
I0307 14:16:05.425751 140071466415872 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.712234973907471, loss=1.6315255165100098
I0307 14:16:44.532527 140071458023168 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.322122573852539, loss=1.4336378574371338
I0307 14:17:23.353808 140071466415872 logging_writer.py:48] [110700] global_step=110700, grad_norm=5.216320037841797, loss=1.6610982418060303
I0307 14:18:01.986806 140071458023168 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.316851615905762, loss=1.539707899093628
I0307 14:18:41.018321 140071466415872 logging_writer.py:48] [110900] global_step=110900, grad_norm=5.118832111358643, loss=1.5475997924804688
I0307 14:19:20.015620 140071458023168 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.561850547790527, loss=1.573317050933838
I0307 14:19:59.054742 140071466415872 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.844839096069336, loss=1.5849446058273315
I0307 14:20:14.338469 140226914178240 spec.py:321] Evaluating on the training split.
I0307 14:20:26.155314 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 14:20:44.317270 140226914178240 spec.py:349] Evaluating on the test split.
I0307 14:20:46.069940 140226914178240 submission_runner.py:469] Time since start: 46502.30s, 	Step: 111140, 	{'train/accuracy': 0.7431440949440002, 'train/loss': 0.9796679615974426, 'validation/accuracy': 0.6749399900436401, 'validation/loss': 1.3159313201904297, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 1.9984906911849976, 'test/num_examples': 10000, 'score': 43399.08817052841, 'total_duration': 46502.30081295967, 'accumulated_submission_time': 43399.08817052841, 'accumulated_eval_time': 3079.502429962158, 'accumulated_logging_time': 12.455272674560547}
I0307 14:20:46.173099 140071458023168 logging_writer.py:48] [111140] accumulated_eval_time=3079.5, accumulated_logging_time=12.4553, accumulated_submission_time=43399.1, global_step=111140, preemption_count=0, score=43399.1, test/accuracy=0.5506, test/loss=1.99849, test/num_examples=10000, total_duration=46502.3, train/accuracy=0.743144, train/loss=0.979668, validation/accuracy=0.67494, validation/loss=1.31593, validation/num_examples=50000
I0307 14:21:09.826941 140071466415872 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.48126220703125, loss=1.487571120262146
I0307 14:21:49.055163 140071458023168 logging_writer.py:48] [111300] global_step=111300, grad_norm=5.148958683013916, loss=1.5727378129959106
2025-03-07 14:21:58.622160: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:22:27.848696 140071466415872 logging_writer.py:48] [111400] global_step=111400, grad_norm=5.348486423492432, loss=1.4976614713668823
I0307 14:23:06.973618 140071458023168 logging_writer.py:48] [111500] global_step=111500, grad_norm=5.05941104888916, loss=1.4756662845611572
I0307 14:23:45.637502 140071466415872 logging_writer.py:48] [111600] global_step=111600, grad_norm=5.813132286071777, loss=1.6571427583694458
I0307 14:24:24.611076 140071458023168 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.760791301727295, loss=1.4796348810195923
I0307 14:25:03.481926 140071466415872 logging_writer.py:48] [111800] global_step=111800, grad_norm=5.316912651062012, loss=1.5809969902038574
I0307 14:25:42.376788 140071458023168 logging_writer.py:48] [111900] global_step=111900, grad_norm=5.526392459869385, loss=1.5568287372589111
I0307 14:26:20.970976 140071466415872 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.701426029205322, loss=1.6886820793151855
I0307 14:26:59.721374 140071458023168 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.9042277336120605, loss=1.4310896396636963
I0307 14:27:38.329801 140071466415872 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.836296081542969, loss=1.5862066745758057
I0307 14:28:17.347419 140071458023168 logging_writer.py:48] [112300] global_step=112300, grad_norm=5.414776802062988, loss=1.5798650979995728
I0307 14:28:56.141474 140071466415872 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.686082363128662, loss=1.5975676774978638
I0307 14:29:16.111219 140226914178240 spec.py:321] Evaluating on the training split.
I0307 14:29:28.010163 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 14:29:44.571845 140226914178240 spec.py:349] Evaluating on the test split.
I0307 14:29:46.323448 140226914178240 submission_runner.py:469] Time since start: 47042.55s, 	Step: 112453, 	{'train/accuracy': 0.7401546239852905, 'train/loss': 0.9847238063812256, 'validation/accuracy': 0.6710799932479858, 'validation/loss': 1.3278801441192627, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.031024217605591, 'test/num_examples': 10000, 'score': 43908.86542510986, 'total_duration': 47042.55430698395, 'accumulated_submission_time': 43908.86542510986, 'accumulated_eval_time': 3109.714515686035, 'accumulated_logging_time': 12.5783212184906}
I0307 14:29:46.444232 140071458023168 logging_writer.py:48] [112453] accumulated_eval_time=3109.71, accumulated_logging_time=12.5783, accumulated_submission_time=43908.9, global_step=112453, preemption_count=0, score=43908.9, test/accuracy=0.5469, test/loss=2.03102, test/num_examples=10000, total_duration=47042.6, train/accuracy=0.740155, train/loss=0.984724, validation/accuracy=0.67108, validation/loss=1.32788, validation/num_examples=50000
I0307 14:30:04.934798 140071466415872 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.524326801300049, loss=1.4742441177368164
2025-03-07 14:30:34.157285: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:30:43.575645 140071458023168 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.758650302886963, loss=1.4896950721740723
I0307 14:31:22.399201 140071466415872 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.830285549163818, loss=1.4835584163665771
I0307 14:32:01.014309 140071458023168 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.723694324493408, loss=1.5726852416992188
I0307 14:32:39.828902 140071466415872 logging_writer.py:48] [112900] global_step=112900, grad_norm=5.389479637145996, loss=1.543267011642456
I0307 14:33:18.454974 140071458023168 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.51914119720459, loss=1.5884521007537842
I0307 14:33:56.981698 140071466415872 logging_writer.py:48] [113100] global_step=113100, grad_norm=5.358192443847656, loss=1.636221170425415
I0307 14:34:35.749457 140071458023168 logging_writer.py:48] [113200] global_step=113200, grad_norm=6.160808086395264, loss=1.6169943809509277
I0307 14:35:14.609213 140071466415872 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.77089786529541, loss=1.5042061805725098
I0307 14:35:53.462401 140071458023168 logging_writer.py:48] [113400] global_step=113400, grad_norm=5.453503131866455, loss=1.5568645000457764
I0307 14:36:32.416461 140071466415872 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.890696048736572, loss=1.5590227842330933
I0307 14:37:11.551350 140071458023168 logging_writer.py:48] [113600] global_step=113600, grad_norm=5.338653087615967, loss=1.5670949220657349
I0307 14:37:50.484297 140071466415872 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.527628421783447, loss=1.4788234233856201
I0307 14:38:16.352109 140226914178240 spec.py:321] Evaluating on the training split.
I0307 14:38:28.193844 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 14:38:44.409552 140226914178240 spec.py:349] Evaluating on the test split.
I0307 14:38:46.153886 140226914178240 submission_runner.py:469] Time since start: 47582.38s, 	Step: 113768, 	{'train/accuracy': 0.7409319281578064, 'train/loss': 0.990680992603302, 'validation/accuracy': 0.6717000007629395, 'validation/loss': 1.3290770053863525, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.061917304992676, 'test/num_examples': 10000, 'score': 44418.603545188904, 'total_duration': 47582.38474416733, 'accumulated_submission_time': 44418.603545188904, 'accumulated_eval_time': 3139.516149520874, 'accumulated_logging_time': 12.728459596633911}
I0307 14:38:46.267325 140071458023168 logging_writer.py:48] [113768] accumulated_eval_time=3139.52, accumulated_logging_time=12.7285, accumulated_submission_time=44418.6, global_step=113768, preemption_count=0, score=44418.6, test/accuracy=0.5364, test/loss=2.06192, test/num_examples=10000, total_duration=47582.4, train/accuracy=0.740932, train/loss=0.990681, validation/accuracy=0.6717, validation/loss=1.32908, validation/num_examples=50000
I0307 14:38:58.970459 140071466415872 logging_writer.py:48] [113800] global_step=113800, grad_norm=4.90947961807251, loss=1.6141235828399658
2025-03-07 14:39:09.275887: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:39:38.007604 140071458023168 logging_writer.py:48] [113900] global_step=113900, grad_norm=5.060672760009766, loss=1.520950198173523
I0307 14:40:16.744146 140071466415872 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.6427998542785645, loss=1.509412407875061
I0307 14:40:55.742912 140071458023168 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.7307329177856445, loss=1.4919040203094482
I0307 14:41:34.513734 140071466415872 logging_writer.py:48] [114200] global_step=114200, grad_norm=5.254693508148193, loss=1.5331685543060303
I0307 14:42:13.278783 140071458023168 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.8872151374816895, loss=1.4787176847457886
I0307 14:42:52.026052 140071466415872 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.974700450897217, loss=1.494423508644104
I0307 14:43:30.613356 140071458023168 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.642997741699219, loss=1.5476642847061157
I0307 14:44:09.547450 140071466415872 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.647910118103027, loss=1.4940859079360962
I0307 14:44:48.325613 140071458023168 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.8218817710876465, loss=1.5307753086090088
I0307 14:45:27.226962 140071466415872 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.574116230010986, loss=1.3982455730438232
I0307 14:46:06.392009 140071458023168 logging_writer.py:48] [114900] global_step=114900, grad_norm=5.06870174407959, loss=1.5769227743148804
I0307 14:46:45.001264 140071466415872 logging_writer.py:48] [115000] global_step=115000, grad_norm=6.768626689910889, loss=1.6795051097869873
2025-03-07 14:47:15.346381: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:47:16.155632 140226914178240 spec.py:321] Evaluating on the training split.
I0307 14:47:28.208584 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 14:47:48.070829 140226914178240 spec.py:349] Evaluating on the test split.
I0307 14:47:49.805532 140226914178240 submission_runner.py:469] Time since start: 48126.04s, 	Step: 115081, 	{'train/accuracy': 0.7419283986091614, 'train/loss': 0.9826892614364624, 'validation/accuracy': 0.6773599982261658, 'validation/loss': 1.3044812679290771, 'validation/num_examples': 50000, 'test/accuracy': 0.5485000014305115, 'test/loss': 2.0043253898620605, 'test/num_examples': 10000, 'score': 44928.30272936821, 'total_duration': 48126.03640413284, 'accumulated_submission_time': 44928.30272936821, 'accumulated_eval_time': 3173.1659231185913, 'accumulated_logging_time': 12.888441801071167}
I0307 14:47:49.922012 140071458023168 logging_writer.py:48] [115081] accumulated_eval_time=3173.17, accumulated_logging_time=12.8884, accumulated_submission_time=44928.3, global_step=115081, preemption_count=0, score=44928.3, test/accuracy=0.5485, test/loss=2.00433, test/num_examples=10000, total_duration=48126, train/accuracy=0.741928, train/loss=0.982689, validation/accuracy=0.67736, validation/loss=1.30448, validation/num_examples=50000
I0307 14:47:57.630371 140071466415872 logging_writer.py:48] [115100] global_step=115100, grad_norm=5.3372039794921875, loss=1.5722088813781738
I0307 14:48:36.164108 140071458023168 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.900106906890869, loss=1.5581560134887695
I0307 14:49:15.042241 140071466415872 logging_writer.py:48] [115300] global_step=115300, grad_norm=5.70323371887207, loss=1.6168373823165894
I0307 14:49:53.885822 140071458023168 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.854547023773193, loss=1.5934456586837769
I0307 14:50:32.590702 140071466415872 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.924496173858643, loss=1.558628797531128
I0307 14:51:11.421775 140071458023168 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.917543888092041, loss=1.4399477243423462
I0307 14:51:50.026440 140071466415872 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.866133213043213, loss=1.5115954875946045
I0307 14:52:28.813045 140071458023168 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.860621452331543, loss=1.479682445526123
I0307 14:53:07.535342 140071466415872 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.917286396026611, loss=1.5675005912780762
I0307 14:53:46.758359 140071458023168 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.9035964012146, loss=1.5704454183578491
I0307 14:54:26.064841 140071466415872 logging_writer.py:48] [116100] global_step=116100, grad_norm=5.3857421875, loss=1.542477011680603
I0307 14:55:04.839502 140071458023168 logging_writer.py:48] [116200] global_step=116200, grad_norm=4.739885330200195, loss=1.464519739151001
I0307 14:55:43.869478 140071466415872 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.724426746368408, loss=1.4159071445465088
2025-03-07 14:55:55.179412: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:56:19.905797 140226914178240 spec.py:321] Evaluating on the training split.
I0307 14:56:31.368371 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 14:56:48.501788 140226914178240 spec.py:349] Evaluating on the test split.
I0307 14:56:50.234333 140226914178240 submission_runner.py:469] Time since start: 48666.47s, 	Step: 116394, 	{'train/accuracy': 0.7452168464660645, 'train/loss': 0.9689076542854309, 'validation/accuracy': 0.674019992351532, 'validation/loss': 1.3074077367782593, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 1.9893537759780884, 'test/num_examples': 10000, 'score': 45438.1211874485, 'total_duration': 48666.465164899826, 'accumulated_submission_time': 45438.1211874485, 'accumulated_eval_time': 3203.4942905902863, 'accumulated_logging_time': 13.026046514511108}
I0307 14:56:50.336929 140071458023168 logging_writer.py:48] [116394] accumulated_eval_time=3203.49, accumulated_logging_time=13.026, accumulated_submission_time=45438.1, global_step=116394, preemption_count=0, score=45438.1, test/accuracy=0.5471, test/loss=1.98935, test/num_examples=10000, total_duration=48666.5, train/accuracy=0.745217, train/loss=0.968908, validation/accuracy=0.67402, validation/loss=1.30741, validation/num_examples=50000
I0307 14:56:53.225777 140071466415872 logging_writer.py:48] [116400] global_step=116400, grad_norm=6.107244968414307, loss=1.486498236656189
I0307 14:57:31.842561 140071458023168 logging_writer.py:48] [116500] global_step=116500, grad_norm=5.083393573760986, loss=1.4418121576309204
I0307 14:58:10.466680 140071466415872 logging_writer.py:48] [116600] global_step=116600, grad_norm=5.643428325653076, loss=1.4937129020690918
I0307 14:58:49.517035 140071458023168 logging_writer.py:48] [116700] global_step=116700, grad_norm=5.464901924133301, loss=1.4837868213653564
I0307 14:59:28.262037 140071466415872 logging_writer.py:48] [116800] global_step=116800, grad_norm=5.6919660568237305, loss=1.6369950771331787
I0307 15:00:06.631461 140071458023168 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.59226131439209, loss=1.4781708717346191
I0307 15:00:45.485788 140071466415872 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.753519535064697, loss=1.4899876117706299
I0307 15:01:24.308035 140071458023168 logging_writer.py:48] [117100] global_step=117100, grad_norm=4.993356704711914, loss=1.4304378032684326
I0307 15:02:02.891946 140071466415872 logging_writer.py:48] [117200] global_step=117200, grad_norm=5.05340051651001, loss=1.6210170984268188
I0307 15:02:41.538669 140071458023168 logging_writer.py:48] [117300] global_step=117300, grad_norm=5.317124366760254, loss=1.567499041557312
I0307 15:03:20.145441 140071466415872 logging_writer.py:48] [117400] global_step=117400, grad_norm=5.417326927185059, loss=1.5642966032028198
I0307 15:03:58.918412 140071458023168 logging_writer.py:48] [117500] global_step=117500, grad_norm=5.155149459838867, loss=1.5933929681777954
2025-03-07 15:04:30.030279: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:04:37.868684 140071466415872 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.797536849975586, loss=1.5376684665679932
I0307 15:05:16.730322 140071458023168 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.662204265594482, loss=1.4020277261734009
I0307 15:05:20.240118 140226914178240 spec.py:321] Evaluating on the training split.
I0307 15:05:32.130445 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 15:05:49.134645 140226914178240 spec.py:349] Evaluating on the test split.
I0307 15:05:50.869566 140226914178240 submission_runner.py:469] Time since start: 49207.10s, 	Step: 117710, 	{'train/accuracy': 0.750996470451355, 'train/loss': 0.944252073764801, 'validation/accuracy': 0.6810599565505981, 'validation/loss': 1.2941615581512451, 'validation/num_examples': 50000, 'test/accuracy': 0.5493000149726868, 'test/loss': 2.010335922241211, 'test/num_examples': 10000, 'score': 45947.84820652008, 'total_duration': 49207.10041928291, 'accumulated_submission_time': 45947.84820652008, 'accumulated_eval_time': 3234.123591184616, 'accumulated_logging_time': 13.164076089859009}
I0307 15:05:50.963805 140071466415872 logging_writer.py:48] [117710] accumulated_eval_time=3234.12, accumulated_logging_time=13.1641, accumulated_submission_time=45947.8, global_step=117710, preemption_count=0, score=45947.8, test/accuracy=0.5493, test/loss=2.01034, test/num_examples=10000, total_duration=49207.1, train/accuracy=0.750996, train/loss=0.944252, validation/accuracy=0.68106, validation/loss=1.29416, validation/num_examples=50000
I0307 15:06:26.606975 140071458023168 logging_writer.py:48] [117800] global_step=117800, grad_norm=5.302807331085205, loss=1.5903253555297852
I0307 15:07:05.270944 140071466415872 logging_writer.py:48] [117900] global_step=117900, grad_norm=5.135970115661621, loss=1.4518909454345703
I0307 15:07:44.182506 140071458023168 logging_writer.py:48] [118000] global_step=118000, grad_norm=5.857701778411865, loss=1.5396703481674194
I0307 15:08:23.163622 140071466415872 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.995333671569824, loss=1.4591047763824463
I0307 15:09:02.105328 140071458023168 logging_writer.py:48] [118200] global_step=118200, grad_norm=5.374842643737793, loss=1.563646674156189
I0307 15:09:40.965315 140071466415872 logging_writer.py:48] [118300] global_step=118300, grad_norm=5.344849109649658, loss=1.575225830078125
I0307 15:10:19.714622 140071458023168 logging_writer.py:48] [118400] global_step=118400, grad_norm=5.0213823318481445, loss=1.4364014863967896
I0307 15:10:58.918356 140071466415872 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.999355316162109, loss=1.4843560457229614
I0307 15:11:38.072575 140071458023168 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.734622478485107, loss=1.4740577936172485
I0307 15:12:17.094027 140071466415872 logging_writer.py:48] [118700] global_step=118700, grad_norm=5.167603969573975, loss=1.4787263870239258
I0307 15:12:56.258038 140071458023168 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.743614673614502, loss=1.4155199527740479
2025-03-07 15:13:08.388098: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:13:35.151876 140071466415872 logging_writer.py:48] [118900] global_step=118900, grad_norm=5.229869842529297, loss=1.4470181465148926
I0307 15:14:14.250612 140071458023168 logging_writer.py:48] [119000] global_step=119000, grad_norm=5.074329853057861, loss=1.5201547145843506
I0307 15:14:21.017549 140226914178240 spec.py:321] Evaluating on the training split.
I0307 15:14:32.969949 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 15:14:51.077229 140226914178240 spec.py:349] Evaluating on the test split.
I0307 15:14:52.828904 140226914178240 submission_runner.py:469] Time since start: 49749.06s, 	Step: 119018, 	{'train/accuracy': 0.7523118257522583, 'train/loss': 0.9412111639976501, 'validation/accuracy': 0.6809399724006653, 'validation/loss': 1.2859958410263062, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 1.975270390510559, 'test/num_examples': 10000, 'score': 46457.72874855995, 'total_duration': 49749.0597383976, 'accumulated_submission_time': 46457.72874855995, 'accumulated_eval_time': 3265.9347772598267, 'accumulated_logging_time': 13.28991150856018}
I0307 15:14:52.924305 140071466415872 logging_writer.py:48] [119018] accumulated_eval_time=3265.93, accumulated_logging_time=13.2899, accumulated_submission_time=46457.7, global_step=119018, preemption_count=0, score=46457.7, test/accuracy=0.5528, test/loss=1.97527, test/num_examples=10000, total_duration=49749.1, train/accuracy=0.752312, train/loss=0.941211, validation/accuracy=0.68094, validation/loss=1.286, validation/num_examples=50000
I0307 15:15:24.995431 140071458023168 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.345578193664551, loss=1.570127010345459
I0307 15:16:03.824259 140071466415872 logging_writer.py:48] [119200] global_step=119200, grad_norm=5.043843746185303, loss=1.418682336807251
I0307 15:16:42.436403 140071458023168 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.504250526428223, loss=1.5488137006759644
I0307 15:17:21.175876 140071466415872 logging_writer.py:48] [119400] global_step=119400, grad_norm=5.817014217376709, loss=1.543054223060608
I0307 15:17:59.648733 140071458023168 logging_writer.py:48] [119500] global_step=119500, grad_norm=5.2695393562316895, loss=1.3291735649108887
I0307 15:18:38.478703 140071466415872 logging_writer.py:48] [119600] global_step=119600, grad_norm=5.052521705627441, loss=1.3552013635635376
I0307 15:19:17.123727 140071458023168 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.484394073486328, loss=1.4438352584838867
I0307 15:19:55.987065 140071466415872 logging_writer.py:48] [119800] global_step=119800, grad_norm=6.187507152557373, loss=1.633367657661438
I0307 15:20:34.717639 140071458023168 logging_writer.py:48] [119900] global_step=119900, grad_norm=5.623954772949219, loss=1.5326898097991943
I0307 15:21:13.374773 140071466415872 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.491977691650391, loss=1.4080191850662231
2025-03-07 15:21:45.340142: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:21:52.167064 140071458023168 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.9860029220581055, loss=1.4162962436676025
I0307 15:22:30.823791 140071466415872 logging_writer.py:48] [120200] global_step=120200, grad_norm=5.586842060089111, loss=1.591399073600769
I0307 15:23:09.783241 140071458023168 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.678093910217285, loss=1.5158650875091553
I0307 15:23:22.899871 140226914178240 spec.py:321] Evaluating on the training split.
I0307 15:23:34.638382 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 15:23:50.941136 140226914178240 spec.py:349] Evaluating on the test split.
I0307 15:23:52.661901 140226914178240 submission_runner.py:469] Time since start: 50288.89s, 	Step: 120335, 	{'train/accuracy': 0.7511360049247742, 'train/loss': 0.9495161175727844, 'validation/accuracy': 0.6824199557304382, 'validation/loss': 1.2806087732315063, 'validation/num_examples': 50000, 'test/accuracy': 0.5596000552177429, 'test/loss': 1.9817901849746704, 'test/num_examples': 10000, 'score': 46967.50566124916, 'total_duration': 50288.89274406433, 'accumulated_submission_time': 46967.50566124916, 'accumulated_eval_time': 3295.6966631412506, 'accumulated_logging_time': 13.438750267028809}
I0307 15:23:52.822207 140071466415872 logging_writer.py:48] [120335] accumulated_eval_time=3295.7, accumulated_logging_time=13.4388, accumulated_submission_time=46967.5, global_step=120335, preemption_count=0, score=46967.5, test/accuracy=0.5596, test/loss=1.98179, test/num_examples=10000, total_duration=50288.9, train/accuracy=0.751136, train/loss=0.949516, validation/accuracy=0.68242, validation/loss=1.28061, validation/num_examples=50000
I0307 15:24:18.106970 140071458023168 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.220903396606445, loss=1.4867548942565918
I0307 15:24:56.665799 140071466415872 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.052524566650391, loss=1.3613694906234741
I0307 15:25:35.587425 140071458023168 logging_writer.py:48] [120600] global_step=120600, grad_norm=5.137267112731934, loss=1.5240490436553955
I0307 15:26:14.578048 140071466415872 logging_writer.py:48] [120700] global_step=120700, grad_norm=5.538069248199463, loss=1.6019123792648315
I0307 15:26:53.194627 140071458023168 logging_writer.py:48] [120800] global_step=120800, grad_norm=5.553351402282715, loss=1.508391261100769
I0307 15:27:31.649829 140071466415872 logging_writer.py:48] [120900] global_step=120900, grad_norm=5.296996593475342, loss=1.4962636232376099
I0307 15:28:10.236687 140071458023168 logging_writer.py:48] [121000] global_step=121000, grad_norm=5.383453845977783, loss=1.4707218408584595
I0307 15:28:49.533528 140071466415872 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.1064324378967285, loss=1.3878461122512817
I0307 15:29:28.513115 140071458023168 logging_writer.py:48] [121200] global_step=121200, grad_norm=5.1395063400268555, loss=1.4165608882904053
I0307 15:30:07.249364 140071466415872 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.131171703338623, loss=1.3859316110610962
2025-03-07 15:30:20.283320: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:30:46.186868 140071458023168 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.133583068847656, loss=1.482087254524231
I0307 15:31:24.976637 140071466415872 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.145909309387207, loss=1.5620068311691284
I0307 15:32:03.527214 140071458023168 logging_writer.py:48] [121600] global_step=121600, grad_norm=5.0909423828125, loss=1.3842662572860718
I0307 15:32:22.976216 140226914178240 spec.py:321] Evaluating on the training split.
I0307 15:32:35.028563 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 15:32:52.204441 140226914178240 spec.py:349] Evaluating on the test split.
I0307 15:32:53.957249 140226914178240 submission_runner.py:469] Time since start: 50830.19s, 	Step: 121651, 	{'train/accuracy': 0.7564771771430969, 'train/loss': 0.919080913066864, 'validation/accuracy': 0.6828799843788147, 'validation/loss': 1.287508249282837, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 1.9882198572158813, 'test/num_examples': 10000, 'score': 47477.48717856407, 'total_duration': 50830.188037633896, 'accumulated_submission_time': 47477.48717856407, 'accumulated_eval_time': 3326.6774871349335, 'accumulated_logging_time': 13.627768278121948}
I0307 15:32:54.079660 140071466415872 logging_writer.py:48] [121651] accumulated_eval_time=3326.68, accumulated_logging_time=13.6278, accumulated_submission_time=47477.5, global_step=121651, preemption_count=0, score=47477.5, test/accuracy=0.556, test/loss=1.98822, test/num_examples=10000, total_duration=50830.2, train/accuracy=0.756477, train/loss=0.919081, validation/accuracy=0.68288, validation/loss=1.28751, validation/num_examples=50000
I0307 15:33:13.757566 140071458023168 logging_writer.py:48] [121700] global_step=121700, grad_norm=5.399082183837891, loss=1.4564019441604614
I0307 15:33:52.200708 140071466415872 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.075603008270264, loss=1.3751068115234375
I0307 15:34:31.001281 140071458023168 logging_writer.py:48] [121900] global_step=121900, grad_norm=5.1803460121154785, loss=1.5187948942184448
I0307 15:35:09.735904 140071466415872 logging_writer.py:48] [122000] global_step=122000, grad_norm=5.395265102386475, loss=1.4272828102111816
I0307 15:35:48.420897 140071458023168 logging_writer.py:48] [122100] global_step=122100, grad_norm=5.128087043762207, loss=1.3777005672454834
I0307 15:36:27.134325 140071466415872 logging_writer.py:48] [122200] global_step=122200, grad_norm=5.9223504066467285, loss=1.4333524703979492
I0307 15:37:06.438495 140071458023168 logging_writer.py:48] [122300] global_step=122300, grad_norm=5.257593631744385, loss=1.4365673065185547
I0307 15:37:45.582637 140071466415872 logging_writer.py:48] [122400] global_step=122400, grad_norm=5.420478343963623, loss=1.5836083889007568
I0307 15:38:24.185516 140071458023168 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.4933247566223145, loss=1.5323879718780518
2025-03-07 15:38:56.974120: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:39:03.048997 140071466415872 logging_writer.py:48] [122600] global_step=122600, grad_norm=5.173582077026367, loss=1.4541488885879517
I0307 15:39:42.031304 140071458023168 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.154149532318115, loss=1.429450511932373
I0307 15:40:21.017618 140071466415872 logging_writer.py:48] [122800] global_step=122800, grad_norm=6.253576755523682, loss=1.4198346138000488
I0307 15:41:00.192882 140071458023168 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.9492292404174805, loss=1.4789769649505615
I0307 15:41:24.142459 140226914178240 spec.py:321] Evaluating on the training split.
I0307 15:41:35.868982 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 15:41:57.005855 140226914178240 spec.py:349] Evaluating on the test split.
I0307 15:41:58.753126 140226914178240 submission_runner.py:469] Time since start: 51374.98s, 	Step: 122963, 	{'train/accuracy': 0.7643893361091614, 'train/loss': 0.8908267617225647, 'validation/accuracy': 0.6933000087738037, 'validation/loss': 1.2455782890319824, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9361087083816528, 'test/num_examples': 10000, 'score': 47987.363550662994, 'total_duration': 51374.98397350311, 'accumulated_submission_time': 47987.363550662994, 'accumulated_eval_time': 3361.288002729416, 'accumulated_logging_time': 13.791656494140625}
I0307 15:41:58.899766 140071466415872 logging_writer.py:48] [122963] accumulated_eval_time=3361.29, accumulated_logging_time=13.7917, accumulated_submission_time=47987.4, global_step=122963, preemption_count=0, score=47987.4, test/accuracy=0.5661, test/loss=1.93611, test/num_examples=10000, total_duration=51375, train/accuracy=0.764389, train/loss=0.890827, validation/accuracy=0.6933, validation/loss=1.24558, validation/num_examples=50000
I0307 15:42:13.605415 140071458023168 logging_writer.py:48] [123000] global_step=123000, grad_norm=6.09595251083374, loss=1.4880622625350952
I0307 15:42:52.138095 140071466415872 logging_writer.py:48] [123100] global_step=123100, grad_norm=5.178609371185303, loss=1.4620290994644165
I0307 15:43:30.761318 140071458023168 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.484731197357178, loss=1.4331703186035156
I0307 15:44:09.197569 140071466415872 logging_writer.py:48] [123300] global_step=123300, grad_norm=4.498444080352783, loss=1.3538185358047485
I0307 15:44:47.378200 140071458023168 logging_writer.py:48] [123400] global_step=123400, grad_norm=6.164676189422607, loss=1.4670692682266235
I0307 15:45:25.849532 140071466415872 logging_writer.py:48] [123500] global_step=123500, grad_norm=5.250245571136475, loss=1.4247722625732422
I0307 15:46:04.487625 140071458023168 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.822710990905762, loss=1.3913153409957886
I0307 15:46:43.245822 140071466415872 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.707719326019287, loss=1.3594318628311157
I0307 15:47:21.588622 140071458023168 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.580106258392334, loss=1.4057064056396484
2025-03-07 15:47:35.336931: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:48:00.241944 140071466415872 logging_writer.py:48] [123900] global_step=123900, grad_norm=5.542504787445068, loss=1.529768943786621
I0307 15:48:38.832739 140071458023168 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.373050689697266, loss=1.5054023265838623
I0307 15:49:17.651224 140071466415872 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.823602676391602, loss=1.517106056213379
I0307 15:49:55.855375 140071458023168 logging_writer.py:48] [124200] global_step=124200, grad_norm=4.999543190002441, loss=1.4345663785934448
I0307 15:50:29.040113 140226914178240 spec.py:321] Evaluating on the training split.
I0307 15:50:41.151587 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 15:51:00.574914 140226914178240 spec.py:349] Evaluating on the test split.
I0307 15:51:02.351383 140226914178240 submission_runner.py:469] Time since start: 51918.58s, 	Step: 124287, 	{'train/accuracy': 0.7543447017669678, 'train/loss': 0.9276888966560364, 'validation/accuracy': 0.6841999888420105, 'validation/loss': 1.2811634540557861, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 1.9944639205932617, 'test/num_examples': 10000, 'score': 48497.32695841789, 'total_duration': 51918.58218955994, 'accumulated_submission_time': 48497.32695841789, 'accumulated_eval_time': 3394.5990793704987, 'accumulated_logging_time': 13.97123908996582}
I0307 15:51:02.487494 140071466415872 logging_writer.py:48] [124287] accumulated_eval_time=3394.6, accumulated_logging_time=13.9712, accumulated_submission_time=48497.3, global_step=124287, preemption_count=0, score=48497.3, test/accuracy=0.5587, test/loss=1.99446, test/num_examples=10000, total_duration=51918.6, train/accuracy=0.754345, train/loss=0.927689, validation/accuracy=0.6842, validation/loss=1.28116, validation/num_examples=50000
I0307 15:51:07.926858 140071458023168 logging_writer.py:48] [124300] global_step=124300, grad_norm=4.773375034332275, loss=1.4066624641418457
I0307 15:51:46.316877 140071466415872 logging_writer.py:48] [124400] global_step=124400, grad_norm=4.8565521240234375, loss=1.3876975774765015
I0307 15:52:24.949737 140071458023168 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.492825508117676, loss=1.5193835496902466
I0307 15:53:03.433890 140071466415872 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.268041133880615, loss=1.483757734298706
I0307 15:53:41.762571 140071458023168 logging_writer.py:48] [124700] global_step=124700, grad_norm=4.691666603088379, loss=1.4420909881591797
I0307 15:54:20.389314 140071466415872 logging_writer.py:48] [124800] global_step=124800, grad_norm=5.176252365112305, loss=1.3528801202774048
I0307 15:54:59.083808 140071458023168 logging_writer.py:48] [124900] global_step=124900, grad_norm=4.853489398956299, loss=1.4896422624588013
I0307 15:55:37.666103 140071466415872 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.155239582061768, loss=1.5284569263458252
2025-03-07 15:56:11.015893: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:56:15.911371 140071458023168 logging_writer.py:48] [125100] global_step=125100, grad_norm=4.750351428985596, loss=1.363808512687683
I0307 15:56:54.415437 140071466415872 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.749210834503174, loss=1.408171534538269
I0307 15:57:33.060959 140071458023168 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.1637492179870605, loss=1.4087146520614624
I0307 15:58:11.682076 140071466415872 logging_writer.py:48] [125400] global_step=125400, grad_norm=5.373340129852295, loss=1.4089879989624023
I0307 15:58:50.071002 140071458023168 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.490708827972412, loss=1.452983021736145
I0307 15:59:28.280460 140071466415872 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.141448020935059, loss=1.41547691822052
I0307 15:59:32.500929 140226914178240 spec.py:321] Evaluating on the training split.
I0307 15:59:44.422380 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 16:00:06.286006 140226914178240 spec.py:349] Evaluating on the test split.
I0307 16:00:08.076928 140226914178240 submission_runner.py:469] Time since start: 52464.31s, 	Step: 125612, 	{'train/accuracy': 0.7652064561843872, 'train/loss': 0.884160578250885, 'validation/accuracy': 0.6893999576568604, 'validation/loss': 1.2497535943984985, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 1.9739079475402832, 'test/num_examples': 10000, 'score': 49007.16653895378, 'total_duration': 52464.30775976181, 'accumulated_submission_time': 49007.16653895378, 'accumulated_eval_time': 3430.1749250888824, 'accumulated_logging_time': 14.13570523262024}
I0307 16:00:08.160893 140071458023168 logging_writer.py:48] [125612] accumulated_eval_time=3430.17, accumulated_logging_time=14.1357, accumulated_submission_time=49007.2, global_step=125612, preemption_count=0, score=49007.2, test/accuracy=0.5626, test/loss=1.97391, test/num_examples=10000, total_duration=52464.3, train/accuracy=0.765206, train/loss=0.884161, validation/accuracy=0.6894, validation/loss=1.24975, validation/num_examples=50000
I0307 16:00:42.344361 140071466415872 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.643764495849609, loss=1.4704114198684692
I0307 16:01:20.573902 140071458023168 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.271956443786621, loss=1.3828468322753906
I0307 16:01:58.643150 140071466415872 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.580957889556885, loss=1.4819358587265015
I0307 16:02:37.255464 140071458023168 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.5160369873046875, loss=1.4919419288635254
I0307 16:03:15.891793 140071466415872 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.2039361000061035, loss=1.2918617725372314
I0307 16:03:54.436880 140071458023168 logging_writer.py:48] [126200] global_step=126200, grad_norm=5.167732238769531, loss=1.3372204303741455
I0307 16:04:32.955408 140071466415872 logging_writer.py:48] [126300] global_step=126300, grad_norm=5.195487022399902, loss=1.3171252012252808
2025-03-07 16:04:47.131713: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:05:11.349617 140071458023168 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.580733776092529, loss=1.466773271560669
I0307 16:05:49.473652 140071466415872 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.3560566902160645, loss=1.3604636192321777
I0307 16:06:27.918397 140071458023168 logging_writer.py:48] [126600] global_step=126600, grad_norm=6.518961429595947, loss=1.4798415899276733
I0307 16:07:06.312680 140071466415872 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.978164196014404, loss=1.4408972263336182
I0307 16:07:44.742611 140071458023168 logging_writer.py:48] [126800] global_step=126800, grad_norm=5.237369060516357, loss=1.4179595708847046
I0307 16:08:23.061283 140071466415872 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.626214981079102, loss=1.419285774230957
I0307 16:08:38.400685 140226914178240 spec.py:321] Evaluating on the training split.
I0307 16:08:50.752257 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 16:09:11.055495 140226914178240 spec.py:349] Evaluating on the test split.
I0307 16:09:12.823296 140226914178240 submission_runner.py:469] Time since start: 53009.05s, 	Step: 126941, 	{'train/accuracy': 0.7679567933082581, 'train/loss': 0.8757691383361816, 'validation/accuracy': 0.6919599771499634, 'validation/loss': 1.2387648820877075, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 1.920729637145996, 'test/num_examples': 10000, 'score': 49517.210122823715, 'total_duration': 53009.05414772034, 'accumulated_submission_time': 49517.210122823715, 'accumulated_eval_time': 3464.5973839759827, 'accumulated_logging_time': 14.271058797836304}
I0307 16:09:12.885553 140071458023168 logging_writer.py:48] [126941] accumulated_eval_time=3464.6, accumulated_logging_time=14.2711, accumulated_submission_time=49517.2, global_step=126941, preemption_count=0, score=49517.2, test/accuracy=0.5685, test/loss=1.92073, test/num_examples=10000, total_duration=53009.1, train/accuracy=0.767957, train/loss=0.875769, validation/accuracy=0.69196, validation/loss=1.23876, validation/num_examples=50000
I0307 16:09:35.959747 140071466415872 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.754124164581299, loss=1.4837185144424438
I0307 16:10:14.375355 140071458023168 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.163458824157715, loss=1.4262126684188843
I0307 16:10:52.657787 140071466415872 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.213881492614746, loss=1.5352429151535034
I0307 16:11:31.567569 140071458023168 logging_writer.py:48] [127300] global_step=127300, grad_norm=4.993093013763428, loss=1.3974472284317017
I0307 16:12:10.055701 140071466415872 logging_writer.py:48] [127400] global_step=127400, grad_norm=6.005310535430908, loss=1.3420653343200684
I0307 16:12:48.150761 140071458023168 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.427079677581787, loss=1.2577868700027466
2025-03-07 16:13:22.016645: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:13:26.414207 140071466415872 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.183773994445801, loss=1.3650438785552979
I0307 16:14:04.937453 140071458023168 logging_writer.py:48] [127700] global_step=127700, grad_norm=4.910594940185547, loss=1.3180136680603027
I0307 16:14:43.232420 140071466415872 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.28679084777832, loss=1.5216995477676392
I0307 16:15:21.385456 140071458023168 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.256145000457764, loss=1.434237003326416
I0307 16:15:59.675725 140071466415872 logging_writer.py:48] [128000] global_step=128000, grad_norm=6.006471633911133, loss=1.4074634313583374
I0307 16:16:38.187756 140071458023168 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.7193169593811035, loss=1.373734712600708
I0307 16:17:16.737603 140071466415872 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.390605926513672, loss=1.4289120435714722
I0307 16:17:42.911496 140226914178240 spec.py:321] Evaluating on the training split.
I0307 16:17:54.519788 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 16:18:12.659315 140226914178240 spec.py:349] Evaluating on the test split.
I0307 16:18:14.436929 140226914178240 submission_runner.py:469] Time since start: 53550.67s, 	Step: 128269, 	{'train/accuracy': 0.7691127061843872, 'train/loss': 0.8599770665168762, 'validation/accuracy': 0.6973400115966797, 'validation/loss': 1.217076063156128, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.912018895149231, 'test/num_examples': 10000, 'score': 50027.059827804565, 'total_duration': 53550.66779232025, 'accumulated_submission_time': 50027.059827804565, 'accumulated_eval_time': 3496.122677087784, 'accumulated_logging_time': 14.364009857177734}
I0307 16:18:14.573388 140071458023168 logging_writer.py:48] [128269] accumulated_eval_time=3496.12, accumulated_logging_time=14.364, accumulated_submission_time=50027.1, global_step=128269, preemption_count=0, score=50027.1, test/accuracy=0.5699, test/loss=1.91202, test/num_examples=10000, total_duration=53550.7, train/accuracy=0.769113, train/loss=0.859977, validation/accuracy=0.69734, validation/loss=1.21708, validation/num_examples=50000
I0307 16:18:26.845190 140071466415872 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.416415691375732, loss=1.4256483316421509
I0307 16:19:05.076127 140071458023168 logging_writer.py:48] [128400] global_step=128400, grad_norm=6.290252685546875, loss=1.4027050733566284
I0307 16:19:43.896503 140071466415872 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.4578022956848145, loss=1.399087905883789
I0307 16:20:22.418967 140071458023168 logging_writer.py:48] [128600] global_step=128600, grad_norm=5.303260326385498, loss=1.4514808654785156
I0307 16:21:00.617597 140071466415872 logging_writer.py:48] [128700] global_step=128700, grad_norm=5.270040512084961, loss=1.2057667970657349
I0307 16:21:39.092883 140071458023168 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.47078800201416, loss=1.3439347743988037
2025-03-07 16:21:54.101045: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:22:17.099413 140071466415872 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.564150333404541, loss=1.424900770187378
I0307 16:22:55.707530 140071458023168 logging_writer.py:48] [129000] global_step=129000, grad_norm=5.599900245666504, loss=1.342685341835022
I0307 16:23:33.753153 140071466415872 logging_writer.py:48] [129100] global_step=129100, grad_norm=6.107982635498047, loss=1.405961275100708
I0307 16:24:12.519020 140071458023168 logging_writer.py:48] [129200] global_step=129200, grad_norm=5.437905311584473, loss=1.4461867809295654
I0307 16:24:50.899450 140071466415872 logging_writer.py:48] [129300] global_step=129300, grad_norm=6.141138553619385, loss=1.420956015586853
I0307 16:25:29.292706 140071458023168 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.595681190490723, loss=1.3949320316314697
I0307 16:26:07.154102 140071466415872 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.428966522216797, loss=1.3573538064956665
I0307 16:26:44.714841 140226914178240 spec.py:321] Evaluating on the training split.
I0307 16:26:56.200023 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 16:27:16.174067 140226914178240 spec.py:349] Evaluating on the test split.
I0307 16:27:17.957435 140226914178240 submission_runner.py:469] Time since start: 54094.19s, 	Step: 129599, 	{'train/accuracy': 0.7689133882522583, 'train/loss': 0.8698284029960632, 'validation/accuracy': 0.6899799704551697, 'validation/loss': 1.2466366291046143, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.942866325378418, 'test/num_examples': 10000, 'score': 50537.025822877884, 'total_duration': 54094.1882815361, 'accumulated_submission_time': 50537.025822877884, 'accumulated_eval_time': 3529.3651161193848, 'accumulated_logging_time': 14.530999183654785}
I0307 16:27:18.010692 140071458023168 logging_writer.py:48] [129599] accumulated_eval_time=3529.37, accumulated_logging_time=14.531, accumulated_submission_time=50537, global_step=129599, preemption_count=0, score=50537, test/accuracy=0.5681, test/loss=1.94287, test/num_examples=10000, total_duration=54094.2, train/accuracy=0.768913, train/loss=0.869828, validation/accuracy=0.68998, validation/loss=1.24664, validation/num_examples=50000
I0307 16:27:18.797082 140071466415872 logging_writer.py:48] [129600] global_step=129600, grad_norm=6.086668014526367, loss=1.3496778011322021
I0307 16:27:57.167759 140071458023168 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.567028999328613, loss=1.4504938125610352
I0307 16:28:35.617977 140071466415872 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.550327301025391, loss=1.385341763496399
I0307 16:29:14.240311 140071458023168 logging_writer.py:48] [129900] global_step=129900, grad_norm=5.789399147033691, loss=1.354506015777588
I0307 16:29:52.995784 140071466415872 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.820001125335693, loss=1.3636399507522583
2025-03-07 16:30:28.001369: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:30:31.628300 140071458023168 logging_writer.py:48] [130100] global_step=130100, grad_norm=5.696739196777344, loss=1.3253774642944336
I0307 16:31:10.375288 140071466415872 logging_writer.py:48] [130200] global_step=130200, grad_norm=5.478867530822754, loss=1.4136439561843872
I0307 16:31:48.770247 140071458023168 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.984801292419434, loss=1.3925435543060303
I0307 16:32:27.191082 140071466415872 logging_writer.py:48] [130400] global_step=130400, grad_norm=5.840465068817139, loss=1.4431531429290771
I0307 16:33:05.735538 140071458023168 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.966100215911865, loss=1.3609167337417603
I0307 16:33:44.207020 140071466415872 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.53066349029541, loss=1.4311355352401733
I0307 16:34:22.589819 140071458023168 logging_writer.py:48] [130700] global_step=130700, grad_norm=5.878970146179199, loss=1.3436288833618164
I0307 16:35:00.787182 140071466415872 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.58599328994751, loss=1.4531291723251343
I0307 16:35:39.158106 140071458023168 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.827070236206055, loss=1.4642001390457153
I0307 16:35:48.292882 140226914178240 spec.py:321] Evaluating on the training split.
I0307 16:35:59.935042 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 16:36:20.594654 140226914178240 spec.py:349] Evaluating on the test split.
I0307 16:36:22.359876 140226914178240 submission_runner.py:469] Time since start: 54638.59s, 	Step: 130925, 	{'train/accuracy': 0.7647879123687744, 'train/loss': 0.8849365711212158, 'validation/accuracy': 0.6901599764823914, 'validation/loss': 1.2479355335235596, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 1.9499568939208984, 'test/num_examples': 10000, 'score': 51047.13235664368, 'total_duration': 54638.59072327614, 'accumulated_submission_time': 51047.13235664368, 'accumulated_eval_time': 3563.431957244873, 'accumulated_logging_time': 14.617443084716797}
I0307 16:36:22.458362 140071466415872 logging_writer.py:48] [130925] accumulated_eval_time=3563.43, accumulated_logging_time=14.6174, accumulated_submission_time=51047.1, global_step=130925, preemption_count=0, score=51047.1, test/accuracy=0.5622, test/loss=1.94996, test/num_examples=10000, total_duration=54638.6, train/accuracy=0.764788, train/loss=0.884937, validation/accuracy=0.69016, validation/loss=1.24794, validation/num_examples=50000
I0307 16:36:51.833482 140071458023168 logging_writer.py:48] [131000] global_step=131000, grad_norm=6.62482213973999, loss=1.423572063446045
I0307 16:37:30.504909 140071466415872 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.954347610473633, loss=1.4833422899246216
I0307 16:38:09.121448 140071458023168 logging_writer.py:48] [131200] global_step=131200, grad_norm=6.03732442855835, loss=1.4297765493392944
I0307 16:38:47.735482 140071466415872 logging_writer.py:48] [131300] global_step=131300, grad_norm=5.750938415527344, loss=1.3306514024734497
2025-03-07 16:39:03.711536: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:39:26.161962 140071458023168 logging_writer.py:48] [131400] global_step=131400, grad_norm=5.530536651611328, loss=1.3679966926574707
I0307 16:40:04.665197 140071466415872 logging_writer.py:48] [131500] global_step=131500, grad_norm=6.269618034362793, loss=1.3533588647842407
I0307 16:40:43.272555 140071458023168 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.449797630310059, loss=1.3198895454406738
I0307 16:41:21.716286 140071466415872 logging_writer.py:48] [131700] global_step=131700, grad_norm=6.74995756149292, loss=1.31383216381073
I0307 16:42:00.101855 140071458023168 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.917754173278809, loss=1.3045542240142822
I0307 16:42:38.687420 140071466415872 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.147667407989502, loss=1.2796143293380737
I0307 16:43:17.327840 140071458023168 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.8510942459106445, loss=1.4184629917144775
I0307 16:43:55.400382 140071466415872 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.730013847351074, loss=1.3202552795410156
I0307 16:44:33.934308 140071458023168 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.851519584655762, loss=1.4332005977630615
I0307 16:44:52.480601 140226914178240 spec.py:321] Evaluating on the training split.
I0307 16:45:04.525272 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 16:45:21.324209 140226914178240 spec.py:349] Evaluating on the test split.
I0307 16:45:23.107762 140226914178240 submission_runner.py:469] Time since start: 55179.34s, 	Step: 132249, 	{'train/accuracy': 0.7755500674247742, 'train/loss': 0.8368673920631409, 'validation/accuracy': 0.7014600038528442, 'validation/loss': 1.2075825929641724, 'validation/num_examples': 50000, 'test/accuracy': 0.5716000199317932, 'test/loss': 1.9136385917663574, 'test/num_examples': 10000, 'score': 51556.9741396904, 'total_duration': 55179.33862376213, 'accumulated_submission_time': 51556.9741396904, 'accumulated_eval_time': 3594.058982849121, 'accumulated_logging_time': 14.752139329910278}
I0307 16:45:23.235208 140071466415872 logging_writer.py:48] [132249] accumulated_eval_time=3594.06, accumulated_logging_time=14.7521, accumulated_submission_time=51557, global_step=132249, preemption_count=0, score=51557, test/accuracy=0.5716, test/loss=1.91364, test/num_examples=10000, total_duration=55179.3, train/accuracy=0.77555, train/loss=0.836867, validation/accuracy=0.70146, validation/loss=1.20758, validation/num_examples=50000
I0307 16:45:43.150311 140071458023168 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.517946243286133, loss=1.3862851858139038
I0307 16:46:21.589361 140071466415872 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.801285266876221, loss=1.3317294120788574
I0307 16:47:00.237022 140071458023168 logging_writer.py:48] [132500] global_step=132500, grad_norm=5.494139194488525, loss=1.4236583709716797
2025-03-07 16:47:35.959487: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:47:38.661253 140071466415872 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.752651214599609, loss=1.4149341583251953
I0307 16:48:17.170211 140071458023168 logging_writer.py:48] [132700] global_step=132700, grad_norm=6.628659248352051, loss=1.4098098278045654
I0307 16:48:55.552114 140071466415872 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.893232822418213, loss=1.4271577596664429
I0307 16:49:33.760906 140071458023168 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.6262125968933105, loss=1.321989893913269
I0307 16:50:12.059739 140071466415872 logging_writer.py:48] [133000] global_step=133000, grad_norm=5.578214645385742, loss=1.3345842361450195
I0307 16:50:50.682536 140071458023168 logging_writer.py:48] [133100] global_step=133100, grad_norm=6.525136947631836, loss=1.2889792919158936
I0307 16:51:29.046470 140071466415872 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.594294548034668, loss=1.3480528593063354
I0307 16:52:07.276074 140071458023168 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.450584411621094, loss=1.2316416501998901
I0307 16:52:45.706560 140071466415872 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.401075839996338, loss=1.3720800876617432
I0307 16:53:24.063013 140071458023168 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.808500289916992, loss=1.3930550813674927
I0307 16:53:53.382226 140226914178240 spec.py:321] Evaluating on the training split.
I0307 16:54:05.219836 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 16:54:24.567105 140226914178240 spec.py:349] Evaluating on the test split.
I0307 16:54:26.346889 140226914178240 submission_runner.py:469] Time since start: 55722.58s, 	Step: 133577, 	{'train/accuracy': 0.7761479616165161, 'train/loss': 0.8254809975624084, 'validation/accuracy': 0.6995799541473389, 'validation/loss': 1.2028008699417114, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.8977516889572144, 'test/num_examples': 10000, 'score': 52066.951180934906, 'total_duration': 55722.577755212784, 'accumulated_submission_time': 52066.951180934906, 'accumulated_eval_time': 3627.0235135555267, 'accumulated_logging_time': 14.905612707138062}
I0307 16:54:26.453655 140071466415872 logging_writer.py:48] [133577] accumulated_eval_time=3627.02, accumulated_logging_time=14.9056, accumulated_submission_time=52067, global_step=133577, preemption_count=0, score=52067, test/accuracy=0.5697, test/loss=1.89775, test/num_examples=10000, total_duration=55722.6, train/accuracy=0.776148, train/loss=0.825481, validation/accuracy=0.69958, validation/loss=1.2028, validation/num_examples=50000
I0307 16:54:35.728570 140071458023168 logging_writer.py:48] [133600] global_step=133600, grad_norm=6.090810775756836, loss=1.4829273223876953
I0307 16:55:14.259124 140071466415872 logging_writer.py:48] [133700] global_step=133700, grad_norm=6.052525520324707, loss=1.3890228271484375
I0307 16:55:52.554815 140071458023168 logging_writer.py:48] [133800] global_step=133800, grad_norm=5.312832832336426, loss=1.2950122356414795
2025-03-07 16:56:09.402213: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:56:30.939909 140071466415872 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.767528533935547, loss=1.4145681858062744
I0307 16:57:09.405076 140071458023168 logging_writer.py:48] [134000] global_step=134000, grad_norm=6.020293235778809, loss=1.248856544494629
I0307 16:57:47.923705 140071466415872 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.320410251617432, loss=1.3822886943817139
I0307 16:58:26.020857 140071458023168 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.770514965057373, loss=1.4132184982299805
I0307 16:59:04.431255 140071466415872 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.619103908538818, loss=1.3528623580932617
I0307 16:59:42.664304 140071458023168 logging_writer.py:48] [134400] global_step=134400, grad_norm=6.105857849121094, loss=1.3461650609970093
I0307 17:00:20.657354 140071466415872 logging_writer.py:48] [134500] global_step=134500, grad_norm=6.5856852531433105, loss=1.329136610031128
I0307 17:00:59.071813 140071458023168 logging_writer.py:48] [134600] global_step=134600, grad_norm=5.409952163696289, loss=1.3313164710998535
I0307 17:01:37.227382 140071466415872 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.780466079711914, loss=1.2862414121627808
I0307 17:02:15.852746 140071458023168 logging_writer.py:48] [134800] global_step=134800, grad_norm=6.07907247543335, loss=1.3846112489700317
I0307 17:02:54.542853 140071466415872 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.927339553833008, loss=1.2682013511657715
I0307 17:02:56.465370 140226914178240 spec.py:321] Evaluating on the training split.
I0307 17:03:08.324583 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 17:03:28.001777 140226914178240 spec.py:349] Evaluating on the test split.
I0307 17:03:29.774707 140226914178240 submission_runner.py:469] Time since start: 56266.01s, 	Step: 134906, 	{'train/accuracy': 0.7825454473495483, 'train/loss': 0.8077905178070068, 'validation/accuracy': 0.7038399577140808, 'validation/loss': 1.1898832321166992, 'validation/num_examples': 50000, 'test/accuracy': 0.573900043964386, 'test/loss': 1.8658047914505005, 'test/num_examples': 10000, 'score': 52576.776794195175, 'total_duration': 56266.00555682182, 'accumulated_submission_time': 52576.776794195175, 'accumulated_eval_time': 3660.33269906044, 'accumulated_logging_time': 15.054564952850342}
I0307 17:03:29.879224 140071458023168 logging_writer.py:48] [134906] accumulated_eval_time=3660.33, accumulated_logging_time=15.0546, accumulated_submission_time=52576.8, global_step=134906, preemption_count=0, score=52576.8, test/accuracy=0.5739, test/loss=1.8658, test/num_examples=10000, total_duration=56266, train/accuracy=0.782545, train/loss=0.807791, validation/accuracy=0.70384, validation/loss=1.18988, validation/num_examples=50000
I0307 17:04:06.321148 140071466415872 logging_writer.py:48] [135000] global_step=135000, grad_norm=6.126906871795654, loss=1.3417797088623047
2025-03-07 17:04:43.172487: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:04:44.881615 140071458023168 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.030908584594727, loss=1.3970919847488403
I0307 17:05:23.494118 140071466415872 logging_writer.py:48] [135200] global_step=135200, grad_norm=5.658831596374512, loss=1.351688265800476
I0307 17:06:02.057455 140071458023168 logging_writer.py:48] [135300] global_step=135300, grad_norm=6.388027667999268, loss=1.256913185119629
I0307 17:06:40.248422 140071466415872 logging_writer.py:48] [135400] global_step=135400, grad_norm=6.729038715362549, loss=1.3000820875167847
I0307 17:07:18.474270 140071458023168 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.657416343688965, loss=1.3438489437103271
I0307 17:07:56.930404 140071466415872 logging_writer.py:48] [135600] global_step=135600, grad_norm=6.587850570678711, loss=1.3981622457504272
I0307 17:08:35.337343 140071458023168 logging_writer.py:48] [135700] global_step=135700, grad_norm=6.557792663574219, loss=1.316516399383545
I0307 17:09:13.607809 140071466415872 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.773375034332275, loss=1.342271089553833
I0307 17:09:51.965980 140071458023168 logging_writer.py:48] [135900] global_step=135900, grad_norm=5.804326057434082, loss=1.3852784633636475
I0307 17:10:30.494496 140071466415872 logging_writer.py:48] [136000] global_step=136000, grad_norm=5.7078022956848145, loss=1.3049368858337402
I0307 17:11:08.947211 140071458023168 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.594441890716553, loss=1.2906681299209595
I0307 17:11:47.581342 140071466415872 logging_writer.py:48] [136200] global_step=136200, grad_norm=6.349765300750732, loss=1.440451741218567
I0307 17:11:59.801152 140226914178240 spec.py:321] Evaluating on the training split.
I0307 17:12:11.447376 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 17:12:28.074017 140226914178240 spec.py:349] Evaluating on the test split.
I0307 17:12:29.827750 140226914178240 submission_runner.py:469] Time since start: 56806.06s, 	Step: 136233, 	{'train/accuracy': 0.7856743931770325, 'train/loss': 0.7930082082748413, 'validation/accuracy': 0.7042999863624573, 'validation/loss': 1.186693787574768, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.861576795578003, 'test/num_examples': 10000, 'score': 53086.47843527794, 'total_duration': 56806.058566093445, 'accumulated_submission_time': 53086.47843527794, 'accumulated_eval_time': 3690.3591141700745, 'accumulated_logging_time': 15.233972072601318}
I0307 17:12:29.955637 140071458023168 logging_writer.py:48] [136233] accumulated_eval_time=3690.36, accumulated_logging_time=15.234, accumulated_submission_time=53086.5, global_step=136233, preemption_count=0, score=53086.5, test/accuracy=0.5826, test/loss=1.86158, test/num_examples=10000, total_duration=56806.1, train/accuracy=0.785674, train/loss=0.793008, validation/accuracy=0.7043, validation/loss=1.18669, validation/num_examples=50000
I0307 17:12:56.297712 140071466415872 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.404181957244873, loss=1.255703091621399
2025-03-07 17:13:14.369738: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:13:35.361137 140071458023168 logging_writer.py:48] [136400] global_step=136400, grad_norm=6.042571067810059, loss=1.380018949508667
I0307 17:14:14.138692 140071466415872 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.0589399337768555, loss=1.260928988456726
I0307 17:14:52.524745 140071458023168 logging_writer.py:48] [136600] global_step=136600, grad_norm=5.838331699371338, loss=1.3615813255310059
I0307 17:15:30.715330 140071466415872 logging_writer.py:48] [136700] global_step=136700, grad_norm=5.758406162261963, loss=1.3141953945159912
I0307 17:16:09.094720 140071458023168 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.971288681030273, loss=1.3309860229492188
I0307 17:16:47.387420 140071466415872 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.917736053466797, loss=1.3514283895492554
I0307 17:17:25.902162 140071458023168 logging_writer.py:48] [137000] global_step=137000, grad_norm=6.094461917877197, loss=1.300980567932129
I0307 17:18:04.360193 140071466415872 logging_writer.py:48] [137100] global_step=137100, grad_norm=5.612171173095703, loss=1.349448323249817
I0307 17:18:42.619648 140071458023168 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.507645606994629, loss=1.1868820190429688
I0307 17:19:21.119187 140071466415872 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.975358009338379, loss=1.389750361442566
I0307 17:19:59.671593 140071458023168 logging_writer.py:48] [137400] global_step=137400, grad_norm=6.196902275085449, loss=1.3136610984802246
I0307 17:20:38.134232 140071466415872 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.030697822570801, loss=1.3239401578903198
I0307 17:21:00.091361 140226914178240 spec.py:321] Evaluating on the training split.
I0307 17:21:11.783206 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 17:21:30.490306 140226914178240 spec.py:349] Evaluating on the test split.
I0307 17:21:32.236647 140226914178240 submission_runner.py:469] Time since start: 57348.47s, 	Step: 137558, 	{'train/accuracy': 0.7804726958274841, 'train/loss': 0.8170868754386902, 'validation/accuracy': 0.7047399878501892, 'validation/loss': 1.1842223405838013, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 1.8777655363082886, 'test/num_examples': 10000, 'score': 53596.4441652298, 'total_duration': 57348.46748781204, 'accumulated_submission_time': 53596.4441652298, 'accumulated_eval_time': 3722.5042386054993, 'accumulated_logging_time': 15.389992475509644}
I0307 17:21:32.329850 140071458023168 logging_writer.py:48] [137558] accumulated_eval_time=3722.5, accumulated_logging_time=15.39, accumulated_submission_time=53596.4, global_step=137558, preemption_count=0, score=53596.4, test/accuracy=0.5744, test/loss=1.87777, test/num_examples=10000, total_duration=57348.5, train/accuracy=0.780473, train/loss=0.817087, validation/accuracy=0.70474, validation/loss=1.18422, validation/num_examples=50000
2025-03-07 17:21:47.975571: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:21:48.781884 140071466415872 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.757796287536621, loss=1.3602436780929565
I0307 17:22:27.330994 140071458023168 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.559043884277344, loss=1.304940104484558
I0307 17:23:05.959847 140071466415872 logging_writer.py:48] [137800] global_step=137800, grad_norm=6.025409698486328, loss=1.1890339851379395
I0307 17:23:44.272585 140071458023168 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.743171215057373, loss=1.2670692205429077
I0307 17:24:22.680239 140071466415872 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.785215377807617, loss=1.2636187076568604
I0307 17:25:00.969319 140071458023168 logging_writer.py:48] [138100] global_step=138100, grad_norm=5.941340923309326, loss=1.259444236755371
I0307 17:25:39.552091 140071466415872 logging_writer.py:48] [138200] global_step=138200, grad_norm=6.109132289886475, loss=1.274839162826538
I0307 17:26:17.841842 140071458023168 logging_writer.py:48] [138300] global_step=138300, grad_norm=6.18820858001709, loss=1.320723533630371
I0307 17:26:56.471442 140071466415872 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.013552188873291, loss=1.2124848365783691
I0307 17:27:35.219625 140071458023168 logging_writer.py:48] [138500] global_step=138500, grad_norm=6.761863708496094, loss=1.3256067037582397
I0307 17:28:13.904962 140071466415872 logging_writer.py:48] [138600] global_step=138600, grad_norm=7.030093193054199, loss=1.201969027519226
I0307 17:28:52.312912 140071458023168 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.338010787963867, loss=1.436464548110962
I0307 17:29:30.400025 140071466415872 logging_writer.py:48] [138800] global_step=138800, grad_norm=6.17749547958374, loss=1.3229061365127563
2025-03-07 17:29:48.866143: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:30:02.567210 140226914178240 spec.py:321] Evaluating on the training split.
I0307 17:30:14.387837 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 17:30:33.967593 140226914178240 spec.py:349] Evaluating on the test split.
I0307 17:30:35.729456 140226914178240 submission_runner.py:469] Time since start: 57891.96s, 	Step: 138885, 	{'train/accuracy': 0.7915935516357422, 'train/loss': 0.7748403549194336, 'validation/accuracy': 0.7110999822616577, 'validation/loss': 1.1624399423599243, 'validation/num_examples': 50000, 'test/accuracy': 0.5832000374794006, 'test/loss': 1.8485795259475708, 'test/num_examples': 10000, 'score': 54106.490668296814, 'total_duration': 57891.960322380066, 'accumulated_submission_time': 54106.490668296814, 'accumulated_eval_time': 3755.666371822357, 'accumulated_logging_time': 15.528958797454834}
I0307 17:30:35.885542 140071458023168 logging_writer.py:48] [138885] accumulated_eval_time=3755.67, accumulated_logging_time=15.529, accumulated_submission_time=54106.5, global_step=138885, preemption_count=0, score=54106.5, test/accuracy=0.5832, test/loss=1.84858, test/num_examples=10000, total_duration=57892, train/accuracy=0.791594, train/loss=0.77484, validation/accuracy=0.7111, validation/loss=1.16244, validation/num_examples=50000
I0307 17:30:41.976027 140071466415872 logging_writer.py:48] [138900] global_step=138900, grad_norm=6.254750728607178, loss=1.3231258392333984
I0307 17:31:20.243869 140071458023168 logging_writer.py:48] [139000] global_step=139000, grad_norm=6.072172164916992, loss=1.3825018405914307
I0307 17:31:58.185838 140071466415872 logging_writer.py:48] [139100] global_step=139100, grad_norm=6.1661248207092285, loss=1.2679541110992432
I0307 17:32:36.190756 140071458023168 logging_writer.py:48] [139200] global_step=139200, grad_norm=6.110182762145996, loss=1.3780949115753174
I0307 17:33:14.384359 140071466415872 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.3611955642700195, loss=1.2287715673446655
I0307 17:33:52.415468 140071458023168 logging_writer.py:48] [139400] global_step=139400, grad_norm=5.414660453796387, loss=1.2999472618103027
I0307 17:34:30.582028 140071466415872 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.286945819854736, loss=1.3225481510162354
I0307 17:35:09.026130 140071458023168 logging_writer.py:48] [139600] global_step=139600, grad_norm=6.7911906242370605, loss=1.2747814655303955
I0307 17:35:47.412666 140071466415872 logging_writer.py:48] [139700] global_step=139700, grad_norm=5.604795932769775, loss=1.1870622634887695
I0307 17:36:26.366940 140071458023168 logging_writer.py:48] [139800] global_step=139800, grad_norm=6.6822428703308105, loss=1.3245198726654053
I0307 17:37:04.831066 140071466415872 logging_writer.py:48] [139900] global_step=139900, grad_norm=6.833901882171631, loss=1.308562159538269
I0307 17:37:43.284727 140071458023168 logging_writer.py:48] [140000] global_step=140000, grad_norm=7.048586845397949, loss=1.256463885307312
2025-03-07 17:38:21.136158: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:38:21.262631 140071466415872 logging_writer.py:48] [140100] global_step=140100, grad_norm=5.771821975708008, loss=1.2880218029022217
I0307 17:38:59.714825 140071458023168 logging_writer.py:48] [140200] global_step=140200, grad_norm=5.6681928634643555, loss=1.3030362129211426
I0307 17:39:05.946127 140226914178240 spec.py:321] Evaluating on the training split.
I0307 17:39:17.576894 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 17:39:36.932731 140226914178240 spec.py:349] Evaluating on the test split.
I0307 17:39:38.659558 140226914178240 submission_runner.py:469] Time since start: 58434.89s, 	Step: 140217, 	{'train/accuracy': 0.794343888759613, 'train/loss': 0.7518271207809448, 'validation/accuracy': 0.7140600085258484, 'validation/loss': 1.1480584144592285, 'validation/num_examples': 50000, 'test/accuracy': 0.5831000208854675, 'test/loss': 1.838874340057373, 'test/num_examples': 10000, 'score': 54616.38758087158, 'total_duration': 58434.89035844803, 'accumulated_submission_time': 54616.38758087158, 'accumulated_eval_time': 3788.379606962204, 'accumulated_logging_time': 15.704994916915894}
I0307 17:39:38.778351 140071466415872 logging_writer.py:48] [140217] accumulated_eval_time=3788.38, accumulated_logging_time=15.705, accumulated_submission_time=54616.4, global_step=140217, preemption_count=0, score=54616.4, test/accuracy=0.5831, test/loss=1.83887, test/num_examples=10000, total_duration=58434.9, train/accuracy=0.794344, train/loss=0.751827, validation/accuracy=0.71406, validation/loss=1.14806, validation/num_examples=50000
I0307 17:40:11.040763 140071458023168 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.138920307159424, loss=1.2398666143417358
I0307 17:40:49.344393 140071466415872 logging_writer.py:48] [140400] global_step=140400, grad_norm=6.75771427154541, loss=1.2697489261627197
I0307 17:41:27.749296 140071458023168 logging_writer.py:48] [140500] global_step=140500, grad_norm=6.13541316986084, loss=1.352520227432251
I0307 17:42:06.148308 140071466415872 logging_writer.py:48] [140600] global_step=140600, grad_norm=5.812935829162598, loss=1.2419612407684326
I0307 17:42:44.508529 140071458023168 logging_writer.py:48] [140700] global_step=140700, grad_norm=6.034899711608887, loss=1.25525963306427
I0307 17:43:22.878762 140071466415872 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.253036022186279, loss=1.257749319076538
I0307 17:44:01.262246 140071458023168 logging_writer.py:48] [140900] global_step=140900, grad_norm=5.9980340003967285, loss=1.105997920036316
I0307 17:44:40.079275 140071466415872 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.7213873863220215, loss=1.3019168376922607
I0307 17:45:18.856556 140071458023168 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.425866603851318, loss=1.325445294380188
I0307 17:45:57.593111 140071466415872 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.4271697998046875, loss=1.1605863571166992
I0307 17:46:36.044425 140071458023168 logging_writer.py:48] [141300] global_step=141300, grad_norm=6.469961643218994, loss=1.2846317291259766
2025-03-07 17:46:55.889209: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:47:14.618548 140071466415872 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.176418781280518, loss=1.2834739685058594
I0307 17:47:53.125080 140071458023168 logging_writer.py:48] [141500] global_step=141500, grad_norm=5.861907482147217, loss=1.2230817079544067
I0307 17:48:08.982328 140226914178240 spec.py:321] Evaluating on the training split.
I0307 17:48:20.607388 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 17:48:37.432922 140226914178240 spec.py:349] Evaluating on the test split.
I0307 17:48:39.193148 140226914178240 submission_runner.py:469] Time since start: 58975.42s, 	Step: 141542, 	{'train/accuracy': 0.7994658350944519, 'train/loss': 0.7493096590042114, 'validation/accuracy': 0.7128799557685852, 'validation/loss': 1.1480590105056763, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.8306844234466553, 'test/num_examples': 10000, 'score': 55126.42038106918, 'total_duration': 58975.423993587494, 'accumulated_submission_time': 55126.42038106918, 'accumulated_eval_time': 3818.5902738571167, 'accumulated_logging_time': 15.850285291671753}
I0307 17:48:39.282070 140071466415872 logging_writer.py:48] [141542] accumulated_eval_time=3818.59, accumulated_logging_time=15.8503, accumulated_submission_time=55126.4, global_step=141542, preemption_count=0, score=55126.4, test/accuracy=0.5884, test/loss=1.83068, test/num_examples=10000, total_duration=58975.4, train/accuracy=0.799466, train/loss=0.74931, validation/accuracy=0.71288, validation/loss=1.14806, validation/num_examples=50000
I0307 17:49:01.996602 140071458023168 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.603015422821045, loss=1.3441144227981567
I0307 17:49:40.219329 140071466415872 logging_writer.py:48] [141700] global_step=141700, grad_norm=6.373875617980957, loss=1.2846897840499878
I0307 17:50:18.510958 140071458023168 logging_writer.py:48] [141800] global_step=141800, grad_norm=6.058206558227539, loss=1.255583643913269
I0307 17:50:56.729414 140071466415872 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.338595390319824, loss=1.2818821668624878
I0307 17:51:35.029600 140071458023168 logging_writer.py:48] [142000] global_step=142000, grad_norm=6.4197821617126465, loss=1.3167774677276611
I0307 17:52:13.173027 140071466415872 logging_writer.py:48] [142100] global_step=142100, grad_norm=6.631161212921143, loss=1.3027445077896118
I0307 17:52:51.486006 140071458023168 logging_writer.py:48] [142200] global_step=142200, grad_norm=5.66880989074707, loss=1.2333378791809082
I0307 17:53:30.046946 140071466415872 logging_writer.py:48] [142300] global_step=142300, grad_norm=6.184308052062988, loss=1.1786472797393799
I0307 17:54:08.575747 140071458023168 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.3394246101379395, loss=1.23810875415802
I0307 17:54:46.662125 140071466415872 logging_writer.py:48] [142500] global_step=142500, grad_norm=6.712619781494141, loss=1.264722466468811
I0307 17:55:25.176035 140071458023168 logging_writer.py:48] [142600] global_step=142600, grad_norm=5.738916873931885, loss=1.1701545715332031
2025-03-07 17:55:25.851921: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:56:04.040256 140071466415872 logging_writer.py:48] [142700] global_step=142700, grad_norm=6.546778678894043, loss=1.2798287868499756
I0307 17:56:42.592340 140071458023168 logging_writer.py:48] [142800] global_step=142800, grad_norm=6.619739055633545, loss=1.3162003755569458
I0307 17:57:09.211936 140226914178240 spec.py:321] Evaluating on the training split.
I0307 17:57:21.346665 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 17:57:40.286807 140226914178240 spec.py:349] Evaluating on the test split.
I0307 17:57:42.035705 140226914178240 submission_runner.py:469] Time since start: 59518.27s, 	Step: 142870, 	{'train/accuracy': 0.8012794852256775, 'train/loss': 0.7226668000221252, 'validation/accuracy': 0.715179979801178, 'validation/loss': 1.1486707925796509, 'validation/num_examples': 50000, 'test/accuracy': 0.5825000405311584, 'test/loss': 1.8680264949798584, 'test/num_examples': 10000, 'score': 55636.181668281555, 'total_duration': 59518.26654577255, 'accumulated_submission_time': 55636.181668281555, 'accumulated_eval_time': 3851.413890838623, 'accumulated_logging_time': 15.96316409111023}
I0307 17:57:42.156025 140071466415872 logging_writer.py:48] [142870] accumulated_eval_time=3851.41, accumulated_logging_time=15.9632, accumulated_submission_time=55636.2, global_step=142870, preemption_count=0, score=55636.2, test/accuracy=0.5825, test/loss=1.86803, test/num_examples=10000, total_duration=59518.3, train/accuracy=0.801279, train/loss=0.722667, validation/accuracy=0.71518, validation/loss=1.14867, validation/num_examples=50000
I0307 17:57:54.116034 140071458023168 logging_writer.py:48] [142900] global_step=142900, grad_norm=5.757441520690918, loss=1.2210941314697266
I0307 17:58:32.883475 140071466415872 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.595775127410889, loss=1.2724578380584717
I0307 17:59:11.379468 140071458023168 logging_writer.py:48] [143100] global_step=143100, grad_norm=5.728357315063477, loss=1.173987627029419
I0307 17:59:49.700977 140071466415872 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.781554698944092, loss=1.2617261409759521
I0307 18:00:28.052271 140071458023168 logging_writer.py:48] [143300] global_step=143300, grad_norm=5.763881683349609, loss=1.2694061994552612
I0307 18:01:06.747104 140071466415872 logging_writer.py:48] [143400] global_step=143400, grad_norm=7.3162360191345215, loss=1.1860238313674927
I0307 18:01:45.330991 140071458023168 logging_writer.py:48] [143500] global_step=143500, grad_norm=6.962270736694336, loss=1.1549315452575684
I0307 18:02:24.018012 140071466415872 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.478817939758301, loss=1.285169005393982
I0307 18:03:02.729482 140071458023168 logging_writer.py:48] [143700] global_step=143700, grad_norm=6.428031921386719, loss=1.3013994693756104
I0307 18:03:41.170062 140071466415872 logging_writer.py:48] [143800] global_step=143800, grad_norm=7.0532355308532715, loss=1.214655876159668
2025-03-07 18:04:01.920411: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:04:19.805511 140071458023168 logging_writer.py:48] [143900] global_step=143900, grad_norm=7.06206750869751, loss=1.2974939346313477
I0307 18:04:58.306714 140071466415872 logging_writer.py:48] [144000] global_step=144000, grad_norm=6.406915187835693, loss=1.2731167078018188
I0307 18:05:36.776218 140071458023168 logging_writer.py:48] [144100] global_step=144100, grad_norm=6.392614364624023, loss=1.2427923679351807
I0307 18:06:12.148148 140226914178240 spec.py:321] Evaluating on the training split.
I0307 18:06:23.836118 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 18:06:43.306933 140226914178240 spec.py:349] Evaluating on the test split.
I0307 18:06:45.075974 140226914178240 submission_runner.py:469] Time since start: 60061.31s, 	Step: 144193, 	{'train/accuracy': 0.8028738498687744, 'train/loss': 0.7296748161315918, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.153074860572815, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 1.8504260778427124, 'test/num_examples': 10000, 'score': 56145.984580755234, 'total_duration': 60061.30682682991, 'accumulated_submission_time': 56145.984580755234, 'accumulated_eval_time': 3884.34156870842, 'accumulated_logging_time': 16.13098454475403}
I0307 18:06:45.190035 140071466415872 logging_writer.py:48] [144193] accumulated_eval_time=3884.34, accumulated_logging_time=16.131, accumulated_submission_time=56146, global_step=144193, preemption_count=0, score=56146, test/accuracy=0.5879, test/loss=1.85043, test/num_examples=10000, total_duration=60061.3, train/accuracy=0.802874, train/loss=0.729675, validation/accuracy=0.71352, validation/loss=1.15307, validation/num_examples=50000
I0307 18:06:48.248126 140071458023168 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.704233646392822, loss=1.211822509765625
I0307 18:07:26.850925 140071466415872 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.610782623291016, loss=1.2672955989837646
I0307 18:08:05.298015 140071458023168 logging_writer.py:48] [144400] global_step=144400, grad_norm=6.074910640716553, loss=1.2776535749435425
I0307 18:08:43.866057 140071466415872 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.435078144073486, loss=1.2708117961883545
I0307 18:09:22.331850 140071458023168 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.501380443572998, loss=1.2452696561813354
I0307 18:10:00.846215 140071466415872 logging_writer.py:48] [144700] global_step=144700, grad_norm=6.516621112823486, loss=1.3351006507873535
I0307 18:10:39.668647 140071458023168 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.5010552406311035, loss=1.216827392578125
I0307 18:11:18.786535 140071466415872 logging_writer.py:48] [144900] global_step=144900, grad_norm=7.4585771560668945, loss=1.2735708951950073
I0307 18:11:57.492522 140071458023168 logging_writer.py:48] [145000] global_step=145000, grad_norm=7.172064304351807, loss=1.2039333581924438
I0307 18:12:35.843894 140071466415872 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.535614490509033, loss=1.2042511701583862
2025-03-07 18:12:37.742135: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:13:14.979832 140071458023168 logging_writer.py:48] [145200] global_step=145200, grad_norm=7.248373031616211, loss=1.2897210121154785
I0307 18:13:53.266311 140071466415872 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.877723217010498, loss=1.2157871723175049
I0307 18:14:31.623966 140071458023168 logging_writer.py:48] [145400] global_step=145400, grad_norm=6.507051944732666, loss=1.213869571685791
I0307 18:15:10.314061 140071466415872 logging_writer.py:48] [145500] global_step=145500, grad_norm=6.84715461730957, loss=1.248297929763794
I0307 18:15:15.505954 140226914178240 spec.py:321] Evaluating on the training split.
I0307 18:15:27.233008 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 18:15:45.286610 140226914178240 spec.py:349] Evaluating on the test split.
I0307 18:15:47.054638 140226914178240 submission_runner.py:469] Time since start: 60603.29s, 	Step: 145514, 	{'train/accuracy': 0.80961012840271, 'train/loss': 0.6954296231269836, 'validation/accuracy': 0.7186200022697449, 'validation/loss': 1.1304233074188232, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.8403096199035645, 'test/num_examples': 10000, 'score': 56656.10156941414, 'total_duration': 60603.285472631454, 'accumulated_submission_time': 56656.10156941414, 'accumulated_eval_time': 3915.8900859355927, 'accumulated_logging_time': 16.29936695098877}
I0307 18:15:47.233072 140071458023168 logging_writer.py:48] [145514] accumulated_eval_time=3915.89, accumulated_logging_time=16.2994, accumulated_submission_time=56656.1, global_step=145514, preemption_count=0, score=56656.1, test/accuracy=0.5914, test/loss=1.84031, test/num_examples=10000, total_duration=60603.3, train/accuracy=0.80961, train/loss=0.69543, validation/accuracy=0.71862, validation/loss=1.13042, validation/num_examples=50000
I0307 18:16:20.578302 140071466415872 logging_writer.py:48] [145600] global_step=145600, grad_norm=7.118161201477051, loss=1.2971110343933105
I0307 18:16:59.331960 140071458023168 logging_writer.py:48] [145700] global_step=145700, grad_norm=6.6435933113098145, loss=1.1403528451919556
I0307 18:17:38.079066 140071466415872 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.481265068054199, loss=1.1919925212860107
I0307 18:18:16.639780 140071458023168 logging_writer.py:48] [145900] global_step=145900, grad_norm=6.315602779388428, loss=1.185758113861084
I0307 18:18:55.312129 140071466415872 logging_writer.py:48] [146000] global_step=146000, grad_norm=7.4447197914123535, loss=1.2165278196334839
I0307 18:19:34.328593 140071458023168 logging_writer.py:48] [146100] global_step=146100, grad_norm=6.3963470458984375, loss=1.240952491760254
I0307 18:20:13.011487 140071466415872 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.768548011779785, loss=1.154000997543335
I0307 18:20:51.833920 140071458023168 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.190673828125, loss=1.1376384496688843
2025-03-07 18:21:13.199375: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:21:30.385868 140071466415872 logging_writer.py:48] [146400] global_step=146400, grad_norm=6.297560214996338, loss=1.0841704607009888
I0307 18:22:08.806397 140071458023168 logging_writer.py:48] [146500] global_step=146500, grad_norm=6.28352689743042, loss=1.1112264394760132
I0307 18:22:47.289897 140071466415872 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.503766059875488, loss=1.3150300979614258
I0307 18:23:26.098577 140071458023168 logging_writer.py:48] [146700] global_step=146700, grad_norm=6.349231243133545, loss=1.177875280380249
I0307 18:24:04.650761 140071466415872 logging_writer.py:48] [146800] global_step=146800, grad_norm=6.6845197677612305, loss=1.1681010723114014
I0307 18:24:17.098946 140226914178240 spec.py:321] Evaluating on the training split.
I0307 18:24:28.703450 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 18:24:45.582373 140226914178240 spec.py:349] Evaluating on the test split.
I0307 18:24:47.325978 140226914178240 submission_runner.py:469] Time since start: 61143.56s, 	Step: 146833, 	{'train/accuracy': 0.8132174611091614, 'train/loss': 0.6854730844497681, 'validation/accuracy': 0.7178399562835693, 'validation/loss': 1.1321442127227783, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8339585065841675, 'test/num_examples': 10000, 'score': 57165.80280208588, 'total_duration': 61143.5567946434, 'accumulated_submission_time': 57165.80280208588, 'accumulated_eval_time': 3946.1169352531433, 'accumulated_logging_time': 16.49998950958252}
I0307 18:24:47.422206 140071458023168 logging_writer.py:48] [146833] accumulated_eval_time=3946.12, accumulated_logging_time=16.5, accumulated_submission_time=57165.8, global_step=146833, preemption_count=0, score=57165.8, test/accuracy=0.5907, test/loss=1.83396, test/num_examples=10000, total_duration=61143.6, train/accuracy=0.813217, train/loss=0.685473, validation/accuracy=0.71784, validation/loss=1.13214, validation/num_examples=50000
I0307 18:25:13.640222 140071466415872 logging_writer.py:48] [146900] global_step=146900, grad_norm=6.912215709686279, loss=1.1409454345703125
I0307 18:25:52.173954 140071458023168 logging_writer.py:48] [147000] global_step=147000, grad_norm=7.206697940826416, loss=1.3220634460449219
I0307 18:26:30.852773 140071466415872 logging_writer.py:48] [147100] global_step=147100, grad_norm=6.2907209396362305, loss=1.1668118238449097
I0307 18:27:09.399469 140071458023168 logging_writer.py:48] [147200] global_step=147200, grad_norm=6.67957067489624, loss=1.1707308292388916
I0307 18:27:48.137814 140071466415872 logging_writer.py:48] [147300] global_step=147300, grad_norm=6.947734355926514, loss=1.2981045246124268
I0307 18:28:27.230188 140071458023168 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.388734817504883, loss=1.1919615268707275
I0307 18:29:06.222295 140071466415872 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.2014946937561035, loss=1.2349475622177124
I0307 18:29:44.912377 140071458023168 logging_writer.py:48] [147600] global_step=147600, grad_norm=6.931009292602539, loss=1.218018651008606
2025-03-07 18:29:47.353569: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:30:23.494875 140071466415872 logging_writer.py:48] [147700] global_step=147700, grad_norm=6.641995906829834, loss=1.1914054155349731
I0307 18:31:01.745419 140071458023168 logging_writer.py:48] [147800] global_step=147800, grad_norm=7.442046165466309, loss=1.2154878377914429
I0307 18:31:39.844956 140071466415872 logging_writer.py:48] [147900] global_step=147900, grad_norm=7.2909932136535645, loss=1.3109643459320068
I0307 18:32:18.219954 140071458023168 logging_writer.py:48] [148000] global_step=148000, grad_norm=6.693180084228516, loss=1.2322053909301758
I0307 18:32:56.987576 140071466415872 logging_writer.py:48] [148100] global_step=148100, grad_norm=6.772108554840088, loss=1.2160149812698364
I0307 18:33:17.529963 140226914178240 spec.py:321] Evaluating on the training split.
I0307 18:33:29.066082 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 18:33:51.816185 140226914178240 spec.py:349] Evaluating on the test split.
I0307 18:33:53.560417 140226914178240 submission_runner.py:469] Time since start: 61689.79s, 	Step: 148154, 	{'train/accuracy': 0.8169443607330322, 'train/loss': 0.6568751931190491, 'validation/accuracy': 0.7255399823188782, 'validation/loss': 1.110706090927124, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.7891204357147217, 'test/num_examples': 10000, 'score': 57675.74361562729, 'total_duration': 61689.79122018814, 'accumulated_submission_time': 57675.74361562729, 'accumulated_eval_time': 3982.1471931934357, 'accumulated_logging_time': 16.61973738670349}
I0307 18:33:53.681890 140071458023168 logging_writer.py:48] [148154] accumulated_eval_time=3982.15, accumulated_logging_time=16.6197, accumulated_submission_time=57675.7, global_step=148154, preemption_count=0, score=57675.7, test/accuracy=0.5997, test/loss=1.78912, test/num_examples=10000, total_duration=61689.8, train/accuracy=0.816944, train/loss=0.656875, validation/accuracy=0.72554, validation/loss=1.11071, validation/num_examples=50000
I0307 18:34:11.709644 140071466415872 logging_writer.py:48] [148200] global_step=148200, grad_norm=5.9061055183410645, loss=1.0674604177474976
I0307 18:34:50.226381 140071458023168 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.321351528167725, loss=1.2119100093841553
I0307 18:35:28.935330 140071466415872 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.028421401977539, loss=1.220299243927002
I0307 18:36:07.304794 140071458023168 logging_writer.py:48] [148500] global_step=148500, grad_norm=7.595479965209961, loss=1.2080559730529785
I0307 18:36:46.534710 140071466415872 logging_writer.py:48] [148600] global_step=148600, grad_norm=6.266524791717529, loss=1.1245046854019165
I0307 18:37:25.464236 140071458023168 logging_writer.py:48] [148700] global_step=148700, grad_norm=6.926930904388428, loss=1.2056437730789185
I0307 18:38:04.298357 140071466415872 logging_writer.py:48] [148800] global_step=148800, grad_norm=7.253540515899658, loss=1.1864418983459473
2025-03-07 18:38:26.354923: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:38:42.947889 140071458023168 logging_writer.py:48] [148900] global_step=148900, grad_norm=7.357998371124268, loss=1.2693021297454834
I0307 18:39:21.487262 140071466415872 logging_writer.py:48] [149000] global_step=149000, grad_norm=7.31448221206665, loss=1.256090521812439
I0307 18:39:59.858940 140071458023168 logging_writer.py:48] [149100] global_step=149100, grad_norm=6.859594345092773, loss=1.156877875328064
I0307 18:40:38.482458 140071466415872 logging_writer.py:48] [149200] global_step=149200, grad_norm=7.43198299407959, loss=1.1560778617858887
I0307 18:41:16.937858 140071458023168 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.866576194763184, loss=1.193950891494751
I0307 18:41:55.459394 140071466415872 logging_writer.py:48] [149400] global_step=149400, grad_norm=7.075830936431885, loss=1.226009488105774
I0307 18:42:23.704159 140226914178240 spec.py:321] Evaluating on the training split.
I0307 18:42:35.607529 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 18:42:54.319682 140226914178240 spec.py:349] Evaluating on the test split.
I0307 18:42:56.063107 140226914178240 submission_runner.py:469] Time since start: 62232.29s, 	Step: 149474, 	{'train/accuracy': 0.8251753449440002, 'train/loss': 0.6369521617889404, 'validation/accuracy': 0.7230599522590637, 'validation/loss': 1.1205451488494873, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.806749701499939, 'test/num_examples': 10000, 'score': 58185.58929872513, 'total_duration': 62232.29395532608, 'accumulated_submission_time': 58185.58929872513, 'accumulated_eval_time': 4014.505989074707, 'accumulated_logging_time': 16.77537965774536}
I0307 18:42:56.143954 140071458023168 logging_writer.py:48] [149474] accumulated_eval_time=4014.51, accumulated_logging_time=16.7754, accumulated_submission_time=58185.6, global_step=149474, preemption_count=0, score=58185.6, test/accuracy=0.5969, test/loss=1.80675, test/num_examples=10000, total_duration=62232.3, train/accuracy=0.825175, train/loss=0.636952, validation/accuracy=0.72306, validation/loss=1.12055, validation/num_examples=50000
I0307 18:43:06.571704 140071466415872 logging_writer.py:48] [149500] global_step=149500, grad_norm=7.668530464172363, loss=1.0998055934906006
I0307 18:43:44.813404 140071458023168 logging_writer.py:48] [149600] global_step=149600, grad_norm=6.905062675476074, loss=1.2368210554122925
I0307 18:44:23.051646 140071466415872 logging_writer.py:48] [149700] global_step=149700, grad_norm=7.529775619506836, loss=1.1887545585632324
I0307 18:45:02.037453 140071458023168 logging_writer.py:48] [149800] global_step=149800, grad_norm=8.060420036315918, loss=1.212011694908142
I0307 18:45:40.727270 140071466415872 logging_writer.py:48] [149900] global_step=149900, grad_norm=6.874208927154541, loss=1.2239596843719482
I0307 18:46:19.532626 140071458023168 logging_writer.py:48] [150000] global_step=150000, grad_norm=6.602407455444336, loss=1.1162426471710205
I0307 18:46:57.846339 140071466415872 logging_writer.py:48] [150100] global_step=150100, grad_norm=7.018804550170898, loss=1.1894580125808716
2025-03-07 18:47:01.244068: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:47:36.692750 140071458023168 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.879103183746338, loss=1.2673054933547974
I0307 18:48:14.848325 140071466415872 logging_writer.py:48] [150300] global_step=150300, grad_norm=6.052695274353027, loss=1.1018000841140747
I0307 18:48:53.287775 140071458023168 logging_writer.py:48] [150400] global_step=150400, grad_norm=6.879291534423828, loss=1.171933889389038
I0307 18:49:31.552980 140071466415872 logging_writer.py:48] [150500] global_step=150500, grad_norm=6.206824779510498, loss=1.165025234222412
I0307 18:50:10.188978 140071458023168 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.1790547370910645, loss=1.1303199529647827
I0307 18:50:48.754371 140071466415872 logging_writer.py:48] [150700] global_step=150700, grad_norm=7.862865924835205, loss=1.2238478660583496
I0307 18:51:26.229127 140226914178240 spec.py:321] Evaluating on the training split.
I0307 18:51:38.110214 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 18:51:56.622093 140226914178240 spec.py:349] Evaluating on the test split.
I0307 18:51:58.374493 140226914178240 submission_runner.py:469] Time since start: 62774.61s, 	Step: 150798, 	{'train/accuracy': 0.8434311151504517, 'train/loss': 0.5761180520057678, 'validation/accuracy': 0.7266199588775635, 'validation/loss': 1.0990161895751953, 'validation/num_examples': 50000, 'test/accuracy': 0.6021000146865845, 'test/loss': 1.8024053573608398, 'test/num_examples': 10000, 'score': 58695.49927163124, 'total_duration': 62774.60535979271, 'accumulated_submission_time': 58695.49927163124, 'accumulated_eval_time': 4046.6512253284454, 'accumulated_logging_time': 16.888478755950928}
I0307 18:51:58.484481 140071458023168 logging_writer.py:48] [150798] accumulated_eval_time=4046.65, accumulated_logging_time=16.8885, accumulated_submission_time=58695.5, global_step=150798, preemption_count=0, score=58695.5, test/accuracy=0.6021, test/loss=1.80241, test/num_examples=10000, total_duration=62774.6, train/accuracy=0.843431, train/loss=0.576118, validation/accuracy=0.72662, validation/loss=1.09902, validation/num_examples=50000
I0307 18:51:59.689971 140071466415872 logging_writer.py:48] [150800] global_step=150800, grad_norm=7.459142684936523, loss=1.1238616704940796
I0307 18:52:38.169666 140071458023168 logging_writer.py:48] [150900] global_step=150900, grad_norm=6.74677848815918, loss=1.1724915504455566
I0307 18:53:17.085636 140071466415872 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.974541664123535, loss=1.053666114807129
I0307 18:53:55.764752 140071458023168 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.44118070602417, loss=1.120333194732666
I0307 18:54:34.545619 140071466415872 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.606346130371094, loss=1.1269947290420532
I0307 18:55:13.220886 140071458023168 logging_writer.py:48] [151300] global_step=151300, grad_norm=6.497806072235107, loss=1.1155390739440918
2025-03-07 18:55:36.280518: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:55:51.838867 140071466415872 logging_writer.py:48] [151400] global_step=151400, grad_norm=7.554688453674316, loss=1.1858875751495361
I0307 18:56:30.406296 140071458023168 logging_writer.py:48] [151500] global_step=151500, grad_norm=7.3669610023498535, loss=1.1166188716888428
I0307 18:57:08.948531 140071466415872 logging_writer.py:48] [151600] global_step=151600, grad_norm=6.947900295257568, loss=1.18019700050354
I0307 18:57:47.589401 140071458023168 logging_writer.py:48] [151700] global_step=151700, grad_norm=7.0830206871032715, loss=1.101070761680603
I0307 18:58:26.197034 140071466415872 logging_writer.py:48] [151800] global_step=151800, grad_norm=7.061875343322754, loss=1.1795917749404907
I0307 18:59:05.117024 140071458023168 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.398414134979248, loss=1.1633884906768799
I0307 18:59:44.264867 140071466415872 logging_writer.py:48] [152000] global_step=152000, grad_norm=6.721769332885742, loss=1.0694115161895752
I0307 19:00:22.982074 140071458023168 logging_writer.py:48] [152100] global_step=152100, grad_norm=6.65804386138916, loss=1.1211743354797363
I0307 19:00:28.760833 140226914178240 spec.py:321] Evaluating on the training split.
I0307 19:00:40.401306 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 19:00:58.907615 140226914178240 spec.py:349] Evaluating on the test split.
I0307 19:01:00.666836 140226914178240 submission_runner.py:469] Time since start: 63316.90s, 	Step: 152116, 	{'train/accuracy': 0.8560267686843872, 'train/loss': 0.5252457857131958, 'validation/accuracy': 0.7270199656486511, 'validation/loss': 1.0983662605285645, 'validation/num_examples': 50000, 'test/accuracy': 0.6023000478744507, 'test/loss': 1.7850979566574097, 'test/num_examples': 10000, 'score': 59205.57086324692, 'total_duration': 63316.897686481476, 'accumulated_submission_time': 59205.57086324692, 'accumulated_eval_time': 4078.55708861351, 'accumulated_logging_time': 17.061365842819214}
I0307 19:01:00.764085 140071466415872 logging_writer.py:48] [152116] accumulated_eval_time=4078.56, accumulated_logging_time=17.0614, accumulated_submission_time=59205.6, global_step=152116, preemption_count=0, score=59205.6, test/accuracy=0.6023, test/loss=1.7851, test/num_examples=10000, total_duration=63316.9, train/accuracy=0.856027, train/loss=0.525246, validation/accuracy=0.72702, validation/loss=1.09837, validation/num_examples=50000
I0307 19:01:33.491361 140071458023168 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.896430015563965, loss=1.1524794101715088
I0307 19:02:12.292805 140071466415872 logging_writer.py:48] [152300] global_step=152300, grad_norm=6.398616313934326, loss=1.0637781620025635
I0307 19:02:51.041848 140071458023168 logging_writer.py:48] [152400] global_step=152400, grad_norm=6.854106903076172, loss=1.110253095626831
I0307 19:03:29.661348 140071466415872 logging_writer.py:48] [152500] global_step=152500, grad_norm=7.911194801330566, loss=1.0722768306732178
I0307 19:04:08.323407 140071458023168 logging_writer.py:48] [152600] global_step=152600, grad_norm=7.623897552490234, loss=1.2907078266143799
2025-03-07 19:04:12.774959: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:04:47.166245 140071466415872 logging_writer.py:48] [152700] global_step=152700, grad_norm=7.128138065338135, loss=1.0703588724136353
I0307 19:05:25.675297 140071458023168 logging_writer.py:48] [152800] global_step=152800, grad_norm=7.573972702026367, loss=1.128682017326355
I0307 19:06:03.958534 140071466415872 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.331015586853027, loss=1.1713833808898926
I0307 19:06:42.694728 140071458023168 logging_writer.py:48] [153000] global_step=153000, grad_norm=8.050604820251465, loss=1.1918257474899292
I0307 19:07:21.232668 140071466415872 logging_writer.py:48] [153100] global_step=153100, grad_norm=7.156939506530762, loss=1.1336065530776978
I0307 19:08:00.233615 140071458023168 logging_writer.py:48] [153200] global_step=153200, grad_norm=7.270766258239746, loss=1.1189461946487427
I0307 19:08:39.209616 140071466415872 logging_writer.py:48] [153300] global_step=153300, grad_norm=6.621957302093506, loss=1.118902325630188
I0307 19:09:17.426001 140071458023168 logging_writer.py:48] [153400] global_step=153400, grad_norm=7.461286544799805, loss=1.0086379051208496
I0307 19:09:30.969023 140226914178240 spec.py:321] Evaluating on the training split.
I0307 19:09:42.767773 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 19:09:59.631714 140226914178240 spec.py:349] Evaluating on the test split.
I0307 19:10:01.368400 140226914178240 submission_runner.py:469] Time since start: 63857.60s, 	Step: 153436, 	{'train/accuracy': 0.8547512292861938, 'train/loss': 0.5223726630210876, 'validation/accuracy': 0.7273199558258057, 'validation/loss': 1.0890791416168213, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7827666997909546, 'test/num_examples': 10000, 'score': 59715.60297012329, 'total_duration': 63857.59925246239, 'accumulated_submission_time': 59715.60297012329, 'accumulated_eval_time': 4108.9563164711, 'accumulated_logging_time': 17.186180114746094}
I0307 19:10:01.485096 140071466415872 logging_writer.py:48] [153436] accumulated_eval_time=4108.96, accumulated_logging_time=17.1862, accumulated_submission_time=59715.6, global_step=153436, preemption_count=0, score=59715.6, test/accuracy=0.6006, test/loss=1.78277, test/num_examples=10000, total_duration=63857.6, train/accuracy=0.854751, train/loss=0.522373, validation/accuracy=0.72732, validation/loss=1.08908, validation/num_examples=50000
I0307 19:10:26.579784 140071458023168 logging_writer.py:48] [153500] global_step=153500, grad_norm=6.886593818664551, loss=1.0562688112258911
I0307 19:11:05.390077 140071466415872 logging_writer.py:48] [153600] global_step=153600, grad_norm=7.141622066497803, loss=1.1247141361236572
I0307 19:11:44.592882 140071458023168 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.613651752471924, loss=1.12186861038208
I0307 19:12:23.645822 140071466415872 logging_writer.py:48] [153800] global_step=153800, grad_norm=6.874948501586914, loss=1.1108455657958984
2025-03-07 19:12:47.911302: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:13:02.581117 140071458023168 logging_writer.py:48] [153900] global_step=153900, grad_norm=6.426705360412598, loss=1.0752290487289429
I0307 19:13:41.512141 140071466415872 logging_writer.py:48] [154000] global_step=154000, grad_norm=7.025485515594482, loss=1.055812120437622
I0307 19:14:20.403769 140071458023168 logging_writer.py:48] [154100] global_step=154100, grad_norm=7.515392780303955, loss=1.1447834968566895
I0307 19:14:59.262190 140071466415872 logging_writer.py:48] [154200] global_step=154200, grad_norm=7.485795497894287, loss=1.0625028610229492
I0307 19:15:38.237481 140071458023168 logging_writer.py:48] [154300] global_step=154300, grad_norm=7.078350067138672, loss=1.0952138900756836
I0307 19:16:17.265728 140071466415872 logging_writer.py:48] [154400] global_step=154400, grad_norm=7.4746012687683105, loss=1.1316168308258057
I0307 19:16:56.208176 140071458023168 logging_writer.py:48] [154500] global_step=154500, grad_norm=6.436975002288818, loss=1.0159180164337158
I0307 19:17:35.110627 140071466415872 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.958174228668213, loss=1.0815316438674927
I0307 19:18:13.847293 140071458023168 logging_writer.py:48] [154700] global_step=154700, grad_norm=6.9273200035095215, loss=1.0474159717559814
I0307 19:18:31.555776 140226914178240 spec.py:321] Evaluating on the training split.
I0307 19:18:43.266071 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 19:19:03.093069 140226914178240 spec.py:349] Evaluating on the test split.
I0307 19:19:04.845858 140226914178240 submission_runner.py:469] Time since start: 64401.08s, 	Step: 154747, 	{'train/accuracy': 0.8681241869926453, 'train/loss': 0.4773489832878113, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.0811139345169067, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7728581428527832, 'test/num_examples': 10000, 'score': 60225.4982881546, 'total_duration': 64401.07671260834, 'accumulated_submission_time': 60225.4982881546, 'accumulated_eval_time': 4142.246251106262, 'accumulated_logging_time': 17.333640336990356}
I0307 19:19:05.001574 140071466415872 logging_writer.py:48] [154747] accumulated_eval_time=4142.25, accumulated_logging_time=17.3336, accumulated_submission_time=60225.5, global_step=154747, preemption_count=0, score=60225.5, test/accuracy=0.6006, test/loss=1.77286, test/num_examples=10000, total_duration=64401.1, train/accuracy=0.868124, train/loss=0.477349, validation/accuracy=0.73012, validation/loss=1.08111, validation/num_examples=50000
I0307 19:19:25.759232 140071458023168 logging_writer.py:48] [154800] global_step=154800, grad_norm=7.057705879211426, loss=1.1078941822052002
I0307 19:20:04.586035 140071466415872 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.73275089263916, loss=1.0288199186325073
I0307 19:20:43.475213 140071458023168 logging_writer.py:48] [155000] global_step=155000, grad_norm=7.691376209259033, loss=1.0294386148452759
I0307 19:21:22.531191 140071466415872 logging_writer.py:48] [155100] global_step=155100, grad_norm=7.353665828704834, loss=1.1337424516677856
2025-03-07 19:21:27.870279: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:22:01.619920 140071458023168 logging_writer.py:48] [155200] global_step=155200, grad_norm=7.5996832847595215, loss=1.1246411800384521
I0307 19:22:40.750127 140071466415872 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.379605770111084, loss=1.0296026468276978
I0307 19:23:19.493004 140071458023168 logging_writer.py:48] [155400] global_step=155400, grad_norm=7.442257404327393, loss=1.152494192123413
I0307 19:23:58.418168 140071466415872 logging_writer.py:48] [155500] global_step=155500, grad_norm=7.500644683837891, loss=1.064497470855713
I0307 19:24:36.810150 140071458023168 logging_writer.py:48] [155600] global_step=155600, grad_norm=6.954253196716309, loss=1.1240335702896118
I0307 19:25:15.517359 140071466415872 logging_writer.py:48] [155700] global_step=155700, grad_norm=7.76998233795166, loss=1.1016992330551147
I0307 19:25:54.470082 140071458023168 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.97792387008667, loss=1.0722194910049438
I0307 19:26:33.671707 140071466415872 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.410775184631348, loss=1.087233304977417
I0307 19:27:12.447509 140071458023168 logging_writer.py:48] [156000] global_step=156000, grad_norm=7.001265048980713, loss=1.0180069208145142
I0307 19:27:35.080715 140226914178240 spec.py:321] Evaluating on the training split.
I0307 19:27:47.013494 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 19:28:05.724701 140226914178240 spec.py:349] Evaluating on the test split.
I0307 19:28:07.457904 140226914178240 submission_runner.py:469] Time since start: 64943.69s, 	Step: 156059, 	{'train/accuracy': 0.8676658272743225, 'train/loss': 0.4702610671520233, 'validation/accuracy': 0.7339000105857849, 'validation/loss': 1.0702953338623047, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.7554395198822021, 'test/num_examples': 10000, 'score': 60735.35158538818, 'total_duration': 64943.68873047829, 'accumulated_submission_time': 60735.35158538818, 'accumulated_eval_time': 4174.623281478882, 'accumulated_logging_time': 17.559131860733032}
I0307 19:28:07.532428 140071466415872 logging_writer.py:48] [156059] accumulated_eval_time=4174.62, accumulated_logging_time=17.5591, accumulated_submission_time=60735.4, global_step=156059, preemption_count=0, score=60735.4, test/accuracy=0.6108, test/loss=1.75544, test/num_examples=10000, total_duration=64943.7, train/accuracy=0.867666, train/loss=0.470261, validation/accuracy=0.7339, validation/loss=1.0703, validation/num_examples=50000
I0307 19:28:23.739295 140071458023168 logging_writer.py:48] [156100] global_step=156100, grad_norm=7.150286674499512, loss=1.0715217590332031
I0307 19:29:02.747895 140071466415872 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.141890048980713, loss=1.080173373222351
I0307 19:29:41.961470 140071458023168 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.043105602264404, loss=1.0777571201324463
2025-03-07 19:30:07.286604: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:30:21.225496 140071466415872 logging_writer.py:48] [156400] global_step=156400, grad_norm=7.2465996742248535, loss=1.0269684791564941
I0307 19:31:00.503786 140071458023168 logging_writer.py:48] [156500] global_step=156500, grad_norm=8.105298042297363, loss=1.0845502614974976
I0307 19:31:39.076321 140071466415872 logging_writer.py:48] [156600] global_step=156600, grad_norm=7.176896095275879, loss=1.1425769329071045
I0307 19:32:17.999203 140071458023168 logging_writer.py:48] [156700] global_step=156700, grad_norm=7.2470245361328125, loss=1.1577025651931763
I0307 19:32:56.956223 140071466415872 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.745017051696777, loss=0.9671804904937744
I0307 19:33:35.821574 140071458023168 logging_writer.py:48] [156900] global_step=156900, grad_norm=7.613349914550781, loss=1.200250506401062
I0307 19:34:14.872215 140071466415872 logging_writer.py:48] [157000] global_step=157000, grad_norm=7.794254302978516, loss=1.1088314056396484
I0307 19:34:53.680132 140071458023168 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.040083408355713, loss=0.9950296878814697
I0307 19:35:32.749756 140071466415872 logging_writer.py:48] [157200] global_step=157200, grad_norm=8.141180992126465, loss=1.0245349407196045
I0307 19:36:11.796254 140071458023168 logging_writer.py:48] [157300] global_step=157300, grad_norm=7.808540344238281, loss=1.0013182163238525
I0307 19:36:37.820798 140226914178240 spec.py:321] Evaluating on the training split.
I0307 19:36:49.479441 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 19:37:09.183971 140226914178240 spec.py:349] Evaluating on the test split.
I0307 19:37:10.937136 140226914178240 submission_runner.py:469] Time since start: 65487.17s, 	Step: 157367, 	{'train/accuracy': 0.8715720772743225, 'train/loss': 0.46182703971862793, 'validation/accuracy': 0.732979953289032, 'validation/loss': 1.0705711841583252, 'validation/num_examples': 50000, 'test/accuracy': 0.6112000346183777, 'test/loss': 1.7597529888153076, 'test/num_examples': 10000, 'score': 61245.39450287819, 'total_duration': 65487.16800689697, 'accumulated_submission_time': 61245.39450287819, 'accumulated_eval_time': 4207.739497184753, 'accumulated_logging_time': 17.73732018470764}
I0307 19:37:11.037753 140071466415872 logging_writer.py:48] [157367] accumulated_eval_time=4207.74, accumulated_logging_time=17.7373, accumulated_submission_time=61245.4, global_step=157367, preemption_count=0, score=61245.4, test/accuracy=0.6112, test/loss=1.75975, test/num_examples=10000, total_duration=65487.2, train/accuracy=0.871572, train/loss=0.461827, validation/accuracy=0.73298, validation/loss=1.07057, validation/num_examples=50000
I0307 19:37:24.360080 140071458023168 logging_writer.py:48] [157400] global_step=157400, grad_norm=7.48107385635376, loss=1.1016054153442383
I0307 19:38:03.641507 140071466415872 logging_writer.py:48] [157500] global_step=157500, grad_norm=7.931681156158447, loss=1.0867661237716675
I0307 19:38:42.161784 140071458023168 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.456459045410156, loss=1.074791669845581
2025-03-07 19:38:48.222314: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:39:21.182665 140071466415872 logging_writer.py:48] [157700] global_step=157700, grad_norm=8.069944381713867, loss=0.9969274401664734
I0307 19:40:00.080412 140071458023168 logging_writer.py:48] [157800] global_step=157800, grad_norm=7.556106090545654, loss=1.0534623861312866
I0307 19:40:38.834837 140071466415872 logging_writer.py:48] [157900] global_step=157900, grad_norm=7.370234966278076, loss=1.0920907258987427
I0307 19:41:17.779030 140071458023168 logging_writer.py:48] [158000] global_step=158000, grad_norm=7.4722490310668945, loss=1.205484390258789
I0307 19:41:56.510585 140071466415872 logging_writer.py:48] [158100] global_step=158100, grad_norm=6.7923054695129395, loss=1.0816682577133179
I0307 19:42:35.614914 140071458023168 logging_writer.py:48] [158200] global_step=158200, grad_norm=8.956912994384766, loss=1.142952799797058
I0307 19:43:14.561162 140071466415872 logging_writer.py:48] [158300] global_step=158300, grad_norm=7.466375350952148, loss=1.0106639862060547
I0307 19:43:53.516234 140071458023168 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.3373284339904785, loss=1.150000810623169
I0307 19:44:32.487687 140071466415872 logging_writer.py:48] [158500] global_step=158500, grad_norm=7.633949279785156, loss=1.0257339477539062
I0307 19:45:11.395516 140071458023168 logging_writer.py:48] [158600] global_step=158600, grad_norm=7.361889839172363, loss=0.9889401197433472
I0307 19:45:41.109267 140226914178240 spec.py:321] Evaluating on the training split.
I0307 19:45:52.999116 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 19:46:13.938427 140226914178240 spec.py:349] Evaluating on the test split.
I0307 19:46:15.705867 140226914178240 submission_runner.py:469] Time since start: 66031.94s, 	Step: 158677, 	{'train/accuracy': 0.8761559128761292, 'train/loss': 0.44003790616989136, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.0588594675064087, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7587109804153442, 'test/num_examples': 10000, 'score': 61755.276366233826, 'total_duration': 66031.93673563004, 'accumulated_submission_time': 61755.276366233826, 'accumulated_eval_time': 4242.335964679718, 'accumulated_logging_time': 17.886207818984985}
I0307 19:46:15.822118 140071466415872 logging_writer.py:48] [158677] accumulated_eval_time=4242.34, accumulated_logging_time=17.8862, accumulated_submission_time=61755.3, global_step=158677, preemption_count=0, score=61755.3, test/accuracy=0.6101, test/loss=1.75871, test/num_examples=10000, total_duration=66031.9, train/accuracy=0.876156, train/loss=0.440038, validation/accuracy=0.73584, validation/loss=1.05886, validation/num_examples=50000
I0307 19:46:25.046285 140071458023168 logging_writer.py:48] [158700] global_step=158700, grad_norm=7.602435111999512, loss=1.1051552295684814
I0307 19:47:03.594009 140071466415872 logging_writer.py:48] [158800] global_step=158800, grad_norm=8.186319351196289, loss=1.07477605342865
2025-03-07 19:47:29.654645: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:47:42.423738 140071458023168 logging_writer.py:48] [158900] global_step=158900, grad_norm=7.9573469161987305, loss=1.0806571245193481
I0307 19:48:21.359451 140071466415872 logging_writer.py:48] [159000] global_step=159000, grad_norm=8.03559684753418, loss=1.100817084312439
I0307 19:49:00.230195 140071458023168 logging_writer.py:48] [159100] global_step=159100, grad_norm=8.096609115600586, loss=1.1193897724151611
I0307 19:49:38.847013 140071466415872 logging_writer.py:48] [159200] global_step=159200, grad_norm=8.508722305297852, loss=1.0491684675216675
I0307 19:50:17.489341 140071458023168 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.486184597015381, loss=1.0943171977996826
I0307 19:50:56.135599 140071466415872 logging_writer.py:48] [159400] global_step=159400, grad_norm=9.89599323272705, loss=1.0938730239868164
I0307 19:51:35.032490 140071458023168 logging_writer.py:48] [159500] global_step=159500, grad_norm=7.683926582336426, loss=0.9901029467582703
I0307 19:52:13.913526 140071466415872 logging_writer.py:48] [159600] global_step=159600, grad_norm=7.392970085144043, loss=1.0997331142425537
I0307 19:52:52.841474 140071458023168 logging_writer.py:48] [159700] global_step=159700, grad_norm=7.95052433013916, loss=1.0933880805969238
I0307 19:53:31.641949 140071466415872 logging_writer.py:48] [159800] global_step=159800, grad_norm=7.712487697601318, loss=1.0123878717422485
I0307 19:54:10.442341 140071458023168 logging_writer.py:48] [159900] global_step=159900, grad_norm=7.362121105194092, loss=1.0527048110961914
I0307 19:54:45.905899 140226914178240 spec.py:321] Evaluating on the training split.
I0307 19:54:57.678546 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 19:55:15.896793 140226914178240 spec.py:349] Evaluating on the test split.
I0307 19:55:17.642808 140226914178240 submission_runner.py:469] Time since start: 66573.87s, 	Step: 159992, 	{'train/accuracy': 0.8791055083274841, 'train/loss': 0.4338916838169098, 'validation/accuracy': 0.7372599840164185, 'validation/loss': 1.0545680522918701, 'validation/num_examples': 50000, 'test/accuracy': 0.6192000508308411, 'test/loss': 1.734291434288025, 'test/num_examples': 10000, 'score': 62265.19534754753, 'total_duration': 66573.87365245819, 'accumulated_submission_time': 62265.19534754753, 'accumulated_eval_time': 4274.072719573975, 'accumulated_logging_time': 18.02484917640686}
I0307 19:55:17.797162 140071466415872 logging_writer.py:48] [159992] accumulated_eval_time=4274.07, accumulated_logging_time=18.0248, accumulated_submission_time=62265.2, global_step=159992, preemption_count=0, score=62265.2, test/accuracy=0.6192, test/loss=1.73429, test/num_examples=10000, total_duration=66573.9, train/accuracy=0.879106, train/loss=0.433892, validation/accuracy=0.73726, validation/loss=1.05457, validation/num_examples=50000
I0307 19:55:21.441899 140071458023168 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.634987831115723, loss=1.0775140523910522
I0307 19:56:00.099856 140071466415872 logging_writer.py:48] [160100] global_step=160100, grad_norm=7.798384189605713, loss=1.0120394229888916
2025-03-07 19:56:06.901233: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:56:38.927600 140071458023168 logging_writer.py:48] [160200] global_step=160200, grad_norm=7.935416221618652, loss=1.0802556276321411
I0307 19:57:17.542627 140071466415872 logging_writer.py:48] [160300] global_step=160300, grad_norm=8.099920272827148, loss=1.0502688884735107
I0307 19:57:56.384507 140071458023168 logging_writer.py:48] [160400] global_step=160400, grad_norm=8.442520141601562, loss=1.0745717287063599
I0307 19:58:35.356583 140071466415872 logging_writer.py:48] [160500] global_step=160500, grad_norm=7.6186723709106445, loss=0.9893907308578491
I0307 19:59:14.220620 140071458023168 logging_writer.py:48] [160600] global_step=160600, grad_norm=8.026360511779785, loss=1.036641001701355
I0307 19:59:53.031180 140071466415872 logging_writer.py:48] [160700] global_step=160700, grad_norm=8.15102481842041, loss=0.952958881855011
I0307 20:00:31.759375 140071458023168 logging_writer.py:48] [160800] global_step=160800, grad_norm=7.8010382652282715, loss=1.089553713798523
I0307 20:01:10.827637 140071466415872 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.308155059814453, loss=0.9923548102378845
I0307 20:01:49.784297 140071458023168 logging_writer.py:48] [161000] global_step=161000, grad_norm=8.013697624206543, loss=1.0425723791122437
I0307 20:02:29.018428 140071466415872 logging_writer.py:48] [161100] global_step=161100, grad_norm=7.98445987701416, loss=1.0254530906677246
I0307 20:03:07.722433 140071458023168 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.769834995269775, loss=1.0539250373840332
I0307 20:03:46.366739 140071466415872 logging_writer.py:48] [161300] global_step=161300, grad_norm=8.113903045654297, loss=1.024814248085022
I0307 20:03:47.828187 140226914178240 spec.py:321] Evaluating on the training split.
I0307 20:03:59.754761 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 20:04:17.747690 140226914178240 spec.py:349] Evaluating on the test split.
I0307 20:04:19.493462 140226914178240 submission_runner.py:469] Time since start: 67115.72s, 	Step: 161305, 	{'train/accuracy': 0.8779097199440002, 'train/loss': 0.4337511658668518, 'validation/accuracy': 0.736579954624176, 'validation/loss': 1.0634077787399292, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.7591530084609985, 'test/num_examples': 10000, 'score': 62775.05073571205, 'total_duration': 67115.72429203987, 'accumulated_submission_time': 62775.05073571205, 'accumulated_eval_time': 4305.737823486328, 'accumulated_logging_time': 18.21061635017395}
I0307 20:04:19.569330 140071458023168 logging_writer.py:48] [161305] accumulated_eval_time=4305.74, accumulated_logging_time=18.2106, accumulated_submission_time=62775.1, global_step=161305, preemption_count=0, score=62775.1, test/accuracy=0.6149, test/loss=1.75915, test/num_examples=10000, total_duration=67115.7, train/accuracy=0.87791, train/loss=0.433751, validation/accuracy=0.73658, validation/loss=1.06341, validation/num_examples=50000
2025-03-07 20:04:44.664875: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:04:56.650362 140071466415872 logging_writer.py:48] [161400] global_step=161400, grad_norm=7.5806355476379395, loss=1.0072509050369263
I0307 20:05:35.670649 140071458023168 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.750660419464111, loss=1.1905994415283203
I0307 20:06:14.158429 140071466415872 logging_writer.py:48] [161600] global_step=161600, grad_norm=8.645557403564453, loss=1.0155311822891235
I0307 20:06:52.843335 140071458023168 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.8523383140563965, loss=0.9708869457244873
I0307 20:07:32.012443 140071466415872 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.551034927368164, loss=1.0198543071746826
I0307 20:08:11.319138 140071458023168 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.485469341278076, loss=1.037067174911499
I0307 20:08:49.897254 140071466415872 logging_writer.py:48] [162000] global_step=162000, grad_norm=7.5228095054626465, loss=0.916293203830719
I0307 20:09:28.673373 140071458023168 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.960795879364014, loss=1.0682291984558105
I0307 20:10:07.764248 140071466415872 logging_writer.py:48] [162200] global_step=162200, grad_norm=7.766276836395264, loss=1.0309181213378906
I0307 20:10:46.467926 140071458023168 logging_writer.py:48] [162300] global_step=162300, grad_norm=7.822150707244873, loss=1.0788078308105469
I0307 20:11:25.081796 140071466415872 logging_writer.py:48] [162400] global_step=162400, grad_norm=8.378901481628418, loss=1.0755891799926758
I0307 20:12:03.483798 140071458023168 logging_writer.py:48] [162500] global_step=162500, grad_norm=8.151196479797363, loss=1.094010353088379
I0307 20:12:42.022387 140071466415872 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.9625067710876465, loss=1.0311425924301147
I0307 20:12:49.844746 140226914178240 spec.py:321] Evaluating on the training split.
2025-03-07 20:12:50.084095: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:13:01.892645 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 20:13:20.057095 140226914178240 spec.py:349] Evaluating on the test split.
I0307 20:13:21.815305 140226914178240 submission_runner.py:469] Time since start: 67658.05s, 	Step: 162621, 	{'train/accuracy': 0.8905851244926453, 'train/loss': 0.39023658633232117, 'validation/accuracy': 0.7434399724006653, 'validation/loss': 1.03712797164917, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.719489574432373, 'test/num_examples': 10000, 'score': 63285.14891028404, 'total_duration': 67658.04615354538, 'accumulated_submission_time': 63285.14891028404, 'accumulated_eval_time': 4337.708229541779, 'accumulated_logging_time': 18.321906805038452}
I0307 20:13:21.934936 140071458023168 logging_writer.py:48] [162621] accumulated_eval_time=4337.71, accumulated_logging_time=18.3219, accumulated_submission_time=63285.1, global_step=162621, preemption_count=0, score=63285.1, test/accuracy=0.6228, test/loss=1.71949, test/num_examples=10000, total_duration=67658, train/accuracy=0.890585, train/loss=0.390237, validation/accuracy=0.74344, validation/loss=1.03713, validation/num_examples=50000
I0307 20:13:52.808761 140071466415872 logging_writer.py:48] [162700] global_step=162700, grad_norm=8.55605697631836, loss=0.9699670076370239
I0307 20:14:31.007602 140071458023168 logging_writer.py:48] [162800] global_step=162800, grad_norm=7.2445244789123535, loss=0.992220401763916
I0307 20:15:09.657860 140071466415872 logging_writer.py:48] [162900] global_step=162900, grad_norm=8.018026351928711, loss=0.9907164573669434
I0307 20:15:48.497148 140071458023168 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.966676712036133, loss=0.9785932898521423
I0307 20:16:27.087169 140071466415872 logging_writer.py:48] [163100] global_step=163100, grad_norm=8.135574340820312, loss=0.9937030076980591
I0307 20:17:05.507807 140071458023168 logging_writer.py:48] [163200] global_step=163200, grad_norm=7.662435054779053, loss=0.9205547571182251
I0307 20:17:44.050764 140071466415872 logging_writer.py:48] [163300] global_step=163300, grad_norm=8.699999809265137, loss=0.971598744392395
I0307 20:18:22.390624 140071458023168 logging_writer.py:48] [163400] global_step=163400, grad_norm=8.152055740356445, loss=1.0256502628326416
I0307 20:19:00.944311 140071466415872 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.313449859619141, loss=0.9536875486373901
I0307 20:19:39.675349 140071458023168 logging_writer.py:48] [163600] global_step=163600, grad_norm=8.706075668334961, loss=1.048897385597229
I0307 20:20:18.269824 140071466415872 logging_writer.py:48] [163700] global_step=163700, grad_norm=7.492115020751953, loss=0.9352477788925171
I0307 20:20:56.885831 140071458023168 logging_writer.py:48] [163800] global_step=163800, grad_norm=7.3551201820373535, loss=1.0262000560760498
2025-03-07 20:21:24.574824: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:21:35.624634 140071466415872 logging_writer.py:48] [163900] global_step=163900, grad_norm=7.312507629394531, loss=0.9142470359802246
I0307 20:21:52.089893 140226914178240 spec.py:321] Evaluating on the training split.
I0307 20:22:04.216563 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 20:22:21.536145 140226914178240 spec.py:349] Evaluating on the test split.
I0307 20:22:23.306198 140226914178240 submission_runner.py:469] Time since start: 68199.54s, 	Step: 163943, 	{'train/accuracy': 0.8871771097183228, 'train/loss': 0.3987239897251129, 'validation/accuracy': 0.7414599657058716, 'validation/loss': 1.0368437767028809, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.732204794883728, 'test/num_examples': 10000, 'score': 63795.12868666649, 'total_duration': 68199.53705382347, 'accumulated_submission_time': 63795.12868666649, 'accumulated_eval_time': 4368.924388885498, 'accumulated_logging_time': 18.475444078445435}
I0307 20:22:23.410222 140071458023168 logging_writer.py:48] [163943] accumulated_eval_time=4368.92, accumulated_logging_time=18.4754, accumulated_submission_time=63795.1, global_step=163943, preemption_count=0, score=63795.1, test/accuracy=0.6148, test/loss=1.7322, test/num_examples=10000, total_duration=68199.5, train/accuracy=0.887177, train/loss=0.398724, validation/accuracy=0.74146, validation/loss=1.03684, validation/num_examples=50000
I0307 20:22:45.940051 140071466415872 logging_writer.py:48] [164000] global_step=164000, grad_norm=8.533764839172363, loss=1.0419301986694336
I0307 20:23:24.694827 140071458023168 logging_writer.py:48] [164100] global_step=164100, grad_norm=7.524948596954346, loss=0.8870493173599243
I0307 20:24:03.556235 140071466415872 logging_writer.py:48] [164200] global_step=164200, grad_norm=7.698880195617676, loss=0.9336860775947571
I0307 20:24:42.043675 140071458023168 logging_writer.py:48] [164300] global_step=164300, grad_norm=7.774429798126221, loss=1.0114909410476685
I0307 20:25:20.529309 140071466415872 logging_writer.py:48] [164400] global_step=164400, grad_norm=8.563901901245117, loss=0.9479438662528992
I0307 20:25:58.872988 140071458023168 logging_writer.py:48] [164500] global_step=164500, grad_norm=8.014669418334961, loss=0.9495719075202942
I0307 20:26:37.394589 140071466415872 logging_writer.py:48] [164600] global_step=164600, grad_norm=8.487621307373047, loss=0.9394896030426025
I0307 20:27:16.282400 140071458023168 logging_writer.py:48] [164700] global_step=164700, grad_norm=8.149088859558105, loss=0.9524682760238647
I0307 20:27:54.824388 140071466415872 logging_writer.py:48] [164800] global_step=164800, grad_norm=8.599987983703613, loss=0.9284497499465942
I0307 20:28:33.535748 140071458023168 logging_writer.py:48] [164900] global_step=164900, grad_norm=8.101496696472168, loss=0.9999949336051941
I0307 20:29:12.065161 140071466415872 logging_writer.py:48] [165000] global_step=165000, grad_norm=8.023409843444824, loss=0.9184284210205078
I0307 20:29:50.390351 140071458023168 logging_writer.py:48] [165100] global_step=165100, grad_norm=9.415594100952148, loss=1.0062885284423828
2025-03-07 20:29:59.234753: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:30:29.202684 140071466415872 logging_writer.py:48] [165200] global_step=165200, grad_norm=7.8012471199035645, loss=0.8580040335655212
I0307 20:30:53.577190 140226914178240 spec.py:321] Evaluating on the training split.
I0307 20:31:05.330891 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 20:31:22.196852 140226914178240 spec.py:349] Evaluating on the test split.
I0307 20:31:23.948815 140226914178240 submission_runner.py:469] Time since start: 68740.18s, 	Step: 165264, 	{'train/accuracy': 0.8905652165412903, 'train/loss': 0.39370325207710266, 'validation/accuracy': 0.7463200092315674, 'validation/loss': 1.0265657901763916, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.7303334474563599, 'test/num_examples': 10000, 'score': 64305.104657649994, 'total_duration': 68740.17964315414, 'accumulated_submission_time': 64305.104657649994, 'accumulated_eval_time': 4399.2958562374115, 'accumulated_logging_time': 18.63028359413147}
I0307 20:31:24.046861 140071458023168 logging_writer.py:48] [165264] accumulated_eval_time=4399.3, accumulated_logging_time=18.6303, accumulated_submission_time=64305.1, global_step=165264, preemption_count=0, score=64305.1, test/accuracy=0.6184, test/loss=1.73033, test/num_examples=10000, total_duration=68740.2, train/accuracy=0.890565, train/loss=0.393703, validation/accuracy=0.74632, validation/loss=1.02657, validation/num_examples=50000
I0307 20:31:38.494123 140071466415872 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.741028785705566, loss=0.864525556564331
I0307 20:32:17.063575 140071458023168 logging_writer.py:48] [165400] global_step=165400, grad_norm=8.197813034057617, loss=0.9175248146057129
I0307 20:32:55.817682 140071466415872 logging_writer.py:48] [165500] global_step=165500, grad_norm=7.868075847625732, loss=0.93131023645401
I0307 20:33:34.964506 140071458023168 logging_writer.py:48] [165600] global_step=165600, grad_norm=7.746169567108154, loss=0.9911409616470337
I0307 20:34:13.780836 140071466415872 logging_writer.py:48] [165700] global_step=165700, grad_norm=7.9705915451049805, loss=1.0186588764190674
I0307 20:34:52.440304 140071458023168 logging_writer.py:48] [165800] global_step=165800, grad_norm=7.9832072257995605, loss=0.9647424221038818
I0307 20:35:30.789060 140071466415872 logging_writer.py:48] [165900] global_step=165900, grad_norm=7.896571636199951, loss=0.9875035285949707
I0307 20:36:09.523327 140071458023168 logging_writer.py:48] [166000] global_step=166000, grad_norm=8.10039234161377, loss=0.9905115962028503
I0307 20:36:48.353124 140071466415872 logging_writer.py:48] [166100] global_step=166100, grad_norm=8.344943046569824, loss=0.9154777526855469
I0307 20:37:27.128385 140071458023168 logging_writer.py:48] [166200] global_step=166200, grad_norm=8.71624755859375, loss=1.0036797523498535
I0307 20:38:05.796641 140071466415872 logging_writer.py:48] [166300] global_step=166300, grad_norm=8.466541290283203, loss=0.9883053302764893
2025-03-07 20:38:34.258285: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:38:44.794646 140071458023168 logging_writer.py:48] [166400] global_step=166400, grad_norm=7.888318061828613, loss=0.9618252515792847
I0307 20:39:23.591821 140071466415872 logging_writer.py:48] [166500] global_step=166500, grad_norm=7.563528537750244, loss=0.9098908305168152
I0307 20:39:53.976521 140226914178240 spec.py:321] Evaluating on the training split.
I0307 20:40:06.046160 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 20:40:25.044752 140226914178240 spec.py:349] Evaluating on the test split.
I0307 20:40:26.777942 140226914178240 submission_runner.py:469] Time since start: 69283.01s, 	Step: 166579, 	{'train/accuracy': 0.8924385905265808, 'train/loss': 0.3779086470603943, 'validation/accuracy': 0.7468599677085876, 'validation/loss': 1.0234558582305908, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.7122259140014648, 'test/num_examples': 10000, 'score': 64814.83959007263, 'total_duration': 69283.00878286362, 'accumulated_submission_time': 64814.83959007263, 'accumulated_eval_time': 4432.097115755081, 'accumulated_logging_time': 18.780981302261353}
I0307 20:40:26.939261 140071458023168 logging_writer.py:48] [166579] accumulated_eval_time=4432.1, accumulated_logging_time=18.781, accumulated_submission_time=64814.8, global_step=166579, preemption_count=0, score=64814.8, test/accuracy=0.6209, test/loss=1.71223, test/num_examples=10000, total_duration=69283, train/accuracy=0.892439, train/loss=0.377909, validation/accuracy=0.74686, validation/loss=1.02346, validation/num_examples=50000
I0307 20:40:35.392950 140071466415872 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.807213306427002, loss=1.0247963666915894
I0307 20:41:13.897768 140071458023168 logging_writer.py:48] [166700] global_step=166700, grad_norm=9.76378059387207, loss=0.9548876881599426
I0307 20:41:52.563601 140071466415872 logging_writer.py:48] [166800] global_step=166800, grad_norm=8.747773170471191, loss=1.0028057098388672
I0307 20:42:30.987699 140071458023168 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.929410934448242, loss=0.8950997591018677
I0307 20:43:09.757062 140071466415872 logging_writer.py:48] [167000] global_step=167000, grad_norm=7.567913055419922, loss=0.9837542176246643
I0307 20:43:48.352962 140071458023168 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.079863548278809, loss=0.9046221971511841
I0307 20:44:26.830457 140071466415872 logging_writer.py:48] [167200] global_step=167200, grad_norm=8.689105033874512, loss=0.9979754090309143
I0307 20:45:05.450865 140071458023168 logging_writer.py:48] [167300] global_step=167300, grad_norm=7.842156410217285, loss=0.8614189624786377
I0307 20:45:44.230304 140071466415872 logging_writer.py:48] [167400] global_step=167400, grad_norm=8.37948226928711, loss=0.950785756111145
I0307 20:46:22.532810 140071458023168 logging_writer.py:48] [167500] global_step=167500, grad_norm=8.41712474822998, loss=0.9924748539924622
I0307 20:47:00.966870 140071466415872 logging_writer.py:48] [167600] global_step=167600, grad_norm=7.877345085144043, loss=0.8851393461227417
2025-03-07 20:47:11.241796: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:47:39.583437 140071458023168 logging_writer.py:48] [167700] global_step=167700, grad_norm=8.199841499328613, loss=0.9660695195198059
I0307 20:48:17.997519 140071466415872 logging_writer.py:48] [167800] global_step=167800, grad_norm=9.000165939331055, loss=0.9200935363769531
I0307 20:48:56.850428 140226914178240 spec.py:321] Evaluating on the training split.
I0307 20:49:08.962962 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 20:49:25.125805 140226914178240 spec.py:349] Evaluating on the test split.
I0307 20:49:26.877813 140226914178240 submission_runner.py:469] Time since start: 69823.11s, 	Step: 167900, 	{'train/accuracy': 0.8959462642669678, 'train/loss': 0.3677658438682556, 'validation/accuracy': 0.7455399632453918, 'validation/loss': 1.025676965713501, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.7042292356491089, 'test/num_examples': 10000, 'score': 65324.58327507973, 'total_duration': 69823.10862851143, 'accumulated_submission_time': 65324.58327507973, 'accumulated_eval_time': 4462.124315023422, 'accumulated_logging_time': 18.968910694122314}
I0307 20:49:26.973667 140071458023168 logging_writer.py:48] [167900] accumulated_eval_time=4462.12, accumulated_logging_time=18.9689, accumulated_submission_time=65324.6, global_step=167900, preemption_count=0, score=65324.6, test/accuracy=0.624, test/loss=1.70423, test/num_examples=10000, total_duration=69823.1, train/accuracy=0.895946, train/loss=0.367766, validation/accuracy=0.74554, validation/loss=1.02568, validation/num_examples=50000
I0307 20:49:27.419025 140071466415872 logging_writer.py:48] [167900] global_step=167900, grad_norm=8.878405570983887, loss=1.0060105323791504
I0307 20:50:06.054830 140071458023168 logging_writer.py:48] [168000] global_step=168000, grad_norm=9.084718704223633, loss=0.9523277282714844
I0307 20:50:44.333038 140071466415872 logging_writer.py:48] [168100] global_step=168100, grad_norm=8.616345405578613, loss=0.9582220315933228
I0307 20:51:22.835751 140071458023168 logging_writer.py:48] [168200] global_step=168200, grad_norm=8.078883171081543, loss=0.9611709117889404
I0307 20:52:01.502858 140071466415872 logging_writer.py:48] [168300] global_step=168300, grad_norm=8.22829818725586, loss=0.9487859010696411
I0307 20:52:39.843460 140071458023168 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.306822299957275, loss=0.8315309882164001
I0307 20:53:18.739864 140071466415872 logging_writer.py:48] [168500] global_step=168500, grad_norm=8.66702651977539, loss=1.0127191543579102
I0307 20:53:57.450780 140071458023168 logging_writer.py:48] [168600] global_step=168600, grad_norm=8.709982872009277, loss=0.9495970606803894
I0307 20:54:35.921108 140071466415872 logging_writer.py:48] [168700] global_step=168700, grad_norm=8.357275009155273, loss=0.9410876035690308
I0307 20:55:14.330519 140071458023168 logging_writer.py:48] [168800] global_step=168800, grad_norm=8.77054214477539, loss=0.8465138673782349
2025-03-07 20:55:43.414846: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:55:52.757586 140071466415872 logging_writer.py:48] [168900] global_step=168900, grad_norm=7.510627746582031, loss=0.9252166748046875
I0307 20:56:31.346628 140071458023168 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.7914323806762695, loss=0.8991708755493164
I0307 20:57:09.697080 140071466415872 logging_writer.py:48] [169100] global_step=169100, grad_norm=7.825447082519531, loss=0.9274145364761353
I0307 20:57:48.558996 140071458023168 logging_writer.py:48] [169200] global_step=169200, grad_norm=7.840338706970215, loss=0.8542031645774841
I0307 20:57:57.029221 140226914178240 spec.py:321] Evaluating on the training split.
I0307 20:58:09.239232 140226914178240 spec.py:333] Evaluating on the validation split.
I0307 20:58:27.643298 140226914178240 spec.py:349] Evaluating on the test split.
I0307 20:58:29.402828 140226914178240 submission_runner.py:469] Time since start: 70365.63s, 	Step: 169223, 	{'train/accuracy': 0.9003108739852905, 'train/loss': 0.35501787066459656, 'validation/accuracy': 0.7491399645805359, 'validation/loss': 1.0223435163497925, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.7016139030456543, 'test/num_examples': 10000, 'score': 65834.42665338516, 'total_duration': 70365.63348674774, 'accumulated_submission_time': 65834.42665338516, 'accumulated_eval_time': 4494.497592926025, 'accumulated_logging_time': 19.13407063484192}
I0307 20:58:29.553667 140071466415872 logging_writer.py:48] [169223] accumulated_eval_time=4494.5, accumulated_logging_time=19.1341, accumulated_submission_time=65834.4, global_step=169223, preemption_count=0, score=65834.4, test/accuracy=0.6249, test/loss=1.70161, test/num_examples=10000, total_duration=70365.6, train/accuracy=0.900311, train/loss=0.355018, validation/accuracy=0.74914, validation/loss=1.02234, validation/num_examples=50000
I0307 20:58:59.538093 140071458023168 logging_writer.py:48] [169300] global_step=169300, grad_norm=8.832366943359375, loss=0.9324709177017212
I0307 20:59:37.676797 140071466415872 logging_writer.py:48] [169400] global_step=169400, grad_norm=8.122591972351074, loss=0.9788928627967834
I0307 21:00:15.948680 140071458023168 logging_writer.py:48] [169500] global_step=169500, grad_norm=8.067917823791504, loss=0.8845702409744263
I0307 21:00:54.656284 140071466415872 logging_writer.py:48] [169600] global_step=169600, grad_norm=8.242046356201172, loss=0.9693855047225952
I0307 21:01:32.926671 140071458023168 logging_writer.py:48] [169700] global_step=169700, grad_norm=8.127169609069824, loss=1.0178431272506714
I0307 21:02:11.427821 140071466415872 logging_writer.py:48] [169800] global_step=169800, grad_norm=8.570807456970215, loss=0.9512892961502075
I0307 21:02:50.022083 140071458023168 logging_writer.py:48] [169900] global_step=169900, grad_norm=9.160197257995605, loss=0.9874467849731445
I0307 21:03:28.749652 140071466415872 logging_writer.py:48] [170000] global_step=170000, grad_norm=9.099713325500488, loss=0.9193772077560425
I0307 21:04:07.240222 140071458023168 logging_writer.py:48] [170100] global_step=170100, grad_norm=8.537020683288574, loss=0.9072242379188538
2025-03-07 21:04:17.410896: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:04:45.669764 140071466415872 logging_writer.py:48] [170200] global_step=170200, grad_norm=8.349285125732422, loss=0.967750072479248
I0307 21:05:23.983764 140071458023168 logging_writer.py:48] [170300] global_step=170300, grad_norm=8.378937721252441, loss=0.9183191061019897
I0307 21:06:02.313051 140071466415872 logging_writer.py:48] [170400] global_step=170400, grad_norm=7.9439311027526855, loss=0.8755490779876709
I0307 21:06:40.727069 140071458023168 logging_writer.py:48] [170500] global_step=170500, grad_norm=8.205048561096191, loss=0.8901455998420715
I0307 21:06:59.503478 140071466415872 logging_writer.py:48] [170549] global_step=170549, preemption_count=0, score=66344.1
I0307 21:07:01.166871 140226914178240 submission_runner.py:646] Tuning trial 3/5
I0307 21:07:01.167035 140226914178240 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 21:07:01.170498 140226914178240 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.001195790828205645, 'train/loss': 6.909979343414307, 'validation/accuracy': 0.00107999995816499, 'validation/loss': 6.910284042358398, 'validation/num_examples': 50000, 'test/accuracy': 0.0017000001389533281, 'test/loss': 6.910176753997803, 'test/num_examples': 10000, 'score': 56.4568145275116, 'total_duration': 149.48430800437927, 'accumulated_submission_time': 56.4568145275116, 'accumulated_eval_time': 93.02730703353882, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1370, {'train/accuracy': 0.0615234375, 'train/loss': 5.465592384338379, 'validation/accuracy': 0.054919999092817307, 'validation/loss': 5.555846691131592, 'validation/num_examples': 50000, 'test/accuracy': 0.03960000351071358, 'test/loss': 5.773700714111328, 'test/num_examples': 10000, 'score': 566.3106379508972, 'total_duration': 694.6561579704285, 'accumulated_submission_time': 566.3106379508972, 'accumulated_eval_time': 128.1309769153595, 'accumulated_logging_time': 0.050501108169555664, 'global_step': 1370, 'preemption_count': 0}), (2717, {'train/accuracy': 0.15286192297935486, 'train/loss': 4.414518356323242, 'validation/accuracy': 0.1349799931049347, 'validation/loss': 4.573461532592773, 'validation/num_examples': 50000, 'test/accuracy': 0.09330000728368759, 'test/loss': 5.005978107452393, 'test/num_examples': 10000, 'score': 1076.203177690506, 'total_duration': 1240.0603058338165, 'accumulated_submission_time': 1076.203177690506, 'accumulated_eval_time': 163.4569754600525, 'accumulated_logging_time': 0.08935856819152832, 'global_step': 2717, 'preemption_count': 0}), (4053, {'train/accuracy': 0.2474290430545807, 'train/loss': 3.6621642112731934, 'validation/accuracy': 0.21581999957561493, 'validation/loss': 3.8797121047973633, 'validation/num_examples': 50000, 'test/accuracy': 0.16040000319480896, 'test/loss': 4.4392547607421875, 'test/num_examples': 10000, 'score': 1586.102175951004, 'total_duration': 1785.41330575943, 'accumulated_submission_time': 1586.102175951004, 'accumulated_eval_time': 198.7308385372162, 'accumulated_logging_time': 0.1478276252746582, 'global_step': 4053, 'preemption_count': 0}), (5393, {'train/accuracy': 0.3272082209587097, 'train/loss': 3.1125736236572266, 'validation/accuracy': 0.28915998339653015, 'validation/loss': 3.350515604019165, 'validation/num_examples': 50000, 'test/accuracy': 0.2079000025987625, 'test/loss': 3.984504461288452, 'test/num_examples': 10000, 'score': 2096.1438636779785, 'total_duration': 2329.0213050842285, 'accumulated_submission_time': 2096.1438636779785, 'accumulated_eval_time': 232.1579167842865, 'accumulated_logging_time': 0.16368651390075684, 'global_step': 5393, 'preemption_count': 0}), (6728, {'train/accuracy': 0.39100366830825806, 'train/loss': 2.7434937953948975, 'validation/accuracy': 0.35189998149871826, 'validation/loss': 2.989835023880005, 'validation/num_examples': 50000, 'test/accuracy': 0.2656000256538391, 'test/loss': 3.6864068508148193, 'test/num_examples': 10000, 'score': 2605.9788506031036, 'total_duration': 2871.6416597366333, 'accumulated_submission_time': 2605.9788506031036, 'accumulated_eval_time': 264.76834535598755, 'accumulated_logging_time': 0.22105741500854492, 'global_step': 6728, 'preemption_count': 0}), (8063, {'train/accuracy': 0.4448740482330322, 'train/loss': 2.421154737472534, 'validation/accuracy': 0.40591999888420105, 'validation/loss': 2.6750271320343018, 'validation/num_examples': 50000, 'test/accuracy': 0.30560001730918884, 'test/loss': 3.3933873176574707, 'test/num_examples': 10000, 'score': 3115.8826031684875, 'total_duration': 3417.5227279663086, 'accumulated_submission_time': 3115.8826031684875, 'accumulated_eval_time': 300.56809878349304, 'accumulated_logging_time': 0.2833092212677002, 'global_step': 8063, 'preemption_count': 0}), (9400, {'train/accuracy': 0.5049226880073547, 'train/loss': 2.1154611110687256, 'validation/accuracy': 0.46025997400283813, 'validation/loss': 2.3665738105773926, 'validation/num_examples': 50000, 'test/accuracy': 0.3505000174045563, 'test/loss': 3.085230588912964, 'test/num_examples': 10000, 'score': 3626.0122554302216, 'total_duration': 3963.9058706760406, 'accumulated_submission_time': 3626.0122554302216, 'accumulated_eval_time': 336.63484930992126, 'accumulated_logging_time': 0.353640079498291, 'global_step': 9400, 'preemption_count': 0}), (10668, {'train/accuracy': 0.5140505433082581, 'train/loss': 2.0655152797698975, 'validation/accuracy': 0.4705599844455719, 'validation/loss': 2.3184568881988525, 'validation/num_examples': 50000, 'test/accuracy': 0.35520002245903015, 'test/loss': 3.0856406688690186, 'test/num_examples': 10000, 'score': 4135.744189023972, 'total_duration': 4513.959764242172, 'accumulated_submission_time': 4135.744189023972, 'accumulated_eval_time': 376.67817425727844, 'accumulated_logging_time': 0.4847090244293213, 'global_step': 10668, 'preemption_count': 0}), (11999, {'train/accuracy': 0.5596699714660645, 'train/loss': 1.837249755859375, 'validation/accuracy': 0.5102399587631226, 'validation/loss': 2.1204795837402344, 'validation/num_examples': 50000, 'test/accuracy': 0.38860002160072327, 'test/loss': 2.864274740219116, 'test/num_examples': 10000, 'score': 4645.69776558876, 'total_duration': 5065.243762254715, 'accumulated_submission_time': 4645.69776558876, 'accumulated_eval_time': 417.79432249069214, 'accumulated_logging_time': 0.5691022872924805, 'global_step': 11999, 'preemption_count': 0}), (13318, {'train/accuracy': 0.5774872303009033, 'train/loss': 1.7372361421585083, 'validation/accuracy': 0.5238800048828125, 'validation/loss': 2.025557041168213, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.7279298305511475, 'test/num_examples': 10000, 'score': 5155.5356965065, 'total_duration': 5615.818393707275, 'accumulated_submission_time': 5155.5356965065, 'accumulated_eval_time': 458.0849578380585, 'accumulated_logging_time': 0.8801577091217041, 'global_step': 13318, 'preemption_count': 0}), (14647, {'train/accuracy': 0.5940887928009033, 'train/loss': 1.6770062446594238, 'validation/accuracy': 0.5428400039672852, 'validation/loss': 1.9536000490188599, 'validation/num_examples': 50000, 'test/accuracy': 0.41750001907348633, 'test/loss': 2.7149999141693115, 'test/num_examples': 10000, 'score': 5665.5519926548, 'total_duration': 6166.154253959656, 'accumulated_submission_time': 5665.5519926548, 'accumulated_eval_time': 498.07856822013855, 'accumulated_logging_time': 1.0771241188049316, 'global_step': 14647, 'preemption_count': 0}), (15976, {'train/accuracy': 0.5974569320678711, 'train/loss': 1.6548250913619995, 'validation/accuracy': 0.5475999712944031, 'validation/loss': 1.9200537204742432, 'validation/num_examples': 50000, 'test/accuracy': 0.43150001764297485, 'test/loss': 2.635930061340332, 'test/num_examples': 10000, 'score': 6175.442023515701, 'total_duration': 6722.430892705917, 'accumulated_submission_time': 6175.442023515701, 'accumulated_eval_time': 544.0917387008667, 'accumulated_logging_time': 1.3202722072601318, 'global_step': 15976, 'preemption_count': 0}), (17302, {'train/accuracy': 0.6120057106018066, 'train/loss': 1.5883805751800537, 'validation/accuracy': 0.5559399724006653, 'validation/loss': 1.879778504371643, 'validation/num_examples': 50000, 'test/accuracy': 0.4369000196456909, 'test/loss': 2.638759136199951, 'test/num_examples': 10000, 'score': 6685.105132341385, 'total_duration': 7271.113498210907, 'accumulated_submission_time': 6685.105132341385, 'accumulated_eval_time': 582.6278820037842, 'accumulated_logging_time': 1.678769826889038, 'global_step': 17302, 'preemption_count': 0}), (18629, {'train/accuracy': 0.6077407598495483, 'train/loss': 1.595623254776001, 'validation/accuracy': 0.5612199902534485, 'validation/loss': 1.8499449491500854, 'validation/num_examples': 50000, 'test/accuracy': 0.43060001730918884, 'test/loss': 2.594538927078247, 'test/num_examples': 10000, 'score': 7195.120244264603, 'total_duration': 7817.490381717682, 'accumulated_submission_time': 7195.120244264603, 'accumulated_eval_time': 618.7875738143921, 'accumulated_logging_time': 1.760805368423462, 'global_step': 18629, 'preemption_count': 0}), (19957, {'train/accuracy': 0.6183633208274841, 'train/loss': 1.564002513885498, 'validation/accuracy': 0.5699599981307983, 'validation/loss': 1.8115588426589966, 'validation/num_examples': 50000, 'test/accuracy': 0.4475000202655792, 'test/loss': 2.5293376445770264, 'test/num_examples': 10000, 'score': 7705.289101362228, 'total_duration': 8365.562325716019, 'accumulated_submission_time': 7705.289101362228, 'accumulated_eval_time': 656.4788012504578, 'accumulated_logging_time': 1.8531458377838135, 'global_step': 19957, 'preemption_count': 0}), (21284, {'train/accuracy': 0.6061463356018066, 'train/loss': 1.610395073890686, 'validation/accuracy': 0.5586599707603455, 'validation/loss': 1.877277135848999, 'validation/num_examples': 50000, 'test/accuracy': 0.4353000223636627, 'test/loss': 2.6211726665496826, 'test/num_examples': 10000, 'score': 8215.293786525726, 'total_duration': 8913.197173833847, 'accumulated_submission_time': 8215.293786525726, 'accumulated_eval_time': 693.8631854057312, 'accumulated_logging_time': 1.9765114784240723, 'global_step': 21284, 'preemption_count': 0}), (22615, {'train/accuracy': 0.6229273080825806, 'train/loss': 1.5311996936798096, 'validation/accuracy': 0.57669997215271, 'validation/loss': 1.7879992723464966, 'validation/num_examples': 50000, 'test/accuracy': 0.4555000364780426, 'test/loss': 2.546802043914795, 'test/num_examples': 10000, 'score': 8725.43916463852, 'total_duration': 9463.329138278961, 'accumulated_submission_time': 8725.43916463852, 'accumulated_eval_time': 733.6403560638428, 'accumulated_logging_time': 2.053323268890381, 'global_step': 22615, 'preemption_count': 0}), (23944, {'train/accuracy': 0.6232661008834839, 'train/loss': 1.5279592275619507, 'validation/accuracy': 0.5792399644851685, 'validation/loss': 1.7681270837783813, 'validation/num_examples': 50000, 'test/accuracy': 0.45320001244544983, 'test/loss': 2.5017197132110596, 'test/num_examples': 10000, 'score': 9235.230093240738, 'total_duration': 10012.078787088394, 'accumulated_submission_time': 9235.230093240738, 'accumulated_eval_time': 772.3834238052368, 'accumulated_logging_time': 2.144423484802246, 'global_step': 23944, 'preemption_count': 0}), (25269, {'train/accuracy': 0.6238440275192261, 'train/loss': 1.5286612510681152, 'validation/accuracy': 0.5754799842834473, 'validation/loss': 1.7961288690567017, 'validation/num_examples': 50000, 'test/accuracy': 0.45830002427101135, 'test/loss': 2.50943922996521, 'test/num_examples': 10000, 'score': 9745.036061286926, 'total_duration': 10563.452746152878, 'accumulated_submission_time': 9745.036061286926, 'accumulated_eval_time': 813.726686000824, 'accumulated_logging_time': 2.2400803565979004, 'global_step': 25269, 'preemption_count': 0}), (26597, {'train/accuracy': 0.6203364133834839, 'train/loss': 1.5316355228424072, 'validation/accuracy': 0.5739200115203857, 'validation/loss': 1.7856478691101074, 'validation/num_examples': 50000, 'test/accuracy': 0.456900030374527, 'test/loss': 2.4875850677490234, 'test/num_examples': 10000, 'score': 10254.811584949493, 'total_duration': 11112.890194416046, 'accumulated_submission_time': 10254.811584949493, 'accumulated_eval_time': 853.15105509758, 'accumulated_logging_time': 2.355114221572876, 'global_step': 26597, 'preemption_count': 0}), (27926, {'train/accuracy': 0.6326729655265808, 'train/loss': 1.4903630018234253, 'validation/accuracy': 0.5846199989318848, 'validation/loss': 1.7444636821746826, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.473721504211426, 'test/num_examples': 10000, 'score': 10764.65004658699, 'total_duration': 11663.659442186356, 'accumulated_submission_time': 10764.65004658699, 'accumulated_eval_time': 893.8211438655853, 'accumulated_logging_time': 2.4903788566589355, 'global_step': 27926, 'preemption_count': 0}), (29257, {'train/accuracy': 0.6325334906578064, 'train/loss': 1.4839988946914673, 'validation/accuracy': 0.5856199860572815, 'validation/loss': 1.7362080812454224, 'validation/num_examples': 50000, 'test/accuracy': 0.46480002999305725, 'test/loss': 2.4623210430145264, 'test/num_examples': 10000, 'score': 11274.75269126892, 'total_duration': 12215.183251619339, 'accumulated_submission_time': 11274.75269126892, 'accumulated_eval_time': 934.9853844642639, 'accumulated_logging_time': 2.6224234104156494, 'global_step': 29257, 'preemption_count': 0}), (30587, {'train/accuracy': 0.634187638759613, 'train/loss': 1.4777313470840454, 'validation/accuracy': 0.5841599702835083, 'validation/loss': 1.73117995262146, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.472856283187866, 'test/num_examples': 10000, 'score': 11784.496736764908, 'total_duration': 12765.502014160156, 'accumulated_submission_time': 11784.496736764908, 'accumulated_eval_time': 975.2840321063995, 'accumulated_logging_time': 2.7707552909851074, 'global_step': 30587, 'preemption_count': 0}), (31914, {'train/accuracy': 0.6349449753761292, 'train/loss': 1.470556378364563, 'validation/accuracy': 0.5889999866485596, 'validation/loss': 1.704036831855774, 'validation/num_examples': 50000, 'test/accuracy': 0.458700031042099, 'test/loss': 2.4663188457489014, 'test/num_examples': 10000, 'score': 12294.268924713135, 'total_duration': 13315.159407377243, 'accumulated_submission_time': 12294.268924713135, 'accumulated_eval_time': 1014.9126207828522, 'accumulated_logging_time': 2.898590087890625, 'global_step': 31914, 'preemption_count': 0}), (33239, {'train/accuracy': 0.6412029266357422, 'train/loss': 1.4462796449661255, 'validation/accuracy': 0.5975599884986877, 'validation/loss': 1.6847479343414307, 'validation/num_examples': 50000, 'test/accuracy': 0.4645000100135803, 'test/loss': 2.4038889408111572, 'test/num_examples': 10000, 'score': 12804.07841539383, 'total_duration': 13867.510663032532, 'accumulated_submission_time': 12804.07841539383, 'accumulated_eval_time': 1057.1949396133423, 'accumulated_logging_time': 3.0320885181427, 'global_step': 33239, 'preemption_count': 0}), (34565, {'train/accuracy': 0.6437938213348389, 'train/loss': 1.4214361906051636, 'validation/accuracy': 0.597819983959198, 'validation/loss': 1.6649647951126099, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3546218872070312, 'test/num_examples': 10000, 'score': 13314.004073619843, 'total_duration': 14417.695690870285, 'accumulated_submission_time': 13314.004073619843, 'accumulated_eval_time': 1097.2255036830902, 'accumulated_logging_time': 3.135422706604004, 'global_step': 34565, 'preemption_count': 0}), (35893, {'train/accuracy': 0.6417410373687744, 'train/loss': 1.4452682733535767, 'validation/accuracy': 0.5956599712371826, 'validation/loss': 1.6790549755096436, 'validation/num_examples': 50000, 'test/accuracy': 0.47140002250671387, 'test/loss': 2.424567699432373, 'test/num_examples': 10000, 'score': 13824.109894275665, 'total_duration': 14966.17204284668, 'accumulated_submission_time': 13824.109894275665, 'accumulated_eval_time': 1135.3580374717712, 'accumulated_logging_time': 3.244441270828247, 'global_step': 35893, 'preemption_count': 0}), (37223, {'train/accuracy': 0.6357820630073547, 'train/loss': 1.4769904613494873, 'validation/accuracy': 0.5919600129127502, 'validation/loss': 1.7029697895050049, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.4114253520965576, 'test/num_examples': 10000, 'score': 14334.019703149796, 'total_duration': 15510.788739204407, 'accumulated_submission_time': 14334.019703149796, 'accumulated_eval_time': 1169.7786898612976, 'accumulated_logging_time': 3.4008822441101074, 'global_step': 37223, 'preemption_count': 0}), (38549, {'train/accuracy': 0.6342474222183228, 'train/loss': 1.4742919206619263, 'validation/accuracy': 0.5970799922943115, 'validation/loss': 1.6918880939483643, 'validation/num_examples': 50000, 'test/accuracy': 0.47130003571510315, 'test/loss': 2.454254627227783, 'test/num_examples': 10000, 'score': 14843.79547548294, 'total_duration': 16058.24912905693, 'accumulated_submission_time': 14843.79547548294, 'accumulated_eval_time': 1207.226440668106, 'accumulated_logging_time': 3.509631633758545, 'global_step': 38549, 'preemption_count': 0}), (39875, {'train/accuracy': 0.6369578838348389, 'train/loss': 1.4644701480865479, 'validation/accuracy': 0.5958399772644043, 'validation/loss': 1.6818727254867554, 'validation/num_examples': 50000, 'test/accuracy': 0.46730002760887146, 'test/loss': 2.422297477722168, 'test/num_examples': 10000, 'score': 15353.945746660233, 'total_duration': 16601.87930560112, 'accumulated_submission_time': 15353.945746660233, 'accumulated_eval_time': 1240.4575395584106, 'accumulated_logging_time': 3.6294965744018555, 'global_step': 39875, 'preemption_count': 0}), (41203, {'train/accuracy': 0.6549545526504517, 'train/loss': 1.3819515705108643, 'validation/accuracy': 0.6101599931716919, 'validation/loss': 1.6125420331954956, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.3199193477630615, 'test/num_examples': 10000, 'score': 15863.868384838104, 'total_duration': 17149.865020275116, 'accumulated_submission_time': 15863.868384838104, 'accumulated_eval_time': 1278.1946349143982, 'accumulated_logging_time': 3.8246634006500244, 'global_step': 41203, 'preemption_count': 0}), (42532, {'train/accuracy': 0.6523038744926453, 'train/loss': 1.3780807256698608, 'validation/accuracy': 0.6062999963760376, 'validation/loss': 1.6266237497329712, 'validation/num_examples': 50000, 'test/accuracy': 0.4823000133037567, 'test/loss': 2.3270530700683594, 'test/num_examples': 10000, 'score': 16373.743404626846, 'total_duration': 17695.276176214218, 'accumulated_submission_time': 16373.743404626846, 'accumulated_eval_time': 1313.4977173805237, 'accumulated_logging_time': 3.9301509857177734, 'global_step': 42532, 'preemption_count': 0}), (43862, {'train/accuracy': 0.6377351880073547, 'train/loss': 1.4740387201309204, 'validation/accuracy': 0.5946399569511414, 'validation/loss': 1.698374629020691, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.415133237838745, 'test/num_examples': 10000, 'score': 16883.619343280792, 'total_duration': 18240.765509843826, 'accumulated_submission_time': 16883.619343280792, 'accumulated_eval_time': 1348.8472847938538, 'accumulated_logging_time': 4.0624213218688965, 'global_step': 43862, 'preemption_count': 0}), (45186, {'train/accuracy': 0.6428970098495483, 'train/loss': 1.4471725225448608, 'validation/accuracy': 0.5976600050926208, 'validation/loss': 1.676727294921875, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.388176202774048, 'test/num_examples': 10000, 'score': 17393.480707406998, 'total_duration': 18783.10109114647, 'accumulated_submission_time': 17393.480707406998, 'accumulated_eval_time': 1380.9968876838684, 'accumulated_logging_time': 4.258647918701172, 'global_step': 45186, 'preemption_count': 0}), (46511, {'train/accuracy': 0.6601961255073547, 'train/loss': 1.3506418466567993, 'validation/accuracy': 0.6137599945068359, 'validation/loss': 1.596824288368225, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.330554485321045, 'test/num_examples': 10000, 'score': 17903.584470272064, 'total_duration': 19331.47368788719, 'accumulated_submission_time': 17903.584470272064, 'accumulated_eval_time': 1419.0063767433167, 'accumulated_logging_time': 4.388972520828247, 'global_step': 46511, 'preemption_count': 0}), (47833, {'train/accuracy': 0.6516262888908386, 'train/loss': 1.397565245628357, 'validation/accuracy': 0.6050400137901306, 'validation/loss': 1.6304758787155151, 'validation/num_examples': 50000, 'test/accuracy': 0.4863000214099884, 'test/loss': 2.3185198307037354, 'test/num_examples': 10000, 'score': 18413.56734395027, 'total_duration': 19876.48544359207, 'accumulated_submission_time': 18413.56734395027, 'accumulated_eval_time': 1453.7678875923157, 'accumulated_logging_time': 4.52807354927063, 'global_step': 47833, 'preemption_count': 0}), (49160, {'train/accuracy': 0.6392099857330322, 'train/loss': 1.465406060218811, 'validation/accuracy': 0.5956199765205383, 'validation/loss': 1.685867190361023, 'validation/num_examples': 50000, 'test/accuracy': 0.4717000126838684, 'test/loss': 2.435591220855713, 'test/num_examples': 10000, 'score': 18923.516672849655, 'total_duration': 20428.022285938263, 'accumulated_submission_time': 18923.516672849655, 'accumulated_eval_time': 1495.1154696941376, 'accumulated_logging_time': 4.638911485671997, 'global_step': 49160, 'preemption_count': 0}), (50485, {'train/accuracy': 0.6545360088348389, 'train/loss': 1.3919093608856201, 'validation/accuracy': 0.6090799570083618, 'validation/loss': 1.623656153678894, 'validation/num_examples': 50000, 'test/accuracy': 0.48840001225471497, 'test/loss': 2.334744691848755, 'test/num_examples': 10000, 'score': 19433.63016796112, 'total_duration': 20977.911613941193, 'accumulated_submission_time': 19433.63016796112, 'accumulated_eval_time': 1534.6340951919556, 'accumulated_logging_time': 4.766113042831421, 'global_step': 50485, 'preemption_count': 0}), (51809, {'train/accuracy': 0.6561503410339355, 'train/loss': 1.3735103607177734, 'validation/accuracy': 0.6130200028419495, 'validation/loss': 1.6109559535980225, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3277089595794678, 'test/num_examples': 10000, 'score': 19943.454622030258, 'total_duration': 21519.445110559464, 'accumulated_submission_time': 19943.454622030258, 'accumulated_eval_time': 1566.1138343811035, 'accumulated_logging_time': 4.866304636001587, 'global_step': 51809, 'preemption_count': 0}), (53137, {'train/accuracy': 0.6599370241165161, 'train/loss': 1.3738901615142822, 'validation/accuracy': 0.6121799945831299, 'validation/loss': 1.6018799543380737, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.2950503826141357, 'test/num_examples': 10000, 'score': 20453.520030498505, 'total_duration': 22064.849479436874, 'accumulated_submission_time': 20453.520030498505, 'accumulated_eval_time': 1601.2248139381409, 'accumulated_logging_time': 4.9659788608551025, 'global_step': 53137, 'preemption_count': 0}), (54467, {'train/accuracy': 0.6570471525192261, 'train/loss': 1.3796769380569458, 'validation/accuracy': 0.6131399869918823, 'validation/loss': 1.6084030866622925, 'validation/num_examples': 50000, 'test/accuracy': 0.4846000373363495, 'test/loss': 2.328246831893921, 'test/num_examples': 10000, 'score': 20963.606187343597, 'total_duration': 22609.580502033234, 'accumulated_submission_time': 20963.606187343597, 'accumulated_eval_time': 1635.589267730713, 'accumulated_logging_time': 5.118034601211548, 'global_step': 54467, 'preemption_count': 0}), (55792, {'train/accuracy': 0.6514269709587097, 'train/loss': 1.3962053060531616, 'validation/accuracy': 0.6047599911689758, 'validation/loss': 1.643538236618042, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.375528335571289, 'test/num_examples': 10000, 'score': 21473.553010225296, 'total_duration': 23158.5870821476, 'accumulated_submission_time': 21473.553010225296, 'accumulated_eval_time': 1674.3125774860382, 'accumulated_logging_time': 5.3265180587768555, 'global_step': 55792, 'preemption_count': 0}), (57120, {'train/accuracy': 0.6553332209587097, 'train/loss': 1.3734850883483887, 'validation/accuracy': 0.6106799840927124, 'validation/loss': 1.6087285280227661, 'validation/num_examples': 50000, 'test/accuracy': 0.4814000129699707, 'test/loss': 2.3519668579101562, 'test/num_examples': 10000, 'score': 21983.369214773178, 'total_duration': 23703.480622291565, 'accumulated_submission_time': 21983.369214773178, 'accumulated_eval_time': 1709.038479566574, 'accumulated_logging_time': 5.549750328063965, 'global_step': 57120, 'preemption_count': 0}), (58445, {'train/accuracy': 0.6597576141357422, 'train/loss': 1.3577125072479248, 'validation/accuracy': 0.6099799871444702, 'validation/loss': 1.612249493598938, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.343747138977051, 'test/num_examples': 10000, 'score': 22493.105979204178, 'total_duration': 24247.226024389267, 'accumulated_submission_time': 22493.105979204178, 'accumulated_eval_time': 1742.772200345993, 'accumulated_logging_time': 5.697880268096924, 'global_step': 58445, 'preemption_count': 0}), (59768, {'train/accuracy': 0.6732102632522583, 'train/loss': 1.3036808967590332, 'validation/accuracy': 0.6162799596786499, 'validation/loss': 1.5882207155227661, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.287404775619507, 'test/num_examples': 10000, 'score': 23002.972229003906, 'total_duration': 24789.504794359207, 'accumulated_submission_time': 23002.972229003906, 'accumulated_eval_time': 1774.911370754242, 'accumulated_logging_time': 5.844316244125366, 'global_step': 59768, 'preemption_count': 0}), (61092, {'train/accuracy': 0.6982023119926453, 'train/loss': 1.1974247694015503, 'validation/accuracy': 0.6164000034332275, 'validation/loss': 1.5848145484924316, 'validation/num_examples': 50000, 'test/accuracy': 0.4921000301837921, 'test/loss': 2.334542989730835, 'test/num_examples': 10000, 'score': 23512.889858961105, 'total_duration': 25331.602241039276, 'accumulated_submission_time': 23512.889858961105, 'accumulated_eval_time': 1806.8402950763702, 'accumulated_logging_time': 5.964541673660278, 'global_step': 61092, 'preemption_count': 0}), (62412, {'train/accuracy': 0.6979631781578064, 'train/loss': 1.1662652492523193, 'validation/accuracy': 0.6166200041770935, 'validation/loss': 1.592406988143921, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.3402371406555176, 'test/num_examples': 10000, 'score': 24022.653463840485, 'total_duration': 25873.348039388657, 'accumulated_submission_time': 24022.653463840485, 'accumulated_eval_time': 1838.527425289154, 'accumulated_logging_time': 6.126060962677002, 'global_step': 62412, 'preemption_count': 0}), (63730, {'train/accuracy': 0.7108777165412903, 'train/loss': 1.1180082559585571, 'validation/accuracy': 0.6242799758911133, 'validation/loss': 1.555962324142456, 'validation/num_examples': 50000, 'test/accuracy': 0.49590003490448, 'test/loss': 2.2857937812805176, 'test/num_examples': 10000, 'score': 24532.78237581253, 'total_duration': 26418.240907907486, 'accumulated_submission_time': 24532.78237581253, 'accumulated_eval_time': 1873.0493907928467, 'accumulated_logging_time': 6.229785919189453, 'global_step': 63730, 'preemption_count': 0}), (65045, {'train/accuracy': 0.7110570669174194, 'train/loss': 1.1315356492996216, 'validation/accuracy': 0.6221199631690979, 'validation/loss': 1.5635194778442383, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.298199415206909, 'test/num_examples': 10000, 'score': 25042.58123755455, 'total_duration': 26959.270816087723, 'accumulated_submission_time': 25042.58123755455, 'accumulated_eval_time': 1903.8921644687653, 'accumulated_logging_time': 6.475632429122925, 'global_step': 65045, 'preemption_count': 0}), (66361, {'train/accuracy': 0.7131098508834839, 'train/loss': 1.1178903579711914, 'validation/accuracy': 0.6258599758148193, 'validation/loss': 1.5511329174041748, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.26155424118042, 'test/num_examples': 10000, 'score': 25552.440224409103, 'total_duration': 27503.78111886978, 'accumulated_submission_time': 25552.440224409103, 'accumulated_eval_time': 1938.2340450286865, 'accumulated_logging_time': 6.642372369766235, 'global_step': 66361, 'preemption_count': 0}), (67676, {'train/accuracy': 0.7088448405265808, 'train/loss': 1.1240360736846924, 'validation/accuracy': 0.6275599598884583, 'validation/loss': 1.530971646308899, 'validation/num_examples': 50000, 'test/accuracy': 0.5067999958992004, 'test/loss': 2.2481958866119385, 'test/num_examples': 10000, 'score': 26062.400289297104, 'total_duration': 28048.813851356506, 'accumulated_submission_time': 26062.400289297104, 'accumulated_eval_time': 1972.988124847412, 'accumulated_logging_time': 6.819470643997192, 'global_step': 67676, 'preemption_count': 0}), (68938, {'train/accuracy': 0.7111766338348389, 'train/loss': 1.121996521949768, 'validation/accuracy': 0.6229400038719177, 'validation/loss': 1.5530850887298584, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.2696895599365234, 'test/num_examples': 10000, 'score': 26572.411048173904, 'total_duration': 28596.78377866745, 'accumulated_submission_time': 26572.411048173904, 'accumulated_eval_time': 2010.643295764923, 'accumulated_logging_time': 6.989057779312134, 'global_step': 68938, 'preemption_count': 0}), (70248, {'train/accuracy': 0.7157405614852905, 'train/loss': 1.101019024848938, 'validation/accuracy': 0.6300199627876282, 'validation/loss': 1.526586651802063, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.280522346496582, 'test/num_examples': 10000, 'score': 27082.355411291122, 'total_duration': 29138.893358707428, 'accumulated_submission_time': 27082.355411291122, 'accumulated_eval_time': 2042.5111951828003, 'accumulated_logging_time': 7.146895408630371, 'global_step': 70248, 'preemption_count': 0}), (71561, {'train/accuracy': 0.7039022445678711, 'train/loss': 1.1633414030075073, 'validation/accuracy': 0.6174399852752686, 'validation/loss': 1.5731068849563599, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.267239570617676, 'test/num_examples': 10000, 'score': 27592.31870174408, 'total_duration': 29682.469063282013, 'accumulated_submission_time': 27592.31870174408, 'accumulated_eval_time': 2075.8732635974884, 'accumulated_logging_time': 7.256907224655151, 'global_step': 71561, 'preemption_count': 0}), (72857, {'train/accuracy': 0.7160195708274841, 'train/loss': 1.0978602170944214, 'validation/accuracy': 0.6301599740982056, 'validation/loss': 1.5278446674346924, 'validation/num_examples': 50000, 'test/accuracy': 0.5060999989509583, 'test/loss': 2.242117166519165, 'test/num_examples': 10000, 'score': 28102.199460029602, 'total_duration': 30224.351070165634, 'accumulated_submission_time': 28102.199460029602, 'accumulated_eval_time': 2107.5155494213104, 'accumulated_logging_time': 7.477160930633545, 'global_step': 72857, 'preemption_count': 0}), (74130, {'train/accuracy': 0.7191087007522583, 'train/loss': 1.100938081741333, 'validation/accuracy': 0.6284999847412109, 'validation/loss': 1.5357930660247803, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.248389482498169, 'test/num_examples': 10000, 'score': 28612.27093076706, 'total_duration': 30768.50037097931, 'accumulated_submission_time': 28612.27093076706, 'accumulated_eval_time': 2141.3312528133392, 'accumulated_logging_time': 7.607385158538818, 'global_step': 74130, 'preemption_count': 0}), (75444, {'train/accuracy': 0.720703125, 'train/loss': 1.0922532081604004, 'validation/accuracy': 0.6306799650192261, 'validation/loss': 1.523364543914795, 'validation/num_examples': 50000, 'test/accuracy': 0.5070000290870667, 'test/loss': 2.2272157669067383, 'test/num_examples': 10000, 'score': 29122.12250471115, 'total_duration': 31312.11866259575, 'accumulated_submission_time': 29122.12250471115, 'accumulated_eval_time': 2174.77023601532, 'accumulated_logging_time': 7.795947074890137, 'global_step': 75444, 'preemption_count': 0}), (76715, {'train/accuracy': 0.7245694994926453, 'train/loss': 1.0543209314346313, 'validation/accuracy': 0.6345599889755249, 'validation/loss': 1.503624439239502, 'validation/num_examples': 50000, 'test/accuracy': 0.5113000273704529, 'test/loss': 2.2342586517333984, 'test/num_examples': 10000, 'score': 29631.933977603912, 'total_duration': 31855.908323049545, 'accumulated_submission_time': 29631.933977603912, 'accumulated_eval_time': 2208.444871902466, 'accumulated_logging_time': 7.9680211544036865, 'global_step': 76715, 'preemption_count': 0}), (77939, {'train/accuracy': 0.6695033311843872, 'train/loss': 1.3183430433273315, 'validation/accuracy': 0.6229999661445618, 'validation/loss': 1.5599374771118164, 'validation/num_examples': 50000, 'test/accuracy': 0.49470001459121704, 'test/loss': 2.2986526489257812, 'test/num_examples': 10000, 'score': 30141.76208639145, 'total_duration': 32397.067548513412, 'accumulated_submission_time': 30141.76208639145, 'accumulated_eval_time': 2239.4893548488617, 'accumulated_logging_time': 8.129781484603882, 'global_step': 77939, 'preemption_count': 0}), (79153, {'train/accuracy': 0.6783920526504517, 'train/loss': 1.2600232362747192, 'validation/accuracy': 0.6340000033378601, 'validation/loss': 1.5178097486495972, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2697792053222656, 'test/num_examples': 10000, 'score': 30651.762511491776, 'total_duration': 32943.97350382805, 'accumulated_submission_time': 30651.762511491776, 'accumulated_eval_time': 2276.105446100235, 'accumulated_logging_time': 8.292530298233032, 'global_step': 79153, 'preemption_count': 0}), (80337, {'train/accuracy': 0.6887954473495483, 'train/loss': 1.2155958414077759, 'validation/accuracy': 0.6378799676895142, 'validation/loss': 1.4846913814544678, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.2073123455047607, 'test/num_examples': 10000, 'score': 31161.91561150551, 'total_duration': 33490.15157008171, 'accumulated_submission_time': 31161.91561150551, 'accumulated_eval_time': 2311.8241789340973, 'accumulated_logging_time': 8.477530479431152, 'global_step': 80337, 'preemption_count': 0}), (81553, {'train/accuracy': 0.6809231638908386, 'train/loss': 1.261443018913269, 'validation/accuracy': 0.6340599656105042, 'validation/loss': 1.5173825025558472, 'validation/num_examples': 50000, 'test/accuracy': 0.5, 'test/loss': 2.232513427734375, 'test/num_examples': 10000, 'score': 31672.052864313126, 'total_duration': 34029.7802426815, 'accumulated_submission_time': 31672.052864313126, 'accumulated_eval_time': 2341.022961139679, 'accumulated_logging_time': 8.647173643112183, 'global_step': 81553, 'preemption_count': 0}), (82679, {'train/accuracy': 0.6881576776504517, 'train/loss': 1.2260911464691162, 'validation/accuracy': 0.636139988899231, 'validation/loss': 1.500401258468628, 'validation/num_examples': 50000, 'test/accuracy': 0.5071000456809998, 'test/loss': 2.20916485786438, 'test/num_examples': 10000, 'score': 32181.98154401779, 'total_duration': 34573.32043719292, 'accumulated_submission_time': 32181.98154401779, 'accumulated_eval_time': 2374.3805763721466, 'accumulated_logging_time': 8.787813425064087, 'global_step': 82679, 'preemption_count': 0}), (83753, {'train/accuracy': 0.7017896771430969, 'train/loss': 1.168305516242981, 'validation/accuracy': 0.6414600014686584, 'validation/loss': 1.4691267013549805, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.1737451553344727, 'test/num_examples': 10000, 'score': 32691.894963741302, 'total_duration': 35124.71170473099, 'accumulated_submission_time': 32691.894963741302, 'accumulated_eval_time': 2415.630859375, 'accumulated_logging_time': 8.907928943634033, 'global_step': 83753, 'preemption_count': 0}), (84881, {'train/accuracy': 0.7056760191917419, 'train/loss': 1.1464003324508667, 'validation/accuracy': 0.6401799917221069, 'validation/loss': 1.4737129211425781, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.1885499954223633, 'test/num_examples': 10000, 'score': 33201.62847113609, 'total_duration': 35669.723321676254, 'accumulated_submission_time': 33201.62847113609, 'accumulated_eval_time': 2450.6020019054413, 'accumulated_logging_time': 9.101006984710693, 'global_step': 84881, 'preemption_count': 0}), (86210, {'train/accuracy': 0.6952925324440002, 'train/loss': 1.19424569606781, 'validation/accuracy': 0.634119987487793, 'validation/loss': 1.5135719776153564, 'validation/num_examples': 50000, 'test/accuracy': 0.5136000514030457, 'test/loss': 2.1956052780151367, 'test/num_examples': 10000, 'score': 33711.33899593353, 'total_duration': 36209.19914627075, 'accumulated_submission_time': 33711.33899593353, 'accumulated_eval_time': 2480.017291545868, 'accumulated_logging_time': 9.311240196228027, 'global_step': 86210, 'preemption_count': 0}), (87530, {'train/accuracy': 0.7108577489852905, 'train/loss': 1.123548984527588, 'validation/accuracy': 0.6423599720001221, 'validation/loss': 1.4598665237426758, 'validation/num_examples': 50000, 'test/accuracy': 0.5231000185012817, 'test/loss': 2.1592886447906494, 'test/num_examples': 10000, 'score': 34221.37405323982, 'total_duration': 36752.89350628853, 'accumulated_submission_time': 34221.37405323982, 'accumulated_eval_time': 2513.4157898426056, 'accumulated_logging_time': 9.43443751335144, 'global_step': 87530, 'preemption_count': 0}), (88849, {'train/accuracy': 0.7043805718421936, 'train/loss': 1.1414951086044312, 'validation/accuracy': 0.6425399780273438, 'validation/loss': 1.4656932353973389, 'validation/num_examples': 50000, 'test/accuracy': 0.5128999948501587, 'test/loss': 2.1934759616851807, 'test/num_examples': 10000, 'score': 34731.19294667244, 'total_duration': 37302.18275594711, 'accumulated_submission_time': 34731.19294667244, 'accumulated_eval_time': 2552.6291258335114, 'accumulated_logging_time': 9.548306226730347, 'global_step': 88849, 'preemption_count': 0}), (90169, {'train/accuracy': 0.693359375, 'train/loss': 1.197272539138794, 'validation/accuracy': 0.6341399550437927, 'validation/loss': 1.5148682594299316, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.227407217025757, 'test/num_examples': 10000, 'score': 35241.16833615303, 'total_duration': 37846.03147697449, 'accumulated_submission_time': 35241.16833615303, 'accumulated_eval_time': 2586.1653969287872, 'accumulated_logging_time': 9.739075183868408, 'global_step': 90169, 'preemption_count': 0}), (91490, {'train/accuracy': 0.7059151530265808, 'train/loss': 1.1385197639465332, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.459367275238037, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.176414966583252, 'test/num_examples': 10000, 'score': 35751.13984942436, 'total_duration': 38389.79814577103, 'accumulated_submission_time': 35751.13984942436, 'accumulated_eval_time': 2619.636850833893, 'accumulated_logging_time': 9.918402910232544, 'global_step': 91490, 'preemption_count': 0}), (92812, {'train/accuracy': 0.7111966013908386, 'train/loss': 1.1184314489364624, 'validation/accuracy': 0.648419976234436, 'validation/loss': 1.4376012086868286, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1643314361572266, 'test/num_examples': 10000, 'score': 36261.22060036659, 'total_duration': 38933.02260828018, 'accumulated_submission_time': 36261.22060036659, 'accumulated_eval_time': 2652.4772338867188, 'accumulated_logging_time': 10.078310251235962, 'global_step': 92812, 'preemption_count': 0}), (94133, {'train/accuracy': 0.711933970451355, 'train/loss': 1.1129710674285889, 'validation/accuracy': 0.6499599814414978, 'validation/loss': 1.4294352531433105, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.1395394802093506, 'test/num_examples': 10000, 'score': 36771.27670288086, 'total_duration': 39474.08042764664, 'accumulated_submission_time': 36771.27670288086, 'accumulated_eval_time': 2683.143434524536, 'accumulated_logging_time': 10.269637107849121, 'global_step': 94133, 'preemption_count': 0}), (95423, {'train/accuracy': 0.7125916481018066, 'train/loss': 1.1086246967315674, 'validation/accuracy': 0.6494799852371216, 'validation/loss': 1.4302985668182373, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1537413597106934, 'test/num_examples': 10000, 'score': 37281.26604413986, 'total_duration': 40014.93439292908, 'accumulated_submission_time': 37281.26604413986, 'accumulated_eval_time': 2713.674413919449, 'accumulated_logging_time': 10.460416555404663, 'global_step': 95423, 'preemption_count': 0}), (96730, {'train/accuracy': 0.7102798223495483, 'train/loss': 1.1264177560806274, 'validation/accuracy': 0.6484999656677246, 'validation/loss': 1.4373013973236084, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.1685001850128174, 'test/num_examples': 10000, 'score': 37791.249816179276, 'total_duration': 40554.43316364288, 'accumulated_submission_time': 37791.249816179276, 'accumulated_eval_time': 2742.9031484127045, 'accumulated_logging_time': 10.597673416137695, 'global_step': 96730, 'preemption_count': 0}), (98039, {'train/accuracy': 0.7233338356018066, 'train/loss': 1.0646077394485474, 'validation/accuracy': 0.6569199562072754, 'validation/loss': 1.3947858810424805, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.0793044567108154, 'test/num_examples': 10000, 'score': 38300.95591688156, 'total_duration': 41096.14505004883, 'accumulated_submission_time': 38300.95591688156, 'accumulated_eval_time': 2774.563080072403, 'accumulated_logging_time': 10.798969268798828, 'global_step': 98039, 'preemption_count': 0}), (99356, {'train/accuracy': 0.714863657951355, 'train/loss': 1.1028037071228027, 'validation/accuracy': 0.648639976978302, 'validation/loss': 1.4381848573684692, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.125122547149658, 'test/num_examples': 10000, 'score': 38810.746773958206, 'total_duration': 41638.66712594032, 'accumulated_submission_time': 38810.746773958206, 'accumulated_eval_time': 2807.0032069683075, 'accumulated_logging_time': 10.945482015609741, 'global_step': 99356, 'preemption_count': 0}), (100670, {'train/accuracy': 0.7261638641357422, 'train/loss': 1.058073878288269, 'validation/accuracy': 0.6642799973487854, 'validation/loss': 1.3675224781036377, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.073992967605591, 'test/num_examples': 10000, 'score': 39320.436344861984, 'total_duration': 42178.76672887802, 'accumulated_submission_time': 39320.436344861984, 'accumulated_eval_time': 2837.099628686905, 'accumulated_logging_time': 11.115626335144043, 'global_step': 100670, 'preemption_count': 0}), (101980, {'train/accuracy': 0.7166374325752258, 'train/loss': 1.0994858741760254, 'validation/accuracy': 0.6571999788284302, 'validation/loss': 1.404793381690979, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.145573854446411, 'test/num_examples': 10000, 'score': 39830.13855481148, 'total_duration': 42720.38946413994, 'accumulated_submission_time': 39830.13855481148, 'accumulated_eval_time': 2868.7190704345703, 'accumulated_logging_time': 11.271819353103638, 'global_step': 101980, 'preemption_count': 0}), (103288, {'train/accuracy': 0.7274593114852905, 'train/loss': 1.061448097229004, 'validation/accuracy': 0.6615200042724609, 'validation/loss': 1.3682620525360107, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.0555880069732666, 'test/num_examples': 10000, 'score': 40340.08786869049, 'total_duration': 43260.96857190132, 'accumulated_submission_time': 40340.08786869049, 'accumulated_eval_time': 2899.0624799728394, 'accumulated_logging_time': 11.415260076522827, 'global_step': 103288, 'preemption_count': 0}), (104595, {'train/accuracy': 0.7285953164100647, 'train/loss': 1.03449547290802, 'validation/accuracy': 0.6672399640083313, 'validation/loss': 1.3548555374145508, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.0678181648254395, 'test/num_examples': 10000, 'score': 40849.828301906586, 'total_duration': 43802.246480464935, 'accumulated_submission_time': 40849.828301906586, 'accumulated_eval_time': 2930.333208322525, 'accumulated_logging_time': 11.538697004318237, 'global_step': 104595, 'preemption_count': 0}), (105902, {'train/accuracy': 0.7267617583274841, 'train/loss': 1.0453345775604248, 'validation/accuracy': 0.663100004196167, 'validation/loss': 1.3641449213027954, 'validation/num_examples': 50000, 'test/accuracy': 0.5380000472068787, 'test/loss': 2.0845773220062256, 'test/num_examples': 10000, 'score': 41359.74016737938, 'total_duration': 44340.24461388588, 'accumulated_submission_time': 41359.74016737938, 'accumulated_eval_time': 2958.0964431762695, 'accumulated_logging_time': 11.720539093017578, 'global_step': 105902, 'preemption_count': 0}), (107210, {'train/accuracy': 0.7234534025192261, 'train/loss': 1.0698288679122925, 'validation/accuracy': 0.6615999937057495, 'validation/loss': 1.3824890851974487, 'validation/num_examples': 50000, 'test/accuracy': 0.5299000144004822, 'test/loss': 2.093621253967285, 'test/num_examples': 10000, 'score': 41869.41143536568, 'total_duration': 44881.80910515785, 'accumulated_submission_time': 41869.41143536568, 'accumulated_eval_time': 2989.6155257225037, 'accumulated_logging_time': 11.95208215713501, 'global_step': 107210, 'preemption_count': 0}), (108519, {'train/accuracy': 0.7308075428009033, 'train/loss': 1.0359797477722168, 'validation/accuracy': 0.6660400032997131, 'validation/loss': 1.3564285039901733, 'validation/num_examples': 50000, 'test/accuracy': 0.5342000126838684, 'test/loss': 2.0638396739959717, 'test/num_examples': 10000, 'score': 42379.39675307274, 'total_duration': 45420.75930452347, 'accumulated_submission_time': 42379.39675307274, 'accumulated_eval_time': 3018.29527759552, 'accumulated_logging_time': 12.095097303390503, 'global_step': 108519, 'preemption_count': 0}), (109830, {'train/accuracy': 0.7222974896430969, 'train/loss': 1.0619254112243652, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.3880425691604614, 'validation/num_examples': 50000, 'test/accuracy': 0.5437999963760376, 'test/loss': 2.074864149093628, 'test/num_examples': 10000, 'score': 42889.14323568344, 'total_duration': 45960.305901527405, 'accumulated_submission_time': 42889.14323568344, 'accumulated_eval_time': 3047.771082639694, 'accumulated_logging_time': 12.279263019561768, 'global_step': 109830, 'preemption_count': 0}), (111140, {'train/accuracy': 0.7431440949440002, 'train/loss': 0.9796679615974426, 'validation/accuracy': 0.6749399900436401, 'validation/loss': 1.3159313201904297, 'validation/num_examples': 50000, 'test/accuracy': 0.5506000518798828, 'test/loss': 1.9984906911849976, 'test/num_examples': 10000, 'score': 43399.08817052841, 'total_duration': 46502.30081295967, 'accumulated_submission_time': 43399.08817052841, 'accumulated_eval_time': 3079.502429962158, 'accumulated_logging_time': 12.455272674560547, 'global_step': 111140, 'preemption_count': 0}), (112453, {'train/accuracy': 0.7401546239852905, 'train/loss': 0.9847238063812256, 'validation/accuracy': 0.6710799932479858, 'validation/loss': 1.3278801441192627, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.031024217605591, 'test/num_examples': 10000, 'score': 43908.86542510986, 'total_duration': 47042.55430698395, 'accumulated_submission_time': 43908.86542510986, 'accumulated_eval_time': 3109.714515686035, 'accumulated_logging_time': 12.5783212184906, 'global_step': 112453, 'preemption_count': 0}), (113768, {'train/accuracy': 0.7409319281578064, 'train/loss': 0.990680992603302, 'validation/accuracy': 0.6717000007629395, 'validation/loss': 1.3290770053863525, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.061917304992676, 'test/num_examples': 10000, 'score': 44418.603545188904, 'total_duration': 47582.38474416733, 'accumulated_submission_time': 44418.603545188904, 'accumulated_eval_time': 3139.516149520874, 'accumulated_logging_time': 12.728459596633911, 'global_step': 113768, 'preemption_count': 0}), (115081, {'train/accuracy': 0.7419283986091614, 'train/loss': 0.9826892614364624, 'validation/accuracy': 0.6773599982261658, 'validation/loss': 1.3044812679290771, 'validation/num_examples': 50000, 'test/accuracy': 0.5485000014305115, 'test/loss': 2.0043253898620605, 'test/num_examples': 10000, 'score': 44928.30272936821, 'total_duration': 48126.03640413284, 'accumulated_submission_time': 44928.30272936821, 'accumulated_eval_time': 3173.1659231185913, 'accumulated_logging_time': 12.888441801071167, 'global_step': 115081, 'preemption_count': 0}), (116394, {'train/accuracy': 0.7452168464660645, 'train/loss': 0.9689076542854309, 'validation/accuracy': 0.674019992351532, 'validation/loss': 1.3074077367782593, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 1.9893537759780884, 'test/num_examples': 10000, 'score': 45438.1211874485, 'total_duration': 48666.465164899826, 'accumulated_submission_time': 45438.1211874485, 'accumulated_eval_time': 3203.4942905902863, 'accumulated_logging_time': 13.026046514511108, 'global_step': 116394, 'preemption_count': 0}), (117710, {'train/accuracy': 0.750996470451355, 'train/loss': 0.944252073764801, 'validation/accuracy': 0.6810599565505981, 'validation/loss': 1.2941615581512451, 'validation/num_examples': 50000, 'test/accuracy': 0.5493000149726868, 'test/loss': 2.010335922241211, 'test/num_examples': 10000, 'score': 45947.84820652008, 'total_duration': 49207.10041928291, 'accumulated_submission_time': 45947.84820652008, 'accumulated_eval_time': 3234.123591184616, 'accumulated_logging_time': 13.164076089859009, 'global_step': 117710, 'preemption_count': 0}), (119018, {'train/accuracy': 0.7523118257522583, 'train/loss': 0.9412111639976501, 'validation/accuracy': 0.6809399724006653, 'validation/loss': 1.2859958410263062, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 1.975270390510559, 'test/num_examples': 10000, 'score': 46457.72874855995, 'total_duration': 49749.0597383976, 'accumulated_submission_time': 46457.72874855995, 'accumulated_eval_time': 3265.9347772598267, 'accumulated_logging_time': 13.28991150856018, 'global_step': 119018, 'preemption_count': 0}), (120335, {'train/accuracy': 0.7511360049247742, 'train/loss': 0.9495161175727844, 'validation/accuracy': 0.6824199557304382, 'validation/loss': 1.2806087732315063, 'validation/num_examples': 50000, 'test/accuracy': 0.5596000552177429, 'test/loss': 1.9817901849746704, 'test/num_examples': 10000, 'score': 46967.50566124916, 'total_duration': 50288.89274406433, 'accumulated_submission_time': 46967.50566124916, 'accumulated_eval_time': 3295.6966631412506, 'accumulated_logging_time': 13.438750267028809, 'global_step': 120335, 'preemption_count': 0}), (121651, {'train/accuracy': 0.7564771771430969, 'train/loss': 0.919080913066864, 'validation/accuracy': 0.6828799843788147, 'validation/loss': 1.287508249282837, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 1.9882198572158813, 'test/num_examples': 10000, 'score': 47477.48717856407, 'total_duration': 50830.188037633896, 'accumulated_submission_time': 47477.48717856407, 'accumulated_eval_time': 3326.6774871349335, 'accumulated_logging_time': 13.627768278121948, 'global_step': 121651, 'preemption_count': 0}), (122963, {'train/accuracy': 0.7643893361091614, 'train/loss': 0.8908267617225647, 'validation/accuracy': 0.6933000087738037, 'validation/loss': 1.2455782890319824, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9361087083816528, 'test/num_examples': 10000, 'score': 47987.363550662994, 'total_duration': 51374.98397350311, 'accumulated_submission_time': 47987.363550662994, 'accumulated_eval_time': 3361.288002729416, 'accumulated_logging_time': 13.791656494140625, 'global_step': 122963, 'preemption_count': 0}), (124287, {'train/accuracy': 0.7543447017669678, 'train/loss': 0.9276888966560364, 'validation/accuracy': 0.6841999888420105, 'validation/loss': 1.2811634540557861, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 1.9944639205932617, 'test/num_examples': 10000, 'score': 48497.32695841789, 'total_duration': 51918.58218955994, 'accumulated_submission_time': 48497.32695841789, 'accumulated_eval_time': 3394.5990793704987, 'accumulated_logging_time': 13.97123908996582, 'global_step': 124287, 'preemption_count': 0}), (125612, {'train/accuracy': 0.7652064561843872, 'train/loss': 0.884160578250885, 'validation/accuracy': 0.6893999576568604, 'validation/loss': 1.2497535943984985, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 1.9739079475402832, 'test/num_examples': 10000, 'score': 49007.16653895378, 'total_duration': 52464.30775976181, 'accumulated_submission_time': 49007.16653895378, 'accumulated_eval_time': 3430.1749250888824, 'accumulated_logging_time': 14.13570523262024, 'global_step': 125612, 'preemption_count': 0}), (126941, {'train/accuracy': 0.7679567933082581, 'train/loss': 0.8757691383361816, 'validation/accuracy': 0.6919599771499634, 'validation/loss': 1.2387648820877075, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 1.920729637145996, 'test/num_examples': 10000, 'score': 49517.210122823715, 'total_duration': 53009.05414772034, 'accumulated_submission_time': 49517.210122823715, 'accumulated_eval_time': 3464.5973839759827, 'accumulated_logging_time': 14.271058797836304, 'global_step': 126941, 'preemption_count': 0}), (128269, {'train/accuracy': 0.7691127061843872, 'train/loss': 0.8599770665168762, 'validation/accuracy': 0.6973400115966797, 'validation/loss': 1.217076063156128, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.912018895149231, 'test/num_examples': 10000, 'score': 50027.059827804565, 'total_duration': 53550.66779232025, 'accumulated_submission_time': 50027.059827804565, 'accumulated_eval_time': 3496.122677087784, 'accumulated_logging_time': 14.364009857177734, 'global_step': 128269, 'preemption_count': 0}), (129599, {'train/accuracy': 0.7689133882522583, 'train/loss': 0.8698284029960632, 'validation/accuracy': 0.6899799704551697, 'validation/loss': 1.2466366291046143, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.942866325378418, 'test/num_examples': 10000, 'score': 50537.025822877884, 'total_duration': 54094.1882815361, 'accumulated_submission_time': 50537.025822877884, 'accumulated_eval_time': 3529.3651161193848, 'accumulated_logging_time': 14.530999183654785, 'global_step': 129599, 'preemption_count': 0}), (130925, {'train/accuracy': 0.7647879123687744, 'train/loss': 0.8849365711212158, 'validation/accuracy': 0.6901599764823914, 'validation/loss': 1.2479355335235596, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 1.9499568939208984, 'test/num_examples': 10000, 'score': 51047.13235664368, 'total_duration': 54638.59072327614, 'accumulated_submission_time': 51047.13235664368, 'accumulated_eval_time': 3563.431957244873, 'accumulated_logging_time': 14.617443084716797, 'global_step': 130925, 'preemption_count': 0}), (132249, {'train/accuracy': 0.7755500674247742, 'train/loss': 0.8368673920631409, 'validation/accuracy': 0.7014600038528442, 'validation/loss': 1.2075825929641724, 'validation/num_examples': 50000, 'test/accuracy': 0.5716000199317932, 'test/loss': 1.9136385917663574, 'test/num_examples': 10000, 'score': 51556.9741396904, 'total_duration': 55179.33862376213, 'accumulated_submission_time': 51556.9741396904, 'accumulated_eval_time': 3594.058982849121, 'accumulated_logging_time': 14.752139329910278, 'global_step': 132249, 'preemption_count': 0}), (133577, {'train/accuracy': 0.7761479616165161, 'train/loss': 0.8254809975624084, 'validation/accuracy': 0.6995799541473389, 'validation/loss': 1.2028008699417114, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.8977516889572144, 'test/num_examples': 10000, 'score': 52066.951180934906, 'total_duration': 55722.577755212784, 'accumulated_submission_time': 52066.951180934906, 'accumulated_eval_time': 3627.0235135555267, 'accumulated_logging_time': 14.905612707138062, 'global_step': 133577, 'preemption_count': 0}), (134906, {'train/accuracy': 0.7825454473495483, 'train/loss': 0.8077905178070068, 'validation/accuracy': 0.7038399577140808, 'validation/loss': 1.1898832321166992, 'validation/num_examples': 50000, 'test/accuracy': 0.573900043964386, 'test/loss': 1.8658047914505005, 'test/num_examples': 10000, 'score': 52576.776794195175, 'total_duration': 56266.00555682182, 'accumulated_submission_time': 52576.776794195175, 'accumulated_eval_time': 3660.33269906044, 'accumulated_logging_time': 15.054564952850342, 'global_step': 134906, 'preemption_count': 0}), (136233, {'train/accuracy': 0.7856743931770325, 'train/loss': 0.7930082082748413, 'validation/accuracy': 0.7042999863624573, 'validation/loss': 1.186693787574768, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.861576795578003, 'test/num_examples': 10000, 'score': 53086.47843527794, 'total_duration': 56806.058566093445, 'accumulated_submission_time': 53086.47843527794, 'accumulated_eval_time': 3690.3591141700745, 'accumulated_logging_time': 15.233972072601318, 'global_step': 136233, 'preemption_count': 0}), (137558, {'train/accuracy': 0.7804726958274841, 'train/loss': 0.8170868754386902, 'validation/accuracy': 0.7047399878501892, 'validation/loss': 1.1842223405838013, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 1.8777655363082886, 'test/num_examples': 10000, 'score': 53596.4441652298, 'total_duration': 57348.46748781204, 'accumulated_submission_time': 53596.4441652298, 'accumulated_eval_time': 3722.5042386054993, 'accumulated_logging_time': 15.389992475509644, 'global_step': 137558, 'preemption_count': 0}), (138885, {'train/accuracy': 0.7915935516357422, 'train/loss': 0.7748403549194336, 'validation/accuracy': 0.7110999822616577, 'validation/loss': 1.1624399423599243, 'validation/num_examples': 50000, 'test/accuracy': 0.5832000374794006, 'test/loss': 1.8485795259475708, 'test/num_examples': 10000, 'score': 54106.490668296814, 'total_duration': 57891.960322380066, 'accumulated_submission_time': 54106.490668296814, 'accumulated_eval_time': 3755.666371822357, 'accumulated_logging_time': 15.528958797454834, 'global_step': 138885, 'preemption_count': 0}), (140217, {'train/accuracy': 0.794343888759613, 'train/loss': 0.7518271207809448, 'validation/accuracy': 0.7140600085258484, 'validation/loss': 1.1480584144592285, 'validation/num_examples': 50000, 'test/accuracy': 0.5831000208854675, 'test/loss': 1.838874340057373, 'test/num_examples': 10000, 'score': 54616.38758087158, 'total_duration': 58434.89035844803, 'accumulated_submission_time': 54616.38758087158, 'accumulated_eval_time': 3788.379606962204, 'accumulated_logging_time': 15.704994916915894, 'global_step': 140217, 'preemption_count': 0}), (141542, {'train/accuracy': 0.7994658350944519, 'train/loss': 0.7493096590042114, 'validation/accuracy': 0.7128799557685852, 'validation/loss': 1.1480590105056763, 'validation/num_examples': 50000, 'test/accuracy': 0.5884000062942505, 'test/loss': 1.8306844234466553, 'test/num_examples': 10000, 'score': 55126.42038106918, 'total_duration': 58975.423993587494, 'accumulated_submission_time': 55126.42038106918, 'accumulated_eval_time': 3818.5902738571167, 'accumulated_logging_time': 15.850285291671753, 'global_step': 141542, 'preemption_count': 0}), (142870, {'train/accuracy': 0.8012794852256775, 'train/loss': 0.7226668000221252, 'validation/accuracy': 0.715179979801178, 'validation/loss': 1.1486707925796509, 'validation/num_examples': 50000, 'test/accuracy': 0.5825000405311584, 'test/loss': 1.8680264949798584, 'test/num_examples': 10000, 'score': 55636.181668281555, 'total_duration': 59518.26654577255, 'accumulated_submission_time': 55636.181668281555, 'accumulated_eval_time': 3851.413890838623, 'accumulated_logging_time': 15.96316409111023, 'global_step': 142870, 'preemption_count': 0}), (144193, {'train/accuracy': 0.8028738498687744, 'train/loss': 0.7296748161315918, 'validation/accuracy': 0.7135199904441833, 'validation/loss': 1.153074860572815, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 1.8504260778427124, 'test/num_examples': 10000, 'score': 56145.984580755234, 'total_duration': 60061.30682682991, 'accumulated_submission_time': 56145.984580755234, 'accumulated_eval_time': 3884.34156870842, 'accumulated_logging_time': 16.13098454475403, 'global_step': 144193, 'preemption_count': 0}), (145514, {'train/accuracy': 0.80961012840271, 'train/loss': 0.6954296231269836, 'validation/accuracy': 0.7186200022697449, 'validation/loss': 1.1304233074188232, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.8403096199035645, 'test/num_examples': 10000, 'score': 56656.10156941414, 'total_duration': 60603.285472631454, 'accumulated_submission_time': 56656.10156941414, 'accumulated_eval_time': 3915.8900859355927, 'accumulated_logging_time': 16.29936695098877, 'global_step': 145514, 'preemption_count': 0}), (146833, {'train/accuracy': 0.8132174611091614, 'train/loss': 0.6854730844497681, 'validation/accuracy': 0.7178399562835693, 'validation/loss': 1.1321442127227783, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.8339585065841675, 'test/num_examples': 10000, 'score': 57165.80280208588, 'total_duration': 61143.5567946434, 'accumulated_submission_time': 57165.80280208588, 'accumulated_eval_time': 3946.1169352531433, 'accumulated_logging_time': 16.49998950958252, 'global_step': 146833, 'preemption_count': 0}), (148154, {'train/accuracy': 0.8169443607330322, 'train/loss': 0.6568751931190491, 'validation/accuracy': 0.7255399823188782, 'validation/loss': 1.110706090927124, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.7891204357147217, 'test/num_examples': 10000, 'score': 57675.74361562729, 'total_duration': 61689.79122018814, 'accumulated_submission_time': 57675.74361562729, 'accumulated_eval_time': 3982.1471931934357, 'accumulated_logging_time': 16.61973738670349, 'global_step': 148154, 'preemption_count': 0}), (149474, {'train/accuracy': 0.8251753449440002, 'train/loss': 0.6369521617889404, 'validation/accuracy': 0.7230599522590637, 'validation/loss': 1.1205451488494873, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.806749701499939, 'test/num_examples': 10000, 'score': 58185.58929872513, 'total_duration': 62232.29395532608, 'accumulated_submission_time': 58185.58929872513, 'accumulated_eval_time': 4014.505989074707, 'accumulated_logging_time': 16.77537965774536, 'global_step': 149474, 'preemption_count': 0}), (150798, {'train/accuracy': 0.8434311151504517, 'train/loss': 0.5761180520057678, 'validation/accuracy': 0.7266199588775635, 'validation/loss': 1.0990161895751953, 'validation/num_examples': 50000, 'test/accuracy': 0.6021000146865845, 'test/loss': 1.8024053573608398, 'test/num_examples': 10000, 'score': 58695.49927163124, 'total_duration': 62774.60535979271, 'accumulated_submission_time': 58695.49927163124, 'accumulated_eval_time': 4046.6512253284454, 'accumulated_logging_time': 16.888478755950928, 'global_step': 150798, 'preemption_count': 0}), (152116, {'train/accuracy': 0.8560267686843872, 'train/loss': 0.5252457857131958, 'validation/accuracy': 0.7270199656486511, 'validation/loss': 1.0983662605285645, 'validation/num_examples': 50000, 'test/accuracy': 0.6023000478744507, 'test/loss': 1.7850979566574097, 'test/num_examples': 10000, 'score': 59205.57086324692, 'total_duration': 63316.897686481476, 'accumulated_submission_time': 59205.57086324692, 'accumulated_eval_time': 4078.55708861351, 'accumulated_logging_time': 17.061365842819214, 'global_step': 152116, 'preemption_count': 0}), (153436, {'train/accuracy': 0.8547512292861938, 'train/loss': 0.5223726630210876, 'validation/accuracy': 0.7273199558258057, 'validation/loss': 1.0890791416168213, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7827666997909546, 'test/num_examples': 10000, 'score': 59715.60297012329, 'total_duration': 63857.59925246239, 'accumulated_submission_time': 59715.60297012329, 'accumulated_eval_time': 4108.9563164711, 'accumulated_logging_time': 17.186180114746094, 'global_step': 153436, 'preemption_count': 0}), (154747, {'train/accuracy': 0.8681241869926453, 'train/loss': 0.4773489832878113, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.0811139345169067, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.7728581428527832, 'test/num_examples': 10000, 'score': 60225.4982881546, 'total_duration': 64401.07671260834, 'accumulated_submission_time': 60225.4982881546, 'accumulated_eval_time': 4142.246251106262, 'accumulated_logging_time': 17.333640336990356, 'global_step': 154747, 'preemption_count': 0}), (156059, {'train/accuracy': 0.8676658272743225, 'train/loss': 0.4702610671520233, 'validation/accuracy': 0.7339000105857849, 'validation/loss': 1.0702953338623047, 'validation/num_examples': 50000, 'test/accuracy': 0.61080002784729, 'test/loss': 1.7554395198822021, 'test/num_examples': 10000, 'score': 60735.35158538818, 'total_duration': 64943.68873047829, 'accumulated_submission_time': 60735.35158538818, 'accumulated_eval_time': 4174.623281478882, 'accumulated_logging_time': 17.559131860733032, 'global_step': 156059, 'preemption_count': 0}), (157367, {'train/accuracy': 0.8715720772743225, 'train/loss': 0.46182703971862793, 'validation/accuracy': 0.732979953289032, 'validation/loss': 1.0705711841583252, 'validation/num_examples': 50000, 'test/accuracy': 0.6112000346183777, 'test/loss': 1.7597529888153076, 'test/num_examples': 10000, 'score': 61245.39450287819, 'total_duration': 65487.16800689697, 'accumulated_submission_time': 61245.39450287819, 'accumulated_eval_time': 4207.739497184753, 'accumulated_logging_time': 17.73732018470764, 'global_step': 157367, 'preemption_count': 0}), (158677, {'train/accuracy': 0.8761559128761292, 'train/loss': 0.44003790616989136, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.0588594675064087, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7587109804153442, 'test/num_examples': 10000, 'score': 61755.276366233826, 'total_duration': 66031.93673563004, 'accumulated_submission_time': 61755.276366233826, 'accumulated_eval_time': 4242.335964679718, 'accumulated_logging_time': 17.886207818984985, 'global_step': 158677, 'preemption_count': 0}), (159992, {'train/accuracy': 0.8791055083274841, 'train/loss': 0.4338916838169098, 'validation/accuracy': 0.7372599840164185, 'validation/loss': 1.0545680522918701, 'validation/num_examples': 50000, 'test/accuracy': 0.6192000508308411, 'test/loss': 1.734291434288025, 'test/num_examples': 10000, 'score': 62265.19534754753, 'total_duration': 66573.87365245819, 'accumulated_submission_time': 62265.19534754753, 'accumulated_eval_time': 4274.072719573975, 'accumulated_logging_time': 18.02484917640686, 'global_step': 159992, 'preemption_count': 0}), (161305, {'train/accuracy': 0.8779097199440002, 'train/loss': 0.4337511658668518, 'validation/accuracy': 0.736579954624176, 'validation/loss': 1.0634077787399292, 'validation/num_examples': 50000, 'test/accuracy': 0.6149000525474548, 'test/loss': 1.7591530084609985, 'test/num_examples': 10000, 'score': 62775.05073571205, 'total_duration': 67115.72429203987, 'accumulated_submission_time': 62775.05073571205, 'accumulated_eval_time': 4305.737823486328, 'accumulated_logging_time': 18.21061635017395, 'global_step': 161305, 'preemption_count': 0}), (162621, {'train/accuracy': 0.8905851244926453, 'train/loss': 0.39023658633232117, 'validation/accuracy': 0.7434399724006653, 'validation/loss': 1.03712797164917, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.719489574432373, 'test/num_examples': 10000, 'score': 63285.14891028404, 'total_duration': 67658.04615354538, 'accumulated_submission_time': 63285.14891028404, 'accumulated_eval_time': 4337.708229541779, 'accumulated_logging_time': 18.321906805038452, 'global_step': 162621, 'preemption_count': 0}), (163943, {'train/accuracy': 0.8871771097183228, 'train/loss': 0.3987239897251129, 'validation/accuracy': 0.7414599657058716, 'validation/loss': 1.0368437767028809, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.732204794883728, 'test/num_examples': 10000, 'score': 63795.12868666649, 'total_duration': 68199.53705382347, 'accumulated_submission_time': 63795.12868666649, 'accumulated_eval_time': 4368.924388885498, 'accumulated_logging_time': 18.475444078445435, 'global_step': 163943, 'preemption_count': 0}), (165264, {'train/accuracy': 0.8905652165412903, 'train/loss': 0.39370325207710266, 'validation/accuracy': 0.7463200092315674, 'validation/loss': 1.0265657901763916, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.7303334474563599, 'test/num_examples': 10000, 'score': 64305.104657649994, 'total_duration': 68740.17964315414, 'accumulated_submission_time': 64305.104657649994, 'accumulated_eval_time': 4399.2958562374115, 'accumulated_logging_time': 18.63028359413147, 'global_step': 165264, 'preemption_count': 0}), (166579, {'train/accuracy': 0.8924385905265808, 'train/loss': 0.3779086470603943, 'validation/accuracy': 0.7468599677085876, 'validation/loss': 1.0234558582305908, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.7122259140014648, 'test/num_examples': 10000, 'score': 64814.83959007263, 'total_duration': 69283.00878286362, 'accumulated_submission_time': 64814.83959007263, 'accumulated_eval_time': 4432.097115755081, 'accumulated_logging_time': 18.780981302261353, 'global_step': 166579, 'preemption_count': 0}), (167900, {'train/accuracy': 0.8959462642669678, 'train/loss': 0.3677658438682556, 'validation/accuracy': 0.7455399632453918, 'validation/loss': 1.025676965713501, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.7042292356491089, 'test/num_examples': 10000, 'score': 65324.58327507973, 'total_duration': 69823.10862851143, 'accumulated_submission_time': 65324.58327507973, 'accumulated_eval_time': 4462.124315023422, 'accumulated_logging_time': 18.968910694122314, 'global_step': 167900, 'preemption_count': 0}), (169223, {'train/accuracy': 0.9003108739852905, 'train/loss': 0.35501787066459656, 'validation/accuracy': 0.7491399645805359, 'validation/loss': 1.0223435163497925, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.7016139030456543, 'test/num_examples': 10000, 'score': 65834.42665338516, 'total_duration': 70365.63348674774, 'accumulated_submission_time': 65834.42665338516, 'accumulated_eval_time': 4494.497592926025, 'accumulated_logging_time': 19.13407063484192, 'global_step': 169223, 'preemption_count': 0})], 'global_step': 170549}
I0307 21:07:01.170753 140226914178240 submission_runner.py:649] Timing: 66344.11473083496
I0307 21:07:01.170796 140226914178240 submission_runner.py:651] Total number of evals: 130
I0307 21:07:01.170823 140226914178240 submission_runner.py:652] ====================
I0307 21:07:01.171026 140226914178240 submission_runner.py:750] Final imagenet_resnet score: 2
