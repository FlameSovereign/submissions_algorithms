python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=1115941927 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-01-18-47.log
2025-03-07 01:19:04.822603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741310345.319710       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741310345.500402       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 01:19:53.835107 140193455334592 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax.
I0307 01:19:56.605868 140193455334592 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 01:19:56.608889 140193455334592 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 01:19:56.628615 140193455334592 submission_runner.py:606] Using RNG seed 1115941927
I0307 01:19:59.649969 140193455334592 submission_runner.py:615] --- Tuning run 2/5 ---
I0307 01:19:59.650181 140193455334592 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_2.
I0307 01:19:59.650403 140193455334592 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_2/hparams.json.
I0307 01:19:59.892293 140193455334592 submission_runner.py:218] Initializing dataset.
I0307 01:20:01.308798 140193455334592 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:20:01.692727 140193455334592 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:20:02.008235 140193455334592 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:20:03.770005 140193455334592 submission_runner.py:229] Initializing model.
I0307 01:20:28.249191 140193455334592 submission_runner.py:272] Initializing optimizer.
I0307 01:20:29.409437 140193455334592 submission_runner.py:279] Initializing metrics bundle.
I0307 01:20:29.409708 140193455334592 submission_runner.py:301] Initializing checkpoint and logger.
I0307 01:20:29.410794 140193455334592 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0307 01:20:29.410896 140193455334592 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_2/meta_data_0.json.
I0307 01:20:29.989010 140193455334592 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_2/flags_0.json.
I0307 01:20:30.244333 140193455334592 submission_runner.py:337] Starting training loop.
I0307 01:21:26.650231 140052554278656 logging_writer.py:48] [0] global_step=0, grad_norm=0.5403334498405457, loss=6.930035591125488
I0307 01:21:27.021978 140193455334592 spec.py:321] Evaluating on the training split.
I0307 01:21:27.477231 140193455334592 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:21:27.501585 140193455334592 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:21:27.542906 140193455334592 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:21:46.819825 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 01:21:47.330646 140193455334592 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:21:47.402385 140193455334592 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:21:47.450239 140193455334592 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:22:32.938628 140193455334592 spec.py:349] Evaluating on the test split.
I0307 01:22:33.422971 140193455334592 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 01:22:33.493321 140193455334592 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 01:22:33.529760 140193455334592 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 01:23:05.752914 140193455334592 submission_runner.py:469] Time since start: 155.51s, 	Step: 1, 	{'train/accuracy': 0.0010363520123064518, 'train/loss': 6.9122090339660645, 'validation/accuracy': 0.0011399999493733048, 'validation/loss': 6.912380218505859, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.912490367889404, 'test/num_examples': 10000, 'score': 56.77741193771362, 'total_duration': 155.50851035118103, 'accumulated_submission_time': 56.77741193771362, 'accumulated_eval_time': 98.73086762428284, 'accumulated_logging_time': 0}
I0307 01:23:05.780360 140037962307328 logging_writer.py:48] [1] accumulated_eval_time=98.7309, accumulated_logging_time=0, accumulated_submission_time=56.7774, global_step=1, preemption_count=0, score=56.7774, test/accuracy=0.0008, test/loss=6.91249, test/num_examples=10000, total_duration=155.509, train/accuracy=0.00103635, train/loss=6.91221, validation/accuracy=0.00114, validation/loss=6.91238, validation/num_examples=50000
I0307 01:23:41.520967 140037878445824 logging_writer.py:48] [100] global_step=100, grad_norm=0.5234072804450989, loss=6.900829792022705
I0307 01:24:17.775701 140037962307328 logging_writer.py:48] [200] global_step=200, grad_norm=0.5551756620407104, loss=6.853192329406738
I0307 01:24:54.692089 140037878445824 logging_writer.py:48] [300] global_step=300, grad_norm=0.5777081251144409, loss=6.7795939445495605
I0307 01:25:31.678075 140037962307328 logging_writer.py:48] [400] global_step=400, grad_norm=0.6118897795677185, loss=6.673569679260254
I0307 01:26:08.721575 140037878445824 logging_writer.py:48] [500] global_step=500, grad_norm=0.6404227614402771, loss=6.590590953826904
I0307 01:26:45.980948 140037962307328 logging_writer.py:48] [600] global_step=600, grad_norm=0.689333438873291, loss=6.552312850952148
I0307 01:27:23.118790 140037878445824 logging_writer.py:48] [700] global_step=700, grad_norm=0.8078746795654297, loss=6.417421340942383
I0307 01:28:00.424127 140037962307328 logging_writer.py:48] [800] global_step=800, grad_norm=0.9282330274581909, loss=6.378983497619629
I0307 01:28:38.098625 140037878445824 logging_writer.py:48] [900] global_step=900, grad_norm=1.0569809675216675, loss=6.3184638023376465
I0307 01:29:15.600756 140037962307328 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.445888638496399, loss=6.210906028747559
I0307 01:29:52.203282 140037878445824 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.066056489944458, loss=6.170142650604248
I0307 01:30:30.133076 140037962307328 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.0602915287017822, loss=6.0996928215026855
I0307 01:31:07.760904 140037878445824 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.0903351306915283, loss=6.043889999389648
I0307 01:31:35.812930 140193455334592 spec.py:321] Evaluating on the training split.
I0307 01:31:47.256058 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 01:32:05.889245 140193455334592 spec.py:349] Evaluating on the test split.
I0307 01:32:07.778267 140193455334592 submission_runner.py:469] Time since start: 697.53s, 	Step: 1376, 	{'train/accuracy': 0.07517538219690323, 'train/loss': 5.3805365562438965, 'validation/accuracy': 0.06591999530792236, 'validation/loss': 5.463790416717529, 'validation/num_examples': 50000, 'test/accuracy': 0.04620000347495079, 'test/loss': 5.659274578094482, 'test/num_examples': 10000, 'score': 566.6240122318268, 'total_duration': 697.5338888168335, 'accumulated_submission_time': 566.6240122318268, 'accumulated_eval_time': 130.6961612701416, 'accumulated_logging_time': 0.050811052322387695}
I0307 01:32:07.807400 140037970700032 logging_writer.py:48] [1376] accumulated_eval_time=130.696, accumulated_logging_time=0.0508111, accumulated_submission_time=566.624, global_step=1376, preemption_count=0, score=566.624, test/accuracy=0.0462, test/loss=5.65927, test/num_examples=10000, total_duration=697.534, train/accuracy=0.0751754, train/loss=5.38054, validation/accuracy=0.06592, validation/loss=5.46379, validation/num_examples=50000
I0307 01:32:17.251223 140037979092736 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.2759037017822266, loss=6.036157131195068
I0307 01:32:54.362836 140037970700032 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.716587543487549, loss=5.955572128295898
I0307 01:33:31.892497 140037979092736 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.8521846532821655, loss=5.84291410446167
I0307 01:34:09.607136 140037970700032 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.8799102306365967, loss=5.835977554321289
I0307 01:34:47.592809 140037979092736 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.969571113586426, loss=5.720129013061523
I0307 01:35:25.593160 140037970700032 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.2320241928100586, loss=5.789240837097168
I0307 01:36:03.237925 140037979092736 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.264753818511963, loss=5.727155685424805
I0307 01:36:41.072085 140037970700032 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.363113880157471, loss=5.695248126983643
I0307 01:37:18.863690 140037979092736 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.5870087146759033, loss=5.5459747314453125
I0307 01:37:56.995154 140037970700032 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.552706718444824, loss=5.624959468841553
I0307 01:38:34.992324 140037979092736 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.938004970550537, loss=5.557101249694824
I0307 01:39:12.717796 140037970700032 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.628304958343506, loss=5.495023250579834
I0307 01:39:50.499381 140037979092736 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.3482964038848877, loss=5.473128318786621
I0307 01:40:28.245569 140037970700032 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.4573416709899902, loss=5.403635501861572
I0307 01:40:38.029714 140193455334592 spec.py:321] Evaluating on the training split.
I0307 01:40:49.249259 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 01:41:11.208695 140193455334592 spec.py:349] Evaluating on the test split.
I0307 01:41:12.979261 140193455334592 submission_runner.py:469] Time since start: 1242.73s, 	Step: 2727, 	{'train/accuracy': 0.17757493257522583, 'train/loss': 4.3514909744262695, 'validation/accuracy': 0.15587998926639557, 'validation/loss': 4.4895405769348145, 'validation/num_examples': 50000, 'test/accuracy': 0.11400000751018524, 'test/loss': 4.868528366088867, 'test/num_examples': 10000, 'score': 1076.6900205612183, 'total_duration': 1242.7348968982697, 'accumulated_submission_time': 1076.6900205612183, 'accumulated_eval_time': 165.64567732810974, 'accumulated_logging_time': 0.09013152122497559}
I0307 01:41:13.012470 140037979092736 logging_writer.py:48] [2727] accumulated_eval_time=165.646, accumulated_logging_time=0.0901315, accumulated_submission_time=1076.69, global_step=2727, preemption_count=0, score=1076.69, test/accuracy=0.114, test/loss=4.86853, test/num_examples=10000, total_duration=1242.73, train/accuracy=0.177575, train/loss=4.35149, validation/accuracy=0.15588, validation/loss=4.48954, validation/num_examples=50000
I0307 01:41:40.980459 140037970700032 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.043166160583496, loss=5.355020999908447
I0307 01:42:18.975540 140037979092736 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.828509569168091, loss=5.329254150390625
I0307 01:42:57.164876 140037970700032 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.3623738288879395, loss=5.258026599884033
I0307 01:43:35.360787 140037979092736 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.96659517288208, loss=5.208767890930176
I0307 01:44:13.489253 140037970700032 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.223341464996338, loss=5.1532368659973145
I0307 01:44:51.221557 140037979092736 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.333470344543457, loss=5.1774373054504395
I0307 01:45:29.332881 140037970700032 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.847597122192383, loss=5.079413890838623
I0307 01:46:07.350953 140037979092736 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.607154369354248, loss=5.008060455322266
I0307 01:46:45.124315 140037970700032 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.400254249572754, loss=5.030945777893066
I0307 01:47:23.425682 140037979092736 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.4385275840759277, loss=5.115830898284912
I0307 01:48:01.288151 140037970700032 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.39720344543457, loss=5.001437187194824
I0307 01:48:39.223804 140037979092736 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.336517095565796, loss=5.017056465148926
I0307 01:49:17.241505 140037970700032 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.053857326507568, loss=4.879876136779785
I0307 01:49:43.161517 140193455334592 spec.py:321] Evaluating on the training split.
I0307 01:49:54.981542 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 01:50:19.791388 140193455334592 spec.py:349] Evaluating on the test split.
I0307 01:50:21.563508 140193455334592 submission_runner.py:469] Time since start: 1791.32s, 	Step: 4069, 	{'train/accuracy': 0.2766461968421936, 'train/loss': 3.614423990249634, 'validation/accuracy': 0.24368000030517578, 'validation/loss': 3.810802698135376, 'validation/num_examples': 50000, 'test/accuracy': 0.18240000307559967, 'test/loss': 4.323792457580566, 'test/num_examples': 10000, 'score': 1586.6931998729706, 'total_duration': 1791.3191423416138, 'accumulated_submission_time': 1586.6931998729706, 'accumulated_eval_time': 204.0476369857788, 'accumulated_logging_time': 0.13822722434997559}
I0307 01:50:21.572046 140037979092736 logging_writer.py:48] [4069] accumulated_eval_time=204.048, accumulated_logging_time=0.138227, accumulated_submission_time=1586.69, global_step=4069, preemption_count=0, score=1586.69, test/accuracy=0.1824, test/loss=4.32379, test/num_examples=10000, total_duration=1791.32, train/accuracy=0.276646, train/loss=3.61442, validation/accuracy=0.24368, validation/loss=3.8108, validation/num_examples=50000
I0307 01:50:33.409682 140037970700032 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.658281087875366, loss=4.9273552894592285
I0307 01:51:11.105262 140037979092736 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.7295942306518555, loss=4.915234565734863
I0307 01:51:48.793599 140037970700032 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.027954578399658, loss=4.771026134490967
I0307 01:52:26.850138 140037979092736 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.96286940574646, loss=4.7569427490234375
I0307 01:53:04.921740 140037970700032 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.319542646408081, loss=4.717616081237793
I0307 01:53:42.739598 140037979092736 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.1852924823760986, loss=4.783652305603027
I0307 01:54:20.554695 140037970700032 logging_writer.py:48] [4700] global_step=4700, grad_norm=5.63408899307251, loss=4.735825538635254
I0307 01:54:57.746945 140037979092736 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.7855794429779053, loss=4.654168128967285
I0307 01:55:35.417834 140037970700032 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.846698522567749, loss=4.634674549102783
I0307 01:56:13.138653 140037979092736 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.4421277046203613, loss=4.659769058227539
I0307 01:56:51.189424 140037970700032 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.3600356578826904, loss=4.61761474609375
I0307 01:57:29.053232 140037979092736 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.4694101810455322, loss=4.587634086608887
I0307 01:58:07.005484 140037970700032 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.6507060527801514, loss=4.515063285827637
I0307 01:58:44.882839 140037979092736 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.436784267425537, loss=4.577516555786133
I0307 01:58:51.689463 140193455334592 spec.py:321] Evaluating on the training split.
I0307 01:59:02.949558 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 01:59:24.512358 140193455334592 spec.py:349] Evaluating on the test split.
I0307 01:59:26.274304 140193455334592 submission_runner.py:469] Time since start: 2336.03s, 	Step: 5419, 	{'train/accuracy': 0.3655931055545807, 'train/loss': 3.089827299118042, 'validation/accuracy': 0.3305799961090088, 'validation/loss': 3.288238763809204, 'validation/num_examples': 50000, 'test/accuracy': 0.248400017619133, 'test/loss': 3.8467321395874023, 'test/num_examples': 10000, 'score': 2096.669495820999, 'total_duration': 2336.029908657074, 'accumulated_submission_time': 2096.669495820999, 'accumulated_eval_time': 238.6324110031128, 'accumulated_logging_time': 0.15430474281311035}
I0307 01:59:26.283554 140037970700032 logging_writer.py:48] [5419] accumulated_eval_time=238.632, accumulated_logging_time=0.154305, accumulated_submission_time=2096.67, global_step=5419, preemption_count=0, score=2096.67, test/accuracy=0.2484, test/loss=3.84673, test/num_examples=10000, total_duration=2336.03, train/accuracy=0.365593, train/loss=3.08983, validation/accuracy=0.33058, validation/loss=3.28824, validation/num_examples=50000
I0307 01:59:57.312973 140037979092736 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.0063164234161377, loss=4.5344672203063965
I0307 02:00:35.464478 140037970700032 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.270338535308838, loss=4.456847190856934
I0307 02:01:13.419220 140037979092736 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.64353609085083, loss=4.446441650390625
I0307 02:01:50.841045 140037970700032 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.4084856510162354, loss=4.347734451293945
I0307 02:02:28.775588 140037979092736 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.673964738845825, loss=4.412464618682861
I0307 02:03:06.583707 140037970700032 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.6198503971099854, loss=4.421947002410889
I0307 02:03:44.509516 140037979092736 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.5725507736206055, loss=4.37348747253418
I0307 02:04:22.525031 140037970700032 logging_writer.py:48] [6200] global_step=6200, grad_norm=4.270666122436523, loss=4.283581256866455
I0307 02:05:00.224439 140037979092736 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.0861117839813232, loss=4.265393257141113
I0307 02:05:38.561520 140037970700032 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.9524011611938477, loss=4.302163124084473
I0307 02:06:16.364894 140037979092736 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.18333101272583, loss=4.284548282623291
I0307 02:06:54.726696 140037970700032 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.6208572387695312, loss=4.298776149749756
I0307 02:07:33.001860 140037979092736 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.5869579315185547, loss=4.333835124969482
I0307 02:07:56.320780 140193455334592 spec.py:321] Evaluating on the training split.
I0307 02:08:08.521497 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 02:08:30.841226 140193455334592 spec.py:349] Evaluating on the test split.
I0307 02:08:32.669880 140193455334592 submission_runner.py:469] Time since start: 2882.43s, 	Step: 6763, 	{'train/accuracy': 0.4204599857330322, 'train/loss': 2.8091726303100586, 'validation/accuracy': 0.38106000423431396, 'validation/loss': 2.999786138534546, 'validation/num_examples': 50000, 'test/accuracy': 0.2891000211238861, 'test/loss': 3.566889524459839, 'test/num_examples': 10000, 'score': 2606.5659952163696, 'total_duration': 2882.425507545471, 'accumulated_submission_time': 2606.5659952163696, 'accumulated_eval_time': 274.9814684391022, 'accumulated_logging_time': 0.17117547988891602}
I0307 02:08:32.694320 140037970700032 logging_writer.py:48] [6763] accumulated_eval_time=274.981, accumulated_logging_time=0.171175, accumulated_submission_time=2606.57, global_step=6763, preemption_count=0, score=2606.57, test/accuracy=0.2891, test/loss=3.56689, test/num_examples=10000, total_duration=2882.43, train/accuracy=0.42046, train/loss=2.80917, validation/accuracy=0.38106, validation/loss=2.99979, validation/num_examples=50000
I0307 02:08:46.984473 140037979092736 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.10227632522583, loss=4.274785041809082
I0307 02:09:25.063147 140037970700032 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.1307895183563232, loss=4.2013139724731445
I0307 02:10:03.093235 140037979092736 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.9111452102661133, loss=4.186390399932861
I0307 02:10:40.716764 140037970700032 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.5854263305664062, loss=4.230979919433594
I0307 02:11:18.877278 140037979092736 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.440683126449585, loss=4.196336269378662
I0307 02:11:56.650189 140037970700032 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.4606964588165283, loss=4.142154216766357
I0307 02:12:34.463836 140037979092736 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.4727189540863037, loss=4.239586353302002
I0307 02:13:12.189083 140037970700032 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.9445196390151978, loss=4.050028324127197
I0307 02:13:49.752068 140037979092736 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.821279525756836, loss=4.094062805175781
I0307 02:14:27.599904 140037970700032 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.8811562061309814, loss=4.072533130645752
I0307 02:15:05.448866 140037979092736 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.6684393882751465, loss=4.055704593658447
I0307 02:15:43.255666 140037970700032 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.719998836517334, loss=4.079675674438477
I0307 02:16:20.909144 140037979092736 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.0624170303344727, loss=3.9947495460510254
I0307 02:16:58.409064 140037970700032 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.2702720165252686, loss=4.001492500305176
I0307 02:17:02.935296 140193455334592 spec.py:321] Evaluating on the training split.
I0307 02:17:15.389045 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 02:17:37.677083 140193455334592 spec.py:349] Evaluating on the test split.
I0307 02:17:39.503506 140193455334592 submission_runner.py:469] Time since start: 3429.26s, 	Step: 8113, 	{'train/accuracy': 0.48084741830825806, 'train/loss': 2.4457743167877197, 'validation/accuracy': 0.43751999735832214, 'validation/loss': 2.665273666381836, 'validation/num_examples': 50000, 'test/accuracy': 0.3246000111103058, 'test/loss': 3.3327462673187256, 'test/num_examples': 10000, 'score': 3116.6582701206207, 'total_duration': 3429.259141921997, 'accumulated_submission_time': 3116.6582701206207, 'accumulated_eval_time': 311.5496428012848, 'accumulated_logging_time': 0.21140384674072266}
I0307 02:17:39.544939 140037979092736 logging_writer.py:48] [8113] accumulated_eval_time=311.55, accumulated_logging_time=0.211404, accumulated_submission_time=3116.66, global_step=8113, preemption_count=0, score=3116.66, test/accuracy=0.3246, test/loss=3.33275, test/num_examples=10000, total_duration=3429.26, train/accuracy=0.480847, train/loss=2.44577, validation/accuracy=0.43752, validation/loss=2.66527, validation/num_examples=50000
I0307 02:18:12.675778 140037970700032 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.428694248199463, loss=4.050724983215332
I0307 02:18:49.966257 140037979092736 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5629138946533203, loss=4.0784993171691895
I0307 02:19:27.920355 140037970700032 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.1850738525390625, loss=3.997913122177124
I0307 02:20:05.755259 140037979092736 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.570688486099243, loss=4.044434547424316
I0307 02:20:43.695431 140037970700032 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.3306655883789062, loss=3.982194662094116
I0307 02:21:21.678193 140037979092736 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.7488603591918945, loss=4.04287052154541
I0307 02:21:59.544166 140037970700032 logging_writer.py:48] [8800] global_step=8800, grad_norm=3.428046226501465, loss=3.969768524169922
I0307 02:22:37.541571 140037979092736 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.6299519538879395, loss=3.9595072269439697
I0307 02:23:15.425448 140037970700032 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.7419207096099854, loss=3.9485645294189453
I0307 02:23:53.276071 140037979092736 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.1691701412200928, loss=3.9665708541870117
I0307 02:24:31.282155 140037970700032 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.0848731994628906, loss=4.043812274932861
I0307 02:25:09.448886 140037979092736 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.0680322647094727, loss=4.024289608001709
I0307 02:25:47.506645 140037970700032 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.8121914863586426, loss=3.902655601501465
I0307 02:26:09.812995 140193455334592 spec.py:321] Evaluating on the training split.
I0307 02:26:25.916939 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 02:26:46.269255 140193455334592 spec.py:349] Evaluating on the test split.
I0307 02:26:48.095795 140193455334592 submission_runner.py:469] Time since start: 3977.85s, 	Step: 9460, 	{'train/accuracy': 0.5285794138908386, 'train/loss': 2.2446634769439697, 'validation/accuracy': 0.48409998416900635, 'validation/loss': 2.468783140182495, 'validation/num_examples': 50000, 'test/accuracy': 0.3736000061035156, 'test/loss': 3.1164748668670654, 'test/num_examples': 10000, 'score': 3626.7777016162872, 'total_duration': 3977.8514297008514, 'accumulated_submission_time': 3626.7777016162872, 'accumulated_eval_time': 349.83240580558777, 'accumulated_logging_time': 0.26887059211730957}
I0307 02:26:48.167891 140037979092736 logging_writer.py:48] [9460] accumulated_eval_time=349.832, accumulated_logging_time=0.268871, accumulated_submission_time=3626.78, global_step=9460, preemption_count=0, score=3626.78, test/accuracy=0.3736, test/loss=3.11647, test/num_examples=10000, total_duration=3977.85, train/accuracy=0.528579, train/loss=2.24466, validation/accuracy=0.4841, validation/loss=2.46878, validation/num_examples=50000
I0307 02:27:03.761020 140037970700032 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.1604294776916504, loss=3.8633265495300293
I0307 02:27:41.936873 140037979092736 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.372802495956421, loss=3.8623032569885254
I0307 02:28:19.942301 140037970700032 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.8177146911621094, loss=3.849874258041382
I0307 02:28:57.962147 140037979092736 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.8981666564941406, loss=3.945690631866455
I0307 02:29:36.170715 140037970700032 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.0240015983581543, loss=3.9025766849517822
I0307 02:30:13.855060 140037979092736 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.0842092037200928, loss=3.859515905380249
I0307 02:30:51.246738 140037970700032 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.4406256675720215, loss=3.8314638137817383
I0307 02:31:28.502152 140037979092736 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.9567513465881348, loss=3.8714756965637207
I0307 02:32:05.824467 140037970700032 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.1220526695251465, loss=3.90175724029541
I0307 02:32:43.278768 140037979092736 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.954631805419922, loss=3.8198375701904297
I0307 02:33:21.111886 140037970700032 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.2272768020629883, loss=3.8123185634613037
I0307 02:33:58.817833 140037979092736 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.6520159244537354, loss=3.7597553730010986
I0307 02:34:37.097267 140037970700032 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.397832155227661, loss=3.8356587886810303
I0307 02:35:14.997042 140037979092736 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.2839155197143555, loss=3.771136999130249
I0307 02:35:18.144310 140193455334592 spec.py:321] Evaluating on the training split.
I0307 02:35:32.642801 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 02:35:54.721545 140193455334592 spec.py:349] Evaluating on the test split.
I0307 02:35:56.520252 140193455334592 submission_runner.py:469] Time since start: 4526.28s, 	Step: 10809, 	{'train/accuracy': 0.5678810477256775, 'train/loss': 2.0701522827148438, 'validation/accuracy': 0.5170199871063232, 'validation/loss': 2.3050551414489746, 'validation/num_examples': 50000, 'test/accuracy': 0.4020000100135803, 'test/loss': 2.9642550945281982, 'test/num_examples': 10000, 'score': 4136.579527139664, 'total_duration': 4526.2758820056915, 'accumulated_submission_time': 4136.579527139664, 'accumulated_eval_time': 388.20830941200256, 'accumulated_logging_time': 0.3499281406402588}
I0307 02:35:56.576609 140037970700032 logging_writer.py:48] [10809] accumulated_eval_time=388.208, accumulated_logging_time=0.349928, accumulated_submission_time=4136.58, global_step=10809, preemption_count=0, score=4136.58, test/accuracy=0.402, test/loss=2.96426, test/num_examples=10000, total_duration=4526.28, train/accuracy=0.567881, train/loss=2.07015, validation/accuracy=0.51702, validation/loss=2.30506, validation/num_examples=50000
I0307 02:36:31.247598 140037979092736 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.7096364498138428, loss=3.7908453941345215
I0307 02:37:09.437543 140037970700032 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.3809027671813965, loss=3.785276174545288
I0307 02:37:47.210786 140037979092736 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.1366658210754395, loss=3.8126039505004883
I0307 02:38:25.413838 140037970700032 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.988178014755249, loss=3.711254119873047
I0307 02:39:02.947180 140037979092736 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.0891873836517334, loss=3.730285406112671
I0307 02:39:40.926337 140037970700032 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.3859895467758179, loss=3.7518105506896973
I0307 02:40:18.898403 140037979092736 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.2930779457092285, loss=3.717841625213623
I0307 02:40:57.303908 140037970700032 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.5166605710983276, loss=3.73187518119812
I0307 02:41:34.736171 140037979092736 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.6190530061721802, loss=3.6786956787109375
I0307 02:42:12.689963 140037970700032 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.5855119228363037, loss=3.7182445526123047
I0307 02:42:50.771736 140037979092736 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.7111461162567139, loss=3.718628406524658
I0307 02:43:28.400180 140037970700032 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.8445122241973877, loss=3.620121955871582
I0307 02:44:06.702833 140037979092736 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.50344181060791, loss=3.693915367126465
I0307 02:44:26.909230 140193455334592 spec.py:321] Evaluating on the training split.
I0307 02:44:41.943137 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 02:45:04.081687 140193455334592 spec.py:349] Evaluating on the test split.
I0307 02:45:05.896294 140193455334592 submission_runner.py:469] Time since start: 5075.65s, 	Step: 12154, 	{'train/accuracy': 0.5869140625, 'train/loss': 1.9589930772781372, 'validation/accuracy': 0.5384199619293213, 'validation/loss': 2.18542218208313, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.8328115940093994, 'test/num_examples': 10000, 'score': 4646.768032312393, 'total_duration': 5075.651927471161, 'accumulated_submission_time': 4646.768032312393, 'accumulated_eval_time': 427.195335149765, 'accumulated_logging_time': 0.4144785404205322}
I0307 02:45:05.933130 140037970700032 logging_writer.py:48] [12154] accumulated_eval_time=427.195, accumulated_logging_time=0.414479, accumulated_submission_time=4646.77, global_step=12154, preemption_count=0, score=4646.77, test/accuracy=0.4158, test/loss=2.83281, test/num_examples=10000, total_duration=5075.65, train/accuracy=0.586914, train/loss=1.95899, validation/accuracy=0.53842, validation/loss=2.18542, validation/num_examples=50000
I0307 02:45:23.912429 140037979092736 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.5320229530334473, loss=3.726120710372925
I0307 02:46:01.888627 140037970700032 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.4005937576293945, loss=3.6749229431152344
I0307 02:46:39.545136 140037979092736 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.9241281747817993, loss=3.7039904594421387
I0307 02:47:21.093579 140037970700032 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.9584795236587524, loss=3.7142701148986816
I0307 02:48:00.304986 140037979092736 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.535382628440857, loss=3.7254714965820312
I0307 02:48:39.688004 140037970700032 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.7804570198059082, loss=3.6782045364379883
I0307 02:49:18.746759 140037979092736 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.530698299407959, loss=3.6589415073394775
I0307 02:49:58.967906 140037970700032 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.2322921752929688, loss=3.5851340293884277
I0307 02:50:36.879471 140037979092736 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.8157082796096802, loss=3.571838140487671
I0307 02:51:14.735758 140037970700032 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.8503730297088623, loss=3.6027731895446777
I0307 02:51:53.001835 140037979092736 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.499841332435608, loss=3.65380597114563
I0307 02:52:51.620670 140037970700032 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4299637079238892, loss=3.608633518218994
I0307 02:53:36.132888 140193455334592 spec.py:321] Evaluating on the training split.
I0307 02:53:53.116631 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 02:54:17.206900 140193455334592 spec.py:349] Evaluating on the test split.
I0307 02:54:18.976647 140193455334592 submission_runner.py:469] Time since start: 5628.73s, 	Step: 13364, 	{'train/accuracy': 0.6243223547935486, 'train/loss': 1.831497311592102, 'validation/accuracy': 0.5644599795341492, 'validation/loss': 2.0840139389038086, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.7294445037841797, 'test/num_examples': 10000, 'score': 5156.835231304169, 'total_duration': 5628.732272863388, 'accumulated_submission_time': 5156.835231304169, 'accumulated_eval_time': 470.03906059265137, 'accumulated_logging_time': 0.4597651958465576}
I0307 02:54:19.046832 140037979092736 logging_writer.py:48] [13364] accumulated_eval_time=470.039, accumulated_logging_time=0.459765, accumulated_submission_time=5156.84, global_step=13364, preemption_count=0, score=5156.84, test/accuracy=0.4473, test/loss=2.72944, test/num_examples=10000, total_duration=5628.73, train/accuracy=0.624322, train/loss=1.8315, validation/accuracy=0.56446, validation/loss=2.08401, validation/num_examples=50000
I0307 02:54:32.893932 140037970700032 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.975096583366394, loss=3.574321985244751
I0307 02:55:26.398557 140037979092736 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.7782515287399292, loss=3.5935137271881104
I0307 02:56:03.411564 140037970700032 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.9489936828613281, loss=3.5310776233673096
I0307 02:57:04.742970 140037979092736 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.7190099954605103, loss=3.581510305404663
I0307 02:58:05.182975 140037970700032 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.3385413885116577, loss=3.6132001876831055
I0307 02:58:53.540743 140037979092736 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.6020909547805786, loss=3.570549726486206
I0307 02:59:32.049749 140037970700032 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.6875730752944946, loss=3.574589252471924
I0307 03:00:10.292689 140037979092736 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5187982320785522, loss=3.5696725845336914
I0307 03:00:48.441587 140037970700032 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.847206950187683, loss=3.5896167755126953
I0307 03:01:26.691005 140037979092736 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.2057225704193115, loss=3.567290782928467
I0307 03:02:04.901244 140037970700032 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.8454787731170654, loss=3.4714338779449463
I0307 03:02:42.518848 140037979092736 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.43772292137146, loss=3.52703595161438
I0307 03:02:49.178351 140193455334592 spec.py:321] Evaluating on the training split.
I0307 03:03:04.085741 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 03:03:26.004516 140193455334592 spec.py:349] Evaluating on the test split.
I0307 03:03:27.795674 140193455334592 submission_runner.py:469] Time since start: 6177.55s, 	Step: 14519, 	{'train/accuracy': 0.6479790806770325, 'train/loss': 1.6753435134887695, 'validation/accuracy': 0.5806999802589417, 'validation/loss': 1.978210687637329, 'validation/num_examples': 50000, 'test/accuracy': 0.45730000734329224, 'test/loss': 2.624202013015747, 'test/num_examples': 10000, 'score': 5666.841965913773, 'total_duration': 6177.551301240921, 'accumulated_submission_time': 5666.841965913773, 'accumulated_eval_time': 508.65633749961853, 'accumulated_logging_time': 0.5429282188415527}
I0307 03:03:27.830688 140037970700032 logging_writer.py:48] [14519] accumulated_eval_time=508.656, accumulated_logging_time=0.542928, accumulated_submission_time=5666.84, global_step=14519, preemption_count=0, score=5666.84, test/accuracy=0.4573, test/loss=2.6242, test/num_examples=10000, total_duration=6177.55, train/accuracy=0.647979, train/loss=1.67534, validation/accuracy=0.5807, validation/loss=1.97821, validation/num_examples=50000
I0307 03:03:59.153762 140037979092736 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.6446208953857422, loss=3.557651996612549
I0307 03:04:39.991209 140037970700032 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.4212093353271484, loss=3.5517234802246094
I0307 03:05:18.398166 140037979092736 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.4031046628952026, loss=3.4306044578552246
I0307 03:05:58.328979 140037970700032 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.582120656967163, loss=3.5190625190734863
I0307 03:07:00.932648 140037979092736 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.7378047704696655, loss=3.48000168800354
I0307 03:07:52.153114 140037970700032 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5676041841506958, loss=3.5580263137817383
I0307 03:08:47.223159 140037979092736 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.5427684783935547, loss=3.617147445678711
I0307 03:09:31.910687 140037970700032 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.48090398311615, loss=3.459869384765625
I0307 03:10:09.249607 140037979092736 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.5108182430267334, loss=3.6641507148742676
I0307 03:10:47.087701 140037970700032 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.8693093061447144, loss=3.494507312774658
I0307 03:11:24.811819 140037979092736 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.5092134475708008, loss=3.5701189041137695
I0307 03:11:58.147479 140193455334592 spec.py:321] Evaluating on the training split.
I0307 03:12:12.614366 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 03:12:35.084367 140193455334592 spec.py:349] Evaluating on the test split.
I0307 03:12:36.846375 140193455334592 submission_runner.py:469] Time since start: 6726.60s, 	Step: 15688, 	{'train/accuracy': 0.6679089665412903, 'train/loss': 1.5840985774993896, 'validation/accuracy': 0.5860399603843689, 'validation/loss': 1.958055853843689, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.608912706375122, 'test/num_examples': 10000, 'score': 6177.019730329514, 'total_duration': 6726.601873874664, 'accumulated_submission_time': 6177.019730329514, 'accumulated_eval_time': 547.355060338974, 'accumulated_logging_time': 0.5853581428527832}
I0307 03:12:36.894897 140037970700032 logging_writer.py:48] [15688] accumulated_eval_time=547.355, accumulated_logging_time=0.585358, accumulated_submission_time=6177.02, global_step=15688, preemption_count=0, score=6177.02, test/accuracy=0.4653, test/loss=2.60891, test/num_examples=10000, total_duration=6726.6, train/accuracy=0.667909, train/loss=1.5841, validation/accuracy=0.58604, validation/loss=1.95806, validation/num_examples=50000
I0307 03:12:41.963234 140037979092736 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.3400816917419434, loss=3.480011463165283
I0307 03:13:21.854414 140037970700032 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.9140582084655762, loss=3.4937050342559814
I0307 03:13:59.868006 140037979092736 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.6380765438079834, loss=3.558422803878784
I0307 03:14:37.932683 140037970700032 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.2460588216781616, loss=3.498680353164673
I0307 03:15:15.847029 140037979092736 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.559982419013977, loss=3.5377278327941895
I0307 03:15:53.915105 140037970700032 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.269002914428711, loss=3.484586715698242
I0307 03:16:37.610994 140037979092736 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.4832550287246704, loss=3.5313708782196045
I0307 03:17:17.235924 140037970700032 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.1990530490875244, loss=3.6255834102630615
I0307 03:17:55.044578 140037979092736 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.4691762924194336, loss=3.4670214653015137
I0307 03:18:33.640232 140037970700032 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.144323706626892, loss=3.4300997257232666
I0307 03:19:12.285696 140037979092736 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.2996915578842163, loss=3.374150037765503
I0307 03:19:50.611108 140037970700032 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.6194285154342651, loss=3.5562257766723633
I0307 03:20:28.122564 140037979092736 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.085568904876709, loss=3.4757156372070312
I0307 03:21:06.864190 140037970700032 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.4178245067596436, loss=3.3884222507476807
I0307 03:21:06.877252 140193455334592 spec.py:321] Evaluating on the training split.
I0307 03:21:20.009831 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 03:21:42.000761 140193455334592 spec.py:349] Evaluating on the test split.
I0307 03:21:43.928325 140193455334592 submission_runner.py:469] Time since start: 7273.68s, 	Step: 17001, 	{'train/accuracy': 0.6952925324440002, 'train/loss': 1.5021928548812866, 'validation/accuracy': 0.6085799932479858, 'validation/loss': 1.8874375820159912, 'validation/num_examples': 50000, 'test/accuracy': 0.4807000160217285, 'test/loss': 2.53777813911438, 'test/num_examples': 10000, 'score': 6686.868481636047, 'total_duration': 7273.683918714523, 'accumulated_submission_time': 6686.868481636047, 'accumulated_eval_time': 584.4060409069061, 'accumulated_logging_time': 0.6412758827209473}
I0307 03:21:43.973674 140037979092736 logging_writer.py:48] [17001] accumulated_eval_time=584.406, accumulated_logging_time=0.641276, accumulated_submission_time=6686.87, global_step=17001, preemption_count=0, score=6686.87, test/accuracy=0.4807, test/loss=2.53778, test/num_examples=10000, total_duration=7273.68, train/accuracy=0.695293, train/loss=1.50219, validation/accuracy=0.60858, validation/loss=1.88744, validation/num_examples=50000
I0307 03:22:31.956138 140037970700032 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.573938250541687, loss=3.512843608856201
I0307 03:23:35.154227 140037979092736 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.5314669609069824, loss=3.492629051208496
I0307 03:24:16.374321 140037970700032 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.585599660873413, loss=3.3388941287994385
I0307 03:24:54.624295 140037979092736 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.3549517393112183, loss=3.436070680618286
I0307 03:25:33.200324 140037970700032 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.379331350326538, loss=3.4698688983917236
I0307 03:26:11.039128 140037979092736 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.2553820610046387, loss=3.526965618133545
I0307 03:26:49.899387 140037970700032 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.500980019569397, loss=3.4243388175964355
I0307 03:27:28.225262 140037979092736 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.6706396341323853, loss=3.4638099670410156
I0307 03:28:06.877278 140037970700032 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.434637188911438, loss=3.4665679931640625
I0307 03:28:45.356401 140037979092736 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.4793100357055664, loss=3.4049978256225586
I0307 03:29:23.966183 140037970700032 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.412007451057434, loss=3.43015718460083
I0307 03:30:02.381368 140037979092736 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.694487452507019, loss=3.373950481414795
I0307 03:30:13.934981 140193455334592 spec.py:321] Evaluating on the training split.
I0307 03:30:28.928529 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 03:30:50.215446 140193455334592 spec.py:349] Evaluating on the test split.
I0307 03:30:52.075112 140193455334592 submission_runner.py:469] Time since start: 7821.83s, 	Step: 18231, 	{'train/accuracy': 0.6957708597183228, 'train/loss': 1.5066120624542236, 'validation/accuracy': 0.6098200082778931, 'validation/loss': 1.8729736804962158, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.5313045978546143, 'test/num_examples': 10000, 'score': 7196.698030948639, 'total_duration': 7821.830721855164, 'accumulated_submission_time': 7196.698030948639, 'accumulated_eval_time': 622.546112537384, 'accumulated_logging_time': 0.6942312717437744}
I0307 03:30:52.151000 140037970700032 logging_writer.py:48] [18231] accumulated_eval_time=622.546, accumulated_logging_time=0.694231, accumulated_submission_time=7196.7, global_step=18231, preemption_count=0, score=7196.7, test/accuracy=0.4827, test/loss=2.5313, test/num_examples=10000, total_duration=7821.83, train/accuracy=0.695771, train/loss=1.50661, validation/accuracy=0.60982, validation/loss=1.87297, validation/num_examples=50000
I0307 03:31:18.747160 140037979092736 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.1961172819137573, loss=3.4456849098205566
I0307 03:31:56.251875 140037970700032 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.3809254169464111, loss=3.3872201442718506
I0307 03:32:36.887435 140037979092736 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.3308995962142944, loss=3.4220733642578125
I0307 03:33:15.226242 140037970700032 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.3278982639312744, loss=3.396306276321411
I0307 03:33:53.796420 140037979092736 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.333470106124878, loss=3.4019322395324707
I0307 03:34:31.660522 140037970700032 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.3531134128570557, loss=3.452019691467285
I0307 03:35:09.517326 140037979092736 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.248038649559021, loss=3.3297505378723145
I0307 03:35:47.729384 140037970700032 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.7266860008239746, loss=3.4295144081115723
I0307 03:36:26.142997 140037979092736 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.3441590070724487, loss=3.518637180328369
I0307 03:37:04.518128 140037970700032 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.6710474491119385, loss=3.4113574028015137
I0307 03:37:42.874617 140037979092736 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.3121318817138672, loss=3.3736374378204346
I0307 03:38:20.891143 140037970700032 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.4838186502456665, loss=3.4079225063323975
I0307 03:38:59.180126 140037979092736 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.261462688446045, loss=3.41257905960083
I0307 03:39:22.189078 140193455334592 spec.py:321] Evaluating on the training split.
I0307 03:39:35.323453 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 03:39:58.928830 140193455334592 spec.py:349] Evaluating on the test split.
I0307 03:40:00.688995 140193455334592 submission_runner.py:469] Time since start: 8370.44s, 	Step: 19561, 	{'train/accuracy': 0.7120735049247742, 'train/loss': 1.403428316116333, 'validation/accuracy': 0.6147199869155884, 'validation/loss': 1.822716236114502, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.4957051277160645, 'test/num_examples': 10000, 'score': 7706.595965862274, 'total_duration': 8370.44462132454, 'accumulated_submission_time': 7706.595965862274, 'accumulated_eval_time': 661.0459854602814, 'accumulated_logging_time': 0.7781500816345215}
I0307 03:40:00.732223 140037970700032 logging_writer.py:48] [19561] accumulated_eval_time=661.046, accumulated_logging_time=0.77815, accumulated_submission_time=7706.6, global_step=19561, preemption_count=0, score=7706.6, test/accuracy=0.4797, test/loss=2.49571, test/num_examples=10000, total_duration=8370.44, train/accuracy=0.712074, train/loss=1.40343, validation/accuracy=0.61472, validation/loss=1.82272, validation/num_examples=50000
I0307 03:40:16.006234 140037979092736 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.2652535438537598, loss=3.4217793941497803
I0307 03:41:07.782754 140037970700032 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.2881910800933838, loss=3.360755443572998
I0307 03:41:48.934491 140037979092736 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.3737969398498535, loss=3.2932660579681396
I0307 03:42:27.408222 140037970700032 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.2944427728652954, loss=3.399672746658325
I0307 03:43:05.458718 140037979092736 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.820662498474121, loss=3.322233200073242
I0307 03:43:44.044890 140037970700032 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.6590849161148071, loss=3.4395744800567627
I0307 03:44:22.420456 140037979092736 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.446141242980957, loss=3.4092328548431396
I0307 03:45:00.708436 140037970700032 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.1828981637954712, loss=3.435638666152954
I0307 03:45:38.906641 140037979092736 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4893821477890015, loss=3.359307289123535
I0307 03:46:17.309666 140037970700032 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.4474983215332031, loss=3.48537278175354
I0307 03:46:55.720939 140037979092736 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.579551100730896, loss=3.4017813205718994
I0307 03:47:33.981035 140037970700032 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.1513817310333252, loss=3.3800554275512695
I0307 03:48:12.357939 140037979092736 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7423477172851562, loss=3.3803634643554688
I0307 03:48:30.953664 140193455334592 spec.py:321] Evaluating on the training split.
I0307 03:48:44.415827 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 03:49:06.237200 140193455334592 spec.py:349] Evaluating on the test split.
I0307 03:49:08.066665 140193455334592 submission_runner.py:469] Time since start: 8917.78s, 	Step: 20850, 	{'train/accuracy': 0.7258848547935486, 'train/loss': 1.3886221647262573, 'validation/accuracy': 0.6249200105667114, 'validation/loss': 1.8135240077972412, 'validation/num_examples': 50000, 'test/accuracy': 0.49650001525878906, 'test/loss': 2.465162515640259, 'test/num_examples': 10000, 'score': 8216.687442302704, 'total_duration': 8917.782804250717, 'accumulated_submission_time': 8216.687442302704, 'accumulated_eval_time': 698.1194541454315, 'accumulated_logging_time': 0.8286125659942627}
I0307 03:49:08.105846 140037970700032 logging_writer.py:48] [20850] accumulated_eval_time=698.119, accumulated_logging_time=0.828613, accumulated_submission_time=8216.69, global_step=20850, preemption_count=0, score=8216.69, test/accuracy=0.4965, test/loss=2.46516, test/num_examples=10000, total_duration=8917.78, train/accuracy=0.725885, train/loss=1.38862, validation/accuracy=0.62492, validation/loss=1.81352, validation/num_examples=50000
I0307 03:49:27.471732 140037979092736 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.4174185991287231, loss=3.31412410736084
I0307 03:50:11.114374 140037970700032 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.1870955228805542, loss=3.2991738319396973
I0307 03:50:49.392734 140037979092736 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.3953057527542114, loss=3.4024505615234375
I0307 03:51:27.544166 140037970700032 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.4124228954315186, loss=3.331174850463867
I0307 03:52:05.863112 140037979092736 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.3473552465438843, loss=3.3012452125549316
I0307 03:52:44.135089 140037970700032 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.2430615425109863, loss=3.406343460083008
I0307 03:53:22.500239 140037979092736 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.3623346090316772, loss=3.4217398166656494
I0307 03:54:00.952524 140037970700032 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.3157963752746582, loss=3.341597557067871
I0307 03:54:39.289629 140037979092736 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.1965283155441284, loss=3.357440948486328
I0307 03:55:17.651681 140037970700032 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2662328481674194, loss=3.3774726390838623
I0307 03:55:56.241397 140037979092736 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.3878263235092163, loss=3.3928334712982178
I0307 03:56:34.738341 140037970700032 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.6633455753326416, loss=3.35984468460083
I0307 03:57:12.716994 140037979092736 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.2908145189285278, loss=3.3015859127044678
I0307 03:57:38.262912 140193455334592 spec.py:321] Evaluating on the training split.
I0307 03:57:54.798076 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 03:58:16.693747 140193455334592 spec.py:349] Evaluating on the test split.
I0307 03:58:18.527082 140193455334592 submission_runner.py:469] Time since start: 9468.26s, 	Step: 22167, 	{'train/accuracy': 0.725605845451355, 'train/loss': 1.304843544960022, 'validation/accuracy': 0.6233199834823608, 'validation/loss': 1.7497003078460693, 'validation/num_examples': 50000, 'test/accuracy': 0.5016000270843506, 'test/loss': 2.394624710083008, 'test/num_examples': 10000, 'score': 8726.709604501724, 'total_duration': 9468.257493019104, 'accumulated_submission_time': 8726.709604501724, 'accumulated_eval_time': 738.3583631515503, 'accumulated_logging_time': 0.8751790523529053}
I0307 03:58:18.614877 140037970700032 logging_writer.py:48] [22167] accumulated_eval_time=738.358, accumulated_logging_time=0.875179, accumulated_submission_time=8726.71, global_step=22167, preemption_count=0, score=8726.71, test/accuracy=0.5016, test/loss=2.39462, test/num_examples=10000, total_duration=9468.26, train/accuracy=0.725606, train/loss=1.30484, validation/accuracy=0.62332, validation/loss=1.7497, validation/num_examples=50000
I0307 03:58:31.632539 140037979092736 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.3215432167053223, loss=3.3666634559631348
I0307 03:59:09.961193 140037970700032 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.2575269937515259, loss=3.287355422973633
I0307 03:59:48.483394 140037979092736 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.3764413595199585, loss=3.364495038986206
I0307 04:00:27.053299 140037970700032 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.345231533050537, loss=3.3469467163085938
I0307 04:01:05.296505 140037979092736 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.3735607862472534, loss=3.3166310787200928
I0307 04:01:43.387146 140037970700032 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.3079757690429688, loss=3.3333029747009277
I0307 04:02:22.244986 140037979092736 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.098354697227478, loss=3.4048757553100586
I0307 04:03:00.689983 140037970700032 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.6211400032043457, loss=3.3879053592681885
I0307 04:03:38.767582 140037979092736 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0644232034683228, loss=3.2750985622406006
I0307 04:04:16.891543 140037970700032 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.49344003200531, loss=3.4009783267974854
I0307 04:04:55.212554 140037979092736 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.4774421453475952, loss=3.2782928943634033
I0307 04:05:33.626336 140037970700032 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.1751372814178467, loss=3.3633272647857666
I0307 04:06:11.980624 140037979092736 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.5960041284561157, loss=3.276823043823242
I0307 04:06:48.849254 140193455334592 spec.py:321] Evaluating on the training split.
I0307 04:07:06.006744 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 04:07:28.644108 140193455334592 spec.py:349] Evaluating on the test split.
I0307 04:07:30.437147 140193455334592 submission_runner.py:469] Time since start: 10020.19s, 	Step: 23498, 	{'train/accuracy': 0.7215202450752258, 'train/loss': 1.3562525510787964, 'validation/accuracy': 0.6268199682235718, 'validation/loss': 1.7633411884307861, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.435206413269043, 'test/num_examples': 10000, 'score': 9236.749849796295, 'total_duration': 10020.192776679993, 'accumulated_submission_time': 9236.749849796295, 'accumulated_eval_time': 779.9462132453918, 'accumulated_logging_time': 0.990614652633667}
I0307 04:07:30.473403 140037970700032 logging_writer.py:48] [23498] accumulated_eval_time=779.946, accumulated_logging_time=0.990615, accumulated_submission_time=9236.75, global_step=23498, preemption_count=0, score=9236.75, test/accuracy=0.4974, test/loss=2.43521, test/num_examples=10000, total_duration=10020.2, train/accuracy=0.72152, train/loss=1.35625, validation/accuracy=0.62682, validation/loss=1.76334, validation/num_examples=50000
I0307 04:07:31.683716 140037979092736 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.2838616371154785, loss=3.353175163269043
I0307 04:08:09.892105 140037970700032 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.2286146879196167, loss=3.3714232444763184
I0307 04:08:48.432515 140037979092736 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.6583969593048096, loss=3.3693323135375977
I0307 04:09:26.692599 140037970700032 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.372456669807434, loss=3.316063642501831
I0307 04:10:05.167896 140037979092736 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.3781514167785645, loss=3.3187508583068848
I0307 04:10:43.442628 140037970700032 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2928751707077026, loss=3.289583206176758
I0307 04:11:22.246768 140037979092736 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.5771636962890625, loss=3.3409194946289062
I0307 04:12:00.263719 140037970700032 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.4049745798110962, loss=3.338960647583008
I0307 04:12:38.657655 140037979092736 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.3126373291015625, loss=3.307642698287964
I0307 04:13:16.892928 140037970700032 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.6227306127548218, loss=3.3856914043426514
I0307 04:13:55.376228 140037979092736 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.217215895652771, loss=3.3270063400268555
I0307 04:14:33.849915 140037970700032 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.3176575899124146, loss=3.3685543537139893
I0307 04:15:12.313927 140037979092736 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.4240142107009888, loss=3.4384570121765137
I0307 04:15:50.378992 140037970700032 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.53533136844635, loss=3.207520008087158
I0307 04:16:00.777951 140193455334592 spec.py:321] Evaluating on the training split.
I0307 04:16:13.883462 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 04:16:36.604344 140193455334592 spec.py:349] Evaluating on the test split.
I0307 04:16:38.413023 140193455334592 submission_runner.py:469] Time since start: 10568.17s, 	Step: 24828, 	{'train/accuracy': 0.7252271771430969, 'train/loss': 1.3312427997589111, 'validation/accuracy': 0.6343199610710144, 'validation/loss': 1.7292760610580444, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.3971753120422363, 'test/num_examples': 10000, 'score': 9746.8926486969, 'total_duration': 10568.168626785278, 'accumulated_submission_time': 9746.8926486969, 'accumulated_eval_time': 817.5812244415283, 'accumulated_logging_time': 1.0601882934570312}
I0307 04:16:38.545767 140037979092736 logging_writer.py:48] [24828] accumulated_eval_time=817.581, accumulated_logging_time=1.06019, accumulated_submission_time=9746.89, global_step=24828, preemption_count=0, score=9746.89, test/accuracy=0.5058, test/loss=2.39718, test/num_examples=10000, total_duration=10568.2, train/accuracy=0.725227, train/loss=1.33124, validation/accuracy=0.63432, validation/loss=1.72928, validation/num_examples=50000
I0307 04:17:06.370978 140037970700032 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.336491346359253, loss=3.339008331298828
I0307 04:17:44.621450 140037979092736 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.531339406967163, loss=3.349910020828247
I0307 04:18:22.973046 140037970700032 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.333476185798645, loss=3.2664856910705566
I0307 04:19:01.181367 140037979092736 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.416273832321167, loss=3.3265039920806885
I0307 04:19:39.306766 140037970700032 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.6754791736602783, loss=3.3780736923217773
I0307 04:20:17.318929 140037979092736 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.2816059589385986, loss=3.2838635444641113
I0307 04:20:54.585961 140037970700032 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.2695865631103516, loss=3.3331470489501953
I0307 04:21:33.311131 140037979092736 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.2900723218917847, loss=3.2448391914367676
I0307 04:22:11.452149 140037970700032 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.9318454265594482, loss=3.3492302894592285
I0307 04:22:49.808105 140037979092736 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.3943902254104614, loss=3.245344400405884
I0307 04:23:28.065810 140037970700032 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.7899936437606812, loss=3.3114166259765625
I0307 04:24:12.315418 140037979092736 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.248886227607727, loss=3.2759382724761963
I0307 04:24:50.710097 140037970700032 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.3567566871643066, loss=3.2924375534057617
I0307 04:25:08.437498 140193455334592 spec.py:321] Evaluating on the training split.
I0307 04:25:24.799983 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 04:25:46.155568 140193455334592 spec.py:349] Evaluating on the test split.
I0307 04:25:47.965483 140193455334592 submission_runner.py:469] Time since start: 11117.72s, 	Step: 26147, 	{'train/accuracy': 0.7312061190605164, 'train/loss': 1.307316541671753, 'validation/accuracy': 0.6405199766159058, 'validation/loss': 1.6915374994277954, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.3624355792999268, 'test/num_examples': 10000, 'score': 10256.627957105637, 'total_duration': 11117.721102952957, 'accumulated_submission_time': 10256.627957105637, 'accumulated_eval_time': 857.1091570854187, 'accumulated_logging_time': 1.2170064449310303}
I0307 04:25:48.062059 140037979092736 logging_writer.py:48] [26147] accumulated_eval_time=857.109, accumulated_logging_time=1.21701, accumulated_submission_time=10256.6, global_step=26147, preemption_count=0, score=10256.6, test/accuracy=0.5128, test/loss=2.36244, test/num_examples=10000, total_duration=11117.7, train/accuracy=0.731206, train/loss=1.30732, validation/accuracy=0.64052, validation/loss=1.69154, validation/num_examples=50000
I0307 04:26:09.113543 140037970700032 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.1646844148635864, loss=3.325542449951172
I0307 04:26:47.473299 140037979092736 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.5729020833969116, loss=3.3132476806640625
I0307 04:27:25.760668 140037970700032 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3223241567611694, loss=3.267366886138916
I0307 04:28:04.082511 140037979092736 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.4750220775604248, loss=3.355062484741211
I0307 04:28:42.737723 140037970700032 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.306030511856079, loss=3.230893850326538
I0307 04:29:20.871100 140037979092736 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.5884615182876587, loss=3.257873773574829
I0307 04:29:59.467972 140037970700032 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.3423924446105957, loss=3.3633170127868652
I0307 04:30:37.565705 140037979092736 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.2320762872695923, loss=3.2479758262634277
I0307 04:31:15.719553 140037970700032 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.235041618347168, loss=3.275242567062378
I0307 04:31:53.892085 140037979092736 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.3089451789855957, loss=3.3050179481506348
I0307 04:32:31.922417 140037970700032 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.3563412427902222, loss=3.2314655780792236
I0307 04:33:10.112159 140037979092736 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.474305272102356, loss=3.2864015102386475
I0307 04:33:48.295055 140037970700032 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.294277548789978, loss=3.252490997314453
I0307 04:34:18.203383 140193455334592 spec.py:321] Evaluating on the training split.
I0307 04:34:36.660803 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 04:34:58.587557 140193455334592 spec.py:349] Evaluating on the test split.
I0307 04:35:00.418834 140193455334592 submission_runner.py:469] Time since start: 11670.17s, 	Step: 27479, 	{'train/accuracy': 0.7090640664100647, 'train/loss': 1.4385793209075928, 'validation/accuracy': 0.6214199662208557, 'validation/loss': 1.8097518682479858, 'validation/num_examples': 50000, 'test/accuracy': 0.4993000328540802, 'test/loss': 2.4568543434143066, 'test/num_examples': 10000, 'score': 10766.312444925308, 'total_duration': 11670.174454450607, 'accumulated_submission_time': 10766.312444925308, 'accumulated_eval_time': 899.324554681778, 'accumulated_logging_time': 1.6392345428466797}
I0307 04:35:00.512963 140037979092736 logging_writer.py:48] [27479] accumulated_eval_time=899.325, accumulated_logging_time=1.63923, accumulated_submission_time=10766.3, global_step=27479, preemption_count=0, score=10766.3, test/accuracy=0.4993, test/loss=2.45685, test/num_examples=10000, total_duration=11670.2, train/accuracy=0.709064, train/loss=1.43858, validation/accuracy=0.62142, validation/loss=1.80975, validation/num_examples=50000
I0307 04:35:08.891121 140037970700032 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5433783531188965, loss=3.306457996368408
I0307 04:35:47.046359 140037979092736 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.096974492073059, loss=3.2596378326416016
I0307 04:36:24.983215 140037970700032 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.3007583618164062, loss=3.292039155960083
I0307 04:37:03.171798 140037979092736 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.3538751602172852, loss=3.3356375694274902
I0307 04:37:40.716917 140037970700032 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.6711863279342651, loss=3.2983055114746094
I0307 04:38:18.211576 140037979092736 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.3122845888137817, loss=3.3168487548828125
I0307 04:38:55.935164 140037970700032 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.5245931148529053, loss=3.3172383308410645
I0307 04:39:33.933431 140037979092736 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.1789921522140503, loss=3.2837915420532227
I0307 04:40:11.796207 140037970700032 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.2770882844924927, loss=3.303274631500244
I0307 04:40:50.183940 140037979092736 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.3125970363616943, loss=3.3374104499816895
I0307 04:41:28.280892 140037970700032 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.3227711915969849, loss=3.2915937900543213
I0307 04:42:06.531777 140037979092736 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.5098416805267334, loss=3.2611308097839355
I0307 04:42:44.909086 140037970700032 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.416951298713684, loss=3.274026393890381
I0307 04:43:22.861007 140037979092736 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.3968214988708496, loss=3.309145927429199
I0307 04:43:30.527446 140193455334592 spec.py:321] Evaluating on the training split.
I0307 04:43:44.557260 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 04:44:07.894811 140193455334592 spec.py:349] Evaluating on the test split.
I0307 04:44:09.722292 140193455334592 submission_runner.py:469] Time since start: 12219.48s, 	Step: 28821, 	{'train/accuracy': 0.7260442972183228, 'train/loss': 1.318651795387268, 'validation/accuracy': 0.6404199600219727, 'validation/loss': 1.6896063089370728, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.336705446243286, 'test/num_examples': 10000, 'score': 11276.162848711014, 'total_duration': 12219.47790980339, 'accumulated_submission_time': 11276.162848711014, 'accumulated_eval_time': 938.5193445682526, 'accumulated_logging_time': 1.7633538246154785}
I0307 04:44:09.817660 140037970700032 logging_writer.py:48] [28821] accumulated_eval_time=938.519, accumulated_logging_time=1.76335, accumulated_submission_time=11276.2, global_step=28821, preemption_count=0, score=11276.2, test/accuracy=0.5202, test/loss=2.33671, test/num_examples=10000, total_duration=12219.5, train/accuracy=0.726044, train/loss=1.31865, validation/accuracy=0.64042, validation/loss=1.68961, validation/num_examples=50000
I0307 04:44:40.366922 140037979092736 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.483729600906372, loss=3.280095100402832
I0307 04:45:18.588761 140037970700032 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.2558709383010864, loss=3.2582929134368896
I0307 04:45:57.182281 140037979092736 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.3963508605957031, loss=3.2541887760162354
I0307 04:46:35.497730 140037970700032 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.2729724645614624, loss=3.3199353218078613
I0307 04:47:14.126126 140037979092736 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.2311910390853882, loss=3.300245523452759
I0307 04:47:52.315558 140037970700032 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.2605273723602295, loss=3.2333760261535645
I0307 04:48:30.617466 140037979092736 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.4608755111694336, loss=3.3011128902435303
I0307 04:49:08.709182 140037970700032 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.4545701742172241, loss=3.2628626823425293
I0307 04:49:47.112323 140037979092736 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.4093074798583984, loss=3.26576828956604
I0307 04:50:25.345000 140037970700032 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.4622776508331299, loss=3.1840710639953613
I0307 04:51:03.940110 140037979092736 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.2350252866744995, loss=3.265933036804199
I0307 04:51:41.961714 140037970700032 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.3126850128173828, loss=3.318833112716675
I0307 04:52:19.692266 140037979092736 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.3399440050125122, loss=3.288484573364258
I0307 04:52:39.777233 140193455334592 spec.py:321] Evaluating on the training split.
I0307 04:52:53.889041 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 04:53:16.959847 140193455334592 spec.py:349] Evaluating on the test split.
I0307 04:53:18.757717 140193455334592 submission_runner.py:469] Time since start: 12768.51s, 	Step: 30154, 	{'train/accuracy': 0.724031388759613, 'train/loss': 1.3208426237106323, 'validation/accuracy': 0.6448799967765808, 'validation/loss': 1.6764817237854004, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.330254316329956, 'test/num_examples': 10000, 'score': 11785.958069086075, 'total_duration': 12768.51333475113, 'accumulated_submission_time': 11785.958069086075, 'accumulated_eval_time': 977.4997777938843, 'accumulated_logging_time': 1.8911731243133545}
I0307 04:53:18.899111 140037970700032 logging_writer.py:48] [30154] accumulated_eval_time=977.5, accumulated_logging_time=1.89117, accumulated_submission_time=11786, global_step=30154, preemption_count=0, score=11786, test/accuracy=0.5199, test/loss=2.33025, test/num_examples=10000, total_duration=12768.5, train/accuracy=0.724031, train/loss=1.32084, validation/accuracy=0.64488, validation/loss=1.67648, validation/num_examples=50000
I0307 04:53:37.053468 140037979092736 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.2426908016204834, loss=3.234801769256592
I0307 04:54:15.364580 140037970700032 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.3644044399261475, loss=3.2055017948150635
I0307 04:54:53.436377 140037979092736 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.3481682538986206, loss=3.284975290298462
I0307 04:55:31.841933 140037970700032 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.5422782897949219, loss=3.271047830581665
I0307 04:56:10.309698 140037979092736 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.306397795677185, loss=3.2896652221679688
I0307 04:56:48.765943 140037970700032 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.453650712966919, loss=3.278759002685547
I0307 04:57:27.259414 140037979092736 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.4638117551803589, loss=3.299923896789551
I0307 04:58:05.697990 140037970700032 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.3094013929367065, loss=3.2455122470855713
I0307 04:58:44.115538 140037979092736 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.394344687461853, loss=3.1854212284088135
I0307 04:59:22.631727 140037970700032 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.338091254234314, loss=3.3097081184387207
I0307 05:00:00.656154 140037979092736 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.3942837715148926, loss=3.2353806495666504
I0307 05:00:38.557962 140037970700032 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.6947314739227295, loss=3.269958972930908
I0307 05:01:16.452282 140037979092736 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.4410219192504883, loss=3.2147276401519775
I0307 05:01:48.884265 140193455334592 spec.py:321] Evaluating on the training split.
I0307 05:02:03.031054 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 05:02:24.598742 140193455334592 spec.py:349] Evaluating on the test split.
I0307 05:02:26.412744 140193455334592 submission_runner.py:469] Time since start: 13316.17s, 	Step: 31486, 	{'train/accuracy': 0.7248883843421936, 'train/loss': 1.3461346626281738, 'validation/accuracy': 0.6422199606895447, 'validation/loss': 1.7050862312316895, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.3555595874786377, 'test/num_examples': 10000, 'score': 12295.776060342789, 'total_duration': 13316.168333530426, 'accumulated_submission_time': 12295.776060342789, 'accumulated_eval_time': 1015.0281777381897, 'accumulated_logging_time': 2.0690689086914062}
I0307 05:02:26.553954 140037970700032 logging_writer.py:48] [31486] accumulated_eval_time=1015.03, accumulated_logging_time=2.06907, accumulated_submission_time=12295.8, global_step=31486, preemption_count=0, score=12295.8, test/accuracy=0.5177, test/loss=2.35556, test/num_examples=10000, total_duration=13316.2, train/accuracy=0.724888, train/loss=1.34613, validation/accuracy=0.64222, validation/loss=1.70509, validation/num_examples=50000
I0307 05:02:32.375194 140037979092736 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.5156431198120117, loss=3.3037936687469482
I0307 05:03:10.432727 140037970700032 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3431885242462158, loss=3.2766640186309814
I0307 05:03:48.667889 140037979092736 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.4024770259857178, loss=3.2569079399108887
I0307 05:04:26.951495 140037970700032 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.4994651079177856, loss=3.303056478500366
I0307 05:05:05.520308 140037979092736 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.221428394317627, loss=3.2387166023254395
I0307 05:05:43.946890 140037970700032 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.3519957065582275, loss=3.3366498947143555
I0307 05:06:22.372316 140037979092736 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.5184389352798462, loss=3.204808473587036
I0307 05:07:00.709745 140037970700032 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3724541664123535, loss=3.262380838394165
I0307 05:07:38.818943 140037979092736 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.4900639057159424, loss=3.2068471908569336
I0307 05:08:17.311659 140037970700032 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.4238911867141724, loss=3.2874834537506104
I0307 05:08:55.776038 140037979092736 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.3837106227874756, loss=3.255134344100952
I0307 05:09:33.597862 140037970700032 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.3268754482269287, loss=3.192683696746826
I0307 05:10:11.698626 140037979092736 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.5392112731933594, loss=3.360083818435669
I0307 05:10:50.077940 140037970700032 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.413018822669983, loss=3.275218963623047
I0307 05:10:56.637592 140193455334592 spec.py:321] Evaluating on the training split.
I0307 05:11:07.351301 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 05:11:33.122783 140193455334592 spec.py:349] Evaluating on the test split.
I0307 05:11:34.884320 140193455334592 submission_runner.py:469] Time since start: 13864.64s, 	Step: 32818, 	{'train/accuracy': 0.7279974222183228, 'train/loss': 1.2941195964813232, 'validation/accuracy': 0.6498599648475647, 'validation/loss': 1.6550451517105103, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.2909834384918213, 'test/num_examples': 10000, 'score': 12805.667831659317, 'total_duration': 13864.63993859291, 'accumulated_submission_time': 12805.667831659317, 'accumulated_eval_time': 1053.274854183197, 'accumulated_logging_time': 2.2729084491729736}
I0307 05:11:34.992069 140037979092736 logging_writer.py:48] [32818] accumulated_eval_time=1053.27, accumulated_logging_time=2.27291, accumulated_submission_time=12805.7, global_step=32818, preemption_count=0, score=12805.7, test/accuracy=0.5257, test/loss=2.29098, test/num_examples=10000, total_duration=13864.6, train/accuracy=0.727997, train/loss=1.29412, validation/accuracy=0.64986, validation/loss=1.65505, validation/num_examples=50000
I0307 05:12:06.728020 140037970700032 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.2796417474746704, loss=3.2874341011047363
I0307 05:12:45.047226 140037979092736 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.2896299362182617, loss=3.181628942489624
I0307 05:13:23.391963 140037970700032 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.3757288455963135, loss=3.2583906650543213
I0307 05:14:01.416157 140037979092736 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.3264658451080322, loss=3.220452070236206
I0307 05:14:39.903753 140037970700032 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.4079116582870483, loss=3.280078411102295
I0307 05:15:18.339368 140037979092736 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.4200572967529297, loss=3.208197832107544
I0307 05:15:56.560439 140037970700032 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.356011986732483, loss=3.329005718231201
I0307 05:16:34.917277 140037979092736 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.4205433130264282, loss=3.2313570976257324
I0307 05:17:13.192039 140037970700032 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.681320071220398, loss=3.187798261642456
I0307 05:17:51.491605 140037979092736 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.3896751403808594, loss=3.2322113513946533
I0307 05:18:29.587146 140037970700032 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.4063745737075806, loss=3.1861748695373535
I0307 05:19:07.925333 140037979092736 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.4246524572372437, loss=3.3196821212768555
I0307 05:19:46.149415 140037970700032 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.2467756271362305, loss=3.255328416824341
I0307 05:20:05.227124 140193455334592 spec.py:321] Evaluating on the training split.
I0307 05:20:16.125315 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 05:20:42.984926 140193455334592 spec.py:349] Evaluating on the test split.
I0307 05:20:44.811980 140193455334592 submission_runner.py:469] Time since start: 14414.57s, 	Step: 34151, 	{'train/accuracy': 0.7143853306770325, 'train/loss': 1.3342565298080444, 'validation/accuracy': 0.6410999894142151, 'validation/loss': 1.6608558893203735, 'validation/num_examples': 50000, 'test/accuracy': 0.5139999985694885, 'test/loss': 2.3487250804901123, 'test/num_examples': 10000, 'score': 13315.745752573013, 'total_duration': 14414.567600011826, 'accumulated_submission_time': 13315.745752573013, 'accumulated_eval_time': 1092.8596727848053, 'accumulated_logging_time': 2.4062435626983643}
I0307 05:20:44.908753 140037979092736 logging_writer.py:48] [34151] accumulated_eval_time=1092.86, accumulated_logging_time=2.40624, accumulated_submission_time=13315.7, global_step=34151, preemption_count=0, score=13315.7, test/accuracy=0.514, test/loss=2.34873, test/num_examples=10000, total_duration=14414.6, train/accuracy=0.714385, train/loss=1.33426, validation/accuracy=0.6411, validation/loss=1.66086, validation/num_examples=50000
I0307 05:21:03.972268 140037970700032 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.3328968286514282, loss=3.283656597137451
I0307 05:21:42.136453 140037979092736 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.5852693319320679, loss=3.228145122528076
I0307 05:22:20.444624 140037970700032 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.5368542671203613, loss=3.2114806175231934
I0307 05:22:58.738608 140037979092736 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.4362196922302246, loss=3.2074406147003174
I0307 05:23:36.902801 140037970700032 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.4055343866348267, loss=3.2601351737976074
I0307 05:24:15.588453 140037979092736 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.4718745946884155, loss=3.292569160461426
I0307 05:24:53.495206 140037970700032 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.4987505674362183, loss=3.250293254852295
I0307 05:25:31.632340 140037979092736 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.4572577476501465, loss=3.2128829956054688
I0307 05:26:10.030994 140037970700032 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.282037377357483, loss=3.2713232040405273
I0307 05:26:48.369525 140037979092736 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.3699917793273926, loss=3.2688803672790527
I0307 05:27:26.748384 140037970700032 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.585215449333191, loss=3.25109601020813
I0307 05:28:05.086901 140037979092736 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.5079840421676636, loss=3.1993160247802734
I0307 05:28:43.387942 140037970700032 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.5791475772857666, loss=3.2875871658325195
I0307 05:29:14.880090 140193455334592 spec.py:321] Evaluating on the training split.
I0307 05:29:25.039000 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 05:29:52.965003 140193455334592 spec.py:349] Evaluating on the test split.
I0307 05:29:54.734621 140193455334592 submission_runner.py:469] Time since start: 14964.49s, 	Step: 35483, 	{'train/accuracy': 0.7329400181770325, 'train/loss': 1.2845704555511475, 'validation/accuracy': 0.6553599834442139, 'validation/loss': 1.6216375827789307, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.2741339206695557, 'test/num_examples': 10000, 'score': 13825.563044309616, 'total_duration': 14964.490134239197, 'accumulated_submission_time': 13825.563044309616, 'accumulated_eval_time': 1132.7140471935272, 'accumulated_logging_time': 2.5275866985321045}
I0307 05:29:54.817005 140037979092736 logging_writer.py:48] [35483] accumulated_eval_time=1132.71, accumulated_logging_time=2.52759, accumulated_submission_time=13825.6, global_step=35483, preemption_count=0, score=13825.6, test/accuracy=0.5293, test/loss=2.27413, test/num_examples=10000, total_duration=14964.5, train/accuracy=0.73294, train/loss=1.28457, validation/accuracy=0.65536, validation/loss=1.62164, validation/num_examples=50000
I0307 05:30:01.832521 140037970700032 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.5601658821105957, loss=3.315854787826538
I0307 05:30:39.967313 140037979092736 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.4711039066314697, loss=3.2235679626464844
I0307 05:31:18.345674 140037970700032 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.6621341705322266, loss=3.2662782669067383
I0307 05:31:56.571259 140037979092736 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.3837236166000366, loss=3.261810779571533
I0307 05:32:34.720948 140037970700032 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.416121244430542, loss=3.2217659950256348
I0307 05:33:12.915147 140037979092736 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.5429316759109497, loss=3.2479844093322754
I0307 05:33:51.238715 140037970700032 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.3807497024536133, loss=3.156888008117676
I0307 05:34:29.207971 140037979092736 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.526633381843567, loss=3.2751073837280273
I0307 05:35:07.351105 140037970700032 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.4768650531768799, loss=3.2599456310272217
I0307 05:35:45.618563 140037979092736 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.5793545246124268, loss=3.244877576828003
I0307 05:36:23.779766 140037970700032 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.394672155380249, loss=3.196348190307617
I0307 05:37:02.025355 140037979092736 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.4702653884887695, loss=3.269935131072998
I0307 05:37:40.223053 140037970700032 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.5593112707138062, loss=3.216068744659424
I0307 05:38:18.505116 140037979092736 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.2650213241577148, loss=3.145913600921631
I0307 05:38:24.999473 140193455334592 spec.py:321] Evaluating on the training split.
I0307 05:38:36.379475 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 05:38:59.129833 140193455334592 spec.py:349] Evaluating on the test split.
I0307 05:39:00.955341 140193455334592 submission_runner.py:469] Time since start: 15510.71s, 	Step: 36818, 	{'train/accuracy': 0.7346938848495483, 'train/loss': 1.2756190299987793, 'validation/accuracy': 0.661080002784729, 'validation/loss': 1.600752830505371, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.245483160018921, 'test/num_examples': 10000, 'score': 14335.594117879868, 'total_duration': 15510.710866689682, 'accumulated_submission_time': 14335.594117879868, 'accumulated_eval_time': 1168.669767856598, 'accumulated_logging_time': 2.630326747894287}
I0307 05:39:01.010105 140037970700032 logging_writer.py:48] [36818] accumulated_eval_time=1168.67, accumulated_logging_time=2.63033, accumulated_submission_time=14335.6, global_step=36818, preemption_count=0, score=14335.6, test/accuracy=0.5347, test/loss=2.24548, test/num_examples=10000, total_duration=15510.7, train/accuracy=0.734694, train/loss=1.27562, validation/accuracy=0.66108, validation/loss=1.60075, validation/num_examples=50000
I0307 05:39:32.621819 140037979092736 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.4993171691894531, loss=3.17405366897583
I0307 05:40:10.962350 140037970700032 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.2681151628494263, loss=3.1683783531188965
I0307 05:40:49.185101 140037979092736 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.3452359437942505, loss=3.2497034072875977
I0307 05:41:27.554509 140037970700032 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.5417630672454834, loss=3.2524545192718506
I0307 05:42:05.869990 140037979092736 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.3899911642074585, loss=3.164341926574707
I0307 05:42:44.222796 140037970700032 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.4932401180267334, loss=3.193525552749634
I0307 05:43:22.380275 140037979092736 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.6023818254470825, loss=3.234227418899536
I0307 05:44:00.826866 140037970700032 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.4743731021881104, loss=3.2125227451324463
I0307 05:44:38.977797 140037979092736 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.374176025390625, loss=3.228795289993286
I0307 05:45:17.332849 140037970700032 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.5927107334136963, loss=3.2063679695129395
I0307 05:45:55.347066 140037979092736 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.5096018314361572, loss=3.1259543895721436
I0307 05:46:33.922666 140037970700032 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.4561899900436401, loss=3.266329526901245
I0307 05:47:12.231970 140037979092736 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.5804201364517212, loss=3.158498764038086
I0307 05:47:30.957026 140193455334592 spec.py:321] Evaluating on the training split.
I0307 05:47:41.787903 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 05:48:04.581783 140193455334592 spec.py:349] Evaluating on the test split.
I0307 05:48:06.390772 140193455334592 submission_runner.py:469] Time since start: 16056.15s, 	Step: 38150, 	{'train/accuracy': 0.725605845451355, 'train/loss': 1.2949401140213013, 'validation/accuracy': 0.6539599895477295, 'validation/loss': 1.6193041801452637, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.2704482078552246, 'test/num_examples': 10000, 'score': 14845.381632089615, 'total_duration': 16056.14630317688, 'accumulated_submission_time': 14845.381632089615, 'accumulated_eval_time': 1204.1033735275269, 'accumulated_logging_time': 2.712885856628418}
I0307 05:48:06.505440 140037970700032 logging_writer.py:48] [38150] accumulated_eval_time=1204.1, accumulated_logging_time=2.71289, accumulated_submission_time=14845.4, global_step=38150, preemption_count=0, score=14845.4, test/accuracy=0.5262, test/loss=2.27045, test/num_examples=10000, total_duration=16056.1, train/accuracy=0.725606, train/loss=1.29494, validation/accuracy=0.65396, validation/loss=1.6193, validation/num_examples=50000
I0307 05:48:25.884050 140037979092736 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.5417335033416748, loss=3.2091774940490723
I0307 05:49:04.272023 140037970700032 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.6021684408187866, loss=3.3278775215148926
I0307 05:49:42.215560 140037979092736 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.365027904510498, loss=3.163557767868042
I0307 05:50:20.471877 140037970700032 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.3566685914993286, loss=3.1970367431640625
I0307 05:50:58.466762 140037979092736 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.7058005332946777, loss=3.2388529777526855
I0307 05:51:36.683669 140037970700032 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.6365976333618164, loss=3.2301740646362305
I0307 05:52:14.958310 140037979092736 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.525862693786621, loss=3.184863805770874
I0307 05:52:53.275825 140037970700032 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.4011621475219727, loss=3.1499977111816406
I0307 05:53:31.493036 140037979092736 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.5771894454956055, loss=3.2762930393218994
I0307 05:54:10.038615 140037970700032 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.5863758325576782, loss=3.2103075981140137
I0307 05:54:48.517709 140037979092736 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.5190168619155884, loss=3.2198636531829834
I0307 05:55:26.669523 140037970700032 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.4503817558288574, loss=3.233487844467163
I0307 05:56:04.878086 140037979092736 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.6028460264205933, loss=3.213108539581299
I0307 05:56:36.454788 140193455334592 spec.py:321] Evaluating on the training split.
I0307 05:56:47.462156 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 05:57:08.759551 140193455334592 spec.py:349] Evaluating on the test split.
I0307 05:57:10.591601 140193455334592 submission_runner.py:469] Time since start: 16600.35s, 	Step: 39483, 	{'train/accuracy': 0.7364675998687744, 'train/loss': 1.2735756635665894, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.6118800640106201, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.2707228660583496, 'test/num_examples': 10000, 'score': 15355.177911043167, 'total_duration': 16600.34713125229, 'accumulated_submission_time': 15355.177911043167, 'accumulated_eval_time': 1238.240047454834, 'accumulated_logging_time': 2.849548578262329}
I0307 05:57:10.749015 140037970700032 logging_writer.py:48] [39483] accumulated_eval_time=1238.24, accumulated_logging_time=2.84955, accumulated_submission_time=15355.2, global_step=39483, preemption_count=0, score=15355.2, test/accuracy=0.5313, test/loss=2.27072, test/num_examples=10000, total_duration=16600.3, train/accuracy=0.736468, train/loss=1.27358, validation/accuracy=0.66106, validation/loss=1.61188, validation/num_examples=50000
I0307 05:57:17.716201 140037979092736 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.444775104522705, loss=3.1540331840515137
I0307 05:57:56.109401 140037970700032 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.573501467704773, loss=3.1579952239990234
I0307 05:58:33.940631 140037979092736 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.7083145380020142, loss=3.20223069190979
I0307 05:59:12.480207 140037970700032 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.5395803451538086, loss=3.259448289871216
I0307 05:59:50.869215 140037979092736 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.5733438730239868, loss=3.2412896156311035
I0307 06:00:29.440268 140037970700032 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.504142165184021, loss=3.2441794872283936
I0307 06:01:07.655491 140037979092736 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.5546557903289795, loss=3.249556303024292
I0307 06:01:45.682700 140037970700032 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.764798879623413, loss=3.2529804706573486
I0307 06:02:23.800555 140037979092736 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.397821068763733, loss=3.130155324935913
I0307 06:03:02.423159 140037970700032 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.6196314096450806, loss=3.266047954559326
I0307 06:03:40.642814 140037979092736 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.5864769220352173, loss=3.161543846130371
I0307 06:04:18.946092 140037970700032 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.5377622842788696, loss=3.254622220993042
I0307 06:04:57.460550 140037979092736 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.4272887706756592, loss=3.1789329051971436
I0307 06:05:35.686774 140037970700032 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6142281293869019, loss=3.1885087490081787
I0307 06:05:40.659652 140193455334592 spec.py:321] Evaluating on the training split.
I0307 06:05:52.306725 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 06:06:13.579096 140193455334592 spec.py:349] Evaluating on the test split.
I0307 06:06:15.390403 140193455334592 submission_runner.py:469] Time since start: 17145.15s, 	Step: 40814, 	{'train/accuracy': 0.729890763759613, 'train/loss': 1.2895914316177368, 'validation/accuracy': 0.6579399704933167, 'validation/loss': 1.6098980903625488, 'validation/num_examples': 50000, 'test/accuracy': 0.5275000333786011, 'test/loss': 2.27048921585083, 'test/num_examples': 10000, 'score': 15864.931150913239, 'total_duration': 17145.14592242241, 'accumulated_submission_time': 15864.931150913239, 'accumulated_eval_time': 1272.970650434494, 'accumulated_logging_time': 3.029849052429199}
I0307 06:06:15.461543 140037979092736 logging_writer.py:48] [40814] accumulated_eval_time=1272.97, accumulated_logging_time=3.02985, accumulated_submission_time=15864.9, global_step=40814, preemption_count=0, score=15864.9, test/accuracy=0.5275, test/loss=2.27049, test/num_examples=10000, total_duration=17145.1, train/accuracy=0.729891, train/loss=1.28959, validation/accuracy=0.65794, validation/loss=1.6099, validation/num_examples=50000
I0307 06:06:48.472061 140037970700032 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.5373362302780151, loss=3.1895253658294678
I0307 06:07:26.839492 140037979092736 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.4328118562698364, loss=3.1786980628967285
I0307 06:08:05.297241 140037970700032 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.5428820848464966, loss=3.1771295070648193
I0307 06:08:43.375003 140037979092736 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.6158149242401123, loss=3.2132134437561035
I0307 06:09:21.440166 140037970700032 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.729004144668579, loss=3.2034950256347656
I0307 06:09:59.827622 140037979092736 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.6329973936080933, loss=3.124135732650757
I0307 06:10:38.248468 140037970700032 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.6804267168045044, loss=3.1734137535095215
I0307 06:11:16.296583 140037979092736 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.620274543762207, loss=3.240286111831665
I0307 06:11:54.803455 140037970700032 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.4729641675949097, loss=3.1643457412719727
I0307 06:12:32.966085 140037979092736 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.4598366022109985, loss=3.105003833770752
I0307 06:13:11.430317 140037970700032 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.4476847648620605, loss=3.1520562171936035
I0307 06:13:49.692698 140037979092736 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.6279122829437256, loss=3.185007333755493
I0307 06:14:27.683156 140037970700032 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.4629813432693481, loss=3.2291951179504395
I0307 06:14:45.615026 140193455334592 spec.py:321] Evaluating on the training split.
I0307 06:14:56.698915 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 06:15:22.265964 140193455334592 spec.py:349] Evaluating on the test split.
I0307 06:15:24.020547 140193455334592 submission_runner.py:469] Time since start: 17693.78s, 	Step: 42148, 	{'train/accuracy': 0.7209422588348389, 'train/loss': 1.3132954835891724, 'validation/accuracy': 0.6492599844932556, 'validation/loss': 1.6357648372650146, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.3070361614227295, 'test/num_examples': 10000, 'score': 16374.926724910736, 'total_duration': 17693.776051282883, 'accumulated_submission_time': 16374.926724910736, 'accumulated_eval_time': 1311.3760063648224, 'accumulated_logging_time': 3.126911163330078}
I0307 06:15:24.095729 140037979092736 logging_writer.py:48] [42148] accumulated_eval_time=1311.38, accumulated_logging_time=3.12691, accumulated_submission_time=16374.9, global_step=42148, preemption_count=0, score=16374.9, test/accuracy=0.5197, test/loss=2.30704, test/num_examples=10000, total_duration=17693.8, train/accuracy=0.720942, train/loss=1.3133, validation/accuracy=0.64926, validation/loss=1.63576, validation/num_examples=50000
I0307 06:15:44.474586 140037970700032 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.5126354694366455, loss=3.2221782207489014
I0307 06:16:22.503793 140037979092736 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6929442882537842, loss=3.2277910709381104
I0307 06:17:00.599574 140037970700032 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.7190057039260864, loss=3.212273120880127
I0307 06:17:38.757880 140037979092736 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.61578369140625, loss=3.2491166591644287
I0307 06:18:16.941153 140037970700032 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6645158529281616, loss=3.275264024734497
I0307 06:18:55.471957 140037979092736 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.5270278453826904, loss=3.1745030879974365
I0307 06:19:33.885129 140037970700032 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.6401091814041138, loss=3.179119110107422
I0307 06:20:12.572013 140037979092736 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.7083160877227783, loss=3.2516672611236572
I0307 06:20:50.889313 140037970700032 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.569429874420166, loss=3.202383279800415
I0307 06:21:29.227910 140037979092736 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.457289695739746, loss=3.137230396270752
I0307 06:22:07.490741 140037970700032 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.774253249168396, loss=3.1548590660095215
I0307 06:22:45.850162 140037979092736 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.7157014608383179, loss=3.149280071258545
I0307 06:23:23.825049 140037970700032 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.6107851266860962, loss=3.213365316390991
I0307 06:23:54.340138 140193455334592 spec.py:321] Evaluating on the training split.
I0307 06:24:05.470081 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 06:24:27.879629 140193455334592 spec.py:349] Evaluating on the test split.
I0307 06:24:29.686683 140193455334592 submission_runner.py:469] Time since start: 18239.44s, 	Step: 43481, 	{'train/accuracy': 0.7349330186843872, 'train/loss': 1.2756978273391724, 'validation/accuracy': 0.659280002117157, 'validation/loss': 1.6124171018600464, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.2553226947784424, 'test/num_examples': 10000, 'score': 16885.014603853226, 'total_duration': 18239.442207098007, 'accumulated_submission_time': 16885.014603853226, 'accumulated_eval_time': 1346.722405910492, 'accumulated_logging_time': 3.2275619506835938}
I0307 06:24:29.769453 140037979092736 logging_writer.py:48] [43481] accumulated_eval_time=1346.72, accumulated_logging_time=3.22756, accumulated_submission_time=16885, global_step=43481, preemption_count=0, score=16885, test/accuracy=0.5315, test/loss=2.25532, test/num_examples=10000, total_duration=18239.4, train/accuracy=0.734933, train/loss=1.2757, validation/accuracy=0.65928, validation/loss=1.61242, validation/num_examples=50000
I0307 06:24:37.482212 140037970700032 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.6538851261138916, loss=3.1512651443481445
I0307 06:25:15.552807 140037979092736 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.5734833478927612, loss=3.212313175201416
I0307 06:25:53.759189 140037970700032 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.536767840385437, loss=3.163247585296631
I0307 06:26:31.882165 140037979092736 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.7157617807388306, loss=3.260246753692627
I0307 06:27:10.192811 140037970700032 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.5412319898605347, loss=3.1369967460632324
I0307 06:27:48.408829 140037979092736 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.076368808746338, loss=3.1407337188720703
I0307 06:28:26.650087 140037970700032 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.563948631286621, loss=3.294304609298706
I0307 06:29:05.082446 140037979092736 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.7734293937683105, loss=3.210247755050659
I0307 06:29:43.437044 140037970700032 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.7190732955932617, loss=3.174635410308838
I0307 06:30:21.508595 140037979092736 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.5769805908203125, loss=3.1680715084075928
I0307 06:31:00.043959 140037970700032 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.5298861265182495, loss=3.219541549682617
I0307 06:31:37.981655 140037979092736 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6062450408935547, loss=3.1736414432525635
I0307 06:32:16.284948 140037970700032 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.6644009351730347, loss=3.1337742805480957
I0307 06:32:54.569588 140037979092736 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.7599480152130127, loss=3.1193370819091797
I0307 06:32:59.937817 140193455334592 spec.py:321] Evaluating on the training split.
I0307 06:33:10.652287 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 06:33:33.457042 140193455334592 spec.py:349] Evaluating on the test split.
I0307 06:33:35.254516 140193455334592 submission_runner.py:469] Time since start: 18785.01s, 	Step: 44815, 	{'train/accuracy': 0.7306082248687744, 'train/loss': 1.2841325998306274, 'validation/accuracy': 0.6571599841117859, 'validation/loss': 1.601897954940796, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.2631285190582275, 'test/num_examples': 10000, 'score': 17395.0259206295, 'total_duration': 18785.01003599167, 'accumulated_submission_time': 17395.0259206295, 'accumulated_eval_time': 1382.0389544963837, 'accumulated_logging_time': 3.3354320526123047}
I0307 06:33:35.321037 140037970700032 logging_writer.py:48] [44815] accumulated_eval_time=1382.04, accumulated_logging_time=3.33543, accumulated_submission_time=17395, global_step=44815, preemption_count=0, score=17395, test/accuracy=0.5321, test/loss=2.26313, test/num_examples=10000, total_duration=18785, train/accuracy=0.730608, train/loss=1.28413, validation/accuracy=0.65716, validation/loss=1.6019, validation/num_examples=50000
I0307 06:34:08.071620 140037979092736 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.6672475337982178, loss=3.1576199531555176
I0307 06:34:46.621510 140037970700032 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.772050380706787, loss=3.1981747150421143
I0307 06:35:24.925934 140037979092736 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.608839988708496, loss=3.1742005348205566
I0307 06:36:03.276009 140037970700032 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.6144429445266724, loss=3.174764633178711
I0307 06:36:41.549995 140037979092736 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.7149239778518677, loss=3.254460573196411
I0307 06:37:19.851820 140037970700032 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.5870012044906616, loss=3.1056790351867676
I0307 06:37:58.460024 140037979092736 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.6786669492721558, loss=3.173086166381836
I0307 06:38:36.904736 140037970700032 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.6652051210403442, loss=3.249484062194824
I0307 06:39:15.223978 140037979092736 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.639513373374939, loss=3.07149600982666
I0307 06:39:53.435173 140037970700032 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.6592259407043457, loss=3.108874797821045
I0307 06:40:31.972158 140037979092736 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.6577235460281372, loss=3.1820812225341797
I0307 06:41:09.998070 140037970700032 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.8535778522491455, loss=3.2373645305633545
I0307 06:41:48.576631 140037979092736 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.6890605688095093, loss=3.1770238876342773
I0307 06:42:05.463402 140193455334592 spec.py:321] Evaluating on the training split.
I0307 06:42:16.269348 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 06:42:42.037379 140193455334592 spec.py:349] Evaluating on the test split.
I0307 06:42:43.836546 140193455334592 submission_runner.py:469] Time since start: 19333.59s, 	Step: 46145, 	{'train/accuracy': 0.7375438213348389, 'train/loss': 1.250101923942566, 'validation/accuracy': 0.6602199673652649, 'validation/loss': 1.5915335416793823, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.260349988937378, 'test/num_examples': 10000, 'score': 17904.98149585724, 'total_duration': 19333.592064142227, 'accumulated_submission_time': 17904.98149585724, 'accumulated_eval_time': 1420.4119472503662, 'accumulated_logging_time': 3.456692695617676}
I0307 06:42:43.945780 140037970700032 logging_writer.py:48] [46145] accumulated_eval_time=1420.41, accumulated_logging_time=3.45669, accumulated_submission_time=17905, global_step=46145, preemption_count=0, score=17905, test/accuracy=0.5318, test/loss=2.26035, test/num_examples=10000, total_duration=19333.6, train/accuracy=0.737544, train/loss=1.2501, validation/accuracy=0.66022, validation/loss=1.59153, validation/num_examples=50000
I0307 06:43:05.475142 140037979092736 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.6834511756896973, loss=3.1929140090942383
I0307 06:43:43.578169 140037970700032 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.7196435928344727, loss=3.1692137718200684
I0307 06:44:21.869535 140037979092736 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.618235468864441, loss=3.091148853302002
I0307 06:45:00.030738 140037970700032 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.0965137481689453, loss=3.2126851081848145
I0307 06:45:38.248715 140037979092736 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.6551532745361328, loss=3.1838531494140625
I0307 06:46:16.565776 140037970700032 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.6711777448654175, loss=3.2003087997436523
I0307 06:46:55.013463 140037979092736 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.8649914264678955, loss=3.190767288208008
I0307 06:47:33.467678 140037970700032 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.8050001859664917, loss=3.1918952465057373
I0307 06:48:11.568664 140037979092736 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.8234918117523193, loss=3.2457239627838135
I0307 06:48:50.347754 140037970700032 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.1551413536071777, loss=3.206515312194824
I0307 06:49:28.549996 140037979092736 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.6205823421478271, loss=3.081547737121582
I0307 06:50:06.676990 140037970700032 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.7040961980819702, loss=3.1846494674682617
I0307 06:50:44.890624 140037979092736 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7420612573623657, loss=3.1790759563446045
I0307 06:51:13.954725 140193455334592 spec.py:321] Evaluating on the training split.
I0307 06:51:25.224186 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 06:51:52.022801 140193455334592 spec.py:349] Evaluating on the test split.
I0307 06:51:53.787940 140193455334592 submission_runner.py:469] Time since start: 19883.54s, 	Step: 47477, 	{'train/accuracy': 0.7295519709587097, 'train/loss': 1.3188793659210205, 'validation/accuracy': 0.6567999720573425, 'validation/loss': 1.6338763236999512, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.2689547538757324, 'test/num_examples': 10000, 'score': 18414.79394221306, 'total_duration': 19883.543442964554, 'accumulated_submission_time': 18414.79394221306, 'accumulated_eval_time': 1460.2449977397919, 'accumulated_logging_time': 3.631133556365967}
I0307 06:51:53.885676 140037970700032 logging_writer.py:48] [47477] accumulated_eval_time=1460.24, accumulated_logging_time=3.63113, accumulated_submission_time=18414.8, global_step=47477, preemption_count=0, score=18414.8, test/accuracy=0.539, test/loss=2.26895, test/num_examples=10000, total_duration=19883.5, train/accuracy=0.729552, train/loss=1.31888, validation/accuracy=0.6568, validation/loss=1.63388, validation/num_examples=50000
I0307 06:52:03.002766 140037979092736 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7181122303009033, loss=3.249537944793701
I0307 06:52:41.515547 140037970700032 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.718909740447998, loss=3.1469547748565674
I0307 06:53:19.795505 140037979092736 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.638407588005066, loss=3.1319098472595215
I0307 06:53:58.157664 140037970700032 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.8861827850341797, loss=3.2464699745178223
I0307 06:54:36.753130 140037979092736 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.8385381698608398, loss=3.1112561225891113
I0307 06:55:14.905625 140037970700032 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.800308108329773, loss=3.231754779815674
I0307 06:55:53.347448 140037979092736 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.789148211479187, loss=3.1723687648773193
I0307 06:56:31.514935 140037970700032 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.6300610303878784, loss=3.132808208465576
I0307 06:57:09.813864 140037979092736 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.6702733039855957, loss=3.2071585655212402
I0307 06:57:48.254834 140037970700032 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.7717690467834473, loss=3.1954009532928467
I0307 06:58:26.414967 140037979092736 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.037010431289673, loss=3.1719791889190674
I0307 06:59:04.810352 140037970700032 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8150084018707275, loss=3.201240062713623
I0307 06:59:43.228208 140037979092736 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.7510532140731812, loss=3.186370849609375
I0307 07:00:21.323585 140037970700032 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.7075929641723633, loss=3.10532283782959
I0307 07:00:23.969702 140193455334592 spec.py:321] Evaluating on the training split.
I0307 07:00:34.670735 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 07:01:00.809257 140193455334592 spec.py:349] Evaluating on the test split.
I0307 07:01:02.608342 140193455334592 submission_runner.py:469] Time since start: 20432.36s, 	Step: 48808, 	{'train/accuracy': 0.734793484210968, 'train/loss': 1.2914808988571167, 'validation/accuracy': 0.666159987449646, 'validation/loss': 1.5947694778442383, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.2721590995788574, 'test/num_examples': 10000, 'score': 18924.725657463074, 'total_duration': 20432.363762140274, 'accumulated_submission_time': 18924.725657463074, 'accumulated_eval_time': 1498.883390903473, 'accumulated_logging_time': 3.7491116523742676}
I0307 07:01:02.696172 140037979092736 logging_writer.py:48] [48808] accumulated_eval_time=1498.88, accumulated_logging_time=3.74911, accumulated_submission_time=18924.7, global_step=48808, preemption_count=0, score=18924.7, test/accuracy=0.5317, test/loss=2.27216, test/num_examples=10000, total_duration=20432.4, train/accuracy=0.734793, train/loss=1.29148, validation/accuracy=0.66616, validation/loss=1.59477, validation/num_examples=50000
I0307 07:01:38.345879 140037970700032 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.9199974536895752, loss=3.071866989135742
I0307 07:02:16.286305 140037979092736 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.9392608404159546, loss=3.1589090824127197
I0307 07:02:54.689905 140037970700032 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.6186585426330566, loss=3.1092939376831055
I0307 07:03:33.062175 140037979092736 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.7477045059204102, loss=3.1640868186950684
I0307 07:04:11.289508 140037970700032 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.891990065574646, loss=3.1403822898864746
I0307 07:04:49.422388 140037979092736 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.8858462572097778, loss=3.2707300186157227
I0307 07:05:27.442313 140037970700032 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7607263326644897, loss=3.149486780166626
I0307 07:06:05.649677 140037979092736 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.822691798210144, loss=3.1905293464660645
I0307 07:06:43.927734 140037970700032 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.816688895225525, loss=3.168567419052124
I0307 07:07:22.345655 140037979092736 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.6959229707717896, loss=3.1175639629364014
I0307 07:08:00.773748 140037970700032 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9007030725479126, loss=3.1697864532470703
I0307 07:08:39.188711 140037979092736 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7216578722000122, loss=3.10768985748291
I0307 07:09:17.470398 140037970700032 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8114302158355713, loss=3.150254011154175
I0307 07:09:32.802389 140193455334592 spec.py:321] Evaluating on the training split.
I0307 07:09:43.711182 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 07:10:10.264992 140193455334592 spec.py:349] Evaluating on the test split.
I0307 07:10:12.035605 140193455334592 submission_runner.py:469] Time since start: 20981.79s, 	Step: 50141, 	{'train/accuracy': 0.7444993257522583, 'train/loss': 1.2479438781738281, 'validation/accuracy': 0.6702799797058105, 'validation/loss': 1.5696496963500977, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.229975700378418, 'test/num_examples': 10000, 'score': 19434.664959430695, 'total_duration': 20981.791118860245, 'accumulated_submission_time': 19434.664959430695, 'accumulated_eval_time': 1538.116448879242, 'accumulated_logging_time': 3.871286630630493}
I0307 07:10:12.099172 140037979092736 logging_writer.py:48] [50141] accumulated_eval_time=1538.12, accumulated_logging_time=3.87129, accumulated_submission_time=19434.7, global_step=50141, preemption_count=0, score=19434.7, test/accuracy=0.5409, test/loss=2.22998, test/num_examples=10000, total_duration=20981.8, train/accuracy=0.744499, train/loss=1.24794, validation/accuracy=0.67028, validation/loss=1.56965, validation/num_examples=50000
I0307 07:10:35.018301 140037970700032 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.9989322423934937, loss=3.102717399597168
I0307 07:11:12.990342 140037979092736 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.8678886890411377, loss=3.1195008754730225
I0307 07:11:50.743036 140037970700032 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.6878427267074585, loss=3.0910940170288086
I0307 07:12:29.063158 140037979092736 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.6931171417236328, loss=3.165881633758545
I0307 07:13:07.611095 140037970700032 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.732798457145691, loss=3.217334747314453
I0307 07:13:45.753873 140037979092736 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.7590758800506592, loss=3.1262989044189453
I0307 07:14:23.988016 140037970700032 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.0211565494537354, loss=3.160550832748413
I0307 07:15:02.683929 140037979092736 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.7296055555343628, loss=3.1540913581848145
I0307 07:15:40.952772 140037970700032 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.71033775806427, loss=3.2202861309051514
I0307 07:16:19.059158 140037979092736 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.8368686437606812, loss=3.1676182746887207
I0307 07:16:57.564792 140037970700032 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7993738651275635, loss=3.1682658195495605
I0307 07:17:35.740731 140037979092736 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.7033456563949585, loss=3.1568102836608887
I0307 07:18:14.055222 140037970700032 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7845240831375122, loss=3.173393964767456
I0307 07:18:42.429612 140193455334592 spec.py:321] Evaluating on the training split.
I0307 07:18:53.327542 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 07:19:19.385260 140193455334592 spec.py:349] Evaluating on the test split.
I0307 07:19:21.179431 140193455334592 submission_runner.py:469] Time since start: 21530.93s, 	Step: 51475, 	{'train/accuracy': 0.7254065275192261, 'train/loss': 1.2962501049041748, 'validation/accuracy': 0.6546599864959717, 'validation/loss': 1.6282118558883667, 'validation/num_examples': 50000, 'test/accuracy': 0.5351000428199768, 'test/loss': 2.254657506942749, 'test/num_examples': 10000, 'score': 19944.81085205078, 'total_duration': 21530.934900045395, 'accumulated_submission_time': 19944.81085205078, 'accumulated_eval_time': 1576.8660669326782, 'accumulated_logging_time': 3.982649564743042}
I0307 07:19:21.257259 140037979092736 logging_writer.py:48] [51475] accumulated_eval_time=1576.87, accumulated_logging_time=3.98265, accumulated_submission_time=19944.8, global_step=51475, preemption_count=0, score=19944.8, test/accuracy=0.5351, test/loss=2.25466, test/num_examples=10000, total_duration=21530.9, train/accuracy=0.725407, train/loss=1.29625, validation/accuracy=0.65466, validation/loss=1.62821, validation/num_examples=50000
I0307 07:19:31.192270 140037970700032 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8674615621566772, loss=3.1827917098999023
I0307 07:20:09.324139 140037979092736 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.8166253566741943, loss=3.1461098194122314
I0307 07:20:47.556761 140037970700032 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.7961910963058472, loss=3.1365461349487305
I0307 07:21:26.110761 140037979092736 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8236923217773438, loss=3.1389598846435547
I0307 07:22:04.510378 140037970700032 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.7050782442092896, loss=3.0909926891326904
I0307 07:22:42.687683 140037979092736 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.8425054550170898, loss=3.2060086727142334
I0307 07:23:21.156791 140037970700032 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.69000244140625, loss=3.132891893386841
I0307 07:23:59.672289 140037979092736 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.895787239074707, loss=3.0926644802093506
I0307 07:24:38.295879 140037970700032 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.715831995010376, loss=3.1481196880340576
I0307 07:25:16.586063 140037979092736 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.8221544027328491, loss=3.2073922157287598
I0307 07:25:55.118536 140037970700032 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.7327300310134888, loss=3.1106438636779785
I0307 07:26:33.675621 140037979092736 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.7180949449539185, loss=3.078962802886963
I0307 07:27:11.745998 140037970700032 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.733986258506775, loss=3.2199809551239014
I0307 07:27:50.035263 140037979092736 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.7731984853744507, loss=3.094439744949341
I0307 07:27:51.202785 140193455334592 spec.py:321] Evaluating on the training split.
I0307 07:28:02.052098 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 07:28:28.111748 140193455334592 spec.py:349] Evaluating on the test split.
I0307 07:28:29.875326 140193455334592 submission_runner.py:469] Time since start: 22079.63s, 	Step: 52804, 	{'train/accuracy': 0.7359693646430969, 'train/loss': 1.2672291994094849, 'validation/accuracy': 0.6695799827575684, 'validation/loss': 1.571102499961853, 'validation/num_examples': 50000, 'test/accuracy': 0.5426000356674194, 'test/loss': 2.2252392768859863, 'test/num_examples': 10000, 'score': 20454.600573539734, 'total_duration': 22079.630780935287, 'accumulated_submission_time': 20454.600573539734, 'accumulated_eval_time': 1615.5383932590485, 'accumulated_logging_time': 4.079102516174316}
I0307 07:28:29.971857 140037970700032 logging_writer.py:48] [52804] accumulated_eval_time=1615.54, accumulated_logging_time=4.0791, accumulated_submission_time=20454.6, global_step=52804, preemption_count=0, score=20454.6, test/accuracy=0.5426, test/loss=2.22524, test/num_examples=10000, total_duration=22079.6, train/accuracy=0.735969, train/loss=1.26723, validation/accuracy=0.66958, validation/loss=1.5711, validation/num_examples=50000
I0307 07:29:07.185281 140037979092736 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.8419814109802246, loss=3.136488437652588
I0307 07:29:45.480906 140037970700032 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8093295097351074, loss=3.1497795581817627
I0307 07:30:23.938987 140037979092736 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.1165454387664795, loss=3.1821978092193604
I0307 07:31:02.565105 140037970700032 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.810270071029663, loss=3.1432673931121826
I0307 07:31:41.109227 140037979092736 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.7429853677749634, loss=3.1531620025634766
I0307 07:32:19.586115 140037970700032 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.760863184928894, loss=3.1988425254821777
I0307 07:32:57.957751 140037979092736 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.7939976453781128, loss=3.187615156173706
I0307 07:33:35.995146 140037970700032 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.8324631452560425, loss=3.1300480365753174
I0307 07:34:14.528166 140037979092736 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.767093539237976, loss=3.1217737197875977
I0307 07:34:52.860234 140037970700032 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.824810266494751, loss=3.1300814151763916
I0307 07:35:31.273847 140037979092736 logging_writer.py:48] [53900] global_step=53900, grad_norm=2.0309970378875732, loss=3.218397378921509
I0307 07:36:09.547176 140037970700032 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.989149570465088, loss=3.1396124362945557
I0307 07:36:47.848424 140037979092736 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.795356035232544, loss=3.1200368404388428
I0307 07:37:00.046979 140193455334592 spec.py:321] Evaluating on the training split.
I0307 07:37:11.072598 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 07:37:34.694160 140193455334592 spec.py:349] Evaluating on the test split.
I0307 07:37:36.495784 140193455334592 submission_runner.py:469] Time since start: 22626.25s, 	Step: 54133, 	{'train/accuracy': 0.7433235049247742, 'train/loss': 1.1952619552612305, 'validation/accuracy': 0.6702600121498108, 'validation/loss': 1.5156363248825073, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.177372455596924, 'test/num_examples': 10000, 'score': 20964.505282878876, 'total_duration': 22626.25130057335, 'accumulated_submission_time': 20964.505282878876, 'accumulated_eval_time': 1651.987048149109, 'accumulated_logging_time': 4.212485313415527}
I0307 07:37:36.579370 140037970700032 logging_writer.py:48] [54133] accumulated_eval_time=1651.99, accumulated_logging_time=4.21249, accumulated_submission_time=20964.5, global_step=54133, preemption_count=0, score=20964.5, test/accuracy=0.5459, test/loss=2.17737, test/num_examples=10000, total_duration=22626.3, train/accuracy=0.743324, train/loss=1.19526, validation/accuracy=0.67026, validation/loss=1.51564, validation/num_examples=50000
I0307 07:38:02.793541 140037979092736 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.008249044418335, loss=3.157977342605591
I0307 07:38:41.406733 140037970700032 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.7385356426239014, loss=3.1485795974731445
I0307 07:39:20.112582 140037979092736 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.7095447778701782, loss=3.1182408332824707
I0307 07:39:58.194566 140037970700032 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.9228651523590088, loss=3.1497368812561035
I0307 07:40:36.472974 140037979092736 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.7529056072235107, loss=3.096581220626831
I0307 07:41:15.038628 140037970700032 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.886701226234436, loss=3.143545627593994
I0307 07:41:53.124260 140037979092736 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.907518744468689, loss=3.1033172607421875
I0307 07:42:31.501882 140037970700032 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.8868294954299927, loss=3.1541900634765625
I0307 07:43:09.784733 140037979092736 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.7973049879074097, loss=3.0951035022735596
I0307 07:43:48.209986 140037970700032 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.7728700637817383, loss=3.153057098388672
I0307 07:44:26.245986 140037979092736 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.7786736488342285, loss=3.093506336212158
I0307 07:45:04.627817 140037970700032 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.0179851055145264, loss=3.155505895614624
I0307 07:45:43.072489 140037979092736 logging_writer.py:48] [55400] global_step=55400, grad_norm=2.1134588718414307, loss=3.0805845260620117
I0307 07:46:06.815466 140193455334592 spec.py:321] Evaluating on the training split.
I0307 07:46:17.204618 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 07:46:40.685129 140193455334592 spec.py:349] Evaluating on the test split.
I0307 07:46:42.483119 140193455334592 submission_runner.py:469] Time since start: 23172.24s, 	Step: 55463, 	{'train/accuracy': 0.7479273080825806, 'train/loss': 1.2544888257980347, 'validation/accuracy': 0.6725999712944031, 'validation/loss': 1.5842063426971436, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.22145676612854, 'test/num_examples': 10000, 'score': 21474.588220357895, 'total_duration': 23172.238622188568, 'accumulated_submission_time': 21474.588220357895, 'accumulated_eval_time': 1687.6545338630676, 'accumulated_logging_time': 4.3140575885772705}
I0307 07:46:42.562515 140037970700032 logging_writer.py:48] [55463] accumulated_eval_time=1687.65, accumulated_logging_time=4.31406, accumulated_submission_time=21474.6, global_step=55463, preemption_count=0, score=21474.6, test/accuracy=0.5473, test/loss=2.22146, test/num_examples=10000, total_duration=23172.2, train/accuracy=0.747927, train/loss=1.25449, validation/accuracy=0.6726, validation/loss=1.58421, validation/num_examples=50000
I0307 07:46:57.487369 140037979092736 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.8951077461242676, loss=3.1532864570617676
I0307 07:47:35.497131 140037970700032 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.1652283668518066, loss=3.0853748321533203
I0307 07:48:13.767832 140037979092736 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.9106334447860718, loss=3.110931634902954
I0307 07:48:52.163723 140037970700032 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.1310043334960938, loss=3.147439956665039
I0307 07:49:30.516621 140037979092736 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.9091612100601196, loss=3.0570669174194336
I0307 07:50:09.107744 140037970700032 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.8331325054168701, loss=3.0978844165802
I0307 07:50:47.450427 140037979092736 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.6772340536117554, loss=3.1346499919891357
I0307 07:51:25.928034 140037970700032 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.7858481407165527, loss=3.0695579051971436
I0307 07:52:04.512570 140037979092736 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8604485988616943, loss=3.1698412895202637
I0307 07:52:42.889525 140037970700032 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.8994426727294922, loss=3.1386425495147705
I0307 07:53:21.276505 140037979092736 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.755903720855713, loss=3.057967185974121
I0307 07:53:59.636783 140037970700032 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9836947917938232, loss=3.0908777713775635
I0307 07:54:38.219301 140037979092736 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.8387095928192139, loss=3.1651763916015625
I0307 07:55:12.731353 140193455334592 spec.py:321] Evaluating on the training split.
I0307 07:55:23.505563 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 07:55:46.011560 140193455334592 spec.py:349] Evaluating on the test split.
I0307 07:55:47.814955 140193455334592 submission_runner.py:469] Time since start: 23717.57s, 	Step: 56791, 	{'train/accuracy': 0.7442004084587097, 'train/loss': 1.228479027748108, 'validation/accuracy': 0.670520007610321, 'validation/loss': 1.559903860092163, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 2.1784212589263916, 'test/num_examples': 10000, 'score': 21984.502583265305, 'total_duration': 23717.57048678398, 'accumulated_submission_time': 21984.502583265305, 'accumulated_eval_time': 1722.7379961013794, 'accumulated_logging_time': 4.507081508636475}
I0307 07:55:47.931319 140037970700032 logging_writer.py:48] [56791] accumulated_eval_time=1722.74, accumulated_logging_time=4.50708, accumulated_submission_time=21984.5, global_step=56791, preemption_count=0, score=21984.5, test/accuracy=0.5524, test/loss=2.17842, test/num_examples=10000, total_duration=23717.6, train/accuracy=0.7442, train/loss=1.22848, validation/accuracy=0.67052, validation/loss=1.5599, validation/num_examples=50000
I0307 07:55:51.899238 140037979092736 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.8597302436828613, loss=3.1623153686523438
I0307 07:56:30.134087 140037970700032 logging_writer.py:48] [56900] global_step=56900, grad_norm=2.0750296115875244, loss=3.147787570953369
I0307 07:57:08.721134 140037979092736 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.8530206680297852, loss=3.169748067855835
I0307 07:57:47.016227 140037970700032 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.9502524137496948, loss=3.07589054107666
I0307 07:58:25.341104 140037979092736 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.9387843608856201, loss=3.1440091133117676
I0307 07:59:04.041343 140037970700032 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.9632368087768555, loss=3.13700795173645
I0307 07:59:42.785495 140037979092736 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.8998196125030518, loss=3.100339412689209
I0307 08:00:21.133823 140037970700032 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8264867067337036, loss=3.069861650466919
I0307 08:00:58.715918 140037979092736 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.9363046884536743, loss=3.1975908279418945
I0307 08:01:36.720829 140037970700032 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9409222602844238, loss=3.118920087814331
I0307 08:02:15.014451 140037979092736 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8408381938934326, loss=3.210235357284546
I0307 08:02:53.599529 140037970700032 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8286124467849731, loss=3.1640303134918213
I0307 08:03:32.044866 140037979092736 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.9567118883132935, loss=3.078089714050293
I0307 08:04:10.288431 140037970700032 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9371932744979858, loss=3.1809842586517334
I0307 08:04:17.965874 140193455334592 spec.py:321] Evaluating on the training split.
I0307 08:04:28.660200 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 08:04:55.275017 140193455334592 spec.py:349] Evaluating on the test split.
I0307 08:04:57.037274 140193455334592 submission_runner.py:469] Time since start: 24266.79s, 	Step: 58121, 	{'train/accuracy': 0.744160532951355, 'train/loss': 1.2386058568954468, 'validation/accuracy': 0.6704199910163879, 'validation/loss': 1.561078667640686, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.18866229057312, 'test/num_examples': 10000, 'score': 22494.334429740906, 'total_duration': 24266.79279279709, 'accumulated_submission_time': 22494.334429740906, 'accumulated_eval_time': 1761.809242963791, 'accumulated_logging_time': 4.6822710037231445}
I0307 08:04:57.136045 140037979092736 logging_writer.py:48] [58121] accumulated_eval_time=1761.81, accumulated_logging_time=4.68227, accumulated_submission_time=22494.3, global_step=58121, preemption_count=0, score=22494.3, test/accuracy=0.5461, test/loss=2.18866, test/num_examples=10000, total_duration=24266.8, train/accuracy=0.744161, train/loss=1.23861, validation/accuracy=0.67042, validation/loss=1.56108, validation/num_examples=50000
I0307 08:05:27.681083 140037970700032 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.8507064580917358, loss=3.0688259601593018
I0307 08:06:05.835903 140037979092736 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.047621250152588, loss=3.1205103397369385
I0307 08:06:44.014646 140037970700032 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9717081785202026, loss=3.148810386657715
I0307 08:07:22.502063 140037979092736 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.9194998741149902, loss=3.0780880451202393
I0307 08:08:00.657741 140037970700032 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.8643423318862915, loss=3.0687577724456787
I0307 08:08:39.017638 140037979092736 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.7655471563339233, loss=3.1179566383361816
I0307 08:09:16.649025 140037970700032 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.024616241455078, loss=3.1622889041900635
I0307 08:09:54.914324 140037979092736 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8317304849624634, loss=3.099301815032959
I0307 08:10:33.429271 140037970700032 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.7259881496429443, loss=3.1202383041381836
I0307 08:11:11.624296 140037979092736 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.0324442386627197, loss=3.1730918884277344
I0307 08:11:49.840079 140037970700032 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.0268452167510986, loss=3.1549031734466553
I0307 08:12:28.706811 140037979092736 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8085839748382568, loss=3.153332233428955
I0307 08:13:06.836548 140037970700032 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.8491851091384888, loss=3.1381924152374268
I0307 08:13:27.276496 140193455334592 spec.py:321] Evaluating on the training split.
I0307 08:13:38.067000 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 08:14:03.919226 140193455334592 spec.py:349] Evaluating on the test split.
I0307 08:14:05.683750 140193455334592 submission_runner.py:469] Time since start: 24815.44s, 	Step: 59454, 	{'train/accuracy': 0.7482461333274841, 'train/loss': 1.2045261859893799, 'validation/accuracy': 0.6738599538803101, 'validation/loss': 1.5239259004592896, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.182633638381958, 'test/num_examples': 10000, 'score': 23004.292164087296, 'total_duration': 24815.439249277115, 'accumulated_submission_time': 23004.292164087296, 'accumulated_eval_time': 1800.2163240909576, 'accumulated_logging_time': 4.818809509277344}
I0307 08:14:05.759118 140037979092736 logging_writer.py:48] [59454] accumulated_eval_time=1800.22, accumulated_logging_time=4.81881, accumulated_submission_time=23004.3, global_step=59454, preemption_count=0, score=23004.3, test/accuracy=0.5482, test/loss=2.18263, test/num_examples=10000, total_duration=24815.4, train/accuracy=0.748246, train/loss=1.20453, validation/accuracy=0.67386, validation/loss=1.52393, validation/num_examples=50000
I0307 08:14:23.762790 140037970700032 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.8667341470718384, loss=3.0561466217041016
I0307 08:15:01.876031 140037979092736 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.889614224433899, loss=3.1531941890716553
I0307 08:15:40.377164 140037970700032 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.8500467538833618, loss=3.0326037406921387
I0307 08:16:19.013316 140037979092736 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.8417508602142334, loss=3.117720127105713
I0307 08:16:57.399513 140037970700032 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.8594765663146973, loss=3.1124045848846436
I0307 08:17:35.625575 140037979092736 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.9533344507217407, loss=3.1086504459381104
I0307 08:18:12.556481 140037970700032 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.022423505783081, loss=3.183100938796997
I0307 08:18:50.743453 140037979092736 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.9446375370025635, loss=3.081498622894287
I0307 08:19:29.133727 140037970700032 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.0176048278808594, loss=3.1189327239990234
I0307 08:20:06.865696 140037979092736 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8209110498428345, loss=3.11596417427063
I0307 08:20:45.019052 140037970700032 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8869417905807495, loss=3.1317241191864014
I0307 08:21:23.335214 140037979092736 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.898446798324585, loss=3.138988971710205
I0307 08:22:01.624140 140037970700032 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.9179002046585083, loss=3.051571846008301
I0307 08:22:36.000790 140193455334592 spec.py:321] Evaluating on the training split.
I0307 08:22:46.537088 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 08:23:12.930210 140193455334592 spec.py:349] Evaluating on the test split.
I0307 08:23:14.692566 140193455334592 submission_runner.py:469] Time since start: 25364.45s, 	Step: 60791, 	{'train/accuracy': 0.7326610088348389, 'train/loss': 1.3098770380020142, 'validation/accuracy': 0.6614999771118164, 'validation/loss': 1.6309702396392822, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.2892611026763916, 'test/num_examples': 10000, 'score': 23514.367705345154, 'total_duration': 25364.448014497757, 'accumulated_submission_time': 23514.367705345154, 'accumulated_eval_time': 1838.9078783988953, 'accumulated_logging_time': 4.915984630584717}
I0307 08:23:14.745590 140037979092736 logging_writer.py:48] [60791] accumulated_eval_time=1838.91, accumulated_logging_time=4.91598, accumulated_submission_time=23514.4, global_step=60791, preemption_count=0, score=23514.4, test/accuracy=0.5307, test/loss=2.28926, test/num_examples=10000, total_duration=25364.4, train/accuracy=0.732661, train/loss=1.30988, validation/accuracy=0.6615, validation/loss=1.63097, validation/num_examples=50000
I0307 08:23:18.569153 140037970700032 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.022402286529541, loss=3.191819667816162
I0307 08:23:56.613066 140037979092736 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.8651783466339111, loss=3.1482956409454346
I0307 08:24:34.861514 140037970700032 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.9920233488082886, loss=3.211021661758423
I0307 08:25:13.054012 140037979092736 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.0994269847869873, loss=3.0636699199676514
I0307 08:25:51.596081 140037970700032 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.060781717300415, loss=3.102609634399414
2025-03-07 08:26:24.601699: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 08:26:29.845991 140037979092736 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9353071451187134, loss=3.066743850708008
I0307 08:27:07.119290 140037970700032 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.9219146966934204, loss=3.135132074356079
I0307 08:27:45.690132 140037979092736 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.225450277328491, loss=3.04689621925354
I0307 08:28:23.780741 140037970700032 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.067420244216919, loss=3.1219615936279297
I0307 08:29:02.089474 140037979092736 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9212042093276978, loss=3.1275644302368164
I0307 08:29:40.147107 140037970700032 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.0947349071502686, loss=3.144840955734253
I0307 08:30:18.419285 140037979092736 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.861944556236267, loss=3.091480016708374
I0307 08:30:56.400133 140037970700032 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.8583688735961914, loss=3.122368335723877
I0307 08:31:34.683725 140037979092736 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.7902121543884277, loss=3.054746627807617
I0307 08:31:44.978120 140193455334592 spec.py:321] Evaluating on the training split.
I0307 08:31:55.786769 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 08:32:21.413486 140193455334592 spec.py:349] Evaluating on the test split.
I0307 08:32:23.207831 140193455334592 submission_runner.py:469] Time since start: 25912.96s, 	Step: 62128, 	{'train/accuracy': 0.7435227632522583, 'train/loss': 1.2181622982025146, 'validation/accuracy': 0.6756199598312378, 'validation/loss': 1.5341556072235107, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.218334674835205, 'test/num_examples': 10000, 'score': 24024.429008722305, 'total_duration': 25912.96332359314, 'accumulated_submission_time': 24024.429008722305, 'accumulated_eval_time': 1877.1374099254608, 'accumulated_logging_time': 4.993806600570679}
I0307 08:32:23.244745 140037970700032 logging_writer.py:48] [62128] accumulated_eval_time=1877.14, accumulated_logging_time=4.99381, accumulated_submission_time=24024.4, global_step=62128, preemption_count=0, score=24024.4, test/accuracy=0.5427, test/loss=2.21833, test/num_examples=10000, total_duration=25913, train/accuracy=0.743523, train/loss=1.21816, validation/accuracy=0.67562, validation/loss=1.53416, validation/num_examples=50000
I0307 08:32:50.960119 140037979092736 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.875090479850769, loss=3.1311538219451904
I0307 08:33:29.132896 140037970700032 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.8314017057418823, loss=3.065347909927368
I0307 08:34:07.276610 140037979092736 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.0710766315460205, loss=3.068871259689331
I0307 08:34:45.938174 140037970700032 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.8659628629684448, loss=3.1191253662109375
2025-03-07 08:34:59.891089: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 08:35:23.476480 140037979092736 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.9145346879959106, loss=3.2085487842559814
I0307 08:36:01.821808 140037970700032 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.0197079181671143, loss=3.065608501434326
I0307 08:36:40.243824 140037979092736 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.975832223892212, loss=3.0775628089904785
I0307 08:37:18.744252 140037970700032 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.026376724243164, loss=3.1647777557373047
I0307 08:37:57.218120 140037979092736 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.2078118324279785, loss=3.093740224838257
I0307 08:38:35.495536 140037970700032 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.8646352291107178, loss=3.0649940967559814
I0307 08:39:13.570815 140037979092736 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.987709641456604, loss=3.1362369060516357
I0307 08:39:51.859102 140037970700032 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.882516622543335, loss=3.088663101196289
I0307 08:40:30.388692 140037979092736 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.765860915184021, loss=2.992253541946411
I0307 08:40:53.373540 140193455334592 spec.py:321] Evaluating on the training split.
I0307 08:41:04.085999 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 08:41:28.761695 140193455334592 spec.py:349] Evaluating on the test split.
I0307 08:41:30.566082 140193455334592 submission_runner.py:469] Time since start: 26460.32s, 	Step: 63461, 	{'train/accuracy': 0.7436822056770325, 'train/loss': 1.2419458627700806, 'validation/accuracy': 0.6692000031471252, 'validation/loss': 1.5672487020492554, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.224569320678711, 'test/num_examples': 10000, 'score': 24534.38946390152, 'total_duration': 26460.321612596512, 'accumulated_submission_time': 24534.38946390152, 'accumulated_eval_time': 1914.329824924469, 'accumulated_logging_time': 5.05517840385437}
I0307 08:41:30.641542 140037970700032 logging_writer.py:48] [63461] accumulated_eval_time=1914.33, accumulated_logging_time=5.05518, accumulated_submission_time=24534.4, global_step=63461, preemption_count=0, score=24534.4, test/accuracy=0.5392, test/loss=2.22457, test/num_examples=10000, total_duration=26460.3, train/accuracy=0.743682, train/loss=1.24195, validation/accuracy=0.6692, validation/loss=1.56725, validation/num_examples=50000
I0307 08:41:45.690484 140037979092736 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.009449005126953, loss=3.0865299701690674
I0307 08:42:23.503560 140037970700032 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8313642740249634, loss=3.1315062046051025
I0307 08:43:01.836168 140037979092736 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.181001901626587, loss=3.0562849044799805
2025-03-07 08:43:33.466649: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 08:43:40.565406 140037970700032 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.8183661699295044, loss=3.079244375228882
I0307 08:44:18.732280 140037979092736 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.1498234272003174, loss=3.0803158283233643
I0307 08:44:56.985170 140037970700032 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.114842653274536, loss=3.1235830783843994
I0307 08:45:35.124404 140037979092736 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.195875883102417, loss=3.1281936168670654
I0307 08:46:13.272320 140037970700032 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.9241466522216797, loss=3.1018147468566895
I0307 08:46:51.825751 140037979092736 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.0478689670562744, loss=3.1095314025878906
I0307 08:47:30.343611 140037970700032 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8703596591949463, loss=3.055729627609253
I0307 08:48:08.717796 140037979092736 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.0451500415802, loss=3.05790114402771
I0307 08:48:47.109570 140037970700032 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.9281408786773682, loss=3.0824389457702637
I0307 08:49:25.642855 140037979092736 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.030775785446167, loss=3.086045742034912
I0307 08:50:00.834618 140193455334592 spec.py:321] Evaluating on the training split.
I0307 08:50:11.277055 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 08:50:36.323698 140193455334592 spec.py:349] Evaluating on the test split.
I0307 08:50:38.123164 140193455334592 submission_runner.py:469] Time since start: 27007.88s, 	Step: 64792, 	{'train/accuracy': 0.7454360723495483, 'train/loss': 1.222221851348877, 'validation/accuracy': 0.6723999977111816, 'validation/loss': 1.5486427545547485, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.189523458480835, 'test/num_examples': 10000, 'score': 25044.406069993973, 'total_duration': 27007.87869477272, 'accumulated_submission_time': 25044.406069993973, 'accumulated_eval_time': 1951.6182436943054, 'accumulated_logging_time': 5.16471791267395}
I0307 08:50:38.226718 140037970700032 logging_writer.py:48] [64792] accumulated_eval_time=1951.62, accumulated_logging_time=5.16472, accumulated_submission_time=25044.4, global_step=64792, preemption_count=0, score=25044.4, test/accuracy=0.5499, test/loss=2.18952, test/num_examples=10000, total_duration=27007.9, train/accuracy=0.745436, train/loss=1.22222, validation/accuracy=0.6724, validation/loss=1.54864, validation/num_examples=50000
I0307 08:50:41.620431 140037979092736 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0481510162353516, loss=3.0962862968444824
I0307 08:51:19.818583 140037970700032 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.8956104516983032, loss=3.0614113807678223
I0307 08:51:58.005815 140037979092736 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.093855619430542, loss=3.043637275695801
2025-03-07 08:52:10.321663: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 08:52:36.228772 140037970700032 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.8569642305374146, loss=3.039750337600708
I0307 08:53:14.192683 140037979092736 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8436051607131958, loss=3.063905954360962
I0307 08:53:52.325179 140037970700032 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.8804967403411865, loss=3.1056809425354004
I0307 08:54:30.191533 140037979092736 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.996788501739502, loss=3.2112579345703125
I0307 08:55:08.656900 140037970700032 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.184516668319702, loss=3.1164517402648926
I0307 08:55:46.901147 140037979092736 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.0340898036956787, loss=3.1154069900512695
I0307 08:56:25.085425 140037970700032 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.968697428703308, loss=3.035794734954834
I0307 08:57:03.506345 140037979092736 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.0261635780334473, loss=3.0679776668548584
I0307 08:57:41.877590 140037970700032 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.9128321409225464, loss=3.144603729248047
I0307 08:58:20.272767 140037979092736 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.9949495792388916, loss=3.1583003997802734
I0307 08:58:58.767252 140037970700032 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9197572469711304, loss=3.034975290298462
I0307 08:59:08.131050 140193455334592 spec.py:321] Evaluating on the training split.
I0307 08:59:18.275859 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 08:59:44.122347 140193455334592 spec.py:349] Evaluating on the test split.
I0307 08:59:45.991134 140193455334592 submission_runner.py:469] Time since start: 27555.75s, 	Step: 66125, 	{'train/accuracy': 0.7592275142669678, 'train/loss': 1.17339026927948, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.501193881034851, 'validation/num_examples': 50000, 'test/accuracy': 0.5594000220298767, 'test/loss': 2.141183376312256, 'test/num_examples': 10000, 'score': 25554.12162542343, 'total_duration': 27555.7465801239, 'accumulated_submission_time': 25554.12162542343, 'accumulated_eval_time': 1989.478102684021, 'accumulated_logging_time': 5.314172983169556}
I0307 08:59:46.077882 140037979092736 logging_writer.py:48] [66125] accumulated_eval_time=1989.48, accumulated_logging_time=5.31417, accumulated_submission_time=25554.1, global_step=66125, preemption_count=0, score=25554.1, test/accuracy=0.5594, test/loss=2.14118, test/num_examples=10000, total_duration=27555.7, train/accuracy=0.759228, train/loss=1.17339, validation/accuracy=0.68738, validation/loss=1.50119, validation/num_examples=50000
I0307 09:00:15.166891 140037970700032 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.124715566635132, loss=3.114142417907715
2025-03-07 09:00:50.451077: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:00:53.625330 140037979092736 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.9975507259368896, loss=3.0965142250061035
I0307 09:01:31.007529 140037970700032 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.1956868171691895, loss=3.1113827228546143
I0307 09:02:08.296695 140037979092736 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.1157400608062744, loss=3.1562273502349854
I0307 09:02:45.529523 140037970700032 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.002861976623535, loss=3.083059072494507
I0307 09:03:22.754699 140037979092736 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.1363542079925537, loss=3.1104259490966797
I0307 09:04:00.324308 140037970700032 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.009216785430908, loss=3.097785711288452
I0307 09:04:37.704876 140037979092736 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.9279048442840576, loss=3.135186195373535
I0307 09:05:15.271653 140037970700032 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.0347213745117188, loss=3.0838775634765625
I0307 09:05:52.833732 140037979092736 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.8283023834228516, loss=3.082763671875
I0307 09:06:29.809746 140037970700032 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.1880218982696533, loss=3.1054630279541016
I0307 09:07:07.959654 140037979092736 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9557493925094604, loss=3.1185061931610107
I0307 09:07:46.617426 140037970700032 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.9763885736465454, loss=3.1314916610717773
I0307 09:08:16.246198 140193455334592 spec.py:321] Evaluating on the training split.
I0307 09:08:27.605721 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 09:08:49.248107 140193455334592 spec.py:349] Evaluating on the test split.
I0307 09:08:51.051394 140193455334592 submission_runner.py:469] Time since start: 28100.81s, 	Step: 67475, 	{'train/accuracy': 0.7492426633834839, 'train/loss': 1.1810312271118164, 'validation/accuracy': 0.6776399612426758, 'validation/loss': 1.5078225135803223, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.161421537399292, 'test/num_examples': 10000, 'score': 26064.113745689392, 'total_duration': 28100.806893110275, 'accumulated_submission_time': 26064.113745689392, 'accumulated_eval_time': 2024.2831251621246, 'accumulated_logging_time': 5.428308725357056}
I0307 09:08:51.158487 140037979092736 logging_writer.py:48] [67475] accumulated_eval_time=2024.28, accumulated_logging_time=5.42831, accumulated_submission_time=26064.1, global_step=67475, preemption_count=0, score=26064.1, test/accuracy=0.5497, test/loss=2.16142, test/num_examples=10000, total_duration=28100.8, train/accuracy=0.749243, train/loss=1.18103, validation/accuracy=0.67764, validation/loss=1.50782, validation/num_examples=50000
I0307 09:09:01.253295 140037970700032 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.0823476314544678, loss=2.9863510131835938
2025-03-07 09:09:16.527294: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:09:38.546673 140037979092736 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.0046112537384033, loss=3.0551397800445557
I0307 09:10:16.069044 140037970700032 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0392889976501465, loss=3.052243709564209
I0307 09:10:54.165173 140037979092736 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.046029567718506, loss=3.093391180038452
I0307 09:11:32.433882 140037970700032 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.8561079502105713, loss=3.0499260425567627
I0307 09:12:10.828577 140037979092736 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.0454061031341553, loss=3.0750932693481445
I0307 09:12:49.412382 140037970700032 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.992824673652649, loss=3.059246063232422
I0307 09:13:27.975230 140037979092736 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.048269510269165, loss=3.155336618423462
I0307 09:14:06.324095 140037970700032 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.907041311264038, loss=3.060835838317871
I0307 09:14:45.256317 140037979092736 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.337228775024414, loss=3.10137939453125
I0307 09:15:23.553118 140037970700032 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.006730318069458, loss=3.0944483280181885
I0307 09:16:02.024965 140037979092736 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.1626453399658203, loss=3.1290183067321777
I0307 09:16:40.105792 140037970700032 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.94802725315094, loss=3.0627098083496094
2025-03-07 09:17:16.915707: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:17:19.188874 140037979092736 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.095879077911377, loss=3.193704843521118
I0307 09:17:21.406893 140193455334592 spec.py:321] Evaluating on the training split.
I0307 09:17:32.354405 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 09:17:52.442006 140193455334592 spec.py:349] Evaluating on the test split.
I0307 09:17:54.241883 140193455334592 submission_runner.py:469] Time since start: 28644.00s, 	Step: 68807, 	{'train/accuracy': 0.7495814561843872, 'train/loss': 1.1970422267913818, 'validation/accuracy': 0.6803999543190002, 'validation/loss': 1.5167268514633179, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.1904869079589844, 'test/num_examples': 10000, 'score': 26574.154461622238, 'total_duration': 28643.99741792679, 'accumulated_submission_time': 26574.154461622238, 'accumulated_eval_time': 2057.117976665497, 'accumulated_logging_time': 5.598376989364624}
I0307 09:17:54.292157 140037970700032 logging_writer.py:48] [68807] accumulated_eval_time=2057.12, accumulated_logging_time=5.59838, accumulated_submission_time=26574.2, global_step=68807, preemption_count=0, score=26574.2, test/accuracy=0.5497, test/loss=2.19049, test/num_examples=10000, total_duration=28644, train/accuracy=0.749581, train/loss=1.19704, validation/accuracy=0.6804, validation/loss=1.51673, validation/num_examples=50000
I0307 09:18:30.348588 140037979092736 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.151693105697632, loss=3.1231672763824463
I0307 09:19:08.536565 140037970700032 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.8886431455612183, loss=3.0325140953063965
I0307 09:19:46.789994 140037979092736 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.064314603805542, loss=3.0202078819274902
I0307 09:20:25.087448 140037970700032 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.0766537189483643, loss=3.1138246059417725
I0307 09:21:03.759008 140037979092736 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.160714864730835, loss=3.0788111686706543
I0307 09:21:42.033159 140037970700032 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.0868539810180664, loss=3.109712839126587
I0307 09:22:20.415388 140037979092736 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.9878472089767456, loss=3.064976692199707
I0307 09:22:58.476530 140037970700032 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.1320407390594482, loss=3.124572515487671
I0307 09:23:37.095446 140037979092736 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.07218861579895, loss=3.064243793487549
I0307 09:24:15.341797 140037970700032 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.001753091812134, loss=3.148026466369629
I0307 09:24:53.899686 140037979092736 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.8888118267059326, loss=3.0546295642852783
I0307 09:25:31.751603 140037970700032 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.1370136737823486, loss=3.0202786922454834
2025-03-07 09:25:49.952277: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:26:10.663476 140037979092736 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.015486478805542, loss=3.036931037902832
I0307 09:26:24.409566 140193455334592 spec.py:321] Evaluating on the training split.
I0307 09:26:36.517086 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 09:27:01.643255 140193455334592 spec.py:349] Evaluating on the test split.
I0307 09:27:03.411217 140193455334592 submission_runner.py:469] Time since start: 29193.17s, 	Step: 70139, 	{'train/accuracy': 0.7550621628761292, 'train/loss': 1.2107903957366943, 'validation/accuracy': 0.6820600032806396, 'validation/loss': 1.5357111692428589, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.1894917488098145, 'test/num_examples': 10000, 'score': 27084.08220767975, 'total_duration': 29193.16672682762, 'accumulated_submission_time': 27084.08220767975, 'accumulated_eval_time': 2096.119467496872, 'accumulated_logging_time': 5.691593408584595}
I0307 09:27:03.520971 140037970700032 logging_writer.py:48] [70139] accumulated_eval_time=2096.12, accumulated_logging_time=5.69159, accumulated_submission_time=27084.1, global_step=70139, preemption_count=0, score=27084.1, test/accuracy=0.551, test/loss=2.18949, test/num_examples=10000, total_duration=29193.2, train/accuracy=0.755062, train/loss=1.21079, validation/accuracy=0.68206, validation/loss=1.53571, validation/num_examples=50000
I0307 09:27:27.389844 140037979092736 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.9857028722763062, loss=3.0384302139282227
I0307 09:28:05.801267 140037970700032 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.9130222797393799, loss=3.0664780139923096
I0307 09:28:43.945867 140037979092736 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.0854644775390625, loss=3.074148654937744
I0307 09:29:21.982733 140037970700032 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.1181507110595703, loss=3.0445504188537598
I0307 09:30:00.253611 140037979092736 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.342196226119995, loss=3.0900745391845703
I0307 09:30:38.380502 140037970700032 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.9442559480667114, loss=2.982679843902588
I0307 09:31:16.407573 140037979092736 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.0863871574401855, loss=3.109327793121338
I0307 09:31:54.664912 140037970700032 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.0562191009521484, loss=3.032348394393921
I0307 09:32:33.062501 140037979092736 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.033186912536621, loss=3.0988118648529053
I0307 09:33:11.328618 140037970700032 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.0319175720214844, loss=3.066678047180176
I0307 09:33:49.376791 140037979092736 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.1755154132843018, loss=3.1453821659088135
2025-03-07 09:34:26.572612: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:34:28.160943 140037970700032 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.9650697708129883, loss=3.0585849285125732
I0307 09:35:05.483878 140037979092736 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.242443323135376, loss=3.0656650066375732
I0307 09:35:33.556487 140193455334592 spec.py:321] Evaluating on the training split.
I0307 09:35:45.064752 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 09:36:04.045590 140193455334592 spec.py:349] Evaluating on the test split.
I0307 09:36:05.858947 140193455334592 submission_runner.py:469] Time since start: 29735.61s, 	Step: 71477, 	{'train/accuracy': 0.7667809128761292, 'train/loss': 1.1389026641845703, 'validation/accuracy': 0.6753199696540833, 'validation/loss': 1.5427058935165405, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.210599184036255, 'test/num_examples': 10000, 'score': 27593.935863018036, 'total_duration': 29735.614179372787, 'accumulated_submission_time': 27593.935863018036, 'accumulated_eval_time': 2128.421490430832, 'accumulated_logging_time': 5.836561441421509}
I0307 09:36:05.979159 140037970700032 logging_writer.py:48] [71477] accumulated_eval_time=2128.42, accumulated_logging_time=5.83656, accumulated_submission_time=27593.9, global_step=71477, preemption_count=0, score=27593.9, test/accuracy=0.5457, test/loss=2.2106, test/num_examples=10000, total_duration=29735.6, train/accuracy=0.766781, train/loss=1.1389, validation/accuracy=0.67532, validation/loss=1.54271, validation/num_examples=50000
I0307 09:36:15.035347 140037979092736 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.9635169506072998, loss=3.0393590927124023
I0307 09:36:53.625122 140037970700032 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.186603546142578, loss=3.076951265335083
I0307 09:37:32.091237 140037979092736 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.0610220432281494, loss=2.993605136871338
I0307 09:38:10.746553 140037970700032 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.193197011947632, loss=3.157848596572876
I0307 09:38:48.959042 140037979092736 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.116420269012451, loss=3.114170551300049
I0307 09:39:27.280748 140037970700032 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.1947927474975586, loss=3.041788101196289
I0307 09:40:05.432659 140037979092736 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.0512948036193848, loss=2.9978365898132324
I0307 09:40:43.925964 140037970700032 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.1890180110931396, loss=3.0325422286987305
I0307 09:41:22.259702 140037979092736 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.058502435684204, loss=3.042111873626709
I0307 09:42:00.638945 140037970700032 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.0630645751953125, loss=3.093811511993408
I0307 09:42:39.393049 140037979092736 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1220991611480713, loss=3.1417036056518555
2025-03-07 09:42:56.965534: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:43:16.866804 140037970700032 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.0647401809692383, loss=3.102602958679199
I0307 09:43:53.588366 140037979092736 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.1529033184051514, loss=3.140768527984619
I0307 09:44:31.455441 140037970700032 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.0037312507629395, loss=3.0771636962890625
I0307 09:44:36.071131 140193455334592 spec.py:321] Evaluating on the training split.
I0307 09:44:47.424896 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 09:45:10.186764 140193455334592 spec.py:349] Evaluating on the test split.
I0307 09:45:11.963702 140193455334592 submission_runner.py:469] Time since start: 30281.72s, 	Step: 72813, 	{'train/accuracy': 0.7913145422935486, 'train/loss': 1.0311424732208252, 'validation/accuracy': 0.679919958114624, 'validation/loss': 1.5190449953079224, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.1426339149475098, 'test/num_examples': 10000, 'score': 28103.840924024582, 'total_duration': 30281.71915125847, 'accumulated_submission_time': 28103.840924024582, 'accumulated_eval_time': 2164.3138427734375, 'accumulated_logging_time': 5.9986889362335205}
I0307 09:45:12.034147 140037979092736 logging_writer.py:48] [72813] accumulated_eval_time=2164.31, accumulated_logging_time=5.99869, accumulated_submission_time=28103.8, global_step=72813, preemption_count=0, score=28103.8, test/accuracy=0.5547, test/loss=2.14263, test/num_examples=10000, total_duration=30281.7, train/accuracy=0.791315, train/loss=1.03114, validation/accuracy=0.67992, validation/loss=1.51904, validation/num_examples=50000
I0307 09:45:45.690206 140037970700032 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.002014398574829, loss=3.102203130722046
I0307 09:46:24.070909 140037979092736 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.0345160961151123, loss=2.9958088397979736
I0307 09:47:02.674398 140037970700032 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.0950751304626465, loss=3.06579327583313
I0307 09:47:40.677780 140037979092736 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.03434681892395, loss=3.075162410736084
I0307 09:48:18.886052 140037970700032 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.126945972442627, loss=3.055701971054077
I0307 09:48:57.088834 140037979092736 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.0724031925201416, loss=3.0207409858703613
I0307 09:49:35.305630 140037970700032 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.1933085918426514, loss=2.9977946281433105
I0307 09:50:13.325587 140037979092736 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.0517828464508057, loss=3.0540053844451904
I0307 09:50:51.721388 140037970700032 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.035660743713379, loss=3.0979275703430176
2025-03-07 09:51:26.521762: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:51:30.232108 140037979092736 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.1026906967163086, loss=3.0518064498901367
I0307 09:52:08.404228 140037970700032 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.1240458488464355, loss=3.127896785736084
I0307 09:52:46.979103 140037979092736 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.2394235134124756, loss=3.0268523693084717
I0307 09:53:25.014923 140037970700032 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0660743713378906, loss=3.0627617835998535
I0307 09:53:42.244053 140193455334592 spec.py:321] Evaluating on the training split.
I0307 09:53:53.142298 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 09:54:18.616791 140193455334592 spec.py:349] Evaluating on the test split.
I0307 09:54:20.387763 140193455334592 submission_runner.py:469] Time since start: 30830.14s, 	Step: 74146, 	{'train/accuracy': 0.7980309128761292, 'train/loss': 1.0135833024978638, 'validation/accuracy': 0.6847800016403198, 'validation/loss': 1.4872812032699585, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 2.140796422958374, 'test/num_examples': 10000, 'score': 28613.878880262375, 'total_duration': 30830.143275022507, 'accumulated_submission_time': 28613.878880262375, 'accumulated_eval_time': 2202.457393884659, 'accumulated_logging_time': 6.097788333892822}
I0307 09:54:20.477758 140037979092736 logging_writer.py:48] [74146] accumulated_eval_time=2202.46, accumulated_logging_time=6.09779, accumulated_submission_time=28613.9, global_step=74146, preemption_count=0, score=28613.9, test/accuracy=0.5621, test/loss=2.1408, test/num_examples=10000, total_duration=30830.1, train/accuracy=0.798031, train/loss=1.01358, validation/accuracy=0.68478, validation/loss=1.48728, validation/num_examples=50000
I0307 09:54:41.231556 140037970700032 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.2602005004882812, loss=3.0705490112304688
I0307 09:55:19.303787 140037979092736 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.0467031002044678, loss=2.9927377700805664
I0307 09:55:57.586588 140037970700032 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.109684705734253, loss=3.030125617980957
I0307 09:56:35.690231 140037979092736 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.087106704711914, loss=3.0131425857543945
I0307 09:57:13.989602 140037970700032 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.229721784591675, loss=3.0732932090759277
I0307 09:57:51.797411 140037979092736 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.1193525791168213, loss=3.1127521991729736
I0307 09:58:30.346814 140037970700032 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.0888915061950684, loss=2.955556869506836
I0307 09:59:08.404815 140037979092736 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.1049113273620605, loss=3.035905122756958
I0307 09:59:46.722437 140037970700032 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.1157870292663574, loss=3.0826523303985596
2025-03-07 10:00:03.234763: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:00:25.078242 140037979092736 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.1734001636505127, loss=3.086421251296997
I0307 10:01:02.690052 140037970700032 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.1308979988098145, loss=3.119741678237915
I0307 10:01:40.513612 140037979092736 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.3069710731506348, loss=3.1746551990509033
I0307 10:02:18.528228 140037970700032 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.168140411376953, loss=3.0721960067749023
I0307 10:02:50.473644 140193455334592 spec.py:321] Evaluating on the training split.
I0307 10:03:01.516179 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 10:03:26.582958 140193455334592 spec.py:349] Evaluating on the test split.
I0307 10:03:28.386382 140193455334592 submission_runner.py:469] Time since start: 31378.14s, 	Step: 75486, 	{'train/accuracy': 0.7831034660339355, 'train/loss': 1.08640718460083, 'validation/accuracy': 0.6774199604988098, 'validation/loss': 1.5339094400405884, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.1947720050811768, 'test/num_examples': 10000, 'score': 29123.67890906334, 'total_duration': 31378.141866207123, 'accumulated_submission_time': 29123.67890906334, 'accumulated_eval_time': 2240.36994600296, 'accumulated_logging_time': 6.240447759628296}
I0307 10:03:28.508226 140037979092736 logging_writer.py:48] [75486] accumulated_eval_time=2240.37, accumulated_logging_time=6.24045, accumulated_submission_time=29123.7, global_step=75486, preemption_count=0, score=29123.7, test/accuracy=0.5479, test/loss=2.19477, test/num_examples=10000, total_duration=31378.1, train/accuracy=0.783103, train/loss=1.08641, validation/accuracy=0.67742, validation/loss=1.53391, validation/num_examples=50000
I0307 10:03:34.278743 140037970700032 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.0827951431274414, loss=3.104795217514038
I0307 10:04:11.961400 140037979092736 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.0742311477661133, loss=3.0726122856140137
I0307 10:04:49.396979 140037970700032 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.9974886178970337, loss=3.042057752609253
I0307 10:05:27.437174 140037979092736 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.960215449333191, loss=2.98746919631958
I0307 10:06:05.345493 140037970700032 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.0774481296539307, loss=3.0388424396514893
I0307 10:06:43.519968 140037979092736 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.9419670104980469, loss=2.998854875564575
I0307 10:07:22.022331 140037970700032 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.138129949569702, loss=3.049661159515381
I0307 10:07:59.945721 140037979092736 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.235462188720703, loss=3.0720839500427246
I0307 10:08:37.518304 140037970700032 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.303088665008545, loss=3.070842981338501
2025-03-07 10:08:38.768726: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:09:15.093570 140037979092736 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.0964720249176025, loss=3.0015206336975098
I0307 10:09:52.545294 140037970700032 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.352524995803833, loss=3.1115987300872803
I0307 10:10:30.433811 140037979092736 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.979670524597168, loss=3.0397820472717285
I0307 10:11:08.198018 140037970700032 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.087184190750122, loss=3.0774407386779785
I0307 10:11:45.542044 140037979092736 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.235527753829956, loss=3.04304575920105
I0307 10:11:58.401124 140193455334592 spec.py:321] Evaluating on the training split.
I0307 10:12:09.882482 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 10:12:35.419840 140193455334592 spec.py:349] Evaluating on the test split.
I0307 10:12:37.197911 140193455334592 submission_runner.py:469] Time since start: 31926.95s, 	Step: 76835, 	{'train/accuracy': 0.7745735049247742, 'train/loss': 1.1141470670700073, 'validation/accuracy': 0.6737799644470215, 'validation/loss': 1.5497111082077026, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.2350151538848877, 'test/num_examples': 10000, 'score': 29633.381766319275, 'total_duration': 31926.95335674286, 'accumulated_submission_time': 29633.381766319275, 'accumulated_eval_time': 2279.166508436203, 'accumulated_logging_time': 6.403590440750122}
I0307 10:12:37.387917 140037970700032 logging_writer.py:48] [76835] accumulated_eval_time=2279.17, accumulated_logging_time=6.40359, accumulated_submission_time=29633.4, global_step=76835, preemption_count=0, score=29633.4, test/accuracy=0.5433, test/loss=2.23502, test/num_examples=10000, total_duration=31927, train/accuracy=0.774574, train/loss=1.11415, validation/accuracy=0.67378, validation/loss=1.54971, validation/num_examples=50000
I0307 10:13:02.328542 140037979092736 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.0869181156158447, loss=3.0461413860321045
I0307 10:13:40.604888 140037970700032 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.0045933723449707, loss=3.0345141887664795
I0307 10:14:18.955302 140037979092736 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.171813488006592, loss=3.033355474472046
I0307 10:14:57.186556 140037970700032 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.3613669872283936, loss=3.119718074798584
I0307 10:15:35.119057 140037979092736 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.182292938232422, loss=3.004916191101074
I0307 10:16:13.445015 140037970700032 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.1251206398010254, loss=3.0318312644958496
I0307 10:16:51.635366 140037979092736 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.1304357051849365, loss=3.1015799045562744
2025-03-07 10:17:13.117192: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:17:30.929928 140037970700032 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.2360098361968994, loss=3.0731887817382812
I0307 10:18:09.096975 140037979092736 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.9884642362594604, loss=3.1056571006774902
I0307 10:18:46.519240 140037970700032 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.3062429428100586, loss=3.023324728012085
I0307 10:19:23.273810 140037979092736 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.1642956733703613, loss=3.055514097213745
I0307 10:20:01.423398 140037970700032 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.3708653450012207, loss=3.0817651748657227
I0307 10:20:38.452256 140037979092736 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.0015058517456055, loss=2.9997894763946533
I0307 10:21:07.244102 140193455334592 spec.py:321] Evaluating on the training split.
I0307 10:21:18.640186 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 10:21:40.395893 140193455334592 spec.py:349] Evaluating on the test split.
I0307 10:21:42.334174 140193455334592 submission_runner.py:469] Time since start: 32472.09s, 	Step: 78179, 	{'train/accuracy': 0.7847377061843872, 'train/loss': 1.0560338497161865, 'validation/accuracy': 0.6899200081825256, 'validation/loss': 1.466614007949829, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.1106247901916504, 'test/num_examples': 10000, 'score': 30143.054849624634, 'total_duration': 32472.089480400085, 'accumulated_submission_time': 30143.054849624634, 'accumulated_eval_time': 2314.256219148636, 'accumulated_logging_time': 6.629779100418091}
I0307 10:21:42.450319 140037970700032 logging_writer.py:48] [78179] accumulated_eval_time=2314.26, accumulated_logging_time=6.62978, accumulated_submission_time=30143.1, global_step=78179, preemption_count=0, score=30143.1, test/accuracy=0.563, test/loss=2.11062, test/num_examples=10000, total_duration=32472.1, train/accuracy=0.784738, train/loss=1.05603, validation/accuracy=0.68992, validation/loss=1.46661, validation/num_examples=50000
I0307 10:21:50.910488 140037979092736 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.326728105545044, loss=3.0880532264709473
I0307 10:22:28.861545 140037970700032 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1570816040039062, loss=3.0670740604400635
I0307 10:23:06.908867 140037979092736 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.167940616607666, loss=3.048240900039673
I0307 10:23:45.030822 140037970700032 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.0841329097747803, loss=3.065255641937256
I0307 10:24:23.402192 140037979092736 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.224672555923462, loss=3.057973623275757
I0307 10:25:01.536082 140037970700032 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.2376604080200195, loss=2.9834699630737305
2025-03-07 10:25:38.966872: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:25:39.624967 140037979092736 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.2925331592559814, loss=3.1155052185058594
I0307 10:26:18.266913 140037970700032 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.2800610065460205, loss=3.069528341293335
I0307 10:26:56.428076 140037979092736 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.10986590385437, loss=3.04418683052063
I0307 10:27:34.514881 140037970700032 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.2828710079193115, loss=3.150841236114502
I0307 10:28:12.724369 140037979092736 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.987900733947754, loss=3.003718137741089
I0307 10:28:50.758741 140037970700032 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.039569139480591, loss=3.044602870941162
I0307 10:29:28.849694 140037979092736 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.2094147205352783, loss=3.1133177280426025
I0307 10:30:06.976188 140037970700032 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.2294254302978516, loss=3.009552478790283
I0307 10:30:12.387661 140193455334592 spec.py:321] Evaluating on the training split.
I0307 10:30:23.855874 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 10:30:47.243905 140193455334592 spec.py:349] Evaluating on the test split.
I0307 10:30:49.043481 140193455334592 submission_runner.py:469] Time since start: 33018.80s, 	Step: 79515, 	{'train/accuracy': 0.7818478941917419, 'train/loss': 1.0438038110733032, 'validation/accuracy': 0.6883800029754639, 'validation/loss': 1.4495753049850464, 'validation/num_examples': 50000, 'test/accuracy': 0.5608000159263611, 'test/loss': 2.113530397415161, 'test/num_examples': 10000, 'score': 30652.80054306984, 'total_duration': 33018.799008369446, 'accumulated_submission_time': 30652.80054306984, 'accumulated_eval_time': 2350.9118955135345, 'accumulated_logging_time': 6.792917966842651}
I0307 10:30:49.137159 140037979092736 logging_writer.py:48] [79515] accumulated_eval_time=2350.91, accumulated_logging_time=6.79292, accumulated_submission_time=30652.8, global_step=79515, preemption_count=0, score=30652.8, test/accuracy=0.5608, test/loss=2.11353, test/num_examples=10000, total_duration=33018.8, train/accuracy=0.781848, train/loss=1.0438, validation/accuracy=0.68838, validation/loss=1.44958, validation/num_examples=50000
I0307 10:31:21.856166 140037970700032 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.3133742809295654, loss=3.0774545669555664
I0307 10:31:59.975373 140037979092736 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.1894044876098633, loss=3.0767974853515625
I0307 10:32:38.074005 140037970700032 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.0273616313934326, loss=2.949481964111328
I0307 10:33:16.291631 140037979092736 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.1496481895446777, loss=2.980776786804199
I0307 10:33:54.537019 140037970700032 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.199044704437256, loss=2.9865856170654297
2025-03-07 10:34:16.843177: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:34:34.213837 140037979092736 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.133334159851074, loss=3.0172433853149414
I0307 10:35:12.099822 140037970700032 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.258273124694824, loss=3.092588186264038
I0307 10:35:49.436102 140037979092736 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.3497724533081055, loss=3.073481798171997
I0307 10:36:26.472557 140037970700032 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.1916306018829346, loss=3.0385665893554688
I0307 10:37:04.019505 140037979092736 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.376035213470459, loss=3.0098979473114014
I0307 10:37:42.946871 140037970700032 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.3321356773376465, loss=3.0451455116271973
I0307 10:38:20.418747 140037979092736 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.2094898223876953, loss=2.993699073791504
I0307 10:38:58.508106 140037970700032 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.1884026527404785, loss=3.008915901184082
I0307 10:39:19.125831 140193455334592 spec.py:321] Evaluating on the training split.
I0307 10:39:30.729310 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 10:39:52.530381 140193455334592 spec.py:349] Evaluating on the test split.
I0307 10:39:54.497469 140193455334592 submission_runner.py:469] Time since start: 33564.25s, 	Step: 80857, 	{'train/accuracy': 0.7804726958274841, 'train/loss': 1.0745245218276978, 'validation/accuracy': 0.6845399737358093, 'validation/loss': 1.4880666732788086, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.132286310195923, 'test/num_examples': 10000, 'score': 31162.61455154419, 'total_duration': 33564.25296974182, 'accumulated_submission_time': 31162.61455154419, 'accumulated_eval_time': 2386.283365011215, 'accumulated_logging_time': 6.913316011428833}
I0307 10:39:54.632373 140037979092736 logging_writer.py:48] [80857] accumulated_eval_time=2386.28, accumulated_logging_time=6.91332, accumulated_submission_time=31162.6, global_step=80857, preemption_count=0, score=31162.6, test/accuracy=0.5593, test/loss=2.13229, test/num_examples=10000, total_duration=33564.3, train/accuracy=0.780473, train/loss=1.07452, validation/accuracy=0.68454, validation/loss=1.48807, validation/num_examples=50000
I0307 10:40:11.305952 140037970700032 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.112321138381958, loss=3.032043933868408
I0307 10:40:49.521771 140037979092736 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.0906944274902344, loss=3.0189883708953857
I0307 10:41:27.967234 140037970700032 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.2539479732513428, loss=2.9607093334198
I0307 10:42:06.679107 140037979092736 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.1076040267944336, loss=2.95957088470459
I0307 10:42:45.084338 140037970700032 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.204747438430786, loss=3.039741277694702
2025-03-07 10:42:48.508900: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:43:23.843724 140037979092736 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.056462287902832, loss=2.912403106689453
I0307 10:44:00.426796 140037970700032 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.338301420211792, loss=3.060112714767456
I0307 10:44:37.931396 140037979092736 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.202634811401367, loss=2.9887514114379883
I0307 10:45:15.092072 140037970700032 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.252263069152832, loss=2.9992940425872803
I0307 10:45:51.621318 140037979092736 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.404352903366089, loss=3.099371910095215
I0307 10:46:28.677651 140037970700032 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.2100536823272705, loss=3.008403778076172
I0307 10:47:05.785949 140037979092736 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.2693612575531006, loss=3.0361995697021484
I0307 10:47:43.019321 140037970700032 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.1233012676239014, loss=3.008270025253296
I0307 10:48:20.517637 140037979092736 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.0937623977661133, loss=3.0052406787872314
I0307 10:48:24.534975 140193455334592 spec.py:321] Evaluating on the training split.
I0307 10:48:36.167021 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 10:48:58.045325 140193455334592 spec.py:349] Evaluating on the test split.
I0307 10:48:59.869856 140193455334592 submission_runner.py:469] Time since start: 34109.63s, 	Step: 82211, 	{'train/accuracy': 0.7780612111091614, 'train/loss': 1.0861520767211914, 'validation/accuracy': 0.6893399953842163, 'validation/loss': 1.480414628982544, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.148196220397949, 'test/num_examples': 10000, 'score': 31672.33935379982, 'total_duration': 34109.625369787216, 'accumulated_submission_time': 31672.33935379982, 'accumulated_eval_time': 2421.618091106415, 'accumulated_logging_time': 7.078527450561523}
I0307 10:48:59.953828 140037970700032 logging_writer.py:48] [82211] accumulated_eval_time=2421.62, accumulated_logging_time=7.07853, accumulated_submission_time=31672.3, global_step=82211, preemption_count=0, score=31672.3, test/accuracy=0.5547, test/loss=2.1482, test/num_examples=10000, total_duration=34109.6, train/accuracy=0.778061, train/loss=1.08615, validation/accuracy=0.68934, validation/loss=1.48041, validation/num_examples=50000
I0307 10:49:34.552125 140037979092736 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.3882193565368652, loss=3.053330421447754
I0307 10:50:12.866742 140037970700032 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.2431821823120117, loss=2.944876194000244
I0307 10:50:51.059989 140037979092736 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.0816445350646973, loss=3.023580551147461
2025-03-07 10:51:12.408182: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:51:28.486565 140037970700032 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.176213026046753, loss=3.0211966037750244
I0307 10:52:05.314797 140037979092736 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.1473493576049805, loss=3.010849952697754
I0307 10:52:42.622540 140037970700032 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.1830499172210693, loss=3.0006823539733887
I0307 10:53:20.350101 140037979092736 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.3059372901916504, loss=2.9992120265960693
I0307 10:53:57.424282 140037970700032 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.3038690090179443, loss=3.045236110687256
I0307 10:54:35.704093 140037979092736 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.2441797256469727, loss=2.9942855834960938
I0307 10:55:13.887728 140037970700032 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.198030471801758, loss=3.0640487670898438
I0307 10:55:51.509464 140037979092736 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.2097327709198, loss=2.9942076206207275
I0307 10:56:29.966741 140037970700032 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.214435577392578, loss=3.0398592948913574
I0307 10:57:07.434987 140037979092736 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.2380149364471436, loss=3.040858507156372
I0307 10:57:30.025393 140193455334592 spec.py:321] Evaluating on the training split.
I0307 10:57:41.595467 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 10:58:04.872900 140193455334592 spec.py:349] Evaluating on the test split.
I0307 10:58:06.625508 140193455334592 submission_runner.py:469] Time since start: 34656.38s, 	Step: 83559, 	{'train/accuracy': 0.7867107391357422, 'train/loss': 1.0743482112884521, 'validation/accuracy': 0.6917399764060974, 'validation/loss': 1.4812990427017212, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 2.110980749130249, 'test/num_examples': 10000, 'score': 32182.227016448975, 'total_duration': 34656.38101768494, 'accumulated_submission_time': 32182.227016448975, 'accumulated_eval_time': 2458.2180569171906, 'accumulated_logging_time': 7.198639392852783}
I0307 10:58:06.776663 140037970700032 logging_writer.py:48] [83559] accumulated_eval_time=2458.22, accumulated_logging_time=7.19864, accumulated_submission_time=32182.2, global_step=83559, preemption_count=0, score=32182.2, test/accuracy=0.5705, test/loss=2.11098, test/num_examples=10000, total_duration=34656.4, train/accuracy=0.786711, train/loss=1.07435, validation/accuracy=0.69174, validation/loss=1.4813, validation/num_examples=50000
I0307 10:58:22.983704 140037979092736 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.2961556911468506, loss=3.022197961807251
I0307 10:59:01.168573 140037970700032 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.3391215801239014, loss=3.101970672607422
I0307 10:59:39.893974 140037979092736 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.153432607650757, loss=2.997943878173828
2025-03-07 10:59:44.467781: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:00:20.551525 140037970700032 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.27289080619812, loss=3.066549301147461
I0307 11:00:58.814982 140037979092736 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.4015119075775146, loss=3.0350027084350586
I0307 11:01:37.089128 140037970700032 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.2420713901519775, loss=2.9976966381073
I0307 11:02:14.235673 140037979092736 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.3081345558166504, loss=2.9732775688171387
I0307 11:02:51.762185 140037970700032 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.4333014488220215, loss=3.032599449157715
I0307 11:03:30.070205 140037979092736 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.2329702377319336, loss=3.0364878177642822
I0307 11:04:07.467026 140037970700032 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.2760727405548096, loss=2.982621669769287
I0307 11:04:45.444273 140037979092736 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.1810593605041504, loss=2.990654706954956
I0307 11:05:23.756072 140037970700032 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.1707701683044434, loss=3.0192103385925293
I0307 11:06:01.800144 140037979092736 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.157611846923828, loss=3.04247784614563
I0307 11:06:36.738077 140193455334592 spec.py:321] Evaluating on the training split.
I0307 11:06:48.553608 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 11:07:10.458544 140193455334592 spec.py:349] Evaluating on the test split.
I0307 11:07:12.215487 140193455334592 submission_runner.py:469] Time since start: 35201.97s, 	Step: 84893, 	{'train/accuracy': 0.7821866869926453, 'train/loss': 1.0713037252426147, 'validation/accuracy': 0.6906599998474121, 'validation/loss': 1.4716929197311401, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.1253280639648438, 'test/num_examples': 10000, 'score': 32692.007880926132, 'total_duration': 35201.97100162506, 'accumulated_submission_time': 32692.007880926132, 'accumulated_eval_time': 2493.6953144073486, 'accumulated_logging_time': 7.387145757675171}
I0307 11:07:12.308672 140037970700032 logging_writer.py:48] [84893] accumulated_eval_time=2493.7, accumulated_logging_time=7.38715, accumulated_submission_time=32692, global_step=84893, preemption_count=0, score=32692, test/accuracy=0.5591, test/loss=2.12533, test/num_examples=10000, total_duration=35202, train/accuracy=0.782187, train/loss=1.0713, validation/accuracy=0.69066, validation/loss=1.47169, validation/num_examples=50000
I0307 11:07:15.288532 140037979092736 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.1294801235198975, loss=2.970515012741089
I0307 11:07:54.094385 140037970700032 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.2852048873901367, loss=3.034430980682373
2025-03-07 11:08:16.331353: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:08:31.898566 140037979092736 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.0999958515167236, loss=3.0126543045043945
I0307 11:09:18.242356 140037970700032 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.2168424129486084, loss=2.9627127647399902
I0307 11:09:56.471148 140037979092736 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.2435388565063477, loss=2.952903985977173
I0307 11:10:34.433749 140037970700032 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.3884201049804688, loss=3.016145706176758
I0307 11:11:12.579705 140037979092736 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.3286852836608887, loss=2.9948198795318604
I0307 11:11:50.875090 140037970700032 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.299835681915283, loss=3.0279171466827393
I0307 11:12:28.891579 140037979092736 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.128157615661621, loss=2.978318691253662
I0307 11:13:06.823683 140037970700032 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.5414376258850098, loss=2.9664764404296875
I0307 11:13:45.026566 140037979092736 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.4115214347839355, loss=3.0369439125061035
I0307 11:14:23.269353 140037970700032 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.594054698944092, loss=3.0598642826080322
I0307 11:15:01.925299 140037979092736 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.260960578918457, loss=3.0133626461029053
I0307 11:15:40.025098 140037970700032 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.400893449783325, loss=2.9930412769317627
I0307 11:15:42.329604 140193455334592 spec.py:321] Evaluating on the training split.
I0307 11:15:53.276952 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 11:16:18.134451 140193455334592 spec.py:349] Evaluating on the test split.
I0307 11:16:19.897498 140193455334592 submission_runner.py:469] Time since start: 35749.65s, 	Step: 86207, 	{'train/accuracy': 0.77933669090271, 'train/loss': 1.0669200420379639, 'validation/accuracy': 0.6860799789428711, 'validation/loss': 1.4645520448684692, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.089719533920288, 'test/num_examples': 10000, 'score': 33201.859541893005, 'total_duration': 35749.65291810036, 'accumulated_submission_time': 33201.859541893005, 'accumulated_eval_time': 2531.262959241867, 'accumulated_logging_time': 7.509037256240845}
I0307 11:16:19.989834 140037979092736 logging_writer.py:48] [86207] accumulated_eval_time=2531.26, accumulated_logging_time=7.50904, accumulated_submission_time=33201.9, global_step=86207, preemption_count=0, score=33201.9, test/accuracy=0.5644, test/loss=2.08972, test/num_examples=10000, total_duration=35749.7, train/accuracy=0.779337, train/loss=1.06692, validation/accuracy=0.68608, validation/loss=1.46455, validation/num_examples=50000
I0307 11:16:55.989620 140037970700032 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.4092674255371094, loss=3.0619943141937256
2025-03-07 11:16:59.154710: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:17:32.738554 140037979092736 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.3346450328826904, loss=3.087815046310425
I0307 11:18:10.152739 140037970700032 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.168445348739624, loss=2.9848439693450928
I0307 11:18:47.159594 140037979092736 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.284140110015869, loss=3.0427706241607666
I0307 11:19:23.958968 140037970700032 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.374946117401123, loss=2.98610782623291
I0307 11:20:01.748372 140037979092736 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.2058229446411133, loss=2.928677558898926
I0307 11:20:39.107088 140037970700032 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.2257227897644043, loss=3.0007925033569336
I0307 11:21:16.722776 140037979092736 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.2060577869415283, loss=2.9883151054382324
I0307 11:21:54.623256 140037970700032 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.17838716506958, loss=3.0133590698242188
I0307 11:22:33.090658 140037979092736 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.4560353755950928, loss=3.070181369781494
I0307 11:23:11.698717 140037970700032 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.213135004043579, loss=3.0366058349609375
I0307 11:23:50.162482 140037979092736 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.3291871547698975, loss=3.0755693912506104
I0307 11:24:28.342002 140037970700032 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.342282772064209, loss=3.014029026031494
I0307 11:24:49.968853 140193455334592 spec.py:321] Evaluating on the training split.
2025-03-07 11:24:51.988464: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:25:02.624675 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 11:25:24.081916 140193455334592 spec.py:349] Evaluating on the test split.
I0307 11:25:25.823024 140193455334592 submission_runner.py:469] Time since start: 36295.58s, 	Step: 87558, 	{'train/accuracy': 0.7837810516357422, 'train/loss': 1.0522605180740356, 'validation/accuracy': 0.6951599717140198, 'validation/loss': 1.4413443803787231, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 2.101987361907959, 'test/num_examples': 10000, 'score': 33711.657796382904, 'total_duration': 36295.57853245735, 'accumulated_submission_time': 33711.657796382904, 'accumulated_eval_time': 2567.116968154907, 'accumulated_logging_time': 7.635222673416138}
I0307 11:25:25.911058 140037979092736 logging_writer.py:48] [87558] accumulated_eval_time=2567.12, accumulated_logging_time=7.63522, accumulated_submission_time=33711.7, global_step=87558, preemption_count=0, score=33711.7, test/accuracy=0.5676, test/loss=2.10199, test/num_examples=10000, total_duration=36295.6, train/accuracy=0.783781, train/loss=1.05226, validation/accuracy=0.69516, validation/loss=1.44134, validation/num_examples=50000
I0307 11:25:42.198137 140037970700032 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.2591123580932617, loss=3.0559659004211426
I0307 11:26:20.355649 140037979092736 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.2225489616394043, loss=3.0207269191741943
I0307 11:26:58.826951 140037970700032 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.2479519844055176, loss=2.961174726486206
I0307 11:27:36.989328 140037979092736 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.368631362915039, loss=3.050457715988159
I0307 11:28:15.093110 140037970700032 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.3377864360809326, loss=3.0447380542755127
I0307 11:28:53.186289 140037979092736 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.2412540912628174, loss=2.9771299362182617
I0307 11:29:31.281540 140037970700032 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3592898845672607, loss=2.971405267715454
I0307 11:30:09.668282 140037979092736 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.388556718826294, loss=3.003368377685547
I0307 11:30:47.913952 140037970700032 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.255833864212036, loss=2.983607292175293
I0307 11:31:26.114646 140037979092736 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.3149280548095703, loss=2.9747231006622314
I0307 11:32:04.554269 140037970700032 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.5825328826904297, loss=3.061091184616089
I0307 11:32:42.600417 140037979092736 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.4059865474700928, loss=2.984589099884033
I0307 11:33:20.423842 140037970700032 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.3705365657806396, loss=2.97929048538208
2025-03-07 11:33:24.731773: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:33:56.039144 140193455334592 spec.py:321] Evaluating on the training split.
I0307 11:34:07.259999 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 11:34:28.830913 140193455334592 spec.py:349] Evaluating on the test split.
I0307 11:34:30.640002 140193455334592 submission_runner.py:469] Time since start: 36840.40s, 	Step: 88896, 	{'train/accuracy': 0.7887236475944519, 'train/loss': 1.0084716081619263, 'validation/accuracy': 0.6981599926948547, 'validation/loss': 1.4085276126861572, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 2.0528955459594727, 'test/num_examples': 10000, 'score': 34221.60295796394, 'total_duration': 36840.39541339874, 'accumulated_submission_time': 34221.60295796394, 'accumulated_eval_time': 2601.7175676822662, 'accumulated_logging_time': 7.762111186981201}
I0307 11:34:30.730391 140037979092736 logging_writer.py:48] [88896] accumulated_eval_time=2601.72, accumulated_logging_time=7.76211, accumulated_submission_time=34221.6, global_step=88896, preemption_count=0, score=34221.6, test/accuracy=0.5717, test/loss=2.0529, test/num_examples=10000, total_duration=36840.4, train/accuracy=0.788724, train/loss=1.00847, validation/accuracy=0.69816, validation/loss=1.40853, validation/num_examples=50000
I0307 11:34:32.633748 140037970700032 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.3181650638580322, loss=3.0142874717712402
I0307 11:35:10.697149 140037979092736 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.3471293449401855, loss=3.0339717864990234
I0307 11:35:49.082195 140037970700032 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.2008249759674072, loss=3.060457229614258
I0307 11:36:27.138805 140037979092736 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.2593772411346436, loss=2.9723258018493652
I0307 11:37:05.320119 140037970700032 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.20160174369812, loss=2.9628264904022217
I0307 11:37:43.668375 140037979092736 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.2757158279418945, loss=3.034970283508301
I0307 11:38:22.074615 140037970700032 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.3932111263275146, loss=3.026913642883301
I0307 11:39:00.531409 140037979092736 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.5633671283721924, loss=3.043804168701172
I0307 11:39:38.629684 140037970700032 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.253035306930542, loss=3.0153865814208984
I0307 11:40:16.808508 140037979092736 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.2765707969665527, loss=2.9549272060394287
I0307 11:40:54.814025 140037970700032 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.3924310207366943, loss=2.959279775619507
I0307 11:41:33.243248 140037979092736 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.3400087356567383, loss=3.0206832885742188
2025-03-07 11:41:54.533593: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:42:11.784471 140037970700032 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.2910878658294678, loss=2.959042549133301
I0307 11:42:49.669107 140037979092736 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.624277114868164, loss=3.1088039875030518
I0307 11:43:00.893617 140193455334592 spec.py:321] Evaluating on the training split.
I0307 11:43:12.207707 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 11:43:32.984190 140193455334592 spec.py:349] Evaluating on the test split.
I0307 11:43:34.773322 140193455334592 submission_runner.py:469] Time since start: 37384.53s, 	Step: 90231, 	{'train/accuracy': 0.7817482352256775, 'train/loss': 1.0631102323532104, 'validation/accuracy': 0.6956200003623962, 'validation/loss': 1.4581043720245361, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 2.08136248588562, 'test/num_examples': 10000, 'score': 34731.56350541115, 'total_duration': 37384.528841257095, 'accumulated_submission_time': 34731.56350541115, 'accumulated_eval_time': 2635.5971190929413, 'accumulated_logging_time': 7.91056752204895}
I0307 11:43:34.876109 140037970700032 logging_writer.py:48] [90231] accumulated_eval_time=2635.6, accumulated_logging_time=7.91057, accumulated_submission_time=34731.6, global_step=90231, preemption_count=0, score=34731.6, test/accuracy=0.5692, test/loss=2.08136, test/num_examples=10000, total_duration=37384.5, train/accuracy=0.781748, train/loss=1.06311, validation/accuracy=0.69562, validation/loss=1.4581, validation/num_examples=50000
I0307 11:44:01.633180 140037979092736 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.323331832885742, loss=3.0142016410827637
I0307 11:44:39.821451 140037970700032 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.4140682220458984, loss=3.050757884979248
I0307 11:45:18.274152 140037979092736 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.427548408508301, loss=3.035292625427246
I0307 11:45:56.368544 140037970700032 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.3650341033935547, loss=2.93672776222229
I0307 11:46:34.512666 140037979092736 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.5438711643218994, loss=3.0243797302246094
I0307 11:47:12.842756 140037970700032 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.4744744300842285, loss=3.029039144515991
I0307 11:47:51.176495 140037979092736 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.3576455116271973, loss=3.039802312850952
I0307 11:48:29.755601 140037970700032 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.390474319458008, loss=3.003260374069214
I0307 11:49:07.985768 140037979092736 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.358973979949951, loss=3.019582509994507
I0307 11:49:46.163223 140037970700032 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.3742640018463135, loss=2.9445860385894775
I0307 11:50:25.410489 140037979092736 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.237529993057251, loss=2.970099925994873
2025-03-07 11:50:32.697996: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:51:04.282576 140037970700032 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.2522690296173096, loss=2.9475176334381104
I0307 11:51:41.900475 140037979092736 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.287693500518799, loss=3.0094656944274902
I0307 11:52:04.903860 140193455334592 spec.py:321] Evaluating on the training split.
I0307 11:52:16.244106 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 11:52:40.413506 140193455334592 spec.py:349] Evaluating on the test split.
I0307 11:52:42.143085 140193455334592 submission_runner.py:469] Time since start: 37931.90s, 	Step: 91563, 	{'train/accuracy': 0.7726601958274841, 'train/loss': 1.1026209592819214, 'validation/accuracy': 0.6838799715042114, 'validation/loss': 1.4902595281600952, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.146847724914551, 'test/num_examples': 10000, 'score': 35241.41734933853, 'total_duration': 37931.89859294891, 'accumulated_submission_time': 35241.41734933853, 'accumulated_eval_time': 2672.8361806869507, 'accumulated_logging_time': 8.042389154434204}
I0307 11:52:42.218980 140037970700032 logging_writer.py:48] [91563] accumulated_eval_time=2672.84, accumulated_logging_time=8.04239, accumulated_submission_time=35241.4, global_step=91563, preemption_count=0, score=35241.4, test/accuracy=0.5598, test/loss=2.14685, test/num_examples=10000, total_duration=37931.9, train/accuracy=0.77266, train/loss=1.10262, validation/accuracy=0.68388, validation/loss=1.49026, validation/num_examples=50000
I0307 11:52:56.668675 140037979092736 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.24189829826355, loss=2.9754865169525146
I0307 11:53:34.957389 140037970700032 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.3819193840026855, loss=3.0356976985931396
I0307 11:54:13.883533 140037979092736 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.334282636642456, loss=3.0096547603607178
I0307 11:54:52.409550 140037970700032 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.2579870223999023, loss=2.979389190673828
I0307 11:55:30.885726 140037979092736 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.2988688945770264, loss=3.03818416595459
I0307 11:56:09.777531 140037970700032 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.3722622394561768, loss=3.0295157432556152
I0307 11:56:48.285769 140037979092736 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.35266375541687, loss=3.0228817462921143
I0307 11:57:26.444911 140037970700032 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.445058822631836, loss=3.0499770641326904
I0307 11:58:04.484503 140037979092736 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.40869402885437, loss=3.0415475368499756
I0307 11:58:42.851970 140037970700032 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.2435154914855957, loss=2.988652229309082
2025-03-07 11:59:12.516614: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:59:25.849513 140037979092736 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.3295235633850098, loss=2.9302029609680176
I0307 12:00:03.383156 140037970700032 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.418623685836792, loss=2.9006457328796387
I0307 12:00:41.671119 140037979092736 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.2128350734710693, loss=2.9731698036193848
I0307 12:01:12.252356 140193455334592 spec.py:321] Evaluating on the training split.
I0307 12:01:23.490217 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 12:01:46.086165 140193455334592 spec.py:349] Evaluating on the test split.
I0307 12:01:47.827781 140193455334592 submission_runner.py:469] Time since start: 38477.58s, 	Step: 92882, 	{'train/accuracy': 0.787527859210968, 'train/loss': 1.003239393234253, 'validation/accuracy': 0.6966999769210815, 'validation/loss': 1.4107553958892822, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 2.0533812046051025, 'test/num_examples': 10000, 'score': 35751.284779548645, 'total_duration': 38477.58324742317, 'accumulated_submission_time': 35751.284779548645, 'accumulated_eval_time': 2708.4114084243774, 'accumulated_logging_time': 8.142436265945435}
I0307 12:01:47.933748 140037970700032 logging_writer.py:48] [92882] accumulated_eval_time=2708.41, accumulated_logging_time=8.14244, accumulated_submission_time=35751.3, global_step=92882, preemption_count=0, score=35751.3, test/accuracy=0.5684, test/loss=2.05338, test/num_examples=10000, total_duration=38477.6, train/accuracy=0.787528, train/loss=1.00324, validation/accuracy=0.6967, validation/loss=1.41076, validation/num_examples=50000
I0307 12:01:55.181619 140037979092736 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.3449509143829346, loss=2.978628158569336
I0307 12:02:33.871301 140037970700032 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.4476466178894043, loss=3.0449202060699463
I0307 12:03:12.350208 140037979092736 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.53104829788208, loss=2.932584524154663
I0307 12:03:50.812987 140037970700032 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.361882209777832, loss=3.0010154247283936
I0307 12:04:29.354553 140037979092736 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.520956516265869, loss=2.959353446960449
I0307 12:05:07.422460 140037970700032 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.3648455142974854, loss=2.9706530570983887
I0307 12:05:44.741716 140037979092736 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.40443754196167, loss=3.0488226413726807
I0307 12:06:23.245891 140037970700032 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.529219627380371, loss=3.003262996673584
I0307 12:07:01.326085 140037979092736 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.191883087158203, loss=2.942281723022461
I0307 12:07:42.027795 140037970700032 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.581132173538208, loss=2.9911837577819824
2025-03-07 12:07:50.677852: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:08:22.727051 140037979092736 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.3878071308135986, loss=2.990748882293701
I0307 12:08:59.950378 140037970700032 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.463637590408325, loss=3.000955820083618
I0307 12:09:38.490988 140037979092736 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.419950485229492, loss=2.924234628677368
I0307 12:10:16.546595 140037970700032 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.279503107070923, loss=2.9325079917907715
I0307 12:10:18.080309 140193455334592 spec.py:321] Evaluating on the training split.
I0307 12:10:29.876600 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 12:10:54.214136 140193455334592 spec.py:349] Evaluating on the test split.
I0307 12:10:55.994178 140193455334592 submission_runner.py:469] Time since start: 39025.75s, 	Step: 94205, 	{'train/accuracy': 0.7894012928009033, 'train/loss': 1.0597296953201294, 'validation/accuracy': 0.7023400068283081, 'validation/loss': 1.4387004375457764, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 2.0974159240722656, 'test/num_examples': 10000, 'score': 36261.24402523041, 'total_duration': 39025.749687194824, 'accumulated_submission_time': 36261.24402523041, 'accumulated_eval_time': 2746.3251185417175, 'accumulated_logging_time': 8.29013967514038}
I0307 12:10:56.069028 140037979092736 logging_writer.py:48] [94205] accumulated_eval_time=2746.33, accumulated_logging_time=8.29014, accumulated_submission_time=36261.2, global_step=94205, preemption_count=0, score=36261.2, test/accuracy=0.5755, test/loss=2.09742, test/num_examples=10000, total_duration=39025.7, train/accuracy=0.789401, train/loss=1.05973, validation/accuracy=0.70234, validation/loss=1.4387, validation/num_examples=50000
I0307 12:11:32.196386 140037970700032 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.369657039642334, loss=3.0189406871795654
I0307 12:12:10.458394 140037979092736 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.2615418434143066, loss=3.010448455810547
I0307 12:12:48.203723 140037970700032 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.5608818531036377, loss=2.974700450897217
I0307 12:13:26.460120 140037979092736 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.5017263889312744, loss=3.0385923385620117
I0307 12:14:04.800123 140037970700032 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.266913890838623, loss=3.0235605239868164
I0307 12:14:43.097687 140037979092736 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.436932325363159, loss=3.0466220378875732
I0307 12:15:21.693708 140037970700032 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.5856192111968994, loss=2.970459222793579
I0307 12:16:00.194859 140037979092736 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.4110677242279053, loss=2.97918438911438
2025-03-07 12:16:27.271199: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:16:39.447821 140037970700032 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.522540807723999, loss=3.0035743713378906
I0307 12:17:16.648591 140037979092736 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.3846476078033447, loss=2.9520416259765625
I0307 12:17:55.168426 140037970700032 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.3001456260681152, loss=3.0409183502197266
I0307 12:18:32.624927 140037979092736 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.5696816444396973, loss=2.996081829071045
I0307 12:19:10.443684 140037970700032 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.2836954593658447, loss=3.0400657653808594
I0307 12:19:26.031466 140193455334592 spec.py:321] Evaluating on the training split.
I0307 12:19:37.037239 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 12:19:57.316859 140193455334592 spec.py:349] Evaluating on the test split.
I0307 12:19:59.110362 140193455334592 submission_runner.py:469] Time since start: 39568.87s, 	Step: 95541, 	{'train/accuracy': 0.7971141338348389, 'train/loss': 0.9971782565116882, 'validation/accuracy': 0.7035399675369263, 'validation/loss': 1.3968464136123657, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 2.0447511672973633, 'test/num_examples': 10000, 'score': 36771.036645650864, 'total_duration': 39568.865891218185, 'accumulated_submission_time': 36771.036645650864, 'accumulated_eval_time': 2779.4038712978363, 'accumulated_logging_time': 8.390016555786133}
I0307 12:19:59.208431 140037979092736 logging_writer.py:48] [95541] accumulated_eval_time=2779.4, accumulated_logging_time=8.39002, accumulated_submission_time=36771, global_step=95541, preemption_count=0, score=36771, test/accuracy=0.5809, test/loss=2.04475, test/num_examples=10000, total_duration=39568.9, train/accuracy=0.797114, train/loss=0.997178, validation/accuracy=0.70354, validation/loss=1.39685, validation/num_examples=50000
I0307 12:20:22.359248 140037970700032 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.2880353927612305, loss=2.9261059761047363
I0307 12:21:00.852453 140037979092736 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.479396343231201, loss=2.9877750873565674
I0307 12:21:39.154752 140037970700032 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.576143503189087, loss=3.0556836128234863
I0307 12:22:17.493291 140037979092736 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.47092342376709, loss=3.0402660369873047
I0307 12:22:55.694457 140037970700032 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.296873092651367, loss=2.9597487449645996
I0307 12:23:33.892814 140037979092736 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.2282042503356934, loss=2.9890456199645996
I0307 12:24:12.504250 140037970700032 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.861722707748413, loss=3.0022642612457275
I0307 12:24:50.928784 140037979092736 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.315608263015747, loss=2.9529714584350586
2025-03-07 12:24:58.079362: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:25:28.745355 140037970700032 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.3935670852661133, loss=2.954273223876953
I0307 12:26:05.938421 140037979092736 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.4323835372924805, loss=2.993727922439575
I0307 12:26:43.153134 140037970700032 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.291898488998413, loss=2.97404408454895
I0307 12:27:20.003937 140037979092736 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.5791964530944824, loss=3.0591225624084473
I0307 12:27:57.270600 140037970700032 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.5112195014953613, loss=2.9862759113311768
I0307 12:28:29.367036 140193455334592 spec.py:321] Evaluating on the training split.
I0307 12:28:41.719965 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 12:29:04.591171 140193455334592 spec.py:349] Evaluating on the test split.
I0307 12:29:06.351518 140193455334592 submission_runner.py:469] Time since start: 40116.11s, 	Step: 96887, 	{'train/accuracy': 0.7918127775192261, 'train/loss': 1.0121196508407593, 'validation/accuracy': 0.6983000040054321, 'validation/loss': 1.4113277196884155, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.1036698818206787, 'test/num_examples': 10000, 'score': 37281.02208542824, 'total_duration': 40116.107038497925, 'accumulated_submission_time': 37281.02208542824, 'accumulated_eval_time': 2816.388203859329, 'accumulated_logging_time': 8.51332712173462}
I0307 12:29:06.463035 140037979092736 logging_writer.py:48] [96887] accumulated_eval_time=2816.39, accumulated_logging_time=8.51333, accumulated_submission_time=37281, global_step=96887, preemption_count=0, score=37281, test/accuracy=0.5618, test/loss=2.10367, test/num_examples=10000, total_duration=40116.1, train/accuracy=0.791813, train/loss=1.01212, validation/accuracy=0.6983, validation/loss=1.41133, validation/num_examples=50000
I0307 12:29:11.829776 140037970700032 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.294128656387329, loss=2.9275856018066406
I0307 12:29:50.187910 140037979092736 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.409288167953491, loss=2.979490280151367
I0307 12:30:28.685775 140037970700032 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.3164873123168945, loss=2.9841103553771973
I0307 12:31:07.358765 140037979092736 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.408834218978882, loss=2.9877634048461914
I0307 12:31:45.844789 140037970700032 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.346773386001587, loss=2.86038875579834
I0307 12:32:24.677502 140037979092736 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.397953510284424, loss=2.8949577808380127
I0307 12:33:02.821494 140037970700032 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.5036227703094482, loss=2.9195289611816406
2025-03-07 12:33:30.110028: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:33:41.572572 140037979092736 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.2999656200408936, loss=2.946758985519409
I0307 12:34:19.211409 140037970700032 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.4563255310058594, loss=2.9896419048309326
I0307 12:34:56.377064 140037979092736 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.615806818008423, loss=3.002372980117798
I0307 12:35:33.665344 140037970700032 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.4796321392059326, loss=2.9566307067871094
I0307 12:36:12.124611 140037979092736 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.463318347930908, loss=2.9941139221191406
I0307 12:36:50.183240 140037970700032 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.310025215148926, loss=2.9425201416015625
I0307 12:37:28.075763 140037979092736 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.4623348712921143, loss=2.9761805534362793
I0307 12:37:36.409210 140193455334592 spec.py:321] Evaluating on the training split.
I0307 12:37:47.529664 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 12:38:08.417756 140193455334592 spec.py:349] Evaluating on the test split.
I0307 12:38:10.205890 140193455334592 submission_runner.py:469] Time since start: 40659.96s, 	Step: 98223, 	{'train/accuracy': 0.7974330186843872, 'train/loss': 0.9960737228393555, 'validation/accuracy': 0.7046799659729004, 'validation/loss': 1.3997188806533813, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 2.0435357093811035, 'test/num_examples': 10000, 'score': 37790.78164982796, 'total_duration': 40659.9614212513, 'accumulated_submission_time': 37790.78164982796, 'accumulated_eval_time': 2850.1847467422485, 'accumulated_logging_time': 8.664451837539673}
I0307 12:38:10.339541 140037970700032 logging_writer.py:48] [98223] accumulated_eval_time=2850.18, accumulated_logging_time=8.66445, accumulated_submission_time=37790.8, global_step=98223, preemption_count=0, score=37790.8, test/accuracy=0.5723, test/loss=2.04354, test/num_examples=10000, total_duration=40660, train/accuracy=0.797433, train/loss=0.996074, validation/accuracy=0.70468, validation/loss=1.39972, validation/num_examples=50000
I0307 12:38:39.643154 140037979092736 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.521738290786743, loss=3.008485794067383
I0307 12:39:17.458582 140037970700032 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.5344693660736084, loss=2.952322244644165
I0307 12:39:55.548502 140037979092736 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.5129966735839844, loss=2.954132556915283
I0307 12:40:33.580364 140037970700032 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.3491902351379395, loss=2.931602954864502
I0307 12:41:11.838423 140037979092736 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.390713930130005, loss=2.9262118339538574
I0307 12:41:49.619902 140037970700032 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.592597007751465, loss=2.9315757751464844
2025-03-07 12:41:54.688084: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:42:27.525715 140037979092736 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.4916832447052, loss=2.931286573410034
I0307 12:43:05.496171 140037970700032 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.3978490829467773, loss=2.9849109649658203
I0307 12:43:43.827380 140037979092736 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.33054518699646, loss=2.903364419937134
I0307 12:44:21.839018 140037970700032 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.4531729221343994, loss=2.9159913063049316
I0307 12:44:59.877560 140037979092736 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.409403085708618, loss=2.9184138774871826
I0307 12:45:38.356302 140037970700032 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.5840630531311035, loss=2.9358651638031006
I0307 12:46:16.948451 140037979092736 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.437265634536743, loss=3.004528522491455
I0307 12:46:40.558019 140193455334592 spec.py:321] Evaluating on the training split.
I0307 12:46:51.246511 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 12:47:21.107163 140193455334592 spec.py:349] Evaluating on the test split.
I0307 12:47:22.839612 140193455334592 submission_runner.py:469] Time since start: 41212.60s, 	Step: 99563, 	{'train/accuracy': 0.7955197691917419, 'train/loss': 1.0354253053665161, 'validation/accuracy': 0.7041599750518799, 'validation/loss': 1.4205228090286255, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.071550130844116, 'test/num_examples': 10000, 'score': 38300.81654429436, 'total_duration': 41212.595126867294, 'accumulated_submission_time': 38300.81654429436, 'accumulated_eval_time': 2892.4662005901337, 'accumulated_logging_time': 8.837025165557861}
I0307 12:47:23.013289 140037970700032 logging_writer.py:48] [99563] accumulated_eval_time=2892.47, accumulated_logging_time=8.83703, accumulated_submission_time=38300.8, global_step=99563, preemption_count=0, score=38300.8, test/accuracy=0.5777, test/loss=2.07155, test/num_examples=10000, total_duration=41212.6, train/accuracy=0.79552, train/loss=1.03543, validation/accuracy=0.70416, validation/loss=1.42052, validation/num_examples=50000
I0307 12:47:37.467565 140037979092736 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.4929606914520264, loss=2.986562728881836
I0307 12:48:15.350584 140037970700032 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.5765342712402344, loss=2.994441509246826
I0307 12:48:53.300968 140037979092736 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.4668283462524414, loss=2.8984131813049316
I0307 12:49:31.627062 140037970700032 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.53479266166687, loss=2.9636759757995605
I0307 12:50:09.724307 140037979092736 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.47853422164917, loss=3.038879632949829
2025-03-07 12:50:35.976896: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:50:47.987682 140037970700032 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.505584478378296, loss=2.949028968811035
I0307 12:51:26.260490 140037979092736 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.3453750610351562, loss=2.886159896850586
I0307 12:52:04.656350 140037970700032 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.6093909740448, loss=3.006488084793091
I0307 12:52:42.783668 140037979092736 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.5966272354125977, loss=3.0445268154144287
I0307 12:53:21.245564 140037970700032 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.5067436695098877, loss=2.9814350605010986
I0307 12:53:59.511685 140037979092736 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.4388997554779053, loss=2.9096312522888184
I0307 12:54:37.734158 140037970700032 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.3792808055877686, loss=2.9446280002593994
I0307 12:55:15.831886 140037979092736 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.548447370529175, loss=2.9747231006622314
I0307 12:55:52.933407 140193455334592 spec.py:321] Evaluating on the training split.
I0307 12:56:03.877853 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 12:56:31.030548 140193455334592 spec.py:349] Evaluating on the test split.
I0307 12:56:32.797214 140193455334592 submission_runner.py:469] Time since start: 41762.55s, 	Step: 100898, 	{'train/accuracy': 0.7993462681770325, 'train/loss': 0.9896190166473389, 'validation/accuracy': 0.711139976978302, 'validation/loss': 1.3836885690689087, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 2.0103890895843506, 'test/num_examples': 10000, 'score': 38810.56899642944, 'total_duration': 41762.55271148682, 'accumulated_submission_time': 38810.56899642944, 'accumulated_eval_time': 2932.329834461212, 'accumulated_logging_time': 9.033179759979248}
I0307 12:56:32.875666 140037970700032 logging_writer.py:48] [100898] accumulated_eval_time=2932.33, accumulated_logging_time=9.03318, accumulated_submission_time=38810.6, global_step=100898, preemption_count=0, score=38810.6, test/accuracy=0.585, test/loss=2.01039, test/num_examples=10000, total_duration=41762.6, train/accuracy=0.799346, train/loss=0.989619, validation/accuracy=0.71114, validation/loss=1.38369, validation/num_examples=50000
I0307 12:56:34.053489 140037979092736 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.353421926498413, loss=2.906336784362793
I0307 12:57:12.069713 140037970700032 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.344881057739258, loss=2.956261396408081
I0307 12:57:50.794517 140037979092736 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.556464195251465, loss=3.0108752250671387
I0307 12:58:29.095762 140037970700032 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.6176042556762695, loss=2.9745123386383057
I0307 12:59:07.408959 140037979092736 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.6068549156188965, loss=2.9889135360717773
2025-03-07 12:59:16.489113: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:59:45.278906 140037970700032 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.5021395683288574, loss=2.932445526123047
I0307 13:00:22.012588 140037979092736 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.4019975662231445, loss=2.9511873722076416
I0307 13:00:59.560643 140037970700032 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.318885564804077, loss=2.8831610679626465
I0307 13:01:37.734150 140037979092736 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.5163967609405518, loss=2.9326443672180176
I0307 13:02:15.129443 140037970700032 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.5099029541015625, loss=2.919560670852661
I0307 13:02:52.748176 140037979092736 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.6186089515686035, loss=2.9439871311187744
I0307 13:03:30.262656 140037970700032 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.537999153137207, loss=2.936156749725342
I0307 13:04:08.528798 140037979092736 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.47485613822937, loss=2.9348721504211426
I0307 13:04:46.588282 140037970700032 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.4051601886749268, loss=2.8791251182556152
I0307 13:05:02.890153 140193455334592 spec.py:321] Evaluating on the training split.
I0307 13:05:14.101946 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 13:05:33.611190 140193455334592 spec.py:349] Evaluating on the test split.
I0307 13:05:35.398125 140193455334592 submission_runner.py:469] Time since start: 42305.15s, 	Step: 102244, 	{'train/accuracy': 0.7999840378761292, 'train/loss': 0.9937681555747986, 'validation/accuracy': 0.711679995059967, 'validation/loss': 1.3843388557434082, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 2.0381040573120117, 'test/num_examples': 10000, 'score': 39320.40218234062, 'total_duration': 42305.153641700745, 'accumulated_submission_time': 39320.40218234062, 'accumulated_eval_time': 2964.837651014328, 'accumulated_logging_time': 9.146535634994507}
I0307 13:05:35.529078 140037979092736 logging_writer.py:48] [102244] accumulated_eval_time=2964.84, accumulated_logging_time=9.14654, accumulated_submission_time=39320.4, global_step=102244, preemption_count=0, score=39320.4, test/accuracy=0.5815, test/loss=2.0381, test/num_examples=10000, total_duration=42305.2, train/accuracy=0.799984, train/loss=0.993768, validation/accuracy=0.71168, validation/loss=1.38434, validation/num_examples=50000
I0307 13:05:57.492411 140037970700032 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.3839406967163086, loss=2.8230223655700684
I0307 13:06:35.755343 140037979092736 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.7485222816467285, loss=3.0171332359313965
I0307 13:07:13.818984 140037970700032 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.3461263179779053, loss=2.9240236282348633
2025-03-07 13:07:43.620375: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:07:53.415732 140037979092736 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.611239194869995, loss=2.9432358741760254
I0307 13:08:31.559737 140037970700032 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.8352177143096924, loss=2.9551806449890137
I0307 13:09:10.217466 140037979092736 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.619208335876465, loss=2.967834234237671
I0307 13:09:48.726837 140037970700032 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.7079696655273438, loss=2.982835531234741
I0307 13:10:27.918116 140037979092736 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.825665235519409, loss=3.0172886848449707
I0307 13:11:07.638354 140037970700032 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.5237350463867188, loss=2.988280773162842
I0307 13:11:46.277332 140037979092736 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.589233160018921, loss=2.9277539253234863
I0307 13:12:23.343008 140037970700032 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.460585355758667, loss=2.889159917831421
I0307 13:13:01.664465 140037979092736 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.535553216934204, loss=2.941260576248169
I0307 13:13:39.198087 140037970700032 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.601850748062134, loss=2.9992876052856445
I0307 13:14:05.607429 140193455334592 spec.py:321] Evaluating on the training split.
I0307 13:14:18.060600 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 13:14:42.720853 140193455334592 spec.py:349] Evaluating on the test split.
I0307 13:14:44.447870 140193455334592 submission_runner.py:469] Time since start: 42854.20s, 	Step: 103571, 	{'train/accuracy': 0.7977519035339355, 'train/loss': 0.9897588491439819, 'validation/accuracy': 0.7047199606895447, 'validation/loss': 1.3940502405166626, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 2.035470485687256, 'test/num_examples': 10000, 'score': 39830.30586838722, 'total_duration': 42854.203349113464, 'accumulated_submission_time': 39830.30586838722, 'accumulated_eval_time': 3003.6779000759125, 'accumulated_logging_time': 9.305868864059448}
I0307 13:14:44.546060 140037979092736 logging_writer.py:48] [103571] accumulated_eval_time=3003.68, accumulated_logging_time=9.30587, accumulated_submission_time=39830.3, global_step=103571, preemption_count=0, score=39830.3, test/accuracy=0.5819, test/loss=2.03547, test/num_examples=10000, total_duration=42854.2, train/accuracy=0.797752, train/loss=0.989759, validation/accuracy=0.70472, validation/loss=1.39405, validation/num_examples=50000
I0307 13:14:56.124504 140037970700032 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.6153337955474854, loss=2.914109468460083
I0307 13:15:34.553210 140037979092736 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.6974480152130127, loss=2.9665260314941406
I0307 13:16:12.768590 140037970700032 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.5708298683166504, loss=2.915217399597168
2025-03-07 13:16:23.854174: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:16:52.816908 140037979092736 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.431994915008545, loss=2.875696897506714
I0307 13:17:28.774340 140037970700032 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.549518585205078, loss=2.9408624172210693
I0307 13:18:06.569364 140037979092736 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.701005458831787, loss=2.912198066711426
I0307 13:18:46.153874 140037970700032 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.733637571334839, loss=2.9797911643981934
I0307 13:19:23.071468 140037979092736 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.48380184173584, loss=2.8929336071014404
I0307 13:20:00.369190 140037970700032 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.8663740158081055, loss=2.97506046295166
I0307 13:20:38.854375 140037979092736 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.4996497631073, loss=2.932119369506836
I0307 13:21:16.792098 140037970700032 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.4052560329437256, loss=2.878389596939087
I0307 13:21:54.838375 140037979092736 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.645796537399292, loss=2.9926223754882812
I0307 13:22:32.918449 140037970700032 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.607914686203003, loss=2.9209113121032715
I0307 13:23:11.413666 140037979092736 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.5560481548309326, loss=2.9416844844818115
I0307 13:23:14.478780 140193455334592 spec.py:321] Evaluating on the training split.
I0307 13:23:26.001681 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 13:23:44.200039 140193455334592 spec.py:349] Evaluating on the test split.
I0307 13:23:46.002041 140193455334592 submission_runner.py:469] Time since start: 43395.76s, 	Step: 104909, 	{'train/accuracy': 0.7970144748687744, 'train/loss': 0.9729068875312805, 'validation/accuracy': 0.7075600028038025, 'validation/loss': 1.3721404075622559, 'validation/num_examples': 50000, 'test/accuracy': 0.585800051689148, 'test/loss': 2.0031237602233887, 'test/num_examples': 10000, 'score': 40340.051884412766, 'total_duration': 43395.75755739212, 'accumulated_submission_time': 40340.051884412766, 'accumulated_eval_time': 3035.2010192871094, 'accumulated_logging_time': 9.445676565170288}
I0307 13:23:46.190914 140037970700032 logging_writer.py:48] [104909] accumulated_eval_time=3035.2, accumulated_logging_time=9.44568, accumulated_submission_time=40340.1, global_step=104909, preemption_count=0, score=40340.1, test/accuracy=0.5858, test/loss=2.00312, test/num_examples=10000, total_duration=43395.8, train/accuracy=0.797014, train/loss=0.972907, validation/accuracy=0.70756, validation/loss=1.37214, validation/num_examples=50000
I0307 13:24:21.021293 140037979092736 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.3694443702697754, loss=2.9722023010253906
2025-03-07 13:24:51.410275: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:25:00.017227 140037970700032 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.578157663345337, loss=2.952803611755371
I0307 13:25:38.439210 140037979092736 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.58076810836792, loss=2.923161506652832
I0307 13:26:16.213605 140037970700032 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.8156275749206543, loss=2.978022813796997
I0307 13:26:53.484588 140037979092736 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.528625011444092, loss=2.9474387168884277
I0307 13:27:30.313479 140037970700032 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.570518970489502, loss=2.9684274196624756
I0307 13:28:08.541135 140037979092736 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.5429162979125977, loss=2.9698822498321533
I0307 13:28:46.602580 140037970700032 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.6260945796966553, loss=2.9380269050598145
I0307 13:29:23.975998 140037979092736 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.576519012451172, loss=2.936474084854126
I0307 13:30:02.265608 140037970700032 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.591179609298706, loss=2.9201555252075195
I0307 13:30:40.176946 140037979092736 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.58984112739563, loss=2.8726794719696045
I0307 13:31:18.103936 140037970700032 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.6156933307647705, loss=2.935739040374756
I0307 13:31:56.274229 140037979092736 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.519394636154175, loss=2.9240241050720215
I0307 13:32:16.140049 140193455334592 spec.py:321] Evaluating on the training split.
I0307 13:32:28.335654 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 13:32:49.506361 140193455334592 spec.py:349] Evaluating on the test split.
I0307 13:32:51.273666 140193455334592 submission_runner.py:469] Time since start: 43941.03s, 	Step: 106254, 	{'train/accuracy': 0.7971341013908386, 'train/loss': 1.0117433071136475, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.4035918712615967, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 2.0467796325683594, 'test/num_examples': 10000, 'score': 40849.81273841858, 'total_duration': 43941.0291826725, 'accumulated_submission_time': 40849.81273841858, 'accumulated_eval_time': 3070.334483385086, 'accumulated_logging_time': 9.674582242965698}
I0307 13:32:51.330043 140037970700032 logging_writer.py:48] [106254] accumulated_eval_time=3070.33, accumulated_logging_time=9.67458, accumulated_submission_time=40849.8, global_step=106254, preemption_count=0, score=40849.8, test/accuracy=0.58, test/loss=2.04678, test/num_examples=10000, total_duration=43941, train/accuracy=0.797134, train/loss=1.01174, validation/accuracy=0.70842, validation/loss=1.40359, validation/num_examples=50000
I0307 13:33:09.091329 140037979092736 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.4348931312561035, loss=2.8832919597625732
2025-03-07 13:33:17.553686: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:33:47.205197 140037970700032 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.6028363704681396, loss=2.913778305053711
I0307 13:34:25.442799 140037979092736 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.885159730911255, loss=2.928903341293335
I0307 13:35:03.574668 140037970700032 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.5153448581695557, loss=2.854877233505249
I0307 13:35:41.748462 140037979092736 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.5017752647399902, loss=2.9124691486358643
I0307 13:36:19.763364 140037970700032 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.627131223678589, loss=2.924880266189575
I0307 13:36:57.754257 140037979092736 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.449669122695923, loss=2.9099466800689697
I0307 13:37:35.897777 140037970700032 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.531785249710083, loss=2.9638843536376953
I0307 13:38:13.976544 140037979092736 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.620614767074585, loss=2.9438812732696533
I0307 13:38:52.270849 140037970700032 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.726113796234131, loss=2.9714529514312744
I0307 13:39:30.637654 140037979092736 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.5895159244537354, loss=2.901962995529175
I0307 13:40:09.002887 140037970700032 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.4470036029815674, loss=2.8100907802581787
I0307 13:40:47.085061 140037979092736 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.450073719024658, loss=2.925825357437134
2025-03-07 13:41:17.206855: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:41:21.339303 140193455334592 spec.py:321] Evaluating on the training split.
I0307 13:41:32.832851 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 13:41:52.622131 140193455334592 spec.py:349] Evaluating on the test split.
I0307 13:41:54.416225 140193455334592 submission_runner.py:469] Time since start: 44484.17s, 	Step: 107589, 	{'train/accuracy': 0.8005022406578064, 'train/loss': 0.9862359166145325, 'validation/accuracy': 0.7067199945449829, 'validation/loss': 1.388258457183838, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 2.0133423805236816, 'test/num_examples': 10000, 'score': 41359.65326118469, 'total_duration': 44484.171738386154, 'accumulated_submission_time': 41359.65326118469, 'accumulated_eval_time': 3103.411251306534, 'accumulated_logging_time': 9.756128787994385}
I0307 13:41:54.494848 140037970700032 logging_writer.py:48] [107589] accumulated_eval_time=3103.41, accumulated_logging_time=9.75613, accumulated_submission_time=41359.7, global_step=107589, preemption_count=0, score=41359.7, test/accuracy=0.5842, test/loss=2.01334, test/num_examples=10000, total_duration=44484.2, train/accuracy=0.800502, train/loss=0.986236, validation/accuracy=0.70672, validation/loss=1.38826, validation/num_examples=50000
I0307 13:41:59.076810 140037979092736 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.6497907638549805, loss=2.9145143032073975
I0307 13:42:36.814743 140037970700032 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.653625249862671, loss=2.894010543823242
I0307 13:43:14.651608 140037979092736 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.6016387939453125, loss=2.936767816543579
I0307 13:43:52.526498 140037970700032 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.7843244075775146, loss=2.9321954250335693
I0307 13:44:30.896582 140037979092736 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.418898105621338, loss=2.8744869232177734
I0307 13:45:08.890954 140037970700032 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.4897499084472656, loss=2.900878429412842
I0307 13:45:47.353672 140037979092736 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.4280738830566406, loss=2.8859119415283203
I0307 13:46:25.630963 140037970700032 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.5742876529693604, loss=2.856627941131592
I0307 13:47:03.966468 140037979092736 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.6761951446533203, loss=2.8442599773406982
I0307 13:47:41.995902 140037970700032 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.52040958404541, loss=2.8775792121887207
I0307 13:48:20.221216 140037979092736 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.7484817504882812, loss=2.9440088272094727
I0307 13:48:58.652345 140037970700032 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.588334798812866, loss=2.946681261062622
I0307 13:49:36.958838 140037979092736 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.609213352203369, loss=2.820862054824829
2025-03-07 13:49:48.910511: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:50:14.855526 140037970700032 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.6652886867523193, loss=2.8998312950134277
I0307 13:50:24.714775 140193455334592 spec.py:321] Evaluating on the training split.
I0307 13:50:36.736465 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 13:50:58.007982 140193455334592 spec.py:349] Evaluating on the test split.
I0307 13:50:59.790383 140193455334592 submission_runner.py:469] Time since start: 45029.55s, 	Step: 108926, 	{'train/accuracy': 0.8112643361091614, 'train/loss': 0.9345428347587585, 'validation/accuracy': 0.716159999370575, 'validation/loss': 1.3388030529022217, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.9794174432754517, 'test/num_examples': 10000, 'score': 41869.69741463661, 'total_duration': 45029.54582500458, 'accumulated_submission_time': 41869.69741463661, 'accumulated_eval_time': 3138.486629486084, 'accumulated_logging_time': 9.864871501922607}
I0307 13:50:59.864794 140037979092736 logging_writer.py:48] [108926] accumulated_eval_time=3138.49, accumulated_logging_time=9.86487, accumulated_submission_time=41869.7, global_step=108926, preemption_count=0, score=41869.7, test/accuracy=0.5941, test/loss=1.97942, test/num_examples=10000, total_duration=45029.5, train/accuracy=0.811264, train/loss=0.934543, validation/accuracy=0.71616, validation/loss=1.3388, validation/num_examples=50000
I0307 13:51:28.746365 140037970700032 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.586819648742676, loss=2.9371941089630127
I0307 13:52:07.275174 140037979092736 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.704246759414673, loss=2.9802486896514893
I0307 13:52:45.594621 140037970700032 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.5687413215637207, loss=2.901785373687744
I0307 13:53:23.712325 140037979092736 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.760903835296631, loss=2.944883346557617
I0307 13:54:02.118420 140037970700032 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.723921298980713, loss=2.8908257484436035
I0307 13:54:40.653066 140037979092736 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.6300268173217773, loss=2.92140793800354
I0307 13:55:18.793480 140037970700032 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.6202526092529297, loss=2.8694612979888916
I0307 13:55:57.366435 140037979092736 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.629700183868408, loss=2.879549026489258
I0307 13:56:36.033696 140037970700032 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.8391664028167725, loss=2.9645838737487793
I0307 13:57:14.494746 140037979092736 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.6290054321289062, loss=2.8619513511657715
I0307 13:57:52.677225 140037970700032 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.7424778938293457, loss=2.8861289024353027
2025-03-07 13:58:26.575802: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:58:33.356258 140037979092736 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.6479578018188477, loss=2.8965320587158203
I0307 13:59:10.575079 140037970700032 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.4537057876586914, loss=2.8667619228363037
I0307 13:59:30.035880 140193455334592 spec.py:321] Evaluating on the training split.
I0307 13:59:42.368288 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 14:00:04.364364 140193455334592 spec.py:349] Evaluating on the test split.
I0307 14:00:06.362513 140193455334592 submission_runner.py:469] Time since start: 45576.12s, 	Step: 110253, 	{'train/accuracy': 0.8097496628761292, 'train/loss': 0.936035692691803, 'validation/accuracy': 0.712399959564209, 'validation/loss': 1.3608921766281128, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.007680892944336, 'test/num_examples': 10000, 'score': 42379.68012213707, 'total_duration': 45576.118019104004, 'accumulated_submission_time': 42379.68012213707, 'accumulated_eval_time': 3174.813100337982, 'accumulated_logging_time': 9.982455730438232}
I0307 14:00:06.450900 140037979092736 logging_writer.py:48] [110253] accumulated_eval_time=3174.81, accumulated_logging_time=9.98246, accumulated_submission_time=42379.7, global_step=110253, preemption_count=0, score=42379.7, test/accuracy=0.5835, test/loss=2.00768, test/num_examples=10000, total_duration=45576.1, train/accuracy=0.80975, train/loss=0.936036, validation/accuracy=0.7124, validation/loss=1.36089, validation/num_examples=50000
I0307 14:00:24.655722 140037970700032 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.8856546878814697, loss=2.938176155090332
I0307 14:01:03.130088 140037979092736 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.677079916000366, loss=2.8667922019958496
I0307 14:01:41.465211 140037970700032 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.554699182510376, loss=2.8572299480438232
I0307 14:02:20.115155 140037979092736 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.6444098949432373, loss=2.8473823070526123
I0307 14:02:58.593329 140037970700032 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.8887579441070557, loss=2.9386403560638428
I0307 14:03:37.002348 140037979092736 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.428584575653076, loss=2.8844122886657715
I0307 14:04:15.477659 140037970700032 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.6415183544158936, loss=2.8823649883270264
I0307 14:04:53.613182 140037979092736 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.762787342071533, loss=2.973041296005249
I0307 14:05:32.059318 140037970700032 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.4969642162323, loss=2.8999831676483154
I0307 14:06:10.401535 140037979092736 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.664297580718994, loss=2.905641794204712
I0307 14:06:48.984577 140037970700032 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.5705854892730713, loss=2.8870997428894043
2025-03-07 14:07:01.244362: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:07:26.307770 140037979092736 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.580050468444824, loss=2.920973539352417
I0307 14:08:03.239565 140037970700032 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.7044661045074463, loss=2.894622564315796
I0307 14:08:36.619011 140193455334592 spec.py:321] Evaluating on the training split.
I0307 14:08:48.105500 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 14:09:11.692138 140193455334592 spec.py:349] Evaluating on the test split.
I0307 14:09:13.439497 140193455334592 submission_runner.py:469] Time since start: 46123.19s, 	Step: 111590, 	{'train/accuracy': 0.8106265664100647, 'train/loss': 0.9283785820007324, 'validation/accuracy': 0.7137799859046936, 'validation/loss': 1.3441743850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 1.9702633619308472, 'test/num_examples': 10000, 'score': 42889.66196060181, 'total_duration': 46123.19499254227, 'accumulated_submission_time': 42889.66196060181, 'accumulated_eval_time': 3211.6334109306335, 'accumulated_logging_time': 10.112252235412598}
I0307 14:09:13.506448 140037979092736 logging_writer.py:48] [111590] accumulated_eval_time=3211.63, accumulated_logging_time=10.1123, accumulated_submission_time=42889.7, global_step=111590, preemption_count=0, score=42889.7, test/accuracy=0.5892, test/loss=1.97026, test/num_examples=10000, total_duration=46123.2, train/accuracy=0.810627, train/loss=0.928379, validation/accuracy=0.71378, validation/loss=1.34417, validation/num_examples=50000
I0307 14:09:17.744168 140037970700032 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.845728874206543, loss=2.9039130210876465
I0307 14:09:56.083738 140037979092736 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.746884346008301, loss=2.8904659748077393
I0307 14:10:34.666296 140037970700032 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.7097909450531006, loss=2.882355213165283
I0307 14:11:13.110151 140037979092736 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.7501509189605713, loss=3.015805244445801
I0307 14:11:51.282132 140037970700032 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.6691601276397705, loss=2.86332368850708
I0307 14:12:29.923202 140037979092736 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.8042030334472656, loss=2.8641345500946045
I0307 14:13:08.152704 140037970700032 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.6861014366149902, loss=2.8952085971832275
I0307 14:13:46.830644 140037979092736 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.982426881790161, loss=2.9254610538482666
I0307 14:14:24.956740 140037970700032 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.6234941482543945, loss=2.8303122520446777
I0307 14:15:03.240516 140037979092736 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.823396682739258, loss=2.890101432800293
2025-03-07 14:15:37.071017: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:15:43.381900 140037970700032 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.700127601623535, loss=2.8969194889068604
I0307 14:16:20.502307 140037979092736 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.918506383895874, loss=2.938847303390503
I0307 14:16:58.442137 140037970700032 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.826904535293579, loss=2.948859214782715
I0307 14:17:36.349062 140037979092736 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.885148048400879, loss=2.8480262756347656
I0307 14:17:43.654238 140193455334592 spec.py:321] Evaluating on the training split.
I0307 14:17:55.045310 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 14:18:20.905320 140193455334592 spec.py:349] Evaluating on the test split.
I0307 14:18:22.654130 140193455334592 submission_runner.py:469] Time since start: 46672.41s, 	Step: 112920, 	{'train/accuracy': 0.8062818646430969, 'train/loss': 0.9538609981536865, 'validation/accuracy': 0.711899995803833, 'validation/loss': 1.3690943717956543, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 2.0119946002960205, 'test/num_examples': 10000, 'score': 43399.64029479027, 'total_duration': 46672.40962266922, 'accumulated_submission_time': 43399.64029479027, 'accumulated_eval_time': 3250.6331191062927, 'accumulated_logging_time': 10.205451250076294}
I0307 14:18:22.747632 140037970700032 logging_writer.py:48] [112920] accumulated_eval_time=3250.63, accumulated_logging_time=10.2055, accumulated_submission_time=43399.6, global_step=112920, preemption_count=0, score=43399.6, test/accuracy=0.5871, test/loss=2.01199, test/num_examples=10000, total_duration=46672.4, train/accuracy=0.806282, train/loss=0.953861, validation/accuracy=0.7119, validation/loss=1.36909, validation/num_examples=50000
I0307 14:18:53.712320 140037979092736 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.725999593734741, loss=2.8936667442321777
I0307 14:19:32.063632 140037970700032 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.674006223678589, loss=2.8990118503570557
I0307 14:20:10.423969 140037979092736 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.668768882751465, loss=2.982574939727783
I0307 14:20:48.953488 140037970700032 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.8506052494049072, loss=2.926649332046509
I0307 14:21:27.531113 140037979092736 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.759892225265503, loss=2.8878660202026367
I0307 14:22:06.209088 140037970700032 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.9332845211029053, loss=2.911769151687622
I0307 14:22:44.486182 140037979092736 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.602565288543701, loss=2.8383755683898926
I0307 14:23:22.994571 140037970700032 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.718134880065918, loss=2.875682830810547
I0307 14:24:01.539842 140037979092736 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.653775691986084, loss=2.8924529552459717
2025-03-07 14:24:15.600240: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:24:39.389557 140037970700032 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.618605136871338, loss=2.8560492992401123
I0307 14:25:16.501813 140037979092736 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.8083443641662598, loss=2.8845903873443604
I0307 14:25:53.492904 140037970700032 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.6749186515808105, loss=2.8498709201812744
I0307 14:26:31.798093 140037979092736 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.6807236671447754, loss=2.8664231300354004
I0307 14:26:52.684263 140193455334592 spec.py:321] Evaluating on the training split.
I0307 14:27:04.155918 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 14:27:29.451795 140193455334592 spec.py:349] Evaluating on the test split.
I0307 14:27:31.174865 140193455334592 submission_runner.py:469] Time since start: 47220.93s, 	Step: 114256, 	{'train/accuracy': 0.8217474222183228, 'train/loss': 0.9386281371116638, 'validation/accuracy': 0.7227199673652649, 'validation/loss': 1.362930417060852, 'validation/num_examples': 50000, 'test/accuracy': 0.598300039768219, 'test/loss': 1.985208511352539, 'test/num_examples': 10000, 'score': 43909.39679098129, 'total_duration': 47220.93038392067, 'accumulated_submission_time': 43909.39679098129, 'accumulated_eval_time': 3289.1235721111298, 'accumulated_logging_time': 10.3337881565094}
I0307 14:27:31.297239 140037970700032 logging_writer.py:48] [114256] accumulated_eval_time=3289.12, accumulated_logging_time=10.3338, accumulated_submission_time=43909.4, global_step=114256, preemption_count=0, score=43909.4, test/accuracy=0.5983, test/loss=1.98521, test/num_examples=10000, total_duration=47220.9, train/accuracy=0.821747, train/loss=0.938628, validation/accuracy=0.72272, validation/loss=1.36293, validation/num_examples=50000
I0307 14:27:48.377880 140037979092736 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.6688930988311768, loss=2.8254988193511963
I0307 14:28:26.491918 140037970700032 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.924455404281616, loss=2.9112370014190674
I0307 14:29:04.871694 140037979092736 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.6965172290802, loss=2.831988573074341
I0307 14:29:42.939342 140037970700032 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.776542901992798, loss=2.9700424671173096
I0307 14:30:21.269115 140037979092736 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.766381025314331, loss=2.9513556957244873
I0307 14:30:59.369819 140037970700032 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.6759626865386963, loss=2.8965253829956055
I0307 14:31:37.667888 140037979092736 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.6233596801757812, loss=2.874030828475952
I0307 14:32:15.965644 140037970700032 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.7463855743408203, loss=2.903620719909668
2025-03-07 14:32:46.141015: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:32:54.597734 140037979092736 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.84197735786438, loss=2.9384989738464355
I0307 14:33:32.762324 140037970700032 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.89748215675354, loss=2.833181381225586
I0307 14:34:10.619968 140037979092736 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.6394455432891846, loss=2.873244047164917
I0307 14:34:48.641120 140037970700032 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.768237590789795, loss=2.931399345397949
I0307 14:35:27.238277 140037979092736 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.9272425174713135, loss=2.8957786560058594
I0307 14:36:01.196877 140193455334592 spec.py:321] Evaluating on the training split.
I0307 14:36:12.633019 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 14:36:36.081964 140193455334592 spec.py:349] Evaluating on the test split.
I0307 14:36:37.865112 140193455334592 submission_runner.py:469] Time since start: 47767.62s, 	Step: 115589, 	{'train/accuracy': 0.8206512928009033, 'train/loss': 0.8927697539329529, 'validation/accuracy': 0.7161799669265747, 'validation/loss': 1.336808204650879, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.9473992586135864, 'test/num_examples': 10000, 'score': 44419.11040997505, 'total_duration': 47767.62062740326, 'accumulated_submission_time': 44419.11040997505, 'accumulated_eval_time': 3325.7916507720947, 'accumulated_logging_time': 10.4962797164917}
I0307 14:36:37.960339 140037970700032 logging_writer.py:48] [115589] accumulated_eval_time=3325.79, accumulated_logging_time=10.4963, accumulated_submission_time=44419.1, global_step=115589, preemption_count=0, score=44419.1, test/accuracy=0.6006, test/loss=1.9474, test/num_examples=10000, total_duration=47767.6, train/accuracy=0.820651, train/loss=0.89277, validation/accuracy=0.71618, validation/loss=1.33681, validation/num_examples=50000
I0307 14:36:42.603536 140037979092736 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.7657766342163086, loss=2.9252686500549316
I0307 14:37:20.706922 140037970700032 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.720395565032959, loss=2.896514892578125
I0307 14:37:58.676064 140037979092736 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.78959059715271, loss=2.9415745735168457
I0307 14:38:37.128380 140037970700032 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.8262100219726562, loss=2.884397029876709
I0307 14:39:15.220587 140037979092736 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.74005126953125, loss=2.814873695373535
I0307 14:39:53.471569 140037970700032 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.880103349685669, loss=2.9417924880981445
I0307 14:40:32.016918 140037979092736 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.719109535217285, loss=2.8606278896331787
I0307 14:41:10.297923 140037970700032 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.893693447113037, loss=2.870903968811035
2025-03-07 14:41:23.857818: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:41:48.556004 140037979092736 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.7901318073272705, loss=2.822532892227173
I0307 14:42:26.902563 140037970700032 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.869666576385498, loss=2.8730053901672363
I0307 14:43:05.271194 140037979092736 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.8188486099243164, loss=2.898381471633911
I0307 14:43:43.410115 140037970700032 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.7670114040374756, loss=2.8581254482269287
I0307 14:44:21.488140 140037979092736 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.6927528381347656, loss=2.8372817039489746
I0307 14:45:00.148103 140037970700032 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.9166505336761475, loss=2.8788094520568848
I0307 14:45:08.197053 140193455334592 spec.py:321] Evaluating on the training split.
I0307 14:45:19.169738 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 14:45:37.827930 140193455334592 spec.py:349] Evaluating on the test split.
I0307 14:45:39.613405 140193455334592 submission_runner.py:469] Time since start: 48309.37s, 	Step: 116922, 	{'train/accuracy': 0.8350406289100647, 'train/loss': 0.8642550110816956, 'validation/accuracy': 0.7200599908828735, 'validation/loss': 1.3416831493377686, 'validation/num_examples': 50000, 'test/accuracy': 0.59170001745224, 'test/loss': 1.9726402759552002, 'test/num_examples': 10000, 'score': 44929.17004656792, 'total_duration': 48309.36885166168, 'accumulated_submission_time': 44929.17004656792, 'accumulated_eval_time': 3357.2077775001526, 'accumulated_logging_time': 10.624828577041626}
I0307 14:45:39.754732 140037979092736 logging_writer.py:48] [116922] accumulated_eval_time=3357.21, accumulated_logging_time=10.6248, accumulated_submission_time=44929.2, global_step=116922, preemption_count=0, score=44929.2, test/accuracy=0.5917, test/loss=1.97264, test/num_examples=10000, total_duration=48309.4, train/accuracy=0.835041, train/loss=0.864255, validation/accuracy=0.72006, validation/loss=1.34168, validation/num_examples=50000
I0307 14:46:10.144303 140037970700032 logging_writer.py:48] [117000] global_step=117000, grad_norm=3.0663418769836426, loss=2.9284133911132812
I0307 14:46:48.312720 140037979092736 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.7215776443481445, loss=2.8700854778289795
I0307 14:47:26.594349 140037970700032 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.608689308166504, loss=2.856830358505249
I0307 14:48:05.117435 140037979092736 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.8552443981170654, loss=2.834878921508789
I0307 14:48:43.413303 140037970700032 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.7746803760528564, loss=2.814512014389038
I0307 14:49:21.724342 140037979092736 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.8102598190307617, loss=2.8123691082000732
2025-03-07 14:49:55.427263: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:50:00.255845 140037970700032 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.802720546722412, loss=2.8656113147735596
I0307 14:50:37.360130 140037979092736 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.965296983718872, loss=2.884521484375
I0307 14:51:14.931519 140037970700032 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.8816025257110596, loss=2.9270076751708984
I0307 14:51:52.744524 140037979092736 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.741612434387207, loss=2.842916488647461
I0307 14:52:31.018295 140037970700032 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.7709498405456543, loss=2.8308708667755127
I0307 14:53:09.195379 140037979092736 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.6518146991729736, loss=2.8287739753723145
I0307 14:53:47.659702 140037970700032 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.8016538619995117, loss=2.9172372817993164
I0307 14:54:09.766961 140193455334592 spec.py:321] Evaluating on the training split.
I0307 14:54:21.054162 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 14:54:44.235894 140193455334592 spec.py:349] Evaluating on the test split.
I0307 14:54:45.981729 140193455334592 submission_runner.py:469] Time since start: 48855.74s, 	Step: 118259, 	{'train/accuracy': 0.85453200340271, 'train/loss': 0.7720656991004944, 'validation/accuracy': 0.7185999751091003, 'validation/loss': 1.3324124813079834, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9789848327636719, 'test/num_examples': 10000, 'score': 45438.995973587036, 'total_duration': 48855.73725390434, 'accumulated_submission_time': 45438.995973587036, 'accumulated_eval_time': 3393.4224004745483, 'accumulated_logging_time': 10.807410717010498}
I0307 14:54:46.109599 140037979092736 logging_writer.py:48] [118259] accumulated_eval_time=3393.42, accumulated_logging_time=10.8074, accumulated_submission_time=45439, global_step=118259, preemption_count=0, score=45439, test/accuracy=0.5919, test/loss=1.97898, test/num_examples=10000, total_duration=48855.7, train/accuracy=0.854532, train/loss=0.772066, validation/accuracy=0.7186, validation/loss=1.33241, validation/num_examples=50000
I0307 14:55:02.358847 140037970700032 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.8695433139801025, loss=2.889909029006958
I0307 14:55:40.949806 140037979092736 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.65993595123291, loss=2.8456578254699707
I0307 14:56:19.528832 140037970700032 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.984402894973755, loss=2.8991141319274902
I0307 14:56:57.863360 140037979092736 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.8321611881256104, loss=2.865846872329712
I0307 14:57:36.188589 140037970700032 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.7713499069213867, loss=2.854905366897583
I0307 14:58:14.716367 140037979092736 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.0573461055755615, loss=2.8839457035064697
2025-03-07 14:58:30.062447: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:58:52.917599 140037970700032 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.8699896335601807, loss=2.833325147628784
I0307 14:59:31.106697 140037979092736 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.7771077156066895, loss=2.8513591289520264
I0307 15:00:09.362827 140037970700032 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.820505380630493, loss=2.835510730743408
I0307 15:00:47.623714 140037979092736 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.890979528427124, loss=2.8319191932678223
I0307 15:01:26.014309 140037970700032 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.8996851444244385, loss=2.8475747108459473
I0307 15:02:04.644928 140037979092736 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.964508056640625, loss=2.8926875591278076
I0307 15:02:43.108357 140037970700032 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.81613826751709, loss=2.85306715965271
I0307 15:03:16.129120 140193455334592 spec.py:321] Evaluating on the training split.
I0307 15:03:27.496362 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 15:03:50.499913 140193455334592 spec.py:349] Evaluating on the test split.
I0307 15:03:52.286039 140193455334592 submission_runner.py:469] Time since start: 49402.04s, 	Step: 119587, 	{'train/accuracy': 0.8564453125, 'train/loss': 0.7863262295722961, 'validation/accuracy': 0.7173199653625488, 'validation/loss': 1.357183814048767, 'validation/num_examples': 50000, 'test/accuracy': 0.5908000469207764, 'test/loss': 1.9870078563690186, 'test/num_examples': 10000, 'score': 45948.78064036369, 'total_duration': 49402.041544914246, 'accumulated_submission_time': 45948.78064036369, 'accumulated_eval_time': 3429.579167842865, 'accumulated_logging_time': 11.026552200317383}
I0307 15:03:52.408323 140037979092736 logging_writer.py:48] [119587] accumulated_eval_time=3429.58, accumulated_logging_time=11.0266, accumulated_submission_time=45948.8, global_step=119587, preemption_count=0, score=45948.8, test/accuracy=0.5908, test/loss=1.98701, test/num_examples=10000, total_duration=49402, train/accuracy=0.856445, train/loss=0.786326, validation/accuracy=0.71732, validation/loss=1.35718, validation/num_examples=50000
I0307 15:03:57.793615 140037970700032 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.9978837966918945, loss=2.891854763031006
I0307 15:04:35.989310 140037979092736 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.928741216659546, loss=2.8687236309051514
I0307 15:05:14.705873 140037970700032 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.746063232421875, loss=2.8258352279663086
I0307 15:05:53.744942 140037979092736 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.935995578765869, loss=2.800786018371582
I0307 15:06:31.890180 140037970700032 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.9410250186920166, loss=2.867928981781006
2025-03-07 15:07:06.982042: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:07:10.861909 140037979092736 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.7800209522247314, loss=2.8970181941986084
I0307 15:07:48.430104 140037970700032 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.9309537410736084, loss=2.826206684112549
I0307 15:08:26.658291 140037979092736 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.022599458694458, loss=2.915048360824585
I0307 15:09:04.907854 140037970700032 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.8268935680389404, loss=2.854248523712158
I0307 15:09:43.447766 140037979092736 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.091972589492798, loss=2.784123182296753
I0307 15:10:21.931426 140037970700032 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.905514717102051, loss=2.843599319458008
I0307 15:11:00.218475 140037979092736 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.8030221462249756, loss=2.851560115814209
I0307 15:11:38.617355 140037970700032 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.833632707595825, loss=2.8784079551696777
I0307 15:12:17.082461 140037979092736 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.872844696044922, loss=2.8635292053222656
I0307 15:12:22.470492 140193455334592 spec.py:321] Evaluating on the training split.
I0307 15:12:33.672249 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 15:12:53.757221 140193455334592 spec.py:349] Evaluating on the test split.
I0307 15:12:55.546589 140193455334592 submission_runner.py:469] Time since start: 49945.30s, 	Step: 120915, 	{'train/accuracy': 0.8581592440605164, 'train/loss': 0.7788785696029663, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.3338935375213623, 'validation/num_examples': 50000, 'test/accuracy': 0.5926000475883484, 'test/loss': 1.9721448421478271, 'test/num_examples': 10000, 'score': 46458.670367240906, 'total_duration': 49945.30210542679, 'accumulated_submission_time': 46458.670367240906, 'accumulated_eval_time': 3462.655120372772, 'accumulated_logging_time': 11.17898178100586}
I0307 15:12:55.658056 140037970700032 logging_writer.py:48] [120915] accumulated_eval_time=3462.66, accumulated_logging_time=11.179, accumulated_submission_time=46458.7, global_step=120915, preemption_count=0, score=46458.7, test/accuracy=0.5926, test/loss=1.97214, test/num_examples=10000, total_duration=49945.3, train/accuracy=0.858159, train/loss=0.778879, validation/accuracy=0.72282, validation/loss=1.33389, validation/num_examples=50000
I0307 15:13:28.734267 140037979092736 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.735889434814453, loss=2.788870096206665
I0307 15:14:07.377956 140037970700032 logging_writer.py:48] [121100] global_step=121100, grad_norm=3.1307644844055176, loss=2.8866546154022217
I0307 15:14:45.926462 140037979092736 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.98049259185791, loss=2.8057661056518555
I0307 15:15:24.601050 140037970700032 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.7634623050689697, loss=2.820652484893799
2025-03-07 15:15:40.608873: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:16:02.476637 140037979092736 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.828676223754883, loss=2.7992103099823
I0307 15:16:40.929646 140037970700032 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.9979095458984375, loss=2.91941237449646
I0307 15:17:19.473850 140037979092736 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.238499641418457, loss=2.890148639678955
I0307 15:17:57.886143 140037970700032 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.8864564895629883, loss=2.8496482372283936
I0307 15:18:36.221799 140037979092736 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.7882437705993652, loss=2.8667871952056885
I0307 15:19:14.606184 140037970700032 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.9795594215393066, loss=2.8165369033813477
I0307 15:19:52.877234 140037979092736 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.98891544342041, loss=2.79548978805542
I0307 15:20:31.500596 140037970700032 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.9001288414001465, loss=2.824190139770508
I0307 15:21:09.749315 140037979092736 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.7578115463256836, loss=2.8271641731262207
I0307 15:21:25.611358 140193455334592 spec.py:321] Evaluating on the training split.
I0307 15:21:37.196910 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 15:22:01.574506 140193455334592 spec.py:349] Evaluating on the test split.
I0307 15:22:03.382753 140193455334592 submission_runner.py:469] Time since start: 50493.14s, 	Step: 122242, 	{'train/accuracy': 0.8538544178009033, 'train/loss': 0.7556219100952148, 'validation/accuracy': 0.7256799936294556, 'validation/loss': 1.2958855628967285, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.9350981712341309, 'test/num_examples': 10000, 'score': 46968.424134492874, 'total_duration': 50493.13824748993, 'accumulated_submission_time': 46968.424134492874, 'accumulated_eval_time': 3500.426337957382, 'accumulated_logging_time': 11.347643375396729}
I0307 15:22:03.497442 140037970700032 logging_writer.py:48] [122242] accumulated_eval_time=3500.43, accumulated_logging_time=11.3476, accumulated_submission_time=46968.4, global_step=122242, preemption_count=0, score=46968.4, test/accuracy=0.5966, test/loss=1.9351, test/num_examples=10000, total_duration=50493.1, train/accuracy=0.853854, train/loss=0.755622, validation/accuracy=0.72568, validation/loss=1.29589, validation/num_examples=50000
I0307 15:22:26.100812 140037979092736 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.0210342407226562, loss=2.8216733932495117
I0307 15:23:04.699343 140037970700032 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.8052937984466553, loss=2.7607531547546387
I0307 15:23:43.341109 140037979092736 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.004519462585449, loss=2.8400866985321045
2025-03-07 15:24:17.536143: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:24:21.789706 140037970700032 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.014417886734009, loss=2.8706164360046387
I0307 15:25:00.138972 140037979092736 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.865739107131958, loss=2.7960946559906006
I0307 15:25:38.204042 140037970700032 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.872612237930298, loss=2.8080203533172607
I0307 15:26:16.317675 140037979092736 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.917445659637451, loss=2.8377609252929688
I0307 15:26:54.889048 140037970700032 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.9618072509765625, loss=2.8511104583740234
I0307 15:27:32.952119 140037979092736 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.1152191162109375, loss=2.8364272117614746
I0307 15:28:11.060339 140037970700032 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.1231954097747803, loss=2.810464382171631
I0307 15:28:49.371803 140037979092736 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.9928417205810547, loss=2.7777771949768066
I0307 15:29:28.437170 140037970700032 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.0667989253997803, loss=2.861483335494995
I0307 15:30:06.490512 140037979092736 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.0250186920166016, loss=2.8941352367401123
I0307 15:30:33.766924 140193455334592 spec.py:321] Evaluating on the training split.
I0307 15:30:45.136382 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 15:31:06.688931 140193455334592 spec.py:349] Evaluating on the test split.
I0307 15:31:08.467843 140193455334592 submission_runner.py:469] Time since start: 51038.22s, 	Step: 123572, 	{'train/accuracy': 0.8538345098495483, 'train/loss': 0.7689257264137268, 'validation/accuracy': 0.7252599596977234, 'validation/loss': 1.309849739074707, 'validation/num_examples': 50000, 'test/accuracy': 0.5989000201225281, 'test/loss': 1.93218195438385, 'test/num_examples': 10000, 'score': 47478.52780914307, 'total_duration': 51038.22334599495, 'accumulated_submission_time': 47478.52780914307, 'accumulated_eval_time': 3535.12708568573, 'accumulated_logging_time': 11.483810186386108}
I0307 15:31:08.540183 140037970700032 logging_writer.py:48] [123572] accumulated_eval_time=3535.13, accumulated_logging_time=11.4838, accumulated_submission_time=47478.5, global_step=123572, preemption_count=0, score=47478.5, test/accuracy=0.5989, test/loss=1.93218, test/num_examples=10000, total_duration=51038.2, train/accuracy=0.853835, train/loss=0.768926, validation/accuracy=0.72526, validation/loss=1.30985, validation/num_examples=50000
I0307 15:31:19.783626 140037979092736 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.9140119552612305, loss=2.770134925842285
I0307 15:31:58.038779 140037970700032 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.9650540351867676, loss=2.8294148445129395
I0307 15:32:36.148325 140037979092736 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.9867732524871826, loss=2.81854510307312
2025-03-07 15:32:53.255533: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:33:14.083952 140037970700032 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.903554677963257, loss=2.818037748336792
I0307 15:33:51.288481 140037979092736 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.1551616191864014, loss=2.8846001625061035
I0307 15:34:29.242189 140037970700032 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.0601205825805664, loss=2.8459243774414062
I0307 15:35:07.485460 140037979092736 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.9614784717559814, loss=2.7924978733062744
I0307 15:35:45.613960 140037970700032 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.990633249282837, loss=2.8540213108062744
I0307 15:36:23.973876 140037979092736 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.0159730911254883, loss=2.870044708251953
I0307 15:37:02.320469 140037970700032 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.9316163063049316, loss=2.886584997177124
I0307 15:37:40.960651 140037979092736 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.1119911670684814, loss=2.8488028049468994
I0307 15:38:18.988119 140037970700032 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.0711746215820312, loss=2.942596673965454
I0307 15:38:56.999411 140037979092736 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0063600540161133, loss=2.796520233154297
I0307 15:39:35.130440 140037970700032 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.068772792816162, loss=2.9270217418670654
I0307 15:39:38.701259 140193455334592 spec.py:321] Evaluating on the training split.
I0307 15:39:50.202999 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 15:40:13.642415 140193455334592 spec.py:349] Evaluating on the test split.
I0307 15:40:15.413321 140193455334592 submission_runner.py:469] Time since start: 51585.17s, 	Step: 124910, 	{'train/accuracy': 0.8482341766357422, 'train/loss': 0.7783138155937195, 'validation/accuracy': 0.7231599688529968, 'validation/loss': 1.3117820024490356, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.9467514753341675, 'test/num_examples': 10000, 'score': 47988.48527741432, 'total_duration': 51585.16881752014, 'accumulated_submission_time': 47988.48527741432, 'accumulated_eval_time': 3571.8389916419983, 'accumulated_logging_time': 11.616332292556763}
I0307 15:40:15.497723 140037979092736 logging_writer.py:48] [124910] accumulated_eval_time=3571.84, accumulated_logging_time=11.6163, accumulated_submission_time=47988.5, global_step=124910, preemption_count=0, score=47988.5, test/accuracy=0.5971, test/loss=1.94675, test/num_examples=10000, total_duration=51585.2, train/accuracy=0.848234, train/loss=0.778314, validation/accuracy=0.72316, validation/loss=1.31178, validation/num_examples=50000
I0307 15:40:50.146225 140037970700032 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.048084020614624, loss=2.861518621444702
2025-03-07 15:41:26.237720: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:41:28.315071 140037979092736 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.978198289871216, loss=2.772395372390747
I0307 15:42:04.785512 140037970700032 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.328320026397705, loss=2.8968701362609863
I0307 15:42:42.184808 140037979092736 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.942322254180908, loss=2.856159210205078
I0307 15:43:19.260177 140037970700032 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.978703022003174, loss=2.814924955368042
I0307 15:43:56.355472 140037979092736 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.885829210281372, loss=2.7989234924316406
I0307 15:44:34.177036 140037970700032 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.0820231437683105, loss=2.8022661209106445
I0307 15:45:10.959109 140037979092736 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.816939115524292, loss=2.7600040435791016
I0307 15:45:48.370697 140037970700032 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.9790680408477783, loss=2.8545916080474854
I0307 15:46:25.077173 140037979092736 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.000999927520752, loss=2.7754604816436768
I0307 15:47:02.205489 140037970700032 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.9921061992645264, loss=2.7524542808532715
I0307 15:47:40.352993 140037979092736 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.906677007675171, loss=2.7934765815734863
I0307 15:48:17.524411 140037970700032 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.1097829341888428, loss=2.782553195953369
I0307 15:48:45.507319 140193455334592 spec.py:321] Evaluating on the training split.
I0307 15:48:56.995669 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 15:49:16.916755 140193455334592 spec.py:349] Evaluating on the test split.
I0307 15:49:18.734011 140193455334592 submission_runner.py:469] Time since start: 52128.49s, 	Step: 126275, 	{'train/accuracy': 0.8556879758834839, 'train/loss': 0.7615275382995605, 'validation/accuracy': 0.7256399989128113, 'validation/loss': 1.2998392581939697, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.93067467212677, 'test/num_examples': 10000, 'score': 48498.32094120979, 'total_duration': 52128.489530563354, 'accumulated_submission_time': 48498.32094120979, 'accumulated_eval_time': 3605.065532207489, 'accumulated_logging_time': 11.724518775939941}
I0307 15:49:18.859815 140037979092736 logging_writer.py:48] [126275] accumulated_eval_time=3605.07, accumulated_logging_time=11.7245, accumulated_submission_time=48498.3, global_step=126275, preemption_count=0, score=48498.3, test/accuracy=0.6008, test/loss=1.93067, test/num_examples=10000, total_duration=52128.5, train/accuracy=0.855688, train/loss=0.761528, validation/accuracy=0.72564, validation/loss=1.29984, validation/num_examples=50000
I0307 15:49:28.916131 140037970700032 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.996398687362671, loss=2.8298559188842773
2025-03-07 15:49:46.317895: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:50:06.238333 140037979092736 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.2450499534606934, loss=2.8510704040527344
I0307 15:50:43.213752 140037970700032 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.9962568283081055, loss=2.7947921752929688
I0307 15:51:20.445949 140037979092736 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.0310187339782715, loss=2.848950147628784
I0307 15:51:57.290415 140037970700032 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.8679256439208984, loss=2.760239601135254
I0307 15:52:34.795764 140037979092736 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.9871621131896973, loss=2.8380799293518066
I0307 15:53:12.277259 140037970700032 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.1527352333068848, loss=2.815992593765259
I0307 15:53:51.012933 140037979092736 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.971245527267456, loss=2.8011727333068848
I0307 15:54:29.083831 140037970700032 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.973104238510132, loss=2.758553981781006
I0307 15:55:07.030335 140037979092736 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.0231239795684814, loss=2.907611608505249
I0307 15:55:45.101432 140037970700032 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.1155972480773926, loss=2.843161106109619
I0307 15:56:23.352475 140037979092736 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.9784882068634033, loss=2.8215017318725586
I0307 15:57:01.763302 140037970700032 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.0038697719573975, loss=2.808729648590088
2025-03-07 15:57:38.283598: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:57:39.753221 140037979092736 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.870358467102051, loss=2.7867674827575684
I0307 15:57:48.997876 140193455334592 spec.py:321] Evaluating on the training split.
I0307 15:58:00.711091 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 15:58:26.445826 140193455334592 spec.py:349] Evaluating on the test split.
I0307 15:58:28.208957 140193455334592 submission_runner.py:469] Time since start: 52677.96s, 	Step: 127626, 	{'train/accuracy': 0.8526387214660645, 'train/loss': 0.7871387600898743, 'validation/accuracy': 0.7267400026321411, 'validation/loss': 1.3124920129776, 'validation/num_examples': 50000, 'test/accuracy': 0.6059000492095947, 'test/loss': 1.9297891855239868, 'test/num_examples': 10000, 'score': 49008.279960632324, 'total_duration': 52677.96447587013, 'accumulated_submission_time': 49008.279960632324, 'accumulated_eval_time': 3644.2764604091644, 'accumulated_logging_time': 11.8839111328125}
I0307 15:58:28.284585 140037970700032 logging_writer.py:48] [127626] accumulated_eval_time=3644.28, accumulated_logging_time=11.8839, accumulated_submission_time=49008.3, global_step=127626, preemption_count=0, score=49008.3, test/accuracy=0.6059, test/loss=1.92979, test/num_examples=10000, total_duration=52678, train/accuracy=0.852639, train/loss=0.787139, validation/accuracy=0.72674, validation/loss=1.31249, validation/num_examples=50000
I0307 15:58:56.825839 140037979092736 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.3818440437316895, loss=2.8923423290252686
I0307 15:59:35.095265 140037970700032 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.9575748443603516, loss=2.7804107666015625
I0307 16:00:13.321499 140037979092736 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.2452738285064697, loss=2.9092578887939453
I0307 16:00:51.434862 140037970700032 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.994961738586426, loss=2.8684885501861572
I0307 16:01:29.981864 140037979092736 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.950991630554199, loss=2.8079683780670166
I0307 16:02:08.191595 140037970700032 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.130737781524658, loss=2.784841537475586
I0307 16:02:46.239739 140037979092736 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.094120502471924, loss=2.8062031269073486
I0307 16:03:24.274722 140037970700032 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.163282871246338, loss=2.8139448165893555
I0307 16:04:02.223194 140037979092736 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.0048811435699463, loss=2.8061110973358154
I0307 16:04:40.094861 140037970700032 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.0414557456970215, loss=2.7953381538391113
I0307 16:05:17.929464 140037979092736 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.0207371711730957, loss=2.79819917678833
I0307 16:05:56.083672 140037970700032 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.0866551399230957, loss=2.7848055362701416
2025-03-07 16:06:12.842508: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:06:34.204122 140037979092736 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.0374338626861572, loss=2.778235912322998
I0307 16:06:58.485877 140193455334592 spec.py:321] Evaluating on the training split.
I0307 16:07:10.308501 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 16:07:32.817516 140193455334592 spec.py:349] Evaluating on the test split.
I0307 16:07:34.633983 140193455334592 submission_runner.py:469] Time since start: 53224.39s, 	Step: 128964, 	{'train/accuracy': 0.8551897406578064, 'train/loss': 0.7733866572380066, 'validation/accuracy': 0.7266799807548523, 'validation/loss': 1.2999855279922485, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.9424817562103271, 'test/num_examples': 10000, 'score': 49518.30748152733, 'total_duration': 53224.38951301575, 'accumulated_submission_time': 49518.30748152733, 'accumulated_eval_time': 3680.424438238144, 'accumulated_logging_time': 11.989202499389648}
I0307 16:07:34.736922 140037970700032 logging_writer.py:48] [128964] accumulated_eval_time=3680.42, accumulated_logging_time=11.9892, accumulated_submission_time=49518.3, global_step=128964, preemption_count=0, score=49518.3, test/accuracy=0.6024, test/loss=1.94248, test/num_examples=10000, total_duration=53224.4, train/accuracy=0.85519, train/loss=0.773387, validation/accuracy=0.72668, validation/loss=1.29999, validation/num_examples=50000
I0307 16:07:48.843888 140037979092736 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.9746358394622803, loss=2.7835006713867188
I0307 16:08:26.769776 140037970700032 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.0833489894866943, loss=2.8079612255096436
I0307 16:09:05.057183 140037979092736 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.970003366470337, loss=2.7857460975646973
I0307 16:09:43.330790 140037970700032 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.9528636932373047, loss=2.7855169773101807
I0307 16:10:21.356730 140037979092736 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.1289424896240234, loss=2.8610804080963135
I0307 16:10:59.381885 140037970700032 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.1013598442077637, loss=2.846538782119751
I0307 16:11:37.494362 140037979092736 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.9358603954315186, loss=2.732450246810913
I0307 16:12:15.567378 140037970700032 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.111419677734375, loss=2.77986741065979
I0307 16:12:53.637346 140037979092736 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.0674920082092285, loss=2.764756679534912
I0307 16:13:31.835468 140037970700032 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.0316214561462402, loss=2.853583574295044
I0307 16:14:10.101105 140037979092736 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.19486141204834, loss=2.8565292358398438
2025-03-07 16:14:46.881795: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:14:48.344454 140037970700032 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.072230100631714, loss=2.817493438720703
I0307 16:15:25.643427 140037979092736 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.1397364139556885, loss=2.7911226749420166
I0307 16:16:03.864841 140037970700032 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.2593584060668945, loss=2.8126018047332764
I0307 16:16:04.644870 140193455334592 spec.py:321] Evaluating on the training split.
I0307 16:16:16.025792 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 16:16:37.189043 140193455334592 spec.py:349] Evaluating on the test split.
I0307 16:16:38.998961 140193455334592 submission_runner.py:469] Time since start: 53768.75s, 	Step: 130303, 	{'train/accuracy': 0.8565050959587097, 'train/loss': 0.7721459865570068, 'validation/accuracy': 0.7292199730873108, 'validation/loss': 1.306473970413208, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.9184318780899048, 'test/num_examples': 10000, 'score': 50028.05168366432, 'total_duration': 53768.754437208176, 'accumulated_submission_time': 50028.05168366432, 'accumulated_eval_time': 3714.7783331871033, 'accumulated_logging_time': 12.112935304641724}
I0307 16:16:39.150487 140037979092736 logging_writer.py:48] [130303] accumulated_eval_time=3714.78, accumulated_logging_time=12.1129, accumulated_submission_time=50028.1, global_step=130303, preemption_count=0, score=50028.1, test/accuracy=0.6073, test/loss=1.91843, test/num_examples=10000, total_duration=53768.8, train/accuracy=0.856505, train/loss=0.772146, validation/accuracy=0.72922, validation/loss=1.30647, validation/num_examples=50000
I0307 16:17:16.225453 140037970700032 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.996258497238159, loss=2.7780768871307373
I0307 16:17:54.446353 140037979092736 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.0335400104522705, loss=2.7382123470306396
I0307 16:18:32.697623 140037970700032 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.460160970687866, loss=2.7707595825195312
I0307 16:19:10.646814 140037979092736 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.4472899436950684, loss=2.85041880607605
I0307 16:19:48.643997 140037970700032 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.9567840099334717, loss=2.7618680000305176
I0307 16:20:26.855935 140037979092736 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.0278494358062744, loss=2.7316155433654785
I0307 16:21:05.158923 140037970700032 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.235090494155884, loss=2.7287163734436035
I0307 16:21:43.434205 140037979092736 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.0617101192474365, loss=2.8304944038391113
I0307 16:22:21.697689 140037970700032 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.225776195526123, loss=2.7976527214050293
I0307 16:23:00.275155 140037979092736 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.0975706577301025, loss=2.7952451705932617
2025-03-07 16:23:18.502070: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:23:37.636025 140037970700032 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.1786906719207764, loss=2.8166027069091797
I0307 16:24:16.017057 140037979092736 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.1010520458221436, loss=2.7935688495635986
I0307 16:24:54.195681 140037970700032 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.269378185272217, loss=2.7882559299468994
I0307 16:25:09.234795 140193455334592 spec.py:321] Evaluating on the training split.
I0307 16:25:20.960822 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 16:25:43.819703 140193455334592 spec.py:349] Evaluating on the test split.
I0307 16:25:45.650385 140193455334592 submission_runner.py:469] Time since start: 54315.41s, 	Step: 131640, 	{'train/accuracy': 0.8598134517669678, 'train/loss': 0.754364013671875, 'validation/accuracy': 0.7331399917602539, 'validation/loss': 1.2859514951705933, 'validation/num_examples': 50000, 'test/accuracy': 0.6078000068664551, 'test/loss': 1.9231277704238892, 'test/num_examples': 10000, 'score': 50537.96726727486, 'total_duration': 54315.40590119362, 'accumulated_submission_time': 50537.96726727486, 'accumulated_eval_time': 3751.193768978119, 'accumulated_logging_time': 12.286328792572021}
I0307 16:25:45.797869 140037979092736 logging_writer.py:48] [131640] accumulated_eval_time=3751.19, accumulated_logging_time=12.2863, accumulated_submission_time=50538, global_step=131640, preemption_count=0, score=50538, test/accuracy=0.6078, test/loss=1.92313, test/num_examples=10000, total_duration=54315.4, train/accuracy=0.859813, train/loss=0.754364, validation/accuracy=0.73314, validation/loss=1.28595, validation/num_examples=50000
I0307 16:26:08.881979 140037970700032 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.119762659072876, loss=2.6694283485412598
I0307 16:26:46.867639 140037979092736 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.1580379009246826, loss=2.796257495880127
I0307 16:27:24.970878 140037970700032 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.1960971355438232, loss=2.823474407196045
I0307 16:28:03.161650 140037979092736 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.927442789077759, loss=2.77493953704834
I0307 16:28:41.054740 140037970700032 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.3481688499450684, loss=2.861130475997925
I0307 16:29:19.361680 140037979092736 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.9075982570648193, loss=2.7654199600219727
I0307 16:29:57.158205 140037970700032 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.3468735218048096, loss=2.8076510429382324
I0307 16:30:35.535535 140037979092736 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.0385570526123047, loss=2.7467503547668457
I0307 16:31:13.995937 140037970700032 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.2329344749450684, loss=2.813471555709839
I0307 16:31:52.301442 140037979092736 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.0926923751831055, loss=2.7183749675750732
2025-03-07 16:31:52.966345: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:32:28.881247 140037970700032 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.3687305450439453, loss=2.7547860145568848
I0307 16:33:05.753311 140037979092736 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2827508449554443, loss=2.787296772003174
I0307 16:33:42.699585 140037970700032 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.0087954998016357, loss=2.756509780883789
I0307 16:34:16.000307 140193455334592 spec.py:321] Evaluating on the training split.
I0307 16:34:27.969043 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 16:34:51.217401 140193455334592 spec.py:349] Evaluating on the test split.
I0307 16:34:53.021159 140193455334592 submission_runner.py:469] Time since start: 54862.78s, 	Step: 132991, 	{'train/accuracy': 0.8626434803009033, 'train/loss': 0.7463467717170715, 'validation/accuracy': 0.7346199750900269, 'validation/loss': 1.275136113166809, 'validation/num_examples': 50000, 'test/accuracy': 0.604200005531311, 'test/loss': 1.9059524536132812, 'test/num_examples': 10000, 'score': 51047.97311878204, 'total_duration': 54862.776564359665, 'accumulated_submission_time': 51047.97311878204, 'accumulated_eval_time': 3788.214375257492, 'accumulated_logging_time': 12.484822273254395}
I0307 16:34:53.147989 140037979092736 logging_writer.py:48] [132991] accumulated_eval_time=3788.21, accumulated_logging_time=12.4848, accumulated_submission_time=51048, global_step=132991, preemption_count=0, score=51048, test/accuracy=0.6042, test/loss=1.90595, test/num_examples=10000, total_duration=54862.8, train/accuracy=0.862643, train/loss=0.746347, validation/accuracy=0.73462, validation/loss=1.27514, validation/num_examples=50000
I0307 16:34:56.902651 140037970700032 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.01173734664917, loss=2.793247938156128
I0307 16:35:34.652661 140037979092736 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.4187419414520264, loss=2.7524569034576416
I0307 16:36:12.855218 140037970700032 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.008772611618042, loss=2.7369203567504883
I0307 16:36:51.054160 140037979092736 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.07023024559021, loss=2.719224214553833
I0307 16:37:29.181956 140037970700032 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.339881181716919, loss=2.7745096683502197
I0307 16:38:07.207391 140037979092736 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.307220220565796, loss=2.731538772583008
I0307 16:38:45.682798 140037970700032 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.3116159439086914, loss=2.767662286758423
I0307 16:39:23.962107 140037979092736 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.113635778427124, loss=2.7473363876342773
I0307 16:40:02.098639 140037970700032 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.04996657371521, loss=2.757720470428467
2025-03-07 16:40:23.466378: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:40:41.079375 140037979092736 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.196108102798462, loss=2.723062753677368
I0307 16:41:17.625513 140037970700032 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.0784285068511963, loss=2.779233455657959
I0307 16:41:54.457106 140037979092736 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.995089054107666, loss=2.8772599697113037
I0307 16:42:30.917616 140037970700032 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.06619930267334, loss=2.778852701187134
I0307 16:43:07.180811 140037979092736 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.1919922828674316, loss=2.7081456184387207
I0307 16:43:23.318624 140193455334592 spec.py:321] Evaluating on the training split.
I0307 16:43:34.559188 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 16:43:56.276552 140193455334592 spec.py:349] Evaluating on the test split.
I0307 16:43:58.047885 140193455334592 submission_runner.py:469] Time since start: 55407.80s, 	Step: 134345, 	{'train/accuracy': 0.8635602593421936, 'train/loss': 0.7222200632095337, 'validation/accuracy': 0.735539972782135, 'validation/loss': 1.2597770690917969, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.896063208580017, 'test/num_examples': 10000, 'score': 51557.96750831604, 'total_duration': 55407.80333042145, 'accumulated_submission_time': 51557.96750831604, 'accumulated_eval_time': 3822.943412542343, 'accumulated_logging_time': 12.640672445297241}
I0307 16:43:58.122582 140037970700032 logging_writer.py:48] [134345] accumulated_eval_time=3822.94, accumulated_logging_time=12.6407, accumulated_submission_time=51558, global_step=134345, preemption_count=0, score=51558, test/accuracy=0.6135, test/loss=1.89606, test/num_examples=10000, total_duration=55407.8, train/accuracy=0.86356, train/loss=0.72222, validation/accuracy=0.73554, validation/loss=1.25978, validation/num_examples=50000
I0307 16:44:19.569565 140037979092736 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.193305730819702, loss=2.7831995487213135
I0307 16:44:57.941914 140037970700032 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.3948543071746826, loss=2.7703540325164795
I0307 16:45:36.178882 140037979092736 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.2310144901275635, loss=2.7518794536590576
I0307 16:46:14.397199 140037970700032 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.3058855533599854, loss=2.7427046298980713
I0307 16:46:52.558659 140037979092736 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.2804322242736816, loss=2.7809033393859863
I0307 16:47:30.915340 140037970700032 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.2573139667510986, loss=2.8448538780212402
I0307 16:48:09.272711 140037979092736 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.071293830871582, loss=2.7164523601531982
I0307 16:48:47.298110 140037970700032 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.3067128658294678, loss=2.8434224128723145
2025-03-07 16:48:49.082212: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:49:24.192883 140037979092736 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.16312575340271, loss=2.745105743408203
I0307 16:50:02.554812 140037970700032 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.1740615367889404, loss=2.7550692558288574
I0307 16:50:41.107832 140037979092736 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.1783547401428223, loss=2.7913262844085693
I0307 16:51:19.409359 140037970700032 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.081831932067871, loss=2.7796480655670166
I0307 16:51:57.519522 140037979092736 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.9972729682922363, loss=2.7454540729522705
I0307 16:52:28.429054 140193455334592 spec.py:321] Evaluating on the training split.
I0307 16:52:40.281440 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 16:53:04.160486 140193455334592 spec.py:349] Evaluating on the test split.
I0307 16:53:05.970725 140193455334592 submission_runner.py:469] Time since start: 55955.73s, 	Step: 135683, 	{'train/accuracy': 0.8566445708274841, 'train/loss': 0.7402517795562744, 'validation/accuracy': 0.733199954032898, 'validation/loss': 1.2630550861358643, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.9112828969955444, 'test/num_examples': 10000, 'score': 52068.079738378525, 'total_duration': 55955.72621059418, 'accumulated_submission_time': 52068.079738378525, 'accumulated_eval_time': 3860.4848976135254, 'accumulated_logging_time': 12.76378846168518}
I0307 16:53:06.095084 140037970700032 logging_writer.py:48] [135683] accumulated_eval_time=3860.48, accumulated_logging_time=12.7638, accumulated_submission_time=52068.1, global_step=135683, preemption_count=0, score=52068.1, test/accuracy=0.6045, test/loss=1.91128, test/num_examples=10000, total_duration=55955.7, train/accuracy=0.856645, train/loss=0.740252, validation/accuracy=0.7332, validation/loss=1.26306, validation/num_examples=50000
I0307 16:53:12.982948 140037979092736 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.305558681488037, loss=2.790827512741089
I0307 16:53:50.875876 140037970700032 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.1775476932525635, loss=2.729053497314453
I0307 16:54:28.906927 140037979092736 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.0961990356445312, loss=2.6858012676239014
I0307 16:55:07.317306 140037970700032 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.048105001449585, loss=2.6531879901885986
I0307 16:55:45.617145 140037979092736 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.2530417442321777, loss=2.7671003341674805
I0307 16:56:23.975455 140037970700032 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.2975330352783203, loss=2.7420530319213867
I0307 16:57:02.104170 140037979092736 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.381883382797241, loss=2.7869558334350586
2025-03-07 16:57:21.413631: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:57:40.342162 140037970700032 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.386916399002075, loss=2.8074722290039062
I0307 16:58:18.434705 140037979092736 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.4138023853302, loss=2.7673089504241943
I0307 16:58:56.544372 140037970700032 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.2468442916870117, loss=2.706881046295166
I0307 16:59:34.571629 140037979092736 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.2542245388031006, loss=2.8190300464630127
I0307 17:00:12.773146 140037970700032 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.2785871028900146, loss=2.6939640045166016
I0307 17:00:51.188954 140037979092736 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.2855193614959717, loss=2.6995177268981934
I0307 17:01:29.104174 140037970700032 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.2304024696350098, loss=2.752249002456665
I0307 17:01:36.296753 140193455334592 spec.py:321] Evaluating on the training split.
I0307 17:01:47.836180 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 17:02:09.729756 140193455334592 spec.py:349] Evaluating on the test split.
I0307 17:02:11.481528 140193455334592 submission_runner.py:469] Time since start: 56501.24s, 	Step: 137020, 	{'train/accuracy': 0.8651147484779358, 'train/loss': 0.7459121346473694, 'validation/accuracy': 0.7380799651145935, 'validation/loss': 1.2728499174118042, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.9153974056243896, 'test/num_examples': 10000, 'score': 52578.08869051933, 'total_duration': 56501.23701739311, 'accumulated_submission_time': 52578.08869051933, 'accumulated_eval_time': 3895.6695017814636, 'accumulated_logging_time': 12.931283950805664}
I0307 17:02:11.683616 140037979092736 logging_writer.py:48] [137020] accumulated_eval_time=3895.67, accumulated_logging_time=12.9313, accumulated_submission_time=52578.1, global_step=137020, preemption_count=0, score=52578.1, test/accuracy=0.6102, test/loss=1.9154, test/num_examples=10000, total_duration=56501.2, train/accuracy=0.865115, train/loss=0.745912, validation/accuracy=0.73808, validation/loss=1.27285, validation/num_examples=50000
I0307 17:02:42.584519 140037970700032 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.203303813934326, loss=2.7675156593322754
I0307 17:03:20.743645 140037979092736 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.30039644241333, loss=2.80220627784729
I0307 17:03:58.918338 140037970700032 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.3282852172851562, loss=2.7780702114105225
I0307 17:04:37.045181 140037979092736 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.244638204574585, loss=2.7614822387695312
I0307 17:05:15.271342 140037970700032 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.2064104080200195, loss=2.724832773208618
I0307 17:05:53.469159 140037979092736 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2552473545074463, loss=2.7533833980560303
2025-03-07 17:05:55.714354: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:06:30.305814 140037970700032 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.523297071456909, loss=2.7992630004882812
I0307 17:07:07.640070 140037979092736 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.2050206661224365, loss=2.7553069591522217
I0307 17:07:44.467435 140037970700032 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.2584917545318604, loss=2.6929948329925537
I0307 17:08:22.480774 140037979092736 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.343398094177246, loss=2.700545072555542
I0307 17:09:00.729006 140037970700032 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.2766239643096924, loss=2.7602086067199707
I0307 17:09:38.671219 140037979092736 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.303291082382202, loss=2.8088645935058594
I0307 17:10:16.515801 140037970700032 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.1554572582244873, loss=2.729154348373413
I0307 17:10:41.564022 140193455334592 spec.py:321] Evaluating on the training split.
I0307 17:10:53.143737 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 17:11:17.323226 140193455334592 spec.py:349] Evaluating on the test split.
I0307 17:11:19.083056 140193455334592 submission_runner.py:469] Time since start: 57048.84s, 	Step: 138366, 	{'train/accuracy': 0.8689413070678711, 'train/loss': 0.6961476802825928, 'validation/accuracy': 0.7372999787330627, 'validation/loss': 1.2360162734985352, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.8734062910079956, 'test/num_examples': 10000, 'score': 53087.78286600113, 'total_duration': 57048.83854794502, 'accumulated_submission_time': 53087.78286600113, 'accumulated_eval_time': 3933.1883544921875, 'accumulated_logging_time': 13.172261476516724}
I0307 17:11:19.178117 140037979092736 logging_writer.py:48] [138366] accumulated_eval_time=3933.19, accumulated_logging_time=13.1723, accumulated_submission_time=53087.8, global_step=138366, preemption_count=0, score=53087.8, test/accuracy=0.6143, test/loss=1.87341, test/num_examples=10000, total_duration=57048.8, train/accuracy=0.868941, train/loss=0.696148, validation/accuracy=0.7373, validation/loss=1.23602, validation/num_examples=50000
I0307 17:11:32.575897 140037970700032 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.303586959838867, loss=2.7083568572998047
I0307 17:12:10.566931 140037979092736 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.493241548538208, loss=2.7265243530273438
I0307 17:12:48.948878 140037970700032 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.2885475158691406, loss=2.735185146331787
I0307 17:13:27.382289 140037979092736 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.3625783920288086, loss=2.7232799530029297
I0307 17:14:05.737767 140037970700032 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.3927834033966064, loss=2.7296688556671143
2025-03-07 17:14:27.812860: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:14:44.006254 140037979092736 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.4946415424346924, loss=2.7370920181274414
I0307 17:15:21.249134 140037970700032 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.220792770385742, loss=2.688117027282715
I0307 17:15:59.596332 140037979092736 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.208070993423462, loss=2.718731164932251
I0307 17:16:37.665427 140037970700032 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.5310659408569336, loss=2.725799083709717
I0307 17:17:15.814181 140037979092736 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.30130672454834, loss=2.687915325164795
I0307 17:17:54.150299 140037970700032 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.3692402839660645, loss=2.813610076904297
I0307 17:18:32.658818 140037979092736 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.383598804473877, loss=2.7708983421325684
I0307 17:19:11.133610 140037970700032 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.4441182613372803, loss=2.7281644344329834
I0307 17:19:48.955795 140037979092736 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.31429386138916, loss=2.71280574798584
I0307 17:19:49.357739 140193455334592 spec.py:321] Evaluating on the training split.
I0307 17:20:00.990990 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 17:20:21.602274 140193455334592 spec.py:349] Evaluating on the test split.
I0307 17:20:23.422759 140193455334592 submission_runner.py:469] Time since start: 57593.18s, 	Step: 139702, 	{'train/accuracy': 0.8683035373687744, 'train/loss': 0.722339391708374, 'validation/accuracy': 0.742579996585846, 'validation/loss': 1.2524217367172241, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.8979796171188354, 'test/num_examples': 10000, 'score': 53597.78376507759, 'total_duration': 57593.178294181824, 'accumulated_submission_time': 53597.78376507759, 'accumulated_eval_time': 3967.25323843956, 'accumulated_logging_time': 13.300051212310791}
I0307 17:20:23.525922 140037970700032 logging_writer.py:48] [139702] accumulated_eval_time=3967.25, accumulated_logging_time=13.3001, accumulated_submission_time=53597.8, global_step=139702, preemption_count=0, score=53597.8, test/accuracy=0.6103, test/loss=1.89798, test/num_examples=10000, total_duration=57593.2, train/accuracy=0.868304, train/loss=0.722339, validation/accuracy=0.74258, validation/loss=1.25242, validation/num_examples=50000
I0307 17:21:01.093701 140037979092736 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.4412848949432373, loss=2.7194926738739014
I0307 17:21:39.858970 140037970700032 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.342487335205078, loss=2.784723997116089
I0307 17:22:18.412631 140037979092736 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.54403018951416, loss=2.799703359603882
I0307 17:22:56.938879 140037970700032 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.375865936279297, loss=2.7395918369293213
2025-03-07 17:22:59.311384: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:23:35.593327 140037979092736 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.3397552967071533, loss=2.7297043800354004
I0307 17:24:13.551026 140037970700032 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.723834753036499, loss=2.7986583709716797
I0307 17:24:51.886306 140037979092736 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.3373591899871826, loss=2.7247488498687744
I0307 17:25:30.652182 140037970700032 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.568110227584839, loss=2.766500473022461
I0307 17:26:09.051818 140037979092736 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.332759141921997, loss=2.785494804382324
I0307 17:26:47.341401 140037970700032 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.371004104614258, loss=2.704678535461426
I0307 17:27:25.698147 140037979092736 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.539543628692627, loss=2.6693601608276367
I0307 17:28:03.736771 140037970700032 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.358546495437622, loss=2.832710027694702
I0307 17:28:42.084667 140037979092736 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.3682398796081543, loss=2.7115118503570557
I0307 17:28:53.513652 140193455334592 spec.py:321] Evaluating on the training split.
I0307 17:29:05.176047 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 17:29:23.040045 140193455334592 spec.py:349] Evaluating on the test split.
I0307 17:29:24.867746 140193455334592 submission_runner.py:469] Time since start: 58134.62s, 	Step: 141031, 	{'train/accuracy': 0.8724290132522583, 'train/loss': 0.704590916633606, 'validation/accuracy': 0.7425199747085571, 'validation/loss': 1.2437777519226074, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.8900768756866455, 'test/num_examples': 10000, 'score': 54107.58808326721, 'total_duration': 58134.62321949005, 'accumulated_submission_time': 54107.58808326721, 'accumulated_eval_time': 3998.6071343421936, 'accumulated_logging_time': 13.444273710250854}
I0307 17:29:24.973495 140037970700032 logging_writer.py:48] [141031] accumulated_eval_time=3998.61, accumulated_logging_time=13.4443, accumulated_submission_time=54107.6, global_step=141031, preemption_count=0, score=54107.6, test/accuracy=0.6142, test/loss=1.89008, test/num_examples=10000, total_duration=58134.6, train/accuracy=0.872429, train/loss=0.704591, validation/accuracy=0.74252, validation/loss=1.24378, validation/num_examples=50000
I0307 17:29:51.897356 140037979092736 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.5376062393188477, loss=2.648212194442749
I0307 17:30:30.160547 140037970700032 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.2096385955810547, loss=2.724295139312744
I0307 17:31:08.800589 140037979092736 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.3574187755584717, loss=2.7292773723602295
2025-03-07 17:31:30.278777: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:31:47.567498 140037970700032 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.36146879196167, loss=2.695039749145508
I0307 17:32:26.145034 140037979092736 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.4202449321746826, loss=2.756844997406006
I0307 17:33:04.225733 140037970700032 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.1020920276641846, loss=2.6625864505767822
I0307 17:33:42.603649 140037979092736 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.482339859008789, loss=2.7115988731384277
I0307 17:34:20.731177 140037970700032 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.4606761932373047, loss=2.703740119934082
I0307 17:34:59.117421 140037979092736 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.6510255336761475, loss=2.6825668811798096
I0307 17:35:37.409135 140037970700032 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.248764991760254, loss=2.7142333984375
I0307 17:36:15.922417 140037979092736 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.4992051124572754, loss=2.727909803390503
I0307 17:36:54.309956 140037970700032 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.4481546878814697, loss=2.7534101009368896
I0307 17:37:32.223854 140037979092736 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.549276351928711, loss=2.8164145946502686
I0307 17:37:55.122186 140193455334592 spec.py:321] Evaluating on the training split.
I0307 17:38:06.965809 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 17:38:26.779941 140193455334592 spec.py:349] Evaluating on the test split.
I0307 17:38:28.636091 140193455334592 submission_runner.py:469] Time since start: 58678.39s, 	Step: 142361, 	{'train/accuracy': 0.8742226958274841, 'train/loss': 0.677759051322937, 'validation/accuracy': 0.7409600019454956, 'validation/loss': 1.2262144088745117, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.8658561706542969, 'test/num_examples': 10000, 'score': 54617.52095079422, 'total_duration': 58678.391629219055, 'accumulated_submission_time': 54617.52095079422, 'accumulated_eval_time': 4032.120908498764, 'accumulated_logging_time': 13.623765707015991}
I0307 17:38:28.760161 140037970700032 logging_writer.py:48] [142361] accumulated_eval_time=4032.12, accumulated_logging_time=13.6238, accumulated_submission_time=54617.5, global_step=142361, preemption_count=0, score=54617.5, test/accuracy=0.6142, test/loss=1.86586, test/num_examples=10000, total_duration=58678.4, train/accuracy=0.874223, train/loss=0.677759, validation/accuracy=0.74096, validation/loss=1.22621, validation/num_examples=50000
I0307 17:38:44.208695 140037979092736 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.395634889602661, loss=2.731062412261963
I0307 17:39:22.376479 140037970700032 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.5228664875030518, loss=2.7486653327941895
I0307 17:40:01.046867 140037979092736 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.6087236404418945, loss=2.7356021404266357
2025-03-07 17:40:05.075300: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:40:38.688777 140037970700032 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.5820624828338623, loss=2.7012295722961426
I0307 17:41:16.528093 140037979092736 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.5764665603637695, loss=2.692026138305664
I0307 17:41:54.658802 140037970700032 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.3560314178466797, loss=2.742997646331787
I0307 17:42:33.088382 140037979092736 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.6423356533050537, loss=2.7206945419311523
I0307 17:43:11.352531 140037970700032 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.5557239055633545, loss=2.715078830718994
I0307 17:43:49.379372 140037979092736 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.4895787239074707, loss=2.6713204383850098
I0307 17:44:27.429821 140037970700032 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.4316816329956055, loss=2.7085795402526855
I0307 17:45:05.911717 140037979092736 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.5149085521698, loss=2.7251715660095215
I0307 17:45:44.084945 140037970700032 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.3109612464904785, loss=2.738414764404297
I0307 17:46:22.471402 140037979092736 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.3971402645111084, loss=2.736879587173462
I0307 17:46:58.679427 140193455334592 spec.py:321] Evaluating on the training split.
I0307 17:47:10.490039 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 17:47:30.177068 140193455334592 spec.py:349] Evaluating on the test split.
I0307 17:47:32.009874 140193455334592 submission_runner.py:469] Time since start: 59221.77s, 	Step: 143695, 	{'train/accuracy': 0.8719506859779358, 'train/loss': 0.6898953914642334, 'validation/accuracy': 0.7416200041770935, 'validation/loss': 1.2386674880981445, 'validation/num_examples': 50000, 'test/accuracy': 0.6150000095367432, 'test/loss': 1.8921301364898682, 'test/num_examples': 10000, 'score': 55127.27566242218, 'total_duration': 59221.765394210815, 'accumulated_submission_time': 55127.27566242218, 'accumulated_eval_time': 4065.4512078762054, 'accumulated_logging_time': 13.768025875091553}
I0307 17:47:32.108721 140037970700032 logging_writer.py:48] [143695] accumulated_eval_time=4065.45, accumulated_logging_time=13.768, accumulated_submission_time=55127.3, global_step=143695, preemption_count=0, score=55127.3, test/accuracy=0.615, test/loss=1.89213, test/num_examples=10000, total_duration=59221.8, train/accuracy=0.871951, train/loss=0.689895, validation/accuracy=0.74162, validation/loss=1.23867, validation/num_examples=50000
I0307 17:47:34.489870 140037979092736 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.6635820865631104, loss=2.708261489868164
I0307 17:48:12.990109 140037970700032 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.568572759628296, loss=2.7459609508514404
2025-03-07 17:48:36.782676: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:48:51.162667 140037979092736 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.3360486030578613, loss=2.7508163452148438
I0307 17:49:29.434022 140037970700032 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.2706167697906494, loss=2.712130069732666
I0307 17:50:07.873037 140037979092736 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.4048423767089844, loss=2.7347488403320312
I0307 17:50:46.086423 140037970700032 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.6299216747283936, loss=2.742689847946167
I0307 17:51:24.439508 140037979092736 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.374081611633301, loss=2.72353196144104
I0307 17:52:02.485185 140037970700032 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.447371482849121, loss=2.7258644104003906
I0307 17:52:41.036470 140037979092736 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.286761999130249, loss=2.704885482788086
I0307 17:53:19.582937 140037970700032 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.503857135772705, loss=2.74122953414917
I0307 17:53:57.859881 140037979092736 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.426928997039795, loss=2.7367091178894043
I0307 17:54:36.321662 140037970700032 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.3580188751220703, loss=2.719503402709961
I0307 17:55:14.890102 140037979092736 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.466655969619751, loss=2.771049976348877
I0307 17:55:53.413523 140037970700032 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.3796565532684326, loss=2.6822409629821777
I0307 17:56:02.278333 140193455334592 spec.py:321] Evaluating on the training split.
I0307 17:56:13.897523 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 17:56:34.303078 140193455334592 spec.py:349] Evaluating on the test split.
I0307 17:56:36.133887 140193455334592 submission_runner.py:469] Time since start: 59765.89s, 	Step: 145024, 	{'train/accuracy': 0.8809988498687744, 'train/loss': 0.6845759153366089, 'validation/accuracy': 0.7438799738883972, 'validation/loss': 1.2611819505691528, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.8859914541244507, 'test/num_examples': 10000, 'score': 55637.26403737068, 'total_duration': 59765.88941693306, 'accumulated_submission_time': 55637.26403737068, 'accumulated_eval_time': 4099.306620121002, 'accumulated_logging_time': 13.90233063697815}
I0307 17:56:36.242371 140037979092736 logging_writer.py:48] [145024] accumulated_eval_time=4099.31, accumulated_logging_time=13.9023, accumulated_submission_time=55637.3, global_step=145024, preemption_count=0, score=55637.3, test/accuracy=0.6174, test/loss=1.88599, test/num_examples=10000, total_duration=59765.9, train/accuracy=0.880999, train/loss=0.684576, validation/accuracy=0.74388, validation/loss=1.26118, validation/num_examples=50000
I0307 17:57:05.760080 140037970700032 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.3931386470794678, loss=2.736971378326416
2025-03-07 17:57:09.431187: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:57:44.227010 140037979092736 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.6322710514068604, loss=2.68212628364563
I0307 17:58:22.849531 140037970700032 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.5428836345672607, loss=2.667174816131592
I0307 17:59:00.938749 140037979092736 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.738995313644409, loss=2.6909539699554443
I0307 17:59:39.707225 140037970700032 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.6178410053253174, loss=2.72978138923645
I0307 18:00:17.910786 140037979092736 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.3621695041656494, loss=2.6879847049713135
I0307 18:00:56.438305 140037970700032 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.6609745025634766, loss=2.7114646434783936
I0307 18:01:35.087599 140037979092736 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.5799834728240967, loss=2.671156406402588
I0307 18:02:13.793851 140037970700032 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.4474644660949707, loss=2.6987626552581787
I0307 18:02:52.294082 140037979092736 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.6760897636413574, loss=2.6929831504821777
I0307 18:03:30.703426 140037970700032 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.5301198959350586, loss=2.7082419395446777
I0307 18:04:09.434995 140037979092736 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.5130858421325684, loss=2.6683948040008545
I0307 18:04:47.990663 140037970700032 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.365515947341919, loss=2.658017635345459
I0307 18:05:06.212557 140193455334592 spec.py:321] Evaluating on the training split.
I0307 18:05:18.164112 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 18:05:35.956019 140193455334592 spec.py:349] Evaluating on the test split.
I0307 18:05:37.773380 140193455334592 submission_runner.py:469] Time since start: 60307.53s, 	Step: 146348, 	{'train/accuracy': 0.8780691623687744, 'train/loss': 0.6677699685096741, 'validation/accuracy': 0.7424399852752686, 'validation/loss': 1.2333136796951294, 'validation/num_examples': 50000, 'test/accuracy': 0.6211000084877014, 'test/loss': 1.8664543628692627, 'test/num_examples': 10000, 'score': 56147.04470014572, 'total_duration': 60307.52887225151, 'accumulated_submission_time': 56147.04470014572, 'accumulated_eval_time': 4130.867275238037, 'accumulated_logging_time': 14.056933403015137}
I0307 18:05:37.843832 140037979092736 logging_writer.py:48] [146348] accumulated_eval_time=4130.87, accumulated_logging_time=14.0569, accumulated_submission_time=56147, global_step=146348, preemption_count=0, score=56147, test/accuracy=0.6211, test/loss=1.86645, test/num_examples=10000, total_duration=60307.5, train/accuracy=0.878069, train/loss=0.66777, validation/accuracy=0.74244, validation/loss=1.23331, validation/num_examples=50000
2025-03-07 18:05:41.785450: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:05:58.339470 140037970700032 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.2570745944976807, loss=2.707818031311035
I0307 18:06:36.566402 140037979092736 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.399773597717285, loss=2.716477394104004
I0307 18:07:15.093542 140037970700032 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.6390552520751953, loss=2.7037971019744873
I0307 18:07:53.856119 140037979092736 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.569516181945801, loss=2.6957364082336426
I0307 18:08:32.546117 140037970700032 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.502346992492676, loss=2.7145931720733643
I0307 18:09:10.897524 140037979092736 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.610502004623413, loss=2.7754602432250977
I0307 18:09:49.307755 140037970700032 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.5340683460235596, loss=2.743837833404541
I0307 18:10:27.767318 140037979092736 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.420454740524292, loss=2.626337766647339
I0307 18:11:06.003169 140037970700032 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.6489295959472656, loss=2.606605052947998
I0307 18:11:44.585996 140037979092736 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.7603847980499268, loss=2.678173780441284
I0307 18:12:22.484227 140037970700032 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.5824429988861084, loss=2.666944980621338
I0307 18:13:00.642132 140037979092736 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.374345064163208, loss=2.6237576007843018
I0307 18:13:39.251738 140037970700032 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.640491485595703, loss=2.7174174785614014
2025-03-07 18:13:43.314036: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:14:07.912051 140193455334592 spec.py:321] Evaluating on the training split.
I0307 18:14:19.427956 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 18:14:38.190005 140193455334592 spec.py:349] Evaluating on the test split.
I0307 18:14:40.017530 140193455334592 submission_runner.py:469] Time since start: 60849.77s, 	Step: 147676, 	{'train/accuracy': 0.8805006146430969, 'train/loss': 0.6831640601158142, 'validation/accuracy': 0.7435399889945984, 'validation/loss': 1.2495644092559814, 'validation/num_examples': 50000, 'test/accuracy': 0.6137000322341919, 'test/loss': 1.893677830696106, 'test/num_examples': 10000, 'score': 56656.94694519043, 'total_duration': 60849.77301621437, 'accumulated_submission_time': 56656.94694519043, 'accumulated_eval_time': 4162.972571134567, 'accumulated_logging_time': 14.150558948516846}
I0307 18:14:40.151818 140037979092736 logging_writer.py:48] [147676] accumulated_eval_time=4162.97, accumulated_logging_time=14.1506, accumulated_submission_time=56656.9, global_step=147676, preemption_count=0, score=56656.9, test/accuracy=0.6137, test/loss=1.89368, test/num_examples=10000, total_duration=60849.8, train/accuracy=0.880501, train/loss=0.683164, validation/accuracy=0.74354, validation/loss=1.24956, validation/num_examples=50000
I0307 18:14:49.829720 140037970700032 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.9542148113250732, loss=2.748457908630371
I0307 18:15:28.011656 140037979092736 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.5236470699310303, loss=2.6833980083465576
I0307 18:16:06.832103 140037970700032 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.485480546951294, loss=2.6639671325683594
I0307 18:16:45.289167 140037979092736 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.393026351928711, loss=2.59891414642334
I0307 18:17:23.639428 140037970700032 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.555337905883789, loss=2.6426281929016113
I0307 18:18:01.963598 140037979092736 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.5465288162231445, loss=2.65285062789917
I0307 18:18:40.309685 140037970700032 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.411167621612549, loss=2.6437184810638428
I0307 18:19:18.407529 140037979092736 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.225975513458252, loss=2.643247604370117
I0307 18:19:56.482044 140037970700032 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.5715484619140625, loss=2.6758573055267334
I0307 18:20:34.663158 140037979092736 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.5628952980041504, loss=2.685523271560669
I0307 18:21:12.901654 140037970700032 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.8082728385925293, loss=2.7016258239746094
I0307 18:21:51.095877 140037979092736 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.590702533721924, loss=2.6940972805023193
2025-03-07 18:22:16.853860: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:22:29.304988 140037970700032 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.433668375015259, loss=2.6905760765075684
I0307 18:23:06.251811 140037979092736 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.700091600418091, loss=2.6567208766937256
I0307 18:23:10.172874 140193455334592 spec.py:321] Evaluating on the training split.
I0307 18:23:22.037550 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 18:23:44.919109 140193455334592 spec.py:349] Evaluating on the test split.
I0307 18:23:46.729860 140193455334592 submission_runner.py:469] Time since start: 61396.49s, 	Step: 149011, 	{'train/accuracy': 0.8844866156578064, 'train/loss': 0.6485022306442261, 'validation/accuracy': 0.745419979095459, 'validation/loss': 1.2260268926620483, 'validation/num_examples': 50000, 'test/accuracy': 0.6216000318527222, 'test/loss': 1.8596113920211792, 'test/num_examples': 10000, 'score': 57166.792831897736, 'total_duration': 61396.48536944389, 'accumulated_submission_time': 57166.792831897736, 'accumulated_eval_time': 4199.529405593872, 'accumulated_logging_time': 14.316476583480835}
I0307 18:23:46.916278 140037970700032 logging_writer.py:48] [149011] accumulated_eval_time=4199.53, accumulated_logging_time=14.3165, accumulated_submission_time=57166.8, global_step=149011, preemption_count=0, score=57166.8, test/accuracy=0.6216, test/loss=1.85961, test/num_examples=10000, total_duration=61396.5, train/accuracy=0.884487, train/loss=0.648502, validation/accuracy=0.74542, validation/loss=1.22603, validation/num_examples=50000
I0307 18:24:21.228794 140037979092736 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.4270389080047607, loss=2.7260589599609375
I0307 18:24:59.669538 140037970700032 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.395663022994995, loss=2.636044502258301
I0307 18:25:38.245847 140037979092736 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.520062208175659, loss=2.7055301666259766
I0307 18:26:16.580380 140037970700032 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.524803638458252, loss=2.6233861446380615
I0307 18:26:54.460607 140037979092736 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.3525185585021973, loss=2.5992279052734375
I0307 18:27:32.744141 140037970700032 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.3345160484313965, loss=2.6128475666046143
I0307 18:28:10.869609 140037979092736 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.7546842098236084, loss=2.6827712059020996
I0307 18:28:48.969653 140037970700032 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.5832247734069824, loss=2.647320032119751
I0307 18:29:27.355217 140037979092736 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.5683021545410156, loss=2.712338447570801
I0307 18:30:05.945311 140037970700032 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.4632787704467773, loss=2.6692044734954834
I0307 18:30:44.526733 140037979092736 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.4942994117736816, loss=2.655388355255127
2025-03-07 18:30:50.749758: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:31:22.013926 140037970700032 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.671476364135742, loss=2.6859562397003174
I0307 18:31:59.973798 140037979092736 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.5960569381713867, loss=2.653210401535034
I0307 18:32:16.817705 140193455334592 spec.py:321] Evaluating on the training split.
I0307 18:32:28.488384 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 18:32:47.884375 140193455334592 spec.py:349] Evaluating on the test split.
I0307 18:32:49.686218 140193455334592 submission_runner.py:469] Time since start: 61939.44s, 	Step: 150344, 	{'train/accuracy': 0.8907046914100647, 'train/loss': 0.6355377435684204, 'validation/accuracy': 0.7493399977684021, 'validation/loss': 1.2205615043640137, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.845856785774231, 'test/num_examples': 10000, 'score': 57676.53068614006, 'total_duration': 61939.44174742699, 'accumulated_submission_time': 57676.53068614006, 'accumulated_eval_time': 4232.397779464722, 'accumulated_logging_time': 14.523430824279785}
I0307 18:32:49.779434 140037970700032 logging_writer.py:48] [150344] accumulated_eval_time=4232.4, accumulated_logging_time=14.5234, accumulated_submission_time=57676.5, global_step=150344, preemption_count=0, score=57676.5, test/accuracy=0.624, test/loss=1.84586, test/num_examples=10000, total_duration=61939.4, train/accuracy=0.890705, train/loss=0.635538, validation/accuracy=0.74934, validation/loss=1.22056, validation/num_examples=50000
I0307 18:33:11.897910 140037979092736 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.7608306407928467, loss=2.661198616027832
I0307 18:33:50.333594 140037970700032 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.8878440856933594, loss=2.71566104888916
I0307 18:34:28.906613 140037979092736 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.8665013313293457, loss=2.6955161094665527
I0307 18:35:07.214817 140037970700032 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.8009817600250244, loss=2.6585237979888916
I0307 18:35:45.578783 140037979092736 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.874194860458374, loss=2.6846115589141846
I0307 18:36:23.688373 140037970700032 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.4029459953308105, loss=2.584202289581299
I0307 18:37:01.993224 140037979092736 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.819258451461792, loss=2.6881227493286133
I0307 18:37:40.331846 140037970700032 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.6619555950164795, loss=2.678760290145874
I0307 18:38:18.610502 140037979092736 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.638078451156616, loss=2.6711108684539795
I0307 18:38:56.791769 140037970700032 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.749981641769409, loss=2.620206356048584
2025-03-07 18:39:21.326835: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:39:35.490669 140037979092736 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.718337059020996, loss=2.6819887161254883
I0307 18:40:13.798978 140037970700032 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.5637614727020264, loss=2.7098071575164795
I0307 18:40:52.270571 140037979092736 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.7142112255096436, loss=2.67006254196167
I0307 18:41:19.822222 140193455334592 spec.py:321] Evaluating on the training split.
I0307 18:41:31.870642 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 18:41:53.805774 140193455334592 spec.py:349] Evaluating on the test split.
I0307 18:41:55.613813 140193455334592 submission_runner.py:469] Time since start: 62485.37s, 	Step: 151673, 	{'train/accuracy': 0.8900271058082581, 'train/loss': 0.641436755657196, 'validation/accuracy': 0.7471599578857422, 'validation/loss': 1.2307078838348389, 'validation/num_examples': 50000, 'test/accuracy': 0.6216000318527222, 'test/loss': 1.854296326637268, 'test/num_examples': 10000, 'score': 58186.35161614418, 'total_duration': 62485.369346380234, 'accumulated_submission_time': 58186.35161614418, 'accumulated_eval_time': 4268.189233541489, 'accumulated_logging_time': 14.695045232772827}
I0307 18:41:55.771064 140037970700032 logging_writer.py:48] [151673] accumulated_eval_time=4268.19, accumulated_logging_time=14.695, accumulated_submission_time=58186.4, global_step=151673, preemption_count=0, score=58186.4, test/accuracy=0.6216, test/loss=1.8543, test/num_examples=10000, total_duration=62485.4, train/accuracy=0.890027, train/loss=0.641437, validation/accuracy=0.74716, validation/loss=1.23071, validation/num_examples=50000
I0307 18:42:06.285304 140037979092736 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.809105396270752, loss=2.6660354137420654
I0307 18:42:44.454002 140037970700032 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.6173315048217773, loss=2.599512815475464
I0307 18:43:22.921219 140037979092736 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.8133928775787354, loss=2.722712278366089
I0307 18:44:01.010787 140037970700032 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.622373104095459, loss=2.6919357776641846
I0307 18:44:39.402543 140037979092736 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.6232564449310303, loss=2.633329153060913
I0307 18:45:17.604317 140037970700032 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.5654103755950928, loss=2.687056303024292
I0307 18:45:55.765568 140037979092736 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.4815306663513184, loss=2.639923572540283
I0307 18:46:34.157611 140037970700032 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.60504412651062, loss=2.6723484992980957
I0307 18:47:12.533075 140037979092736 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.8397316932678223, loss=2.6550307273864746
I0307 18:47:51.084412 140037970700032 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.6249773502349854, loss=2.652404546737671
2025-03-07 18:47:58.042060: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:48:29.083838 140037979092736 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.6723809242248535, loss=2.6391472816467285
I0307 18:49:07.291281 140037970700032 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.690547466278076, loss=2.7430777549743652
I0307 18:49:45.764908 140037979092736 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.8655784130096436, loss=2.6446051597595215
I0307 18:50:24.096709 140037970700032 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.524428129196167, loss=2.6106011867523193
I0307 18:50:26.003342 140193455334592 spec.py:321] Evaluating on the training split.
I0307 18:50:37.992968 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 18:50:58.699998 140193455334592 spec.py:349] Evaluating on the test split.
I0307 18:51:00.509685 140193455334592 submission_runner.py:469] Time since start: 63030.27s, 	Step: 153006, 	{'train/accuracy': 0.8962651491165161, 'train/loss': 0.6050325036048889, 'validation/accuracy': 0.7501800060272217, 'validation/loss': 1.20828378200531, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8369616270065308, 'test/num_examples': 10000, 'score': 58696.41811990738, 'total_duration': 63030.2652027607, 'accumulated_submission_time': 58696.41811990738, 'accumulated_eval_time': 4302.695431470871, 'accumulated_logging_time': 14.874669551849365}
I0307 18:51:00.675559 140037979092736 logging_writer.py:48] [153006] accumulated_eval_time=4302.7, accumulated_logging_time=14.8747, accumulated_submission_time=58696.4, global_step=153006, preemption_count=0, score=58696.4, test/accuracy=0.6212, test/loss=1.83696, test/num_examples=10000, total_duration=63030.3, train/accuracy=0.896265, train/loss=0.605033, validation/accuracy=0.75018, validation/loss=1.20828, validation/num_examples=50000
I0307 18:51:37.070187 140037970700032 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.9031214714050293, loss=2.7099545001983643
I0307 18:52:15.604363 140037979092736 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.71355938911438, loss=2.7072370052337646
I0307 18:52:54.212342 140037970700032 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.090854167938232, loss=2.6066582202911377
I0307 18:53:32.754249 140037979092736 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.600191116333008, loss=2.644880533218384
I0307 18:54:11.087839 140037970700032 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.5123183727264404, loss=2.6076266765594482
I0307 18:54:49.737970 140037979092736 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.709141969680786, loss=2.6549055576324463
I0307 18:55:28.355838 140037970700032 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.686495304107666, loss=2.6457204818725586
I0307 18:56:07.006279 140037979092736 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.8420920372009277, loss=2.6675586700439453
2025-03-07 18:56:33.750880: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:56:45.399234 140037970700032 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.6882481575012207, loss=2.6291751861572266
I0307 18:57:24.028204 140037979092736 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.7452478408813477, loss=2.7251739501953125
I0307 18:58:02.167775 140037970700032 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.5567920207977295, loss=2.677994728088379
I0307 18:58:40.634115 140037979092736 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.590578317642212, loss=2.6355397701263428
I0307 18:59:18.769795 140037970700032 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.7974398136138916, loss=2.642897605895996
I0307 18:59:30.615893 140193455334592 spec.py:321] Evaluating on the training split.
I0307 18:59:42.053142 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 19:00:00.041934 140193455334592 spec.py:349] Evaluating on the test split.
I0307 19:00:01.846189 140193455334592 submission_runner.py:469] Time since start: 63571.60s, 	Step: 154332, 	{'train/accuracy': 0.8984175324440002, 'train/loss': 0.5849341750144958, 'validation/accuracy': 0.7518599629402161, 'validation/loss': 1.1981676816940308, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.820199728012085, 'test/num_examples': 10000, 'score': 59206.18372344971, 'total_duration': 63571.60168457031, 'accumulated_submission_time': 59206.18372344971, 'accumulated_eval_time': 4333.925555944443, 'accumulated_logging_time': 15.071545362472534}
I0307 19:00:01.940134 140037979092736 logging_writer.py:48] [154332] accumulated_eval_time=4333.93, accumulated_logging_time=15.0715, accumulated_submission_time=59206.2, global_step=154332, preemption_count=0, score=59206.2, test/accuracy=0.6256, test/loss=1.8202, test/num_examples=10000, total_duration=63571.6, train/accuracy=0.898418, train/loss=0.584934, validation/accuracy=0.75186, validation/loss=1.19817, validation/num_examples=50000
I0307 19:00:28.343956 140037970700032 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.921079397201538, loss=2.654658079147339
I0307 19:01:06.938220 140037979092736 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.6634888648986816, loss=2.6598846912384033
I0307 19:01:45.092729 140037970700032 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.506666660308838, loss=2.6518876552581787
I0307 19:02:23.336623 140037979092736 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.464756488800049, loss=2.6356475353240967
I0307 19:03:01.793782 140037970700032 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.5758886337280273, loss=2.6636672019958496
I0307 19:03:39.987786 140037979092736 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.5300049781799316, loss=2.6516966819763184
I0307 19:04:18.591817 140037970700032 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.7622084617614746, loss=2.626056432723999
I0307 19:04:57.286083 140037979092736 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.572383165359497, loss=2.6830456256866455
2025-03-07 19:05:05.653318: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:05:34.364275 140037970700032 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.6597886085510254, loss=2.623598337173462
I0307 19:06:12.967912 140037979092736 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.6116278171539307, loss=2.64263916015625
I0307 19:06:51.274021 140037970700032 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.786489248275757, loss=2.592646598815918
I0307 19:07:29.847258 140037979092736 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.588235378265381, loss=2.5918312072753906
I0307 19:08:08.097478 140037970700032 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.9101552963256836, loss=2.6729516983032227
I0307 19:08:31.935737 140193455334592 spec.py:321] Evaluating on the training split.
I0307 19:08:43.669867 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 19:09:06.561279 140193455334592 spec.py:349] Evaluating on the test split.
I0307 19:09:08.373054 140193455334592 submission_runner.py:469] Time since start: 64118.13s, 	Step: 155663, 	{'train/accuracy': 0.8993741869926453, 'train/loss': 0.5903911590576172, 'validation/accuracy': 0.7518999576568604, 'validation/loss': 1.2005704641342163, 'validation/num_examples': 50000, 'test/accuracy': 0.6236000061035156, 'test/loss': 1.833459496498108, 'test/num_examples': 10000, 'score': 59716.01258230209, 'total_duration': 64118.128554582596, 'accumulated_submission_time': 59716.01258230209, 'accumulated_eval_time': 4370.362705945969, 'accumulated_logging_time': 15.189541578292847}
I0307 19:09:08.463750 140037979092736 logging_writer.py:48] [155663] accumulated_eval_time=4370.36, accumulated_logging_time=15.1895, accumulated_submission_time=59716, global_step=155663, preemption_count=0, score=59716, test/accuracy=0.6236, test/loss=1.83346, test/num_examples=10000, total_duration=64118.1, train/accuracy=0.899374, train/loss=0.590391, validation/accuracy=0.7519, validation/loss=1.20057, validation/num_examples=50000
I0307 19:09:22.942808 140037970700032 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.67755389213562, loss=2.602346181869507
I0307 19:10:01.126752 140037979092736 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.5780274868011475, loss=2.6329803466796875
I0307 19:10:39.348220 140037970700032 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.6012539863586426, loss=2.629976272583008
I0307 19:11:17.542918 140037979092736 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.084560394287109, loss=2.608093500137329
I0307 19:11:56.006091 140037970700032 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.5328269004821777, loss=2.5929036140441895
I0307 19:12:34.462225 140037979092736 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.009002208709717, loss=2.6047093868255615
I0307 19:13:13.104489 140037970700032 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.7987613677978516, loss=2.654721975326538
2025-03-07 19:13:39.054183: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:13:51.938795 140037979092736 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.979696750640869, loss=2.6293864250183105
I0307 19:14:30.574999 140037970700032 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.069755554199219, loss=2.6285154819488525
I0307 19:15:09.163478 140037979092736 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.835313320159912, loss=2.6076748371124268
I0307 19:15:47.485505 140037970700032 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.8777997493743896, loss=2.610959529876709
I0307 19:16:26.026008 140037979092736 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.666069269180298, loss=2.637084722518921
I0307 19:17:04.766482 140037970700032 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.5851707458496094, loss=2.579990863800049
I0307 19:17:38.791290 140193455334592 spec.py:321] Evaluating on the training split.
I0307 19:17:50.372459 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 19:18:11.421607 140193455334592 spec.py:349] Evaluating on the test split.
I0307 19:18:13.254800 140193455334592 submission_runner.py:469] Time since start: 64663.01s, 	Step: 156989, 	{'train/accuracy': 0.9019052982330322, 'train/loss': 0.5845792293548584, 'validation/accuracy': 0.7516999840736389, 'validation/loss': 1.2074382305145264, 'validation/num_examples': 50000, 'test/accuracy': 0.6251000165939331, 'test/loss': 1.8385114669799805, 'test/num_examples': 10000, 'score': 60226.180965662, 'total_duration': 64663.010313272476, 'accumulated_submission_time': 60226.180965662, 'accumulated_eval_time': 4404.826059579849, 'accumulated_logging_time': 15.29827094078064}
I0307 19:18:13.358044 140037979092736 logging_writer.py:48] [156989] accumulated_eval_time=4404.83, accumulated_logging_time=15.2983, accumulated_submission_time=60226.2, global_step=156989, preemption_count=0, score=60226.2, test/accuracy=0.6251, test/loss=1.83851, test/num_examples=10000, total_duration=64663, train/accuracy=0.901905, train/loss=0.584579, validation/accuracy=0.7517, validation/loss=1.20744, validation/num_examples=50000
I0307 19:18:17.978291 140037970700032 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.8550846576690674, loss=2.7228779792785645
I0307 19:18:56.072001 140037979092736 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.801219940185547, loss=2.6446547508239746
I0307 19:19:34.764902 140037970700032 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.935032606124878, loss=2.5748376846313477
I0307 19:20:13.878065 140037979092736 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.758812189102173, loss=2.586454153060913
I0307 19:20:52.442162 140037970700032 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.8463070392608643, loss=2.6738109588623047
I0307 19:21:30.808052 140037979092736 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.200555324554443, loss=2.6697580814361572
I0307 19:22:09.150418 140037970700032 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.110433101654053, loss=2.695073366165161
2025-03-07 19:22:19.074457: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:22:47.585595 140037979092736 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.072503566741943, loss=2.7085537910461426
I0307 19:23:25.900910 140037970700032 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.954684257507324, loss=2.682581901550293
I0307 19:24:03.264088 140037979092736 logging_writer.py:48] [157900] global_step=157900, grad_norm=3.852372884750366, loss=2.587022542953491
I0307 19:24:40.462379 140037970700032 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.681929349899292, loss=2.6382322311401367
I0307 19:25:17.904034 140037979092736 logging_writer.py:48] [158100] global_step=158100, grad_norm=3.7093708515167236, loss=2.5652377605438232
I0307 19:25:55.133109 140037970700032 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.452416181564331, loss=2.5655808448791504
I0307 19:26:32.535693 140037979092736 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.7845075130462646, loss=2.612362861633301
I0307 19:26:43.388689 140193455334592 spec.py:321] Evaluating on the training split.
I0307 19:26:55.898254 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 19:27:15.179215 140193455334592 spec.py:349] Evaluating on the test split.
I0307 19:27:16.994328 140193455334592 submission_runner.py:469] Time since start: 65206.75s, 	Step: 158330, 	{'train/accuracy': 0.9014269709587097, 'train/loss': 0.5914669632911682, 'validation/accuracy': 0.7501199841499329, 'validation/loss': 1.213230013847351, 'validation/num_examples': 50000, 'test/accuracy': 0.6188000440597534, 'test/loss': 1.8594297170639038, 'test/num_examples': 10000, 'score': 60735.99801135063, 'total_duration': 65206.74983716011, 'accumulated_submission_time': 60735.99801135063, 'accumulated_eval_time': 4438.431537151337, 'accumulated_logging_time': 15.468393564224243}
I0307 19:27:17.083463 140037970700032 logging_writer.py:48] [158330] accumulated_eval_time=4438.43, accumulated_logging_time=15.4684, accumulated_submission_time=60736, global_step=158330, preemption_count=0, score=60736, test/accuracy=0.6188, test/loss=1.85943, test/num_examples=10000, total_duration=65206.7, train/accuracy=0.901427, train/loss=0.591467, validation/accuracy=0.75012, validation/loss=1.21323, validation/num_examples=50000
I0307 19:27:44.245910 140037979092736 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.7688026428222656, loss=2.580476760864258
I0307 19:28:22.711116 140037970700032 logging_writer.py:48] [158500] global_step=158500, grad_norm=3.9228789806365967, loss=2.632392406463623
I0307 19:29:00.933373 140037979092736 logging_writer.py:48] [158600] global_step=158600, grad_norm=3.7594285011291504, loss=2.572580575942993
I0307 19:29:39.379172 140037970700032 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.7168474197387695, loss=2.623643398284912
I0307 19:30:17.941013 140037979092736 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.7909889221191406, loss=2.5687084197998047
2025-03-07 19:30:47.748058: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:30:57.350546 140037970700032 logging_writer.py:48] [158900] global_step=158900, grad_norm=3.7810544967651367, loss=2.629598617553711
I0307 19:31:35.451004 140037979092736 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.851628303527832, loss=2.6068809032440186
I0307 19:32:13.402202 140037970700032 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.01258659362793, loss=2.5988478660583496
I0307 19:32:50.913848 140037979092736 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.067460536956787, loss=2.6091575622558594
I0307 19:33:29.086510 140037970700032 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.004824161529541, loss=2.624936580657959
I0307 19:34:06.539658 140037979092736 logging_writer.py:48] [159400] global_step=159400, grad_norm=3.7740979194641113, loss=2.602342367172241
I0307 19:34:44.490692 140037970700032 logging_writer.py:48] [159500] global_step=159500, grad_norm=3.7984695434570312, loss=2.582563877105713
I0307 19:35:23.040560 140037979092736 logging_writer.py:48] [159600] global_step=159600, grad_norm=3.619224786758423, loss=2.588346481323242
I0307 19:35:47.372364 140193455334592 spec.py:321] Evaluating on the training split.
I0307 19:35:59.741754 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 19:36:16.967834 140193455334592 spec.py:349] Evaluating on the test split.
I0307 19:36:18.785830 140193455334592 submission_runner.py:469] Time since start: 65748.54s, 	Step: 159665, 	{'train/accuracy': 0.9070272445678711, 'train/loss': 0.5723469853401184, 'validation/accuracy': 0.7541399598121643, 'validation/loss': 1.203861951828003, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8244379758834839, 'test/num_examples': 10000, 'score': 61246.104724645615, 'total_duration': 65748.54129958153, 'accumulated_submission_time': 61246.104724645615, 'accumulated_eval_time': 4469.844804286957, 'accumulated_logging_time': 15.594008445739746}
I0307 19:36:18.899613 140037970700032 logging_writer.py:48] [159665] accumulated_eval_time=4469.84, accumulated_logging_time=15.594, accumulated_submission_time=61246.1, global_step=159665, preemption_count=0, score=61246.1, test/accuracy=0.6299, test/loss=1.82444, test/num_examples=10000, total_duration=65748.5, train/accuracy=0.907027, train/loss=0.572347, validation/accuracy=0.75414, validation/loss=1.20386, validation/num_examples=50000
I0307 19:36:32.635773 140037979092736 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.8922340869903564, loss=2.620051860809326
I0307 19:37:10.970267 140037970700032 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.9191737174987793, loss=2.681063413619995
I0307 19:37:49.225013 140037979092736 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.6543092727661133, loss=2.561469554901123
I0307 19:38:27.636402 140037970700032 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.854243516921997, loss=2.5763087272644043
I0307 19:39:06.198107 140037979092736 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.801412582397461, loss=2.6062662601470947
2025-03-07 19:39:16.033917: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:39:43.869461 140037970700032 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.051764488220215, loss=2.6513314247131348
I0307 19:40:21.466156 140037979092736 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.7065186500549316, loss=2.546337127685547
I0307 19:40:59.847211 140037970700032 logging_writer.py:48] [160400] global_step=160400, grad_norm=3.910982608795166, loss=2.6548945903778076
I0307 19:41:38.028542 140037979092736 logging_writer.py:48] [160500] global_step=160500, grad_norm=3.829977035522461, loss=2.580441474914551
I0307 19:42:16.264388 140037970700032 logging_writer.py:48] [160600] global_step=160600, grad_norm=3.984588623046875, loss=2.640052080154419
I0307 19:42:54.517463 140037979092736 logging_writer.py:48] [160700] global_step=160700, grad_norm=3.9622910022735596, loss=2.6315126419067383
I0307 19:43:33.417816 140037970700032 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.154013633728027, loss=2.632154941558838
I0307 19:44:11.865794 140037979092736 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.905771017074585, loss=2.6209044456481934
I0307 19:44:48.906601 140193455334592 spec.py:321] Evaluating on the training split.
I0307 19:45:00.850110 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 19:45:19.708319 140193455334592 spec.py:349] Evaluating on the test split.
I0307 19:45:21.505037 140193455334592 submission_runner.py:469] Time since start: 66291.26s, 	Step: 160997, 	{'train/accuracy': 0.9129663109779358, 'train/loss': 0.5551027655601501, 'validation/accuracy': 0.7541199922561646, 'validation/loss': 1.1970536708831787, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.8307443857192993, 'test/num_examples': 10000, 'score': 61755.937997341156, 'total_duration': 66291.26049780846, 'accumulated_submission_time': 61755.937997341156, 'accumulated_eval_time': 4502.443028450012, 'accumulated_logging_time': 15.737564086914062}
I0307 19:45:21.619214 140037970700032 logging_writer.py:48] [160997] accumulated_eval_time=4502.44, accumulated_logging_time=15.7376, accumulated_submission_time=61755.9, global_step=160997, preemption_count=0, score=61755.9, test/accuracy=0.6284, test/loss=1.83074, test/num_examples=10000, total_duration=66291.3, train/accuracy=0.912966, train/loss=0.555103, validation/accuracy=0.75412, validation/loss=1.19705, validation/num_examples=50000
I0307 19:45:23.172113 140037979092736 logging_writer.py:48] [161000] global_step=161000, grad_norm=3.5721237659454346, loss=2.550072431564331
I0307 19:46:01.572641 140037970700032 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.119767665863037, loss=2.6240737438201904
I0307 19:46:40.199379 140037979092736 logging_writer.py:48] [161200] global_step=161200, grad_norm=3.9122366905212402, loss=2.556668758392334
I0307 19:47:18.550799 140037970700032 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.22595739364624, loss=2.5769827365875244
2025-03-07 19:47:47.460727: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:47:57.012909 140037979092736 logging_writer.py:48] [161400] global_step=161400, grad_norm=3.666189670562744, loss=2.522603750228882
I0307 19:48:35.210993 140037970700032 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.021404266357422, loss=2.6600451469421387
I0307 19:49:13.564870 140037979092736 logging_writer.py:48] [161600] global_step=161600, grad_norm=3.928302526473999, loss=2.5427420139312744
I0307 19:49:52.034975 140037970700032 logging_writer.py:48] [161700] global_step=161700, grad_norm=3.8870089054107666, loss=2.635873317718506
I0307 19:50:30.581454 140037979092736 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.879260540008545, loss=2.599491596221924
I0307 19:51:09.105566 140037970700032 logging_writer.py:48] [161900] global_step=161900, grad_norm=3.948021173477173, loss=2.606964111328125
I0307 19:51:47.594178 140037979092736 logging_writer.py:48] [162000] global_step=162000, grad_norm=3.6903460025787354, loss=2.598378896713257
I0307 19:52:26.099238 140037970700032 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.8385097980499268, loss=2.5725767612457275
I0307 19:53:04.601520 140037979092736 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.022848129272461, loss=2.5753767490386963
I0307 19:53:43.208014 140037970700032 logging_writer.py:48] [162300] global_step=162300, grad_norm=3.751891613006592, loss=2.5423073768615723
I0307 19:53:51.773873 140193455334592 spec.py:321] Evaluating on the training split.
I0307 19:54:03.385332 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 19:54:22.401621 140193455334592 spec.py:349] Evaluating on the test split.
I0307 19:54:24.203274 140193455334592 submission_runner.py:469] Time since start: 66833.96s, 	Step: 162323, 	{'train/accuracy': 0.9132055044174194, 'train/loss': 0.5452498197555542, 'validation/accuracy': 0.7566199898719788, 'validation/loss': 1.1877886056900024, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8170104026794434, 'test/num_examples': 10000, 'score': 62265.92627620697, 'total_duration': 66833.95874142647, 'accumulated_submission_time': 62265.92627620697, 'accumulated_eval_time': 4534.872227668762, 'accumulated_logging_time': 15.87644100189209}
I0307 19:54:24.322849 140037979092736 logging_writer.py:48] [162323] accumulated_eval_time=4534.87, accumulated_logging_time=15.8764, accumulated_submission_time=62265.9, global_step=162323, preemption_count=0, score=62265.9, test/accuracy=0.6274, test/loss=1.81701, test/num_examples=10000, total_duration=66834, train/accuracy=0.913206, train/loss=0.54525, validation/accuracy=0.75662, validation/loss=1.18779, validation/num_examples=50000
I0307 19:54:54.182194 140037970700032 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.169789791107178, loss=2.699824094772339
I0307 19:55:32.760050 140037979092736 logging_writer.py:48] [162500] global_step=162500, grad_norm=3.988091230392456, loss=2.6036596298217773
I0307 19:56:11.420103 140037970700032 logging_writer.py:48] [162600] global_step=162600, grad_norm=3.7535886764526367, loss=2.614406108856201
2025-03-07 19:56:22.367680: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:56:48.817074 140037979092736 logging_writer.py:48] [162700] global_step=162700, grad_norm=3.7529969215393066, loss=2.548264980316162
I0307 19:57:26.623082 140037970700032 logging_writer.py:48] [162800] global_step=162800, grad_norm=3.757624864578247, loss=2.562899589538574
I0307 19:58:05.208197 140037979092736 logging_writer.py:48] [162900] global_step=162900, grad_norm=3.955881118774414, loss=2.667654037475586
I0307 19:58:43.655132 140037970700032 logging_writer.py:48] [163000] global_step=163000, grad_norm=3.901254653930664, loss=2.561155319213867
I0307 19:59:21.953074 140037979092736 logging_writer.py:48] [163100] global_step=163100, grad_norm=3.811979055404663, loss=2.6050100326538086
I0307 20:00:00.171153 140037970700032 logging_writer.py:48] [163200] global_step=163200, grad_norm=3.8051979541778564, loss=2.5963165760040283
I0307 20:00:38.479343 140037979092736 logging_writer.py:48] [163300] global_step=163300, grad_norm=3.9831769466400146, loss=2.5845558643341064
I0307 20:01:16.749870 140037970700032 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.214592933654785, loss=2.6174168586730957
I0307 20:01:55.187298 140037979092736 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.609701633453369, loss=2.6837196350097656
I0307 20:02:33.583637 140037970700032 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.294629096984863, loss=2.665980815887451
I0307 20:02:54.292200 140193455334592 spec.py:321] Evaluating on the training split.
I0307 20:03:06.054749 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 20:03:23.893279 140193455334592 spec.py:349] Evaluating on the test split.
I0307 20:03:25.688902 140193455334592 submission_runner.py:469] Time since start: 67375.44s, 	Step: 163655, 	{'train/accuracy': 0.9172512292861938, 'train/loss': 0.5303930640220642, 'validation/accuracy': 0.7572199702262878, 'validation/loss': 1.1877232789993286, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8155666589736938, 'test/num_examples': 10000, 'score': 62775.66317367554, 'total_duration': 67375.44443655014, 'accumulated_submission_time': 62775.66317367554, 'accumulated_eval_time': 4566.268794298172, 'accumulated_logging_time': 16.082500219345093}
I0307 20:03:25.784793 140037979092736 logging_writer.py:48] [163655] accumulated_eval_time=4566.27, accumulated_logging_time=16.0825, accumulated_submission_time=62775.7, global_step=163655, preemption_count=0, score=62775.7, test/accuracy=0.6282, test/loss=1.81557, test/num_examples=10000, total_duration=67375.4, train/accuracy=0.917251, train/loss=0.530393, validation/accuracy=0.75722, validation/loss=1.18772, validation/num_examples=50000
I0307 20:03:43.519887 140037970700032 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.9580750465393066, loss=2.6032683849334717
I0307 20:04:22.069412 140037979092736 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.9399917125701904, loss=2.5760855674743652
2025-03-07 20:04:52.095041: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:05:00.197431 140037970700032 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.847904682159424, loss=2.5750467777252197
I0307 20:05:38.663669 140037979092736 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.986907958984375, loss=2.626577615737915
I0307 20:06:16.860719 140037970700032 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.253903865814209, loss=2.6282763481140137
I0307 20:06:55.116794 140037979092736 logging_writer.py:48] [164200] global_step=164200, grad_norm=3.8713862895965576, loss=2.691821813583374
I0307 20:07:33.512064 140037970700032 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.082669258117676, loss=2.536924362182617
I0307 20:08:12.034965 140037979092736 logging_writer.py:48] [164400] global_step=164400, grad_norm=3.860133647918701, loss=2.6269190311431885
I0307 20:08:50.517901 140037970700032 logging_writer.py:48] [164500] global_step=164500, grad_norm=3.8021318912506104, loss=2.582899570465088
I0307 20:09:28.876755 140037979092736 logging_writer.py:48] [164600] global_step=164600, grad_norm=3.8416435718536377, loss=2.625779390335083
I0307 20:10:06.991508 140037970700032 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.269495964050293, loss=2.6207237243652344
I0307 20:10:45.198473 140037979092736 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.0349249839782715, loss=2.615220546722412
I0307 20:11:23.358789 140037970700032 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.012012958526611, loss=2.577108860015869
I0307 20:11:55.792667 140193455334592 spec.py:321] Evaluating on the training split.
I0307 20:12:07.279767 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 20:12:23.963243 140193455334592 spec.py:349] Evaluating on the test split.
I0307 20:12:25.843046 140193455334592 submission_runner.py:469] Time since start: 67915.60s, 	Step: 164985, 	{'train/accuracy': 0.9249242544174194, 'train/loss': 0.5228111743927002, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.2048935890197754, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.822440505027771, 'test/num_examples': 10000, 'score': 63285.50475859642, 'total_duration': 67915.59853625298, 'accumulated_submission_time': 63285.50475859642, 'accumulated_eval_time': 4596.318994283676, 'accumulated_logging_time': 16.199601650238037}
I0307 20:12:25.992666 140037979092736 logging_writer.py:48] [164985] accumulated_eval_time=4596.32, accumulated_logging_time=16.1996, accumulated_submission_time=63285.5, global_step=164985, preemption_count=0, score=63285.5, test/accuracy=0.6295, test/loss=1.82244, test/num_examples=10000, total_duration=67915.6, train/accuracy=0.924924, train/loss=0.522811, validation/accuracy=0.7551, validation/loss=1.20489, validation/num_examples=50000
I0307 20:12:32.383693 140037970700032 logging_writer.py:48] [165000] global_step=165000, grad_norm=3.82314133644104, loss=2.5413341522216797
I0307 20:13:10.418030 140037979092736 logging_writer.py:48] [165100] global_step=165100, grad_norm=3.9726831912994385, loss=2.6385347843170166
2025-03-07 20:13:22.396131: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:13:48.199158 140037970700032 logging_writer.py:48] [165200] global_step=165200, grad_norm=3.764010190963745, loss=2.5507261753082275
I0307 20:14:26.795822 140037979092736 logging_writer.py:48] [165300] global_step=165300, grad_norm=3.6131505966186523, loss=2.5544416904449463
I0307 20:15:05.336505 140037970700032 logging_writer.py:48] [165400] global_step=165400, grad_norm=3.87637996673584, loss=2.556816339492798
I0307 20:15:44.046894 140037979092736 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.172840118408203, loss=2.580902576446533
I0307 20:16:22.782923 140037970700032 logging_writer.py:48] [165600] global_step=165600, grad_norm=3.7454164028167725, loss=2.5964245796203613
I0307 20:17:01.673979 140037979092736 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.142688274383545, loss=2.5856878757476807
I0307 20:17:40.247596 140037970700032 logging_writer.py:48] [165800] global_step=165800, grad_norm=3.717834711074829, loss=2.564978837966919
I0307 20:18:18.725434 140037979092736 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.7550048828125, loss=2.6144845485687256
I0307 20:18:57.445749 140037970700032 logging_writer.py:48] [166000] global_step=166000, grad_norm=3.8639328479766846, loss=2.6307544708251953
I0307 20:19:36.521066 140037979092736 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.8991239070892334, loss=2.5368096828460693
I0307 20:20:15.026594 140037970700032 logging_writer.py:48] [166200] global_step=166200, grad_norm=3.8546910285949707, loss=2.611459732055664
I0307 20:20:53.414061 140037979092736 logging_writer.py:48] [166300] global_step=166300, grad_norm=3.9943811893463135, loss=2.5340895652770996
I0307 20:20:56.121755 140193455334592 spec.py:321] Evaluating on the training split.
I0307 20:21:08.016705 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 20:21:28.671266 140193455334592 spec.py:349] Evaluating on the test split.
I0307 20:21:30.469269 140193455334592 submission_runner.py:469] Time since start: 68460.22s, 	Step: 166308, 	{'train/accuracy': 0.9310028553009033, 'train/loss': 0.48925212025642395, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.187283992767334, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.807860255241394, 'test/num_examples': 10000, 'score': 63795.45100450516, 'total_duration': 68460.22480273247, 'accumulated_submission_time': 63795.45100450516, 'accumulated_eval_time': 4630.666370630264, 'accumulated_logging_time': 16.386629343032837}
I0307 20:21:30.588209 140037970700032 logging_writer.py:48] [166308] accumulated_eval_time=4630.67, accumulated_logging_time=16.3866, accumulated_submission_time=63795.5, global_step=166308, preemption_count=0, score=63795.5, test/accuracy=0.6314, test/loss=1.80786, test/num_examples=10000, total_duration=68460.2, train/accuracy=0.931003, train/loss=0.489252, validation/accuracy=0.75728, validation/loss=1.18728, validation/num_examples=50000
2025-03-07 20:22:00.089934: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:22:06.890302 140037979092736 logging_writer.py:48] [166400] global_step=166400, grad_norm=3.8791751861572266, loss=2.5607826709747314
I0307 20:22:43.896550 140037970700032 logging_writer.py:48] [166500] global_step=166500, grad_norm=3.897095203399658, loss=2.5668351650238037
I0307 20:23:22.418552 140037979092736 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.017321586608887, loss=2.5751240253448486
I0307 20:24:00.525862 140037970700032 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.014113903045654, loss=2.547699451446533
I0307 20:24:38.993545 140037979092736 logging_writer.py:48] [166800] global_step=166800, grad_norm=3.7918338775634766, loss=2.546736001968384
I0307 20:25:17.298812 140037970700032 logging_writer.py:48] [166900] global_step=166900, grad_norm=3.9475033283233643, loss=2.5274155139923096
I0307 20:25:56.075496 140037979092736 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.145630836486816, loss=2.6116530895233154
I0307 20:26:34.266900 140037970700032 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.182353496551514, loss=2.5899932384490967
I0307 20:27:12.803080 140037979092736 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.061877727508545, loss=2.5931313037872314
I0307 20:27:51.512470 140037970700032 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.8144071102142334, loss=2.5332813262939453
I0307 20:28:30.226821 140037979092736 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.8599369525909424, loss=2.535844326019287
I0307 20:29:08.754374 140037970700032 logging_writer.py:48] [167500] global_step=167500, grad_norm=3.889603614807129, loss=2.574971914291382
I0307 20:29:47.608232 140037979092736 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.075047969818115, loss=2.552518367767334
2025-03-07 20:29:58.923977: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:30:00.600691 140193455334592 spec.py:321] Evaluating on the training split.
I0307 20:30:12.544042 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 20:30:33.210913 140193455334592 spec.py:349] Evaluating on the test split.
I0307 20:30:35.013322 140193455334592 submission_runner.py:469] Time since start: 69004.77s, 	Step: 167635, 	{'train/accuracy': 0.9331951141357422, 'train/loss': 0.4766234755516052, 'validation/accuracy': 0.7599599957466125, 'validation/loss': 1.1863263845443726, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.8089114427566528, 'test/num_examples': 10000, 'score': 64305.28494763374, 'total_duration': 69004.76878523827, 'accumulated_submission_time': 64305.28494763374, 'accumulated_eval_time': 4665.078794956207, 'accumulated_logging_time': 16.537473440170288}
I0307 20:30:35.115524 140037970700032 logging_writer.py:48] [167635] accumulated_eval_time=4665.08, accumulated_logging_time=16.5375, accumulated_submission_time=64305.3, global_step=167635, preemption_count=0, score=64305.3, test/accuracy=0.6341, test/loss=1.80891, test/num_examples=10000, total_duration=69004.8, train/accuracy=0.933195, train/loss=0.476623, validation/accuracy=0.75996, validation/loss=1.18633, validation/num_examples=50000
I0307 20:31:00.655138 140037979092736 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.005481243133545, loss=2.5562920570373535
I0307 20:31:39.263176 140037970700032 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.147704601287842, loss=2.6080479621887207
I0307 20:32:17.669202 140037979092736 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.524913311004639, loss=2.604578733444214
I0307 20:32:55.830842 140037970700032 logging_writer.py:48] [168000] global_step=168000, grad_norm=3.716111183166504, loss=2.527862787246704
I0307 20:33:33.794366 140037979092736 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.109706401824951, loss=2.5890555381774902
I0307 20:34:12.198022 140037970700032 logging_writer.py:48] [168200] global_step=168200, grad_norm=3.8109941482543945, loss=2.5784873962402344
I0307 20:34:50.700611 140037979092736 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.578890800476074, loss=2.6680009365081787
I0307 20:35:29.215591 140037970700032 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.076545238494873, loss=2.5563859939575195
I0307 20:36:07.717334 140037979092736 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.08858060836792, loss=2.6048362255096436
I0307 20:36:46.141192 140037970700032 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.010543346405029, loss=2.5437991619110107
I0307 20:37:24.858132 140037979092736 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.8333373069763184, loss=2.5083675384521484
I0307 20:38:03.417283 140037970700032 logging_writer.py:48] [168800] global_step=168800, grad_norm=3.9074387550354004, loss=2.558363914489746
2025-03-07 20:38:34.302351: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:38:41.810894 140037979092736 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.097837924957275, loss=2.631112575531006
I0307 20:39:05.322602 140193455334592 spec.py:321] Evaluating on the training split.
I0307 20:39:17.116674 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 20:39:36.472226 140193455334592 spec.py:349] Evaluating on the test split.
I0307 20:39:38.210714 140193455334592 submission_runner.py:469] Time since start: 69547.97s, 	Step: 168962, 	{'train/accuracy': 0.9358856678009033, 'train/loss': 0.46779465675354004, 'validation/accuracy': 0.7589600086212158, 'validation/loss': 1.1821156740188599, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.8026305437088013, 'test/num_examples': 10000, 'score': 64815.3239068985, 'total_duration': 69547.96621322632, 'accumulated_submission_time': 64815.3239068985, 'accumulated_eval_time': 4697.9667365550995, 'accumulated_logging_time': 16.665486097335815}
I0307 20:39:38.396360 140037970700032 logging_writer.py:48] [168962] accumulated_eval_time=4697.97, accumulated_logging_time=16.6655, accumulated_submission_time=64815.3, global_step=168962, preemption_count=0, score=64815.3, test/accuracy=0.6367, test/loss=1.80263, test/num_examples=10000, total_duration=69548, train/accuracy=0.935886, train/loss=0.467795, validation/accuracy=0.75896, validation/loss=1.18212, validation/num_examples=50000
I0307 20:39:53.506697 140037979092736 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.094598770141602, loss=2.535689353942871
I0307 20:40:31.643853 140037970700032 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.707752704620361, loss=2.606785535812378
I0307 20:41:09.927154 140037979092736 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.0719313621521, loss=2.6026744842529297
I0307 20:41:48.460507 140037970700032 logging_writer.py:48] [169300] global_step=169300, grad_norm=3.658545970916748, loss=2.507559299468994
I0307 20:42:26.666959 140037979092736 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.112861633300781, loss=2.5675134658813477
I0307 20:43:04.793319 140037970700032 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.134371757507324, loss=2.6035072803497314
I0307 20:43:43.195205 140037979092736 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.224445343017578, loss=2.5880777835845947
I0307 20:44:21.367295 140037970700032 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.171385288238525, loss=2.5712084770202637
I0307 20:44:59.730390 140037979092736 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.338084697723389, loss=2.6114306449890137
I0307 20:45:38.284003 140037970700032 logging_writer.py:48] [169900] global_step=169900, grad_norm=3.881732225418091, loss=2.54584002494812
I0307 20:46:16.925915 140037979092736 logging_writer.py:48] [170000] global_step=170000, grad_norm=3.7871193885803223, loss=2.5387868881225586
I0307 20:46:54.780830 140037970700032 logging_writer.py:48] [170100] global_step=170100, grad_norm=3.872417449951172, loss=2.4931960105895996
2025-03-07 20:47:09.539890: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:47:33.924917 140037979092736 logging_writer.py:48] [170200] global_step=170200, grad_norm=3.9555280208587646, loss=2.5372955799102783
I0307 20:48:08.485827 140193455334592 spec.py:321] Evaluating on the training split.
I0307 20:48:21.042912 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 20:48:40.233316 140193455334592 spec.py:349] Evaluating on the test split.
I0307 20:48:42.050330 140193455334592 submission_runner.py:469] Time since start: 70091.81s, 	Step: 170293, 	{'train/accuracy': 0.9328364133834839, 'train/loss': 0.47571054100990295, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.179027795791626, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.7956260442733765, 'test/num_examples': 10000, 'score': 65325.182131290436, 'total_duration': 70091.8058116436, 'accumulated_submission_time': 65325.182131290436, 'accumulated_eval_time': 4731.531051874161, 'accumulated_logging_time': 16.934987545013428}
I0307 20:48:42.174332 140037970700032 logging_writer.py:48] [170293] accumulated_eval_time=4731.53, accumulated_logging_time=16.935, accumulated_submission_time=65325.2, global_step=170293, preemption_count=0, score=65325.2, test/accuracy=0.6378, test/loss=1.79563, test/num_examples=10000, total_duration=70091.8, train/accuracy=0.932836, train/loss=0.475711, validation/accuracy=0.75988, validation/loss=1.17903, validation/num_examples=50000
I0307 20:48:45.289505 140037979092736 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.143505573272705, loss=2.547780990600586
I0307 20:49:23.564377 140037970700032 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.265138149261475, loss=2.5871834754943848
I0307 20:50:02.356118 140037979092736 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.17738676071167, loss=2.569100856781006
I0307 20:50:41.016043 140037970700032 logging_writer.py:48] [170600] global_step=170600, grad_norm=3.924952507019043, loss=2.5628433227539062
I0307 20:51:19.676775 140037979092736 logging_writer.py:48] [170700] global_step=170700, grad_norm=3.8624989986419678, loss=2.535388469696045
I0307 20:51:58.145485 140037970700032 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.330460548400879, loss=2.6118829250335693
I0307 20:52:36.340043 140037979092736 logging_writer.py:48] [170900] global_step=170900, grad_norm=3.649334669113159, loss=2.530452251434326
I0307 20:53:14.534055 140037970700032 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.109607696533203, loss=2.5907020568847656
I0307 20:53:52.691043 140037979092736 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.651517868041992, loss=2.556678295135498
I0307 20:54:29.854703 140037970700032 logging_writer.py:48] [171200] global_step=171200, grad_norm=3.7837629318237305, loss=2.5146803855895996
I0307 20:55:08.067898 140037979092736 logging_writer.py:48] [171300] global_step=171300, grad_norm=3.889704704284668, loss=2.539600372314453
2025-03-07 20:55:43.583251: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:55:48.229711 140037970700032 logging_writer.py:48] [171400] global_step=171400, grad_norm=3.9718680381774902, loss=2.5254578590393066
I0307 20:56:25.323976 140037979092736 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.158899307250977, loss=2.531599283218384
I0307 20:57:03.390836 140037970700032 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.020759582519531, loss=2.5466244220733643
I0307 20:57:12.164806 140193455334592 spec.py:321] Evaluating on the training split.
I0307 20:57:24.994545 140193455334592 spec.py:333] Evaluating on the validation split.
I0307 20:57:46.140083 140193455334592 spec.py:349] Evaluating on the test split.
I0307 20:57:47.913924 140193455334592 submission_runner.py:469] Time since start: 70637.67s, 	Step: 171624, 	{'train/accuracy': 0.9365034699440002, 'train/loss': 0.46841076016426086, 'validation/accuracy': 0.7608199715614319, 'validation/loss': 1.1785054206848145, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.7958484888076782, 'test/num_examples': 10000, 'score': 65834.9653031826, 'total_duration': 70637.66944098473, 'accumulated_submission_time': 65834.9653031826, 'accumulated_eval_time': 4767.2800278663635, 'accumulated_logging_time': 17.119905471801758}
I0307 20:57:48.041718 140037979092736 logging_writer.py:48] [171624] accumulated_eval_time=4767.28, accumulated_logging_time=17.1199, accumulated_submission_time=65835, global_step=171624, preemption_count=0, score=65835, test/accuracy=0.6412, test/loss=1.79585, test/num_examples=10000, total_duration=70637.7, train/accuracy=0.936503, train/loss=0.468411, validation/accuracy=0.76082, validation/loss=1.17851, validation/num_examples=50000
I0307 20:58:18.350234 140037970700032 logging_writer.py:48] [171700] global_step=171700, grad_norm=3.67067551612854, loss=2.498645067214966
I0307 20:58:56.438124 140037979092736 logging_writer.py:48] [171800] global_step=171800, grad_norm=3.9954311847686768, loss=2.528700828552246
I0307 20:59:34.604236 140037970700032 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.108398914337158, loss=2.5182273387908936
I0307 21:00:13.029828 140037979092736 logging_writer.py:48] [172000] global_step=172000, grad_norm=3.9517018795013428, loss=2.593078851699829
I0307 21:00:51.303750 140037970700032 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.088514804840088, loss=2.5675976276397705
I0307 21:01:29.557854 140037979092736 logging_writer.py:48] [172200] global_step=172200, grad_norm=3.862412214279175, loss=2.5378034114837646
I0307 21:02:07.920867 140037970700032 logging_writer.py:48] [172300] global_step=172300, grad_norm=3.6163275241851807, loss=2.4947938919067383
I0307 21:02:46.393457 140037979092736 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.094338893890381, loss=2.567106246948242
I0307 21:03:25.352594 140037970700032 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.112919330596924, loss=2.5953681468963623
I0307 21:04:03.775113 140037979092736 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.094855308532715, loss=2.5484158992767334
2025-03-07 21:04:17.366069: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:04:42.275469 140037970700032 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.1948466300964355, loss=2.5447373390197754
I0307 21:05:20.861939 140037979092736 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.179544925689697, loss=2.595724105834961
I0307 21:05:59.380198 140037970700032 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.049676418304443, loss=2.5912864208221436
I0307 21:06:18.287212 140037979092736 logging_writer.py:48] [172950] global_step=172950, preemption_count=0, score=66344.4
I0307 21:06:19.824245 140193455334592 submission_runner.py:646] Tuning trial 2/5
I0307 21:06:19.824441 140193455334592 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0307 21:06:19.828420 140193455334592 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010363520123064518, 'train/loss': 6.9122090339660645, 'validation/accuracy': 0.0011399999493733048, 'validation/loss': 6.912380218505859, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.912490367889404, 'test/num_examples': 10000, 'score': 56.77741193771362, 'total_duration': 155.50851035118103, 'accumulated_submission_time': 56.77741193771362, 'accumulated_eval_time': 98.73086762428284, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1376, {'train/accuracy': 0.07517538219690323, 'train/loss': 5.3805365562438965, 'validation/accuracy': 0.06591999530792236, 'validation/loss': 5.463790416717529, 'validation/num_examples': 50000, 'test/accuracy': 0.04620000347495079, 'test/loss': 5.659274578094482, 'test/num_examples': 10000, 'score': 566.6240122318268, 'total_duration': 697.5338888168335, 'accumulated_submission_time': 566.6240122318268, 'accumulated_eval_time': 130.6961612701416, 'accumulated_logging_time': 0.050811052322387695, 'global_step': 1376, 'preemption_count': 0}), (2727, {'train/accuracy': 0.17757493257522583, 'train/loss': 4.3514909744262695, 'validation/accuracy': 0.15587998926639557, 'validation/loss': 4.4895405769348145, 'validation/num_examples': 50000, 'test/accuracy': 0.11400000751018524, 'test/loss': 4.868528366088867, 'test/num_examples': 10000, 'score': 1076.6900205612183, 'total_duration': 1242.7348968982697, 'accumulated_submission_time': 1076.6900205612183, 'accumulated_eval_time': 165.64567732810974, 'accumulated_logging_time': 0.09013152122497559, 'global_step': 2727, 'preemption_count': 0}), (4069, {'train/accuracy': 0.2766461968421936, 'train/loss': 3.614423990249634, 'validation/accuracy': 0.24368000030517578, 'validation/loss': 3.810802698135376, 'validation/num_examples': 50000, 'test/accuracy': 0.18240000307559967, 'test/loss': 4.323792457580566, 'test/num_examples': 10000, 'score': 1586.6931998729706, 'total_duration': 1791.3191423416138, 'accumulated_submission_time': 1586.6931998729706, 'accumulated_eval_time': 204.0476369857788, 'accumulated_logging_time': 0.13822722434997559, 'global_step': 4069, 'preemption_count': 0}), (5419, {'train/accuracy': 0.3655931055545807, 'train/loss': 3.089827299118042, 'validation/accuracy': 0.3305799961090088, 'validation/loss': 3.288238763809204, 'validation/num_examples': 50000, 'test/accuracy': 0.248400017619133, 'test/loss': 3.8467321395874023, 'test/num_examples': 10000, 'score': 2096.669495820999, 'total_duration': 2336.029908657074, 'accumulated_submission_time': 2096.669495820999, 'accumulated_eval_time': 238.6324110031128, 'accumulated_logging_time': 0.15430474281311035, 'global_step': 5419, 'preemption_count': 0}), (6763, {'train/accuracy': 0.4204599857330322, 'train/loss': 2.8091726303100586, 'validation/accuracy': 0.38106000423431396, 'validation/loss': 2.999786138534546, 'validation/num_examples': 50000, 'test/accuracy': 0.2891000211238861, 'test/loss': 3.566889524459839, 'test/num_examples': 10000, 'score': 2606.5659952163696, 'total_duration': 2882.425507545471, 'accumulated_submission_time': 2606.5659952163696, 'accumulated_eval_time': 274.9814684391022, 'accumulated_logging_time': 0.17117547988891602, 'global_step': 6763, 'preemption_count': 0}), (8113, {'train/accuracy': 0.48084741830825806, 'train/loss': 2.4457743167877197, 'validation/accuracy': 0.43751999735832214, 'validation/loss': 2.665273666381836, 'validation/num_examples': 50000, 'test/accuracy': 0.3246000111103058, 'test/loss': 3.3327462673187256, 'test/num_examples': 10000, 'score': 3116.6582701206207, 'total_duration': 3429.259141921997, 'accumulated_submission_time': 3116.6582701206207, 'accumulated_eval_time': 311.5496428012848, 'accumulated_logging_time': 0.21140384674072266, 'global_step': 8113, 'preemption_count': 0}), (9460, {'train/accuracy': 0.5285794138908386, 'train/loss': 2.2446634769439697, 'validation/accuracy': 0.48409998416900635, 'validation/loss': 2.468783140182495, 'validation/num_examples': 50000, 'test/accuracy': 0.3736000061035156, 'test/loss': 3.1164748668670654, 'test/num_examples': 10000, 'score': 3626.7777016162872, 'total_duration': 3977.8514297008514, 'accumulated_submission_time': 3626.7777016162872, 'accumulated_eval_time': 349.83240580558777, 'accumulated_logging_time': 0.26887059211730957, 'global_step': 9460, 'preemption_count': 0}), (10809, {'train/accuracy': 0.5678810477256775, 'train/loss': 2.0701522827148438, 'validation/accuracy': 0.5170199871063232, 'validation/loss': 2.3050551414489746, 'validation/num_examples': 50000, 'test/accuracy': 0.4020000100135803, 'test/loss': 2.9642550945281982, 'test/num_examples': 10000, 'score': 4136.579527139664, 'total_duration': 4526.2758820056915, 'accumulated_submission_time': 4136.579527139664, 'accumulated_eval_time': 388.20830941200256, 'accumulated_logging_time': 0.3499281406402588, 'global_step': 10809, 'preemption_count': 0}), (12154, {'train/accuracy': 0.5869140625, 'train/loss': 1.9589930772781372, 'validation/accuracy': 0.5384199619293213, 'validation/loss': 2.18542218208313, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.8328115940093994, 'test/num_examples': 10000, 'score': 4646.768032312393, 'total_duration': 5075.651927471161, 'accumulated_submission_time': 4646.768032312393, 'accumulated_eval_time': 427.195335149765, 'accumulated_logging_time': 0.4144785404205322, 'global_step': 12154, 'preemption_count': 0}), (13364, {'train/accuracy': 0.6243223547935486, 'train/loss': 1.831497311592102, 'validation/accuracy': 0.5644599795341492, 'validation/loss': 2.0840139389038086, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.7294445037841797, 'test/num_examples': 10000, 'score': 5156.835231304169, 'total_duration': 5628.732272863388, 'accumulated_submission_time': 5156.835231304169, 'accumulated_eval_time': 470.03906059265137, 'accumulated_logging_time': 0.4597651958465576, 'global_step': 13364, 'preemption_count': 0}), (14519, {'train/accuracy': 0.6479790806770325, 'train/loss': 1.6753435134887695, 'validation/accuracy': 0.5806999802589417, 'validation/loss': 1.978210687637329, 'validation/num_examples': 50000, 'test/accuracy': 0.45730000734329224, 'test/loss': 2.624202013015747, 'test/num_examples': 10000, 'score': 5666.841965913773, 'total_duration': 6177.551301240921, 'accumulated_submission_time': 5666.841965913773, 'accumulated_eval_time': 508.65633749961853, 'accumulated_logging_time': 0.5429282188415527, 'global_step': 14519, 'preemption_count': 0}), (15688, {'train/accuracy': 0.6679089665412903, 'train/loss': 1.5840985774993896, 'validation/accuracy': 0.5860399603843689, 'validation/loss': 1.958055853843689, 'validation/num_examples': 50000, 'test/accuracy': 0.4653000235557556, 'test/loss': 2.608912706375122, 'test/num_examples': 10000, 'score': 6177.019730329514, 'total_duration': 6726.601873874664, 'accumulated_submission_time': 6177.019730329514, 'accumulated_eval_time': 547.355060338974, 'accumulated_logging_time': 0.5853581428527832, 'global_step': 15688, 'preemption_count': 0}), (17001, {'train/accuracy': 0.6952925324440002, 'train/loss': 1.5021928548812866, 'validation/accuracy': 0.6085799932479858, 'validation/loss': 1.8874375820159912, 'validation/num_examples': 50000, 'test/accuracy': 0.4807000160217285, 'test/loss': 2.53777813911438, 'test/num_examples': 10000, 'score': 6686.868481636047, 'total_duration': 7273.683918714523, 'accumulated_submission_time': 6686.868481636047, 'accumulated_eval_time': 584.4060409069061, 'accumulated_logging_time': 0.6412758827209473, 'global_step': 17001, 'preemption_count': 0}), (18231, {'train/accuracy': 0.6957708597183228, 'train/loss': 1.5066120624542236, 'validation/accuracy': 0.6098200082778931, 'validation/loss': 1.8729736804962158, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.5313045978546143, 'test/num_examples': 10000, 'score': 7196.698030948639, 'total_duration': 7821.830721855164, 'accumulated_submission_time': 7196.698030948639, 'accumulated_eval_time': 622.546112537384, 'accumulated_logging_time': 0.6942312717437744, 'global_step': 18231, 'preemption_count': 0}), (19561, {'train/accuracy': 0.7120735049247742, 'train/loss': 1.403428316116333, 'validation/accuracy': 0.6147199869155884, 'validation/loss': 1.822716236114502, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.4957051277160645, 'test/num_examples': 10000, 'score': 7706.595965862274, 'total_duration': 8370.44462132454, 'accumulated_submission_time': 7706.595965862274, 'accumulated_eval_time': 661.0459854602814, 'accumulated_logging_time': 0.7781500816345215, 'global_step': 19561, 'preemption_count': 0}), (20850, {'train/accuracy': 0.7258848547935486, 'train/loss': 1.3886221647262573, 'validation/accuracy': 0.6249200105667114, 'validation/loss': 1.8135240077972412, 'validation/num_examples': 50000, 'test/accuracy': 0.49650001525878906, 'test/loss': 2.465162515640259, 'test/num_examples': 10000, 'score': 8216.687442302704, 'total_duration': 8917.782804250717, 'accumulated_submission_time': 8216.687442302704, 'accumulated_eval_time': 698.1194541454315, 'accumulated_logging_time': 0.8286125659942627, 'global_step': 20850, 'preemption_count': 0}), (22167, {'train/accuracy': 0.725605845451355, 'train/loss': 1.304843544960022, 'validation/accuracy': 0.6233199834823608, 'validation/loss': 1.7497003078460693, 'validation/num_examples': 50000, 'test/accuracy': 0.5016000270843506, 'test/loss': 2.394624710083008, 'test/num_examples': 10000, 'score': 8726.709604501724, 'total_duration': 9468.257493019104, 'accumulated_submission_time': 8726.709604501724, 'accumulated_eval_time': 738.3583631515503, 'accumulated_logging_time': 0.8751790523529053, 'global_step': 22167, 'preemption_count': 0}), (23498, {'train/accuracy': 0.7215202450752258, 'train/loss': 1.3562525510787964, 'validation/accuracy': 0.6268199682235718, 'validation/loss': 1.7633411884307861, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.435206413269043, 'test/num_examples': 10000, 'score': 9236.749849796295, 'total_duration': 10020.192776679993, 'accumulated_submission_time': 9236.749849796295, 'accumulated_eval_time': 779.9462132453918, 'accumulated_logging_time': 0.990614652633667, 'global_step': 23498, 'preemption_count': 0}), (24828, {'train/accuracy': 0.7252271771430969, 'train/loss': 1.3312427997589111, 'validation/accuracy': 0.6343199610710144, 'validation/loss': 1.7292760610580444, 'validation/num_examples': 50000, 'test/accuracy': 0.5058000087738037, 'test/loss': 2.3971753120422363, 'test/num_examples': 10000, 'score': 9746.8926486969, 'total_duration': 10568.168626785278, 'accumulated_submission_time': 9746.8926486969, 'accumulated_eval_time': 817.5812244415283, 'accumulated_logging_time': 1.0601882934570312, 'global_step': 24828, 'preemption_count': 0}), (26147, {'train/accuracy': 0.7312061190605164, 'train/loss': 1.307316541671753, 'validation/accuracy': 0.6405199766159058, 'validation/loss': 1.6915374994277954, 'validation/num_examples': 50000, 'test/accuracy': 0.5128000378608704, 'test/loss': 2.3624355792999268, 'test/num_examples': 10000, 'score': 10256.627957105637, 'total_duration': 11117.721102952957, 'accumulated_submission_time': 10256.627957105637, 'accumulated_eval_time': 857.1091570854187, 'accumulated_logging_time': 1.2170064449310303, 'global_step': 26147, 'preemption_count': 0}), (27479, {'train/accuracy': 0.7090640664100647, 'train/loss': 1.4385793209075928, 'validation/accuracy': 0.6214199662208557, 'validation/loss': 1.8097518682479858, 'validation/num_examples': 50000, 'test/accuracy': 0.4993000328540802, 'test/loss': 2.4568543434143066, 'test/num_examples': 10000, 'score': 10766.312444925308, 'total_duration': 11670.174454450607, 'accumulated_submission_time': 10766.312444925308, 'accumulated_eval_time': 899.324554681778, 'accumulated_logging_time': 1.6392345428466797, 'global_step': 27479, 'preemption_count': 0}), (28821, {'train/accuracy': 0.7260442972183228, 'train/loss': 1.318651795387268, 'validation/accuracy': 0.6404199600219727, 'validation/loss': 1.6896063089370728, 'validation/num_examples': 50000, 'test/accuracy': 0.5202000141143799, 'test/loss': 2.336705446243286, 'test/num_examples': 10000, 'score': 11276.162848711014, 'total_duration': 12219.47790980339, 'accumulated_submission_time': 11276.162848711014, 'accumulated_eval_time': 938.5193445682526, 'accumulated_logging_time': 1.7633538246154785, 'global_step': 28821, 'preemption_count': 0}), (30154, {'train/accuracy': 0.724031388759613, 'train/loss': 1.3208426237106323, 'validation/accuracy': 0.6448799967765808, 'validation/loss': 1.6764817237854004, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.330254316329956, 'test/num_examples': 10000, 'score': 11785.958069086075, 'total_duration': 12768.51333475113, 'accumulated_submission_time': 11785.958069086075, 'accumulated_eval_time': 977.4997777938843, 'accumulated_logging_time': 1.8911731243133545, 'global_step': 30154, 'preemption_count': 0}), (31486, {'train/accuracy': 0.7248883843421936, 'train/loss': 1.3461346626281738, 'validation/accuracy': 0.6422199606895447, 'validation/loss': 1.7050862312316895, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.3555595874786377, 'test/num_examples': 10000, 'score': 12295.776060342789, 'total_duration': 13316.168333530426, 'accumulated_submission_time': 12295.776060342789, 'accumulated_eval_time': 1015.0281777381897, 'accumulated_logging_time': 2.0690689086914062, 'global_step': 31486, 'preemption_count': 0}), (32818, {'train/accuracy': 0.7279974222183228, 'train/loss': 1.2941195964813232, 'validation/accuracy': 0.6498599648475647, 'validation/loss': 1.6550451517105103, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.2909834384918213, 'test/num_examples': 10000, 'score': 12805.667831659317, 'total_duration': 13864.63993859291, 'accumulated_submission_time': 12805.667831659317, 'accumulated_eval_time': 1053.274854183197, 'accumulated_logging_time': 2.2729084491729736, 'global_step': 32818, 'preemption_count': 0}), (34151, {'train/accuracy': 0.7143853306770325, 'train/loss': 1.3342565298080444, 'validation/accuracy': 0.6410999894142151, 'validation/loss': 1.6608558893203735, 'validation/num_examples': 50000, 'test/accuracy': 0.5139999985694885, 'test/loss': 2.3487250804901123, 'test/num_examples': 10000, 'score': 13315.745752573013, 'total_duration': 14414.567600011826, 'accumulated_submission_time': 13315.745752573013, 'accumulated_eval_time': 1092.8596727848053, 'accumulated_logging_time': 2.4062435626983643, 'global_step': 34151, 'preemption_count': 0}), (35483, {'train/accuracy': 0.7329400181770325, 'train/loss': 1.2845704555511475, 'validation/accuracy': 0.6553599834442139, 'validation/loss': 1.6216375827789307, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.2741339206695557, 'test/num_examples': 10000, 'score': 13825.563044309616, 'total_duration': 14964.490134239197, 'accumulated_submission_time': 13825.563044309616, 'accumulated_eval_time': 1132.7140471935272, 'accumulated_logging_time': 2.5275866985321045, 'global_step': 35483, 'preemption_count': 0}), (36818, {'train/accuracy': 0.7346938848495483, 'train/loss': 1.2756190299987793, 'validation/accuracy': 0.661080002784729, 'validation/loss': 1.600752830505371, 'validation/num_examples': 50000, 'test/accuracy': 0.5347000360488892, 'test/loss': 2.245483160018921, 'test/num_examples': 10000, 'score': 14335.594117879868, 'total_duration': 15510.710866689682, 'accumulated_submission_time': 14335.594117879868, 'accumulated_eval_time': 1168.669767856598, 'accumulated_logging_time': 2.630326747894287, 'global_step': 36818, 'preemption_count': 0}), (38150, {'train/accuracy': 0.725605845451355, 'train/loss': 1.2949401140213013, 'validation/accuracy': 0.6539599895477295, 'validation/loss': 1.6193041801452637, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.2704482078552246, 'test/num_examples': 10000, 'score': 14845.381632089615, 'total_duration': 16056.14630317688, 'accumulated_submission_time': 14845.381632089615, 'accumulated_eval_time': 1204.1033735275269, 'accumulated_logging_time': 2.712885856628418, 'global_step': 38150, 'preemption_count': 0}), (39483, {'train/accuracy': 0.7364675998687744, 'train/loss': 1.2735756635665894, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.6118800640106201, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.2707228660583496, 'test/num_examples': 10000, 'score': 15355.177911043167, 'total_duration': 16600.34713125229, 'accumulated_submission_time': 15355.177911043167, 'accumulated_eval_time': 1238.240047454834, 'accumulated_logging_time': 2.849548578262329, 'global_step': 39483, 'preemption_count': 0}), (40814, {'train/accuracy': 0.729890763759613, 'train/loss': 1.2895914316177368, 'validation/accuracy': 0.6579399704933167, 'validation/loss': 1.6098980903625488, 'validation/num_examples': 50000, 'test/accuracy': 0.5275000333786011, 'test/loss': 2.27048921585083, 'test/num_examples': 10000, 'score': 15864.931150913239, 'total_duration': 17145.14592242241, 'accumulated_submission_time': 15864.931150913239, 'accumulated_eval_time': 1272.970650434494, 'accumulated_logging_time': 3.029849052429199, 'global_step': 40814, 'preemption_count': 0}), (42148, {'train/accuracy': 0.7209422588348389, 'train/loss': 1.3132954835891724, 'validation/accuracy': 0.6492599844932556, 'validation/loss': 1.6357648372650146, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.3070361614227295, 'test/num_examples': 10000, 'score': 16374.926724910736, 'total_duration': 17693.776051282883, 'accumulated_submission_time': 16374.926724910736, 'accumulated_eval_time': 1311.3760063648224, 'accumulated_logging_time': 3.126911163330078, 'global_step': 42148, 'preemption_count': 0}), (43481, {'train/accuracy': 0.7349330186843872, 'train/loss': 1.2756978273391724, 'validation/accuracy': 0.659280002117157, 'validation/loss': 1.6124171018600464, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.2553226947784424, 'test/num_examples': 10000, 'score': 16885.014603853226, 'total_duration': 18239.442207098007, 'accumulated_submission_time': 16885.014603853226, 'accumulated_eval_time': 1346.722405910492, 'accumulated_logging_time': 3.2275619506835938, 'global_step': 43481, 'preemption_count': 0}), (44815, {'train/accuracy': 0.7306082248687744, 'train/loss': 1.2841325998306274, 'validation/accuracy': 0.6571599841117859, 'validation/loss': 1.601897954940796, 'validation/num_examples': 50000, 'test/accuracy': 0.5321000218391418, 'test/loss': 2.2631285190582275, 'test/num_examples': 10000, 'score': 17395.0259206295, 'total_duration': 18785.01003599167, 'accumulated_submission_time': 17395.0259206295, 'accumulated_eval_time': 1382.0389544963837, 'accumulated_logging_time': 3.3354320526123047, 'global_step': 44815, 'preemption_count': 0}), (46145, {'train/accuracy': 0.7375438213348389, 'train/loss': 1.250101923942566, 'validation/accuracy': 0.6602199673652649, 'validation/loss': 1.5915335416793823, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.260349988937378, 'test/num_examples': 10000, 'score': 17904.98149585724, 'total_duration': 19333.592064142227, 'accumulated_submission_time': 17904.98149585724, 'accumulated_eval_time': 1420.4119472503662, 'accumulated_logging_time': 3.456692695617676, 'global_step': 46145, 'preemption_count': 0}), (47477, {'train/accuracy': 0.7295519709587097, 'train/loss': 1.3188793659210205, 'validation/accuracy': 0.6567999720573425, 'validation/loss': 1.6338763236999512, 'validation/num_examples': 50000, 'test/accuracy': 0.5390000343322754, 'test/loss': 2.2689547538757324, 'test/num_examples': 10000, 'score': 18414.79394221306, 'total_duration': 19883.543442964554, 'accumulated_submission_time': 18414.79394221306, 'accumulated_eval_time': 1460.2449977397919, 'accumulated_logging_time': 3.631133556365967, 'global_step': 47477, 'preemption_count': 0}), (48808, {'train/accuracy': 0.734793484210968, 'train/loss': 1.2914808988571167, 'validation/accuracy': 0.666159987449646, 'validation/loss': 1.5947694778442383, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.2721590995788574, 'test/num_examples': 10000, 'score': 18924.725657463074, 'total_duration': 20432.363762140274, 'accumulated_submission_time': 18924.725657463074, 'accumulated_eval_time': 1498.883390903473, 'accumulated_logging_time': 3.7491116523742676, 'global_step': 48808, 'preemption_count': 0}), (50141, {'train/accuracy': 0.7444993257522583, 'train/loss': 1.2479438781738281, 'validation/accuracy': 0.6702799797058105, 'validation/loss': 1.5696496963500977, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.229975700378418, 'test/num_examples': 10000, 'score': 19434.664959430695, 'total_duration': 20981.791118860245, 'accumulated_submission_time': 19434.664959430695, 'accumulated_eval_time': 1538.116448879242, 'accumulated_logging_time': 3.871286630630493, 'global_step': 50141, 'preemption_count': 0}), (51475, {'train/accuracy': 0.7254065275192261, 'train/loss': 1.2962501049041748, 'validation/accuracy': 0.6546599864959717, 'validation/loss': 1.6282118558883667, 'validation/num_examples': 50000, 'test/accuracy': 0.5351000428199768, 'test/loss': 2.254657506942749, 'test/num_examples': 10000, 'score': 19944.81085205078, 'total_duration': 21530.934900045395, 'accumulated_submission_time': 19944.81085205078, 'accumulated_eval_time': 1576.8660669326782, 'accumulated_logging_time': 3.982649564743042, 'global_step': 51475, 'preemption_count': 0}), (52804, {'train/accuracy': 0.7359693646430969, 'train/loss': 1.2672291994094849, 'validation/accuracy': 0.6695799827575684, 'validation/loss': 1.571102499961853, 'validation/num_examples': 50000, 'test/accuracy': 0.5426000356674194, 'test/loss': 2.2252392768859863, 'test/num_examples': 10000, 'score': 20454.600573539734, 'total_duration': 22079.630780935287, 'accumulated_submission_time': 20454.600573539734, 'accumulated_eval_time': 1615.5383932590485, 'accumulated_logging_time': 4.079102516174316, 'global_step': 52804, 'preemption_count': 0}), (54133, {'train/accuracy': 0.7433235049247742, 'train/loss': 1.1952619552612305, 'validation/accuracy': 0.6702600121498108, 'validation/loss': 1.5156363248825073, 'validation/num_examples': 50000, 'test/accuracy': 0.5459000468254089, 'test/loss': 2.177372455596924, 'test/num_examples': 10000, 'score': 20964.505282878876, 'total_duration': 22626.25130057335, 'accumulated_submission_time': 20964.505282878876, 'accumulated_eval_time': 1651.987048149109, 'accumulated_logging_time': 4.212485313415527, 'global_step': 54133, 'preemption_count': 0}), (55463, {'train/accuracy': 0.7479273080825806, 'train/loss': 1.2544888257980347, 'validation/accuracy': 0.6725999712944031, 'validation/loss': 1.5842063426971436, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.22145676612854, 'test/num_examples': 10000, 'score': 21474.588220357895, 'total_duration': 23172.238622188568, 'accumulated_submission_time': 21474.588220357895, 'accumulated_eval_time': 1687.6545338630676, 'accumulated_logging_time': 4.3140575885772705, 'global_step': 55463, 'preemption_count': 0}), (56791, {'train/accuracy': 0.7442004084587097, 'train/loss': 1.228479027748108, 'validation/accuracy': 0.670520007610321, 'validation/loss': 1.559903860092163, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 2.1784212589263916, 'test/num_examples': 10000, 'score': 21984.502583265305, 'total_duration': 23717.57048678398, 'accumulated_submission_time': 21984.502583265305, 'accumulated_eval_time': 1722.7379961013794, 'accumulated_logging_time': 4.507081508636475, 'global_step': 56791, 'preemption_count': 0}), (58121, {'train/accuracy': 0.744160532951355, 'train/loss': 1.2386058568954468, 'validation/accuracy': 0.6704199910163879, 'validation/loss': 1.561078667640686, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.18866229057312, 'test/num_examples': 10000, 'score': 22494.334429740906, 'total_duration': 24266.79279279709, 'accumulated_submission_time': 22494.334429740906, 'accumulated_eval_time': 1761.809242963791, 'accumulated_logging_time': 4.6822710037231445, 'global_step': 58121, 'preemption_count': 0}), (59454, {'train/accuracy': 0.7482461333274841, 'train/loss': 1.2045261859893799, 'validation/accuracy': 0.6738599538803101, 'validation/loss': 1.5239259004592896, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.182633638381958, 'test/num_examples': 10000, 'score': 23004.292164087296, 'total_duration': 24815.439249277115, 'accumulated_submission_time': 23004.292164087296, 'accumulated_eval_time': 1800.2163240909576, 'accumulated_logging_time': 4.818809509277344, 'global_step': 59454, 'preemption_count': 0}), (60791, {'train/accuracy': 0.7326610088348389, 'train/loss': 1.3098770380020142, 'validation/accuracy': 0.6614999771118164, 'validation/loss': 1.6309702396392822, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.2892611026763916, 'test/num_examples': 10000, 'score': 23514.367705345154, 'total_duration': 25364.448014497757, 'accumulated_submission_time': 23514.367705345154, 'accumulated_eval_time': 1838.9078783988953, 'accumulated_logging_time': 4.915984630584717, 'global_step': 60791, 'preemption_count': 0}), (62128, {'train/accuracy': 0.7435227632522583, 'train/loss': 1.2181622982025146, 'validation/accuracy': 0.6756199598312378, 'validation/loss': 1.5341556072235107, 'validation/num_examples': 50000, 'test/accuracy': 0.5427000522613525, 'test/loss': 2.218334674835205, 'test/num_examples': 10000, 'score': 24024.429008722305, 'total_duration': 25912.96332359314, 'accumulated_submission_time': 24024.429008722305, 'accumulated_eval_time': 1877.1374099254608, 'accumulated_logging_time': 4.993806600570679, 'global_step': 62128, 'preemption_count': 0}), (63461, {'train/accuracy': 0.7436822056770325, 'train/loss': 1.2419458627700806, 'validation/accuracy': 0.6692000031471252, 'validation/loss': 1.5672487020492554, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.224569320678711, 'test/num_examples': 10000, 'score': 24534.38946390152, 'total_duration': 26460.321612596512, 'accumulated_submission_time': 24534.38946390152, 'accumulated_eval_time': 1914.329824924469, 'accumulated_logging_time': 5.05517840385437, 'global_step': 63461, 'preemption_count': 0}), (64792, {'train/accuracy': 0.7454360723495483, 'train/loss': 1.222221851348877, 'validation/accuracy': 0.6723999977111816, 'validation/loss': 1.5486427545547485, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.189523458480835, 'test/num_examples': 10000, 'score': 25044.406069993973, 'total_duration': 27007.87869477272, 'accumulated_submission_time': 25044.406069993973, 'accumulated_eval_time': 1951.6182436943054, 'accumulated_logging_time': 5.16471791267395, 'global_step': 64792, 'preemption_count': 0}), (66125, {'train/accuracy': 0.7592275142669678, 'train/loss': 1.17339026927948, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.501193881034851, 'validation/num_examples': 50000, 'test/accuracy': 0.5594000220298767, 'test/loss': 2.141183376312256, 'test/num_examples': 10000, 'score': 25554.12162542343, 'total_duration': 27555.7465801239, 'accumulated_submission_time': 25554.12162542343, 'accumulated_eval_time': 1989.478102684021, 'accumulated_logging_time': 5.314172983169556, 'global_step': 66125, 'preemption_count': 0}), (67475, {'train/accuracy': 0.7492426633834839, 'train/loss': 1.1810312271118164, 'validation/accuracy': 0.6776399612426758, 'validation/loss': 1.5078225135803223, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.161421537399292, 'test/num_examples': 10000, 'score': 26064.113745689392, 'total_duration': 28100.806893110275, 'accumulated_submission_time': 26064.113745689392, 'accumulated_eval_time': 2024.2831251621246, 'accumulated_logging_time': 5.428308725357056, 'global_step': 67475, 'preemption_count': 0}), (68807, {'train/accuracy': 0.7495814561843872, 'train/loss': 1.1970422267913818, 'validation/accuracy': 0.6803999543190002, 'validation/loss': 1.5167268514633179, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.1904869079589844, 'test/num_examples': 10000, 'score': 26574.154461622238, 'total_duration': 28643.99741792679, 'accumulated_submission_time': 26574.154461622238, 'accumulated_eval_time': 2057.117976665497, 'accumulated_logging_time': 5.598376989364624, 'global_step': 68807, 'preemption_count': 0}), (70139, {'train/accuracy': 0.7550621628761292, 'train/loss': 1.2107903957366943, 'validation/accuracy': 0.6820600032806396, 'validation/loss': 1.5357111692428589, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.1894917488098145, 'test/num_examples': 10000, 'score': 27084.08220767975, 'total_duration': 29193.16672682762, 'accumulated_submission_time': 27084.08220767975, 'accumulated_eval_time': 2096.119467496872, 'accumulated_logging_time': 5.691593408584595, 'global_step': 70139, 'preemption_count': 0}), (71477, {'train/accuracy': 0.7667809128761292, 'train/loss': 1.1389026641845703, 'validation/accuracy': 0.6753199696540833, 'validation/loss': 1.5427058935165405, 'validation/num_examples': 50000, 'test/accuracy': 0.5457000136375427, 'test/loss': 2.210599184036255, 'test/num_examples': 10000, 'score': 27593.935863018036, 'total_duration': 29735.614179372787, 'accumulated_submission_time': 27593.935863018036, 'accumulated_eval_time': 2128.421490430832, 'accumulated_logging_time': 5.836561441421509, 'global_step': 71477, 'preemption_count': 0}), (72813, {'train/accuracy': 0.7913145422935486, 'train/loss': 1.0311424732208252, 'validation/accuracy': 0.679919958114624, 'validation/loss': 1.5190449953079224, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.1426339149475098, 'test/num_examples': 10000, 'score': 28103.840924024582, 'total_duration': 30281.71915125847, 'accumulated_submission_time': 28103.840924024582, 'accumulated_eval_time': 2164.3138427734375, 'accumulated_logging_time': 5.9986889362335205, 'global_step': 72813, 'preemption_count': 0}), (74146, {'train/accuracy': 0.7980309128761292, 'train/loss': 1.0135833024978638, 'validation/accuracy': 0.6847800016403198, 'validation/loss': 1.4872812032699585, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 2.140796422958374, 'test/num_examples': 10000, 'score': 28613.878880262375, 'total_duration': 30830.143275022507, 'accumulated_submission_time': 28613.878880262375, 'accumulated_eval_time': 2202.457393884659, 'accumulated_logging_time': 6.097788333892822, 'global_step': 74146, 'preemption_count': 0}), (75486, {'train/accuracy': 0.7831034660339355, 'train/loss': 1.08640718460083, 'validation/accuracy': 0.6774199604988098, 'validation/loss': 1.5339094400405884, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.1947720050811768, 'test/num_examples': 10000, 'score': 29123.67890906334, 'total_duration': 31378.141866207123, 'accumulated_submission_time': 29123.67890906334, 'accumulated_eval_time': 2240.36994600296, 'accumulated_logging_time': 6.240447759628296, 'global_step': 75486, 'preemption_count': 0}), (76835, {'train/accuracy': 0.7745735049247742, 'train/loss': 1.1141470670700073, 'validation/accuracy': 0.6737799644470215, 'validation/loss': 1.5497111082077026, 'validation/num_examples': 50000, 'test/accuracy': 0.5433000326156616, 'test/loss': 2.2350151538848877, 'test/num_examples': 10000, 'score': 29633.381766319275, 'total_duration': 31926.95335674286, 'accumulated_submission_time': 29633.381766319275, 'accumulated_eval_time': 2279.166508436203, 'accumulated_logging_time': 6.403590440750122, 'global_step': 76835, 'preemption_count': 0}), (78179, {'train/accuracy': 0.7847377061843872, 'train/loss': 1.0560338497161865, 'validation/accuracy': 0.6899200081825256, 'validation/loss': 1.466614007949829, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.1106247901916504, 'test/num_examples': 10000, 'score': 30143.054849624634, 'total_duration': 32472.089480400085, 'accumulated_submission_time': 30143.054849624634, 'accumulated_eval_time': 2314.256219148636, 'accumulated_logging_time': 6.629779100418091, 'global_step': 78179, 'preemption_count': 0}), (79515, {'train/accuracy': 0.7818478941917419, 'train/loss': 1.0438038110733032, 'validation/accuracy': 0.6883800029754639, 'validation/loss': 1.4495753049850464, 'validation/num_examples': 50000, 'test/accuracy': 0.5608000159263611, 'test/loss': 2.113530397415161, 'test/num_examples': 10000, 'score': 30652.80054306984, 'total_duration': 33018.799008369446, 'accumulated_submission_time': 30652.80054306984, 'accumulated_eval_time': 2350.9118955135345, 'accumulated_logging_time': 6.792917966842651, 'global_step': 79515, 'preemption_count': 0}), (80857, {'train/accuracy': 0.7804726958274841, 'train/loss': 1.0745245218276978, 'validation/accuracy': 0.6845399737358093, 'validation/loss': 1.4880666732788086, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.132286310195923, 'test/num_examples': 10000, 'score': 31162.61455154419, 'total_duration': 33564.25296974182, 'accumulated_submission_time': 31162.61455154419, 'accumulated_eval_time': 2386.283365011215, 'accumulated_logging_time': 6.913316011428833, 'global_step': 80857, 'preemption_count': 0}), (82211, {'train/accuracy': 0.7780612111091614, 'train/loss': 1.0861520767211914, 'validation/accuracy': 0.6893399953842163, 'validation/loss': 1.480414628982544, 'validation/num_examples': 50000, 'test/accuracy': 0.5547000169754028, 'test/loss': 2.148196220397949, 'test/num_examples': 10000, 'score': 31672.33935379982, 'total_duration': 34109.625369787216, 'accumulated_submission_time': 31672.33935379982, 'accumulated_eval_time': 2421.618091106415, 'accumulated_logging_time': 7.078527450561523, 'global_step': 82211, 'preemption_count': 0}), (83559, {'train/accuracy': 0.7867107391357422, 'train/loss': 1.0743482112884521, 'validation/accuracy': 0.6917399764060974, 'validation/loss': 1.4812990427017212, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 2.110980749130249, 'test/num_examples': 10000, 'score': 32182.227016448975, 'total_duration': 34656.38101768494, 'accumulated_submission_time': 32182.227016448975, 'accumulated_eval_time': 2458.2180569171906, 'accumulated_logging_time': 7.198639392852783, 'global_step': 83559, 'preemption_count': 0}), (84893, {'train/accuracy': 0.7821866869926453, 'train/loss': 1.0713037252426147, 'validation/accuracy': 0.6906599998474121, 'validation/loss': 1.4716929197311401, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.1253280639648438, 'test/num_examples': 10000, 'score': 32692.007880926132, 'total_duration': 35201.97100162506, 'accumulated_submission_time': 32692.007880926132, 'accumulated_eval_time': 2493.6953144073486, 'accumulated_logging_time': 7.387145757675171, 'global_step': 84893, 'preemption_count': 0}), (86207, {'train/accuracy': 0.77933669090271, 'train/loss': 1.0669200420379639, 'validation/accuracy': 0.6860799789428711, 'validation/loss': 1.4645520448684692, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.089719533920288, 'test/num_examples': 10000, 'score': 33201.859541893005, 'total_duration': 35749.65291810036, 'accumulated_submission_time': 33201.859541893005, 'accumulated_eval_time': 2531.262959241867, 'accumulated_logging_time': 7.509037256240845, 'global_step': 86207, 'preemption_count': 0}), (87558, {'train/accuracy': 0.7837810516357422, 'train/loss': 1.0522605180740356, 'validation/accuracy': 0.6951599717140198, 'validation/loss': 1.4413443803787231, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 2.101987361907959, 'test/num_examples': 10000, 'score': 33711.657796382904, 'total_duration': 36295.57853245735, 'accumulated_submission_time': 33711.657796382904, 'accumulated_eval_time': 2567.116968154907, 'accumulated_logging_time': 7.635222673416138, 'global_step': 87558, 'preemption_count': 0}), (88896, {'train/accuracy': 0.7887236475944519, 'train/loss': 1.0084716081619263, 'validation/accuracy': 0.6981599926948547, 'validation/loss': 1.4085276126861572, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 2.0528955459594727, 'test/num_examples': 10000, 'score': 34221.60295796394, 'total_duration': 36840.39541339874, 'accumulated_submission_time': 34221.60295796394, 'accumulated_eval_time': 2601.7175676822662, 'accumulated_logging_time': 7.762111186981201, 'global_step': 88896, 'preemption_count': 0}), (90231, {'train/accuracy': 0.7817482352256775, 'train/loss': 1.0631102323532104, 'validation/accuracy': 0.6956200003623962, 'validation/loss': 1.4581043720245361, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 2.08136248588562, 'test/num_examples': 10000, 'score': 34731.56350541115, 'total_duration': 37384.528841257095, 'accumulated_submission_time': 34731.56350541115, 'accumulated_eval_time': 2635.5971190929413, 'accumulated_logging_time': 7.91056752204895, 'global_step': 90231, 'preemption_count': 0}), (91563, {'train/accuracy': 0.7726601958274841, 'train/loss': 1.1026209592819214, 'validation/accuracy': 0.6838799715042114, 'validation/loss': 1.4902595281600952, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.146847724914551, 'test/num_examples': 10000, 'score': 35241.41734933853, 'total_duration': 37931.89859294891, 'accumulated_submission_time': 35241.41734933853, 'accumulated_eval_time': 2672.8361806869507, 'accumulated_logging_time': 8.042389154434204, 'global_step': 91563, 'preemption_count': 0}), (92882, {'train/accuracy': 0.787527859210968, 'train/loss': 1.003239393234253, 'validation/accuracy': 0.6966999769210815, 'validation/loss': 1.4107553958892822, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 2.0533812046051025, 'test/num_examples': 10000, 'score': 35751.284779548645, 'total_duration': 38477.58324742317, 'accumulated_submission_time': 35751.284779548645, 'accumulated_eval_time': 2708.4114084243774, 'accumulated_logging_time': 8.142436265945435, 'global_step': 92882, 'preemption_count': 0}), (94205, {'train/accuracy': 0.7894012928009033, 'train/loss': 1.0597296953201294, 'validation/accuracy': 0.7023400068283081, 'validation/loss': 1.4387004375457764, 'validation/num_examples': 50000, 'test/accuracy': 0.5755000114440918, 'test/loss': 2.0974159240722656, 'test/num_examples': 10000, 'score': 36261.24402523041, 'total_duration': 39025.749687194824, 'accumulated_submission_time': 36261.24402523041, 'accumulated_eval_time': 2746.3251185417175, 'accumulated_logging_time': 8.29013967514038, 'global_step': 94205, 'preemption_count': 0}), (95541, {'train/accuracy': 0.7971141338348389, 'train/loss': 0.9971782565116882, 'validation/accuracy': 0.7035399675369263, 'validation/loss': 1.3968464136123657, 'validation/num_examples': 50000, 'test/accuracy': 0.5809000134468079, 'test/loss': 2.0447511672973633, 'test/num_examples': 10000, 'score': 36771.036645650864, 'total_duration': 39568.865891218185, 'accumulated_submission_time': 36771.036645650864, 'accumulated_eval_time': 2779.4038712978363, 'accumulated_logging_time': 8.390016555786133, 'global_step': 95541, 'preemption_count': 0}), (96887, {'train/accuracy': 0.7918127775192261, 'train/loss': 1.0121196508407593, 'validation/accuracy': 0.6983000040054321, 'validation/loss': 1.4113277196884155, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 2.1036698818206787, 'test/num_examples': 10000, 'score': 37281.02208542824, 'total_duration': 40116.107038497925, 'accumulated_submission_time': 37281.02208542824, 'accumulated_eval_time': 2816.388203859329, 'accumulated_logging_time': 8.51332712173462, 'global_step': 96887, 'preemption_count': 0}), (98223, {'train/accuracy': 0.7974330186843872, 'train/loss': 0.9960737228393555, 'validation/accuracy': 0.7046799659729004, 'validation/loss': 1.3997188806533813, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 2.0435357093811035, 'test/num_examples': 10000, 'score': 37790.78164982796, 'total_duration': 40659.9614212513, 'accumulated_submission_time': 37790.78164982796, 'accumulated_eval_time': 2850.1847467422485, 'accumulated_logging_time': 8.664451837539673, 'global_step': 98223, 'preemption_count': 0}), (99563, {'train/accuracy': 0.7955197691917419, 'train/loss': 1.0354253053665161, 'validation/accuracy': 0.7041599750518799, 'validation/loss': 1.4205228090286255, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.071550130844116, 'test/num_examples': 10000, 'score': 38300.81654429436, 'total_duration': 41212.595126867294, 'accumulated_submission_time': 38300.81654429436, 'accumulated_eval_time': 2892.4662005901337, 'accumulated_logging_time': 8.837025165557861, 'global_step': 99563, 'preemption_count': 0}), (100898, {'train/accuracy': 0.7993462681770325, 'train/loss': 0.9896190166473389, 'validation/accuracy': 0.711139976978302, 'validation/loss': 1.3836885690689087, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 2.0103890895843506, 'test/num_examples': 10000, 'score': 38810.56899642944, 'total_duration': 41762.55271148682, 'accumulated_submission_time': 38810.56899642944, 'accumulated_eval_time': 2932.329834461212, 'accumulated_logging_time': 9.033179759979248, 'global_step': 100898, 'preemption_count': 0}), (102244, {'train/accuracy': 0.7999840378761292, 'train/loss': 0.9937681555747986, 'validation/accuracy': 0.711679995059967, 'validation/loss': 1.3843388557434082, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 2.0381040573120117, 'test/num_examples': 10000, 'score': 39320.40218234062, 'total_duration': 42305.153641700745, 'accumulated_submission_time': 39320.40218234062, 'accumulated_eval_time': 2964.837651014328, 'accumulated_logging_time': 9.146535634994507, 'global_step': 102244, 'preemption_count': 0}), (103571, {'train/accuracy': 0.7977519035339355, 'train/loss': 0.9897588491439819, 'validation/accuracy': 0.7047199606895447, 'validation/loss': 1.3940502405166626, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 2.035470485687256, 'test/num_examples': 10000, 'score': 39830.30586838722, 'total_duration': 42854.203349113464, 'accumulated_submission_time': 39830.30586838722, 'accumulated_eval_time': 3003.6779000759125, 'accumulated_logging_time': 9.305868864059448, 'global_step': 103571, 'preemption_count': 0}), (104909, {'train/accuracy': 0.7970144748687744, 'train/loss': 0.9729068875312805, 'validation/accuracy': 0.7075600028038025, 'validation/loss': 1.3721404075622559, 'validation/num_examples': 50000, 'test/accuracy': 0.585800051689148, 'test/loss': 2.0031237602233887, 'test/num_examples': 10000, 'score': 40340.051884412766, 'total_duration': 43395.75755739212, 'accumulated_submission_time': 40340.051884412766, 'accumulated_eval_time': 3035.2010192871094, 'accumulated_logging_time': 9.445676565170288, 'global_step': 104909, 'preemption_count': 0}), (106254, {'train/accuracy': 0.7971341013908386, 'train/loss': 1.0117433071136475, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.4035918712615967, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 2.0467796325683594, 'test/num_examples': 10000, 'score': 40849.81273841858, 'total_duration': 43941.0291826725, 'accumulated_submission_time': 40849.81273841858, 'accumulated_eval_time': 3070.334483385086, 'accumulated_logging_time': 9.674582242965698, 'global_step': 106254, 'preemption_count': 0}), (107589, {'train/accuracy': 0.8005022406578064, 'train/loss': 0.9862359166145325, 'validation/accuracy': 0.7067199945449829, 'validation/loss': 1.388258457183838, 'validation/num_examples': 50000, 'test/accuracy': 0.5842000246047974, 'test/loss': 2.0133423805236816, 'test/num_examples': 10000, 'score': 41359.65326118469, 'total_duration': 44484.171738386154, 'accumulated_submission_time': 41359.65326118469, 'accumulated_eval_time': 3103.411251306534, 'accumulated_logging_time': 9.756128787994385, 'global_step': 107589, 'preemption_count': 0}), (108926, {'train/accuracy': 0.8112643361091614, 'train/loss': 0.9345428347587585, 'validation/accuracy': 0.716159999370575, 'validation/loss': 1.3388030529022217, 'validation/num_examples': 50000, 'test/accuracy': 0.5940999984741211, 'test/loss': 1.9794174432754517, 'test/num_examples': 10000, 'score': 41869.69741463661, 'total_duration': 45029.54582500458, 'accumulated_submission_time': 41869.69741463661, 'accumulated_eval_time': 3138.486629486084, 'accumulated_logging_time': 9.864871501922607, 'global_step': 108926, 'preemption_count': 0}), (110253, {'train/accuracy': 0.8097496628761292, 'train/loss': 0.936035692691803, 'validation/accuracy': 0.712399959564209, 'validation/loss': 1.3608921766281128, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.007680892944336, 'test/num_examples': 10000, 'score': 42379.68012213707, 'total_duration': 45576.118019104004, 'accumulated_submission_time': 42379.68012213707, 'accumulated_eval_time': 3174.813100337982, 'accumulated_logging_time': 9.982455730438232, 'global_step': 110253, 'preemption_count': 0}), (111590, {'train/accuracy': 0.8106265664100647, 'train/loss': 0.9283785820007324, 'validation/accuracy': 0.7137799859046936, 'validation/loss': 1.3441743850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 1.9702633619308472, 'test/num_examples': 10000, 'score': 42889.66196060181, 'total_duration': 46123.19499254227, 'accumulated_submission_time': 42889.66196060181, 'accumulated_eval_time': 3211.6334109306335, 'accumulated_logging_time': 10.112252235412598, 'global_step': 111590, 'preemption_count': 0}), (112920, {'train/accuracy': 0.8062818646430969, 'train/loss': 0.9538609981536865, 'validation/accuracy': 0.711899995803833, 'validation/loss': 1.3690943717956543, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 2.0119946002960205, 'test/num_examples': 10000, 'score': 43399.64029479027, 'total_duration': 46672.40962266922, 'accumulated_submission_time': 43399.64029479027, 'accumulated_eval_time': 3250.6331191062927, 'accumulated_logging_time': 10.205451250076294, 'global_step': 112920, 'preemption_count': 0}), (114256, {'train/accuracy': 0.8217474222183228, 'train/loss': 0.9386281371116638, 'validation/accuracy': 0.7227199673652649, 'validation/loss': 1.362930417060852, 'validation/num_examples': 50000, 'test/accuracy': 0.598300039768219, 'test/loss': 1.985208511352539, 'test/num_examples': 10000, 'score': 43909.39679098129, 'total_duration': 47220.93038392067, 'accumulated_submission_time': 43909.39679098129, 'accumulated_eval_time': 3289.1235721111298, 'accumulated_logging_time': 10.3337881565094, 'global_step': 114256, 'preemption_count': 0}), (115589, {'train/accuracy': 0.8206512928009033, 'train/loss': 0.8927697539329529, 'validation/accuracy': 0.7161799669265747, 'validation/loss': 1.336808204650879, 'validation/num_examples': 50000, 'test/accuracy': 0.600600004196167, 'test/loss': 1.9473992586135864, 'test/num_examples': 10000, 'score': 44419.11040997505, 'total_duration': 47767.62062740326, 'accumulated_submission_time': 44419.11040997505, 'accumulated_eval_time': 3325.7916507720947, 'accumulated_logging_time': 10.4962797164917, 'global_step': 115589, 'preemption_count': 0}), (116922, {'train/accuracy': 0.8350406289100647, 'train/loss': 0.8642550110816956, 'validation/accuracy': 0.7200599908828735, 'validation/loss': 1.3416831493377686, 'validation/num_examples': 50000, 'test/accuracy': 0.59170001745224, 'test/loss': 1.9726402759552002, 'test/num_examples': 10000, 'score': 44929.17004656792, 'total_duration': 48309.36885166168, 'accumulated_submission_time': 44929.17004656792, 'accumulated_eval_time': 3357.2077775001526, 'accumulated_logging_time': 10.624828577041626, 'global_step': 116922, 'preemption_count': 0}), (118259, {'train/accuracy': 0.85453200340271, 'train/loss': 0.7720656991004944, 'validation/accuracy': 0.7185999751091003, 'validation/loss': 1.3324124813079834, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.9789848327636719, 'test/num_examples': 10000, 'score': 45438.995973587036, 'total_duration': 48855.73725390434, 'accumulated_submission_time': 45438.995973587036, 'accumulated_eval_time': 3393.4224004745483, 'accumulated_logging_time': 10.807410717010498, 'global_step': 118259, 'preemption_count': 0}), (119587, {'train/accuracy': 0.8564453125, 'train/loss': 0.7863262295722961, 'validation/accuracy': 0.7173199653625488, 'validation/loss': 1.357183814048767, 'validation/num_examples': 50000, 'test/accuracy': 0.5908000469207764, 'test/loss': 1.9870078563690186, 'test/num_examples': 10000, 'score': 45948.78064036369, 'total_duration': 49402.041544914246, 'accumulated_submission_time': 45948.78064036369, 'accumulated_eval_time': 3429.579167842865, 'accumulated_logging_time': 11.026552200317383, 'global_step': 119587, 'preemption_count': 0}), (120915, {'train/accuracy': 0.8581592440605164, 'train/loss': 0.7788785696029663, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.3338935375213623, 'validation/num_examples': 50000, 'test/accuracy': 0.5926000475883484, 'test/loss': 1.9721448421478271, 'test/num_examples': 10000, 'score': 46458.670367240906, 'total_duration': 49945.30210542679, 'accumulated_submission_time': 46458.670367240906, 'accumulated_eval_time': 3462.655120372772, 'accumulated_logging_time': 11.17898178100586, 'global_step': 120915, 'preemption_count': 0}), (122242, {'train/accuracy': 0.8538544178009033, 'train/loss': 0.7556219100952148, 'validation/accuracy': 0.7256799936294556, 'validation/loss': 1.2958855628967285, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.9350981712341309, 'test/num_examples': 10000, 'score': 46968.424134492874, 'total_duration': 50493.13824748993, 'accumulated_submission_time': 46968.424134492874, 'accumulated_eval_time': 3500.426337957382, 'accumulated_logging_time': 11.347643375396729, 'global_step': 122242, 'preemption_count': 0}), (123572, {'train/accuracy': 0.8538345098495483, 'train/loss': 0.7689257264137268, 'validation/accuracy': 0.7252599596977234, 'validation/loss': 1.309849739074707, 'validation/num_examples': 50000, 'test/accuracy': 0.5989000201225281, 'test/loss': 1.93218195438385, 'test/num_examples': 10000, 'score': 47478.52780914307, 'total_duration': 51038.22334599495, 'accumulated_submission_time': 47478.52780914307, 'accumulated_eval_time': 3535.12708568573, 'accumulated_logging_time': 11.483810186386108, 'global_step': 123572, 'preemption_count': 0}), (124910, {'train/accuracy': 0.8482341766357422, 'train/loss': 0.7783138155937195, 'validation/accuracy': 0.7231599688529968, 'validation/loss': 1.3117820024490356, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.9467514753341675, 'test/num_examples': 10000, 'score': 47988.48527741432, 'total_duration': 51585.16881752014, 'accumulated_submission_time': 47988.48527741432, 'accumulated_eval_time': 3571.8389916419983, 'accumulated_logging_time': 11.616332292556763, 'global_step': 124910, 'preemption_count': 0}), (126275, {'train/accuracy': 0.8556879758834839, 'train/loss': 0.7615275382995605, 'validation/accuracy': 0.7256399989128113, 'validation/loss': 1.2998392581939697, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.93067467212677, 'test/num_examples': 10000, 'score': 48498.32094120979, 'total_duration': 52128.489530563354, 'accumulated_submission_time': 48498.32094120979, 'accumulated_eval_time': 3605.065532207489, 'accumulated_logging_time': 11.724518775939941, 'global_step': 126275, 'preemption_count': 0}), (127626, {'train/accuracy': 0.8526387214660645, 'train/loss': 0.7871387600898743, 'validation/accuracy': 0.7267400026321411, 'validation/loss': 1.3124920129776, 'validation/num_examples': 50000, 'test/accuracy': 0.6059000492095947, 'test/loss': 1.9297891855239868, 'test/num_examples': 10000, 'score': 49008.279960632324, 'total_duration': 52677.96447587013, 'accumulated_submission_time': 49008.279960632324, 'accumulated_eval_time': 3644.2764604091644, 'accumulated_logging_time': 11.8839111328125, 'global_step': 127626, 'preemption_count': 0}), (128964, {'train/accuracy': 0.8551897406578064, 'train/loss': 0.7733866572380066, 'validation/accuracy': 0.7266799807548523, 'validation/loss': 1.2999855279922485, 'validation/num_examples': 50000, 'test/accuracy': 0.602400004863739, 'test/loss': 1.9424817562103271, 'test/num_examples': 10000, 'score': 49518.30748152733, 'total_duration': 53224.38951301575, 'accumulated_submission_time': 49518.30748152733, 'accumulated_eval_time': 3680.424438238144, 'accumulated_logging_time': 11.989202499389648, 'global_step': 128964, 'preemption_count': 0}), (130303, {'train/accuracy': 0.8565050959587097, 'train/loss': 0.7721459865570068, 'validation/accuracy': 0.7292199730873108, 'validation/loss': 1.306473970413208, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.9184318780899048, 'test/num_examples': 10000, 'score': 50028.05168366432, 'total_duration': 53768.754437208176, 'accumulated_submission_time': 50028.05168366432, 'accumulated_eval_time': 3714.7783331871033, 'accumulated_logging_time': 12.112935304641724, 'global_step': 130303, 'preemption_count': 0}), (131640, {'train/accuracy': 0.8598134517669678, 'train/loss': 0.754364013671875, 'validation/accuracy': 0.7331399917602539, 'validation/loss': 1.2859514951705933, 'validation/num_examples': 50000, 'test/accuracy': 0.6078000068664551, 'test/loss': 1.9231277704238892, 'test/num_examples': 10000, 'score': 50537.96726727486, 'total_duration': 54315.40590119362, 'accumulated_submission_time': 50537.96726727486, 'accumulated_eval_time': 3751.193768978119, 'accumulated_logging_time': 12.286328792572021, 'global_step': 131640, 'preemption_count': 0}), (132991, {'train/accuracy': 0.8626434803009033, 'train/loss': 0.7463467717170715, 'validation/accuracy': 0.7346199750900269, 'validation/loss': 1.275136113166809, 'validation/num_examples': 50000, 'test/accuracy': 0.604200005531311, 'test/loss': 1.9059524536132812, 'test/num_examples': 10000, 'score': 51047.97311878204, 'total_duration': 54862.776564359665, 'accumulated_submission_time': 51047.97311878204, 'accumulated_eval_time': 3788.214375257492, 'accumulated_logging_time': 12.484822273254395, 'global_step': 132991, 'preemption_count': 0}), (134345, {'train/accuracy': 0.8635602593421936, 'train/loss': 0.7222200632095337, 'validation/accuracy': 0.735539972782135, 'validation/loss': 1.2597770690917969, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.896063208580017, 'test/num_examples': 10000, 'score': 51557.96750831604, 'total_duration': 55407.80333042145, 'accumulated_submission_time': 51557.96750831604, 'accumulated_eval_time': 3822.943412542343, 'accumulated_logging_time': 12.640672445297241, 'global_step': 134345, 'preemption_count': 0}), (135683, {'train/accuracy': 0.8566445708274841, 'train/loss': 0.7402517795562744, 'validation/accuracy': 0.733199954032898, 'validation/loss': 1.2630550861358643, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.9112828969955444, 'test/num_examples': 10000, 'score': 52068.079738378525, 'total_duration': 55955.72621059418, 'accumulated_submission_time': 52068.079738378525, 'accumulated_eval_time': 3860.4848976135254, 'accumulated_logging_time': 12.76378846168518, 'global_step': 135683, 'preemption_count': 0}), (137020, {'train/accuracy': 0.8651147484779358, 'train/loss': 0.7459121346473694, 'validation/accuracy': 0.7380799651145935, 'validation/loss': 1.2728499174118042, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.9153974056243896, 'test/num_examples': 10000, 'score': 52578.08869051933, 'total_duration': 56501.23701739311, 'accumulated_submission_time': 52578.08869051933, 'accumulated_eval_time': 3895.6695017814636, 'accumulated_logging_time': 12.931283950805664, 'global_step': 137020, 'preemption_count': 0}), (138366, {'train/accuracy': 0.8689413070678711, 'train/loss': 0.6961476802825928, 'validation/accuracy': 0.7372999787330627, 'validation/loss': 1.2360162734985352, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.8734062910079956, 'test/num_examples': 10000, 'score': 53087.78286600113, 'total_duration': 57048.83854794502, 'accumulated_submission_time': 53087.78286600113, 'accumulated_eval_time': 3933.1883544921875, 'accumulated_logging_time': 13.172261476516724, 'global_step': 138366, 'preemption_count': 0}), (139702, {'train/accuracy': 0.8683035373687744, 'train/loss': 0.722339391708374, 'validation/accuracy': 0.742579996585846, 'validation/loss': 1.2524217367172241, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.8979796171188354, 'test/num_examples': 10000, 'score': 53597.78376507759, 'total_duration': 57593.178294181824, 'accumulated_submission_time': 53597.78376507759, 'accumulated_eval_time': 3967.25323843956, 'accumulated_logging_time': 13.300051212310791, 'global_step': 139702, 'preemption_count': 0}), (141031, {'train/accuracy': 0.8724290132522583, 'train/loss': 0.704590916633606, 'validation/accuracy': 0.7425199747085571, 'validation/loss': 1.2437777519226074, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.8900768756866455, 'test/num_examples': 10000, 'score': 54107.58808326721, 'total_duration': 58134.62321949005, 'accumulated_submission_time': 54107.58808326721, 'accumulated_eval_time': 3998.6071343421936, 'accumulated_logging_time': 13.444273710250854, 'global_step': 141031, 'preemption_count': 0}), (142361, {'train/accuracy': 0.8742226958274841, 'train/loss': 0.677759051322937, 'validation/accuracy': 0.7409600019454956, 'validation/loss': 1.2262144088745117, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.8658561706542969, 'test/num_examples': 10000, 'score': 54617.52095079422, 'total_duration': 58678.391629219055, 'accumulated_submission_time': 54617.52095079422, 'accumulated_eval_time': 4032.120908498764, 'accumulated_logging_time': 13.623765707015991, 'global_step': 142361, 'preemption_count': 0}), (143695, {'train/accuracy': 0.8719506859779358, 'train/loss': 0.6898953914642334, 'validation/accuracy': 0.7416200041770935, 'validation/loss': 1.2386674880981445, 'validation/num_examples': 50000, 'test/accuracy': 0.6150000095367432, 'test/loss': 1.8921301364898682, 'test/num_examples': 10000, 'score': 55127.27566242218, 'total_duration': 59221.765394210815, 'accumulated_submission_time': 55127.27566242218, 'accumulated_eval_time': 4065.4512078762054, 'accumulated_logging_time': 13.768025875091553, 'global_step': 143695, 'preemption_count': 0}), (145024, {'train/accuracy': 0.8809988498687744, 'train/loss': 0.6845759153366089, 'validation/accuracy': 0.7438799738883972, 'validation/loss': 1.2611819505691528, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.8859914541244507, 'test/num_examples': 10000, 'score': 55637.26403737068, 'total_duration': 59765.88941693306, 'accumulated_submission_time': 55637.26403737068, 'accumulated_eval_time': 4099.306620121002, 'accumulated_logging_time': 13.90233063697815, 'global_step': 145024, 'preemption_count': 0}), (146348, {'train/accuracy': 0.8780691623687744, 'train/loss': 0.6677699685096741, 'validation/accuracy': 0.7424399852752686, 'validation/loss': 1.2333136796951294, 'validation/num_examples': 50000, 'test/accuracy': 0.6211000084877014, 'test/loss': 1.8664543628692627, 'test/num_examples': 10000, 'score': 56147.04470014572, 'total_duration': 60307.52887225151, 'accumulated_submission_time': 56147.04470014572, 'accumulated_eval_time': 4130.867275238037, 'accumulated_logging_time': 14.056933403015137, 'global_step': 146348, 'preemption_count': 0}), (147676, {'train/accuracy': 0.8805006146430969, 'train/loss': 0.6831640601158142, 'validation/accuracy': 0.7435399889945984, 'validation/loss': 1.2495644092559814, 'validation/num_examples': 50000, 'test/accuracy': 0.6137000322341919, 'test/loss': 1.893677830696106, 'test/num_examples': 10000, 'score': 56656.94694519043, 'total_duration': 60849.77301621437, 'accumulated_submission_time': 56656.94694519043, 'accumulated_eval_time': 4162.972571134567, 'accumulated_logging_time': 14.150558948516846, 'global_step': 147676, 'preemption_count': 0}), (149011, {'train/accuracy': 0.8844866156578064, 'train/loss': 0.6485022306442261, 'validation/accuracy': 0.745419979095459, 'validation/loss': 1.2260268926620483, 'validation/num_examples': 50000, 'test/accuracy': 0.6216000318527222, 'test/loss': 1.8596113920211792, 'test/num_examples': 10000, 'score': 57166.792831897736, 'total_duration': 61396.48536944389, 'accumulated_submission_time': 57166.792831897736, 'accumulated_eval_time': 4199.529405593872, 'accumulated_logging_time': 14.316476583480835, 'global_step': 149011, 'preemption_count': 0}), (150344, {'train/accuracy': 0.8907046914100647, 'train/loss': 0.6355377435684204, 'validation/accuracy': 0.7493399977684021, 'validation/loss': 1.2205615043640137, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.845856785774231, 'test/num_examples': 10000, 'score': 57676.53068614006, 'total_duration': 61939.44174742699, 'accumulated_submission_time': 57676.53068614006, 'accumulated_eval_time': 4232.397779464722, 'accumulated_logging_time': 14.523430824279785, 'global_step': 150344, 'preemption_count': 0}), (151673, {'train/accuracy': 0.8900271058082581, 'train/loss': 0.641436755657196, 'validation/accuracy': 0.7471599578857422, 'validation/loss': 1.2307078838348389, 'validation/num_examples': 50000, 'test/accuracy': 0.6216000318527222, 'test/loss': 1.854296326637268, 'test/num_examples': 10000, 'score': 58186.35161614418, 'total_duration': 62485.369346380234, 'accumulated_submission_time': 58186.35161614418, 'accumulated_eval_time': 4268.189233541489, 'accumulated_logging_time': 14.695045232772827, 'global_step': 151673, 'preemption_count': 0}), (153006, {'train/accuracy': 0.8962651491165161, 'train/loss': 0.6050325036048889, 'validation/accuracy': 0.7501800060272217, 'validation/loss': 1.20828378200531, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8369616270065308, 'test/num_examples': 10000, 'score': 58696.41811990738, 'total_duration': 63030.2652027607, 'accumulated_submission_time': 58696.41811990738, 'accumulated_eval_time': 4302.695431470871, 'accumulated_logging_time': 14.874669551849365, 'global_step': 153006, 'preemption_count': 0}), (154332, {'train/accuracy': 0.8984175324440002, 'train/loss': 0.5849341750144958, 'validation/accuracy': 0.7518599629402161, 'validation/loss': 1.1981676816940308, 'validation/num_examples': 50000, 'test/accuracy': 0.6256000399589539, 'test/loss': 1.820199728012085, 'test/num_examples': 10000, 'score': 59206.18372344971, 'total_duration': 63571.60168457031, 'accumulated_submission_time': 59206.18372344971, 'accumulated_eval_time': 4333.925555944443, 'accumulated_logging_time': 15.071545362472534, 'global_step': 154332, 'preemption_count': 0}), (155663, {'train/accuracy': 0.8993741869926453, 'train/loss': 0.5903911590576172, 'validation/accuracy': 0.7518999576568604, 'validation/loss': 1.2005704641342163, 'validation/num_examples': 50000, 'test/accuracy': 0.6236000061035156, 'test/loss': 1.833459496498108, 'test/num_examples': 10000, 'score': 59716.01258230209, 'total_duration': 64118.128554582596, 'accumulated_submission_time': 59716.01258230209, 'accumulated_eval_time': 4370.362705945969, 'accumulated_logging_time': 15.189541578292847, 'global_step': 155663, 'preemption_count': 0}), (156989, {'train/accuracy': 0.9019052982330322, 'train/loss': 0.5845792293548584, 'validation/accuracy': 0.7516999840736389, 'validation/loss': 1.2074382305145264, 'validation/num_examples': 50000, 'test/accuracy': 0.6251000165939331, 'test/loss': 1.8385114669799805, 'test/num_examples': 10000, 'score': 60226.180965662, 'total_duration': 64663.010313272476, 'accumulated_submission_time': 60226.180965662, 'accumulated_eval_time': 4404.826059579849, 'accumulated_logging_time': 15.29827094078064, 'global_step': 156989, 'preemption_count': 0}), (158330, {'train/accuracy': 0.9014269709587097, 'train/loss': 0.5914669632911682, 'validation/accuracy': 0.7501199841499329, 'validation/loss': 1.213230013847351, 'validation/num_examples': 50000, 'test/accuracy': 0.6188000440597534, 'test/loss': 1.8594297170639038, 'test/num_examples': 10000, 'score': 60735.99801135063, 'total_duration': 65206.74983716011, 'accumulated_submission_time': 60735.99801135063, 'accumulated_eval_time': 4438.431537151337, 'accumulated_logging_time': 15.468393564224243, 'global_step': 158330, 'preemption_count': 0}), (159665, {'train/accuracy': 0.9070272445678711, 'train/loss': 0.5723469853401184, 'validation/accuracy': 0.7541399598121643, 'validation/loss': 1.203861951828003, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8244379758834839, 'test/num_examples': 10000, 'score': 61246.104724645615, 'total_duration': 65748.54129958153, 'accumulated_submission_time': 61246.104724645615, 'accumulated_eval_time': 4469.844804286957, 'accumulated_logging_time': 15.594008445739746, 'global_step': 159665, 'preemption_count': 0}), (160997, {'train/accuracy': 0.9129663109779358, 'train/loss': 0.5551027655601501, 'validation/accuracy': 0.7541199922561646, 'validation/loss': 1.1970536708831787, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.8307443857192993, 'test/num_examples': 10000, 'score': 61755.937997341156, 'total_duration': 66291.26049780846, 'accumulated_submission_time': 61755.937997341156, 'accumulated_eval_time': 4502.443028450012, 'accumulated_logging_time': 15.737564086914062, 'global_step': 160997, 'preemption_count': 0}), (162323, {'train/accuracy': 0.9132055044174194, 'train/loss': 0.5452498197555542, 'validation/accuracy': 0.7566199898719788, 'validation/loss': 1.1877886056900024, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.8170104026794434, 'test/num_examples': 10000, 'score': 62265.92627620697, 'total_duration': 66833.95874142647, 'accumulated_submission_time': 62265.92627620697, 'accumulated_eval_time': 4534.872227668762, 'accumulated_logging_time': 15.87644100189209, 'global_step': 162323, 'preemption_count': 0}), (163655, {'train/accuracy': 0.9172512292861938, 'train/loss': 0.5303930640220642, 'validation/accuracy': 0.7572199702262878, 'validation/loss': 1.1877232789993286, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.8155666589736938, 'test/num_examples': 10000, 'score': 62775.66317367554, 'total_duration': 67375.44443655014, 'accumulated_submission_time': 62775.66317367554, 'accumulated_eval_time': 4566.268794298172, 'accumulated_logging_time': 16.082500219345093, 'global_step': 163655, 'preemption_count': 0}), (164985, {'train/accuracy': 0.9249242544174194, 'train/loss': 0.5228111743927002, 'validation/accuracy': 0.7550999522209167, 'validation/loss': 1.2048935890197754, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.822440505027771, 'test/num_examples': 10000, 'score': 63285.50475859642, 'total_duration': 67915.59853625298, 'accumulated_submission_time': 63285.50475859642, 'accumulated_eval_time': 4596.318994283676, 'accumulated_logging_time': 16.199601650238037, 'global_step': 164985, 'preemption_count': 0}), (166308, {'train/accuracy': 0.9310028553009033, 'train/loss': 0.48925212025642395, 'validation/accuracy': 0.7572799921035767, 'validation/loss': 1.187283992767334, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.807860255241394, 'test/num_examples': 10000, 'score': 63795.45100450516, 'total_duration': 68460.22480273247, 'accumulated_submission_time': 63795.45100450516, 'accumulated_eval_time': 4630.666370630264, 'accumulated_logging_time': 16.386629343032837, 'global_step': 166308, 'preemption_count': 0}), (167635, {'train/accuracy': 0.9331951141357422, 'train/loss': 0.4766234755516052, 'validation/accuracy': 0.7599599957466125, 'validation/loss': 1.1863263845443726, 'validation/num_examples': 50000, 'test/accuracy': 0.6341000199317932, 'test/loss': 1.8089114427566528, 'test/num_examples': 10000, 'score': 64305.28494763374, 'total_duration': 69004.76878523827, 'accumulated_submission_time': 64305.28494763374, 'accumulated_eval_time': 4665.078794956207, 'accumulated_logging_time': 16.537473440170288, 'global_step': 167635, 'preemption_count': 0}), (168962, {'train/accuracy': 0.9358856678009033, 'train/loss': 0.46779465675354004, 'validation/accuracy': 0.7589600086212158, 'validation/loss': 1.1821156740188599, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.8026305437088013, 'test/num_examples': 10000, 'score': 64815.3239068985, 'total_duration': 69547.96621322632, 'accumulated_submission_time': 64815.3239068985, 'accumulated_eval_time': 4697.9667365550995, 'accumulated_logging_time': 16.665486097335815, 'global_step': 168962, 'preemption_count': 0}), (170293, {'train/accuracy': 0.9328364133834839, 'train/loss': 0.47571054100990295, 'validation/accuracy': 0.759880006313324, 'validation/loss': 1.179027795791626, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.7956260442733765, 'test/num_examples': 10000, 'score': 65325.182131290436, 'total_duration': 70091.8058116436, 'accumulated_submission_time': 65325.182131290436, 'accumulated_eval_time': 4731.531051874161, 'accumulated_logging_time': 16.934987545013428, 'global_step': 170293, 'preemption_count': 0}), (171624, {'train/accuracy': 0.9365034699440002, 'train/loss': 0.46841076016426086, 'validation/accuracy': 0.7608199715614319, 'validation/loss': 1.1785054206848145, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.7958484888076782, 'test/num_examples': 10000, 'score': 65834.9653031826, 'total_duration': 70637.66944098473, 'accumulated_submission_time': 65834.9653031826, 'accumulated_eval_time': 4767.2800278663635, 'accumulated_logging_time': 17.119905471801758, 'global_step': 171624, 'preemption_count': 0})], 'global_step': 172950}
I0307 21:06:19.828804 140193455334592 submission_runner.py:649] Timing: 66344.44334840775
I0307 21:06:19.828852 140193455334592 submission_runner.py:651] Total number of evals: 130
I0307 21:06:19.828881 140193455334592 submission_runner.py:652] ====================
I0307 21:06:19.829070 140193455334592 submission_runner.py:750] Final imagenet_resnet score: 1
