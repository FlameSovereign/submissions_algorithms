python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-1096693655 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-00-59-18.log
2025-03-07 00:59:37.649692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741309178.170643       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741309178.301897       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 01:00:27.210925 140441807221952 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax.
I0307 01:00:29.647710 140441807221952 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 01:00:29.650485 140441807221952 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 01:00:29.677740 140441807221952 submission_runner.py:606] Using RNG seed -1096693655
I0307 01:00:32.845669 140441807221952 submission_runner.py:615] --- Tuning run 1/5 ---
I0307 01:00:32.845864 140441807221952 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_1.
I0307 01:00:32.846049 140441807221952 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_1/hparams.json.
I0307 01:00:33.084077 140441807221952 submission_runner.py:218] Initializing dataset.
I0307 01:00:34.721698 140441807221952 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:00:35.051881 140441807221952 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:00:35.426533 140441807221952 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:00:37.460484 140441807221952 submission_runner.py:229] Initializing model.
I0307 01:01:02.495736 140441807221952 submission_runner.py:272] Initializing optimizer.
I0307 01:01:03.691511 140441807221952 submission_runner.py:279] Initializing metrics bundle.
I0307 01:01:03.691771 140441807221952 submission_runner.py:301] Initializing checkpoint and logger.
I0307 01:01:03.692978 140441807221952 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0307 01:01:03.693085 140441807221952 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0307 01:01:04.258367 140441807221952 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_1/flags_0.json.
I0307 01:01:04.495650 140441807221952 submission_runner.py:337] Starting training loop.
I0307 01:02:03.837518 140304237709056 logging_writer.py:48] [0] global_step=0, grad_norm=0.6111066937446594, loss=6.936004161834717
I0307 01:02:04.182862 140441807221952 spec.py:321] Evaluating on the training split.
I0307 01:02:04.649458 140441807221952 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:02:04.674073 140441807221952 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:02:04.717023 140441807221952 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:02:24.843816 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 01:02:25.404807 140441807221952 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:02:25.431306 140441807221952 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 01:02:25.640206 140441807221952 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 01:03:05.411604 140441807221952 spec.py:349] Evaluating on the test split.
I0307 01:03:05.886687 140441807221952 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 01:03:05.922382 140441807221952 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 01:03:05.959517 140441807221952 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 01:03:33.840123 140441807221952 submission_runner.py:469] Time since start: 149.34s, 	Step: 1, 	{'train/accuracy': 0.0008769132546149194, 'train/loss': 6.912663459777832, 'validation/accuracy': 0.0010599999222904444, 'validation/loss': 6.912087917327881, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912038803100586, 'test/num_examples': 10000, 'score': 59.687002420425415, 'total_duration': 149.34441924095154, 'accumulated_submission_time': 59.687002420425415, 'accumulated_eval_time': 89.65718460083008, 'accumulated_logging_time': 0}
I0307 01:03:33.852076 140286701328128 logging_writer.py:48] [1] accumulated_eval_time=89.6572, accumulated_logging_time=0, accumulated_submission_time=59.687, global_step=1, preemption_count=0, score=59.687, test/accuracy=0.0009, test/loss=6.91204, test/num_examples=10000, total_duration=149.344, train/accuracy=0.000876913, train/loss=6.91266, validation/accuracy=0.00106, validation/loss=6.91209, validation/num_examples=50000
I0307 01:04:10.474297 140286692935424 logging_writer.py:48] [100] global_step=100, grad_norm=0.5988179445266724, loss=6.907304763793945
I0307 01:04:47.322253 140286701328128 logging_writer.py:48] [200] global_step=200, grad_norm=0.5876765847206116, loss=6.862673282623291
I0307 01:05:24.158666 140286692935424 logging_writer.py:48] [300] global_step=300, grad_norm=0.5848390460014343, loss=6.803579807281494
I0307 01:06:01.545344 140286701328128 logging_writer.py:48] [400] global_step=400, grad_norm=0.6730841994285583, loss=6.718993663787842
I0307 01:06:39.170681 140286692935424 logging_writer.py:48] [500] global_step=500, grad_norm=0.7090136408805847, loss=6.638854026794434
I0307 01:07:16.727524 140286701328128 logging_writer.py:48] [600] global_step=600, grad_norm=0.7489510774612427, loss=6.5333662033081055
I0307 01:07:53.874266 140286692935424 logging_writer.py:48] [700] global_step=700, grad_norm=0.7710532546043396, loss=6.467787742614746
I0307 01:08:30.771208 140286701328128 logging_writer.py:48] [800] global_step=800, grad_norm=0.8011860251426697, loss=6.348940849304199
I0307 01:09:06.864949 140286692935424 logging_writer.py:48] [900] global_step=900, grad_norm=1.1950719356536865, loss=6.206971645355225
I0307 01:09:43.933395 140286701328128 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1520404815673828, loss=6.124955654144287
I0307 01:10:21.245061 140286692935424 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.363969326019287, loss=6.088718891143799
I0307 01:10:58.590732 140286701328128 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.3689674139022827, loss=6.016356468200684
I0307 01:11:36.127664 140286692935424 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.227632999420166, loss=5.925502300262451
I0307 01:12:04.149762 140441807221952 spec.py:321] Evaluating on the training split.
I0307 01:12:15.792280 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 01:12:42.106447 140441807221952 spec.py:349] Evaluating on the test split.
I0307 01:12:43.966061 140441807221952 submission_runner.py:469] Time since start: 699.47s, 	Step: 1375, 	{'train/accuracy': 0.06507094949483871, 'train/loss': 5.495771408081055, 'validation/accuracy': 0.056619998067617416, 'validation/loss': 5.574432849884033, 'validation/num_examples': 50000, 'test/accuracy': 0.04050000011920929, 'test/loss': 5.766232490539551, 'test/num_examples': 10000, 'score': 569.7683362960815, 'total_duration': 699.4703891277313, 'accumulated_submission_time': 569.7683362960815, 'accumulated_eval_time': 129.47345995903015, 'accumulated_logging_time': 0.04252290725708008}
I0307 01:12:43.988914 140286709720832 logging_writer.py:48] [1375] accumulated_eval_time=129.473, accumulated_logging_time=0.0425229, accumulated_submission_time=569.768, global_step=1375, preemption_count=0, score=569.768, test/accuracy=0.0405, test/loss=5.76623, test/num_examples=10000, total_duration=699.47, train/accuracy=0.0650709, train/loss=5.49577, validation/accuracy=0.05662, validation/loss=5.57443, validation/num_examples=50000
I0307 01:12:53.790404 140286718113536 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.9281631708145142, loss=5.895606994628906
I0307 01:13:30.226308 140286709720832 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.201376438140869, loss=5.8081207275390625
I0307 01:14:07.655668 140286718113536 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.978553056716919, loss=5.756277084350586
I0307 01:14:44.782939 140286709720832 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.8572717905044556, loss=5.697408676147461
I0307 01:15:22.618962 140286718113536 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.806744337081909, loss=5.622543811798096
I0307 01:16:00.946425 140286709720832 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.650376558303833, loss=5.51568078994751
I0307 01:16:38.680371 140286718113536 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.1571404933929443, loss=5.500442028045654
I0307 01:17:14.798994 140286709720832 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.7536628246307373, loss=5.462699890136719
I0307 01:17:53.526335 140286718113536 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.396787643432617, loss=5.448772430419922
I0307 01:18:32.449167 140286709720832 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.5292916297912598, loss=5.325904369354248
I0307 01:19:10.843703 140286718113536 logging_writer.py:48] [2400] global_step=2400, grad_norm=7.2197585105896, loss=5.269399642944336
I0307 01:19:49.166642 140286709720832 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.1517839431762695, loss=5.332073211669922
I0307 01:20:27.561031 140286718113536 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.9529290199279785, loss=5.214668273925781
I0307 01:21:05.618325 140286709720832 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.1803436279296875, loss=5.223849296569824
I0307 01:21:14.179614 140441807221952 spec.py:321] Evaluating on the training split.
I0307 01:21:26.026362 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 01:21:49.650253 140441807221952 spec.py:349] Evaluating on the test split.
I0307 01:21:51.475481 140441807221952 submission_runner.py:469] Time since start: 1246.98s, 	Step: 2723, 	{'train/accuracy': 0.16027583181858063, 'train/loss': 4.423295021057129, 'validation/accuracy': 0.13835999369621277, 'validation/loss': 4.570390701293945, 'validation/num_examples': 50000, 'test/accuracy': 0.09890000522136688, 'test/loss': 4.93082857131958, 'test/num_examples': 10000, 'score': 1079.7922539710999, 'total_duration': 1246.979810476303, 'accumulated_submission_time': 1079.7922539710999, 'accumulated_eval_time': 166.76930689811707, 'accumulated_logging_time': 0.07344627380371094}
I0307 01:21:51.510390 140286718113536 logging_writer.py:48] [2723] accumulated_eval_time=166.769, accumulated_logging_time=0.0734463, accumulated_submission_time=1079.79, global_step=2723, preemption_count=0, score=1079.79, test/accuracy=0.0989, test/loss=4.93083, test/num_examples=10000, total_duration=1246.98, train/accuracy=0.160276, train/loss=4.4233, validation/accuracy=0.13836, validation/loss=4.57039, validation/num_examples=50000
I0307 01:22:21.514586 140286709720832 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.558844566345215, loss=5.128941535949707
I0307 01:22:59.537570 140286718113536 logging_writer.py:48] [2900] global_step=2900, grad_norm=7.247929096221924, loss=5.158764839172363
I0307 01:23:37.550155 140286709720832 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.9976966381073, loss=5.142644882202148
I0307 01:24:15.959177 140286718113536 logging_writer.py:48] [3100] global_step=3100, grad_norm=5.695166110992432, loss=5.081337928771973
I0307 01:24:54.373683 140286709720832 logging_writer.py:48] [3200] global_step=3200, grad_norm=5.435308456420898, loss=5.003199577331543
I0307 01:25:32.247783 140286718113536 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.5540666580200195, loss=4.935771465301514
I0307 01:26:10.407115 140286709720832 logging_writer.py:48] [3400] global_step=3400, grad_norm=6.2933349609375, loss=4.973268508911133
I0307 01:26:48.521760 140286718113536 logging_writer.py:48] [3500] global_step=3500, grad_norm=7.080990314483643, loss=4.789222240447998
I0307 01:27:26.813289 140286709720832 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.885681629180908, loss=4.770713806152344
I0307 01:28:04.575023 140286718113536 logging_writer.py:48] [3700] global_step=3700, grad_norm=7.874936103820801, loss=4.765507221221924
I0307 01:28:42.546044 140286709720832 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.078790187835693, loss=4.724145412445068
I0307 01:29:20.357713 140286718113536 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.469127178192139, loss=4.619626998901367
I0307 01:29:58.654443 140286709720832 logging_writer.py:48] [4000] global_step=4000, grad_norm=6.624565124511719, loss=4.744354248046875
I0307 01:30:21.690469 140441807221952 spec.py:321] Evaluating on the training split.
I0307 01:30:33.443019 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 01:30:59.147614 140441807221952 spec.py:349] Evaluating on the test split.
I0307 01:31:00.974574 140441807221952 submission_runner.py:469] Time since start: 1796.48s, 	Step: 4061, 	{'train/accuracy': 0.2468710094690323, 'train/loss': 3.7364847660064697, 'validation/accuracy': 0.21823999285697937, 'validation/loss': 3.923434019088745, 'validation/num_examples': 50000, 'test/accuracy': 0.1648000031709671, 'test/loss': 4.43960428237915, 'test/num_examples': 10000, 'score': 1589.8382947444916, 'total_duration': 1796.4788868427277, 'accumulated_submission_time': 1589.8382947444916, 'accumulated_eval_time': 206.05336236953735, 'accumulated_logging_time': 0.11658859252929688}
I0307 01:31:01.022655 140286718113536 logging_writer.py:48] [4061] accumulated_eval_time=206.053, accumulated_logging_time=0.116589, accumulated_submission_time=1589.84, global_step=4061, preemption_count=0, score=1589.84, test/accuracy=0.1648, test/loss=4.4396, test/num_examples=10000, total_duration=1796.48, train/accuracy=0.246871, train/loss=3.73648, validation/accuracy=0.21824, validation/loss=3.92343, validation/num_examples=50000
I0307 01:31:16.269818 140286709720832 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.504387855529785, loss=4.567761421203613
I0307 01:31:54.383115 140286718113536 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.7164571285247803, loss=4.575310230255127
I0307 01:32:32.158065 140286709720832 logging_writer.py:48] [4300] global_step=4300, grad_norm=5.332082748413086, loss=4.5254435539245605
I0307 01:33:10.307935 140286718113536 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.264992713928223, loss=4.522636413574219
I0307 01:33:48.611801 140286709720832 logging_writer.py:48] [4500] global_step=4500, grad_norm=6.269865036010742, loss=4.485287189483643
I0307 01:34:26.700274 140286718113536 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.738192081451416, loss=4.393563270568848
I0307 01:35:04.667203 140286709720832 logging_writer.py:48] [4700] global_step=4700, grad_norm=6.790830612182617, loss=4.454888820648193
I0307 01:35:42.845080 140286718113536 logging_writer.py:48] [4800] global_step=4800, grad_norm=5.707364082336426, loss=4.352209568023682
I0307 01:36:20.990689 140286709720832 logging_writer.py:48] [4900] global_step=4900, grad_norm=7.826595783233643, loss=4.410211563110352
I0307 01:36:59.020649 140286718113536 logging_writer.py:48] [5000] global_step=5000, grad_norm=6.030762195587158, loss=4.228542804718018
I0307 01:37:37.214772 140286709720832 logging_writer.py:48] [5100] global_step=5100, grad_norm=6.2865400314331055, loss=4.232460021972656
I0307 01:38:15.249662 140286718113536 logging_writer.py:48] [5200] global_step=5200, grad_norm=9.783402442932129, loss=4.166717052459717
I0307 01:38:53.172220 140286709720832 logging_writer.py:48] [5300] global_step=5300, grad_norm=5.6710944175720215, loss=4.173815727233887
I0307 01:39:30.934598 140286718113536 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.954745292663574, loss=4.238461494445801
I0307 01:39:31.291087 140441807221952 spec.py:321] Evaluating on the training split.
I0307 01:39:42.468954 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 01:40:06.870923 140441807221952 spec.py:349] Evaluating on the test split.
I0307 01:40:08.710662 140441807221952 submission_runner.py:469] Time since start: 2344.21s, 	Step: 5402, 	{'train/accuracy': 0.3337252736091614, 'train/loss': 3.171891450881958, 'validation/accuracy': 0.2961199879646301, 'validation/loss': 3.382331371307373, 'validation/num_examples': 50000, 'test/accuracy': 0.21650001406669617, 'test/loss': 3.9546666145324707, 'test/num_examples': 10000, 'score': 2099.9575295448303, 'total_duration': 2344.214997768402, 'accumulated_submission_time': 2099.9575295448303, 'accumulated_eval_time': 243.47290802001953, 'accumulated_logging_time': 0.18411469459533691}
I0307 01:40:08.720560 140286709720832 logging_writer.py:48] [5402] accumulated_eval_time=243.473, accumulated_logging_time=0.184115, accumulated_submission_time=2099.96, global_step=5402, preemption_count=0, score=2099.96, test/accuracy=0.2165, test/loss=3.95467, test/num_examples=10000, total_duration=2344.21, train/accuracy=0.333725, train/loss=3.17189, validation/accuracy=0.29612, validation/loss=3.38233, validation/num_examples=50000
I0307 01:40:46.245752 140286718113536 logging_writer.py:48] [5500] global_step=5500, grad_norm=7.263976097106934, loss=4.095211982727051
I0307 01:41:24.337046 140286709720832 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.77316951751709, loss=4.192101955413818
I0307 01:42:02.729264 140286718113536 logging_writer.py:48] [5700] global_step=5700, grad_norm=9.158906936645508, loss=4.168209075927734
I0307 01:42:40.954221 140286709720832 logging_writer.py:48] [5800] global_step=5800, grad_norm=8.989973068237305, loss=4.087770462036133
I0307 01:43:19.073283 140286718113536 logging_writer.py:48] [5900] global_step=5900, grad_norm=6.432831287384033, loss=4.060121536254883
I0307 01:43:57.388691 140286709720832 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.966743469238281, loss=4.04160737991333
I0307 01:44:35.568675 140286718113536 logging_writer.py:48] [6100] global_step=6100, grad_norm=7.958139896392822, loss=4.0141401290893555
I0307 01:45:13.608187 140286709720832 logging_writer.py:48] [6200] global_step=6200, grad_norm=8.167445182800293, loss=3.920927047729492
I0307 01:45:51.619263 140286718113536 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.7687785625457764, loss=3.9445595741271973
I0307 01:46:29.774543 140286709720832 logging_writer.py:48] [6400] global_step=6400, grad_norm=5.171440124511719, loss=3.96231746673584
I0307 01:47:07.995985 140286718113536 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.379876613616943, loss=3.8700034618377686
I0307 01:47:46.267517 140286709720832 logging_writer.py:48] [6600] global_step=6600, grad_norm=6.899600505828857, loss=3.8754429817199707
I0307 01:48:24.154534 140286718113536 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.293696880340576, loss=3.8667802810668945
I0307 01:48:39.054542 140441807221952 spec.py:321] Evaluating on the training split.
I0307 01:48:50.797688 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 01:49:13.613600 140441807221952 spec.py:349] Evaluating on the test split.
I0307 01:49:15.547043 140441807221952 submission_runner.py:469] Time since start: 2891.05s, 	Step: 6740, 	{'train/accuracy': 0.41338488459587097, 'train/loss': 2.7279350757598877, 'validation/accuracy': 0.3744199872016907, 'validation/loss': 2.935438394546509, 'validation/num_examples': 50000, 'test/accuracy': 0.2815000116825104, 'test/loss': 3.5451743602752686, 'test/num_examples': 10000, 'score': 2610.165326356888, 'total_duration': 2891.051376104355, 'accumulated_submission_time': 2610.165326356888, 'accumulated_eval_time': 279.9653720855713, 'accumulated_logging_time': 0.20302915573120117}
I0307 01:49:15.610392 140286709720832 logging_writer.py:48] [6740] accumulated_eval_time=279.965, accumulated_logging_time=0.203029, accumulated_submission_time=2610.17, global_step=6740, preemption_count=0, score=2610.17, test/accuracy=0.2815, test/loss=3.54517, test/num_examples=10000, total_duration=2891.05, train/accuracy=0.413385, train/loss=2.72794, validation/accuracy=0.37442, validation/loss=2.93544, validation/num_examples=50000
I0307 01:49:38.764763 140286718113536 logging_writer.py:48] [6800] global_step=6800, grad_norm=6.303064346313477, loss=3.8735170364379883
I0307 01:50:16.698115 140286709720832 logging_writer.py:48] [6900] global_step=6900, grad_norm=6.337143421173096, loss=3.6878857612609863
I0307 01:50:55.120284 140286718113536 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.114985942840576, loss=3.7541909217834473
I0307 01:51:33.340297 140286709720832 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.703003406524658, loss=3.6796987056732178
I0307 01:52:11.361089 140286718113536 logging_writer.py:48] [7200] global_step=7200, grad_norm=7.193143844604492, loss=3.7688541412353516
I0307 01:52:49.449370 140286709720832 logging_writer.py:48] [7300] global_step=7300, grad_norm=7.143012046813965, loss=3.6733832359313965
I0307 01:53:27.545294 140286718113536 logging_writer.py:48] [7400] global_step=7400, grad_norm=6.724180698394775, loss=3.7681846618652344
I0307 01:54:05.898446 140286709720832 logging_writer.py:48] [7500] global_step=7500, grad_norm=6.934002876281738, loss=3.775878667831421
I0307 01:54:43.947903 140286718113536 logging_writer.py:48] [7600] global_step=7600, grad_norm=5.949797630310059, loss=3.736253023147583
I0307 01:55:22.008775 140286709720832 logging_writer.py:48] [7700] global_step=7700, grad_norm=5.295412540435791, loss=3.690199851989746
I0307 01:56:00.058694 140286718113536 logging_writer.py:48] [7800] global_step=7800, grad_norm=6.297497272491455, loss=3.735903739929199
I0307 01:56:38.414213 140286709720832 logging_writer.py:48] [7900] global_step=7900, grad_norm=9.360600471496582, loss=3.662865161895752
I0307 01:57:16.473940 140286718113536 logging_writer.py:48] [8000] global_step=8000, grad_norm=8.38803482055664, loss=3.556975841522217
I0307 01:57:45.567774 140441807221952 spec.py:321] Evaluating on the training split.
I0307 01:57:58.185047 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 01:58:20.482578 140441807221952 spec.py:349] Evaluating on the test split.
I0307 01:58:22.302931 140441807221952 submission_runner.py:469] Time since start: 3437.81s, 	Step: 8078, 	{'train/accuracy': 0.46257174015045166, 'train/loss': 2.4476497173309326, 'validation/accuracy': 0.4191199839115143, 'validation/loss': 2.6637425422668457, 'validation/num_examples': 50000, 'test/accuracy': 0.31940001249313354, 'test/loss': 3.3245441913604736, 'test/num_examples': 10000, 'score': 3119.9823246002197, 'total_duration': 3437.807263612747, 'accumulated_submission_time': 3119.9823246002197, 'accumulated_eval_time': 316.70049834251404, 'accumulated_logging_time': 0.28307342529296875}
I0307 01:58:22.346538 140286709720832 logging_writer.py:48] [8078] accumulated_eval_time=316.7, accumulated_logging_time=0.283073, accumulated_submission_time=3119.98, global_step=8078, preemption_count=0, score=3119.98, test/accuracy=0.3194, test/loss=3.32454, test/num_examples=10000, total_duration=3437.81, train/accuracy=0.462572, train/loss=2.44765, validation/accuracy=0.41912, validation/loss=2.66374, validation/num_examples=50000
I0307 01:58:31.082679 140286718113536 logging_writer.py:48] [8100] global_step=8100, grad_norm=8.18902587890625, loss=3.668788433074951
I0307 01:59:09.294302 140286709720832 logging_writer.py:48] [8200] global_step=8200, grad_norm=7.559918403625488, loss=3.52492094039917
I0307 01:59:47.336475 140286718113536 logging_writer.py:48] [8300] global_step=8300, grad_norm=6.611817836761475, loss=3.553515911102295
I0307 02:00:25.744389 140286709720832 logging_writer.py:48] [8400] global_step=8400, grad_norm=9.342528343200684, loss=3.6612095832824707
I0307 02:01:03.951034 140286718113536 logging_writer.py:48] [8500] global_step=8500, grad_norm=6.346855640411377, loss=3.6155636310577393
I0307 02:01:42.119827 140286709720832 logging_writer.py:48] [8600] global_step=8600, grad_norm=4.589868068695068, loss=3.5137581825256348
I0307 02:02:20.455337 140286718113536 logging_writer.py:48] [8700] global_step=8700, grad_norm=7.718297004699707, loss=3.4220967292785645
I0307 02:02:58.352330 140286709720832 logging_writer.py:48] [8800] global_step=8800, grad_norm=5.137820720672607, loss=3.534688711166382
I0307 02:03:36.220369 140286718113536 logging_writer.py:48] [8900] global_step=8900, grad_norm=5.830598831176758, loss=3.4255125522613525
I0307 02:04:14.709062 140286709720832 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.957871198654175, loss=3.522571086883545
I0307 02:04:52.810800 140286718113536 logging_writer.py:48] [9100] global_step=9100, grad_norm=8.512601852416992, loss=3.4146432876586914
I0307 02:05:30.642677 140286709720832 logging_writer.py:48] [9200] global_step=9200, grad_norm=6.20466423034668, loss=3.4822700023651123
I0307 02:06:08.524413 140286718113536 logging_writer.py:48] [9300] global_step=9300, grad_norm=8.538886070251465, loss=3.4363853931427
I0307 02:06:46.643808 140286709720832 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.92566442489624, loss=3.484483242034912
I0307 02:06:52.404863 140441807221952 spec.py:321] Evaluating on the training split.
I0307 02:07:05.341480 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 02:07:29.003746 140441807221952 spec.py:349] Evaluating on the test split.
I0307 02:07:30.862588 140441807221952 submission_runner.py:469] Time since start: 3986.37s, 	Step: 9416, 	{'train/accuracy': 0.5102439522743225, 'train/loss': 2.1807687282562256, 'validation/accuracy': 0.4664599895477295, 'validation/loss': 2.418881416320801, 'validation/num_examples': 50000, 'test/accuracy': 0.3565000295639038, 'test/loss': 3.0698392391204834, 'test/num_examples': 10000, 'score': 3629.902883529663, 'total_duration': 3986.3669214248657, 'accumulated_submission_time': 3629.902883529663, 'accumulated_eval_time': 355.1581892967224, 'accumulated_logging_time': 0.34018468856811523}
I0307 02:07:30.897791 140286718113536 logging_writer.py:48] [9416] accumulated_eval_time=355.158, accumulated_logging_time=0.340185, accumulated_submission_time=3629.9, global_step=9416, preemption_count=0, score=3629.9, test/accuracy=0.3565, test/loss=3.06984, test/num_examples=10000, total_duration=3986.37, train/accuracy=0.510244, train/loss=2.18077, validation/accuracy=0.46646, validation/loss=2.41888, validation/num_examples=50000
I0307 02:08:03.061798 140286709720832 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.478185653686523, loss=3.4529898166656494
I0307 02:08:41.510068 140286718113536 logging_writer.py:48] [9600] global_step=9600, grad_norm=7.032705307006836, loss=3.379143476486206
I0307 02:09:19.493678 140286709720832 logging_writer.py:48] [9700] global_step=9700, grad_norm=5.9577178955078125, loss=3.3090100288391113
I0307 02:09:57.709699 140286718113536 logging_writer.py:48] [9800] global_step=9800, grad_norm=6.319032192230225, loss=3.429037094116211
I0307 02:10:35.747457 140286709720832 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.262283802032471, loss=3.2984023094177246
I0307 02:11:14.052091 140286718113536 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.802354335784912, loss=3.454362154006958
I0307 02:11:52.227331 140286709720832 logging_writer.py:48] [10100] global_step=10100, grad_norm=7.702557563781738, loss=3.320089817047119
I0307 02:12:30.291550 140286718113536 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.683308124542236, loss=3.3703014850616455
I0307 02:13:08.707469 140286709720832 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.655044078826904, loss=3.2968294620513916
I0307 02:13:47.308364 140286718113536 logging_writer.py:48] [10400] global_step=10400, grad_norm=6.149105072021484, loss=3.3815486431121826
I0307 02:14:25.361220 140286709720832 logging_writer.py:48] [10500] global_step=10500, grad_norm=6.761172771453857, loss=3.4183754920959473
I0307 02:15:02.023048 140286718113536 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.904218912124634, loss=3.2439522743225098
I0307 02:15:39.429878 140286709720832 logging_writer.py:48] [10700] global_step=10700, grad_norm=5.0125555992126465, loss=3.2275233268737793
I0307 02:16:01.113452 140441807221952 spec.py:321] Evaluating on the training split.
I0307 02:16:15.607129 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 02:16:42.980016 140441807221952 spec.py:349] Evaluating on the test split.
I0307 02:16:44.815117 140441807221952 submission_runner.py:469] Time since start: 4540.32s, 	Step: 10758, 	{'train/accuracy': 0.5457589030265808, 'train/loss': 2.0431270599365234, 'validation/accuracy': 0.49823999404907227, 'validation/loss': 2.268129587173462, 'validation/num_examples': 50000, 'test/accuracy': 0.3832000195980072, 'test/loss': 2.920487642288208, 'test/num_examples': 10000, 'score': 4139.9532606601715, 'total_duration': 4540.319451332092, 'accumulated_submission_time': 4139.9532606601715, 'accumulated_eval_time': 398.85981941223145, 'accumulated_logging_time': 0.39961886405944824}
I0307 02:16:44.897846 140286718113536 logging_writer.py:48] [10758] accumulated_eval_time=398.86, accumulated_logging_time=0.399619, accumulated_submission_time=4139.95, global_step=10758, preemption_count=0, score=4139.95, test/accuracy=0.3832, test/loss=2.92049, test/num_examples=10000, total_duration=4540.32, train/accuracy=0.545759, train/loss=2.04313, validation/accuracy=0.49824, validation/loss=2.26813, validation/num_examples=50000
I0307 02:17:01.376004 140286709720832 logging_writer.py:48] [10800] global_step=10800, grad_norm=5.459064483642578, loss=3.254363536834717
I0307 02:17:39.921630 140286718113536 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.708284378051758, loss=3.3315062522888184
I0307 02:18:18.969562 140286709720832 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.541443824768066, loss=3.253389358520508
I0307 02:18:57.289675 140286718113536 logging_writer.py:48] [11100] global_step=11100, grad_norm=6.356518268585205, loss=3.27479887008667
I0307 02:19:35.478083 140286709720832 logging_writer.py:48] [11200] global_step=11200, grad_norm=5.504539489746094, loss=3.204516887664795
I0307 02:20:14.138055 140286718113536 logging_writer.py:48] [11300] global_step=11300, grad_norm=11.14477252960205, loss=3.2421529293060303
I0307 02:20:52.772608 140286709720832 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.966871738433838, loss=3.240880250930786
I0307 02:21:31.240918 140286718113536 logging_writer.py:48] [11500] global_step=11500, grad_norm=4.5891242027282715, loss=3.251840591430664
I0307 02:22:09.626089 140286709720832 logging_writer.py:48] [11600] global_step=11600, grad_norm=6.332174301147461, loss=3.2339136600494385
I0307 02:22:48.156452 140286718113536 logging_writer.py:48] [11700] global_step=11700, grad_norm=4.084080696105957, loss=3.2548744678497314
I0307 02:23:26.607081 140286709720832 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.567527770996094, loss=3.3006317615509033
I0307 02:24:04.710015 140286718113536 logging_writer.py:48] [11900] global_step=11900, grad_norm=5.868065357208252, loss=3.256627321243286
I0307 02:24:42.932775 140286709720832 logging_writer.py:48] [12000] global_step=12000, grad_norm=9.008437156677246, loss=3.2048702239990234
I0307 02:25:14.823055 140441807221952 spec.py:321] Evaluating on the training split.
I0307 02:25:33.896521 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 02:26:01.347987 140441807221952 spec.py:349] Evaluating on the test split.
I0307 02:26:03.173005 140441807221952 submission_runner.py:469] Time since start: 5098.63s, 	Step: 12084, 	{'train/accuracy': 0.5666852593421936, 'train/loss': 1.9149045944213867, 'validation/accuracy': 0.5190799832344055, 'validation/loss': 2.1563785076141357, 'validation/num_examples': 50000, 'test/accuracy': 0.4027000069618225, 'test/loss': 2.8268380165100098, 'test/num_examples': 10000, 'score': 4649.722097635269, 'total_duration': 5098.629235982895, 'accumulated_submission_time': 4649.722097635269, 'accumulated_eval_time': 447.16163396835327, 'accumulated_logging_time': 0.4916808605194092}
I0307 02:26:03.418646 140286718113536 logging_writer.py:48] [12084] accumulated_eval_time=447.162, accumulated_logging_time=0.491681, accumulated_submission_time=4649.72, global_step=12084, preemption_count=0, score=4649.72, test/accuracy=0.4027, test/loss=2.82684, test/num_examples=10000, total_duration=5098.63, train/accuracy=0.566685, train/loss=1.9149, validation/accuracy=0.51908, validation/loss=2.15638, validation/num_examples=50000
I0307 02:26:09.880594 140286709720832 logging_writer.py:48] [12100] global_step=12100, grad_norm=7.976348876953125, loss=3.1675853729248047
I0307 02:26:48.327594 140286718113536 logging_writer.py:48] [12200] global_step=12200, grad_norm=5.718511581420898, loss=3.1406450271606445
I0307 02:27:26.490507 140286709720832 logging_writer.py:48] [12300] global_step=12300, grad_norm=8.391427040100098, loss=3.205369710922241
I0307 02:28:04.467293 140286718113536 logging_writer.py:48] [12400] global_step=12400, grad_norm=5.882297039031982, loss=3.1047284603118896
I0307 02:28:42.989350 140286709720832 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.67548131942749, loss=3.147752285003662
I0307 02:29:21.680111 140286718113536 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.667942047119141, loss=3.198281764984131
I0307 02:29:59.910390 140286709720832 logging_writer.py:48] [12700] global_step=12700, grad_norm=7.199463367462158, loss=3.079557418823242
I0307 02:30:38.012206 140286718113536 logging_writer.py:48] [12800] global_step=12800, grad_norm=6.262989521026611, loss=3.2173287868499756
I0307 02:31:17.143731 140286709720832 logging_writer.py:48] [12900] global_step=12900, grad_norm=10.950761795043945, loss=3.3043975830078125
I0307 02:31:55.351760 140286718113536 logging_writer.py:48] [13000] global_step=13000, grad_norm=8.654533386230469, loss=3.134300947189331
I0307 02:32:33.604359 140286709720832 logging_writer.py:48] [13100] global_step=13100, grad_norm=5.485382556915283, loss=3.1225686073303223
I0307 02:33:11.909861 140286718113536 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.156955242156982, loss=3.1258277893066406
I0307 02:33:51.872872 140286709720832 logging_writer.py:48] [13300] global_step=13300, grad_norm=10.287741661071777, loss=3.158815622329712
I0307 02:34:30.158132 140286718113536 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.489306449890137, loss=3.1632304191589355
I0307 02:34:33.198965 140441807221952 spec.py:321] Evaluating on the training split.
I0307 02:34:50.219660 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 02:35:15.027300 140441807221952 spec.py:349] Evaluating on the test split.
I0307 02:35:16.789402 140441807221952 submission_runner.py:469] Time since start: 5652.29s, 	Step: 13409, 	{'train/accuracy': 0.5757732391357422, 'train/loss': 1.8449188470840454, 'validation/accuracy': 0.5312199592590332, 'validation/loss': 2.069065809249878, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.76556658744812, 'test/num_examples': 10000, 'score': 5159.316724538803, 'total_duration': 5652.29358792305, 'accumulated_submission_time': 5159.316724538803, 'accumulated_eval_time': 490.75188994407654, 'accumulated_logging_time': 0.7742214202880859}
I0307 02:35:16.842010 140286709720832 logging_writer.py:48] [13409] accumulated_eval_time=490.752, accumulated_logging_time=0.774221, accumulated_submission_time=5159.32, global_step=13409, preemption_count=0, score=5159.32, test/accuracy=0.4086, test/loss=2.76557, test/num_examples=10000, total_duration=5652.29, train/accuracy=0.575773, train/loss=1.84492, validation/accuracy=0.53122, validation/loss=2.06907, validation/num_examples=50000
I0307 02:35:51.585474 140286718113536 logging_writer.py:48] [13500] global_step=13500, grad_norm=8.24843692779541, loss=3.241565227508545
I0307 02:36:29.577989 140286709720832 logging_writer.py:48] [13600] global_step=13600, grad_norm=4.89341926574707, loss=3.08516263961792
I0307 02:37:07.714150 140286718113536 logging_writer.py:48] [13700] global_step=13700, grad_norm=6.712615489959717, loss=3.0431034564971924
I0307 02:37:46.575727 140286709720832 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.997361898422241, loss=3.1040334701538086
I0307 02:38:24.546053 140286718113536 logging_writer.py:48] [13900] global_step=13900, grad_norm=8.789813041687012, loss=3.0868570804595947
I0307 02:39:02.961313 140286709720832 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.2232842445373535, loss=3.189683437347412
I0307 02:39:41.113455 140286718113536 logging_writer.py:48] [14100] global_step=14100, grad_norm=6.543583869934082, loss=3.1287424564361572
I0307 02:40:19.458382 140286709720832 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.812878131866455, loss=3.067303419113159
I0307 02:40:57.599335 140286718113536 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.4557275772094727, loss=3.050065040588379
I0307 02:41:35.687413 140286709720832 logging_writer.py:48] [14400] global_step=14400, grad_norm=5.8157148361206055, loss=3.1543262004852295
I0307 02:42:14.349165 140286718113536 logging_writer.py:48] [14500] global_step=14500, grad_norm=7.273156642913818, loss=3.092313289642334
I0307 02:42:52.421971 140286709720832 logging_writer.py:48] [14600] global_step=14600, grad_norm=4.1916632652282715, loss=3.1191458702087402
I0307 02:43:30.651003 140286718113536 logging_writer.py:48] [14700] global_step=14700, grad_norm=5.679533004760742, loss=3.0323519706726074
I0307 02:43:47.024214 140441807221952 spec.py:321] Evaluating on the training split.
I0307 02:44:05.067967 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 02:44:30.326448 140441807221952 spec.py:349] Evaluating on the test split.
I0307 02:44:32.088770 140441807221952 submission_runner.py:469] Time since start: 6207.59s, 	Step: 14744, 	{'train/accuracy': 0.5953443646430969, 'train/loss': 1.7729371786117554, 'validation/accuracy': 0.5464999675750732, 'validation/loss': 2.009995937347412, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.664354085922241, 'test/num_examples': 10000, 'score': 5669.328103303909, 'total_duration': 6207.592904090881, 'accumulated_submission_time': 5669.328103303909, 'accumulated_eval_time': 535.8162145614624, 'accumulated_logging_time': 0.8554446697235107}
I0307 02:44:32.198955 140286709720832 logging_writer.py:48] [14744] accumulated_eval_time=535.816, accumulated_logging_time=0.855445, accumulated_submission_time=5669.33, global_step=14744, preemption_count=0, score=5669.33, test/accuracy=0.4294, test/loss=2.66435, test/num_examples=10000, total_duration=6207.59, train/accuracy=0.595344, train/loss=1.77294, validation/accuracy=0.5465, validation/loss=2.01, validation/num_examples=50000
I0307 02:44:54.070753 140286718113536 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.243923187255859, loss=3.0519027709960938
I0307 02:45:32.285878 140286709720832 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.804598331451416, loss=3.0768423080444336
I0307 02:46:10.536503 140286718113536 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.915531873703003, loss=3.127328395843506
I0307 02:46:49.040990 140286709720832 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.866739749908447, loss=3.1117284297943115
I0307 02:47:27.229068 140286718113536 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.886852979660034, loss=2.9562745094299316
I0307 02:48:05.539288 140286709720832 logging_writer.py:48] [15300] global_step=15300, grad_norm=5.369106769561768, loss=2.9913177490234375
I0307 02:48:43.894364 140286718113536 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.607060432434082, loss=3.077118158340454
I0307 02:49:22.007328 140286709720832 logging_writer.py:48] [15500] global_step=15500, grad_norm=7.585596561431885, loss=2.992232322692871
I0307 02:50:00.135717 140286718113536 logging_writer.py:48] [15600] global_step=15600, grad_norm=5.64902925491333, loss=3.099060297012329
I0307 02:50:38.486005 140286709720832 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.970323324203491, loss=2.9573991298675537
I0307 02:51:16.601623 140286718113536 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.93895149230957, loss=2.943131923675537
I0307 02:51:54.815847 140286709720832 logging_writer.py:48] [15900] global_step=15900, grad_norm=9.016685485839844, loss=3.060670852661133
I0307 02:52:33.194938 140286718113536 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.3765721321105957, loss=2.9420223236083984
I0307 02:53:02.155157 140441807221952 spec.py:321] Evaluating on the training split.
I0307 02:53:21.838019 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 02:53:46.068383 140441807221952 spec.py:349] Evaluating on the test split.
I0307 02:53:47.833589 140441807221952 submission_runner.py:469] Time since start: 6763.34s, 	Step: 16077, 	{'train/accuracy': 0.6067641973495483, 'train/loss': 1.731878638267517, 'validation/accuracy': 0.5576199889183044, 'validation/loss': 1.9619148969650269, 'validation/num_examples': 50000, 'test/accuracy': 0.43380001187324524, 'test/loss': 2.630448579788208, 'test/num_examples': 10000, 'score': 6179.088294744492, 'total_duration': 6763.337777853012, 'accumulated_submission_time': 6179.088294744492, 'accumulated_eval_time': 581.494470834732, 'accumulated_logging_time': 1.0182383060455322}
I0307 02:53:48.091136 140286709720832 logging_writer.py:48] [16077] accumulated_eval_time=581.494, accumulated_logging_time=1.01824, accumulated_submission_time=6179.09, global_step=16077, preemption_count=0, score=6179.09, test/accuracy=0.4338, test/loss=2.63045, test/num_examples=10000, total_duration=6763.34, train/accuracy=0.606764, train/loss=1.73188, validation/accuracy=0.55762, validation/loss=1.96191, validation/num_examples=50000
I0307 02:53:57.273112 140286718113536 logging_writer.py:48] [16100] global_step=16100, grad_norm=5.748347282409668, loss=3.0655531883239746
I0307 02:54:35.248453 140286709720832 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.852004528045654, loss=2.9790477752685547
I0307 02:55:13.371967 140286718113536 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.272985935211182, loss=3.041661500930786
I0307 02:55:51.339257 140286709720832 logging_writer.py:48] [16400] global_step=16400, grad_norm=4.944154262542725, loss=3.0547640323638916
I0307 02:56:29.474883 140286718113536 logging_writer.py:48] [16500] global_step=16500, grad_norm=5.8557515144348145, loss=3.011960506439209
I0307 02:57:07.476792 140286709720832 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.726068019866943, loss=2.9777817726135254
I0307 02:57:45.589811 140286718113536 logging_writer.py:48] [16700] global_step=16700, grad_norm=7.0152130126953125, loss=3.0073578357696533
I0307 02:58:24.202119 140286709720832 logging_writer.py:48] [16800] global_step=16800, grad_norm=5.43702507019043, loss=3.0307154655456543
I0307 02:59:02.470113 140286718113536 logging_writer.py:48] [16900] global_step=16900, grad_norm=4.3956708908081055, loss=3.0930588245391846
I0307 02:59:40.975843 140286709720832 logging_writer.py:48] [17000] global_step=17000, grad_norm=6.672926902770996, loss=3.0316033363342285
I0307 03:00:18.989459 140286718113536 logging_writer.py:48] [17100] global_step=17100, grad_norm=4.577980995178223, loss=3.0027239322662354
I0307 03:00:58.047338 140286709720832 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.821355104446411, loss=2.9489362239837646
I0307 03:01:37.477725 140286718113536 logging_writer.py:48] [17300] global_step=17300, grad_norm=5.1392998695373535, loss=3.008862257003784
I0307 03:02:15.846258 140286709720832 logging_writer.py:48] [17400] global_step=17400, grad_norm=4.720167636871338, loss=3.0405986309051514
I0307 03:02:18.240130 140441807221952 spec.py:321] Evaluating on the training split.
I0307 03:02:34.251559 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 03:02:57.268171 140441807221952 spec.py:349] Evaluating on the test split.
I0307 03:02:59.038781 140441807221952 submission_runner.py:469] Time since start: 7314.54s, 	Step: 17407, 	{'train/accuracy': 0.6162906289100647, 'train/loss': 1.6918208599090576, 'validation/accuracy': 0.5649999976158142, 'validation/loss': 1.9248992204666138, 'validation/num_examples': 50000, 'test/accuracy': 0.4497000277042389, 'test/loss': 2.5673370361328125, 'test/num_examples': 10000, 'score': 6689.049119949341, 'total_duration': 7314.5429520606995, 'accumulated_submission_time': 6689.049119949341, 'accumulated_eval_time': 622.2929475307465, 'accumulated_logging_time': 1.319702386856079}
I0307 03:02:59.114523 140286718113536 logging_writer.py:48] [17407] accumulated_eval_time=622.293, accumulated_logging_time=1.3197, accumulated_submission_time=6689.05, global_step=17407, preemption_count=0, score=6689.05, test/accuracy=0.4497, test/loss=2.56734, test/num_examples=10000, total_duration=7314.54, train/accuracy=0.616291, train/loss=1.69182, validation/accuracy=0.565, validation/loss=1.9249, validation/num_examples=50000
I0307 03:03:35.290480 140286709720832 logging_writer.py:48] [17500] global_step=17500, grad_norm=5.4155988693237305, loss=2.979497194290161
I0307 03:04:14.143967 140286718113536 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.8579630851745605, loss=2.9609763622283936
I0307 03:04:52.201846 140286709720832 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.4076614379882812, loss=3.0694665908813477
I0307 03:05:30.425689 140286718113536 logging_writer.py:48] [17800] global_step=17800, grad_norm=5.370155334472656, loss=2.924025774002075
I0307 03:06:08.884195 140286709720832 logging_writer.py:48] [17900] global_step=17900, grad_norm=5.99316930770874, loss=3.008342742919922
I0307 03:06:47.601069 140286718113536 logging_writer.py:48] [18000] global_step=18000, grad_norm=5.563685894012451, loss=3.0497398376464844
I0307 03:07:26.039739 140286709720832 logging_writer.py:48] [18100] global_step=18100, grad_norm=4.127371788024902, loss=2.986072301864624
I0307 03:08:04.298061 140286718113536 logging_writer.py:48] [18200] global_step=18200, grad_norm=7.973081111907959, loss=3.0517773628234863
I0307 03:08:42.830356 140286709720832 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.651825189590454, loss=2.9431819915771484
I0307 03:09:21.310100 140286718113536 logging_writer.py:48] [18400] global_step=18400, grad_norm=4.858179569244385, loss=2.9647421836853027
I0307 03:09:59.452632 140286709720832 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.502832412719727, loss=3.023409366607666
I0307 03:10:37.750383 140286718113536 logging_writer.py:48] [18600] global_step=18600, grad_norm=8.672981262207031, loss=2.9865622520446777
I0307 03:11:16.204990 140286709720832 logging_writer.py:48] [18700] global_step=18700, grad_norm=5.2926411628723145, loss=3.1148486137390137
I0307 03:11:29.218130 140441807221952 spec.py:321] Evaluating on the training split.
I0307 03:11:44.864097 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 03:12:09.902385 140441807221952 spec.py:349] Evaluating on the test split.
I0307 03:12:11.677038 140441807221952 submission_runner.py:469] Time since start: 7867.18s, 	Step: 18735, 	{'train/accuracy': 0.6155332922935486, 'train/loss': 1.6905982494354248, 'validation/accuracy': 0.5715199708938599, 'validation/loss': 1.8928349018096924, 'validation/num_examples': 50000, 'test/accuracy': 0.44440001249313354, 'test/loss': 2.5997352600097656, 'test/num_examples': 10000, 'score': 7198.982068538666, 'total_duration': 7867.1813633441925, 'accumulated_submission_time': 7198.982068538666, 'accumulated_eval_time': 664.7518112659454, 'accumulated_logging_time': 1.4425053596496582}
I0307 03:12:11.728229 140286718113536 logging_writer.py:48] [18735] accumulated_eval_time=664.752, accumulated_logging_time=1.44251, accumulated_submission_time=7198.98, global_step=18735, preemption_count=0, score=7198.98, test/accuracy=0.4444, test/loss=2.59974, test/num_examples=10000, total_duration=7867.18, train/accuracy=0.615533, train/loss=1.6906, validation/accuracy=0.57152, validation/loss=1.89283, validation/num_examples=50000
I0307 03:12:37.405183 140286709720832 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.728950023651123, loss=3.0459177494049072
I0307 03:13:15.627437 140286718113536 logging_writer.py:48] [18900] global_step=18900, grad_norm=4.345396995544434, loss=3.0642282962799072
I0307 03:13:53.755781 140286709720832 logging_writer.py:48] [19000] global_step=19000, grad_norm=5.5383758544921875, loss=2.9191555976867676
I0307 03:14:32.465561 140286718113536 logging_writer.py:48] [19100] global_step=19100, grad_norm=4.921992778778076, loss=3.0308449268341064
I0307 03:15:11.119035 140286709720832 logging_writer.py:48] [19200] global_step=19200, grad_norm=4.814098834991455, loss=2.974942207336426
I0307 03:15:49.418013 140286718113536 logging_writer.py:48] [19300] global_step=19300, grad_norm=4.197328090667725, loss=2.9986441135406494
I0307 03:16:27.950525 140286709720832 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.2814180850982666, loss=3.0853805541992188
I0307 03:17:06.395389 140286718113536 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.684138298034668, loss=3.0033459663391113
I0307 03:17:44.849363 140286709720832 logging_writer.py:48] [19600] global_step=19600, grad_norm=4.179633617401123, loss=2.9730899333953857
I0307 03:18:23.337599 140286718113536 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.382141590118408, loss=3.0185792446136475
I0307 03:19:02.097347 140286709720832 logging_writer.py:48] [19800] global_step=19800, grad_norm=5.066102027893066, loss=2.962557792663574
I0307 03:19:40.598889 140286718113536 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.619587421417236, loss=2.9141616821289062
I0307 03:20:19.132252 140286709720832 logging_writer.py:48] [20000] global_step=20000, grad_norm=4.019031047821045, loss=2.991352081298828
I0307 03:20:41.943155 140441807221952 spec.py:321] Evaluating on the training split.
I0307 03:20:58.749851 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 03:21:22.390364 140441807221952 spec.py:349] Evaluating on the test split.
I0307 03:21:24.208019 140441807221952 submission_runner.py:469] Time since start: 8419.71s, 	Step: 20059, 	{'train/accuracy': 0.6094347834587097, 'train/loss': 1.745519757270813, 'validation/accuracy': 0.5615999698638916, 'validation/loss': 1.9652775526046753, 'validation/num_examples': 50000, 'test/accuracy': 0.44280001521110535, 'test/loss': 2.613088607788086, 'test/num_examples': 10000, 'score': 7709.056797742844, 'total_duration': 8419.71234869957, 'accumulated_submission_time': 7709.056797742844, 'accumulated_eval_time': 707.0166399478912, 'accumulated_logging_time': 1.5013160705566406}
I0307 03:21:24.236955 140286718113536 logging_writer.py:48] [20059] accumulated_eval_time=707.017, accumulated_logging_time=1.50132, accumulated_submission_time=7709.06, global_step=20059, preemption_count=0, score=7709.06, test/accuracy=0.4428, test/loss=2.61309, test/num_examples=10000, total_duration=8419.71, train/accuracy=0.609435, train/loss=1.74552, validation/accuracy=0.5616, validation/loss=1.96528, validation/num_examples=50000
I0307 03:21:40.313201 140286709720832 logging_writer.py:48] [20100] global_step=20100, grad_norm=5.4775919914245605, loss=2.9374866485595703
I0307 03:22:18.678179 140286718113536 logging_writer.py:48] [20200] global_step=20200, grad_norm=5.4506378173828125, loss=2.884345293045044
I0307 03:22:57.132135 140286709720832 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.6953113079071045, loss=3.0270590782165527
I0307 03:23:36.010400 140286718113536 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.10545015335083, loss=2.8917417526245117
I0307 03:24:14.499219 140286709720832 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.71779465675354, loss=2.985872268676758
I0307 03:24:52.814827 140286718113536 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.410650730133057, loss=3.04398775100708
I0307 03:25:30.909126 140286709720832 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.2471699714660645, loss=2.9480667114257812
I0307 03:26:09.297268 140286718113536 logging_writer.py:48] [20800] global_step=20800, grad_norm=4.648309230804443, loss=2.9876604080200195
I0307 03:26:47.806154 140286709720832 logging_writer.py:48] [20900] global_step=20900, grad_norm=4.038115501403809, loss=3.027289628982544
I0307 03:27:25.982411 140286718113536 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.024149179458618, loss=2.940426826477051
I0307 03:28:04.316816 140286709720832 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.2580158710479736, loss=2.887042284011841
I0307 03:28:42.277904 140286718113536 logging_writer.py:48] [21200] global_step=21200, grad_norm=4.862061977386475, loss=2.9161741733551025
I0307 03:29:20.961259 140286709720832 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.961735486984253, loss=3.034162998199463
I0307 03:29:54.312799 140441807221952 spec.py:321] Evaluating on the training split.
I0307 03:30:13.152839 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 03:30:35.469875 140441807221952 spec.py:349] Evaluating on the test split.
I0307 03:30:37.290411 140441807221952 submission_runner.py:469] Time since start: 8972.79s, 	Step: 21387, 	{'train/accuracy': 0.6323740482330322, 'train/loss': 1.6397472620010376, 'validation/accuracy': 0.5831999778747559, 'validation/loss': 1.8600594997406006, 'validation/num_examples': 50000, 'test/accuracy': 0.46400001645088196, 'test/loss': 2.525916576385498, 'test/num_examples': 10000, 'score': 8218.98792386055, 'total_duration': 8972.79462981224, 'accumulated_submission_time': 8218.98792386055, 'accumulated_eval_time': 749.994124174118, 'accumulated_logging_time': 1.5383195877075195}
I0307 03:30:37.425177 140286718113536 logging_writer.py:48] [21387] accumulated_eval_time=749.994, accumulated_logging_time=1.53832, accumulated_submission_time=8218.99, global_step=21387, preemption_count=0, score=8218.99, test/accuracy=0.464, test/loss=2.52592, test/num_examples=10000, total_duration=8972.79, train/accuracy=0.632374, train/loss=1.63975, validation/accuracy=0.5832, validation/loss=1.86006, validation/num_examples=50000
I0307 03:30:42.960211 140286709720832 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.2395598888397217, loss=3.009932518005371
I0307 03:31:21.425659 140286718113536 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.403414487838745, loss=2.9484453201293945
I0307 03:31:59.885536 140286709720832 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.3980631828308105, loss=2.896005630493164
I0307 03:32:38.075452 140286718113536 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.214948654174805, loss=2.9411776065826416
I0307 03:33:16.671254 140286709720832 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.700899124145508, loss=3.0168569087982178
I0307 03:33:54.952637 140286718113536 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.5338709354400635, loss=2.93660569190979
I0307 03:34:33.293155 140286709720832 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.225636959075928, loss=2.938735246658325
I0307 03:35:11.781877 140286718113536 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.1100871562957764, loss=2.894702434539795
I0307 03:35:50.022674 140286709720832 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.7856907844543457, loss=2.9438226222991943
I0307 03:36:28.388108 140286718113536 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.1998608112335205, loss=2.9099390506744385
I0307 03:37:06.726838 140286709720832 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.775278329849243, loss=2.895881175994873
I0307 03:37:44.935624 140286718113536 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.2075142860412598, loss=2.9524104595184326
I0307 03:38:23.432188 140286709720832 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.553163528442383, loss=2.8896305561065674
I0307 03:39:02.423344 140286718113536 logging_writer.py:48] [22700] global_step=22700, grad_norm=4.450013160705566, loss=2.938033103942871
I0307 03:39:07.404149 140441807221952 spec.py:321] Evaluating on the training split.
I0307 03:39:22.193032 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 03:39:45.458414 140441807221952 spec.py:349] Evaluating on the test split.
I0307 03:39:47.283337 140441807221952 submission_runner.py:469] Time since start: 9522.79s, 	Step: 22714, 	{'train/accuracy': 0.6126434803009033, 'train/loss': 1.6913046836853027, 'validation/accuracy': 0.5663599967956543, 'validation/loss': 1.9140921831130981, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.6022934913635254, 'test/num_examples': 10000, 'score': 8728.7809882164, 'total_duration': 9522.787516832352, 'accumulated_submission_time': 8728.7809882164, 'accumulated_eval_time': 789.8731219768524, 'accumulated_logging_time': 1.7260372638702393}
I0307 03:39:47.372174 140286709720832 logging_writer.py:48] [22714] accumulated_eval_time=789.873, accumulated_logging_time=1.72604, accumulated_submission_time=8728.78, global_step=22714, preemption_count=0, score=8728.78, test/accuracy=0.4397, test/loss=2.60229, test/num_examples=10000, total_duration=9522.79, train/accuracy=0.612643, train/loss=1.6913, validation/accuracy=0.56636, validation/loss=1.91409, validation/num_examples=50000
I0307 03:40:20.447576 140286718113536 logging_writer.py:48] [22800] global_step=22800, grad_norm=4.495849132537842, loss=2.904547929763794
I0307 03:40:58.888898 140286709720832 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.546114921569824, loss=3.0052523612976074
I0307 03:41:36.991546 140286718113536 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.0956497192382812, loss=2.862027645111084
I0307 03:42:14.981005 140286709720832 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.175897121429443, loss=2.9515557289123535
I0307 03:42:53.442266 140286718113536 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.853860855102539, loss=2.9189987182617188
I0307 03:43:31.843672 140286709720832 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.2049193382263184, loss=2.838855743408203
I0307 03:44:10.229825 140286718113536 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.013932704925537, loss=2.935779333114624
I0307 03:44:48.307053 140286709720832 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.7687268257141113, loss=2.9179868698120117
I0307 03:45:26.441171 140286718113536 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.5808682441711426, loss=2.978754997253418
I0307 03:46:04.820043 140286709720832 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.483355760574341, loss=3.0101795196533203
I0307 03:46:43.290411 140286718113536 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.016472339630127, loss=2.9194207191467285
I0307 03:47:21.666809 140286709720832 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.9742515087127686, loss=2.88874888420105
I0307 03:48:00.084466 140286718113536 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.1977384090423584, loss=2.9289939403533936
I0307 03:48:17.568149 140441807221952 spec.py:321] Evaluating on the training split.
I0307 03:48:31.465919 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 03:48:54.984454 140441807221952 spec.py:349] Evaluating on the test split.
I0307 03:48:56.754120 140441807221952 submission_runner.py:469] Time since start: 10072.26s, 	Step: 24047, 	{'train/accuracy': 0.6250398755073547, 'train/loss': 1.670770525932312, 'validation/accuracy': 0.5806999802589417, 'validation/loss': 1.8805358409881592, 'validation/num_examples': 50000, 'test/accuracy': 0.4545000195503235, 'test/loss': 2.547241449356079, 'test/num_examples': 10000, 'score': 9238.825613737106, 'total_duration': 10072.258450746536, 'accumulated_submission_time': 9238.825613737106, 'accumulated_eval_time': 829.0590574741364, 'accumulated_logging_time': 1.8356165885925293}
I0307 03:48:56.810266 140286709720832 logging_writer.py:48] [24047] accumulated_eval_time=829.059, accumulated_logging_time=1.83562, accumulated_submission_time=9238.83, global_step=24047, preemption_count=0, score=9238.83, test/accuracy=0.4545, test/loss=2.54724, test/num_examples=10000, total_duration=10072.3, train/accuracy=0.62504, train/loss=1.67077, validation/accuracy=0.5807, validation/loss=1.88054, validation/num_examples=50000
I0307 03:49:17.673846 140286718113536 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.097482919692993, loss=2.9047365188598633
I0307 03:49:55.752671 140286709720832 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.4679670333862305, loss=2.931334972381592
I0307 03:50:33.901217 140286718113536 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.4241456985473633, loss=2.9478530883789062
I0307 03:51:12.299798 140286709720832 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.64485764503479, loss=2.862699508666992
I0307 03:51:50.474445 140286718113536 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.213343620300293, loss=2.8382437229156494
I0307 03:52:28.562288 140286709720832 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.000779390335083, loss=2.932772159576416
I0307 03:53:06.966424 140286718113536 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.510819435119629, loss=3.001865863800049
I0307 03:53:45.249294 140286709720832 logging_writer.py:48] [24800] global_step=24800, grad_norm=4.37615966796875, loss=2.8542706966400146
I0307 03:54:23.715576 140286718113536 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.5134215354919434, loss=2.949141502380371
I0307 03:55:02.043855 140286709720832 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.6944327354431152, loss=2.850263833999634
I0307 03:55:40.361855 140286718113536 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.191535234451294, loss=2.9386470317840576
I0307 03:56:18.701149 140286709720832 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.7597172260284424, loss=2.8981363773345947
I0307 03:56:57.105248 140286718113536 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.1750853061676025, loss=2.8677828311920166
I0307 03:57:26.989883 140441807221952 spec.py:321] Evaluating on the training split.
I0307 03:57:42.914879 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 03:58:07.806458 140441807221952 spec.py:349] Evaluating on the test split.
I0307 03:58:09.572671 140441807221952 submission_runner.py:469] Time since start: 10625.08s, 	Step: 25378, 	{'train/accuracy': 0.6317163705825806, 'train/loss': 1.5977365970611572, 'validation/accuracy': 0.5841599702835083, 'validation/loss': 1.8265553712844849, 'validation/num_examples': 50000, 'test/accuracy': 0.4545000195503235, 'test/loss': 2.5185999870300293, 'test/num_examples': 10000, 'score': 9748.862476348877, 'total_duration': 10625.076827526093, 'accumulated_submission_time': 9748.862476348877, 'accumulated_eval_time': 871.6416375637054, 'accumulated_logging_time': 1.8998379707336426}
I0307 03:58:09.635893 140286709720832 logging_writer.py:48] [25378] accumulated_eval_time=871.642, accumulated_logging_time=1.89984, accumulated_submission_time=9748.86, global_step=25378, preemption_count=0, score=9748.86, test/accuracy=0.4545, test/loss=2.5186, test/num_examples=10000, total_duration=10625.1, train/accuracy=0.631716, train/loss=1.59774, validation/accuracy=0.58416, validation/loss=1.82656, validation/num_examples=50000
I0307 03:58:18.648398 140286718113536 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.4458277225494385, loss=2.9697675704956055
I0307 03:58:56.707753 140286709720832 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.8892900943756104, loss=2.877699851989746
I0307 03:59:35.348511 140286718113536 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.7771785259246826, loss=2.8995590209960938
I0307 04:00:13.550685 140286709720832 logging_writer.py:48] [25700] global_step=25700, grad_norm=4.293582439422607, loss=2.8958683013916016
I0307 04:00:51.731373 140286718113536 logging_writer.py:48] [25800] global_step=25800, grad_norm=4.001474380493164, loss=2.7982139587402344
I0307 04:01:30.161613 140286709720832 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.725545883178711, loss=2.7976760864257812
I0307 04:02:08.281966 140286718113536 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.938460111618042, loss=2.8165218830108643
I0307 04:02:46.431433 140286709720832 logging_writer.py:48] [26100] global_step=26100, grad_norm=4.198449611663818, loss=2.973783254623413
I0307 04:03:24.906684 140286718113536 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.607520341873169, loss=2.9315619468688965
I0307 04:04:03.465701 140286709720832 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.330660820007324, loss=2.880716323852539
I0307 04:04:41.846330 140286718113536 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.1610939502716064, loss=2.836622476577759
I0307 04:05:19.793661 140286709720832 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.0709376335144043, loss=2.906677484512329
I0307 04:05:58.294177 140286718113536 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.416118621826172, loss=2.8880887031555176
I0307 04:06:36.572312 140286709720832 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.5956931114196777, loss=2.8863685131073
I0307 04:06:39.608635 140441807221952 spec.py:321] Evaluating on the training split.
I0307 04:06:55.864170 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 04:07:22.897615 140441807221952 spec.py:349] Evaluating on the test split.
I0307 04:07:24.665761 140441807221952 submission_runner.py:469] Time since start: 11180.17s, 	Step: 26709, 	{'train/accuracy': 0.6319754123687744, 'train/loss': 1.5863648653030396, 'validation/accuracy': 0.5883399844169617, 'validation/loss': 1.7912288904190063, 'validation/num_examples': 50000, 'test/accuracy': 0.4711000323295593, 'test/loss': 2.4544482231140137, 'test/num_examples': 10000, 'score': 10258.641316652298, 'total_duration': 11180.16995716095, 'accumulated_submission_time': 10258.641316652298, 'accumulated_eval_time': 916.6985991001129, 'accumulated_logging_time': 2.0172619819641113}
I0307 04:07:24.783419 140286718113536 logging_writer.py:48] [26709] accumulated_eval_time=916.699, accumulated_logging_time=2.01726, accumulated_submission_time=10258.6, global_step=26709, preemption_count=0, score=10258.6, test/accuracy=0.4711, test/loss=2.45445, test/num_examples=10000, total_duration=11180.2, train/accuracy=0.631975, train/loss=1.58636, validation/accuracy=0.58834, validation/loss=1.79123, validation/num_examples=50000
I0307 04:07:59.697180 140286709720832 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.0883495807647705, loss=2.9531843662261963
I0307 04:08:37.702362 140286718113536 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.1159608364105225, loss=2.87945556640625
I0307 04:09:16.013346 140286709720832 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.685849905014038, loss=2.7412452697753906
I0307 04:09:54.480349 140286718113536 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.465268135070801, loss=2.9181413650512695
I0307 04:10:32.517771 140286709720832 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.0051679611206055, loss=2.880913019180298
I0307 04:11:10.874696 140286718113536 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.9120407104492188, loss=2.873406410217285
I0307 04:11:49.059061 140286709720832 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.2470571994781494, loss=2.8707542419433594
I0307 04:12:26.783980 140286718113536 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.493797779083252, loss=2.853729009628296
I0307 04:13:05.475182 140286709720832 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.3861725330352783, loss=2.924408197402954
I0307 04:13:43.860174 140286718113536 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.785299062728882, loss=2.935838460922241
I0307 04:14:22.119853 140286709720832 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.400304079055786, loss=2.822417736053467
I0307 04:15:00.246243 140286718113536 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.364025354385376, loss=2.8737707138061523
I0307 04:15:38.490924 140286709720832 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.848738193511963, loss=2.7863571643829346
I0307 04:15:54.732792 140441807221952 spec.py:321] Evaluating on the training split.
I0307 04:16:12.940803 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 04:16:35.672817 140441807221952 spec.py:349] Evaluating on the test split.
I0307 04:16:37.466847 140441807221952 submission_runner.py:469] Time since start: 11732.97s, 	Step: 28043, 	{'train/accuracy': 0.6358418464660645, 'train/loss': 1.5619195699691772, 'validation/accuracy': 0.5920000076293945, 'validation/loss': 1.7784985303878784, 'validation/num_examples': 50000, 'test/accuracy': 0.4733000099658966, 'test/loss': 2.4413235187530518, 'test/num_examples': 10000, 'score': 10768.428705453873, 'total_duration': 11732.971014261246, 'accumulated_submission_time': 10768.428705453873, 'accumulated_eval_time': 959.4324729442596, 'accumulated_logging_time': 2.158151149749756}
I0307 04:16:37.576459 140286718113536 logging_writer.py:48] [28043] accumulated_eval_time=959.432, accumulated_logging_time=2.15815, accumulated_submission_time=10768.4, global_step=28043, preemption_count=0, score=10768.4, test/accuracy=0.4733, test/loss=2.44132, test/num_examples=10000, total_duration=11733, train/accuracy=0.635842, train/loss=1.56192, validation/accuracy=0.592, validation/loss=1.7785, validation/num_examples=50000
I0307 04:16:59.884312 140286709720832 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.5779330730438232, loss=2.827885627746582
I0307 04:17:38.225523 140286718113536 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.312763214111328, loss=2.844705581665039
I0307 04:18:16.221765 140286709720832 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.553874969482422, loss=2.831437826156616
I0307 04:18:54.741270 140286718113536 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.478450298309326, loss=2.8420732021331787
I0307 04:19:33.420011 140286709720832 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.2959647178649902, loss=2.897280693054199
I0307 04:20:12.065979 140286718113536 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.32833194732666, loss=2.854367733001709
I0307 04:20:50.278818 140286709720832 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.3296616077423096, loss=2.8845605850219727
I0307 04:21:28.794920 140286718113536 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.8372466564178467, loss=2.843667507171631
I0307 04:22:07.262182 140286709720832 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.4258716106414795, loss=2.7831149101257324
I0307 04:22:45.712006 140286718113536 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.157172679901123, loss=2.8228447437286377
I0307 04:23:23.797669 140286709720832 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.1763999462127686, loss=2.9400503635406494
I0307 04:24:02.049495 140286718113536 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.969919204711914, loss=2.986238956451416
I0307 04:24:40.462170 140286709720832 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.537862777709961, loss=2.7854554653167725
I0307 04:25:07.588899 140441807221952 spec.py:321] Evaluating on the training split.
I0307 04:25:25.623214 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 04:25:48.459284 140441807221952 spec.py:349] Evaluating on the test split.
I0307 04:25:50.269977 140441807221952 submission_runner.py:469] Time since start: 12285.77s, 	Step: 29372, 	{'train/accuracy': 0.630879282951355, 'train/loss': 1.5892046689987183, 'validation/accuracy': 0.5872799754142761, 'validation/loss': 1.80474054813385, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.5089316368103027, 'test/num_examples': 10000, 'score': 11278.280355215073, 'total_duration': 12285.77416563034, 'accumulated_submission_time': 11278.280355215073, 'accumulated_eval_time': 1002.1133708953857, 'accumulated_logging_time': 2.294841766357422}
I0307 04:25:50.362652 140286718113536 logging_writer.py:48] [29372] accumulated_eval_time=1002.11, accumulated_logging_time=2.29484, accumulated_submission_time=11278.3, global_step=29372, preemption_count=0, score=11278.3, test/accuracy=0.4604, test/loss=2.50893, test/num_examples=10000, total_duration=12285.8, train/accuracy=0.630879, train/loss=1.5892, validation/accuracy=0.58728, validation/loss=1.80474, validation/num_examples=50000
I0307 04:26:01.486245 140286709720832 logging_writer.py:48] [29400] global_step=29400, grad_norm=4.1918110847473145, loss=2.810873031616211
I0307 04:26:39.521280 140286718113536 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.9106478691101074, loss=2.8639259338378906
I0307 04:27:17.765341 140286709720832 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.3412468433380127, loss=2.8093509674072266
I0307 04:27:56.237329 140286718113536 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.8735475540161133, loss=2.8203558921813965
I0307 04:28:34.446106 140286709720832 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.5085909366607666, loss=2.808638334274292
I0307 04:29:12.493921 140286718113536 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.0035910606384277, loss=2.823378562927246
I0307 04:29:50.884459 140286709720832 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.6811535358428955, loss=2.781550168991089
I0307 04:30:29.800265 140286718113536 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.085707902908325, loss=2.8242268562316895
I0307 04:31:08.040868 140286709720832 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.7344484329223633, loss=2.8718671798706055
I0307 04:31:46.399524 140286718113536 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.408226728439331, loss=2.833362340927124
I0307 04:32:24.829249 140286709720832 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.8502211570739746, loss=2.846097946166992
I0307 04:33:02.986245 140286718113536 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.3272511959075928, loss=2.8126072883605957
I0307 04:33:41.129861 140286709720832 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.0252013206481934, loss=2.8444881439208984
I0307 04:34:19.226030 140286718113536 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.395937204360962, loss=2.829702854156494
I0307 04:34:20.417405 140441807221952 spec.py:321] Evaluating on the training split.
I0307 04:34:36.054879 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 04:34:58.548369 140441807221952 spec.py:349] Evaluating on the test split.
I0307 04:35:00.379144 140441807221952 submission_runner.py:469] Time since start: 12835.88s, 	Step: 30704, 	{'train/accuracy': 0.6367586255073547, 'train/loss': 1.5946401357650757, 'validation/accuracy': 0.5898199677467346, 'validation/loss': 1.814432144165039, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.4608688354492188, 'test/num_examples': 10000, 'score': 11788.173129558563, 'total_duration': 12835.88332104683, 'accumulated_submission_time': 11788.173129558563, 'accumulated_eval_time': 1042.0749201774597, 'accumulated_logging_time': 2.4168710708618164}
I0307 04:35:00.472566 140286709720832 logging_writer.py:48] [30704] accumulated_eval_time=1042.07, accumulated_logging_time=2.41687, accumulated_submission_time=11788.2, global_step=30704, preemption_count=0, score=11788.2, test/accuracy=0.4693, test/loss=2.46087, test/num_examples=10000, total_duration=12835.9, train/accuracy=0.636759, train/loss=1.59464, validation/accuracy=0.58982, validation/loss=1.81443, validation/num_examples=50000
I0307 04:35:37.467263 140286718113536 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.0271637439727783, loss=2.8844211101531982
I0307 04:36:15.909673 140286709720832 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.1989657878875732, loss=2.786618709564209
I0307 04:36:54.146749 140286718113536 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.583003282546997, loss=2.8801307678222656
I0307 04:37:32.029667 140286709720832 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.0348191261291504, loss=2.9444961547851562
I0307 04:38:10.146380 140286718113536 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.29811954498291, loss=2.79925537109375
I0307 04:38:48.555211 140286709720832 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.8921258449554443, loss=2.772915840148926
I0307 04:39:26.802761 140286718113536 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.8954861164093018, loss=2.861269235610962
I0307 04:40:05.049115 140286709720832 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.7672667503356934, loss=2.867262363433838
I0307 04:40:43.676845 140286718113536 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.9862899780273438, loss=2.8685085773468018
I0307 04:41:21.642602 140286709720832 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.0742883682250977, loss=2.8998045921325684
I0307 04:41:59.922441 140286718113536 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.976461887359619, loss=2.8324241638183594
I0307 04:42:38.282670 140286709720832 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.8657422065734863, loss=2.886361837387085
I0307 04:43:16.860835 140286718113536 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.359062910079956, loss=2.9347357749938965
I0307 04:43:30.419880 140441807221952 spec.py:321] Evaluating on the training split.
I0307 04:43:50.168248 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 04:44:12.127665 140441807221952 spec.py:349] Evaluating on the test split.
I0307 04:44:13.946414 140441807221952 submission_runner.py:469] Time since start: 13389.45s, 	Step: 32037, 	{'train/accuracy': 0.6462053656578064, 'train/loss': 1.5323504209518433, 'validation/accuracy': 0.5992400050163269, 'validation/loss': 1.7477208375930786, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.4496960639953613, 'test/num_examples': 10000, 'score': 12297.967391490936, 'total_duration': 13389.450607061386, 'accumulated_submission_time': 12297.967391490936, 'accumulated_eval_time': 1085.6012825965881, 'accumulated_logging_time': 2.5240790843963623}
I0307 04:44:14.027119 140286709720832 logging_writer.py:48] [32037] accumulated_eval_time=1085.6, accumulated_logging_time=2.52408, accumulated_submission_time=12298, global_step=32037, preemption_count=0, score=12298, test/accuracy=0.4722, test/loss=2.4497, test/num_examples=10000, total_duration=13389.5, train/accuracy=0.646205, train/loss=1.53235, validation/accuracy=0.59924, validation/loss=1.74772, validation/num_examples=50000
I0307 04:44:38.217032 140286718113536 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.899407148361206, loss=2.8291828632354736
I0307 04:45:16.287126 140286709720832 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.6038455963134766, loss=2.820232391357422
I0307 04:45:54.758938 140286718113536 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.0436220169067383, loss=2.8030285835266113
I0307 04:46:33.010380 140286709720832 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.3572258949279785, loss=2.888653039932251
I0307 04:47:11.165877 140286718113536 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.3975837230682373, loss=2.902287721633911
I0307 04:47:49.882248 140286709720832 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.0380377769470215, loss=2.7312753200531006
I0307 04:48:28.093094 140286718113536 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.7664403915405273, loss=2.8690011501312256
I0307 04:49:06.555071 140286709720832 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.306403636932373, loss=2.8894221782684326
I0307 04:49:44.700082 140286718113536 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.686640501022339, loss=2.901094675064087
I0307 04:50:22.673810 140286709720832 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.9651718139648438, loss=2.8516845703125
I0307 04:51:00.618482 140286718113536 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.8306186199188232, loss=2.819894313812256
I0307 04:51:39.393702 140286709720832 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.016345739364624, loss=2.8676137924194336
I0307 04:52:17.444054 140286718113536 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.0866575241088867, loss=2.8641889095306396
I0307 04:52:44.010970 140441807221952 spec.py:321] Evaluating on the training split.
I0307 04:52:55.943594 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 04:53:25.921180 140441807221952 spec.py:349] Evaluating on the test split.
I0307 04:53:27.690148 140441807221952 submission_runner.py:469] Time since start: 13943.19s, 	Step: 33370, 	{'train/accuracy': 0.6450095772743225, 'train/loss': 1.5158864259719849, 'validation/accuracy': 0.601099967956543, 'validation/loss': 1.7310526371002197, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.4251773357391357, 'test/num_examples': 10000, 'score': 12807.780698776245, 'total_duration': 13943.194332838058, 'accumulated_submission_time': 12807.780698776245, 'accumulated_eval_time': 1129.2802798748016, 'accumulated_logging_time': 2.6369564533233643}
I0307 04:53:27.832214 140286709720832 logging_writer.py:48] [33370] accumulated_eval_time=1129.28, accumulated_logging_time=2.63696, accumulated_submission_time=12807.8, global_step=33370, preemption_count=0, score=12807.8, test/accuracy=0.4708, test/loss=2.42518, test/num_examples=10000, total_duration=13943.2, train/accuracy=0.64501, train/loss=1.51589, validation/accuracy=0.6011, validation/loss=1.73105, validation/num_examples=50000
I0307 04:53:39.699311 140286718113536 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.895120859146118, loss=2.8276915550231934
I0307 04:54:17.941544 140286709720832 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.885344982147217, loss=2.878936529159546
I0307 04:54:56.106441 140286718113536 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.092428207397461, loss=2.8159217834472656
I0307 04:55:34.350863 140286709720832 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.58182430267334, loss=2.765575647354126
I0307 04:56:12.998117 140286718113536 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.631957530975342, loss=2.759917974472046
I0307 04:56:51.108843 140286709720832 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.8783397674560547, loss=2.892777442932129
I0307 04:57:29.447451 140286718113536 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.801342725753784, loss=2.7606241703033447
I0307 04:58:07.813673 140286709720832 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.9118597507476807, loss=2.8638463020324707
I0307 04:58:46.197063 140286718113536 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.3264100551605225, loss=2.802187204360962
I0307 04:59:24.041879 140286709720832 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.048757314682007, loss=2.838261842727661
I0307 05:00:02.303364 140286718113536 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.916185140609741, loss=2.756211996078491
I0307 05:00:40.665295 140286709720832 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.260862350463867, loss=2.74232816696167
I0307 05:01:18.914711 140286718113536 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.7998576164245605, loss=2.841890335083008
I0307 05:01:56.875004 140286709720832 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.2148098945617676, loss=2.8494739532470703
I0307 05:01:58.068362 140441807221952 spec.py:321] Evaluating on the training split.
I0307 05:02:11.257383 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 05:02:39.870031 140441807221952 spec.py:349] Evaluating on the test split.
I0307 05:02:41.685769 140441807221952 submission_runner.py:469] Time since start: 14497.19s, 	Step: 34704, 	{'train/accuracy': 0.6413025856018066, 'train/loss': 1.5720946788787842, 'validation/accuracy': 0.5981799960136414, 'validation/loss': 1.7655549049377441, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.4356882572174072, 'test/num_examples': 10000, 'score': 13317.845072031021, 'total_duration': 14497.189989566803, 'accumulated_submission_time': 13317.845072031021, 'accumulated_eval_time': 1172.8975405693054, 'accumulated_logging_time': 2.8115742206573486}
I0307 05:02:41.776006 140286718113536 logging_writer.py:48] [34704] accumulated_eval_time=1172.9, accumulated_logging_time=2.81157, accumulated_submission_time=13317.8, global_step=34704, preemption_count=0, score=13317.8, test/accuracy=0.4792, test/loss=2.43569, test/num_examples=10000, total_duration=14497.2, train/accuracy=0.641303, train/loss=1.57209, validation/accuracy=0.59818, validation/loss=1.76555, validation/num_examples=50000
I0307 05:03:19.104809 140286709720832 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.42440128326416, loss=2.864847183227539
I0307 05:03:57.298957 140286718113536 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.9119131565093994, loss=2.8040192127227783
I0307 05:04:35.270514 140286709720832 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.45381498336792, loss=2.960477352142334
I0307 05:05:14.161795 140286718113536 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.7658793926239014, loss=2.8119235038757324
I0307 05:05:52.506038 140286709720832 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.446986675262451, loss=2.813793420791626
I0307 05:06:30.751864 140286718113536 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.1914572715759277, loss=2.8166768550872803
I0307 05:07:09.119992 140286709720832 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.6642792224884033, loss=2.8012094497680664
I0307 05:07:47.405810 140286718113536 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.7074458599090576, loss=2.853693962097168
I0307 05:08:25.804995 140286709720832 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.0105035305023193, loss=2.824247360229492
I0307 05:09:04.343218 140286718113536 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.072359800338745, loss=2.841261386871338
I0307 05:09:42.457025 140286709720832 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.8032174110412598, loss=2.740522623062134
I0307 05:10:20.732327 140286718113536 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.9548020362854004, loss=2.7618751525878906
I0307 05:10:58.974648 140286709720832 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.7911367416381836, loss=2.795727491378784
I0307 05:11:11.721050 140441807221952 spec.py:321] Evaluating on the training split.
I0307 05:11:23.950710 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 05:11:49.048621 140441807221952 spec.py:349] Evaluating on the test split.
I0307 05:11:50.843489 140441807221952 submission_runner.py:469] Time since start: 15046.35s, 	Step: 36034, 	{'train/accuracy': 0.650809109210968, 'train/loss': 1.5241730213165283, 'validation/accuracy': 0.6029999852180481, 'validation/loss': 1.7280184030532837, 'validation/num_examples': 50000, 'test/accuracy': 0.47700002789497375, 'test/loss': 2.4105193614959717, 'test/num_examples': 10000, 'score': 13827.63047003746, 'total_duration': 15046.347669839859, 'accumulated_submission_time': 13827.63047003746, 'accumulated_eval_time': 1212.0197942256927, 'accumulated_logging_time': 2.9248199462890625}
I0307 05:11:50.933222 140286718113536 logging_writer.py:48] [36034] accumulated_eval_time=1212.02, accumulated_logging_time=2.92482, accumulated_submission_time=13827.6, global_step=36034, preemption_count=0, score=13827.6, test/accuracy=0.477, test/loss=2.41052, test/num_examples=10000, total_duration=15046.3, train/accuracy=0.650809, train/loss=1.52417, validation/accuracy=0.603, validation/loss=1.72802, validation/num_examples=50000
I0307 05:12:16.710575 140286709720832 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.0191714763641357, loss=2.8512916564941406
I0307 05:12:54.718666 140286718113536 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.1659839153289795, loss=2.828563690185547
I0307 05:13:33.469400 140286709720832 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.8072938919067383, loss=2.7970612049102783
I0307 05:14:11.810593 140286718113536 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.8600237369537354, loss=2.7963380813598633
I0307 05:14:50.032866 140286709720832 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.9008772373199463, loss=2.8772382736206055
I0307 05:15:28.423424 140286718113536 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.207977533340454, loss=2.8668131828308105
I0307 05:16:06.748541 140286709720832 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.5893359184265137, loss=2.8516316413879395
I0307 05:16:44.903399 140286718113536 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.8501064777374268, loss=2.758542060852051
I0307 05:17:23.408690 140286709720832 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.5351858139038086, loss=2.7267231941223145
I0307 05:18:02.196761 140286718113536 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.5910251140594482, loss=2.7558751106262207
I0307 05:18:40.131981 140286709720832 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.5544703006744385, loss=2.825566053390503
I0307 05:19:18.256851 140286718113536 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.055752754211426, loss=2.804760456085205
I0307 05:19:56.362762 140286709720832 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.0067555904388428, loss=2.732090473175049
I0307 05:20:20.866067 140441807221952 spec.py:321] Evaluating on the training split.
I0307 05:20:33.271490 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 05:20:57.027338 140441807221952 spec.py:349] Evaluating on the test split.
I0307 05:20:58.878983 140441807221952 submission_runner.py:469] Time since start: 15594.38s, 	Step: 37365, 	{'train/accuracy': 0.6429169178009033, 'train/loss': 1.5372689962387085, 'validation/accuracy': 0.6027399897575378, 'validation/loss': 1.7396049499511719, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.3982303142547607, 'test/num_examples': 10000, 'score': 14337.376689195633, 'total_duration': 15594.383026599884, 'accumulated_submission_time': 14337.376689195633, 'accumulated_eval_time': 1250.0323853492737, 'accumulated_logging_time': 3.0655133724212646}
I0307 05:20:59.027642 140286718113536 logging_writer.py:48] [37365] accumulated_eval_time=1250.03, accumulated_logging_time=3.06551, accumulated_submission_time=14337.4, global_step=37365, preemption_count=0, score=14337.4, test/accuracy=0.4843, test/loss=2.39823, test/num_examples=10000, total_duration=15594.4, train/accuracy=0.642917, train/loss=1.53727, validation/accuracy=0.60274, validation/loss=1.7396, validation/num_examples=50000
I0307 05:21:13.151524 140286709720832 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.3755929470062256, loss=2.850172996520996
I0307 05:21:51.212070 140286718113536 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.8554046154022217, loss=2.7699224948883057
I0307 05:22:29.606024 140286709720832 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.7883918285369873, loss=2.810319185256958
I0307 05:23:07.896731 140286718113536 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.2244040966033936, loss=2.8901314735412598
I0307 05:23:46.187640 140286709720832 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.909515142440796, loss=2.794619083404541
I0307 05:24:24.712990 140286718113536 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.865114450454712, loss=2.7523844242095947
I0307 05:25:03.010151 140286709720832 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.9601006507873535, loss=2.7205638885498047
I0307 05:25:41.371573 140286718113536 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.509120464324951, loss=2.7535362243652344
I0307 05:26:19.424980 140286709720832 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.078282356262207, loss=2.675290107727051
I0307 05:26:57.708632 140286718113536 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.0163586139678955, loss=2.789051055908203
I0307 05:27:35.829567 140286709720832 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.346569061279297, loss=2.8170225620269775
I0307 05:28:14.054876 140286718113536 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.1267693042755127, loss=2.75455904006958
I0307 05:28:52.128681 140286709720832 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.962003707885742, loss=2.904305934906006
I0307 05:29:29.081897 140441807221952 spec.py:321] Evaluating on the training split.
I0307 05:29:41.588476 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 05:30:07.494872 140441807221952 spec.py:349] Evaluating on the test split.
I0307 05:30:09.313322 140441807221952 submission_runner.py:469] Time since start: 16144.82s, 	Step: 38697, 	{'train/accuracy': 0.6528618931770325, 'train/loss': 1.5006095170974731, 'validation/accuracy': 0.6132799983024597, 'validation/loss': 1.6886672973632812, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.3485679626464844, 'test/num_examples': 10000, 'score': 14847.254745483398, 'total_duration': 16144.817528009415, 'accumulated_submission_time': 14847.254745483398, 'accumulated_eval_time': 1290.2636535167694, 'accumulated_logging_time': 3.2474365234375}
I0307 05:30:09.388386 140286718113536 logging_writer.py:48] [38697] accumulated_eval_time=1290.26, accumulated_logging_time=3.24744, accumulated_submission_time=14847.3, global_step=38697, preemption_count=0, score=14847.3, test/accuracy=0.4879, test/loss=2.34857, test/num_examples=10000, total_duration=16144.8, train/accuracy=0.652862, train/loss=1.50061, validation/accuracy=0.61328, validation/loss=1.68867, validation/num_examples=50000
I0307 05:30:11.010335 140286709720832 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.0798819065093994, loss=2.8325259685516357
I0307 05:30:49.809675 140286718113536 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.582735300064087, loss=2.8547942638397217
I0307 05:31:28.534792 140286709720832 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.7366371154785156, loss=2.775379180908203
I0307 05:32:06.947365 140286718113536 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.3120148181915283, loss=2.719794273376465
I0307 05:32:45.633262 140286709720832 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.950582981109619, loss=2.8117263317108154
I0307 05:33:24.139237 140286718113536 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.2733402252197266, loss=2.812796115875244
I0307 05:34:02.437359 140286709720832 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.101814031600952, loss=2.7159032821655273
I0307 05:34:40.642281 140286718113536 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.665827512741089, loss=2.79290509223938
I0307 05:35:18.778784 140286709720832 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.3846797943115234, loss=2.8658323287963867
I0307 05:35:56.908936 140286718113536 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.9059555530548096, loss=2.567711591720581
I0307 05:36:35.090010 140286709720832 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.7205326557159424, loss=2.760561943054199
I0307 05:37:13.802706 140286718113536 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.3775906562805176, loss=2.7939412593841553
I0307 05:37:51.994373 140286709720832 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.9377856254577637, loss=2.736564874649048
I0307 05:38:30.462706 140286718113536 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.4286584854125977, loss=2.779372215270996
I0307 05:38:39.588140 140441807221952 spec.py:321] Evaluating on the training split.
I0307 05:38:52.150504 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 05:39:11.585061 140441807221952 spec.py:349] Evaluating on the test split.
I0307 05:39:13.445162 140441807221952 submission_runner.py:469] Time since start: 16688.95s, 	Step: 40025, 	{'train/accuracy': 0.6515266299247742, 'train/loss': 1.521810531616211, 'validation/accuracy': 0.6094399690628052, 'validation/loss': 1.7248657941818237, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.3783953189849854, 'test/num_examples': 10000, 'score': 15357.241574287415, 'total_duration': 16688.94926762581, 'accumulated_submission_time': 15357.241574287415, 'accumulated_eval_time': 1324.1204161643982, 'accumulated_logging_time': 3.393284320831299}
I0307 05:39:13.575713 140286709720832 logging_writer.py:48] [40025] accumulated_eval_time=1324.12, accumulated_logging_time=3.39328, accumulated_submission_time=15357.2, global_step=40025, preemption_count=0, score=15357.2, test/accuracy=0.4905, test/loss=2.3784, test/num_examples=10000, total_duration=16688.9, train/accuracy=0.651527, train/loss=1.52181, validation/accuracy=0.60944, validation/loss=1.72487, validation/num_examples=50000
I0307 05:39:43.041694 140286718113536 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.6168298721313477, loss=2.8168935775756836
I0307 05:40:21.506762 140286709720832 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.7522356510162354, loss=2.841475009918213
I0307 05:40:59.944660 140286718113536 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.7237191200256348, loss=2.7403769493103027
I0307 05:41:38.196409 140286709720832 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.977463960647583, loss=2.809762954711914
I0307 05:42:16.430234 140286718113536 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.9550747871398926, loss=2.842773675918579
I0307 05:42:54.490452 140286709720832 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.0595476627349854, loss=2.7976763248443604
I0307 05:43:32.917710 140286718113536 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.3951668739318848, loss=2.7235841751098633
I0307 05:44:11.329024 140286709720832 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.6171844005584717, loss=2.774193525314331
I0307 05:44:49.660576 140286718113536 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.6316475868225098, loss=2.7116777896881104
I0307 05:45:27.962818 140286709720832 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.6239511966705322, loss=2.82735276222229
I0307 05:46:05.997909 140286718113536 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.7193918228149414, loss=2.8048505783081055
I0307 05:46:44.135886 140286709720832 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.93717360496521, loss=2.771268129348755
I0307 05:47:22.372977 140286718113536 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.4048619270324707, loss=2.7668542861938477
I0307 05:47:43.715517 140441807221952 spec.py:321] Evaluating on the training split.
I0307 05:47:56.695883 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 05:48:26.724964 140441807221952 spec.py:349] Evaluating on the test split.
I0307 05:48:28.527252 140441807221952 submission_runner.py:469] Time since start: 17244.03s, 	Step: 41357, 	{'train/accuracy': 0.6451091766357422, 'train/loss': 1.5666347742080688, 'validation/accuracy': 0.6051200032234192, 'validation/loss': 1.7515000104904175, 'validation/num_examples': 50000, 'test/accuracy': 0.4847000241279602, 'test/loss': 2.42962646484375, 'test/num_examples': 10000, 'score': 15867.194030761719, 'total_duration': 17244.031442165375, 'accumulated_submission_time': 15867.194030761719, 'accumulated_eval_time': 1368.9319858551025, 'accumulated_logging_time': 3.5648722648620605}
I0307 05:48:28.636072 140286709720832 logging_writer.py:48] [41357] accumulated_eval_time=1368.93, accumulated_logging_time=3.56487, accumulated_submission_time=15867.2, global_step=41357, preemption_count=0, score=15867.2, test/accuracy=0.4847, test/loss=2.42963, test/num_examples=10000, total_duration=17244, train/accuracy=0.645109, train/loss=1.56663, validation/accuracy=0.60512, validation/loss=1.7515, validation/num_examples=50000
I0307 05:48:45.559474 140286718113536 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.252000331878662, loss=2.742351531982422
I0307 05:49:23.827257 140286709720832 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.820222854614258, loss=2.7401061058044434
I0307 05:50:01.981369 140286718113536 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.2461392879486084, loss=2.836944580078125
I0307 05:50:40.153114 140286709720832 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.977184295654297, loss=2.8265504837036133
I0307 05:51:18.462177 140286718113536 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.1705188751220703, loss=2.8074724674224854
I0307 05:51:57.029437 140286709720832 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.0042357444763184, loss=2.7452094554901123
I0307 05:52:35.178112 140286718113536 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.943145513534546, loss=2.709730863571167
I0307 05:53:13.421793 140286709720832 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.339832305908203, loss=2.718878984451294
I0307 05:53:51.718206 140286718113536 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.9307608604431152, loss=2.725780725479126
I0307 05:54:30.306246 140286709720832 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.1315438747406006, loss=2.7234725952148438
I0307 05:55:08.832914 140286718113536 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.555697202682495, loss=2.689077615737915
I0307 05:55:47.595276 140286709720832 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.0400233268737793, loss=2.81648588180542
I0307 05:56:27.241213 140286718113536 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.2922050952911377, loss=2.728248357772827
I0307 05:56:58.688275 140441807221952 spec.py:321] Evaluating on the training split.
I0307 05:57:11.048928 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 05:57:36.892911 140441807221952 spec.py:349] Evaluating on the test split.
I0307 05:57:38.701472 140441807221952 submission_runner.py:469] Time since start: 17794.21s, 	Step: 42683, 	{'train/accuracy': 0.6377550959587097, 'train/loss': 1.5750277042388916, 'validation/accuracy': 0.5985400080680847, 'validation/loss': 1.7739797830581665, 'validation/num_examples': 50000, 'test/accuracy': 0.47530001401901245, 'test/loss': 2.4557299613952637, 'test/num_examples': 10000, 'score': 16377.078849554062, 'total_duration': 17794.2056555748, 'accumulated_submission_time': 16377.078849554062, 'accumulated_eval_time': 1408.9450025558472, 'accumulated_logging_time': 3.6987690925598145}
I0307 05:57:38.793395 140286709720832 logging_writer.py:48] [42683] accumulated_eval_time=1408.95, accumulated_logging_time=3.69877, accumulated_submission_time=16377.1, global_step=42683, preemption_count=0, score=16377.1, test/accuracy=0.4753, test/loss=2.45573, test/num_examples=10000, total_duration=17794.2, train/accuracy=0.637755, train/loss=1.57503, validation/accuracy=0.59854, validation/loss=1.77398, validation/num_examples=50000
I0307 05:57:45.672731 140286718113536 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.1945130825042725, loss=2.7167320251464844
I0307 05:58:23.447316 140286709720832 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.097226619720459, loss=2.838137626647949
I0307 05:59:01.567568 140286718113536 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.7665088176727295, loss=2.750365734100342
I0307 05:59:40.157276 140286709720832 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.773115634918213, loss=2.7411305904388428
I0307 06:00:18.430176 140286718113536 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.3374319076538086, loss=2.775743007659912
I0307 06:00:56.575812 140286709720832 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.1753079891204834, loss=2.8295786380767822
I0307 06:01:34.713220 140286718113536 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.930504560470581, loss=2.8367185592651367
I0307 06:02:13.134143 140286709720832 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.830101251602173, loss=2.7296223640441895
I0307 06:02:51.095500 140286718113536 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.8025147914886475, loss=2.736355781555176
I0307 06:03:29.586793 140286709720832 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.015254497528076, loss=2.7966771125793457
I0307 06:04:07.838995 140286718113536 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.028048038482666, loss=2.727996826171875
I0307 06:04:46.406869 140286709720832 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.3581125736236572, loss=2.768887519836426
I0307 06:05:25.057918 140286718113536 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.9310317039489746, loss=2.7128801345825195
I0307 06:06:03.249057 140286709720832 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.8881657123565674, loss=2.867835760116577
I0307 06:06:08.975740 140441807221952 spec.py:321] Evaluating on the training split.
I0307 06:06:21.717710 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 06:06:42.999388 140441807221952 spec.py:349] Evaluating on the test split.
I0307 06:06:44.817908 140441807221952 submission_runner.py:469] Time since start: 18340.32s, 	Step: 44016, 	{'train/accuracy': 0.6437141299247742, 'train/loss': 1.5236297845840454, 'validation/accuracy': 0.6046199798583984, 'validation/loss': 1.7220348119735718, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.3902587890625, 'test/num_examples': 10000, 'score': 16887.092682361603, 'total_duration': 18340.322093486786, 'accumulated_submission_time': 16887.092682361603, 'accumulated_eval_time': 1444.7869918346405, 'accumulated_logging_time': 3.818232297897339}
I0307 06:06:44.934835 140286718113536 logging_writer.py:48] [44016] accumulated_eval_time=1444.79, accumulated_logging_time=3.81823, accumulated_submission_time=16887.1, global_step=44016, preemption_count=0, score=16887.1, test/accuracy=0.4822, test/loss=2.39026, test/num_examples=10000, total_duration=18340.3, train/accuracy=0.643714, train/loss=1.52363, validation/accuracy=0.60462, validation/loss=1.72203, validation/num_examples=50000
I0307 06:07:17.476479 140286709720832 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.4878129959106445, loss=2.7634024620056152
I0307 06:07:55.319735 140286718113536 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.9956674575805664, loss=2.777672052383423
I0307 06:08:33.870346 140286709720832 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.9136481285095215, loss=2.6894826889038086
I0307 06:09:11.427211 140286718113536 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.059767246246338, loss=2.753385305404663
I0307 06:09:49.278036 140286709720832 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.4598007202148438, loss=2.7764039039611816
I0307 06:10:27.681989 140286718113536 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.7589356899261475, loss=2.698547840118408
I0307 06:11:05.970589 140286709720832 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.8858494758605957, loss=2.713031768798828
I0307 06:11:44.385118 140286718113536 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.942972183227539, loss=2.8865861892700195
I0307 06:12:23.109437 140286709720832 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.7671258449554443, loss=2.7437307834625244
I0307 06:13:00.866230 140286718113536 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.0848257541656494, loss=2.7685017585754395
I0307 06:13:39.373826 140286709720832 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.097757577896118, loss=2.643528461456299
I0307 06:14:17.844121 140286718113536 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.7956786155700684, loss=2.738895893096924
I0307 06:14:55.972808 140286709720832 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.7862606048583984, loss=2.7300100326538086
I0307 06:15:14.896630 140441807221952 spec.py:321] Evaluating on the training split.
I0307 06:15:27.377047 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 06:15:52.231915 140441807221952 spec.py:349] Evaluating on the test split.
I0307 06:15:54.023248 140441807221952 submission_runner.py:469] Time since start: 18889.53s, 	Step: 45351, 	{'train/accuracy': 0.6461654901504517, 'train/loss': 1.5231139659881592, 'validation/accuracy': 0.606719970703125, 'validation/loss': 1.7186791896820068, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.3695437908172607, 'test/num_examples': 10000, 'score': 17396.886875629425, 'total_duration': 18889.527441740036, 'accumulated_submission_time': 17396.886875629425, 'accumulated_eval_time': 1483.9134411811829, 'accumulated_logging_time': 3.961378574371338}
I0307 06:15:54.133770 140286718113536 logging_writer.py:48] [45351] accumulated_eval_time=1483.91, accumulated_logging_time=3.96138, accumulated_submission_time=17396.9, global_step=45351, preemption_count=0, score=17396.9, test/accuracy=0.4852, test/loss=2.36954, test/num_examples=10000, total_duration=18889.5, train/accuracy=0.646165, train/loss=1.52311, validation/accuracy=0.60672, validation/loss=1.71868, validation/num_examples=50000
I0307 06:16:13.138110 140286709720832 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.082498073577881, loss=2.787158727645874
I0307 06:16:51.120216 140286718113536 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.102832555770874, loss=2.776616096496582
I0307 06:17:29.822463 140286709720832 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.135039806365967, loss=2.81602144241333
I0307 06:18:07.978083 140286718113536 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.734452724456787, loss=2.7091331481933594
I0307 06:18:46.495376 140286709720832 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.0602478981018066, loss=2.833313226699829
I0307 06:19:24.982148 140286718113536 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.666158676147461, loss=2.649238109588623
I0307 06:20:03.345430 140286709720832 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.3496994972229004, loss=2.704779863357544
I0307 06:20:41.973801 140286718113536 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.9195351600646973, loss=2.8336427211761475
I0307 06:21:19.937892 140286709720832 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.1404342651367188, loss=2.730419874191284
I0307 06:21:58.949370 140286718113536 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.853365421295166, loss=2.755021572113037
I0307 06:22:37.452782 140286709720832 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.985562562942505, loss=2.7891488075256348
I0307 06:23:15.561913 140286718113536 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.213987112045288, loss=2.810349941253662
I0307 06:23:53.235548 140286709720832 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.9062659740448, loss=2.784003496170044
I0307 06:24:24.114405 140441807221952 spec.py:321] Evaluating on the training split.
I0307 06:24:36.676830 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 06:25:02.437288 140441807221952 spec.py:349] Evaluating on the test split.
I0307 06:25:04.248268 140441807221952 submission_runner.py:469] Time since start: 19439.75s, 	Step: 46683, 	{'train/accuracy': 0.654715359210968, 'train/loss': 1.4919129610061646, 'validation/accuracy': 0.6101599931716919, 'validation/loss': 1.697442650794983, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.3467772006988525, 'test/num_examples': 10000, 'score': 17906.682953596115, 'total_duration': 19439.752460956573, 'accumulated_submission_time': 17906.682953596115, 'accumulated_eval_time': 1524.047131538391, 'accumulated_logging_time': 4.112897872924805}
I0307 06:25:04.349178 140286718113536 logging_writer.py:48] [46683] accumulated_eval_time=1524.05, accumulated_logging_time=4.1129, accumulated_submission_time=17906.7, global_step=46683, preemption_count=0, score=17906.7, test/accuracy=0.4935, test/loss=2.34678, test/num_examples=10000, total_duration=19439.8, train/accuracy=0.654715, train/loss=1.49191, validation/accuracy=0.61016, validation/loss=1.69744, validation/num_examples=50000
I0307 06:25:11.250383 140286709720832 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.731339454650879, loss=2.6567296981811523
I0307 06:25:49.518980 140286718113536 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.261817693710327, loss=2.807955741882324
I0307 06:26:27.766716 140286709720832 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.0108747482299805, loss=2.744035005569458
I0307 06:27:06.233326 140286718113536 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.0540049076080322, loss=2.726315975189209
I0307 06:27:44.595408 140286709720832 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.2062060832977295, loss=2.7817420959472656
I0307 06:28:22.997868 140286718113536 logging_writer.py:48] [47200] global_step=47200, grad_norm=2.9356188774108887, loss=2.8258719444274902
I0307 06:29:01.581294 140286709720832 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.950061082839966, loss=2.744553565979004
I0307 06:29:39.989997 140286718113536 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.908351421356201, loss=2.798935651779175
I0307 06:30:18.131067 140286709720832 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.8485608100891113, loss=2.7135324478149414
I0307 06:30:56.725523 140286718113536 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.971919059753418, loss=2.7634787559509277
I0307 06:31:35.098123 140286709720832 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.3451523780822754, loss=2.769788980484009
I0307 06:32:13.324841 140286718113536 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.725809335708618, loss=2.7964835166931152
I0307 06:32:51.626269 140286709720832 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.9976043701171875, loss=2.761169195175171
I0307 06:33:30.318304 140286718113536 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.250757932662964, loss=2.7682394981384277
I0307 06:33:34.593814 140441807221952 spec.py:321] Evaluating on the training split.
I0307 06:33:46.829595 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 06:34:10.039944 140441807221952 spec.py:349] Evaluating on the test split.
I0307 06:34:11.814856 140441807221952 submission_runner.py:469] Time since start: 19987.32s, 	Step: 48012, 	{'train/accuracy': 0.6553930044174194, 'train/loss': 1.4746084213256836, 'validation/accuracy': 0.6146799921989441, 'validation/loss': 1.6603604555130005, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.3318798542022705, 'test/num_examples': 10000, 'score': 18416.766710042953, 'total_duration': 19987.31906557083, 'accumulated_submission_time': 18416.766710042953, 'accumulated_eval_time': 1561.2680158615112, 'accumulated_logging_time': 4.236192941665649}
I0307 06:34:11.896003 140286709720832 logging_writer.py:48] [48012] accumulated_eval_time=1561.27, accumulated_logging_time=4.23619, accumulated_submission_time=18416.8, global_step=48012, preemption_count=0, score=18416.8, test/accuracy=0.4946, test/loss=2.33188, test/num_examples=10000, total_duration=19987.3, train/accuracy=0.655393, train/loss=1.47461, validation/accuracy=0.61468, validation/loss=1.66036, validation/num_examples=50000
I0307 06:34:46.183212 140286718113536 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.0996556282043457, loss=2.7195534706115723
I0307 06:35:24.257883 140286709720832 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.1813971996307373, loss=2.72240948677063
I0307 06:36:02.470761 140286718113536 logging_writer.py:48] [48300] global_step=48300, grad_norm=2.9397146701812744, loss=2.8293874263763428
I0307 06:36:40.944990 140286709720832 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.078840732574463, loss=2.750312566757202
I0307 06:37:19.174969 140286718113536 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.875681161880493, loss=2.7605884075164795
I0307 06:37:57.890224 140286709720832 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.1917686462402344, loss=2.783769130706787
I0307 06:38:36.539128 140286718113536 logging_writer.py:48] [48700] global_step=48700, grad_norm=2.7300779819488525, loss=2.711968421936035
I0307 06:39:15.171105 140286709720832 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.226959466934204, loss=2.823439598083496
I0307 06:39:53.708610 140286718113536 logging_writer.py:48] [48900] global_step=48900, grad_norm=2.8166565895080566, loss=2.772915840148926
I0307 06:40:32.092994 140286709720832 logging_writer.py:48] [49000] global_step=49000, grad_norm=2.900059700012207, loss=2.6563808917999268
I0307 06:41:10.437671 140286718113536 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.0680551528930664, loss=2.7467761039733887
I0307 06:41:48.544722 140286709720832 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.2104902267456055, loss=2.7056641578674316
I0307 06:42:26.861885 140286718113536 logging_writer.py:48] [49300] global_step=49300, grad_norm=2.9641385078430176, loss=2.76582670211792
I0307 06:42:42.150306 140441807221952 spec.py:321] Evaluating on the training split.
I0307 06:42:54.534249 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 06:43:20.615499 140441807221952 spec.py:349] Evaluating on the test split.
I0307 06:43:22.363693 140441807221952 submission_runner.py:469] Time since start: 20537.87s, 	Step: 49341, 	{'train/accuracy': 0.6593989133834839, 'train/loss': 1.4982556104660034, 'validation/accuracy': 0.614799976348877, 'validation/loss': 1.7008064985275269, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3613393306732178, 'test/num_examples': 10000, 'score': 18926.852604150772, 'total_duration': 20537.86784219742, 'accumulated_submission_time': 18926.852604150772, 'accumulated_eval_time': 1601.4811866283417, 'accumulated_logging_time': 4.3443663120269775}
I0307 06:43:22.454700 140286709720832 logging_writer.py:48] [49341] accumulated_eval_time=1601.48, accumulated_logging_time=4.34437, accumulated_submission_time=18926.9, global_step=49341, preemption_count=0, score=18926.9, test/accuracy=0.4901, test/loss=2.36134, test/num_examples=10000, total_duration=20537.9, train/accuracy=0.659399, train/loss=1.49826, validation/accuracy=0.6148, validation/loss=1.70081, validation/num_examples=50000
I0307 06:43:45.732819 140286718113536 logging_writer.py:48] [49400] global_step=49400, grad_norm=2.913065195083618, loss=2.735196828842163
I0307 06:44:24.420638 140286709720832 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.27315354347229, loss=2.775609254837036
I0307 06:45:03.103563 140286718113536 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.17134165763855, loss=2.632725238800049
I0307 06:45:41.402831 140286709720832 logging_writer.py:48] [49700] global_step=49700, grad_norm=2.8810694217681885, loss=2.7535367012023926
I0307 06:46:19.390644 140286718113536 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.8916308879852295, loss=2.7658424377441406
I0307 06:46:57.802917 140286709720832 logging_writer.py:48] [49900] global_step=49900, grad_norm=2.9136288166046143, loss=2.6807689666748047
I0307 06:47:37.165505 140286718113536 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.058072090148926, loss=2.690736770629883
I0307 06:48:16.271930 140286709720832 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.3038077354431152, loss=2.7313337326049805
I0307 06:48:54.837912 140286718113536 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.1200098991394043, loss=2.7380053997039795
I0307 06:49:33.340369 140286709720832 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.616081953048706, loss=2.814035177230835
I0307 06:50:11.570392 140286718113536 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.26655650138855, loss=2.7416319847106934
I0307 06:50:50.014446 140286709720832 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.1026477813720703, loss=2.7752397060394287
I0307 06:51:28.258943 140286718113536 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.366360664367676, loss=2.8528971672058105
I0307 06:51:52.568146 140441807221952 spec.py:321] Evaluating on the training split.
I0307 06:52:04.596635 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 06:52:31.288741 140441807221952 spec.py:349] Evaluating on the test split.
I0307 06:52:33.059952 140441807221952 submission_runner.py:469] Time since start: 21088.56s, 	Step: 50664, 	{'train/accuracy': 0.6714165806770325, 'train/loss': 1.4457722902297974, 'validation/accuracy': 0.6180199980735779, 'validation/loss': 1.6856416463851929, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.351414680480957, 'test/num_examples': 10000, 'score': 19436.79271888733, 'total_duration': 21088.564143180847, 'accumulated_submission_time': 19436.79271888733, 'accumulated_eval_time': 1641.9728224277496, 'accumulated_logging_time': 4.463685750961304}
I0307 06:52:33.157116 140286709720832 logging_writer.py:48] [50664] accumulated_eval_time=1641.97, accumulated_logging_time=4.46369, accumulated_submission_time=19436.8, global_step=50664, preemption_count=0, score=19436.8, test/accuracy=0.4927, test/loss=2.35141, test/num_examples=10000, total_duration=21088.6, train/accuracy=0.671417, train/loss=1.44577, validation/accuracy=0.61802, validation/loss=1.68564, validation/num_examples=50000
I0307 06:52:47.597660 140286718113536 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.218717336654663, loss=2.7866477966308594
I0307 06:53:25.924180 140286709720832 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.848125696182251, loss=2.709747076034546
I0307 06:54:03.894883 140286718113536 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.1802263259887695, loss=2.749293327331543
I0307 06:54:42.310095 140286709720832 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.044344663619995, loss=2.722625494003296
I0307 06:55:20.424568 140286718113536 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.9343509674072266, loss=2.7879505157470703
I0307 06:55:58.816849 140286709720832 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.061237096786499, loss=2.7427544593811035
I0307 06:56:37.663714 140286718113536 logging_writer.py:48] [51300] global_step=51300, grad_norm=2.9020583629608154, loss=2.7160308361053467
I0307 06:57:16.144147 140286709720832 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.1014034748077393, loss=2.7880520820617676
I0307 06:57:54.482970 140286718113536 logging_writer.py:48] [51500] global_step=51500, grad_norm=2.9252731800079346, loss=2.6602044105529785
I0307 06:58:32.703409 140286709720832 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.4821388721466064, loss=2.768052101135254
I0307 06:59:11.151945 140286718113536 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.9806954860687256, loss=2.6559653282165527
I0307 06:59:49.820882 140286709720832 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.0259718894958496, loss=2.7551519870758057
I0307 07:00:28.230319 140286718113536 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.0320305824279785, loss=2.748777389526367
I0307 07:01:03.145152 140441807221952 spec.py:321] Evaluating on the training split.
I0307 07:01:15.436176 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 07:01:40.996511 140441807221952 spec.py:349] Evaluating on the test split.
I0307 07:01:42.800800 140441807221952 submission_runner.py:469] Time since start: 21638.31s, 	Step: 51992, 	{'train/accuracy': 0.6884565949440002, 'train/loss': 1.3327465057373047, 'validation/accuracy': 0.6139199733734131, 'validation/loss': 1.6658188104629517, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.347844362258911, 'test/num_examples': 10000, 'score': 19946.618819713593, 'total_duration': 21638.305015087128, 'accumulated_submission_time': 19946.618819713593, 'accumulated_eval_time': 1681.6283202171326, 'accumulated_logging_time': 4.586970090866089}
I0307 07:01:42.855105 140286709720832 logging_writer.py:48] [51992] accumulated_eval_time=1681.63, accumulated_logging_time=4.58697, accumulated_submission_time=19946.6, global_step=51992, preemption_count=0, score=19946.6, test/accuracy=0.4914, test/loss=2.34784, test/num_examples=10000, total_duration=21638.3, train/accuracy=0.688457, train/loss=1.33275, validation/accuracy=0.61392, validation/loss=1.66582, validation/num_examples=50000
I0307 07:01:46.493615 140286718113536 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.1812760829925537, loss=2.6901988983154297
I0307 07:02:24.677762 140286709720832 logging_writer.py:48] [52100] global_step=52100, grad_norm=2.8306472301483154, loss=2.737156629562378
I0307 07:03:02.632768 140286718113536 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.203233242034912, loss=2.592897891998291
I0307 07:03:40.980865 140286709720832 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.796252489089966, loss=2.6512668132781982
I0307 07:04:19.465611 140286718113536 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.041635274887085, loss=2.765313148498535
I0307 07:04:57.702572 140286709720832 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.1558496952056885, loss=2.8232765197753906
I0307 07:05:36.445069 140286718113536 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.4933931827545166, loss=2.7125742435455322
I0307 07:06:14.869448 140286709720832 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.3250057697296143, loss=2.7729332447052
I0307 07:06:53.043333 140286718113536 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.3738911151885986, loss=2.8224527835845947
I0307 07:07:31.597933 140286709720832 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.8049464225769043, loss=2.7365901470184326
I0307 07:08:09.798521 140286718113536 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.3340110778808594, loss=2.737495183944702
I0307 07:08:48.379695 140286709720832 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.161888360977173, loss=2.7980873584747314
I0307 07:09:26.790265 140286718113536 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.334589719772339, loss=2.804993152618408
I0307 07:10:05.283726 140286709720832 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.260138988494873, loss=2.787764549255371
I0307 07:10:13.196965 140441807221952 spec.py:321] Evaluating on the training split.
I0307 07:10:25.640486 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 07:10:50.038413 140441807221952 spec.py:349] Evaluating on the test split.
I0307 07:10:51.835377 140441807221952 submission_runner.py:469] Time since start: 22187.34s, 	Step: 53321, 	{'train/accuracy': 0.7049385905265808, 'train/loss': 1.3262349367141724, 'validation/accuracy': 0.6204400062561035, 'validation/loss': 1.6946985721588135, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.383591890335083, 'test/num_examples': 10000, 'score': 20456.7491106987, 'total_duration': 22187.339588165283, 'accumulated_submission_time': 20456.7491106987, 'accumulated_eval_time': 1720.2665786743164, 'accumulated_logging_time': 4.716232776641846}
I0307 07:10:51.935119 140286718113536 logging_writer.py:48] [53321] accumulated_eval_time=1720.27, accumulated_logging_time=4.71623, accumulated_submission_time=20456.7, global_step=53321, preemption_count=0, score=20456.7, test/accuracy=0.4864, test/loss=2.38359, test/num_examples=10000, total_duration=22187.3, train/accuracy=0.704939, train/loss=1.32623, validation/accuracy=0.62044, validation/loss=1.6947, validation/num_examples=50000
I0307 07:11:22.486569 140286709720832 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.115645408630371, loss=2.7017650604248047
I0307 07:12:00.944056 140286718113536 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.542630434036255, loss=2.66255521774292
I0307 07:12:39.212692 140286709720832 logging_writer.py:48] [53600] global_step=53600, grad_norm=2.8757574558258057, loss=2.749826431274414
I0307 07:13:17.137981 140286718113536 logging_writer.py:48] [53700] global_step=53700, grad_norm=2.8154444694519043, loss=2.754439353942871
I0307 07:13:55.570384 140286709720832 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.563947916030884, loss=2.7246205806732178
I0307 07:14:34.174483 140286718113536 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.903747797012329, loss=2.7297778129577637
I0307 07:15:12.069797 140286709720832 logging_writer.py:48] [54000] global_step=54000, grad_norm=2.856400489807129, loss=2.720144748687744
I0307 07:15:50.525712 140286718113536 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.078258752822876, loss=2.7279481887817383
I0307 07:16:28.810427 140286709720832 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.5182745456695557, loss=2.7294721603393555
I0307 07:17:07.143961 140286718113536 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.282860517501831, loss=2.7726047039031982
I0307 07:17:45.997839 140286709720832 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.5387589931488037, loss=2.832301378250122
I0307 07:18:24.467975 140286718113536 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.413963794708252, loss=2.75940203666687
I0307 07:19:02.367149 140286709720832 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.730679988861084, loss=2.6866631507873535
I0307 07:19:21.925849 140441807221952 spec.py:321] Evaluating on the training split.
I0307 07:19:34.493190 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 07:19:59.106970 140441807221952 spec.py:349] Evaluating on the test split.
I0307 07:20:00.855867 140441807221952 submission_runner.py:469] Time since start: 22736.36s, 	Step: 54652, 	{'train/accuracy': 0.7073301672935486, 'train/loss': 1.2522268295288086, 'validation/accuracy': 0.6242200136184692, 'validation/loss': 1.6253339052200317, 'validation/num_examples': 50000, 'test/accuracy': 0.49400001764297485, 'test/loss': 2.3052542209625244, 'test/num_examples': 10000, 'score': 20966.58277463913, 'total_duration': 22736.360019207, 'accumulated_submission_time': 20966.58277463913, 'accumulated_eval_time': 1759.1963891983032, 'accumulated_logging_time': 4.834095001220703}
I0307 07:20:00.997512 140286718113536 logging_writer.py:48] [54652] accumulated_eval_time=1759.2, accumulated_logging_time=4.8341, accumulated_submission_time=20966.6, global_step=54652, preemption_count=0, score=20966.6, test/accuracy=0.494, test/loss=2.30525, test/num_examples=10000, total_duration=22736.4, train/accuracy=0.70733, train/loss=1.25223, validation/accuracy=0.62422, validation/loss=1.62533, validation/num_examples=50000
I0307 07:20:19.875536 140286709720832 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.1509127616882324, loss=2.6754837036132812
I0307 07:20:58.059529 140286718113536 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.014533519744873, loss=2.7205238342285156
I0307 07:21:36.174856 140286709720832 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.5722272396087646, loss=2.7268667221069336
I0307 07:22:14.703884 140286718113536 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.7846007347106934, loss=2.7782349586486816
I0307 07:22:54.144389 140286709720832 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.1118650436401367, loss=2.673128366470337
I0307 07:23:33.665468 140286718113536 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.7686352729797363, loss=2.778660774230957
I0307 07:24:12.293416 140286709720832 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.468820571899414, loss=2.6349732875823975
I0307 07:24:50.760817 140286718113536 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.548483371734619, loss=2.754596710205078
I0307 07:25:29.491230 140286709720832 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.2694759368896484, loss=2.5952351093292236
I0307 07:26:07.915997 140286718113536 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.5772714614868164, loss=2.795423746109009
I0307 07:26:46.150082 140286709720832 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.002089262008667, loss=2.7808196544647217
I0307 07:27:24.673686 140286718113536 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.2068393230438232, loss=2.7601821422576904
I0307 07:28:02.777625 140286709720832 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.9092414379119873, loss=2.700218677520752
I0307 07:28:31.156366 140441807221952 spec.py:321] Evaluating on the training split.
I0307 07:28:43.537351 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 07:29:06.362580 140441807221952 spec.py:349] Evaluating on the test split.
I0307 07:29:08.152528 140441807221952 submission_runner.py:469] Time since start: 23283.66s, 	Step: 55975, 	{'train/accuracy': 0.7026267647743225, 'train/loss': 1.2670122385025024, 'validation/accuracy': 0.6241199970245361, 'validation/loss': 1.6346774101257324, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.31564998626709, 'test/num_examples': 10000, 'score': 21476.535758018494, 'total_duration': 23283.65671491623, 'accumulated_submission_time': 21476.535758018494, 'accumulated_eval_time': 1796.1923732757568, 'accumulated_logging_time': 5.038048267364502}
I0307 07:29:08.264707 140286718113536 logging_writer.py:48] [55975] accumulated_eval_time=1796.19, accumulated_logging_time=5.03805, accumulated_submission_time=21476.5, global_step=55975, preemption_count=0, score=21476.5, test/accuracy=0.4981, test/loss=2.31565, test/num_examples=10000, total_duration=23283.7, train/accuracy=0.702627, train/loss=1.26701, validation/accuracy=0.62412, validation/loss=1.63468, validation/num_examples=50000
I0307 07:29:18.296718 140286709720832 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.292560577392578, loss=2.685825824737549
I0307 07:29:56.901246 140286718113536 logging_writer.py:48] [56100] global_step=56100, grad_norm=2.982652425765991, loss=2.6451120376586914
I0307 07:30:35.355073 140286709720832 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.962064743041992, loss=2.7150816917419434
I0307 07:31:13.882699 140286718113536 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.102620840072632, loss=2.660431385040283
I0307 07:31:52.244453 140286709720832 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.2191412448883057, loss=2.6280136108398438
I0307 07:32:30.531359 140286718113536 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.065861463546753, loss=2.833423376083374
I0307 07:33:09.480007 140286709720832 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.1508359909057617, loss=2.7833964824676514
I0307 07:33:47.455528 140286718113536 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.5106568336486816, loss=2.7711548805236816
I0307 07:34:25.885929 140286709720832 logging_writer.py:48] [56800] global_step=56800, grad_norm=2.992307424545288, loss=2.6935770511627197
I0307 07:35:04.063974 140286718113536 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.2453269958496094, loss=2.8215408325195312
I0307 07:35:42.575967 140286709720832 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.740431547164917, loss=2.716045379638672
I0307 07:36:21.052952 140286718113536 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.6265718936920166, loss=2.773954153060913
I0307 07:36:59.016699 140286709720832 logging_writer.py:48] [57200] global_step=57200, grad_norm=2.739250659942627, loss=2.7352957725524902
I0307 07:37:37.331690 140286718113536 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.0737035274505615, loss=2.617077589035034
I0307 07:37:38.488989 140441807221952 spec.py:321] Evaluating on the training split.
I0307 07:37:50.973437 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 07:38:13.053750 140441807221952 spec.py:349] Evaluating on the test split.
I0307 07:38:14.837221 140441807221952 submission_runner.py:469] Time since start: 23830.34s, 	Step: 57304, 	{'train/accuracy': 0.6915856003761292, 'train/loss': 1.3624995946884155, 'validation/accuracy': 0.6171799898147583, 'validation/loss': 1.6947439908981323, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.380115270614624, 'test/num_examples': 10000, 'score': 21986.57546186447, 'total_duration': 23830.341426849365, 'accumulated_submission_time': 21986.57546186447, 'accumulated_eval_time': 1832.5404443740845, 'accumulated_logging_time': 5.195877552032471}
I0307 07:38:14.909356 140286709720832 logging_writer.py:48] [57304] accumulated_eval_time=1832.54, accumulated_logging_time=5.19588, accumulated_submission_time=21986.6, global_step=57304, preemption_count=0, score=21986.6, test/accuracy=0.4945, test/loss=2.38012, test/num_examples=10000, total_duration=23830.3, train/accuracy=0.691586, train/loss=1.3625, validation/accuracy=0.61718, validation/loss=1.69474, validation/num_examples=50000
I0307 07:38:52.124461 140286718113536 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.0421106815338135, loss=2.6443428993225098
I0307 07:39:30.514216 140286709720832 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.2640132904052734, loss=2.7154436111450195
I0307 07:40:09.102708 140286718113536 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.1847145557403564, loss=2.647090435028076
I0307 07:40:47.483094 140286709720832 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.17632794380188, loss=2.7325844764709473
I0307 07:41:26.141273 140286718113536 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.0183589458465576, loss=2.7800498008728027
I0307 07:42:04.292276 140286709720832 logging_writer.py:48] [57900] global_step=57900, grad_norm=2.9919960498809814, loss=2.7102792263031006
I0307 07:42:42.725210 140286718113536 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.714723825454712, loss=2.808137893676758
I0307 07:43:21.056739 140286709720832 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.1185412406921387, loss=2.6157219409942627
I0307 07:43:59.104902 140286718113536 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.860577344894409, loss=2.7003538608551025
I0307 07:44:37.050493 140286709720832 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.97011661529541, loss=2.7291786670684814
I0307 07:45:15.314811 140286718113536 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.299790859222412, loss=2.7439069747924805
I0307 07:45:53.664993 140286709720832 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.5310757160186768, loss=2.694380521774292
I0307 07:46:31.961303 140286718113536 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.9503214359283447, loss=2.707521915435791
I0307 07:46:44.848319 140441807221952 spec.py:321] Evaluating on the training split.
I0307 07:46:56.896962 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 07:47:18.650349 140441807221952 spec.py:349] Evaluating on the test split.
I0307 07:47:20.400459 140441807221952 submission_runner.py:469] Time since start: 24375.90s, 	Step: 58634, 	{'train/accuracy': 0.6810028553009033, 'train/loss': 1.4068670272827148, 'validation/accuracy': 0.6153799891471863, 'validation/loss': 1.7070156335830688, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.3978285789489746, 'test/num_examples': 10000, 'score': 22496.357810735703, 'total_duration': 24375.90441966057, 'accumulated_submission_time': 22496.357810735703, 'accumulated_eval_time': 1868.0921771526337, 'accumulated_logging_time': 5.289499044418335}
I0307 07:47:20.538604 140286709720832 logging_writer.py:48] [58634] accumulated_eval_time=1868.09, accumulated_logging_time=5.2895, accumulated_submission_time=22496.4, global_step=58634, preemption_count=0, score=22496.4, test/accuracy=0.4874, test/loss=2.39783, test/num_examples=10000, total_duration=24375.9, train/accuracy=0.681003, train/loss=1.40687, validation/accuracy=0.61538, validation/loss=1.70702, validation/num_examples=50000
I0307 07:47:46.381139 140286718113536 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.473212242126465, loss=2.6071951389312744
I0307 07:48:25.251699 140286709720832 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.914823293685913, loss=2.6757423877716064
I0307 07:49:03.979714 140286718113536 logging_writer.py:48] [58900] global_step=58900, grad_norm=2.8412365913391113, loss=2.74241304397583
I0307 07:49:42.479942 140286709720832 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.649634838104248, loss=2.7547719478607178
I0307 07:50:20.974522 140286718113536 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.972179412841797, loss=2.6828696727752686
I0307 07:50:59.279876 140286709720832 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.908332586288452, loss=2.678959846496582
I0307 07:51:37.839776 140286718113536 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.640197277069092, loss=2.7478694915771484
I0307 07:52:16.012578 140286709720832 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.686185121536255, loss=2.747694492340088
I0307 07:52:54.659413 140286718113536 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.440199851989746, loss=2.75287127494812
I0307 07:53:33.187002 140286709720832 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.049956798553467, loss=2.685640335083008
I0307 07:54:11.450981 140286718113536 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.112075090408325, loss=2.705366373062134
I0307 07:54:49.689673 140286709720832 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.768045425415039, loss=2.710529088973999
I0307 07:55:28.566193 140286718113536 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.945629358291626, loss=2.697690963745117
I0307 07:55:50.469859 140441807221952 spec.py:321] Evaluating on the training split.
I0307 07:56:02.827477 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 07:56:27.358577 140441807221952 spec.py:349] Evaluating on the test split.
I0307 07:56:29.153295 140441807221952 submission_runner.py:469] Time since start: 24924.66s, 	Step: 59956, 	{'train/accuracy': 0.6892139315605164, 'train/loss': 1.3383091688156128, 'validation/accuracy': 0.6202799677848816, 'validation/loss': 1.6595367193222046, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.3100039958953857, 'test/num_examples': 10000, 'score': 23006.12411880493, 'total_duration': 24924.65745139122, 'accumulated_submission_time': 23006.12411880493, 'accumulated_eval_time': 1906.775414466858, 'accumulated_logging_time': 5.4525697231292725}
I0307 07:56:29.288378 140286709720832 logging_writer.py:48] [59956] accumulated_eval_time=1906.78, accumulated_logging_time=5.45257, accumulated_submission_time=23006.1, global_step=59956, preemption_count=0, score=23006.1, test/accuracy=0.501, test/loss=2.31, test/num_examples=10000, total_duration=24924.7, train/accuracy=0.689214, train/loss=1.33831, validation/accuracy=0.62028, validation/loss=1.65954, validation/num_examples=50000
I0307 07:56:46.520305 140286718113536 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.124931812286377, loss=2.631852149963379
I0307 07:57:25.711440 140286709720832 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.110102891921997, loss=2.606271743774414
I0307 07:58:04.135157 140286718113536 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.881085157394409, loss=2.794311285018921
I0307 07:58:42.633662 140286709720832 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.6108200550079346, loss=2.6910340785980225
I0307 07:59:20.996712 140286718113536 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.030917167663574, loss=2.7524807453155518
I0307 07:59:59.327482 140286709720832 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.924217462539673, loss=2.78747296333313
I0307 08:00:37.197864 140286718113536 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.4746320247650146, loss=2.6731138229370117
I0307 08:01:15.780966 140286709720832 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.799504041671753, loss=2.722862720489502
I0307 08:01:54.137480 140286718113536 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.3499202728271484, loss=2.664705514907837
I0307 08:02:32.551431 140286709720832 logging_writer.py:48] [60900] global_step=60900, grad_norm=2.9145374298095703, loss=2.711364984512329
I0307 08:03:10.618265 140286718113536 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.406506299972534, loss=2.6965420246124268
I0307 08:03:49.224300 140286709720832 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.066298007965088, loss=2.7054295539855957
I0307 08:04:27.809668 140286718113536 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.541341543197632, loss=2.7251946926116943
I0307 08:04:59.212508 140441807221952 spec.py:321] Evaluating on the training split.
I0307 08:05:11.664595 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 08:05:41.178291 140441807221952 spec.py:349] Evaluating on the test split.
I0307 08:05:42.974830 140441807221952 submission_runner.py:469] Time since start: 25478.48s, 	Step: 61282, 	{'train/accuracy': 0.6900709271430969, 'train/loss': 1.3561550378799438, 'validation/accuracy': 0.6269999742507935, 'validation/loss': 1.6514174938201904, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.339822292327881, 'test/num_examples': 10000, 'score': 23515.868244171143, 'total_duration': 25478.479033708572, 'accumulated_submission_time': 23515.868244171143, 'accumulated_eval_time': 1950.537579536438, 'accumulated_logging_time': 5.629275560379028}
I0307 08:05:43.055620 140286709720832 logging_writer.py:48] [61282] accumulated_eval_time=1950.54, accumulated_logging_time=5.62928, accumulated_submission_time=23515.9, global_step=61282, preemption_count=0, score=23515.9, test/accuracy=0.5001, test/loss=2.33982, test/num_examples=10000, total_duration=25478.5, train/accuracy=0.690071, train/loss=1.35616, validation/accuracy=0.627, validation/loss=1.65142, validation/num_examples=50000
I0307 08:05:50.717031 140286718113536 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.1531496047973633, loss=2.6456871032714844
I0307 08:06:28.639176 140286709720832 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.1258926391601562, loss=2.6563782691955566
I0307 08:07:07.023543 140286718113536 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.597407102584839, loss=2.743528366088867
I0307 08:07:45.396238 140286709720832 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.0593180656433105, loss=2.641305923461914
I0307 08:08:23.710036 140286718113536 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.3160526752471924, loss=2.7200255393981934
I0307 08:09:02.183162 140286709720832 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.4073197841644287, loss=2.678631544113159
I0307 08:09:40.718701 140286718113536 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.1452386379241943, loss=2.690387725830078
I0307 08:10:19.409098 140286709720832 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.159076690673828, loss=2.653304100036621
I0307 08:10:57.988878 140286718113536 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.2312235832214355, loss=2.6873526573181152
I0307 08:11:36.570282 140286709720832 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.923811435699463, loss=2.7256197929382324
I0307 08:12:15.120014 140286718113536 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.1495068073272705, loss=2.6973907947540283
I0307 08:12:53.562747 140286709720832 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.9009652137756348, loss=2.72173810005188
I0307 08:13:32.063102 140286718113536 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.1815881729125977, loss=2.7240538597106934
I0307 08:14:10.831431 140286709720832 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.9004952907562256, loss=2.7744524478912354
I0307 08:14:13.217449 140441807221952 spec.py:321] Evaluating on the training split.
I0307 08:14:25.426795 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 08:14:48.182056 140441807221952 spec.py:349] Evaluating on the test split.
I0307 08:14:49.958814 140441807221952 submission_runner.py:469] Time since start: 26025.46s, 	Step: 62607, 	{'train/accuracy': 0.6828364133834839, 'train/loss': 1.3486058712005615, 'validation/accuracy': 0.6195999979972839, 'validation/loss': 1.648579478263855, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.3349251747131348, 'test/num_examples': 10000, 'score': 24025.852508068085, 'total_duration': 26025.462947130203, 'accumulated_submission_time': 24025.852508068085, 'accumulated_eval_time': 1987.2787127494812, 'accumulated_logging_time': 5.746744155883789}
I0307 08:14:50.058311 140286718113536 logging_writer.py:48] [62607] accumulated_eval_time=1987.28, accumulated_logging_time=5.74674, accumulated_submission_time=24025.9, global_step=62607, preemption_count=0, score=24025.9, test/accuracy=0.4954, test/loss=2.33493, test/num_examples=10000, total_duration=26025.5, train/accuracy=0.682836, train/loss=1.34861, validation/accuracy=0.6196, validation/loss=1.64858, validation/num_examples=50000
I0307 08:15:26.213786 140286709720832 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.644719362258911, loss=2.7334647178649902
I0307 08:16:04.820679 140286718113536 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.22377872467041, loss=2.790306329727173
I0307 08:16:43.598046 140286709720832 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.4852962493896484, loss=2.657191276550293
I0307 08:17:21.867376 140286718113536 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.9535934925079346, loss=2.732797384262085
I0307 08:18:00.276712 140286709720832 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.179166793823242, loss=2.834533214569092
I0307 08:18:38.392444 140286718113536 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.8253674507141113, loss=2.6053504943847656
I0307 08:19:16.585062 140286709720832 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.1383721828460693, loss=2.7765285968780518
I0307 08:19:54.999684 140286718113536 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.2314770221710205, loss=2.6911163330078125
I0307 08:20:33.547737 140286709720832 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.8046107292175293, loss=2.5987977981567383
I0307 08:21:12.005761 140286718113536 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.0122663974761963, loss=2.808236837387085
I0307 08:21:50.391757 140286709720832 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.0940983295440674, loss=2.712250232696533
I0307 08:22:29.244605 140286718113536 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.944312810897827, loss=2.6706151962280273
I0307 08:23:08.513596 140286709720832 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.9049558639526367, loss=2.6037509441375732
I0307 08:23:20.296507 140441807221952 spec.py:321] Evaluating on the training split.
I0307 08:23:32.811367 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 08:23:58.748103 140441807221952 spec.py:349] Evaluating on the test split.
I0307 08:24:00.491760 140441807221952 submission_runner.py:469] Time since start: 26576.00s, 	Step: 63932, 	{'train/accuracy': 0.6830955147743225, 'train/loss': 1.3851529359817505, 'validation/accuracy': 0.6187399625778198, 'validation/loss': 1.6893764734268188, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.3248541355133057, 'test/num_examples': 10000, 'score': 24535.916409254074, 'total_duration': 26575.995939731598, 'accumulated_submission_time': 24535.916409254074, 'accumulated_eval_time': 2027.4737813472748, 'accumulated_logging_time': 5.882742881774902}
I0307 08:24:00.584552 140286718113536 logging_writer.py:48] [63932] accumulated_eval_time=2027.47, accumulated_logging_time=5.88274, accumulated_submission_time=24535.9, global_step=63932, preemption_count=0, score=24535.9, test/accuracy=0.4976, test/loss=2.32485, test/num_examples=10000, total_duration=26576, train/accuracy=0.683096, train/loss=1.38515, validation/accuracy=0.61874, validation/loss=1.68938, validation/num_examples=50000
I0307 08:24:26.854411 140286709720832 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.3301312923431396, loss=2.6367087364196777
I0307 08:25:04.676130 140286718113536 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.055335760116577, loss=2.6701467037200928
I0307 08:25:43.252921 140286709720832 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.0729620456695557, loss=2.6240954399108887
I0307 08:26:21.765428 140286718113536 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.8925881385803223, loss=2.7872684001922607
I0307 08:27:00.388886 140286709720832 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.867802381515503, loss=2.7064783573150635
I0307 08:27:39.055920 140286718113536 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.746208906173706, loss=2.730242967605591
I0307 08:28:17.298290 140286709720832 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.6481738090515137, loss=2.658574342727661
I0307 08:28:55.442848 140286718113536 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.571206569671631, loss=2.6085572242736816
I0307 08:29:34.233167 140286709720832 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.8666038513183594, loss=2.61692214012146
I0307 08:30:12.857501 140286718113536 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.9894280433654785, loss=2.733060359954834
I0307 08:30:51.715171 140286709720832 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.932685136795044, loss=2.708174228668213
I0307 08:31:30.634042 140286718113536 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.2668869495391846, loss=2.703117847442627
I0307 08:32:08.873613 140286709720832 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.8888752460479736, loss=2.7168009281158447
I0307 08:32:30.838142 140441807221952 spec.py:321] Evaluating on the training split.
I0307 08:32:43.173876 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 08:33:04.096371 140441807221952 spec.py:349] Evaluating on the test split.
I0307 08:33:05.901037 140441807221952 submission_runner.py:469] Time since start: 27121.41s, 	Step: 65259, 	{'train/accuracy': 0.6919044852256775, 'train/loss': 1.3145473003387451, 'validation/accuracy': 0.6337999701499939, 'validation/loss': 1.5892457962036133, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.269313335418701, 'test/num_examples': 10000, 'score': 25046.002423286438, 'total_duration': 27121.405230998993, 'accumulated_submission_time': 25046.002423286438, 'accumulated_eval_time': 2062.5365097522736, 'accumulated_logging_time': 6.0029826164245605}
I0307 08:33:05.982260 140286718113536 logging_writer.py:48] [65259] accumulated_eval_time=2062.54, accumulated_logging_time=6.00298, accumulated_submission_time=25046, global_step=65259, preemption_count=0, score=25046, test/accuracy=0.5085, test/loss=2.26931, test/num_examples=10000, total_duration=27121.4, train/accuracy=0.691904, train/loss=1.31455, validation/accuracy=0.6338, validation/loss=1.58925, validation/num_examples=50000
I0307 08:33:22.172700 140286709720832 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.004930257797241, loss=2.685920000076294
I0307 08:34:00.700961 140286718113536 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.3464884757995605, loss=2.6305079460144043
I0307 08:34:38.843953 140286709720832 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.1987907886505127, loss=2.7372052669525146
I0307 08:35:17.451659 140286718113536 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.7845206260681152, loss=2.6707587242126465
I0307 08:35:55.921861 140286709720832 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.7093164920806885, loss=2.7233517169952393
I0307 08:36:34.490409 140286718113536 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.9473655223846436, loss=2.5981788635253906
I0307 08:37:13.345544 140286709720832 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.835317611694336, loss=2.660322666168213
I0307 08:37:51.644100 140286718113536 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.2989163398742676, loss=2.6593077182769775
I0307 08:38:30.150786 140286709720832 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.542280912399292, loss=2.669682741165161
I0307 08:39:08.491226 140286718113536 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.0990731716156006, loss=2.773073673248291
I0307 08:39:47.163084 140286709720832 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.132232904434204, loss=2.7049598693847656
I0307 08:40:26.203042 140286718113536 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.288327217102051, loss=2.74346661567688
I0307 08:41:04.449439 140286709720832 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.65016508102417, loss=2.7043094635009766
I0307 08:41:35.924299 140441807221952 spec.py:321] Evaluating on the training split.
I0307 08:41:47.804358 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 08:42:19.570178 140441807221952 spec.py:349] Evaluating on the test split.
I0307 08:42:21.362268 140441807221952 submission_runner.py:469] Time since start: 27676.87s, 	Step: 66583, 	{'train/accuracy': 0.68558669090271, 'train/loss': 1.3441452980041504, 'validation/accuracy': 0.6242799758911133, 'validation/loss': 1.6398649215698242, 'validation/num_examples': 50000, 'test/accuracy': 0.5005000233650208, 'test/loss': 2.3072714805603027, 'test/num_examples': 10000, 'score': 25555.77280497551, 'total_duration': 27676.866458654404, 'accumulated_submission_time': 25555.77280497551, 'accumulated_eval_time': 2107.9743111133575, 'accumulated_logging_time': 6.117950677871704}
I0307 08:42:21.457308 140286718113536 logging_writer.py:48] [66583] accumulated_eval_time=2107.97, accumulated_logging_time=6.11795, accumulated_submission_time=25555.8, global_step=66583, preemption_count=0, score=25555.8, test/accuracy=0.5005, test/loss=2.30727, test/num_examples=10000, total_duration=27676.9, train/accuracy=0.685587, train/loss=1.34415, validation/accuracy=0.62428, validation/loss=1.63986, validation/num_examples=50000
I0307 08:42:28.392055 140286709720832 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.8733937740325928, loss=2.6418850421905518
I0307 08:43:06.395938 140286718113536 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.289915084838867, loss=2.7005550861358643
I0307 08:43:44.472125 140286709720832 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.767434597015381, loss=2.680433988571167
I0307 08:44:22.657368 140286718113536 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.779383420944214, loss=2.6692514419555664
I0307 08:45:01.037321 140286709720832 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.997145175933838, loss=2.7121689319610596
I0307 08:45:39.895757 140286718113536 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.479323148727417, loss=2.767237424850464
I0307 08:46:18.158114 140286709720832 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.2230489253997803, loss=2.625612258911133
I0307 08:46:56.609221 140286718113536 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.33514666557312, loss=2.6602845191955566
I0307 08:47:35.637389 140286709720832 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.9816482067108154, loss=2.708460807800293
I0307 08:48:14.265505 140286718113536 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.8896477222442627, loss=2.6130075454711914
I0307 08:48:52.810528 140286709720832 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.720285177230835, loss=2.7296693325042725
I0307 08:49:31.393120 140286718113536 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.2087883949279785, loss=2.6806046962738037
I0307 08:50:09.587278 140286709720832 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.110340118408203, loss=2.600278377532959
I0307 08:50:48.056726 140286718113536 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.014979362487793, loss=2.7722702026367188
I0307 08:50:51.574722 140441807221952 spec.py:321] Evaluating on the training split.
I0307 08:51:03.989728 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 08:51:31.610896 140441807221952 spec.py:349] Evaluating on the test split.
I0307 08:51:33.398669 140441807221952 submission_runner.py:469] Time since start: 28228.90s, 	Step: 67910, 	{'train/accuracy': 0.678730845451355, 'train/loss': 1.3553584814071655, 'validation/accuracy': 0.6238200068473816, 'validation/loss': 1.6288461685180664, 'validation/num_examples': 50000, 'test/accuracy': 0.49550002813339233, 'test/loss': 2.3253026008605957, 'test/num_examples': 10000, 'score': 26065.72079849243, 'total_duration': 28228.902816295624, 'accumulated_submission_time': 26065.72079849243, 'accumulated_eval_time': 2149.7980675697327, 'accumulated_logging_time': 6.2447285652160645}
I0307 08:51:33.575418 140286709720832 logging_writer.py:48] [67910] accumulated_eval_time=2149.8, accumulated_logging_time=6.24473, accumulated_submission_time=26065.7, global_step=67910, preemption_count=0, score=26065.7, test/accuracy=0.4955, test/loss=2.3253, test/num_examples=10000, total_duration=28228.9, train/accuracy=0.678731, train/loss=1.35536, validation/accuracy=0.62382, validation/loss=1.62885, validation/num_examples=50000
I0307 08:52:08.182397 140286718113536 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.8509039878845215, loss=2.657754421234131
I0307 08:52:46.620510 140286709720832 logging_writer.py:48] [68100] global_step=68100, grad_norm=4.3112688064575195, loss=2.7114217281341553
I0307 08:53:24.875556 140286718113536 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.21250057220459, loss=2.6578927040100098
I0307 08:54:03.690529 140286709720832 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.4480934143066406, loss=2.6984927654266357
I0307 08:54:41.826855 140286718113536 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.094190835952759, loss=2.6325790882110596
I0307 08:55:20.387177 140286709720832 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.3114500045776367, loss=2.5967025756835938
I0307 08:55:58.993542 140286718113536 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.575361490249634, loss=2.675380229949951
I0307 08:56:37.332842 140286709720832 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.0948991775512695, loss=2.7445292472839355
I0307 08:57:15.980257 140286718113536 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.241743803024292, loss=2.648008346557617
I0307 08:57:54.912655 140286709720832 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.2658872604370117, loss=2.8384432792663574
I0307 08:58:33.782115 140286718113536 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.935702085494995, loss=2.695028305053711
I0307 08:59:12.335114 140286709720832 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.8583834171295166, loss=2.62845778465271
I0307 08:59:51.048171 140286718113536 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.904616355895996, loss=2.6502556800842285
I0307 09:00:03.632065 140441807221952 spec.py:321] Evaluating on the training split.
I0307 09:00:15.949687 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 09:00:39.044724 140441807221952 spec.py:349] Evaluating on the test split.
I0307 09:00:40.836118 140441807221952 submission_runner.py:469] Time since start: 28776.34s, 	Step: 69233, 	{'train/accuracy': 0.6938974857330322, 'train/loss': 1.3298683166503906, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.611121416091919, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2512662410736084, 'test/num_examples': 10000, 'score': 26575.613506555557, 'total_duration': 28776.34026670456, 'accumulated_submission_time': 26575.613506555557, 'accumulated_eval_time': 2187.0019011497498, 'accumulated_logging_time': 6.442857027053833}
I0307 09:00:40.920584 140286709720832 logging_writer.py:48] [69233] accumulated_eval_time=2187, accumulated_logging_time=6.44286, accumulated_submission_time=26575.6, global_step=69233, preemption_count=0, score=26575.6, test/accuracy=0.5121, test/loss=2.25127, test/num_examples=10000, total_duration=28776.3, train/accuracy=0.693897, train/loss=1.32987, validation/accuracy=0.63204, validation/loss=1.61112, validation/num_examples=50000
I0307 09:01:07.120463 140286718113536 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.042877674102783, loss=2.5933074951171875
I0307 09:01:46.111133 140286709720832 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.571953058242798, loss=2.725083351135254
I0307 09:02:24.762610 140286718113536 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.2896909713745117, loss=2.661799430847168
I0307 09:03:03.490423 140286709720832 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.1950788497924805, loss=2.518484592437744
I0307 09:03:41.840021 140286718113536 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.73297381401062, loss=2.5710370540618896
I0307 09:04:20.440249 140286709720832 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.98673677444458, loss=2.628321886062622
I0307 09:04:59.947850 140286718113536 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.2877309322357178, loss=2.579555034637451
I0307 09:05:40.177091 140286709720832 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.457411289215088, loss=2.563746690750122
I0307 09:06:19.670828 140286718113536 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.5798113346099854, loss=2.66227650642395
I0307 09:06:58.857282 140286709720832 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.0080082416534424, loss=2.680799722671509
I0307 09:07:37.752190 140286718113536 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.2582547664642334, loss=2.649019479751587
I0307 09:08:16.692814 140286709720832 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.351445436477661, loss=2.603853940963745
I0307 09:08:55.539834 140286718113536 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.8108227252960205, loss=2.582606554031372
I0307 09:09:11.187104 140441807221952 spec.py:321] Evaluating on the training split.
I0307 09:09:23.602782 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 09:09:47.420605 140441807221952 spec.py:349] Evaluating on the test split.
I0307 09:09:49.215528 140441807221952 submission_runner.py:469] Time since start: 29324.72s, 	Step: 70541, 	{'train/accuracy': 0.6960698366165161, 'train/loss': 1.3099411725997925, 'validation/accuracy': 0.6351799964904785, 'validation/loss': 1.5836288928985596, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.235600709915161, 'test/num_examples': 10000, 'score': 27085.704206466675, 'total_duration': 29324.719745397568, 'accumulated_submission_time': 27085.704206466675, 'accumulated_eval_time': 2225.030177116394, 'accumulated_logging_time': 6.551129341125488}
I0307 09:09:49.329867 140286709720832 logging_writer.py:48] [70541] accumulated_eval_time=2225.03, accumulated_logging_time=6.55113, accumulated_submission_time=27085.7, global_step=70541, preemption_count=0, score=27085.7, test/accuracy=0.5108, test/loss=2.2356, test/num_examples=10000, total_duration=29324.7, train/accuracy=0.69607, train/loss=1.30994, validation/accuracy=0.63518, validation/loss=1.58363, validation/num_examples=50000
I0307 09:10:13.062008 140286718113536 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.204625129699707, loss=2.643625020980835
I0307 09:10:52.060472 140286709720832 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.3877735137939453, loss=2.7139663696289062
I0307 09:11:32.181244 140286718113536 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.211575508117676, loss=2.5771875381469727
I0307 09:12:11.904856 140286709720832 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.0853347778320312, loss=2.7126362323760986
I0307 09:12:51.371020 140286718113536 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.9705071449279785, loss=2.6269898414611816
I0307 09:13:30.753070 140286709720832 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.89329195022583, loss=2.606320858001709
I0307 09:14:11.382487 140286718113536 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.918588876724243, loss=2.5698437690734863
I0307 09:14:50.871979 140286709720832 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.1746068000793457, loss=2.6290485858917236
I0307 09:15:30.144890 140286718113536 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.324706554412842, loss=2.815265655517578
I0307 09:16:10.076582 140286709720832 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.0324900150299072, loss=2.7359981536865234
I0307 09:16:49.468492 140286718113536 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.26051664352417, loss=2.6924185752868652
I0307 09:17:28.968435 140286709720832 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.4398019313812256, loss=2.7245688438415527
I0307 09:18:09.244016 140286718113536 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.1754462718963623, loss=2.5262601375579834
I0307 09:18:19.266109 140441807221952 spec.py:321] Evaluating on the training split.
I0307 09:18:31.751714 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 09:18:52.747829 140441807221952 spec.py:349] Evaluating on the test split.
I0307 09:18:54.531383 140441807221952 submission_runner.py:469] Time since start: 29870.04s, 	Step: 71826, 	{'train/accuracy': 0.6858657598495483, 'train/loss': 1.3627768754959106, 'validation/accuracy': 0.6281200051307678, 'validation/loss': 1.6324516534805298, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.318972110748291, 'test/num_examples': 10000, 'score': 27595.434324502945, 'total_duration': 29870.035591840744, 'accumulated_submission_time': 27595.434324502945, 'accumulated_eval_time': 2260.295294046402, 'accumulated_logging_time': 6.7141430377960205}
I0307 09:18:54.624640 140286709720832 logging_writer.py:48] [71826] accumulated_eval_time=2260.3, accumulated_logging_time=6.71414, accumulated_submission_time=27595.4, global_step=71826, preemption_count=0, score=27595.4, test/accuracy=0.5009, test/loss=2.31897, test/num_examples=10000, total_duration=29870, train/accuracy=0.685866, train/loss=1.36278, validation/accuracy=0.62812, validation/loss=1.63245, validation/num_examples=50000
I0307 09:19:24.399416 140286718113536 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.3455874919891357, loss=2.6623599529266357
I0307 09:20:03.983268 140286709720832 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.110999822616577, loss=2.5797479152679443
I0307 09:20:43.392119 140286718113536 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.0731682777404785, loss=2.590811014175415
I0307 09:21:23.621582 140286709720832 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.7354249954223633, loss=2.6667091846466064
I0307 09:22:03.542683 140286718113536 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.3129913806915283, loss=2.7487757205963135
I0307 09:22:45.282768 140286709720832 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.325100898742676, loss=2.660618305206299
I0307 09:23:26.308076 140286718113536 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.0134825706481934, loss=2.607905626296997
2025-03-07 09:23:43.959082: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:24:06.410577 140286709720832 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.3119025230407715, loss=2.661862850189209
I0307 09:24:46.114422 140286718113536 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.022979736328125, loss=2.731264352798462
I0307 09:25:25.725790 140286709720832 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.3328857421875, loss=2.6739609241485596
I0307 09:26:05.626589 140286718113536 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.321103811264038, loss=2.710247039794922
I0307 09:26:45.137115 140286709720832 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.1815433502197266, loss=2.62117862701416
I0307 09:27:24.538847 140286718113536 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.771423101425171, loss=2.63885498046875
I0307 09:27:24.552628 140441807221952 spec.py:321] Evaluating on the training split.
I0307 09:27:37.351942 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 09:27:56.115725 140441807221952 spec.py:349] Evaluating on the test split.
I0307 09:27:57.897669 140441807221952 submission_runner.py:469] Time since start: 30413.40s, 	Step: 73101, 	{'train/accuracy': 0.6825972199440002, 'train/loss': 1.3581565618515015, 'validation/accuracy': 0.6251199841499329, 'validation/loss': 1.628553032875061, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.3095176219940186, 'test/num_examples': 10000, 'score': 28105.17962193489, 'total_duration': 30413.40187072754, 'accumulated_submission_time': 28105.17962193489, 'accumulated_eval_time': 2293.640163898468, 'accumulated_logging_time': 6.834480285644531}
I0307 09:27:57.988233 140286709720832 logging_writer.py:48] [73101] accumulated_eval_time=2293.64, accumulated_logging_time=6.83448, accumulated_submission_time=28105.2, global_step=73101, preemption_count=0, score=28105.2, test/accuracy=0.499, test/loss=2.30952, test/num_examples=10000, total_duration=30413.4, train/accuracy=0.682597, train/loss=1.35816, validation/accuracy=0.62512, validation/loss=1.62855, validation/num_examples=50000
I0307 09:28:37.426987 140286718113536 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.392381429672241, loss=2.6956491470336914
I0307 09:29:17.434282 140286709720832 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.107146978378296, loss=2.59417462348938
I0307 09:29:57.499721 140286718113536 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.3001444339752197, loss=2.701892852783203
I0307 09:30:36.755339 140286709720832 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.259986639022827, loss=2.6621475219726562
I0307 09:31:16.260511 140286718113536 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.953425407409668, loss=2.5860302448272705
I0307 09:31:55.944202 140286709720832 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.9829225540161133, loss=2.6528379917144775
I0307 09:32:35.848235 140286718113536 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.1811904907226562, loss=2.5860233306884766
I0307 09:33:57.180364 140286709720832 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.3055636882781982, loss=2.6310670375823975
I0307 09:34:36.893004 140286718113536 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.0541350841522217, loss=2.7373392581939697
I0307 09:35:17.001374 140286709720832 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.6425554752349854, loss=2.6814634799957275
I0307 09:35:57.099353 140286718113536 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.3173136711120605, loss=2.6619017124176025
I0307 09:36:28.042078 140441807221952 spec.py:321] Evaluating on the training split.
I0307 09:36:40.593981 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 09:37:03.735668 140441807221952 spec.py:349] Evaluating on the test split.
I0307 09:37:05.528910 140441807221952 submission_runner.py:469] Time since start: 30961.03s, 	Step: 74279, 	{'train/accuracy': 0.7035036683082581, 'train/loss': 1.2861485481262207, 'validation/accuracy': 0.6349599957466125, 'validation/loss': 1.6019941568374634, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.2620930671691895, 'test/num_examples': 10000, 'score': 28615.057426452637, 'total_duration': 30961.0330824852, 'accumulated_submission_time': 28615.057426452637, 'accumulated_eval_time': 2331.126804113388, 'accumulated_logging_time': 6.954662084579468}
I0307 09:37:05.629940 140286709720832 logging_writer.py:48] [74279] accumulated_eval_time=2331.13, accumulated_logging_time=6.95466, accumulated_submission_time=28615.1, global_step=74279, preemption_count=0, score=28615.1, test/accuracy=0.5112, test/loss=2.26209, test/num_examples=10000, total_duration=30961, train/accuracy=0.703504, train/loss=1.28615, validation/accuracy=0.63496, validation/loss=1.60199, validation/num_examples=50000
I0307 09:37:14.381029 140286718113536 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.092432737350464, loss=2.6233325004577637
I0307 09:37:54.038247 140286709720832 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.460930585861206, loss=2.5475378036499023
I0307 09:38:34.209514 140286718113536 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.137779712677002, loss=2.6080102920532227
I0307 09:39:14.045443 140286709720832 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.910278558731079, loss=2.5094799995422363
I0307 09:39:54.076284 140286718113536 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.225146532058716, loss=2.541698932647705
I0307 09:40:34.093761 140286709720832 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.094095468521118, loss=2.6001062393188477
I0307 09:41:13.966668 140286718113536 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.0756123065948486, loss=2.6221187114715576
I0307 09:41:53.288927 140286709720832 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.0078070163726807, loss=2.6808652877807617
I0307 09:42:32.698238 140286718113536 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.004502534866333, loss=2.619978904724121
I0307 09:43:11.945135 140286709720832 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.4943864345550537, loss=2.755202531814575
I0307 09:43:51.759660 140286718113536 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.042912483215332, loss=2.5051956176757812
I0307 09:44:31.100848 140286709720832 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.1910440921783447, loss=2.6269776821136475
I0307 09:45:10.905245 140286718113536 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.557003974914551, loss=2.653043031692505
I0307 09:45:35.534937 140441807221952 spec.py:321] Evaluating on the training split.
I0307 09:45:47.858035 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 09:46:12.031594 140441807221952 spec.py:349] Evaluating on the test split.
I0307 09:46:13.854180 140441807221952 submission_runner.py:469] Time since start: 31509.36s, 	Step: 75563, 	{'train/accuracy': 0.7042809128761292, 'train/loss': 1.2806239128112793, 'validation/accuracy': 0.6369199752807617, 'validation/loss': 1.5822803974151611, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.261535167694092, 'test/num_examples': 10000, 'score': 29124.73771929741, 'total_duration': 31509.358362436295, 'accumulated_submission_time': 29124.73771929741, 'accumulated_eval_time': 2369.4458730220795, 'accumulated_logging_time': 7.12560248374939}
I0307 09:46:13.970659 140286709720832 logging_writer.py:48] [75563] accumulated_eval_time=2369.45, accumulated_logging_time=7.1256, accumulated_submission_time=29124.7, global_step=75563, preemption_count=0, score=29124.7, test/accuracy=0.5149, test/loss=2.26154, test/num_examples=10000, total_duration=31509.4, train/accuracy=0.704281, train/loss=1.28062, validation/accuracy=0.63692, validation/loss=1.58228, validation/num_examples=50000
I0307 09:46:29.353441 140286718113536 logging_writer.py:48] [75600] global_step=75600, grad_norm=4.020870208740234, loss=2.6358604431152344
I0307 09:47:08.704262 140286709720832 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.6587345600128174, loss=2.65069580078125
I0307 09:47:48.477148 140286718113536 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.257762908935547, loss=2.651942014694214
I0307 09:48:28.087709 140286709720832 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.8769640922546387, loss=2.6406607627868652
I0307 09:49:07.358781 140286718113536 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.152482271194458, loss=2.6401474475860596
I0307 09:49:47.133699 140286709720832 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.112517833709717, loss=2.6228833198547363
I0307 09:50:27.377404 140286718113536 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.565513849258423, loss=2.7572755813598633
I0307 09:51:08.756962 140286709720832 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.340996026992798, loss=2.578866958618164
I0307 09:51:47.726962 140286718113536 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.1598963737487793, loss=2.623749256134033
I0307 09:52:27.772325 140286709720832 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.4091475009918213, loss=2.6621100902557373
I0307 09:53:07.820401 140286718113536 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.9326469898223877, loss=2.610032081604004
I0307 09:53:47.747777 140286709720832 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.3933959007263184, loss=2.6559572219848633
I0307 09:54:27.527009 140286718113536 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.178694248199463, loss=2.6571240425109863
I0307 09:54:44.190392 140441807221952 spec.py:321] Evaluating on the training split.
I0307 09:54:56.766109 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 09:55:23.492643 140441807221952 spec.py:349] Evaluating on the test split.
I0307 09:55:25.307003 140441807221952 submission_runner.py:469] Time since start: 32060.81s, 	Step: 76843, 	{'train/accuracy': 0.7054368257522583, 'train/loss': 1.251882791519165, 'validation/accuracy': 0.6315000057220459, 'validation/loss': 1.5915164947509766, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.22709321975708, 'test/num_examples': 10000, 'score': 29634.777524232864, 'total_duration': 32060.8112244606, 'accumulated_submission_time': 29634.777524232864, 'accumulated_eval_time': 2410.5623364448547, 'accumulated_logging_time': 7.266575336456299}
I0307 09:55:25.417656 140286709720832 logging_writer.py:48] [76843] accumulated_eval_time=2410.56, accumulated_logging_time=7.26658, accumulated_submission_time=29634.8, global_step=76843, preemption_count=0, score=29634.8, test/accuracy=0.5108, test/loss=2.22709, test/num_examples=10000, total_duration=32060.8, train/accuracy=0.705437, train/loss=1.25188, validation/accuracy=0.6315, validation/loss=1.59152, validation/num_examples=50000
I0307 09:55:48.375714 140286718113536 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.6857197284698486, loss=2.6742935180664062
I0307 09:56:28.166450 140286709720832 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.3296637535095215, loss=2.593191623687744
I0307 09:57:07.865644 140286718113536 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.567532777786255, loss=2.5754952430725098
I0307 09:57:47.336843 140286709720832 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.36836838722229, loss=2.58162784576416
I0307 09:58:26.586496 140286718113536 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.9415335655212402, loss=2.573747396469116
I0307 09:59:06.076060 140286709720832 logging_writer.py:48] [77400] global_step=77400, grad_norm=4.280339241027832, loss=2.715789556503296
I0307 09:59:44.986246 140286718113536 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.4860219955444336, loss=2.574091911315918
I0307 10:00:24.361444 140286709720832 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.360959768295288, loss=2.7002317905426025
I0307 10:01:03.279747 140286718113536 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.0201683044433594, loss=2.670107126235962
I0307 10:01:42.658275 140286709720832 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.253561496734619, loss=2.6239452362060547
I0307 10:02:22.181423 140286718113536 logging_writer.py:48] [77900] global_step=77900, grad_norm=4.284835338592529, loss=2.6961312294006348
I0307 10:03:01.649355 140286709720832 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.4211196899414062, loss=2.6832079887390137
I0307 10:03:41.466684 140286718113536 logging_writer.py:48] [78100] global_step=78100, grad_norm=4.460380554199219, loss=2.679670572280884
I0307 10:03:55.514222 140441807221952 spec.py:321] Evaluating on the training split.
I0307 10:04:08.083153 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 10:04:27.737930 140441807221952 spec.py:349] Evaluating on the test split.
I0307 10:04:29.562603 140441807221952 submission_runner.py:469] Time since start: 32605.07s, 	Step: 78136, 	{'train/accuracy': 0.7095025181770325, 'train/loss': 1.2386481761932373, 'validation/accuracy': 0.6372799873352051, 'validation/loss': 1.564919352531433, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.224498987197876, 'test/num_examples': 10000, 'score': 30144.693754911423, 'total_duration': 32605.066781044006, 'accumulated_submission_time': 30144.693754911423, 'accumulated_eval_time': 2444.6105239391327, 'accumulated_logging_time': 7.402559041976929}
I0307 10:04:29.682744 140286709720832 logging_writer.py:48] [78136] accumulated_eval_time=2444.61, accumulated_logging_time=7.40256, accumulated_submission_time=30144.7, global_step=78136, preemption_count=0, score=30144.7, test/accuracy=0.5114, test/loss=2.2245, test/num_examples=10000, total_duration=32605.1, train/accuracy=0.709503, train/loss=1.23865, validation/accuracy=0.63728, validation/loss=1.56492, validation/num_examples=50000
I0307 10:04:54.991885 140286718113536 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.8107106685638428, loss=2.571885108947754
I0307 10:05:34.397593 140286709720832 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.9811041355133057, loss=2.6479575634002686
I0307 10:06:13.880360 140286718113536 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.492114543914795, loss=2.5629215240478516
I0307 10:06:53.391893 140286709720832 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.732261896133423, loss=2.5575191974639893
I0307 10:07:32.829740 140286718113536 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.362438678741455, loss=2.5998613834381104
I0307 10:08:12.562435 140286709720832 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.2626612186431885, loss=2.6575920581817627
I0307 10:08:52.586254 140286718113536 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.0507776737213135, loss=2.519493341445923
I0307 10:09:31.596200 140286709720832 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.2454376220703125, loss=2.604234457015991
I0307 10:10:10.791055 140286718113536 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.8430848121643066, loss=2.6690845489501953
I0307 10:10:50.187736 140286709720832 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.3199141025543213, loss=2.5842487812042236
I0307 10:11:29.481242 140286718113536 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.2779786586761475, loss=2.561218500137329
I0307 10:12:09.021322 140286709720832 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.6819357872009277, loss=2.552403211593628
I0307 10:12:48.382505 140286718113536 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.2780838012695312, loss=2.5661768913269043
I0307 10:12:59.655871 140441807221952 spec.py:321] Evaluating on the training split.
I0307 10:13:11.783812 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 10:13:29.663954 140441807221952 spec.py:349] Evaluating on the test split.
I0307 10:13:31.481569 140441807221952 submission_runner.py:469] Time since start: 33146.99s, 	Step: 79430, 	{'train/accuracy': 0.7155014276504517, 'train/loss': 1.2211562395095825, 'validation/accuracy': 0.6422399878501892, 'validation/loss': 1.5530271530151367, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.220170736312866, 'test/num_examples': 10000, 'score': 30654.48482823372, 'total_duration': 33146.98575806618, 'accumulated_submission_time': 30654.48482823372, 'accumulated_eval_time': 2476.4360489845276, 'accumulated_logging_time': 7.548407077789307}
I0307 10:13:31.603423 140286709720832 logging_writer.py:48] [79430] accumulated_eval_time=2476.44, accumulated_logging_time=7.54841, accumulated_submission_time=30654.5, global_step=79430, preemption_count=0, score=30654.5, test/accuracy=0.5179, test/loss=2.22017, test/num_examples=10000, total_duration=33147, train/accuracy=0.715501, train/loss=1.22116, validation/accuracy=0.64224, validation/loss=1.55303, validation/num_examples=50000
I0307 10:13:59.576704 140286718113536 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.0624611377716064, loss=2.636491537094116
I0307 10:14:39.035491 140286709720832 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.19677996635437, loss=2.625356674194336
I0307 10:15:18.567567 140286718113536 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.421912908554077, loss=2.6635475158691406
I0307 10:15:58.921557 140286709720832 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.6452324390411377, loss=2.5206024646759033
I0307 10:16:38.736653 140286718113536 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.161217451095581, loss=2.6274633407592773
I0307 10:17:17.912215 140286709720832 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.262291431427002, loss=2.6137640476226807
I0307 10:18:42.914915 140286718113536 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.377269983291626, loss=2.536052703857422
I0307 10:19:22.379035 140286709720832 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.234337568283081, loss=2.609468698501587
I0307 10:20:02.154997 140286718113536 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.996784210205078, loss=2.579347610473633
I0307 10:20:41.788966 140286709720832 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.675316095352173, loss=2.5932457447052
I0307 10:21:21.258945 140286718113536 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.7341692447662354, loss=2.599907636642456
I0307 10:22:00.591936 140286709720832 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.3809103965759277, loss=2.652287721633911
I0307 10:22:01.749032 140441807221952 spec.py:321] Evaluating on the training split.
I0307 10:22:14.140083 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 10:22:33.509820 140441807221952 spec.py:349] Evaluating on the test split.
I0307 10:22:35.320024 140441807221952 submission_runner.py:469] Time since start: 33690.82s, 	Step: 80604, 	{'train/accuracy': 0.7147042155265808, 'train/loss': 1.2504810094833374, 'validation/accuracy': 0.6301599740982056, 'validation/loss': 1.6160557270050049, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2704062461853027, 'test/num_examples': 10000, 'score': 31164.466303110123, 'total_duration': 33690.82419300079, 'accumulated_submission_time': 31164.466303110123, 'accumulated_eval_time': 2510.006842136383, 'accumulated_logging_time': 7.69445013999939}
I0307 10:22:35.442950 140286718113536 logging_writer.py:48] [80604] accumulated_eval_time=2510.01, accumulated_logging_time=7.69445, accumulated_submission_time=31164.5, global_step=80604, preemption_count=0, score=31164.5, test/accuracy=0.5085, test/loss=2.27041, test/num_examples=10000, total_duration=33690.8, train/accuracy=0.714704, train/loss=1.25048, validation/accuracy=0.63016, validation/loss=1.61606, validation/num_examples=50000
I0307 10:23:14.073780 140286709720832 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.3118069171905518, loss=2.664207935333252
I0307 10:23:53.674596 140286718113536 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.3718833923339844, loss=2.610684633255005
I0307 10:24:34.141226 140286709720832 logging_writer.py:48] [80900] global_step=80900, grad_norm=4.104185104370117, loss=2.616050958633423
I0307 10:25:14.061728 140286718113536 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.463991641998291, loss=2.6794700622558594
I0307 10:25:53.967731 140286709720832 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.0638086795806885, loss=2.562973737716675
I0307 10:26:33.786607 140286718113536 logging_writer.py:48] [81200] global_step=81200, grad_norm=3.1579911708831787, loss=2.560370445251465
I0307 10:27:13.412146 140286709720832 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.673337936401367, loss=2.6347310543060303
I0307 10:27:52.905914 140286718113536 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.859816551208496, loss=2.6175169944763184
I0307 10:28:32.821177 140286709720832 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.453634738922119, loss=2.6258182525634766
I0307 10:29:12.569403 140286718113536 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.94953989982605, loss=2.550769567489624
I0307 10:29:52.983107 140286709720832 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.6044118404388428, loss=2.5415642261505127
I0307 10:30:32.636635 140286718113536 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.3473501205444336, loss=2.6117396354675293
I0307 10:31:05.603476 140441807221952 spec.py:321] Evaluating on the training split.
I0307 10:31:17.877028 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 10:31:36.762385 140441807221952 spec.py:349] Evaluating on the test split.
I0307 10:31:38.586087 140441807221952 submission_runner.py:469] Time since start: 34234.09s, 	Step: 81884, 	{'train/accuracy': 0.7067123651504517, 'train/loss': 1.2666953802108765, 'validation/accuracy': 0.641979992389679, 'validation/loss': 1.565320372581482, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.241347551345825, 'test/num_examples': 10000, 'score': 31674.444312810898, 'total_duration': 34234.090277671814, 'accumulated_submission_time': 31674.444312810898, 'accumulated_eval_time': 2542.989287853241, 'accumulated_logging_time': 7.848722457885742}
I0307 10:31:38.715523 140286709720832 logging_writer.py:48] [81884] accumulated_eval_time=2542.99, accumulated_logging_time=7.84872, accumulated_submission_time=31674.4, global_step=81884, preemption_count=0, score=31674.4, test/accuracy=0.5139, test/loss=2.24135, test/num_examples=10000, total_duration=34234.1, train/accuracy=0.706712, train/loss=1.2667, validation/accuracy=0.64198, validation/loss=1.56532, validation/num_examples=50000
I0307 10:31:45.338289 140286718113536 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.4637577533721924, loss=2.686378002166748
I0307 10:32:24.633775 140286709720832 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.184422016143799, loss=2.5968570709228516
I0307 10:33:04.653131 140286718113536 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.140732765197754, loss=2.5009944438934326
I0307 10:33:45.495163 140286709720832 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.450791120529175, loss=2.569599151611328
I0307 10:34:24.788882 140286718113536 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.492006540298462, loss=2.6421711444854736
I0307 10:35:03.704140 140286709720832 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.3367953300476074, loss=2.6381726264953613
I0307 10:35:42.959648 140286718113536 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.1311094760894775, loss=2.63779616355896
I0307 10:36:22.464610 140286709720832 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.3242664337158203, loss=2.532433032989502
I0307 10:37:01.663204 140286718113536 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.6319518089294434, loss=2.6513373851776123
I0307 10:37:40.261431 140286709720832 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.4626851081848145, loss=2.587252616882324
I0307 10:38:21.993191 140286718113536 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.271152973175049, loss=2.5870754718780518
I0307 10:39:03.970685 140286709720832 logging_writer.py:48] [83000] global_step=83000, grad_norm=3.2792158126831055, loss=2.581317901611328
I0307 10:39:43.154826 140286718113536 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.1346044540405273, loss=2.720475196838379
I0307 10:40:08.927428 140441807221952 spec.py:321] Evaluating on the training split.
I0307 10:40:22.562422 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 10:40:49.235337 140441807221952 spec.py:349] Evaluating on the test split.
I0307 10:40:51.045327 140441807221952 submission_runner.py:469] Time since start: 34786.55s, 	Step: 83166, 	{'train/accuracy': 0.6972456574440002, 'train/loss': 1.319401502609253, 'validation/accuracy': 0.64274001121521, 'validation/loss': 1.566603183746338, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.237445116043091, 'test/num_examples': 10000, 'score': 32184.478353500366, 'total_duration': 34786.54955625534, 'accumulated_submission_time': 32184.478353500366, 'accumulated_eval_time': 2585.1070516109467, 'accumulated_logging_time': 8.006536960601807}
I0307 10:40:51.189645 140286709720832 logging_writer.py:48] [83166] accumulated_eval_time=2585.11, accumulated_logging_time=8.00654, accumulated_submission_time=32184.5, global_step=83166, preemption_count=0, score=32184.5, test/accuracy=0.5158, test/loss=2.23745, test/num_examples=10000, total_duration=34786.5, train/accuracy=0.697246, train/loss=1.3194, validation/accuracy=0.64274, validation/loss=1.5666, validation/num_examples=50000
I0307 10:41:05.276835 140286718113536 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.4057860374450684, loss=2.6953253746032715
I0307 10:41:46.190470 140286709720832 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.43149995803833, loss=2.618316173553467
I0307 10:42:54.033174 140286718113536 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.8307290077209473, loss=2.539133310317993
I0307 10:43:36.967145 140286709720832 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.178420066833496, loss=2.524634599685669
I0307 10:44:18.463297 140286718113536 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.7100982666015625, loss=2.634946584701538
I0307 10:44:58.224474 140286709720832 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.6700522899627686, loss=2.577150344848633
I0307 10:45:37.679386 140286718113536 logging_writer.py:48] [83800] global_step=83800, grad_norm=3.5287721157073975, loss=2.561641216278076
I0307 10:47:00.729600 140286709720832 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.128490447998047, loss=2.5155200958251953
I0307 10:47:39.907795 140286718113536 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.492671012878418, loss=2.6210925579071045
I0307 10:48:19.571702 140286709720832 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.7654972076416016, loss=2.632507562637329
I0307 10:48:59.809331 140286718113536 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.274437427520752, loss=2.6049275398254395
I0307 10:49:21.157682 140441807221952 spec.py:321] Evaluating on the training split.
I0307 10:49:34.126865 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 10:49:54.178462 140441807221952 spec.py:349] Evaluating on the test split.
I0307 10:49:55.981559 140441807221952 submission_runner.py:469] Time since start: 35331.49s, 	Step: 84254, 	{'train/accuracy': 0.6948142647743225, 'train/loss': 1.307032823562622, 'validation/accuracy': 0.645039975643158, 'validation/loss': 1.5362550020217896, 'validation/num_examples': 50000, 'test/accuracy': 0.527400016784668, 'test/loss': 2.1915056705474854, 'test/num_examples': 10000, 'score': 32694.28601527214, 'total_duration': 35331.485731840134, 'accumulated_submission_time': 32694.28601527214, 'accumulated_eval_time': 2619.9307358264923, 'accumulated_logging_time': 8.1838538646698}
I0307 10:49:56.073804 140286709720832 logging_writer.py:48] [84254] accumulated_eval_time=2619.93, accumulated_logging_time=8.18385, accumulated_submission_time=32694.3, global_step=84254, preemption_count=0, score=32694.3, test/accuracy=0.5274, test/loss=2.19151, test/num_examples=10000, total_duration=35331.5, train/accuracy=0.694814, train/loss=1.30703, validation/accuracy=0.64504, validation/loss=1.53626, validation/num_examples=50000
I0307 10:50:24.221287 140286718113536 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.178375720977783, loss=2.5528812408447266
I0307 10:51:04.407872 140286709720832 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.92868971824646, loss=2.767442464828491
I0307 10:51:44.618911 140286718113536 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.280050277709961, loss=2.594170093536377
I0307 10:52:24.546554 140286709720832 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.403607130050659, loss=2.6467514038085938
I0307 10:53:04.287015 140286718113536 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.8305118083953857, loss=2.581695556640625
I0307 10:53:44.101793 140286709720832 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.5918397903442383, loss=2.5727224349975586
I0307 10:54:24.274475 140286718113536 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.1130316257476807, loss=2.5856072902679443
I0307 10:55:03.667792 140286709720832 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.238208293914795, loss=2.558021068572998
I0307 10:55:44.117597 140286718113536 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.3253159523010254, loss=2.580780267715454
I0307 10:56:23.795374 140286709720832 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.084153413772583, loss=2.5944817066192627
I0307 10:57:07.015415 140286718113536 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.9568374156951904, loss=2.5713915824890137
I0307 10:57:53.063608 140286709720832 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.8359880447387695, loss=2.567218065261841
I0307 10:58:26.086165 140441807221952 spec.py:321] Evaluating on the training split.
I0307 10:58:39.974366 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 10:58:57.135825 140441807221952 spec.py:349] Evaluating on the test split.
I0307 10:58:58.943375 140441807221952 submission_runner.py:469] Time since start: 35874.45s, 	Step: 85472, 	{'train/accuracy': 0.6954918503761292, 'train/loss': 1.322993516921997, 'validation/accuracy': 0.6441999673843384, 'validation/loss': 1.5568430423736572, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.2075045108795166, 'test/num_examples': 10000, 'score': 33204.14257335663, 'total_duration': 35874.44759464264, 'accumulated_submission_time': 33204.14257335663, 'accumulated_eval_time': 2652.787799358368, 'accumulated_logging_time': 8.293099641799927}
I0307 10:58:59.060417 140286718113536 logging_writer.py:48] [85472] accumulated_eval_time=2652.79, accumulated_logging_time=8.2931, accumulated_submission_time=33204.1, global_step=85472, preemption_count=0, score=33204.1, test/accuracy=0.5223, test/loss=2.2075, test/num_examples=10000, total_duration=35874.4, train/accuracy=0.695492, train/loss=1.32299, validation/accuracy=0.6442, validation/loss=1.55684, validation/num_examples=50000
I0307 10:59:10.716719 140286709720832 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.4708473682403564, loss=2.5898280143737793
I0307 10:59:52.388148 140286718113536 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.669750928878784, loss=2.6016998291015625
I0307 11:00:34.871775 140286709720832 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.5752391815185547, loss=2.672954559326172
I0307 11:01:16.238206 140286718113536 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.2464118003845215, loss=2.511281728744507
I0307 11:01:56.492792 140286709720832 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.368354082107544, loss=2.528675079345703
I0307 11:02:37.451346 140286718113536 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.702810049057007, loss=2.635948419570923
I0307 11:03:19.744375 140286709720832 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.571014165878296, loss=2.5935258865356445
I0307 11:03:59.934371 140286718113536 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.6787145137786865, loss=2.5784316062927246
I0307 11:04:40.266143 140286709720832 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.63432240486145, loss=2.581571340560913
2025-03-07 11:04:44.372890: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:05:19.893992 140286718113536 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.0251963138580322, loss=2.562507390975952
I0307 11:05:59.470223 140286709720832 logging_writer.py:48] [86500] global_step=86500, grad_norm=3.463073253631592, loss=2.6312851905822754
I0307 11:06:39.740718 140286718113536 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.2308051586151123, loss=2.5206360816955566
I0307 11:07:19.748311 140286709720832 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.8325047492980957, loss=2.6014137268066406
I0307 11:07:29.156297 140441807221952 spec.py:321] Evaluating on the training split.
I0307 11:07:41.775864 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 11:08:11.228332 140441807221952 spec.py:349] Evaluating on the test split.
I0307 11:08:13.052994 140441807221952 submission_runner.py:469] Time since start: 36428.56s, 	Step: 86725, 	{'train/accuracy': 0.69925856590271, 'train/loss': 1.2917720079421997, 'validation/accuracy': 0.6479200124740601, 'validation/loss': 1.5204137563705444, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.183321475982666, 'test/num_examples': 10000, 'score': 33714.05351233482, 'total_duration': 36428.55696058273, 'accumulated_submission_time': 33714.05351233482, 'accumulated_eval_time': 2696.684096097946, 'accumulated_logging_time': 8.448325395584106}
I0307 11:08:13.194699 140286718113536 logging_writer.py:48] [86725] accumulated_eval_time=2696.68, accumulated_logging_time=8.44833, accumulated_submission_time=33714.1, global_step=86725, preemption_count=0, score=33714.1, test/accuracy=0.5212, test/loss=2.18332, test/num_examples=10000, total_duration=36428.6, train/accuracy=0.699259, train/loss=1.29177, validation/accuracy=0.64792, validation/loss=1.52041, validation/num_examples=50000
I0307 11:08:46.943409 140286709720832 logging_writer.py:48] [86800] global_step=86800, grad_norm=3.795970916748047, loss=2.5485990047454834
I0307 11:09:38.814073 140286718113536 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.6802093982696533, loss=2.603053092956543
I0307 11:10:18.894056 140286709720832 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.2845005989074707, loss=2.616574764251709
I0307 11:10:58.818981 140286718113536 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.29923677444458, loss=2.6887826919555664
I0307 11:11:45.351231 140286709720832 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.5242373943328857, loss=2.6257123947143555
I0307 11:12:24.101743 140286718113536 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.3685712814331055, loss=2.5853164196014404
I0307 11:13:04.186572 140286709720832 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.723663091659546, loss=2.6045541763305664
I0307 11:13:52.128510 140286718113536 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.007634162902832, loss=2.6155383586883545
I0307 11:14:33.259722 140286709720832 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.7725157737731934, loss=2.633815288543701
I0307 11:15:12.639352 140286718113536 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.277940511703491, loss=2.5276663303375244
I0307 11:15:52.804058 140286709720832 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.9531967639923096, loss=2.529162883758545
I0307 11:16:33.053193 140286718113536 logging_writer.py:48] [87900] global_step=87900, grad_norm=3.665489673614502, loss=2.580986738204956
I0307 11:16:43.105975 140441807221952 spec.py:321] Evaluating on the training split.
I0307 11:16:55.968938 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 11:17:22.980628 140441807221952 spec.py:349] Evaluating on the test split.
I0307 11:17:24.774831 140441807221952 submission_runner.py:469] Time since start: 36980.28s, 	Step: 87926, 	{'train/accuracy': 0.7059550285339355, 'train/loss': 1.2714194059371948, 'validation/accuracy': 0.6505999565124512, 'validation/loss': 1.5271527767181396, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.1504061222076416, 'test/num_examples': 10000, 'score': 34223.7682595253, 'total_duration': 36980.279029369354, 'accumulated_submission_time': 34223.7682595253, 'accumulated_eval_time': 2738.352782011032, 'accumulated_logging_time': 8.64872932434082}
I0307 11:17:24.868762 140286709720832 logging_writer.py:48] [87926] accumulated_eval_time=2738.35, accumulated_logging_time=8.64873, accumulated_submission_time=34223.8, global_step=87926, preemption_count=0, score=34223.8, test/accuracy=0.5265, test/loss=2.15041, test/num_examples=10000, total_duration=36980.3, train/accuracy=0.705955, train/loss=1.27142, validation/accuracy=0.6506, validation/loss=1.52715, validation/num_examples=50000
I0307 11:17:54.616539 140286718113536 logging_writer.py:48] [88000] global_step=88000, grad_norm=3.7406065464019775, loss=2.575774669647217
I0307 11:18:36.292976 140286709720832 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.9756202697753906, loss=2.482447624206543
I0307 11:19:20.534094 140286718113536 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.165390968322754, loss=2.554896116256714
I0307 11:20:03.201259 140286709720832 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.6747653484344482, loss=2.5361721515655518
I0307 11:20:42.449115 140286718113536 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.884979724884033, loss=2.580185890197754
I0307 11:21:21.991296 140286709720832 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.020573616027832, loss=2.5740344524383545
I0307 11:22:02.667360 140286718113536 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.27313756942749, loss=2.5619232654571533
I0307 11:22:43.273080 140286709720832 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.8493213653564453, loss=2.581591844558716
I0307 11:23:23.260904 140286718113536 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.431199312210083, loss=2.6016194820404053
I0307 11:24:02.839266 140286709720832 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.8658523559570312, loss=2.6646196842193604
I0307 11:24:42.319504 140286718113536 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.8657023906707764, loss=2.5807981491088867
I0307 11:25:23.621162 140286709720832 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.7174370288848877, loss=2.5400044918060303
I0307 11:25:55.164133 140441807221952 spec.py:321] Evaluating on the training split.
I0307 11:26:08.417528 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 11:26:30.564399 140441807221952 spec.py:349] Evaluating on the test split.
I0307 11:26:32.369104 140441807221952 submission_runner.py:469] Time since start: 37527.87s, 	Step: 89179, 	{'train/accuracy': 0.6988599896430969, 'train/loss': 1.2979094982147217, 'validation/accuracy': 0.6479799747467041, 'validation/loss': 1.541835069656372, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1743595600128174, 'test/num_examples': 10000, 'score': 34733.876125097275, 'total_duration': 37527.873280763626, 'accumulated_submission_time': 34733.876125097275, 'accumulated_eval_time': 2775.5575659275055, 'accumulated_logging_time': 8.787016868591309}
I0307 11:26:32.495838 140286718113536 logging_writer.py:48] [89179] accumulated_eval_time=2775.56, accumulated_logging_time=8.78702, accumulated_submission_time=34733.9, global_step=89179, preemption_count=0, score=34733.9, test/accuracy=0.5223, test/loss=2.17436, test/num_examples=10000, total_duration=37527.9, train/accuracy=0.69886, train/loss=1.29791, validation/accuracy=0.64798, validation/loss=1.54184, validation/num_examples=50000
I0307 11:26:41.406402 140286709720832 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.5554871559143066, loss=2.580139636993408
I0307 11:27:21.459837 140286718113536 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.6941287517547607, loss=2.564239025115967
I0307 11:28:01.808931 140286709720832 logging_writer.py:48] [89400] global_step=89400, grad_norm=4.020434379577637, loss=2.5764036178588867
I0307 11:28:41.894867 140286718113536 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.320314645767212, loss=2.536681652069092
I0307 11:29:21.913661 140286709720832 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.7489795684814453, loss=2.5884170532226562
I0307 11:30:01.916823 140286718113536 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.4187190532684326, loss=2.574803590774536
I0307 11:30:42.061512 140286709720832 logging_writer.py:48] [89800] global_step=89800, grad_norm=3.386206865310669, loss=2.6916277408599854
I0307 11:31:22.492463 140286718113536 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.8584320545196533, loss=2.5360352993011475
I0307 11:32:03.484578 140286709720832 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.3843586444854736, loss=2.6236000061035156
I0307 11:32:46.280944 140286718113536 logging_writer.py:48] [90100] global_step=90100, grad_norm=3.898282527923584, loss=2.5591509342193604
I0307 11:33:26.351052 140286709720832 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.8758392333984375, loss=2.5780787467956543
I0307 11:34:06.506758 140286718113536 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.942171573638916, loss=2.6318724155426025
I0307 11:34:46.311338 140286709720832 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.6372313499450684, loss=2.56862211227417
I0307 11:35:02.509346 140441807221952 spec.py:321] Evaluating on the training split.
I0307 11:35:16.386908 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 11:35:33.361657 140441807221952 spec.py:349] Evaluating on the test split.
I0307 11:35:35.174864 140441807221952 submission_runner.py:469] Time since start: 38070.68s, 	Step: 90438, 	{'train/accuracy': 0.7082669138908386, 'train/loss': 1.253284215927124, 'validation/accuracy': 0.6511200070381165, 'validation/loss': 1.5215113162994385, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.2034687995910645, 'test/num_examples': 10000, 'score': 35243.666279792786, 'total_duration': 38070.67905211449, 'accumulated_submission_time': 35243.666279792786, 'accumulated_eval_time': 2808.2229022979736, 'accumulated_logging_time': 8.992034196853638}
I0307 11:35:35.287034 140286718113536 logging_writer.py:48] [90438] accumulated_eval_time=2808.22, accumulated_logging_time=8.99203, accumulated_submission_time=35243.7, global_step=90438, preemption_count=0, score=35243.7, test/accuracy=0.5201, test/loss=2.20347, test/num_examples=10000, total_duration=38070.7, train/accuracy=0.708267, train/loss=1.25328, validation/accuracy=0.65112, validation/loss=1.52151, validation/num_examples=50000
I0307 11:36:00.388884 140286709720832 logging_writer.py:48] [90500] global_step=90500, grad_norm=3.3500301837921143, loss=2.5605006217956543
I0307 11:36:40.252000 140286718113536 logging_writer.py:48] [90600] global_step=90600, grad_norm=3.265011787414551, loss=2.5258991718292236
I0307 11:37:24.522021 140286709720832 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.623887300491333, loss=2.579664707183838
I0307 11:38:09.614013 140286718113536 logging_writer.py:48] [90800] global_step=90800, grad_norm=3.604186773300171, loss=2.4708809852600098
I0307 11:38:55.329370 140286709720832 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.3591291904449463, loss=2.628328800201416
I0307 11:39:42.239996 140286718113536 logging_writer.py:48] [91000] global_step=91000, grad_norm=3.562410354614258, loss=2.5265238285064697
I0307 11:40:46.703215 140286709720832 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.78159761428833, loss=2.5360116958618164
I0307 11:41:35.941415 140286718113536 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.831240653991699, loss=2.6594417095184326
I0307 11:42:26.160170 140286709720832 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.333069324493408, loss=2.4595870971679688
I0307 11:43:21.075913 140286718113536 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.9631705284118652, loss=2.4812164306640625
I0307 11:44:05.449245 140441807221952 spec.py:321] Evaluating on the training split.
I0307 11:44:19.210398 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 11:44:39.484286 140441807221952 spec.py:349] Evaluating on the test split.
I0307 11:44:41.288619 140441807221952 submission_runner.py:469] Time since start: 38616.79s, 	Step: 91493, 	{'train/accuracy': 0.7190091013908386, 'train/loss': 1.214176058769226, 'validation/accuracy': 0.6571599841117859, 'validation/loss': 1.4979792833328247, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.1469013690948486, 'test/num_examples': 10000, 'score': 35753.68126511574, 'total_duration': 38616.792809963226, 'accumulated_submission_time': 35753.68126511574, 'accumulated_eval_time': 2844.0621078014374, 'accumulated_logging_time': 9.13358187675476}
I0307 11:44:41.450110 140286709720832 logging_writer.py:48] [91493] accumulated_eval_time=2844.06, accumulated_logging_time=9.13358, accumulated_submission_time=35753.7, global_step=91493, preemption_count=0, score=35753.7, test/accuracy=0.5315, test/loss=2.1469, test/num_examples=10000, total_duration=38616.8, train/accuracy=0.719009, train/loss=1.21418, validation/accuracy=0.65716, validation/loss=1.49798, validation/num_examples=50000
I0307 11:44:44.785097 140286718113536 logging_writer.py:48] [91500] global_step=91500, grad_norm=3.675497055053711, loss=2.608630418777466
I0307 11:45:25.651144 140286709720832 logging_writer.py:48] [91600] global_step=91600, grad_norm=3.8918774127960205, loss=2.568403959274292
I0307 11:46:19.856997 140286718113536 logging_writer.py:48] [91700] global_step=91700, grad_norm=3.7761807441711426, loss=2.5432567596435547
I0307 11:47:33.177616 140286709720832 logging_writer.py:48] [91800] global_step=91800, grad_norm=3.5598833560943604, loss=2.5313053131103516
I0307 11:48:24.556507 140286718113536 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.170169830322266, loss=2.5942816734313965
I0307 11:49:16.858747 140286709720832 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.4955945014953613, loss=2.571556568145752
I0307 11:50:13.081981 140286718113536 logging_writer.py:48] [92100] global_step=92100, grad_norm=3.288335084915161, loss=2.5610618591308594
I0307 11:50:59.323617 140286709720832 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.4200329780578613, loss=2.4941959381103516
I0307 11:51:38.985670 140286718113536 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.63598895072937, loss=2.596750259399414
I0307 11:52:18.721130 140286709720832 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.8835291862487793, loss=2.5907492637634277
I0307 11:53:02.207961 140286718113536 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.858950614929199, loss=2.5618667602539062
I0307 11:53:11.331072 140441807221952 spec.py:321] Evaluating on the training split.
I0307 11:53:24.710058 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 11:53:41.767570 140441807221952 spec.py:349] Evaluating on the test split.
I0307 11:53:43.577522 140441807221952 submission_runner.py:469] Time since start: 39159.08s, 	Step: 92522, 	{'train/accuracy': 0.7293526530265808, 'train/loss': 1.141593098640442, 'validation/accuracy': 0.6570799946784973, 'validation/loss': 1.4659343957901, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.122898817062378, 'test/num_examples': 10000, 'score': 36263.41307759285, 'total_duration': 39159.08170723915, 'accumulated_submission_time': 36263.41307759285, 'accumulated_eval_time': 2876.30837726593, 'accumulated_logging_time': 9.328445672988892}
I0307 11:53:43.688552 140286709720832 logging_writer.py:48] [92522] accumulated_eval_time=2876.31, accumulated_logging_time=9.32845, accumulated_submission_time=36263.4, global_step=92522, preemption_count=0, score=36263.4, test/accuracy=0.5362, test/loss=2.1229, test/num_examples=10000, total_duration=39159.1, train/accuracy=0.729353, train/loss=1.14159, validation/accuracy=0.65708, validation/loss=1.46593, validation/num_examples=50000
I0307 11:54:15.293376 140286718113536 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.205810308456421, loss=2.566986560821533
I0307 11:54:55.443529 140286709720832 logging_writer.py:48] [92700] global_step=92700, grad_norm=3.3313937187194824, loss=2.55037784576416
I0307 11:55:35.742995 140286718113536 logging_writer.py:48] [92800] global_step=92800, grad_norm=3.4674527645111084, loss=2.603132724761963
I0307 11:56:17.924787 140286709720832 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.57987642288208, loss=2.6625661849975586
I0307 11:57:08.471257 140286718113536 logging_writer.py:48] [93000] global_step=93000, grad_norm=3.914634943008423, loss=2.565251588821411
I0307 11:58:20.691066 140286709720832 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.211454391479492, loss=2.6245412826538086
I0307 11:59:20.890856 140286718113536 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.7064225673675537, loss=2.5795624256134033
I0307 12:00:03.350226 140286709720832 logging_writer.py:48] [93300] global_step=93300, grad_norm=3.7431581020355225, loss=2.613632917404175
I0307 12:00:44.263087 140286718113536 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.6788456439971924, loss=2.505363941192627
I0307 12:01:27.174913 140286709720832 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.112745761871338, loss=2.6084086894989014
I0307 12:02:09.721282 140286718113536 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.1649930477142334, loss=2.4967291355133057
I0307 12:02:13.778175 140441807221952 spec.py:321] Evaluating on the training split.
I0307 12:02:27.877706 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 12:02:43.702288 140441807221952 spec.py:349] Evaluating on the test split.
I0307 12:02:45.534943 140441807221952 submission_runner.py:469] Time since start: 39701.04s, 	Step: 93610, 	{'train/accuracy': 0.7206034660339355, 'train/loss': 1.2120383977890015, 'validation/accuracy': 0.657759964466095, 'validation/loss': 1.4817472696304321, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.1682701110839844, 'test/num_examples': 10000, 'score': 36773.33986592293, 'total_duration': 39701.03913116455, 'accumulated_submission_time': 36773.33986592293, 'accumulated_eval_time': 2908.064968109131, 'accumulated_logging_time': 9.480426549911499}
I0307 12:02:45.698441 140286709720832 logging_writer.py:48] [93610] accumulated_eval_time=2908.06, accumulated_logging_time=9.48043, accumulated_submission_time=36773.3, global_step=93610, preemption_count=0, score=36773.3, test/accuracy=0.5288, test/loss=2.16827, test/num_examples=10000, total_duration=39701, train/accuracy=0.720603, train/loss=1.21204, validation/accuracy=0.65776, validation/loss=1.48175, validation/num_examples=50000
I0307 12:03:30.438607 140286718113536 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.487128973007202, loss=2.51590633392334
I0307 12:04:19.251446 140286709720832 logging_writer.py:48] [93800] global_step=93800, grad_norm=3.85225248336792, loss=2.500596761703491
I0307 12:05:03.976572 140286718113536 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.014136791229248, loss=2.5284159183502197
I0307 12:05:47.856409 140286709720832 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.8060965538024902, loss=2.563755989074707
I0307 12:06:45.062936 140286718113536 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.408665418624878, loss=2.4947173595428467
I0307 12:07:27.968918 140286709720832 logging_writer.py:48] [94200] global_step=94200, grad_norm=3.688903331756592, loss=2.5429749488830566
I0307 12:08:22.570782 140286718113536 logging_writer.py:48] [94300] global_step=94300, grad_norm=3.7125766277313232, loss=2.543978691101074
I0307 12:09:14.043164 140286709720832 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.090388774871826, loss=2.5054397583007812
I0307 12:09:58.409894 140286718113536 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.816002130508423, loss=2.499011993408203
I0307 12:10:38.544714 140286709720832 logging_writer.py:48] [94600] global_step=94600, grad_norm=3.803877830505371, loss=2.5594489574432373
I0307 12:11:15.575805 140441807221952 spec.py:321] Evaluating on the training split.
I0307 12:11:29.043998 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 12:11:42.538936 140441807221952 spec.py:349] Evaluating on the test split.
I0307 12:11:44.331007 140441807221952 submission_runner.py:469] Time since start: 40239.84s, 	Step: 94692, 	{'train/accuracy': 0.7155811190605164, 'train/loss': 1.202919363975525, 'validation/accuracy': 0.6636199951171875, 'validation/loss': 1.4502438306808472, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.110671043395996, 'test/num_examples': 10000, 'score': 37283.05540347099, 'total_duration': 40239.83520245552, 'accumulated_submission_time': 37283.05540347099, 'accumulated_eval_time': 2936.82000041008, 'accumulated_logging_time': 9.683889627456665}
I0307 12:11:44.436614 140286718113536 logging_writer.py:48] [94692] accumulated_eval_time=2936.82, accumulated_logging_time=9.68389, accumulated_submission_time=37283.1, global_step=94692, preemption_count=0, score=37283.1, test/accuracy=0.5337, test/loss=2.11067, test/num_examples=10000, total_duration=40239.8, train/accuracy=0.715581, train/loss=1.20292, validation/accuracy=0.66362, validation/loss=1.45024, validation/num_examples=50000
I0307 12:11:47.924502 140286709720832 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.590766668319702, loss=2.4805474281311035
I0307 12:12:28.356257 140286718113536 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.336246967315674, loss=2.4828195571899414
I0307 12:13:19.700989 140286709720832 logging_writer.py:48] [94900] global_step=94900, grad_norm=3.8394312858581543, loss=2.4534454345703125
I0307 12:14:11.687453 140286718113536 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.5408518314361572, loss=2.4705464839935303
I0307 12:15:29.205937 140286709720832 logging_writer.py:48] [95100] global_step=95100, grad_norm=3.488234758377075, loss=2.482811689376831
I0307 12:16:18.981997 140286718113536 logging_writer.py:48] [95200] global_step=95200, grad_norm=3.441904306411743, loss=2.5910961627960205
I0307 12:16:59.249730 140286709720832 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.320147752761841, loss=2.489762306213379
I0307 12:17:39.359771 140286718113536 logging_writer.py:48] [95400] global_step=95400, grad_norm=3.6190834045410156, loss=2.591524124145508
I0307 12:18:19.441926 140286709720832 logging_writer.py:48] [95500] global_step=95500, grad_norm=3.433385133743286, loss=2.557003974914551
I0307 12:18:59.537606 140286718113536 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.150090217590332, loss=2.5080466270446777
I0307 12:19:53.858184 140286709720832 logging_writer.py:48] [95700] global_step=95700, grad_norm=3.345292806625366, loss=2.372744560241699
I0307 12:20:14.364384 140441807221952 spec.py:321] Evaluating on the training split.
I0307 12:20:27.877582 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 12:20:43.099176 140441807221952 spec.py:349] Evaluating on the test split.
I0307 12:20:44.913644 140441807221952 submission_runner.py:469] Time since start: 40780.42s, 	Step: 95736, 	{'train/accuracy': 0.7122528553009033, 'train/loss': 1.2573579549789429, 'validation/accuracy': 0.6598799824714661, 'validation/loss': 1.5021029710769653, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.1572322845458984, 'test/num_examples': 10000, 'score': 37792.80916953087, 'total_duration': 40780.41782069206, 'accumulated_submission_time': 37792.80916953087, 'accumulated_eval_time': 2967.3690683841705, 'accumulated_logging_time': 9.848324537277222}
I0307 12:20:45.040801 140286718113536 logging_writer.py:48] [95736] accumulated_eval_time=2967.37, accumulated_logging_time=9.84832, accumulated_submission_time=37792.8, global_step=95736, preemption_count=0, score=37792.8, test/accuracy=0.5339, test/loss=2.15723, test/num_examples=10000, total_duration=40780.4, train/accuracy=0.712253, train/loss=1.25736, validation/accuracy=0.65988, validation/loss=1.5021, validation/num_examples=50000
I0307 12:21:15.273752 140286709720832 logging_writer.py:48] [95800] global_step=95800, grad_norm=3.511699914932251, loss=2.602182626724243
I0307 12:22:03.927314 140286718113536 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.8552305698394775, loss=2.5522446632385254
I0307 12:22:46.395446 140286709720832 logging_writer.py:48] [96000] global_step=96000, grad_norm=3.787674903869629, loss=2.6242504119873047
I0307 12:23:31.125250 140286718113536 logging_writer.py:48] [96100] global_step=96100, grad_norm=3.8132097721099854, loss=2.425320625305176
I0307 12:24:26.259920 140286709720832 logging_writer.py:48] [96200] global_step=96200, grad_norm=3.747638463973999, loss=2.575368881225586
I0307 12:25:18.564024 140286718113536 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.375594615936279, loss=2.587869167327881
I0307 12:26:03.703499 140286709720832 logging_writer.py:48] [96400] global_step=96400, grad_norm=3.938490390777588, loss=2.6412506103515625
I0307 12:26:46.967802 140286718113536 logging_writer.py:48] [96500] global_step=96500, grad_norm=3.7766025066375732, loss=2.5609145164489746
I0307 12:28:00.180107 140286709720832 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.5949671268463135, loss=2.513145923614502
I0307 12:29:15.114259 140441807221952 spec.py:321] Evaluating on the training split.
I0307 12:29:28.942714 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 12:29:51.349348 140441807221952 spec.py:349] Evaluating on the test split.
I0307 12:29:53.117595 140441807221952 submission_runner.py:469] Time since start: 41328.62s, 	Step: 96688, 	{'train/accuracy': 0.7226163744926453, 'train/loss': 1.1980310678482056, 'validation/accuracy': 0.6561599969863892, 'validation/loss': 1.5003060102462769, 'validation/num_examples': 50000, 'test/accuracy': 0.5284000039100647, 'test/loss': 2.1565113067626953, 'test/num_examples': 10000, 'score': 38302.71747303009, 'total_duration': 41328.621789455414, 'accumulated_submission_time': 38302.71747303009, 'accumulated_eval_time': 3005.3722336292267, 'accumulated_logging_time': 10.034788370132446}
I0307 12:29:53.241170 140286718113536 logging_writer.py:48] [96688] accumulated_eval_time=3005.37, accumulated_logging_time=10.0348, accumulated_submission_time=38302.7, global_step=96688, preemption_count=0, score=38302.7, test/accuracy=0.5284, test/loss=2.15651, test/num_examples=10000, total_duration=41328.6, train/accuracy=0.722616, train/loss=1.19803, validation/accuracy=0.65616, validation/loss=1.50031, validation/num_examples=50000
I0307 12:29:58.495456 140286709720832 logging_writer.py:48] [96700] global_step=96700, grad_norm=3.614532232284546, loss=2.544752836227417
I0307 12:30:52.439192 140286718113536 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.3228325843811035, loss=2.5772345066070557
I0307 12:31:48.191037 140286709720832 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.670851230621338, loss=2.541390895843506
I0307 12:32:30.399032 140286718113536 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.4902589321136475, loss=2.5558903217315674
I0307 12:33:10.978146 140286709720832 logging_writer.py:48] [97100] global_step=97100, grad_norm=4.41196346282959, loss=2.6852829456329346
I0307 12:33:53.047505 140286718113536 logging_writer.py:48] [97200] global_step=97200, grad_norm=3.6978769302368164, loss=2.525090217590332
I0307 12:34:37.128414 140286709720832 logging_writer.py:48] [97300] global_step=97300, grad_norm=3.9007174968719482, loss=2.507672071456909
I0307 12:35:50.774761 140286718113536 logging_writer.py:48] [97400] global_step=97400, grad_norm=3.9023633003234863, loss=2.612973213195801
I0307 12:37:05.770376 140286709720832 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.9012115001678467, loss=2.506962776184082
2025-03-07 12:37:39.998512: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:38:17.145534 140286718113536 logging_writer.py:48] [97600] global_step=97600, grad_norm=3.3283395767211914, loss=2.4959423542022705
I0307 12:38:23.601388 140441807221952 spec.py:321] Evaluating on the training split.
I0307 12:38:36.955091 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 12:39:00.510731 140441807221952 spec.py:349] Evaluating on the test split.
I0307 12:39:02.278455 140441807221952 submission_runner.py:469] Time since start: 41877.78s, 	Step: 97605, 	{'train/accuracy': 0.746113657951355, 'train/loss': 1.1029852628707886, 'validation/accuracy': 0.661579966545105, 'validation/loss': 1.4784252643585205, 'validation/num_examples': 50000, 'test/accuracy': 0.5336000323295593, 'test/loss': 2.1493115425109863, 'test/num_examples': 10000, 'score': 38812.940517663956, 'total_duration': 41877.78264307976, 'accumulated_submission_time': 38812.940517663956, 'accumulated_eval_time': 3044.049122095108, 'accumulated_logging_time': 10.192920923233032}
I0307 12:39:02.390377 140286709720832 logging_writer.py:48] [97605] accumulated_eval_time=3044.05, accumulated_logging_time=10.1929, accumulated_submission_time=38812.9, global_step=97605, preemption_count=0, score=38812.9, test/accuracy=0.5336, test/loss=2.14931, test/num_examples=10000, total_duration=41877.8, train/accuracy=0.746114, train/loss=1.10299, validation/accuracy=0.66158, validation/loss=1.47843, validation/num_examples=50000
I0307 12:39:42.532989 140286718113536 logging_writer.py:48] [97700] global_step=97700, grad_norm=3.415980815887451, loss=2.3874635696411133
I0307 12:40:27.411616 140286709720832 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.7474489212036133, loss=2.5750246047973633
I0307 12:41:36.289191 140286718113536 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.1527886390686035, loss=2.588893413543701
I0307 12:42:33.503234 140286709720832 logging_writer.py:48] [98000] global_step=98000, grad_norm=3.677434206008911, loss=2.6142547130584717
I0307 12:43:48.346105 140286718113536 logging_writer.py:48] [98100] global_step=98100, grad_norm=3.893517017364502, loss=2.510542392730713
I0307 12:44:47.372208 140286709720832 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.3716466426849365, loss=2.561749219894409
I0307 12:46:03.288477 140286718113536 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.400117874145508, loss=2.526385545730591
I0307 12:46:57.556282 140286709720832 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.952348232269287, loss=2.459948778152466
I0307 12:47:32.479205 140441807221952 spec.py:321] Evaluating on the training split.
I0307 12:47:46.291795 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 12:48:12.638784 140441807221952 spec.py:349] Evaluating on the test split.
I0307 12:48:14.364532 140441807221952 submission_runner.py:469] Time since start: 42429.87s, 	Step: 98468, 	{'train/accuracy': 0.7076889276504517, 'train/loss': 1.2911409139633179, 'validation/accuracy': 0.655299961566925, 'validation/loss': 1.529115915298462, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.1901800632476807, 'test/num_examples': 10000, 'score': 39322.892761707306, 'total_duration': 42429.86869621277, 'accumulated_submission_time': 39322.892761707306, 'accumulated_eval_time': 3085.9342501163483, 'accumulated_logging_time': 10.346590995788574}
I0307 12:48:14.490572 140286718113536 logging_writer.py:48] [98468] accumulated_eval_time=3085.93, accumulated_logging_time=10.3466, accumulated_submission_time=39322.9, global_step=98468, preemption_count=0, score=39322.9, test/accuracy=0.5298, test/loss=2.19018, test/num_examples=10000, total_duration=42429.9, train/accuracy=0.707689, train/loss=1.29114, validation/accuracy=0.6553, validation/loss=1.52912, validation/num_examples=50000
I0307 12:48:27.713606 140286709720832 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.180548667907715, loss=2.436365842819214
I0307 12:49:09.992190 140286718113536 logging_writer.py:48] [98600] global_step=98600, grad_norm=3.916778087615967, loss=2.5124380588531494
I0307 12:49:48.622519 140286709720832 logging_writer.py:48] [98700] global_step=98700, grad_norm=3.6898834705352783, loss=2.5929598808288574
I0307 12:50:27.999869 140286718113536 logging_writer.py:48] [98800] global_step=98800, grad_norm=3.7552599906921387, loss=2.490328311920166
2025-03-07 12:50:37.226849: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:51:50.755980 140286709720832 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.176971435546875, loss=2.516885757446289
I0307 12:52:38.173741 140286718113536 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.079334735870361, loss=2.5511746406555176
I0307 12:53:34.417033 140286709720832 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.095897674560547, loss=2.511641502380371
I0307 12:54:37.447755 140286718113536 logging_writer.py:48] [99200] global_step=99200, grad_norm=3.546220064163208, loss=2.6040685176849365
I0307 12:55:29.229297 140286709720832 logging_writer.py:48] [99300] global_step=99300, grad_norm=3.2564635276794434, loss=2.4640133380889893
I0307 12:56:44.902776 140441807221952 spec.py:321] Evaluating on the training split.
I0307 12:56:58.354086 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 12:57:21.577889 140441807221952 spec.py:349] Evaluating on the test split.
I0307 12:57:23.356781 140441807221952 submission_runner.py:469] Time since start: 42978.86s, 	Step: 99366, 	{'train/accuracy': 0.7267019748687744, 'train/loss': 1.1657360792160034, 'validation/accuracy': 0.6635199785232544, 'validation/loss': 1.4535737037658691, 'validation/num_examples': 50000, 'test/accuracy': 0.5396000146865845, 'test/loss': 2.093968629837036, 'test/num_examples': 10000, 'score': 39833.15639948845, 'total_duration': 42978.86096596718, 'accumulated_submission_time': 39833.15639948845, 'accumulated_eval_time': 3124.388088464737, 'accumulated_logging_time': 10.52055811882019}
I0307 12:57:23.472933 140286718113536 logging_writer.py:48] [99366] accumulated_eval_time=3124.39, accumulated_logging_time=10.5206, accumulated_submission_time=39833.2, global_step=99366, preemption_count=0, score=39833.2, test/accuracy=0.5396, test/loss=2.09397, test/num_examples=10000, total_duration=42978.9, train/accuracy=0.726702, train/loss=1.16574, validation/accuracy=0.66352, validation/loss=1.45357, validation/num_examples=50000
I0307 12:57:37.196611 140286709720832 logging_writer.py:48] [99400] global_step=99400, grad_norm=3.569741725921631, loss=2.5586771965026855
I0307 12:58:25.545361 140286718113536 logging_writer.py:48] [99500] global_step=99500, grad_norm=3.6785271167755127, loss=2.5379180908203125
I0307 12:59:19.918317 140286709720832 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.561079263687134, loss=2.4685182571411133
I0307 13:00:19.614849 140286718113536 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.052727699279785, loss=2.486192226409912
I0307 13:02:02.109106 140286709720832 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.6109769344329834, loss=2.5431175231933594
I0307 13:03:10.351708 140286718113536 logging_writer.py:48] [99900] global_step=99900, grad_norm=3.899740695953369, loss=2.5409369468688965
I0307 13:04:03.900557 140286709720832 logging_writer.py:48] [100000] global_step=100000, grad_norm=3.5833377838134766, loss=2.4877681732177734
I0307 13:05:17.251820 140286718113536 logging_writer.py:48] [100100] global_step=100100, grad_norm=3.943021059036255, loss=2.5483646392822266
I0307 13:05:53.638901 140441807221952 spec.py:321] Evaluating on the training split.
I0307 13:06:06.441065 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 13:06:31.451522 140441807221952 spec.py:349] Evaluating on the test split.
I0307 13:06:33.441205 140441807221952 submission_runner.py:469] Time since start: 43528.95s, 	Step: 100134, 	{'train/accuracy': 0.72269606590271, 'train/loss': 1.208102822303772, 'validation/accuracy': 0.6705999970436096, 'validation/loss': 1.442450761795044, 'validation/num_examples': 50000, 'test/accuracy': 0.5446000099182129, 'test/loss': 2.1073343753814697, 'test/num_examples': 10000, 'score': 40343.191679000854, 'total_duration': 43528.9454100132, 'accumulated_submission_time': 40343.191679000854, 'accumulated_eval_time': 3164.1902389526367, 'accumulated_logging_time': 10.683191061019897}
I0307 13:06:33.607574 140286709720832 logging_writer.py:48] [100134] accumulated_eval_time=3164.19, accumulated_logging_time=10.6832, accumulated_submission_time=40343.2, global_step=100134, preemption_count=0, score=40343.2, test/accuracy=0.5446, test/loss=2.10733, test/num_examples=10000, total_duration=43528.9, train/accuracy=0.722696, train/loss=1.2081, validation/accuracy=0.6706, validation/loss=1.44245, validation/num_examples=50000
I0307 13:07:03.951058 140286718113536 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.00852108001709, loss=2.630772352218628
I0307 13:07:52.180494 140286709720832 logging_writer.py:48] [100300] global_step=100300, grad_norm=3.5287163257598877, loss=2.5678398609161377
I0307 13:08:38.860774 140286718113536 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.291173934936523, loss=2.454340934753418
I0307 13:09:28.615424 140286709720832 logging_writer.py:48] [100500] global_step=100500, grad_norm=3.952578544616699, loss=2.6050755977630615
I0307 13:10:25.850608 140286718113536 logging_writer.py:48] [100600] global_step=100600, grad_norm=3.9352614879608154, loss=2.589340925216675
I0307 13:11:38.743123 140286709720832 logging_writer.py:48] [100700] global_step=100700, grad_norm=3.567298173904419, loss=2.4713239669799805
I0307 13:12:52.174744 140286718113536 logging_writer.py:48] [100800] global_step=100800, grad_norm=3.7502388954162598, loss=2.481856107711792
I0307 13:13:41.126652 140286709720832 logging_writer.py:48] [100900] global_step=100900, grad_norm=3.8970437049865723, loss=2.546116590499878
I0307 13:14:40.182180 140286718113536 logging_writer.py:48] [101000] global_step=101000, grad_norm=3.895949363708496, loss=2.4817991256713867
I0307 13:15:04.294168 140441807221952 spec.py:321] Evaluating on the training split.
I0307 13:15:15.767452 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 13:15:36.300774 140441807221952 spec.py:349] Evaluating on the test split.
I0307 13:15:38.070128 140441807221952 submission_runner.py:469] Time since start: 44073.57s, 	Step: 101030, 	{'train/accuracy': 0.7226362824440002, 'train/loss': 1.182054042816162, 'validation/accuracy': 0.6683399677276611, 'validation/loss': 1.4326106309890747, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.083530902862549, 'test/num_examples': 10000, 'score': 40853.705493211746, 'total_duration': 44073.57433414459, 'accumulated_submission_time': 40853.705493211746, 'accumulated_eval_time': 3197.9660420417786, 'accumulated_logging_time': 10.921911716461182}
I0307 13:15:38.190411 140286709720832 logging_writer.py:48] [101030] accumulated_eval_time=3197.97, accumulated_logging_time=10.9219, accumulated_submission_time=40853.7, global_step=101030, preemption_count=0, score=40853.7, test/accuracy=0.5416, test/loss=2.08353, test/num_examples=10000, total_duration=44073.6, train/accuracy=0.722636, train/loss=1.18205, validation/accuracy=0.66834, validation/loss=1.43261, validation/num_examples=50000
I0307 13:16:07.982219 140286718113536 logging_writer.py:48] [101100] global_step=101100, grad_norm=3.8611819744110107, loss=2.53251314163208
I0307 13:17:04.155791 140286709720832 logging_writer.py:48] [101200] global_step=101200, grad_norm=3.9625720977783203, loss=2.4245777130126953
I0307 13:18:19.098231 140286718113536 logging_writer.py:48] [101300] global_step=101300, grad_norm=3.4921088218688965, loss=2.5701651573181152
I0307 13:19:58.157894 140286709720832 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.7046749591827393, loss=2.4733662605285645
I0307 13:20:41.189172 140286718113536 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.064439296722412, loss=2.395308256149292
I0307 13:21:41.515686 140286709720832 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.26972770690918, loss=2.6046559810638428
I0307 13:22:29.276219 140286718113536 logging_writer.py:48] [101700] global_step=101700, grad_norm=3.6493043899536133, loss=2.4748311042785645
I0307 13:23:24.264391 140286709720832 logging_writer.py:48] [101800] global_step=101800, grad_norm=3.7889928817749023, loss=2.572706699371338
I0307 13:24:08.433137 140441807221952 spec.py:321] Evaluating on the training split.
I0307 13:24:20.850884 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 13:24:42.547893 140441807221952 spec.py:349] Evaluating on the test split.
I0307 13:24:44.288064 140441807221952 submission_runner.py:469] Time since start: 44619.79s, 	Step: 101880, 	{'train/accuracy': 0.7289739847183228, 'train/loss': 1.1760597229003906, 'validation/accuracy': 0.6608999967575073, 'validation/loss': 1.4754045009613037, 'validation/num_examples': 50000, 'test/accuracy': 0.5332000255584717, 'test/loss': 2.1427299976348877, 'test/num_examples': 10000, 'score': 41363.79951810837, 'total_duration': 44619.7922410965, 'accumulated_submission_time': 41363.79951810837, 'accumulated_eval_time': 3233.8207857608795, 'accumulated_logging_time': 11.097736358642578}
I0307 13:24:44.443262 140286718113536 logging_writer.py:48] [101880] accumulated_eval_time=3233.82, accumulated_logging_time=11.0977, accumulated_submission_time=41363.8, global_step=101880, preemption_count=0, score=41363.8, test/accuracy=0.5332, test/loss=2.14273, test/num_examples=10000, total_duration=44619.8, train/accuracy=0.728974, train/loss=1.17606, validation/accuracy=0.6609, validation/loss=1.4754, validation/num_examples=50000
I0307 13:24:52.705141 140286709720832 logging_writer.py:48] [101900] global_step=101900, grad_norm=3.5144596099853516, loss=2.4394009113311768
I0307 13:25:33.541833 140286718113536 logging_writer.py:48] [102000] global_step=102000, grad_norm=3.6931254863739014, loss=2.443248987197876
I0307 13:26:28.462955 140286709720832 logging_writer.py:48] [102100] global_step=102100, grad_norm=3.7617335319519043, loss=2.483102798461914
I0307 13:27:45.530738 140286718113536 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.453497409820557, loss=2.4778640270233154
I0307 13:28:25.067047 140286709720832 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.068819522857666, loss=2.5372154712677
I0307 13:29:13.658446 140286718113536 logging_writer.py:48] [102400] global_step=102400, grad_norm=3.9327120780944824, loss=2.5138227939605713
I0307 13:30:10.965263 140286709720832 logging_writer.py:48] [102500] global_step=102500, grad_norm=3.5913662910461426, loss=2.480343818664551
I0307 13:31:04.702043 140286718113536 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.055794715881348, loss=2.60910701751709
I0307 13:31:46.176685 140286709720832 logging_writer.py:48] [102700] global_step=102700, grad_norm=3.6502344608306885, loss=2.4751598834991455
I0307 13:32:31.198086 140286718113536 logging_writer.py:48] [102800] global_step=102800, grad_norm=3.5925490856170654, loss=2.545210599899292
I0307 13:33:14.514771 140441807221952 spec.py:321] Evaluating on the training split.
I0307 13:33:27.306356 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 13:33:45.107615 140441807221952 spec.py:349] Evaluating on the test split.
I0307 13:33:46.876157 140441807221952 submission_runner.py:469] Time since start: 45162.38s, 	Step: 102898, 	{'train/accuracy': 0.7476084232330322, 'train/loss': 1.092307448387146, 'validation/accuracy': 0.6632199883460999, 'validation/loss': 1.4626216888427734, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1078989505767822, 'test/num_examples': 10000, 'score': 41873.72729253769, 'total_duration': 45162.38035964966, 'accumulated_submission_time': 41873.72729253769, 'accumulated_eval_time': 3266.1820130348206, 'accumulated_logging_time': 11.284513711929321}
I0307 13:33:47.007347 140286709720832 logging_writer.py:48] [102898] accumulated_eval_time=3266.18, accumulated_logging_time=11.2845, accumulated_submission_time=41873.7, global_step=102898, preemption_count=0, score=41873.7, test/accuracy=0.5398, test/loss=2.1079, test/num_examples=10000, total_duration=45162.4, train/accuracy=0.747608, train/loss=1.09231, validation/accuracy=0.66322, validation/loss=1.46262, validation/num_examples=50000
I0307 13:33:48.167348 140286718113536 logging_writer.py:48] [102900] global_step=102900, grad_norm=3.919304132461548, loss=2.5026845932006836
I0307 13:34:37.480725 140286709720832 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.183966159820557, loss=2.485306978225708
I0307 13:36:19.311002 140286718113536 logging_writer.py:48] [103100] global_step=103100, grad_norm=3.3036201000213623, loss=2.4488158226013184
I0307 13:38:03.803680 140286709720832 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.204127788543701, loss=2.5013561248779297
I0307 13:39:02.913784 140286718113536 logging_writer.py:48] [103300] global_step=103300, grad_norm=3.7671303749084473, loss=2.5246407985687256
I0307 13:40:10.934503 140286709720832 logging_writer.py:48] [103400] global_step=103400, grad_norm=3.92840838432312, loss=2.4375641345977783
I0307 13:41:02.677431 140286718113536 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.250636577606201, loss=2.635068893432617
I0307 13:41:48.533654 140286709720832 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.133738040924072, loss=2.550870418548584
I0307 13:42:16.920399 140441807221952 spec.py:321] Evaluating on the training split.
I0307 13:42:29.166025 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 13:42:51.453803 140441807221952 spec.py:349] Evaluating on the test split.
I0307 13:42:53.259998 140441807221952 submission_runner.py:469] Time since start: 45708.71s, 	Step: 103658, 	{'train/accuracy': 0.7261240482330322, 'train/loss': 1.1724026203155518, 'validation/accuracy': 0.6685999631881714, 'validation/loss': 1.4457732439041138, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.1093924045562744, 'test/num_examples': 10000, 'score': 42383.5020198822, 'total_duration': 45708.71291232109, 'accumulated_submission_time': 42383.5020198822, 'accumulated_eval_time': 3302.4701614379883, 'accumulated_logging_time': 11.470155954360962}
I0307 13:42:53.392580 140286718113536 logging_writer.py:48] [103658] accumulated_eval_time=3302.47, accumulated_logging_time=11.4702, accumulated_submission_time=42383.5, global_step=103658, preemption_count=0, score=42383.5, test/accuracy=0.5443, test/loss=2.10939, test/num_examples=10000, total_duration=45708.7, train/accuracy=0.726124, train/loss=1.1724, validation/accuracy=0.6686, validation/loss=1.44577, validation/num_examples=50000
I0307 13:43:35.737211 140286709720832 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.087926864624023, loss=2.5120770931243896
I0307 13:44:35.569520 140286718113536 logging_writer.py:48] [103800] global_step=103800, grad_norm=3.9274098873138428, loss=2.4984638690948486
I0307 13:45:30.824917 140286709720832 logging_writer.py:48] [103900] global_step=103900, grad_norm=3.818819284439087, loss=2.5302915573120117
I0307 13:47:04.699508 140286718113536 logging_writer.py:48] [104000] global_step=104000, grad_norm=3.9387176036834717, loss=2.581651449203491
I0307 13:48:01.728422 140286709720832 logging_writer.py:48] [104100] global_step=104100, grad_norm=3.6077277660369873, loss=2.64924693107605
I0307 13:48:59.363350 140286718113536 logging_writer.py:48] [104200] global_step=104200, grad_norm=3.9830942153930664, loss=2.4925637245178223
I0307 13:50:31.943241 140286709720832 logging_writer.py:48] [104300] global_step=104300, grad_norm=3.5434513092041016, loss=2.489591598510742
I0307 13:51:25.254261 140441807221952 spec.py:321] Evaluating on the training split.
I0307 13:51:35.432005 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 13:51:59.526149 140441807221952 spec.py:349] Evaluating on the test split.
I0307 13:52:01.316901 140441807221952 submission_runner.py:469] Time since start: 46256.78s, 	Step: 104326, 	{'train/accuracy': 0.7581911683082581, 'train/loss': 1.048638105392456, 'validation/accuracy': 0.6729399561882019, 'validation/loss': 1.427574634552002, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 2.08412504196167, 'test/num_examples': 10000, 'score': 42895.25928902626, 'total_duration': 46256.783266067505, 'accumulated_submission_time': 42895.25928902626, 'accumulated_eval_time': 3338.494806289673, 'accumulated_logging_time': 11.632615327835083}
I0307 13:52:01.441324 140286718113536 logging_writer.py:48] [104326] accumulated_eval_time=3338.49, accumulated_logging_time=11.6326, accumulated_submission_time=42895.3, global_step=104326, preemption_count=0, score=42895.3, test/accuracy=0.549, test/loss=2.08413, test/num_examples=10000, total_duration=46256.8, train/accuracy=0.758191, train/loss=1.04864, validation/accuracy=0.67294, validation/loss=1.42757, validation/num_examples=50000
I0307 13:53:52.398413 140286709720832 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.317809104919434, loss=2.460630178451538
I0307 13:55:21.561577 140286718113536 logging_writer.py:48] [104500] global_step=104500, grad_norm=3.976640224456787, loss=2.5149383544921875
I0307 13:56:44.899484 140286709720832 logging_writer.py:48] [104600] global_step=104600, grad_norm=3.7062160968780518, loss=2.519233226776123
I0307 13:57:39.278919 140286718113536 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.293257713317871, loss=2.519443988800049
I0307 13:58:22.475297 140286709720832 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.3091044425964355, loss=2.514453172683716
I0307 13:59:40.380287 140286718113536 logging_writer.py:48] [104900] global_step=104900, grad_norm=3.688380479812622, loss=2.4431912899017334
I0307 14:00:31.464822 140441807221952 spec.py:321] Evaluating on the training split.
I0307 14:00:44.090768 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 14:01:09.539175 140441807221952 spec.py:349] Evaluating on the test split.
I0307 14:01:11.261791 140441807221952 submission_runner.py:469] Time since start: 46806.77s, 	Step: 104978, 	{'train/accuracy': 0.7311264276504517, 'train/loss': 1.1743803024291992, 'validation/accuracy': 0.670960009098053, 'validation/loss': 1.4403882026672363, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.1016106605529785, 'test/num_examples': 10000, 'score': 43405.132981061935, 'total_duration': 46806.76611876488, 'accumulated_submission_time': 43405.132981061935, 'accumulated_eval_time': 3378.291741847992, 'accumulated_logging_time': 11.83401107788086}
I0307 14:01:11.303556 140286709720832 logging_writer.py:48] [104978] accumulated_eval_time=3378.29, accumulated_logging_time=11.834, accumulated_submission_time=43405.1, global_step=104978, preemption_count=0, score=43405.1, test/accuracy=0.5434, test/loss=2.10161, test/num_examples=10000, total_duration=46806.8, train/accuracy=0.731126, train/loss=1.17438, validation/accuracy=0.67096, validation/loss=1.44039, validation/num_examples=50000
I0307 14:01:20.686215 140286718113536 logging_writer.py:48] [105000] global_step=105000, grad_norm=3.9462063312530518, loss=2.50527286529541
I0307 14:02:13.241612 140286709720832 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.401298999786377, loss=2.4734461307525635
I0307 14:03:03.178470 140286718113536 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.325628757476807, loss=2.5163230895996094
I0307 14:04:31.888240 140286709720832 logging_writer.py:48] [105300] global_step=105300, grad_norm=3.6506807804107666, loss=2.447749376296997
I0307 14:06:01.445143 140286718113536 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.011455535888672, loss=2.3794028759002686
I0307 14:07:14.744408 140286709720832 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.186805725097656, loss=2.5854973793029785
I0307 14:09:36.510306 140286718113536 logging_writer.py:48] [105600] global_step=105600, grad_norm=3.955913543701172, loss=2.4572646617889404
I0307 14:09:42.271880 140441807221952 spec.py:321] Evaluating on the training split.
I0307 14:09:54.009744 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 14:10:17.744359 140441807221952 spec.py:349] Evaluating on the test split.
I0307 14:10:19.497855 140441807221952 submission_runner.py:469] Time since start: 47355.00s, 	Step: 105605, 	{'train/accuracy': 0.7604432106018066, 'train/loss': 1.0388811826705933, 'validation/accuracy': 0.6730599999427795, 'validation/loss': 1.4137260913848877, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.055131673812866, 'test/num_examples': 10000, 'score': 43915.99709391594, 'total_duration': 47355.00218129158, 'accumulated_submission_time': 43915.99709391594, 'accumulated_eval_time': 3415.5176730155945, 'accumulated_logging_time': 11.910252809524536}
I0307 14:10:19.532100 140286709720832 logging_writer.py:48] [105605] accumulated_eval_time=3415.52, accumulated_logging_time=11.9103, accumulated_submission_time=43916, global_step=105605, preemption_count=0, score=43916, test/accuracy=0.5508, test/loss=2.05513, test/num_examples=10000, total_duration=47355, train/accuracy=0.760443, train/loss=1.03888, validation/accuracy=0.67306, validation/loss=1.41373, validation/num_examples=50000
I0307 14:11:12.672112 140286718113536 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.251558780670166, loss=2.523196220397949
I0307 14:13:09.207631 140286709720832 logging_writer.py:48] [105800] global_step=105800, grad_norm=3.9175031185150146, loss=2.4895339012145996
I0307 14:15:13.601270 140286718113536 logging_writer.py:48] [105900] global_step=105900, grad_norm=3.6753532886505127, loss=2.4715099334716797
I0307 14:16:11.891912 140286709720832 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.104045867919922, loss=2.459239959716797
I0307 14:17:21.509001 140286718113536 logging_writer.py:48] [106100] global_step=106100, grad_norm=3.8144896030426025, loss=2.5292768478393555
I0307 14:18:52.939028 140441807221952 spec.py:321] Evaluating on the training split.
I0307 14:19:03.762014 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 14:19:29.566592 140441807221952 spec.py:349] Evaluating on the test split.
I0307 14:19:31.541447 140441807221952 submission_runner.py:469] Time since start: 47907.05s, 	Step: 106180, 	{'train/accuracy': 0.734793484210968, 'train/loss': 1.148658275604248, 'validation/accuracy': 0.6734600067138672, 'validation/loss': 1.4293200969696045, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.091336488723755, 'test/num_examples': 10000, 'score': 44429.33230805397, 'total_duration': 47907.04577422142, 'accumulated_submission_time': 44429.33230805397, 'accumulated_eval_time': 3454.1200568675995, 'accumulated_logging_time': 11.952481508255005}
I0307 14:19:31.573976 140286709720832 logging_writer.py:48] [106180] accumulated_eval_time=3454.12, accumulated_logging_time=11.9525, accumulated_submission_time=44429.3, global_step=106180, preemption_count=0, score=44429.3, test/accuracy=0.5448, test/loss=2.09134, test/num_examples=10000, total_duration=47907, train/accuracy=0.734793, train/loss=1.14866, validation/accuracy=0.67346, validation/loss=1.42932, validation/num_examples=50000
I0307 14:19:55.168237 140286718113536 logging_writer.py:48] [106200] global_step=106200, grad_norm=3.590832471847534, loss=2.5195391178131104
I0307 14:21:26.106453 140286709720832 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.224474906921387, loss=2.5620570182800293
I0307 14:22:25.276862 140286718113536 logging_writer.py:48] [106400] global_step=106400, grad_norm=3.9392964839935303, loss=2.456505060195923
I0307 14:23:32.482218 140286709720832 logging_writer.py:48] [106500] global_step=106500, grad_norm=3.6297547817230225, loss=2.401087999343872
I0307 14:24:58.573576 140286718113536 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.226434230804443, loss=2.458242893218994
I0307 14:26:19.439312 140286709720832 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.312078475952148, loss=2.4358551502227783
I0307 14:27:36.793688 140286718113536 logging_writer.py:48] [106800] global_step=106800, grad_norm=4.24495267868042, loss=2.4909744262695312
I0307 14:28:01.955726 140441807221952 spec.py:321] Evaluating on the training split.
I0307 14:28:13.420319 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 14:28:37.733061 140441807221952 spec.py:349] Evaluating on the test split.
I0307 14:28:39.583261 140441807221952 submission_runner.py:469] Time since start: 48455.09s, 	Step: 106835, 	{'train/accuracy': 0.7259646058082581, 'train/loss': 1.1890969276428223, 'validation/accuracy': 0.6716200113296509, 'validation/loss': 1.442348837852478, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.103611707687378, 'test/num_examples': 10000, 'score': 44939.625539302826, 'total_duration': 48455.08759522438, 'accumulated_submission_time': 44939.625539302826, 'accumulated_eval_time': 3491.7475571632385, 'accumulated_logging_time': 11.99902868270874}
I0307 14:28:39.640242 140286709720832 logging_writer.py:48] [106835] accumulated_eval_time=3491.75, accumulated_logging_time=11.999, accumulated_submission_time=44939.6, global_step=106835, preemption_count=0, score=44939.6, test/accuracy=0.5443, test/loss=2.10361, test/num_examples=10000, total_duration=48455.1, train/accuracy=0.725965, train/loss=1.1891, validation/accuracy=0.67162, validation/loss=1.44235, validation/num_examples=50000
I0307 14:29:29.351448 140286718113536 logging_writer.py:48] [106900] global_step=106900, grad_norm=3.947744607925415, loss=2.461510181427002
I0307 14:30:46.304157 140286709720832 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.9411067962646484, loss=2.4960105419158936
I0307 14:32:09.398232 140286718113536 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.161674976348877, loss=2.523362159729004
I0307 14:33:55.152988 140286709720832 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.0267014503479, loss=2.5581133365631104
I0307 14:34:39.293920 140286718113536 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.101947784423828, loss=2.486105442047119
I0307 14:36:19.764507 140286709720832 logging_writer.py:48] [107400] global_step=107400, grad_norm=3.838674783706665, loss=2.4664082527160645
I0307 14:37:09.743875 140441807221952 spec.py:321] Evaluating on the training split.
I0307 14:37:21.912957 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 14:37:46.233259 140441807221952 spec.py:349] Evaluating on the test split.
I0307 14:37:48.004433 140441807221952 submission_runner.py:469] Time since start: 49003.51s, 	Step: 107455, 	{'train/accuracy': 0.740254282951355, 'train/loss': 1.113765835762024, 'validation/accuracy': 0.6744599938392639, 'validation/loss': 1.4126404523849487, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.088254690170288, 'test/num_examples': 10000, 'score': 45449.64934015274, 'total_duration': 49003.50874090195, 'accumulated_submission_time': 45449.64934015274, 'accumulated_eval_time': 3530.0080597400665, 'accumulated_logging_time': 12.065387725830078}
I0307 14:37:48.059349 140286718113536 logging_writer.py:48] [107455] accumulated_eval_time=3530.01, accumulated_logging_time=12.0654, accumulated_submission_time=45449.6, global_step=107455, preemption_count=0, score=45449.6, test/accuracy=0.5445, test/loss=2.08825, test/num_examples=10000, total_duration=49003.5, train/accuracy=0.740254, train/loss=1.11377, validation/accuracy=0.67446, validation/loss=1.41264, validation/num_examples=50000
I0307 14:38:10.806389 140286709720832 logging_writer.py:48] [107500] global_step=107500, grad_norm=3.7980282306671143, loss=2.4799957275390625
I0307 14:39:28.165188 140286718113536 logging_writer.py:48] [107600] global_step=107600, grad_norm=3.708989381790161, loss=2.4257781505584717
I0307 14:40:42.790883 140286709720832 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.111051082611084, loss=2.4908719062805176
I0307 14:41:40.261965 140286718113536 logging_writer.py:48] [107800] global_step=107800, grad_norm=3.7978594303131104, loss=2.5042402744293213
I0307 14:43:59.301297 140286709720832 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.36800479888916, loss=2.507736921310425
I0307 14:46:18.639154 140441807221952 spec.py:321] Evaluating on the training split.
I0307 14:46:28.819429 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 14:46:49.395169 140441807221952 spec.py:349] Evaluating on the test split.
I0307 14:46:51.143859 140441807221952 submission_runner.py:469] Time since start: 49546.65s, 	Step: 107958, 	{'train/accuracy': 0.7295519709587097, 'train/loss': 1.1634418964385986, 'validation/accuracy': 0.6712200045585632, 'validation/loss': 1.4275529384613037, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.0764360427856445, 'test/num_examples': 10000, 'score': 45960.163200855255, 'total_duration': 49546.64817047119, 'accumulated_submission_time': 45960.163200855255, 'accumulated_eval_time': 3562.512717962265, 'accumulated_logging_time': 12.128931760787964}
I0307 14:46:51.185065 140286718113536 logging_writer.py:48] [107958] accumulated_eval_time=3562.51, accumulated_logging_time=12.1289, accumulated_submission_time=45960.2, global_step=107958, preemption_count=0, score=45960.2, test/accuracy=0.543, test/loss=2.07644, test/num_examples=10000, total_duration=49546.6, train/accuracy=0.729552, train/loss=1.16344, validation/accuracy=0.67122, validation/loss=1.42755, validation/num_examples=50000
I0307 14:47:13.400503 140286709720832 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.476975440979004, loss=2.496323347091675
I0307 14:49:08.303981 140286718113536 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.068507194519043, loss=2.4450032711029053
I0307 14:51:03.442254 140286709720832 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.0403876304626465, loss=2.48126482963562
I0307 14:53:06.963851 140286718113536 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.21196985244751, loss=2.4786014556884766
I0307 14:54:36.645926 140286709720832 logging_writer.py:48] [108400] global_step=108400, grad_norm=3.926471710205078, loss=2.4749884605407715
I0307 14:55:21.304127 140441807221952 spec.py:321] Evaluating on the training split.
I0307 14:55:33.536174 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 14:56:01.232096 140441807221952 spec.py:349] Evaluating on the test split.
I0307 14:56:02.946134 140441807221952 submission_runner.py:469] Time since start: 50098.45s, 	Step: 108471, 	{'train/accuracy': 0.7596659660339355, 'train/loss': 1.0406839847564697, 'validation/accuracy': 0.6787399649620056, 'validation/loss': 1.395565152168274, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.047342300415039, 'test/num_examples': 10000, 'score': 46470.21558427811, 'total_duration': 50098.450464725494, 'accumulated_submission_time': 46470.21558427811, 'accumulated_eval_time': 3604.154694080353, 'accumulated_logging_time': 12.17865252494812}
I0307 14:56:02.988577 140286718113536 logging_writer.py:48] [108471] accumulated_eval_time=3604.15, accumulated_logging_time=12.1787, accumulated_submission_time=46470.2, global_step=108471, preemption_count=0, score=46470.2, test/accuracy=0.5502, test/loss=2.04734, test/num_examples=10000, total_duration=50098.5, train/accuracy=0.759666, train/loss=1.04068, validation/accuracy=0.67874, validation/loss=1.39557, validation/num_examples=50000
I0307 14:56:27.144207 140286709720832 logging_writer.py:48] [108500] global_step=108500, grad_norm=3.819445848464966, loss=2.4909942150115967
I0307 14:58:23.229802 140286718113536 logging_writer.py:48] [108600] global_step=108600, grad_norm=3.7210536003112793, loss=2.491036891937256
I0307 14:59:41.717598 140286709720832 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.459015369415283, loss=2.4509148597717285
I0307 15:01:14.053680 140286718113536 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.1157917976379395, loss=2.4906857013702393
I0307 15:02:42.785282 140286709720832 logging_writer.py:48] [108900] global_step=108900, grad_norm=3.6671104431152344, loss=2.4708945751190186
I0307 15:04:12.094148 140286718113536 logging_writer.py:48] [109000] global_step=109000, grad_norm=4.211702346801758, loss=2.451978921890259
I0307 15:04:33.268576 140441807221952 spec.py:321] Evaluating on the training split.
I0307 15:04:44.976874 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 15:05:13.125963 140441807221952 spec.py:349] Evaluating on the test split.
I0307 15:05:14.890645 140441807221952 submission_runner.py:469] Time since start: 50650.39s, 	Step: 109025, 	{'train/accuracy': 0.7336973547935486, 'train/loss': 1.1386672258377075, 'validation/accuracy': 0.6717000007629395, 'validation/loss': 1.4257686138153076, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.0792343616485596, 'test/num_examples': 10000, 'score': 46980.4241335392, 'total_duration': 50650.39496302605, 'accumulated_submission_time': 46980.4241335392, 'accumulated_eval_time': 3645.7767181396484, 'accumulated_logging_time': 12.230543613433838}
I0307 15:05:14.928448 140286709720832 logging_writer.py:48] [109025] accumulated_eval_time=3645.78, accumulated_logging_time=12.2305, accumulated_submission_time=46980.4, global_step=109025, preemption_count=0, score=46980.4, test/accuracy=0.5474, test/loss=2.07923, test/num_examples=10000, total_duration=50650.4, train/accuracy=0.733697, train/loss=1.13867, validation/accuracy=0.6717, validation/loss=1.42577, validation/num_examples=50000
I0307 15:06:20.717668 140286718113536 logging_writer.py:48] [109100] global_step=109100, grad_norm=4.478821754455566, loss=2.412844657897949
I0307 15:07:42.665876 140286709720832 logging_writer.py:48] [109200] global_step=109200, grad_norm=3.9849181175231934, loss=2.471287727355957
I0307 15:09:04.225438 140286718113536 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.085190296173096, loss=2.55820369720459
I0307 15:11:32.842045 140286709720832 logging_writer.py:48] [109400] global_step=109400, grad_norm=3.7242965698242188, loss=2.4045565128326416
I0307 15:13:46.369375 140441807221952 spec.py:321] Evaluating on the training split.
I0307 15:13:57.129770 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 15:14:17.927647 140441807221952 spec.py:349] Evaluating on the test split.
I0307 15:14:19.664358 140441807221952 submission_runner.py:469] Time since start: 51195.17s, 	Step: 109473, 	{'train/accuracy': 0.732441782951355, 'train/loss': 1.1370259523391724, 'validation/accuracy': 0.6749199628829956, 'validation/loss': 1.4062894582748413, 'validation/num_examples': 50000, 'test/accuracy': 0.5485000014305115, 'test/loss': 2.0683977603912354, 'test/num_examples': 10000, 'score': 47491.805413007736, 'total_duration': 51195.168675899506, 'accumulated_submission_time': 47491.805413007736, 'accumulated_eval_time': 3679.0716648101807, 'accumulated_logging_time': 12.277462482452393}
I0307 15:14:19.701252 140286718113536 logging_writer.py:48] [109473] accumulated_eval_time=3679.07, accumulated_logging_time=12.2775, accumulated_submission_time=47491.8, global_step=109473, preemption_count=0, score=47491.8, test/accuracy=0.5485, test/loss=2.0684, test/num_examples=10000, total_duration=51195.2, train/accuracy=0.732442, train/loss=1.13703, validation/accuracy=0.67492, validation/loss=1.40629, validation/num_examples=50000
I0307 15:14:44.808720 140286709720832 logging_writer.py:48] [109500] global_step=109500, grad_norm=3.921438217163086, loss=2.4996228218078613
I0307 15:16:13.933179 140286718113536 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.572682857513428, loss=2.389728546142578
I0307 15:17:43.270237 140286709720832 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.170568466186523, loss=2.4787960052490234
I0307 15:18:39.725778 140286718113536 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.208169460296631, loss=2.4149744510650635
I0307 15:19:36.528257 140286709720832 logging_writer.py:48] [109900] global_step=109900, grad_norm=3.603436231613159, loss=2.446199893951416
I0307 15:22:12.720219 140286718113536 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.2527594566345215, loss=2.5156989097595215
I0307 15:22:50.141525 140441807221952 spec.py:321] Evaluating on the training split.
I0307 15:23:01.870700 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 15:23:24.714220 140441807221952 spec.py:349] Evaluating on the test split.
I0307 15:23:26.472429 140441807221952 submission_runner.py:469] Time since start: 51741.98s, 	Step: 110042, 	{'train/accuracy': 0.7405532598495483, 'train/loss': 1.0917880535125732, 'validation/accuracy': 0.6676200032234192, 'validation/loss': 1.4223884344100952, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.0675055980682373, 'test/num_examples': 10000, 'score': 48002.15768957138, 'total_duration': 51741.97675895691, 'accumulated_submission_time': 48002.15768957138, 'accumulated_eval_time': 3715.4025366306305, 'accumulated_logging_time': 12.340536117553711}
I0307 15:23:26.512473 140286709720832 logging_writer.py:48] [110042] accumulated_eval_time=3715.4, accumulated_logging_time=12.3405, accumulated_submission_time=48002.2, global_step=110042, preemption_count=0, score=48002.2, test/accuracy=0.5473, test/loss=2.06751, test/num_examples=10000, total_duration=51742, train/accuracy=0.740553, train/loss=1.09179, validation/accuracy=0.66762, validation/loss=1.42239, validation/num_examples=50000
I0307 15:24:15.982513 140286718113536 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.362088680267334, loss=2.4643967151641846
I0307 15:25:54.577718 140286709720832 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.246210098266602, loss=2.465757369995117
I0307 15:27:07.234883 140286718113536 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.46343469619751, loss=2.460881471633911
I0307 15:29:03.381047 140286709720832 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.4510016441345215, loss=2.439986228942871
I0307 15:30:20.518365 140286718113536 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.136325836181641, loss=2.483818531036377
I0307 15:31:56.707291 140441807221952 spec.py:321] Evaluating on the training split.
I0307 15:32:07.707336 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 15:32:27.554089 140441807221952 spec.py:349] Evaluating on the test split.
I0307 15:32:29.314647 140441807221952 submission_runner.py:469] Time since start: 52284.82s, 	Step: 110570, 	{'train/accuracy': 0.7365473508834839, 'train/loss': 1.146472454071045, 'validation/accuracy': 0.6718800067901611, 'validation/loss': 1.433701515197754, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.1005101203918457, 'test/num_examples': 10000, 'score': 48512.28666305542, 'total_duration': 52284.8189573288, 'accumulated_submission_time': 48512.28666305542, 'accumulated_eval_time': 3748.0098464488983, 'accumulated_logging_time': 12.388327836990356}
I0307 15:32:29.393018 140286709720832 logging_writer.py:48] [110570] accumulated_eval_time=3748.01, accumulated_logging_time=12.3883, accumulated_submission_time=48512.3, global_step=110570, preemption_count=0, score=48512.3, test/accuracy=0.5439, test/loss=2.10051, test/num_examples=10000, total_duration=52284.8, train/accuracy=0.736547, train/loss=1.14647, validation/accuracy=0.67188, validation/loss=1.4337, validation/num_examples=50000
I0307 15:32:55.264811 140286718113536 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.696169376373291, loss=2.532644271850586
I0307 15:35:17.956739 140286709720832 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.908148288726807, loss=2.339953899383545
I0307 15:39:25.948679 140286718113536 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.3983988761901855, loss=2.5721805095672607
I0307 15:41:02.311077 140441807221952 spec.py:321] Evaluating on the training split.
I0307 15:41:13.186795 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 15:41:37.951483 140441807221952 spec.py:349] Evaluating on the test split.
I0307 15:41:39.718760 140441807221952 submission_runner.py:469] Time since start: 52835.22s, 	Step: 110824, 	{'train/accuracy': 0.7321029901504517, 'train/loss': 1.1768193244934082, 'validation/accuracy': 0.6729199886322021, 'validation/loss': 1.434424638748169, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.091046094894409, 'test/num_examples': 10000, 'score': 49025.169120788574, 'total_duration': 52835.22308135033, 'accumulated_submission_time': 49025.169120788574, 'accumulated_eval_time': 3785.417489528656, 'accumulated_logging_time': 12.4746675491333}
I0307 15:41:39.740475 140286709720832 logging_writer.py:48] [110824] accumulated_eval_time=3785.42, accumulated_logging_time=12.4747, accumulated_submission_time=49025.2, global_step=110824, preemption_count=0, score=49025.2, test/accuracy=0.5482, test/loss=2.09105, test/num_examples=10000, total_duration=52835.2, train/accuracy=0.732103, train/loss=1.17682, validation/accuracy=0.67292, validation/loss=1.43442, validation/num_examples=50000
I0307 15:44:22.658918 140286718113536 logging_writer.py:48] [110900] global_step=110900, grad_norm=3.9293131828308105, loss=2.496631622314453
I0307 15:45:17.749873 140286709720832 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.487853527069092, loss=2.3844637870788574
I0307 15:47:03.306435 140286718113536 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.389981269836426, loss=2.5000100135803223
I0307 15:49:25.066809 140286709720832 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.142446517944336, loss=2.446171283721924
I0307 15:50:10.647642 140441807221952 spec.py:321] Evaluating on the training split.
I0307 15:50:21.697458 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 15:50:43.775129 140441807221952 spec.py:349] Evaluating on the test split.
I0307 15:50:45.549811 140441807221952 submission_runner.py:469] Time since start: 53381.05s, 	Step: 111230, 	{'train/accuracy': 0.7729392647743225, 'train/loss': 0.9798215627670288, 'validation/accuracy': 0.6782599687576294, 'validation/loss': 1.3746293783187866, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 2.004383087158203, 'test/num_examples': 10000, 'score': 49536.02373075485, 'total_duration': 53381.05413508415, 'accumulated_submission_time': 49536.02373075485, 'accumulated_eval_time': 3820.319619178772, 'accumulated_logging_time': 12.504190444946289}
I0307 15:50:45.588251 140286718113536 logging_writer.py:48] [111230] accumulated_eval_time=3820.32, accumulated_logging_time=12.5042, accumulated_submission_time=49536, global_step=111230, preemption_count=0, score=49536, test/accuracy=0.5632, test/loss=2.00438, test/num_examples=10000, total_duration=53381.1, train/accuracy=0.772939, train/loss=0.979822, validation/accuracy=0.67826, validation/loss=1.37463, validation/num_examples=50000
I0307 15:52:48.635648 140286709720832 logging_writer.py:48] [111300] global_step=111300, grad_norm=3.903869390487671, loss=2.4128026962280273
I0307 15:54:39.580924 140286718113536 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.587046146392822, loss=2.4389023780822754
I0307 15:56:48.655108 140286709720832 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.106635570526123, loss=2.4381775856018066
I0307 15:59:17.533057 140441807221952 spec.py:321] Evaluating on the training split.
I0307 15:59:28.648606 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 15:59:53.457248 140441807221952 spec.py:349] Evaluating on the test split.
I0307 15:59:55.207220 140441807221952 submission_runner.py:469] Time since start: 53930.71s, 	Step: 111596, 	{'train/accuracy': 0.7443997263908386, 'train/loss': 1.092374563217163, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.3958985805511475, 'validation/num_examples': 50000, 'test/accuracy': 0.5548000335693359, 'test/loss': 2.033613681793213, 'test/num_examples': 10000, 'score': 50047.921122312546, 'total_duration': 53930.71153640747, 'accumulated_submission_time': 50047.921122312546, 'accumulated_eval_time': 3857.993744134903, 'accumulated_logging_time': 12.550712823867798}
I0307 15:59:55.253908 140286718113536 logging_writer.py:48] [111596] accumulated_eval_time=3857.99, accumulated_logging_time=12.5507, accumulated_submission_time=50047.9, global_step=111596, preemption_count=0, score=50047.9, test/accuracy=0.5548, test/loss=2.03361, test/num_examples=10000, total_duration=53930.7, train/accuracy=0.7444, train/loss=1.09237, validation/accuracy=0.67576, validation/loss=1.3959, validation/num_examples=50000
I0307 15:59:57.200741 140286709720832 logging_writer.py:48] [111600] global_step=111600, grad_norm=3.9942378997802734, loss=2.442560911178589
I0307 16:03:19.619976 140286718113536 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.623804569244385, loss=2.4727067947387695
I0307 16:05:02.353147 140286709720832 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.234059810638428, loss=2.476004123687744
I0307 16:07:22.246562 140286718113536 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.803257942199707, loss=2.4529130458831787
I0307 16:08:25.702854 140441807221952 spec.py:321] Evaluating on the training split.
I0307 16:08:36.681050 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 16:08:59.220030 140441807221952 spec.py:349] Evaluating on the test split.
I0307 16:09:01.112173 140441807221952 submission_runner.py:469] Time since start: 54476.62s, 	Step: 111934, 	{'train/accuracy': 0.7419881820678711, 'train/loss': 1.1028839349746704, 'validation/accuracy': 0.6772599816322327, 'validation/loss': 1.3917112350463867, 'validation/num_examples': 50000, 'test/accuracy': 0.5563000440597534, 'test/loss': 2.0379831790924072, 'test/num_examples': 10000, 'score': 50558.29797911644, 'total_duration': 54476.6165034771, 'accumulated_submission_time': 50558.29797911644, 'accumulated_eval_time': 3893.403036594391, 'accumulated_logging_time': 12.632005453109741}
I0307 16:09:01.223100 140286709720832 logging_writer.py:48] [111934] accumulated_eval_time=3893.4, accumulated_logging_time=12.632, accumulated_submission_time=50558.3, global_step=111934, preemption_count=0, score=50558.3, test/accuracy=0.5563, test/loss=2.03798, test/num_examples=10000, total_duration=54476.6, train/accuracy=0.741988, train/loss=1.10288, validation/accuracy=0.67726, validation/loss=1.39171, validation/num_examples=50000
I0307 16:11:01.568401 140286718113536 logging_writer.py:48] [112000] global_step=112000, grad_norm=3.9229161739349365, loss=2.4080758094787598
I0307 16:16:22.735081 140286709720832 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.415338039398193, loss=2.4310030937194824
I0307 16:17:34.727555 140441807221952 spec.py:321] Evaluating on the training split.
I0307 16:17:45.412298 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 16:18:07.177622 140441807221952 spec.py:349] Evaluating on the test split.
I0307 16:18:09.070385 140441807221952 submission_runner.py:469] Time since start: 55024.57s, 	Step: 112118, 	{'train/accuracy': 0.7438217401504517, 'train/loss': 1.1100208759307861, 'validation/accuracy': 0.6826199889183044, 'validation/loss': 1.3858392238616943, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.031019926071167, 'test/num_examples': 10000, 'score': 51071.773574113846, 'total_duration': 55024.574649333954, 'accumulated_submission_time': 51071.773574113846, 'accumulated_eval_time': 3927.745766401291, 'accumulated_logging_time': 12.75107717514038}
I0307 16:18:09.115844 140286718113536 logging_writer.py:48] [112118] accumulated_eval_time=3927.75, accumulated_logging_time=12.7511, accumulated_submission_time=51071.8, global_step=112118, preemption_count=0, score=51071.8, test/accuracy=0.5582, test/loss=2.03102, test/num_examples=10000, total_duration=55024.6, train/accuracy=0.743822, train/loss=1.11002, validation/accuracy=0.68262, validation/loss=1.38584, validation/num_examples=50000
I0307 16:21:54.725064 140286709720832 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.161303520202637, loss=2.3590152263641357
I0307 16:23:12.993434 140286718113536 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.000853538513184, loss=2.465506076812744
I0307 16:24:32.240519 140286709720832 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.816578388214111, loss=2.3800714015960693
I0307 16:25:57.202543 140286718113536 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.679522514343262, loss=2.3723888397216797
I0307 16:26:39.503625 140441807221952 spec.py:321] Evaluating on the training split.
I0307 16:26:50.456803 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 16:27:10.476306 140441807221952 spec.py:349] Evaluating on the test split.
I0307 16:27:12.208280 140441807221952 submission_runner.py:469] Time since start: 55567.71s, 	Step: 112540, 	{'train/accuracy': 0.7454360723495483, 'train/loss': 1.0766855478286743, 'validation/accuracy': 0.6845200061798096, 'validation/loss': 1.3572968244552612, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 1.9948618412017822, 'test/num_examples': 10000, 'score': 51582.09742999077, 'total_duration': 55567.712595939636, 'accumulated_submission_time': 51582.09742999077, 'accumulated_eval_time': 3960.4503729343414, 'accumulated_logging_time': 12.814718246459961}
I0307 16:27:12.242369 140286709720832 logging_writer.py:48] [112540] accumulated_eval_time=3960.45, accumulated_logging_time=12.8147, accumulated_submission_time=51582.1, global_step=112540, preemption_count=0, score=51582.1, test/accuracy=0.5623, test/loss=1.99486, test/num_examples=10000, total_duration=55567.7, train/accuracy=0.745436, train/loss=1.07669, validation/accuracy=0.68452, validation/loss=1.3573, validation/num_examples=50000
I0307 16:27:56.870850 140286718113536 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.535854339599609, loss=2.4984326362609863
I0307 16:28:49.448627 140286709720832 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.360501766204834, loss=2.4744961261749268
I0307 16:29:41.809258 140286718113536 logging_writer.py:48] [112800] global_step=112800, grad_norm=3.9525699615478516, loss=2.539313316345215
I0307 16:31:58.590411 140286709720832 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.000997543334961, loss=2.4230587482452393
I0307 16:34:31.933854 140286718113536 logging_writer.py:48] [113000] global_step=113000, grad_norm=3.7264111042022705, loss=2.452401876449585
I0307 16:35:42.847483 140441807221952 spec.py:321] Evaluating on the training split.
I0307 16:35:54.544275 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 16:36:17.021514 140441807221952 spec.py:349] Evaluating on the test split.
I0307 16:36:18.776538 140441807221952 submission_runner.py:469] Time since start: 56114.28s, 	Step: 113080, 	{'train/accuracy': 0.7481664419174194, 'train/loss': 1.0672670602798462, 'validation/accuracy': 0.6777399778366089, 'validation/loss': 1.3826467990875244, 'validation/num_examples': 50000, 'test/accuracy': 0.553100049495697, 'test/loss': 2.0283591747283936, 'test/num_examples': 10000, 'score': 52092.63468718529, 'total_duration': 56114.28086948395, 'accumulated_submission_time': 52092.63468718529, 'accumulated_eval_time': 3996.3794016838074, 'accumulated_logging_time': 12.856613159179688}
I0307 16:36:18.857341 140286709720832 logging_writer.py:48] [113080] accumulated_eval_time=3996.38, accumulated_logging_time=12.8566, accumulated_submission_time=52092.6, global_step=113080, preemption_count=0, score=52092.6, test/accuracy=0.5531, test/loss=2.02836, test/num_examples=10000, total_duration=56114.3, train/accuracy=0.748166, train/loss=1.06727, validation/accuracy=0.67774, validation/loss=1.38265, validation/num_examples=50000
I0307 16:36:27.086039 140286718113536 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.408462047576904, loss=2.4881649017333984
I0307 16:38:15.110377 140286709720832 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.120349884033203, loss=2.401263952255249
I0307 16:42:54.001981 140286718113536 logging_writer.py:48] [113300] global_step=113300, grad_norm=3.9933955669403076, loss=2.397350311279297
I0307 16:44:50.311937 140441807221952 spec.py:321] Evaluating on the training split.
I0307 16:45:00.973618 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 16:45:24.322852 140441807221952 spec.py:349] Evaluating on the test split.
I0307 16:45:26.085070 140441807221952 submission_runner.py:469] Time since start: 56661.59s, 	Step: 113328, 	{'train/accuracy': 0.7432238459587097, 'train/loss': 1.0936839580535889, 'validation/accuracy': 0.6787599921226501, 'validation/loss': 1.3772202730178833, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 2.026643753051758, 'test/num_examples': 10000, 'score': 52604.05321931839, 'total_duration': 56661.589386463165, 'accumulated_submission_time': 52604.05321931839, 'accumulated_eval_time': 4032.152496099472, 'accumulated_logging_time': 12.945146560668945}
I0307 16:45:26.109761 140286709720832 logging_writer.py:48] [113328] accumulated_eval_time=4032.15, accumulated_logging_time=12.9451, accumulated_submission_time=52604.1, global_step=113328, preemption_count=0, score=52604.1, test/accuracy=0.5568, test/loss=2.02664, test/num_examples=10000, total_duration=56661.6, train/accuracy=0.743224, train/loss=1.09368, validation/accuracy=0.67876, validation/loss=1.37722, validation/num_examples=50000
I0307 16:48:16.706976 140286718113536 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.058297634124756, loss=2.4477598667144775
I0307 16:49:46.533878 140286709720832 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.132928371429443, loss=2.4055662155151367
I0307 16:51:16.909809 140286718113536 logging_writer.py:48] [113600] global_step=113600, grad_norm=3.9038474559783936, loss=2.432851791381836
I0307 16:52:47.972554 140286709720832 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.171542644500732, loss=2.4463155269622803
I0307 16:53:57.685143 140441807221952 spec.py:321] Evaluating on the training split.
I0307 16:54:08.658495 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 16:54:27.939557 140441807221952 spec.py:349] Evaluating on the test split.
I0307 16:54:29.676640 140441807221952 submission_runner.py:469] Time since start: 57205.18s, 	Step: 113742, 	{'train/accuracy': 0.7355309128761292, 'train/loss': 1.1383002996444702, 'validation/accuracy': 0.6761199831962585, 'validation/loss': 1.3979442119598389, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.0567944049835205, 'test/num_examples': 10000, 'score': 53115.574845314026, 'total_duration': 57205.180955410004, 'accumulated_submission_time': 53115.574845314026, 'accumulated_eval_time': 4064.14395070076, 'accumulated_logging_time': 12.977898836135864}
I0307 16:54:29.741937 140286718113536 logging_writer.py:48] [113742] accumulated_eval_time=4064.14, accumulated_logging_time=12.9779, accumulated_submission_time=53115.6, global_step=113742, preemption_count=0, score=53115.6, test/accuracy=0.5496, test/loss=2.05679, test/num_examples=10000, total_duration=57205.2, train/accuracy=0.735531, train/loss=1.1383, validation/accuracy=0.67612, validation/loss=1.39794, validation/num_examples=50000
I0307 16:55:54.237592 140286709720832 logging_writer.py:48] [113800] global_step=113800, grad_norm=4.306413650512695, loss=2.433177947998047
I0307 16:59:50.703377 140286718113536 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.2153639793396, loss=2.452169418334961
I0307 17:02:33.982071 140286709720832 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.285546779632568, loss=2.473052501678467
I0307 17:02:59.928813 140441807221952 spec.py:321] Evaluating on the training split.
I0307 17:03:10.012798 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 17:03:30.912008 140441807221952 spec.py:349] Evaluating on the test split.
I0307 17:03:32.681366 140441807221952 submission_runner.py:469] Time since start: 57748.19s, 	Step: 114025, 	{'train/accuracy': 0.7551219463348389, 'train/loss': 1.058273196220398, 'validation/accuracy': 0.6785399913787842, 'validation/loss': 1.4052555561065674, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.0645456314086914, 'test/num_examples': 10000, 'score': 53625.72285366058, 'total_duration': 57748.1856815815, 'accumulated_submission_time': 53625.72285366058, 'accumulated_eval_time': 4096.896467208862, 'accumulated_logging_time': 13.050889015197754}
I0307 17:03:32.734820 140286718113536 logging_writer.py:48] [114025] accumulated_eval_time=4096.9, accumulated_logging_time=13.0509, accumulated_submission_time=53625.7, global_step=114025, preemption_count=0, score=53625.7, test/accuracy=0.5521, test/loss=2.06455, test/num_examples=10000, total_duration=57748.2, train/accuracy=0.755122, train/loss=1.05827, validation/accuracy=0.67854, validation/loss=1.40526, validation/num_examples=50000
I0307 17:05:39.337418 140286709720832 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.088406085968018, loss=2.5225133895874023
I0307 17:09:12.991258 140286718113536 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.38477897644043, loss=2.4526708126068115
I0307 17:12:03.135907 140441807221952 spec.py:321] Evaluating on the training split.
I0307 17:12:14.066283 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 17:12:36.006621 140441807221952 spec.py:349] Evaluating on the test split.
I0307 17:12:37.750016 140441807221952 submission_runner.py:469] Time since start: 58293.25s, 	Step: 114280, 	{'train/accuracy': 0.7571747303009033, 'train/loss': 1.0325630903244019, 'validation/accuracy': 0.6838600039482117, 'validation/loss': 1.3688435554504395, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 2.0225744247436523, 'test/num_examples': 10000, 'score': 54136.0850789547, 'total_duration': 58293.25433254242, 'accumulated_submission_time': 54136.0850789547, 'accumulated_eval_time': 4131.510539770126, 'accumulated_logging_time': 13.113523244857788}
I0307 17:12:37.772518 140286709720832 logging_writer.py:48] [114280] accumulated_eval_time=4131.51, accumulated_logging_time=13.1135, accumulated_submission_time=54136.1, global_step=114280, preemption_count=0, score=54136.1, test/accuracy=0.5565, test/loss=2.02257, test/num_examples=10000, total_duration=58293.3, train/accuracy=0.757175, train/loss=1.03256, validation/accuracy=0.68386, validation/loss=1.36884, validation/num_examples=50000
I0307 17:12:57.027776 140286718113536 logging_writer.py:48] [114300] global_step=114300, grad_norm=3.823075294494629, loss=2.431293487548828
I0307 17:16:28.582580 140286709720832 logging_writer.py:48] [114400] global_step=114400, grad_norm=3.9220361709594727, loss=2.412036657333374
I0307 17:21:07.943770 140441807221952 spec.py:321] Evaluating on the training split.
I0307 17:21:18.513850 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 17:21:42.562018 140441807221952 spec.py:349] Evaluating on the test split.
I0307 17:21:44.335379 140441807221952 submission_runner.py:469] Time since start: 58839.84s, 	Step: 114472, 	{'train/accuracy': 0.7515744566917419, 'train/loss': 1.0687155723571777, 'validation/accuracy': 0.6802600026130676, 'validation/loss': 1.387001395225525, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.039818525314331, 'test/num_examples': 10000, 'score': 54646.22646856308, 'total_duration': 58839.83969449997, 'accumulated_submission_time': 54646.22646856308, 'accumulated_eval_time': 4167.902105808258, 'accumulated_logging_time': 13.143961191177368}
I0307 17:21:44.357365 140286718113536 logging_writer.py:48] [114472] accumulated_eval_time=4167.9, accumulated_logging_time=13.144, accumulated_submission_time=54646.2, global_step=114472, preemption_count=0, score=54646.2, test/accuracy=0.5584, test/loss=2.03982, test/num_examples=10000, total_duration=58839.8, train/accuracy=0.751574, train/loss=1.06872, validation/accuracy=0.68026, validation/loss=1.387, validation/num_examples=50000
I0307 17:23:25.213197 140286709720832 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.186505317687988, loss=2.392164945602417
I0307 17:30:18.257608 140441807221952 spec.py:321] Evaluating on the training split.
I0307 17:30:29.045950 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 17:30:54.067059 140441807221952 spec.py:349] Evaluating on the test split.
I0307 17:30:55.832064 140441807221952 submission_runner.py:469] Time since start: 59391.34s, 	Step: 114598, 	{'train/accuracy': 0.751375138759613, 'train/loss': 1.070949673652649, 'validation/accuracy': 0.6862999796867371, 'validation/loss': 1.3652663230895996, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.0168349742889404, 'test/num_examples': 10000, 'score': 55160.103695869446, 'total_duration': 59391.33639764786, 'accumulated_submission_time': 55160.103695869446, 'accumulated_eval_time': 4205.476538181305, 'accumulated_logging_time': 13.175290822982788}
I0307 17:30:55.854722 140286718113536 logging_writer.py:48] [114598] accumulated_eval_time=4205.48, accumulated_logging_time=13.1753, accumulated_submission_time=55160.1, global_step=114598, preemption_count=0, score=55160.1, test/accuracy=0.5611, test/loss=2.01683, test/num_examples=10000, total_duration=59391.3, train/accuracy=0.751375, train/loss=1.07095, validation/accuracy=0.6863, validation/loss=1.36527, validation/num_examples=50000
I0307 17:30:56.971786 140286709720832 logging_writer.py:48] [114600] global_step=114600, grad_norm=3.9813315868377686, loss=2.4500572681427
I0307 17:33:00.675012 140286718113536 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.160809516906738, loss=2.435648202896118
I0307 17:34:21.866211 140286709720832 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.8678178787231445, loss=2.5025675296783447
I0307 17:35:44.110291 140286718113536 logging_writer.py:48] [114900] global_step=114900, grad_norm=3.6891238689422607, loss=2.3672733306884766
I0307 17:37:09.534893 140286709720832 logging_writer.py:48] [115000] global_step=115000, grad_norm=4.00098180770874, loss=2.343479633331299
I0307 17:38:30.134858 140286718113536 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.334181308746338, loss=2.4506444931030273
I0307 17:39:25.891059 140441807221952 spec.py:321] Evaluating on the training split.
I0307 17:39:38.156557 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 17:40:10.488704 140441807221952 spec.py:349] Evaluating on the test split.
I0307 17:40:12.245639 140441807221952 submission_runner.py:469] Time since start: 59947.75s, 	Step: 115187, 	{'train/accuracy': 0.7390784025192261, 'train/loss': 1.119966983795166, 'validation/accuracy': 0.6820799708366394, 'validation/loss': 1.3853365182876587, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 2.063185214996338, 'test/num_examples': 10000, 'score': 55670.06543755531, 'total_duration': 59947.74996447563, 'accumulated_submission_time': 55670.06543755531, 'accumulated_eval_time': 4251.831089496613, 'accumulated_logging_time': 13.205965995788574}
I0307 17:40:12.291850 140286709720832 logging_writer.py:48] [115187] accumulated_eval_time=4251.83, accumulated_logging_time=13.206, accumulated_submission_time=55670.1, global_step=115187, preemption_count=0, score=55670.1, test/accuracy=0.554, test/loss=2.06319, test/num_examples=10000, total_duration=59947.7, train/accuracy=0.739078, train/loss=1.11997, validation/accuracy=0.68208, validation/loss=1.38534, validation/num_examples=50000
I0307 17:40:19.093951 140286718113536 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.075311183929443, loss=2.389878749847412
I0307 17:41:25.857793 140286709720832 logging_writer.py:48] [115300] global_step=115300, grad_norm=3.8575732707977295, loss=2.440934658050537
I0307 17:42:30.109503 140286718113536 logging_writer.py:48] [115400] global_step=115400, grad_norm=3.9998109340667725, loss=2.3882110118865967
I0307 17:43:34.374133 140286709720832 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.004600524902344, loss=2.517343282699585
I0307 17:44:38.879411 140286718113536 logging_writer.py:48] [115600] global_step=115600, grad_norm=3.8530185222625732, loss=2.347475051879883
I0307 17:45:43.060021 140286709720832 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.79957389831543, loss=2.5169143676757812
I0307 17:46:47.078295 140286718113536 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.052485942840576, loss=2.473599910736084
I0307 17:47:51.188847 140286709720832 logging_writer.py:48] [115900] global_step=115900, grad_norm=3.8257908821105957, loss=2.37807559967041
I0307 17:48:42.759893 140441807221952 spec.py:321] Evaluating on the training split.
I0307 17:48:55.109044 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 17:49:14.175867 140441807221952 spec.py:349] Evaluating on the test split.
I0307 17:49:15.941105 140441807221952 submission_runner.py:469] Time since start: 60491.45s, 	Step: 115981, 	{'train/accuracy': 0.7488639950752258, 'train/loss': 1.076809287071228, 'validation/accuracy': 0.6796799898147583, 'validation/loss': 1.3794686794281006, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.046754837036133, 'test/num_examples': 10000, 'score': 56179.215597867966, 'total_duration': 60491.445437669754, 'accumulated_submission_time': 56179.215597867966, 'accumulated_eval_time': 4285.012269258499, 'accumulated_logging_time': 14.478042125701904}
I0307 17:49:15.984044 140286718113536 logging_writer.py:48] [115981] accumulated_eval_time=4285.01, accumulated_logging_time=14.478, accumulated_submission_time=56179.2, global_step=115981, preemption_count=0, score=56179.2, test/accuracy=0.5528, test/loss=2.04675, test/num_examples=10000, total_duration=60491.4, train/accuracy=0.748864, train/loss=1.07681, validation/accuracy=0.67968, validation/loss=1.37947, validation/num_examples=50000
I0307 17:49:23.937833 140286709720832 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.057794570922852, loss=2.4194366931915283
I0307 17:50:25.487234 140286718113536 logging_writer.py:48] [116100] global_step=116100, grad_norm=3.9440081119537354, loss=2.4883110523223877
I0307 17:51:29.789609 140286709720832 logging_writer.py:48] [116200] global_step=116200, grad_norm=6.0733795166015625, loss=2.4133553504943848
I0307 17:52:34.083811 140286718113536 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.276572227478027, loss=2.4259254932403564
2025-03-07 17:52:59.538461: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:53:39.924784 140286709720832 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.138981342315674, loss=2.430356502532959
I0307 17:54:44.192649 140286718113536 logging_writer.py:48] [116500] global_step=116500, grad_norm=3.8813977241516113, loss=2.411067008972168
I0307 17:55:48.048202 140286709720832 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.74044132232666, loss=2.389390468597412
I0307 17:56:52.267589 140286718113536 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.453908920288086, loss=2.361318588256836
I0307 17:57:46.101573 140441807221952 spec.py:321] Evaluating on the training split.
I0307 17:57:58.343562 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 17:58:20.241839 140441807221952 spec.py:349] Evaluating on the test split.
I0307 17:58:21.999614 140441807221952 submission_runner.py:469] Time since start: 61037.50s, 	Step: 116785, 	{'train/accuracy': 0.7436822056770325, 'train/loss': 1.1006927490234375, 'validation/accuracy': 0.6809799671173096, 'validation/loss': 1.379116177558899, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 2.0484087467193604, 'test/num_examples': 10000, 'score': 56689.229739665985, 'total_duration': 61037.50391745567, 'accumulated_submission_time': 56689.229739665985, 'accumulated_eval_time': 4320.910253286362, 'accumulated_logging_time': 14.534140348434448}
I0307 17:58:22.069213 140286709720832 logging_writer.py:48] [116785] accumulated_eval_time=4320.91, accumulated_logging_time=14.5341, accumulated_submission_time=56689.2, global_step=116785, preemption_count=0, score=56689.2, test/accuracy=0.5558, test/loss=2.04841, test/num_examples=10000, total_duration=61037.5, train/accuracy=0.743682, train/loss=1.10069, validation/accuracy=0.68098, validation/loss=1.37912, validation/num_examples=50000
I0307 17:58:28.331539 140286718113536 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.2783002853393555, loss=2.525517463684082
I0307 17:59:31.833334 140286709720832 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.640841245651245, loss=2.363577365875244
I0307 18:01:50.395231 140286718113536 logging_writer.py:48] [117000] global_step=117000, grad_norm=3.937962770462036, loss=2.409686803817749
I0307 18:04:33.413020 140286709720832 logging_writer.py:48] [117100] global_step=117100, grad_norm=4.0078043937683105, loss=2.4200515747070312
I0307 18:06:52.913506 140441807221952 spec.py:321] Evaluating on the training split.
I0307 18:07:03.821329 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 18:07:27.149566 140441807221952 spec.py:349] Evaluating on the test split.
I0307 18:07:28.921180 140441807221952 submission_runner.py:469] Time since start: 61584.43s, 	Step: 117138, 	{'train/accuracy': 0.7645089030265808, 'train/loss': 1.015527367591858, 'validation/accuracy': 0.6846399903297424, 'validation/loss': 1.359680414199829, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 2.0225794315338135, 'test/num_examples': 10000, 'score': 57199.9940328598, 'total_duration': 61584.42549967766, 'accumulated_submission_time': 57199.9940328598, 'accumulated_eval_time': 4356.917885303497, 'accumulated_logging_time': 14.643930673599243}
I0307 18:07:28.948007 140286718113536 logging_writer.py:48] [117138] accumulated_eval_time=4356.92, accumulated_logging_time=14.6439, accumulated_submission_time=57200, global_step=117138, preemption_count=0, score=57200, test/accuracy=0.558, test/loss=2.02258, test/num_examples=10000, total_duration=61584.4, train/accuracy=0.764509, train/loss=1.01553, validation/accuracy=0.68464, validation/loss=1.35968, validation/num_examples=50000
I0307 18:10:53.092045 140286709720832 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.149105072021484, loss=2.403472423553467
I0307 18:12:22.294195 140286718113536 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.349660873413086, loss=2.416386842727661
I0307 18:13:51.155229 140286709720832 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.039981365203857, loss=2.44107723236084
I0307 18:15:23.612573 140286718113536 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.426097869873047, loss=2.4076709747314453
I0307 18:15:58.931707 140441807221952 spec.py:321] Evaluating on the training split.
I0307 18:16:10.485897 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 18:16:33.634995 140441807221952 spec.py:349] Evaluating on the test split.
I0307 18:16:35.342864 140441807221952 submission_runner.py:469] Time since start: 62130.85s, 	Step: 117540, 	{'train/accuracy': 0.7486646771430969, 'train/loss': 1.0716880559921265, 'validation/accuracy': 0.6812599897384644, 'validation/loss': 1.3699307441711426, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 2.0308005809783936, 'test/num_examples': 10000, 'score': 57709.923468351364, 'total_duration': 62130.84720039368, 'accumulated_submission_time': 57709.923468351364, 'accumulated_eval_time': 4393.329013824463, 'accumulated_logging_time': 14.679791927337646}
I0307 18:16:35.409236 140286709720832 logging_writer.py:48] [117540] accumulated_eval_time=4393.33, accumulated_logging_time=14.6798, accumulated_submission_time=57709.9, global_step=117540, preemption_count=0, score=57709.9, test/accuracy=0.5595, test/loss=2.0308, test/num_examples=10000, total_duration=62130.8, train/accuracy=0.748665, train/loss=1.07169, validation/accuracy=0.68126, validation/loss=1.36993, validation/num_examples=50000
2025-03-07 18:17:11.858724: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:17:21.992148 140286718113536 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.400487899780273, loss=2.3651418685913086
I0307 18:18:48.900081 140286709720832 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.345109939575195, loss=2.340519428253174
I0307 18:20:16.148744 140286718113536 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.239898204803467, loss=2.4697022438049316
I0307 18:21:43.936941 140286709720832 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.073694705963135, loss=2.4087014198303223
I0307 18:23:11.060571 140286718113536 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.120137691497803, loss=2.397207021713257
I0307 18:24:38.370710 140286709720832 logging_writer.py:48] [118100] global_step=118100, grad_norm=3.9788601398468018, loss=2.3021364212036133
I0307 18:25:05.465696 140441807221952 spec.py:321] Evaluating on the training split.
I0307 18:25:17.159981 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 18:25:41.441604 140441807221952 spec.py:349] Evaluating on the test split.
I0307 18:25:43.198643 140441807221952 submission_runner.py:469] Time since start: 62678.70s, 	Step: 118132, 	{'train/accuracy': 0.7505779266357422, 'train/loss': 1.0568245649337769, 'validation/accuracy': 0.6892799735069275, 'validation/loss': 1.3315318822860718, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 2.0038721561431885, 'test/num_examples': 10000, 'score': 58219.88133907318, 'total_duration': 62678.7029542923, 'accumulated_submission_time': 58219.88133907318, 'accumulated_eval_time': 4431.0619122982025, 'accumulated_logging_time': 14.777708053588867}
I0307 18:25:43.237846 140286718113536 logging_writer.py:48] [118132] accumulated_eval_time=4431.06, accumulated_logging_time=14.7777, accumulated_submission_time=58219.9, global_step=118132, preemption_count=0, score=58219.9, test/accuracy=0.5638, test/loss=2.00387, test/num_examples=10000, total_duration=62678.7, train/accuracy=0.750578, train/loss=1.05682, validation/accuracy=0.68928, validation/loss=1.33153, validation/num_examples=50000
I0307 18:26:35.517694 140286709720832 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.029759407043457, loss=2.376912832260132
I0307 18:28:02.265106 140286718113536 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.68782901763916, loss=2.4495365619659424
I0307 18:29:29.383062 140286709720832 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.599775314331055, loss=2.3980326652526855
I0307 18:30:59.487463 140286718113536 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.594573497772217, loss=2.477128267288208
I0307 18:32:46.663829 140286709720832 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.668800354003906, loss=2.5237390995025635
I0307 18:34:13.757858 140441807221952 spec.py:321] Evaluating on the training split.
I0307 18:34:25.090345 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 18:34:48.229900 140441807221952 spec.py:349] Evaluating on the test split.
I0307 18:34:49.958963 140441807221952 submission_runner.py:469] Time since start: 63225.46s, 	Step: 118682, 	{'train/accuracy': 0.7507772445678711, 'train/loss': 1.0511842966079712, 'validation/accuracy': 0.6802600026130676, 'validation/loss': 1.3835629224777222, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 2.040649652481079, 'test/num_examples': 10000, 'score': 58730.331020116806, 'total_duration': 63225.463284015656, 'accumulated_submission_time': 58730.331020116806, 'accumulated_eval_time': 4467.262981176376, 'accumulated_logging_time': 14.82481837272644}
I0307 18:34:50.051908 140286718113536 logging_writer.py:48] [118682] accumulated_eval_time=4467.26, accumulated_logging_time=14.8248, accumulated_submission_time=58730.3, global_step=118682, preemption_count=0, score=58730.3, test/accuracy=0.5509, test/loss=2.04065, test/num_examples=10000, total_duration=63225.5, train/accuracy=0.750777, train/loss=1.05118, validation/accuracy=0.68026, validation/loss=1.38356, validation/num_examples=50000
I0307 18:34:57.529140 140286709720832 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.058767318725586, loss=2.4717140197753906
I0307 18:36:51.965348 140286718113536 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.451053142547607, loss=2.3704423904418945
2025-03-07 18:37:35.780386: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:38:40.751929 140286709720832 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.819205284118652, loss=2.4048867225646973
I0307 18:41:10.726401 140286718113536 logging_writer.py:48] [119000] global_step=119000, grad_norm=4.636425971984863, loss=2.400913953781128
I0307 18:43:20.470737 140441807221952 spec.py:321] Evaluating on the training split.
I0307 18:43:31.408598 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 18:43:53.826177 140441807221952 spec.py:349] Evaluating on the test split.
I0307 18:43:55.602933 140441807221952 submission_runner.py:469] Time since start: 63771.11s, 	Step: 119063, 	{'train/accuracy': 0.7574537396430969, 'train/loss': 1.0407602787017822, 'validation/accuracy': 0.6904399991035461, 'validation/loss': 1.3321399688720703, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 1.992639183998108, 'test/num_examples': 10000, 'score': 59240.697813510895, 'total_duration': 63771.10726213455, 'accumulated_submission_time': 59240.697813510895, 'accumulated_eval_time': 4502.3951551914215, 'accumulated_logging_time': 14.925670623779297}
I0307 18:43:55.696470 140286709720832 logging_writer.py:48] [119063] accumulated_eval_time=4502.4, accumulated_logging_time=14.9257, accumulated_submission_time=59240.7, global_step=119063, preemption_count=0, score=59240.7, test/accuracy=0.5627, test/loss=1.99264, test/num_examples=10000, total_duration=63771.1, train/accuracy=0.757454, train/loss=1.04076, validation/accuracy=0.69044, validation/loss=1.33214, validation/num_examples=50000
I0307 18:44:54.369854 140286718113536 logging_writer.py:48] [119100] global_step=119100, grad_norm=4.586243629455566, loss=2.4624199867248535
I0307 18:48:24.843871 140286709720832 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.347192764282227, loss=2.3152143955230713
I0307 18:51:54.523301 140286718113536 logging_writer.py:48] [119300] global_step=119300, grad_norm=4.2182698249816895, loss=2.3098323345184326
I0307 18:52:26.199133 140441807221952 spec.py:321] Evaluating on the training split.
I0307 18:52:36.000330 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 18:52:58.418487 140441807221952 spec.py:349] Evaluating on the test split.
I0307 18:53:00.154703 140441807221952 submission_runner.py:469] Time since start: 64315.66s, 	Step: 119316, 	{'train/accuracy': 0.7449178695678711, 'train/loss': 1.0808199644088745, 'validation/accuracy': 0.6895399689674377, 'validation/loss': 1.3459185361862183, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 1.9806833267211914, 'test/num_examples': 10000, 'score': 59751.13297390938, 'total_duration': 64315.65903496742, 'accumulated_submission_time': 59751.13297390938, 'accumulated_eval_time': 4536.350691080093, 'accumulated_logging_time': 15.058576822280884}
I0307 18:53:00.177886 140286709720832 logging_writer.py:48] [119316] accumulated_eval_time=4536.35, accumulated_logging_time=15.0586, accumulated_submission_time=59751.1, global_step=119316, preemption_count=0, score=59751.1, test/accuracy=0.5666, test/loss=1.98068, test/num_examples=10000, total_duration=64315.7, train/accuracy=0.744918, train/loss=1.08082, validation/accuracy=0.68954, validation/loss=1.34592, validation/num_examples=50000
I0307 18:55:38.915993 140286718113536 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.326218605041504, loss=2.318267822265625
I0307 18:59:09.158754 140286709720832 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.027099609375, loss=2.340052366256714
I0307 19:01:30.714217 140441807221952 spec.py:321] Evaluating on the training split.
I0307 19:01:41.585063 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 19:02:03.531753 140441807221952 spec.py:349] Evaluating on the test split.
I0307 19:02:05.280230 140441807221952 submission_runner.py:469] Time since start: 64860.78s, 	Step: 119568, 	{'train/accuracy': 0.7529097199440002, 'train/loss': 1.060800313949585, 'validation/accuracy': 0.6879000067710876, 'validation/loss': 1.3469411134719849, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.982959270477295, 'test/num_examples': 10000, 'score': 60261.63347887993, 'total_duration': 64860.78455853462, 'accumulated_submission_time': 60261.63347887993, 'accumulated_eval_time': 4570.916675567627, 'accumulated_logging_time': 15.08935546875}
I0307 19:02:05.303204 140286718113536 logging_writer.py:48] [119568] accumulated_eval_time=4570.92, accumulated_logging_time=15.0894, accumulated_submission_time=60261.6, global_step=119568, preemption_count=0, score=60261.6, test/accuracy=0.5675, test/loss=1.98296, test/num_examples=10000, total_duration=64860.8, train/accuracy=0.75291, train/loss=1.0608, validation/accuracy=0.6879, validation/loss=1.34694, validation/num_examples=50000
I0307 19:02:54.027522 140286709720832 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.295295715332031, loss=2.42495059967041
I0307 19:06:11.366085 140286718113536 logging_writer.py:48] [119700] global_step=119700, grad_norm=4.08773136138916, loss=2.4137558937072754
I0307 19:08:32.602085 140286709720832 logging_writer.py:48] [119800] global_step=119800, grad_norm=4.316481590270996, loss=2.4734439849853516
I0307 19:10:36.187794 140441807221952 spec.py:321] Evaluating on the training split.
I0307 19:10:47.194294 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 19:11:08.119884 140441807221952 spec.py:349] Evaluating on the test split.
I0307 19:11:09.860629 140441807221952 submission_runner.py:469] Time since start: 65405.36s, 	Step: 119888, 	{'train/accuracy': 0.7716039419174194, 'train/loss': 0.9782085418701172, 'validation/accuracy': 0.6869800090789795, 'validation/loss': 1.3563183546066284, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 2.0083489418029785, 'test/num_examples': 10000, 'score': 60772.47364616394, 'total_duration': 65405.3649623394, 'accumulated_submission_time': 60772.47364616394, 'accumulated_eval_time': 4604.589485406876, 'accumulated_logging_time': 15.120202541351318}
I0307 19:11:09.917160 140286718113536 logging_writer.py:48] [119888] accumulated_eval_time=4604.59, accumulated_logging_time=15.1202, accumulated_submission_time=60772.5, global_step=119888, preemption_count=0, score=60772.5, test/accuracy=0.5612, test/loss=2.00835, test/num_examples=10000, total_duration=65405.4, train/accuracy=0.771604, train/loss=0.978209, validation/accuracy=0.68698, validation/loss=1.35632, validation/num_examples=50000
I0307 19:11:15.188618 140286709720832 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.961839199066162, loss=2.3928403854370117
I0307 19:13:38.204886 140286718113536 logging_writer.py:48] [120000] global_step=120000, grad_norm=4.263631343841553, loss=2.33408784866333
I0307 19:15:59.531949 140286709720832 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.540234088897705, loss=2.41518497467041
I0307 19:18:21.352225 140286718113536 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.691318035125732, loss=2.369114398956299
I0307 19:19:41.024204 140441807221952 spec.py:321] Evaluating on the training split.
I0307 19:19:52.160807 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 19:20:12.212787 140441807221952 spec.py:349] Evaluating on the test split.
I0307 19:20:13.935992 140441807221952 submission_runner.py:469] Time since start: 65949.44s, 	Step: 120257, 	{'train/accuracy': 0.759207546710968, 'train/loss': 1.0321961641311646, 'validation/accuracy': 0.6893399953842163, 'validation/loss': 1.3460503816604614, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.0130231380462646, 'test/num_examples': 10000, 'score': 61283.52868890762, 'total_duration': 65949.44031786919, 'accumulated_submission_time': 61283.52868890762, 'accumulated_eval_time': 4637.501240730286, 'accumulated_logging_time': 15.185400485992432}
I0307 19:20:14.007175 140286709720832 logging_writer.py:48] [120257] accumulated_eval_time=4637.5, accumulated_logging_time=15.1854, accumulated_submission_time=61283.5, global_step=120257, preemption_count=0, score=61283.5, test/accuracy=0.5569, test/loss=2.01302, test/num_examples=10000, total_duration=65949.4, train/accuracy=0.759208, train/loss=1.0322, validation/accuracy=0.68934, validation/loss=1.34605, validation/num_examples=50000
I0307 19:21:03.344589 140286718113536 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.001089096069336, loss=2.3612656593322754
I0307 19:24:54.919365 140286709720832 logging_writer.py:48] [120400] global_step=120400, grad_norm=4.126173496246338, loss=2.3215231895446777
I0307 19:28:45.918287 140441807221952 spec.py:321] Evaluating on the training split.
I0307 19:28:56.345606 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 19:29:20.664426 140441807221952 spec.py:349] Evaluating on the test split.
I0307 19:29:22.420892 140441807221952 submission_runner.py:469] Time since start: 66497.93s, 	Step: 120456, 	{'train/accuracy': 0.7532684803009033, 'train/loss': 1.061436653137207, 'validation/accuracy': 0.6860600113868713, 'validation/loss': 1.3529579639434814, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 2.0226051807403564, 'test/num_examples': 10000, 'score': 61795.408885240555, 'total_duration': 66497.92521595955, 'accumulated_submission_time': 61795.408885240555, 'accumulated_eval_time': 4674.003812074661, 'accumulated_logging_time': 15.26399850845337}
I0307 19:29:22.443875 140286718113536 logging_writer.py:48] [120456] accumulated_eval_time=4674, accumulated_logging_time=15.264, accumulated_submission_time=61795.4, global_step=120456, preemption_count=0, score=61795.4, test/accuracy=0.5615, test/loss=2.02261, test/num_examples=10000, total_duration=66497.9, train/accuracy=0.753268, train/loss=1.06144, validation/accuracy=0.68606, validation/loss=1.35296, validation/num_examples=50000
I0307 19:32:11.338485 140286709720832 logging_writer.py:48] [120500] global_step=120500, grad_norm=4.663577556610107, loss=2.41937518119812
I0307 19:37:56.485659 140441807221952 spec.py:321] Evaluating on the training split.
I0307 19:38:07.091619 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 19:38:29.303894 140441807221952 spec.py:349] Evaluating on the test split.
I0307 19:38:31.061738 140441807221952 submission_runner.py:469] Time since start: 67046.57s, 	Step: 120582, 	{'train/accuracy': 0.7522122263908386, 'train/loss': 1.0544838905334473, 'validation/accuracy': 0.6832199692726135, 'validation/loss': 1.3549047708511353, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 1.9933422803878784, 'test/num_examples': 10000, 'score': 62309.38175058365, 'total_duration': 67046.56606912613, 'accumulated_submission_time': 62309.38175058365, 'accumulated_eval_time': 4708.579865694046, 'accumulated_logging_time': 15.342258930206299}
I0307 19:38:31.085243 140286718113536 logging_writer.py:48] [120582] accumulated_eval_time=4708.58, accumulated_logging_time=15.3423, accumulated_submission_time=62309.4, global_step=120582, preemption_count=0, score=62309.4, test/accuracy=0.5582, test/loss=1.99334, test/num_examples=10000, total_duration=67046.6, train/accuracy=0.752212, train/loss=1.05448, validation/accuracy=0.68322, validation/loss=1.3549, validation/num_examples=50000
I0307 19:39:28.044199 140286709720832 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.8834004402160645, loss=2.405172109603882
I0307 19:45:10.330291 140286718113536 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.9560725688934326, loss=2.4018912315368652
I0307 19:47:01.346779 140441807221952 spec.py:321] Evaluating on the training split.
I0307 19:47:12.110515 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 19:47:33.416825 140441807221952 spec.py:349] Evaluating on the test split.
I0307 19:47:35.173403 140441807221952 submission_runner.py:469] Time since start: 67590.68s, 	Step: 120753, 	{'train/accuracy': 0.7425661683082581, 'train/loss': 1.1025018692016602, 'validation/accuracy': 0.681439995765686, 'validation/loss': 1.3880294561386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.076122283935547, 'test/num_examples': 10000, 'score': 62819.61688256264, 'total_duration': 67590.67770910263, 'accumulated_submission_time': 62819.61688256264, 'accumulated_eval_time': 4742.406438112259, 'accumulated_logging_time': 15.373538494110107}
I0307 19:47:35.198611 140286709720832 logging_writer.py:48] [120753] accumulated_eval_time=4742.41, accumulated_logging_time=15.3735, accumulated_submission_time=62819.6, global_step=120753, preemption_count=0, score=62819.6, test/accuracy=0.5461, test/loss=2.07612, test/num_examples=10000, total_duration=67590.7, train/accuracy=0.742566, train/loss=1.1025, validation/accuracy=0.68144, validation/loss=1.38803, validation/num_examples=50000
I0307 19:48:57.898049 140286718113536 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.250900745391846, loss=2.536637783050537
I0307 19:52:30.222922 140286709720832 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.100988388061523, loss=2.3632917404174805
I0307 19:54:54.032343 140286718113536 logging_writer.py:48] [121000] global_step=121000, grad_norm=4.705892562866211, loss=2.316869020462036
I0307 19:56:06.024318 140441807221952 spec.py:321] Evaluating on the training split.
I0307 19:56:17.402123 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 19:56:38.244127 140441807221952 spec.py:349] Evaluating on the test split.
I0307 19:56:39.979152 140441807221952 submission_runner.py:469] Time since start: 68135.48s, 	Step: 121068, 	{'train/accuracy': 0.7512356638908386, 'train/loss': 1.0647872686386108, 'validation/accuracy': 0.6884199976921082, 'validation/loss': 1.3385746479034424, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 1.9781147241592407, 'test/num_examples': 10000, 'score': 63330.39852619171, 'total_duration': 68135.48347973824, 'accumulated_submission_time': 63330.39852619171, 'accumulated_eval_time': 4776.361238956451, 'accumulated_logging_time': 15.406874179840088}
I0307 19:56:40.073535 140286709720832 logging_writer.py:48] [121068] accumulated_eval_time=4776.36, accumulated_logging_time=15.4069, accumulated_submission_time=63330.4, global_step=121068, preemption_count=0, score=63330.4, test/accuracy=0.5644, test/loss=1.97811, test/num_examples=10000, total_duration=68135.5, train/accuracy=0.751236, train/loss=1.06479, validation/accuracy=0.68842, validation/loss=1.33857, validation/num_examples=50000
I0307 19:57:01.890817 140286718113536 logging_writer.py:48] [121100] global_step=121100, grad_norm=4.429792881011963, loss=2.3808517456054688
I0307 19:58:52.593139 140286709720832 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.899110794067383, loss=2.413468837738037
I0307 20:00:40.907730 140286718113536 logging_writer.py:48] [121300] global_step=121300, grad_norm=4.3826518058776855, loss=2.3127832412719727
2025-03-07 20:01:28.019411: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:02:30.818996 140286709720832 logging_writer.py:48] [121400] global_step=121400, grad_norm=4.006439208984375, loss=2.286652088165283
I0307 20:04:19.237493 140286718113536 logging_writer.py:48] [121500] global_step=121500, grad_norm=4.474902629852295, loss=2.3442792892456055
I0307 20:05:10.826733 140441807221952 spec.py:321] Evaluating on the training split.
I0307 20:05:22.233762 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 20:05:42.218039 140441807221952 spec.py:349] Evaluating on the test split.
I0307 20:05:43.939072 140441807221952 submission_runner.py:469] Time since start: 68679.44s, 	Step: 121549, 	{'train/accuracy': 0.7716438174247742, 'train/loss': 0.9776421189308167, 'validation/accuracy': 0.6939199566841125, 'validation/loss': 1.3251116275787354, 'validation/num_examples': 50000, 'test/accuracy': 0.5641000270843506, 'test/loss': 1.987453579902649, 'test/num_examples': 10000, 'score': 63841.09016704559, 'total_duration': 68679.44339680672, 'accumulated_submission_time': 63841.09016704559, 'accumulated_eval_time': 4809.473536491394, 'accumulated_logging_time': 15.50875449180603}
I0307 20:05:44.022653 140286709720832 logging_writer.py:48] [121549] accumulated_eval_time=4809.47, accumulated_logging_time=15.5088, accumulated_submission_time=63841.1, global_step=121549, preemption_count=0, score=63841.1, test/accuracy=0.5641, test/loss=1.98745, test/num_examples=10000, total_duration=68679.4, train/accuracy=0.771644, train/loss=0.977642, validation/accuracy=0.69392, validation/loss=1.32511, validation/num_examples=50000
I0307 20:06:29.594076 140286718113536 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.23592472076416, loss=2.2997140884399414
I0307 20:08:17.730285 140286709720832 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.556027412414551, loss=2.35878849029541
I0307 20:10:05.329337 140286718113536 logging_writer.py:48] [121800] global_step=121800, grad_norm=4.263605117797852, loss=2.389286756515503
I0307 20:11:53.189690 140286709720832 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.6047468185424805, loss=2.4714138507843018
I0307 20:13:41.160457 140286718113536 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.104372024536133, loss=2.3040554523468018
I0307 20:14:14.632094 140441807221952 spec.py:321] Evaluating on the training split.
I0307 20:14:26.097921 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 20:14:45.725002 140441807221952 spec.py:349] Evaluating on the test split.
I0307 20:14:47.468434 140441807221952 submission_runner.py:469] Time since start: 69222.97s, 	Step: 122032, 	{'train/accuracy': 0.7590680718421936, 'train/loss': 1.01444411277771, 'validation/accuracy': 0.6910799741744995, 'validation/loss': 1.3137145042419434, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 1.9690567255020142, 'test/num_examples': 10000, 'score': 64351.63581061363, 'total_duration': 69222.97276234627, 'accumulated_submission_time': 64351.63581061363, 'accumulated_eval_time': 4842.309837818146, 'accumulated_logging_time': 15.599915742874146}
I0307 20:14:47.524685 140286709720832 logging_writer.py:48] [122032] accumulated_eval_time=4842.31, accumulated_logging_time=15.5999, accumulated_submission_time=64351.6, global_step=122032, preemption_count=0, score=64351.6, test/accuracy=0.5628, test/loss=1.96906, test/num_examples=10000, total_duration=69223, train/accuracy=0.759068, train/loss=1.01444, validation/accuracy=0.69108, validation/loss=1.31371, validation/num_examples=50000
I0307 20:15:50.079897 140286718113536 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.447225570678711, loss=2.263979434967041
I0307 20:17:36.839548 140286709720832 logging_writer.py:48] [122200] global_step=122200, grad_norm=4.7866058349609375, loss=2.4710018634796143
I0307 20:19:24.637119 140286718113536 logging_writer.py:48] [122300] global_step=122300, grad_norm=4.111833572387695, loss=2.357677459716797
I0307 20:21:11.969179 140286709720832 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.863393306732178, loss=2.412065267562866
I0307 20:22:59.681995 140286718113536 logging_writer.py:48] [122500] global_step=122500, grad_norm=4.504353046417236, loss=2.4527082443237305
I0307 20:23:17.999555 140441807221952 spec.py:321] Evaluating on the training split.
I0307 20:23:28.904823 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 20:23:51.293505 140441807221952 spec.py:349] Evaluating on the test split.
I0307 20:23:53.000508 140441807221952 submission_runner.py:469] Time since start: 69768.50s, 	Step: 122518, 	{'train/accuracy': 0.76171875, 'train/loss': 1.0199787616729736, 'validation/accuracy': 0.6930599808692932, 'validation/loss': 1.3248945474624634, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9553985595703125, 'test/num_examples': 10000, 'score': 64862.038177490234, 'total_duration': 69768.50483179092, 'accumulated_submission_time': 64862.038177490234, 'accumulated_eval_time': 4877.310750961304, 'accumulated_logging_time': 15.674070358276367}
I0307 20:23:53.055923 140286709720832 logging_writer.py:48] [122518] accumulated_eval_time=4877.31, accumulated_logging_time=15.6741, accumulated_submission_time=64862, global_step=122518, preemption_count=0, score=64862, test/accuracy=0.5727, test/loss=1.9554, test/num_examples=10000, total_duration=69768.5, train/accuracy=0.761719, train/loss=1.01998, validation/accuracy=0.69306, validation/loss=1.32489, validation/num_examples=50000
2025-03-07 20:25:07.724825: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:25:14.781773 140286718113536 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.073831558227539, loss=2.3705825805664062
I0307 20:27:01.718304 140286709720832 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.059834003448486, loss=2.4074971675872803
I0307 20:28:49.900625 140286718113536 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.831659317016602, loss=2.4536373615264893
I0307 20:30:37.924918 140286709720832 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.295132160186768, loss=2.3114702701568604
I0307 20:32:23.417083 140441807221952 spec.py:321] Evaluating on the training split.
I0307 20:32:34.662903 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 20:32:57.279260 140441807221952 spec.py:349] Evaluating on the test split.
I0307 20:32:59.027711 140441807221952 submission_runner.py:469] Time since start: 70314.53s, 	Step: 122999, 	{'train/accuracy': 0.7655851244926453, 'train/loss': 1.011163592338562, 'validation/accuracy': 0.6908800005912781, 'validation/loss': 1.3551948070526123, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9953523874282837, 'test/num_examples': 10000, 'score': 65372.33504295349, 'total_duration': 70314.53204369545, 'accumulated_submission_time': 65372.33504295349, 'accumulated_eval_time': 4912.92134976387, 'accumulated_logging_time': 15.737920761108398}
I0307 20:32:59.127243 140286718113536 logging_writer.py:48] [122999] accumulated_eval_time=4912.92, accumulated_logging_time=15.7379, accumulated_submission_time=65372.3, global_step=122999, preemption_count=0, score=65372.3, test/accuracy=0.5661, test/loss=1.99535, test/num_examples=10000, total_duration=70314.5, train/accuracy=0.765585, train/loss=1.01116, validation/accuracy=0.69088, validation/loss=1.35519, validation/num_examples=50000
I0307 20:32:59.840511 140286709720832 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.596992015838623, loss=2.3921127319335938
I0307 20:34:38.106597 140286718113536 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.964184284210205, loss=2.389995574951172
I0307 20:36:25.447939 140286709720832 logging_writer.py:48] [123200] global_step=123200, grad_norm=4.063697338104248, loss=2.3449814319610596
I0307 20:38:12.988130 140286718113536 logging_writer.py:48] [123300] global_step=123300, grad_norm=4.22396993637085, loss=2.340210437774658
I0307 20:40:00.806010 140286709720832 logging_writer.py:48] [123400] global_step=123400, grad_norm=4.036313056945801, loss=2.292703628540039
I0307 20:41:30.469590 140441807221952 spec.py:321] Evaluating on the training split.
I0307 20:41:41.315846 140441807221952 spec.py:333] Evaluating on the validation split.
I0307 20:42:07.967144 140441807221952 spec.py:349] Evaluating on the test split.
I0307 20:42:09.738005 140441807221952 submission_runner.py:469] Time since start: 70865.24s, 	Step: 123463, 	{'train/accuracy': 0.7614795565605164, 'train/loss': 1.0319074392318726, 'validation/accuracy': 0.6915000081062317, 'validation/loss': 1.3308998346328735, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 1.982467532157898, 'test/num_examples': 10000, 'score': 65883.61578512192, 'total_duration': 70865.24232411385, 'accumulated_submission_time': 65883.61578512192, 'accumulated_eval_time': 4952.189725399017, 'accumulated_logging_time': 15.84632682800293}
I0307 20:42:09.843977 140286718113536 logging_writer.py:48] [123463] accumulated_eval_time=4952.19, accumulated_logging_time=15.8463, accumulated_submission_time=65883.6, global_step=123463, preemption_count=0, score=65883.6, test/accuracy=0.5618, test/loss=1.98247, test/num_examples=10000, total_duration=70865.2, train/accuracy=0.76148, train/loss=1.03191, validation/accuracy=0.6915, validation/loss=1.3309, validation/num_examples=50000
I0307 20:43:32.143456 140286709720832 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.516541481018066, loss=2.3951897621154785
I0307 20:47:05.570198 140286718113536 logging_writer.py:48] [123600] global_step=123600, grad_norm=4.553116798400879, loss=2.3939788341522217
I0307 20:50:40.769747 140286709720832 logging_writer.py:48] [123700] global_step=123700, preemption_count=0, score=66394.5
I0307 20:50:42.444606 140441807221952 submission_runner.py:646] Tuning trial 1/5
I0307 20:50:42.453370 140441807221952 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 20:50:42.457613 140441807221952 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008769132546149194, 'train/loss': 6.912663459777832, 'validation/accuracy': 0.0010599999222904444, 'validation/loss': 6.912087917327881, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.912038803100586, 'test/num_examples': 10000, 'score': 59.687002420425415, 'total_duration': 149.34441924095154, 'accumulated_submission_time': 59.687002420425415, 'accumulated_eval_time': 89.65718460083008, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1375, {'train/accuracy': 0.06507094949483871, 'train/loss': 5.495771408081055, 'validation/accuracy': 0.056619998067617416, 'validation/loss': 5.574432849884033, 'validation/num_examples': 50000, 'test/accuracy': 0.04050000011920929, 'test/loss': 5.766232490539551, 'test/num_examples': 10000, 'score': 569.7683362960815, 'total_duration': 699.4703891277313, 'accumulated_submission_time': 569.7683362960815, 'accumulated_eval_time': 129.47345995903015, 'accumulated_logging_time': 0.04252290725708008, 'global_step': 1375, 'preemption_count': 0}), (2723, {'train/accuracy': 0.16027583181858063, 'train/loss': 4.423295021057129, 'validation/accuracy': 0.13835999369621277, 'validation/loss': 4.570390701293945, 'validation/num_examples': 50000, 'test/accuracy': 0.09890000522136688, 'test/loss': 4.93082857131958, 'test/num_examples': 10000, 'score': 1079.7922539710999, 'total_duration': 1246.979810476303, 'accumulated_submission_time': 1079.7922539710999, 'accumulated_eval_time': 166.76930689811707, 'accumulated_logging_time': 0.07344627380371094, 'global_step': 2723, 'preemption_count': 0}), (4061, {'train/accuracy': 0.2468710094690323, 'train/loss': 3.7364847660064697, 'validation/accuracy': 0.21823999285697937, 'validation/loss': 3.923434019088745, 'validation/num_examples': 50000, 'test/accuracy': 0.1648000031709671, 'test/loss': 4.43960428237915, 'test/num_examples': 10000, 'score': 1589.8382947444916, 'total_duration': 1796.4788868427277, 'accumulated_submission_time': 1589.8382947444916, 'accumulated_eval_time': 206.05336236953735, 'accumulated_logging_time': 0.11658859252929688, 'global_step': 4061, 'preemption_count': 0}), (5402, {'train/accuracy': 0.3337252736091614, 'train/loss': 3.171891450881958, 'validation/accuracy': 0.2961199879646301, 'validation/loss': 3.382331371307373, 'validation/num_examples': 50000, 'test/accuracy': 0.21650001406669617, 'test/loss': 3.9546666145324707, 'test/num_examples': 10000, 'score': 2099.9575295448303, 'total_duration': 2344.214997768402, 'accumulated_submission_time': 2099.9575295448303, 'accumulated_eval_time': 243.47290802001953, 'accumulated_logging_time': 0.18411469459533691, 'global_step': 5402, 'preemption_count': 0}), (6740, {'train/accuracy': 0.41338488459587097, 'train/loss': 2.7279350757598877, 'validation/accuracy': 0.3744199872016907, 'validation/loss': 2.935438394546509, 'validation/num_examples': 50000, 'test/accuracy': 0.2815000116825104, 'test/loss': 3.5451743602752686, 'test/num_examples': 10000, 'score': 2610.165326356888, 'total_duration': 2891.051376104355, 'accumulated_submission_time': 2610.165326356888, 'accumulated_eval_time': 279.9653720855713, 'accumulated_logging_time': 0.20302915573120117, 'global_step': 6740, 'preemption_count': 0}), (8078, {'train/accuracy': 0.46257174015045166, 'train/loss': 2.4476497173309326, 'validation/accuracy': 0.4191199839115143, 'validation/loss': 2.6637425422668457, 'validation/num_examples': 50000, 'test/accuracy': 0.31940001249313354, 'test/loss': 3.3245441913604736, 'test/num_examples': 10000, 'score': 3119.9823246002197, 'total_duration': 3437.807263612747, 'accumulated_submission_time': 3119.9823246002197, 'accumulated_eval_time': 316.70049834251404, 'accumulated_logging_time': 0.28307342529296875, 'global_step': 8078, 'preemption_count': 0}), (9416, {'train/accuracy': 0.5102439522743225, 'train/loss': 2.1807687282562256, 'validation/accuracy': 0.4664599895477295, 'validation/loss': 2.418881416320801, 'validation/num_examples': 50000, 'test/accuracy': 0.3565000295639038, 'test/loss': 3.0698392391204834, 'test/num_examples': 10000, 'score': 3629.902883529663, 'total_duration': 3986.3669214248657, 'accumulated_submission_time': 3629.902883529663, 'accumulated_eval_time': 355.1581892967224, 'accumulated_logging_time': 0.34018468856811523, 'global_step': 9416, 'preemption_count': 0}), (10758, {'train/accuracy': 0.5457589030265808, 'train/loss': 2.0431270599365234, 'validation/accuracy': 0.49823999404907227, 'validation/loss': 2.268129587173462, 'validation/num_examples': 50000, 'test/accuracy': 0.3832000195980072, 'test/loss': 2.920487642288208, 'test/num_examples': 10000, 'score': 4139.9532606601715, 'total_duration': 4540.319451332092, 'accumulated_submission_time': 4139.9532606601715, 'accumulated_eval_time': 398.85981941223145, 'accumulated_logging_time': 0.39961886405944824, 'global_step': 10758, 'preemption_count': 0}), (12084, {'train/accuracy': 0.5666852593421936, 'train/loss': 1.9149045944213867, 'validation/accuracy': 0.5190799832344055, 'validation/loss': 2.1563785076141357, 'validation/num_examples': 50000, 'test/accuracy': 0.4027000069618225, 'test/loss': 2.8268380165100098, 'test/num_examples': 10000, 'score': 4649.722097635269, 'total_duration': 5098.629235982895, 'accumulated_submission_time': 4649.722097635269, 'accumulated_eval_time': 447.16163396835327, 'accumulated_logging_time': 0.4916808605194092, 'global_step': 12084, 'preemption_count': 0}), (13409, {'train/accuracy': 0.5757732391357422, 'train/loss': 1.8449188470840454, 'validation/accuracy': 0.5312199592590332, 'validation/loss': 2.069065809249878, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.76556658744812, 'test/num_examples': 10000, 'score': 5159.316724538803, 'total_duration': 5652.29358792305, 'accumulated_submission_time': 5159.316724538803, 'accumulated_eval_time': 490.75188994407654, 'accumulated_logging_time': 0.7742214202880859, 'global_step': 13409, 'preemption_count': 0}), (14744, {'train/accuracy': 0.5953443646430969, 'train/loss': 1.7729371786117554, 'validation/accuracy': 0.5464999675750732, 'validation/loss': 2.009995937347412, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.664354085922241, 'test/num_examples': 10000, 'score': 5669.328103303909, 'total_duration': 6207.592904090881, 'accumulated_submission_time': 5669.328103303909, 'accumulated_eval_time': 535.8162145614624, 'accumulated_logging_time': 0.8554446697235107, 'global_step': 14744, 'preemption_count': 0}), (16077, {'train/accuracy': 0.6067641973495483, 'train/loss': 1.731878638267517, 'validation/accuracy': 0.5576199889183044, 'validation/loss': 1.9619148969650269, 'validation/num_examples': 50000, 'test/accuracy': 0.43380001187324524, 'test/loss': 2.630448579788208, 'test/num_examples': 10000, 'score': 6179.088294744492, 'total_duration': 6763.337777853012, 'accumulated_submission_time': 6179.088294744492, 'accumulated_eval_time': 581.494470834732, 'accumulated_logging_time': 1.0182383060455322, 'global_step': 16077, 'preemption_count': 0}), (17407, {'train/accuracy': 0.6162906289100647, 'train/loss': 1.6918208599090576, 'validation/accuracy': 0.5649999976158142, 'validation/loss': 1.9248992204666138, 'validation/num_examples': 50000, 'test/accuracy': 0.4497000277042389, 'test/loss': 2.5673370361328125, 'test/num_examples': 10000, 'score': 6689.049119949341, 'total_duration': 7314.5429520606995, 'accumulated_submission_time': 6689.049119949341, 'accumulated_eval_time': 622.2929475307465, 'accumulated_logging_time': 1.319702386856079, 'global_step': 17407, 'preemption_count': 0}), (18735, {'train/accuracy': 0.6155332922935486, 'train/loss': 1.6905982494354248, 'validation/accuracy': 0.5715199708938599, 'validation/loss': 1.8928349018096924, 'validation/num_examples': 50000, 'test/accuracy': 0.44440001249313354, 'test/loss': 2.5997352600097656, 'test/num_examples': 10000, 'score': 7198.982068538666, 'total_duration': 7867.1813633441925, 'accumulated_submission_time': 7198.982068538666, 'accumulated_eval_time': 664.7518112659454, 'accumulated_logging_time': 1.4425053596496582, 'global_step': 18735, 'preemption_count': 0}), (20059, {'train/accuracy': 0.6094347834587097, 'train/loss': 1.745519757270813, 'validation/accuracy': 0.5615999698638916, 'validation/loss': 1.9652775526046753, 'validation/num_examples': 50000, 'test/accuracy': 0.44280001521110535, 'test/loss': 2.613088607788086, 'test/num_examples': 10000, 'score': 7709.056797742844, 'total_duration': 8419.71234869957, 'accumulated_submission_time': 7709.056797742844, 'accumulated_eval_time': 707.0166399478912, 'accumulated_logging_time': 1.5013160705566406, 'global_step': 20059, 'preemption_count': 0}), (21387, {'train/accuracy': 0.6323740482330322, 'train/loss': 1.6397472620010376, 'validation/accuracy': 0.5831999778747559, 'validation/loss': 1.8600594997406006, 'validation/num_examples': 50000, 'test/accuracy': 0.46400001645088196, 'test/loss': 2.525916576385498, 'test/num_examples': 10000, 'score': 8218.98792386055, 'total_duration': 8972.79462981224, 'accumulated_submission_time': 8218.98792386055, 'accumulated_eval_time': 749.994124174118, 'accumulated_logging_time': 1.5383195877075195, 'global_step': 21387, 'preemption_count': 0}), (22714, {'train/accuracy': 0.6126434803009033, 'train/loss': 1.6913046836853027, 'validation/accuracy': 0.5663599967956543, 'validation/loss': 1.9140921831130981, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.6022934913635254, 'test/num_examples': 10000, 'score': 8728.7809882164, 'total_duration': 9522.787516832352, 'accumulated_submission_time': 8728.7809882164, 'accumulated_eval_time': 789.8731219768524, 'accumulated_logging_time': 1.7260372638702393, 'global_step': 22714, 'preemption_count': 0}), (24047, {'train/accuracy': 0.6250398755073547, 'train/loss': 1.670770525932312, 'validation/accuracy': 0.5806999802589417, 'validation/loss': 1.8805358409881592, 'validation/num_examples': 50000, 'test/accuracy': 0.4545000195503235, 'test/loss': 2.547241449356079, 'test/num_examples': 10000, 'score': 9238.825613737106, 'total_duration': 10072.258450746536, 'accumulated_submission_time': 9238.825613737106, 'accumulated_eval_time': 829.0590574741364, 'accumulated_logging_time': 1.8356165885925293, 'global_step': 24047, 'preemption_count': 0}), (25378, {'train/accuracy': 0.6317163705825806, 'train/loss': 1.5977365970611572, 'validation/accuracy': 0.5841599702835083, 'validation/loss': 1.8265553712844849, 'validation/num_examples': 50000, 'test/accuracy': 0.4545000195503235, 'test/loss': 2.5185999870300293, 'test/num_examples': 10000, 'score': 9748.862476348877, 'total_duration': 10625.076827526093, 'accumulated_submission_time': 9748.862476348877, 'accumulated_eval_time': 871.6416375637054, 'accumulated_logging_time': 1.8998379707336426, 'global_step': 25378, 'preemption_count': 0}), (26709, {'train/accuracy': 0.6319754123687744, 'train/loss': 1.5863648653030396, 'validation/accuracy': 0.5883399844169617, 'validation/loss': 1.7912288904190063, 'validation/num_examples': 50000, 'test/accuracy': 0.4711000323295593, 'test/loss': 2.4544482231140137, 'test/num_examples': 10000, 'score': 10258.641316652298, 'total_duration': 11180.16995716095, 'accumulated_submission_time': 10258.641316652298, 'accumulated_eval_time': 916.6985991001129, 'accumulated_logging_time': 2.0172619819641113, 'global_step': 26709, 'preemption_count': 0}), (28043, {'train/accuracy': 0.6358418464660645, 'train/loss': 1.5619195699691772, 'validation/accuracy': 0.5920000076293945, 'validation/loss': 1.7784985303878784, 'validation/num_examples': 50000, 'test/accuracy': 0.4733000099658966, 'test/loss': 2.4413235187530518, 'test/num_examples': 10000, 'score': 10768.428705453873, 'total_duration': 11732.971014261246, 'accumulated_submission_time': 10768.428705453873, 'accumulated_eval_time': 959.4324729442596, 'accumulated_logging_time': 2.158151149749756, 'global_step': 28043, 'preemption_count': 0}), (29372, {'train/accuracy': 0.630879282951355, 'train/loss': 1.5892046689987183, 'validation/accuracy': 0.5872799754142761, 'validation/loss': 1.80474054813385, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.5089316368103027, 'test/num_examples': 10000, 'score': 11278.280355215073, 'total_duration': 12285.77416563034, 'accumulated_submission_time': 11278.280355215073, 'accumulated_eval_time': 1002.1133708953857, 'accumulated_logging_time': 2.294841766357422, 'global_step': 29372, 'preemption_count': 0}), (30704, {'train/accuracy': 0.6367586255073547, 'train/loss': 1.5946401357650757, 'validation/accuracy': 0.5898199677467346, 'validation/loss': 1.814432144165039, 'validation/num_examples': 50000, 'test/accuracy': 0.4693000316619873, 'test/loss': 2.4608688354492188, 'test/num_examples': 10000, 'score': 11788.173129558563, 'total_duration': 12835.88332104683, 'accumulated_submission_time': 11788.173129558563, 'accumulated_eval_time': 1042.0749201774597, 'accumulated_logging_time': 2.4168710708618164, 'global_step': 30704, 'preemption_count': 0}), (32037, {'train/accuracy': 0.6462053656578064, 'train/loss': 1.5323504209518433, 'validation/accuracy': 0.5992400050163269, 'validation/loss': 1.7477208375930786, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.4496960639953613, 'test/num_examples': 10000, 'score': 12297.967391490936, 'total_duration': 13389.450607061386, 'accumulated_submission_time': 12297.967391490936, 'accumulated_eval_time': 1085.6012825965881, 'accumulated_logging_time': 2.5240790843963623, 'global_step': 32037, 'preemption_count': 0}), (33370, {'train/accuracy': 0.6450095772743225, 'train/loss': 1.5158864259719849, 'validation/accuracy': 0.601099967956543, 'validation/loss': 1.7310526371002197, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.4251773357391357, 'test/num_examples': 10000, 'score': 12807.780698776245, 'total_duration': 13943.194332838058, 'accumulated_submission_time': 12807.780698776245, 'accumulated_eval_time': 1129.2802798748016, 'accumulated_logging_time': 2.6369564533233643, 'global_step': 33370, 'preemption_count': 0}), (34704, {'train/accuracy': 0.6413025856018066, 'train/loss': 1.5720946788787842, 'validation/accuracy': 0.5981799960136414, 'validation/loss': 1.7655549049377441, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.4356882572174072, 'test/num_examples': 10000, 'score': 13317.845072031021, 'total_duration': 14497.189989566803, 'accumulated_submission_time': 13317.845072031021, 'accumulated_eval_time': 1172.8975405693054, 'accumulated_logging_time': 2.8115742206573486, 'global_step': 34704, 'preemption_count': 0}), (36034, {'train/accuracy': 0.650809109210968, 'train/loss': 1.5241730213165283, 'validation/accuracy': 0.6029999852180481, 'validation/loss': 1.7280184030532837, 'validation/num_examples': 50000, 'test/accuracy': 0.47700002789497375, 'test/loss': 2.4105193614959717, 'test/num_examples': 10000, 'score': 13827.63047003746, 'total_duration': 15046.347669839859, 'accumulated_submission_time': 13827.63047003746, 'accumulated_eval_time': 1212.0197942256927, 'accumulated_logging_time': 2.9248199462890625, 'global_step': 36034, 'preemption_count': 0}), (37365, {'train/accuracy': 0.6429169178009033, 'train/loss': 1.5372689962387085, 'validation/accuracy': 0.6027399897575378, 'validation/loss': 1.7396049499511719, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.3982303142547607, 'test/num_examples': 10000, 'score': 14337.376689195633, 'total_duration': 15594.383026599884, 'accumulated_submission_time': 14337.376689195633, 'accumulated_eval_time': 1250.0323853492737, 'accumulated_logging_time': 3.0655133724212646, 'global_step': 37365, 'preemption_count': 0}), (38697, {'train/accuracy': 0.6528618931770325, 'train/loss': 1.5006095170974731, 'validation/accuracy': 0.6132799983024597, 'validation/loss': 1.6886672973632812, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.3485679626464844, 'test/num_examples': 10000, 'score': 14847.254745483398, 'total_duration': 16144.817528009415, 'accumulated_submission_time': 14847.254745483398, 'accumulated_eval_time': 1290.2636535167694, 'accumulated_logging_time': 3.2474365234375, 'global_step': 38697, 'preemption_count': 0}), (40025, {'train/accuracy': 0.6515266299247742, 'train/loss': 1.521810531616211, 'validation/accuracy': 0.6094399690628052, 'validation/loss': 1.7248657941818237, 'validation/num_examples': 50000, 'test/accuracy': 0.4905000329017639, 'test/loss': 2.3783953189849854, 'test/num_examples': 10000, 'score': 15357.241574287415, 'total_duration': 16688.94926762581, 'accumulated_submission_time': 15357.241574287415, 'accumulated_eval_time': 1324.1204161643982, 'accumulated_logging_time': 3.393284320831299, 'global_step': 40025, 'preemption_count': 0}), (41357, {'train/accuracy': 0.6451091766357422, 'train/loss': 1.5666347742080688, 'validation/accuracy': 0.6051200032234192, 'validation/loss': 1.7515000104904175, 'validation/num_examples': 50000, 'test/accuracy': 0.4847000241279602, 'test/loss': 2.42962646484375, 'test/num_examples': 10000, 'score': 15867.194030761719, 'total_duration': 17244.031442165375, 'accumulated_submission_time': 15867.194030761719, 'accumulated_eval_time': 1368.9319858551025, 'accumulated_logging_time': 3.5648722648620605, 'global_step': 41357, 'preemption_count': 0}), (42683, {'train/accuracy': 0.6377550959587097, 'train/loss': 1.5750277042388916, 'validation/accuracy': 0.5985400080680847, 'validation/loss': 1.7739797830581665, 'validation/num_examples': 50000, 'test/accuracy': 0.47530001401901245, 'test/loss': 2.4557299613952637, 'test/num_examples': 10000, 'score': 16377.078849554062, 'total_duration': 17794.2056555748, 'accumulated_submission_time': 16377.078849554062, 'accumulated_eval_time': 1408.9450025558472, 'accumulated_logging_time': 3.6987690925598145, 'global_step': 42683, 'preemption_count': 0}), (44016, {'train/accuracy': 0.6437141299247742, 'train/loss': 1.5236297845840454, 'validation/accuracy': 0.6046199798583984, 'validation/loss': 1.7220348119735718, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.3902587890625, 'test/num_examples': 10000, 'score': 16887.092682361603, 'total_duration': 18340.322093486786, 'accumulated_submission_time': 16887.092682361603, 'accumulated_eval_time': 1444.7869918346405, 'accumulated_logging_time': 3.818232297897339, 'global_step': 44016, 'preemption_count': 0}), (45351, {'train/accuracy': 0.6461654901504517, 'train/loss': 1.5231139659881592, 'validation/accuracy': 0.606719970703125, 'validation/loss': 1.7186791896820068, 'validation/num_examples': 50000, 'test/accuracy': 0.48520001769065857, 'test/loss': 2.3695437908172607, 'test/num_examples': 10000, 'score': 17396.886875629425, 'total_duration': 18889.527441740036, 'accumulated_submission_time': 17396.886875629425, 'accumulated_eval_time': 1483.9134411811829, 'accumulated_logging_time': 3.961378574371338, 'global_step': 45351, 'preemption_count': 0}), (46683, {'train/accuracy': 0.654715359210968, 'train/loss': 1.4919129610061646, 'validation/accuracy': 0.6101599931716919, 'validation/loss': 1.697442650794983, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.3467772006988525, 'test/num_examples': 10000, 'score': 17906.682953596115, 'total_duration': 19439.752460956573, 'accumulated_submission_time': 17906.682953596115, 'accumulated_eval_time': 1524.047131538391, 'accumulated_logging_time': 4.112897872924805, 'global_step': 46683, 'preemption_count': 0}), (48012, {'train/accuracy': 0.6553930044174194, 'train/loss': 1.4746084213256836, 'validation/accuracy': 0.6146799921989441, 'validation/loss': 1.6603604555130005, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.3318798542022705, 'test/num_examples': 10000, 'score': 18416.766710042953, 'total_duration': 19987.31906557083, 'accumulated_submission_time': 18416.766710042953, 'accumulated_eval_time': 1561.2680158615112, 'accumulated_logging_time': 4.236192941665649, 'global_step': 48012, 'preemption_count': 0}), (49341, {'train/accuracy': 0.6593989133834839, 'train/loss': 1.4982556104660034, 'validation/accuracy': 0.614799976348877, 'validation/loss': 1.7008064985275269, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3613393306732178, 'test/num_examples': 10000, 'score': 18926.852604150772, 'total_duration': 20537.86784219742, 'accumulated_submission_time': 18926.852604150772, 'accumulated_eval_time': 1601.4811866283417, 'accumulated_logging_time': 4.3443663120269775, 'global_step': 49341, 'preemption_count': 0}), (50664, {'train/accuracy': 0.6714165806770325, 'train/loss': 1.4457722902297974, 'validation/accuracy': 0.6180199980735779, 'validation/loss': 1.6856416463851929, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.351414680480957, 'test/num_examples': 10000, 'score': 19436.79271888733, 'total_duration': 21088.564143180847, 'accumulated_submission_time': 19436.79271888733, 'accumulated_eval_time': 1641.9728224277496, 'accumulated_logging_time': 4.463685750961304, 'global_step': 50664, 'preemption_count': 0}), (51992, {'train/accuracy': 0.6884565949440002, 'train/loss': 1.3327465057373047, 'validation/accuracy': 0.6139199733734131, 'validation/loss': 1.6658188104629517, 'validation/num_examples': 50000, 'test/accuracy': 0.4914000332355499, 'test/loss': 2.347844362258911, 'test/num_examples': 10000, 'score': 19946.618819713593, 'total_duration': 21638.305015087128, 'accumulated_submission_time': 19946.618819713593, 'accumulated_eval_time': 1681.6283202171326, 'accumulated_logging_time': 4.586970090866089, 'global_step': 51992, 'preemption_count': 0}), (53321, {'train/accuracy': 0.7049385905265808, 'train/loss': 1.3262349367141724, 'validation/accuracy': 0.6204400062561035, 'validation/loss': 1.6946985721588135, 'validation/num_examples': 50000, 'test/accuracy': 0.4864000082015991, 'test/loss': 2.383591890335083, 'test/num_examples': 10000, 'score': 20456.7491106987, 'total_duration': 22187.339588165283, 'accumulated_submission_time': 20456.7491106987, 'accumulated_eval_time': 1720.2665786743164, 'accumulated_logging_time': 4.716232776641846, 'global_step': 53321, 'preemption_count': 0}), (54652, {'train/accuracy': 0.7073301672935486, 'train/loss': 1.2522268295288086, 'validation/accuracy': 0.6242200136184692, 'validation/loss': 1.6253339052200317, 'validation/num_examples': 50000, 'test/accuracy': 0.49400001764297485, 'test/loss': 2.3052542209625244, 'test/num_examples': 10000, 'score': 20966.58277463913, 'total_duration': 22736.360019207, 'accumulated_submission_time': 20966.58277463913, 'accumulated_eval_time': 1759.1963891983032, 'accumulated_logging_time': 4.834095001220703, 'global_step': 54652, 'preemption_count': 0}), (55975, {'train/accuracy': 0.7026267647743225, 'train/loss': 1.2670122385025024, 'validation/accuracy': 0.6241199970245361, 'validation/loss': 1.6346774101257324, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.31564998626709, 'test/num_examples': 10000, 'score': 21476.535758018494, 'total_duration': 23283.65671491623, 'accumulated_submission_time': 21476.535758018494, 'accumulated_eval_time': 1796.1923732757568, 'accumulated_logging_time': 5.038048267364502, 'global_step': 55975, 'preemption_count': 0}), (57304, {'train/accuracy': 0.6915856003761292, 'train/loss': 1.3624995946884155, 'validation/accuracy': 0.6171799898147583, 'validation/loss': 1.6947439908981323, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.380115270614624, 'test/num_examples': 10000, 'score': 21986.57546186447, 'total_duration': 23830.341426849365, 'accumulated_submission_time': 21986.57546186447, 'accumulated_eval_time': 1832.5404443740845, 'accumulated_logging_time': 5.195877552032471, 'global_step': 57304, 'preemption_count': 0}), (58634, {'train/accuracy': 0.6810028553009033, 'train/loss': 1.4068670272827148, 'validation/accuracy': 0.6153799891471863, 'validation/loss': 1.7070156335830688, 'validation/num_examples': 50000, 'test/accuracy': 0.48740002512931824, 'test/loss': 2.3978285789489746, 'test/num_examples': 10000, 'score': 22496.357810735703, 'total_duration': 24375.90441966057, 'accumulated_submission_time': 22496.357810735703, 'accumulated_eval_time': 1868.0921771526337, 'accumulated_logging_time': 5.289499044418335, 'global_step': 58634, 'preemption_count': 0}), (59956, {'train/accuracy': 0.6892139315605164, 'train/loss': 1.3383091688156128, 'validation/accuracy': 0.6202799677848816, 'validation/loss': 1.6595367193222046, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.3100039958953857, 'test/num_examples': 10000, 'score': 23006.12411880493, 'total_duration': 24924.65745139122, 'accumulated_submission_time': 23006.12411880493, 'accumulated_eval_time': 1906.775414466858, 'accumulated_logging_time': 5.4525697231292725, 'global_step': 59956, 'preemption_count': 0}), (61282, {'train/accuracy': 0.6900709271430969, 'train/loss': 1.3561550378799438, 'validation/accuracy': 0.6269999742507935, 'validation/loss': 1.6514174938201904, 'validation/num_examples': 50000, 'test/accuracy': 0.5001000165939331, 'test/loss': 2.339822292327881, 'test/num_examples': 10000, 'score': 23515.868244171143, 'total_duration': 25478.479033708572, 'accumulated_submission_time': 23515.868244171143, 'accumulated_eval_time': 1950.537579536438, 'accumulated_logging_time': 5.629275560379028, 'global_step': 61282, 'preemption_count': 0}), (62607, {'train/accuracy': 0.6828364133834839, 'train/loss': 1.3486058712005615, 'validation/accuracy': 0.6195999979972839, 'validation/loss': 1.648579478263855, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.3349251747131348, 'test/num_examples': 10000, 'score': 24025.852508068085, 'total_duration': 26025.462947130203, 'accumulated_submission_time': 24025.852508068085, 'accumulated_eval_time': 1987.2787127494812, 'accumulated_logging_time': 5.746744155883789, 'global_step': 62607, 'preemption_count': 0}), (63932, {'train/accuracy': 0.6830955147743225, 'train/loss': 1.3851529359817505, 'validation/accuracy': 0.6187399625778198, 'validation/loss': 1.6893764734268188, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.3248541355133057, 'test/num_examples': 10000, 'score': 24535.916409254074, 'total_duration': 26575.995939731598, 'accumulated_submission_time': 24535.916409254074, 'accumulated_eval_time': 2027.4737813472748, 'accumulated_logging_time': 5.882742881774902, 'global_step': 63932, 'preemption_count': 0}), (65259, {'train/accuracy': 0.6919044852256775, 'train/loss': 1.3145473003387451, 'validation/accuracy': 0.6337999701499939, 'validation/loss': 1.5892457962036133, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.269313335418701, 'test/num_examples': 10000, 'score': 25046.002423286438, 'total_duration': 27121.405230998993, 'accumulated_submission_time': 25046.002423286438, 'accumulated_eval_time': 2062.5365097522736, 'accumulated_logging_time': 6.0029826164245605, 'global_step': 65259, 'preemption_count': 0}), (66583, {'train/accuracy': 0.68558669090271, 'train/loss': 1.3441452980041504, 'validation/accuracy': 0.6242799758911133, 'validation/loss': 1.6398649215698242, 'validation/num_examples': 50000, 'test/accuracy': 0.5005000233650208, 'test/loss': 2.3072714805603027, 'test/num_examples': 10000, 'score': 25555.77280497551, 'total_duration': 27676.866458654404, 'accumulated_submission_time': 25555.77280497551, 'accumulated_eval_time': 2107.9743111133575, 'accumulated_logging_time': 6.117950677871704, 'global_step': 66583, 'preemption_count': 0}), (67910, {'train/accuracy': 0.678730845451355, 'train/loss': 1.3553584814071655, 'validation/accuracy': 0.6238200068473816, 'validation/loss': 1.6288461685180664, 'validation/num_examples': 50000, 'test/accuracy': 0.49550002813339233, 'test/loss': 2.3253026008605957, 'test/num_examples': 10000, 'score': 26065.72079849243, 'total_duration': 28228.902816295624, 'accumulated_submission_time': 26065.72079849243, 'accumulated_eval_time': 2149.7980675697327, 'accumulated_logging_time': 6.2447285652160645, 'global_step': 67910, 'preemption_count': 0}), (69233, {'train/accuracy': 0.6938974857330322, 'train/loss': 1.3298683166503906, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.611121416091919, 'validation/num_examples': 50000, 'test/accuracy': 0.5121000409126282, 'test/loss': 2.2512662410736084, 'test/num_examples': 10000, 'score': 26575.613506555557, 'total_duration': 28776.34026670456, 'accumulated_submission_time': 26575.613506555557, 'accumulated_eval_time': 2187.0019011497498, 'accumulated_logging_time': 6.442857027053833, 'global_step': 69233, 'preemption_count': 0}), (70541, {'train/accuracy': 0.6960698366165161, 'train/loss': 1.3099411725997925, 'validation/accuracy': 0.6351799964904785, 'validation/loss': 1.5836288928985596, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.235600709915161, 'test/num_examples': 10000, 'score': 27085.704206466675, 'total_duration': 29324.719745397568, 'accumulated_submission_time': 27085.704206466675, 'accumulated_eval_time': 2225.030177116394, 'accumulated_logging_time': 6.551129341125488, 'global_step': 70541, 'preemption_count': 0}), (71826, {'train/accuracy': 0.6858657598495483, 'train/loss': 1.3627768754959106, 'validation/accuracy': 0.6281200051307678, 'validation/loss': 1.6324516534805298, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.318972110748291, 'test/num_examples': 10000, 'score': 27595.434324502945, 'total_duration': 29870.035591840744, 'accumulated_submission_time': 27595.434324502945, 'accumulated_eval_time': 2260.295294046402, 'accumulated_logging_time': 6.7141430377960205, 'global_step': 71826, 'preemption_count': 0}), (73101, {'train/accuracy': 0.6825972199440002, 'train/loss': 1.3581565618515015, 'validation/accuracy': 0.6251199841499329, 'validation/loss': 1.628553032875061, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.3095176219940186, 'test/num_examples': 10000, 'score': 28105.17962193489, 'total_duration': 30413.40187072754, 'accumulated_submission_time': 28105.17962193489, 'accumulated_eval_time': 2293.640163898468, 'accumulated_logging_time': 6.834480285644531, 'global_step': 73101, 'preemption_count': 0}), (74279, {'train/accuracy': 0.7035036683082581, 'train/loss': 1.2861485481262207, 'validation/accuracy': 0.6349599957466125, 'validation/loss': 1.6019941568374634, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.2620930671691895, 'test/num_examples': 10000, 'score': 28615.057426452637, 'total_duration': 30961.0330824852, 'accumulated_submission_time': 28615.057426452637, 'accumulated_eval_time': 2331.126804113388, 'accumulated_logging_time': 6.954662084579468, 'global_step': 74279, 'preemption_count': 0}), (75563, {'train/accuracy': 0.7042809128761292, 'train/loss': 1.2806239128112793, 'validation/accuracy': 0.6369199752807617, 'validation/loss': 1.5822803974151611, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.261535167694092, 'test/num_examples': 10000, 'score': 29124.73771929741, 'total_duration': 31509.358362436295, 'accumulated_submission_time': 29124.73771929741, 'accumulated_eval_time': 2369.4458730220795, 'accumulated_logging_time': 7.12560248374939, 'global_step': 75563, 'preemption_count': 0}), (76843, {'train/accuracy': 0.7054368257522583, 'train/loss': 1.251882791519165, 'validation/accuracy': 0.6315000057220459, 'validation/loss': 1.5915164947509766, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.22709321975708, 'test/num_examples': 10000, 'score': 29634.777524232864, 'total_duration': 32060.8112244606, 'accumulated_submission_time': 29634.777524232864, 'accumulated_eval_time': 2410.5623364448547, 'accumulated_logging_time': 7.266575336456299, 'global_step': 76843, 'preemption_count': 0}), (78136, {'train/accuracy': 0.7095025181770325, 'train/loss': 1.2386481761932373, 'validation/accuracy': 0.6372799873352051, 'validation/loss': 1.564919352531433, 'validation/num_examples': 50000, 'test/accuracy': 0.511400043964386, 'test/loss': 2.224498987197876, 'test/num_examples': 10000, 'score': 30144.693754911423, 'total_duration': 32605.066781044006, 'accumulated_submission_time': 30144.693754911423, 'accumulated_eval_time': 2444.6105239391327, 'accumulated_logging_time': 7.402559041976929, 'global_step': 78136, 'preemption_count': 0}), (79430, {'train/accuracy': 0.7155014276504517, 'train/loss': 1.2211562395095825, 'validation/accuracy': 0.6422399878501892, 'validation/loss': 1.5530271530151367, 'validation/num_examples': 50000, 'test/accuracy': 0.5179000496864319, 'test/loss': 2.220170736312866, 'test/num_examples': 10000, 'score': 30654.48482823372, 'total_duration': 33146.98575806618, 'accumulated_submission_time': 30654.48482823372, 'accumulated_eval_time': 2476.4360489845276, 'accumulated_logging_time': 7.548407077789307, 'global_step': 79430, 'preemption_count': 0}), (80604, {'train/accuracy': 0.7147042155265808, 'train/loss': 1.2504810094833374, 'validation/accuracy': 0.6301599740982056, 'validation/loss': 1.6160557270050049, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2704062461853027, 'test/num_examples': 10000, 'score': 31164.466303110123, 'total_duration': 33690.82419300079, 'accumulated_submission_time': 31164.466303110123, 'accumulated_eval_time': 2510.006842136383, 'accumulated_logging_time': 7.69445013999939, 'global_step': 80604, 'preemption_count': 0}), (81884, {'train/accuracy': 0.7067123651504517, 'train/loss': 1.2666953802108765, 'validation/accuracy': 0.641979992389679, 'validation/loss': 1.565320372581482, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.241347551345825, 'test/num_examples': 10000, 'score': 31674.444312810898, 'total_duration': 34234.090277671814, 'accumulated_submission_time': 31674.444312810898, 'accumulated_eval_time': 2542.989287853241, 'accumulated_logging_time': 7.848722457885742, 'global_step': 81884, 'preemption_count': 0}), (83166, {'train/accuracy': 0.6972456574440002, 'train/loss': 1.319401502609253, 'validation/accuracy': 0.64274001121521, 'validation/loss': 1.566603183746338, 'validation/num_examples': 50000, 'test/accuracy': 0.5157999992370605, 'test/loss': 2.237445116043091, 'test/num_examples': 10000, 'score': 32184.478353500366, 'total_duration': 34786.54955625534, 'accumulated_submission_time': 32184.478353500366, 'accumulated_eval_time': 2585.1070516109467, 'accumulated_logging_time': 8.006536960601807, 'global_step': 83166, 'preemption_count': 0}), (84254, {'train/accuracy': 0.6948142647743225, 'train/loss': 1.307032823562622, 'validation/accuracy': 0.645039975643158, 'validation/loss': 1.5362550020217896, 'validation/num_examples': 50000, 'test/accuracy': 0.527400016784668, 'test/loss': 2.1915056705474854, 'test/num_examples': 10000, 'score': 32694.28601527214, 'total_duration': 35331.485731840134, 'accumulated_submission_time': 32694.28601527214, 'accumulated_eval_time': 2619.9307358264923, 'accumulated_logging_time': 8.1838538646698, 'global_step': 84254, 'preemption_count': 0}), (85472, {'train/accuracy': 0.6954918503761292, 'train/loss': 1.322993516921997, 'validation/accuracy': 0.6441999673843384, 'validation/loss': 1.5568430423736572, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.2075045108795166, 'test/num_examples': 10000, 'score': 33204.14257335663, 'total_duration': 35874.44759464264, 'accumulated_submission_time': 33204.14257335663, 'accumulated_eval_time': 2652.787799358368, 'accumulated_logging_time': 8.293099641799927, 'global_step': 85472, 'preemption_count': 0}), (86725, {'train/accuracy': 0.69925856590271, 'train/loss': 1.2917720079421997, 'validation/accuracy': 0.6479200124740601, 'validation/loss': 1.5204137563705444, 'validation/num_examples': 50000, 'test/accuracy': 0.5212000012397766, 'test/loss': 2.183321475982666, 'test/num_examples': 10000, 'score': 33714.05351233482, 'total_duration': 36428.55696058273, 'accumulated_submission_time': 33714.05351233482, 'accumulated_eval_time': 2696.684096097946, 'accumulated_logging_time': 8.448325395584106, 'global_step': 86725, 'preemption_count': 0}), (87926, {'train/accuracy': 0.7059550285339355, 'train/loss': 1.2714194059371948, 'validation/accuracy': 0.6505999565124512, 'validation/loss': 1.5271527767181396, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.1504061222076416, 'test/num_examples': 10000, 'score': 34223.7682595253, 'total_duration': 36980.279029369354, 'accumulated_submission_time': 34223.7682595253, 'accumulated_eval_time': 2738.352782011032, 'accumulated_logging_time': 8.64872932434082, 'global_step': 87926, 'preemption_count': 0}), (89179, {'train/accuracy': 0.6988599896430969, 'train/loss': 1.2979094982147217, 'validation/accuracy': 0.6479799747467041, 'validation/loss': 1.541835069656372, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1743595600128174, 'test/num_examples': 10000, 'score': 34733.876125097275, 'total_duration': 37527.873280763626, 'accumulated_submission_time': 34733.876125097275, 'accumulated_eval_time': 2775.5575659275055, 'accumulated_logging_time': 8.787016868591309, 'global_step': 89179, 'preemption_count': 0}), (90438, {'train/accuracy': 0.7082669138908386, 'train/loss': 1.253284215927124, 'validation/accuracy': 0.6511200070381165, 'validation/loss': 1.5215113162994385, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.2034687995910645, 'test/num_examples': 10000, 'score': 35243.666279792786, 'total_duration': 38070.67905211449, 'accumulated_submission_time': 35243.666279792786, 'accumulated_eval_time': 2808.2229022979736, 'accumulated_logging_time': 8.992034196853638, 'global_step': 90438, 'preemption_count': 0}), (91493, {'train/accuracy': 0.7190091013908386, 'train/loss': 1.214176058769226, 'validation/accuracy': 0.6571599841117859, 'validation/loss': 1.4979792833328247, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.1469013690948486, 'test/num_examples': 10000, 'score': 35753.68126511574, 'total_duration': 38616.792809963226, 'accumulated_submission_time': 35753.68126511574, 'accumulated_eval_time': 2844.0621078014374, 'accumulated_logging_time': 9.13358187675476, 'global_step': 91493, 'preemption_count': 0}), (92522, {'train/accuracy': 0.7293526530265808, 'train/loss': 1.141593098640442, 'validation/accuracy': 0.6570799946784973, 'validation/loss': 1.4659343957901, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.122898817062378, 'test/num_examples': 10000, 'score': 36263.41307759285, 'total_duration': 39159.08170723915, 'accumulated_submission_time': 36263.41307759285, 'accumulated_eval_time': 2876.30837726593, 'accumulated_logging_time': 9.328445672988892, 'global_step': 92522, 'preemption_count': 0}), (93610, {'train/accuracy': 0.7206034660339355, 'train/loss': 1.2120383977890015, 'validation/accuracy': 0.657759964466095, 'validation/loss': 1.4817472696304321, 'validation/num_examples': 50000, 'test/accuracy': 0.5288000106811523, 'test/loss': 2.1682701110839844, 'test/num_examples': 10000, 'score': 36773.33986592293, 'total_duration': 39701.03913116455, 'accumulated_submission_time': 36773.33986592293, 'accumulated_eval_time': 2908.064968109131, 'accumulated_logging_time': 9.480426549911499, 'global_step': 93610, 'preemption_count': 0}), (94692, {'train/accuracy': 0.7155811190605164, 'train/loss': 1.202919363975525, 'validation/accuracy': 0.6636199951171875, 'validation/loss': 1.4502438306808472, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.110671043395996, 'test/num_examples': 10000, 'score': 37283.05540347099, 'total_duration': 40239.83520245552, 'accumulated_submission_time': 37283.05540347099, 'accumulated_eval_time': 2936.82000041008, 'accumulated_logging_time': 9.683889627456665, 'global_step': 94692, 'preemption_count': 0}), (95736, {'train/accuracy': 0.7122528553009033, 'train/loss': 1.2573579549789429, 'validation/accuracy': 0.6598799824714661, 'validation/loss': 1.5021029710769653, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.1572322845458984, 'test/num_examples': 10000, 'score': 37792.80916953087, 'total_duration': 40780.41782069206, 'accumulated_submission_time': 37792.80916953087, 'accumulated_eval_time': 2967.3690683841705, 'accumulated_logging_time': 9.848324537277222, 'global_step': 95736, 'preemption_count': 0}), (96688, {'train/accuracy': 0.7226163744926453, 'train/loss': 1.1980310678482056, 'validation/accuracy': 0.6561599969863892, 'validation/loss': 1.5003060102462769, 'validation/num_examples': 50000, 'test/accuracy': 0.5284000039100647, 'test/loss': 2.1565113067626953, 'test/num_examples': 10000, 'score': 38302.71747303009, 'total_duration': 41328.621789455414, 'accumulated_submission_time': 38302.71747303009, 'accumulated_eval_time': 3005.3722336292267, 'accumulated_logging_time': 10.034788370132446, 'global_step': 96688, 'preemption_count': 0}), (97605, {'train/accuracy': 0.746113657951355, 'train/loss': 1.1029852628707886, 'validation/accuracy': 0.661579966545105, 'validation/loss': 1.4784252643585205, 'validation/num_examples': 50000, 'test/accuracy': 0.5336000323295593, 'test/loss': 2.1493115425109863, 'test/num_examples': 10000, 'score': 38812.940517663956, 'total_duration': 41877.78264307976, 'accumulated_submission_time': 38812.940517663956, 'accumulated_eval_time': 3044.049122095108, 'accumulated_logging_time': 10.192920923233032, 'global_step': 97605, 'preemption_count': 0}), (98468, {'train/accuracy': 0.7076889276504517, 'train/loss': 1.2911409139633179, 'validation/accuracy': 0.655299961566925, 'validation/loss': 1.529115915298462, 'validation/num_examples': 50000, 'test/accuracy': 0.5297999978065491, 'test/loss': 2.1901800632476807, 'test/num_examples': 10000, 'score': 39322.892761707306, 'total_duration': 42429.86869621277, 'accumulated_submission_time': 39322.892761707306, 'accumulated_eval_time': 3085.9342501163483, 'accumulated_logging_time': 10.346590995788574, 'global_step': 98468, 'preemption_count': 0}), (99366, {'train/accuracy': 0.7267019748687744, 'train/loss': 1.1657360792160034, 'validation/accuracy': 0.6635199785232544, 'validation/loss': 1.4535737037658691, 'validation/num_examples': 50000, 'test/accuracy': 0.5396000146865845, 'test/loss': 2.093968629837036, 'test/num_examples': 10000, 'score': 39833.15639948845, 'total_duration': 42978.86096596718, 'accumulated_submission_time': 39833.15639948845, 'accumulated_eval_time': 3124.388088464737, 'accumulated_logging_time': 10.52055811882019, 'global_step': 99366, 'preemption_count': 0}), (100134, {'train/accuracy': 0.72269606590271, 'train/loss': 1.208102822303772, 'validation/accuracy': 0.6705999970436096, 'validation/loss': 1.442450761795044, 'validation/num_examples': 50000, 'test/accuracy': 0.5446000099182129, 'test/loss': 2.1073343753814697, 'test/num_examples': 10000, 'score': 40343.191679000854, 'total_duration': 43528.9454100132, 'accumulated_submission_time': 40343.191679000854, 'accumulated_eval_time': 3164.1902389526367, 'accumulated_logging_time': 10.683191061019897, 'global_step': 100134, 'preemption_count': 0}), (101030, {'train/accuracy': 0.7226362824440002, 'train/loss': 1.182054042816162, 'validation/accuracy': 0.6683399677276611, 'validation/loss': 1.4326106309890747, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.083530902862549, 'test/num_examples': 10000, 'score': 40853.705493211746, 'total_duration': 44073.57433414459, 'accumulated_submission_time': 40853.705493211746, 'accumulated_eval_time': 3197.9660420417786, 'accumulated_logging_time': 10.921911716461182, 'global_step': 101030, 'preemption_count': 0}), (101880, {'train/accuracy': 0.7289739847183228, 'train/loss': 1.1760597229003906, 'validation/accuracy': 0.6608999967575073, 'validation/loss': 1.4754045009613037, 'validation/num_examples': 50000, 'test/accuracy': 0.5332000255584717, 'test/loss': 2.1427299976348877, 'test/num_examples': 10000, 'score': 41363.79951810837, 'total_duration': 44619.7922410965, 'accumulated_submission_time': 41363.79951810837, 'accumulated_eval_time': 3233.8207857608795, 'accumulated_logging_time': 11.097736358642578, 'global_step': 101880, 'preemption_count': 0}), (102898, {'train/accuracy': 0.7476084232330322, 'train/loss': 1.092307448387146, 'validation/accuracy': 0.6632199883460999, 'validation/loss': 1.4626216888427734, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1078989505767822, 'test/num_examples': 10000, 'score': 41873.72729253769, 'total_duration': 45162.38035964966, 'accumulated_submission_time': 41873.72729253769, 'accumulated_eval_time': 3266.1820130348206, 'accumulated_logging_time': 11.284513711929321, 'global_step': 102898, 'preemption_count': 0}), (103658, {'train/accuracy': 0.7261240482330322, 'train/loss': 1.1724026203155518, 'validation/accuracy': 0.6685999631881714, 'validation/loss': 1.4457732439041138, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.1093924045562744, 'test/num_examples': 10000, 'score': 42383.5020198822, 'total_duration': 45708.71291232109, 'accumulated_submission_time': 42383.5020198822, 'accumulated_eval_time': 3302.4701614379883, 'accumulated_logging_time': 11.470155954360962, 'global_step': 103658, 'preemption_count': 0}), (104326, {'train/accuracy': 0.7581911683082581, 'train/loss': 1.048638105392456, 'validation/accuracy': 0.6729399561882019, 'validation/loss': 1.427574634552002, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 2.08412504196167, 'test/num_examples': 10000, 'score': 42895.25928902626, 'total_duration': 46256.783266067505, 'accumulated_submission_time': 42895.25928902626, 'accumulated_eval_time': 3338.494806289673, 'accumulated_logging_time': 11.632615327835083, 'global_step': 104326, 'preemption_count': 0}), (104978, {'train/accuracy': 0.7311264276504517, 'train/loss': 1.1743803024291992, 'validation/accuracy': 0.670960009098053, 'validation/loss': 1.4403882026672363, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.1016106605529785, 'test/num_examples': 10000, 'score': 43405.132981061935, 'total_duration': 46806.76611876488, 'accumulated_submission_time': 43405.132981061935, 'accumulated_eval_time': 3378.291741847992, 'accumulated_logging_time': 11.83401107788086, 'global_step': 104978, 'preemption_count': 0}), (105605, {'train/accuracy': 0.7604432106018066, 'train/loss': 1.0388811826705933, 'validation/accuracy': 0.6730599999427795, 'validation/loss': 1.4137260913848877, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.055131673812866, 'test/num_examples': 10000, 'score': 43915.99709391594, 'total_duration': 47355.00218129158, 'accumulated_submission_time': 43915.99709391594, 'accumulated_eval_time': 3415.5176730155945, 'accumulated_logging_time': 11.910252809524536, 'global_step': 105605, 'preemption_count': 0}), (106180, {'train/accuracy': 0.734793484210968, 'train/loss': 1.148658275604248, 'validation/accuracy': 0.6734600067138672, 'validation/loss': 1.4293200969696045, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.091336488723755, 'test/num_examples': 10000, 'score': 44429.33230805397, 'total_duration': 47907.04577422142, 'accumulated_submission_time': 44429.33230805397, 'accumulated_eval_time': 3454.1200568675995, 'accumulated_logging_time': 11.952481508255005, 'global_step': 106180, 'preemption_count': 0}), (106835, {'train/accuracy': 0.7259646058082581, 'train/loss': 1.1890969276428223, 'validation/accuracy': 0.6716200113296509, 'validation/loss': 1.442348837852478, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.103611707687378, 'test/num_examples': 10000, 'score': 44939.625539302826, 'total_duration': 48455.08759522438, 'accumulated_submission_time': 44939.625539302826, 'accumulated_eval_time': 3491.7475571632385, 'accumulated_logging_time': 11.99902868270874, 'global_step': 106835, 'preemption_count': 0}), (107455, {'train/accuracy': 0.740254282951355, 'train/loss': 1.113765835762024, 'validation/accuracy': 0.6744599938392639, 'validation/loss': 1.4126404523849487, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.088254690170288, 'test/num_examples': 10000, 'score': 45449.64934015274, 'total_duration': 49003.50874090195, 'accumulated_submission_time': 45449.64934015274, 'accumulated_eval_time': 3530.0080597400665, 'accumulated_logging_time': 12.065387725830078, 'global_step': 107455, 'preemption_count': 0}), (107958, {'train/accuracy': 0.7295519709587097, 'train/loss': 1.1634418964385986, 'validation/accuracy': 0.6712200045585632, 'validation/loss': 1.4275529384613037, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.0764360427856445, 'test/num_examples': 10000, 'score': 45960.163200855255, 'total_duration': 49546.64817047119, 'accumulated_submission_time': 45960.163200855255, 'accumulated_eval_time': 3562.512717962265, 'accumulated_logging_time': 12.128931760787964, 'global_step': 107958, 'preemption_count': 0}), (108471, {'train/accuracy': 0.7596659660339355, 'train/loss': 1.0406839847564697, 'validation/accuracy': 0.6787399649620056, 'validation/loss': 1.395565152168274, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.047342300415039, 'test/num_examples': 10000, 'score': 46470.21558427811, 'total_duration': 50098.450464725494, 'accumulated_submission_time': 46470.21558427811, 'accumulated_eval_time': 3604.154694080353, 'accumulated_logging_time': 12.17865252494812, 'global_step': 108471, 'preemption_count': 0}), (109025, {'train/accuracy': 0.7336973547935486, 'train/loss': 1.1386672258377075, 'validation/accuracy': 0.6717000007629395, 'validation/loss': 1.4257686138153076, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.0792343616485596, 'test/num_examples': 10000, 'score': 46980.4241335392, 'total_duration': 50650.39496302605, 'accumulated_submission_time': 46980.4241335392, 'accumulated_eval_time': 3645.7767181396484, 'accumulated_logging_time': 12.230543613433838, 'global_step': 109025, 'preemption_count': 0}), (109473, {'train/accuracy': 0.732441782951355, 'train/loss': 1.1370259523391724, 'validation/accuracy': 0.6749199628829956, 'validation/loss': 1.4062894582748413, 'validation/num_examples': 50000, 'test/accuracy': 0.5485000014305115, 'test/loss': 2.0683977603912354, 'test/num_examples': 10000, 'score': 47491.805413007736, 'total_duration': 51195.168675899506, 'accumulated_submission_time': 47491.805413007736, 'accumulated_eval_time': 3679.0716648101807, 'accumulated_logging_time': 12.277462482452393, 'global_step': 109473, 'preemption_count': 0}), (110042, {'train/accuracy': 0.7405532598495483, 'train/loss': 1.0917880535125732, 'validation/accuracy': 0.6676200032234192, 'validation/loss': 1.4223884344100952, 'validation/num_examples': 50000, 'test/accuracy': 0.5473000407218933, 'test/loss': 2.0675055980682373, 'test/num_examples': 10000, 'score': 48002.15768957138, 'total_duration': 51741.97675895691, 'accumulated_submission_time': 48002.15768957138, 'accumulated_eval_time': 3715.4025366306305, 'accumulated_logging_time': 12.340536117553711, 'global_step': 110042, 'preemption_count': 0}), (110570, {'train/accuracy': 0.7365473508834839, 'train/loss': 1.146472454071045, 'validation/accuracy': 0.6718800067901611, 'validation/loss': 1.433701515197754, 'validation/num_examples': 50000, 'test/accuracy': 0.5439000129699707, 'test/loss': 2.1005101203918457, 'test/num_examples': 10000, 'score': 48512.28666305542, 'total_duration': 52284.8189573288, 'accumulated_submission_time': 48512.28666305542, 'accumulated_eval_time': 3748.0098464488983, 'accumulated_logging_time': 12.388327836990356, 'global_step': 110570, 'preemption_count': 0}), (110824, {'train/accuracy': 0.7321029901504517, 'train/loss': 1.1768193244934082, 'validation/accuracy': 0.6729199886322021, 'validation/loss': 1.434424638748169, 'validation/num_examples': 50000, 'test/accuracy': 0.5482000112533569, 'test/loss': 2.091046094894409, 'test/num_examples': 10000, 'score': 49025.169120788574, 'total_duration': 52835.22308135033, 'accumulated_submission_time': 49025.169120788574, 'accumulated_eval_time': 3785.417489528656, 'accumulated_logging_time': 12.4746675491333, 'global_step': 110824, 'preemption_count': 0}), (111230, {'train/accuracy': 0.7729392647743225, 'train/loss': 0.9798215627670288, 'validation/accuracy': 0.6782599687576294, 'validation/loss': 1.3746293783187866, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 2.004383087158203, 'test/num_examples': 10000, 'score': 49536.02373075485, 'total_duration': 53381.05413508415, 'accumulated_submission_time': 49536.02373075485, 'accumulated_eval_time': 3820.319619178772, 'accumulated_logging_time': 12.504190444946289, 'global_step': 111230, 'preemption_count': 0}), (111596, {'train/accuracy': 0.7443997263908386, 'train/loss': 1.092374563217163, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.3958985805511475, 'validation/num_examples': 50000, 'test/accuracy': 0.5548000335693359, 'test/loss': 2.033613681793213, 'test/num_examples': 10000, 'score': 50047.921122312546, 'total_duration': 53930.71153640747, 'accumulated_submission_time': 50047.921122312546, 'accumulated_eval_time': 3857.993744134903, 'accumulated_logging_time': 12.550712823867798, 'global_step': 111596, 'preemption_count': 0}), (111934, {'train/accuracy': 0.7419881820678711, 'train/loss': 1.1028839349746704, 'validation/accuracy': 0.6772599816322327, 'validation/loss': 1.3917112350463867, 'validation/num_examples': 50000, 'test/accuracy': 0.5563000440597534, 'test/loss': 2.0379831790924072, 'test/num_examples': 10000, 'score': 50558.29797911644, 'total_duration': 54476.6165034771, 'accumulated_submission_time': 50558.29797911644, 'accumulated_eval_time': 3893.403036594391, 'accumulated_logging_time': 12.632005453109741, 'global_step': 111934, 'preemption_count': 0}), (112118, {'train/accuracy': 0.7438217401504517, 'train/loss': 1.1100208759307861, 'validation/accuracy': 0.6826199889183044, 'validation/loss': 1.3858392238616943, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.031019926071167, 'test/num_examples': 10000, 'score': 51071.773574113846, 'total_duration': 55024.574649333954, 'accumulated_submission_time': 51071.773574113846, 'accumulated_eval_time': 3927.745766401291, 'accumulated_logging_time': 12.75107717514038, 'global_step': 112118, 'preemption_count': 0}), (112540, {'train/accuracy': 0.7454360723495483, 'train/loss': 1.0766855478286743, 'validation/accuracy': 0.6845200061798096, 'validation/loss': 1.3572968244552612, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 1.9948618412017822, 'test/num_examples': 10000, 'score': 51582.09742999077, 'total_duration': 55567.712595939636, 'accumulated_submission_time': 51582.09742999077, 'accumulated_eval_time': 3960.4503729343414, 'accumulated_logging_time': 12.814718246459961, 'global_step': 112540, 'preemption_count': 0}), (113080, {'train/accuracy': 0.7481664419174194, 'train/loss': 1.0672670602798462, 'validation/accuracy': 0.6777399778366089, 'validation/loss': 1.3826467990875244, 'validation/num_examples': 50000, 'test/accuracy': 0.553100049495697, 'test/loss': 2.0283591747283936, 'test/num_examples': 10000, 'score': 52092.63468718529, 'total_duration': 56114.28086948395, 'accumulated_submission_time': 52092.63468718529, 'accumulated_eval_time': 3996.3794016838074, 'accumulated_logging_time': 12.856613159179688, 'global_step': 113080, 'preemption_count': 0}), (113328, {'train/accuracy': 0.7432238459587097, 'train/loss': 1.0936839580535889, 'validation/accuracy': 0.6787599921226501, 'validation/loss': 1.3772202730178833, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 2.026643753051758, 'test/num_examples': 10000, 'score': 52604.05321931839, 'total_duration': 56661.589386463165, 'accumulated_submission_time': 52604.05321931839, 'accumulated_eval_time': 4032.152496099472, 'accumulated_logging_time': 12.945146560668945, 'global_step': 113328, 'preemption_count': 0}), (113742, {'train/accuracy': 0.7355309128761292, 'train/loss': 1.1383002996444702, 'validation/accuracy': 0.6761199831962585, 'validation/loss': 1.3979442119598389, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.0567944049835205, 'test/num_examples': 10000, 'score': 53115.574845314026, 'total_duration': 57205.180955410004, 'accumulated_submission_time': 53115.574845314026, 'accumulated_eval_time': 4064.14395070076, 'accumulated_logging_time': 12.977898836135864, 'global_step': 113742, 'preemption_count': 0}), (114025, {'train/accuracy': 0.7551219463348389, 'train/loss': 1.058273196220398, 'validation/accuracy': 0.6785399913787842, 'validation/loss': 1.4052555561065674, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.0645456314086914, 'test/num_examples': 10000, 'score': 53625.72285366058, 'total_duration': 57748.1856815815, 'accumulated_submission_time': 53625.72285366058, 'accumulated_eval_time': 4096.896467208862, 'accumulated_logging_time': 13.050889015197754, 'global_step': 114025, 'preemption_count': 0}), (114280, {'train/accuracy': 0.7571747303009033, 'train/loss': 1.0325630903244019, 'validation/accuracy': 0.6838600039482117, 'validation/loss': 1.3688435554504395, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 2.0225744247436523, 'test/num_examples': 10000, 'score': 54136.0850789547, 'total_duration': 58293.25433254242, 'accumulated_submission_time': 54136.0850789547, 'accumulated_eval_time': 4131.510539770126, 'accumulated_logging_time': 13.113523244857788, 'global_step': 114280, 'preemption_count': 0}), (114472, {'train/accuracy': 0.7515744566917419, 'train/loss': 1.0687155723571777, 'validation/accuracy': 0.6802600026130676, 'validation/loss': 1.387001395225525, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.039818525314331, 'test/num_examples': 10000, 'score': 54646.22646856308, 'total_duration': 58839.83969449997, 'accumulated_submission_time': 54646.22646856308, 'accumulated_eval_time': 4167.902105808258, 'accumulated_logging_time': 13.143961191177368, 'global_step': 114472, 'preemption_count': 0}), (114598, {'train/accuracy': 0.751375138759613, 'train/loss': 1.070949673652649, 'validation/accuracy': 0.6862999796867371, 'validation/loss': 1.3652663230895996, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.0168349742889404, 'test/num_examples': 10000, 'score': 55160.103695869446, 'total_duration': 59391.33639764786, 'accumulated_submission_time': 55160.103695869446, 'accumulated_eval_time': 4205.476538181305, 'accumulated_logging_time': 13.175290822982788, 'global_step': 114598, 'preemption_count': 0}), (115187, {'train/accuracy': 0.7390784025192261, 'train/loss': 1.119966983795166, 'validation/accuracy': 0.6820799708366394, 'validation/loss': 1.3853365182876587, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 2.063185214996338, 'test/num_examples': 10000, 'score': 55670.06543755531, 'total_duration': 59947.74996447563, 'accumulated_submission_time': 55670.06543755531, 'accumulated_eval_time': 4251.831089496613, 'accumulated_logging_time': 13.205965995788574, 'global_step': 115187, 'preemption_count': 0}), (115981, {'train/accuracy': 0.7488639950752258, 'train/loss': 1.076809287071228, 'validation/accuracy': 0.6796799898147583, 'validation/loss': 1.3794686794281006, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.046754837036133, 'test/num_examples': 10000, 'score': 56179.215597867966, 'total_duration': 60491.445437669754, 'accumulated_submission_time': 56179.215597867966, 'accumulated_eval_time': 4285.012269258499, 'accumulated_logging_time': 14.478042125701904, 'global_step': 115981, 'preemption_count': 0}), (116785, {'train/accuracy': 0.7436822056770325, 'train/loss': 1.1006927490234375, 'validation/accuracy': 0.6809799671173096, 'validation/loss': 1.379116177558899, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 2.0484087467193604, 'test/num_examples': 10000, 'score': 56689.229739665985, 'total_duration': 61037.50391745567, 'accumulated_submission_time': 56689.229739665985, 'accumulated_eval_time': 4320.910253286362, 'accumulated_logging_time': 14.534140348434448, 'global_step': 116785, 'preemption_count': 0}), (117138, {'train/accuracy': 0.7645089030265808, 'train/loss': 1.015527367591858, 'validation/accuracy': 0.6846399903297424, 'validation/loss': 1.359680414199829, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 2.0225794315338135, 'test/num_examples': 10000, 'score': 57199.9940328598, 'total_duration': 61584.42549967766, 'accumulated_submission_time': 57199.9940328598, 'accumulated_eval_time': 4356.917885303497, 'accumulated_logging_time': 14.643930673599243, 'global_step': 117138, 'preemption_count': 0}), (117540, {'train/accuracy': 0.7486646771430969, 'train/loss': 1.0716880559921265, 'validation/accuracy': 0.6812599897384644, 'validation/loss': 1.3699307441711426, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 2.0308005809783936, 'test/num_examples': 10000, 'score': 57709.923468351364, 'total_duration': 62130.84720039368, 'accumulated_submission_time': 57709.923468351364, 'accumulated_eval_time': 4393.329013824463, 'accumulated_logging_time': 14.679791927337646, 'global_step': 117540, 'preemption_count': 0}), (118132, {'train/accuracy': 0.7505779266357422, 'train/loss': 1.0568245649337769, 'validation/accuracy': 0.6892799735069275, 'validation/loss': 1.3315318822860718, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 2.0038721561431885, 'test/num_examples': 10000, 'score': 58219.88133907318, 'total_duration': 62678.7029542923, 'accumulated_submission_time': 58219.88133907318, 'accumulated_eval_time': 4431.0619122982025, 'accumulated_logging_time': 14.777708053588867, 'global_step': 118132, 'preemption_count': 0}), (118682, {'train/accuracy': 0.7507772445678711, 'train/loss': 1.0511842966079712, 'validation/accuracy': 0.6802600026130676, 'validation/loss': 1.3835629224777222, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 2.040649652481079, 'test/num_examples': 10000, 'score': 58730.331020116806, 'total_duration': 63225.463284015656, 'accumulated_submission_time': 58730.331020116806, 'accumulated_eval_time': 4467.262981176376, 'accumulated_logging_time': 14.82481837272644, 'global_step': 118682, 'preemption_count': 0}), (119063, {'train/accuracy': 0.7574537396430969, 'train/loss': 1.0407602787017822, 'validation/accuracy': 0.6904399991035461, 'validation/loss': 1.3321399688720703, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 1.992639183998108, 'test/num_examples': 10000, 'score': 59240.697813510895, 'total_duration': 63771.10726213455, 'accumulated_submission_time': 59240.697813510895, 'accumulated_eval_time': 4502.3951551914215, 'accumulated_logging_time': 14.925670623779297, 'global_step': 119063, 'preemption_count': 0}), (119316, {'train/accuracy': 0.7449178695678711, 'train/loss': 1.0808199644088745, 'validation/accuracy': 0.6895399689674377, 'validation/loss': 1.3459185361862183, 'validation/num_examples': 50000, 'test/accuracy': 0.5666000247001648, 'test/loss': 1.9806833267211914, 'test/num_examples': 10000, 'score': 59751.13297390938, 'total_duration': 64315.65903496742, 'accumulated_submission_time': 59751.13297390938, 'accumulated_eval_time': 4536.350691080093, 'accumulated_logging_time': 15.058576822280884, 'global_step': 119316, 'preemption_count': 0}), (119568, {'train/accuracy': 0.7529097199440002, 'train/loss': 1.060800313949585, 'validation/accuracy': 0.6879000067710876, 'validation/loss': 1.3469411134719849, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.982959270477295, 'test/num_examples': 10000, 'score': 60261.63347887993, 'total_duration': 64860.78455853462, 'accumulated_submission_time': 60261.63347887993, 'accumulated_eval_time': 4570.916675567627, 'accumulated_logging_time': 15.08935546875, 'global_step': 119568, 'preemption_count': 0}), (119888, {'train/accuracy': 0.7716039419174194, 'train/loss': 0.9782085418701172, 'validation/accuracy': 0.6869800090789795, 'validation/loss': 1.3563183546066284, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 2.0083489418029785, 'test/num_examples': 10000, 'score': 60772.47364616394, 'total_duration': 65405.3649623394, 'accumulated_submission_time': 60772.47364616394, 'accumulated_eval_time': 4604.589485406876, 'accumulated_logging_time': 15.120202541351318, 'global_step': 119888, 'preemption_count': 0}), (120257, {'train/accuracy': 0.759207546710968, 'train/loss': 1.0321961641311646, 'validation/accuracy': 0.6893399953842163, 'validation/loss': 1.3460503816604614, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 2.0130231380462646, 'test/num_examples': 10000, 'score': 61283.52868890762, 'total_duration': 65949.44031786919, 'accumulated_submission_time': 61283.52868890762, 'accumulated_eval_time': 4637.501240730286, 'accumulated_logging_time': 15.185400485992432, 'global_step': 120257, 'preemption_count': 0}), (120456, {'train/accuracy': 0.7532684803009033, 'train/loss': 1.061436653137207, 'validation/accuracy': 0.6860600113868713, 'validation/loss': 1.3529579639434814, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 2.0226051807403564, 'test/num_examples': 10000, 'score': 61795.408885240555, 'total_duration': 66497.92521595955, 'accumulated_submission_time': 61795.408885240555, 'accumulated_eval_time': 4674.003812074661, 'accumulated_logging_time': 15.26399850845337, 'global_step': 120456, 'preemption_count': 0}), (120582, {'train/accuracy': 0.7522122263908386, 'train/loss': 1.0544838905334473, 'validation/accuracy': 0.6832199692726135, 'validation/loss': 1.3549047708511353, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 1.9933422803878784, 'test/num_examples': 10000, 'score': 62309.38175058365, 'total_duration': 67046.56606912613, 'accumulated_submission_time': 62309.38175058365, 'accumulated_eval_time': 4708.579865694046, 'accumulated_logging_time': 15.342258930206299, 'global_step': 120582, 'preemption_count': 0}), (120753, {'train/accuracy': 0.7425661683082581, 'train/loss': 1.1025018692016602, 'validation/accuracy': 0.681439995765686, 'validation/loss': 1.3880294561386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.076122283935547, 'test/num_examples': 10000, 'score': 62819.61688256264, 'total_duration': 67590.67770910263, 'accumulated_submission_time': 62819.61688256264, 'accumulated_eval_time': 4742.406438112259, 'accumulated_logging_time': 15.373538494110107, 'global_step': 120753, 'preemption_count': 0}), (121068, {'train/accuracy': 0.7512356638908386, 'train/loss': 1.0647872686386108, 'validation/accuracy': 0.6884199976921082, 'validation/loss': 1.3385746479034424, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 1.9781147241592407, 'test/num_examples': 10000, 'score': 63330.39852619171, 'total_duration': 68135.48347973824, 'accumulated_submission_time': 63330.39852619171, 'accumulated_eval_time': 4776.361238956451, 'accumulated_logging_time': 15.406874179840088, 'global_step': 121068, 'preemption_count': 0}), (121549, {'train/accuracy': 0.7716438174247742, 'train/loss': 0.9776421189308167, 'validation/accuracy': 0.6939199566841125, 'validation/loss': 1.3251116275787354, 'validation/num_examples': 50000, 'test/accuracy': 0.5641000270843506, 'test/loss': 1.987453579902649, 'test/num_examples': 10000, 'score': 63841.09016704559, 'total_duration': 68679.44339680672, 'accumulated_submission_time': 63841.09016704559, 'accumulated_eval_time': 4809.473536491394, 'accumulated_logging_time': 15.50875449180603, 'global_step': 121549, 'preemption_count': 0}), (122032, {'train/accuracy': 0.7590680718421936, 'train/loss': 1.01444411277771, 'validation/accuracy': 0.6910799741744995, 'validation/loss': 1.3137145042419434, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 1.9690567255020142, 'test/num_examples': 10000, 'score': 64351.63581061363, 'total_duration': 69222.97276234627, 'accumulated_submission_time': 64351.63581061363, 'accumulated_eval_time': 4842.309837818146, 'accumulated_logging_time': 15.599915742874146, 'global_step': 122032, 'preemption_count': 0}), (122518, {'train/accuracy': 0.76171875, 'train/loss': 1.0199787616729736, 'validation/accuracy': 0.6930599808692932, 'validation/loss': 1.3248945474624634, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9553985595703125, 'test/num_examples': 10000, 'score': 64862.038177490234, 'total_duration': 69768.50483179092, 'accumulated_submission_time': 64862.038177490234, 'accumulated_eval_time': 4877.310750961304, 'accumulated_logging_time': 15.674070358276367, 'global_step': 122518, 'preemption_count': 0}), (122999, {'train/accuracy': 0.7655851244926453, 'train/loss': 1.011163592338562, 'validation/accuracy': 0.6908800005912781, 'validation/loss': 1.3551948070526123, 'validation/num_examples': 50000, 'test/accuracy': 0.566100001335144, 'test/loss': 1.9953523874282837, 'test/num_examples': 10000, 'score': 65372.33504295349, 'total_duration': 70314.53204369545, 'accumulated_submission_time': 65372.33504295349, 'accumulated_eval_time': 4912.92134976387, 'accumulated_logging_time': 15.737920761108398, 'global_step': 122999, 'preemption_count': 0}), (123463, {'train/accuracy': 0.7614795565605164, 'train/loss': 1.0319074392318726, 'validation/accuracy': 0.6915000081062317, 'validation/loss': 1.3308998346328735, 'validation/num_examples': 50000, 'test/accuracy': 0.5618000030517578, 'test/loss': 1.982467532157898, 'test/num_examples': 10000, 'score': 65883.61578512192, 'total_duration': 70865.24232411385, 'accumulated_submission_time': 65883.61578512192, 'accumulated_eval_time': 4952.189725399017, 'accumulated_logging_time': 15.84632682800293, 'global_step': 123463, 'preemption_count': 0})], 'global_step': 123700}
I0307 20:50:42.457838 140441807221952 submission_runner.py:649] Timing: 66394.4845483303
I0307 20:50:42.457885 140441807221952 submission_runner.py:651] Total number of evals: 130
I0307 20:50:42.457916 140441807221952 submission_runner.py:652] ====================
I0307 20:50:42.458157 140441807221952 submission_runner.py:750] Final imagenet_resnet score: 0
