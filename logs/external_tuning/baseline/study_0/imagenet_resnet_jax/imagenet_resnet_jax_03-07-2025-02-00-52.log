python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-243557195 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-02-00-52.log
2025-03-07 02:01:09.081881: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741312869.728856       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741312869.898644       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 02:01:57.698277 139912818214080 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax.
I0307 02:02:00.399203 139912818214080 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 02:02:00.402463 139912818214080 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 02:02:00.418101 139912818214080 submission_runner.py:606] Using RNG seed -243557195
I0307 02:02:05.670565 139912818214080 submission_runner.py:615] --- Tuning run 4/5 ---
I0307 02:02:05.670777 139912818214080 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_4.
I0307 02:02:05.670981 139912818214080 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_4/hparams.json.
I0307 02:02:05.906062 139912818214080 submission_runner.py:218] Initializing dataset.
I0307 02:02:07.292597 139912818214080 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:02:07.658067 139912818214080 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:02:08.023682 139912818214080 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:02:09.644895 139912818214080 submission_runner.py:229] Initializing model.
I0307 02:02:33.337126 139912818214080 submission_runner.py:272] Initializing optimizer.
I0307 02:02:34.416632 139912818214080 submission_runner.py:279] Initializing metrics bundle.
I0307 02:02:34.416856 139912818214080 submission_runner.py:301] Initializing checkpoint and logger.
I0307 02:02:34.417883 139912818214080 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0307 02:02:34.417984 139912818214080 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_4/meta_data_0.json.
I0307 02:02:35.044325 139912818214080 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/imagenet_resnet_jax/trial_4/flags_0.json.
I0307 02:02:35.369628 139912818214080 submission_runner.py:337] Starting training loop.
I0307 02:03:34.971930 139776402831104 logging_writer.py:48] [0] global_step=0, grad_norm=0.6946976184844971, loss=6.915707111358643
I0307 02:03:35.270097 139912818214080 spec.py:321] Evaluating on the training split.
I0307 02:03:35.754400 139912818214080 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:03:35.779888 139912818214080 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:03:35.823800 139912818214080 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:03:55.285441 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 02:03:55.742132 139912818214080 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:03:55.750262 139912818214080 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:03:55.788398 139912818214080 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:04:32.447957 139912818214080 spec.py:349] Evaluating on the test split.
I0307 02:04:32.911932 139912818214080 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:04:32.932018 139912818214080 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 02:04:32.968209 139912818214080 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:04:50.524236 139912818214080 submission_runner.py:469] Time since start: 135.15s, 	Step: 1, 	{'train/accuracy': 0.001215720665641129, 'train/loss': 6.912454128265381, 'validation/accuracy': 0.0012400000123307109, 'validation/loss': 6.913532257080078, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.913326263427734, 'test/num_examples': 10000, 'score': 59.900206565856934, 'total_duration': 135.1545467376709, 'accumulated_submission_time': 59.900206565856934, 'accumulated_eval_time': 75.25408744812012, 'accumulated_logging_time': 0}
I0307 02:04:50.572924 139758051235584 logging_writer.py:48] [1] accumulated_eval_time=75.2541, accumulated_logging_time=0, accumulated_submission_time=59.9002, global_step=1, preemption_count=0, score=59.9002, test/accuracy=0.0013, test/loss=6.91333, test/num_examples=10000, total_duration=135.155, train/accuracy=0.00121572, train/loss=6.91245, validation/accuracy=0.00124, validation/loss=6.91353, validation/num_examples=50000
I0307 02:05:26.367888 139757766047488 logging_writer.py:48] [100] global_step=100, grad_norm=0.7728766202926636, loss=6.634190082550049
I0307 02:06:03.717038 139758051235584 logging_writer.py:48] [200] global_step=200, grad_norm=0.9815942049026489, loss=6.34224796295166
I0307 02:06:40.910214 139757766047488 logging_writer.py:48] [300] global_step=300, grad_norm=3.0840625762939453, loss=6.057109832763672
I0307 02:07:18.417245 139758051235584 logging_writer.py:48] [400] global_step=400, grad_norm=2.0276601314544678, loss=5.815421104431152
I0307 02:07:56.915672 139757766047488 logging_writer.py:48] [500] global_step=500, grad_norm=2.221343994140625, loss=5.615054130554199
I0307 02:08:34.830031 139758051235584 logging_writer.py:48] [600] global_step=600, grad_norm=3.9221272468566895, loss=5.384097099304199
I0307 02:09:12.356080 139757766047488 logging_writer.py:48] [700] global_step=700, grad_norm=4.063136577606201, loss=5.1528730392456055
I0307 02:09:49.747876 139758051235584 logging_writer.py:48] [800] global_step=800, grad_norm=4.697947025299072, loss=5.069684982299805
I0307 02:10:26.770912 139757766047488 logging_writer.py:48] [900] global_step=900, grad_norm=5.483761787414551, loss=4.846164703369141
I0307 02:11:03.914674 139758051235584 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.059831380844116, loss=4.686249256134033
I0307 02:11:41.512141 139757766047488 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.317841529846191, loss=4.512338638305664
I0307 02:12:19.354524 139758051235584 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.7860772609710693, loss=4.374645709991455
I0307 02:12:56.283624 139757766047488 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.3556692600250244, loss=4.382840156555176
I0307 02:13:20.545092 139912818214080 spec.py:321] Evaluating on the training split.
I0307 02:13:32.835738 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 02:13:52.189590 139912818214080 spec.py:349] Evaluating on the test split.
I0307 02:13:54.085296 139912818214080 submission_runner.py:469] Time since start: 678.72s, 	Step: 1366, 	{'train/accuracy': 0.20770886540412903, 'train/loss': 3.9941461086273193, 'validation/accuracy': 0.16985999047756195, 'validation/loss': 4.296290397644043, 'validation/num_examples': 50000, 'test/accuracy': 0.12450000643730164, 'test/loss': 4.820620059967041, 'test/num_examples': 10000, 'score': 569.6955945491791, 'total_duration': 678.7156174182892, 'accumulated_submission_time': 569.6955945491791, 'accumulated_eval_time': 108.79424858093262, 'accumulated_logging_time': 0.05858349800109863}
I0307 02:13:54.126490 139758059628288 logging_writer.py:48] [1366] accumulated_eval_time=108.794, accumulated_logging_time=0.0585835, accumulated_submission_time=569.696, global_step=1366, preemption_count=0, score=569.696, test/accuracy=0.1245, test/loss=4.82062, test/num_examples=10000, total_duration=678.716, train/accuracy=0.207709, train/loss=3.99415, validation/accuracy=0.16986, validation/loss=4.29629, validation/num_examples=50000
I0307 02:14:07.700563 139758068020992 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.6026368141174316, loss=4.135376930236816
I0307 02:14:45.574091 139758059628288 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.5639851093292236, loss=4.063507556915283
I0307 02:15:22.845498 139758068020992 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.4365456104278564, loss=3.9866466522216797
I0307 02:15:59.937486 139758059628288 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.269394636154175, loss=3.8879780769348145
I0307 02:16:37.900016 139758068020992 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.7958271503448486, loss=3.883391857147217
I0307 02:17:16.217075 139758059628288 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.9436557292938232, loss=3.632798910140991
I0307 02:17:54.001218 139758068020992 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.8883562088012695, loss=3.6005771160125732
I0307 02:18:32.350553 139758059628288 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.9437057971954346, loss=3.4453396797180176
I0307 02:19:10.475507 139758068020992 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.6743836402893066, loss=3.5972750186920166
I0307 02:19:49.089350 139758059628288 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.2957677841186523, loss=3.4296281337738037
I0307 02:20:27.595427 139758068020992 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.4167505502700806, loss=3.3227503299713135
I0307 02:21:05.460485 139758059628288 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.5029813051223755, loss=3.33232045173645
I0307 02:21:43.260001 139758068020992 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.7266746759414673, loss=3.2195160388946533
I0307 02:22:21.589317 139758059628288 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.4234811067581177, loss=3.2198362350463867
I0307 02:22:24.235208 139912818214080 spec.py:321] Evaluating on the training split.
I0307 02:22:35.922615 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 02:22:56.199946 139912818214080 spec.py:349] Evaluating on the test split.
I0307 02:22:57.987088 139912818214080 submission_runner.py:469] Time since start: 1222.62s, 	Step: 2708, 	{'train/accuracy': 0.36136797070503235, 'train/loss': 2.944725513458252, 'validation/accuracy': 0.3172000050544739, 'validation/loss': 3.2365479469299316, 'validation/num_examples': 50000, 'test/accuracy': 0.24220001697540283, 'test/loss': 3.8796088695526123, 'test/num_examples': 10000, 'score': 1079.633470773697, 'total_duration': 1222.617424249649, 'accumulated_submission_time': 1079.633470773697, 'accumulated_eval_time': 142.54613542556763, 'accumulated_logging_time': 0.1194760799407959}
I0307 02:22:58.049683 139758068020992 logging_writer.py:48] [2708] accumulated_eval_time=142.546, accumulated_logging_time=0.119476, accumulated_submission_time=1079.63, global_step=2708, preemption_count=0, score=1079.63, test/accuracy=0.2422, test/loss=3.87961, test/num_examples=10000, total_duration=1222.62, train/accuracy=0.361368, train/loss=2.94473, validation/accuracy=0.3172, validation/loss=3.23655, validation/num_examples=50000
I0307 02:23:33.944300 139758059628288 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.6418713331222534, loss=3.2664287090301514
I0307 02:24:12.071916 139758068020992 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.3375234603881836, loss=3.1417832374572754
I0307 02:24:50.520395 139758059628288 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4083198308944702, loss=3.0847573280334473
I0307 02:25:28.890322 139758068020992 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.247686743736267, loss=3.1509273052215576
I0307 02:26:06.941630 139758059628288 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2523598670959473, loss=3.1461191177368164
I0307 02:26:44.970834 139758068020992 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8036287426948547, loss=3.132944107055664
I0307 02:27:23.173482 139758059628288 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.066952109336853, loss=3.0697832107543945
I0307 02:28:01.907747 139758068020992 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2480298280715942, loss=3.014158248901367
I0307 02:28:40.644961 139758059628288 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.4495993852615356, loss=3.0892863273620605
I0307 02:29:18.998986 139758068020992 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.2954999208450317, loss=3.0760912895202637
I0307 02:29:57.137501 139758059628288 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.0565897226333618, loss=3.0394437313079834
I0307 02:30:35.477914 139758068020992 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.090119481086731, loss=2.905395984649658
I0307 02:31:13.910395 139758059628288 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.0410985946655273, loss=2.9894237518310547
I0307 02:31:28.106270 139912818214080 spec.py:321] Evaluating on the training split.
I0307 02:31:39.135214 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 02:32:04.487341 139912818214080 spec.py:349] Evaluating on the test split.
I0307 02:32:06.240924 139912818214080 submission_runner.py:469] Time since start: 1770.87s, 	Step: 4038, 	{'train/accuracy': 0.3820551633834839, 'train/loss': 2.8290624618530273, 'validation/accuracy': 0.3379199802875519, 'validation/loss': 3.129014253616333, 'validation/num_examples': 50000, 'test/accuracy': 0.2587999999523163, 'test/loss': 3.7857720851898193, 'test/num_examples': 10000, 'score': 1589.5423862934113, 'total_duration': 1770.871235370636, 'accumulated_submission_time': 1589.5423862934113, 'accumulated_eval_time': 180.68074917793274, 'accumulated_logging_time': 0.1895906925201416}
I0307 02:32:06.316200 139758068020992 logging_writer.py:48] [4038] accumulated_eval_time=180.681, accumulated_logging_time=0.189591, accumulated_submission_time=1589.54, global_step=4038, preemption_count=0, score=1589.54, test/accuracy=0.2588, test/loss=3.78577, test/num_examples=10000, total_duration=1770.87, train/accuracy=0.382055, train/loss=2.82906, validation/accuracy=0.33792, validation/loss=3.12901, validation/num_examples=50000
I0307 02:32:30.518222 139758059628288 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.0169122219085693, loss=2.905254602432251
I0307 02:33:08.894227 139758068020992 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9860024452209473, loss=2.7767772674560547
I0307 02:33:47.085154 139758059628288 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8043678998947144, loss=2.713853597640991
I0307 02:34:24.868265 139758068020992 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9511049389839172, loss=2.8857944011688232
I0307 02:35:02.873450 139758059628288 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.9254606366157532, loss=2.876573324203491
I0307 02:35:41.538779 139758068020992 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.054808497428894, loss=2.659437656402588
I0307 02:36:19.455730 139758059628288 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8269445300102234, loss=2.8874874114990234
I0307 02:36:57.068673 139758068020992 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9769624471664429, loss=2.752598285675049
I0307 02:37:34.446174 139758059628288 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.811073899269104, loss=2.713742971420288
I0307 02:38:12.636046 139758068020992 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8501906394958496, loss=2.828981876373291
I0307 02:38:50.804813 139758059628288 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7948657870292664, loss=2.6356518268585205
I0307 02:39:29.235548 139758068020992 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7570860385894775, loss=2.7075252532958984
I0307 02:40:07.612692 139758059628288 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7886649370193481, loss=2.6001288890838623
I0307 02:40:36.551330 139912818214080 spec.py:321] Evaluating on the training split.
I0307 02:40:48.736781 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 02:41:05.706384 139912818214080 spec.py:349] Evaluating on the test split.
I0307 02:41:07.520437 139912818214080 submission_runner.py:469] Time since start: 2312.15s, 	Step: 5376, 	{'train/accuracy': 0.3952885866165161, 'train/loss': 2.7969157695770264, 'validation/accuracy': 0.3600800037384033, 'validation/loss': 3.0530054569244385, 'validation/num_examples': 50000, 'test/accuracy': 0.2687000036239624, 'test/loss': 3.796846389770508, 'test/num_examples': 10000, 'score': 2099.603739500046, 'total_duration': 2312.1507647037506, 'accumulated_submission_time': 2099.603739500046, 'accumulated_eval_time': 211.6498167514801, 'accumulated_logging_time': 0.2729067802429199}
I0307 02:41:07.561784 139758068020992 logging_writer.py:48] [5376] accumulated_eval_time=211.65, accumulated_logging_time=0.272907, accumulated_submission_time=2099.6, global_step=5376, preemption_count=0, score=2099.6, test/accuracy=0.2687, test/loss=3.79685, test/num_examples=10000, total_duration=2312.15, train/accuracy=0.395289, train/loss=2.79692, validation/accuracy=0.36008, validation/loss=3.05301, validation/num_examples=50000
I0307 02:41:17.815190 139758059628288 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.9124351143836975, loss=2.8521881103515625
I0307 02:41:56.066306 139758068020992 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.0751885175704956, loss=2.762965440750122
I0307 02:42:34.367516 139758059628288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.7825811505317688, loss=2.6291401386260986
I0307 02:43:12.856703 139758068020992 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9061416387557983, loss=2.7534914016723633
I0307 02:43:51.272445 139758059628288 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.0483819246292114, loss=2.6011643409729004
I0307 02:44:29.525056 139758068020992 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.0094887018203735, loss=2.680372714996338
I0307 02:45:07.684413 139758059628288 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.0121995210647583, loss=2.594963312149048
I0307 02:45:45.990508 139758068020992 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.887662410736084, loss=2.7721285820007324
I0307 02:46:24.246415 139758059628288 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.0111689567565918, loss=2.639348268508911
I0307 02:47:02.764415 139758068020992 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9423186182975769, loss=2.6785988807678223
I0307 02:47:41.475462 139758059628288 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.1826982498168945, loss=2.6290438175201416
I0307 02:48:19.704159 139758068020992 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.026168942451477, loss=2.6591670513153076
I0307 02:48:58.157345 139758059628288 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8994242548942566, loss=2.5145630836486816
I0307 02:49:36.123292 139758068020992 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.9866331219673157, loss=2.594265937805176
I0307 02:49:37.587482 139912818214080 spec.py:321] Evaluating on the training split.
I0307 02:49:48.693419 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 02:50:10.296701 139912818214080 spec.py:349] Evaluating on the test split.
I0307 02:50:12.042098 139912818214080 submission_runner.py:469] Time since start: 2856.67s, 	Step: 6705, 	{'train/accuracy': 0.3795439898967743, 'train/loss': 2.885197877883911, 'validation/accuracy': 0.3422999978065491, 'validation/loss': 3.1532857418060303, 'validation/num_examples': 50000, 'test/accuracy': 0.26500001549720764, 'test/loss': 3.8443524837493896, 'test/num_examples': 10000, 'score': 2609.510282754898, 'total_duration': 2856.672435760498, 'accumulated_submission_time': 2609.510282754898, 'accumulated_eval_time': 246.10440015792847, 'accumulated_logging_time': 0.3231070041656494}
I0307 02:50:12.108225 139758059628288 logging_writer.py:48] [6705] accumulated_eval_time=246.104, accumulated_logging_time=0.323107, accumulated_submission_time=2609.51, global_step=6705, preemption_count=0, score=2609.51, test/accuracy=0.265, test/loss=3.84435, test/num_examples=10000, total_duration=2856.67, train/accuracy=0.379544, train/loss=2.8852, validation/accuracy=0.3423, validation/loss=3.15329, validation/num_examples=50000
I0307 02:50:49.459417 139758068020992 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.02024245262146, loss=2.510742664337158
I0307 02:51:27.938588 139758059628288 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9795167446136475, loss=2.6603970527648926
I0307 02:52:06.078887 139758068020992 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.981857180595398, loss=2.518887758255005
I0307 02:52:44.733072 139758059628288 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.0625063180923462, loss=2.547548532485962
I0307 02:53:23.526831 139758068020992 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9976794719696045, loss=2.619595527648926
I0307 02:54:02.388527 139758059628288 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.0733163356781006, loss=2.562089443206787
I0307 02:54:41.066320 139758068020992 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0281156301498413, loss=2.516477108001709
I0307 02:55:19.525896 139758059628288 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.0199410915374756, loss=2.3647308349609375
I0307 02:55:58.177710 139758068020992 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.0385414361953735, loss=2.5341997146606445
I0307 02:56:36.318120 139758059628288 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8760400414466858, loss=2.4927120208740234
I0307 02:57:14.442873 139758068020992 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.871783971786499, loss=2.524312734603882
I0307 02:57:52.796113 139758059628288 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9463509917259216, loss=2.527015209197998
I0307 02:58:31.278222 139758068020992 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8967171907424927, loss=2.5327320098876953
I0307 02:58:42.380991 139912818214080 spec.py:321] Evaluating on the training split.
I0307 02:58:52.564250 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 02:59:12.316941 139912818214080 spec.py:349] Evaluating on the test split.
I0307 02:59:14.093375 139912818214080 submission_runner.py:469] Time since start: 3398.72s, 	Step: 8030, 	{'train/accuracy': 0.4034598171710968, 'train/loss': 2.741560697555542, 'validation/accuracy': 0.36569997668266296, 'validation/loss': 2.990696907043457, 'validation/num_examples': 50000, 'test/accuracy': 0.28060001134872437, 'test/loss': 3.71583890914917, 'test/num_examples': 10000, 'score': 3119.6531267166138, 'total_duration': 3398.7237045764923, 'accumulated_submission_time': 3119.6531267166138, 'accumulated_eval_time': 277.8167440891266, 'accumulated_logging_time': 0.39674830436706543}
I0307 02:59:14.152916 139758059628288 logging_writer.py:48] [8030] accumulated_eval_time=277.817, accumulated_logging_time=0.396748, accumulated_submission_time=3119.65, global_step=8030, preemption_count=0, score=3119.65, test/accuracy=0.2806, test/loss=3.71584, test/num_examples=10000, total_duration=3398.72, train/accuracy=0.40346, train/loss=2.74156, validation/accuracy=0.3657, validation/loss=2.9907, validation/num_examples=50000
I0307 02:59:41.955687 139758068020992 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9963287115097046, loss=2.720071315765381
I0307 03:00:20.601367 139758059628288 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.0188071727752686, loss=2.5665202140808105
I0307 03:00:59.195589 139758068020992 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0505388975143433, loss=2.582043170928955
I0307 03:01:37.581918 139758059628288 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.001813292503357, loss=2.6142678260803223
I0307 03:02:15.719981 139758068020992 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0521714687347412, loss=2.490706205368042
I0307 03:02:53.902977 139758059628288 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0800445079803467, loss=2.3414981365203857
I0307 03:03:32.654674 139758068020992 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0290268659591675, loss=2.4393045902252197
I0307 03:04:10.803686 139758059628288 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.003983736038208, loss=2.5482919216156006
I0307 03:04:48.534927 139758068020992 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0632116794586182, loss=2.5673489570617676
I0307 03:05:26.694479 139758059628288 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0348011255264282, loss=2.5402023792266846
I0307 03:06:04.886020 139758068020992 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.080451250076294, loss=2.5102598667144775
I0307 03:06:43.120794 139758059628288 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9762240052223206, loss=2.5731005668640137
I0307 03:07:21.287895 139758068020992 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.0196142196655273, loss=2.450197219848633
I0307 03:07:44.239300 139912818214080 spec.py:321] Evaluating on the training split.
I0307 03:07:55.103424 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 03:08:15.653634 139912818214080 spec.py:349] Evaluating on the test split.
I0307 03:08:17.427760 139912818214080 submission_runner.py:469] Time since start: 3942.06s, 	Step: 9361, 	{'train/accuracy': 0.42532286047935486, 'train/loss': 2.592045307159424, 'validation/accuracy': 0.3952599763870239, 'validation/loss': 2.8141472339630127, 'validation/num_examples': 50000, 'test/accuracy': 0.30320000648498535, 'test/loss': 3.5196926593780518, 'test/num_examples': 10000, 'score': 3629.6068353652954, 'total_duration': 3942.0580966472626, 'accumulated_submission_time': 3629.6068353652954, 'accumulated_eval_time': 311.00517106056213, 'accumulated_logging_time': 0.46403074264526367}
I0307 03:08:17.512737 139758059628288 logging_writer.py:48] [9361] accumulated_eval_time=311.005, accumulated_logging_time=0.464031, accumulated_submission_time=3629.61, global_step=9361, preemption_count=0, score=3629.61, test/accuracy=0.3032, test/loss=3.51969, test/num_examples=10000, total_duration=3942.06, train/accuracy=0.425323, train/loss=2.59205, validation/accuracy=0.39526, validation/loss=2.81415, validation/num_examples=50000
I0307 03:08:33.144478 139758068020992 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.9214341640472412, loss=2.492410182952881
I0307 03:09:10.785051 139758059628288 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9321130514144897, loss=2.4808449745178223
I0307 03:09:48.770203 139758068020992 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8917825818061829, loss=2.3970155715942383
I0307 03:10:26.731955 139758059628288 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.9706562161445618, loss=2.4480834007263184
I0307 03:11:04.737468 139758068020992 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.0891633033752441, loss=2.577706813812256
I0307 03:11:42.947211 139758059628288 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.2010400295257568, loss=2.4047064781188965
I0307 03:12:20.967398 139758068020992 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9673588871955872, loss=2.482598304748535
I0307 03:12:59.490587 139758059628288 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9454172253608704, loss=2.313436269760132
I0307 03:13:37.653360 139758068020992 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.0942317247390747, loss=2.5349040031433105
I0307 03:14:15.582262 139758059628288 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.0825426578521729, loss=2.3752875328063965
I0307 03:14:53.913518 139758068020992 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9728896021842957, loss=2.518528461456299
I0307 03:15:31.859447 139758059628288 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9653340578079224, loss=2.4300472736358643
I0307 03:16:10.138163 139758068020992 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.124367594718933, loss=2.498835325241089
I0307 03:16:47.574403 139912818214080 spec.py:321] Evaluating on the training split.
I0307 03:16:59.250313 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 03:17:21.011599 139912818214080 spec.py:349] Evaluating on the test split.
I0307 03:17:22.789457 139912818214080 submission_runner.py:469] Time since start: 4487.42s, 	Step: 10698, 	{'train/accuracy': 0.3426339328289032, 'train/loss': 3.183932065963745, 'validation/accuracy': 0.31365999579429626, 'validation/loss': 3.3830549716949463, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 4.124727725982666, 'test/num_examples': 10000, 'score': 4139.5288438797, 'total_duration': 4487.41979265213, 'accumulated_submission_time': 4139.5288438797, 'accumulated_eval_time': 346.22021484375, 'accumulated_logging_time': 0.5639228820800781}
I0307 03:17:22.830143 139758059628288 logging_writer.py:48] [10698] accumulated_eval_time=346.22, accumulated_logging_time=0.563923, accumulated_submission_time=4139.53, global_step=10698, preemption_count=0, score=4139.53, test/accuracy=0.2416, test/loss=4.12473, test/num_examples=10000, total_duration=4487.42, train/accuracy=0.342634, train/loss=3.18393, validation/accuracy=0.31366, validation/loss=3.38305, validation/num_examples=50000
I0307 03:17:24.659974 139758068020992 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.134416937828064, loss=2.324169874191284
I0307 03:18:02.662023 139758059628288 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.042404294013977, loss=2.402256488800049
I0307 03:18:41.065922 139758068020992 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9952085018157959, loss=2.3075613975524902
I0307 03:19:19.368650 139758059628288 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.1718679666519165, loss=2.491591453552246
I0307 03:19:56.740738 139758068020992 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9854239821434021, loss=2.3743629455566406
I0307 03:20:33.876300 139758059628288 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.141517996788025, loss=2.5729849338531494
I0307 03:21:11.879783 139758068020992 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9225113987922668, loss=2.4342610836029053
I0307 03:21:49.103032 139758059628288 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.0418806076049805, loss=2.4758989810943604
I0307 03:22:53.540621 139758068020992 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9664842486381531, loss=2.4583215713500977
I0307 03:23:35.727803 139758059628288 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.1573129892349243, loss=2.4441840648651123
I0307 03:24:13.949980 139758068020992 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9591109156608582, loss=2.4172897338867188
I0307 03:24:52.438231 139758059628288 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.045556902885437, loss=2.4416491985321045
I0307 03:25:30.623715 139758068020992 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0324087142944336, loss=2.4509029388427734
I0307 03:25:52.866975 139912818214080 spec.py:321] Evaluating on the training split.
I0307 03:26:05.465854 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 03:26:29.525881 139912818214080 spec.py:349] Evaluating on the test split.
I0307 03:26:31.268973 139912818214080 submission_runner.py:469] Time since start: 5035.90s, 	Step: 11959, 	{'train/accuracy': 0.28597337007522583, 'train/loss': 3.6522598266601562, 'validation/accuracy': 0.2602199912071228, 'validation/loss': 3.873152732849121, 'validation/num_examples': 50000, 'test/accuracy': 0.19220000505447388, 'test/loss': 4.627013683319092, 'test/num_examples': 10000, 'score': 4649.423471689224, 'total_duration': 5035.899295806885, 'accumulated_submission_time': 4649.423471689224, 'accumulated_eval_time': 384.6221663951874, 'accumulated_logging_time': 0.6122465133666992}
I0307 03:26:31.346096 139758059628288 logging_writer.py:48] [11959] accumulated_eval_time=384.622, accumulated_logging_time=0.612247, accumulated_submission_time=4649.42, global_step=11959, preemption_count=0, score=4649.42, test/accuracy=0.1922, test/loss=4.62701, test/num_examples=10000, total_duration=5035.9, train/accuracy=0.285973, train/loss=3.65226, validation/accuracy=0.26022, validation/loss=3.87315, validation/num_examples=50000
I0307 03:26:47.794463 139758068020992 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.1076000928878784, loss=2.452275276184082
I0307 03:27:26.196756 139758059628288 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1701748371124268, loss=2.4962034225463867
I0307 03:28:04.712868 139758068020992 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.1091349124908447, loss=2.4328131675720215
I0307 03:28:43.260917 139758059628288 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9191163778305054, loss=2.2974209785461426
I0307 03:29:21.869386 139758068020992 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1048057079315186, loss=2.442997455596924
I0307 03:30:00.153088 139758059628288 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.0965149402618408, loss=2.5241527557373047
I0307 03:30:38.859501 139758068020992 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9697744250297546, loss=2.4027106761932373
I0307 03:31:17.594757 139758059628288 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.1074861288070679, loss=2.445979595184326
I0307 03:31:56.374353 139758068020992 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0004147291183472, loss=2.5711302757263184
I0307 03:32:34.814273 139758059628288 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.099241018295288, loss=2.39312744140625
I0307 03:33:13.015977 139758068020992 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.090164303779602, loss=2.471862554550171
I0307 03:33:51.548671 139758059628288 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.046655297279358, loss=2.447258472442627
I0307 03:34:30.025027 139758068020992 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.08949613571167, loss=2.5318305492401123
I0307 03:35:01.387983 139912818214080 spec.py:321] Evaluating on the training split.
I0307 03:35:20.792725 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 03:35:43.250902 139912818214080 spec.py:349] Evaluating on the test split.
I0307 03:35:45.014687 139912818214080 submission_runner.py:469] Time since start: 5589.65s, 	Step: 13283, 	{'train/accuracy': 0.218610480427742, 'train/loss': 4.382891654968262, 'validation/accuracy': 0.19755999743938446, 'validation/loss': 4.544258117675781, 'validation/num_examples': 50000, 'test/accuracy': 0.14790000021457672, 'test/loss': 5.183443546295166, 'test/num_examples': 10000, 'score': 5159.331223726273, 'total_duration': 5589.645015716553, 'accumulated_submission_time': 5159.331223726273, 'accumulated_eval_time': 428.24883246421814, 'accumulated_logging_time': 0.6965365409851074}
I0307 03:35:45.119570 139758059628288 logging_writer.py:48] [13283] accumulated_eval_time=428.249, accumulated_logging_time=0.696537, accumulated_submission_time=5159.33, global_step=13283, preemption_count=0, score=5159.33, test/accuracy=0.1479, test/loss=5.18344, test/num_examples=10000, total_duration=5589.65, train/accuracy=0.21861, train/loss=4.38289, validation/accuracy=0.19756, validation/loss=4.54426, validation/num_examples=50000
I0307 03:35:52.543805 139758068020992 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.0305850505828857, loss=2.312295913696289
I0307 03:36:30.367705 139758059628288 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.197722315788269, loss=2.372753620147705
I0307 03:37:08.236239 139758068020992 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0478370189666748, loss=2.286050796508789
I0307 03:37:46.057136 139758059628288 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.1046370267868042, loss=2.5819473266601562
I0307 03:38:24.060971 139758068020992 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9727081656455994, loss=2.361240863800049
I0307 03:39:02.308813 139758059628288 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9509687423706055, loss=2.489943504333496
I0307 03:39:40.749281 139758068020992 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.1324570178985596, loss=2.3114147186279297
I0307 03:40:19.157742 139758059628288 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.0368781089782715, loss=2.466892957687378
I0307 03:40:57.444841 139758068020992 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.1676620244979858, loss=2.4751217365264893
I0307 03:41:36.006559 139758059628288 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.050088882446289, loss=2.4766440391540527
I0307 03:42:14.187315 139758068020992 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0758686065673828, loss=2.2816576957702637
I0307 03:42:52.603191 139758059628288 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.198455572128296, loss=2.3751487731933594
I0307 03:43:30.935375 139758068020992 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.9103946685791016, loss=2.2711474895477295
I0307 03:44:11.308403 139758059628288 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0113681554794312, loss=2.3615760803222656
I0307 03:44:15.108061 139912818214080 spec.py:321] Evaluating on the training split.
I0307 03:44:31.011322 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 03:44:54.206284 139912818214080 spec.py:349] Evaluating on the test split.
I0307 03:44:55.960497 139912818214080 submission_runner.py:469] Time since start: 6140.59s, 	Step: 14611, 	{'train/accuracy': 0.16260762512683868, 'train/loss': 4.795761585235596, 'validation/accuracy': 0.15109999477863312, 'validation/loss': 4.97237491607666, 'validation/num_examples': 50000, 'test/accuracy': 0.11820000410079956, 'test/loss': 5.5623555183410645, 'test/num_examples': 10000, 'score': 5669.181756258011, 'total_duration': 6140.590811014175, 'accumulated_submission_time': 5669.181756258011, 'accumulated_eval_time': 469.1012122631073, 'accumulated_logging_time': 0.815903902053833}
I0307 03:44:56.045029 139758068020992 logging_writer.py:48] [14611] accumulated_eval_time=469.101, accumulated_logging_time=0.815904, accumulated_submission_time=5669.18, global_step=14611, preemption_count=0, score=5669.18, test/accuracy=0.1182, test/loss=5.56236, test/num_examples=10000, total_duration=6140.59, train/accuracy=0.162608, train/loss=4.79576, validation/accuracy=0.1511, validation/loss=4.97237, validation/num_examples=50000
I0307 03:45:30.806139 139758059628288 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.040431261062622, loss=2.430687427520752
I0307 03:46:09.058295 139758068020992 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.2374109029769897, loss=2.420919179916382
I0307 03:46:47.704058 139758059628288 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.1974281072616577, loss=2.4164083003997803
I0307 03:47:26.319775 139758068020992 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9819604754447937, loss=2.3122165203094482
I0307 03:48:05.006096 139758059628288 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0808184146881104, loss=2.4625141620635986
I0307 03:48:43.633109 139758068020992 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.113223910331726, loss=2.5614538192749023
I0307 03:49:21.845118 139758059628288 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.0188835859298706, loss=2.4124491214752197
I0307 03:50:00.241086 139758068020992 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.015039324760437, loss=2.2825655937194824
I0307 03:50:38.718265 139758059628288 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.126572847366333, loss=2.263172149658203
I0307 03:51:17.287249 139758068020992 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9185053110122681, loss=2.3735501766204834
I0307 03:51:55.667509 139758059628288 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9134306311607361, loss=2.2611474990844727
I0307 03:52:33.669841 139758068020992 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.022633671760559, loss=2.312932014465332
I0307 03:53:11.789442 139758059628288 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.970111072063446, loss=2.4291765689849854
I0307 03:53:26.058861 139912818214080 spec.py:321] Evaluating on the training split.
I0307 03:53:43.864246 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 03:54:08.187336 139912818214080 spec.py:349] Evaluating on the test split.
I0307 03:54:09.960211 139912818214080 submission_runner.py:469] Time since start: 6694.59s, 	Step: 15938, 	{'train/accuracy': 0.12396364659070969, 'train/loss': 5.637012004852295, 'validation/accuracy': 0.11581999808549881, 'validation/loss': 5.7450385093688965, 'validation/num_examples': 50000, 'test/accuracy': 0.08190000057220459, 'test/loss': 6.5927534103393555, 'test/num_examples': 10000, 'score': 6179.066228866577, 'total_duration': 6694.590527057648, 'accumulated_submission_time': 6179.066228866577, 'accumulated_eval_time': 513.0025107860565, 'accumulated_logging_time': 0.9078075885772705}
I0307 03:54:10.125844 139758068020992 logging_writer.py:48] [15938] accumulated_eval_time=513.003, accumulated_logging_time=0.907808, accumulated_submission_time=6179.07, global_step=15938, preemption_count=0, score=6179.07, test/accuracy=0.0819, test/loss=6.59275, test/num_examples=10000, total_duration=6694.59, train/accuracy=0.123964, train/loss=5.63701, validation/accuracy=0.11582, validation/loss=5.74504, validation/num_examples=50000
I0307 03:54:34.578756 139758059628288 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.0142805576324463, loss=2.2825067043304443
I0307 03:55:12.932981 139758068020992 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.0738933086395264, loss=2.385301113128662
I0307 03:55:51.403844 139758059628288 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.061597466468811, loss=2.4319534301757812
I0307 03:56:29.863291 139758068020992 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.0639989376068115, loss=2.3662607669830322
I0307 03:57:08.437261 139758059628288 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9910722374916077, loss=2.3045284748077393
I0307 03:57:46.920124 139758068020992 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.1009979248046875, loss=2.296909809112549
I0307 03:58:25.351746 139758059628288 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0594735145568848, loss=2.4690802097320557
I0307 03:59:04.016194 139758068020992 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0018055438995361, loss=2.4028806686401367
I0307 03:59:42.557731 139758059628288 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9764664173126221, loss=2.270077705383301
I0307 04:00:21.298047 139758068020992 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8822599649429321, loss=2.3249125480651855
I0307 04:00:59.896848 139758059628288 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.0042991638183594, loss=2.2963361740112305
I0307 04:01:38.115284 139758068020992 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.1213655471801758, loss=2.4472005367279053
I0307 04:02:16.524073 139758059628288 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0436760187149048, loss=2.351233720779419
I0307 04:02:40.284190 139912818214080 spec.py:321] Evaluating on the training split.
I0307 04:02:54.896584 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 04:03:16.889916 139912818214080 spec.py:349] Evaluating on the test split.
I0307 04:03:18.656730 139912818214080 submission_runner.py:469] Time since start: 7243.29s, 	Step: 17263, 	{'train/accuracy': 0.30383050441741943, 'train/loss': 3.4845948219299316, 'validation/accuracy': 0.2800399959087372, 'validation/loss': 3.731367349624634, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.455304145812988, 'test/num_examples': 10000, 'score': 6689.095885276794, 'total_duration': 7243.2870461940765, 'accumulated_submission_time': 6689.095885276794, 'accumulated_eval_time': 551.3749973773956, 'accumulated_logging_time': 1.080744743347168}
I0307 04:03:18.747164 139758068020992 logging_writer.py:48] [17263] accumulated_eval_time=551.375, accumulated_logging_time=1.08074, accumulated_submission_time=6689.1, global_step=17263, preemption_count=0, score=6689.1, test/accuracy=0.2082, test/loss=4.4553, test/num_examples=10000, total_duration=7243.29, train/accuracy=0.303831, train/loss=3.48459, validation/accuracy=0.28004, validation/loss=3.73137, validation/num_examples=50000
I0307 04:03:33.608471 139758059628288 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0689640045166016, loss=2.3719654083251953
I0307 04:04:12.098281 139758068020992 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.030622124671936, loss=2.4368715286254883
I0307 04:04:50.613107 139758059628288 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.9847297668457031, loss=2.271843433380127
I0307 04:05:29.106085 139758068020992 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0640231370925903, loss=2.3678674697875977
I0307 04:06:07.453873 139758059628288 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.0450619459152222, loss=2.384803056716919
I0307 04:06:45.369138 139758068020992 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.0032541751861572, loss=2.3605241775512695
I0307 04:07:23.642601 139758059628288 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.1046148538589478, loss=2.4448888301849365
I0307 04:08:01.937809 139758068020992 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1176344156265259, loss=2.3787455558776855
I0307 04:08:40.079420 139758059628288 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.1263911724090576, loss=2.324604034423828
I0307 04:09:18.443621 139758068020992 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.043372631072998, loss=2.244903326034546
I0307 04:09:56.233751 139758059628288 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.0140612125396729, loss=2.2822582721710205
I0307 04:10:34.637539 139758068020992 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.9675991535186768, loss=2.303555965423584
I0307 04:11:13.186498 139758059628288 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1469284296035767, loss=2.3506386280059814
I0307 04:11:49.033569 139912818214080 spec.py:321] Evaluating on the training split.
I0307 04:12:02.339653 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 04:12:25.182308 139912818214080 spec.py:349] Evaluating on the test split.
I0307 04:12:26.929812 139912818214080 submission_runner.py:469] Time since start: 7791.56s, 	Step: 18595, 	{'train/accuracy': 0.3763352930545807, 'train/loss': 2.8804357051849365, 'validation/accuracy': 0.34373998641967773, 'validation/loss': 3.0832808017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.2556000053882599, 'test/loss': 3.8573572635650635, 'test/num_examples': 10000, 'score': 7199.254940748215, 'total_duration': 7791.56015086174, 'accumulated_submission_time': 7199.254940748215, 'accumulated_eval_time': 589.2712116241455, 'accumulated_logging_time': 1.1785659790039062}
I0307 04:12:26.955164 139758068020992 logging_writer.py:48] [18595] accumulated_eval_time=589.271, accumulated_logging_time=1.17857, accumulated_submission_time=7199.25, global_step=18595, preemption_count=0, score=7199.25, test/accuracy=0.2556, test/loss=3.85736, test/num_examples=10000, total_duration=7791.56, train/accuracy=0.376335, train/loss=2.88044, validation/accuracy=0.34374, validation/loss=3.08328, validation/num_examples=50000
I0307 04:12:29.679340 139758059628288 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0825715065002441, loss=2.4378085136413574
I0307 04:13:07.787485 139758068020992 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.0738651752471924, loss=2.3838863372802734
I0307 04:13:46.419089 139758059628288 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0767723321914673, loss=2.434196710586548
I0307 04:14:25.025096 139758068020992 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.1148920059204102, loss=2.37056565284729
I0307 04:15:03.184518 139758059628288 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9962026476860046, loss=2.3945655822753906
I0307 04:15:41.671341 139758068020992 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0984333753585815, loss=2.3883886337280273
I0307 04:16:20.099952 139758059628288 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.3666337728500366, loss=2.2958648204803467
I0307 04:16:58.760273 139758068020992 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.1681149005889893, loss=2.371945381164551
I0307 04:17:37.198434 139758059628288 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.009979486465454, loss=2.2161293029785156
I0307 04:18:15.998694 139758068020992 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.2831084728240967, loss=2.520186185836792
I0307 04:18:54.374878 139758059628288 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.1316310167312622, loss=2.2590274810791016
I0307 04:19:32.842030 139758068020992 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0911062955856323, loss=2.384784698486328
I0307 04:20:11.145565 139758059628288 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1270744800567627, loss=2.3850152492523193
I0307 04:20:49.278600 139758068020992 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0385620594024658, loss=2.4340102672576904
I0307 04:20:56.935760 139912818214080 spec.py:321] Evaluating on the training split.
I0307 04:21:12.289692 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 04:21:37.212330 139912818214080 spec.py:349] Evaluating on the test split.
I0307 04:21:38.955481 139912818214080 submission_runner.py:469] Time since start: 8343.59s, 	Step: 19921, 	{'train/accuracy': 0.19389747083187103, 'train/loss': 4.529382228851318, 'validation/accuracy': 0.17983999848365784, 'validation/loss': 4.646400451660156, 'validation/num_examples': 50000, 'test/accuracy': 0.12810000777244568, 'test/loss': 5.335065841674805, 'test/num_examples': 10000, 'score': 7709.075330257416, 'total_duration': 8343.58575296402, 'accumulated_submission_time': 7709.075330257416, 'accumulated_eval_time': 631.2908473014832, 'accumulated_logging_time': 1.2385201454162598}
I0307 04:21:39.047293 139758059628288 logging_writer.py:48] [19921] accumulated_eval_time=631.291, accumulated_logging_time=1.23852, accumulated_submission_time=7709.08, global_step=19921, preemption_count=0, score=7709.08, test/accuracy=0.1281, test/loss=5.33507, test/num_examples=10000, total_duration=8343.59, train/accuracy=0.193897, train/loss=4.52938, validation/accuracy=0.17984, validation/loss=4.6464, validation/num_examples=50000
I0307 04:22:09.741860 139758068020992 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1667413711547852, loss=2.447423219680786
I0307 04:22:48.205324 139758059628288 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.1357645988464355, loss=2.4210643768310547
I0307 04:23:26.151098 139758068020992 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1203006505966187, loss=2.333585500717163
I0307 04:24:04.421925 139758059628288 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.9999157786369324, loss=2.2921791076660156
I0307 04:24:43.054044 139758068020992 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0738880634307861, loss=2.3351502418518066
I0307 04:25:21.546969 139758059628288 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.0307420492172241, loss=2.3800675868988037
I0307 04:25:59.723648 139758068020992 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.047471284866333, loss=2.3761208057403564
I0307 04:26:37.948843 139758059628288 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.9960541725158691, loss=2.3637099266052246
I0307 04:27:16.167212 139758068020992 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0385541915893555, loss=2.246570110321045
I0307 04:27:54.553625 139758059628288 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.0496104955673218, loss=2.267066240310669
I0307 04:28:32.767264 139758068020992 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0411280393600464, loss=2.391602039337158
I0307 04:29:10.960593 139758059628288 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0652052164077759, loss=2.247230052947998
I0307 04:29:49.035186 139758068020992 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.0346029996871948, loss=2.4016757011413574
I0307 04:30:09.292196 139912818214080 spec.py:321] Evaluating on the training split.
I0307 04:30:22.629649 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 04:30:44.561250 139912818214080 spec.py:349] Evaluating on the test split.
I0307 04:30:46.359145 139912818214080 submission_runner.py:469] Time since start: 8890.96s, 	Step: 21254, 	{'train/accuracy': 0.2977120578289032, 'train/loss': 3.6996922492980957, 'validation/accuracy': 0.2754800021648407, 'validation/loss': 3.8890020847320557, 'validation/num_examples': 50000, 'test/accuracy': 0.21160000562667847, 'test/loss': 4.60891056060791, 'test/num_examples': 10000, 'score': 8219.180475473404, 'total_duration': 8890.96362566948, 'accumulated_submission_time': 8219.180475473404, 'accumulated_eval_time': 668.3319108486176, 'accumulated_logging_time': 1.3454127311706543}
I0307 04:30:46.457020 139758059628288 logging_writer.py:48] [21254] accumulated_eval_time=668.332, accumulated_logging_time=1.34541, accumulated_submission_time=8219.18, global_step=21254, preemption_count=0, score=8219.18, test/accuracy=0.2116, test/loss=4.60891, test/num_examples=10000, total_duration=8890.96, train/accuracy=0.297712, train/loss=3.69969, validation/accuracy=0.27548, validation/loss=3.889, validation/num_examples=50000
I0307 04:31:04.627457 139758068020992 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0192898511886597, loss=2.444692611694336
I0307 04:31:42.892288 139758059628288 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.008992075920105, loss=2.2324740886688232
I0307 04:32:21.366516 139758068020992 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.186926007270813, loss=2.378743886947632
I0307 04:33:00.025759 139758059628288 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.003841519355774, loss=2.2156360149383545
I0307 04:33:38.535192 139758068020992 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2184633016586304, loss=2.4473652839660645
I0307 04:34:16.669740 139758059628288 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2267272472381592, loss=2.4127912521362305
I0307 04:34:55.170348 139758068020992 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.0894405841827393, loss=2.3981566429138184
I0307 04:35:33.820575 139758059628288 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0644408464431763, loss=2.381814956665039
I0307 04:36:12.251075 139758068020992 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1585773229599, loss=2.3991403579711914
I0307 04:36:50.306269 139758059628288 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2340830564498901, loss=2.316011428833008
I0307 04:37:28.410030 139758068020992 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0097556114196777, loss=2.232149839401245
I0307 04:38:06.688937 139758059628288 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.0424937009811401, loss=2.246898651123047
I0307 04:38:45.075544 139758068020992 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.09138023853302, loss=2.478090524673462
I0307 04:39:16.345831 139912818214080 spec.py:321] Evaluating on the training split.
I0307 04:39:29.599886 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 04:39:51.762094 139912818214080 spec.py:349] Evaluating on the test split.
I0307 04:39:53.516122 139912818214080 submission_runner.py:469] Time since start: 9438.15s, 	Step: 22582, 	{'train/accuracy': 0.3062220811843872, 'train/loss': 3.46956729888916, 'validation/accuracy': 0.28735998272895813, 'validation/loss': 3.6494970321655273, 'validation/num_examples': 50000, 'test/accuracy': 0.21560001373291016, 'test/loss': 4.420133113861084, 'test/num_examples': 10000, 'score': 8728.941935539246, 'total_duration': 9438.146457910538, 'accumulated_submission_time': 8728.941935539246, 'accumulated_eval_time': 705.5021767616272, 'accumulated_logging_time': 1.4514343738555908}
I0307 04:39:53.539117 139758059628288 logging_writer.py:48] [22582] accumulated_eval_time=705.502, accumulated_logging_time=1.45143, accumulated_submission_time=8728.94, global_step=22582, preemption_count=0, score=8728.94, test/accuracy=0.2156, test/loss=4.42013, test/num_examples=10000, total_duration=9438.15, train/accuracy=0.306222, train/loss=3.46957, validation/accuracy=0.28736, validation/loss=3.6495, validation/num_examples=50000
I0307 04:40:01.322115 139758068020992 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.147310495376587, loss=2.3437857627868652
I0307 04:40:39.512935 139758059628288 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.0640733242034912, loss=2.288914442062378
I0307 04:41:17.741608 139758068020992 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0621662139892578, loss=2.3748745918273926
I0307 04:41:55.689010 139758059628288 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.1144531965255737, loss=2.331387519836426
I0307 04:42:33.842483 139758068020992 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.1138858795166016, loss=2.3618876934051514
I0307 04:43:12.131442 139758059628288 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0736849308013916, loss=2.2631828784942627
I0307 04:43:50.109506 139758068020992 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.1088162660598755, loss=2.3202223777770996
I0307 04:44:28.458263 139758059628288 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.0605446100234985, loss=2.3938589096069336
I0307 04:45:06.808047 139758068020992 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0759809017181396, loss=2.357862949371338
I0307 04:45:45.425760 139758059628288 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.9769624471664429, loss=2.243953227996826
I0307 04:46:23.651807 139758068020992 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.1726881265640259, loss=2.2976362705230713
I0307 04:47:02.018414 139758059628288 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.1448638439178467, loss=2.387251853942871
I0307 04:47:48.035017 139758068020992 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.9734079241752625, loss=2.2521159648895264
I0307 04:48:23.812455 139912818214080 spec.py:321] Evaluating on the training split.
I0307 04:48:40.707862 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 04:49:01.385456 139912818214080 spec.py:349] Evaluating on the test split.
I0307 04:49:03.187553 139912818214080 submission_runner.py:469] Time since start: 9987.82s, 	Step: 23886, 	{'train/accuracy': 0.1809430718421936, 'train/loss': 4.816465854644775, 'validation/accuracy': 0.17107999324798584, 'validation/loss': 4.9599432945251465, 'validation/num_examples': 50000, 'test/accuracy': 0.12790000438690186, 'test/loss': 5.585414886474609, 'test/num_examples': 10000, 'score': 9239.083890199661, 'total_duration': 9987.817855119705, 'accumulated_submission_time': 9239.083890199661, 'accumulated_eval_time': 744.8772070407867, 'accumulated_logging_time': 1.48252272605896}
I0307 04:49:03.247458 139758059628288 logging_writer.py:48] [23886] accumulated_eval_time=744.877, accumulated_logging_time=1.48252, accumulated_submission_time=9239.08, global_step=23886, preemption_count=0, score=9239.08, test/accuracy=0.1279, test/loss=5.58541, test/num_examples=10000, total_duration=9987.82, train/accuracy=0.180943, train/loss=4.81647, validation/accuracy=0.17108, validation/loss=4.95994, validation/num_examples=50000
I0307 04:49:08.888858 139758068020992 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.042293667793274, loss=2.373762845993042
I0307 04:49:47.372631 139758059628288 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2087316513061523, loss=2.3907904624938965
I0307 04:50:25.547285 139758068020992 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.1836161613464355, loss=2.2967729568481445
I0307 04:51:03.788986 139758059628288 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.1469169855117798, loss=2.3482165336608887
I0307 04:51:42.232067 139758068020992 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.2212413549423218, loss=2.317422866821289
I0307 04:52:20.598526 139758059628288 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0134276151657104, loss=2.3874034881591797
I0307 04:52:58.741125 139758068020992 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0615218877792358, loss=2.315471649169922
I0307 04:53:37.238113 139758059628288 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.216003179550171, loss=2.215219259262085
I0307 04:54:15.660697 139758068020992 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0807867050170898, loss=2.209094285964966
I0307 04:54:54.176585 139758059628288 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0893750190734863, loss=2.2483866214752197
I0307 04:55:32.597211 139758068020992 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0510598421096802, loss=2.4468436241149902
I0307 04:56:10.706110 139758059628288 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0439950227737427, loss=2.242183208465576
I0307 04:56:49.337079 139758068020992 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.2388955354690552, loss=2.461779832839966
I0307 04:57:27.637044 139758059628288 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0724680423736572, loss=2.409320592880249
I0307 04:57:33.319458 139912818214080 spec.py:321] Evaluating on the training split.
I0307 04:57:46.978285 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 04:58:08.990805 139912818214080 spec.py:349] Evaluating on the test split.
I0307 04:58:10.774341 139912818214080 submission_runner.py:469] Time since start: 10535.39s, 	Step: 25216, 	{'train/accuracy': 0.2444196343421936, 'train/loss': 3.90531063079834, 'validation/accuracy': 0.23587998747825623, 'validation/loss': 4.027276515960693, 'validation/num_examples': 50000, 'test/accuracy': 0.17240001261234283, 'test/loss': 4.772428512573242, 'test/num_examples': 10000, 'score': 9749.014069080353, 'total_duration': 10535.390718221664, 'accumulated_submission_time': 9749.014069080353, 'accumulated_eval_time': 782.3180966377258, 'accumulated_logging_time': 1.5590429306030273}
I0307 04:58:10.817293 139758068020992 logging_writer.py:48] [25216] accumulated_eval_time=782.318, accumulated_logging_time=1.55904, accumulated_submission_time=9749.01, global_step=25216, preemption_count=0, score=9749.01, test/accuracy=0.1724, test/loss=4.77243, test/num_examples=10000, total_duration=10535.4, train/accuracy=0.24442, train/loss=3.90531, validation/accuracy=0.23588, validation/loss=4.02728, validation/num_examples=50000
I0307 04:58:43.903068 139758059628288 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.0534894466400146, loss=2.3245224952697754
I0307 04:59:22.155281 139758068020992 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0676161050796509, loss=2.324537754058838
I0307 05:00:00.469342 139758059628288 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1734493970870972, loss=2.3894402980804443
I0307 05:00:39.128273 139758068020992 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1467324495315552, loss=2.306570291519165
I0307 05:01:17.771885 139758059628288 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1095876693725586, loss=2.2309410572052
I0307 05:01:56.306992 139758068020992 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.2107727527618408, loss=2.3638405799865723
I0307 05:02:35.050194 139758059628288 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0742592811584473, loss=2.3331055641174316
I0307 05:03:13.506763 139758068020992 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.1877384185791016, loss=2.247203826904297
I0307 05:03:51.888772 139758059628288 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9680106043815613, loss=2.2556886672973633
I0307 05:04:30.399232 139758068020992 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.33150315284729, loss=2.274914503097534
I0307 05:05:09.085273 139758059628288 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.1190121173858643, loss=2.374354124069214
I0307 05:05:47.507276 139758068020992 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.00057053565979, loss=2.278040885925293
I0307 05:06:26.023748 139758059628288 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1106034517288208, loss=2.4045615196228027
I0307 05:06:40.903674 139912818214080 spec.py:321] Evaluating on the training split.
I0307 05:06:56.289326 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 05:07:17.845489 139912818214080 spec.py:349] Evaluating on the test split.
I0307 05:07:19.625359 139912818214080 submission_runner.py:469] Time since start: 11084.26s, 	Step: 26539, 	{'train/accuracy': 0.12880659103393555, 'train/loss': 5.402222633361816, 'validation/accuracy': 0.12727999687194824, 'validation/loss': 5.434912204742432, 'validation/num_examples': 50000, 'test/accuracy': 0.09180000424385071, 'test/loss': 6.047041893005371, 'test/num_examples': 10000, 'score': 10258.950868606567, 'total_duration': 11084.255680322647, 'accumulated_submission_time': 10258.950868606567, 'accumulated_eval_time': 821.0397350788116, 'accumulated_logging_time': 1.624593734741211}
I0307 05:07:19.752221 139758068020992 logging_writer.py:48] [26539] accumulated_eval_time=821.04, accumulated_logging_time=1.62459, accumulated_submission_time=10259, global_step=26539, preemption_count=0, score=10259, test/accuracy=0.0918, test/loss=6.04704, test/num_examples=10000, total_duration=11084.3, train/accuracy=0.128807, train/loss=5.40222, validation/accuracy=0.12728, validation/loss=5.43491, validation/num_examples=50000
I0307 05:07:43.856047 139758059628288 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0600826740264893, loss=2.4186031818389893
I0307 05:08:22.350306 139758068020992 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0663124322891235, loss=2.28134822845459
I0307 05:09:00.671862 139758059628288 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.091615915298462, loss=2.281424045562744
I0307 05:09:38.956114 139758068020992 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1198906898498535, loss=2.221776008605957
I0307 05:10:17.723346 139758059628288 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.101257562637329, loss=2.3772993087768555
I0307 05:10:55.666874 139758068020992 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2221324443817139, loss=2.3490095138549805
I0307 05:11:34.013525 139758059628288 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.9933533668518066, loss=2.2155380249023438
I0307 05:12:12.544983 139758068020992 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.0227943658828735, loss=2.263796091079712
I0307 05:12:50.759082 139758059628288 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.0638060569763184, loss=2.5225284099578857
I0307 05:13:29.115666 139758068020992 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.2892764806747437, loss=2.468935489654541
I0307 05:14:07.293269 139758059628288 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.1279292106628418, loss=2.3084733486175537
I0307 05:14:45.488162 139758068020992 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.361384630203247, loss=2.4553639888763428
I0307 05:15:24.011856 139758059628288 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.082197904586792, loss=2.216822862625122
I0307 05:15:49.820938 139912818214080 spec.py:321] Evaluating on the training split.
I0307 05:16:04.179819 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 05:16:25.033345 139912818214080 spec.py:349] Evaluating on the test split.
I0307 05:16:26.828094 139912818214080 submission_runner.py:469] Time since start: 11631.46s, 	Step: 27868, 	{'train/accuracy': 0.07362085580825806, 'train/loss': 5.873807430267334, 'validation/accuracy': 0.06932000070810318, 'validation/loss': 5.973196506500244, 'validation/num_examples': 50000, 'test/accuracy': 0.05470000207424164, 'test/loss': 6.279242992401123, 'test/num_examples': 10000, 'score': 10768.8849670887, 'total_duration': 11631.458408355713, 'accumulated_submission_time': 10768.8849670887, 'accumulated_eval_time': 858.046835899353, 'accumulated_logging_time': 1.7599942684173584}
I0307 05:16:26.982825 139758068020992 logging_writer.py:48] [27868] accumulated_eval_time=858.047, accumulated_logging_time=1.75999, accumulated_submission_time=10768.9, global_step=27868, preemption_count=0, score=10768.9, test/accuracy=0.0547, test/loss=6.27924, test/num_examples=10000, total_duration=11631.5, train/accuracy=0.0736209, train/loss=5.87381, validation/accuracy=0.06932, validation/loss=5.9732, validation/num_examples=50000
I0307 05:16:39.957180 139758059628288 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0823829174041748, loss=2.3341901302337646
I0307 05:17:18.325132 139758068020992 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.986755907535553, loss=2.2673113346099854
I0307 05:17:56.696035 139758059628288 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3083165884017944, loss=2.342127799987793
I0307 05:18:35.188347 139758068020992 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.1931514739990234, loss=2.3976047039031982
I0307 05:19:13.468422 139758059628288 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.1989668607711792, loss=2.3168811798095703
I0307 05:19:51.346154 139758068020992 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.152060866355896, loss=2.252354860305786
I0307 05:20:29.284642 139758059628288 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1399102210998535, loss=2.2999932765960693
I0307 05:21:07.418032 139758068020992 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2730820178985596, loss=2.3374481201171875
I0307 05:21:45.661416 139758059628288 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.1684225797653198, loss=2.312399387359619
I0307 05:22:23.789932 139758068020992 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.1171653270721436, loss=2.443028450012207
I0307 05:23:02.642606 139758059628288 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.1469889879226685, loss=2.3090639114379883
I0307 05:23:40.825684 139758068020992 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.0700709819793701, loss=2.1692018508911133
I0307 05:24:19.381457 139758059628288 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.1278767585754395, loss=2.3185436725616455
I0307 05:24:56.998627 139912818214080 spec.py:321] Evaluating on the training split.
I0307 05:25:11.938962 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 05:25:33.956832 139912818214080 spec.py:349] Evaluating on the test split.
I0307 05:25:35.745403 139912818214080 submission_runner.py:469] Time since start: 12180.38s, 	Step: 29200, 	{'train/accuracy': 0.2259446680545807, 'train/loss': 4.034019947052002, 'validation/accuracy': 0.21357999742031097, 'validation/loss': 4.158266067504883, 'validation/num_examples': 50000, 'test/accuracy': 0.15410000085830688, 'test/loss': 4.854555130004883, 'test/num_examples': 10000, 'score': 11278.755184650421, 'total_duration': 12180.375717163086, 'accumulated_submission_time': 11278.755184650421, 'accumulated_eval_time': 896.7935652732849, 'accumulated_logging_time': 1.932185411453247}
I0307 05:25:35.849015 139758068020992 logging_writer.py:48] [29200] accumulated_eval_time=896.794, accumulated_logging_time=1.93219, accumulated_submission_time=11278.8, global_step=29200, preemption_count=0, score=11278.8, test/accuracy=0.1541, test/loss=4.85456, test/num_examples=10000, total_duration=12180.4, train/accuracy=0.225945, train/loss=4.03402, validation/accuracy=0.21358, validation/loss=4.15827, validation/num_examples=50000
I0307 05:25:36.591363 139758059628288 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.1516159772872925, loss=2.4197616577148438
I0307 05:26:14.941624 139758068020992 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.2540504932403564, loss=2.4252591133117676
I0307 05:26:53.304488 139758059628288 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.1437346935272217, loss=2.3029887676239014
I0307 05:27:31.710545 139758068020992 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.1335997581481934, loss=2.2700157165527344
I0307 05:28:09.932364 139758059628288 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.1816372871398926, loss=2.289595603942871
I0307 05:28:48.217004 139758068020992 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.1448019742965698, loss=2.271732807159424
I0307 05:29:26.778674 139758059628288 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.046822428703308, loss=2.2359514236450195
I0307 05:30:05.114059 139758068020992 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.1229372024536133, loss=2.417085647583008
I0307 05:30:43.191930 139758059628288 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.0996454954147339, loss=2.149791955947876
I0307 05:31:21.489678 139758068020992 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0890778303146362, loss=2.2139804363250732
I0307 05:31:59.994510 139758059628288 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.1085618734359741, loss=2.246954917907715
I0307 05:32:38.255512 139758068020992 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.1214898824691772, loss=2.306720733642578
I0307 05:33:16.634196 139758059628288 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.3249566555023193, loss=2.346865653991699
I0307 05:33:54.777011 139758068020992 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0936518907546997, loss=2.2147903442382812
I0307 05:34:05.858250 139912818214080 spec.py:321] Evaluating on the training split.
I0307 05:34:23.894461 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 05:34:45.662393 139912818214080 spec.py:349] Evaluating on the test split.
I0307 05:34:47.432831 139912818214080 submission_runner.py:469] Time since start: 12732.06s, 	Step: 30530, 	{'train/accuracy': 0.30466756224632263, 'train/loss': 3.3982272148132324, 'validation/accuracy': 0.28832000494003296, 'validation/loss': 3.5290591716766357, 'validation/num_examples': 50000, 'test/accuracy': 0.2184000164270401, 'test/loss': 4.302485466003418, 'test/num_examples': 10000, 'score': 11788.61396408081, 'total_duration': 12732.06314778328, 'accumulated_submission_time': 11788.61396408081, 'accumulated_eval_time': 938.3680939674377, 'accumulated_logging_time': 2.0627119541168213}
I0307 05:34:47.539473 139758059628288 logging_writer.py:48] [30530] accumulated_eval_time=938.368, accumulated_logging_time=2.06271, accumulated_submission_time=11788.6, global_step=30530, preemption_count=0, score=11788.6, test/accuracy=0.2184, test/loss=4.30249, test/num_examples=10000, total_duration=12732.1, train/accuracy=0.304668, train/loss=3.39823, validation/accuracy=0.28832, validation/loss=3.52906, validation/num_examples=50000
I0307 05:35:15.416991 139758068020992 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.1656025648117065, loss=2.3039417266845703
I0307 05:35:53.628929 139758059628288 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.2637112140655518, loss=2.334857940673828
I0307 05:36:32.040202 139758068020992 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.229171633720398, loss=2.3331007957458496
I0307 05:37:10.274882 139758059628288 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0967669486999512, loss=2.3104429244995117
I0307 05:37:48.718898 139758068020992 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1503957509994507, loss=2.234025001525879
I0307 05:38:26.979563 139758059628288 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.145557165145874, loss=2.3886663913726807
I0307 05:39:04.976223 139758068020992 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.1140289306640625, loss=2.42177152633667
I0307 05:39:43.333312 139758059628288 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.1442878246307373, loss=2.3330743312835693
I0307 05:40:21.672757 139758068020992 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.1079057455062866, loss=2.2637767791748047
I0307 05:40:59.844141 139758059628288 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.2321099042892456, loss=2.2831332683563232
I0307 05:41:38.258096 139758068020992 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.159751296043396, loss=2.166428804397583
I0307 05:42:16.368882 139758059628288 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1343064308166504, loss=2.264871597290039
I0307 05:42:54.742641 139758068020992 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.1003413200378418, loss=2.305359363555908
I0307 05:43:17.753944 139912818214080 spec.py:321] Evaluating on the training split.
I0307 05:43:33.987396 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 05:43:53.558894 139912818214080 spec.py:349] Evaluating on the test split.
I0307 05:43:55.304484 139912818214080 submission_runner.py:469] Time since start: 13279.93s, 	Step: 31861, 	{'train/accuracy': 0.3386479616165161, 'train/loss': 3.306255578994751, 'validation/accuracy': 0.30875998735427856, 'validation/loss': 3.505404233932495, 'validation/num_examples': 50000, 'test/accuracy': 0.2314000129699707, 'test/loss': 4.227850437164307, 'test/num_examples': 10000, 'score': 12298.672335624695, 'total_duration': 13279.934798002243, 'accumulated_submission_time': 12298.672335624695, 'accumulated_eval_time': 975.9185807704926, 'accumulated_logging_time': 2.2037296295166016}
I0307 05:43:55.398535 139758059628288 logging_writer.py:48] [31861] accumulated_eval_time=975.919, accumulated_logging_time=2.20373, accumulated_submission_time=12298.7, global_step=31861, preemption_count=0, score=12298.7, test/accuracy=0.2314, test/loss=4.22785, test/num_examples=10000, total_duration=13279.9, train/accuracy=0.338648, train/loss=3.30626, validation/accuracy=0.30876, validation/loss=3.5054, validation/num_examples=50000
I0307 05:44:10.929310 139758068020992 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.114986777305603, loss=2.248539924621582
I0307 05:44:49.237031 139758059628288 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.029054880142212, loss=2.2594761848449707
I0307 05:45:27.676322 139758068020992 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0940985679626465, loss=2.408447504043579
I0307 05:46:06.381862 139758059628288 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.1439287662506104, loss=2.524104595184326
I0307 05:46:44.810130 139758068020992 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.158399224281311, loss=2.3785440921783447
I0307 05:47:23.069659 139758059628288 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.1519954204559326, loss=2.376126289367676
I0307 05:48:01.313040 139758068020992 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0544402599334717, loss=2.3047094345092773
I0307 05:48:39.364175 139758059628288 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.090633749961853, loss=2.2540132999420166
I0307 05:49:17.504750 139758068020992 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1146513223648071, loss=2.304668426513672
I0307 05:49:55.956736 139758059628288 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.078899621963501, loss=2.2988333702087402
I0307 05:50:34.116864 139758068020992 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.1691114902496338, loss=2.209282159805298
I0307 05:51:12.185765 139758059628288 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.134496808052063, loss=2.3015451431274414
I0307 05:51:50.792435 139758068020992 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.0785022974014282, loss=2.2457451820373535
I0307 05:52:25.478674 139912818214080 spec.py:321] Evaluating on the training split.
I0307 05:52:36.995093 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 05:53:05.214354 139912818214080 spec.py:349] Evaluating on the test split.
I0307 05:53:06.944306 139912818214080 submission_runner.py:469] Time since start: 13831.57s, 	Step: 33191, 	{'train/accuracy': 0.07876275479793549, 'train/loss': 6.050768852233887, 'validation/accuracy': 0.0728599950671196, 'validation/loss': 6.17998743057251, 'validation/num_examples': 50000, 'test/accuracy': 0.05450000241398811, 'test/loss': 6.591578483581543, 'test/num_examples': 10000, 'score': 12808.601141929626, 'total_duration': 13831.57458782196, 'accumulated_submission_time': 12808.601141929626, 'accumulated_eval_time': 1017.3841388225555, 'accumulated_logging_time': 2.323406219482422}
I0307 05:53:07.064748 139758059628288 logging_writer.py:48] [33191] accumulated_eval_time=1017.38, accumulated_logging_time=2.32341, accumulated_submission_time=12808.6, global_step=33191, preemption_count=0, score=12808.6, test/accuracy=0.0545, test/loss=6.59158, test/num_examples=10000, total_duration=13831.6, train/accuracy=0.0787628, train/loss=6.05077, validation/accuracy=0.07286, validation/loss=6.17999, validation/num_examples=50000
I0307 05:53:10.915297 139758068020992 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.132104754447937, loss=2.255006790161133
I0307 05:53:49.405360 139758059628288 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.138190746307373, loss=2.3209228515625
I0307 05:54:27.510937 139758068020992 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1681928634643555, loss=2.283640146255493
I0307 05:55:05.974437 139758059628288 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.2121498584747314, loss=2.335520029067993
I0307 05:55:44.093944 139758068020992 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.091455340385437, loss=2.250614881515503
I0307 05:56:22.532341 139758059628288 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.1374448537826538, loss=2.238024950027466
I0307 05:57:00.861759 139758068020992 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1272600889205933, loss=2.409290075302124
I0307 05:57:39.260184 139758059628288 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.0490548610687256, loss=2.338176965713501
I0307 05:58:17.571439 139758068020992 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.26608145236969, loss=2.309134006500244
I0307 05:58:55.699286 139758059628288 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.1675875186920166, loss=2.3349409103393555
I0307 05:59:33.998289 139758068020992 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.124160885810852, loss=2.385021209716797
I0307 06:00:11.977468 139758059628288 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.1434093713760376, loss=2.2857892513275146
I0307 06:00:50.255075 139758068020992 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.2456374168395996, loss=2.3330259323120117
I0307 06:01:28.897492 139758059628288 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.0820804834365845, loss=2.234243631362915
I0307 06:01:37.090221 139912818214080 spec.py:321] Evaluating on the training split.
I0307 06:01:49.173513 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 06:02:13.070785 139912818214080 spec.py:349] Evaluating on the test split.
I0307 06:02:14.816538 139912818214080 submission_runner.py:469] Time since start: 14379.45s, 	Step: 34522, 	{'train/accuracy': 0.21404655277729034, 'train/loss': 4.391355037689209, 'validation/accuracy': 0.2027599960565567, 'validation/loss': 4.467723846435547, 'validation/num_examples': 50000, 'test/accuracy': 0.14710000157356262, 'test/loss': 5.210992336273193, 'test/num_examples': 10000, 'score': 13318.472622156143, 'total_duration': 14379.44684457779, 'accumulated_submission_time': 13318.472622156143, 'accumulated_eval_time': 1055.1103928089142, 'accumulated_logging_time': 2.466771364212036}
I0307 06:02:14.972445 139758068020992 logging_writer.py:48] [34522] accumulated_eval_time=1055.11, accumulated_logging_time=2.46677, accumulated_submission_time=13318.5, global_step=34522, preemption_count=0, score=13318.5, test/accuracy=0.1471, test/loss=5.21099, test/num_examples=10000, total_duration=14379.4, train/accuracy=0.214047, train/loss=4.39136, validation/accuracy=0.20276, validation/loss=4.46772, validation/num_examples=50000
I0307 06:02:45.112733 139758059628288 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.2657362222671509, loss=2.2283120155334473
I0307 06:03:23.504591 139758068020992 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.1212900876998901, loss=2.3181257247924805
I0307 06:04:01.990928 139758059628288 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.184946060180664, loss=2.2459793090820312
I0307 06:04:40.064371 139758068020992 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.1505824327468872, loss=2.3254618644714355
I0307 06:05:18.137479 139758059628288 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.3249973058700562, loss=2.2380943298339844
I0307 06:05:56.508627 139758068020992 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1539620161056519, loss=2.338947057723999
I0307 06:06:34.656873 139758059628288 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.143218755722046, loss=2.298682928085327
I0307 06:07:12.908856 139758068020992 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.2888635396957397, loss=2.302272081375122
I0307 06:07:51.376105 139758059628288 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.1820164918899536, loss=2.2613680362701416
I0307 06:08:29.302479 139758068020992 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.0811686515808105, loss=2.368149518966675
I0307 06:09:07.791673 139758059628288 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.133234977722168, loss=2.306356191635132
I0307 06:09:46.288703 139758068020992 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.1330103874206543, loss=2.1955695152282715
I0307 06:10:25.117253 139758059628288 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1177797317504883, loss=2.248499631881714
I0307 06:10:44.958372 139912818214080 spec.py:321] Evaluating on the training split.
I0307 06:10:56.829146 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 06:11:20.908568 139912818214080 spec.py:349] Evaluating on the test split.
I0307 06:11:22.633841 139912818214080 submission_runner.py:469] Time since start: 14927.26s, 	Step: 35853, 	{'train/accuracy': 0.318359375, 'train/loss': 3.36989164352417, 'validation/accuracy': 0.29971998929977417, 'validation/loss': 3.5208065509796143, 'validation/num_examples': 50000, 'test/accuracy': 0.2217000126838684, 'test/loss': 4.265127658843994, 'test/num_examples': 10000, 'score': 13828.306865930557, 'total_duration': 14927.264145374298, 'accumulated_submission_time': 13828.306865930557, 'accumulated_eval_time': 1092.7857944965363, 'accumulated_logging_time': 2.64522385597229}
I0307 06:11:22.721277 139758068020992 logging_writer.py:48] [35853] accumulated_eval_time=1092.79, accumulated_logging_time=2.64522, accumulated_submission_time=13828.3, global_step=35853, preemption_count=0, score=13828.3, test/accuracy=0.2217, test/loss=4.26513, test/num_examples=10000, total_duration=14927.3, train/accuracy=0.318359, train/loss=3.36989, validation/accuracy=0.29972, validation/loss=3.52081, validation/num_examples=50000
I0307 06:11:40.920591 139758059628288 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.2225309610366821, loss=2.223968029022217
I0307 06:12:18.848452 139758068020992 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1573994159698486, loss=2.3015012741088867
I0307 06:12:57.237447 139758059628288 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.1788358688354492, loss=2.3966381549835205
I0307 06:13:35.287936 139758068020992 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1739356517791748, loss=2.1468305587768555
I0307 06:14:13.545746 139758059628288 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1150157451629639, loss=2.287560224533081
I0307 06:14:51.235688 139758068020992 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.077910304069519, loss=2.4573917388916016
I0307 06:15:29.546912 139758059628288 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1104460954666138, loss=2.3040177822113037
I0307 06:16:08.233402 139758068020992 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.080350637435913, loss=2.2453036308288574
I0307 06:16:46.560087 139758059628288 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.212825894355774, loss=2.3832075595855713
I0307 06:17:24.781190 139758068020992 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.1699061393737793, loss=2.360889434814453
I0307 06:18:03.247962 139758059628288 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.2113096714019775, loss=2.3804194927215576
I0307 06:18:41.425622 139758068020992 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0557578802108765, loss=2.3058204650878906
I0307 06:19:19.574960 139758059628288 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.2323566675186157, loss=2.3148996829986572
I0307 06:19:53.002939 139912818214080 spec.py:321] Evaluating on the training split.
I0307 06:20:04.803544 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 06:20:29.787478 139912818214080 spec.py:349] Evaluating on the test split.
I0307 06:20:31.513196 139912818214080 submission_runner.py:469] Time since start: 15476.14s, 	Step: 37189, 	{'train/accuracy': 0.2909359037876129, 'train/loss': 3.5204501152038574, 'validation/accuracy': 0.27619999647140503, 'validation/loss': 3.640540599822998, 'validation/num_examples': 50000, 'test/accuracy': 0.1973000019788742, 'test/loss': 4.319897651672363, 'test/num_examples': 10000, 'score': 14338.42866897583, 'total_duration': 15476.143499135971, 'accumulated_submission_time': 14338.42866897583, 'accumulated_eval_time': 1131.2959897518158, 'accumulated_logging_time': 2.762072801589966}
I0307 06:20:31.632926 139758068020992 logging_writer.py:48] [37189] accumulated_eval_time=1131.3, accumulated_logging_time=2.76207, accumulated_submission_time=14338.4, global_step=37189, preemption_count=0, score=14338.4, test/accuracy=0.1973, test/loss=4.3199, test/num_examples=10000, total_duration=15476.1, train/accuracy=0.290936, train/loss=3.52045, validation/accuracy=0.2762, validation/loss=3.64054, validation/num_examples=50000
I0307 06:20:36.306052 139758059628288 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.0961405038833618, loss=2.2213292121887207
I0307 06:21:14.647279 139758068020992 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.1031428575515747, loss=2.2849841117858887
I0307 06:21:53.032531 139758059628288 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.1545836925506592, loss=2.3087527751922607
I0307 06:22:31.092975 139758068020992 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1304655075073242, loss=2.3237595558166504
I0307 06:23:09.553181 139758059628288 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.0852001905441284, loss=2.338406801223755
I0307 06:23:47.743589 139758068020992 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.1196857690811157, loss=2.2372124195098877
I0307 06:24:26.127036 139758059628288 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.120181679725647, loss=2.3193137645721436
I0307 06:25:04.232951 139758068020992 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.229597568511963, loss=2.365797281265259
I0307 06:25:42.464047 139758059628288 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.173985242843628, loss=2.239973783493042
I0307 06:26:20.411290 139758068020992 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1306508779525757, loss=2.2648215293884277
I0307 06:26:58.606913 139758059628288 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1842186450958252, loss=2.265207529067993
I0307 06:27:36.777364 139758068020992 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.105027437210083, loss=2.3438596725463867
I0307 06:28:15.119139 139758059628288 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.0566797256469727, loss=2.314448595046997
I0307 06:28:53.055387 139758068020992 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.332762360572815, loss=2.31119441986084
I0307 06:29:01.597537 139912818214080 spec.py:321] Evaluating on the training split.
I0307 06:29:13.422610 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 06:29:35.899369 139912818214080 spec.py:349] Evaluating on the test split.
I0307 06:29:37.638985 139912818214080 submission_runner.py:469] Time since start: 16022.27s, 	Step: 38523, 	{'train/accuracy': 0.21765385568141937, 'train/loss': 4.412486553192139, 'validation/accuracy': 0.2094999998807907, 'validation/loss': 4.522482872009277, 'validation/num_examples': 50000, 'test/accuracy': 0.15930001437664032, 'test/loss': 5.155503749847412, 'test/num_examples': 10000, 'score': 14848.20858669281, 'total_duration': 16022.269285440445, 'accumulated_submission_time': 14848.20858669281, 'accumulated_eval_time': 1167.337368965149, 'accumulated_logging_time': 2.9397571086883545}
I0307 06:29:37.765665 139758059628288 logging_writer.py:48] [38523] accumulated_eval_time=1167.34, accumulated_logging_time=2.93976, accumulated_submission_time=14848.2, global_step=38523, preemption_count=0, score=14848.2, test/accuracy=0.1593, test/loss=5.1555, test/num_examples=10000, total_duration=16022.3, train/accuracy=0.217654, train/loss=4.41249, validation/accuracy=0.2095, validation/loss=4.52248, validation/num_examples=50000
I0307 06:30:07.615978 139758068020992 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.1988940238952637, loss=2.4115962982177734
I0307 06:30:45.919693 139758059628288 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.1142418384552002, loss=2.351854085922241
I0307 06:31:24.231771 139758068020992 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.2416789531707764, loss=2.3024139404296875
I0307 06:32:02.545726 139758059628288 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2036868333816528, loss=2.352898120880127
I0307 06:32:40.498168 139758068020992 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.184084415435791, loss=2.2963690757751465
I0307 06:33:19.184257 139758059628288 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1724884510040283, loss=2.2516703605651855
I0307 06:33:57.849701 139758068020992 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.1731178760528564, loss=2.394906997680664
I0307 06:34:35.978184 139758059628288 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.1013867855072021, loss=2.2256219387054443
I0307 06:35:14.130478 139758068020992 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.1274579763412476, loss=2.1657605171203613
I0307 06:35:52.347041 139758059628288 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.2660049200057983, loss=2.2203879356384277
I0307 06:36:30.572012 139758068020992 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1107659339904785, loss=2.2418484687805176
I0307 06:37:09.013056 139758059628288 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.166433572769165, loss=2.348750114440918
I0307 06:37:47.134438 139758068020992 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.150300145149231, loss=2.304457426071167
I0307 06:38:07.702924 139912818214080 spec.py:321] Evaluating on the training split.
I0307 06:38:19.568597 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 06:38:42.342872 139912818214080 spec.py:349] Evaluating on the test split.
I0307 06:38:44.060442 139912818214080 submission_runner.py:469] Time since start: 16568.69s, 	Step: 39855, 	{'train/accuracy': 0.22122129797935486, 'train/loss': 4.288172245025635, 'validation/accuracy': 0.20493999123573303, 'validation/loss': 4.422561168670654, 'validation/num_examples': 50000, 'test/accuracy': 0.1485000103712082, 'test/loss': 5.135503768920898, 'test/num_examples': 10000, 'score': 15357.993515491486, 'total_duration': 16568.690752267838, 'accumulated_submission_time': 15357.993515491486, 'accumulated_eval_time': 1203.6948282718658, 'accumulated_logging_time': 3.0898118019104004}
I0307 06:38:44.156269 139758059628288 logging_writer.py:48] [39855] accumulated_eval_time=1203.69, accumulated_logging_time=3.08981, accumulated_submission_time=15358, global_step=39855, preemption_count=0, score=15358, test/accuracy=0.1485, test/loss=5.1355, test/num_examples=10000, total_duration=16568.7, train/accuracy=0.221221, train/loss=4.28817, validation/accuracy=0.20494, validation/loss=4.42256, validation/num_examples=50000
I0307 06:39:07.023595 139758068020992 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0975885391235352, loss=2.2329258918762207
I0307 06:39:44.593429 139758059628288 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.1448010206222534, loss=2.279672145843506
I0307 06:40:23.022491 139758068020992 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.1353758573532104, loss=2.3647942543029785
I0307 06:41:01.300428 139758059628288 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.2410622835159302, loss=2.188403606414795
I0307 06:41:39.806548 139758068020992 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.307538628578186, loss=2.3440396785736084
I0307 06:42:17.758257 139758059628288 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.1252293586730957, loss=2.2787868976593018
I0307 06:42:56.101577 139758068020992 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.2558207511901855, loss=2.294931173324585
I0307 06:43:34.745380 139758059628288 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.0640504360198975, loss=2.208155393600464
I0307 06:44:13.219990 139758068020992 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.267046332359314, loss=2.357546091079712
I0307 06:44:51.482889 139758059628288 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.095207691192627, loss=2.2067248821258545
I0307 06:45:29.738195 139758068020992 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.168314814567566, loss=2.20033860206604
I0307 06:46:07.730491 139758059628288 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.2141506671905518, loss=2.2599902153015137
I0307 06:46:46.008006 139758068020992 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1355867385864258, loss=2.2955727577209473
I0307 06:47:14.219177 139912818214080 spec.py:321] Evaluating on the training split.
I0307 06:47:26.004728 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 06:47:50.298526 139912818214080 spec.py:349] Evaluating on the test split.
I0307 06:47:52.017025 139912818214080 submission_runner.py:469] Time since start: 17116.65s, 	Step: 41175, 	{'train/accuracy': 0.25414541363716125, 'train/loss': 4.127640724182129, 'validation/accuracy': 0.2437399923801422, 'validation/loss': 4.241576671600342, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.957355976104736, 'test/num_examples': 10000, 'score': 15867.907405138016, 'total_duration': 17116.647346735, 'accumulated_submission_time': 15867.907405138016, 'accumulated_eval_time': 1241.492629289627, 'accumulated_logging_time': 3.203706741333008}
I0307 06:47:52.111232 139758059628288 logging_writer.py:48] [41175] accumulated_eval_time=1241.49, accumulated_logging_time=3.20371, accumulated_submission_time=15867.9, global_step=41175, preemption_count=0, score=15867.9, test/accuracy=0.1791, test/loss=4.95736, test/num_examples=10000, total_duration=17116.6, train/accuracy=0.254145, train/loss=4.12764, validation/accuracy=0.24374, validation/loss=4.24158, validation/num_examples=50000
I0307 06:48:02.241655 139758068020992 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.229121446609497, loss=2.2365665435791016
I0307 06:48:40.377698 139758059628288 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.1767730712890625, loss=2.295229434967041
I0307 06:49:19.025161 139758068020992 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.2208486795425415, loss=2.213334321975708
I0307 06:49:57.192579 139758059628288 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.122913122177124, loss=2.307447671890259
I0307 06:50:35.399726 139758068020992 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.0585803985595703, loss=2.3340237140655518
I0307 06:51:13.753657 139758059628288 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.2865805625915527, loss=2.246091365814209
I0307 06:51:52.202871 139758068020992 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.1362653970718384, loss=2.3164782524108887
I0307 06:52:30.576152 139758059628288 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1349436044692993, loss=2.147401809692383
I0307 06:53:08.855799 139758068020992 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.1513292789459229, loss=2.310255289077759
I0307 06:53:47.197052 139758059628288 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.2035326957702637, loss=2.3868207931518555
I0307 06:54:25.567941 139758068020992 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.2407636642456055, loss=2.3338637351989746
I0307 06:55:03.796061 139758059628288 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1847023963928223, loss=2.34765362739563
I0307 06:55:42.486956 139758068020992 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.300961971282959, loss=2.3201656341552734
I0307 06:56:20.781692 139758059628288 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.2123767137527466, loss=2.3054184913635254
I0307 06:56:22.326566 139912818214080 spec.py:321] Evaluating on the training split.
I0307 06:56:34.567145 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 06:56:56.182031 139912818214080 spec.py:349] Evaluating on the test split.
I0307 06:56:57.906510 139912818214080 submission_runner.py:469] Time since start: 17662.54s, 	Step: 42505, 	{'train/accuracy': 0.14799903333187103, 'train/loss': 5.23631477355957, 'validation/accuracy': 0.13412000238895416, 'validation/loss': 5.44006872177124, 'validation/num_examples': 50000, 'test/accuracy': 0.11070000380277634, 'test/loss': 5.8600640296936035, 'test/num_examples': 10000, 'score': 16377.969247341156, 'total_duration': 17662.536828279495, 'accumulated_submission_time': 16377.969247341156, 'accumulated_eval_time': 1277.072520017624, 'accumulated_logging_time': 3.3218369483947754}
I0307 06:56:57.979029 139758068020992 logging_writer.py:48] [42505] accumulated_eval_time=1277.07, accumulated_logging_time=3.32184, accumulated_submission_time=16378, global_step=42505, preemption_count=0, score=16378, test/accuracy=0.1107, test/loss=5.86006, test/num_examples=10000, total_duration=17662.5, train/accuracy=0.147999, train/loss=5.23631, validation/accuracy=0.13412, validation/loss=5.44007, validation/num_examples=50000
I0307 06:57:34.940675 139758059628288 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.1609547138214111, loss=2.2523722648620605
I0307 06:58:13.273407 139758068020992 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0619608163833618, loss=2.2728610038757324
I0307 06:58:51.499259 139758059628288 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2975674867630005, loss=2.261691093444824
I0307 06:59:29.877798 139758068020992 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.2430500984191895, loss=2.358971118927002
I0307 07:00:08.366579 139758059628288 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.1995196342468262, loss=2.233617067337036
I0307 07:00:46.789040 139758068020992 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.1526620388031006, loss=2.3027148246765137
I0307 07:01:25.363356 139758059628288 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.1947212219238281, loss=2.3020684719085693
I0307 07:02:03.788331 139758068020992 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.1560449600219727, loss=2.3526723384857178
I0307 07:02:42.033743 139758059628288 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.3903878927230835, loss=2.2976694107055664
I0307 07:03:20.264611 139758068020992 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.1098856925964355, loss=2.3091917037963867
I0307 07:03:58.555990 139758059628288 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.2189429998397827, loss=2.3390491008758545
I0307 07:04:37.327919 139758068020992 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.2179780006408691, loss=2.2533202171325684
I0307 07:05:15.836864 139758059628288 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.08847177028656, loss=2.252362012863159
I0307 07:05:28.244129 139912818214080 spec.py:321] Evaluating on the training split.
I0307 07:05:40.113999 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 07:06:04.365065 139912818214080 spec.py:349] Evaluating on the test split.
I0307 07:06:06.088932 139912818214080 submission_runner.py:469] Time since start: 18210.72s, 	Step: 43833, 	{'train/accuracy': 0.32900190353393555, 'train/loss': 3.3557276725769043, 'validation/accuracy': 0.31233999133110046, 'validation/loss': 3.501643419265747, 'validation/num_examples': 50000, 'test/accuracy': 0.2331000119447708, 'test/loss': 4.316805839538574, 'test/num_examples': 10000, 'score': 16888.078696250916, 'total_duration': 18210.71925163269, 'accumulated_submission_time': 16888.078696250916, 'accumulated_eval_time': 1314.917272567749, 'accumulated_logging_time': 3.4193406105041504}
I0307 07:06:06.169732 139758068020992 logging_writer.py:48] [43833] accumulated_eval_time=1314.92, accumulated_logging_time=3.41934, accumulated_submission_time=16888.1, global_step=43833, preemption_count=0, score=16888.1, test/accuracy=0.2331, test/loss=4.31681, test/num_examples=10000, total_duration=18210.7, train/accuracy=0.329002, train/loss=3.35573, validation/accuracy=0.31234, validation/loss=3.50164, validation/num_examples=50000
I0307 07:06:32.006489 139758059628288 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.399661660194397, loss=2.4336116313934326
I0307 07:07:10.174134 139758068020992 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0883564949035645, loss=2.1580991744995117
I0307 07:07:48.421239 139758059628288 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.2223560810089111, loss=2.303978681564331
I0307 07:08:26.672092 139758068020992 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.1287928819656372, loss=2.285773515701294
I0307 07:09:04.985451 139758059628288 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.2821346521377563, loss=2.280986785888672
I0307 07:09:43.282185 139758068020992 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.2919540405273438, loss=2.1698670387268066
I0307 07:10:21.875807 139758059628288 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.2626373767852783, loss=2.2231454849243164
I0307 07:11:00.212082 139758068020992 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2278640270233154, loss=2.200648069381714
I0307 07:11:38.434875 139758059628288 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.0992330312728882, loss=2.1655991077423096
I0307 07:12:17.029090 139758068020992 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.1953692436218262, loss=2.199554443359375
I0307 07:12:55.454900 139758059628288 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.0937297344207764, loss=2.1861884593963623
I0307 07:13:33.629489 139758068020992 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.2310105562210083, loss=2.1991255283355713
I0307 07:14:12.109523 139758059628288 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.330011248588562, loss=2.2160820960998535
I0307 07:14:36.493743 139912818214080 spec.py:321] Evaluating on the training split.
I0307 07:14:48.436262 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 07:15:08.030674 139912818214080 spec.py:349] Evaluating on the test split.
I0307 07:15:09.795428 139912818214080 submission_runner.py:469] Time since start: 18754.43s, 	Step: 45164, 	{'train/accuracy': 0.21699616312980652, 'train/loss': 4.519068241119385, 'validation/accuracy': 0.20430000126361847, 'validation/loss': 4.622311592102051, 'validation/num_examples': 50000, 'test/accuracy': 0.14430001378059387, 'test/loss': 5.447676658630371, 'test/num_examples': 10000, 'score': 17398.228604078293, 'total_duration': 18754.42572402954, 'accumulated_submission_time': 17398.228604078293, 'accumulated_eval_time': 1348.2188844680786, 'accumulated_logging_time': 3.5425851345062256}
I0307 07:15:09.873341 139758068020992 logging_writer.py:48] [45164] accumulated_eval_time=1348.22, accumulated_logging_time=3.54259, accumulated_submission_time=17398.2, global_step=45164, preemption_count=0, score=17398.2, test/accuracy=0.1443, test/loss=5.44768, test/num_examples=10000, total_duration=18754.4, train/accuracy=0.216996, train/loss=4.51907, validation/accuracy=0.2043, validation/loss=4.62231, validation/num_examples=50000
I0307 07:15:24.214170 139758059628288 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.1671801805496216, loss=2.2397773265838623
I0307 07:16:02.504141 139758068020992 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.082370400428772, loss=2.403318166732788
I0307 07:16:40.772225 139758059628288 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.2593165636062622, loss=2.418567419052124
I0307 07:17:19.211853 139758068020992 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.2686376571655273, loss=2.2635087966918945
I0307 07:17:57.333039 139758059628288 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.1211234331130981, loss=2.2539608478546143
I0307 07:18:35.209512 139758068020992 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1398333311080933, loss=2.244710922241211
I0307 07:19:13.320263 139758059628288 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.229376196861267, loss=2.312659740447998
I0307 07:19:51.791801 139758068020992 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.2022740840911865, loss=2.2132742404937744
I0307 07:20:30.420814 139758059628288 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1892149448394775, loss=2.274920701980591
I0307 07:21:09.422814 139758068020992 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.1330773830413818, loss=2.2518131732940674
I0307 07:21:47.866210 139758059628288 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.2998929023742676, loss=2.3266983032226562
I0307 07:22:26.231987 139758068020992 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.209948182106018, loss=2.321749210357666
I0307 07:23:04.879193 139758059628288 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.1852225065231323, loss=2.299403429031372
I0307 07:23:39.918425 139912818214080 spec.py:321] Evaluating on the training split.
I0307 07:23:51.745353 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 07:24:15.920097 139912818214080 spec.py:349] Evaluating on the test split.
I0307 07:24:17.653865 139912818214080 submission_runner.py:469] Time since start: 19302.28s, 	Step: 46492, 	{'train/accuracy': 0.24208784103393555, 'train/loss': 3.907804489135742, 'validation/accuracy': 0.2263999879360199, 'validation/loss': 4.015890598297119, 'validation/num_examples': 50000, 'test/accuracy': 0.15520000457763672, 'test/loss': 4.751544952392578, 'test/num_examples': 10000, 'score': 17908.11704468727, 'total_duration': 19302.28417825699, 'accumulated_submission_time': 17908.11704468727, 'accumulated_eval_time': 1385.9542672634125, 'accumulated_logging_time': 3.6488208770751953}
I0307 07:24:17.786819 139758068020992 logging_writer.py:48] [46492] accumulated_eval_time=1385.95, accumulated_logging_time=3.64882, accumulated_submission_time=17908.1, global_step=46492, preemption_count=0, score=17908.1, test/accuracy=0.1552, test/loss=4.75154, test/num_examples=10000, total_duration=19302.3, train/accuracy=0.242088, train/loss=3.9078, validation/accuracy=0.2264, validation/loss=4.01589, validation/num_examples=50000
I0307 07:24:21.249856 139758059628288 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1723283529281616, loss=2.2509591579437256
I0307 07:24:59.336078 139758068020992 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1836028099060059, loss=2.2532405853271484
I0307 07:25:37.878509 139758059628288 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.3488928079605103, loss=2.242567300796509
I0307 07:26:16.216933 139758068020992 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.2129453420639038, loss=2.317685842514038
I0307 07:26:54.771670 139758059628288 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.169891595840454, loss=2.267042398452759
I0307 07:27:32.929957 139758068020992 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.112842321395874, loss=2.22689151763916
I0307 07:28:10.953481 139758059628288 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.2723499536514282, loss=2.2505624294281006
I0307 07:28:49.455578 139758068020992 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.2059262990951538, loss=2.18680477142334
I0307 07:29:28.003456 139758059628288 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.2866886854171753, loss=2.2128806114196777
I0307 07:30:06.194176 139758068020992 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.1651983261108398, loss=2.215895414352417
I0307 07:30:44.242491 139758059628288 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1095912456512451, loss=2.2312028408050537
I0307 07:31:22.598529 139758068020992 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1664652824401855, loss=2.302422523498535
I0307 07:32:00.686836 139758059628288 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.1512596607208252, loss=2.1606943607330322
I0307 07:32:38.862544 139758068020992 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.1760566234588623, loss=2.2168169021606445
I0307 07:32:48.000319 139912818214080 spec.py:321] Evaluating on the training split.
I0307 07:32:59.974341 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 07:33:22.235864 139912818214080 spec.py:349] Evaluating on the test split.
I0307 07:33:23.937710 139912818214080 submission_runner.py:469] Time since start: 19848.57s, 	Step: 47825, 	{'train/accuracy': 0.2200852930545807, 'train/loss': 4.274899482727051, 'validation/accuracy': 0.21249999105930328, 'validation/loss': 4.3490166664123535, 'validation/num_examples': 50000, 'test/accuracy': 0.15320000052452087, 'test/loss': 5.190557479858398, 'test/num_examples': 10000, 'score': 18418.18158507347, 'total_duration': 19848.56801390648, 'accumulated_submission_time': 18418.18158507347, 'accumulated_eval_time': 1421.891592502594, 'accumulated_logging_time': 3.8012969493865967}
I0307 07:33:24.049934 139758059628288 logging_writer.py:48] [47825] accumulated_eval_time=1421.89, accumulated_logging_time=3.8013, accumulated_submission_time=18418.2, global_step=47825, preemption_count=0, score=18418.2, test/accuracy=0.1532, test/loss=5.19056, test/num_examples=10000, total_duration=19848.6, train/accuracy=0.220085, train/loss=4.2749, validation/accuracy=0.2125, validation/loss=4.34902, validation/num_examples=50000
I0307 07:33:52.925804 139758068020992 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.405623435974121, loss=2.2373385429382324
I0307 07:34:31.389754 139758059628288 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.1718175411224365, loss=2.2525320053100586
I0307 07:35:09.479996 139758068020992 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.199479579925537, loss=2.310330390930176
I0307 07:35:47.562744 139758059628288 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.228691577911377, loss=2.2702012062072754
I0307 07:36:25.851652 139758068020992 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.11629319190979, loss=2.259732723236084
I0307 07:37:04.378490 139758059628288 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1379165649414062, loss=2.177119493484497
I0307 07:37:42.617386 139758068020992 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2079448699951172, loss=2.2879152297973633
I0307 07:38:20.738260 139758059628288 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.2481820583343506, loss=2.1545491218566895
I0307 07:38:59.120375 139758068020992 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.3463681936264038, loss=2.228996753692627
I0307 07:39:37.525405 139758059628288 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.1932016611099243, loss=2.2633883953094482
I0307 07:40:15.985950 139758068020992 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.251390814781189, loss=2.1487250328063965
I0307 07:40:54.058398 139758059628288 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.3287380933761597, loss=2.241187810897827
I0307 07:41:32.670405 139758068020992 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.3231076002120972, loss=2.237794876098633
I0307 07:41:54.017492 139912818214080 spec.py:321] Evaluating on the training split.
I0307 07:42:05.837599 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 07:42:28.358223 139912818214080 spec.py:349] Evaluating on the test split.
I0307 07:42:30.108393 139912818214080 submission_runner.py:469] Time since start: 20394.74s, 	Step: 49157, 	{'train/accuracy': 0.35070550441741943, 'train/loss': 3.1125409603118896, 'validation/accuracy': 0.329259991645813, 'validation/loss': 3.23136043548584, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 3.971595525741577, 'test/num_examples': 10000, 'score': 18927.99786257744, 'total_duration': 20394.738711595535, 'accumulated_submission_time': 18927.99786257744, 'accumulated_eval_time': 1457.9824705123901, 'accumulated_logging_time': 3.936058521270752}
I0307 07:42:30.199597 139758059628288 logging_writer.py:48] [49157] accumulated_eval_time=1457.98, accumulated_logging_time=3.93606, accumulated_submission_time=18928, global_step=49157, preemption_count=0, score=18928, test/accuracy=0.2457, test/loss=3.9716, test/num_examples=10000, total_duration=20394.7, train/accuracy=0.350706, train/loss=3.11254, validation/accuracy=0.32926, validation/loss=3.23136, validation/num_examples=50000
I0307 07:42:47.130604 139758068020992 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.1532992124557495, loss=2.1721513271331787
I0307 07:43:25.071262 139758059628288 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.2574756145477295, loss=2.396785259246826
I0307 07:44:03.417313 139758068020992 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.212221384048462, loss=2.258970260620117
I0307 07:44:41.625412 139758059628288 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1623984575271606, loss=2.2351479530334473
I0307 07:45:20.553066 139758068020992 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.3505663871765137, loss=2.4024734497070312
I0307 07:45:58.594237 139758059628288 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0773429870605469, loss=2.2619237899780273
I0307 07:46:36.852338 139758068020992 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.177037239074707, loss=2.225647211074829
I0307 07:47:15.081578 139758059628288 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.2379603385925293, loss=2.273841619491577
I0307 07:47:53.038520 139758068020992 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.2106822729110718, loss=2.2505714893341064
I0307 07:48:31.273512 139758059628288 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.2199896574020386, loss=2.281888961791992
I0307 07:49:09.624404 139758068020992 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.1722956895828247, loss=2.1863603591918945
I0307 07:49:48.204842 139758059628288 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.1808966398239136, loss=2.3377068042755127
I0307 07:50:26.398606 139758068020992 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2175182104110718, loss=2.1645474433898926
I0307 07:51:00.269602 139912818214080 spec.py:321] Evaluating on the training split.
I0307 07:51:11.922795 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 07:51:35.292650 139912818214080 spec.py:349] Evaluating on the test split.
I0307 07:51:37.021030 139912818214080 submission_runner.py:469] Time since start: 20941.65s, 	Step: 50489, 	{'train/accuracy': 0.2566366195678711, 'train/loss': 4.03180456161499, 'validation/accuracy': 0.24271999299526215, 'validation/loss': 4.188366889953613, 'validation/num_examples': 50000, 'test/accuracy': 0.1826000064611435, 'test/loss': 4.849823474884033, 'test/num_examples': 10000, 'score': 19437.906849861145, 'total_duration': 20941.65132212639, 'accumulated_submission_time': 19437.906849861145, 'accumulated_eval_time': 1494.733823299408, 'accumulated_logging_time': 4.056396484375}
I0307 07:51:37.149262 139758059628288 logging_writer.py:48] [50489] accumulated_eval_time=1494.73, accumulated_logging_time=4.0564, accumulated_submission_time=19437.9, global_step=50489, preemption_count=0, score=19437.9, test/accuracy=0.1826, test/loss=4.84982, test/num_examples=10000, total_duration=20941.7, train/accuracy=0.256637, train/loss=4.0318, validation/accuracy=0.24272, validation/loss=4.18837, validation/num_examples=50000
I0307 07:51:41.836794 139758068020992 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.2510451078414917, loss=2.2882418632507324
I0307 07:52:19.834884 139758059628288 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.2283596992492676, loss=2.3130197525024414
I0307 07:52:58.155523 139758068020992 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.164114236831665, loss=2.145033597946167
I0307 07:53:36.774845 139758059628288 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.2287821769714355, loss=2.2122340202331543
I0307 07:54:14.987119 139758068020992 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.3271678686141968, loss=2.216937303543091
I0307 07:54:53.385871 139758059628288 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1944888830184937, loss=2.2379813194274902
I0307 07:55:31.445986 139758068020992 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.2035822868347168, loss=2.3222007751464844
I0307 07:56:09.948134 139758059628288 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.0554497241973877, loss=2.225731611251831
I0307 07:56:48.398625 139758068020992 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.2303147315979004, loss=2.1599721908569336
I0307 07:57:27.068427 139758059628288 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.2083666324615479, loss=2.132150888442993
I0307 07:58:05.660947 139758068020992 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.193915843963623, loss=2.159351348876953
I0307 07:58:44.192044 139758059628288 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.196855068206787, loss=2.168647050857544
I0307 07:59:22.418912 139758068020992 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.2150613069534302, loss=2.162360429763794
I0307 08:00:00.948861 139758059628288 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.6664097309112549, loss=2.3451993465423584
I0307 08:00:07.112527 139912818214080 spec.py:321] Evaluating on the training split.
I0307 08:00:18.671777 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 08:00:40.082953 139912818214080 spec.py:349] Evaluating on the test split.
I0307 08:00:41.798702 139912818214080 submission_runner.py:469] Time since start: 21486.43s, 	Step: 51817, 	{'train/accuracy': 0.27431440353393555, 'train/loss': 3.9140548706054688, 'validation/accuracy': 0.26034000515937805, 'validation/loss': 4.0571513175964355, 'validation/num_examples': 50000, 'test/accuracy': 0.18700000643730164, 'test/loss': 4.955191612243652, 'test/num_examples': 10000, 'score': 19947.713540554047, 'total_duration': 21486.429004192352, 'accumulated_submission_time': 19947.713540554047, 'accumulated_eval_time': 1529.4199328422546, 'accumulated_logging_time': 4.213247299194336}
I0307 08:00:41.893724 139758068020992 logging_writer.py:48] [51817] accumulated_eval_time=1529.42, accumulated_logging_time=4.21325, accumulated_submission_time=19947.7, global_step=51817, preemption_count=0, score=19947.7, test/accuracy=0.187, test/loss=4.95519, test/num_examples=10000, total_duration=21486.4, train/accuracy=0.274314, train/loss=3.91405, validation/accuracy=0.26034, validation/loss=4.05715, validation/num_examples=50000
I0307 08:01:14.108054 139758059628288 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2520726919174194, loss=2.263840675354004
I0307 08:01:52.317356 139758068020992 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2960864305496216, loss=2.2139995098114014
I0307 08:02:30.713103 139758059628288 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.2809538841247559, loss=2.298722267150879
I0307 08:03:09.027004 139758068020992 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.3344995975494385, loss=2.2877609729766846
I0307 08:03:47.366144 139758059628288 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.2382619380950928, loss=2.2644107341766357
I0307 08:04:25.653515 139758068020992 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.1537102460861206, loss=2.243506669998169
I0307 08:05:04.581119 139758059628288 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.1612255573272705, loss=2.3050284385681152
I0307 08:05:43.214481 139758068020992 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.152829885482788, loss=2.2810230255126953
I0307 08:06:21.600672 139758059628288 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.2863060235977173, loss=2.0870540142059326
I0307 08:06:59.655346 139758068020992 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.1754695177078247, loss=2.2080743312835693
I0307 08:07:37.970803 139758059628288 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.2786997556686401, loss=2.302598237991333
I0307 08:08:16.392378 139758068020992 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.238944172859192, loss=2.1184682846069336
I0307 08:08:54.549030 139758059628288 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.215306282043457, loss=2.119272232055664
I0307 08:09:11.828501 139912818214080 spec.py:321] Evaluating on the training split.
I0307 08:09:23.494569 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 08:09:45.896473 139912818214080 spec.py:349] Evaluating on the test split.
I0307 08:09:47.648991 139912818214080 submission_runner.py:469] Time since start: 22032.28s, 	Step: 53146, 	{'train/accuracy': 0.2919921875, 'train/loss': 3.5867514610290527, 'validation/accuracy': 0.2764599919319153, 'validation/loss': 3.7457399368286133, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.466219902038574, 'test/num_examples': 10000, 'score': 20457.487042188644, 'total_duration': 22032.279284477234, 'accumulated_submission_time': 20457.487042188644, 'accumulated_eval_time': 1565.2403440475464, 'accumulated_logging_time': 4.335663557052612}
I0307 08:09:47.786800 139758068020992 logging_writer.py:48] [53146] accumulated_eval_time=1565.24, accumulated_logging_time=4.33566, accumulated_submission_time=20457.5, global_step=53146, preemption_count=0, score=20457.5, test/accuracy=0.2082, test/loss=4.46622, test/num_examples=10000, total_duration=22032.3, train/accuracy=0.291992, train/loss=3.58675, validation/accuracy=0.27646, validation/loss=3.74574, validation/num_examples=50000
I0307 08:10:08.776992 139758059628288 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2317984104156494, loss=2.313153028488159
I0307 08:11:12.103844 139758068020992 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.1664000749588013, loss=2.260725736618042
I0307 08:11:50.332143 139758059628288 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.1611435413360596, loss=2.267672538757324
I0307 08:12:28.878278 139758068020992 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2403111457824707, loss=2.2917709350585938
I0307 08:13:07.060291 139758059628288 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.1972599029541016, loss=2.247204065322876
I0307 08:13:45.430649 139758068020992 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.320285677909851, loss=2.3143908977508545
I0307 08:14:23.795451 139758059628288 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.405995488166809, loss=2.2815561294555664
I0307 08:15:02.443582 139758068020992 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.276328682899475, loss=2.345414400100708
I0307 08:15:40.824353 139758059628288 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.2866007089614868, loss=2.3752708435058594
I0307 08:16:19.305949 139758068020992 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.240414023399353, loss=2.076246500015259
I0307 08:16:57.742125 139758059628288 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.1019771099090576, loss=2.18343448638916
I0307 08:17:36.062851 139758068020992 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.100180745124817, loss=2.1438982486724854
I0307 08:18:14.505099 139758059628288 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.281426191329956, loss=2.391650438308716
I0307 08:18:17.916372 139912818214080 spec.py:321] Evaluating on the training split.
I0307 08:18:29.531207 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 08:18:52.218990 139912818214080 spec.py:349] Evaluating on the test split.
I0307 08:18:53.954639 139912818214080 submission_runner.py:469] Time since start: 22578.58s, 	Step: 54410, 	{'train/accuracy': 0.29711416363716125, 'train/loss': 3.75480055809021, 'validation/accuracy': 0.2734600007534027, 'validation/loss': 3.9515786170959473, 'validation/num_examples': 50000, 'test/accuracy': 0.21810001134872437, 'test/loss': 4.581789970397949, 'test/num_examples': 10000, 'score': 20967.469806194305, 'total_duration': 22578.584963560104, 'accumulated_submission_time': 20967.469806194305, 'accumulated_eval_time': 1601.2785637378693, 'accumulated_logging_time': 4.4938881397247314}
I0307 08:18:54.026278 139758068020992 logging_writer.py:48] [54410] accumulated_eval_time=1601.28, accumulated_logging_time=4.49389, accumulated_submission_time=20967.5, global_step=54410, preemption_count=0, score=20967.5, test/accuracy=0.2181, test/loss=4.58179, test/num_examples=10000, total_duration=22578.6, train/accuracy=0.297114, train/loss=3.7548, validation/accuracy=0.27346, validation/loss=3.95158, validation/num_examples=50000
I0307 08:19:28.850321 139758059628288 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2475192546844482, loss=2.324476957321167
I0307 08:20:07.024968 139758068020992 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.3508228063583374, loss=2.373335599899292
I0307 08:20:45.668980 139758059628288 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.179876685142517, loss=2.1660189628601074
I0307 08:21:24.110986 139758068020992 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.2400131225585938, loss=2.226757049560547
I0307 08:22:02.267466 139758059628288 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.265812873840332, loss=2.241774320602417
I0307 08:22:40.638331 139758068020992 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2114200592041016, loss=2.156085729598999
I0307 08:23:18.791866 139758059628288 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.1924488544464111, loss=2.1960980892181396
I0307 08:23:57.041531 139758068020992 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.173586368560791, loss=2.1920430660247803
I0307 08:24:35.410345 139758059628288 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1818841695785522, loss=2.4305734634399414
I0307 08:25:13.614011 139758068020992 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.1888781785964966, loss=2.1401724815368652
I0307 08:25:51.758362 139758059628288 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.3165783882141113, loss=2.337700843811035
I0307 08:26:29.824187 139758068020992 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.2347195148468018, loss=2.0791397094726562
I0307 08:27:08.217178 139758059628288 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.262008547782898, loss=2.142259120941162
I0307 08:27:23.972839 139912818214080 spec.py:321] Evaluating on the training split.
I0307 08:27:35.430380 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 08:27:53.209260 139912818214080 spec.py:349] Evaluating on the test split.
I0307 08:27:54.960089 139912818214080 submission_runner.py:469] Time since start: 23119.59s, 	Step: 55742, 	{'train/accuracy': 0.2243303507566452, 'train/loss': 4.711413383483887, 'validation/accuracy': 0.20999999344348907, 'validation/loss': 4.8930134773254395, 'validation/num_examples': 50000, 'test/accuracy': 0.15960000455379486, 'test/loss': 5.658714294433594, 'test/num_examples': 10000, 'score': 21477.22707939148, 'total_duration': 23119.590398311615, 'accumulated_submission_time': 21477.22707939148, 'accumulated_eval_time': 1632.2657532691956, 'accumulated_logging_time': 4.616487979888916}
I0307 08:27:55.135041 139758068020992 logging_writer.py:48] [55742] accumulated_eval_time=1632.27, accumulated_logging_time=4.61649, accumulated_submission_time=21477.2, global_step=55742, preemption_count=0, score=21477.2, test/accuracy=0.1596, test/loss=5.65871, test/num_examples=10000, total_duration=23119.6, train/accuracy=0.22433, train/loss=4.71141, validation/accuracy=0.21, validation/loss=4.89301, validation/num_examples=50000
I0307 08:28:17.744286 139758059628288 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.3426610231399536, loss=2.2449893951416016
I0307 08:28:55.995477 139758068020992 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.1445232629776, loss=2.2116527557373047
I0307 08:29:34.502385 139758059628288 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.4484150409698486, loss=2.2194435596466064
I0307 08:30:13.004038 139758068020992 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.311896800994873, loss=2.3108811378479004
I0307 08:30:51.419913 139758059628288 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.4620096683502197, loss=2.3985090255737305
I0307 08:31:29.804932 139758068020992 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.3069772720336914, loss=2.1612186431884766
I0307 08:32:08.319239 139758059628288 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2274965047836304, loss=2.2568774223327637
I0307 08:32:46.628227 139758068020992 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2833890914916992, loss=2.182844877243042
I0307 08:33:24.958137 139758059628288 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.3579250574111938, loss=2.3592307567596436
I0307 08:34:03.490953 139758068020992 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2419607639312744, loss=2.173069953918457
I0307 08:34:42.040059 139758059628288 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2730669975280762, loss=2.289400577545166
I0307 08:35:20.310461 139758068020992 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.1934454441070557, loss=2.1725711822509766
I0307 08:35:58.583864 139758059628288 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.3146498203277588, loss=2.2107558250427246
I0307 08:36:24.965899 139912818214080 spec.py:321] Evaluating on the training split.
I0307 08:36:36.972221 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 08:36:59.281785 139912818214080 spec.py:349] Evaluating on the test split.
I0307 08:37:00.975411 139912818214080 submission_runner.py:469] Time since start: 23665.61s, 	Step: 57070, 	{'train/accuracy': 0.07864317297935486, 'train/loss': 6.809846878051758, 'validation/accuracy': 0.07552000135183334, 'validation/loss': 6.8568267822265625, 'validation/num_examples': 50000, 'test/accuracy': 0.05350000411272049, 'test/loss': 7.474111080169678, 'test/num_examples': 10000, 'score': 21986.890318870544, 'total_duration': 23665.605720758438, 'accumulated_submission_time': 21986.890318870544, 'accumulated_eval_time': 1668.275202035904, 'accumulated_logging_time': 4.815795421600342}
I0307 08:37:01.043147 139758068020992 logging_writer.py:48] [57070] accumulated_eval_time=1668.28, accumulated_logging_time=4.8158, accumulated_submission_time=21986.9, global_step=57070, preemption_count=0, score=21986.9, test/accuracy=0.0535, test/loss=7.47411, test/num_examples=10000, total_duration=23665.6, train/accuracy=0.0786432, train/loss=6.80985, validation/accuracy=0.07552, validation/loss=6.85683, validation/num_examples=50000
I0307 08:37:12.978524 139758059628288 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.3166954517364502, loss=2.288559913635254
I0307 08:37:51.345295 139758068020992 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.290217399597168, loss=2.228060722351074
I0307 08:38:29.836554 139758059628288 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1969380378723145, loss=2.0688934326171875
I0307 08:39:08.060546 139758068020992 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.2405191659927368, loss=2.25053071975708
I0307 08:39:46.148549 139758059628288 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.1792939901351929, loss=2.1668827533721924
I0307 08:40:24.669550 139758068020992 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.190138339996338, loss=2.2204689979553223
I0307 08:41:02.572225 139758059628288 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.2532713413238525, loss=2.208648204803467
I0307 08:41:40.885333 139758068020992 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1886203289031982, loss=2.2584598064422607
I0307 08:42:19.122395 139758059628288 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2685250043869019, loss=2.2811708450317383
I0307 08:42:57.371760 139758068020992 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.3297301530838013, loss=2.2238216400146484
I0307 08:43:35.597715 139758059628288 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.3182647228240967, loss=2.141434907913208
I0307 08:44:13.868166 139758068020992 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.2590196132659912, loss=2.117501735687256
I0307 08:44:51.518576 139758059628288 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.2700642347335815, loss=2.260413408279419
I0307 08:45:29.839580 139758068020992 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.3800625801086426, loss=2.2779688835144043
I0307 08:45:31.005027 139912818214080 spec.py:321] Evaluating on the training split.
I0307 08:45:43.075375 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 08:46:06.706812 139912818214080 spec.py:349] Evaluating on the test split.
I0307 08:46:08.430701 139912818214080 submission_runner.py:469] Time since start: 24213.06s, 	Step: 58404, 	{'train/accuracy': 0.2693319320678711, 'train/loss': 3.7230355739593506, 'validation/accuracy': 0.2600799798965454, 'validation/loss': 3.8133041858673096, 'validation/num_examples': 50000, 'test/accuracy': 0.18960000574588776, 'test/loss': 4.524122714996338, 'test/num_examples': 10000, 'score': 22496.671526670456, 'total_duration': 24213.061018705368, 'accumulated_submission_time': 22496.671526670456, 'accumulated_eval_time': 1705.700838804245, 'accumulated_logging_time': 4.917435646057129}
I0307 08:46:08.562803 139758059628288 logging_writer.py:48] [58404] accumulated_eval_time=1705.7, accumulated_logging_time=4.91744, accumulated_submission_time=22496.7, global_step=58404, preemption_count=0, score=22496.7, test/accuracy=0.1896, test/loss=4.52412, test/num_examples=10000, total_duration=24213.1, train/accuracy=0.269332, train/loss=3.72304, validation/accuracy=0.26008, validation/loss=3.8133, validation/num_examples=50000
I0307 08:46:45.848317 139758068020992 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1848994493484497, loss=2.1432790756225586
I0307 08:47:24.178948 139758059628288 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.2362602949142456, loss=2.1876630783081055
I0307 08:48:02.575810 139758068020992 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.1573834419250488, loss=2.1047682762145996
I0307 08:48:41.036730 139758059628288 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.3487645387649536, loss=2.151212453842163
I0307 08:49:18.793214 139758068020992 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.376139760017395, loss=2.18207049369812
I0307 08:49:56.750511 139758059628288 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.3459306955337524, loss=2.252346992492676
I0307 08:50:34.696421 139758068020992 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.1765317916870117, loss=2.1890807151794434
I0307 08:51:12.563165 139758059628288 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1785134077072144, loss=2.265738010406494
I0307 08:51:50.798236 139758068020992 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.3435313701629639, loss=2.206129789352417
I0307 08:52:29.202720 139758059628288 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.204357624053955, loss=2.21891713142395
I0307 08:53:07.527140 139758068020992 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.484319806098938, loss=2.385535717010498
I0307 08:53:45.709481 139758059628288 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.3020312786102295, loss=2.2714781761169434
I0307 08:54:23.696254 139758068020992 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.1798126697540283, loss=2.256683111190796
I0307 08:54:38.652930 139912818214080 spec.py:321] Evaluating on the training split.
I0307 08:54:50.546903 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 08:55:14.327282 139912818214080 spec.py:349] Evaluating on the test split.
I0307 08:55:16.066359 139912818214080 submission_runner.py:469] Time since start: 24760.70s, 	Step: 59740, 	{'train/accuracy': 0.23052853345870972, 'train/loss': 4.335879802703857, 'validation/accuracy': 0.22307999432086945, 'validation/loss': 4.3867316246032715, 'validation/num_examples': 50000, 'test/accuracy': 0.15730001032352448, 'test/loss': 5.150058269500732, 'test/num_examples': 10000, 'score': 23006.582505464554, 'total_duration': 24760.69665169716, 'accumulated_submission_time': 23006.582505464554, 'accumulated_eval_time': 1743.114189863205, 'accumulated_logging_time': 5.083102464675903}
I0307 08:55:16.180949 139758059628288 logging_writer.py:48] [59740] accumulated_eval_time=1743.11, accumulated_logging_time=5.0831, accumulated_submission_time=23006.6, global_step=59740, preemption_count=0, score=23006.6, test/accuracy=0.1573, test/loss=5.15006, test/num_examples=10000, total_duration=24760.7, train/accuracy=0.230529, train/loss=4.33588, validation/accuracy=0.22308, validation/loss=4.38673, validation/num_examples=50000
I0307 08:55:39.363649 139758068020992 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.253883957862854, loss=2.151907205581665
I0307 08:56:17.546279 139758059628288 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.1947954893112183, loss=2.1728177070617676
I0307 08:56:55.675820 139758068020992 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.2535947561264038, loss=2.3158915042877197
I0307 08:58:15.704769 139758059628288 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2999314069747925, loss=2.1800196170806885
I0307 08:58:53.450905 139758068020992 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.311381459236145, loss=2.249997615814209
I0307 08:59:31.185441 139758059628288 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.2765611410140991, loss=2.2529783248901367
I0307 09:00:08.698087 139758068020992 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.2849235534667969, loss=2.301079034805298
I0307 09:00:47.116842 139758059628288 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.28974187374115, loss=2.282109498977661
I0307 09:01:24.629019 139758068020992 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1894359588623047, loss=2.292952060699463
I0307 09:02:01.743411 139758059628288 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.3492282629013062, loss=2.065556049346924
I0307 09:02:40.341072 139758068020992 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.2685881853103638, loss=2.2783234119415283
I0307 09:03:19.103020 139758059628288 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2932624816894531, loss=2.100506544113159
I0307 09:03:46.174036 139912818214080 spec.py:321] Evaluating on the training split.
I0307 09:03:58.730912 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 09:04:20.373300 139912818214080 spec.py:349] Evaluating on the test split.
I0307 09:04:22.111133 139912818214080 submission_runner.py:469] Time since start: 25306.74s, 	Step: 60971, 	{'train/accuracy': 0.31204161047935486, 'train/loss': 3.484349012374878, 'validation/accuracy': 0.29197999835014343, 'validation/loss': 3.6320223808288574, 'validation/num_examples': 50000, 'test/accuracy': 0.2126000076532364, 'test/loss': 4.427322864532471, 'test/num_examples': 10000, 'score': 23516.41524028778, 'total_duration': 25306.741443395615, 'accumulated_submission_time': 23516.41524028778, 'accumulated_eval_time': 1779.05122756958, 'accumulated_logging_time': 5.224490642547607}
I0307 09:04:22.212472 139758068020992 logging_writer.py:48] [60971] accumulated_eval_time=1779.05, accumulated_logging_time=5.22449, accumulated_submission_time=23516.4, global_step=60971, preemption_count=0, score=23516.4, test/accuracy=0.2126, test/loss=4.42732, test/num_examples=10000, total_duration=25306.7, train/accuracy=0.312042, train/loss=3.48435, validation/accuracy=0.29198, validation/loss=3.63202, validation/num_examples=50000
I0307 09:04:33.701039 139758059628288 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2094248533248901, loss=2.220787525177002
I0307 09:05:11.845498 139758068020992 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2902554273605347, loss=2.2998132705688477
I0307 09:05:50.858171 139758059628288 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.290662169456482, loss=2.1608235836029053
I0307 09:06:29.522759 139758068020992 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.34737229347229, loss=2.290278196334839
I0307 09:07:07.346244 139758059628288 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.288405179977417, loss=2.1810667514801025
I0307 09:07:46.264721 139758068020992 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.278379201889038, loss=2.1291534900665283
I0307 09:08:24.145994 139758059628288 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.369417667388916, loss=2.150141954421997
I0307 09:09:01.692215 139758068020992 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.3316550254821777, loss=2.2099385261535645
I0307 09:09:40.163507 139758059628288 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.245934247970581, loss=2.112351417541504
I0307 09:10:18.166572 139758068020992 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.3448237180709839, loss=2.250605583190918
I0307 09:10:56.649411 139758059628288 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2801191806793213, loss=2.2710418701171875
I0307 09:11:35.099128 139758068020992 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.3346747159957886, loss=2.15092134475708
I0307 09:12:14.019412 139758059628288 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.371504545211792, loss=2.2022504806518555
I0307 09:12:52.884380 139912818214080 spec.py:321] Evaluating on the training split.
I0307 09:13:04.774291 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 09:13:24.771315 139912818214080 spec.py:349] Evaluating on the test split.
I0307 09:13:26.511292 139912818214080 submission_runner.py:469] Time since start: 25851.14s, 	Step: 62300, 	{'train/accuracy': 0.08432318270206451, 'train/loss': 6.182665824890137, 'validation/accuracy': 0.07967999577522278, 'validation/loss': 6.289052486419678, 'validation/num_examples': 50000, 'test/accuracy': 0.05650000274181366, 'test/loss': 6.748199462890625, 'test/num_examples': 10000, 'score': 24026.88319182396, 'total_duration': 25851.14160013199, 'accumulated_submission_time': 24026.88319182396, 'accumulated_eval_time': 1812.6780879497528, 'accumulated_logging_time': 5.386282205581665}
I0307 09:13:26.592175 139758068020992 logging_writer.py:48] [62300] accumulated_eval_time=1812.68, accumulated_logging_time=5.38628, accumulated_submission_time=24026.9, global_step=62300, preemption_count=0, score=24026.9, test/accuracy=0.0565, test/loss=6.7482, test/num_examples=10000, total_duration=25851.1, train/accuracy=0.0843232, train/loss=6.18267, validation/accuracy=0.07968, validation/loss=6.28905, validation/num_examples=50000
I0307 09:13:27.024147 139758059628288 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.330858826637268, loss=2.2640511989593506
I0307 09:14:05.000430 139758068020992 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.3074419498443604, loss=2.0711967945098877
I0307 09:14:43.720417 139758059628288 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.401541829109192, loss=2.3077452182769775
I0307 09:15:22.976372 139758068020992 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.2438499927520752, loss=2.1239726543426514
I0307 09:16:00.570555 139758059628288 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.272862195968628, loss=2.3379499912261963
I0307 09:16:38.015347 139758068020992 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.2801004648208618, loss=2.1477251052856445
I0307 09:17:16.256713 139758059628288 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.2235772609710693, loss=2.2425522804260254
I0307 09:17:54.406491 139758068020992 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.3113716840744019, loss=2.1982803344726562
I0307 09:18:33.006733 139758059628288 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.4418827295303345, loss=2.232968807220459
I0307 09:19:11.618397 139758068020992 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.327618956565857, loss=2.260930061340332
I0307 09:19:50.054913 139758059628288 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.3582665920257568, loss=2.151602029800415
I0307 09:20:28.488817 139758068020992 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.3731995820999146, loss=2.08695125579834
I0307 09:21:06.888991 139758059628288 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2223021984100342, loss=2.1543636322021484
I0307 09:21:45.144344 139758068020992 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.5334824323654175, loss=2.151952028274536
I0307 09:21:56.725142 139912818214080 spec.py:321] Evaluating on the training split.
I0307 09:22:09.167836 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 09:22:31.048050 139912818214080 spec.py:349] Evaluating on the test split.
I0307 09:22:32.761633 139912818214080 submission_runner.py:469] Time since start: 26397.39s, 	Step: 63631, 	{'train/accuracy': 0.20364317297935486, 'train/loss': 4.377975940704346, 'validation/accuracy': 0.19047999382019043, 'validation/loss': 4.559451580047607, 'validation/num_examples': 50000, 'test/accuracy': 0.13510000705718994, 'test/loss': 5.253306865692139, 'test/num_examples': 10000, 'score': 24536.85511612892, 'total_duration': 26397.39195251465, 'accumulated_submission_time': 24536.85511612892, 'accumulated_eval_time': 1848.7145290374756, 'accumulated_logging_time': 5.483468770980835}
I0307 09:22:32.875861 139758059628288 logging_writer.py:48] [63631] accumulated_eval_time=1848.71, accumulated_logging_time=5.48347, accumulated_submission_time=24536.9, global_step=63631, preemption_count=0, score=24536.9, test/accuracy=0.1351, test/loss=5.25331, test/num_examples=10000, total_duration=26397.4, train/accuracy=0.203643, train/loss=4.37798, validation/accuracy=0.19048, validation/loss=4.55945, validation/num_examples=50000
I0307 09:22:59.907325 139758068020992 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.3715029954910278, loss=2.2107763290405273
I0307 09:23:38.274876 139758059628288 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.3838077783584595, loss=2.367436647415161
I0307 09:24:16.346026 139758068020992 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3714251518249512, loss=2.2493813037872314
I0307 09:24:54.874926 139758059628288 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.245688796043396, loss=2.1587886810302734
I0307 09:25:33.495974 139758068020992 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.319519281387329, loss=2.302678346633911
I0307 09:26:12.106947 139758059628288 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.306089162826538, loss=2.180217742919922
I0307 09:26:49.928841 139758068020992 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.2606087923049927, loss=2.1677141189575195
I0307 09:27:30.328565 139758059628288 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.3148505687713623, loss=2.224323034286499
I0307 09:28:11.301544 139758068020992 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3934801816940308, loss=2.3617095947265625
I0307 09:28:49.454774 139758059628288 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.2250866889953613, loss=2.206821918487549
I0307 09:29:28.029443 139758068020992 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.3695379495620728, loss=2.2195661067962646
I0307 09:30:06.379563 139758059628288 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.3498996496200562, loss=2.2240586280822754
I0307 09:30:44.950572 139758068020992 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.4661016464233398, loss=2.175685405731201
I0307 09:31:02.976585 139912818214080 spec.py:321] Evaluating on the training split.
I0307 09:31:14.903495 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 09:31:31.876286 139912818214080 spec.py:349] Evaluating on the test split.
I0307 09:31:33.626004 139912818214080 submission_runner.py:469] Time since start: 26938.26s, 	Step: 64948, 	{'train/accuracy': 0.12232939898967743, 'train/loss': 5.541277885437012, 'validation/accuracy': 0.11481999605894089, 'validation/loss': 5.661109447479248, 'validation/num_examples': 50000, 'test/accuracy': 0.08340000361204147, 'test/loss': 6.302217483520508, 'test/num_examples': 10000, 'score': 25046.79501414299, 'total_duration': 26938.256316184998, 'accumulated_submission_time': 25046.79501414299, 'accumulated_eval_time': 1879.363888502121, 'accumulated_logging_time': 5.617506980895996}
I0307 09:31:33.751965 139758059628288 logging_writer.py:48] [64948] accumulated_eval_time=1879.36, accumulated_logging_time=5.61751, accumulated_submission_time=25046.8, global_step=64948, preemption_count=0, score=25046.8, test/accuracy=0.0834, test/loss=6.30222, test/num_examples=10000, total_duration=26938.3, train/accuracy=0.122329, train/loss=5.54128, validation/accuracy=0.11482, validation/loss=5.66111, validation/num_examples=50000
I0307 09:31:53.966907 139758068020992 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1888513565063477, loss=2.061490774154663
I0307 09:33:14.251982 139758059628288 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.3058526515960693, loss=2.2527270317077637
I0307 09:33:51.227973 139758068020992 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.362082839012146, loss=2.112952709197998
I0307 09:34:29.493911 139758059628288 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.2370595932006836, loss=2.220712661743164
I0307 09:35:09.593671 139758068020992 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.3468883037567139, loss=2.101466417312622
I0307 09:35:48.516340 139758059628288 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.2560185194015503, loss=2.2666964530944824
I0307 09:36:27.293034 139758068020992 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.289711356163025, loss=2.199439525604248
I0307 09:37:05.079415 139758059628288 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.4651427268981934, loss=2.129967212677002
I0307 09:37:43.146114 139758068020992 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.389223337173462, loss=2.119610548019409
I0307 09:38:20.956194 139758059628288 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.3815841674804688, loss=2.100942611694336
I0307 09:38:59.765090 139758068020992 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.2412172555923462, loss=2.2679927349090576
I0307 09:39:43.887753 139758059628288 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.2095016241073608, loss=2.076319456100464
I0307 09:40:03.679597 139912818214080 spec.py:321] Evaluating on the training split.
I0307 09:40:15.909439 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 09:40:36.391857 139912818214080 spec.py:349] Evaluating on the test split.
I0307 09:40:38.111128 139912818214080 submission_runner.py:469] Time since start: 27482.74s, 	Step: 66146, 	{'train/accuracy': 0.21041932702064514, 'train/loss': 4.359518051147461, 'validation/accuracy': 0.19565999507904053, 'validation/loss': 4.513914108276367, 'validation/num_examples': 50000, 'test/accuracy': 0.14990000426769257, 'test/loss': 5.104888916015625, 'test/num_examples': 10000, 'score': 25556.569621801376, 'total_duration': 27482.741450071335, 'accumulated_submission_time': 25556.569621801376, 'accumulated_eval_time': 1913.795380115509, 'accumulated_logging_time': 5.76784348487854}
I0307 09:40:38.230556 139758068020992 logging_writer.py:48] [66146] accumulated_eval_time=1913.8, accumulated_logging_time=5.76784, accumulated_submission_time=25556.6, global_step=66146, preemption_count=0, score=25556.6, test/accuracy=0.1499, test/loss=5.10489, test/num_examples=10000, total_duration=27482.7, train/accuracy=0.210419, train/loss=4.35952, validation/accuracy=0.19566, validation/loss=4.51391, validation/num_examples=50000
I0307 09:40:59.335818 139758059628288 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.3164552450180054, loss=2.1456456184387207
2025-03-07 09:41:33.227346: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:41:37.448686 139758068020992 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2498723268508911, loss=2.0894486904144287
I0307 09:42:15.861218 139758059628288 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.306993842124939, loss=2.1597886085510254
I0307 09:42:54.604275 139758068020992 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1885552406311035, loss=2.13732647895813
I0307 09:43:34.785598 139758059628288 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.2544004917144775, loss=2.278198003768921
I0307 09:44:13.997318 139758068020992 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.373551845550537, loss=2.2285537719726562
I0307 09:44:52.267573 139758059628288 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.3001832962036133, loss=2.2067887783050537
I0307 09:45:30.669862 139758068020992 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3439874649047852, loss=2.1123437881469727
I0307 09:46:09.157356 139758059628288 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.3178904056549072, loss=2.1781716346740723
I0307 09:46:47.732177 139758068020992 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.2466778755187988, loss=2.116252899169922
I0307 09:47:26.207058 139758059628288 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.4614754915237427, loss=2.237190008163452
I0307 09:48:04.756778 139758068020992 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.5548124313354492, loss=2.225713014602661
I0307 09:48:45.876451 139758059628288 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2449067831039429, loss=2.1739039421081543
I0307 09:49:08.284557 139912818214080 spec.py:321] Evaluating on the training split.
I0307 09:49:20.668437 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 09:49:44.383589 139912818214080 spec.py:349] Evaluating on the test split.
I0307 09:49:46.076621 139912818214080 submission_runner.py:469] Time since start: 28030.71s, 	Step: 67457, 	{'train/accuracy': 0.10122369229793549, 'train/loss': 5.660772800445557, 'validation/accuracy': 0.10147999972105026, 'validation/loss': 5.681365966796875, 'validation/num_examples': 50000, 'test/accuracy': 0.0731000006198883, 'test/loss': 6.2830281257629395, 'test/num_examples': 10000, 'score': 26066.36860203743, 'total_duration': 28030.706936120987, 'accumulated_submission_time': 26066.36860203743, 'accumulated_eval_time': 1951.58740067482, 'accumulated_logging_time': 6.006271839141846}
I0307 09:49:46.178238 139758068020992 logging_writer.py:48] [67457] accumulated_eval_time=1951.59, accumulated_logging_time=6.00627, accumulated_submission_time=26066.4, global_step=67457, preemption_count=0, score=26066.4, test/accuracy=0.0731, test/loss=6.28303, test/num_examples=10000, total_duration=28030.7, train/accuracy=0.101224, train/loss=5.66077, validation/accuracy=0.10148, validation/loss=5.68137, validation/num_examples=50000
I0307 09:50:03.186851 139758059628288 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3792070150375366, loss=2.3410215377807617
I0307 09:50:41.130758 139758068020992 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.3918578624725342, loss=2.112377166748047
I0307 09:51:18.713566 139758059628288 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.3515510559082031, loss=2.270146369934082
I0307 09:51:56.000181 139758068020992 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.3631458282470703, loss=2.179945945739746
I0307 09:52:34.791146 139758059628288 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.263961672782898, loss=2.1407759189605713
I0307 09:53:14.573275 139758068020992 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3468241691589355, loss=2.165172576904297
I0307 09:53:58.820837 139758059628288 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.5144217014312744, loss=2.1656301021575928
I0307 09:54:39.294381 139758068020992 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.2988919019699097, loss=2.258049964904785
I0307 09:55:19.871666 139758059628288 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.2667558193206787, loss=2.0957422256469727
I0307 09:56:08.324393 139758068020992 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3408401012420654, loss=2.225557327270508
I0307 09:56:46.765053 139758059628288 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.3076932430267334, loss=2.2788774967193604
I0307 09:58:14.258006 139758068020992 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.36650550365448, loss=2.1859097480773926
I0307 09:58:16.490702 139912818214080 spec.py:321] Evaluating on the training split.
I0307 09:58:29.167586 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 09:58:55.999353 139912818214080 spec.py:349] Evaluating on the test split.
I0307 09:58:57.693557 139912818214080 submission_runner.py:469] Time since start: 28582.32s, 	Step: 68604, 	{'train/accuracy': 0.24226722121238708, 'train/loss': 4.013217926025391, 'validation/accuracy': 0.23211999237537384, 'validation/loss': 4.1081156730651855, 'validation/num_examples': 50000, 'test/accuracy': 0.16790001094341278, 'test/loss': 4.842030048370361, 'test/num_examples': 10000, 'score': 26576.530514001846, 'total_duration': 28582.323868989944, 'accumulated_submission_time': 26576.530514001846, 'accumulated_eval_time': 1992.7902088165283, 'accumulated_logging_time': 6.138944864273071}
I0307 09:58:57.761049 139758059628288 logging_writer.py:48] [68604] accumulated_eval_time=1992.79, accumulated_logging_time=6.13894, accumulated_submission_time=26576.5, global_step=68604, preemption_count=0, score=26576.5, test/accuracy=0.1679, test/loss=4.84203, test/num_examples=10000, total_duration=28582.3, train/accuracy=0.242267, train/loss=4.01322, validation/accuracy=0.23212, validation/loss=4.10812, validation/num_examples=50000
I0307 10:00:17.926686 139758068020992 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.262239694595337, loss=2.1891136169433594
I0307 10:00:56.158997 139758059628288 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.3504379987716675, loss=2.212249755859375
I0307 10:01:34.379640 139758068020992 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.3260637521743774, loss=2.18412446975708
I0307 10:02:12.955799 139758059628288 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.410455584526062, loss=2.147210121154785
I0307 10:02:51.128012 139758068020992 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.2907447814941406, loss=2.1276450157165527
I0307 10:03:29.840156 139758059628288 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.301763892173767, loss=2.334667921066284
I0307 10:04:08.340820 139758068020992 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.1750394105911255, loss=2.203583240509033
I0307 10:04:47.174334 139758059628288 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.2700271606445312, loss=2.0284616947174072
I0307 10:05:29.754605 139758068020992 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.337037444114685, loss=2.1885008811950684
I0307 10:06:13.376541 139758059628288 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.3302626609802246, loss=2.068023920059204
I0307 10:07:00.382768 139758068020992 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.6003724336624146, loss=2.2633566856384277
I0307 10:07:27.811840 139912818214080 spec.py:321] Evaluating on the training split.
I0307 10:07:40.141240 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 10:08:06.038805 139912818214080 spec.py:349] Evaluating on the test split.
I0307 10:08:07.747637 139912818214080 submission_runner.py:469] Time since start: 29132.38s, 	Step: 69761, 	{'train/accuracy': 0.4194435477256775, 'train/loss': 2.6731088161468506, 'validation/accuracy': 0.3912599980831146, 'validation/loss': 2.885143280029297, 'validation/num_examples': 50000, 'test/accuracy': 0.304500013589859, 'test/loss': 3.616581916809082, 'test/num_examples': 10000, 'score': 27086.441253900528, 'total_duration': 29132.377957344055, 'accumulated_submission_time': 27086.441253900528, 'accumulated_eval_time': 2032.7259666919708, 'accumulated_logging_time': 6.227500915527344}
I0307 10:08:07.907080 139758059628288 logging_writer.py:48] [69761] accumulated_eval_time=2032.73, accumulated_logging_time=6.2275, accumulated_submission_time=27086.4, global_step=69761, preemption_count=0, score=27086.4, test/accuracy=0.3045, test/loss=3.61658, test/num_examples=10000, total_duration=29132.4, train/accuracy=0.419444, train/loss=2.67311, validation/accuracy=0.39126, validation/loss=2.88514, validation/num_examples=50000
I0307 10:08:23.016470 139758068020992 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.2942814826965332, loss=2.1190319061279297
I0307 10:09:06.628708 139758059628288 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3566218614578247, loss=2.278965473175049
I0307 10:09:44.842575 139758068020992 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.6533968448638916, loss=2.1622812747955322
I0307 10:10:23.273668 139758059628288 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.334173560142517, loss=2.1051437854766846
I0307 10:11:01.347272 139758068020992 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.312745451927185, loss=2.1017093658447266
I0307 10:11:42.166694 139758059628288 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1677063703536987, loss=2.0778536796569824
I0307 10:12:23.147998 139758068020992 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.3184987306594849, loss=2.1633870601654053
I0307 10:13:08.021297 139758059628288 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.4023710489273071, loss=2.255746841430664
I0307 10:13:51.443802 139758068020992 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.4416393041610718, loss=2.1809558868408203
I0307 10:14:35.141390 139758059628288 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.3614528179168701, loss=2.2527828216552734
I0307 10:15:15.723145 139758068020992 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3249932527542114, loss=2.153068780899048
I0307 10:16:00.508658 139758059628288 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.3593273162841797, loss=2.262124538421631
I0307 10:16:37.810447 139912818214080 spec.py:321] Evaluating on the training split.
I0307 10:16:50.105201 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 10:17:11.918294 139912818214080 spec.py:349] Evaluating on the test split.
I0307 10:17:13.628017 139912818214080 submission_runner.py:469] Time since start: 29678.26s, 	Step: 70997, 	{'train/accuracy': 0.38020166754722595, 'train/loss': 2.990177869796753, 'validation/accuracy': 0.35207998752593994, 'validation/loss': 3.190067768096924, 'validation/num_examples': 50000, 'test/accuracy': 0.2696000039577484, 'test/loss': 3.9622013568878174, 'test/num_examples': 10000, 'score': 27596.176204681396, 'total_duration': 29678.258331775665, 'accumulated_submission_time': 27596.176204681396, 'accumulated_eval_time': 2068.5434789657593, 'accumulated_logging_time': 6.431675434112549}
I0307 10:17:13.762974 139758068020992 logging_writer.py:48] [70997] accumulated_eval_time=2068.54, accumulated_logging_time=6.43168, accumulated_submission_time=27596.2, global_step=70997, preemption_count=0, score=27596.2, test/accuracy=0.2696, test/loss=3.9622, test/num_examples=10000, total_duration=29678.3, train/accuracy=0.380202, train/loss=2.99018, validation/accuracy=0.35208, validation/loss=3.19007, validation/num_examples=50000
I0307 10:17:15.230559 139758059628288 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3944274187088013, loss=2.1358675956726074
I0307 10:17:53.671312 139758068020992 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.319687843322754, loss=2.1346986293792725
I0307 10:18:31.739243 139758059628288 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.4077242612838745, loss=2.1576039791107178
2025-03-07 10:19:08.158027: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:19:09.659310 139758068020992 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3390554189682007, loss=2.1790971755981445
I0307 10:20:32.746749 139758059628288 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.375936508178711, loss=2.1899662017822266
I0307 10:21:11.551546 139758068020992 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2913988828659058, loss=2.1759541034698486
I0307 10:21:50.837990 139758059628288 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.3717937469482422, loss=2.189408779144287
I0307 10:22:29.431339 139758068020992 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3395289182662964, loss=2.2405216693878174
I0307 10:23:07.630557 139758059628288 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.578384280204773, loss=2.16332745552063
I0307 10:23:45.896672 139758068020992 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.413447618484497, loss=2.1899611949920654
I0307 10:24:24.456661 139758059628288 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.2792315483093262, loss=2.1034159660339355
I0307 10:25:01.266145 139758068020992 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.2765178680419922, loss=2.0957584381103516
I0307 10:25:41.582249 139758059628288 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.3085132837295532, loss=2.075653076171875
I0307 10:25:43.744934 139912818214080 spec.py:321] Evaluating on the training split.
I0307 10:25:55.919185 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 10:26:18.898002 139912818214080 spec.py:349] Evaluating on the test split.
I0307 10:26:20.580248 139912818214080 submission_runner.py:469] Time since start: 30225.21s, 	Step: 72207, 	{'train/accuracy': 0.405970960855484, 'train/loss': 2.7443697452545166, 'validation/accuracy': 0.37849998474121094, 'validation/loss': 2.8891847133636475, 'validation/num_examples': 50000, 'test/accuracy': 0.28110000491142273, 'test/loss': 3.6352696418762207, 'test/num_examples': 10000, 'score': 28106.001940727234, 'total_duration': 30225.210569143295, 'accumulated_submission_time': 28106.001940727234, 'accumulated_eval_time': 2105.3787446022034, 'accumulated_logging_time': 6.595596551895142}
I0307 10:26:20.676428 139758068020992 logging_writer.py:48] [72207] accumulated_eval_time=2105.38, accumulated_logging_time=6.5956, accumulated_submission_time=28106, global_step=72207, preemption_count=0, score=28106, test/accuracy=0.2811, test/loss=3.63527, test/num_examples=10000, total_duration=30225.2, train/accuracy=0.405971, train/loss=2.74437, validation/accuracy=0.3785, validation/loss=2.88918, validation/num_examples=50000
I0307 10:26:56.646545 139758059628288 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.2837448120117188, loss=2.078596591949463
I0307 10:27:42.274876 139758068020992 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.323130488395691, loss=1.9477708339691162
I0307 10:28:28.298935 139758059628288 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.279173731803894, loss=2.2471859455108643
I0307 10:29:08.116603 139758068020992 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.3613512516021729, loss=2.230896234512329
I0307 10:29:46.961079 139758059628288 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.4103832244873047, loss=2.2806756496429443
I0307 10:30:28.267349 139758068020992 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3095866441726685, loss=2.0776712894439697
I0307 10:31:12.499332 139758059628288 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.3394371271133423, loss=2.0555193424224854
I0307 10:31:58.661563 139758068020992 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.349075198173523, loss=2.120283842086792
I0307 10:32:44.345734 139758059628288 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.3658335208892822, loss=2.1926612854003906
I0307 10:33:30.850723 139758068020992 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.4090813398361206, loss=2.183042049407959
I0307 10:34:19.626289 139758059628288 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.2942631244659424, loss=2.054072141647339
I0307 10:34:50.958527 139912818214080 spec.py:321] Evaluating on the training split.
I0307 10:35:03.533943 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 10:35:23.001888 139912818214080 spec.py:349] Evaluating on the test split.
I0307 10:35:24.717146 139912818214080 submission_runner.py:469] Time since start: 30769.35s, 	Step: 73357, 	{'train/accuracy': 0.2795161008834839, 'train/loss': 3.9444632530212402, 'validation/accuracy': 0.26486000418663025, 'validation/loss': 4.131694316864014, 'validation/num_examples': 50000, 'test/accuracy': 0.20240001380443573, 'test/loss': 4.866608619689941, 'test/num_examples': 10000, 'score': 28616.11544251442, 'total_duration': 30769.347465991974, 'accumulated_submission_time': 28616.11544251442, 'accumulated_eval_time': 2139.1373126506805, 'accumulated_logging_time': 6.7432496547698975}
I0307 10:35:24.815417 139758068020992 logging_writer.py:48] [73357] accumulated_eval_time=2139.14, accumulated_logging_time=6.74325, accumulated_submission_time=28616.1, global_step=73357, preemption_count=0, score=28616.1, test/accuracy=0.2024, test/loss=4.86661, test/num_examples=10000, total_duration=30769.3, train/accuracy=0.279516, train/loss=3.94446, validation/accuracy=0.26486, validation/loss=4.13169, validation/num_examples=50000
I0307 10:35:44.442237 139758059628288 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4227081537246704, loss=2.255767822265625
I0307 10:36:36.130985 139758068020992 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2934136390686035, loss=2.05480694770813
I0307 10:37:22.181520 139758059628288 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.3189691305160522, loss=2.1177940368652344
I0307 10:38:03.770880 139758068020992 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.4598253965377808, loss=2.174901008605957
I0307 10:38:45.705369 139758059628288 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3191572427749634, loss=2.018894910812378
I0307 10:39:23.599035 139758068020992 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.3031576871871948, loss=2.116436004638672
I0307 10:40:01.768128 139758059628288 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.3560718297958374, loss=2.2434425354003906
I0307 10:40:39.406825 139758068020992 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.2797465324401855, loss=2.1080851554870605
I0307 10:41:17.320431 139758059628288 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.2927522659301758, loss=2.1879992485046387
I0307 10:41:55.707172 139758068020992 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.4222767353057861, loss=2.149777412414551
I0307 10:42:34.094177 139758059628288 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.4703428745269775, loss=2.211627244949341
I0307 10:43:12.073753 139758068020992 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.3758442401885986, loss=2.1951615810394287
I0307 10:43:50.328172 139758059628288 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.3880186080932617, loss=2.130387306213379
I0307 10:43:55.083642 139912818214080 spec.py:321] Evaluating on the training split.
I0307 10:44:07.085399 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 10:44:31.528446 139912818214080 spec.py:349] Evaluating on the test split.
I0307 10:44:33.220794 139912818214080 submission_runner.py:469] Time since start: 31317.85s, 	Step: 74613, 	{'train/accuracy': 0.38201528787612915, 'train/loss': 2.8985865116119385, 'validation/accuracy': 0.35471999645233154, 'validation/loss': 3.091930627822876, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.7877094745635986, 'test/num_examples': 10000, 'score': 29126.236311912537, 'total_duration': 31317.851099014282, 'accumulated_submission_time': 29126.236311912537, 'accumulated_eval_time': 2177.2743995189667, 'accumulated_logging_time': 6.862349033355713}
I0307 10:44:33.342744 139758068020992 logging_writer.py:48] [74613] accumulated_eval_time=2177.27, accumulated_logging_time=6.86235, accumulated_submission_time=29126.2, global_step=74613, preemption_count=0, score=29126.2, test/accuracy=0.2675, test/loss=3.78771, test/num_examples=10000, total_duration=31317.9, train/accuracy=0.382015, train/loss=2.89859, validation/accuracy=0.35472, validation/loss=3.09193, validation/num_examples=50000
I0307 10:45:07.647678 139758059628288 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3216224908828735, loss=2.2445061206817627
I0307 10:45:55.959888 139758068020992 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.5844917297363281, loss=2.199718952178955
I0307 10:46:41.448973 139758059628288 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.3771580457687378, loss=2.2047035694122314
I0307 10:47:22.781466 139758068020992 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.4707587957382202, loss=2.216057777404785
I0307 10:48:05.233483 139758059628288 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.4634147882461548, loss=2.220723867416382
I0307 10:48:44.398078 139758068020992 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.4249645471572876, loss=2.156540632247925
I0307 10:50:03.639147 139758059628288 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.3955167531967163, loss=2.1269662380218506
I0307 10:50:52.096320 139758068020992 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.3689829111099243, loss=2.1658101081848145
I0307 10:51:34.817756 139758059628288 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.4696882963180542, loss=2.1930253505706787
I0307 10:52:16.064994 139758068020992 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.4508776664733887, loss=2.1345112323760986
I0307 10:53:01.891806 139758059628288 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.1673916578292847, loss=2.0952188968658447
I0307 10:53:03.337384 139912818214080 spec.py:321] Evaluating on the training split.
I0307 10:53:15.756759 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 10:53:37.645817 139912818214080 spec.py:349] Evaluating on the test split.
I0307 10:53:39.394946 139912818214080 submission_runner.py:469] Time since start: 31864.03s, 	Step: 75704, 	{'train/accuracy': 0.2507772445678711, 'train/loss': 3.8978397846221924, 'validation/accuracy': 0.23555999994277954, 'validation/loss': 4.033202648162842, 'validation/num_examples': 50000, 'test/accuracy': 0.1672000139951706, 'test/loss': 4.81596565246582, 'test/num_examples': 10000, 'score': 29635.734853982925, 'total_duration': 31864.025255203247, 'accumulated_submission_time': 29635.734853982925, 'accumulated_eval_time': 2213.3318994045258, 'accumulated_logging_time': 7.369262218475342}
I0307 10:53:39.502998 139758068020992 logging_writer.py:48] [75704] accumulated_eval_time=2213.33, accumulated_logging_time=7.36926, accumulated_submission_time=29635.7, global_step=75704, preemption_count=0, score=29635.7, test/accuracy=0.1672, test/loss=4.81597, test/num_examples=10000, total_duration=31864, train/accuracy=0.250777, train/loss=3.89784, validation/accuracy=0.23556, validation/loss=4.0332, validation/num_examples=50000
I0307 10:54:37.322237 139758059628288 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.469382405281067, loss=2.1795554161071777
I0307 10:55:16.426454 139758068020992 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3887089490890503, loss=2.0865564346313477
I0307 10:55:57.171229 139758059628288 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.4919273853302002, loss=2.335103988647461
I0307 10:56:44.742305 139758068020992 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.4130126237869263, loss=2.1446588039398193
I0307 10:57:31.647160 139758059628288 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.4619805812835693, loss=2.112109661102295
I0307 10:58:13.452289 139758068020992 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.4871068000793457, loss=2.0975255966186523
I0307 10:58:52.668773 139758059628288 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.3234457969665527, loss=2.233582019805908
I0307 10:59:34.478443 139758068020992 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.3884518146514893, loss=2.0466227531433105
I0307 11:00:18.413616 139758059628288 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.5689111948013306, loss=2.089303493499756
I0307 11:00:58.675219 139758068020992 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.369124174118042, loss=2.177560329437256
I0307 11:01:51.629049 139758059628288 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.3793489933013916, loss=2.066699981689453
I0307 11:02:09.626746 139912818214080 spec.py:321] Evaluating on the training split.
I0307 11:02:22.057705 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 11:02:41.322283 139912818214080 spec.py:349] Evaluating on the test split.
I0307 11:02:43.093008 139912818214080 submission_runner.py:469] Time since start: 32407.72s, 	Step: 76832, 	{'train/accuracy': 0.2830835282802582, 'train/loss': 3.5965940952301025, 'validation/accuracy': 0.2624799907207489, 'validation/loss': 3.7933030128479004, 'validation/num_examples': 50000, 'test/accuracy': 0.18660001456737518, 'test/loss': 4.628851413726807, 'test/num_examples': 10000, 'score': 30145.685677289963, 'total_duration': 32407.723314523697, 'accumulated_submission_time': 30145.685677289963, 'accumulated_eval_time': 2246.798100233078, 'accumulated_logging_time': 7.535661220550537}
I0307 11:02:43.241274 139758068020992 logging_writer.py:48] [76832] accumulated_eval_time=2246.8, accumulated_logging_time=7.53566, accumulated_submission_time=30145.7, global_step=76832, preemption_count=0, score=30145.7, test/accuracy=0.1866, test/loss=4.62885, test/num_examples=10000, total_duration=32407.7, train/accuracy=0.283084, train/loss=3.59659, validation/accuracy=0.26248, validation/loss=3.7933, validation/num_examples=50000
I0307 11:03:09.653446 139758059628288 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.3507531881332397, loss=2.056875228881836
I0307 11:03:55.357988 139758068020992 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4359829425811768, loss=2.209355592727661
I0307 11:04:48.660397 139758059628288 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.517048716545105, loss=2.077528953552246
I0307 11:05:40.057476 139758068020992 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.4031693935394287, loss=2.115407943725586
I0307 11:06:25.786003 139758059628288 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.377793312072754, loss=2.121159553527832
I0307 11:07:14.371702 139758068020992 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4237881898880005, loss=2.073105573654175
I0307 11:07:55.385888 139758059628288 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.430726408958435, loss=2.1713409423828125
I0307 11:08:35.111117 139758068020992 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.3479870557785034, loss=2.175713062286377
I0307 11:09:13.047828 139758059628288 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.391780972480774, loss=2.148188829421997
I0307 11:09:54.983346 139758068020992 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.3439700603485107, loss=2.0933799743652344
I0307 11:10:33.196969 139758059628288 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.4577397108078003, loss=2.1960041522979736
I0307 11:11:11.122679 139758068020992 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.3692588806152344, loss=2.1023693084716797
I0307 11:11:13.482301 139912818214080 spec.py:321] Evaluating on the training split.
I0307 11:11:25.929221 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 11:11:42.742829 139912818214080 spec.py:349] Evaluating on the test split.
I0307 11:11:44.529538 139912818214080 submission_runner.py:469] Time since start: 32949.16s, 	Step: 78006, 	{'train/accuracy': 0.4375, 'train/loss': 2.5181257724761963, 'validation/accuracy': 0.41373997926712036, 'validation/loss': 2.6819775104522705, 'validation/num_examples': 50000, 'test/accuracy': 0.3184000253677368, 'test/loss': 3.4338059425354004, 'test/num_examples': 10000, 'score': 30655.76484131813, 'total_duration': 32949.15985393524, 'accumulated_submission_time': 30655.76484131813, 'accumulated_eval_time': 2277.8452911376953, 'accumulated_logging_time': 7.725235462188721}
I0307 11:11:44.654653 139758059628288 logging_writer.py:48] [78006] accumulated_eval_time=2277.85, accumulated_logging_time=7.72524, accumulated_submission_time=30655.8, global_step=78006, preemption_count=0, score=30655.8, test/accuracy=0.3184, test/loss=3.43381, test/num_examples=10000, total_duration=32949.2, train/accuracy=0.4375, train/loss=2.51813, validation/accuracy=0.41374, validation/loss=2.68198, validation/num_examples=50000
I0307 11:12:27.525910 139758068020992 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.5019150972366333, loss=2.2487680912017822
I0307 11:13:16.447975 139758059628288 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.343505620956421, loss=2.063632011413574
I0307 11:14:28.225541 139758068020992 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.4058741331100464, loss=2.012434959411621
I0307 11:15:20.141082 139758059628288 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.3986150026321411, loss=2.115460157394409
I0307 11:16:00.215121 139758068020992 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.5778627395629883, loss=2.1871509552001953
I0307 11:16:48.946902 139758059628288 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.3748658895492554, loss=2.2284200191497803
I0307 11:17:40.111876 139758068020992 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.3049226999282837, loss=2.047253131866455
I0307 11:18:27.278999 139758059628288 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.471888780593872, loss=2.1976559162139893
I0307 11:19:10.117015 139758068020992 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.3891202211380005, loss=2.1880593299865723
I0307 11:19:49.642417 139758059628288 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.7334449291229248, loss=2.1548237800598145
I0307 11:20:14.851652 139912818214080 spec.py:321] Evaluating on the training split.
I0307 11:20:26.918285 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 11:20:45.260231 139912818214080 spec.py:349] Evaluating on the test split.
I0307 11:20:47.008740 139912818214080 submission_runner.py:469] Time since start: 33491.64s, 	Step: 79059, 	{'train/accuracy': 0.4021444320678711, 'train/loss': 2.7515079975128174, 'validation/accuracy': 0.38843998312950134, 'validation/loss': 2.870887041091919, 'validation/num_examples': 50000, 'test/accuracy': 0.29010000824928284, 'test/loss': 3.6156668663024902, 'test/num_examples': 10000, 'score': 31165.823544979095, 'total_duration': 33491.63904905319, 'accumulated_submission_time': 31165.823544979095, 'accumulated_eval_time': 2310.0023300647736, 'accumulated_logging_time': 7.882008790969849}
I0307 11:20:47.115809 139758068020992 logging_writer.py:48] [79059] accumulated_eval_time=2310, accumulated_logging_time=7.88201, accumulated_submission_time=31165.8, global_step=79059, preemption_count=0, score=31165.8, test/accuracy=0.2901, test/loss=3.61567, test/num_examples=10000, total_duration=33491.6, train/accuracy=0.402144, train/loss=2.75151, validation/accuracy=0.38844, validation/loss=2.87089, validation/num_examples=50000
I0307 11:21:12.004846 139758059628288 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.5540215969085693, loss=2.216454029083252
I0307 11:21:54.179654 139758068020992 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.4094266891479492, loss=2.1260011196136475
I0307 11:22:39.086601 139758059628288 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.4096190929412842, loss=2.128706693649292
I0307 11:23:29.139084 139758068020992 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.3712142705917358, loss=2.115391969680786
I0307 11:24:22.453852 139758059628288 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.3502094745635986, loss=2.057137966156006
I0307 11:25:07.531141 139758068020992 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.4097280502319336, loss=2.1991190910339355
I0307 11:26:10.173352 139758059628288 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.413815975189209, loss=2.2050328254699707
I0307 11:27:04.297670 139758068020992 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.5934720039367676, loss=2.1204326152801514
I0307 11:27:59.133040 139758059628288 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.46878981590271, loss=2.0798518657684326
I0307 11:28:44.204003 139758068020992 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.38675057888031, loss=2.1858069896698
2025-03-07 11:29:07.547511: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:29:18.132163 139912818214080 spec.py:321] Evaluating on the training split.
I0307 11:29:30.369894 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 11:29:51.206985 139912818214080 spec.py:349] Evaluating on the test split.
I0307 11:29:52.924165 139912818214080 submission_runner.py:469] Time since start: 34037.55s, 	Step: 80068, 	{'train/accuracy': 0.39716199040412903, 'train/loss': 2.8456647396087646, 'validation/accuracy': 0.3685999810695648, 'validation/loss': 3.0420327186584473, 'validation/num_examples': 50000, 'test/accuracy': 0.27310001850128174, 'test/loss': 3.8727002143859863, 'test/num_examples': 10000, 'score': 31676.695521593094, 'total_duration': 34037.5544860363, 'accumulated_submission_time': 31676.695521593094, 'accumulated_eval_time': 2344.794293165207, 'accumulated_logging_time': 8.031684637069702}
I0307 11:29:53.070989 139758059628288 logging_writer.py:48] [80068] accumulated_eval_time=2344.79, accumulated_logging_time=8.03168, accumulated_submission_time=31676.7, global_step=80068, preemption_count=0, score=31676.7, test/accuracy=0.2731, test/loss=3.8727, test/num_examples=10000, total_duration=34037.6, train/accuracy=0.397162, train/loss=2.84566, validation/accuracy=0.3686, validation/loss=3.04203, validation/num_examples=50000
I0307 11:30:14.843488 139758068020992 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.7787070274353027, loss=2.0660810470581055
I0307 11:30:59.209687 139758059628288 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.3860092163085938, loss=2.135921001434326
I0307 11:31:54.931009 139758068020992 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.4936580657958984, loss=2.161381959915161
I0307 11:32:52.886801 139758059628288 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.4637706279754639, loss=2.1861395835876465
I0307 11:33:42.494090 139758068020992 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.4046730995178223, loss=2.0781893730163574
I0307 11:34:32.171939 139758059628288 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.3016458749771118, loss=2.032548666000366
I0307 11:35:34.270012 139758068020992 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.4197489023208618, loss=2.097416639328003
I0307 11:36:35.837924 139758059628288 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.6459733247756958, loss=2.086531639099121
I0307 11:37:32.338443 139758068020992 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.3271856307983398, loss=2.040322780609131
I0307 11:38:23.371030 139912818214080 spec.py:321] Evaluating on the training split.
I0307 11:38:35.578996 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 11:39:00.443968 139912818214080 spec.py:349] Evaluating on the test split.
I0307 11:39:02.167157 139912818214080 submission_runner.py:469] Time since start: 34586.80s, 	Step: 80971, 	{'train/accuracy': 0.4471261203289032, 'train/loss': 2.471245765686035, 'validation/accuracy': 0.4171399772167206, 'validation/loss': 2.709916830062866, 'validation/num_examples': 50000, 'test/accuracy': 0.3068000078201294, 'test/loss': 3.6072494983673096, 'test/num_examples': 10000, 'score': 32186.881184339523, 'total_duration': 34586.79746770859, 'accumulated_submission_time': 32186.881184339523, 'accumulated_eval_time': 2383.5903754234314, 'accumulated_logging_time': 8.202873229980469}
I0307 11:39:02.260369 139758059628288 logging_writer.py:48] [80971] accumulated_eval_time=2383.59, accumulated_logging_time=8.20287, accumulated_submission_time=32186.9, global_step=80971, preemption_count=0, score=32186.9, test/accuracy=0.3068, test/loss=3.60725, test/num_examples=10000, total_duration=34586.8, train/accuracy=0.447126, train/loss=2.47125, validation/accuracy=0.41714, validation/loss=2.70992, validation/num_examples=50000
I0307 11:39:13.575329 139758068020992 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.4362872838974, loss=2.0512826442718506
I0307 11:40:01.067903 139758059628288 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.4168329238891602, loss=2.052114725112915
I0307 11:41:02.625257 139758068020992 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.3636348247528076, loss=2.1675896644592285
I0307 11:41:50.579336 139758059628288 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.4061113595962524, loss=2.159139633178711
I0307 11:42:29.878892 139758068020992 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.4532352685928345, loss=2.162689685821533
I0307 11:43:08.697312 139758059628288 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.5729382038116455, loss=2.0063211917877197
I0307 11:43:46.192960 139758068020992 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.554135799407959, loss=2.2385804653167725
I0307 11:44:25.865116 139758059628288 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.4761346578598022, loss=2.0846328735351562
I0307 11:45:06.029657 139758068020992 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.3981757164001465, loss=2.0710811614990234
I0307 11:45:50.791740 139758059628288 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.567861557006836, loss=2.13494873046875
I0307 11:46:56.311248 139758068020992 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.5273691415786743, loss=2.214791774749756
I0307 11:47:32.468184 139912818214080 spec.py:321] Evaluating on the training split.
I0307 11:47:45.312123 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 11:48:10.960082 139912818214080 spec.py:349] Evaluating on the test split.
I0307 11:48:12.686047 139912818214080 submission_runner.py:469] Time since start: 35137.32s, 	Step: 82062, 	{'train/accuracy': 0.24774792790412903, 'train/loss': 4.247570991516113, 'validation/accuracy': 0.21781998872756958, 'validation/loss': 4.5438642501831055, 'validation/num_examples': 50000, 'test/accuracy': 0.17160001397132874, 'test/loss': 5.091366291046143, 'test/num_examples': 10000, 'score': 32696.94563627243, 'total_duration': 35137.31633806229, 'accumulated_submission_time': 32696.94563627243, 'accumulated_eval_time': 2423.8081605434418, 'accumulated_logging_time': 8.330325365066528}
I0307 11:48:12.841196 139758059628288 logging_writer.py:48] [82062] accumulated_eval_time=2423.81, accumulated_logging_time=8.33033, accumulated_submission_time=32696.9, global_step=82062, preemption_count=0, score=32696.9, test/accuracy=0.1716, test/loss=5.09137, test/num_examples=10000, total_duration=35137.3, train/accuracy=0.247748, train/loss=4.24757, validation/accuracy=0.21782, validation/loss=4.54386, validation/num_examples=50000
I0307 11:48:27.656967 139758068020992 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.4138503074645996, loss=2.031846523284912
I0307 11:49:08.215990 139758059628288 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.5237809419631958, loss=2.2605128288269043
I0307 11:49:56.504144 139758068020992 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3365187644958496, loss=2.072763204574585
I0307 11:50:50.076821 139758059628288 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3789080381393433, loss=1.9979603290557861
I0307 11:52:24.278741 139758068020992 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.4560898542404175, loss=2.108123302459717
2025-03-07 11:52:57.186251: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:53:15.721152 139758059628288 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.3989189863204956, loss=2.193294048309326
I0307 11:54:00.240139 139758068020992 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.4737788438796997, loss=2.050623655319214
I0307 11:55:12.566453 139758059628288 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.505029559135437, loss=2.1909008026123047
I0307 11:56:46.567269 139912818214080 spec.py:321] Evaluating on the training split.
I0307 11:56:58.158240 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 11:57:20.767328 139912818214080 spec.py:349] Evaluating on the test split.
I0307 11:57:22.529276 139912818214080 submission_runner.py:469] Time since start: 35687.16s, 	Step: 82882, 	{'train/accuracy': 0.31377550959587097, 'train/loss': 3.415020704269409, 'validation/accuracy': 0.2882799804210663, 'validation/loss': 3.6132724285125732, 'validation/num_examples': 50000, 'test/accuracy': 0.22190001606941223, 'test/loss': 4.257192134857178, 'test/num_examples': 10000, 'score': 33210.56051301956, 'total_duration': 35687.1595954895, 'accumulated_submission_time': 33210.56051301956, 'accumulated_eval_time': 2459.7701184749603, 'accumulated_logging_time': 8.514837265014648}
I0307 11:57:22.637954 139758068020992 logging_writer.py:48] [82882] accumulated_eval_time=2459.77, accumulated_logging_time=8.51484, accumulated_submission_time=33210.6, global_step=82882, preemption_count=0, score=33210.6, test/accuracy=0.2219, test/loss=4.25719, test/num_examples=10000, total_duration=35687.2, train/accuracy=0.313776, train/loss=3.41502, validation/accuracy=0.28828, validation/loss=3.61327, validation/num_examples=50000
I0307 11:57:29.676459 139758059628288 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.6323769092559814, loss=2.168783187866211
I0307 11:58:35.129094 139758068020992 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.4038242101669312, loss=2.027374267578125
I0307 11:59:16.772994 139758059628288 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.522753119468689, loss=2.0785696506500244
I0307 12:00:02.187507 139758068020992 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.3968173265457153, loss=2.0343475341796875
I0307 12:00:57.000058 139758059628288 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.3992890119552612, loss=1.9298415184020996
I0307 12:01:48.294852 139758068020992 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.440860629081726, loss=2.0549702644348145
I0307 12:02:34.962318 139758059628288 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.3340882062911987, loss=2.1961236000061035
I0307 12:03:25.079237 139758068020992 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.3854761123657227, loss=2.176077127456665
I0307 12:04:24.739440 139758059628288 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.6849907636642456, loss=2.1141929626464844
I0307 12:05:09.507764 139758068020992 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.4728049039840698, loss=2.011843681335449
2025-03-07 12:05:15.140333: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:05:52.779648 139912818214080 spec.py:321] Evaluating on the training split.
I0307 12:06:05.083415 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 12:06:27.135429 139912818214080 spec.py:349] Evaluating on the test split.
I0307 12:06:28.836180 139912818214080 submission_runner.py:469] Time since start: 36233.47s, 	Step: 83882, 	{'train/accuracy': 0.4203603267669678, 'train/loss': 2.688044548034668, 'validation/accuracy': 0.3874799907207489, 'validation/loss': 2.9031968116760254, 'validation/num_examples': 50000, 'test/accuracy': 0.2930000126361847, 'test/loss': 3.65958833694458, 'test/num_examples': 10000, 'score': 33720.56858229637, 'total_duration': 36233.466488838196, 'accumulated_submission_time': 33720.56858229637, 'accumulated_eval_time': 2495.8265924453735, 'accumulated_logging_time': 8.658860206604004}
I0307 12:06:28.938264 139758059628288 logging_writer.py:48] [83882] accumulated_eval_time=2495.83, accumulated_logging_time=8.65886, accumulated_submission_time=33720.6, global_step=83882, preemption_count=0, score=33720.6, test/accuracy=0.293, test/loss=3.65959, test/num_examples=10000, total_duration=36233.5, train/accuracy=0.42036, train/loss=2.68804, validation/accuracy=0.38748, validation/loss=2.9032, validation/num_examples=50000
I0307 12:06:36.135429 139758068020992 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.4047002792358398, loss=2.165783166885376
I0307 12:07:43.284241 139758059628288 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.5096099376678467, loss=1.9690351486206055
I0307 12:08:47.392062 139758068020992 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.6041204929351807, loss=2.135230541229248
I0307 12:10:18.697066 139758059628288 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.4597197771072388, loss=2.0479156970977783
I0307 12:11:05.881338 139758068020992 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.5234673023223877, loss=2.057013988494873
I0307 12:12:18.044210 139758059628288 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.5822057723999023, loss=2.2368292808532715
I0307 12:13:10.858257 139758068020992 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.5245481729507446, loss=2.052316188812256
I0307 12:14:09.355195 139758059628288 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.6807200908660889, loss=2.0935630798339844
I0307 12:14:59.191954 139912818214080 spec.py:321] Evaluating on the training split.
I0307 12:15:11.486472 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 12:15:32.413075 139912818214080 spec.py:349] Evaluating on the test split.
I0307 12:15:34.112873 139912818214080 submission_runner.py:469] Time since start: 36778.74s, 	Step: 84681, 	{'train/accuracy': 0.42881056666374207, 'train/loss': 2.6365716457366943, 'validation/accuracy': 0.37379997968673706, 'validation/loss': 3.011368751525879, 'validation/num_examples': 50000, 'test/accuracy': 0.28540000319480896, 'test/loss': 3.734708070755005, 'test/num_examples': 10000, 'score': 34230.711030721664, 'total_duration': 36778.74319434166, 'accumulated_submission_time': 34230.711030721664, 'accumulated_eval_time': 2530.7474744319916, 'accumulated_logging_time': 8.789774656295776}
I0307 12:15:34.265254 139758068020992 logging_writer.py:48] [84681] accumulated_eval_time=2530.75, accumulated_logging_time=8.78977, accumulated_submission_time=34230.7, global_step=84681, preemption_count=0, score=34230.7, test/accuracy=0.2854, test/loss=3.73471, test/num_examples=10000, total_duration=36778.7, train/accuracy=0.428811, train/loss=2.63657, validation/accuracy=0.3738, validation/loss=3.01137, validation/num_examples=50000
I0307 12:15:41.918615 139758059628288 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.4983885288238525, loss=2.1438679695129395
I0307 12:16:24.442994 139758068020992 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.4877055883407593, loss=2.168776035308838
I0307 12:17:34.751765 139758059628288 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.4164648056030273, loss=2.044593095779419
I0307 12:18:37.995869 139758068020992 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.4626083374023438, loss=2.1791253089904785
I0307 12:19:32.353769 139758059628288 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.5357295274734497, loss=2.0813517570495605
I0307 12:20:12.687435 139758068020992 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.4739075899124146, loss=2.1580843925476074
I0307 12:20:57.791496 139758059628288 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.5881277322769165, loss=2.0147712230682373
I0307 12:22:39.131562 139758068020992 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.4218906164169312, loss=2.0636239051818848
I0307 12:23:43.450116 139758059628288 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.4841015338897705, loss=2.0011768341064453
I0307 12:24:04.600143 139912818214080 spec.py:321] Evaluating on the training split.
I0307 12:24:16.112174 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 12:24:35.790492 139912818214080 spec.py:349] Evaluating on the test split.
I0307 12:24:37.516615 139912818214080 submission_runner.py:469] Time since start: 37322.15s, 	Step: 85537, 	{'train/accuracy': 0.32495614886283875, 'train/loss': 3.3678696155548096, 'validation/accuracy': 0.30107998847961426, 'validation/loss': 3.5798227787017822, 'validation/num_examples': 50000, 'test/accuracy': 0.2298000156879425, 'test/loss': 4.289230823516846, 'test/num_examples': 10000, 'score': 34740.91636013985, 'total_duration': 37322.14693522453, 'accumulated_submission_time': 34740.91636013985, 'accumulated_eval_time': 2563.6638991832733, 'accumulated_logging_time': 8.986862659454346}
I0307 12:24:37.592892 139758068020992 logging_writer.py:48] [85537] accumulated_eval_time=2563.66, accumulated_logging_time=8.98686, accumulated_submission_time=34740.9, global_step=85537, preemption_count=0, score=34740.9, test/accuracy=0.2298, test/loss=4.28923, test/num_examples=10000, total_duration=37322.1, train/accuracy=0.324956, train/loss=3.36787, validation/accuracy=0.30108, validation/loss=3.57982, validation/num_examples=50000
I0307 12:25:16.654258 139758059628288 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.4384207725524902, loss=2.0266480445861816
I0307 12:26:05.140164 139758068020992 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.5292924642562866, loss=2.0731399059295654
I0307 12:27:00.460917 139758059628288 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.3861792087554932, loss=2.0099809169769287
I0307 12:28:17.862851 139758068020992 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.487341284751892, loss=2.080000400543213
I0307 12:29:17.612053 139758059628288 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.558730125427246, loss=2.0676090717315674
I0307 12:30:30.240620 139758068020992 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.437769889831543, loss=2.0313732624053955
I0307 12:31:41.559507 139758059628288 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.4961405992507935, loss=2.1577260494232178
I0307 12:33:08.499587 139912818214080 spec.py:321] Evaluating on the training split.
I0307 12:33:18.764564 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 12:33:36.246085 139912818214080 spec.py:349] Evaluating on the test split.
I0307 12:33:37.991968 139912818214080 submission_runner.py:469] Time since start: 37862.60s, 	Step: 86283, 	{'train/accuracy': 0.4278738796710968, 'train/loss': 2.6401331424713135, 'validation/accuracy': 0.3942199945449829, 'validation/loss': 2.8568708896636963, 'validation/num_examples': 50000, 'test/accuracy': 0.30420002341270447, 'test/loss': 3.6280579566955566, 'test/num_examples': 10000, 'score': 35251.72738862038, 'total_duration': 37862.59804439545, 'accumulated_submission_time': 35251.72738862038, 'accumulated_eval_time': 2593.1319942474365, 'accumulated_logging_time': 9.08389687538147}
I0307 12:33:38.084242 139758068020992 logging_writer.py:48] [86283] accumulated_eval_time=2593.13, accumulated_logging_time=9.0839, accumulated_submission_time=35251.7, global_step=86283, preemption_count=0, score=35251.7, test/accuracy=0.3042, test/loss=3.62806, test/num_examples=10000, total_duration=37862.6, train/accuracy=0.427874, train/loss=2.64013, validation/accuracy=0.39422, validation/loss=2.85687, validation/num_examples=50000
I0307 12:33:48.653130 139758059628288 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.6258065700531006, loss=2.166168451309204
I0307 12:35:30.372471 139758068020992 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.5092968940734863, loss=2.1798579692840576
I0307 12:36:44.583000 139758059628288 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.5560561418533325, loss=2.148451805114746
I0307 12:37:55.166430 139758068020992 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.5574662685394287, loss=2.024747371673584
I0307 12:40:12.710141 139758059628288 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.5444612503051758, loss=2.02822208404541
I0307 12:41:02.447682 139758068020992 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.6451311111450195, loss=2.0946521759033203
I0307 12:42:08.464172 139912818214080 spec.py:321] Evaluating on the training split.
I0307 12:42:18.994475 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 12:42:41.366240 139912818214080 spec.py:349] Evaluating on the test split.
I0307 12:42:43.090328 139912818214080 submission_runner.py:469] Time since start: 38407.70s, 	Step: 86872, 	{'train/accuracy': 0.36463648080825806, 'train/loss': 3.0720016956329346, 'validation/accuracy': 0.3403799831867218, 'validation/loss': 3.224980592727661, 'validation/num_examples': 50000, 'test/accuracy': 0.24290001392364502, 'test/loss': 4.040991306304932, 'test/num_examples': 10000, 'score': 35762.01755452156, 'total_duration': 38407.703194856644, 'accumulated_submission_time': 35762.01755452156, 'accumulated_eval_time': 2627.7406697273254, 'accumulated_logging_time': 9.208647966384888}
I0307 12:42:43.140567 139758059628288 logging_writer.py:48] [86872] accumulated_eval_time=2627.74, accumulated_logging_time=9.20865, accumulated_submission_time=35762, global_step=86872, preemption_count=0, score=35762, test/accuracy=0.2429, test/loss=4.04099, test/num_examples=10000, total_duration=38407.7, train/accuracy=0.364636, train/loss=3.072, validation/accuracy=0.34038, validation/loss=3.22498, validation/num_examples=50000
I0307 12:42:56.778323 139758068020992 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5588257312774658, loss=2.1645350456237793
I0307 12:44:19.165971 139758059628288 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.5988423824310303, loss=1.9979372024536133
I0307 12:45:15.043687 139758068020992 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.527077555656433, loss=2.0390567779541016
I0307 12:46:18.111287 139758059628288 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.483594298362732, loss=1.9663587808609009
I0307 12:47:05.710664 139758068020992 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.638449788093567, loss=2.162299871444702
I0307 12:48:22.776159 139758059628288 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.40707528591156, loss=2.011718273162842
I0307 12:49:33.352121 139758068020992 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.5562111139297485, loss=2.0996313095092773
I0307 12:50:59.245639 139758059628288 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.4751322269439697, loss=2.108316421508789
I0307 12:51:13.121193 139912818214080 spec.py:321] Evaluating on the training split.
I0307 12:51:23.088274 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 12:51:42.116261 139912818214080 spec.py:349] Evaluating on the test split.
I0307 12:51:43.889039 139912818214080 submission_runner.py:469] Time since start: 38948.47s, 	Step: 87613, 	{'train/accuracy': 0.3198740482330322, 'train/loss': 3.4488463401794434, 'validation/accuracy': 0.29503998160362244, 'validation/loss': 3.62949800491333, 'validation/num_examples': 50000, 'test/accuracy': 0.2118000090122223, 'test/loss': 4.455244064331055, 'test/num_examples': 10000, 'score': 36271.91620469093, 'total_duration': 38948.47041392326, 'accumulated_submission_time': 36271.91620469093, 'accumulated_eval_time': 2658.45951962471, 'accumulated_logging_time': 9.26658296585083}
I0307 12:51:43.981061 139758068020992 logging_writer.py:48] [87613] accumulated_eval_time=2658.46, accumulated_logging_time=9.26658, accumulated_submission_time=36271.9, global_step=87613, preemption_count=0, score=36271.9, test/accuracy=0.2118, test/loss=4.45524, test/num_examples=10000, total_duration=38948.5, train/accuracy=0.319874, train/loss=3.44885, validation/accuracy=0.29504, validation/loss=3.6295, validation/num_examples=50000
I0307 12:52:29.288593 139758059628288 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.557840347290039, loss=2.2153899669647217
I0307 12:53:27.799686 139758068020992 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.4852116107940674, loss=2.014781951904297
I0307 12:54:25.039813 139758059628288 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.5309863090515137, loss=2.193665027618408
I0307 12:55:16.329903 139758068020992 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.4662928581237793, loss=1.9970207214355469
I0307 12:56:12.126079 139758059628288 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.435174822807312, loss=2.149777889251709
I0307 12:58:11.892665 139758068020992 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.461766004562378, loss=2.0847997665405273
I0307 12:59:34.570327 139758059628288 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.5132393836975098, loss=2.0878939628601074
I0307 13:00:14.222176 139912818214080 spec.py:321] Evaluating on the training split.
I0307 13:00:24.541302 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 13:00:46.811448 139912818214080 spec.py:349] Evaluating on the test split.
I0307 13:00:48.526310 139912818214080 submission_runner.py:469] Time since start: 39493.13s, 	Step: 88353, 	{'train/accuracy': 0.4383968412876129, 'train/loss': 2.566208839416504, 'validation/accuracy': 0.4149799942970276, 'validation/loss': 2.726715326309204, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.6193692684173584, 'test/num_examples': 10000, 'score': 36782.07646083832, 'total_duration': 39493.12785601616, 'accumulated_submission_time': 36782.07646083832, 'accumulated_eval_time': 2692.734828710556, 'accumulated_logging_time': 9.36575198173523}
I0307 13:00:48.594693 139758068020992 logging_writer.py:48] [88353] accumulated_eval_time=2692.73, accumulated_logging_time=9.36575, accumulated_submission_time=36782.1, global_step=88353, preemption_count=0, score=36782.1, test/accuracy=0.3023, test/loss=3.61937, test/num_examples=10000, total_duration=39493.1, train/accuracy=0.438397, train/loss=2.56621, validation/accuracy=0.41498, validation/loss=2.72672, validation/num_examples=50000
I0307 13:01:18.743613 139758059628288 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.4901338815689087, loss=2.0581164360046387
I0307 13:02:13.419103 139758068020992 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.4584873914718628, loss=2.134453535079956
I0307 13:04:22.212140 139758059628288 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.468140959739685, loss=2.0021541118621826
I0307 13:05:40.326966 139758068020992 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.5645047426223755, loss=2.0677473545074463
I0307 13:06:40.429890 139758059628288 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.4904897212982178, loss=2.203299045562744
I0307 13:07:35.191964 139758068020992 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.4556318521499634, loss=2.005842685699463
I0307 13:08:52.451900 139758059628288 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.5728909969329834, loss=1.9931402206420898
I0307 13:09:18.774234 139912818214080 spec.py:321] Evaluating on the training split.
I0307 13:09:29.710444 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 13:09:51.242249 139912818214080 spec.py:349] Evaluating on the test split.
I0307 13:09:52.943443 139912818214080 submission_runner.py:469] Time since start: 40037.56s, 	Step: 89032, 	{'train/accuracy': 0.5041653513908386, 'train/loss': 2.136971950531006, 'validation/accuracy': 0.4671799838542938, 'validation/loss': 2.3669753074645996, 'validation/num_examples': 50000, 'test/accuracy': 0.36180001497268677, 'test/loss': 3.137204885482788, 'test/num_examples': 10000, 'score': 37292.15191030502, 'total_duration': 40037.56240582466, 'accumulated_submission_time': 37292.15191030502, 'accumulated_eval_time': 2726.892630338669, 'accumulated_logging_time': 9.470914602279663}
I0307 13:09:53.031827 139758068020992 logging_writer.py:48] [89032] accumulated_eval_time=2726.89, accumulated_logging_time=9.47091, accumulated_submission_time=37292.2, global_step=89032, preemption_count=0, score=37292.2, test/accuracy=0.3618, test/loss=3.1372, test/num_examples=10000, total_duration=40037.6, train/accuracy=0.504165, train/loss=2.13697, validation/accuracy=0.46718, validation/loss=2.36698, validation/num_examples=50000
I0307 13:10:38.191401 139758059628288 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.5345755815505981, loss=2.141951084136963
I0307 13:11:58.529320 139758068020992 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.4188461303710938, loss=2.036702871322632
I0307 13:13:25.923950 139758059628288 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.68950617313385, loss=2.044556140899658
I0307 13:14:52.094010 139758068020992 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.509479284286499, loss=2.1476335525512695
I0307 13:16:23.781298 139758059628288 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.5757430791854858, loss=2.1039586067199707
I0307 13:18:03.683523 139758068020992 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.4371055364608765, loss=1.9761457443237305
I0307 13:18:23.652834 139912818214080 spec.py:321] Evaluating on the training split.
I0307 13:18:34.774060 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 13:19:00.408504 139912818214080 spec.py:349] Evaluating on the test split.
I0307 13:19:02.108160 139912818214080 submission_runner.py:469] Time since start: 40586.74s, 	Step: 89630, 	{'train/accuracy': 0.28491708636283875, 'train/loss': 3.8181982040405273, 'validation/accuracy': 0.2721799910068512, 'validation/loss': 3.9344396591186523, 'validation/num_examples': 50000, 'test/accuracy': 0.19480000436306, 'test/loss': 4.78629732131958, 'test/num_examples': 10000, 'score': 37802.704761981964, 'total_duration': 40586.73848104477, 'accumulated_submission_time': 37802.704761981964, 'accumulated_eval_time': 2765.3479180336, 'accumulated_logging_time': 9.566975355148315}
I0307 13:19:02.148957 139758059628288 logging_writer.py:48] [89630] accumulated_eval_time=2765.35, accumulated_logging_time=9.56698, accumulated_submission_time=37802.7, global_step=89630, preemption_count=0, score=37802.7, test/accuracy=0.1948, test/loss=4.7863, test/num_examples=10000, total_duration=40586.7, train/accuracy=0.284917, train/loss=3.8182, validation/accuracy=0.27218, validation/loss=3.93444, validation/num_examples=50000
I0307 13:19:32.825926 139758068020992 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.602782964706421, loss=2.0643138885498047
I0307 13:20:15.613743 139758059628288 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.5799553394317627, loss=2.0557899475097656
I0307 13:22:18.275054 139758068020992 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.5123077630996704, loss=2.0728118419647217
I0307 13:23:27.649103 139758059628288 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.5188051462173462, loss=2.1347291469573975
I0307 13:24:23.582371 139758068020992 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6568838357925415, loss=2.209702491760254
I0307 13:26:00.097870 139758059628288 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.3722398281097412, loss=1.977065920829773
I0307 13:27:24.994174 139758068020992 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.6536004543304443, loss=2.1235578060150146
I0307 13:27:32.364883 139912818214080 spec.py:321] Evaluating on the training split.
I0307 13:27:43.523487 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 13:28:07.380070 139912818214080 spec.py:349] Evaluating on the test split.
I0307 13:28:09.078768 139912818214080 submission_runner.py:469] Time since start: 41133.71s, 	Step: 90315, 	{'train/accuracy': 0.4287707209587097, 'train/loss': 2.583172559738159, 'validation/accuracy': 0.39183998107910156, 'validation/loss': 2.7954955101013184, 'validation/num_examples': 50000, 'test/accuracy': 0.3083000183105469, 'test/loss': 3.5039920806884766, 'test/num_examples': 10000, 'score': 38312.84613490105, 'total_duration': 41133.709064245224, 'accumulated_submission_time': 38312.84613490105, 'accumulated_eval_time': 2802.061729669571, 'accumulated_logging_time': 9.615619659423828}
I0307 13:28:09.111909 139758059628288 logging_writer.py:48] [90315] accumulated_eval_time=2802.06, accumulated_logging_time=9.61562, accumulated_submission_time=38312.8, global_step=90315, preemption_count=0, score=38312.8, test/accuracy=0.3083, test/loss=3.50399, test/num_examples=10000, total_duration=41133.7, train/accuracy=0.428771, train/loss=2.58317, validation/accuracy=0.39184, validation/loss=2.7955, validation/num_examples=50000
I0307 13:29:36.434864 139758068020992 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.5445212125778198, loss=2.0859785079956055
I0307 13:31:06.175175 139758059628288 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.6764858961105347, loss=2.1209819316864014
I0307 13:33:34.139414 139758068020992 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.6699204444885254, loss=2.0288796424865723
I0307 13:35:40.833474 139758059628288 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.477430820465088, loss=2.016047954559326
I0307 13:36:39.372680 139912818214080 spec.py:321] Evaluating on the training split.
I0307 13:36:49.658476 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 13:37:08.723898 139912818214080 spec.py:349] Evaluating on the test split.
I0307 13:37:10.471807 139912818214080 submission_runner.py:469] Time since start: 41675.10s, 	Step: 90761, 	{'train/accuracy': 0.3580596148967743, 'train/loss': 3.140453577041626, 'validation/accuracy': 0.32791998982429504, 'validation/loss': 3.3639862537384033, 'validation/num_examples': 50000, 'test/accuracy': 0.24990001320838928, 'test/loss': 4.090452194213867, 'test/num_examples': 10000, 'score': 38823.021700143814, 'total_duration': 41675.10214686394, 'accumulated_submission_time': 38823.021700143814, 'accumulated_eval_time': 2833.1608271598816, 'accumulated_logging_time': 9.68972134590149}
I0307 13:37:10.503613 139758068020992 logging_writer.py:48] [90761] accumulated_eval_time=2833.16, accumulated_logging_time=9.68972, accumulated_submission_time=38823, global_step=90761, preemption_count=0, score=38823, test/accuracy=0.2499, test/loss=4.09045, test/num_examples=10000, total_duration=41675.1, train/accuracy=0.35806, train/loss=3.14045, validation/accuracy=0.32792, validation/loss=3.36399, validation/num_examples=50000
I0307 13:37:41.730806 139758059628288 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.7409614324569702, loss=2.1122100353240967
I0307 13:39:30.511038 139758068020992 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.4562621116638184, loss=2.0612905025482178
I0307 13:40:46.405388 139758059628288 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.5307538509368896, loss=2.1082680225372314
I0307 13:42:48.178539 139758068020992 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.5429503917694092, loss=2.043020725250244
I0307 13:44:06.550545 139758059628288 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.4786105155944824, loss=2.1185033321380615
I0307 13:45:38.455173 139758068020992 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.5021069049835205, loss=2.0149662494659424
I0307 13:45:41.278527 139912818214080 spec.py:321] Evaluating on the training split.
I0307 13:45:51.318321 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 13:46:10.092492 139912818214080 spec.py:349] Evaluating on the test split.
I0307 13:46:11.825726 139912818214080 submission_runner.py:469] Time since start: 42216.46s, 	Step: 91303, 	{'train/accuracy': 0.22564572095870972, 'train/loss': 4.367197513580322, 'validation/accuracy': 0.20673999190330505, 'validation/loss': 4.565398216247559, 'validation/num_examples': 50000, 'test/accuracy': 0.16140000522136688, 'test/loss': 5.176565647125244, 'test/num_examples': 10000, 'score': 39333.71317434311, 'total_duration': 42216.456063985825, 'accumulated_submission_time': 39333.71317434311, 'accumulated_eval_time': 2863.70800447464, 'accumulated_logging_time': 9.75185513496399}
I0307 13:46:11.868834 139758059628288 logging_writer.py:48] [91303] accumulated_eval_time=2863.71, accumulated_logging_time=9.75186, accumulated_submission_time=39333.7, global_step=91303, preemption_count=0, score=39333.7, test/accuracy=0.1614, test/loss=5.17657, test/num_examples=10000, total_duration=42216.5, train/accuracy=0.225646, train/loss=4.3672, validation/accuracy=0.20674, validation/loss=4.5654, validation/num_examples=50000
I0307 13:48:36.918495 139758068020992 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.4477771520614624, loss=2.125807285308838
I0307 13:50:02.642213 139758059628288 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.4599158763885498, loss=2.055402994155884
I0307 13:52:01.354340 139758068020992 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.5112818479537964, loss=1.9658787250518799
I0307 13:53:06.731252 139758059628288 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.4683880805969238, loss=2.0944526195526123
I0307 13:54:42.674195 139912818214080 spec.py:321] Evaluating on the training split.
I0307 13:54:52.992949 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 13:55:12.989914 139912818214080 spec.py:349] Evaluating on the test split.
I0307 13:55:14.719325 139912818214080 submission_runner.py:469] Time since start: 42759.35s, 	Step: 91793, 	{'train/accuracy': 0.4008290767669678, 'train/loss': 2.850691795349121, 'validation/accuracy': 0.3708399832248688, 'validation/loss': 3.0268020629882812, 'validation/num_examples': 50000, 'test/accuracy': 0.2800000011920929, 'test/loss': 3.766608476638794, 'test/num_examples': 10000, 'score': 39844.46280670166, 'total_duration': 42759.34964585304, 'accumulated_submission_time': 39844.46280670166, 'accumulated_eval_time': 2895.7530965805054, 'accumulated_logging_time': 9.802462816238403}
I0307 13:55:14.763188 139758068020992 logging_writer.py:48] [91793] accumulated_eval_time=2895.75, accumulated_logging_time=9.80246, accumulated_submission_time=39844.5, global_step=91793, preemption_count=0, score=39844.5, test/accuracy=0.28, test/loss=3.76661, test/num_examples=10000, total_duration=42759.3, train/accuracy=0.400829, train/loss=2.85069, validation/accuracy=0.37084, validation/loss=3.0268, validation/num_examples=50000
I0307 13:55:17.847347 139758059628288 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.5067732334136963, loss=2.1858792304992676
I0307 13:56:58.762850 139758068020992 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.617283821105957, loss=2.1062464714050293
I0307 14:00:03.825279 139758059628288 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.5173040628433228, loss=2.0257081985473633
I0307 14:02:15.522992 139758068020992 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.5474979877471924, loss=2.0380947589874268
I0307 14:03:47.469293 139912818214080 spec.py:321] Evaluating on the training split.
I0307 14:03:56.418659 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 14:04:20.501707 139912818214080 spec.py:349] Evaluating on the test split.
I0307 14:04:22.214417 139912818214080 submission_runner.py:469] Time since start: 43306.84s, 	Step: 92123, 	{'train/accuracy': 0.47897401452064514, 'train/loss': 2.2947795391082764, 'validation/accuracy': 0.45124000310897827, 'validation/loss': 2.4649317264556885, 'validation/num_examples': 50000, 'test/accuracy': 0.34880000352859497, 'test/loss': 3.142186164855957, 'test/num_examples': 10000, 'score': 40357.12898850441, 'total_duration': 43306.84474802017, 'accumulated_submission_time': 40357.12898850441, 'accumulated_eval_time': 2930.49818277359, 'accumulated_logging_time': 9.853858947753906}
I0307 14:04:22.232891 139758059628288 logging_writer.py:48] [92123] accumulated_eval_time=2930.5, accumulated_logging_time=9.85386, accumulated_submission_time=40357.1, global_step=92123, preemption_count=0, score=40357.1, test/accuracy=0.3488, test/loss=3.14219, test/num_examples=10000, total_duration=43306.8, train/accuracy=0.478974, train/loss=2.29478, validation/accuracy=0.45124, validation/loss=2.46493, validation/num_examples=50000
I0307 14:05:53.926342 139758068020992 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.8168774843215942, loss=2.022376537322998
I0307 14:07:18.907458 139758059628288 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.5382589101791382, loss=2.1599698066711426
I0307 14:09:46.685304 139758068020992 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.6451199054718018, loss=2.0164685249328613
I0307 14:11:09.501151 139758059628288 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.5010679960250854, loss=1.9698131084442139
I0307 14:12:06.821297 139758068020992 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.7886970043182373, loss=2.053673505783081
I0307 14:12:52.311604 139912818214080 spec.py:321] Evaluating on the training split.
I0307 14:13:02.959351 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 14:13:26.559137 139912818214080 spec.py:349] Evaluating on the test split.
I0307 14:13:28.243422 139912818214080 submission_runner.py:469] Time since start: 43852.87s, 	Step: 92664, 	{'train/accuracy': 0.32746732234954834, 'train/loss': 3.3227267265319824, 'validation/accuracy': 0.30983999371528625, 'validation/loss': 3.4916634559631348, 'validation/num_examples': 50000, 'test/accuracy': 0.2353000044822693, 'test/loss': 4.212726593017578, 'test/num_examples': 10000, 'score': 40867.14444565773, 'total_duration': 43852.87376022339, 'accumulated_submission_time': 40867.14444565773, 'accumulated_eval_time': 2966.429989337921, 'accumulated_logging_time': 9.880236387252808}
I0307 14:13:28.323059 139758059628288 logging_writer.py:48] [92664] accumulated_eval_time=2966.43, accumulated_logging_time=9.88024, accumulated_submission_time=40867.1, global_step=92664, preemption_count=0, score=40867.1, test/accuracy=0.2353, test/loss=4.21273, test/num_examples=10000, total_duration=43852.9, train/accuracy=0.327467, train/loss=3.32273, validation/accuracy=0.30984, validation/loss=3.49166, validation/num_examples=50000
I0307 14:13:50.922730 139758068020992 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.4767333269119263, loss=1.9451926946640015
I0307 14:15:12.526385 139758059628288 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.6662042140960693, loss=2.2539572715759277
I0307 14:16:59.894950 139758068020992 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.6127240657806396, loss=2.1595449447631836
I0307 14:19:06.829511 139758059628288 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.5064210891723633, loss=2.214228630065918
I0307 14:21:59.789389 139912818214080 spec.py:321] Evaluating on the training split.
I0307 14:22:10.007110 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 14:22:29.762611 139912818214080 spec.py:349] Evaluating on the test split.
I0307 14:22:31.472008 139912818214080 submission_runner.py:469] Time since start: 44396.10s, 	Step: 93082, 	{'train/accuracy': 0.3913823366165161, 'train/loss': 2.8736515045166016, 'validation/accuracy': 0.3610199987888336, 'validation/loss': 3.1132686138153076, 'validation/num_examples': 50000, 'test/accuracy': 0.27810001373291016, 'test/loss': 3.883187770843506, 'test/num_examples': 10000, 'score': 41378.56245803833, 'total_duration': 44396.10234570503, 'accumulated_submission_time': 41378.56245803833, 'accumulated_eval_time': 2998.1125757694244, 'accumulated_logging_time': 9.967206239700317}
I0307 14:22:31.499666 139758068020992 logging_writer.py:48] [93082] accumulated_eval_time=2998.11, accumulated_logging_time=9.96721, accumulated_submission_time=41378.6, global_step=93082, preemption_count=0, score=41378.6, test/accuracy=0.2781, test/loss=3.88319, test/num_examples=10000, total_duration=44396.1, train/accuracy=0.391382, train/loss=2.87365, validation/accuracy=0.36102, validation/loss=3.11327, validation/num_examples=50000
I0307 14:22:43.138525 139758059628288 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.663751244544983, loss=2.0784401893615723
I0307 14:24:39.560708 139758068020992 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.6137114763259888, loss=1.9816107749938965
I0307 14:27:04.815488 139758059628288 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7753229141235352, loss=2.029303789138794
I0307 14:29:01.201032 139758068020992 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.5551508665084839, loss=2.0475735664367676
I0307 14:30:06.836145 139758059628288 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.6090792417526245, loss=2.0823967456817627
I0307 14:31:02.213012 139912818214080 spec.py:321] Evaluating on the training split.
I0307 14:31:12.147655 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 14:31:34.368864 139912818214080 spec.py:349] Evaluating on the test split.
I0307 14:31:36.056625 139912818214080 submission_runner.py:469] Time since start: 44940.69s, 	Step: 93520, 	{'train/accuracy': 0.22303491830825806, 'train/loss': 4.289080619812012, 'validation/accuracy': 0.21389999985694885, 'validation/loss': 4.370250701904297, 'validation/num_examples': 50000, 'test/accuracy': 0.15790000557899475, 'test/loss': 5.089090347290039, 'test/num_examples': 10000, 'score': 41889.2252702713, 'total_duration': 44940.686950445175, 'accumulated_submission_time': 41889.2252702713, 'accumulated_eval_time': 3031.9561438560486, 'accumulated_logging_time': 10.002298831939697}
I0307 14:31:36.101241 139758068020992 logging_writer.py:48] [93520] accumulated_eval_time=3031.96, accumulated_logging_time=10.0023, accumulated_submission_time=41889.2, global_step=93520, preemption_count=0, score=41889.2, test/accuracy=0.1579, test/loss=5.08909, test/num_examples=10000, total_duration=44940.7, train/accuracy=0.223035, train/loss=4.28908, validation/accuracy=0.2139, validation/loss=4.37025, validation/num_examples=50000
I0307 14:32:53.376506 139758059628288 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.6567039489746094, loss=2.075908899307251
I0307 14:33:57.107865 139758068020992 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.66254460811615, loss=2.1874287128448486
I0307 14:35:19.240872 139758059628288 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.570510745048523, loss=1.9579472541809082
I0307 14:38:30.720206 139758068020992 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.7074209451675415, loss=2.1573574542999268
I0307 14:40:07.427846 139912818214080 spec.py:321] Evaluating on the training split.
I0307 14:40:17.484166 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 14:40:37.330067 139912818214080 spec.py:349] Evaluating on the test split.
I0307 14:40:39.064166 139912818214080 submission_runner.py:469] Time since start: 45483.69s, 	Step: 93969, 	{'train/accuracy': 0.39714205265045166, 'train/loss': 2.807849407196045, 'validation/accuracy': 0.376120001077652, 'validation/loss': 2.949687957763672, 'validation/num_examples': 50000, 'test/accuracy': 0.28230002522468567, 'test/loss': 3.65767240524292, 'test/num_examples': 10000, 'score': 42400.49991130829, 'total_duration': 45483.69449329376, 'accumulated_submission_time': 42400.49991130829, 'accumulated_eval_time': 3063.5924298763275, 'accumulated_logging_time': 10.055891513824463}
I0307 14:40:39.106216 139758059628288 logging_writer.py:48] [93969] accumulated_eval_time=3063.59, accumulated_logging_time=10.0559, accumulated_submission_time=42400.5, global_step=93969, preemption_count=0, score=42400.5, test/accuracy=0.2823, test/loss=3.65767, test/num_examples=10000, total_duration=45483.7, train/accuracy=0.397142, train/loss=2.80785, validation/accuracy=0.37612, validation/loss=2.94969, validation/num_examples=50000
I0307 14:41:25.531819 139758068020992 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.6467950344085693, loss=2.179366111755371
I0307 14:43:20.298135 139758059628288 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.6546180248260498, loss=2.1091203689575195
I0307 14:45:42.309607 139758068020992 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.6022592782974243, loss=2.0360312461853027
I0307 14:47:31.253932 139758059628288 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.5917994976043701, loss=1.9656305313110352
I0307 14:49:09.550058 139912818214080 spec.py:321] Evaluating on the training split.
I0307 14:49:20.303336 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 14:49:39.739033 139912818214080 spec.py:349] Evaluating on the test split.
I0307 14:49:41.454080 139912818214080 submission_runner.py:469] Time since start: 46026.08s, 	Step: 94392, 	{'train/accuracy': 0.4871053695678711, 'train/loss': 2.297102451324463, 'validation/accuracy': 0.4463599920272827, 'validation/loss': 2.5426440238952637, 'validation/num_examples': 50000, 'test/accuracy': 0.3425000309944153, 'test/loss': 3.2957942485809326, 'test/num_examples': 10000, 'score': 42910.89403343201, 'total_duration': 46026.08441734314, 'accumulated_submission_time': 42910.89403343201, 'accumulated_eval_time': 3095.4964191913605, 'accumulated_logging_time': 10.105541706085205}
I0307 14:49:41.515615 139758068020992 logging_writer.py:48] [94392] accumulated_eval_time=3095.5, accumulated_logging_time=10.1055, accumulated_submission_time=42910.9, global_step=94392, preemption_count=0, score=42910.9, test/accuracy=0.3425, test/loss=3.29579, test/num_examples=10000, total_duration=46026.1, train/accuracy=0.487105, train/loss=2.2971, validation/accuracy=0.44636, validation/loss=2.54264, validation/num_examples=50000
I0307 14:49:44.867175 139758059628288 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.6171438694000244, loss=2.0181002616882324
I0307 14:52:22.628411 139758068020992 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.5465598106384277, loss=2.0205836296081543
I0307 14:56:14.020250 139758059628288 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.5453473329544067, loss=1.9280565977096558
I0307 14:57:34.772016 139758068020992 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.7575926780700684, loss=2.028125524520874
I0307 14:58:11.830611 139912818214080 spec.py:321] Evaluating on the training split.
I0307 14:58:22.348147 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 14:58:42.894794 139912818214080 spec.py:349] Evaluating on the test split.
I0307 14:58:44.588509 139912818214080 submission_runner.py:469] Time since start: 46569.22s, 	Step: 94747, 	{'train/accuracy': 0.36930006742477417, 'train/loss': 3.0641403198242188, 'validation/accuracy': 0.3443399965763092, 'validation/loss': 3.2247462272644043, 'validation/num_examples': 50000, 'test/accuracy': 0.25050002336502075, 'test/loss': 4.108883857727051, 'test/num_examples': 10000, 'score': 43421.15713977814, 'total_duration': 46569.21884703636, 'accumulated_submission_time': 43421.15713977814, 'accumulated_eval_time': 3128.2542958259583, 'accumulated_logging_time': 10.18239688873291}
I0307 14:58:44.622014 139758059628288 logging_writer.py:48] [94747] accumulated_eval_time=3128.25, accumulated_logging_time=10.1824, accumulated_submission_time=43421.2, global_step=94747, preemption_count=0, score=43421.2, test/accuracy=0.2505, test/loss=4.10888, test/num_examples=10000, total_duration=46569.2, train/accuracy=0.3693, train/loss=3.06414, validation/accuracy=0.34434, validation/loss=3.22475, validation/num_examples=50000
I0307 14:59:21.689376 139758068020992 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.7666012048721313, loss=1.9656479358673096
I0307 15:02:14.677097 139758059628288 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.5514851808547974, loss=1.983250617980957
I0307 15:05:30.936538 139758068020992 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.662208914756775, loss=1.929019570350647
I0307 15:07:03.072318 139758059628288 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.5795716047286987, loss=2.0978593826293945
I0307 15:07:14.792872 139912818214080 spec.py:321] Evaluating on the training split.
I0307 15:07:24.412340 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 15:07:43.900658 139912818214080 spec.py:349] Evaluating on the test split.
I0307 15:07:45.643880 139912818214080 submission_runner.py:469] Time since start: 47110.27s, 	Step: 95112, 	{'train/accuracy': 0.4338727593421936, 'train/loss': 2.610846757888794, 'validation/accuracy': 0.4057199954986572, 'validation/loss': 2.7837162017822266, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.6431634426116943, 'test/num_examples': 10000, 'score': 43931.28394365311, 'total_duration': 47110.27421617508, 'accumulated_submission_time': 43931.28394365311, 'accumulated_eval_time': 3159.1052787303925, 'accumulated_logging_time': 10.224416017532349}
I0307 15:07:45.726835 139758068020992 logging_writer.py:48] [95112] accumulated_eval_time=3159.11, accumulated_logging_time=10.2244, accumulated_submission_time=43931.3, global_step=95112, preemption_count=0, score=43931.3, test/accuracy=0.3023, test/loss=3.64316, test/num_examples=10000, total_duration=47110.3, train/accuracy=0.433873, train/loss=2.61085, validation/accuracy=0.40572, validation/loss=2.78372, validation/num_examples=50000
I0307 15:09:12.670219 139758059628288 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.6834776401519775, loss=1.92702317237854
I0307 15:11:23.998551 139758068020992 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.6163609027862549, loss=2.0337281227111816
I0307 15:13:48.067939 139758059628288 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.585781455039978, loss=1.9500113725662231
I0307 15:15:10.674023 139758068020992 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.6524735689163208, loss=2.0174529552459717
I0307 15:16:15.818752 139912818214080 spec.py:321] Evaluating on the training split.
I0307 15:16:26.240383 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 15:16:48.694809 139912818214080 spec.py:349] Evaluating on the test split.
I0307 15:16:50.379382 139912818214080 submission_runner.py:469] Time since start: 47655.01s, 	Step: 95581, 	{'train/accuracy': 0.35321667790412903, 'train/loss': 3.0999114513397217, 'validation/accuracy': 0.33013999462127686, 'validation/loss': 3.2671823501586914, 'validation/num_examples': 50000, 'test/accuracy': 0.2615000009536743, 'test/loss': 3.94413685798645, 'test/num_examples': 10000, 'score': 44441.315484046936, 'total_duration': 47655.0097014904, 'accumulated_submission_time': 44441.315484046936, 'accumulated_eval_time': 3193.665872335434, 'accumulated_logging_time': 10.321303367614746}
I0307 15:16:50.466127 139758059628288 logging_writer.py:48] [95581] accumulated_eval_time=3193.67, accumulated_logging_time=10.3213, accumulated_submission_time=44441.3, global_step=95581, preemption_count=0, score=44441.3, test/accuracy=0.2615, test/loss=3.94414, test/num_examples=10000, total_duration=47655, train/accuracy=0.353217, train/loss=3.09991, validation/accuracy=0.33014, validation/loss=3.26718, validation/num_examples=50000
I0307 15:16:58.049889 139758068020992 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.5912760496139526, loss=2.1277036666870117
I0307 15:18:15.950032 139758059628288 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.688673734664917, loss=2.0919666290283203
I0307 15:19:38.677720 139758068020992 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.5397913455963135, loss=1.994563102722168
I0307 15:21:46.816402 139758059628288 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.6915825605392456, loss=2.0144519805908203
I0307 15:22:59.745755 139758068020992 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.7844195365905762, loss=1.9244815111160278
I0307 15:23:53.671592 139758059628288 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.526054859161377, loss=1.9976623058319092
I0307 15:24:48.519909 139758068020992 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.5914340019226074, loss=2.028674602508545
I0307 15:25:20.410282 139912818214080 spec.py:321] Evaluating on the training split.
I0307 15:25:31.340181 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 15:25:53.529193 139912818214080 spec.py:349] Evaluating on the test split.
I0307 15:25:55.251336 139912818214080 submission_runner.py:469] Time since start: 48199.88s, 	Step: 96265, 	{'train/accuracy': 0.3660913407802582, 'train/loss': 3.0898873805999756, 'validation/accuracy': 0.3385999798774719, 'validation/loss': 3.291914224624634, 'validation/num_examples': 50000, 'test/accuracy': 0.26270002126693726, 'test/loss': 3.944162607192993, 'test/num_examples': 10000, 'score': 44951.182299375534, 'total_duration': 48199.88158273697, 'accumulated_submission_time': 44951.182299375534, 'accumulated_eval_time': 3228.5068147182465, 'accumulated_logging_time': 10.416137456893921}
I0307 15:25:55.336960 139758059628288 logging_writer.py:48] [96265] accumulated_eval_time=3228.51, accumulated_logging_time=10.4161, accumulated_submission_time=44951.2, global_step=96265, preemption_count=0, score=44951.2, test/accuracy=0.2627, test/loss=3.94416, test/num_examples=10000, total_duration=48199.9, train/accuracy=0.366091, train/loss=3.08989, validation/accuracy=0.3386, validation/loss=3.29191, validation/num_examples=50000
I0307 15:26:09.409167 139758068020992 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.686591625213623, loss=2.116757869720459
I0307 15:28:31.429344 139758059628288 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.6113884449005127, loss=2.1561973094940186
I0307 15:31:33.666806 139758068020992 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.9501081705093384, loss=1.918631672859192
I0307 15:34:04.385564 139758059628288 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.763068675994873, loss=2.0126893520355225
I0307 15:34:25.544321 139912818214080 spec.py:321] Evaluating on the training split.
I0307 15:34:36.010389 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 15:34:54.172335 139912818214080 spec.py:349] Evaluating on the test split.
I0307 15:34:55.910968 139912818214080 submission_runner.py:469] Time since start: 48740.54s, 	Step: 96612, 	{'train/accuracy': 0.5189133882522583, 'train/loss': 2.064518928527832, 'validation/accuracy': 0.48527997732162476, 'validation/loss': 2.2548553943634033, 'validation/num_examples': 50000, 'test/accuracy': 0.3700000047683716, 'test/loss': 3.0916426181793213, 'test/num_examples': 10000, 'score': 45461.34050989151, 'total_duration': 48740.54130315781, 'accumulated_submission_time': 45461.34050989151, 'accumulated_eval_time': 3258.8734402656555, 'accumulated_logging_time': 10.515653133392334}
I0307 15:34:55.964503 139758068020992 logging_writer.py:48] [96612] accumulated_eval_time=3258.87, accumulated_logging_time=10.5157, accumulated_submission_time=45461.3, global_step=96612, preemption_count=0, score=45461.3, test/accuracy=0.37, test/loss=3.09164, test/num_examples=10000, total_duration=48740.5, train/accuracy=0.518913, train/loss=2.06452, validation/accuracy=0.48528, validation/loss=2.25486, validation/num_examples=50000
I0307 15:37:44.339765 139758059628288 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.7676717042922974, loss=1.9554188251495361
I0307 15:39:34.573459 139758068020992 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.4896996021270752, loss=2.0293357372283936
I0307 15:41:22.368365 139758059628288 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.5377438068389893, loss=2.0481998920440674
I0307 15:43:10.873576 139758068020992 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.6662845611572266, loss=1.9948217868804932
I0307 15:43:25.961759 139912818214080 spec.py:321] Evaluating on the training split.
I0307 15:43:36.253005 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 15:43:57.783353 139912818214080 spec.py:349] Evaluating on the test split.
I0307 15:43:59.498153 139912818214080 submission_runner.py:469] Time since start: 49284.13s, 	Step: 97015, 	{'train/accuracy': 0.35431280732154846, 'train/loss': 3.215914487838745, 'validation/accuracy': 0.31977999210357666, 'validation/loss': 3.4847309589385986, 'validation/num_examples': 50000, 'test/accuracy': 0.22770000994205475, 'test/loss': 4.4796953201293945, 'test/num_examples': 10000, 'score': 45971.2893846035, 'total_duration': 49284.12849187851, 'accumulated_submission_time': 45971.2893846035, 'accumulated_eval_time': 3292.4098057746887, 'accumulated_logging_time': 10.576824188232422}
I0307 15:43:59.564909 139758059628288 logging_writer.py:48] [97015] accumulated_eval_time=3292.41, accumulated_logging_time=10.5768, accumulated_submission_time=45971.3, global_step=97015, preemption_count=0, score=45971.3, test/accuracy=0.2277, test/loss=4.4797, test/num_examples=10000, total_duration=49284.1, train/accuracy=0.354313, train/loss=3.21591, validation/accuracy=0.31978, validation/loss=3.48473, validation/num_examples=50000
I0307 15:48:27.254626 139758068020992 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.5806745290756226, loss=2.007063388824463
I0307 15:51:33.600944 139758059628288 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.7275201082229614, loss=1.9200735092163086
I0307 15:52:30.135559 139912818214080 spec.py:321] Evaluating on the training split.
I0307 15:52:40.431555 139912818214080 spec.py:333] Evaluating on the validation split.
2025-03-07 15:52:42.572250: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 1073741824 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:52:42.618713: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 966367744 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:52:42.623266: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 869731072 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:52:42.627315: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 782758144 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:52:42.634160: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 704482304 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:52:42.750134: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 634034176 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 15:52:42.755930: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 570630912 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
I0307 15:53:01.621751 139912818214080 spec.py:349] Evaluating on the test split.
I0307 15:53:03.293994 139912818214080 submission_runner.py:469] Time since start: 49827.92s, 	Step: 97276, 	{'train/accuracy': 0.45874521136283875, 'train/loss': 2.3809010982513428, 'validation/accuracy': 0.41867998242378235, 'validation/loss': 2.6263821125030518, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.4157767295837402, 'test/num_examples': 10000, 'score': 46481.82843899727, 'total_duration': 49827.92433190346, 'accumulated_submission_time': 46481.82843899727, 'accumulated_eval_time': 3325.568233728409, 'accumulated_logging_time': 10.65055513381958}
I0307 15:53:03.328248 139758068020992 logging_writer.py:48] [97276] accumulated_eval_time=3325.57, accumulated_logging_time=10.6506, accumulated_submission_time=46481.8, global_step=97276, preemption_count=0, score=46481.8, test/accuracy=0.3115, test/loss=3.41578, test/num_examples=10000, total_duration=49827.9, train/accuracy=0.458745, train/loss=2.3809, validation/accuracy=0.41868, validation/loss=2.62638, validation/num_examples=50000
I0307 15:53:14.420211 139758059628288 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.7061244249343872, loss=1.9684371948242188
I0307 15:54:38.658552 139758068020992 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.628376841545105, loss=2.068676233291626
I0307 15:55:59.576112 139758059628288 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.5496541261672974, loss=1.9827373027801514
I0307 15:57:18.522918 139758068020992 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.8093147277832031, loss=2.033681631088257
I0307 15:58:23.592149 139758059628288 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.6322814226150513, loss=2.1053688526153564
I0307 16:00:59.838967 139758068020992 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.6021509170532227, loss=2.0745689868927
I0307 16:01:34.191235 139912818214080 spec.py:321] Evaluating on the training split.
I0307 16:01:43.961344 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 16:02:04.823820 139912818214080 spec.py:349] Evaluating on the test split.
I0307 16:02:06.556367 139912818214080 submission_runner.py:469] Time since start: 50371.19s, 	Step: 97809, 	{'train/accuracy': 0.40796396136283875, 'train/loss': 2.8251566886901855, 'validation/accuracy': 0.38001999258995056, 'validation/loss': 3.025785207748413, 'validation/num_examples': 50000, 'test/accuracy': 0.2787000238895416, 'test/loss': 3.8887994289398193, 'test/num_examples': 10000, 'score': 46992.6064491272, 'total_duration': 50371.18669986725, 'accumulated_submission_time': 46992.6064491272, 'accumulated_eval_time': 3357.9333279132843, 'accumulated_logging_time': 10.716936588287354}
I0307 16:02:06.621477 139758059628288 logging_writer.py:48] [97809] accumulated_eval_time=3357.93, accumulated_logging_time=10.7169, accumulated_submission_time=46992.6, global_step=97809, preemption_count=0, score=46992.6, test/accuracy=0.2787, test/loss=3.8888, test/num_examples=10000, total_duration=50371.2, train/accuracy=0.407964, train/loss=2.82516, validation/accuracy=0.38002, validation/loss=3.02579, validation/num_examples=50000
I0307 16:08:14.848203 139758068020992 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.6493916511535645, loss=2.0177011489868164
I0307 16:10:36.766498 139912818214080 spec.py:321] Evaluating on the training split.
I0307 16:10:46.750152 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 16:11:05.657369 139912818214080 spec.py:349] Evaluating on the test split.
I0307 16:11:07.372792 139912818214080 submission_runner.py:469] Time since start: 50912.00s, 	Step: 97946, 	{'train/accuracy': 0.33814969658851624, 'train/loss': 3.244490146636963, 'validation/accuracy': 0.3164199888706207, 'validation/loss': 3.3999385833740234, 'validation/num_examples': 50000, 'test/accuracy': 0.23410001397132874, 'test/loss': 4.17090368270874, 'test/num_examples': 10000, 'score': 47502.73087024689, 'total_duration': 50912.00312900543, 'accumulated_submission_time': 47502.73087024689, 'accumulated_eval_time': 3388.53959274292, 'accumulated_logging_time': 10.789500713348389}
I0307 16:11:07.391450 139758059628288 logging_writer.py:48] [97946] accumulated_eval_time=3388.54, accumulated_logging_time=10.7895, accumulated_submission_time=47502.7, global_step=97946, preemption_count=0, score=47502.7, test/accuracy=0.2341, test/loss=4.1709, test/num_examples=10000, total_duration=50912, train/accuracy=0.33815, train/loss=3.24449, validation/accuracy=0.31642, validation/loss=3.39994, validation/num_examples=50000
I0307 16:11:56.449090 139758068020992 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.9230799674987793, loss=1.95005202293396
I0307 16:13:44.486114 139758059628288 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.6808478832244873, loss=2.0145206451416016
I0307 16:15:32.609785 139758068020992 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.9485887289047241, loss=1.9914437532424927
I0307 16:17:20.522911 139758059628288 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.857898473739624, loss=2.070786952972412
I0307 16:19:37.472496 139912818214080 spec.py:321] Evaluating on the training split.
I0307 16:19:47.512682 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 16:20:09.592612 139912818214080 spec.py:349] Evaluating on the test split.
I0307 16:20:11.275964 139912818214080 submission_runner.py:469] Time since start: 51455.91s, 	Step: 98369, 	{'train/accuracy': 0.3689413070678711, 'train/loss': 3.0342400074005127, 'validation/accuracy': 0.353659987449646, 'validation/loss': 3.1662850379943848, 'validation/num_examples': 50000, 'test/accuracy': 0.26360002160072327, 'test/loss': 3.998591661453247, 'test/num_examples': 10000, 'score': 48012.76383924484, 'total_duration': 51455.906281232834, 'accumulated_submission_time': 48012.76383924484, 'accumulated_eval_time': 3422.3430075645447, 'accumulated_logging_time': 10.816771268844604}
I0307 16:20:11.374189 139758068020992 logging_writer.py:48] [98369] accumulated_eval_time=3422.34, accumulated_logging_time=10.8168, accumulated_submission_time=48012.8, global_step=98369, preemption_count=0, score=48012.8, test/accuracy=0.2636, test/loss=3.99859, test/num_examples=10000, total_duration=51455.9, train/accuracy=0.368941, train/loss=3.03424, validation/accuracy=0.35366, validation/loss=3.16629, validation/num_examples=50000
I0307 16:22:02.993012 139758059628288 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.5944809913635254, loss=1.9711099863052368
I0307 16:24:42.971919 139758068020992 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.6962631940841675, loss=1.9171524047851562
I0307 16:26:33.943260 139758059628288 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.6213338375091553, loss=1.917284369468689
I0307 16:28:25.360202 139758068020992 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.7702397108078003, loss=2.143214702606201
I0307 16:28:41.663908 139912818214080 spec.py:321] Evaluating on the training split.
I0307 16:28:51.931472 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 16:29:11.190940 139912818214080 spec.py:349] Evaluating on the test split.
I0307 16:29:12.887513 139912818214080 submission_runner.py:469] Time since start: 51997.52s, 	Step: 98716, 	{'train/accuracy': 0.4282326102256775, 'train/loss': 2.578181743621826, 'validation/accuracy': 0.3993400037288666, 'validation/loss': 2.7811942100524902, 'validation/num_examples': 50000, 'test/accuracy': 0.2945000231266022, 'test/loss': 3.5721287727355957, 'test/num_examples': 10000, 'score': 48523.00835061073, 'total_duration': 51997.51784777641, 'accumulated_submission_time': 48523.00835061073, 'accumulated_eval_time': 3453.566575527191, 'accumulated_logging_time': 10.925185918807983}
I0307 16:29:12.923363 139758059628288 logging_writer.py:48] [98716] accumulated_eval_time=3453.57, accumulated_logging_time=10.9252, accumulated_submission_time=48523, global_step=98716, preemption_count=0, score=48523, test/accuracy=0.2945, test/loss=3.57213, test/num_examples=10000, total_duration=51997.5, train/accuracy=0.428233, train/loss=2.57818, validation/accuracy=0.39934, validation/loss=2.78119, validation/num_examples=50000
I0307 16:30:34.968190 139758068020992 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.5701189041137695, loss=1.9620380401611328
I0307 16:32:24.800118 139758059628288 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.713106632232666, loss=2.0145256519317627
I0307 16:37:13.062557 139758068020992 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.807904601097107, loss=2.1488358974456787
I0307 16:37:46.572407 139912818214080 spec.py:321] Evaluating on the training split.
I0307 16:37:56.580034 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 16:38:20.392975 139912818214080 spec.py:349] Evaluating on the test split.
I0307 16:38:22.125534 139912818214080 submission_runner.py:469] Time since start: 52546.76s, 	Step: 99009, 	{'train/accuracy': 0.5184749364852905, 'train/loss': 2.068596363067627, 'validation/accuracy': 0.48495998978614807, 'validation/loss': 2.285196542739868, 'validation/num_examples': 50000, 'test/accuracy': 0.37040001153945923, 'test/loss': 3.048356056213379, 'test/num_examples': 10000, 'score': 49036.61528468132, 'total_duration': 52546.755868434906, 'accumulated_submission_time': 49036.61528468132, 'accumulated_eval_time': 3489.1196777820587, 'accumulated_logging_time': 10.975955247879028}
I0307 16:38:22.144575 139758059628288 logging_writer.py:48] [99009] accumulated_eval_time=3489.12, accumulated_logging_time=10.976, accumulated_submission_time=49036.6, global_step=99009, preemption_count=0, score=49036.6, test/accuracy=0.3704, test/loss=3.04836, test/num_examples=10000, total_duration=52546.8, train/accuracy=0.518475, train/loss=2.0686, validation/accuracy=0.48496, validation/loss=2.2852, validation/num_examples=50000
I0307 16:44:34.254999 139758068020992 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.9315564632415771, loss=2.010188579559326
I0307 16:46:53.692533 139912818214080 spec.py:321] Evaluating on the training split.
I0307 16:47:04.002308 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 16:47:24.698780 139912818214080 spec.py:349] Evaluating on the test split.
I0307 16:47:26.379230 139912818214080 submission_runner.py:469] Time since start: 53091.01s, 	Step: 99134, 	{'train/accuracy': 0.4529057741165161, 'train/loss': 2.449897289276123, 'validation/accuracy': 0.4195599853992462, 'validation/loss': 2.701817512512207, 'validation/num_examples': 50000, 'test/accuracy': 0.3126000165939331, 'test/loss': 3.5325403213500977, 'test/num_examples': 10000, 'score': 49548.14441919327, 'total_duration': 53091.00955462456, 'accumulated_submission_time': 49548.14441919327, 'accumulated_eval_time': 3521.806339740753, 'accumulated_logging_time': 11.002286434173584}
I0307 16:47:26.398174 139758059628288 logging_writer.py:48] [99134] accumulated_eval_time=3521.81, accumulated_logging_time=11.0023, accumulated_submission_time=49548.1, global_step=99134, preemption_count=0, score=49548.1, test/accuracy=0.3126, test/loss=3.53254, test/num_examples=10000, total_duration=53091, train/accuracy=0.452906, train/loss=2.4499, validation/accuracy=0.41956, validation/loss=2.70182, validation/num_examples=50000
I0307 16:51:49.055281 139758068020992 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.585785984992981, loss=1.8393574953079224
I0307 16:55:59.195949 139912818214080 spec.py:321] Evaluating on the training split.
I0307 16:56:09.052582 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 16:56:31.174235 139912818214080 spec.py:349] Evaluating on the test split.
I0307 16:56:32.901148 139912818214080 submission_runner.py:469] Time since start: 53637.53s, 	Step: 99260, 	{'train/accuracy': 0.3771723508834839, 'train/loss': 2.9567601680755615, 'validation/accuracy': 0.35297998785972595, 'validation/loss': 3.1187455654144287, 'validation/num_examples': 50000, 'test/accuracy': 0.27230000495910645, 'test/loss': 3.815753698348999, 'test/num_examples': 10000, 'score': 50060.92329573631, 'total_duration': 53637.53148174286, 'accumulated_submission_time': 50060.92329573631, 'accumulated_eval_time': 3555.511514186859, 'accumulated_logging_time': 11.028517723083496}
I0307 16:56:32.920744 139758059628288 logging_writer.py:48] [99260] accumulated_eval_time=3555.51, accumulated_logging_time=11.0285, accumulated_submission_time=50060.9, global_step=99260, preemption_count=0, score=50060.9, test/accuracy=0.2723, test/loss=3.81575, test/num_examples=10000, total_duration=53637.5, train/accuracy=0.377172, train/loss=2.95676, validation/accuracy=0.35298, validation/loss=3.11875, validation/num_examples=50000
I0307 16:59:06.070391 139758068020992 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.7081433534622192, loss=2.081728935241699
I0307 17:05:05.495494 139912818214080 spec.py:321] Evaluating on the training split.
I0307 17:05:15.531244 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 17:05:35.181892 139912818214080 spec.py:349] Evaluating on the test split.
I0307 17:05:36.904522 139912818214080 submission_runner.py:469] Time since start: 54181.53s, 	Step: 99386, 	{'train/accuracy': 0.433613657951355, 'train/loss': 2.59604811668396, 'validation/accuracy': 0.4080999791622162, 'validation/loss': 2.7744081020355225, 'validation/num_examples': 50000, 'test/accuracy': 0.3216000199317932, 'test/loss': 3.504655599594116, 'test/num_examples': 10000, 'score': 50573.47844362259, 'total_duration': 54181.534838438034, 'accumulated_submission_time': 50573.47844362259, 'accumulated_eval_time': 3586.9204914569855, 'accumulated_logging_time': 11.055777311325073}
I0307 17:05:36.965148 139758059628288 logging_writer.py:48] [99386] accumulated_eval_time=3586.92, accumulated_logging_time=11.0558, accumulated_submission_time=50573.5, global_step=99386, preemption_count=0, score=50573.5, test/accuracy=0.3216, test/loss=3.50466, test/num_examples=10000, total_duration=54181.5, train/accuracy=0.433614, train/loss=2.59605, validation/accuracy=0.4081, validation/loss=2.77441, validation/num_examples=50000
I0307 17:06:15.239940 139758068020992 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.8123576641082764, loss=2.0257842540740967
I0307 17:09:49.646333 139758059628288 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.7217599153518677, loss=1.9876004457473755
I0307 17:13:24.816966 139758068020992 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.74118971824646, loss=2.1681933403015137
I0307 17:14:08.132392 139912818214080 spec.py:321] Evaluating on the training split.
I0307 17:14:17.988507 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 17:14:36.545383 139912818214080 spec.py:349] Evaluating on the test split.
I0307 17:14:38.281781 139912818214080 submission_runner.py:469] Time since start: 54722.91s, 	Step: 99621, 	{'train/accuracy': 0.23301976919174194, 'train/loss': 4.406154155731201, 'validation/accuracy': 0.2148599922657013, 'validation/loss': 4.579807281494141, 'validation/num_examples': 50000, 'test/accuracy': 0.147800013422966, 'test/loss': 5.427378177642822, 'test/num_examples': 10000, 'score': 51084.60216808319, 'total_duration': 54722.91211724281, 'accumulated_submission_time': 51084.60216808319, 'accumulated_eval_time': 3617.06986451149, 'accumulated_logging_time': 11.137469291687012}
I0307 17:14:38.302119 139758059628288 logging_writer.py:48] [99621] accumulated_eval_time=3617.07, accumulated_logging_time=11.1375, accumulated_submission_time=51084.6, global_step=99621, preemption_count=0, score=51084.6, test/accuracy=0.1478, test/loss=5.42738, test/num_examples=10000, total_duration=54722.9, train/accuracy=0.23302, train/loss=4.40615, validation/accuracy=0.21486, validation/loss=4.57981, validation/num_examples=50000
I0307 17:17:08.251317 139758068020992 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.6573598384857178, loss=1.9351210594177246
I0307 17:19:30.233021 139758059628288 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.7034614086151123, loss=1.9084680080413818
I0307 17:22:09.361655 139758068020992 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.775395154953003, loss=2.0135374069213867
I0307 17:23:09.502261 139912818214080 spec.py:321] Evaluating on the training split.
I0307 17:23:19.656282 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 17:23:38.381951 139912818214080 spec.py:349] Evaluating on the test split.
I0307 17:23:40.087131 139912818214080 submission_runner.py:469] Time since start: 55264.72s, 	Step: 99935, 	{'train/accuracy': 0.5286790132522583, 'train/loss': 2.0169661045074463, 'validation/accuracy': 0.48673999309539795, 'validation/loss': 2.246558427810669, 'validation/num_examples': 50000, 'test/accuracy': 0.3815000057220459, 'test/loss': 2.918832540512085, 'test/num_examples': 10000, 'score': 51595.76218628883, 'total_duration': 55264.71746754646, 'accumulated_submission_time': 51595.76218628883, 'accumulated_eval_time': 3647.6547033786774, 'accumulated_logging_time': 11.165179967880249}
I0307 17:23:40.132480 139758059628288 logging_writer.py:48] [99935] accumulated_eval_time=3647.65, accumulated_logging_time=11.1652, accumulated_submission_time=51595.8, global_step=99935, preemption_count=0, score=51595.8, test/accuracy=0.3815, test/loss=2.91883, test/num_examples=10000, total_duration=55264.7, train/accuracy=0.528679, train/loss=2.01697, validation/accuracy=0.48674, validation/loss=2.24656, validation/num_examples=50000
I0307 17:24:59.446653 139758068020992 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.589658260345459, loss=2.1307973861694336
2025-03-07 17:26:52.236298: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:27:10.162843 139758059628288 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.638299584388733, loss=2.094470500946045
I0307 17:28:16.112033 139758068020992 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.5822925567626953, loss=1.953741431236267
I0307 17:29:21.638023 139758059628288 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.7359694242477417, loss=1.9697917699813843
I0307 17:30:27.032386 139758068020992 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.6616052389144897, loss=1.9734632968902588
I0307 17:31:32.790424 139758059628288 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.7660582065582275, loss=1.982830286026001
I0307 17:32:10.534867 139912818214080 spec.py:321] Evaluating on the training split.
I0307 17:32:20.448073 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 17:32:40.298486 139912818214080 spec.py:349] Evaluating on the test split.
I0307 17:32:42.018458 139912818214080 submission_runner.py:469] Time since start: 55806.65s, 	Step: 100558, 	{'train/accuracy': 0.45621412992477417, 'train/loss': 2.3994035720825195, 'validation/accuracy': 0.4229399859905243, 'validation/loss': 2.616429567337036, 'validation/num_examples': 50000, 'test/accuracy': 0.3193000257015228, 'test/loss': 3.3530325889587402, 'test/num_examples': 10000, 'score': 52106.0928940773, 'total_duration': 55806.64876008034, 'accumulated_submission_time': 52106.0928940773, 'accumulated_eval_time': 3679.138229370117, 'accumulated_logging_time': 11.218993902206421}
I0307 17:32:42.068062 139758068020992 logging_writer.py:48] [100558] accumulated_eval_time=3679.14, accumulated_logging_time=11.219, accumulated_submission_time=52106.1, global_step=100558, preemption_count=0, score=52106.1, test/accuracy=0.3193, test/loss=3.35303, test/num_examples=10000, total_duration=55806.6, train/accuracy=0.456214, train/loss=2.3994, validation/accuracy=0.42294, validation/loss=2.61643, validation/num_examples=50000
I0307 17:33:02.907598 139758059628288 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.5660215616226196, loss=1.9794363975524902
I0307 17:34:09.100260 139758068020992 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.669519305229187, loss=2.044675827026367
I0307 17:35:14.299668 139758059628288 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.6850534677505493, loss=1.9519520998001099
I0307 17:36:19.738819 139758068020992 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.6861492395401, loss=2.0297768115997314
I0307 17:37:25.307601 139758059628288 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.7733722925186157, loss=2.0176563262939453
I0307 17:38:30.905131 139758068020992 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.6107208728790283, loss=1.9661102294921875
I0307 17:39:37.068031 139758059628288 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.6283581256866455, loss=1.9744999408721924
I0307 17:40:43.036815 139758068020992 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.7036930322647095, loss=1.898256540298462
2025-03-07 17:40:59.922166: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:41:12.469188 139912818214080 spec.py:321] Evaluating on the training split.
I0307 17:41:22.755676 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 17:41:41.741753 139912818214080 spec.py:349] Evaluating on the test split.
I0307 17:41:43.455654 139912818214080 submission_runner.py:469] Time since start: 56348.09s, 	Step: 101344, 	{'train/accuracy': 0.3787866532802582, 'train/loss': 2.9305360317230225, 'validation/accuracy': 0.3491799831390381, 'validation/loss': 3.1213467121124268, 'validation/num_examples': 50000, 'test/accuracy': 0.2630999982357025, 'test/loss': 3.8277428150177, 'test/num_examples': 10000, 'score': 52616.40606665611, 'total_duration': 56348.085983514786, 'accumulated_submission_time': 52616.40606665611, 'accumulated_eval_time': 3710.124653339386, 'accumulated_logging_time': 11.277005195617676}
I0307 17:41:43.490445 139758059628288 logging_writer.py:48] [101344] accumulated_eval_time=3710.12, accumulated_logging_time=11.277, accumulated_submission_time=52616.4, global_step=101344, preemption_count=0, score=52616.4, test/accuracy=0.2631, test/loss=3.82774, test/num_examples=10000, total_duration=56348.1, train/accuracy=0.378787, train/loss=2.93054, validation/accuracy=0.34918, validation/loss=3.12135, validation/num_examples=50000
I0307 17:42:15.199301 139758068020992 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.6002062559127808, loss=2.0468239784240723
I0307 17:43:20.697518 139758059628288 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.8925423622131348, loss=1.9809608459472656
I0307 17:44:25.882400 139758068020992 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.6422055959701538, loss=1.917067289352417
I0307 17:45:31.321362 139758059628288 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.0794694423675537, loss=2.0662262439727783
I0307 17:46:36.564525 139758068020992 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.7030360698699951, loss=1.9211513996124268
I0307 17:47:41.877909 139758059628288 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.539989709854126, loss=1.9595861434936523
I0307 17:48:47.565024 139758068020992 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.8731410503387451, loss=2.0353918075561523
I0307 17:49:53.135912 139758059628288 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.7205376625061035, loss=1.8459157943725586
I0307 17:50:13.997381 139912818214080 spec.py:321] Evaluating on the training split.
I0307 17:50:24.732180 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 17:50:50.810634 139912818214080 spec.py:349] Evaluating on the test split.
I0307 17:50:52.486012 139912818214080 submission_runner.py:469] Time since start: 56897.12s, 	Step: 102133, 	{'train/accuracy': 0.4890584945678711, 'train/loss': 2.236358642578125, 'validation/accuracy': 0.4620800018310547, 'validation/loss': 2.4164700508117676, 'validation/num_examples': 50000, 'test/accuracy': 0.33980002999305725, 'test/loss': 3.2522990703582764, 'test/num_examples': 10000, 'score': 53126.82693171501, 'total_duration': 56897.11634063721, 'accumulated_submission_time': 53126.82693171501, 'accumulated_eval_time': 3748.613242149353, 'accumulated_logging_time': 11.318917751312256}
I0307 17:50:52.580311 139758068020992 logging_writer.py:48] [102133] accumulated_eval_time=3748.61, accumulated_logging_time=11.3189, accumulated_submission_time=53126.8, global_step=102133, preemption_count=0, score=53126.8, test/accuracy=0.3398, test/loss=3.2523, test/num_examples=10000, total_duration=56897.1, train/accuracy=0.489058, train/loss=2.23636, validation/accuracy=0.46208, validation/loss=2.41647, validation/num_examples=50000
I0307 17:51:43.011202 139758059628288 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.7260501384735107, loss=2.0102458000183105
I0307 17:53:31.325860 139758068020992 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.702318549156189, loss=1.871656060218811
I0307 17:55:22.470730 139758059628288 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.0472519397735596, loss=2.1228671073913574
I0307 17:57:11.961094 139758068020992 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.7224845886230469, loss=2.04892635345459
2025-03-07 17:58:35.125175: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:59:02.562359 139758059628288 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.6573286056518555, loss=1.8704968690872192
I0307 17:59:23.475598 139912818214080 spec.py:321] Evaluating on the training split.
I0307 17:59:33.894883 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 17:59:52.891456 139912818214080 spec.py:349] Evaluating on the test split.
I0307 17:59:54.587599 139912818214080 submission_runner.py:469] Time since start: 57439.22s, 	Step: 102620, 	{'train/accuracy': 0.3781489133834839, 'train/loss': 3.062340497970581, 'validation/accuracy': 0.3501800000667572, 'validation/loss': 3.2510364055633545, 'validation/num_examples': 50000, 'test/accuracy': 0.2632000148296356, 'test/loss': 4.07974910736084, 'test/num_examples': 10000, 'score': 53637.658563137054, 'total_duration': 57439.217901945114, 'accumulated_submission_time': 53637.658563137054, 'accumulated_eval_time': 3779.7251873016357, 'accumulated_logging_time': 11.429038524627686}
I0307 17:59:54.623267 139758068020992 logging_writer.py:48] [102620] accumulated_eval_time=3779.73, accumulated_logging_time=11.429, accumulated_submission_time=53637.7, global_step=102620, preemption_count=0, score=53637.7, test/accuracy=0.2632, test/loss=4.07975, test/num_examples=10000, total_duration=57439.2, train/accuracy=0.378149, train/loss=3.06234, validation/accuracy=0.35018, validation/loss=3.25104, validation/num_examples=50000
I0307 18:01:12.025831 139758059628288 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.5958025455474854, loss=1.8777461051940918
I0307 18:04:22.868860 139758068020992 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.803958535194397, loss=2.004500389099121
I0307 18:08:28.868747 139912818214080 spec.py:321] Evaluating on the training split.
I0307 18:08:39.102802 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 18:09:01.463630 139912818214080 spec.py:349] Evaluating on the test split.
I0307 18:09:03.169165 139912818214080 submission_runner.py:469] Time since start: 57987.80s, 	Step: 102859, 	{'train/accuracy': 0.49683114886283875, 'train/loss': 2.1844348907470703, 'validation/accuracy': 0.4382999837398529, 'validation/loss': 2.5294387340545654, 'validation/num_examples': 50000, 'test/accuracy': 0.3286000192165375, 'test/loss': 3.3371737003326416, 'test/num_examples': 10000, 'score': 54151.823229551315, 'total_duration': 57987.799485206604, 'accumulated_submission_time': 54151.823229551315, 'accumulated_eval_time': 3814.025567293167, 'accumulated_logging_time': 11.522158145904541}
I0307 18:09:03.188763 139758059628288 logging_writer.py:48] [102859] accumulated_eval_time=3814.03, accumulated_logging_time=11.5222, accumulated_submission_time=54151.8, global_step=102859, preemption_count=0, score=54151.8, test/accuracy=0.3286, test/loss=3.33717, test/num_examples=10000, total_duration=57987.8, train/accuracy=0.496831, train/loss=2.18443, validation/accuracy=0.4383, validation/loss=2.52944, validation/num_examples=50000
I0307 18:11:39.726597 139758068020992 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.7030428647994995, loss=1.8652093410491943
I0307 18:17:27.742881 139758059628288 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.730734944343567, loss=1.9424570798873901
I0307 18:17:34.316442 139912818214080 spec.py:321] Evaluating on the training split.
I0307 18:17:43.976709 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 18:18:02.565067 139912818214080 spec.py:349] Evaluating on the test split.
I0307 18:18:04.313163 139912818214080 submission_runner.py:469] Time since start: 58528.94s, 	Step: 103004, 	{'train/accuracy': 0.5252710580825806, 'train/loss': 2.0207221508026123, 'validation/accuracy': 0.48225998878479004, 'validation/loss': 2.2848520278930664, 'validation/num_examples': 50000, 'test/accuracy': 0.3782000243663788, 'test/loss': 3.0130953788757324, 'test/num_examples': 10000, 'score': 54662.929846286774, 'total_duration': 58528.94349980354, 'accumulated_submission_time': 54662.929846286774, 'accumulated_eval_time': 3844.022253513336, 'accumulated_logging_time': 11.548789978027344}
I0307 18:18:04.333429 139758068020992 logging_writer.py:48] [103004] accumulated_eval_time=3844.02, accumulated_logging_time=11.5488, accumulated_submission_time=54662.9, global_step=103004, preemption_count=0, score=54662.9, test/accuracy=0.3782, test/loss=3.0131, test/num_examples=10000, total_duration=58528.9, train/accuracy=0.525271, train/loss=2.02072, validation/accuracy=0.48226, validation/loss=2.28485, validation/num_examples=50000
I0307 18:21:11.062933 139758059628288 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.5480455160140991, loss=1.9191796779632568
I0307 18:24:45.227903 139758068020992 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.6657778024673462, loss=2.0827438831329346
I0307 18:26:36.088043 139912818214080 spec.py:321] Evaluating on the training split.
I0307 18:26:46.558396 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 18:27:06.860514 139912818214080 spec.py:349] Evaluating on the test split.
I0307 18:27:08.599534 139912818214080 submission_runner.py:469] Time since start: 59073.23s, 	Step: 103253, 	{'train/accuracy': 0.4022839665412903, 'train/loss': 2.8692519664764404, 'validation/accuracy': 0.3688800036907196, 'validation/loss': 3.0915398597717285, 'validation/num_examples': 50000, 'test/accuracy': 0.2703000009059906, 'test/loss': 3.864097833633423, 'test/num_examples': 10000, 'score': 55174.65123987198, 'total_duration': 59073.22983765602, 'accumulated_submission_time': 55174.65123987198, 'accumulated_eval_time': 3876.5336968898773, 'accumulated_logging_time': 11.576876163482666}
I0307 18:27:08.621373 139758059628288 logging_writer.py:48] [103253] accumulated_eval_time=3876.53, accumulated_logging_time=11.5769, accumulated_submission_time=55174.7, global_step=103253, preemption_count=0, score=55174.7, test/accuracy=0.2703, test/loss=3.8641, test/num_examples=10000, total_duration=59073.2, train/accuracy=0.402284, train/loss=2.86925, validation/accuracy=0.36888, validation/loss=3.09154, validation/num_examples=50000
I0307 18:28:31.102002 139758068020992 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.7678523063659668, loss=1.9713956117630005
I0307 18:32:04.534243 139758059628288 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.9050312042236328, loss=1.9930179119110107
I0307 18:34:26.581676 139758068020992 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.7077118158340454, loss=2.019834280014038
I0307 18:35:39.541100 139912818214080 spec.py:321] Evaluating on the training split.
I0307 18:35:49.915870 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 18:36:07.551769 139912818214080 spec.py:349] Evaluating on the test split.
I0307 18:36:09.263467 139912818214080 submission_runner.py:469] Time since start: 59613.89s, 	Step: 103567, 	{'train/accuracy': 0.43405213952064514, 'train/loss': 2.575512409210205, 'validation/accuracy': 0.4001599848270416, 'validation/loss': 2.821866273880005, 'validation/num_examples': 50000, 'test/accuracy': 0.313400000333786, 'test/loss': 3.5424423217773438, 'test/num_examples': 10000, 'score': 55685.53236722946, 'total_duration': 59613.893795251846, 'accumulated_submission_time': 55685.53236722946, 'accumulated_eval_time': 3906.2560238838196, 'accumulated_logging_time': 11.606302976608276}
I0307 18:36:09.324737 139758059628288 logging_writer.py:48] [103567] accumulated_eval_time=3906.26, accumulated_logging_time=11.6063, accumulated_submission_time=55685.5, global_step=103567, preemption_count=0, score=55685.5, test/accuracy=0.3134, test/loss=3.54244, test/num_examples=10000, total_duration=59613.9, train/accuracy=0.434052, train/loss=2.57551, validation/accuracy=0.40016, validation/loss=2.82187, validation/num_examples=50000
I0307 18:36:35.920609 139758068020992 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.5986801385879517, loss=1.9501862525939941
I0307 18:38:26.978099 139758059628288 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.606012225151062, loss=1.9016966819763184
I0307 18:40:15.431000 139758068020992 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.7755687236785889, loss=2.096228837966919
I0307 18:42:06.223012 139758059628288 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.6413304805755615, loss=1.8711262941360474
I0307 18:43:55.435995 139758068020992 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.5947996377944946, loss=1.9348335266113281
I0307 18:44:40.041669 139912818214080 spec.py:321] Evaluating on the training split.
I0307 18:44:50.652194 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 18:45:10.506803 139912818214080 spec.py:349] Evaluating on the test split.
I0307 18:45:12.193630 139912818214080 submission_runner.py:469] Time since start: 60156.82s, 	Step: 104042, 	{'train/accuracy': 0.402363657951355, 'train/loss': 2.842728614807129, 'validation/accuracy': 0.37845999002456665, 'validation/loss': 3.0030181407928467, 'validation/num_examples': 50000, 'test/accuracy': 0.2802000045776367, 'test/loss': 3.836970806121826, 'test/num_examples': 10000, 'score': 56196.19255423546, 'total_duration': 60156.82395243645, 'accumulated_submission_time': 56196.19255423546, 'accumulated_eval_time': 3938.407937526703, 'accumulated_logging_time': 11.675800800323486}
I0307 18:45:12.254816 139758059628288 logging_writer.py:48] [104042] accumulated_eval_time=3938.41, accumulated_logging_time=11.6758, accumulated_submission_time=56196.2, global_step=104042, preemption_count=0, score=56196.2, test/accuracy=0.2802, test/loss=3.83697, test/num_examples=10000, total_duration=60156.8, train/accuracy=0.402364, train/loss=2.84273, validation/accuracy=0.37846, validation/loss=3.00302, validation/num_examples=50000
I0307 18:46:05.445875 139758068020992 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.8080905675888062, loss=2.013462781906128
I0307 18:52:54.080204 139758059628288 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.6319661140441895, loss=1.963110327720642
I0307 18:53:43.847581 139912818214080 spec.py:321] Evaluating on the training split.
I0307 18:53:53.670991 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 18:54:15.154989 139912818214080 spec.py:349] Evaluating on the test split.
I0307 18:54:16.875183 139912818214080 submission_runner.py:469] Time since start: 60701.51s, 	Step: 104213, 	{'train/accuracy': 0.49565526843070984, 'train/loss': 2.2708935737609863, 'validation/accuracy': 0.4642599821090698, 'validation/loss': 2.4458322525024414, 'validation/num_examples': 50000, 'test/accuracy': 0.3509000241756439, 'test/loss': 3.3124797344207764, 'test/num_examples': 10000, 'score': 56707.75966858864, 'total_duration': 60701.50550246239, 'accumulated_submission_time': 56707.75966858864, 'accumulated_eval_time': 3971.4354889392853, 'accumulated_logging_time': 11.74555253982544}
I0307 18:54:16.898684 139758068020992 logging_writer.py:48] [104213] accumulated_eval_time=3971.44, accumulated_logging_time=11.7456, accumulated_submission_time=56707.8, global_step=104213, preemption_count=0, score=56707.8, test/accuracy=0.3509, test/loss=3.31248, test/num_examples=10000, total_duration=60701.5, train/accuracy=0.495655, train/loss=2.27089, validation/accuracy=0.46426, validation/loss=2.44583, validation/num_examples=50000
I0307 19:00:09.311284 139758059628288 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.6691619157791138, loss=1.9538699388504028
I0307 19:02:50.064542 139912818214080 spec.py:321] Evaluating on the training split.
I0307 19:03:00.007848 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 19:03:20.502025 139912818214080 spec.py:349] Evaluating on the test split.
I0307 19:03:22.210824 139912818214080 submission_runner.py:469] Time since start: 61246.84s, 	Step: 104339, 	{'train/accuracy': 0.47604432702064514, 'train/loss': 2.316443681716919, 'validation/accuracy': 0.4379599988460541, 'validation/loss': 2.608400821685791, 'validation/num_examples': 50000, 'test/accuracy': 0.3419000208377838, 'test/loss': 3.3561463356018066, 'test/num_examples': 10000, 'score': 57220.905958652496, 'total_duration': 61246.84115743637, 'accumulated_submission_time': 57220.905958652496, 'accumulated_eval_time': 4003.5817382335663, 'accumulated_logging_time': 11.776703357696533}
I0307 19:03:22.231487 139758068020992 logging_writer.py:48] [104339] accumulated_eval_time=4003.58, accumulated_logging_time=11.7767, accumulated_submission_time=57220.9, global_step=104339, preemption_count=0, score=57220.9, test/accuracy=0.3419, test/loss=3.35615, test/num_examples=10000, total_duration=61246.8, train/accuracy=0.476044, train/loss=2.31644, validation/accuracy=0.43796, validation/loss=2.6084, validation/num_examples=50000
I0307 19:07:24.723378 139758059628288 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.841612458229065, loss=1.9350117444992065
I0307 19:11:52.640203 139912818214080 spec.py:321] Evaluating on the training split.
I0307 19:12:02.723644 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 19:12:24.482822 139912818214080 spec.py:349] Evaluating on the test split.
I0307 19:12:26.189005 139912818214080 submission_runner.py:469] Time since start: 61790.82s, 	Step: 104464, 	{'train/accuracy': 0.45184949040412903, 'train/loss': 2.413158655166626, 'validation/accuracy': 0.4070200026035309, 'validation/loss': 2.7595162391662598, 'validation/num_examples': 50000, 'test/accuracy': 0.3068000078201294, 'test/loss': 3.546330213546753, 'test/num_examples': 10000, 'score': 57731.295924663544, 'total_duration': 61790.819328308105, 'accumulated_submission_time': 57731.295924663544, 'accumulated_eval_time': 4037.1305060386658, 'accumulated_logging_time': 11.80481219291687}
I0307 19:12:26.209343 139758068020992 logging_writer.py:48] [104464] accumulated_eval_time=4037.13, accumulated_logging_time=11.8048, accumulated_submission_time=57731.3, global_step=104464, preemption_count=0, score=57731.3, test/accuracy=0.3068, test/loss=3.54633, test/num_examples=10000, total_duration=61790.8, train/accuracy=0.451849, train/loss=2.41316, validation/accuracy=0.40702, validation/loss=2.75952, validation/num_examples=50000
I0307 19:14:40.562425 139758059628288 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.7813564538955688, loss=2.0327794551849365
I0307 19:20:56.622691 139912818214080 spec.py:321] Evaluating on the training split.
I0307 19:21:06.070379 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 19:21:26.664948 139912818214080 spec.py:349] Evaluating on the test split.
I0307 19:21:28.381722 139912818214080 submission_runner.py:469] Time since start: 62333.01s, 	Step: 104589, 	{'train/accuracy': 0.45436063408851624, 'train/loss': 2.4469242095947266, 'validation/accuracy': 0.4161199927330017, 'validation/loss': 2.7373082637786865, 'validation/num_examples': 50000, 'test/accuracy': 0.30470001697540283, 'test/loss': 3.6589841842651367, 'test/num_examples': 10000, 'score': 58241.68999147415, 'total_duration': 62333.01205277443, 'accumulated_submission_time': 58241.68999147415, 'accumulated_eval_time': 4068.8895003795624, 'accumulated_logging_time': 11.832237243652344}
I0307 19:21:28.402636 139758068020992 logging_writer.py:48] [104589] accumulated_eval_time=4068.89, accumulated_logging_time=11.8322, accumulated_submission_time=58241.7, global_step=104589, preemption_count=0, score=58241.7, test/accuracy=0.3047, test/loss=3.65898, test/num_examples=10000, total_duration=62333, train/accuracy=0.454361, train/loss=2.44692, validation/accuracy=0.41612, validation/loss=2.73731, validation/num_examples=50000
I0307 19:21:55.986814 139758059628288 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.6147897243499756, loss=2.007206916809082
I0307 19:27:49.223603 139758068020992 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.76908540725708, loss=1.9353013038635254
I0307 19:29:59.589324 139912818214080 spec.py:321] Evaluating on the training split.
I0307 19:30:09.926275 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 19:30:29.845919 139912818214080 spec.py:349] Evaluating on the test split.
I0307 19:30:31.568152 139912818214080 submission_runner.py:469] Time since start: 62876.20s, 	Step: 104767, 	{'train/accuracy': 0.29326769709587097, 'train/loss': 3.776369571685791, 'validation/accuracy': 0.26848000288009644, 'validation/loss': 3.9485995769500732, 'validation/num_examples': 50000, 'test/accuracy': 0.1924000084400177, 'test/loss': 4.802672863006592, 'test/num_examples': 10000, 'score': 58752.85178112984, 'total_duration': 62876.198471069336, 'accumulated_submission_time': 58752.85178112984, 'accumulated_eval_time': 4100.86829161644, 'accumulated_logging_time': 11.860480546951294}
I0307 19:30:31.596172 139758059628288 logging_writer.py:48] [104767] accumulated_eval_time=4100.87, accumulated_logging_time=11.8605, accumulated_submission_time=58752.9, global_step=104767, preemption_count=0, score=58752.9, test/accuracy=0.1924, test/loss=4.80267, test/num_examples=10000, total_duration=62876.2, train/accuracy=0.293268, train/loss=3.77637, validation/accuracy=0.26848, validation/loss=3.9486, validation/num_examples=50000
I0307 19:31:23.996289 139758068020992 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.7162503004074097, loss=1.8644561767578125
I0307 19:35:08.224252 139758059628288 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.6510803699493408, loss=1.990774154663086
I0307 19:38:48.150151 139758068020992 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.814785361289978, loss=1.9605413675308228
I0307 19:39:03.428103 139912818214080 spec.py:321] Evaluating on the training split.
I0307 19:39:13.848335 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 19:39:32.978270 139912818214080 spec.py:349] Evaluating on the test split.
I0307 19:39:34.697199 139912818214080 submission_runner.py:469] Time since start: 63419.33s, 	Step: 105008, 	{'train/accuracy': 0.4742107689380646, 'train/loss': 2.3242995738983154, 'validation/accuracy': 0.44703999161720276, 'validation/loss': 2.5240776538848877, 'validation/num_examples': 50000, 'test/accuracy': 0.3303000032901764, 'test/loss': 3.39839243888855, 'test/num_examples': 10000, 'score': 59264.650086164474, 'total_duration': 63419.32753610611, 'accumulated_submission_time': 59264.650086164474, 'accumulated_eval_time': 4132.137366771698, 'accumulated_logging_time': 11.898420095443726}
I0307 19:39:34.718291 139758059628288 logging_writer.py:48] [105008] accumulated_eval_time=4132.14, accumulated_logging_time=11.8984, accumulated_submission_time=59264.7, global_step=105008, preemption_count=0, score=59264.7, test/accuracy=0.3303, test/loss=3.39839, test/num_examples=10000, total_duration=63419.3, train/accuracy=0.474211, train/loss=2.3243, validation/accuracy=0.44704, validation/loss=2.52408, validation/num_examples=50000
I0307 19:42:32.570214 139758068020992 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.5547077655792236, loss=1.8082066774368286
I0307 19:46:06.933420 139758059628288 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.6852811574935913, loss=1.9886634349822998
I0307 19:48:06.355881 139912818214080 spec.py:321] Evaluating on the training split.
I0307 19:48:16.514140 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 19:48:36.192080 139912818214080 spec.py:349] Evaluating on the test split.
I0307 19:48:37.888537 139912818214080 submission_runner.py:469] Time since start: 63962.52s, 	Step: 105257, 	{'train/accuracy': 0.4795519709587097, 'train/loss': 2.3220150470733643, 'validation/accuracy': 0.4465799927711487, 'validation/loss': 2.5077991485595703, 'validation/num_examples': 50000, 'test/accuracy': 0.34950003027915955, 'test/loss': 3.1804964542388916, 'test/num_examples': 10000, 'score': 59776.257386446, 'total_duration': 63962.51886796951, 'accumulated_submission_time': 59776.257386446, 'accumulated_eval_time': 4163.669984817505, 'accumulated_logging_time': 11.9269540309906}
I0307 19:48:37.925041 139758068020992 logging_writer.py:48] [105257] accumulated_eval_time=4163.67, accumulated_logging_time=11.927, accumulated_submission_time=59776.3, global_step=105257, preemption_count=0, score=59776.3, test/accuracy=0.3495, test/loss=3.1805, test/num_examples=10000, total_duration=63962.5, train/accuracy=0.479552, train/loss=2.32202, validation/accuracy=0.44658, validation/loss=2.5078, validation/num_examples=50000
I0307 19:49:50.111032 139758059628288 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.6460973024368286, loss=1.8948012590408325
I0307 19:53:21.822719 139758068020992 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.7853481769561768, loss=2.0626115798950195
I0307 19:56:53.381449 139758059628288 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.9674640893936157, loss=2.051201820373535
I0307 19:57:08.370284 139912818214080 spec.py:321] Evaluating on the training split.
I0307 19:57:18.626310 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 19:57:39.263104 139912818214080 spec.py:349] Evaluating on the test split.
I0307 19:57:40.958962 139912818214080 submission_runner.py:469] Time since start: 64505.59s, 	Step: 105508, 	{'train/accuracy': 0.30115991830825806, 'train/loss': 3.639763593673706, 'validation/accuracy': 0.2847200036048889, 'validation/loss': 3.7651751041412354, 'validation/num_examples': 50000, 'test/accuracy': 0.1980000138282776, 'test/loss': 4.674007415771484, 'test/num_examples': 10000, 'score': 60286.67100572586, 'total_duration': 64505.58929228783, 'accumulated_submission_time': 60286.67100572586, 'accumulated_eval_time': 4196.258621931076, 'accumulated_logging_time': 11.97074007987976}
I0307 19:57:41.006540 139758068020992 logging_writer.py:48] [105508] accumulated_eval_time=4196.26, accumulated_logging_time=11.9707, accumulated_submission_time=60286.7, global_step=105508, preemption_count=0, score=60286.7, test/accuracy=0.198, test/loss=4.67401, test/num_examples=10000, total_duration=64505.6, train/accuracy=0.30116, train/loss=3.63976, validation/accuracy=0.28472, validation/loss=3.76518, validation/num_examples=50000
I0307 20:00:36.096043 139758059628288 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.811890959739685, loss=2.1135449409484863
I0307 20:04:08.369734 139758068020992 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.830626368522644, loss=1.9205180406570435
I0307 20:06:11.593008 139912818214080 spec.py:321] Evaluating on the training split.
I0307 20:06:21.568203 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 20:06:41.833576 139912818214080 spec.py:349] Evaluating on the test split.
I0307 20:06:43.551744 139912818214080 submission_runner.py:469] Time since start: 65048.18s, 	Step: 105759, 	{'train/accuracy': 0.4709223508834839, 'train/loss': 2.3817429542541504, 'validation/accuracy': 0.4405999779701233, 'validation/loss': 2.5922012329101562, 'validation/num_examples': 50000, 'test/accuracy': 0.34310001134872437, 'test/loss': 3.3202974796295166, 'test/num_examples': 10000, 'score': 60797.19760775566, 'total_duration': 65048.18206739426, 'accumulated_submission_time': 60797.19760775566, 'accumulated_eval_time': 4228.21732378006, 'accumulated_logging_time': 12.05405592918396}
I0307 20:06:43.590617 139758059628288 logging_writer.py:48] [105759] accumulated_eval_time=4228.22, accumulated_logging_time=12.0541, accumulated_submission_time=60797.2, global_step=105759, preemption_count=0, score=60797.2, test/accuracy=0.3431, test/loss=3.3203, test/num_examples=10000, total_duration=65048.2, train/accuracy=0.470922, train/loss=2.38174, validation/accuracy=0.4406, validation/loss=2.5922, validation/num_examples=50000
I0307 20:07:52.089027 139758068020992 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.670668363571167, loss=1.930647611618042
I0307 20:11:22.991833 139758059628288 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.8392232656478882, loss=2.030244827270508
I0307 20:14:22.020075 139758068020992 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.7869118452072144, loss=1.8672963380813599
I0307 20:15:14.011982 139912818214080 spec.py:321] Evaluating on the training split.
I0307 20:15:23.837466 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 20:15:44.333441 139912818214080 spec.py:349] Evaluating on the test split.
I0307 20:15:46.001097 139912818214080 submission_runner.py:469] Time since start: 65590.63s, 	Step: 106037, 	{'train/accuracy': 0.4760642349720001, 'train/loss': 2.329906702041626, 'validation/accuracy': 0.424919992685318, 'validation/loss': 2.6766209602355957, 'validation/num_examples': 50000, 'test/accuracy': 0.3246000111103058, 'test/loss': 3.507951259613037, 'test/num_examples': 10000, 'score': 61307.57697224617, 'total_duration': 65590.63140892982, 'accumulated_submission_time': 61307.57697224617, 'accumulated_eval_time': 4260.206381559372, 'accumulated_logging_time': 12.109410047531128}
I0307 20:15:46.100004 139758059628288 logging_writer.py:48] [106037] accumulated_eval_time=4260.21, accumulated_logging_time=12.1094, accumulated_submission_time=61307.6, global_step=106037, preemption_count=0, score=61307.6, test/accuracy=0.3246, test/loss=3.50795, test/num_examples=10000, total_duration=65590.6, train/accuracy=0.476064, train/loss=2.32991, validation/accuracy=0.42492, validation/loss=2.67662, validation/num_examples=50000
I0307 20:17:03.625082 139758068020992 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.8533934354782104, loss=2.0499274730682373
I0307 20:19:27.251161 139758059628288 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.848387598991394, loss=2.006779670715332
I0307 20:21:50.361431 139758068020992 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.7918304204940796, loss=1.825384259223938
I0307 20:24:14.837661 139758059628288 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.7557389736175537, loss=2.0334651470184326
I0307 20:24:16.261979 139912818214080 spec.py:321] Evaluating on the training split.
I0307 20:24:26.368701 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 20:24:46.512643 139912818214080 spec.py:349] Evaluating on the test split.
I0307 20:24:48.196565 139912818214080 submission_runner.py:469] Time since start: 66132.83s, 	Step: 106402, 	{'train/accuracy': 0.4287906587123871, 'train/loss': 2.6931350231170654, 'validation/accuracy': 0.40595999360084534, 'validation/loss': 2.883967399597168, 'validation/num_examples': 50000, 'test/accuracy': 0.29520002007484436, 'test/loss': 3.810413122177124, 'test/num_examples': 10000, 'score': 61817.69603538513, 'total_duration': 66132.82690024376, 'accumulated_submission_time': 61817.69603538513, 'accumulated_eval_time': 4292.140942335129, 'accumulated_logging_time': 12.21591854095459}
I0307 20:24:48.238419 139758068020992 logging_writer.py:48] [106402] accumulated_eval_time=4292.14, accumulated_logging_time=12.2159, accumulated_submission_time=61817.7, global_step=106402, preemption_count=0, score=61817.7, test/accuracy=0.2952, test/loss=3.81041, test/num_examples=10000, total_duration=66132.8, train/accuracy=0.428791, train/loss=2.69314, validation/accuracy=0.40596, validation/loss=2.88397, validation/num_examples=50000
I0307 20:26:55.414079 139758059628288 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.113213539123535, loss=2.064486265182495
I0307 20:29:19.070650 139758068020992 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.6919455528259277, loss=1.9651631116867065
I0307 20:31:42.756055 139758059628288 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.9418208599090576, loss=1.9148883819580078
I0307 20:33:18.931271 139912818214080 spec.py:321] Evaluating on the training split.
I0307 20:33:29.110780 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 20:33:47.866283 139912818214080 spec.py:349] Evaluating on the test split.
I0307 20:33:49.573437 139912818214080 submission_runner.py:469] Time since start: 66674.20s, 	Step: 106768, 	{'train/accuracy': 0.4228515625, 'train/loss': 2.7113466262817383, 'validation/accuracy': 0.3920799791812897, 'validation/loss': 2.909982204437256, 'validation/num_examples': 50000, 'test/accuracy': 0.30000001192092896, 'test/loss': 3.680807590484619, 'test/num_examples': 10000, 'score': 62328.346529483795, 'total_duration': 66674.2037396431, 'accumulated_submission_time': 62328.346529483795, 'accumulated_eval_time': 4322.783039569855, 'accumulated_logging_time': 12.264996528625488}
I0307 20:33:49.611219 139758068020992 logging_writer.py:48] [106768] accumulated_eval_time=4322.78, accumulated_logging_time=12.265, accumulated_submission_time=62328.3, global_step=106768, preemption_count=0, score=62328.3, test/accuracy=0.3, test/loss=3.68081, test/num_examples=10000, total_duration=66674.2, train/accuracy=0.422852, train/loss=2.71135, validation/accuracy=0.39208, validation/loss=2.90998, validation/num_examples=50000
I0307 20:34:23.653692 139758059628288 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.2697818279266357, loss=2.175447463989258
I0307 20:36:46.611768 139758068020992 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.8562899827957153, loss=2.0031003952026367
I0307 20:39:10.442535 139758059628288 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.7214267253875732, loss=2.043792247772217
I0307 20:41:34.519409 139758068020992 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.8919355869293213, loss=2.080538034439087
I0307 20:42:20.518400 139912818214080 spec.py:321] Evaluating on the training split.
I0307 20:42:30.929367 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 20:42:49.326332 139912818214080 spec.py:349] Evaluating on the test split.
I0307 20:42:51.020087 139912818214080 submission_runner.py:469] Time since start: 67215.65s, 	Step: 107133, 	{'train/accuracy': 0.4027024805545807, 'train/loss': 2.8465893268585205, 'validation/accuracy': 0.3800399899482727, 'validation/loss': 3.021078586578369, 'validation/num_examples': 50000, 'test/accuracy': 0.2847000062465668, 'test/loss': 3.833341598510742, 'test/num_examples': 10000, 'score': 62839.21178007126, 'total_duration': 67215.6504266262, 'accumulated_submission_time': 62839.21178007126, 'accumulated_eval_time': 4353.284695148468, 'accumulated_logging_time': 12.310251951217651}
I0307 20:42:51.061189 139758059628288 logging_writer.py:48] [107133] accumulated_eval_time=4353.28, accumulated_logging_time=12.3103, accumulated_submission_time=62839.2, global_step=107133, preemption_count=0, score=62839.2, test/accuracy=0.2847, test/loss=3.83334, test/num_examples=10000, total_duration=67215.7, train/accuracy=0.402702, train/loss=2.84659, validation/accuracy=0.38004, validation/loss=3.02108, validation/num_examples=50000
I0307 20:44:14.184684 139758068020992 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.9230226278305054, loss=1.9531275033950806
I0307 20:46:03.253929 139758059628288 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.6665937900543213, loss=2.023681640625
I0307 20:47:53.726302 139758068020992 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.649510383605957, loss=1.8589645624160767
I0307 20:49:43.172698 139758059628288 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.9282881021499634, loss=2.0271527767181396
I0307 20:51:21.365056 139912818214080 spec.py:321] Evaluating on the training split.
I0307 20:51:31.821689 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 20:51:50.475867 139912818214080 spec.py:349] Evaluating on the test split.
I0307 20:51:52.166523 139912818214080 submission_runner.py:469] Time since start: 67756.80s, 	Step: 107590, 	{'train/accuracy': 0.5300940871238708, 'train/loss': 1.9932917356491089, 'validation/accuracy': 0.4852199852466583, 'validation/loss': 2.2798359394073486, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 3.096484422683716, 'test/num_examples': 10000, 'score': 63349.440811634064, 'total_duration': 67756.79685592651, 'accumulated_submission_time': 63349.440811634064, 'accumulated_eval_time': 4384.086124897003, 'accumulated_logging_time': 12.380398273468018}
I0307 20:51:52.242600 139758068020992 logging_writer.py:48] [107590] accumulated_eval_time=4384.09, accumulated_logging_time=12.3804, accumulated_submission_time=63349.4, global_step=107590, preemption_count=0, score=63349.4, test/accuracy=0.3679, test/loss=3.09648, test/num_examples=10000, total_duration=67756.8, train/accuracy=0.530094, train/loss=1.99329, validation/accuracy=0.48522, validation/loss=2.27984, validation/num_examples=50000
I0307 20:51:57.475832 139758059628288 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.7736027240753174, loss=1.8734328746795654
I0307 20:53:43.405449 139758068020992 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.9528555870056152, loss=1.9606162309646606
I0307 20:55:28.956099 139758059628288 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.7401283979415894, loss=1.874241590499878
I0307 20:57:14.383432 139758068020992 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.760847806930542, loss=2.04607892036438
I0307 20:59:00.051804 139758059628288 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.9054484367370605, loss=1.9485242366790771
I0307 21:00:22.824385 139912818214080 spec.py:321] Evaluating on the training split.
I0307 21:00:32.879422 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 21:00:52.610176 139912818214080 spec.py:349] Evaluating on the test split.
I0307 21:00:54.280778 139912818214080 submission_runner.py:469] Time since start: 68298.91s, 	Step: 108079, 	{'train/accuracy': 0.31851881742477417, 'train/loss': 3.447589159011841, 'validation/accuracy': 0.2963799834251404, 'validation/loss': 3.582941770553589, 'validation/num_examples': 50000, 'test/accuracy': 0.21620000898838043, 'test/loss': 4.323112487792969, 'test/num_examples': 10000, 'score': 63858.88493680954, 'total_duration': 68298.9111161232, 'accumulated_submission_time': 63858.88493680954, 'accumulated_eval_time': 4415.542486667633, 'accumulated_logging_time': 13.546090126037598}
I0307 21:00:54.326392 139758068020992 logging_writer.py:48] [108079] accumulated_eval_time=4415.54, accumulated_logging_time=13.5461, accumulated_submission_time=63858.9, global_step=108079, preemption_count=0, score=63858.9, test/accuracy=0.2162, test/loss=4.32311, test/num_examples=10000, total_duration=68298.9, train/accuracy=0.318519, train/loss=3.44759, validation/accuracy=0.29638, validation/loss=3.58294, validation/num_examples=50000
I0307 21:01:05.535322 139758059628288 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.676142692565918, loss=1.9498426914215088
I0307 21:02:55.699984 139758068020992 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.8646587133407593, loss=1.959574580192566
I0307 21:04:41.995374 139758059628288 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.744086503982544, loss=1.8676246404647827
I0307 21:06:28.620355 139758068020992 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.9276434183120728, loss=1.9738717079162598
I0307 21:08:15.005551 139758059628288 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.7774583101272583, loss=1.9192626476287842
I0307 21:09:25.272243 139912818214080 spec.py:321] Evaluating on the training split.
I0307 21:09:34.938835 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 21:09:52.587903 139912818214080 spec.py:349] Evaluating on the test split.
I0307 21:09:54.301842 139912818214080 submission_runner.py:469] Time since start: 68838.93s, 	Step: 108567, 	{'train/accuracy': 0.5208466053009033, 'train/loss': 2.0815060138702393, 'validation/accuracy': 0.4865799844264984, 'validation/loss': 2.298184871673584, 'validation/num_examples': 50000, 'test/accuracy': 0.3669000267982483, 'test/loss': 3.120516777038574, 'test/num_examples': 10000, 'score': 64369.774394750595, 'total_duration': 68838.93217086792, 'accumulated_submission_time': 64369.774394750595, 'accumulated_eval_time': 4444.572045087814, 'accumulated_logging_time': 13.598825693130493}
I0307 21:09:54.381336 139758068020992 logging_writer.py:48] [108567] accumulated_eval_time=4444.57, accumulated_logging_time=13.5988, accumulated_submission_time=64369.8, global_step=108567, preemption_count=0, score=64369.8, test/accuracy=0.3669, test/loss=3.12052, test/num_examples=10000, total_duration=68838.9, train/accuracy=0.520847, train/loss=2.08151, validation/accuracy=0.48658, validation/loss=2.29818, validation/num_examples=50000
I0307 21:10:18.115685 139758059628288 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.7631416320800781, loss=2.0322108268737793
I0307 21:12:03.544335 139758068020992 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.6726515293121338, loss=1.9543389081954956
I0307 21:13:49.519027 139758059628288 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.7787188291549683, loss=1.8893163204193115
2025-03-07 21:14:23.610801: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:15:37.732854 139758068020992 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.8089250326156616, loss=1.8497360944747925
I0307 21:17:24.643260 139758059628288 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.8783800601959229, loss=1.92997145652771
I0307 21:18:24.677091 139912818214080 spec.py:321] Evaluating on the training split.
I0307 21:18:34.382935 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 21:18:53.284138 139912818214080 spec.py:349] Evaluating on the test split.
I0307 21:18:54.998493 139912818214080 submission_runner.py:469] Time since start: 69379.63s, 	Step: 109057, 	{'train/accuracy': 0.4429607689380646, 'train/loss': 2.5519204139709473, 'validation/accuracy': 0.4018999934196472, 'validation/loss': 2.8337395191192627, 'validation/num_examples': 50000, 'test/accuracy': 0.30790001153945923, 'test/loss': 3.607595920562744, 'test/num_examples': 10000, 'score': 64880.013041973114, 'total_duration': 69379.6287984848, 'accumulated_submission_time': 64880.013041973114, 'accumulated_eval_time': 4474.893383741379, 'accumulated_logging_time': 13.686125040054321}
I0307 21:18:55.042120 139758068020992 logging_writer.py:48] [109057] accumulated_eval_time=4474.89, accumulated_logging_time=13.6861, accumulated_submission_time=64880, global_step=109057, preemption_count=0, score=64880, test/accuracy=0.3079, test/loss=3.6076, test/num_examples=10000, total_duration=69379.6, train/accuracy=0.442961, train/loss=2.55192, validation/accuracy=0.4019, validation/loss=2.83374, validation/num_examples=50000
I0307 21:19:30.872194 139758059628288 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.697231650352478, loss=1.8042497634887695
I0307 21:21:17.516385 139758068020992 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.928743839263916, loss=2.033658027648926
I0307 21:23:04.079541 139758059628288 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.8417272567749023, loss=1.8407526016235352
I0307 21:24:51.187325 139758068020992 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.8063591718673706, loss=1.891481876373291
I0307 21:26:37.697977 139758059628288 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.948732852935791, loss=1.9676513671875
I0307 21:27:25.602461 139912818214080 spec.py:321] Evaluating on the training split.
I0307 21:27:36.004232 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 21:27:55.038908 139912818214080 spec.py:349] Evaluating on the test split.
I0307 21:27:56.723786 139912818214080 submission_runner.py:469] Time since start: 69921.35s, 	Step: 109546, 	{'train/accuracy': 0.48519212007522583, 'train/loss': 2.2884650230407715, 'validation/accuracy': 0.45701998472213745, 'validation/loss': 2.473346710205078, 'validation/num_examples': 50000, 'test/accuracy': 0.3483000099658966, 'test/loss': 3.234506845474243, 'test/num_examples': 10000, 'score': 65390.51763892174, 'total_duration': 69921.35412359238, 'accumulated_submission_time': 65390.51763892174, 'accumulated_eval_time': 4506.014676809311, 'accumulated_logging_time': 13.736923217773438}
I0307 21:27:56.783738 139758068020992 logging_writer.py:48] [109546] accumulated_eval_time=4506.01, accumulated_logging_time=13.7369, accumulated_submission_time=65390.5, global_step=109546, preemption_count=0, score=65390.5, test/accuracy=0.3483, test/loss=3.23451, test/num_examples=10000, total_duration=69921.4, train/accuracy=0.485192, train/loss=2.28847, validation/accuracy=0.45702, validation/loss=2.47335, validation/num_examples=50000
I0307 21:28:45.235547 139758059628288 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.8054665327072144, loss=1.9311357736587524
I0307 21:30:30.517699 139758068020992 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.781059741973877, loss=1.9108798503875732
I0307 21:32:16.571031 139758059628288 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.953216314315796, loss=1.9497874975204468
I0307 21:34:03.418793 139758068020992 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.7396823167800903, loss=1.8287668228149414
I0307 21:35:49.885318 139758059628288 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.8924566507339478, loss=2.0071611404418945
I0307 21:36:27.385064 139912818214080 spec.py:321] Evaluating on the training split.
I0307 21:36:37.502023 139912818214080 spec.py:333] Evaluating on the validation split.
I0307 21:36:55.884869 139912818214080 spec.py:349] Evaluating on the test split.
I0307 21:36:57.574548 139912818214080 submission_runner.py:469] Time since start: 70462.20s, 	Step: 110036, 	{'train/accuracy': 0.5681600570678711, 'train/loss': 1.824359655380249, 'validation/accuracy': 0.5266199707984924, 'validation/loss': 2.049992799758911, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.771702766418457, 'test/num_examples': 10000, 'score': 65901.06157374382, 'total_duration': 70462.20488548279, 'accumulated_submission_time': 65901.06157374382, 'accumulated_eval_time': 4536.2041301727295, 'accumulated_logging_time': 13.804651975631714}
I0307 21:36:57.635272 139758068020992 logging_writer.py:48] [110036] accumulated_eval_time=4536.2, accumulated_logging_time=13.8047, accumulated_submission_time=65901.1, global_step=110036, preemption_count=0, score=65901.1, test/accuracy=0.417, test/loss=2.7717, test/num_examples=10000, total_duration=70462.2, train/accuracy=0.56816, train/loss=1.82436, validation/accuracy=0.52662, validation/loss=2.04999, validation/num_examples=50000
2025-03-07 21:37:37.632929: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:37:57.089483 139758059628288 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.9303594827651978, loss=1.8760203123092651
I0307 21:39:42.620357 139758068020992 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.790492057800293, loss=1.9701175689697266
I0307 21:41:28.955467 139758059628288 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.8703441619873047, loss=2.01979660987854
I0307 21:43:15.681682 139758068020992 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.7661068439483643, loss=1.7139623165130615
I0307 21:45:02.029495 139758059628288 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.9589359760284424, loss=1.9111886024475098
I0307 21:45:28.395310 139758068020992 logging_writer.py:48] [110526] global_step=110526, preemption_count=0, score=66411.7
I0307 21:45:29.691042 139912818214080 submission_runner.py:646] Tuning trial 4/5
I0307 21:45:29.717438 139912818214080 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0307 21:45:29.721035 139912818214080 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.001215720665641129, 'train/loss': 6.912454128265381, 'validation/accuracy': 0.0012400000123307109, 'validation/loss': 6.913532257080078, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.913326263427734, 'test/num_examples': 10000, 'score': 59.900206565856934, 'total_duration': 135.1545467376709, 'accumulated_submission_time': 59.900206565856934, 'accumulated_eval_time': 75.25408744812012, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1366, {'train/accuracy': 0.20770886540412903, 'train/loss': 3.9941461086273193, 'validation/accuracy': 0.16985999047756195, 'validation/loss': 4.296290397644043, 'validation/num_examples': 50000, 'test/accuracy': 0.12450000643730164, 'test/loss': 4.820620059967041, 'test/num_examples': 10000, 'score': 569.6955945491791, 'total_duration': 678.7156174182892, 'accumulated_submission_time': 569.6955945491791, 'accumulated_eval_time': 108.79424858093262, 'accumulated_logging_time': 0.05858349800109863, 'global_step': 1366, 'preemption_count': 0}), (2708, {'train/accuracy': 0.36136797070503235, 'train/loss': 2.944725513458252, 'validation/accuracy': 0.3172000050544739, 'validation/loss': 3.2365479469299316, 'validation/num_examples': 50000, 'test/accuracy': 0.24220001697540283, 'test/loss': 3.8796088695526123, 'test/num_examples': 10000, 'score': 1079.633470773697, 'total_duration': 1222.617424249649, 'accumulated_submission_time': 1079.633470773697, 'accumulated_eval_time': 142.54613542556763, 'accumulated_logging_time': 0.1194760799407959, 'global_step': 2708, 'preemption_count': 0}), (4038, {'train/accuracy': 0.3820551633834839, 'train/loss': 2.8290624618530273, 'validation/accuracy': 0.3379199802875519, 'validation/loss': 3.129014253616333, 'validation/num_examples': 50000, 'test/accuracy': 0.2587999999523163, 'test/loss': 3.7857720851898193, 'test/num_examples': 10000, 'score': 1589.5423862934113, 'total_duration': 1770.871235370636, 'accumulated_submission_time': 1589.5423862934113, 'accumulated_eval_time': 180.68074917793274, 'accumulated_logging_time': 0.1895906925201416, 'global_step': 4038, 'preemption_count': 0}), (5376, {'train/accuracy': 0.3952885866165161, 'train/loss': 2.7969157695770264, 'validation/accuracy': 0.3600800037384033, 'validation/loss': 3.0530054569244385, 'validation/num_examples': 50000, 'test/accuracy': 0.2687000036239624, 'test/loss': 3.796846389770508, 'test/num_examples': 10000, 'score': 2099.603739500046, 'total_duration': 2312.1507647037506, 'accumulated_submission_time': 2099.603739500046, 'accumulated_eval_time': 211.6498167514801, 'accumulated_logging_time': 0.2729067802429199, 'global_step': 5376, 'preemption_count': 0}), (6705, {'train/accuracy': 0.3795439898967743, 'train/loss': 2.885197877883911, 'validation/accuracy': 0.3422999978065491, 'validation/loss': 3.1532857418060303, 'validation/num_examples': 50000, 'test/accuracy': 0.26500001549720764, 'test/loss': 3.8443524837493896, 'test/num_examples': 10000, 'score': 2609.510282754898, 'total_duration': 2856.672435760498, 'accumulated_submission_time': 2609.510282754898, 'accumulated_eval_time': 246.10440015792847, 'accumulated_logging_time': 0.3231070041656494, 'global_step': 6705, 'preemption_count': 0}), (8030, {'train/accuracy': 0.4034598171710968, 'train/loss': 2.741560697555542, 'validation/accuracy': 0.36569997668266296, 'validation/loss': 2.990696907043457, 'validation/num_examples': 50000, 'test/accuracy': 0.28060001134872437, 'test/loss': 3.71583890914917, 'test/num_examples': 10000, 'score': 3119.6531267166138, 'total_duration': 3398.7237045764923, 'accumulated_submission_time': 3119.6531267166138, 'accumulated_eval_time': 277.8167440891266, 'accumulated_logging_time': 0.39674830436706543, 'global_step': 8030, 'preemption_count': 0}), (9361, {'train/accuracy': 0.42532286047935486, 'train/loss': 2.592045307159424, 'validation/accuracy': 0.3952599763870239, 'validation/loss': 2.8141472339630127, 'validation/num_examples': 50000, 'test/accuracy': 0.30320000648498535, 'test/loss': 3.5196926593780518, 'test/num_examples': 10000, 'score': 3629.6068353652954, 'total_duration': 3942.0580966472626, 'accumulated_submission_time': 3629.6068353652954, 'accumulated_eval_time': 311.00517106056213, 'accumulated_logging_time': 0.46403074264526367, 'global_step': 9361, 'preemption_count': 0}), (10698, {'train/accuracy': 0.3426339328289032, 'train/loss': 3.183932065963745, 'validation/accuracy': 0.31365999579429626, 'validation/loss': 3.3830549716949463, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 4.124727725982666, 'test/num_examples': 10000, 'score': 4139.5288438797, 'total_duration': 4487.41979265213, 'accumulated_submission_time': 4139.5288438797, 'accumulated_eval_time': 346.22021484375, 'accumulated_logging_time': 0.5639228820800781, 'global_step': 10698, 'preemption_count': 0}), (11959, {'train/accuracy': 0.28597337007522583, 'train/loss': 3.6522598266601562, 'validation/accuracy': 0.2602199912071228, 'validation/loss': 3.873152732849121, 'validation/num_examples': 50000, 'test/accuracy': 0.19220000505447388, 'test/loss': 4.627013683319092, 'test/num_examples': 10000, 'score': 4649.423471689224, 'total_duration': 5035.899295806885, 'accumulated_submission_time': 4649.423471689224, 'accumulated_eval_time': 384.6221663951874, 'accumulated_logging_time': 0.6122465133666992, 'global_step': 11959, 'preemption_count': 0}), (13283, {'train/accuracy': 0.218610480427742, 'train/loss': 4.382891654968262, 'validation/accuracy': 0.19755999743938446, 'validation/loss': 4.544258117675781, 'validation/num_examples': 50000, 'test/accuracy': 0.14790000021457672, 'test/loss': 5.183443546295166, 'test/num_examples': 10000, 'score': 5159.331223726273, 'total_duration': 5589.645015716553, 'accumulated_submission_time': 5159.331223726273, 'accumulated_eval_time': 428.24883246421814, 'accumulated_logging_time': 0.6965365409851074, 'global_step': 13283, 'preemption_count': 0}), (14611, {'train/accuracy': 0.16260762512683868, 'train/loss': 4.795761585235596, 'validation/accuracy': 0.15109999477863312, 'validation/loss': 4.97237491607666, 'validation/num_examples': 50000, 'test/accuracy': 0.11820000410079956, 'test/loss': 5.5623555183410645, 'test/num_examples': 10000, 'score': 5669.181756258011, 'total_duration': 6140.590811014175, 'accumulated_submission_time': 5669.181756258011, 'accumulated_eval_time': 469.1012122631073, 'accumulated_logging_time': 0.815903902053833, 'global_step': 14611, 'preemption_count': 0}), (15938, {'train/accuracy': 0.12396364659070969, 'train/loss': 5.637012004852295, 'validation/accuracy': 0.11581999808549881, 'validation/loss': 5.7450385093688965, 'validation/num_examples': 50000, 'test/accuracy': 0.08190000057220459, 'test/loss': 6.5927534103393555, 'test/num_examples': 10000, 'score': 6179.066228866577, 'total_duration': 6694.590527057648, 'accumulated_submission_time': 6179.066228866577, 'accumulated_eval_time': 513.0025107860565, 'accumulated_logging_time': 0.9078075885772705, 'global_step': 15938, 'preemption_count': 0}), (17263, {'train/accuracy': 0.30383050441741943, 'train/loss': 3.4845948219299316, 'validation/accuracy': 0.2800399959087372, 'validation/loss': 3.731367349624634, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.455304145812988, 'test/num_examples': 10000, 'score': 6689.095885276794, 'total_duration': 7243.2870461940765, 'accumulated_submission_time': 6689.095885276794, 'accumulated_eval_time': 551.3749973773956, 'accumulated_logging_time': 1.080744743347168, 'global_step': 17263, 'preemption_count': 0}), (18595, {'train/accuracy': 0.3763352930545807, 'train/loss': 2.8804357051849365, 'validation/accuracy': 0.34373998641967773, 'validation/loss': 3.0832808017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.2556000053882599, 'test/loss': 3.8573572635650635, 'test/num_examples': 10000, 'score': 7199.254940748215, 'total_duration': 7791.56015086174, 'accumulated_submission_time': 7199.254940748215, 'accumulated_eval_time': 589.2712116241455, 'accumulated_logging_time': 1.1785659790039062, 'global_step': 18595, 'preemption_count': 0}), (19921, {'train/accuracy': 0.19389747083187103, 'train/loss': 4.529382228851318, 'validation/accuracy': 0.17983999848365784, 'validation/loss': 4.646400451660156, 'validation/num_examples': 50000, 'test/accuracy': 0.12810000777244568, 'test/loss': 5.335065841674805, 'test/num_examples': 10000, 'score': 7709.075330257416, 'total_duration': 8343.58575296402, 'accumulated_submission_time': 7709.075330257416, 'accumulated_eval_time': 631.2908473014832, 'accumulated_logging_time': 1.2385201454162598, 'global_step': 19921, 'preemption_count': 0}), (21254, {'train/accuracy': 0.2977120578289032, 'train/loss': 3.6996922492980957, 'validation/accuracy': 0.2754800021648407, 'validation/loss': 3.8890020847320557, 'validation/num_examples': 50000, 'test/accuracy': 0.21160000562667847, 'test/loss': 4.60891056060791, 'test/num_examples': 10000, 'score': 8219.180475473404, 'total_duration': 8890.96362566948, 'accumulated_submission_time': 8219.180475473404, 'accumulated_eval_time': 668.3319108486176, 'accumulated_logging_time': 1.3454127311706543, 'global_step': 21254, 'preemption_count': 0}), (22582, {'train/accuracy': 0.3062220811843872, 'train/loss': 3.46956729888916, 'validation/accuracy': 0.28735998272895813, 'validation/loss': 3.6494970321655273, 'validation/num_examples': 50000, 'test/accuracy': 0.21560001373291016, 'test/loss': 4.420133113861084, 'test/num_examples': 10000, 'score': 8728.941935539246, 'total_duration': 9438.146457910538, 'accumulated_submission_time': 8728.941935539246, 'accumulated_eval_time': 705.5021767616272, 'accumulated_logging_time': 1.4514343738555908, 'global_step': 22582, 'preemption_count': 0}), (23886, {'train/accuracy': 0.1809430718421936, 'train/loss': 4.816465854644775, 'validation/accuracy': 0.17107999324798584, 'validation/loss': 4.9599432945251465, 'validation/num_examples': 50000, 'test/accuracy': 0.12790000438690186, 'test/loss': 5.585414886474609, 'test/num_examples': 10000, 'score': 9239.083890199661, 'total_duration': 9987.817855119705, 'accumulated_submission_time': 9239.083890199661, 'accumulated_eval_time': 744.8772070407867, 'accumulated_logging_time': 1.48252272605896, 'global_step': 23886, 'preemption_count': 0}), (25216, {'train/accuracy': 0.2444196343421936, 'train/loss': 3.90531063079834, 'validation/accuracy': 0.23587998747825623, 'validation/loss': 4.027276515960693, 'validation/num_examples': 50000, 'test/accuracy': 0.17240001261234283, 'test/loss': 4.772428512573242, 'test/num_examples': 10000, 'score': 9749.014069080353, 'total_duration': 10535.390718221664, 'accumulated_submission_time': 9749.014069080353, 'accumulated_eval_time': 782.3180966377258, 'accumulated_logging_time': 1.5590429306030273, 'global_step': 25216, 'preemption_count': 0}), (26539, {'train/accuracy': 0.12880659103393555, 'train/loss': 5.402222633361816, 'validation/accuracy': 0.12727999687194824, 'validation/loss': 5.434912204742432, 'validation/num_examples': 50000, 'test/accuracy': 0.09180000424385071, 'test/loss': 6.047041893005371, 'test/num_examples': 10000, 'score': 10258.950868606567, 'total_duration': 11084.255680322647, 'accumulated_submission_time': 10258.950868606567, 'accumulated_eval_time': 821.0397350788116, 'accumulated_logging_time': 1.624593734741211, 'global_step': 26539, 'preemption_count': 0}), (27868, {'train/accuracy': 0.07362085580825806, 'train/loss': 5.873807430267334, 'validation/accuracy': 0.06932000070810318, 'validation/loss': 5.973196506500244, 'validation/num_examples': 50000, 'test/accuracy': 0.05470000207424164, 'test/loss': 6.279242992401123, 'test/num_examples': 10000, 'score': 10768.8849670887, 'total_duration': 11631.458408355713, 'accumulated_submission_time': 10768.8849670887, 'accumulated_eval_time': 858.046835899353, 'accumulated_logging_time': 1.7599942684173584, 'global_step': 27868, 'preemption_count': 0}), (29200, {'train/accuracy': 0.2259446680545807, 'train/loss': 4.034019947052002, 'validation/accuracy': 0.21357999742031097, 'validation/loss': 4.158266067504883, 'validation/num_examples': 50000, 'test/accuracy': 0.15410000085830688, 'test/loss': 4.854555130004883, 'test/num_examples': 10000, 'score': 11278.755184650421, 'total_duration': 12180.375717163086, 'accumulated_submission_time': 11278.755184650421, 'accumulated_eval_time': 896.7935652732849, 'accumulated_logging_time': 1.932185411453247, 'global_step': 29200, 'preemption_count': 0}), (30530, {'train/accuracy': 0.30466756224632263, 'train/loss': 3.3982272148132324, 'validation/accuracy': 0.28832000494003296, 'validation/loss': 3.5290591716766357, 'validation/num_examples': 50000, 'test/accuracy': 0.2184000164270401, 'test/loss': 4.302485466003418, 'test/num_examples': 10000, 'score': 11788.61396408081, 'total_duration': 12732.06314778328, 'accumulated_submission_time': 11788.61396408081, 'accumulated_eval_time': 938.3680939674377, 'accumulated_logging_time': 2.0627119541168213, 'global_step': 30530, 'preemption_count': 0}), (31861, {'train/accuracy': 0.3386479616165161, 'train/loss': 3.306255578994751, 'validation/accuracy': 0.30875998735427856, 'validation/loss': 3.505404233932495, 'validation/num_examples': 50000, 'test/accuracy': 0.2314000129699707, 'test/loss': 4.227850437164307, 'test/num_examples': 10000, 'score': 12298.672335624695, 'total_duration': 13279.934798002243, 'accumulated_submission_time': 12298.672335624695, 'accumulated_eval_time': 975.9185807704926, 'accumulated_logging_time': 2.2037296295166016, 'global_step': 31861, 'preemption_count': 0}), (33191, {'train/accuracy': 0.07876275479793549, 'train/loss': 6.050768852233887, 'validation/accuracy': 0.0728599950671196, 'validation/loss': 6.17998743057251, 'validation/num_examples': 50000, 'test/accuracy': 0.05450000241398811, 'test/loss': 6.591578483581543, 'test/num_examples': 10000, 'score': 12808.601141929626, 'total_duration': 13831.57458782196, 'accumulated_submission_time': 12808.601141929626, 'accumulated_eval_time': 1017.3841388225555, 'accumulated_logging_time': 2.323406219482422, 'global_step': 33191, 'preemption_count': 0}), (34522, {'train/accuracy': 0.21404655277729034, 'train/loss': 4.391355037689209, 'validation/accuracy': 0.2027599960565567, 'validation/loss': 4.467723846435547, 'validation/num_examples': 50000, 'test/accuracy': 0.14710000157356262, 'test/loss': 5.210992336273193, 'test/num_examples': 10000, 'score': 13318.472622156143, 'total_duration': 14379.44684457779, 'accumulated_submission_time': 13318.472622156143, 'accumulated_eval_time': 1055.1103928089142, 'accumulated_logging_time': 2.466771364212036, 'global_step': 34522, 'preemption_count': 0}), (35853, {'train/accuracy': 0.318359375, 'train/loss': 3.36989164352417, 'validation/accuracy': 0.29971998929977417, 'validation/loss': 3.5208065509796143, 'validation/num_examples': 50000, 'test/accuracy': 0.2217000126838684, 'test/loss': 4.265127658843994, 'test/num_examples': 10000, 'score': 13828.306865930557, 'total_duration': 14927.264145374298, 'accumulated_submission_time': 13828.306865930557, 'accumulated_eval_time': 1092.7857944965363, 'accumulated_logging_time': 2.64522385597229, 'global_step': 35853, 'preemption_count': 0}), (37189, {'train/accuracy': 0.2909359037876129, 'train/loss': 3.5204501152038574, 'validation/accuracy': 0.27619999647140503, 'validation/loss': 3.640540599822998, 'validation/num_examples': 50000, 'test/accuracy': 0.1973000019788742, 'test/loss': 4.319897651672363, 'test/num_examples': 10000, 'score': 14338.42866897583, 'total_duration': 15476.143499135971, 'accumulated_submission_time': 14338.42866897583, 'accumulated_eval_time': 1131.2959897518158, 'accumulated_logging_time': 2.762072801589966, 'global_step': 37189, 'preemption_count': 0}), (38523, {'train/accuracy': 0.21765385568141937, 'train/loss': 4.412486553192139, 'validation/accuracy': 0.2094999998807907, 'validation/loss': 4.522482872009277, 'validation/num_examples': 50000, 'test/accuracy': 0.15930001437664032, 'test/loss': 5.155503749847412, 'test/num_examples': 10000, 'score': 14848.20858669281, 'total_duration': 16022.269285440445, 'accumulated_submission_time': 14848.20858669281, 'accumulated_eval_time': 1167.337368965149, 'accumulated_logging_time': 2.9397571086883545, 'global_step': 38523, 'preemption_count': 0}), (39855, {'train/accuracy': 0.22122129797935486, 'train/loss': 4.288172245025635, 'validation/accuracy': 0.20493999123573303, 'validation/loss': 4.422561168670654, 'validation/num_examples': 50000, 'test/accuracy': 0.1485000103712082, 'test/loss': 5.135503768920898, 'test/num_examples': 10000, 'score': 15357.993515491486, 'total_duration': 16568.690752267838, 'accumulated_submission_time': 15357.993515491486, 'accumulated_eval_time': 1203.6948282718658, 'accumulated_logging_time': 3.0898118019104004, 'global_step': 39855, 'preemption_count': 0}), (41175, {'train/accuracy': 0.25414541363716125, 'train/loss': 4.127640724182129, 'validation/accuracy': 0.2437399923801422, 'validation/loss': 4.241576671600342, 'validation/num_examples': 50000, 'test/accuracy': 0.17910000681877136, 'test/loss': 4.957355976104736, 'test/num_examples': 10000, 'score': 15867.907405138016, 'total_duration': 17116.647346735, 'accumulated_submission_time': 15867.907405138016, 'accumulated_eval_time': 1241.492629289627, 'accumulated_logging_time': 3.203706741333008, 'global_step': 41175, 'preemption_count': 0}), (42505, {'train/accuracy': 0.14799903333187103, 'train/loss': 5.23631477355957, 'validation/accuracy': 0.13412000238895416, 'validation/loss': 5.44006872177124, 'validation/num_examples': 50000, 'test/accuracy': 0.11070000380277634, 'test/loss': 5.8600640296936035, 'test/num_examples': 10000, 'score': 16377.969247341156, 'total_duration': 17662.536828279495, 'accumulated_submission_time': 16377.969247341156, 'accumulated_eval_time': 1277.072520017624, 'accumulated_logging_time': 3.3218369483947754, 'global_step': 42505, 'preemption_count': 0}), (43833, {'train/accuracy': 0.32900190353393555, 'train/loss': 3.3557276725769043, 'validation/accuracy': 0.31233999133110046, 'validation/loss': 3.501643419265747, 'validation/num_examples': 50000, 'test/accuracy': 0.2331000119447708, 'test/loss': 4.316805839538574, 'test/num_examples': 10000, 'score': 16888.078696250916, 'total_duration': 18210.71925163269, 'accumulated_submission_time': 16888.078696250916, 'accumulated_eval_time': 1314.917272567749, 'accumulated_logging_time': 3.4193406105041504, 'global_step': 43833, 'preemption_count': 0}), (45164, {'train/accuracy': 0.21699616312980652, 'train/loss': 4.519068241119385, 'validation/accuracy': 0.20430000126361847, 'validation/loss': 4.622311592102051, 'validation/num_examples': 50000, 'test/accuracy': 0.14430001378059387, 'test/loss': 5.447676658630371, 'test/num_examples': 10000, 'score': 17398.228604078293, 'total_duration': 18754.42572402954, 'accumulated_submission_time': 17398.228604078293, 'accumulated_eval_time': 1348.2188844680786, 'accumulated_logging_time': 3.5425851345062256, 'global_step': 45164, 'preemption_count': 0}), (46492, {'train/accuracy': 0.24208784103393555, 'train/loss': 3.907804489135742, 'validation/accuracy': 0.2263999879360199, 'validation/loss': 4.015890598297119, 'validation/num_examples': 50000, 'test/accuracy': 0.15520000457763672, 'test/loss': 4.751544952392578, 'test/num_examples': 10000, 'score': 17908.11704468727, 'total_duration': 19302.28417825699, 'accumulated_submission_time': 17908.11704468727, 'accumulated_eval_time': 1385.9542672634125, 'accumulated_logging_time': 3.6488208770751953, 'global_step': 46492, 'preemption_count': 0}), (47825, {'train/accuracy': 0.2200852930545807, 'train/loss': 4.274899482727051, 'validation/accuracy': 0.21249999105930328, 'validation/loss': 4.3490166664123535, 'validation/num_examples': 50000, 'test/accuracy': 0.15320000052452087, 'test/loss': 5.190557479858398, 'test/num_examples': 10000, 'score': 18418.18158507347, 'total_duration': 19848.56801390648, 'accumulated_submission_time': 18418.18158507347, 'accumulated_eval_time': 1421.891592502594, 'accumulated_logging_time': 3.8012969493865967, 'global_step': 47825, 'preemption_count': 0}), (49157, {'train/accuracy': 0.35070550441741943, 'train/loss': 3.1125409603118896, 'validation/accuracy': 0.329259991645813, 'validation/loss': 3.23136043548584, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 3.971595525741577, 'test/num_examples': 10000, 'score': 18927.99786257744, 'total_duration': 20394.738711595535, 'accumulated_submission_time': 18927.99786257744, 'accumulated_eval_time': 1457.9824705123901, 'accumulated_logging_time': 3.936058521270752, 'global_step': 49157, 'preemption_count': 0}), (50489, {'train/accuracy': 0.2566366195678711, 'train/loss': 4.03180456161499, 'validation/accuracy': 0.24271999299526215, 'validation/loss': 4.188366889953613, 'validation/num_examples': 50000, 'test/accuracy': 0.1826000064611435, 'test/loss': 4.849823474884033, 'test/num_examples': 10000, 'score': 19437.906849861145, 'total_duration': 20941.65132212639, 'accumulated_submission_time': 19437.906849861145, 'accumulated_eval_time': 1494.733823299408, 'accumulated_logging_time': 4.056396484375, 'global_step': 50489, 'preemption_count': 0}), (51817, {'train/accuracy': 0.27431440353393555, 'train/loss': 3.9140548706054688, 'validation/accuracy': 0.26034000515937805, 'validation/loss': 4.0571513175964355, 'validation/num_examples': 50000, 'test/accuracy': 0.18700000643730164, 'test/loss': 4.955191612243652, 'test/num_examples': 10000, 'score': 19947.713540554047, 'total_duration': 21486.429004192352, 'accumulated_submission_time': 19947.713540554047, 'accumulated_eval_time': 1529.4199328422546, 'accumulated_logging_time': 4.213247299194336, 'global_step': 51817, 'preemption_count': 0}), (53146, {'train/accuracy': 0.2919921875, 'train/loss': 3.5867514610290527, 'validation/accuracy': 0.2764599919319153, 'validation/loss': 3.7457399368286133, 'validation/num_examples': 50000, 'test/accuracy': 0.20820000767707825, 'test/loss': 4.466219902038574, 'test/num_examples': 10000, 'score': 20457.487042188644, 'total_duration': 22032.279284477234, 'accumulated_submission_time': 20457.487042188644, 'accumulated_eval_time': 1565.2403440475464, 'accumulated_logging_time': 4.335663557052612, 'global_step': 53146, 'preemption_count': 0}), (54410, {'train/accuracy': 0.29711416363716125, 'train/loss': 3.75480055809021, 'validation/accuracy': 0.2734600007534027, 'validation/loss': 3.9515786170959473, 'validation/num_examples': 50000, 'test/accuracy': 0.21810001134872437, 'test/loss': 4.581789970397949, 'test/num_examples': 10000, 'score': 20967.469806194305, 'total_duration': 22578.584963560104, 'accumulated_submission_time': 20967.469806194305, 'accumulated_eval_time': 1601.2785637378693, 'accumulated_logging_time': 4.4938881397247314, 'global_step': 54410, 'preemption_count': 0}), (55742, {'train/accuracy': 0.2243303507566452, 'train/loss': 4.711413383483887, 'validation/accuracy': 0.20999999344348907, 'validation/loss': 4.8930134773254395, 'validation/num_examples': 50000, 'test/accuracy': 0.15960000455379486, 'test/loss': 5.658714294433594, 'test/num_examples': 10000, 'score': 21477.22707939148, 'total_duration': 23119.590398311615, 'accumulated_submission_time': 21477.22707939148, 'accumulated_eval_time': 1632.2657532691956, 'accumulated_logging_time': 4.616487979888916, 'global_step': 55742, 'preemption_count': 0}), (57070, {'train/accuracy': 0.07864317297935486, 'train/loss': 6.809846878051758, 'validation/accuracy': 0.07552000135183334, 'validation/loss': 6.8568267822265625, 'validation/num_examples': 50000, 'test/accuracy': 0.05350000411272049, 'test/loss': 7.474111080169678, 'test/num_examples': 10000, 'score': 21986.890318870544, 'total_duration': 23665.605720758438, 'accumulated_submission_time': 21986.890318870544, 'accumulated_eval_time': 1668.275202035904, 'accumulated_logging_time': 4.815795421600342, 'global_step': 57070, 'preemption_count': 0}), (58404, {'train/accuracy': 0.2693319320678711, 'train/loss': 3.7230355739593506, 'validation/accuracy': 0.2600799798965454, 'validation/loss': 3.8133041858673096, 'validation/num_examples': 50000, 'test/accuracy': 0.18960000574588776, 'test/loss': 4.524122714996338, 'test/num_examples': 10000, 'score': 22496.671526670456, 'total_duration': 24213.061018705368, 'accumulated_submission_time': 22496.671526670456, 'accumulated_eval_time': 1705.700838804245, 'accumulated_logging_time': 4.917435646057129, 'global_step': 58404, 'preemption_count': 0}), (59740, {'train/accuracy': 0.23052853345870972, 'train/loss': 4.335879802703857, 'validation/accuracy': 0.22307999432086945, 'validation/loss': 4.3867316246032715, 'validation/num_examples': 50000, 'test/accuracy': 0.15730001032352448, 'test/loss': 5.150058269500732, 'test/num_examples': 10000, 'score': 23006.582505464554, 'total_duration': 24760.69665169716, 'accumulated_submission_time': 23006.582505464554, 'accumulated_eval_time': 1743.114189863205, 'accumulated_logging_time': 5.083102464675903, 'global_step': 59740, 'preemption_count': 0}), (60971, {'train/accuracy': 0.31204161047935486, 'train/loss': 3.484349012374878, 'validation/accuracy': 0.29197999835014343, 'validation/loss': 3.6320223808288574, 'validation/num_examples': 50000, 'test/accuracy': 0.2126000076532364, 'test/loss': 4.427322864532471, 'test/num_examples': 10000, 'score': 23516.41524028778, 'total_duration': 25306.741443395615, 'accumulated_submission_time': 23516.41524028778, 'accumulated_eval_time': 1779.05122756958, 'accumulated_logging_time': 5.224490642547607, 'global_step': 60971, 'preemption_count': 0}), (62300, {'train/accuracy': 0.08432318270206451, 'train/loss': 6.182665824890137, 'validation/accuracy': 0.07967999577522278, 'validation/loss': 6.289052486419678, 'validation/num_examples': 50000, 'test/accuracy': 0.05650000274181366, 'test/loss': 6.748199462890625, 'test/num_examples': 10000, 'score': 24026.88319182396, 'total_duration': 25851.14160013199, 'accumulated_submission_time': 24026.88319182396, 'accumulated_eval_time': 1812.6780879497528, 'accumulated_logging_time': 5.386282205581665, 'global_step': 62300, 'preemption_count': 0}), (63631, {'train/accuracy': 0.20364317297935486, 'train/loss': 4.377975940704346, 'validation/accuracy': 0.19047999382019043, 'validation/loss': 4.559451580047607, 'validation/num_examples': 50000, 'test/accuracy': 0.13510000705718994, 'test/loss': 5.253306865692139, 'test/num_examples': 10000, 'score': 24536.85511612892, 'total_duration': 26397.39195251465, 'accumulated_submission_time': 24536.85511612892, 'accumulated_eval_time': 1848.7145290374756, 'accumulated_logging_time': 5.483468770980835, 'global_step': 63631, 'preemption_count': 0}), (64948, {'train/accuracy': 0.12232939898967743, 'train/loss': 5.541277885437012, 'validation/accuracy': 0.11481999605894089, 'validation/loss': 5.661109447479248, 'validation/num_examples': 50000, 'test/accuracy': 0.08340000361204147, 'test/loss': 6.302217483520508, 'test/num_examples': 10000, 'score': 25046.79501414299, 'total_duration': 26938.256316184998, 'accumulated_submission_time': 25046.79501414299, 'accumulated_eval_time': 1879.363888502121, 'accumulated_logging_time': 5.617506980895996, 'global_step': 64948, 'preemption_count': 0}), (66146, {'train/accuracy': 0.21041932702064514, 'train/loss': 4.359518051147461, 'validation/accuracy': 0.19565999507904053, 'validation/loss': 4.513914108276367, 'validation/num_examples': 50000, 'test/accuracy': 0.14990000426769257, 'test/loss': 5.104888916015625, 'test/num_examples': 10000, 'score': 25556.569621801376, 'total_duration': 27482.741450071335, 'accumulated_submission_time': 25556.569621801376, 'accumulated_eval_time': 1913.795380115509, 'accumulated_logging_time': 5.76784348487854, 'global_step': 66146, 'preemption_count': 0}), (67457, {'train/accuracy': 0.10122369229793549, 'train/loss': 5.660772800445557, 'validation/accuracy': 0.10147999972105026, 'validation/loss': 5.681365966796875, 'validation/num_examples': 50000, 'test/accuracy': 0.0731000006198883, 'test/loss': 6.2830281257629395, 'test/num_examples': 10000, 'score': 26066.36860203743, 'total_duration': 28030.706936120987, 'accumulated_submission_time': 26066.36860203743, 'accumulated_eval_time': 1951.58740067482, 'accumulated_logging_time': 6.006271839141846, 'global_step': 67457, 'preemption_count': 0}), (68604, {'train/accuracy': 0.24226722121238708, 'train/loss': 4.013217926025391, 'validation/accuracy': 0.23211999237537384, 'validation/loss': 4.1081156730651855, 'validation/num_examples': 50000, 'test/accuracy': 0.16790001094341278, 'test/loss': 4.842030048370361, 'test/num_examples': 10000, 'score': 26576.530514001846, 'total_duration': 28582.323868989944, 'accumulated_submission_time': 26576.530514001846, 'accumulated_eval_time': 1992.7902088165283, 'accumulated_logging_time': 6.138944864273071, 'global_step': 68604, 'preemption_count': 0}), (69761, {'train/accuracy': 0.4194435477256775, 'train/loss': 2.6731088161468506, 'validation/accuracy': 0.3912599980831146, 'validation/loss': 2.885143280029297, 'validation/num_examples': 50000, 'test/accuracy': 0.304500013589859, 'test/loss': 3.616581916809082, 'test/num_examples': 10000, 'score': 27086.441253900528, 'total_duration': 29132.377957344055, 'accumulated_submission_time': 27086.441253900528, 'accumulated_eval_time': 2032.7259666919708, 'accumulated_logging_time': 6.227500915527344, 'global_step': 69761, 'preemption_count': 0}), (70997, {'train/accuracy': 0.38020166754722595, 'train/loss': 2.990177869796753, 'validation/accuracy': 0.35207998752593994, 'validation/loss': 3.190067768096924, 'validation/num_examples': 50000, 'test/accuracy': 0.2696000039577484, 'test/loss': 3.9622013568878174, 'test/num_examples': 10000, 'score': 27596.176204681396, 'total_duration': 29678.258331775665, 'accumulated_submission_time': 27596.176204681396, 'accumulated_eval_time': 2068.5434789657593, 'accumulated_logging_time': 6.431675434112549, 'global_step': 70997, 'preemption_count': 0}), (72207, {'train/accuracy': 0.405970960855484, 'train/loss': 2.7443697452545166, 'validation/accuracy': 0.37849998474121094, 'validation/loss': 2.8891847133636475, 'validation/num_examples': 50000, 'test/accuracy': 0.28110000491142273, 'test/loss': 3.6352696418762207, 'test/num_examples': 10000, 'score': 28106.001940727234, 'total_duration': 30225.210569143295, 'accumulated_submission_time': 28106.001940727234, 'accumulated_eval_time': 2105.3787446022034, 'accumulated_logging_time': 6.595596551895142, 'global_step': 72207, 'preemption_count': 0}), (73357, {'train/accuracy': 0.2795161008834839, 'train/loss': 3.9444632530212402, 'validation/accuracy': 0.26486000418663025, 'validation/loss': 4.131694316864014, 'validation/num_examples': 50000, 'test/accuracy': 0.20240001380443573, 'test/loss': 4.866608619689941, 'test/num_examples': 10000, 'score': 28616.11544251442, 'total_duration': 30769.347465991974, 'accumulated_submission_time': 28616.11544251442, 'accumulated_eval_time': 2139.1373126506805, 'accumulated_logging_time': 6.7432496547698975, 'global_step': 73357, 'preemption_count': 0}), (74613, {'train/accuracy': 0.38201528787612915, 'train/loss': 2.8985865116119385, 'validation/accuracy': 0.35471999645233154, 'validation/loss': 3.091930627822876, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.7877094745635986, 'test/num_examples': 10000, 'score': 29126.236311912537, 'total_duration': 31317.851099014282, 'accumulated_submission_time': 29126.236311912537, 'accumulated_eval_time': 2177.2743995189667, 'accumulated_logging_time': 6.862349033355713, 'global_step': 74613, 'preemption_count': 0}), (75704, {'train/accuracy': 0.2507772445678711, 'train/loss': 3.8978397846221924, 'validation/accuracy': 0.23555999994277954, 'validation/loss': 4.033202648162842, 'validation/num_examples': 50000, 'test/accuracy': 0.1672000139951706, 'test/loss': 4.81596565246582, 'test/num_examples': 10000, 'score': 29635.734853982925, 'total_duration': 31864.025255203247, 'accumulated_submission_time': 29635.734853982925, 'accumulated_eval_time': 2213.3318994045258, 'accumulated_logging_time': 7.369262218475342, 'global_step': 75704, 'preemption_count': 0}), (76832, {'train/accuracy': 0.2830835282802582, 'train/loss': 3.5965940952301025, 'validation/accuracy': 0.2624799907207489, 'validation/loss': 3.7933030128479004, 'validation/num_examples': 50000, 'test/accuracy': 0.18660001456737518, 'test/loss': 4.628851413726807, 'test/num_examples': 10000, 'score': 30145.685677289963, 'total_duration': 32407.723314523697, 'accumulated_submission_time': 30145.685677289963, 'accumulated_eval_time': 2246.798100233078, 'accumulated_logging_time': 7.535661220550537, 'global_step': 76832, 'preemption_count': 0}), (78006, {'train/accuracy': 0.4375, 'train/loss': 2.5181257724761963, 'validation/accuracy': 0.41373997926712036, 'validation/loss': 2.6819775104522705, 'validation/num_examples': 50000, 'test/accuracy': 0.3184000253677368, 'test/loss': 3.4338059425354004, 'test/num_examples': 10000, 'score': 30655.76484131813, 'total_duration': 32949.15985393524, 'accumulated_submission_time': 30655.76484131813, 'accumulated_eval_time': 2277.8452911376953, 'accumulated_logging_time': 7.725235462188721, 'global_step': 78006, 'preemption_count': 0}), (79059, {'train/accuracy': 0.4021444320678711, 'train/loss': 2.7515079975128174, 'validation/accuracy': 0.38843998312950134, 'validation/loss': 2.870887041091919, 'validation/num_examples': 50000, 'test/accuracy': 0.29010000824928284, 'test/loss': 3.6156668663024902, 'test/num_examples': 10000, 'score': 31165.823544979095, 'total_duration': 33491.63904905319, 'accumulated_submission_time': 31165.823544979095, 'accumulated_eval_time': 2310.0023300647736, 'accumulated_logging_time': 7.882008790969849, 'global_step': 79059, 'preemption_count': 0}), (80068, {'train/accuracy': 0.39716199040412903, 'train/loss': 2.8456647396087646, 'validation/accuracy': 0.3685999810695648, 'validation/loss': 3.0420327186584473, 'validation/num_examples': 50000, 'test/accuracy': 0.27310001850128174, 'test/loss': 3.8727002143859863, 'test/num_examples': 10000, 'score': 31676.695521593094, 'total_duration': 34037.5544860363, 'accumulated_submission_time': 31676.695521593094, 'accumulated_eval_time': 2344.794293165207, 'accumulated_logging_time': 8.031684637069702, 'global_step': 80068, 'preemption_count': 0}), (80971, {'train/accuracy': 0.4471261203289032, 'train/loss': 2.471245765686035, 'validation/accuracy': 0.4171399772167206, 'validation/loss': 2.709916830062866, 'validation/num_examples': 50000, 'test/accuracy': 0.3068000078201294, 'test/loss': 3.6072494983673096, 'test/num_examples': 10000, 'score': 32186.881184339523, 'total_duration': 34586.79746770859, 'accumulated_submission_time': 32186.881184339523, 'accumulated_eval_time': 2383.5903754234314, 'accumulated_logging_time': 8.202873229980469, 'global_step': 80971, 'preemption_count': 0}), (82062, {'train/accuracy': 0.24774792790412903, 'train/loss': 4.247570991516113, 'validation/accuracy': 0.21781998872756958, 'validation/loss': 4.5438642501831055, 'validation/num_examples': 50000, 'test/accuracy': 0.17160001397132874, 'test/loss': 5.091366291046143, 'test/num_examples': 10000, 'score': 32696.94563627243, 'total_duration': 35137.31633806229, 'accumulated_submission_time': 32696.94563627243, 'accumulated_eval_time': 2423.8081605434418, 'accumulated_logging_time': 8.330325365066528, 'global_step': 82062, 'preemption_count': 0}), (82882, {'train/accuracy': 0.31377550959587097, 'train/loss': 3.415020704269409, 'validation/accuracy': 0.2882799804210663, 'validation/loss': 3.6132724285125732, 'validation/num_examples': 50000, 'test/accuracy': 0.22190001606941223, 'test/loss': 4.257192134857178, 'test/num_examples': 10000, 'score': 33210.56051301956, 'total_duration': 35687.1595954895, 'accumulated_submission_time': 33210.56051301956, 'accumulated_eval_time': 2459.7701184749603, 'accumulated_logging_time': 8.514837265014648, 'global_step': 82882, 'preemption_count': 0}), (83882, {'train/accuracy': 0.4203603267669678, 'train/loss': 2.688044548034668, 'validation/accuracy': 0.3874799907207489, 'validation/loss': 2.9031968116760254, 'validation/num_examples': 50000, 'test/accuracy': 0.2930000126361847, 'test/loss': 3.65958833694458, 'test/num_examples': 10000, 'score': 33720.56858229637, 'total_duration': 36233.466488838196, 'accumulated_submission_time': 33720.56858229637, 'accumulated_eval_time': 2495.8265924453735, 'accumulated_logging_time': 8.658860206604004, 'global_step': 83882, 'preemption_count': 0}), (84681, {'train/accuracy': 0.42881056666374207, 'train/loss': 2.6365716457366943, 'validation/accuracy': 0.37379997968673706, 'validation/loss': 3.011368751525879, 'validation/num_examples': 50000, 'test/accuracy': 0.28540000319480896, 'test/loss': 3.734708070755005, 'test/num_examples': 10000, 'score': 34230.711030721664, 'total_duration': 36778.74319434166, 'accumulated_submission_time': 34230.711030721664, 'accumulated_eval_time': 2530.7474744319916, 'accumulated_logging_time': 8.789774656295776, 'global_step': 84681, 'preemption_count': 0}), (85537, {'train/accuracy': 0.32495614886283875, 'train/loss': 3.3678696155548096, 'validation/accuracy': 0.30107998847961426, 'validation/loss': 3.5798227787017822, 'validation/num_examples': 50000, 'test/accuracy': 0.2298000156879425, 'test/loss': 4.289230823516846, 'test/num_examples': 10000, 'score': 34740.91636013985, 'total_duration': 37322.14693522453, 'accumulated_submission_time': 34740.91636013985, 'accumulated_eval_time': 2563.6638991832733, 'accumulated_logging_time': 8.986862659454346, 'global_step': 85537, 'preemption_count': 0}), (86283, {'train/accuracy': 0.4278738796710968, 'train/loss': 2.6401331424713135, 'validation/accuracy': 0.3942199945449829, 'validation/loss': 2.8568708896636963, 'validation/num_examples': 50000, 'test/accuracy': 0.30420002341270447, 'test/loss': 3.6280579566955566, 'test/num_examples': 10000, 'score': 35251.72738862038, 'total_duration': 37862.59804439545, 'accumulated_submission_time': 35251.72738862038, 'accumulated_eval_time': 2593.1319942474365, 'accumulated_logging_time': 9.08389687538147, 'global_step': 86283, 'preemption_count': 0}), (86872, {'train/accuracy': 0.36463648080825806, 'train/loss': 3.0720016956329346, 'validation/accuracy': 0.3403799831867218, 'validation/loss': 3.224980592727661, 'validation/num_examples': 50000, 'test/accuracy': 0.24290001392364502, 'test/loss': 4.040991306304932, 'test/num_examples': 10000, 'score': 35762.01755452156, 'total_duration': 38407.703194856644, 'accumulated_submission_time': 35762.01755452156, 'accumulated_eval_time': 2627.7406697273254, 'accumulated_logging_time': 9.208647966384888, 'global_step': 86872, 'preemption_count': 0}), (87613, {'train/accuracy': 0.3198740482330322, 'train/loss': 3.4488463401794434, 'validation/accuracy': 0.29503998160362244, 'validation/loss': 3.62949800491333, 'validation/num_examples': 50000, 'test/accuracy': 0.2118000090122223, 'test/loss': 4.455244064331055, 'test/num_examples': 10000, 'score': 36271.91620469093, 'total_duration': 38948.47041392326, 'accumulated_submission_time': 36271.91620469093, 'accumulated_eval_time': 2658.45951962471, 'accumulated_logging_time': 9.26658296585083, 'global_step': 87613, 'preemption_count': 0}), (88353, {'train/accuracy': 0.4383968412876129, 'train/loss': 2.566208839416504, 'validation/accuracy': 0.4149799942970276, 'validation/loss': 2.726715326309204, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.6193692684173584, 'test/num_examples': 10000, 'score': 36782.07646083832, 'total_duration': 39493.12785601616, 'accumulated_submission_time': 36782.07646083832, 'accumulated_eval_time': 2692.734828710556, 'accumulated_logging_time': 9.36575198173523, 'global_step': 88353, 'preemption_count': 0}), (89032, {'train/accuracy': 0.5041653513908386, 'train/loss': 2.136971950531006, 'validation/accuracy': 0.4671799838542938, 'validation/loss': 2.3669753074645996, 'validation/num_examples': 50000, 'test/accuracy': 0.36180001497268677, 'test/loss': 3.137204885482788, 'test/num_examples': 10000, 'score': 37292.15191030502, 'total_duration': 40037.56240582466, 'accumulated_submission_time': 37292.15191030502, 'accumulated_eval_time': 2726.892630338669, 'accumulated_logging_time': 9.470914602279663, 'global_step': 89032, 'preemption_count': 0}), (89630, {'train/accuracy': 0.28491708636283875, 'train/loss': 3.8181982040405273, 'validation/accuracy': 0.2721799910068512, 'validation/loss': 3.9344396591186523, 'validation/num_examples': 50000, 'test/accuracy': 0.19480000436306, 'test/loss': 4.78629732131958, 'test/num_examples': 10000, 'score': 37802.704761981964, 'total_duration': 40586.73848104477, 'accumulated_submission_time': 37802.704761981964, 'accumulated_eval_time': 2765.3479180336, 'accumulated_logging_time': 9.566975355148315, 'global_step': 89630, 'preemption_count': 0}), (90315, {'train/accuracy': 0.4287707209587097, 'train/loss': 2.583172559738159, 'validation/accuracy': 0.39183998107910156, 'validation/loss': 2.7954955101013184, 'validation/num_examples': 50000, 'test/accuracy': 0.3083000183105469, 'test/loss': 3.5039920806884766, 'test/num_examples': 10000, 'score': 38312.84613490105, 'total_duration': 41133.709064245224, 'accumulated_submission_time': 38312.84613490105, 'accumulated_eval_time': 2802.061729669571, 'accumulated_logging_time': 9.615619659423828, 'global_step': 90315, 'preemption_count': 0}), (90761, {'train/accuracy': 0.3580596148967743, 'train/loss': 3.140453577041626, 'validation/accuracy': 0.32791998982429504, 'validation/loss': 3.3639862537384033, 'validation/num_examples': 50000, 'test/accuracy': 0.24990001320838928, 'test/loss': 4.090452194213867, 'test/num_examples': 10000, 'score': 38823.021700143814, 'total_duration': 41675.10214686394, 'accumulated_submission_time': 38823.021700143814, 'accumulated_eval_time': 2833.1608271598816, 'accumulated_logging_time': 9.68972134590149, 'global_step': 90761, 'preemption_count': 0}), (91303, {'train/accuracy': 0.22564572095870972, 'train/loss': 4.367197513580322, 'validation/accuracy': 0.20673999190330505, 'validation/loss': 4.565398216247559, 'validation/num_examples': 50000, 'test/accuracy': 0.16140000522136688, 'test/loss': 5.176565647125244, 'test/num_examples': 10000, 'score': 39333.71317434311, 'total_duration': 42216.456063985825, 'accumulated_submission_time': 39333.71317434311, 'accumulated_eval_time': 2863.70800447464, 'accumulated_logging_time': 9.75185513496399, 'global_step': 91303, 'preemption_count': 0}), (91793, {'train/accuracy': 0.4008290767669678, 'train/loss': 2.850691795349121, 'validation/accuracy': 0.3708399832248688, 'validation/loss': 3.0268020629882812, 'validation/num_examples': 50000, 'test/accuracy': 0.2800000011920929, 'test/loss': 3.766608476638794, 'test/num_examples': 10000, 'score': 39844.46280670166, 'total_duration': 42759.34964585304, 'accumulated_submission_time': 39844.46280670166, 'accumulated_eval_time': 2895.7530965805054, 'accumulated_logging_time': 9.802462816238403, 'global_step': 91793, 'preemption_count': 0}), (92123, {'train/accuracy': 0.47897401452064514, 'train/loss': 2.2947795391082764, 'validation/accuracy': 0.45124000310897827, 'validation/loss': 2.4649317264556885, 'validation/num_examples': 50000, 'test/accuracy': 0.34880000352859497, 'test/loss': 3.142186164855957, 'test/num_examples': 10000, 'score': 40357.12898850441, 'total_duration': 43306.84474802017, 'accumulated_submission_time': 40357.12898850441, 'accumulated_eval_time': 2930.49818277359, 'accumulated_logging_time': 9.853858947753906, 'global_step': 92123, 'preemption_count': 0}), (92664, {'train/accuracy': 0.32746732234954834, 'train/loss': 3.3227267265319824, 'validation/accuracy': 0.30983999371528625, 'validation/loss': 3.4916634559631348, 'validation/num_examples': 50000, 'test/accuracy': 0.2353000044822693, 'test/loss': 4.212726593017578, 'test/num_examples': 10000, 'score': 40867.14444565773, 'total_duration': 43852.87376022339, 'accumulated_submission_time': 40867.14444565773, 'accumulated_eval_time': 2966.429989337921, 'accumulated_logging_time': 9.880236387252808, 'global_step': 92664, 'preemption_count': 0}), (93082, {'train/accuracy': 0.3913823366165161, 'train/loss': 2.8736515045166016, 'validation/accuracy': 0.3610199987888336, 'validation/loss': 3.1132686138153076, 'validation/num_examples': 50000, 'test/accuracy': 0.27810001373291016, 'test/loss': 3.883187770843506, 'test/num_examples': 10000, 'score': 41378.56245803833, 'total_duration': 44396.10234570503, 'accumulated_submission_time': 41378.56245803833, 'accumulated_eval_time': 2998.1125757694244, 'accumulated_logging_time': 9.967206239700317, 'global_step': 93082, 'preemption_count': 0}), (93520, {'train/accuracy': 0.22303491830825806, 'train/loss': 4.289080619812012, 'validation/accuracy': 0.21389999985694885, 'validation/loss': 4.370250701904297, 'validation/num_examples': 50000, 'test/accuracy': 0.15790000557899475, 'test/loss': 5.089090347290039, 'test/num_examples': 10000, 'score': 41889.2252702713, 'total_duration': 44940.686950445175, 'accumulated_submission_time': 41889.2252702713, 'accumulated_eval_time': 3031.9561438560486, 'accumulated_logging_time': 10.002298831939697, 'global_step': 93520, 'preemption_count': 0}), (93969, {'train/accuracy': 0.39714205265045166, 'train/loss': 2.807849407196045, 'validation/accuracy': 0.376120001077652, 'validation/loss': 2.949687957763672, 'validation/num_examples': 50000, 'test/accuracy': 0.28230002522468567, 'test/loss': 3.65767240524292, 'test/num_examples': 10000, 'score': 42400.49991130829, 'total_duration': 45483.69449329376, 'accumulated_submission_time': 42400.49991130829, 'accumulated_eval_time': 3063.5924298763275, 'accumulated_logging_time': 10.055891513824463, 'global_step': 93969, 'preemption_count': 0}), (94392, {'train/accuracy': 0.4871053695678711, 'train/loss': 2.297102451324463, 'validation/accuracy': 0.4463599920272827, 'validation/loss': 2.5426440238952637, 'validation/num_examples': 50000, 'test/accuracy': 0.3425000309944153, 'test/loss': 3.2957942485809326, 'test/num_examples': 10000, 'score': 42910.89403343201, 'total_duration': 46026.08441734314, 'accumulated_submission_time': 42910.89403343201, 'accumulated_eval_time': 3095.4964191913605, 'accumulated_logging_time': 10.105541706085205, 'global_step': 94392, 'preemption_count': 0}), (94747, {'train/accuracy': 0.36930006742477417, 'train/loss': 3.0641403198242188, 'validation/accuracy': 0.3443399965763092, 'validation/loss': 3.2247462272644043, 'validation/num_examples': 50000, 'test/accuracy': 0.25050002336502075, 'test/loss': 4.108883857727051, 'test/num_examples': 10000, 'score': 43421.15713977814, 'total_duration': 46569.21884703636, 'accumulated_submission_time': 43421.15713977814, 'accumulated_eval_time': 3128.2542958259583, 'accumulated_logging_time': 10.18239688873291, 'global_step': 94747, 'preemption_count': 0}), (95112, {'train/accuracy': 0.4338727593421936, 'train/loss': 2.610846757888794, 'validation/accuracy': 0.4057199954986572, 'validation/loss': 2.7837162017822266, 'validation/num_examples': 50000, 'test/accuracy': 0.30230000615119934, 'test/loss': 3.6431634426116943, 'test/num_examples': 10000, 'score': 43931.28394365311, 'total_duration': 47110.27421617508, 'accumulated_submission_time': 43931.28394365311, 'accumulated_eval_time': 3159.1052787303925, 'accumulated_logging_time': 10.224416017532349, 'global_step': 95112, 'preemption_count': 0}), (95581, {'train/accuracy': 0.35321667790412903, 'train/loss': 3.0999114513397217, 'validation/accuracy': 0.33013999462127686, 'validation/loss': 3.2671823501586914, 'validation/num_examples': 50000, 'test/accuracy': 0.2615000009536743, 'test/loss': 3.94413685798645, 'test/num_examples': 10000, 'score': 44441.315484046936, 'total_duration': 47655.0097014904, 'accumulated_submission_time': 44441.315484046936, 'accumulated_eval_time': 3193.665872335434, 'accumulated_logging_time': 10.321303367614746, 'global_step': 95581, 'preemption_count': 0}), (96265, {'train/accuracy': 0.3660913407802582, 'train/loss': 3.0898873805999756, 'validation/accuracy': 0.3385999798774719, 'validation/loss': 3.291914224624634, 'validation/num_examples': 50000, 'test/accuracy': 0.26270002126693726, 'test/loss': 3.944162607192993, 'test/num_examples': 10000, 'score': 44951.182299375534, 'total_duration': 48199.88158273697, 'accumulated_submission_time': 44951.182299375534, 'accumulated_eval_time': 3228.5068147182465, 'accumulated_logging_time': 10.416137456893921, 'global_step': 96265, 'preemption_count': 0}), (96612, {'train/accuracy': 0.5189133882522583, 'train/loss': 2.064518928527832, 'validation/accuracy': 0.48527997732162476, 'validation/loss': 2.2548553943634033, 'validation/num_examples': 50000, 'test/accuracy': 0.3700000047683716, 'test/loss': 3.0916426181793213, 'test/num_examples': 10000, 'score': 45461.34050989151, 'total_duration': 48740.54130315781, 'accumulated_submission_time': 45461.34050989151, 'accumulated_eval_time': 3258.8734402656555, 'accumulated_logging_time': 10.515653133392334, 'global_step': 96612, 'preemption_count': 0}), (97015, {'train/accuracy': 0.35431280732154846, 'train/loss': 3.215914487838745, 'validation/accuracy': 0.31977999210357666, 'validation/loss': 3.4847309589385986, 'validation/num_examples': 50000, 'test/accuracy': 0.22770000994205475, 'test/loss': 4.4796953201293945, 'test/num_examples': 10000, 'score': 45971.2893846035, 'total_duration': 49284.12849187851, 'accumulated_submission_time': 45971.2893846035, 'accumulated_eval_time': 3292.4098057746887, 'accumulated_logging_time': 10.576824188232422, 'global_step': 97015, 'preemption_count': 0}), (97276, {'train/accuracy': 0.45874521136283875, 'train/loss': 2.3809010982513428, 'validation/accuracy': 0.41867998242378235, 'validation/loss': 2.6263821125030518, 'validation/num_examples': 50000, 'test/accuracy': 0.31150001287460327, 'test/loss': 3.4157767295837402, 'test/num_examples': 10000, 'score': 46481.82843899727, 'total_duration': 49827.92433190346, 'accumulated_submission_time': 46481.82843899727, 'accumulated_eval_time': 3325.568233728409, 'accumulated_logging_time': 10.65055513381958, 'global_step': 97276, 'preemption_count': 0}), (97809, {'train/accuracy': 0.40796396136283875, 'train/loss': 2.8251566886901855, 'validation/accuracy': 0.38001999258995056, 'validation/loss': 3.025785207748413, 'validation/num_examples': 50000, 'test/accuracy': 0.2787000238895416, 'test/loss': 3.8887994289398193, 'test/num_examples': 10000, 'score': 46992.6064491272, 'total_duration': 50371.18669986725, 'accumulated_submission_time': 46992.6064491272, 'accumulated_eval_time': 3357.9333279132843, 'accumulated_logging_time': 10.716936588287354, 'global_step': 97809, 'preemption_count': 0}), (97946, {'train/accuracy': 0.33814969658851624, 'train/loss': 3.244490146636963, 'validation/accuracy': 0.3164199888706207, 'validation/loss': 3.3999385833740234, 'validation/num_examples': 50000, 'test/accuracy': 0.23410001397132874, 'test/loss': 4.17090368270874, 'test/num_examples': 10000, 'score': 47502.73087024689, 'total_duration': 50912.00312900543, 'accumulated_submission_time': 47502.73087024689, 'accumulated_eval_time': 3388.53959274292, 'accumulated_logging_time': 10.789500713348389, 'global_step': 97946, 'preemption_count': 0}), (98369, {'train/accuracy': 0.3689413070678711, 'train/loss': 3.0342400074005127, 'validation/accuracy': 0.353659987449646, 'validation/loss': 3.1662850379943848, 'validation/num_examples': 50000, 'test/accuracy': 0.26360002160072327, 'test/loss': 3.998591661453247, 'test/num_examples': 10000, 'score': 48012.76383924484, 'total_duration': 51455.906281232834, 'accumulated_submission_time': 48012.76383924484, 'accumulated_eval_time': 3422.3430075645447, 'accumulated_logging_time': 10.816771268844604, 'global_step': 98369, 'preemption_count': 0}), (98716, {'train/accuracy': 0.4282326102256775, 'train/loss': 2.578181743621826, 'validation/accuracy': 0.3993400037288666, 'validation/loss': 2.7811942100524902, 'validation/num_examples': 50000, 'test/accuracy': 0.2945000231266022, 'test/loss': 3.5721287727355957, 'test/num_examples': 10000, 'score': 48523.00835061073, 'total_duration': 51997.51784777641, 'accumulated_submission_time': 48523.00835061073, 'accumulated_eval_time': 3453.566575527191, 'accumulated_logging_time': 10.925185918807983, 'global_step': 98716, 'preemption_count': 0}), (99009, {'train/accuracy': 0.5184749364852905, 'train/loss': 2.068596363067627, 'validation/accuracy': 0.48495998978614807, 'validation/loss': 2.285196542739868, 'validation/num_examples': 50000, 'test/accuracy': 0.37040001153945923, 'test/loss': 3.048356056213379, 'test/num_examples': 10000, 'score': 49036.61528468132, 'total_duration': 52546.755868434906, 'accumulated_submission_time': 49036.61528468132, 'accumulated_eval_time': 3489.1196777820587, 'accumulated_logging_time': 10.975955247879028, 'global_step': 99009, 'preemption_count': 0}), (99134, {'train/accuracy': 0.4529057741165161, 'train/loss': 2.449897289276123, 'validation/accuracy': 0.4195599853992462, 'validation/loss': 2.701817512512207, 'validation/num_examples': 50000, 'test/accuracy': 0.3126000165939331, 'test/loss': 3.5325403213500977, 'test/num_examples': 10000, 'score': 49548.14441919327, 'total_duration': 53091.00955462456, 'accumulated_submission_time': 49548.14441919327, 'accumulated_eval_time': 3521.806339740753, 'accumulated_logging_time': 11.002286434173584, 'global_step': 99134, 'preemption_count': 0}), (99260, {'train/accuracy': 0.3771723508834839, 'train/loss': 2.9567601680755615, 'validation/accuracy': 0.35297998785972595, 'validation/loss': 3.1187455654144287, 'validation/num_examples': 50000, 'test/accuracy': 0.27230000495910645, 'test/loss': 3.815753698348999, 'test/num_examples': 10000, 'score': 50060.92329573631, 'total_duration': 53637.53148174286, 'accumulated_submission_time': 50060.92329573631, 'accumulated_eval_time': 3555.511514186859, 'accumulated_logging_time': 11.028517723083496, 'global_step': 99260, 'preemption_count': 0}), (99386, {'train/accuracy': 0.433613657951355, 'train/loss': 2.59604811668396, 'validation/accuracy': 0.4080999791622162, 'validation/loss': 2.7744081020355225, 'validation/num_examples': 50000, 'test/accuracy': 0.3216000199317932, 'test/loss': 3.504655599594116, 'test/num_examples': 10000, 'score': 50573.47844362259, 'total_duration': 54181.534838438034, 'accumulated_submission_time': 50573.47844362259, 'accumulated_eval_time': 3586.9204914569855, 'accumulated_logging_time': 11.055777311325073, 'global_step': 99386, 'preemption_count': 0}), (99621, {'train/accuracy': 0.23301976919174194, 'train/loss': 4.406154155731201, 'validation/accuracy': 0.2148599922657013, 'validation/loss': 4.579807281494141, 'validation/num_examples': 50000, 'test/accuracy': 0.147800013422966, 'test/loss': 5.427378177642822, 'test/num_examples': 10000, 'score': 51084.60216808319, 'total_duration': 54722.91211724281, 'accumulated_submission_time': 51084.60216808319, 'accumulated_eval_time': 3617.06986451149, 'accumulated_logging_time': 11.137469291687012, 'global_step': 99621, 'preemption_count': 0}), (99935, {'train/accuracy': 0.5286790132522583, 'train/loss': 2.0169661045074463, 'validation/accuracy': 0.48673999309539795, 'validation/loss': 2.246558427810669, 'validation/num_examples': 50000, 'test/accuracy': 0.3815000057220459, 'test/loss': 2.918832540512085, 'test/num_examples': 10000, 'score': 51595.76218628883, 'total_duration': 55264.71746754646, 'accumulated_submission_time': 51595.76218628883, 'accumulated_eval_time': 3647.6547033786774, 'accumulated_logging_time': 11.165179967880249, 'global_step': 99935, 'preemption_count': 0}), (100558, {'train/accuracy': 0.45621412992477417, 'train/loss': 2.3994035720825195, 'validation/accuracy': 0.4229399859905243, 'validation/loss': 2.616429567337036, 'validation/num_examples': 50000, 'test/accuracy': 0.3193000257015228, 'test/loss': 3.3530325889587402, 'test/num_examples': 10000, 'score': 52106.0928940773, 'total_duration': 55806.64876008034, 'accumulated_submission_time': 52106.0928940773, 'accumulated_eval_time': 3679.138229370117, 'accumulated_logging_time': 11.218993902206421, 'global_step': 100558, 'preemption_count': 0}), (101344, {'train/accuracy': 0.3787866532802582, 'train/loss': 2.9305360317230225, 'validation/accuracy': 0.3491799831390381, 'validation/loss': 3.1213467121124268, 'validation/num_examples': 50000, 'test/accuracy': 0.2630999982357025, 'test/loss': 3.8277428150177, 'test/num_examples': 10000, 'score': 52616.40606665611, 'total_duration': 56348.085983514786, 'accumulated_submission_time': 52616.40606665611, 'accumulated_eval_time': 3710.124653339386, 'accumulated_logging_time': 11.277005195617676, 'global_step': 101344, 'preemption_count': 0}), (102133, {'train/accuracy': 0.4890584945678711, 'train/loss': 2.236358642578125, 'validation/accuracy': 0.4620800018310547, 'validation/loss': 2.4164700508117676, 'validation/num_examples': 50000, 'test/accuracy': 0.33980002999305725, 'test/loss': 3.2522990703582764, 'test/num_examples': 10000, 'score': 53126.82693171501, 'total_duration': 56897.11634063721, 'accumulated_submission_time': 53126.82693171501, 'accumulated_eval_time': 3748.613242149353, 'accumulated_logging_time': 11.318917751312256, 'global_step': 102133, 'preemption_count': 0}), (102620, {'train/accuracy': 0.3781489133834839, 'train/loss': 3.062340497970581, 'validation/accuracy': 0.3501800000667572, 'validation/loss': 3.2510364055633545, 'validation/num_examples': 50000, 'test/accuracy': 0.2632000148296356, 'test/loss': 4.07974910736084, 'test/num_examples': 10000, 'score': 53637.658563137054, 'total_duration': 57439.217901945114, 'accumulated_submission_time': 53637.658563137054, 'accumulated_eval_time': 3779.7251873016357, 'accumulated_logging_time': 11.429038524627686, 'global_step': 102620, 'preemption_count': 0}), (102859, {'train/accuracy': 0.49683114886283875, 'train/loss': 2.1844348907470703, 'validation/accuracy': 0.4382999837398529, 'validation/loss': 2.5294387340545654, 'validation/num_examples': 50000, 'test/accuracy': 0.3286000192165375, 'test/loss': 3.3371737003326416, 'test/num_examples': 10000, 'score': 54151.823229551315, 'total_duration': 57987.799485206604, 'accumulated_submission_time': 54151.823229551315, 'accumulated_eval_time': 3814.025567293167, 'accumulated_logging_time': 11.522158145904541, 'global_step': 102859, 'preemption_count': 0}), (103004, {'train/accuracy': 0.5252710580825806, 'train/loss': 2.0207221508026123, 'validation/accuracy': 0.48225998878479004, 'validation/loss': 2.2848520278930664, 'validation/num_examples': 50000, 'test/accuracy': 0.3782000243663788, 'test/loss': 3.0130953788757324, 'test/num_examples': 10000, 'score': 54662.929846286774, 'total_duration': 58528.94349980354, 'accumulated_submission_time': 54662.929846286774, 'accumulated_eval_time': 3844.022253513336, 'accumulated_logging_time': 11.548789978027344, 'global_step': 103004, 'preemption_count': 0}), (103253, {'train/accuracy': 0.4022839665412903, 'train/loss': 2.8692519664764404, 'validation/accuracy': 0.3688800036907196, 'validation/loss': 3.0915398597717285, 'validation/num_examples': 50000, 'test/accuracy': 0.2703000009059906, 'test/loss': 3.864097833633423, 'test/num_examples': 10000, 'score': 55174.65123987198, 'total_duration': 59073.22983765602, 'accumulated_submission_time': 55174.65123987198, 'accumulated_eval_time': 3876.5336968898773, 'accumulated_logging_time': 11.576876163482666, 'global_step': 103253, 'preemption_count': 0}), (103567, {'train/accuracy': 0.43405213952064514, 'train/loss': 2.575512409210205, 'validation/accuracy': 0.4001599848270416, 'validation/loss': 2.821866273880005, 'validation/num_examples': 50000, 'test/accuracy': 0.313400000333786, 'test/loss': 3.5424423217773438, 'test/num_examples': 10000, 'score': 55685.53236722946, 'total_duration': 59613.893795251846, 'accumulated_submission_time': 55685.53236722946, 'accumulated_eval_time': 3906.2560238838196, 'accumulated_logging_time': 11.606302976608276, 'global_step': 103567, 'preemption_count': 0}), (104042, {'train/accuracy': 0.402363657951355, 'train/loss': 2.842728614807129, 'validation/accuracy': 0.37845999002456665, 'validation/loss': 3.0030181407928467, 'validation/num_examples': 50000, 'test/accuracy': 0.2802000045776367, 'test/loss': 3.836970806121826, 'test/num_examples': 10000, 'score': 56196.19255423546, 'total_duration': 60156.82395243645, 'accumulated_submission_time': 56196.19255423546, 'accumulated_eval_time': 3938.407937526703, 'accumulated_logging_time': 11.675800800323486, 'global_step': 104042, 'preemption_count': 0}), (104213, {'train/accuracy': 0.49565526843070984, 'train/loss': 2.2708935737609863, 'validation/accuracy': 0.4642599821090698, 'validation/loss': 2.4458322525024414, 'validation/num_examples': 50000, 'test/accuracy': 0.3509000241756439, 'test/loss': 3.3124797344207764, 'test/num_examples': 10000, 'score': 56707.75966858864, 'total_duration': 60701.50550246239, 'accumulated_submission_time': 56707.75966858864, 'accumulated_eval_time': 3971.4354889392853, 'accumulated_logging_time': 11.74555253982544, 'global_step': 104213, 'preemption_count': 0}), (104339, {'train/accuracy': 0.47604432702064514, 'train/loss': 2.316443681716919, 'validation/accuracy': 0.4379599988460541, 'validation/loss': 2.608400821685791, 'validation/num_examples': 50000, 'test/accuracy': 0.3419000208377838, 'test/loss': 3.3561463356018066, 'test/num_examples': 10000, 'score': 57220.905958652496, 'total_duration': 61246.84115743637, 'accumulated_submission_time': 57220.905958652496, 'accumulated_eval_time': 4003.5817382335663, 'accumulated_logging_time': 11.776703357696533, 'global_step': 104339, 'preemption_count': 0}), (104464, {'train/accuracy': 0.45184949040412903, 'train/loss': 2.413158655166626, 'validation/accuracy': 0.4070200026035309, 'validation/loss': 2.7595162391662598, 'validation/num_examples': 50000, 'test/accuracy': 0.3068000078201294, 'test/loss': 3.546330213546753, 'test/num_examples': 10000, 'score': 57731.295924663544, 'total_duration': 61790.819328308105, 'accumulated_submission_time': 57731.295924663544, 'accumulated_eval_time': 4037.1305060386658, 'accumulated_logging_time': 11.80481219291687, 'global_step': 104464, 'preemption_count': 0}), (104589, {'train/accuracy': 0.45436063408851624, 'train/loss': 2.4469242095947266, 'validation/accuracy': 0.4161199927330017, 'validation/loss': 2.7373082637786865, 'validation/num_examples': 50000, 'test/accuracy': 0.30470001697540283, 'test/loss': 3.6589841842651367, 'test/num_examples': 10000, 'score': 58241.68999147415, 'total_duration': 62333.01205277443, 'accumulated_submission_time': 58241.68999147415, 'accumulated_eval_time': 4068.8895003795624, 'accumulated_logging_time': 11.832237243652344, 'global_step': 104589, 'preemption_count': 0}), (104767, {'train/accuracy': 0.29326769709587097, 'train/loss': 3.776369571685791, 'validation/accuracy': 0.26848000288009644, 'validation/loss': 3.9485995769500732, 'validation/num_examples': 50000, 'test/accuracy': 0.1924000084400177, 'test/loss': 4.802672863006592, 'test/num_examples': 10000, 'score': 58752.85178112984, 'total_duration': 62876.198471069336, 'accumulated_submission_time': 58752.85178112984, 'accumulated_eval_time': 4100.86829161644, 'accumulated_logging_time': 11.860480546951294, 'global_step': 104767, 'preemption_count': 0}), (105008, {'train/accuracy': 0.4742107689380646, 'train/loss': 2.3242995738983154, 'validation/accuracy': 0.44703999161720276, 'validation/loss': 2.5240776538848877, 'validation/num_examples': 50000, 'test/accuracy': 0.3303000032901764, 'test/loss': 3.39839243888855, 'test/num_examples': 10000, 'score': 59264.650086164474, 'total_duration': 63419.32753610611, 'accumulated_submission_time': 59264.650086164474, 'accumulated_eval_time': 4132.137366771698, 'accumulated_logging_time': 11.898420095443726, 'global_step': 105008, 'preemption_count': 0}), (105257, {'train/accuracy': 0.4795519709587097, 'train/loss': 2.3220150470733643, 'validation/accuracy': 0.4465799927711487, 'validation/loss': 2.5077991485595703, 'validation/num_examples': 50000, 'test/accuracy': 0.34950003027915955, 'test/loss': 3.1804964542388916, 'test/num_examples': 10000, 'score': 59776.257386446, 'total_duration': 63962.51886796951, 'accumulated_submission_time': 59776.257386446, 'accumulated_eval_time': 4163.669984817505, 'accumulated_logging_time': 11.9269540309906, 'global_step': 105257, 'preemption_count': 0}), (105508, {'train/accuracy': 0.30115991830825806, 'train/loss': 3.639763593673706, 'validation/accuracy': 0.2847200036048889, 'validation/loss': 3.7651751041412354, 'validation/num_examples': 50000, 'test/accuracy': 0.1980000138282776, 'test/loss': 4.674007415771484, 'test/num_examples': 10000, 'score': 60286.67100572586, 'total_duration': 64505.58929228783, 'accumulated_submission_time': 60286.67100572586, 'accumulated_eval_time': 4196.258621931076, 'accumulated_logging_time': 11.97074007987976, 'global_step': 105508, 'preemption_count': 0}), (105759, {'train/accuracy': 0.4709223508834839, 'train/loss': 2.3817429542541504, 'validation/accuracy': 0.4405999779701233, 'validation/loss': 2.5922012329101562, 'validation/num_examples': 50000, 'test/accuracy': 0.34310001134872437, 'test/loss': 3.3202974796295166, 'test/num_examples': 10000, 'score': 60797.19760775566, 'total_duration': 65048.18206739426, 'accumulated_submission_time': 60797.19760775566, 'accumulated_eval_time': 4228.21732378006, 'accumulated_logging_time': 12.05405592918396, 'global_step': 105759, 'preemption_count': 0}), (106037, {'train/accuracy': 0.4760642349720001, 'train/loss': 2.329906702041626, 'validation/accuracy': 0.424919992685318, 'validation/loss': 2.6766209602355957, 'validation/num_examples': 50000, 'test/accuracy': 0.3246000111103058, 'test/loss': 3.507951259613037, 'test/num_examples': 10000, 'score': 61307.57697224617, 'total_duration': 65590.63140892982, 'accumulated_submission_time': 61307.57697224617, 'accumulated_eval_time': 4260.206381559372, 'accumulated_logging_time': 12.109410047531128, 'global_step': 106037, 'preemption_count': 0}), (106402, {'train/accuracy': 0.4287906587123871, 'train/loss': 2.6931350231170654, 'validation/accuracy': 0.40595999360084534, 'validation/loss': 2.883967399597168, 'validation/num_examples': 50000, 'test/accuracy': 0.29520002007484436, 'test/loss': 3.810413122177124, 'test/num_examples': 10000, 'score': 61817.69603538513, 'total_duration': 66132.82690024376, 'accumulated_submission_time': 61817.69603538513, 'accumulated_eval_time': 4292.140942335129, 'accumulated_logging_time': 12.21591854095459, 'global_step': 106402, 'preemption_count': 0}), (106768, {'train/accuracy': 0.4228515625, 'train/loss': 2.7113466262817383, 'validation/accuracy': 0.3920799791812897, 'validation/loss': 2.909982204437256, 'validation/num_examples': 50000, 'test/accuracy': 0.30000001192092896, 'test/loss': 3.680807590484619, 'test/num_examples': 10000, 'score': 62328.346529483795, 'total_duration': 66674.2037396431, 'accumulated_submission_time': 62328.346529483795, 'accumulated_eval_time': 4322.783039569855, 'accumulated_logging_time': 12.264996528625488, 'global_step': 106768, 'preemption_count': 0}), (107133, {'train/accuracy': 0.4027024805545807, 'train/loss': 2.8465893268585205, 'validation/accuracy': 0.3800399899482727, 'validation/loss': 3.021078586578369, 'validation/num_examples': 50000, 'test/accuracy': 0.2847000062465668, 'test/loss': 3.833341598510742, 'test/num_examples': 10000, 'score': 62839.21178007126, 'total_duration': 67215.6504266262, 'accumulated_submission_time': 62839.21178007126, 'accumulated_eval_time': 4353.284695148468, 'accumulated_logging_time': 12.310251951217651, 'global_step': 107133, 'preemption_count': 0}), (107590, {'train/accuracy': 0.5300940871238708, 'train/loss': 1.9932917356491089, 'validation/accuracy': 0.4852199852466583, 'validation/loss': 2.2798359394073486, 'validation/num_examples': 50000, 'test/accuracy': 0.367900013923645, 'test/loss': 3.096484422683716, 'test/num_examples': 10000, 'score': 63349.440811634064, 'total_duration': 67756.79685592651, 'accumulated_submission_time': 63349.440811634064, 'accumulated_eval_time': 4384.086124897003, 'accumulated_logging_time': 12.380398273468018, 'global_step': 107590, 'preemption_count': 0}), (108079, {'train/accuracy': 0.31851881742477417, 'train/loss': 3.447589159011841, 'validation/accuracy': 0.2963799834251404, 'validation/loss': 3.582941770553589, 'validation/num_examples': 50000, 'test/accuracy': 0.21620000898838043, 'test/loss': 4.323112487792969, 'test/num_examples': 10000, 'score': 63858.88493680954, 'total_duration': 68298.9111161232, 'accumulated_submission_time': 63858.88493680954, 'accumulated_eval_time': 4415.542486667633, 'accumulated_logging_time': 13.546090126037598, 'global_step': 108079, 'preemption_count': 0}), (108567, {'train/accuracy': 0.5208466053009033, 'train/loss': 2.0815060138702393, 'validation/accuracy': 0.4865799844264984, 'validation/loss': 2.298184871673584, 'validation/num_examples': 50000, 'test/accuracy': 0.3669000267982483, 'test/loss': 3.120516777038574, 'test/num_examples': 10000, 'score': 64369.774394750595, 'total_duration': 68838.93217086792, 'accumulated_submission_time': 64369.774394750595, 'accumulated_eval_time': 4444.572045087814, 'accumulated_logging_time': 13.598825693130493, 'global_step': 108567, 'preemption_count': 0}), (109057, {'train/accuracy': 0.4429607689380646, 'train/loss': 2.5519204139709473, 'validation/accuracy': 0.4018999934196472, 'validation/loss': 2.8337395191192627, 'validation/num_examples': 50000, 'test/accuracy': 0.30790001153945923, 'test/loss': 3.607595920562744, 'test/num_examples': 10000, 'score': 64880.013041973114, 'total_duration': 69379.6287984848, 'accumulated_submission_time': 64880.013041973114, 'accumulated_eval_time': 4474.893383741379, 'accumulated_logging_time': 13.686125040054321, 'global_step': 109057, 'preemption_count': 0}), (109546, {'train/accuracy': 0.48519212007522583, 'train/loss': 2.2884650230407715, 'validation/accuracy': 0.45701998472213745, 'validation/loss': 2.473346710205078, 'validation/num_examples': 50000, 'test/accuracy': 0.3483000099658966, 'test/loss': 3.234506845474243, 'test/num_examples': 10000, 'score': 65390.51763892174, 'total_duration': 69921.35412359238, 'accumulated_submission_time': 65390.51763892174, 'accumulated_eval_time': 4506.014676809311, 'accumulated_logging_time': 13.736923217773438, 'global_step': 109546, 'preemption_count': 0}), (110036, {'train/accuracy': 0.5681600570678711, 'train/loss': 1.824359655380249, 'validation/accuracy': 0.5266199707984924, 'validation/loss': 2.049992799758911, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.771702766418457, 'test/num_examples': 10000, 'score': 65901.06157374382, 'total_duration': 70462.20488548279, 'accumulated_submission_time': 65901.06157374382, 'accumulated_eval_time': 4536.2041301727295, 'accumulated_logging_time': 13.804651975631714, 'global_step': 110036, 'preemption_count': 0})], 'global_step': 110526}
I0307 21:45:29.721305 139912818214080 submission_runner.py:649] Timing: 66411.74748253822
I0307 21:45:29.721350 139912818214080 submission_runner.py:651] Total number of evals: 130
I0307 21:45:29.721379 139912818214080 submission_runner.py:652] ====================
I0307 21:45:29.721575 139912818214080 submission_runner.py:750] Final imagenet_resnet score: 3
