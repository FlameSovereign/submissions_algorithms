python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=1619809009 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-36.log
2025-03-05 19:12:37.073821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201957.100941       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201957.108171       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:43.916252 139728945038528 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax.
I0305 19:12:44.869689 139728945038528 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:44.872887 139728945038528 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:44.874687 139728945038528 submission_runner.py:606] Using RNG seed 1619809009
I0305 19:12:45.472731 139728945038528 submission_runner.py:615] --- Tuning run 2/5 ---
I0305 19:12:45.472904 139728945038528 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_2.
I0305 19:12:45.473079 139728945038528 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_2/hparams.json.
I0305 19:12:45.706687 139728945038528 submission_runner.py:218] Initializing dataset.
I0305 19:12:45.873340 139728945038528 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:45.879809 139728945038528 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:45.953714 139728945038528 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:47.284933 139728945038528 submission_runner.py:229] Initializing model.
I0305 19:13:31.538117 139728945038528 submission_runner.py:272] Initializing optimizer.
I0305 19:13:32.424267 139728945038528 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:32.424521 139728945038528 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:32.425577 139728945038528 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_2 with prefix checkpoint_
I0305 19:13:32.425688 139728945038528 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_2/meta_data_0.json.
I0305 19:13:32.425853 139728945038528 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:32.425913 139728945038528 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:32.609087 139728945038528 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_2/flags_0.json.
I0305 19:13:32.645137 139728945038528 submission_runner.py:337] Starting training loop.
I0305 19:13:59.287515 139592669370112 logging_writer.py:48] [0] global_step=0, grad_norm=5.497297763824463, loss=11.10626220703125
I0305 19:13:59.351161 139728945038528 spec.py:321] Evaluating on the training split.
I0305 19:13:59.353711 139728945038528 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:59.357748 139728945038528 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:59.391256 139728945038528 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:05.517879 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 19:19:09.959716 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 19:19:09.986016 139728945038528 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:10.019366 139728945038528 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:10.052308 139728945038528 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:15.257131 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 19:24:13.552038 139728945038528 spec.py:349] Evaluating on the test split.
I0305 19:24:13.554734 139728945038528 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:13.558837 139728945038528 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:13.591519 139728945038528 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:16.330128 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 19:29:14.437064 139728945038528 submission_runner.py:469] Time since start: 941.79s, 	Step: 1, 	{'train/accuracy': 0.0006014798418618739, 'train/loss': 11.105691909790039, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.148694038391113, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.146646499633789, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.70587730407715, 'total_duration': 941.7918515205383, 'accumulated_submission_time': 26.70587730407715, 'accumulated_eval_time': 915.0858404636383, 'accumulated_logging_time': 0}
I0305 19:29:14.444587 139586135078656 logging_writer.py:48] [1] accumulated_eval_time=915.086, accumulated_logging_time=0, accumulated_submission_time=26.7059, global_step=1, preemption_count=0, score=26.7059, test/accuracy=0.000718341, test/bleu=0, test/loss=11.1466, test/num_examples=3003, total_duration=941.792, train/accuracy=0.00060148, train/bleu=0, train/loss=11.1057, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.1487, validation/num_examples=3000
I0305 19:29:48.975297 139586126685952 logging_writer.py:48] [100] global_step=100, grad_norm=0.25003328919410706, loss=9.006104469299316
I0305 19:30:23.450313 139586135078656 logging_writer.py:48] [200] global_step=200, grad_norm=0.23367947340011597, loss=8.663529396057129
I0305 19:30:57.960918 139586126685952 logging_writer.py:48] [300] global_step=300, grad_norm=0.6855263710021973, loss=8.276155471801758
I0305 19:31:32.507904 139586135078656 logging_writer.py:48] [400] global_step=400, grad_norm=0.7233957052230835, loss=7.986793518066406
I0305 19:32:07.100332 139586126685952 logging_writer.py:48] [500] global_step=500, grad_norm=0.44658219814300537, loss=7.720769882202148
I0305 19:32:41.670197 139586135078656 logging_writer.py:48] [600] global_step=600, grad_norm=0.8014897108078003, loss=7.609440326690674
I0305 19:33:16.267993 139586126685952 logging_writer.py:48] [700] global_step=700, grad_norm=0.9160163402557373, loss=7.432421684265137
I0305 19:33:50.802644 139586135078656 logging_writer.py:48] [800] global_step=800, grad_norm=0.6407467722892761, loss=7.153822898864746
I0305 19:34:25.342669 139586126685952 logging_writer.py:48] [900] global_step=900, grad_norm=0.6057806015014648, loss=7.023602485656738
I0305 19:34:59.834581 139586135078656 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5569967031478882, loss=6.913290500640869
I0305 19:35:34.370584 139586126685952 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6520048975944519, loss=6.7791948318481445
I0305 19:36:08.906914 139586135078656 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.844960629940033, loss=6.643844127655029
I0305 19:36:43.507782 139586126685952 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5427057147026062, loss=6.491842746734619
I0305 19:37:18.037258 139586135078656 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5153172016143799, loss=6.429331302642822
I0305 19:37:52.602810 139586126685952 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6119392514228821, loss=6.308252334594727
I0305 19:38:27.175490 139586135078656 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7688025832176208, loss=6.2389750480651855
I0305 19:39:01.710791 139586126685952 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5966526865959167, loss=6.139710903167725
I0305 19:39:36.276248 139586135078656 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6805282831192017, loss=6.070979595184326
I0305 19:40:10.865975 139586126685952 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5801745057106018, loss=6.000800132751465
I0305 19:40:45.458269 139586135078656 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.643957257270813, loss=5.852223873138428
I0305 19:41:20.045825 139586126685952 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6259230375289917, loss=5.789207458496094
I0305 19:41:54.635500 139586135078656 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6923004388809204, loss=5.6833038330078125
I0305 19:42:29.220158 139586126685952 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7937275767326355, loss=5.634119510650635
I0305 19:43:03.852476 139586135078656 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5697010159492493, loss=5.509006023406982
I0305 19:43:14.570651 139728945038528 spec.py:321] Evaluating on the training split.
I0305 19:43:17.158976 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 19:46:04.851604 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 19:46:07.435500 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 19:48:53.714762 139728945038528 spec.py:349] Evaluating on the test split.
I0305 19:48:56.296795 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 19:51:35.458807 139728945038528 submission_runner.py:469] Time since start: 2282.81s, 	Step: 2432, 	{'train/accuracy': 0.4331774413585663, 'train/loss': 3.8138489723205566, 'train/bleu': 13.576943366928587, 'validation/accuracy': 0.42132845520973206, 'validation/loss': 3.9137299060821533, 'validation/bleu': 10.100368024043531, 'validation/num_examples': 3000, 'test/accuracy': 0.40939635038375854, 'test/loss': 4.084228515625, 'test/bleu': 8.340447735070596, 'test/num_examples': 3003, 'score': 866.6493525505066, 'total_duration': 2282.8135888576508, 'accumulated_submission_time': 866.6493525505066, 'accumulated_eval_time': 1415.9739322662354, 'accumulated_logging_time': 0.01611948013305664}
I0305 19:51:35.468549 139586126685952 logging_writer.py:48] [2432] accumulated_eval_time=1415.97, accumulated_logging_time=0.0161195, accumulated_submission_time=866.649, global_step=2432, preemption_count=0, score=866.649, test/accuracy=0.409396, test/bleu=8.34045, test/loss=4.08423, test/num_examples=3003, total_duration=2282.81, train/accuracy=0.433177, train/bleu=13.5769, train/loss=3.81385, validation/accuracy=0.421328, validation/bleu=10.1004, validation/loss=3.91373, validation/num_examples=3000
I0305 19:51:59.112867 139586135078656 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5464414954185486, loss=5.443313121795654
I0305 19:52:33.554932 139586126685952 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6971393823623657, loss=5.362107276916504
I0305 19:53:08.080696 139586135078656 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4704749286174774, loss=5.307438373565674
I0305 19:53:42.639865 139586126685952 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.4900106191635132, loss=5.179917812347412
I0305 19:54:17.232238 139586135078656 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5651896595954895, loss=5.195614814758301
I0305 19:54:51.796434 139586126685952 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.4371740221977234, loss=5.189281940460205
I0305 19:55:26.415960 139586135078656 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6004578471183777, loss=5.190476417541504
I0305 19:56:01.005661 139586126685952 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5748618245124817, loss=5.084755897521973
I0305 19:56:35.617297 139586135078656 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.4273185431957245, loss=5.063498020172119
I0305 19:57:10.179207 139586126685952 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6260465383529663, loss=4.994102478027344
I0305 19:57:44.821196 139586135078656 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.45295965671539307, loss=4.969797134399414
I0305 19:58:19.397110 139586126685952 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.40993794798851013, loss=4.894265651702881
I0305 19:58:53.966360 139586135078656 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.44447386264801025, loss=4.890649795532227
I0305 19:59:28.594287 139586126685952 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.41271278262138367, loss=4.796254634857178
I0305 20:00:03.189288 139586135078656 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.43887919187545776, loss=4.859179496765137
I0305 20:00:37.788574 139586126685952 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.4062875807285309, loss=4.874273300170898
I0305 20:01:12.376343 139586135078656 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.37103062868118286, loss=4.792663097381592
I0305 20:01:46.970049 139586126685952 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.40536803007125854, loss=4.807926177978516
I0305 20:02:21.554729 139586135078656 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3751794695854187, loss=4.733066558837891
I0305 20:02:56.125153 139586126685952 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.4290317893028259, loss=4.730892658233643
I0305 20:03:30.731733 139586135078656 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.4682554304599762, loss=4.807217121124268
I0305 20:04:05.335849 139586126685952 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3422897756099701, loss=4.7470855712890625
I0305 20:04:39.908082 139586135078656 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.3318866789340973, loss=4.693653106689453
I0305 20:05:14.489454 139586126685952 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.38167139887809753, loss=4.690240383148193
I0305 20:05:35.564648 139728945038528 spec.py:321] Evaluating on the training split.
I0305 20:05:38.150055 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:08:29.049185 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 20:08:31.629725 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:11:29.605781 139728945038528 spec.py:349] Evaluating on the test split.
I0305 20:11:32.173152 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:14:07.527491 139728945038528 submission_runner.py:469] Time since start: 3634.88s, 	Step: 4862, 	{'train/accuracy': 0.5495039820671082, 'train/loss': 2.786299228668213, 'train/bleu': 24.33146164553042, 'validation/accuracy': 0.5557931661605835, 'validation/loss': 2.7292561531066895, 'validation/bleu': 20.817781616158218, 'validation/num_examples': 3000, 'test/accuracy': 0.5562855005264282, 'test/loss': 2.7564375400543213, 'test/bleu': 19.679039332551106, 'test/num_examples': 3003, 'score': 1706.5789091587067, 'total_duration': 3634.882288455963, 'accumulated_submission_time': 1706.5789091587067, 'accumulated_eval_time': 1927.9367167949677, 'accumulated_logging_time': 0.03448677062988281}
I0305 20:14:07.537441 139586135078656 logging_writer.py:48] [4862] accumulated_eval_time=1927.94, accumulated_logging_time=0.0344868, accumulated_submission_time=1706.58, global_step=4862, preemption_count=0, score=1706.58, test/accuracy=0.556286, test/bleu=19.679, test/loss=2.75644, test/num_examples=3003, total_duration=3634.88, train/accuracy=0.549504, train/bleu=24.3315, train/loss=2.7863, validation/accuracy=0.555793, validation/bleu=20.8178, validation/loss=2.72926, validation/num_examples=3000
I0305 20:14:20.962722 139586126685952 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3255676329135895, loss=4.680585861206055
I0305 20:14:55.435249 139586135078656 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.3759070038795471, loss=4.636605739593506
I0305 20:15:29.966193 139586126685952 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3511996567249298, loss=4.7498040199279785
I0305 20:16:04.547536 139586135078656 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.33683285117149353, loss=4.631525039672852
I0305 20:16:39.154788 139586126685952 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.29653507471084595, loss=4.651587009429932
I0305 20:17:13.741285 139586135078656 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.3437773883342743, loss=4.6079792976379395
I0305 20:17:48.328049 139586126685952 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.32876312732696533, loss=4.558083534240723
I0305 20:18:22.930706 139586135078656 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.31547480821609497, loss=4.55109167098999
I0305 20:18:57.550629 139586126685952 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.2812109887599945, loss=4.479055404663086
I0305 20:19:32.109988 139586135078656 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.30529433488845825, loss=4.532756328582764
I0305 20:20:06.687756 139586126685952 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.3134213387966156, loss=4.509823322296143
I0305 20:20:41.208750 139586135078656 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.2744675278663635, loss=4.518533706665039
I0305 20:21:15.748494 139586126685952 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.2807122468948364, loss=4.522360801696777
I0305 20:21:50.330538 139586135078656 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.26107707619667053, loss=4.5361714363098145
I0305 20:22:24.846755 139585807927040 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.27696889638900757, loss=4.55767297744751
I0305 20:22:59.244213 139585799534336 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.25741565227508545, loss=4.457085609436035
I0305 20:23:33.645617 139585807927040 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.24144122004508972, loss=4.52121114730835
I0305 20:24:08.081234 139585799534336 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2501634955406189, loss=4.455667495727539
I0305 20:24:42.530223 139585807927040 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.27445709705352783, loss=4.468713760375977
I0305 20:25:17.021621 139585799534336 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.2569260001182556, loss=4.479036331176758
I0305 20:25:51.477460 139585807927040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.2722439467906952, loss=4.399615287780762
I0305 20:26:25.933077 139585799534336 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.23271222412586212, loss=4.457679271697998
I0305 20:27:00.405070 139585807927040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.21695120632648468, loss=4.463278770446777
I0305 20:27:34.894797 139585799534336 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.2291496843099594, loss=4.394291877746582
I0305 20:28:07.643317 139728945038528 spec.py:321] Evaluating on the training split.
I0305 20:28:10.236376 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:30:47.534383 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 20:30:50.101567 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:33:37.843060 139728945038528 spec.py:349] Evaluating on the test split.
I0305 20:33:40.410066 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:36:07.228416 139728945038528 submission_runner.py:469] Time since start: 4954.58s, 	Step: 7296, 	{'train/accuracy': 0.591151237487793, 'train/loss': 2.3724465370178223, 'train/bleu': 27.554069144902588, 'validation/accuracy': 0.5970261693000793, 'validation/loss': 2.31310772895813, 'validation/bleu': 23.6990810584944, 'validation/num_examples': 3000, 'test/accuracy': 0.603441059589386, 'test/loss': 2.292757511138916, 'test/bleu': 22.587176736752028, 'test/num_examples': 3003, 'score': 2546.517216682434, 'total_duration': 4954.5831990242, 'accumulated_submission_time': 2546.517216682434, 'accumulated_eval_time': 2407.5217504501343, 'accumulated_logging_time': 0.05305743217468262}
I0305 20:36:07.238942 139585807927040 logging_writer.py:48] [7296] accumulated_eval_time=2407.52, accumulated_logging_time=0.0530574, accumulated_submission_time=2546.52, global_step=7296, preemption_count=0, score=2546.52, test/accuracy=0.603441, test/bleu=22.5872, test/loss=2.29276, test/num_examples=3003, total_duration=4954.58, train/accuracy=0.591151, train/bleu=27.5541, train/loss=2.37245, validation/accuracy=0.597026, validation/bleu=23.6991, validation/loss=2.31311, validation/num_examples=3000
I0305 20:36:08.969087 139585799534336 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.21363011002540588, loss=4.41987943649292
I0305 20:36:43.255151 139585807927040 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.24584974348545074, loss=4.415895462036133
I0305 20:37:17.639185 139585799534336 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.26834166049957275, loss=4.381600856781006
I0305 20:37:52.099957 139585807927040 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.21945278346538544, loss=4.293478012084961
I0305 20:38:26.575398 139585799534336 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.23631519079208374, loss=4.325231552124023
I0305 20:39:01.047845 139585807927040 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.2244616448879242, loss=4.291228771209717
I0305 20:39:35.551106 139585799534336 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.23670637607574463, loss=4.358130931854248
I0305 20:40:10.008140 139585807927040 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.24158579111099243, loss=4.413845539093018
I0305 20:40:44.473004 139585799534336 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.2271415889263153, loss=4.295412540435791
I0305 20:41:18.966153 139585807927040 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.2045488804578781, loss=4.389414310455322
I0305 20:41:53.443170 139585799534336 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.19751591980457306, loss=4.243329048156738
I0305 20:42:27.900389 139585807927040 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.232322558760643, loss=4.361377716064453
I0305 20:43:02.333648 139585799534336 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.2596137225627899, loss=4.296310901641846
I0305 20:43:36.780963 139585807927040 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.2072419822216034, loss=4.358803749084473
I0305 20:44:11.247635 139585799534336 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.19333525002002716, loss=4.349814414978027
I0305 20:44:45.724074 139585807927040 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.18730756640434265, loss=4.250156402587891
I0305 20:45:20.205641 139585799534336 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.19133810698986053, loss=4.278653144836426
I0305 20:45:54.765714 139585807927040 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.20547600090503693, loss=4.307911396026611
I0305 20:46:29.254944 139585799534336 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.18489781022071838, loss=4.321652889251709
I0305 20:47:03.724253 139585807927040 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.21099814772605896, loss=4.2640228271484375
I0305 20:47:38.198871 139585799534336 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.18664303421974182, loss=4.269843101501465
I0305 20:48:12.690671 139585807927040 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.19008681178092957, loss=4.257264137268066
I0305 20:48:47.180030 139585799534336 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.18397559225559235, loss=4.270798683166504
I0305 20:49:21.626549 139585807927040 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.2090853601694107, loss=4.24650764465332
I0305 20:49:56.062816 139585799534336 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.2031247615814209, loss=4.257835388183594
I0305 20:50:07.426135 139728945038528 spec.py:321] Evaluating on the training split.
I0305 20:50:10.009070 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:52:50.379275 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 20:52:52.945163 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:55:34.601715 139728945038528 spec.py:349] Evaluating on the test split.
I0305 20:55:37.176452 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 20:58:04.357769 139728945038528 submission_runner.py:469] Time since start: 6271.71s, 	Step: 9734, 	{'train/accuracy': 0.606626033782959, 'train/loss': 2.255824327468872, 'train/bleu': 28.55987648512636, 'validation/accuracy': 0.6215237379074097, 'validation/loss': 2.139256000518799, 'validation/bleu': 25.231698333907953, 'validation/num_examples': 3000, 'test/accuracy': 0.629845917224884, 'test/loss': 2.0967488288879395, 'test/bleu': 24.160584648951673, 'test/num_examples': 3003, 'score': 3386.540834903717, 'total_duration': 6271.71257686615, 'accumulated_submission_time': 3386.540834903717, 'accumulated_eval_time': 2884.4533355236053, 'accumulated_logging_time': 0.07225346565246582}
I0305 20:58:04.367522 139585807927040 logging_writer.py:48] [9734] accumulated_eval_time=2884.45, accumulated_logging_time=0.0722535, accumulated_submission_time=3386.54, global_step=9734, preemption_count=0, score=3386.54, test/accuracy=0.629846, test/bleu=24.1606, test/loss=2.09675, test/num_examples=3003, total_duration=6271.71, train/accuracy=0.606626, train/bleu=28.5599, train/loss=2.25582, validation/accuracy=0.621524, validation/bleu=25.2317, validation/loss=2.13926, validation/num_examples=3000
I0305 20:58:27.354497 139585799534336 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.20052701234817505, loss=4.250672340393066
I0305 20:59:01.623209 139585807927040 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.18820159137248993, loss=4.295013904571533
I0305 20:59:36.006295 139585799534336 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.17271235585212708, loss=4.188350677490234
I0305 21:00:10.492988 139585807927040 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.18319185078144073, loss=4.256039619445801
I0305 21:00:44.874583 139585799534336 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.1802041083574295, loss=4.268782615661621
I0305 21:01:19.313274 139585807927040 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.16910669207572937, loss=4.232746601104736
I0305 21:01:53.776303 139585799534336 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18869444727897644, loss=4.221468925476074
I0305 21:02:28.224800 139585807927040 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.1792018860578537, loss=4.25949764251709
I0305 21:03:02.689315 139585799534336 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.19211317598819733, loss=4.188982963562012
I0305 21:03:37.177130 139585807927040 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.21057631075382233, loss=4.194422721862793
I0305 21:04:11.612326 139585799534336 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.17431195080280304, loss=4.246809959411621
I0305 21:04:46.081457 139585807927040 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1742120236158371, loss=4.245273113250732
I0305 21:05:20.576408 139585799534336 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.17290565371513367, loss=4.172662258148193
I0305 21:05:55.055616 139585807927040 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.18242640793323517, loss=4.222793102264404
I0305 21:06:29.544090 139585799534336 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.17482760548591614, loss=4.2552971839904785
I0305 21:07:03.997020 139585807927040 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1938668191432953, loss=4.2006516456604
I0305 21:07:38.429011 139585799534336 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.174796462059021, loss=4.207406044006348
I0305 21:08:12.848468 139585807927040 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17218798398971558, loss=4.184095859527588
I0305 21:08:47.286945 139585799534336 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.16889245808124542, loss=4.164912700653076
I0305 21:09:21.720063 139585807927040 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.16674527525901794, loss=4.200320243835449
I0305 21:09:56.170859 139585799534336 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.19746635854244232, loss=4.214442729949951
I0305 21:10:30.588307 139585807927040 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.17249642312526703, loss=4.2370171546936035
I0305 21:11:05.039911 139585799534336 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.16605456173419952, loss=4.160838603973389
I0305 21:11:39.512112 139585807927040 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.17752930521965027, loss=4.148608684539795
I0305 21:12:04.662089 139728945038528 spec.py:321] Evaluating on the training split.
I0305 21:12:07.238427 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 21:15:34.545030 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 21:15:37.123898 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 21:18:21.137220 139728945038528 spec.py:349] Evaluating on the test split.
I0305 21:18:23.711260 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 21:20:57.734741 139728945038528 submission_runner.py:469] Time since start: 7645.09s, 	Step: 12174, 	{'train/accuracy': 0.6185905933380127, 'train/loss': 2.1589958667755127, 'train/bleu': 29.70556923681752, 'validation/accuracy': 0.6356141567230225, 'validation/loss': 2.0175116062164307, 'validation/bleu': 26.385825583809147, 'validation/num_examples': 3000, 'test/accuracy': 0.6444792151451111, 'test/loss': 1.9658397436141968, 'test/bleu': 25.258389185895084, 'test/num_examples': 3003, 'score': 4226.681246757507, 'total_duration': 7645.089530467987, 'accumulated_submission_time': 4226.681246757507, 'accumulated_eval_time': 3417.525922060013, 'accumulated_logging_time': 0.09152913093566895}
I0305 21:20:57.744951 139585799534336 logging_writer.py:48] [12174] accumulated_eval_time=3417.53, accumulated_logging_time=0.0915291, accumulated_submission_time=4226.68, global_step=12174, preemption_count=0, score=4226.68, test/accuracy=0.644479, test/bleu=25.2584, test/loss=1.96584, test/num_examples=3003, total_duration=7645.09, train/accuracy=0.618591, train/bleu=29.7056, train/loss=2.159, validation/accuracy=0.635614, validation/bleu=26.3858, validation/loss=2.01751, validation/num_examples=3000
I0305 21:21:06.992953 139585807927040 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.16238059103488922, loss=4.197518348693848
I0305 21:21:41.307049 139585799534336 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.16579574346542358, loss=4.130070686340332
I0305 21:22:15.640733 139585807927040 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.17272821068763733, loss=4.1868977546691895
I0305 21:22:50.067819 139585799534336 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1629587560892105, loss=4.198375701904297
I0305 21:23:24.581559 139585807927040 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.18439652025699615, loss=4.168538570404053
I0305 21:23:59.108467 139585799534336 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.16440314054489136, loss=4.133831024169922
I0305 21:24:33.646078 139585807927040 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.16165407001972198, loss=4.216875076293945
I0305 21:25:08.150152 139585799534336 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.15861018002033234, loss=4.1297454833984375
I0305 21:25:42.688961 139585807927040 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.16499735414981842, loss=4.200908660888672
I0305 21:26:17.228050 139585799534336 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.1548270881175995, loss=4.134698390960693
I0305 21:26:51.749689 139585807927040 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.16085262596607208, loss=4.171976089477539
I0305 21:27:26.304635 139585799534336 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.16173973679542542, loss=4.171677112579346
I0305 21:28:00.864346 139585807927040 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.19929474592208862, loss=4.220450401306152
I0305 21:28:35.420705 139585799534336 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.16445507109165192, loss=4.101199150085449
I0305 21:29:09.946260 139585807927040 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.1701221466064453, loss=4.12668514251709
I0305 21:29:44.463681 139585799534336 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.1660965383052826, loss=4.078295707702637
I0305 21:30:18.987069 139585807927040 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.18107347190380096, loss=4.103969573974609
I0305 21:30:53.495479 139585799534336 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.16270360350608826, loss=4.1057610511779785
I0305 21:31:28.004104 139585807927040 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.16009247303009033, loss=4.12606954574585
I0305 21:32:02.502591 139585799534336 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.19853340089321136, loss=4.148571491241455
I0305 21:32:37.040968 139585807927040 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.17642831802368164, loss=4.1225361824035645
I0305 21:33:11.563056 139585799534336 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1621679961681366, loss=4.080774784088135
I0305 21:33:46.075132 139585807927040 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.17098623514175415, loss=4.082245349884033
I0305 21:34:20.587923 139585799534336 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.23139677941799164, loss=4.105960369110107
I0305 21:34:55.073298 139585807927040 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.1670636385679245, loss=4.129702568054199
I0305 21:34:57.862342 139728945038528 spec.py:321] Evaluating on the training split.
I0305 21:35:00.441917 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 21:39:01.822146 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 21:39:04.389976 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 21:41:46.079987 139728945038528 spec.py:349] Evaluating on the test split.
I0305 21:41:48.646452 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 21:44:12.066635 139728945038528 submission_runner.py:469] Time since start: 9039.42s, 	Step: 14609, 	{'train/accuracy': 0.6285078525543213, 'train/loss': 2.0630898475646973, 'train/bleu': 30.303598075164416, 'validation/accuracy': 0.6444886922836304, 'validation/loss': 1.9426811933517456, 'validation/bleu': 27.059750884116987, 'validation/num_examples': 3000, 'test/accuracy': 0.6535859107971191, 'test/loss': 1.8884230852127075, 'test/bleu': 26.348372843234774, 'test/num_examples': 3003, 'score': 5066.6530611515045, 'total_duration': 9039.421432256699, 'accumulated_submission_time': 5066.6530611515045, 'accumulated_eval_time': 3971.73015165329, 'accumulated_logging_time': 0.11051726341247559}
I0305 21:44:12.076620 139585799534336 logging_writer.py:48] [14609] accumulated_eval_time=3971.73, accumulated_logging_time=0.110517, accumulated_submission_time=5066.65, global_step=14609, preemption_count=0, score=5066.65, test/accuracy=0.653586, test/bleu=26.3484, test/loss=1.88842, test/num_examples=3003, total_duration=9039.42, train/accuracy=0.628508, train/bleu=30.3036, train/loss=2.06309, validation/accuracy=0.644489, validation/bleu=27.0598, validation/loss=1.94268, validation/num_examples=3000
I0305 21:44:43.703597 139585807927040 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.17344588041305542, loss=4.180625915527344
I0305 21:45:18.106687 139585799534336 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.16052095592021942, loss=4.115100860595703
I0305 21:45:52.595165 139585807927040 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.16655302047729492, loss=4.143210411071777
I0305 21:46:27.108126 139585799534336 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.1522451490163803, loss=4.065430641174316
I0305 21:47:01.626968 139585807927040 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.1853456050157547, loss=4.0823187828063965
I0305 21:47:36.142810 139585799534336 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.16480861604213715, loss=4.068675518035889
I0305 21:48:10.659889 139585807927040 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.16369366645812988, loss=4.131510257720947
I0305 21:48:45.213745 139585799534336 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.17360861599445343, loss=4.151752471923828
I0305 21:49:19.745991 139585807927040 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.15941429138183594, loss=4.115696430206299
I0305 21:49:54.281745 139585799534336 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.17091959714889526, loss=4.090991497039795
I0305 21:50:28.812653 139585807927040 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.19365128874778748, loss=4.019368648529053
I0305 21:51:03.361760 139585799534336 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.15704086422920227, loss=4.092203617095947
I0305 21:51:37.906704 139585807927040 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.17010466754436493, loss=4.058628082275391
I0305 21:52:12.428120 139585799534336 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1883174180984497, loss=4.069618225097656
I0305 21:52:46.927368 139585807927040 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.156008780002594, loss=4.055419445037842
I0305 21:53:21.421741 139585799534336 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.1710509955883026, loss=4.015693664550781
I0305 21:53:55.909263 139585807927040 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.16112524271011353, loss=4.09440279006958
I0305 21:54:30.378754 139585799534336 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.18988707661628723, loss=4.126874923706055
I0305 21:55:04.838396 139585807927040 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.22275640070438385, loss=4.027066230773926
I0305 21:55:39.340624 139585799534336 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2052697092294693, loss=4.141775131225586
I0305 21:56:13.815264 139585807927040 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.15740102529525757, loss=4.139328956604004
I0305 21:56:48.284135 139585799534336 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.15370947122573853, loss=4.064153671264648
I0305 21:57:22.799799 139585807927040 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15264226496219635, loss=4.088723659515381
I0305 21:57:57.275241 139585799534336 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.159579798579216, loss=4.149604320526123
I0305 21:58:12.121293 139728945038528 spec.py:321] Evaluating on the training split.
I0305 21:58:14.711861 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:01:12.374696 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 22:01:14.954800 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:03:56.446608 139728945038528 spec.py:349] Evaluating on the test split.
I0305 22:03:59.027653 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:06:19.094524 139728945038528 submission_runner.py:469] Time since start: 10366.45s, 	Step: 17044, 	{'train/accuracy': 0.6341263055801392, 'train/loss': 2.021695375442505, 'train/bleu': 30.77153944461597, 'validation/accuracy': 0.651336133480072, 'validation/loss': 1.888048768043518, 'validation/bleu': 27.395823434019217, 'validation/num_examples': 3000, 'test/accuracy': 0.6611980199813843, 'test/loss': 1.8268219232559204, 'test/bleu': 27.128141825744525, 'test/num_examples': 3003, 'score': 5906.551867485046, 'total_duration': 10366.449325561523, 'accumulated_submission_time': 5906.551867485046, 'accumulated_eval_time': 4458.703329563141, 'accumulated_logging_time': 0.12897300720214844}
I0305 22:06:19.105100 139585807927040 logging_writer.py:48] [17044] accumulated_eval_time=4458.7, accumulated_logging_time=0.128973, accumulated_submission_time=5906.55, global_step=17044, preemption_count=0, score=5906.55, test/accuracy=0.661198, test/bleu=27.1281, test/loss=1.82682, test/num_examples=3003, total_duration=10366.4, train/accuracy=0.634126, train/bleu=30.7715, train/loss=2.0217, validation/accuracy=0.651336, validation/bleu=27.3958, validation/loss=1.88805, validation/num_examples=3000
I0305 22:06:38.722682 139585799534336 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.20226071774959564, loss=4.0720086097717285
I0305 22:07:13.116494 139585807927040 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.16755375266075134, loss=4.081204414367676
I0305 22:07:47.564438 139585799534336 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.15679819881916046, loss=4.024349212646484
I0305 22:08:22.060856 139585807927040 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.15922673046588898, loss=4.057087421417236
I0305 22:08:56.580787 139585799534336 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.17760422825813293, loss=4.03271484375
I0305 22:09:31.115131 139585807927040 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.15069086849689484, loss=4.116005897521973
I0305 22:10:05.654748 139585799534336 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.1747937798500061, loss=4.097901344299316
I0305 22:10:40.199309 139585807927040 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.15664361417293549, loss=4.058821201324463
I0305 22:11:14.748009 139585799534336 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.1931602656841278, loss=4.081103801727295
I0305 22:11:49.293996 139585807927040 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.158928781747818, loss=4.077718257904053
I0305 22:12:23.824697 139585799534336 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.1598593145608902, loss=4.048903942108154
I0305 22:12:58.360285 139585807927040 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1608182191848755, loss=4.024871826171875
I0305 22:13:32.890188 139585799534336 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.1856766939163208, loss=4.054181098937988
I0305 22:14:07.475844 139585807927040 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.17129744589328766, loss=4.0974016189575195
I0305 22:14:42.010630 139585799534336 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.18026788532733917, loss=4.086708068847656
I0305 22:15:16.565449 139585807927040 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.16245074570178986, loss=4.066125869750977
I0305 22:15:51.102717 139585799534336 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.2107028067111969, loss=4.057293891906738
I0305 22:16:25.623226 139585807927040 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.16986088454723358, loss=3.983445405960083
I0305 22:17:00.365371 139585799534336 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.15262188017368317, loss=4.044242858886719
I0305 22:17:35.033317 139585807927040 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.16833190619945526, loss=4.039894104003906
I0305 22:18:09.717294 139585799534336 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.16093382239341736, loss=4.079834938049316
I0305 22:18:44.376503 139585807927040 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.2395917922258377, loss=3.993882417678833
I0305 22:19:19.059184 139585799534336 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.15065418183803558, loss=3.973417043685913
I0305 22:19:53.709348 139585807927040 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1498178392648697, loss=3.979053497314453
I0305 22:20:19.342071 139728945038528 spec.py:321] Evaluating on the training split.
I0305 22:20:21.936231 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:23:01.978015 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 22:23:04.559501 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:25:40.799722 139728945038528 spec.py:349] Evaluating on the test split.
I0305 22:25:43.390201 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:28:07.908585 139728945038528 submission_runner.py:469] Time since start: 11675.26s, 	Step: 19475, 	{'train/accuracy': 0.648202657699585, 'train/loss': 1.9112379550933838, 'train/bleu': 32.06620675384898, 'validation/accuracy': 0.656181275844574, 'validation/loss': 1.8411140441894531, 'validation/bleu': 28.217390071231478, 'validation/num_examples': 3000, 'test/accuracy': 0.6665855646133423, 'test/loss': 1.7744221687316895, 'test/bleu': 27.6295625742881, 'test/num_examples': 3003, 'score': 6746.63994717598, 'total_duration': 11675.263387441635, 'accumulated_submission_time': 6746.63994717598, 'accumulated_eval_time': 4927.269788742065, 'accumulated_logging_time': 0.14797019958496094}
I0305 22:28:07.919762 139585799534336 logging_writer.py:48] [19475] accumulated_eval_time=4927.27, accumulated_logging_time=0.14797, accumulated_submission_time=6746.64, global_step=19475, preemption_count=0, score=6746.64, test/accuracy=0.666586, test/bleu=27.6296, test/loss=1.77442, test/num_examples=3003, total_duration=11675.3, train/accuracy=0.648203, train/bleu=32.0662, train/loss=1.91124, validation/accuracy=0.656181, validation/bleu=28.2174, validation/loss=1.84111, validation/num_examples=3000
I0305 22:28:16.898856 139585807927040 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.17880947887897491, loss=3.993410110473633
I0305 22:28:51.422550 139585799534336 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.17723076045513153, loss=4.037064552307129
I0305 22:29:26.053769 139585807927040 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.18269169330596924, loss=4.106162071228027
I0305 22:30:00.706140 139585799534336 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.1738656759262085, loss=4.049178600311279
I0305 22:30:35.396012 139585807927040 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.17754486203193665, loss=4.044290542602539
I0305 22:31:10.077150 139585799534336 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.1779565066099167, loss=4.062789440155029
I0305 22:31:44.763183 139585807927040 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.1536894589662552, loss=3.9488253593444824
I0305 22:32:19.444643 139585799534336 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.16504395008087158, loss=4.0474629402160645
I0305 22:32:54.121581 139585807927040 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.20492567121982574, loss=4.000341892242432
I0305 22:33:28.786983 139585799534336 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.15859971940517426, loss=4.038415431976318
I0305 22:34:03.447729 139585807927040 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.17652876675128937, loss=4.074836254119873
I0305 22:34:38.156475 139585799534336 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.2550560235977173, loss=4.078307628631592
I0305 22:35:12.838781 139585807927040 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.18862135708332062, loss=4.020271301269531
I0305 22:35:47.523019 139585799534336 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.15068241953849792, loss=4.005366802215576
I0305 22:36:22.214955 139585807927040 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.17521251738071442, loss=3.996769666671753
I0305 22:36:56.866972 139585799534336 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.1775166392326355, loss=3.9450340270996094
I0305 22:37:31.537370 139585807927040 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.18652725219726562, loss=4.04999303817749
I0305 22:38:06.248028 139585799534336 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.1877373605966568, loss=4.030459880828857
I0305 22:38:40.929375 139585807927040 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.1982373148202896, loss=4.034612655639648
I0305 22:39:15.535943 139585799534336 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.16331566870212555, loss=3.960297107696533
I0305 22:39:50.194016 139585807927040 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.20269298553466797, loss=4.028741836547852
I0305 22:40:24.856497 139585799534336 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.179228276014328, loss=4.019286155700684
I0305 22:40:59.460123 139585807927040 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.16160982847213745, loss=3.9869751930236816
I0305 22:41:34.081884 139585799534336 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.17200839519500732, loss=4.006227016448975
I0305 22:42:08.000513 139728945038528 spec.py:321] Evaluating on the training split.
I0305 22:42:10.599618 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:44:53.540904 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 22:44:56.129812 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:47:31.787614 139728945038528 spec.py:349] Evaluating on the test split.
I0305 22:47:34.369997 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 22:49:52.722856 139728945038528 submission_runner.py:469] Time since start: 12980.08s, 	Step: 21899, 	{'train/accuracy': 0.6450713276863098, 'train/loss': 1.9377447366714478, 'train/bleu': 31.763554323748103, 'validation/accuracy': 0.6597533226013184, 'validation/loss': 1.828452229499817, 'validation/bleu': 28.277150609295997, 'validation/num_examples': 3000, 'test/accuracy': 0.669957160949707, 'test/loss': 1.7682262659072876, 'test/bleu': 27.96035951029598, 'test/num_examples': 3003, 'score': 7586.576191663742, 'total_duration': 12980.077657699585, 'accumulated_submission_time': 7586.576191663742, 'accumulated_eval_time': 5391.992084741592, 'accumulated_logging_time': 0.1683058738708496}
I0305 22:49:52.735818 139585807927040 logging_writer.py:48] [21899] accumulated_eval_time=5391.99, accumulated_logging_time=0.168306, accumulated_submission_time=7586.58, global_step=21899, preemption_count=0, score=7586.58, test/accuracy=0.669957, test/bleu=27.9604, test/loss=1.76823, test/num_examples=3003, total_duration=12980.1, train/accuracy=0.645071, train/bleu=31.7636, train/loss=1.93774, validation/accuracy=0.659753, validation/bleu=28.2772, validation/loss=1.82845, validation/num_examples=3000
I0305 22:49:53.443682 139585799534336 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.18569375574588776, loss=3.990983009338379
I0305 22:50:27.886787 139585807927040 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.16096705198287964, loss=3.9796226024627686
I0305 22:51:02.407591 139585799534336 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.18253158032894135, loss=3.9600045680999756
I0305 22:51:37.022263 139585807927040 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.15979835391044617, loss=4.003025054931641
I0305 22:52:11.674952 139585799534336 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.17258353531360626, loss=4.014290809631348
I0305 22:52:46.293566 139585807927040 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.1738145649433136, loss=4.00853157043457
I0305 22:53:20.899767 139585799534336 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.24124646186828613, loss=4.010822772979736
I0305 22:53:55.520870 139585807927040 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.25404995679855347, loss=3.9965295791625977
I0305 22:54:30.164241 139585799534336 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.2003575563430786, loss=4.0145440101623535
I0305 22:55:04.818533 139585807927040 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.17164576053619385, loss=3.951996326446533
I0305 22:55:39.489982 139585799534336 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.1820463091135025, loss=3.9881505966186523
I0305 22:56:14.165638 139585807927040 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.18077492713928223, loss=4.0126471519470215
I0305 22:56:48.821840 139585799534336 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.27361923456192017, loss=4.055533409118652
I0305 22:57:23.488180 139585807927040 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.17619842290878296, loss=3.9613802433013916
I0305 22:57:58.132394 139585799534336 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.17648857831954956, loss=4.019913673400879
I0305 22:58:32.770908 139585807927040 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.19799353182315826, loss=3.9617319107055664
I0305 22:59:07.424149 139585799534336 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.19253264367580414, loss=3.9732112884521484
I0305 22:59:42.094122 139585807927040 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.18792052567005157, loss=3.9928805828094482
I0305 23:00:16.756743 139585799534336 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.18689358234405518, loss=4.069796085357666
I0305 23:00:51.428413 139585807927040 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.15655997395515442, loss=3.960643768310547
I0305 23:01:26.051022 139585799534336 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.1673843264579773, loss=3.9899070262908936
I0305 23:02:00.691601 139585807927040 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.17567194998264313, loss=3.941249370574951
I0305 23:02:35.335109 139585799534336 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.20519714057445526, loss=3.9545936584472656
I0305 23:03:09.993916 139585807927040 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.20058535039424896, loss=3.91640043258667
I0305 23:03:44.650355 139585799534336 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.267470121383667, loss=4.031064987182617
I0305 23:03:52.970425 139728945038528 spec.py:321] Evaluating on the training split.
I0305 23:03:55.566342 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:06:50.622493 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 23:06:53.206945 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:09:28.873645 139728945038528 spec.py:349] Evaluating on the test split.
I0305 23:09:31.461658 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:11:50.352987 139728945038528 submission_runner.py:469] Time since start: 14297.71s, 	Step: 24325, 	{'train/accuracy': 0.6450738906860352, 'train/loss': 1.9415960311889648, 'train/bleu': 31.445378915533798, 'validation/accuracy': 0.6627444624900818, 'validation/loss': 1.8064852952957153, 'validation/bleu': 28.563607860536997, 'validation/num_examples': 3000, 'test/accuracy': 0.674012303352356, 'test/loss': 1.7404903173446655, 'test/bleu': 27.924541841110994, 'test/num_examples': 3003, 'score': 8426.670614242554, 'total_duration': 14297.707791090012, 'accumulated_submission_time': 8426.670614242554, 'accumulated_eval_time': 5869.374609231949, 'accumulated_logging_time': 0.19014525413513184}
I0305 23:11:50.365628 139585807927040 logging_writer.py:48] [24325] accumulated_eval_time=5869.37, accumulated_logging_time=0.190145, accumulated_submission_time=8426.67, global_step=24325, preemption_count=0, score=8426.67, test/accuracy=0.674012, test/bleu=27.9245, test/loss=1.74049, test/num_examples=3003, total_duration=14297.7, train/accuracy=0.645074, train/bleu=31.4454, train/loss=1.9416, validation/accuracy=0.662744, validation/bleu=28.5636, validation/loss=1.80649, validation/num_examples=3000
I0305 23:12:16.598396 139585799534336 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.17803215980529785, loss=3.964480400085449
I0305 23:12:51.142798 139585807927040 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.17253243923187256, loss=3.931436061859131
I0305 23:13:25.778509 139585799534336 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.2123601883649826, loss=3.976733446121216
I0305 23:14:00.456921 139585807927040 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2117636650800705, loss=3.928617477416992
I0305 23:14:35.134252 139585799534336 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.1811474710702896, loss=4.002005577087402
I0305 23:15:09.828413 139585807927040 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.18507824838161469, loss=3.9109649658203125
I0305 23:15:44.524793 139585799534336 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.18432264029979706, loss=3.984257221221924
I0305 23:16:19.218952 139585807927040 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2285069078207016, loss=4.033517360687256
I0305 23:16:53.783156 139585799534336 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.18214716017246246, loss=3.9378387928009033
I0305 23:17:28.345998 139585807927040 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.1894066333770752, loss=3.9543967247009277
I0305 23:18:02.911817 139585799534336 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2284782975912094, loss=4.004820823669434
I0305 23:18:37.488347 139585807927040 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.19417747855186462, loss=3.941056489944458
I0305 23:19:12.070069 139585799534336 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.20011357963085175, loss=3.896803379058838
I0305 23:19:46.674786 139585807927040 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.1976422369480133, loss=4.0274200439453125
I0305 23:20:21.267720 139585799534336 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.19510260224342346, loss=4.012173652648926
I0305 23:20:55.834816 139585807927040 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.17260868847370148, loss=3.916668176651001
I0305 23:21:30.381573 139585799534336 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.19077818095684052, loss=3.989189624786377
I0305 23:22:04.923334 139585807927040 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.19776302576065063, loss=3.9730422496795654
I0305 23:22:39.478566 139585799534336 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.20625916123390198, loss=3.9803333282470703
I0305 23:23:14.046000 139585807927040 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.17813071608543396, loss=4.008060932159424
I0305 23:23:48.644366 139585799534336 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.22232848405838013, loss=3.988075017929077
I0305 23:24:23.207459 139585807927040 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.18514734506607056, loss=3.9658801555633545
I0305 23:24:57.719118 139585799534336 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2833189368247986, loss=3.958604574203491
I0305 23:25:32.250934 139585807927040 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.17505738139152527, loss=3.944817066192627
I0305 23:25:50.578557 139728945038528 spec.py:321] Evaluating on the training split.
I0305 23:25:53.175713 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:29:25.625855 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 23:29:28.202747 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:32:19.792373 139728945038528 spec.py:349] Evaluating on the test split.
I0305 23:32:22.375751 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:34:41.920901 139728945038528 submission_runner.py:469] Time since start: 15669.28s, 	Step: 26754, 	{'train/accuracy': 0.6521549224853516, 'train/loss': 1.8771387338638306, 'train/bleu': 32.064094664084806, 'validation/accuracy': 0.6654512882232666, 'validation/loss': 1.7797573804855347, 'validation/bleu': 28.631837261711482, 'validation/num_examples': 3000, 'test/accuracy': 0.6766886711120605, 'test/loss': 1.7108386754989624, 'test/bleu': 28.251831056819952, 'test/num_examples': 3003, 'score': 9266.741669893265, 'total_duration': 15669.27569770813, 'accumulated_submission_time': 9266.741669893265, 'accumulated_eval_time': 6400.71689248085, 'accumulated_logging_time': 0.2111063003540039}
I0305 23:34:41.933101 139585799534336 logging_writer.py:48] [26754] accumulated_eval_time=6400.72, accumulated_logging_time=0.211106, accumulated_submission_time=9266.74, global_step=26754, preemption_count=0, score=9266.74, test/accuracy=0.676689, test/bleu=28.2518, test/loss=1.71084, test/num_examples=3003, total_duration=15669.3, train/accuracy=0.652155, train/bleu=32.0641, train/loss=1.87714, validation/accuracy=0.665451, validation/bleu=28.6318, validation/loss=1.77976, validation/num_examples=3000
I0305 23:34:58.120401 139585807927040 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.3187624216079712, loss=4.043145179748535
I0305 23:35:32.565020 139585799534336 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.19612695276737213, loss=3.98714280128479
I0305 23:36:07.072587 139585807927040 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.20302774012088776, loss=3.985201835632324
I0305 23:36:41.651659 139585799534336 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.211213156580925, loss=3.928922414779663
I0305 23:37:16.191829 139585807927040 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.21339958906173706, loss=3.942796468734741
I0305 23:37:50.735969 139585799534336 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.20150360465049744, loss=3.978250503540039
I0305 23:38:25.272635 139585807927040 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2968175709247589, loss=3.9203968048095703
I0305 23:38:59.802017 139585799534336 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.18796047568321228, loss=3.9672176837921143
I0305 23:39:34.330353 139585807927040 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.28447550535202026, loss=3.980668306350708
I0305 23:40:08.871746 139585799534336 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.18670709431171417, loss=3.882150173187256
I0305 23:40:43.445515 139585807927040 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.1949324756860733, loss=3.962439775466919
I0305 23:41:17.983283 139585799534336 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.23912370204925537, loss=3.9348392486572266
I0305 23:41:52.528041 139585807927040 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.22306619584560394, loss=3.941094398498535
I0305 23:42:27.047089 139585799534336 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.1821526437997818, loss=3.952435255050659
I0305 23:43:01.576578 139585807927040 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.2175772786140442, loss=3.949193000793457
I0305 23:43:36.105537 139585799534336 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.2335091233253479, loss=3.966845989227295
I0305 23:44:10.686160 139585807927040 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.1853301227092743, loss=3.959022045135498
I0305 23:44:45.197227 139585799534336 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.22080858051776886, loss=3.9891741275787354
I0305 23:45:19.747342 139585807927040 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18224886059761047, loss=3.9650182723999023
I0305 23:45:54.278434 139585799534336 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.19492408633232117, loss=4.011882781982422
I0305 23:46:28.806659 139585807927040 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.20509271323680878, loss=3.9503769874572754
I0305 23:47:03.380967 139585799534336 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.2608928978443146, loss=3.9495503902435303
I0305 23:47:37.889828 139585807927040 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.20408359169960022, loss=3.936307907104492
I0305 23:48:12.453574 139585799534336 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.29255664348602295, loss=3.930875062942505
I0305 23:48:42.148865 139728945038528 spec.py:321] Evaluating on the training split.
I0305 23:48:44.739621 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:51:56.734383 139728945038528 spec.py:333] Evaluating on the validation split.
I0305 23:51:59.327984 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:54:45.126246 139728945038528 spec.py:349] Evaluating on the test split.
I0305 23:54:47.709259 139728945038528 workload.py:181] Translating evaluation dataset.
I0305 23:57:19.818113 139728945038528 submission_runner.py:469] Time since start: 17027.17s, 	Step: 29187, 	{'train/accuracy': 0.650421142578125, 'train/loss': 1.9072725772857666, 'train/bleu': 32.30471685665179, 'validation/accuracy': 0.6668232679367065, 'validation/loss': 1.778612494468689, 'validation/bleu': 28.771572168028452, 'validation/num_examples': 3000, 'test/accuracy': 0.6789827346801758, 'test/loss': 1.7037239074707031, 'test/bleu': 28.25634350282276, 'test/num_examples': 3003, 'score': 10106.813430070877, 'total_duration': 17027.17290496826, 'accumulated_submission_time': 10106.813430070877, 'accumulated_eval_time': 6918.386072635651, 'accumulated_logging_time': 0.23152470588684082}
I0305 23:57:19.830849 139585807927040 logging_writer.py:48] [29187] accumulated_eval_time=6918.39, accumulated_logging_time=0.231525, accumulated_submission_time=10106.8, global_step=29187, preemption_count=0, score=10106.8, test/accuracy=0.678983, test/bleu=28.2563, test/loss=1.70372, test/num_examples=3003, total_duration=17027.2, train/accuracy=0.650421, train/bleu=32.3047, train/loss=1.90727, validation/accuracy=0.666823, validation/bleu=28.7716, validation/loss=1.77861, validation/num_examples=3000
I0305 23:57:24.659540 139585799534336 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.20442503690719604, loss=3.972395658493042
I0305 23:57:59.041453 139585807927040 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.21666677296161652, loss=3.9589242935180664
I0305 23:58:33.528913 139585799534336 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.1915271282196045, loss=3.9178359508514404
I0305 23:59:08.047497 139585807927040 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.20753461122512817, loss=3.9182326793670654
I0305 23:59:42.583158 139585799534336 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.1977609395980835, loss=3.9456756114959717
I0306 00:00:17.141520 139585807927040 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.31907919049263, loss=3.908862352371216
I0306 00:00:51.690281 139585799534336 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.24297240376472473, loss=3.9090800285339355
I0306 00:01:26.227835 139585807927040 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.2324124127626419, loss=3.9235918521881104
I0306 00:02:00.802031 139585799534336 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.20441271364688873, loss=3.9360742568969727
I0306 00:02:35.334573 139585807927040 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.2697901129722595, loss=4.036514759063721
I0306 00:03:09.880592 139585799534336 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.19084511697292328, loss=3.932563304901123
I0306 00:03:44.419397 139585807927040 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3282069265842438, loss=3.940523862838745
I0306 00:04:18.981287 139585799534336 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.2495444416999817, loss=3.9347405433654785
I0306 00:04:53.546792 139585807927040 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.19146598875522614, loss=3.8900463581085205
I0306 00:05:28.120156 139585799534336 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.1933162361383438, loss=3.923450469970703
I0306 00:06:02.666713 139585807927040 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.2243204265832901, loss=3.9725325107574463
I0306 00:06:37.227059 139585799534336 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.18900156021118164, loss=3.8983514308929443
I0306 00:07:11.814790 139585807927040 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.20408380031585693, loss=3.9579899311065674
I0306 00:07:46.384276 139585799534336 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.18162314593791962, loss=3.8867650032043457
I0306 00:08:20.927794 139585807927040 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.21481940150260925, loss=3.908707857131958
I0306 00:08:55.468466 139585799534336 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.18940649926662445, loss=3.9687392711639404
I0306 00:09:30.030713 139585807927040 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.2044154405593872, loss=3.8692843914031982
I0306 00:10:04.673032 139585799534336 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.21362313628196716, loss=3.9939942359924316
I0306 00:10:39.358784 139585807927040 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.1900850236415863, loss=3.916583776473999
I0306 00:11:14.025635 139585799534336 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.20271770656108856, loss=3.899740219116211
I0306 00:11:19.930901 139728945038528 spec.py:321] Evaluating on the training split.
I0306 00:11:22.528865 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 00:14:48.545192 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 00:14:51.122155 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 00:17:40.071305 139728945038528 spec.py:349] Evaluating on the test split.
I0306 00:17:42.647672 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 00:20:17.404738 139728945038528 submission_runner.py:469] Time since start: 18404.76s, 	Step: 31618, 	{'train/accuracy': 0.6737024188041687, 'train/loss': 1.7493079900741577, 'train/bleu': 33.489701153894806, 'validation/accuracy': 0.6671322584152222, 'validation/loss': 1.766648769378662, 'validation/bleu': 28.473035679454313, 'validation/num_examples': 3000, 'test/accuracy': 0.6787741780281067, 'test/loss': 1.69500732421875, 'test/bleu': 28.127657665565273, 'test/num_examples': 3003, 'score': 10946.771188020706, 'total_duration': 18404.759540081024, 'accumulated_submission_time': 10946.771188020706, 'accumulated_eval_time': 7455.859852075577, 'accumulated_logging_time': 0.25223636627197266}
I0306 00:20:17.418223 139585807927040 logging_writer.py:48] [31618] accumulated_eval_time=7455.86, accumulated_logging_time=0.252236, accumulated_submission_time=10946.8, global_step=31618, preemption_count=0, score=10946.8, test/accuracy=0.678774, test/bleu=28.1277, test/loss=1.69501, test/num_examples=3003, total_duration=18404.8, train/accuracy=0.673702, train/bleu=33.4897, train/loss=1.74931, validation/accuracy=0.667132, validation/bleu=28.473, validation/loss=1.76665, validation/num_examples=3000
I0306 00:20:46.009141 139585799534336 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.28553858399391174, loss=3.9259610176086426
I0306 00:21:20.513605 139585807927040 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.19922342896461487, loss=3.893808126449585
I0306 00:21:55.096173 139585799534336 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.21140561997890472, loss=3.917773485183716
I0306 00:22:29.736745 139585807927040 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.21600529551506042, loss=3.9528656005859375
I0306 00:23:04.378376 139585799534336 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2252441942691803, loss=3.923943519592285
I0306 00:23:38.997625 139585807927040 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.2965255379676819, loss=3.9543750286102295
I0306 00:24:13.641887 139585799534336 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.23696421086788177, loss=3.9541494846343994
I0306 00:24:48.270571 139585807927040 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.24628056585788727, loss=3.9594290256500244
I0306 00:25:22.902396 139585799534336 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.19446507096290588, loss=3.922431230545044
I0306 00:25:57.493667 139585807927040 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.2918646037578583, loss=3.894066572189331
I0306 00:26:32.108417 139585799534336 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.22606287896633148, loss=3.9029088020324707
I0306 00:27:06.742382 139585807927040 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.2175835818052292, loss=3.907181739807129
I0306 00:27:41.366995 139585799534336 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.21282796561717987, loss=3.9665234088897705
I0306 00:28:15.975395 139585807927040 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.20823439955711365, loss=3.84879469871521
I0306 00:28:50.600582 139585799534336 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.306365042924881, loss=3.8744382858276367
I0306 00:29:25.238639 139585807927040 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.1931396722793579, loss=3.8832130432128906
I0306 00:29:59.907294 139585799534336 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.20100216567516327, loss=3.920640230178833
I0306 00:30:34.515733 139585807927040 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2119104117155075, loss=3.888944149017334
I0306 00:31:09.142125 139585799534336 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.2223319709300995, loss=3.9205381870269775
I0306 00:31:43.751456 139585807927040 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.20897065103054047, loss=3.9156579971313477
I0306 00:32:18.359452 139585799534336 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.20860250294208527, loss=3.9037234783172607
I0306 00:32:52.972182 139585807927040 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.21179261803627014, loss=3.9237608909606934
I0306 00:33:27.567920 139585799534336 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.301024854183197, loss=3.9486851692199707
I0306 00:34:02.160231 139585807927040 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.31179988384246826, loss=3.964324474334717
I0306 00:34:17.735145 139728945038528 spec.py:321] Evaluating on the training split.
I0306 00:34:20.333011 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 00:39:11.146173 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 00:39:13.718551 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 00:42:28.084114 139728945038528 spec.py:349] Evaluating on the test split.
I0306 00:42:30.657934 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 00:45:24.371696 139728945038528 submission_runner.py:469] Time since start: 19911.73s, 	Step: 34046, 	{'train/accuracy': 0.6540790796279907, 'train/loss': 1.8604586124420166, 'train/bleu': 32.39988299936474, 'validation/accuracy': 0.6703458428382874, 'validation/loss': 1.751020073890686, 'validation/bleu': 28.84627260482706, 'validation/num_examples': 3000, 'test/accuracy': 0.6827830076217651, 'test/loss': 1.6761033535003662, 'test/bleu': 28.54834134368054, 'test/num_examples': 3003, 'score': 11786.942027807236, 'total_duration': 19911.726499557495, 'accumulated_submission_time': 11786.942027807236, 'accumulated_eval_time': 8122.496347665787, 'accumulated_logging_time': 0.27518367767333984}
I0306 00:45:24.384263 139585799534336 logging_writer.py:48] [34046] accumulated_eval_time=8122.5, accumulated_logging_time=0.275184, accumulated_submission_time=11786.9, global_step=34046, preemption_count=0, score=11786.9, test/accuracy=0.682783, test/bleu=28.5483, test/loss=1.6761, test/num_examples=3003, total_duration=19911.7, train/accuracy=0.654079, train/bleu=32.3999, train/loss=1.86046, validation/accuracy=0.670346, validation/bleu=28.8463, validation/loss=1.75102, validation/num_examples=3000
I0306 00:45:43.286365 139585807927040 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.21016672253608704, loss=3.974824905395508
I0306 00:46:17.652510 139585799534336 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.25134262442588806, loss=3.9240949153900146
I0306 00:46:52.112534 139585807927040 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.8540617227554321, loss=4.739583492279053
I0306 00:47:26.669798 139585799534336 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.2520864009857178, loss=4.005559921264648
I0306 00:48:01.229678 139585807927040 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.20200186967849731, loss=3.9458508491516113
I0306 00:48:35.789409 139585799534336 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.21036075055599213, loss=3.943502187728882
I0306 00:49:10.320653 139585807927040 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.23850946128368378, loss=3.904782295227051
I0306 00:49:44.854563 139585799534336 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.20393195748329163, loss=3.9386184215545654
I0306 00:50:19.364599 139585807927040 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.20839032530784607, loss=3.906691074371338
I0306 00:50:53.893775 139585799534336 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.2369941920042038, loss=3.9763684272766113
I0306 00:51:28.384913 139585807927040 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2754417955875397, loss=3.8413803577423096
I0306 00:52:02.929797 139585799534336 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.21496029198169708, loss=3.9526615142822266
I0306 00:52:37.442770 139585807927040 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.20448732376098633, loss=3.8734073638916016
I0306 00:53:11.968380 139585799534336 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.22335049510002136, loss=3.8746321201324463
I0306 00:53:46.530786 139585807927040 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2266986221075058, loss=3.8866326808929443
I0306 00:54:21.054990 139585799534336 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.20356367528438568, loss=3.9375922679901123
I0306 00:54:55.568789 139585807927040 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2206612378358841, loss=3.938945770263672
I0306 00:55:30.110130 139585799534336 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.23527133464813232, loss=3.9378409385681152
I0306 00:56:04.649191 139585807927040 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.24455426633358002, loss=3.894648790359497
I0306 00:56:39.178934 139585799534336 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.20784202218055725, loss=3.8801026344299316
I0306 00:57:13.721674 139585807927040 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.21859146654605865, loss=3.8723254203796387
I0306 00:57:48.279325 139585799534336 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.23748156428337097, loss=3.908632278442383
I0306 00:58:22.783723 139585807927040 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.22461964190006256, loss=3.960312604904175
I0306 00:58:57.292545 139585799534336 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.22674155235290527, loss=3.8733086585998535
I0306 00:59:24.570853 139728945038528 spec.py:321] Evaluating on the training split.
I0306 00:59:27.167600 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:02:18.456832 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 01:02:21.038259 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:04:52.116379 139728945038528 spec.py:349] Evaluating on the test split.
I0306 01:04:54.682090 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:07:19.496742 139728945038528 submission_runner.py:469] Time since start: 21226.85s, 	Step: 36480, 	{'train/accuracy': 0.656191349029541, 'train/loss': 1.8564461469650269, 'train/bleu': 32.71964261283881, 'validation/accuracy': 0.672904372215271, 'validation/loss': 1.7244051694869995, 'validation/bleu': 29.39053925889315, 'validation/num_examples': 3000, 'test/accuracy': 0.6844050288200378, 'test/loss': 1.6516268253326416, 'test/bleu': 29.11269486238484, 'test/num_examples': 3003, 'score': 12626.98415851593, 'total_duration': 21226.851529359818, 'accumulated_submission_time': 12626.98415851593, 'accumulated_eval_time': 8597.422164678574, 'accumulated_logging_time': 0.29601097106933594}
I0306 01:07:19.510862 139585807927040 logging_writer.py:48] [36480] accumulated_eval_time=8597.42, accumulated_logging_time=0.296011, accumulated_submission_time=12627, global_step=36480, preemption_count=0, score=12627, test/accuracy=0.684405, test/bleu=29.1127, test/loss=1.65163, test/num_examples=3003, total_duration=21226.9, train/accuracy=0.656191, train/bleu=32.7196, train/loss=1.85645, validation/accuracy=0.672904, validation/bleu=29.3905, validation/loss=1.72441, validation/num_examples=3000
I0306 01:07:26.723637 139585799534336 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.22051912546157837, loss=3.908860206604004
I0306 01:08:01.070015 139585807927040 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.23269620537757874, loss=3.9397475719451904
I0306 01:08:35.519075 139585799534336 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.25732704997062683, loss=3.949317455291748
I0306 01:09:10.051191 139585807927040 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.21547271311283112, loss=4.003346920013428
I0306 01:09:44.626600 139585799534336 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2543412744998932, loss=3.9594085216522217
I0306 01:10:19.174800 139585807927040 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.22019189596176147, loss=3.901693105697632
I0306 01:10:53.728491 139585799534336 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3018224537372589, loss=3.917868137359619
I0306 01:11:28.255031 139585807927040 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.22838765382766724, loss=3.934354066848755
I0306 01:12:02.785579 139585799534336 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2261224240064621, loss=3.922254800796509
I0306 01:12:37.319612 139585807927040 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.20927070081233978, loss=3.849410057067871
I0306 01:13:11.846231 139585799534336 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.20812954008579254, loss=3.9029390811920166
I0306 01:13:46.388530 139585807927040 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.3123187720775604, loss=3.933742046356201
I0306 01:14:20.926795 139585799534336 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.3237546384334564, loss=3.8559536933898926
I0306 01:14:55.463368 139585807927040 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3020514249801636, loss=3.954287052154541
I0306 01:15:29.984726 139585799534336 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.26534175872802734, loss=3.9262983798980713
I0306 01:16:04.508977 139585807927040 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2616959512233734, loss=3.990441083908081
I0306 01:16:39.033360 139585799534336 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.2176915854215622, loss=3.9378414154052734
I0306 01:17:13.551178 139585807927040 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.24183084070682526, loss=3.906315803527832
I0306 01:17:48.060199 139585799534336 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.29802367091178894, loss=3.898634672164917
I0306 01:18:22.595168 139585807927040 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.2586107850074768, loss=3.8769419193267822
I0306 01:18:57.114758 139585799534336 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.24775853753089905, loss=3.8872618675231934
I0306 01:19:31.624078 139585807927040 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2106388360261917, loss=3.8952817916870117
I0306 01:20:06.141464 139585799534336 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2291049361228943, loss=3.905829429626465
I0306 01:20:40.648415 139585807927040 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.22329328954219818, loss=3.9113733768463135
I0306 01:21:15.154668 139585799534336 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.24415259063243866, loss=3.9589765071868896
I0306 01:21:19.653565 139728945038528 spec.py:321] Evaluating on the training split.
I0306 01:21:22.237789 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:24:20.527479 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 01:24:23.099856 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:27:06.365623 139728945038528 spec.py:349] Evaluating on the test split.
I0306 01:27:08.950018 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:29:45.572204 139728945038528 submission_runner.py:469] Time since start: 22572.93s, 	Step: 38914, 	{'train/accuracy': 0.6621062159538269, 'train/loss': 1.7953685522079468, 'train/bleu': 33.04036161641257, 'validation/accuracy': 0.6734976768493652, 'validation/loss': 1.7133029699325562, 'validation/bleu': 29.526641713747885, 'validation/num_examples': 3000, 'test/accuracy': 0.6868149638175964, 'test/loss': 1.6366941928863525, 'test/bleu': 29.106778132713853, 'test/num_examples': 3003, 'score': 13466.982836484909, 'total_duration': 22572.9270131588, 'accumulated_submission_time': 13466.982836484909, 'accumulated_eval_time': 9103.34075307846, 'accumulated_logging_time': 0.3195013999938965}
I0306 01:29:45.584950 139585807927040 logging_writer.py:48] [38914] accumulated_eval_time=9103.34, accumulated_logging_time=0.319501, accumulated_submission_time=13467, global_step=38914, preemption_count=0, score=13467, test/accuracy=0.686815, test/bleu=29.1068, test/loss=1.63669, test/num_examples=3003, total_duration=22572.9, train/accuracy=0.662106, train/bleu=33.0404, train/loss=1.79537, validation/accuracy=0.673498, validation/bleu=29.5266, validation/loss=1.7133, validation/num_examples=3000
I0306 01:30:15.468096 139585799534336 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.22703656554222107, loss=3.9581193923950195
I0306 01:30:49.859066 139585807927040 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.22887635231018066, loss=3.914834499359131
I0306 01:31:24.320807 139585799534336 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.20689502358436584, loss=3.8640823364257812
I0306 01:31:58.766386 139585807927040 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.22107715904712677, loss=3.921300172805786
I0306 01:32:33.231134 139585799534336 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.6079686880111694, loss=3.976358652114868
I0306 01:33:07.697956 139585807927040 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2536541819572449, loss=3.9290921688079834
I0306 01:33:42.130701 139585799534336 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.2469843029975891, loss=3.9402456283569336
I0306 01:34:16.586277 139585807927040 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2845584452152252, loss=3.884667158126831
I0306 01:34:50.993150 139585799534336 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.24648597836494446, loss=3.9021410942077637
I0306 01:35:25.461202 139585807927040 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.26007917523384094, loss=3.8848683834075928
I0306 01:35:59.940011 139585799534336 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.22828145325183868, loss=3.905163288116455
I0306 01:36:34.440729 139585807927040 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.22338680922985077, loss=3.8636555671691895
I0306 01:37:08.933286 139585799534336 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.2966731786727905, loss=7.960888862609863
I0306 01:37:43.350784 139585807927040 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.4369419515132904, loss=5.685500144958496
I0306 01:38:17.794837 139585799534336 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.0763320922851562, loss=5.63330078125
I0306 01:38:52.277072 139585807927040 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6854583621025085, loss=5.5144429206848145
I0306 01:39:26.760852 139585799534336 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3533898591995239, loss=5.470141410827637
I0306 01:40:01.229619 139585807927040 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6169314384460449, loss=5.504051208496094
I0306 01:40:35.735733 139585799534336 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.8674586415290833, loss=5.450272083282471
I0306 01:41:10.261529 139585807927040 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.9494876861572266, loss=5.521608352661133
I0306 01:41:44.765742 139585799534336 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.5585809350013733, loss=5.383262634277344
I0306 01:42:19.261653 139585807927040 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.9391379356384277, loss=5.341434001922607
I0306 01:42:53.794184 139585799534336 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.33687546849250793, loss=3.957838773727417
I0306 01:43:28.332964 139585807927040 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.6518362760543823, loss=3.9118006229400635
I0306 01:43:45.581797 139728945038528 spec.py:321] Evaluating on the training split.
I0306 01:43:48.163024 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:46:40.883217 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 01:46:43.450197 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:49:26.051824 139728945038528 spec.py:349] Evaluating on the test split.
I0306 01:49:28.623791 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 01:51:57.237336 139728945038528 submission_runner.py:469] Time since start: 23904.59s, 	Step: 41351, 	{'train/accuracy': 0.6548977494239807, 'train/loss': 1.8532079458236694, 'train/bleu': 32.18002793011718, 'validation/accuracy': 0.6690974831581116, 'validation/loss': 1.7556068897247314, 'validation/bleu': 28.939800157785346, 'validation/num_examples': 3000, 'test/accuracy': 0.6816591620445251, 'test/loss': 1.6871181726455688, 'test/bleu': 28.38503886598118, 'test/num_examples': 3003, 'score': 14306.836514949799, 'total_duration': 23904.592145204544, 'accumulated_submission_time': 14306.836514949799, 'accumulated_eval_time': 9594.996240377426, 'accumulated_logging_time': 0.3401522636413574}
I0306 01:51:57.250446 139585799534336 logging_writer.py:48] [41351] accumulated_eval_time=9595, accumulated_logging_time=0.340152, accumulated_submission_time=14306.8, global_step=41351, preemption_count=0, score=14306.8, test/accuracy=0.681659, test/bleu=28.385, test/loss=1.68712, test/num_examples=3003, total_duration=23904.6, train/accuracy=0.654898, train/bleu=32.18, train/loss=1.85321, validation/accuracy=0.669097, validation/bleu=28.9398, validation/loss=1.75561, validation/num_examples=3000
I0306 01:52:14.464707 139585807927040 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.23752140998840332, loss=3.9175992012023926
I0306 01:52:48.864747 139585799534336 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.22896701097488403, loss=3.8964529037475586
I0306 01:53:23.339794 139585807927040 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.24206139147281647, loss=3.9878482818603516
I0306 01:53:57.806802 139585799534336 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.23495052754878998, loss=3.889130115509033
I0306 01:54:32.320468 139585807927040 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.23822203278541565, loss=3.926631450653076
I0306 01:55:06.823778 139585799534336 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.20456531643867493, loss=3.858581066131592
I0306 01:55:41.330247 139585807927040 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2160792052745819, loss=3.8809659481048584
I0306 01:56:15.851174 139585799534336 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.22426198422908783, loss=3.8290069103240967
I0306 01:56:50.449468 139585807927040 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.25098302960395813, loss=3.8749356269836426
I0306 01:57:24.966065 139585799534336 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.2361263930797577, loss=3.8704822063446045
I0306 01:57:59.516552 139585807927040 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.24415579438209534, loss=3.9040112495422363
I0306 01:58:34.063294 139585799534336 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2506711781024933, loss=3.891356945037842
I0306 01:59:08.611689 139585807927040 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.22802932560443878, loss=3.946155548095703
I0306 01:59:43.131956 139585799534336 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2720213532447815, loss=3.918309450149536
I0306 02:00:17.652403 139585807927040 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.21770496666431427, loss=3.8902177810668945
I0306 02:00:52.170727 139585799534336 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.2407490313053131, loss=3.8556759357452393
I0306 02:01:26.723397 139585807927040 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.2356855869293213, loss=3.9256093502044678
I0306 02:02:01.234190 139585799534336 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.236399307847023, loss=3.936154842376709
I0306 02:02:35.762547 139585807927040 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.2646605968475342, loss=3.8460209369659424
I0306 02:03:10.300728 139585799534336 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.23878976702690125, loss=3.862424850463867
I0306 02:03:44.808016 139585807927040 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.2540278136730194, loss=3.8974227905273438
I0306 02:04:19.328340 139585799534336 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.30117374658584595, loss=3.9106199741363525
I0306 02:04:53.851302 139585807927040 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.2290515899658203, loss=3.850935697555542
I0306 02:05:28.380901 139585799534336 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.2650940418243408, loss=3.879255771636963
I0306 02:05:57.380450 139728945038528 spec.py:321] Evaluating on the training split.
I0306 02:05:59.964477 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:09:04.294558 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 02:09:06.862319 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:11:56.739715 139728945038528 spec.py:349] Evaluating on the test split.
I0306 02:11:59.317805 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:14:28.960904 139728945038528 submission_runner.py:469] Time since start: 25256.32s, 	Step: 43785, 	{'train/accuracy': 0.6581637859344482, 'train/loss': 1.8355387449264526, 'train/bleu': 32.47519658754718, 'validation/accuracy': 0.6738560795783997, 'validation/loss': 1.7100189924240112, 'validation/bleu': 29.491395898936098, 'validation/num_examples': 3000, 'test/accuracy': 0.6895145177841187, 'test/loss': 1.632834553718567, 'test/bleu': 29.402075118264293, 'test/num_examples': 3003, 'score': 15146.827372789383, 'total_duration': 25256.31569504738, 'accumulated_submission_time': 15146.827372789383, 'accumulated_eval_time': 10106.576641321182, 'accumulated_logging_time': 0.3613290786743164}
I0306 02:14:28.974070 139585807927040 logging_writer.py:48] [43785] accumulated_eval_time=10106.6, accumulated_logging_time=0.361329, accumulated_submission_time=15146.8, global_step=43785, preemption_count=0, score=15146.8, test/accuracy=0.689515, test/bleu=29.4021, test/loss=1.63283, test/num_examples=3003, total_duration=25256.3, train/accuracy=0.658164, train/bleu=32.4752, train/loss=1.83554, validation/accuracy=0.673856, validation/bleu=29.4914, validation/loss=1.71002, validation/num_examples=3000
I0306 02:14:34.496482 139585799534336 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.24518360197544098, loss=3.873150110244751
I0306 02:15:08.800241 139585807927040 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3205678164958954, loss=3.852747678756714
I0306 02:15:43.210982 139585799534336 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2873486876487732, loss=3.8741211891174316
I0306 02:16:17.750282 139585807927040 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.2306285947561264, loss=3.8916738033294678
I0306 02:16:52.319710 139585799534336 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2800884544849396, loss=3.883086919784546
I0306 02:17:26.806990 139585807927040 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.22597679495811462, loss=3.8634278774261475
I0306 02:18:01.276469 139585799534336 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.23142394423484802, loss=3.8964123725891113
I0306 02:18:35.736366 139585807927040 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2379622608423233, loss=3.893037796020508
I0306 02:19:10.182648 139585799534336 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2599659562110901, loss=3.9418537616729736
I0306 02:19:44.698560 139585807927040 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.22823497653007507, loss=3.874084949493408
I0306 02:20:19.162938 139585799534336 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.25000816583633423, loss=3.9004733562469482
I0306 02:20:53.616006 139585807927040 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.24312762916088104, loss=3.894702434539795
I0306 02:21:28.124164 139585799534336 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.24950911104679108, loss=3.8898749351501465
I0306 02:22:02.637225 139585807927040 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.28234007954597473, loss=3.8391449451446533
I0306 02:22:37.135667 139585799534336 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2349260002374649, loss=3.856395959854126
I0306 02:23:11.675372 139585807927040 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.22944362461566925, loss=3.92942214012146
I0306 02:23:46.225184 139585799534336 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.28139567375183105, loss=3.864358425140381
I0306 02:24:20.807110 139585807927040 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2806002199649811, loss=3.8722105026245117
I0306 02:24:55.327664 139585799534336 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.2782417833805084, loss=3.8438358306884766
I0306 02:25:29.892033 139585807927040 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.23334071040153503, loss=3.8614487648010254
I0306 02:26:04.461531 139585799534336 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.26039770245552063, loss=3.9328463077545166
I0306 02:26:39.000621 139585807927040 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2219700664281845, loss=3.8946144580841064
I0306 02:27:13.544819 139585799534336 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.2631751000881195, loss=3.9220616817474365
I0306 02:27:48.105070 139585807927040 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2327832281589508, loss=3.8790385723114014
I0306 02:28:22.664380 139585799534336 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2560043931007385, loss=3.9397330284118652
I0306 02:28:29.224668 139728945038528 spec.py:321] Evaluating on the training split.
I0306 02:28:31.812680 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:31:28.044428 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 02:31:30.626534 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:34:05.058410 139728945038528 spec.py:349] Evaluating on the test split.
I0306 02:34:07.628713 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:36:33.849050 139728945038528 submission_runner.py:469] Time since start: 26581.20s, 	Step: 46220, 	{'train/accuracy': 0.6648356914520264, 'train/loss': 1.7803852558135986, 'train/bleu': 32.83018393407643, 'validation/accuracy': 0.6768842935562134, 'validation/loss': 1.6940646171569824, 'validation/bleu': 29.590773203338436, 'validation/num_examples': 3000, 'test/accuracy': 0.6891322135925293, 'test/loss': 1.6162021160125732, 'test/bleu': 28.902535716821703, 'test/num_examples': 3003, 'score': 15986.932428598404, 'total_duration': 26581.20386004448, 'accumulated_submission_time': 15986.932428598404, 'accumulated_eval_time': 10591.200972557068, 'accumulated_logging_time': 0.38266682624816895}
I0306 02:36:33.863347 139585807927040 logging_writer.py:48] [46220] accumulated_eval_time=10591.2, accumulated_logging_time=0.382667, accumulated_submission_time=15986.9, global_step=46220, preemption_count=0, score=15986.9, test/accuracy=0.689132, test/bleu=28.9025, test/loss=1.6162, test/num_examples=3003, total_duration=26581.2, train/accuracy=0.664836, train/bleu=32.8302, train/loss=1.78039, validation/accuracy=0.676884, validation/bleu=29.5908, validation/loss=1.69406, validation/num_examples=3000
I0306 02:37:01.732954 139585799534336 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.24320101737976074, loss=3.881913900375366
I0306 02:37:36.117470 139585807927040 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.21943485736846924, loss=3.886713981628418
I0306 02:38:10.632313 139585799534336 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.24772463738918304, loss=3.875148057937622
I0306 02:38:45.160436 139585807927040 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.24516314268112183, loss=3.9158120155334473
I0306 02:39:19.682018 139585799534336 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.24283431470394135, loss=3.879566192626953
I0306 02:39:54.206916 139585807927040 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2513446807861328, loss=3.8853132724761963
I0306 02:40:28.692646 139585799534336 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2336123138666153, loss=3.865295648574829
I0306 02:41:03.182858 139585807927040 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.22723276913166046, loss=3.9402644634246826
I0306 02:41:37.673812 139585799534336 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.24649159610271454, loss=3.891270160675049
I0306 02:42:12.175296 139585807927040 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.25805407762527466, loss=3.9160542488098145
I0306 02:42:46.672970 139585799534336 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2474835067987442, loss=3.874330759048462
I0306 02:43:21.192273 139585807927040 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.30141791701316833, loss=3.8732802867889404
I0306 02:43:55.690907 139585799534336 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2521476149559021, loss=3.9245402812957764
I0306 02:44:30.183585 139585807927040 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.25664910674095154, loss=3.9392471313476562
I0306 02:45:04.686281 139585799534336 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.22564609348773956, loss=3.8711087703704834
I0306 02:45:39.227727 139585807927040 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.23837682604789734, loss=3.901171922683716
I0306 02:46:13.738771 139585799534336 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.25844117999076843, loss=3.867910385131836
I0306 02:46:48.222617 139585807927040 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.22302953898906708, loss=3.8647427558898926
I0306 02:47:22.744075 139585799534336 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.2924990653991699, loss=3.877842426300049
I0306 02:47:57.258303 139585807927040 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.22576232254505157, loss=3.892852783203125
I0306 02:48:31.781874 139585799534336 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.23447231948375702, loss=3.8713769912719727
I0306 02:49:06.267245 139585807927040 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.2511674463748932, loss=3.8853232860565186
I0306 02:49:40.807304 139585799534336 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.27283304929733276, loss=3.851555824279785
I0306 02:50:15.308000 139585807927040 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2548896372318268, loss=3.8241147994995117
I0306 02:50:33.949001 139728945038528 spec.py:321] Evaluating on the training split.
I0306 02:50:36.538614 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:53:46.224068 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 02:53:48.799789 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:56:25.771539 139728945038528 spec.py:349] Evaluating on the test split.
I0306 02:56:28.353578 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 02:58:49.622586 139728945038528 submission_runner.py:469] Time since start: 27916.98s, 	Step: 48655, 	{'train/accuracy': 0.6591668128967285, 'train/loss': 1.8186081647872925, 'train/bleu': 32.76420166333463, 'validation/accuracy': 0.6789237260818481, 'validation/loss': 1.6839874982833862, 'validation/bleu': 29.88723209938831, 'validation/num_examples': 3000, 'test/accuracy': 0.6919012665748596, 'test/loss': 1.6048399209976196, 'test/bleu': 29.40601672076325, 'test/num_examples': 3003, 'score': 16826.880041360855, 'total_duration': 27916.977389335632, 'accumulated_submission_time': 16826.880041360855, 'accumulated_eval_time': 11086.87451839447, 'accumulated_logging_time': 0.4049539566040039}
I0306 02:58:49.636270 139585799534336 logging_writer.py:48] [48655] accumulated_eval_time=11086.9, accumulated_logging_time=0.404954, accumulated_submission_time=16826.9, global_step=48655, preemption_count=0, score=16826.9, test/accuracy=0.691901, test/bleu=29.406, test/loss=1.60484, test/num_examples=3003, total_duration=27917, train/accuracy=0.659167, train/bleu=32.7642, train/loss=1.81861, validation/accuracy=0.678924, validation/bleu=29.8872, validation/loss=1.68399, validation/num_examples=3000
I0306 02:59:05.445822 139585807927040 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.25361374020576477, loss=3.9290900230407715
I0306 02:59:39.833219 139585799534336 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.23156018555164337, loss=3.7884459495544434
I0306 03:00:14.281787 139585807927040 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.28278082609176636, loss=3.8223628997802734
I0306 03:00:48.759999 139585799534336 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.24736733734607697, loss=3.8466033935546875
I0306 03:01:23.278953 139585807927040 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.24377405643463135, loss=3.8452370166778564
I0306 03:01:57.757039 139585799534336 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2399066984653473, loss=3.915555238723755
I0306 03:02:32.288478 139585807927040 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.24808819591999054, loss=3.91306209564209
I0306 03:03:06.772940 139585799534336 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.33163782954216003, loss=3.876145601272583
I0306 03:03:41.309730 139585807927040 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.26514989137649536, loss=3.8953185081481934
I0306 03:04:15.787740 139585799534336 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.29187238216400146, loss=3.899085760116577
I0306 03:04:50.285275 139585807927040 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.28558874130249023, loss=3.8769876956939697
I0306 03:05:24.785060 139585799534336 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.24831339716911316, loss=3.9037461280822754
I0306 03:05:59.284611 139585807927040 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.24652473628520966, loss=3.8732903003692627
I0306 03:06:33.792378 139585799534336 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2423895001411438, loss=3.8704946041107178
I0306 03:07:08.268764 139585807927040 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.26812943816185, loss=3.839808702468872
I0306 03:07:42.760152 139585799534336 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3232399821281433, loss=3.8442535400390625
I0306 03:08:17.265364 139585807927040 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.25736096501350403, loss=3.8240203857421875
I0306 03:08:51.784062 139585799534336 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.2658838927745819, loss=3.8704214096069336
I0306 03:09:26.316922 139585807927040 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.2570038437843323, loss=3.876129388809204
I0306 03:10:00.827084 139585799534336 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.2515726089477539, loss=3.831435441970825
I0306 03:10:35.355152 139585807927040 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.27345678210258484, loss=3.8455300331115723
I0306 03:11:09.863871 139585799534336 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2506031095981598, loss=3.856321096420288
I0306 03:11:44.434561 139585807927040 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2855648696422577, loss=3.865412712097168
I0306 03:12:19.013484 139585799534336 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.28752902150154114, loss=3.8080384731292725
I0306 03:12:49.763943 139728945038528 spec.py:321] Evaluating on the training split.
I0306 03:12:52.353098 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 03:15:45.432362 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 03:15:48.007731 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 03:18:21.741035 139728945038528 spec.py:349] Evaluating on the test split.
I0306 03:18:24.324907 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 03:20:45.460736 139728945038528 submission_runner.py:469] Time since start: 29232.82s, 	Step: 51090, 	{'train/accuracy': 0.6771486401557922, 'train/loss': 1.7057360410690308, 'train/bleu': 33.412505197693676, 'validation/accuracy': 0.6787012219429016, 'validation/loss': 1.6801316738128662, 'validation/bleu': 29.96067433413804, 'validation/num_examples': 3000, 'test/accuracy': 0.6920403242111206, 'test/loss': 1.6022100448608398, 'test/bleu': 29.503772858230175, 'test/num_examples': 3003, 'score': 17666.867208242416, 'total_duration': 29232.815542697906, 'accumulated_submission_time': 17666.867208242416, 'accumulated_eval_time': 11562.571260929108, 'accumulated_logging_time': 0.42783093452453613}
I0306 03:20:45.474991 139585807927040 logging_writer.py:48] [51090] accumulated_eval_time=11562.6, accumulated_logging_time=0.427831, accumulated_submission_time=17666.9, global_step=51090, preemption_count=0, score=17666.9, test/accuracy=0.69204, test/bleu=29.5038, test/loss=1.60221, test/num_examples=3003, total_duration=29232.8, train/accuracy=0.677149, train/bleu=33.4125, train/loss=1.70574, validation/accuracy=0.678701, validation/bleu=29.9607, validation/loss=1.68013, validation/num_examples=3000
I0306 03:20:49.250606 139585799534336 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.2716766893863678, loss=3.89707612991333
I0306 03:21:23.655943 139585807927040 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.28006842732429504, loss=3.8571481704711914
I0306 03:21:58.124840 139585799534336 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.24126139283180237, loss=3.8548614978790283
I0306 03:22:32.638070 139585807927040 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.2622668445110321, loss=3.8173279762268066
I0306 03:23:07.170460 139585799534336 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2322404533624649, loss=3.862431764602661
I0306 03:23:41.700738 139585807927040 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.23744584619998932, loss=3.807905435562134
I0306 03:24:16.194380 139585799534336 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2737331688404083, loss=3.792712688446045
I0306 03:24:50.682199 139585807927040 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.24414747953414917, loss=3.8626418113708496
I0306 03:25:25.197071 139585799534336 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2709170877933502, loss=3.8730180263519287
I0306 03:25:59.682247 139585807927040 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.25590965151786804, loss=3.8341457843780518
I0306 03:26:34.201388 139585799534336 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.2782144248485565, loss=3.8090503215789795
I0306 03:27:08.707474 139585807927040 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.25019481778144836, loss=3.9195456504821777
I0306 03:27:43.273884 139585799534336 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.29313090443611145, loss=3.8431684970855713
I0306 03:28:17.835819 139585807927040 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.2675324082374573, loss=3.835705041885376
I0306 03:28:52.388649 139585799534336 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.26800915598869324, loss=3.8864219188690186
I0306 03:29:26.935364 139585807927040 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3022962510585785, loss=3.8603453636169434
I0306 03:30:01.502898 139585799534336 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2654835879802704, loss=3.873326539993286
I0306 03:30:36.054432 139585807927040 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.24109403789043427, loss=3.8566997051239014
I0306 03:31:10.584032 139585799534336 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.24921388924121857, loss=3.8143770694732666
I0306 03:31:45.125958 139585807927040 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.2487853318452835, loss=3.9052512645721436
I0306 03:32:19.682135 139585799534336 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.31921422481536865, loss=3.843372106552124
I0306 03:32:54.226850 139585807927040 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.35832566022872925, loss=3.8707690238952637
I0306 03:33:28.768773 139585799534336 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.25347888469696045, loss=3.8400180339813232
I0306 03:34:03.292715 139585807927040 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.23551981151103973, loss=3.854116678237915
I0306 03:34:37.850575 139585799534336 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.26803073287010193, loss=3.8508574962615967
I0306 03:34:45.475480 139728945038528 spec.py:321] Evaluating on the training split.
I0306 03:34:48.076672 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 03:38:04.180598 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 03:38:06.762637 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 03:40:58.079796 139728945038528 spec.py:349] Evaluating on the test split.
I0306 03:41:00.651176 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 03:43:22.838856 139728945038528 submission_runner.py:469] Time since start: 30590.19s, 	Step: 53523, 	{'train/accuracy': 0.6669300198554993, 'train/loss': 1.775338888168335, 'train/bleu': 33.32501722718088, 'validation/accuracy': 0.6813957095146179, 'validation/loss': 1.6677093505859375, 'validation/bleu': 29.834117716804315, 'validation/num_examples': 3000, 'test/accuracy': 0.6934654116630554, 'test/loss': 1.5937355756759644, 'test/bleu': 29.575533275258035, 'test/num_examples': 3003, 'score': 18506.72165608406, 'total_duration': 30590.193658828735, 'accumulated_submission_time': 18506.72165608406, 'accumulated_eval_time': 12079.934581756592, 'accumulated_logging_time': 0.45119237899780273}
I0306 03:43:22.853976 139585807927040 logging_writer.py:48] [53523] accumulated_eval_time=12079.9, accumulated_logging_time=0.451192, accumulated_submission_time=18506.7, global_step=53523, preemption_count=0, score=18506.7, test/accuracy=0.693465, test/bleu=29.5755, test/loss=1.59374, test/num_examples=3003, total_duration=30590.2, train/accuracy=0.66693, train/bleu=33.325, train/loss=1.77534, validation/accuracy=0.681396, validation/bleu=29.8341, validation/loss=1.66771, validation/num_examples=3000
I0306 03:43:49.651865 139585799534336 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2604607045650482, loss=3.8880856037139893
I0306 03:44:24.034220 139585807927040 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2635010778903961, loss=3.809051275253296
I0306 03:44:58.502461 139585799534336 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.251632422208786, loss=3.8859200477600098
I0306 03:45:33.000647 139585807927040 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.41274550557136536, loss=3.9136385917663574
I0306 03:46:07.490154 139585799534336 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.22743497788906097, loss=3.8280580043792725
I0306 03:46:41.978173 139585807927040 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.380221962928772, loss=3.8764936923980713
I0306 03:47:16.472140 139585799534336 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.28073275089263916, loss=3.848731517791748
I0306 03:47:50.992907 139585807927040 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.23537179827690125, loss=3.846132278442383
I0306 03:48:25.533029 139585799534336 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.23164986073970795, loss=3.7988104820251465
I0306 03:49:00.030126 139585807927040 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2524471580982208, loss=3.8055498600006104
I0306 03:49:34.582735 139585799534336 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.2464483082294464, loss=3.855294942855835
I0306 03:50:09.118386 139585807927040 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.2589423656463623, loss=3.8952505588531494
I0306 03:50:43.715962 139585799534336 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2530665099620819, loss=3.865056276321411
I0306 03:51:18.278622 139585807927040 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.30998992919921875, loss=3.827568769454956
I0306 03:51:52.817288 139585799534336 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2364174872636795, loss=3.863765239715576
I0306 03:52:27.334839 139585807927040 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2336129993200302, loss=3.826622486114502
I0306 03:53:01.898043 139585799534336 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2497130185365677, loss=3.7846415042877197
I0306 03:53:36.429419 139585807927040 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.24583332240581512, loss=3.901608467102051
I0306 03:54:10.980155 139585799534336 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.24660775065422058, loss=3.794252395629883
I0306 03:54:45.525308 139585807927040 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.23507879674434662, loss=3.791665554046631
I0306 03:55:20.090992 139585799534336 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.25197431445121765, loss=3.8412249088287354
I0306 03:55:54.660061 139585807927040 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.29466190934181213, loss=3.8211469650268555
I0306 03:56:29.198585 139585799534336 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2906595468521118, loss=3.8599860668182373
I0306 03:57:03.731815 139585807927040 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2646245062351227, loss=3.9123146533966064
I0306 03:57:23.048250 139728945038528 spec.py:321] Evaluating on the training split.
I0306 03:57:25.641844 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:00:48.329060 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 04:00:50.904786 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:03:21.413662 139728945038528 spec.py:349] Evaluating on the test split.
I0306 04:03:23.994007 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:05:43.987473 139728945038528 submission_runner.py:469] Time since start: 31931.34s, 	Step: 55957, 	{'train/accuracy': 0.666933000087738, 'train/loss': 1.770076036453247, 'train/bleu': 33.305574965278794, 'validation/accuracy': 0.6826317310333252, 'validation/loss': 1.6626839637756348, 'validation/bleu': 30.028262973587278, 'validation/num_examples': 3000, 'test/accuracy': 0.6949136853218079, 'test/loss': 1.5791844129562378, 'test/bleu': 29.408539074892637, 'test/num_examples': 3003, 'score': 19346.77387547493, 'total_duration': 31931.342279672623, 'accumulated_submission_time': 19346.77387547493, 'accumulated_eval_time': 12580.87377333641, 'accumulated_logging_time': 0.4745297431945801}
I0306 04:05:44.002238 139585799534336 logging_writer.py:48] [55957] accumulated_eval_time=12580.9, accumulated_logging_time=0.47453, accumulated_submission_time=19346.8, global_step=55957, preemption_count=0, score=19346.8, test/accuracy=0.694914, test/bleu=29.4085, test/loss=1.57918, test/num_examples=3003, total_duration=31931.3, train/accuracy=0.666933, train/bleu=33.3056, train/loss=1.77008, validation/accuracy=0.682632, validation/bleu=30.0283, validation/loss=1.66268, validation/num_examples=3000
I0306 04:05:59.132592 139585807927040 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2686362564563751, loss=3.822543144226074
I0306 04:06:33.501755 139585799534336 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.24337685108184814, loss=3.8697621822357178
I0306 04:07:07.950824 139585807927040 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2577289342880249, loss=3.847501039505005
I0306 04:07:42.450959 139585799534336 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.24025876820087433, loss=3.7759907245635986
I0306 04:08:16.952754 139585807927040 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.23168452084064484, loss=3.7632322311401367
I0306 04:08:51.465748 139585799534336 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.268653005361557, loss=3.8645708560943604
I0306 04:09:25.948757 139585807927040 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.24016156792640686, loss=3.8594467639923096
I0306 04:10:00.443542 139585799534336 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.2887610197067261, loss=3.841646671295166
I0306 04:10:34.889906 139585807927040 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.2893516719341278, loss=3.788588047027588
I0306 04:11:09.439457 139585799534336 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2776520848274231, loss=3.812753915786743
I0306 04:11:43.954394 139585807927040 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2687179148197174, loss=3.882275342941284
I0306 04:12:18.463804 139585799534336 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.27424946427345276, loss=3.8676702976226807
I0306 04:12:52.969690 139585807927040 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2430441528558731, loss=3.8232502937316895
I0306 04:13:27.486771 139585799534336 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.26780059933662415, loss=3.883925437927246
I0306 04:14:01.997071 139585807927040 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.24929045140743256, loss=3.8213605880737305
I0306 04:14:36.489107 139585799534336 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3117170035839081, loss=3.8329243659973145
I0306 04:15:11.038876 139585807927040 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2611301839351654, loss=3.8229823112487793
I0306 04:15:45.598770 139585799534336 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2546568512916565, loss=3.822458505630493
I0306 04:16:20.157770 139585807927040 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.26076167821884155, loss=3.843248128890991
I0306 04:16:54.666705 139585799534336 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.25438299775123596, loss=3.7868542671203613
I0306 04:17:29.175110 139585807927040 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2983860969543457, loss=3.8543479442596436
I0306 04:18:03.704952 139585799534336 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.24960821866989136, loss=3.7726242542266846
I0306 04:18:38.224702 139585807927040 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.2497013360261917, loss=3.837751626968384
I0306 04:19:12.786714 139585799534336 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2620021402835846, loss=3.8741977214813232
I0306 04:19:44.221538 139728945038528 spec.py:321] Evaluating on the training split.
I0306 04:19:46.821698 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:22:46.336029 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 04:22:48.911014 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:25:30.874753 139728945038528 spec.py:349] Evaluating on the test split.
I0306 04:25:33.452323 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:28:01.896923 139728945038528 submission_runner.py:469] Time since start: 33269.25s, 	Step: 58392, 	{'train/accuracy': 0.6707125902175903, 'train/loss': 1.7425974607467651, 'train/bleu': 33.46114318731096, 'validation/accuracy': 0.681074321269989, 'validation/loss': 1.660438895225525, 'validation/bleu': 30.04406041744336, 'validation/num_examples': 3000, 'test/accuracy': 0.6967558860778809, 'test/loss': 1.5745389461517334, 'test/bleu': 29.827795343351212, 'test/num_examples': 3003, 'score': 20186.85280251503, 'total_duration': 33269.251730680466, 'accumulated_submission_time': 20186.85280251503, 'accumulated_eval_time': 13078.549107789993, 'accumulated_logging_time': 0.49728965759277344}
I0306 04:28:01.912039 139585807927040 logging_writer.py:48] [58392] accumulated_eval_time=13078.5, accumulated_logging_time=0.49729, accumulated_submission_time=20186.9, global_step=58392, preemption_count=0, score=20186.9, test/accuracy=0.696756, test/bleu=29.8278, test/loss=1.57454, test/num_examples=3003, total_duration=33269.3, train/accuracy=0.670713, train/bleu=33.4611, train/loss=1.7426, validation/accuracy=0.681074, validation/bleu=30.0441, validation/loss=1.66044, validation/num_examples=3000
I0306 04:28:05.027252 139585799534336 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2755710184574127, loss=3.875155448913574
I0306 04:28:39.425147 139585807927040 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.25137296319007874, loss=3.7530319690704346
I0306 04:29:13.875661 139585799534336 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2527863681316376, loss=3.8247361183166504
I0306 04:29:48.367001 139585807927040 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.27905598282814026, loss=3.857938528060913
I0306 04:30:22.917955 139585799534336 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.3911930322647095, loss=3.9051010608673096
I0306 04:30:57.439941 139585807927040 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.2427399456501007, loss=3.816746950149536
I0306 04:31:31.967229 139585799534336 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.27983155846595764, loss=3.9118764400482178
I0306 04:32:06.504458 139585807927040 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.242974191904068, loss=3.8484315872192383
I0306 04:32:41.052280 139585799534336 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.25143197178840637, loss=3.808800220489502
I0306 04:33:15.607977 139585807927040 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2606106698513031, loss=3.851668119430542
I0306 04:33:50.161720 139585799534336 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.28366968035697937, loss=3.808690071105957
I0306 04:34:24.697794 139585807927040 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2363259494304657, loss=3.851376533508301
I0306 04:34:59.193608 139585799534336 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.24940288066864014, loss=3.800903558731079
I0306 04:35:33.723620 139585807927040 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3026809096336365, loss=3.798311710357666
I0306 04:36:08.236474 139585799534336 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.2910138964653015, loss=3.846867084503174
I0306 04:36:42.779835 139585807927040 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.2412872016429901, loss=3.7893266677856445
I0306 04:37:17.291225 139585799534336 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.23477208614349365, loss=3.774054527282715
I0306 04:37:51.803857 139585807927040 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.2992199957370758, loss=3.871234655380249
I0306 04:38:26.322912 139585799534336 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.2390352487564087, loss=3.7754039764404297
I0306 04:39:00.839919 139585807927040 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.27582812309265137, loss=3.8250744342803955
I0306 04:39:35.380775 139585799534336 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2775360345840454, loss=3.877636432647705
I0306 04:40:09.916148 139585807927040 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.29598286747932434, loss=3.8144125938415527
I0306 04:40:44.455392 139585799534336 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.25309741497039795, loss=3.7886648178100586
I0306 04:41:18.991123 139585807927040 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.2813250720500946, loss=3.8045883178710938
I0306 04:41:53.531022 139585799534336 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.29812949895858765, loss=3.7311341762542725
I0306 04:42:02.162942 139728945038528 spec.py:321] Evaluating on the training split.
I0306 04:42:04.756229 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:45:00.090381 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 04:45:02.667491 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:47:37.194195 139728945038528 spec.py:349] Evaluating on the test split.
I0306 04:47:39.775076 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 04:50:02.096123 139728945038528 submission_runner.py:469] Time since start: 34589.45s, 	Step: 60826, 	{'train/accuracy': 0.6699889898300171, 'train/loss': 1.7481650114059448, 'train/bleu': 33.44281571904775, 'validation/accuracy': 0.6814327836036682, 'validation/loss': 1.6548084020614624, 'validation/bleu': 29.71787668238941, 'validation/num_examples': 3000, 'test/accuracy': 0.6954929828643799, 'test/loss': 1.5683162212371826, 'test/bleu': 29.90589343113518, 'test/num_examples': 3003, 'score': 21026.96267771721, 'total_duration': 34589.45092916489, 'accumulated_submission_time': 21026.96267771721, 'accumulated_eval_time': 13558.482238769531, 'accumulated_logging_time': 0.521662712097168}
I0306 04:50:02.112057 139585807927040 logging_writer.py:48] [60826] accumulated_eval_time=13558.5, accumulated_logging_time=0.521663, accumulated_submission_time=21027, global_step=60826, preemption_count=0, score=21027, test/accuracy=0.695493, test/bleu=29.9059, test/loss=1.56832, test/num_examples=3003, total_duration=34589.5, train/accuracy=0.669989, train/bleu=33.4428, train/loss=1.74817, validation/accuracy=0.681433, validation/bleu=29.7179, validation/loss=1.65481, validation/num_examples=3000
I0306 04:50:27.865669 139585799534336 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.2634863257408142, loss=3.855879068374634
I0306 04:51:02.287106 139585807927040 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2531868815422058, loss=3.8454763889312744
I0306 04:51:36.807157 139585799534336 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.27726951241493225, loss=3.833294630050659
I0306 04:52:11.316842 139585807927040 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.2781563401222229, loss=3.819420576095581
I0306 04:52:45.884315 139585799534336 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.36029914021492004, loss=3.874567985534668
I0306 04:53:20.463171 139585807927040 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.26248541474342346, loss=3.7772645950317383
I0306 04:53:54.999683 139585799534336 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2598777711391449, loss=3.7988266944885254
I0306 04:54:29.564923 139585807927040 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.3065330982208252, loss=3.8013644218444824
I0306 04:55:04.114777 139585799534336 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.2639733552932739, loss=3.854299783706665
I0306 04:55:38.642521 139585807927040 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.2824305295944214, loss=3.7457427978515625
I0306 04:56:13.201606 139585799534336 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.27809178829193115, loss=3.78192138671875
I0306 04:56:47.722708 139585807927040 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.263468474149704, loss=3.8026561737060547
I0306 04:57:22.265239 139585799534336 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.24895595014095306, loss=3.8183844089508057
I0306 04:57:56.829304 139585807927040 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.2542416751384735, loss=3.8097383975982666
I0306 04:58:31.339128 139585799534336 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.2509494721889496, loss=3.8504254817962646
I0306 04:59:05.878726 139585807927040 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.2769700586795807, loss=3.861017942428589
I0306 04:59:40.434804 139585799534336 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.27507975697517395, loss=3.8577210903167725
I0306 05:00:14.998749 139585807927040 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2712075710296631, loss=3.802119731903076
I0306 05:00:49.531234 139585799534336 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.29957786202430725, loss=3.8735527992248535
I0306 05:01:24.131469 139585807927040 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2654688358306885, loss=3.8329825401306152
I0306 05:01:58.693317 139585799534336 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2688864767551422, loss=3.765500068664551
I0306 05:02:33.221974 139585807927040 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2517397701740265, loss=3.7854347229003906
I0306 05:03:07.712828 139585799534336 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.24700656533241272, loss=3.834925889968872
I0306 05:03:42.243049 139585807927040 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2856837511062622, loss=3.883984088897705
I0306 05:04:02.263928 139728945038528 spec.py:321] Evaluating on the training split.
I0306 05:04:04.852078 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:08:11.396582 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 05:08:13.967626 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:11:25.325538 139728945038528 spec.py:349] Evaluating on the test split.
I0306 05:11:27.915880 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:14:22.669927 139728945038528 submission_runner.py:469] Time since start: 36050.02s, 	Step: 63259, 	{'train/accuracy': 0.6864909529685974, 'train/loss': 1.6464632749557495, 'train/bleu': 34.64048438768395, 'validation/accuracy': 0.6840160489082336, 'validation/loss': 1.6528875827789307, 'validation/bleu': 29.941256370407146, 'validation/num_examples': 3000, 'test/accuracy': 0.6983663439750671, 'test/loss': 1.5656907558441162, 'test/bleu': 30.09434157512473, 'test/num_examples': 3003, 'score': 21866.97625184059, 'total_duration': 36050.02473783493, 'accumulated_submission_time': 21866.97625184059, 'accumulated_eval_time': 14178.888189792633, 'accumulated_logging_time': 0.5458991527557373}
I0306 05:14:22.685298 139585799534336 logging_writer.py:48] [63259] accumulated_eval_time=14178.9, accumulated_logging_time=0.545899, accumulated_submission_time=21867, global_step=63259, preemption_count=0, score=21867, test/accuracy=0.698366, test/bleu=30.0943, test/loss=1.56569, test/num_examples=3003, total_duration=36050, train/accuracy=0.686491, train/bleu=34.6405, train/loss=1.64646, validation/accuracy=0.684016, validation/bleu=29.9413, validation/loss=1.65289, validation/num_examples=3000
I0306 05:14:37.118804 139585807927040 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.2569694221019745, loss=3.8162684440612793
I0306 05:15:11.486182 139585799534336 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2609175443649292, loss=3.8347556591033936
I0306 05:15:45.923598 139585807927040 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2801401615142822, loss=3.8054938316345215
I0306 05:16:20.414443 139585799534336 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.26243558526039124, loss=3.7680506706237793
I0306 05:16:54.902468 139585807927040 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.270280122756958, loss=3.815201997756958
I0306 05:17:29.429250 139585799534336 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.25934791564941406, loss=3.8346056938171387
I0306 05:18:03.966162 139585807927040 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2631450593471527, loss=3.7920491695404053
I0306 05:18:38.497089 139585799534336 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.26666390895843506, loss=3.794581651687622
I0306 05:19:12.997102 139585807927040 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.24603290855884552, loss=3.8339619636535645
I0306 05:19:47.516435 139585799534336 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.26141053438186646, loss=3.7459282875061035
I0306 05:20:22.014351 139585807927040 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.26350778341293335, loss=3.828476905822754
I0306 05:20:56.513108 139585799534336 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.24822543561458588, loss=3.7687220573425293
I0306 05:21:31.053996 139585807927040 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.27468761801719666, loss=3.743764638900757
I0306 05:22:05.566005 139585799534336 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2510669231414795, loss=3.7550859451293945
I0306 05:22:40.135859 139585807927040 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.2925935685634613, loss=3.7563772201538086
I0306 05:23:14.656266 139585799534336 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.24962055683135986, loss=3.8017077445983887
I0306 05:23:49.172040 139585807927040 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2713264226913452, loss=3.825815200805664
I0306 05:24:23.663674 139585799534336 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2675633132457733, loss=3.8268985748291016
I0306 05:24:58.169036 139585807927040 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.26530933380126953, loss=3.7671427726745605
I0306 05:25:32.680890 139585799534336 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.31100332736968994, loss=3.8545939922332764
I0306 05:26:07.155271 139585807927040 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.26549485325813293, loss=3.800123691558838
I0306 05:26:41.677368 139585799534336 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.25173428654670715, loss=3.788996934890747
I0306 05:27:16.185075 139585807927040 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2626114785671234, loss=3.835283041000366
I0306 05:27:50.676905 139585799534336 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2668890058994293, loss=3.8576745986938477
I0306 05:28:22.774146 139728945038528 spec.py:321] Evaluating on the training split.
I0306 05:28:25.366567 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:32:16.429135 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 05:32:19.009039 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:35:37.749238 139728945038528 spec.py:349] Evaluating on the test split.
I0306 05:35:40.333137 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:38:37.600080 139728945038528 submission_runner.py:469] Time since start: 37504.95s, 	Step: 65694, 	{'train/accuracy': 0.677840530872345, 'train/loss': 1.696791172027588, 'train/bleu': 34.0050230761124, 'validation/accuracy': 0.6847823262214661, 'validation/loss': 1.636297345161438, 'validation/bleu': 29.378267685922438, 'validation/num_examples': 3000, 'test/accuracy': 0.6993395686149597, 'test/loss': 1.5508731603622437, 'test/bleu': 30.028605062406868, 'test/num_examples': 3003, 'score': 22706.9242041111, 'total_duration': 37504.954886198044, 'accumulated_submission_time': 22706.9242041111, 'accumulated_eval_time': 14793.71407198906, 'accumulated_logging_time': 0.5694427490234375}
I0306 05:38:37.616028 139585807927040 logging_writer.py:48] [65694] accumulated_eval_time=14793.7, accumulated_logging_time=0.569443, accumulated_submission_time=22706.9, global_step=65694, preemption_count=0, score=22706.9, test/accuracy=0.69934, test/bleu=30.0286, test/loss=1.55087, test/num_examples=3003, total_duration=37505, train/accuracy=0.677841, train/bleu=34.005, train/loss=1.69679, validation/accuracy=0.684782, validation/bleu=29.3783, validation/loss=1.6363, validation/num_examples=3000
I0306 05:38:40.038600 139585799534336 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.28265830874443054, loss=3.8241565227508545
I0306 05:39:14.407861 139585807927040 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.2757515609264374, loss=3.7734897136688232
I0306 05:39:48.874073 139585799534336 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2568506598472595, loss=3.8216001987457275
I0306 05:40:23.428176 139585807927040 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.31208765506744385, loss=3.834139823913574
I0306 05:40:57.939268 139585799534336 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2736767530441284, loss=3.812243700027466
I0306 05:41:32.457013 139585807927040 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.27065667510032654, loss=3.8119378089904785
I0306 05:42:07.000066 139585799534336 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.307097852230072, loss=3.8134560585021973
I0306 05:42:41.525406 139585807927040 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.25993812084198, loss=3.7632431983947754
I0306 05:43:16.029155 139585799534336 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.27061736583709717, loss=3.8148059844970703
I0306 05:43:50.555544 139585807927040 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2761658728122711, loss=3.730707883834839
I0306 05:44:25.091060 139585799534336 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.25071483850479126, loss=3.775641918182373
I0306 05:44:59.571297 139585807927040 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.28466707468032837, loss=3.7988884449005127
I0306 05:45:34.071226 139585799534336 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.26398831605911255, loss=3.8424761295318604
I0306 05:46:08.556041 139585807927040 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3516445755958557, loss=3.8183281421661377
I0306 05:46:43.073431 139585799534336 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3173794448375702, loss=3.7657132148742676
I0306 05:47:17.602938 139585807927040 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2844507396221161, loss=3.8058087825775146
I0306 05:47:52.132749 139585799534336 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.26990750432014465, loss=3.762730121612549
I0306 05:48:26.676123 139585807927040 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.270207941532135, loss=3.816959857940674
I0306 05:49:01.168105 139585799534336 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.2667122185230255, loss=3.769031524658203
I0306 05:49:35.674455 139585807927040 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.27683505415916443, loss=3.8390350341796875
I0306 05:50:10.184103 139585799534336 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.26865577697753906, loss=3.848400115966797
I0306 05:50:44.679208 139585807927040 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.27709847688674927, loss=3.756702423095703
I0306 05:51:19.198751 139585799534336 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.37366530299186707, loss=3.767338275909424
I0306 05:51:53.685908 139585807927040 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.24843157827854156, loss=3.7800748348236084
I0306 05:52:28.200861 139585799534336 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.26768922805786133, loss=3.789257764816284
I0306 05:52:37.864194 139728945038528 spec.py:321] Evaluating on the training split.
I0306 05:52:40.450028 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:56:48.808157 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 05:56:51.382866 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 05:59:53.503138 139728945038528 spec.py:349] Evaluating on the test split.
I0306 05:59:56.079520 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 06:02:37.329485 139728945038528 submission_runner.py:469] Time since start: 38944.68s, 	Step: 68129, 	{'train/accuracy': 0.6753095388412476, 'train/loss': 1.7260905504226685, 'train/bleu': 33.71668410141926, 'validation/accuracy': 0.6850171685218811, 'validation/loss': 1.6412872076034546, 'validation/bleu': 30.19386598732298, 'validation/num_examples': 3000, 'test/accuracy': 0.7011702060699463, 'test/loss': 1.5512686967849731, 'test/bleu': 30.474693185976136, 'test/num_examples': 3003, 'score': 23547.028981924057, 'total_duration': 38944.6842880249, 'accumulated_submission_time': 23547.028981924057, 'accumulated_eval_time': 15393.179308652878, 'accumulated_logging_time': 0.594799280166626}
I0306 06:02:37.346161 139585807927040 logging_writer.py:48] [68129] accumulated_eval_time=15393.2, accumulated_logging_time=0.594799, accumulated_submission_time=23547, global_step=68129, preemption_count=0, score=23547, test/accuracy=0.70117, test/bleu=30.4747, test/loss=1.55127, test/num_examples=3003, total_duration=38944.7, train/accuracy=0.67531, train/bleu=33.7167, train/loss=1.72609, validation/accuracy=0.685017, validation/bleu=30.1939, validation/loss=1.64129, validation/num_examples=3000
I0306 06:03:02.035149 139585799534336 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.2847863733768463, loss=3.7970972061157227
I0306 06:03:36.403226 139585807927040 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.2789643108844757, loss=3.840336322784424
I0306 06:04:10.876124 139585799534336 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.41925859451293945, loss=3.7667272090911865
I0306 06:04:45.392594 139585807927040 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.28120285272598267, loss=3.773317813873291
I0306 06:05:19.940822 139585799534336 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2778066098690033, loss=3.836293935775757
I0306 06:05:54.457939 139585807927040 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.2948456406593323, loss=3.7937309741973877
I0306 06:06:28.982540 139585799534336 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2904967963695526, loss=3.8572585582733154
I0306 06:07:03.492469 139585807927040 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.27043938636779785, loss=3.775869607925415
I0306 06:07:38.005659 139585799534336 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.271618127822876, loss=3.803478240966797
I0306 06:08:12.501502 139585807927040 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.286731094121933, loss=3.7949411869049072
I0306 06:08:47.030691 139585799534336 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2926868200302124, loss=3.78245210647583
I0306 06:09:21.569408 139585807927040 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.28719839453697205, loss=3.8081326484680176
I0306 06:09:56.085775 139585799534336 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.27619078755378723, loss=3.7614567279815674
I0306 06:10:30.619054 139585807927040 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.29136255383491516, loss=3.751101493835449
I0306 06:11:05.135019 139585799534336 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.281354159116745, loss=3.7928617000579834
I0306 06:11:39.648204 139585807927040 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.27129802107810974, loss=3.7960734367370605
I0306 06:12:14.119010 139585799534336 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2768232822418213, loss=3.750666856765747
I0306 06:12:48.619676 139585807927040 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.26377925276756287, loss=3.757232427597046
I0306 06:13:23.160099 139585799534336 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.2932232618331909, loss=3.7923030853271484
I0306 06:13:57.680285 139585807927040 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.27977463603019714, loss=3.748253345489502
I0306 06:14:32.186780 139585799534336 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2593057155609131, loss=3.757838010787964
I0306 06:15:06.694398 139585807927040 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.288051962852478, loss=3.796457052230835
I0306 06:15:41.200475 139585799534336 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.25495120882987976, loss=3.8138554096221924
I0306 06:16:15.686297 139585807927040 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2691696882247925, loss=3.8184974193573
I0306 06:16:37.454908 139728945038528 spec.py:321] Evaluating on the training split.
I0306 06:16:40.045203 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 06:19:44.755381 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 06:19:47.330648 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 06:22:20.834827 139728945038528 spec.py:349] Evaluating on the test split.
I0306 06:22:23.404989 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 06:24:49.562981 139728945038528 submission_runner.py:469] Time since start: 40276.92s, 	Step: 70564, 	{'train/accuracy': 0.6855647563934326, 'train/loss': 1.6549091339111328, 'train/bleu': 34.63081360624592, 'validation/accuracy': 0.6872419714927673, 'validation/loss': 1.634840965270996, 'validation/bleu': 30.33755720852211, 'validation/num_examples': 3000, 'test/accuracy': 0.7016915678977966, 'test/loss': 1.5487984418869019, 'test/bleu': 30.195941341092176, 'test/num_examples': 3003, 'score': 24386.994763851166, 'total_duration': 40276.91777634621, 'accumulated_submission_time': 24386.994763851166, 'accumulated_eval_time': 15885.287318229675, 'accumulated_logging_time': 0.6199855804443359}
I0306 06:24:49.579019 139585799534336 logging_writer.py:48] [70564] accumulated_eval_time=15885.3, accumulated_logging_time=0.619986, accumulated_submission_time=24387, global_step=70564, preemption_count=0, score=24387, test/accuracy=0.701692, test/bleu=30.1959, test/loss=1.5488, test/num_examples=3003, total_duration=40276.9, train/accuracy=0.685565, train/bleu=34.6308, train/loss=1.65491, validation/accuracy=0.687242, validation/bleu=30.3376, validation/loss=1.63484, validation/num_examples=3000
I0306 06:25:02.313654 139585807927040 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.29819339513778687, loss=3.7835681438446045
I0306 06:25:36.676396 139585799534336 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.27719080448150635, loss=3.797830104827881
I0306 06:26:11.083670 139585807927040 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.27426397800445557, loss=3.739180564880371
I0306 06:26:45.580296 139585799534336 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.32299134135246277, loss=3.780179262161255
I0306 06:27:20.053324 139585807927040 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.2762739658355713, loss=3.8201000690460205
I0306 06:27:54.553864 139585799534336 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2723901569843292, loss=3.7393693923950195
I0306 06:28:29.055543 139585807927040 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.282610684633255, loss=3.7536611557006836
I0306 06:29:03.576107 139585799534336 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.2691008746623993, loss=3.794471025466919
I0306 06:29:38.129072 139585807927040 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2943973243236542, loss=3.7731032371520996
I0306 06:30:12.626424 139585799534336 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2654186487197876, loss=3.7513582706451416
I0306 06:30:47.149048 139585807927040 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.28332144021987915, loss=3.7410566806793213
I0306 06:31:21.671914 139585799534336 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.25315991044044495, loss=3.7865114212036133
I0306 06:31:56.199733 139585807927040 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.28136584162712097, loss=3.7894227504730225
I0306 06:32:30.698760 139585799534336 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2937886714935303, loss=3.801938056945801
I0306 06:33:05.204079 139585807927040 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.26714253425598145, loss=3.7555742263793945
I0306 06:33:39.700962 139585799534336 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.27552303671836853, loss=3.8188669681549072
I0306 06:34:14.269150 139585807927040 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2820073664188385, loss=3.778412103652954
I0306 06:34:48.896762 139585799534336 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.27820608019828796, loss=3.755922555923462
I0306 06:35:23.621833 139585807927040 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.29094991087913513, loss=3.753880500793457
I0306 06:35:58.254767 139585799534336 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.29444655776023865, loss=3.770822048187256
I0306 06:36:32.870604 139585807927040 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.28453752398490906, loss=3.7807183265686035
I0306 06:37:07.461171 139585799534336 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.28840163350105286, loss=3.7706737518310547
I0306 06:37:42.078104 139585807927040 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3000398576259613, loss=3.767984390258789
I0306 06:38:16.669582 139585799534336 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.29358768463134766, loss=3.7556395530700684
I0306 06:38:49.901398 139728945038528 spec.py:321] Evaluating on the training split.
I0306 06:38:52.508123 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 06:41:58.098029 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 06:42:00.675204 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 06:44:33.107483 139728945038528 spec.py:349] Evaluating on the test split.
I0306 06:44:35.693593 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 06:47:01.407058 139728945038528 submission_runner.py:469] Time since start: 41608.76s, 	Step: 72997, 	{'train/accuracy': 0.681778609752655, 'train/loss': 1.6693395376205444, 'train/bleu': 34.38910298854706, 'validation/accuracy': 0.6862037777900696, 'validation/loss': 1.623153567314148, 'validation/bleu': 30.032077680468113, 'validation/num_examples': 3000, 'test/accuracy': 0.7008805274963379, 'test/loss': 1.5367275476455688, 'test/bleu': 30.272934798687444, 'test/num_examples': 3003, 'score': 25227.169445753098, 'total_duration': 41608.761862277985, 'accumulated_submission_time': 25227.169445753098, 'accumulated_eval_time': 16376.792926311493, 'accumulated_logging_time': 0.6444394588470459}
I0306 06:47:01.425039 139585807927040 logging_writer.py:48] [72997] accumulated_eval_time=16376.8, accumulated_logging_time=0.644439, accumulated_submission_time=25227.2, global_step=72997, preemption_count=0, score=25227.2, test/accuracy=0.700881, test/bleu=30.2729, test/loss=1.53673, test/num_examples=3003, total_duration=41608.8, train/accuracy=0.681779, train/bleu=34.3891, train/loss=1.66934, validation/accuracy=0.686204, validation/bleu=30.0321, validation/loss=1.62315, validation/num_examples=3000
I0306 06:47:02.818808 139585799534336 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2986527383327484, loss=3.769472360610962
I0306 06:47:37.275845 139585807927040 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3010973036289215, loss=3.776705503463745
I0306 06:48:11.819968 139585799534336 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.33522868156433105, loss=3.8064017295837402
I0306 06:48:46.417012 139585807927040 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.27541106939315796, loss=3.7832515239715576
I0306 06:49:21.083356 139585799534336 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.28072503209114075, loss=3.7828938961029053
I0306 06:49:55.746011 139585807927040 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.2825957238674164, loss=3.8122482299804688
I0306 06:50:30.341902 139585799534336 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.26597097516059875, loss=3.7931594848632812
I0306 06:51:04.943219 139585807927040 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.27690091729164124, loss=3.789393186569214
I0306 06:51:39.581862 139585799534336 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.27450352907180786, loss=3.759218454360962
I0306 06:52:14.237952 139585807927040 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3048241138458252, loss=3.7382595539093018
I0306 06:52:48.821798 139585799534336 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.2964906692504883, loss=3.779785633087158
I0306 06:53:23.424857 139585807927040 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3069841265678406, loss=3.778233766555786
I0306 06:53:58.047080 139585799534336 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.27599865198135376, loss=3.7416229248046875
I0306 06:54:32.667023 139585807927040 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.27843889594078064, loss=3.7568399906158447
I0306 06:55:07.284090 139585799534336 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.26378312706947327, loss=3.7450125217437744
I0306 06:55:41.930952 139585807927040 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.29056316614151, loss=3.71817684173584
I0306 06:56:16.556340 139585799534336 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.32264184951782227, loss=3.7731637954711914
I0306 06:56:51.179592 139585807927040 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.28126639127731323, loss=3.791701316833496
I0306 06:57:25.811031 139585799534336 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.273946613073349, loss=3.728579521179199
I0306 06:58:00.457908 139585807927040 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.28711646795272827, loss=3.7377915382385254
I0306 06:58:35.079633 139585799534336 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2716205418109894, loss=3.7451062202453613
I0306 06:59:09.650703 139585807927040 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2679122984409332, loss=3.730339527130127
I0306 06:59:44.257449 139585799534336 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.2652314603328705, loss=3.7823092937469482
I0306 07:00:18.917215 139585807927040 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.29215145111083984, loss=3.830787420272827
I0306 07:00:53.422595 139585799534336 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.31236541271209717, loss=3.755385160446167
I0306 07:01:01.717472 139728945038528 spec.py:321] Evaluating on the training split.
I0306 07:01:04.313021 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:04:56.551958 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 07:04:59.136977 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:08:39.092486 139728945038528 spec.py:349] Evaluating on the test split.
I0306 07:08:41.679395 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:11:36.416894 139728945038528 submission_runner.py:469] Time since start: 43083.77s, 	Step: 75425, 	{'train/accuracy': 0.7086588740348816, 'train/loss': 1.5295348167419434, 'train/bleu': 36.34188702445827, 'validation/accuracy': 0.6886757612228394, 'validation/loss': 1.6262376308441162, 'validation/bleu': 30.03541192494135, 'validation/num_examples': 3000, 'test/accuracy': 0.7031398415565491, 'test/loss': 1.5329296588897705, 'test/bleu': 30.549202153996845, 'test/num_examples': 3003, 'score': 26067.319740772247, 'total_duration': 43083.77170085907, 'accumulated_submission_time': 26067.319740772247, 'accumulated_eval_time': 17011.492313861847, 'accumulated_logging_time': 0.6715385913848877}
I0306 07:11:36.435037 139585807927040 logging_writer.py:48] [75425] accumulated_eval_time=17011.5, accumulated_logging_time=0.671539, accumulated_submission_time=26067.3, global_step=75425, preemption_count=0, score=26067.3, test/accuracy=0.70314, test/bleu=30.5492, test/loss=1.53293, test/num_examples=3003, total_duration=43083.8, train/accuracy=0.708659, train/bleu=36.3419, train/loss=1.52953, validation/accuracy=0.688676, validation/bleu=30.0354, validation/loss=1.62624, validation/num_examples=3000
I0306 07:12:02.573642 139585799534336 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.29986637830734253, loss=3.7593417167663574
I0306 07:12:37.024242 139585807927040 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.2933610677719116, loss=3.7601780891418457
I0306 07:13:11.565515 139585799534336 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.2856890559196472, loss=3.8098132610321045
I0306 07:13:46.095983 139585807927040 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2719787657260895, loss=3.747208833694458
I0306 07:14:20.658747 139585799534336 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2841998338699341, loss=3.836296796798706
I0306 07:14:55.129695 139585807927040 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.2962349057197571, loss=3.7621066570281982
I0306 07:15:29.660897 139585799534336 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.2862496078014374, loss=3.7574784755706787
I0306 07:16:04.163607 139585807927040 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.27808550000190735, loss=3.7792818546295166
I0306 07:16:38.672993 139585799534336 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.2806743383407593, loss=3.741138458251953
I0306 07:17:13.235873 139585807927040 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2717326581478119, loss=3.724168300628662
I0306 07:17:47.805434 139585799534336 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.2822060286998749, loss=3.701486349105835
I0306 07:18:22.311928 139585807927040 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.27839866280555725, loss=3.7508392333984375
I0306 07:18:56.858700 139585799534336 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2668008804321289, loss=3.7003509998321533
I0306 07:19:31.415726 139585807927040 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.2781657874584198, loss=3.7176883220672607
I0306 07:20:05.927501 139585799534336 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.28594255447387695, loss=3.6926400661468506
I0306 07:20:40.462620 139585807927040 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.2915315628051758, loss=3.684288740158081
I0306 07:21:14.989469 139585799534336 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.2850135266780853, loss=3.7706358432769775
I0306 07:21:49.538294 139585807927040 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2940472662448883, loss=3.7666378021240234
I0306 07:22:24.101173 139585799534336 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.3187088370323181, loss=3.7567858695983887
I0306 07:22:58.642971 139585807927040 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.28461194038391113, loss=3.672024965286255
I0306 07:23:33.198217 139585799534336 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2903567850589752, loss=3.7000999450683594
I0306 07:24:07.725101 139585807927040 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.28469276428222656, loss=3.7572147846221924
I0306 07:24:42.266879 139585799534336 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.283948689699173, loss=3.7138876914978027
I0306 07:25:16.800985 139585807927040 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.28862252831459045, loss=3.767697811126709
I0306 07:25:36.517844 139728945038528 spec.py:321] Evaluating on the training split.
I0306 07:25:39.116558 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:28:46.225535 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 07:28:48.802670 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:31:44.798074 139728945038528 spec.py:349] Evaluating on the test split.
I0306 07:31:47.386498 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:34:13.127297 139728945038528 submission_runner.py:469] Time since start: 44440.48s, 	Step: 77858, 	{'train/accuracy': 0.6870450973510742, 'train/loss': 1.6383070945739746, 'train/bleu': 34.218379954871295, 'validation/accuracy': 0.6886386871337891, 'validation/loss': 1.614620566368103, 'validation/bleu': 30.69233472604982, 'validation/num_examples': 3000, 'test/accuracy': 0.704194188117981, 'test/loss': 1.5244420766830444, 'test/bleu': 30.502246668094934, 'test/num_examples': 3003, 'score': 26907.26141357422, 'total_duration': 44440.482093811035, 'accumulated_submission_time': 26907.26141357422, 'accumulated_eval_time': 17528.10170865059, 'accumulated_logging_time': 0.6982929706573486}
I0306 07:34:13.147895 139585799534336 logging_writer.py:48] [77858] accumulated_eval_time=17528.1, accumulated_logging_time=0.698293, accumulated_submission_time=26907.3, global_step=77858, preemption_count=0, score=26907.3, test/accuracy=0.704194, test/bleu=30.5022, test/loss=1.52444, test/num_examples=3003, total_duration=44440.5, train/accuracy=0.687045, train/bleu=34.2184, train/loss=1.63831, validation/accuracy=0.688639, validation/bleu=30.6923, validation/loss=1.61462, validation/num_examples=3000
I0306 07:34:27.914634 139585807927040 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.2800198495388031, loss=3.7369747161865234
I0306 07:35:02.349963 139585799534336 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.2967897057533264, loss=3.756042718887329
I0306 07:35:36.854755 139585807927040 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2854231894016266, loss=3.7672839164733887
I0306 07:36:11.375313 139585799534336 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.29909732937812805, loss=3.779340982437134
I0306 07:36:45.931996 139585807927040 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2922388017177582, loss=3.7597944736480713
I0306 07:37:20.435525 139585799534336 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.28245705366134644, loss=3.7727773189544678
I0306 07:37:54.966789 139585807927040 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2751410901546478, loss=3.7430355548858643
I0306 07:38:29.496262 139585799534336 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2810652256011963, loss=3.7516043186187744
I0306 07:39:03.999096 139585807927040 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2937491536140442, loss=3.742107629776001
I0306 07:39:38.564171 139585799534336 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2801883816719055, loss=3.7167153358459473
I0306 07:40:13.141822 139585807927040 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.29509395360946655, loss=3.72882342338562
I0306 07:40:47.701346 139585799534336 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3049459159374237, loss=3.7525527477264404
I0306 07:41:22.254154 139585807927040 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.2911520004272461, loss=3.7035579681396484
I0306 07:41:56.825658 139585799534336 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2886884808540344, loss=3.73126482963562
I0306 07:42:31.372043 139585807927040 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2876751720905304, loss=3.715397596359253
I0306 07:43:05.922614 139585799534336 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.29531124234199524, loss=3.7706844806671143
I0306 07:43:40.496045 139585807927040 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3080369830131531, loss=3.743243455886841
I0306 07:44:15.045681 139585799534336 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.32678458094596863, loss=3.779219627380371
I0306 07:44:49.608427 139585807927040 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3020600974559784, loss=3.701767921447754
I0306 07:45:24.175663 139585799534336 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.29773595929145813, loss=3.798102378845215
I0306 07:45:58.743179 139585807927040 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.283337265253067, loss=3.739129066467285
I0306 07:46:33.319398 139585799534336 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2904474139213562, loss=3.7256181240081787
I0306 07:47:07.877641 139585807927040 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.300413578748703, loss=3.7538506984710693
I0306 07:47:42.452985 139585799534336 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.2994902431964874, loss=3.749387264251709
I0306 07:48:13.192614 139728945038528 spec.py:321] Evaluating on the training split.
I0306 07:48:15.782997 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:51:24.606905 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 07:51:27.200705 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:54:06.986236 139728945038528 spec.py:349] Evaluating on the test split.
I0306 07:54:09.568917 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 07:56:31.082654 139728945038528 submission_runner.py:469] Time since start: 45778.44s, 	Step: 80290, 	{'train/accuracy': 0.6847395896911621, 'train/loss': 1.6509279012680054, 'train/bleu': 34.75730539010838, 'validation/accuracy': 0.6892814040184021, 'validation/loss': 1.6139445304870605, 'validation/bleu': 30.21594870415296, 'validation/num_examples': 3000, 'test/accuracy': 0.7038350105285645, 'test/loss': 1.5222080945968628, 'test/bleu': 30.3614958104415, 'test/num_examples': 3003, 'score': 27747.162865400314, 'total_duration': 45778.43746089935, 'accumulated_submission_time': 27747.162865400314, 'accumulated_eval_time': 18025.99171447754, 'accumulated_logging_time': 0.7288024425506592}
I0306 07:56:31.101610 139585807927040 logging_writer.py:48] [80290] accumulated_eval_time=18026, accumulated_logging_time=0.728802, accumulated_submission_time=27747.2, global_step=80290, preemption_count=0, score=27747.2, test/accuracy=0.703835, test/bleu=30.3615, test/loss=1.52221, test/num_examples=3003, total_duration=45778.4, train/accuracy=0.68474, train/bleu=34.7573, train/loss=1.65093, validation/accuracy=0.689281, validation/bleu=30.2159, validation/loss=1.61394, validation/num_examples=3000
I0306 07:56:34.879530 139585799534336 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.28401774168014526, loss=3.743609666824341
I0306 07:57:09.167622 139585807927040 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.2918919622898102, loss=3.685892343521118
I0306 07:57:43.565889 139585799534336 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.2896272838115692, loss=3.704563617706299
I0306 07:58:18.027591 139585807927040 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.291547566652298, loss=3.753143787384033
I0306 07:58:52.532819 139585799534336 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.30882754921913147, loss=3.739499807357788
I0306 07:59:27.038934 139585807927040 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.3122757077217102, loss=3.735614776611328
I0306 08:00:01.563020 139585799534336 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.30725806951522827, loss=3.6944539546966553
I0306 08:00:36.061886 139585807927040 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3014301359653473, loss=3.7422995567321777
I0306 08:01:10.575249 139585799534336 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3272993564605713, loss=3.705822706222534
I0306 08:01:45.074372 139585807927040 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.30778855085372925, loss=3.7490651607513428
I0306 08:02:19.588916 139585799534336 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.2953811287879944, loss=3.7454018592834473
I0306 08:02:54.087128 139585807927040 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.28083258867263794, loss=3.6821768283843994
I0306 08:03:28.606404 139585799534336 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.28520798683166504, loss=3.7058401107788086
I0306 08:04:03.175696 139585807927040 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.31201010942459106, loss=3.71427059173584
I0306 08:04:37.785675 139585799534336 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.32818520069122314, loss=3.7519898414611816
I0306 08:05:12.420487 139585807927040 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.3122897446155548, loss=3.704479694366455
I0306 08:05:47.033812 139585799534336 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.2984406352043152, loss=3.673178195953369
I0306 08:06:21.642319 139585807927040 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.29333046078681946, loss=3.7414910793304443
I0306 08:06:56.234975 139585799534336 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2947348654270172, loss=3.686431646347046
I0306 08:07:30.847255 139585807927040 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.29781219363212585, loss=3.6767475605010986
I0306 08:08:05.446110 139585799534336 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.28922510147094727, loss=3.7087244987487793
I0306 08:08:40.066465 139585807927040 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.3176865875720978, loss=3.7669336795806885
I0306 08:09:14.661432 139585799534336 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2970028519630432, loss=3.6950643062591553
I0306 08:09:49.272706 139585807927040 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3134171962738037, loss=3.750516176223755
I0306 08:10:23.879140 139585799534336 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3071289360523224, loss=3.7977373600006104
I0306 08:10:31.144812 139728945038528 spec.py:321] Evaluating on the training split.
I0306 08:10:33.742486 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 08:14:08.760926 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 08:14:11.354202 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 08:17:06.038485 139728945038528 spec.py:349] Evaluating on the test split.
I0306 08:17:08.620374 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 08:19:51.758022 139728945038528 submission_runner.py:469] Time since start: 47179.11s, 	Step: 82722, 	{'train/accuracy': 0.6983464360237122, 'train/loss': 1.5770505666732788, 'train/bleu': 35.86736348843697, 'validation/accuracy': 0.6901342272758484, 'validation/loss': 1.6129884719848633, 'validation/bleu': 30.500834903965156, 'validation/num_examples': 3000, 'test/accuracy': 0.7043911814689636, 'test/loss': 1.5171200037002563, 'test/bleu': 30.50947870347675, 'test/num_examples': 3003, 'score': 28587.064844608307, 'total_duration': 47179.11281776428, 'accumulated_submission_time': 28587.064844608307, 'accumulated_eval_time': 18586.60485959053, 'accumulated_logging_time': 0.7558863162994385}
I0306 08:19:51.777354 139585807927040 logging_writer.py:48] [82722] accumulated_eval_time=18586.6, accumulated_logging_time=0.755886, accumulated_submission_time=28587.1, global_step=82722, preemption_count=0, score=28587.1, test/accuracy=0.704391, test/bleu=30.5095, test/loss=1.51712, test/num_examples=3003, total_duration=47179.1, train/accuracy=0.698346, train/bleu=35.8674, train/loss=1.57705, validation/accuracy=0.690134, validation/bleu=30.5008, validation/loss=1.61299, validation/num_examples=3000
I0306 08:20:18.993769 139585799534336 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.3037495017051697, loss=3.731034994125366
I0306 08:20:53.481970 139585807927040 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2937086224555969, loss=3.700002908706665
I0306 08:21:28.046215 139585799534336 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.2842341363430023, loss=3.6881489753723145
I0306 08:22:02.701601 139585807927040 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.2923595607280731, loss=3.6686272621154785
I0306 08:22:37.302683 139585799534336 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.2939714193344116, loss=3.6901395320892334
I0306 08:23:11.890099 139585807927040 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.31069472432136536, loss=3.653726816177368
I0306 08:23:46.499098 139585799534336 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3296208083629608, loss=3.7972538471221924
I0306 08:24:21.114546 139585807927040 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.310596227645874, loss=3.762319564819336
I0306 08:24:55.721108 139585799534336 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.32130467891693115, loss=3.725278615951538
I0306 08:25:30.366105 139585807927040 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3628026843070984, loss=3.7310214042663574
I0306 08:26:04.959405 139585799534336 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.30571261048316956, loss=3.7494418621063232
I0306 08:26:39.599540 139585807927040 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.3138749301433563, loss=3.7218098640441895
I0306 08:27:14.187964 139585799534336 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.29938459396362305, loss=3.7113654613494873
I0306 08:27:48.782631 139585807927040 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.32040461897850037, loss=3.7661361694335938
I0306 08:28:23.436640 139585799534336 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.303933322429657, loss=3.7433934211730957
I0306 08:28:58.037262 139585807927040 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.29801061749458313, loss=3.702704906463623
I0306 08:29:32.658255 139585799534336 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3133041560649872, loss=3.7297065258026123
I0306 08:30:07.328450 139585807927040 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.314501017332077, loss=3.79956316947937
I0306 08:30:41.942514 139585799534336 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.30461829900741577, loss=3.7392969131469727
I0306 08:31:16.562614 139585807927040 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.30140990018844604, loss=3.738713502883911
I0306 08:31:51.201389 139585799534336 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.305510014295578, loss=3.7121846675872803
I0306 08:32:25.827626 139585807927040 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.2980552911758423, loss=3.7076761722564697
I0306 08:33:00.433829 139585799534336 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3075164258480072, loss=3.767460584640503
I0306 08:33:35.054337 139585807927040 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.30516940355300903, loss=3.757718324661255
I0306 08:33:52.025323 139728945038528 spec.py:321] Evaluating on the training split.
I0306 08:33:54.622127 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 08:38:02.921418 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 08:38:05.510680 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 08:40:50.139328 139728945038528 spec.py:349] Evaluating on the test split.
I0306 08:40:52.722575 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 08:43:34.597238 139728945038528 submission_runner.py:469] Time since start: 48601.95s, 	Step: 85150, 	{'train/accuracy': 0.6927785873413086, 'train/loss': 1.6079564094543457, 'train/bleu': 34.66924844843533, 'validation/accuracy': 0.6919758915901184, 'validation/loss': 1.6066898107528687, 'validation/bleu': 30.674863044152158, 'validation/num_examples': 3000, 'test/accuracy': 0.7052717208862305, 'test/loss': 1.5154688358306885, 'test/bleu': 30.428448696397215, 'test/num_examples': 3003, 'score': 29427.17177772522, 'total_duration': 48601.95202708244, 'accumulated_submission_time': 29427.17177772522, 'accumulated_eval_time': 19169.176704883575, 'accumulated_logging_time': 0.7832789421081543}
I0306 08:43:34.618016 139585799534336 logging_writer.py:48] [85150] accumulated_eval_time=19169.2, accumulated_logging_time=0.783279, accumulated_submission_time=29427.2, global_step=85150, preemption_count=0, score=29427.2, test/accuracy=0.705272, test/bleu=30.4284, test/loss=1.51547, test/num_examples=3003, total_duration=48602, train/accuracy=0.692779, train/bleu=34.6692, train/loss=1.60796, validation/accuracy=0.691976, validation/bleu=30.6749, validation/loss=1.60669, validation/num_examples=3000
I0306 08:43:52.179898 139585807927040 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.2964470684528351, loss=3.6645050048828125
I0306 08:44:26.676185 139585799534336 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.3111177086830139, loss=3.7322585582733154
I0306 08:45:01.206701 139585807927040 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.29168182611465454, loss=3.702737808227539
I0306 08:45:35.782849 139585799534336 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.2997771203517914, loss=3.6949543952941895
I0306 08:46:10.369202 139585807927040 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.3288286328315735, loss=3.7042653560638428
I0306 08:46:44.960654 139585799534336 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.2972189486026764, loss=3.7003064155578613
I0306 08:47:19.550849 139585807927040 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.31207191944122314, loss=3.7484030723571777
I0306 08:47:54.118550 139585799534336 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.3201618790626526, loss=3.7275390625
I0306 08:48:28.717271 139585807927040 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.31658750772476196, loss=3.7352077960968018
I0306 08:49:03.267004 139585799534336 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3073982894420624, loss=3.7282190322875977
I0306 08:49:37.884807 139585807927040 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.3058970868587494, loss=3.710482120513916
I0306 08:50:12.482687 139585799534336 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.33049559593200684, loss=3.7601962089538574
I0306 08:50:47.141925 139585807927040 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.30196884274482727, loss=3.733469247817993
I0306 08:51:21.714334 139585799534336 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3126296103000641, loss=3.6883485317230225
I0306 08:51:56.297320 139585807927040 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.312862366437912, loss=3.766700029373169
I0306 08:52:30.907641 139585799534336 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3229118287563324, loss=3.652186632156372
I0306 08:53:05.495648 139585807927040 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3279498815536499, loss=3.7030627727508545
I0306 08:53:40.094326 139585799534336 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2996409833431244, loss=3.7266345024108887
I0306 08:54:14.684026 139585807927040 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.32163727283477783, loss=3.7088770866394043
I0306 08:54:49.280771 139585799534336 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.3155958652496338, loss=3.732909917831421
I0306 08:55:23.849182 139585807927040 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3182111978530884, loss=3.7302610874176025
I0306 08:55:58.407631 139585799534336 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.29530444741249084, loss=3.716168165206909
I0306 08:56:32.942903 139585807927040 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3059551417827606, loss=3.720799207687378
I0306 08:57:07.496535 139585799534336 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3291122317314148, loss=3.766777753829956
I0306 08:57:34.783735 139728945038528 spec.py:321] Evaluating on the training split.
I0306 08:57:37.387053 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:01:14.341902 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 09:01:16.920366 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:04:16.927565 139728945038528 spec.py:349] Evaluating on the test split.
I0306 09:04:19.505619 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:07:02.505075 139728945038528 submission_runner.py:469] Time since start: 50009.86s, 	Step: 87580, 	{'train/accuracy': 0.6951361894607544, 'train/loss': 1.5975396633148193, 'train/bleu': 35.160757170782254, 'validation/accuracy': 0.6920006275177002, 'validation/loss': 1.602245569229126, 'validation/bleu': 30.709869624910418, 'validation/num_examples': 3000, 'test/accuracy': 0.7063491940498352, 'test/loss': 1.5054000616073608, 'test/bleu': 30.687179895869622, 'test/num_examples': 3003, 'score': 30267.196674108505, 'total_duration': 50009.85988354683, 'accumulated_submission_time': 30267.196674108505, 'accumulated_eval_time': 19736.897996664047, 'accumulated_logging_time': 0.8125011920928955}
I0306 09:07:02.523296 139585807927040 logging_writer.py:48] [87580] accumulated_eval_time=19736.9, accumulated_logging_time=0.812501, accumulated_submission_time=30267.2, global_step=87580, preemption_count=0, score=30267.2, test/accuracy=0.706349, test/bleu=30.6872, test/loss=1.5054, test/num_examples=3003, total_duration=50009.9, train/accuracy=0.695136, train/bleu=35.1608, train/loss=1.59754, validation/accuracy=0.692001, validation/bleu=30.7099, validation/loss=1.60225, validation/num_examples=3000
I0306 09:07:09.757537 139585799534336 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.3205600380897522, loss=3.710676908493042
I0306 09:07:44.142736 139585807927040 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.3110371530056, loss=3.7168068885803223
I0306 09:08:18.630984 139585799534336 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3118634819984436, loss=3.727937936782837
I0306 09:08:53.074847 139585807927040 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.31038472056388855, loss=3.721644639968872
I0306 09:09:27.569939 139585799534336 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.33306360244750977, loss=3.708336114883423
I0306 09:10:02.044080 139585807927040 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.308681458234787, loss=3.6427035331726074
I0306 09:10:36.534995 139585799534336 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3140795826911926, loss=3.6605007648468018
I0306 09:11:11.044201 139585807927040 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.30449795722961426, loss=3.662407159805298
I0306 09:11:45.580923 139585799534336 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.32454174757003784, loss=3.681699514389038
I0306 09:12:20.049649 139585807927040 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3446522653102875, loss=3.701162815093994
I0306 09:12:54.532581 139585799534336 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3051793873310089, loss=3.725954294204712
I0306 09:13:29.050598 139585807927040 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.3094150125980377, loss=3.6692776679992676
I0306 09:14:03.571046 139585799534336 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.33421453833580017, loss=3.736492156982422
I0306 09:14:38.073339 139585807927040 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3358125388622284, loss=3.710232973098755
I0306 09:15:12.589060 139585799534336 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3119145333766937, loss=3.678959608078003
I0306 09:15:47.091875 139585807927040 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.32024019956588745, loss=3.7088680267333984
I0306 09:16:21.617909 139585799534336 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3183653652667999, loss=3.679762125015259
I0306 09:16:56.112102 139585807927040 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.306364506483078, loss=3.7058284282684326
I0306 09:17:30.647411 139585799534336 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.318983256816864, loss=3.720189332962036
I0306 09:18:05.171227 139585807927040 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.3193999230861664, loss=3.7201149463653564
I0306 09:18:39.704211 139585799534336 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.3049330413341522, loss=3.694657802581787
I0306 09:19:14.218359 139585807927040 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3336009085178375, loss=3.7392818927764893
I0306 09:19:48.739628 139585799534336 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.3386426568031311, loss=3.711670398712158
I0306 09:20:23.242205 139585807927040 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.3206762373447418, loss=3.636134624481201
I0306 09:20:57.778773 139585799534336 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.32468345761299133, loss=3.7047390937805176
I0306 09:21:02.622730 139728945038528 spec.py:321] Evaluating on the training split.
I0306 09:21:05.216552 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:25:08.122964 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 09:25:10.697234 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:28:18.704664 139728945038528 spec.py:349] Evaluating on the test split.
I0306 09:28:21.284968 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:31:12.756170 139728945038528 submission_runner.py:469] Time since start: 51460.11s, 	Step: 90015, 	{'train/accuracy': 0.6980314254760742, 'train/loss': 1.5766218900680542, 'train/bleu': 35.87063683778328, 'validation/accuracy': 0.6913949847221375, 'validation/loss': 1.603234887123108, 'validation/bleu': 30.573886381808343, 'validation/num_examples': 3000, 'test/accuracy': 0.7065693736076355, 'test/loss': 1.5079749822616577, 'test/bleu': 30.42844960953163, 'test/num_examples': 3003, 'score': 31107.15092396736, 'total_duration': 51460.110966444016, 'accumulated_submission_time': 31107.15092396736, 'accumulated_eval_time': 20347.03137230873, 'accumulated_logging_time': 0.8392703533172607}
I0306 09:31:12.776484 139585807927040 logging_writer.py:48] [90015] accumulated_eval_time=20347, accumulated_logging_time=0.83927, accumulated_submission_time=31107.2, global_step=90015, preemption_count=0, score=31107.2, test/accuracy=0.706569, test/bleu=30.4284, test/loss=1.50797, test/num_examples=3003, total_duration=51460.1, train/accuracy=0.698031, train/bleu=35.8706, train/loss=1.57662, validation/accuracy=0.691395, validation/bleu=30.5739, validation/loss=1.60323, validation/num_examples=3000
I0306 09:31:42.318862 139585799534336 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3466750979423523, loss=3.6953036785125732
I0306 09:32:16.745985 139585807927040 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.3042079508304596, loss=3.730525016784668
I0306 09:32:51.243377 139585799534336 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.32671669125556946, loss=3.718604803085327
I0306 09:33:25.773530 139585807927040 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3412027955055237, loss=3.6460020542144775
I0306 09:34:00.290226 139585799534336 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.31939762830734253, loss=3.6852266788482666
I0306 09:34:34.831673 139585807927040 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.3061957359313965, loss=3.672922134399414
I0306 09:35:09.373687 139585799534336 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.3196953237056732, loss=3.6969048976898193
I0306 09:35:43.907519 139585807927040 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3296753764152527, loss=3.692856550216675
I0306 09:36:18.423516 139585799534336 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.32384198904037476, loss=3.732332468032837
I0306 09:36:52.940610 139585807927040 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.316866397857666, loss=3.7129971981048584
I0306 09:37:27.490945 139585799534336 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.33008474111557007, loss=3.6834027767181396
I0306 09:38:02.019219 139585807927040 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3292902112007141, loss=3.6644675731658936
I0306 09:38:36.558056 139585799534336 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.33270785212516785, loss=3.6553425788879395
I0306 09:39:11.078796 139585807927040 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.3295518159866333, loss=3.6947245597839355
I0306 09:39:45.639637 139585799534336 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.33818262815475464, loss=3.6891815662384033
I0306 09:40:20.176304 139585807927040 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.32159796357154846, loss=3.709989309310913
I0306 09:40:54.706272 139585799534336 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.33393701910972595, loss=3.6886942386627197
I0306 09:41:29.193978 139585807927040 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.30560311675071716, loss=3.6639037132263184
I0306 09:42:03.745985 139585799534336 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.32724079489707947, loss=3.7180628776550293
I0306 09:42:38.272223 139585807927040 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3334040939807892, loss=3.668757200241089
I0306 09:43:12.819408 139585799534336 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.36192819476127625, loss=3.6762120723724365
I0306 09:43:47.334678 139585807927040 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.34469863772392273, loss=3.6846141815185547
I0306 09:44:21.859810 139585799534336 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.310099333524704, loss=3.6375327110290527
I0306 09:44:56.348608 139585807927040 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3405596613883972, loss=3.715336799621582
I0306 09:45:12.906451 139728945038528 spec.py:321] Evaluating on the training split.
I0306 09:45:15.499138 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:49:56.612596 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 09:49:59.203912 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:53:57.008145 139728945038528 spec.py:349] Evaluating on the test split.
I0306 09:53:59.582428 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 09:57:30.790827 139728945038528 submission_runner.py:469] Time since start: 53038.15s, 	Step: 92449, 	{'train/accuracy': 0.6985284090042114, 'train/loss': 1.5681596994400024, 'train/bleu': 33.70677954684077, 'validation/accuracy': 0.6919758915901184, 'validation/loss': 1.5996581315994263, 'validation/bleu': 29.411841594614266, 'validation/num_examples': 3000, 'test/accuracy': 0.7064187526702881, 'test/loss': 1.505696177482605, 'test/bleu': 30.525565022949287, 'test/num_examples': 3003, 'score': 31947.136999607086, 'total_duration': 53038.14563202858, 'accumulated_submission_time': 31947.136999607086, 'accumulated_eval_time': 21084.915694475174, 'accumulated_logging_time': 0.8686017990112305}
I0306 09:57:30.809397 139585799534336 logging_writer.py:48] [92449] accumulated_eval_time=21084.9, accumulated_logging_time=0.868602, accumulated_submission_time=31947.1, global_step=92449, preemption_count=0, score=31947.1, test/accuracy=0.706419, test/bleu=30.5256, test/loss=1.5057, test/num_examples=3003, total_duration=53038.1, train/accuracy=0.698528, train/bleu=33.7068, train/loss=1.56816, validation/accuracy=0.691976, validation/bleu=29.4118, validation/loss=1.59966, validation/num_examples=3000
I0306 09:57:48.672630 139585807927040 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.32941150665283203, loss=3.664332866668701
I0306 09:58:23.094740 139585799534336 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.32294899225234985, loss=3.6236915588378906
I0306 09:58:57.560867 139585807927040 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.32237160205841064, loss=3.68418288230896
I0306 09:59:32.071111 139585799534336 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.3389042913913727, loss=3.712520122528076
I0306 10:00:06.608729 139585807927040 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3436061143875122, loss=3.668416976928711
I0306 10:00:41.175348 139585799534336 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3292987048625946, loss=3.664438009262085
I0306 10:01:15.694224 139585807927040 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.3285215497016907, loss=3.667736768722534
I0306 10:01:50.224801 139585799534336 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.3389536142349243, loss=3.6662979125976562
I0306 10:02:24.738426 139585807927040 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3295075595378876, loss=3.712193489074707
I0306 10:02:59.215803 139585799534336 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.34558019042015076, loss=3.6467719078063965
I0306 10:03:33.762568 139585807927040 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.33534348011016846, loss=3.6601505279541016
I0306 10:04:08.276264 139585799534336 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3375573456287384, loss=3.7009189128875732
I0306 10:04:42.799258 139585807927040 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.32742807269096375, loss=3.6847660541534424
I0306 10:05:17.303635 139585799534336 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.3258306086063385, loss=3.669527769088745
I0306 10:05:51.817240 139585807927040 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3181368410587311, loss=3.6042661666870117
I0306 10:06:26.311376 139585799534336 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.33177652955055237, loss=3.6848583221435547
I0306 10:07:00.815490 139585807927040 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.3277814984321594, loss=3.662820816040039
I0306 10:07:35.258460 139585799534336 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.32479092478752136, loss=3.6796464920043945
I0306 10:08:09.679159 139585807927040 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.3532366454601288, loss=3.6628589630126953
I0306 10:08:44.115608 139585799534336 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.34564778208732605, loss=3.709951877593994
I0306 10:09:18.547478 139585807927040 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3391193151473999, loss=3.6651484966278076
I0306 10:09:52.974087 139585799534336 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3279162049293518, loss=3.6851744651794434
I0306 10:10:27.438482 139585807927040 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.3329773247241974, loss=3.722944974899292
I0306 10:11:01.857072 139585799534336 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.33516567945480347, loss=3.656552314758301
I0306 10:11:31.109222 139728945038528 spec.py:321] Evaluating on the training split.
I0306 10:11:33.696257 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 10:15:15.914744 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 10:15:18.485866 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 10:18:34.888191 139728945038528 spec.py:349] Evaluating on the test split.
I0306 10:18:37.476432 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 10:21:23.905100 139728945038528 submission_runner.py:469] Time since start: 54471.26s, 	Step: 94886, 	{'train/accuracy': 0.712945282459259, 'train/loss': 1.4973441362380981, 'train/bleu': 36.56059427749191, 'validation/accuracy': 0.691234290599823, 'validation/loss': 1.5975940227508545, 'validation/bleu': 30.73743738671405, 'validation/num_examples': 3000, 'test/accuracy': 0.7065925002098083, 'test/loss': 1.500529408454895, 'test/bleu': 30.544001278206462, 'test/num_examples': 3003, 'score': 32787.29338145256, 'total_duration': 54471.259895563126, 'accumulated_submission_time': 32787.29338145256, 'accumulated_eval_time': 21677.711508512497, 'accumulated_logging_time': 0.8960928916931152}
I0306 10:21:23.925923 139585807927040 logging_writer.py:48] [94886] accumulated_eval_time=21677.7, accumulated_logging_time=0.896093, accumulated_submission_time=32787.3, global_step=94886, preemption_count=0, score=32787.3, test/accuracy=0.706593, test/bleu=30.544, test/loss=1.50053, test/num_examples=3003, total_duration=54471.3, train/accuracy=0.712945, train/bleu=36.5606, train/loss=1.49734, validation/accuracy=0.691234, validation/bleu=30.7374, validation/loss=1.59759, validation/num_examples=3000
I0306 10:21:29.059356 139585799534336 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.33927351236343384, loss=3.6716365814208984
I0306 10:22:03.342354 139585807927040 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.32148727774620056, loss=3.6688764095306396
I0306 10:22:37.681137 139585799534336 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3227386176586151, loss=3.6626205444335938
I0306 10:23:12.077853 139585807927040 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.33210599422454834, loss=3.5956852436065674
I0306 10:23:46.448432 139585799534336 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3371754288673401, loss=3.703568458557129
I0306 10:24:20.854039 139585807927040 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3489739000797272, loss=3.648721694946289
I0306 10:24:55.243573 139585799534336 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.323201060295105, loss=3.639049768447876
I0306 10:25:29.642836 139585807927040 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.33233916759490967, loss=3.643510580062866
I0306 10:26:04.061609 139585799534336 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.32944366335868835, loss=3.614443778991699
I0306 10:26:38.499192 139585807927040 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3278966546058655, loss=3.6644906997680664
I0306 10:27:12.926621 139585799534336 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.33101320266723633, loss=3.736605644226074
I0306 10:27:47.356947 139585807927040 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.34342628717422485, loss=3.687704563140869
I0306 10:28:21.800637 139585799534336 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3193872570991516, loss=3.652961254119873
I0306 10:28:56.243347 139585807927040 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.33766838908195496, loss=3.6592113971710205
I0306 10:29:30.674505 139585799534336 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.3499731719493866, loss=3.6762869358062744
I0306 10:30:05.098501 139585807927040 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.33485347032546997, loss=3.670346736907959
I0306 10:30:39.511291 139585799534336 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.34726083278656006, loss=3.6576199531555176
I0306 10:31:13.964184 139585807927040 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.3494965136051178, loss=3.7027528285980225
I0306 10:31:48.405885 139585799534336 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.34764599800109863, loss=3.686992645263672
I0306 10:32:22.849427 139585807927040 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3357287645339966, loss=3.6555755138397217
I0306 10:32:57.296873 139585799534336 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.33710813522338867, loss=3.7038140296936035
I0306 10:33:31.749945 139585807927040 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3359835743904114, loss=3.6624248027801514
I0306 10:34:06.162229 139585799534336 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.33987343311309814, loss=3.6667354106903076
I0306 10:34:40.577562 139585807927040 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3307951092720032, loss=3.6284291744232178
I0306 10:35:15.060611 139585799534336 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.3300705850124359, loss=3.6526734828948975
I0306 10:35:24.058443 139728945038528 spec.py:321] Evaluating on the training split.
I0306 10:35:26.660386 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 10:38:48.552493 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 10:38:51.131642 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 10:41:36.933806 139728945038528 spec.py:349] Evaluating on the test split.
I0306 10:41:39.521776 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 10:44:07.002193 139728945038528 submission_runner.py:469] Time since start: 55834.36s, 	Step: 97327, 	{'train/accuracy': 0.7056571245193481, 'train/loss': 1.540151834487915, 'train/bleu': 36.06669453236822, 'validation/accuracy': 0.6928287148475647, 'validation/loss': 1.5963643789291382, 'validation/bleu': 30.697447607687916, 'validation/num_examples': 3000, 'test/accuracy': 0.7076815962791443, 'test/loss': 1.5003292560577393, 'test/bleu': 30.647556352681164, 'test/num_examples': 3003, 'score': 33627.28326129913, 'total_duration': 55834.35699033737, 'accumulated_submission_time': 33627.28326129913, 'accumulated_eval_time': 22200.655200004578, 'accumulated_logging_time': 0.9258041381835938}
I0306 10:44:07.021986 139585807927040 logging_writer.py:48] [97327] accumulated_eval_time=22200.7, accumulated_logging_time=0.925804, accumulated_submission_time=33627.3, global_step=97327, preemption_count=0, score=33627.3, test/accuracy=0.707682, test/bleu=30.6476, test/loss=1.50033, test/num_examples=3003, total_duration=55834.4, train/accuracy=0.705657, train/bleu=36.0667, train/loss=1.54015, validation/accuracy=0.692829, validation/bleu=30.6974, validation/loss=1.59636, validation/num_examples=3000
I0306 10:44:32.426035 139585799534336 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3590065538883209, loss=3.6942055225372314
I0306 10:45:06.762571 139585807927040 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3471721112728119, loss=3.6505355834960938
I0306 10:45:41.169311 139585799534336 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.3544873893260956, loss=3.6921894550323486
I0306 10:46:15.612309 139585807927040 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.3368682563304901, loss=3.6933813095092773
I0306 10:46:50.010074 139585799534336 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.342009037733078, loss=3.670241594314575
I0306 10:47:24.439656 139585807927040 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.35547882318496704, loss=3.6515555381774902
I0306 10:47:58.870726 139585799534336 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.34204480051994324, loss=3.6990628242492676
I0306 10:48:33.283586 139585807927040 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.32758310437202454, loss=3.6535871028900146
I0306 10:49:07.709864 139585799534336 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.33737558126449585, loss=3.5726068019866943
I0306 10:49:42.161380 139585807927040 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.369061142206192, loss=3.7173526287078857
I0306 10:50:16.599536 139585799534336 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3468368649482727, loss=3.6148059368133545
I0306 10:50:51.085229 139585807927040 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.35848551988601685, loss=3.6419079303741455
I0306 10:51:25.562787 139585799534336 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.33545079827308655, loss=3.6199066638946533
I0306 10:52:00.003524 139585807927040 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.3429196774959564, loss=3.638796091079712
I0306 10:52:34.428395 139585799534336 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3408830761909485, loss=3.617845296859741
I0306 10:53:08.849310 139585807927040 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.35623449087142944, loss=3.64451265335083
I0306 10:53:43.285677 139585799534336 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3452513813972473, loss=3.663856029510498
I0306 10:54:17.710387 139585807927040 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.35956794023513794, loss=3.659381151199341
I0306 10:54:52.125346 139585799534336 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3375210762023926, loss=3.6280648708343506
I0306 10:55:26.526699 139585807927040 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3398117423057556, loss=3.6640918254852295
I0306 10:56:00.945231 139585799534336 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3305729925632477, loss=3.6485180854797363
I0306 10:56:35.366542 139585807927040 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.3519833981990814, loss=3.6345455646514893
I0306 10:57:09.762391 139585799534336 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3380158245563507, loss=3.6290886402130127
I0306 10:57:44.162621 139585807927040 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.3432590365409851, loss=3.6300764083862305
I0306 10:58:07.230751 139728945038528 spec.py:321] Evaluating on the training split.
I0306 10:58:09.816943 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:01:56.401383 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 11:01:58.975856 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:04:53.700217 139728945038528 spec.py:349] Evaluating on the test split.
I0306 11:04:56.285436 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:07:31.008797 139728945038528 submission_runner.py:469] Time since start: 57238.36s, 	Step: 99768, 	{'train/accuracy': 0.706973671913147, 'train/loss': 1.5277687311172485, 'train/bleu': 36.22577724462365, 'validation/accuracy': 0.6921118497848511, 'validation/loss': 1.5964674949645996, 'validation/bleu': 30.42248224816676, 'validation/num_examples': 3000, 'test/accuracy': 0.7088170647621155, 'test/loss': 1.4991157054901123, 'test/bleu': 30.425525234732874, 'test/num_examples': 3003, 'score': 34467.35411262512, 'total_duration': 57238.36360526085, 'accumulated_submission_time': 34467.35411262512, 'accumulated_eval_time': 22764.433198213577, 'accumulated_logging_time': 0.9539906978607178}
I0306 11:07:31.028279 139585799534336 logging_writer.py:48] [99768] accumulated_eval_time=22764.4, accumulated_logging_time=0.953991, accumulated_submission_time=34467.4, global_step=99768, preemption_count=0, score=34467.4, test/accuracy=0.708817, test/bleu=30.4255, test/loss=1.49912, test/num_examples=3003, total_duration=57238.4, train/accuracy=0.706974, train/bleu=36.2258, train/loss=1.52777, validation/accuracy=0.692112, validation/bleu=30.4225, validation/loss=1.59647, validation/num_examples=3000
I0306 11:07:42.324393 139585807927040 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.34792500734329224, loss=3.610997438430786
I0306 11:08:16.588022 139585799534336 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.3479769825935364, loss=3.642812967300415
I0306 11:08:50.896705 139585807927040 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.35671642422676086, loss=3.639126777648926
I0306 11:09:25.264954 139585799534336 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.3542185127735138, loss=3.6775882244110107
I0306 11:09:59.672899 139585807927040 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3567100465297699, loss=3.6222312450408936
I0306 11:10:34.091537 139585799534336 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.34335577487945557, loss=3.6062023639678955
I0306 11:11:08.547000 139585807927040 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.34482747316360474, loss=3.6236653327941895
I0306 11:11:43.073081 139585799534336 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3490231931209564, loss=3.636608123779297
I0306 11:12:17.585020 139585807927040 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.327222615480423, loss=3.582138776779175
I0306 11:12:52.109626 139585799534336 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.35520991683006287, loss=3.6514499187469482
I0306 11:13:26.649398 139585807927040 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.345135360956192, loss=3.600212574005127
I0306 11:14:01.151936 139585799534336 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.35921964049339294, loss=3.672168493270874
I0306 11:14:35.675705 139585807927040 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3432210683822632, loss=3.668529510498047
I0306 11:15:10.186143 139585799534336 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3720661997795105, loss=3.626692295074463
I0306 11:15:44.733404 139585807927040 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.3680720031261444, loss=3.671503782272339
I0306 11:16:19.256850 139585799534336 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.37538647651672363, loss=3.673280715942383
I0306 11:16:53.785311 139585807927040 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3519086539745331, loss=3.7048702239990234
I0306 11:17:28.261297 139585799534336 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.35253655910491943, loss=3.6944708824157715
I0306 11:18:02.768528 139585807927040 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.38436630368232727, loss=3.6790173053741455
I0306 11:18:37.242427 139585799534336 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.34613165259361267, loss=3.610184669494629
I0306 11:19:11.796340 139585807927040 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.36459389328956604, loss=3.6308820247650146
I0306 11:19:46.286698 139585799534336 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.36418813467025757, loss=3.6464524269104004
I0306 11:20:20.767735 139585807927040 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3412823975086212, loss=3.6517083644866943
I0306 11:20:55.242758 139585799534336 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3595931828022003, loss=3.626920223236084
I0306 11:21:29.733057 139585807927040 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.358195036649704, loss=3.64019775390625
I0306 11:21:31.122737 139728945038528 spec.py:321] Evaluating on the training split.
I0306 11:21:33.715968 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:25:11.411422 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 11:25:13.993494 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:28:04.253981 139728945038528 spec.py:349] Evaluating on the test split.
I0306 11:28:06.830871 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:30:33.804192 139728945038528 submission_runner.py:469] Time since start: 58621.16s, 	Step: 102205, 	{'train/accuracy': 0.7132712602615356, 'train/loss': 1.490501046180725, 'train/bleu': 36.99400538292106, 'validation/accuracy': 0.6925073862075806, 'validation/loss': 1.5926662683486938, 'validation/bleu': 30.66009220308909, 'validation/num_examples': 3000, 'test/accuracy': 0.7083999514579773, 'test/loss': 1.4959198236465454, 'test/bleu': 30.454271785550382, 'test/num_examples': 3003, 'score': 35307.30923914909, 'total_duration': 58621.15897846222, 'accumulated_submission_time': 35307.30923914909, 'accumulated_eval_time': 23307.114575624466, 'accumulated_logging_time': 0.9816803932189941}
I0306 11:30:33.823822 139585799534336 logging_writer.py:48] [102205] accumulated_eval_time=23307.1, accumulated_logging_time=0.98168, accumulated_submission_time=35307.3, global_step=102205, preemption_count=0, score=35307.3, test/accuracy=0.7084, test/bleu=30.4543, test/loss=1.49592, test/num_examples=3003, total_duration=58621.2, train/accuracy=0.713271, train/bleu=36.994, train/loss=1.4905, validation/accuracy=0.692507, validation/bleu=30.6601, validation/loss=1.59267, validation/num_examples=3000
I0306 11:31:06.786347 139585807927040 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.36510223150253296, loss=3.651383638381958
I0306 11:31:41.261152 139585799534336 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.36452123522758484, loss=3.649451971054077
I0306 11:32:15.719188 139585807927040 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.37152937054634094, loss=3.6594862937927246
I0306 11:32:50.194438 139585799534336 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.348296582698822, loss=3.616914987564087
I0306 11:33:24.721677 139585807927040 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.36069878935813904, loss=3.674700975418091
I0306 11:33:59.227745 139585799534336 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3456365764141083, loss=3.632126808166504
I0306 11:34:33.756686 139585807927040 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.35743796825408936, loss=3.5968992710113525
I0306 11:35:08.298791 139585799534336 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3557364046573639, loss=3.621474027633667
I0306 11:35:42.820048 139585807927040 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.37018847465515137, loss=3.651097059249878
I0306 11:36:17.304674 139585799534336 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.36458173394203186, loss=3.6169075965881348
I0306 11:36:51.782409 139585807927040 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.3526923656463623, loss=3.6190617084503174
I0306 11:37:26.300823 139585799534336 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.34156084060668945, loss=3.6153664588928223
I0306 11:38:00.800995 139585807927040 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.36278608441352844, loss=3.6325457096099854
I0306 11:38:35.327194 139585799534336 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3492192327976227, loss=3.6175146102905273
I0306 11:39:09.822443 139585807927040 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3572802245616913, loss=3.665921926498413
I0306 11:39:44.302138 139585799534336 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3416118621826172, loss=3.5852246284484863
I0306 11:40:18.766410 139585807927040 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3570138216018677, loss=3.649585008621216
I0306 11:40:53.234556 139585799534336 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.36374223232269287, loss=3.6654233932495117
I0306 11:41:27.739758 139585807927040 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.3508147895336151, loss=3.6289799213409424
I0306 11:42:02.252460 139585799534336 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.35146352648735046, loss=3.598095178604126
I0306 11:42:36.750820 139585807927040 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.35699763894081116, loss=3.6606805324554443
I0306 11:43:11.242205 139585799534336 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.35947200655937195, loss=3.6745409965515137
I0306 11:43:45.767071 139585807927040 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.36256933212280273, loss=3.641205072402954
I0306 11:44:20.290147 139585799534336 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3683440387248993, loss=3.7096188068389893
I0306 11:44:34.102348 139728945038528 spec.py:321] Evaluating on the training split.
I0306 11:44:36.697949 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:48:00.169378 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 11:48:02.747340 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:50:46.844252 139728945038528 spec.py:349] Evaluating on the test split.
I0306 11:50:49.415395 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 11:53:11.876506 139728945038528 submission_runner.py:469] Time since start: 59979.23s, 	Step: 104641, 	{'train/accuracy': 0.7109016180038452, 'train/loss': 1.5084280967712402, 'train/bleu': 36.867365187038665, 'validation/accuracy': 0.693038821220398, 'validation/loss': 1.5927646160125732, 'validation/bleu': 30.692969029625925, 'validation/num_examples': 3000, 'test/accuracy': 0.7092341780662537, 'test/loss': 1.4920985698699951, 'test/bleu': 30.7083934753596, 'test/num_examples': 3003, 'score': 36147.451006650925, 'total_duration': 59979.2313015461, 'accumulated_submission_time': 36147.451006650925, 'accumulated_eval_time': 23824.888671875, 'accumulated_logging_time': 1.0095503330230713}
I0306 11:53:11.896195 139585807927040 logging_writer.py:48] [104641] accumulated_eval_time=23824.9, accumulated_logging_time=1.00955, accumulated_submission_time=36147.5, global_step=104641, preemption_count=0, score=36147.5, test/accuracy=0.709234, test/bleu=30.7084, test/loss=1.4921, test/num_examples=3003, total_duration=59979.2, train/accuracy=0.710902, train/bleu=36.8674, train/loss=1.50843, validation/accuracy=0.693039, validation/bleu=30.693, validation/loss=1.59276, validation/num_examples=3000
I0306 11:53:32.546324 139585799534336 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.3659408688545227, loss=3.579913377761841
I0306 11:54:06.923273 139585807927040 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3448313772678375, loss=3.6097559928894043
I0306 11:54:41.330557 139585799534336 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.35476741194725037, loss=3.6267952919006348
I0306 11:55:15.826233 139585807927040 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.37542924284935, loss=3.646364212036133
I0306 11:55:50.340396 139585799534336 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.35774800181388855, loss=3.631732702255249
I0306 11:56:24.862565 139585807927040 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3831215500831604, loss=3.6139636039733887
I0306 11:56:59.362029 139585799534336 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.36043480038642883, loss=3.64113712310791
I0306 11:57:33.904119 139585807927040 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3712131977081299, loss=3.649761438369751
I0306 11:58:08.427682 139585799534336 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.35505062341690063, loss=3.5967981815338135
I0306 11:58:42.929285 139585807927040 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.36790114641189575, loss=3.6603827476501465
I0306 11:59:17.454320 139585799534336 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.36720895767211914, loss=3.62941575050354
I0306 11:59:51.971417 139585807927040 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.3563935458660126, loss=3.602989435195923
I0306 12:00:26.486014 139585799534336 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.344899445772171, loss=3.605172634124756
I0306 12:01:01.000113 139585807927040 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.35539713501930237, loss=3.6360254287719727
I0306 12:01:35.528835 139585799534336 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.34345588088035583, loss=3.600492238998413
I0306 12:02:10.059696 139585807927040 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.35460516810417175, loss=3.603912830352783
I0306 12:02:44.587327 139585799534336 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.36136409640312195, loss=3.619565010070801
I0306 12:03:19.106436 139585807927040 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3629341125488281, loss=3.5622549057006836
I0306 12:03:53.629282 139585799534336 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.3754398226737976, loss=3.6353561878204346
I0306 12:04:28.161235 139585807927040 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3551022708415985, loss=3.6623706817626953
I0306 12:05:02.711065 139585799534336 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3722515106201172, loss=3.6506502628326416
I0306 12:05:37.233465 139585807927040 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.37762558460235596, loss=3.680434226989746
I0306 12:06:11.813028 139585799534336 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.35367274284362793, loss=3.633762836456299
I0306 12:06:46.342852 139585807927040 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3569221794605255, loss=3.605483055114746
I0306 12:07:11.908773 139728945038528 spec.py:321] Evaluating on the training split.
I0306 12:07:14.508261 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 12:11:13.364885 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 12:11:15.933136 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 12:14:27.071424 139728945038528 spec.py:349] Evaluating on the test split.
I0306 12:14:29.640538 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 12:17:25.890528 139728945038528 submission_runner.py:469] Time since start: 61433.25s, 	Step: 107075, 	{'train/accuracy': 0.7198181748390198, 'train/loss': 1.464626669883728, 'train/bleu': 37.13337191162911, 'validation/accuracy': 0.6937928199768066, 'validation/loss': 1.5929930210113525, 'validation/bleu': 30.62094856894801, 'validation/num_examples': 3000, 'test/accuracy': 0.7091878056526184, 'test/loss': 1.4930214881896973, 'test/bleu': 30.48783987525846, 'test/num_examples': 3003, 'score': 36987.32492160797, 'total_duration': 61433.24531030655, 'accumulated_submission_time': 36987.32492160797, 'accumulated_eval_time': 24438.870349645615, 'accumulated_logging_time': 1.0374226570129395}
I0306 12:17:25.911075 139585799534336 logging_writer.py:48] [107075] accumulated_eval_time=24438.9, accumulated_logging_time=1.03742, accumulated_submission_time=36987.3, global_step=107075, preemption_count=0, score=36987.3, test/accuracy=0.709188, test/bleu=30.4878, test/loss=1.49302, test/num_examples=3003, total_duration=61433.2, train/accuracy=0.719818, train/bleu=37.1334, train/loss=1.46463, validation/accuracy=0.693793, validation/bleu=30.6209, validation/loss=1.59299, validation/num_examples=3000
I0306 12:17:34.849942 139585807927040 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.36553865671157837, loss=3.6357312202453613
I0306 12:18:09.224335 139585799534336 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3627466559410095, loss=3.609879970550537
I0306 12:18:43.660766 139585807927040 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3525454103946686, loss=3.619476556777954
I0306 12:19:18.147690 139585799534336 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3609844446182251, loss=3.62926983833313
I0306 12:19:52.637464 139585807927040 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.37189981341362, loss=3.6522347927093506
I0306 12:20:27.168326 139585799534336 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.35636311769485474, loss=3.638615131378174
I0306 12:21:01.686712 139585807927040 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.3727286159992218, loss=3.6314916610717773
I0306 12:21:36.187468 139585799534336 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.3894091248512268, loss=3.655182361602783
I0306 12:22:10.684520 139585807927040 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3732439875602722, loss=3.629469871520996
I0306 12:22:45.187872 139585799534336 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.37360528111457825, loss=3.6098294258117676
I0306 12:23:19.735258 139585807927040 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.3505302965641022, loss=3.59568190574646
I0306 12:23:54.232851 139585799534336 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.37514883279800415, loss=3.638366937637329
I0306 12:24:28.767231 139585807927040 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3677004277706146, loss=3.622915267944336
I0306 12:25:03.285632 139585799534336 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3590964674949646, loss=3.6312010288238525
I0306 12:25:37.800528 139585807927040 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3728295564651489, loss=3.684887647628784
I0306 12:26:12.320511 139585799534336 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.3807448148727417, loss=3.705472469329834
I0306 12:26:46.848064 139585807927040 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3556665778160095, loss=3.6174721717834473
I0306 12:27:21.381666 139585799534336 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.36033564805984497, loss=3.5780866146087646
I0306 12:27:55.902642 139585807927040 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.3617490828037262, loss=3.651210069656372
I0306 12:28:30.423236 139585799534336 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3768969178199768, loss=3.6041676998138428
I0306 12:29:04.959324 139585807927040 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3690623939037323, loss=3.6631598472595215
I0306 12:29:39.487657 139585799534336 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.3629864752292633, loss=3.5977578163146973
I0306 12:30:14.004679 139585807927040 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.36391130089759827, loss=3.63999080657959
I0306 12:30:48.514720 139585799534336 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3659419119358063, loss=3.649383544921875
I0306 12:31:22.981812 139585807927040 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.35513225197792053, loss=3.583472490310669
I0306 12:31:26.089098 139728945038528 spec.py:321] Evaluating on the training split.
I0306 12:31:28.679019 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 12:35:20.017737 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 12:35:22.601521 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 12:39:09.730581 139728945038528 spec.py:349] Evaluating on the test split.
I0306 12:39:12.310064 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 12:42:20.436322 139728945038528 submission_runner.py:469] Time since start: 62927.79s, 	Step: 109510, 	{'train/accuracy': 0.7160464525222778, 'train/loss': 1.4835240840911865, 'train/bleu': 37.020364100600595, 'validation/accuracy': 0.6932119131088257, 'validation/loss': 1.5894633531570435, 'validation/bleu': 30.869406392541663, 'validation/num_examples': 3000, 'test/accuracy': 0.7097207903862, 'test/loss': 1.4902266263961792, 'test/bleu': 30.625667607350362, 'test/num_examples': 3003, 'score': 37827.364022016525, 'total_duration': 62927.7910630703, 'accumulated_submission_time': 37827.364022016525, 'accumulated_eval_time': 25093.217452287674, 'accumulated_logging_time': 1.0657727718353271}
I0306 12:42:20.456993 139585799534336 logging_writer.py:48] [109510] accumulated_eval_time=25093.2, accumulated_logging_time=1.06577, accumulated_submission_time=37827.4, global_step=109510, preemption_count=0, score=37827.4, test/accuracy=0.709721, test/bleu=30.6257, test/loss=1.49023, test/num_examples=3003, total_duration=62927.8, train/accuracy=0.716046, train/bleu=37.0204, train/loss=1.48352, validation/accuracy=0.693212, validation/bleu=30.8694, validation/loss=1.58946, validation/num_examples=3000
I0306 12:42:51.644355 139585807927040 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.3643101453781128, loss=3.6513919830322266
I0306 12:43:26.042178 139585799534336 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.36165904998779297, loss=3.587937355041504
I0306 12:44:00.511397 139585807927040 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.3711991012096405, loss=3.651989221572876
I0306 12:44:35.000285 139585799534336 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.3613346517086029, loss=3.5855255126953125
I0306 12:45:09.490819 139585807927040 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.3626393973827362, loss=3.6232266426086426
I0306 12:45:43.979869 139585799534336 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.3652791380882263, loss=3.597784996032715
I0306 12:46:18.481045 139585807927040 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3510027229785919, loss=3.5684397220611572
I0306 12:46:52.998156 139585799534336 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.37815403938293457, loss=3.6735525131225586
I0306 12:47:27.500978 139585807927040 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3526378571987152, loss=3.560861349105835
I0306 12:48:02.016333 139585799534336 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3385656476020813, loss=3.5887935161590576
I0306 12:48:36.516703 139585807927040 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.38052254915237427, loss=3.6217806339263916
I0306 12:49:11.073102 139585799534336 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.3761129677295685, loss=3.649961233139038
I0306 12:49:45.613971 139585807927040 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.35911300778388977, loss=3.5726475715637207
I0306 12:50:20.167600 139585799534336 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3774074912071228, loss=3.662307024002075
I0306 12:50:54.700765 139585807927040 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.36725208163261414, loss=3.6034562587738037
I0306 12:51:29.213629 139585799534336 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3773171007633209, loss=3.6244683265686035
I0306 12:52:03.725832 139585807927040 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3736276626586914, loss=3.6609702110290527
I0306 12:52:38.248217 139585799534336 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.3517549932003021, loss=3.5946192741394043
I0306 12:53:12.789056 139585807927040 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.3628079295158386, loss=3.644137144088745
I0306 12:53:47.304279 139585799534336 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3813202381134033, loss=3.599907159805298
I0306 12:54:21.835859 139585807927040 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3770490288734436, loss=3.6421825885772705
I0306 12:54:56.322551 139585799534336 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3614780008792877, loss=3.6049368381500244
I0306 12:55:30.853826 139585807927040 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.36474403738975525, loss=3.632544994354248
I0306 12:56:05.424481 139585799534336 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.37058696150779724, loss=3.6356611251831055
I0306 12:56:20.611875 139728945038528 spec.py:321] Evaluating on the training split.
I0306 12:56:23.203493 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 12:59:59.089303 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 13:00:01.669231 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:03:10.648264 139728945038528 spec.py:349] Evaluating on the test split.
I0306 13:03:13.246232 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:06:19.499114 139728945038528 submission_runner.py:469] Time since start: 64366.85s, 	Step: 111945, 	{'train/accuracy': 0.7171757817268372, 'train/loss': 1.4727190732955933, 'train/bleu': 37.249524917809964, 'validation/accuracy': 0.6934590935707092, 'validation/loss': 1.590230941772461, 'validation/bleu': 30.72372489582176, 'validation/num_examples': 3000, 'test/accuracy': 0.7098714113235474, 'test/loss': 1.4911916255950928, 'test/bleu': 30.531728809041763, 'test/num_examples': 3003, 'score': 38667.379287958145, 'total_duration': 64366.853920698166, 'accumulated_submission_time': 38667.379287958145, 'accumulated_eval_time': 25692.104635715485, 'accumulated_logging_time': 1.0947635173797607}
I0306 13:06:19.519470 139585807927040 logging_writer.py:48] [111945] accumulated_eval_time=25692.1, accumulated_logging_time=1.09476, accumulated_submission_time=38667.4, global_step=111945, preemption_count=0, score=38667.4, test/accuracy=0.709871, test/bleu=30.5317, test/loss=1.49119, test/num_examples=3003, total_duration=64366.9, train/accuracy=0.717176, train/bleu=37.2495, train/loss=1.47272, validation/accuracy=0.693459, validation/bleu=30.7237, validation/loss=1.59023, validation/num_examples=3000
I0306 13:06:38.786486 139585799534336 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3931974172592163, loss=3.7091283798217773
I0306 13:07:13.203015 139585807927040 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.3595552444458008, loss=3.622706651687622
I0306 13:07:47.695018 139585799534336 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.3714281916618347, loss=3.5912914276123047
I0306 13:08:22.217094 139585807927040 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.3560711741447449, loss=3.603623867034912
I0306 13:08:56.749803 139585799534336 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.36922281980514526, loss=3.5964035987854004
I0306 13:09:31.247421 139585807927040 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.35598084330558777, loss=3.569284439086914
I0306 13:10:05.805746 139585799534336 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.35355594754219055, loss=3.582275152206421
I0306 13:10:40.356996 139585807927040 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.3520498275756836, loss=3.590048313140869
I0306 13:11:14.869427 139585799534336 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3810972273349762, loss=3.669203996658325
I0306 13:11:49.412857 139585807927040 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3655897080898285, loss=3.6012346744537354
I0306 13:12:23.826088 139585799534336 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.3607584536075592, loss=3.6015632152557373
I0306 13:12:58.236423 139585807927040 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.36280110478401184, loss=3.6276650428771973
I0306 13:13:32.656579 139585799534336 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.3744974136352539, loss=3.5857443809509277
I0306 13:14:07.062749 139585807927040 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.36561116576194763, loss=3.576111316680908
I0306 13:14:41.502186 139585799534336 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3567429482936859, loss=3.619462251663208
I0306 13:15:15.949995 139585807927040 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.36145341396331787, loss=3.589742660522461
I0306 13:15:50.379867 139585799534336 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.36094996333122253, loss=3.5662682056427
I0306 13:16:24.799598 139585807927040 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.3706880509853363, loss=3.693225860595703
I0306 13:16:59.227625 139585799534336 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.36483246088027954, loss=3.6471261978149414
I0306 13:17:33.647895 139585807927040 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.38169825077056885, loss=3.647081136703491
I0306 13:18:08.005082 139585799534336 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3533889949321747, loss=3.5706567764282227
I0306 13:18:42.434989 139585807927040 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.37247660756111145, loss=3.6536524295806885
I0306 13:19:16.821569 139585799534336 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3785618543624878, loss=3.6632776260375977
I0306 13:19:51.255876 139585807927040 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3828955292701721, loss=3.6925623416900635
I0306 13:20:19.820190 139728945038528 spec.py:321] Evaluating on the training split.
I0306 13:20:22.410448 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:23:54.228791 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 13:23:56.814562 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:27:33.338694 139728945038528 spec.py:349] Evaluating on the test split.
I0306 13:27:35.932043 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:30:27.487621 139728945038528 submission_runner.py:469] Time since start: 65814.84s, 	Step: 114384, 	{'train/accuracy': 0.721375048160553, 'train/loss': 1.4613536596298218, 'train/bleu': 37.160900060316926, 'validation/accuracy': 0.6936073899269104, 'validation/loss': 1.5902669429779053, 'validation/bleu': 30.69106878084488, 'validation/num_examples': 3000, 'test/accuracy': 0.7098829746246338, 'test/loss': 1.4899355173110962, 'test/bleu': 30.56318763413373, 'test/num_examples': 3003, 'score': 39507.539105415344, 'total_duration': 65814.84243154526, 'accumulated_submission_time': 39507.539105415344, 'accumulated_eval_time': 26299.772020578384, 'accumulated_logging_time': 1.124370813369751}
I0306 13:30:27.508575 139585799534336 logging_writer.py:48] [114384] accumulated_eval_time=26299.8, accumulated_logging_time=1.12437, accumulated_submission_time=39507.5, global_step=114384, preemption_count=0, score=39507.5, test/accuracy=0.709883, test/bleu=30.5632, test/loss=1.48994, test/num_examples=3003, total_duration=65814.8, train/accuracy=0.721375, train/bleu=37.1609, train/loss=1.46135, validation/accuracy=0.693607, validation/bleu=30.6911, validation/loss=1.59027, validation/num_examples=3000
I0306 13:30:33.350799 139585807927040 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3641853630542755, loss=3.6545674800872803
I0306 13:31:07.612793 139585799534336 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.35559189319610596, loss=3.5688557624816895
I0306 13:31:41.938411 139585807927040 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.3595498502254486, loss=3.5989315509796143
I0306 13:32:16.319570 139585799534336 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.37099385261535645, loss=3.6116530895233154
I0306 13:32:50.689809 139585807927040 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3502735495567322, loss=3.5759475231170654
I0306 13:33:25.062733 139585799534336 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.35598671436309814, loss=3.609652042388916
I0306 13:33:59.489801 139585807927040 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3579712212085724, loss=3.6339385509490967
I0306 13:34:33.911912 139585799534336 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.36570414900779724, loss=3.6302404403686523
I0306 13:35:08.331935 139585807927040 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.37822622060775757, loss=3.6818859577178955
I0306 13:35:42.778840 139585799534336 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.35948970913887024, loss=3.626894235610962
I0306 13:36:17.248314 139585807927040 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.35963913798332214, loss=3.5660581588745117
I0306 13:36:51.698842 139585799534336 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.36503326892852783, loss=3.6594414710998535
I0306 13:37:26.160144 139585807927040 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.36561912298202515, loss=3.608980894088745
I0306 13:38:00.610025 139585799534336 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.38093098998069763, loss=3.6365737915039062
I0306 13:38:35.053276 139585807927040 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.38516029715538025, loss=3.668299674987793
I0306 13:39:09.492295 139585799534336 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.36155322194099426, loss=3.640704393386841
I0306 13:39:43.939758 139585807927040 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.3581220507621765, loss=3.5625667572021484
I0306 13:40:18.343373 139585799534336 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.3607630133628845, loss=3.6559054851531982
I0306 13:40:52.783094 139585807927040 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.36112797260284424, loss=3.639538288116455
I0306 13:41:27.206236 139585799534336 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.390188992023468, loss=3.6344056129455566
I0306 13:42:01.628184 139585807927040 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.3589550256729126, loss=3.620511770248413
I0306 13:42:36.062115 139585799534336 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3694320619106293, loss=3.620012044906616
I0306 13:43:10.481981 139585807927040 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.352487176656723, loss=3.6259217262268066
I0306 13:43:44.895412 139585799534336 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.3608132600784302, loss=3.5938308238983154
I0306 13:44:19.327600 139585807927040 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.376061350107193, loss=3.5525028705596924
I0306 13:44:27.598448 139728945038528 spec.py:321] Evaluating on the training split.
I0306 13:44:30.187561 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:48:06.557389 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 13:48:09.142369 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:51:38.493422 139728945038528 spec.py:349] Evaluating on the test split.
I0306 13:51:41.077946 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 13:54:22.157491 139728945038528 submission_runner.py:469] Time since start: 67249.51s, 	Step: 116825, 	{'train/accuracy': 0.7166643738746643, 'train/loss': 1.4843631982803345, 'train/bleu': 37.80544840602722, 'validation/accuracy': 0.6938793063163757, 'validation/loss': 1.5891107320785522, 'validation/bleu': 30.48159391912738, 'validation/num_examples': 3000, 'test/accuracy': 0.7102537155151367, 'test/loss': 1.4885772466659546, 'test/bleu': 30.69140672913999, 'test/num_examples': 3003, 'score': 40347.48541808128, 'total_duration': 67249.51229739189, 'accumulated_submission_time': 40347.48541808128, 'accumulated_eval_time': 26894.331008672714, 'accumulated_logging_time': 1.153806447982788}
I0306 13:54:22.178404 139585799534336 logging_writer.py:48] [116825] accumulated_eval_time=26894.3, accumulated_logging_time=1.15381, accumulated_submission_time=40347.5, global_step=116825, preemption_count=0, score=40347.5, test/accuracy=0.710254, test/bleu=30.6914, test/loss=1.48858, test/num_examples=3003, total_duration=67249.5, train/accuracy=0.716664, train/bleu=37.8054, train/loss=1.48436, validation/accuracy=0.693879, validation/bleu=30.4816, validation/loss=1.58911, validation/num_examples=3000
I0306 13:54:48.208705 139585807927040 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.37397652864456177, loss=3.57664155960083
I0306 13:55:22.522737 139585799534336 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.36193615198135376, loss=3.599673271179199
I0306 13:55:56.884474 139585807927040 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.3585392236709595, loss=3.607779026031494
I0306 13:56:31.315902 139585799534336 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.36332085728645325, loss=3.6209475994110107
I0306 13:57:05.747411 139585807927040 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.35306599736213684, loss=3.5845937728881836
I0306 13:57:40.188620 139585799534336 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.35406041145324707, loss=3.647686719894409
I0306 13:58:14.593939 139585807927040 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.36990752816200256, loss=3.5797646045684814
I0306 13:58:49.025182 139585799534336 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3577520251274109, loss=3.60056471824646
I0306 13:59:23.467109 139585807927040 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3701014220714569, loss=3.6118781566619873
I0306 13:59:57.891464 139585799534336 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.35392484068870544, loss=3.570359468460083
I0306 14:00:32.324315 139585807927040 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.3749998211860657, loss=3.610635995864868
I0306 14:01:06.777749 139585799534336 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.34891799092292786, loss=3.5701303482055664
I0306 14:01:41.259604 139585807927040 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.3436955511569977, loss=3.567932605743408
I0306 14:02:15.709262 139585799534336 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.3912495970726013, loss=3.6270558834075928
I0306 14:02:50.149584 139585807927040 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.35896605253219604, loss=3.557560920715332
I0306 14:03:24.593825 139585799534336 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3688056170940399, loss=3.5872042179107666
I0306 14:03:59.011018 139585807927040 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3756830394268036, loss=3.580348253250122
I0306 14:04:33.468186 139585799534336 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.36491096019744873, loss=3.6159746646881104
I0306 14:05:07.910987 139585807927040 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.38819247484207153, loss=3.635093927383423
I0306 14:05:42.379668 139585799534336 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.35659563541412354, loss=3.623610258102417
I0306 14:06:16.817629 139585807927040 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.3746739625930786, loss=3.6513519287109375
I0306 14:06:51.292296 139585799534336 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3729953169822693, loss=3.606119394302368
I0306 14:07:25.780173 139585807927040 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.365500807762146, loss=3.5887410640716553
I0306 14:08:00.261883 139585799534336 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3733478784561157, loss=3.591370105743408
I0306 14:08:22.360260 139728945038528 spec.py:321] Evaluating on the training split.
I0306 14:08:24.953025 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 14:12:16.455719 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 14:12:19.036280 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 14:15:36.385792 139728945038528 spec.py:349] Evaluating on the test split.
I0306 14:15:38.964582 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 14:18:22.578265 139728945038528 submission_runner.py:469] Time since start: 68689.93s, 	Step: 119265, 	{'train/accuracy': 0.7212005853652954, 'train/loss': 1.4631843566894531, 'train/bleu': 37.47871857655585, 'validation/accuracy': 0.6936815977096558, 'validation/loss': 1.5897399187088013, 'validation/bleu': 30.697700256353706, 'validation/num_examples': 3000, 'test/accuracy': 0.7104275226593018, 'test/loss': 1.4894130229949951, 'test/bleu': 30.61822686843059, 'test/num_examples': 3003, 'score': 41187.52528619766, 'total_duration': 68689.93306279182, 'accumulated_submission_time': 41187.52528619766, 'accumulated_eval_time': 27494.548951864243, 'accumulated_logging_time': 1.1830172538757324}
I0306 14:18:22.602272 139585807927040 logging_writer.py:48] [119265] accumulated_eval_time=27494.5, accumulated_logging_time=1.18302, accumulated_submission_time=41187.5, global_step=119265, preemption_count=0, score=41187.5, test/accuracy=0.710428, test/bleu=30.6182, test/loss=1.48941, test/num_examples=3003, total_duration=68689.9, train/accuracy=0.721201, train/bleu=37.4787, train/loss=1.46318, validation/accuracy=0.693682, validation/bleu=30.6977, validation/loss=1.58974, validation/num_examples=3000
I0306 14:18:34.933893 139585799534336 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.37293103337287903, loss=3.57780385017395
I0306 14:19:09.263081 139585807927040 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3658464550971985, loss=3.6234326362609863
I0306 14:19:43.700445 139585799534336 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.36993297934532166, loss=3.5710904598236084
I0306 14:20:18.123863 139585807927040 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3742962181568146, loss=3.613474130630493
I0306 14:20:52.623922 139585799534336 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.35773152112960815, loss=3.5824155807495117
I0306 14:21:27.094465 139585807927040 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.36724987626075745, loss=3.632416009902954
I0306 14:22:01.624584 139585799534336 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.35164588689804077, loss=3.613877773284912
I0306 14:22:36.114915 139585807927040 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.35777586698532104, loss=3.6766104698181152
I0306 14:23:10.630023 139585799534336 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.36615389585494995, loss=3.5949959754943848
I0306 14:23:45.118026 139585807927040 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.37805089354515076, loss=3.6265740394592285
I0306 14:24:19.675346 139585799534336 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.35890811681747437, loss=3.577653169631958
I0306 14:24:54.176858 139585807927040 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.35799041390419006, loss=3.555218458175659
I0306 14:25:28.705999 139585799534336 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3674464225769043, loss=3.6090736389160156
I0306 14:26:03.269772 139585807927040 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.35759472846984863, loss=3.5536651611328125
I0306 14:26:37.808379 139585799534336 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.383066326379776, loss=3.586413860321045
I0306 14:27:12.324059 139585807927040 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.36750468611717224, loss=3.6533429622650146
I0306 14:27:46.884333 139585799534336 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.36787164211273193, loss=3.5951590538024902
I0306 14:28:21.408349 139585807927040 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.35789960622787476, loss=3.566892385482788
I0306 14:28:55.945436 139585799534336 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.35693469643592834, loss=3.6017069816589355
I0306 14:29:30.487437 139585807927040 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.37860050797462463, loss=3.591949939727783
I0306 14:30:05.000303 139585799534336 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.374626100063324, loss=3.6637122631073
I0306 14:30:39.522225 139585807927040 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3632442355155945, loss=3.6276252269744873
I0306 14:31:14.049157 139585799534336 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.37681320309638977, loss=3.6616556644439697
I0306 14:31:48.563374 139585807927040 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.371722936630249, loss=3.6351113319396973
I0306 14:32:22.764286 139728945038528 spec.py:321] Evaluating on the training split.
I0306 14:32:25.362021 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 14:36:09.373439 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 14:36:11.959348 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 14:39:32.419695 139728945038528 spec.py:349] Evaluating on the test split.
I0306 14:39:35.002513 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 14:42:19.360752 139728945038528 submission_runner.py:469] Time since start: 70126.72s, 	Step: 121700, 	{'train/accuracy': 0.7197885513305664, 'train/loss': 1.4657936096191406, 'train/bleu': 37.07738726529021, 'validation/accuracy': 0.6935950517654419, 'validation/loss': 1.5897175073623657, 'validation/bleu': 30.69227093333282, 'validation/num_examples': 3000, 'test/accuracy': 0.7104159593582153, 'test/loss': 1.4894319772720337, 'test/bleu': 30.63635486067032, 'test/num_examples': 3003, 'score': 42027.54215502739, 'total_duration': 70126.71555447578, 'accumulated_submission_time': 42027.54215502739, 'accumulated_eval_time': 28091.14536499977, 'accumulated_logging_time': 1.2160611152648926}
I0306 14:42:19.383335 139585799534336 logging_writer.py:48] [121700] accumulated_eval_time=28091.1, accumulated_logging_time=1.21606, accumulated_submission_time=42027.5, global_step=121700, preemption_count=0, score=42027.5, test/accuracy=0.710416, test/bleu=30.6364, test/loss=1.48943, test/num_examples=3003, total_duration=70126.7, train/accuracy=0.719789, train/bleu=37.0774, train/loss=1.46579, validation/accuracy=0.693595, validation/bleu=30.6923, validation/loss=1.58972, validation/num_examples=3000
I0306 14:42:19.735875 139585807927040 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.37524205446243286, loss=3.605165958404541
I0306 14:42:54.083692 139585799534336 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.3691087067127228, loss=3.6719627380371094
I0306 14:43:28.480743 139585807927040 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3547368049621582, loss=3.5659778118133545
I0306 14:44:02.962288 139585799534336 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.3659501075744629, loss=3.612299919128418
I0306 14:44:37.452129 139585807927040 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.3762550950050354, loss=3.6089744567871094
I0306 14:45:11.953234 139585799534336 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.3725247383117676, loss=3.6097679138183594
I0306 14:45:46.442153 139585807927040 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3673851191997528, loss=3.6138007640838623
I0306 14:46:20.951942 139585799534336 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.37436148524284363, loss=3.60557222366333
I0306 14:46:55.468043 139585807927040 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.37103182077407837, loss=3.593313694000244
I0306 14:47:30.009147 139585799534336 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.390301376581192, loss=3.6701693534851074
I0306 14:48:04.537582 139585807927040 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.3683964014053345, loss=3.5756900310516357
I0306 14:48:39.058525 139585799534336 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.36242616176605225, loss=3.5758535861968994
I0306 14:49:13.567157 139585807927040 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.3784979283809662, loss=3.6173248291015625
I0306 14:49:48.043327 139585799534336 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.34987303614616394, loss=3.551318645477295
I0306 14:50:22.566343 139585807927040 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.35684990882873535, loss=3.5616846084594727
I0306 14:50:57.101320 139585799534336 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3634647727012634, loss=3.6421940326690674
I0306 14:51:31.663084 139585807927040 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.3619776666164398, loss=3.6330440044403076
I0306 14:52:06.160484 139585799534336 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3581044375896454, loss=3.5704967975616455
I0306 14:52:40.692238 139585807927040 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.36363381147384644, loss=3.6226613521575928
I0306 14:53:15.205372 139585799534336 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.36829543113708496, loss=3.55424165725708
I0306 14:53:49.702185 139585807927040 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3715532720088959, loss=3.6147053241729736
I0306 14:54:24.211346 139585799534336 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.38125330209732056, loss=3.6829116344451904
I0306 14:54:58.706972 139585807927040 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.36320850253105164, loss=3.579920768737793
I0306 14:55:33.217514 139585799534336 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.37626171112060547, loss=3.6022915840148926
I0306 14:56:07.711827 139585807927040 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.35376644134521484, loss=3.603158950805664
I0306 14:56:19.442001 139728945038528 spec.py:321] Evaluating on the training split.
I0306 14:56:22.039685 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 15:00:00.614754 139728945038528 spec.py:333] Evaluating on the validation split.
I0306 15:00:03.192919 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 15:03:23.233469 139728945038528 spec.py:349] Evaluating on the test split.
I0306 15:03:25.818664 139728945038528 workload.py:181] Translating evaluation dataset.
I0306 15:06:09.997617 139728945038528 submission_runner.py:469] Time since start: 71557.35s, 	Step: 124135, 	{'train/accuracy': 0.7191888093948364, 'train/loss': 1.4706958532333374, 'train/bleu': 37.77670475204979, 'validation/accuracy': 0.6935950517654419, 'validation/loss': 1.5897175073623657, 'validation/bleu': 30.69227093333282, 'validation/num_examples': 3000, 'test/accuracy': 0.7104159593582153, 'test/loss': 1.4894319772720337, 'test/bleu': 30.63635486067032, 'test/num_examples': 3003, 'score': 42867.45379781723, 'total_duration': 71557.35241913795, 'accumulated_submission_time': 42867.45379781723, 'accumulated_eval_time': 28681.700924873352, 'accumulated_logging_time': 1.2470524311065674}
I0306 15:06:10.019860 139585799534336 logging_writer.py:48] [124135] accumulated_eval_time=28681.7, accumulated_logging_time=1.24705, accumulated_submission_time=42867.5, global_step=124135, preemption_count=0, score=42867.5, test/accuracy=0.710416, test/bleu=30.6364, test/loss=1.48943, test/num_examples=3003, total_duration=71557.4, train/accuracy=0.719189, train/bleu=37.7767, train/loss=1.4707, validation/accuracy=0.693595, validation/bleu=30.6923, validation/loss=1.58972, validation/num_examples=3000
I0306 15:06:32.731705 139585807927040 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.3677464425563812, loss=3.6582913398742676
I0306 15:07:07.147361 139585799534336 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.37007656693458557, loss=3.6103909015655518
I0306 15:07:41.630677 139585807927040 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.35602104663848877, loss=3.604182004928589
I0306 15:08:16.143090 139585799534336 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.36679545044898987, loss=3.574951171875
I0306 15:08:50.711960 139585807927040 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.36473506689071655, loss=3.6114444732666016
I0306 15:09:25.252560 139585799534336 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.3645585775375366, loss=3.5609207153320312
I0306 15:09:59.796508 139585807927040 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.39105862379074097, loss=3.6169304847717285
I0306 15:10:34.361662 139585799534336 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.3703988790512085, loss=3.600795269012451
I0306 15:11:08.886141 139585807927040 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.36543008685112, loss=3.6204254627227783
I0306 15:11:43.423217 139585799534336 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.3646750748157501, loss=3.6565325260162354
I0306 15:12:17.993805 139585807927040 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.3624393343925476, loss=3.6172592639923096
I0306 15:12:52.513921 139585799534336 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3649698495864868, loss=3.6196129322052
I0306 15:13:27.014627 139585807927040 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.3690536320209503, loss=3.5744314193725586
I0306 15:14:01.530539 139585799534336 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.3691628575325012, loss=3.6218886375427246
I0306 15:14:35.988286 139585807927040 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.3672632873058319, loss=3.588902473449707
I0306 15:15:10.501358 139585799534336 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.38304629921913147, loss=3.686197519302368
I0306 15:15:45.015150 139585807927040 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.37390583753585815, loss=3.7021679878234863
I0306 15:16:19.735606 139585799534336 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.35843366384506226, loss=3.584057092666626
I0306 15:16:54.312296 139585807927040 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.3797530233860016, loss=3.6393332481384277
I0306 15:17:28.935682 139585799534336 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.3460248112678528, loss=3.5293400287628174
I0306 15:18:03.501979 139585807927040 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.3520255386829376, loss=3.6033921241760254
I0306 15:18:38.157039 139585799534336 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.36146026849746704, loss=3.615588665008545
I0306 15:19:12.753621 139585807927040 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.35373666882514954, loss=3.586232900619507
I0306 15:19:47.376066 139585799534336 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.364711731672287, loss=3.5981626510620117
I0306 15:20:10.235545 139585807927040 logging_writer.py:48] [126567] global_step=126567, preemption_count=0, score=43707.5
I0306 15:20:10.262876 139728945038528 submission_runner.py:646] Tuning trial 2/5
I0306 15:20:10.263021 139728945038528 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0306 15:20:10.267444 139728945038528 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006014798418618739, 'train/loss': 11.105691909790039, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.148694038391113, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.146646499633789, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.70587730407715, 'total_duration': 941.7918515205383, 'accumulated_submission_time': 26.70587730407715, 'accumulated_eval_time': 915.0858404636383, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2432, {'train/accuracy': 0.4331774413585663, 'train/loss': 3.8138489723205566, 'train/bleu': 13.576943366928587, 'validation/accuracy': 0.42132845520973206, 'validation/loss': 3.9137299060821533, 'validation/bleu': 10.100368024043531, 'validation/num_examples': 3000, 'test/accuracy': 0.40939635038375854, 'test/loss': 4.084228515625, 'test/bleu': 8.340447735070596, 'test/num_examples': 3003, 'score': 866.6493525505066, 'total_duration': 2282.8135888576508, 'accumulated_submission_time': 866.6493525505066, 'accumulated_eval_time': 1415.9739322662354, 'accumulated_logging_time': 0.01611948013305664, 'global_step': 2432, 'preemption_count': 0}), (4862, {'train/accuracy': 0.5495039820671082, 'train/loss': 2.786299228668213, 'train/bleu': 24.33146164553042, 'validation/accuracy': 0.5557931661605835, 'validation/loss': 2.7292561531066895, 'validation/bleu': 20.817781616158218, 'validation/num_examples': 3000, 'test/accuracy': 0.5562855005264282, 'test/loss': 2.7564375400543213, 'test/bleu': 19.679039332551106, 'test/num_examples': 3003, 'score': 1706.5789091587067, 'total_duration': 3634.882288455963, 'accumulated_submission_time': 1706.5789091587067, 'accumulated_eval_time': 1927.9367167949677, 'accumulated_logging_time': 0.03448677062988281, 'global_step': 4862, 'preemption_count': 0}), (7296, {'train/accuracy': 0.591151237487793, 'train/loss': 2.3724465370178223, 'train/bleu': 27.554069144902588, 'validation/accuracy': 0.5970261693000793, 'validation/loss': 2.31310772895813, 'validation/bleu': 23.6990810584944, 'validation/num_examples': 3000, 'test/accuracy': 0.603441059589386, 'test/loss': 2.292757511138916, 'test/bleu': 22.587176736752028, 'test/num_examples': 3003, 'score': 2546.517216682434, 'total_duration': 4954.5831990242, 'accumulated_submission_time': 2546.517216682434, 'accumulated_eval_time': 2407.5217504501343, 'accumulated_logging_time': 0.05305743217468262, 'global_step': 7296, 'preemption_count': 0}), (9734, {'train/accuracy': 0.606626033782959, 'train/loss': 2.255824327468872, 'train/bleu': 28.55987648512636, 'validation/accuracy': 0.6215237379074097, 'validation/loss': 2.139256000518799, 'validation/bleu': 25.231698333907953, 'validation/num_examples': 3000, 'test/accuracy': 0.629845917224884, 'test/loss': 2.0967488288879395, 'test/bleu': 24.160584648951673, 'test/num_examples': 3003, 'score': 3386.540834903717, 'total_duration': 6271.71257686615, 'accumulated_submission_time': 3386.540834903717, 'accumulated_eval_time': 2884.4533355236053, 'accumulated_logging_time': 0.07225346565246582, 'global_step': 9734, 'preemption_count': 0}), (12174, {'train/accuracy': 0.6185905933380127, 'train/loss': 2.1589958667755127, 'train/bleu': 29.70556923681752, 'validation/accuracy': 0.6356141567230225, 'validation/loss': 2.0175116062164307, 'validation/bleu': 26.385825583809147, 'validation/num_examples': 3000, 'test/accuracy': 0.6444792151451111, 'test/loss': 1.9658397436141968, 'test/bleu': 25.258389185895084, 'test/num_examples': 3003, 'score': 4226.681246757507, 'total_duration': 7645.089530467987, 'accumulated_submission_time': 4226.681246757507, 'accumulated_eval_time': 3417.525922060013, 'accumulated_logging_time': 0.09152913093566895, 'global_step': 12174, 'preemption_count': 0}), (14609, {'train/accuracy': 0.6285078525543213, 'train/loss': 2.0630898475646973, 'train/bleu': 30.303598075164416, 'validation/accuracy': 0.6444886922836304, 'validation/loss': 1.9426811933517456, 'validation/bleu': 27.059750884116987, 'validation/num_examples': 3000, 'test/accuracy': 0.6535859107971191, 'test/loss': 1.8884230852127075, 'test/bleu': 26.348372843234774, 'test/num_examples': 3003, 'score': 5066.6530611515045, 'total_duration': 9039.421432256699, 'accumulated_submission_time': 5066.6530611515045, 'accumulated_eval_time': 3971.73015165329, 'accumulated_logging_time': 0.11051726341247559, 'global_step': 14609, 'preemption_count': 0}), (17044, {'train/accuracy': 0.6341263055801392, 'train/loss': 2.021695375442505, 'train/bleu': 30.77153944461597, 'validation/accuracy': 0.651336133480072, 'validation/loss': 1.888048768043518, 'validation/bleu': 27.395823434019217, 'validation/num_examples': 3000, 'test/accuracy': 0.6611980199813843, 'test/loss': 1.8268219232559204, 'test/bleu': 27.128141825744525, 'test/num_examples': 3003, 'score': 5906.551867485046, 'total_duration': 10366.449325561523, 'accumulated_submission_time': 5906.551867485046, 'accumulated_eval_time': 4458.703329563141, 'accumulated_logging_time': 0.12897300720214844, 'global_step': 17044, 'preemption_count': 0}), (19475, {'train/accuracy': 0.648202657699585, 'train/loss': 1.9112379550933838, 'train/bleu': 32.06620675384898, 'validation/accuracy': 0.656181275844574, 'validation/loss': 1.8411140441894531, 'validation/bleu': 28.217390071231478, 'validation/num_examples': 3000, 'test/accuracy': 0.6665855646133423, 'test/loss': 1.7744221687316895, 'test/bleu': 27.6295625742881, 'test/num_examples': 3003, 'score': 6746.63994717598, 'total_duration': 11675.263387441635, 'accumulated_submission_time': 6746.63994717598, 'accumulated_eval_time': 4927.269788742065, 'accumulated_logging_time': 0.14797019958496094, 'global_step': 19475, 'preemption_count': 0}), (21899, {'train/accuracy': 0.6450713276863098, 'train/loss': 1.9377447366714478, 'train/bleu': 31.763554323748103, 'validation/accuracy': 0.6597533226013184, 'validation/loss': 1.828452229499817, 'validation/bleu': 28.277150609295997, 'validation/num_examples': 3000, 'test/accuracy': 0.669957160949707, 'test/loss': 1.7682262659072876, 'test/bleu': 27.96035951029598, 'test/num_examples': 3003, 'score': 7586.576191663742, 'total_duration': 12980.077657699585, 'accumulated_submission_time': 7586.576191663742, 'accumulated_eval_time': 5391.992084741592, 'accumulated_logging_time': 0.1683058738708496, 'global_step': 21899, 'preemption_count': 0}), (24325, {'train/accuracy': 0.6450738906860352, 'train/loss': 1.9415960311889648, 'train/bleu': 31.445378915533798, 'validation/accuracy': 0.6627444624900818, 'validation/loss': 1.8064852952957153, 'validation/bleu': 28.563607860536997, 'validation/num_examples': 3000, 'test/accuracy': 0.674012303352356, 'test/loss': 1.7404903173446655, 'test/bleu': 27.924541841110994, 'test/num_examples': 3003, 'score': 8426.670614242554, 'total_duration': 14297.707791090012, 'accumulated_submission_time': 8426.670614242554, 'accumulated_eval_time': 5869.374609231949, 'accumulated_logging_time': 0.19014525413513184, 'global_step': 24325, 'preemption_count': 0}), (26754, {'train/accuracy': 0.6521549224853516, 'train/loss': 1.8771387338638306, 'train/bleu': 32.064094664084806, 'validation/accuracy': 0.6654512882232666, 'validation/loss': 1.7797573804855347, 'validation/bleu': 28.631837261711482, 'validation/num_examples': 3000, 'test/accuracy': 0.6766886711120605, 'test/loss': 1.7108386754989624, 'test/bleu': 28.251831056819952, 'test/num_examples': 3003, 'score': 9266.741669893265, 'total_duration': 15669.27569770813, 'accumulated_submission_time': 9266.741669893265, 'accumulated_eval_time': 6400.71689248085, 'accumulated_logging_time': 0.2111063003540039, 'global_step': 26754, 'preemption_count': 0}), (29187, {'train/accuracy': 0.650421142578125, 'train/loss': 1.9072725772857666, 'train/bleu': 32.30471685665179, 'validation/accuracy': 0.6668232679367065, 'validation/loss': 1.778612494468689, 'validation/bleu': 28.771572168028452, 'validation/num_examples': 3000, 'test/accuracy': 0.6789827346801758, 'test/loss': 1.7037239074707031, 'test/bleu': 28.25634350282276, 'test/num_examples': 3003, 'score': 10106.813430070877, 'total_duration': 17027.17290496826, 'accumulated_submission_time': 10106.813430070877, 'accumulated_eval_time': 6918.386072635651, 'accumulated_logging_time': 0.23152470588684082, 'global_step': 29187, 'preemption_count': 0}), (31618, {'train/accuracy': 0.6737024188041687, 'train/loss': 1.7493079900741577, 'train/bleu': 33.489701153894806, 'validation/accuracy': 0.6671322584152222, 'validation/loss': 1.766648769378662, 'validation/bleu': 28.473035679454313, 'validation/num_examples': 3000, 'test/accuracy': 0.6787741780281067, 'test/loss': 1.69500732421875, 'test/bleu': 28.127657665565273, 'test/num_examples': 3003, 'score': 10946.771188020706, 'total_duration': 18404.759540081024, 'accumulated_submission_time': 10946.771188020706, 'accumulated_eval_time': 7455.859852075577, 'accumulated_logging_time': 0.25223636627197266, 'global_step': 31618, 'preemption_count': 0}), (34046, {'train/accuracy': 0.6540790796279907, 'train/loss': 1.8604586124420166, 'train/bleu': 32.39988299936474, 'validation/accuracy': 0.6703458428382874, 'validation/loss': 1.751020073890686, 'validation/bleu': 28.84627260482706, 'validation/num_examples': 3000, 'test/accuracy': 0.6827830076217651, 'test/loss': 1.6761033535003662, 'test/bleu': 28.54834134368054, 'test/num_examples': 3003, 'score': 11786.942027807236, 'total_duration': 19911.726499557495, 'accumulated_submission_time': 11786.942027807236, 'accumulated_eval_time': 8122.496347665787, 'accumulated_logging_time': 0.27518367767333984, 'global_step': 34046, 'preemption_count': 0}), (36480, {'train/accuracy': 0.656191349029541, 'train/loss': 1.8564461469650269, 'train/bleu': 32.71964261283881, 'validation/accuracy': 0.672904372215271, 'validation/loss': 1.7244051694869995, 'validation/bleu': 29.39053925889315, 'validation/num_examples': 3000, 'test/accuracy': 0.6844050288200378, 'test/loss': 1.6516268253326416, 'test/bleu': 29.11269486238484, 'test/num_examples': 3003, 'score': 12626.98415851593, 'total_duration': 21226.851529359818, 'accumulated_submission_time': 12626.98415851593, 'accumulated_eval_time': 8597.422164678574, 'accumulated_logging_time': 0.29601097106933594, 'global_step': 36480, 'preemption_count': 0}), (38914, {'train/accuracy': 0.6621062159538269, 'train/loss': 1.7953685522079468, 'train/bleu': 33.04036161641257, 'validation/accuracy': 0.6734976768493652, 'validation/loss': 1.7133029699325562, 'validation/bleu': 29.526641713747885, 'validation/num_examples': 3000, 'test/accuracy': 0.6868149638175964, 'test/loss': 1.6366941928863525, 'test/bleu': 29.106778132713853, 'test/num_examples': 3003, 'score': 13466.982836484909, 'total_duration': 22572.9270131588, 'accumulated_submission_time': 13466.982836484909, 'accumulated_eval_time': 9103.34075307846, 'accumulated_logging_time': 0.3195013999938965, 'global_step': 38914, 'preemption_count': 0}), (41351, {'train/accuracy': 0.6548977494239807, 'train/loss': 1.8532079458236694, 'train/bleu': 32.18002793011718, 'validation/accuracy': 0.6690974831581116, 'validation/loss': 1.7556068897247314, 'validation/bleu': 28.939800157785346, 'validation/num_examples': 3000, 'test/accuracy': 0.6816591620445251, 'test/loss': 1.6871181726455688, 'test/bleu': 28.38503886598118, 'test/num_examples': 3003, 'score': 14306.836514949799, 'total_duration': 23904.592145204544, 'accumulated_submission_time': 14306.836514949799, 'accumulated_eval_time': 9594.996240377426, 'accumulated_logging_time': 0.3401522636413574, 'global_step': 41351, 'preemption_count': 0}), (43785, {'train/accuracy': 0.6581637859344482, 'train/loss': 1.8355387449264526, 'train/bleu': 32.47519658754718, 'validation/accuracy': 0.6738560795783997, 'validation/loss': 1.7100189924240112, 'validation/bleu': 29.491395898936098, 'validation/num_examples': 3000, 'test/accuracy': 0.6895145177841187, 'test/loss': 1.632834553718567, 'test/bleu': 29.402075118264293, 'test/num_examples': 3003, 'score': 15146.827372789383, 'total_duration': 25256.31569504738, 'accumulated_submission_time': 15146.827372789383, 'accumulated_eval_time': 10106.576641321182, 'accumulated_logging_time': 0.3613290786743164, 'global_step': 43785, 'preemption_count': 0}), (46220, {'train/accuracy': 0.6648356914520264, 'train/loss': 1.7803852558135986, 'train/bleu': 32.83018393407643, 'validation/accuracy': 0.6768842935562134, 'validation/loss': 1.6940646171569824, 'validation/bleu': 29.590773203338436, 'validation/num_examples': 3000, 'test/accuracy': 0.6891322135925293, 'test/loss': 1.6162021160125732, 'test/bleu': 28.902535716821703, 'test/num_examples': 3003, 'score': 15986.932428598404, 'total_duration': 26581.20386004448, 'accumulated_submission_time': 15986.932428598404, 'accumulated_eval_time': 10591.200972557068, 'accumulated_logging_time': 0.38266682624816895, 'global_step': 46220, 'preemption_count': 0}), (48655, {'train/accuracy': 0.6591668128967285, 'train/loss': 1.8186081647872925, 'train/bleu': 32.76420166333463, 'validation/accuracy': 0.6789237260818481, 'validation/loss': 1.6839874982833862, 'validation/bleu': 29.88723209938831, 'validation/num_examples': 3000, 'test/accuracy': 0.6919012665748596, 'test/loss': 1.6048399209976196, 'test/bleu': 29.40601672076325, 'test/num_examples': 3003, 'score': 16826.880041360855, 'total_duration': 27916.977389335632, 'accumulated_submission_time': 16826.880041360855, 'accumulated_eval_time': 11086.87451839447, 'accumulated_logging_time': 0.4049539566040039, 'global_step': 48655, 'preemption_count': 0}), (51090, {'train/accuracy': 0.6771486401557922, 'train/loss': 1.7057360410690308, 'train/bleu': 33.412505197693676, 'validation/accuracy': 0.6787012219429016, 'validation/loss': 1.6801316738128662, 'validation/bleu': 29.96067433413804, 'validation/num_examples': 3000, 'test/accuracy': 0.6920403242111206, 'test/loss': 1.6022100448608398, 'test/bleu': 29.503772858230175, 'test/num_examples': 3003, 'score': 17666.867208242416, 'total_duration': 29232.815542697906, 'accumulated_submission_time': 17666.867208242416, 'accumulated_eval_time': 11562.571260929108, 'accumulated_logging_time': 0.42783093452453613, 'global_step': 51090, 'preemption_count': 0}), (53523, {'train/accuracy': 0.6669300198554993, 'train/loss': 1.775338888168335, 'train/bleu': 33.32501722718088, 'validation/accuracy': 0.6813957095146179, 'validation/loss': 1.6677093505859375, 'validation/bleu': 29.834117716804315, 'validation/num_examples': 3000, 'test/accuracy': 0.6934654116630554, 'test/loss': 1.5937355756759644, 'test/bleu': 29.575533275258035, 'test/num_examples': 3003, 'score': 18506.72165608406, 'total_duration': 30590.193658828735, 'accumulated_submission_time': 18506.72165608406, 'accumulated_eval_time': 12079.934581756592, 'accumulated_logging_time': 0.45119237899780273, 'global_step': 53523, 'preemption_count': 0}), (55957, {'train/accuracy': 0.666933000087738, 'train/loss': 1.770076036453247, 'train/bleu': 33.305574965278794, 'validation/accuracy': 0.6826317310333252, 'validation/loss': 1.6626839637756348, 'validation/bleu': 30.028262973587278, 'validation/num_examples': 3000, 'test/accuracy': 0.6949136853218079, 'test/loss': 1.5791844129562378, 'test/bleu': 29.408539074892637, 'test/num_examples': 3003, 'score': 19346.77387547493, 'total_duration': 31931.342279672623, 'accumulated_submission_time': 19346.77387547493, 'accumulated_eval_time': 12580.87377333641, 'accumulated_logging_time': 0.4745297431945801, 'global_step': 55957, 'preemption_count': 0}), (58392, {'train/accuracy': 0.6707125902175903, 'train/loss': 1.7425974607467651, 'train/bleu': 33.46114318731096, 'validation/accuracy': 0.681074321269989, 'validation/loss': 1.660438895225525, 'validation/bleu': 30.04406041744336, 'validation/num_examples': 3000, 'test/accuracy': 0.6967558860778809, 'test/loss': 1.5745389461517334, 'test/bleu': 29.827795343351212, 'test/num_examples': 3003, 'score': 20186.85280251503, 'total_duration': 33269.251730680466, 'accumulated_submission_time': 20186.85280251503, 'accumulated_eval_time': 13078.549107789993, 'accumulated_logging_time': 0.49728965759277344, 'global_step': 58392, 'preemption_count': 0}), (60826, {'train/accuracy': 0.6699889898300171, 'train/loss': 1.7481650114059448, 'train/bleu': 33.44281571904775, 'validation/accuracy': 0.6814327836036682, 'validation/loss': 1.6548084020614624, 'validation/bleu': 29.71787668238941, 'validation/num_examples': 3000, 'test/accuracy': 0.6954929828643799, 'test/loss': 1.5683162212371826, 'test/bleu': 29.90589343113518, 'test/num_examples': 3003, 'score': 21026.96267771721, 'total_duration': 34589.45092916489, 'accumulated_submission_time': 21026.96267771721, 'accumulated_eval_time': 13558.482238769531, 'accumulated_logging_time': 0.521662712097168, 'global_step': 60826, 'preemption_count': 0}), (63259, {'train/accuracy': 0.6864909529685974, 'train/loss': 1.6464632749557495, 'train/bleu': 34.64048438768395, 'validation/accuracy': 0.6840160489082336, 'validation/loss': 1.6528875827789307, 'validation/bleu': 29.941256370407146, 'validation/num_examples': 3000, 'test/accuracy': 0.6983663439750671, 'test/loss': 1.5656907558441162, 'test/bleu': 30.09434157512473, 'test/num_examples': 3003, 'score': 21866.97625184059, 'total_duration': 36050.02473783493, 'accumulated_submission_time': 21866.97625184059, 'accumulated_eval_time': 14178.888189792633, 'accumulated_logging_time': 0.5458991527557373, 'global_step': 63259, 'preemption_count': 0}), (65694, {'train/accuracy': 0.677840530872345, 'train/loss': 1.696791172027588, 'train/bleu': 34.0050230761124, 'validation/accuracy': 0.6847823262214661, 'validation/loss': 1.636297345161438, 'validation/bleu': 29.378267685922438, 'validation/num_examples': 3000, 'test/accuracy': 0.6993395686149597, 'test/loss': 1.5508731603622437, 'test/bleu': 30.028605062406868, 'test/num_examples': 3003, 'score': 22706.9242041111, 'total_duration': 37504.954886198044, 'accumulated_submission_time': 22706.9242041111, 'accumulated_eval_time': 14793.71407198906, 'accumulated_logging_time': 0.5694427490234375, 'global_step': 65694, 'preemption_count': 0}), (68129, {'train/accuracy': 0.6753095388412476, 'train/loss': 1.7260905504226685, 'train/bleu': 33.71668410141926, 'validation/accuracy': 0.6850171685218811, 'validation/loss': 1.6412872076034546, 'validation/bleu': 30.19386598732298, 'validation/num_examples': 3000, 'test/accuracy': 0.7011702060699463, 'test/loss': 1.5512686967849731, 'test/bleu': 30.474693185976136, 'test/num_examples': 3003, 'score': 23547.028981924057, 'total_duration': 38944.6842880249, 'accumulated_submission_time': 23547.028981924057, 'accumulated_eval_time': 15393.179308652878, 'accumulated_logging_time': 0.594799280166626, 'global_step': 68129, 'preemption_count': 0}), (70564, {'train/accuracy': 0.6855647563934326, 'train/loss': 1.6549091339111328, 'train/bleu': 34.63081360624592, 'validation/accuracy': 0.6872419714927673, 'validation/loss': 1.634840965270996, 'validation/bleu': 30.33755720852211, 'validation/num_examples': 3000, 'test/accuracy': 0.7016915678977966, 'test/loss': 1.5487984418869019, 'test/bleu': 30.195941341092176, 'test/num_examples': 3003, 'score': 24386.994763851166, 'total_duration': 40276.91777634621, 'accumulated_submission_time': 24386.994763851166, 'accumulated_eval_time': 15885.287318229675, 'accumulated_logging_time': 0.6199855804443359, 'global_step': 70564, 'preemption_count': 0}), (72997, {'train/accuracy': 0.681778609752655, 'train/loss': 1.6693395376205444, 'train/bleu': 34.38910298854706, 'validation/accuracy': 0.6862037777900696, 'validation/loss': 1.623153567314148, 'validation/bleu': 30.032077680468113, 'validation/num_examples': 3000, 'test/accuracy': 0.7008805274963379, 'test/loss': 1.5367275476455688, 'test/bleu': 30.272934798687444, 'test/num_examples': 3003, 'score': 25227.169445753098, 'total_duration': 41608.761862277985, 'accumulated_submission_time': 25227.169445753098, 'accumulated_eval_time': 16376.792926311493, 'accumulated_logging_time': 0.6444394588470459, 'global_step': 72997, 'preemption_count': 0}), (75425, {'train/accuracy': 0.7086588740348816, 'train/loss': 1.5295348167419434, 'train/bleu': 36.34188702445827, 'validation/accuracy': 0.6886757612228394, 'validation/loss': 1.6262376308441162, 'validation/bleu': 30.03541192494135, 'validation/num_examples': 3000, 'test/accuracy': 0.7031398415565491, 'test/loss': 1.5329296588897705, 'test/bleu': 30.549202153996845, 'test/num_examples': 3003, 'score': 26067.319740772247, 'total_duration': 43083.77170085907, 'accumulated_submission_time': 26067.319740772247, 'accumulated_eval_time': 17011.492313861847, 'accumulated_logging_time': 0.6715385913848877, 'global_step': 75425, 'preemption_count': 0}), (77858, {'train/accuracy': 0.6870450973510742, 'train/loss': 1.6383070945739746, 'train/bleu': 34.218379954871295, 'validation/accuracy': 0.6886386871337891, 'validation/loss': 1.614620566368103, 'validation/bleu': 30.69233472604982, 'validation/num_examples': 3000, 'test/accuracy': 0.704194188117981, 'test/loss': 1.5244420766830444, 'test/bleu': 30.502246668094934, 'test/num_examples': 3003, 'score': 26907.26141357422, 'total_duration': 44440.482093811035, 'accumulated_submission_time': 26907.26141357422, 'accumulated_eval_time': 17528.10170865059, 'accumulated_logging_time': 0.6982929706573486, 'global_step': 77858, 'preemption_count': 0}), (80290, {'train/accuracy': 0.6847395896911621, 'train/loss': 1.6509279012680054, 'train/bleu': 34.75730539010838, 'validation/accuracy': 0.6892814040184021, 'validation/loss': 1.6139445304870605, 'validation/bleu': 30.21594870415296, 'validation/num_examples': 3000, 'test/accuracy': 0.7038350105285645, 'test/loss': 1.5222080945968628, 'test/bleu': 30.3614958104415, 'test/num_examples': 3003, 'score': 27747.162865400314, 'total_duration': 45778.43746089935, 'accumulated_submission_time': 27747.162865400314, 'accumulated_eval_time': 18025.99171447754, 'accumulated_logging_time': 0.7288024425506592, 'global_step': 80290, 'preemption_count': 0}), (82722, {'train/accuracy': 0.6983464360237122, 'train/loss': 1.5770505666732788, 'train/bleu': 35.86736348843697, 'validation/accuracy': 0.6901342272758484, 'validation/loss': 1.6129884719848633, 'validation/bleu': 30.500834903965156, 'validation/num_examples': 3000, 'test/accuracy': 0.7043911814689636, 'test/loss': 1.5171200037002563, 'test/bleu': 30.50947870347675, 'test/num_examples': 3003, 'score': 28587.064844608307, 'total_duration': 47179.11281776428, 'accumulated_submission_time': 28587.064844608307, 'accumulated_eval_time': 18586.60485959053, 'accumulated_logging_time': 0.7558863162994385, 'global_step': 82722, 'preemption_count': 0}), (85150, {'train/accuracy': 0.6927785873413086, 'train/loss': 1.6079564094543457, 'train/bleu': 34.66924844843533, 'validation/accuracy': 0.6919758915901184, 'validation/loss': 1.6066898107528687, 'validation/bleu': 30.674863044152158, 'validation/num_examples': 3000, 'test/accuracy': 0.7052717208862305, 'test/loss': 1.5154688358306885, 'test/bleu': 30.428448696397215, 'test/num_examples': 3003, 'score': 29427.17177772522, 'total_duration': 48601.95202708244, 'accumulated_submission_time': 29427.17177772522, 'accumulated_eval_time': 19169.176704883575, 'accumulated_logging_time': 0.7832789421081543, 'global_step': 85150, 'preemption_count': 0}), (87580, {'train/accuracy': 0.6951361894607544, 'train/loss': 1.5975396633148193, 'train/bleu': 35.160757170782254, 'validation/accuracy': 0.6920006275177002, 'validation/loss': 1.602245569229126, 'validation/bleu': 30.709869624910418, 'validation/num_examples': 3000, 'test/accuracy': 0.7063491940498352, 'test/loss': 1.5054000616073608, 'test/bleu': 30.687179895869622, 'test/num_examples': 3003, 'score': 30267.196674108505, 'total_duration': 50009.85988354683, 'accumulated_submission_time': 30267.196674108505, 'accumulated_eval_time': 19736.897996664047, 'accumulated_logging_time': 0.8125011920928955, 'global_step': 87580, 'preemption_count': 0}), (90015, {'train/accuracy': 0.6980314254760742, 'train/loss': 1.5766218900680542, 'train/bleu': 35.87063683778328, 'validation/accuracy': 0.6913949847221375, 'validation/loss': 1.603234887123108, 'validation/bleu': 30.573886381808343, 'validation/num_examples': 3000, 'test/accuracy': 0.7065693736076355, 'test/loss': 1.5079749822616577, 'test/bleu': 30.42844960953163, 'test/num_examples': 3003, 'score': 31107.15092396736, 'total_duration': 51460.110966444016, 'accumulated_submission_time': 31107.15092396736, 'accumulated_eval_time': 20347.03137230873, 'accumulated_logging_time': 0.8392703533172607, 'global_step': 90015, 'preemption_count': 0}), (92449, {'train/accuracy': 0.6985284090042114, 'train/loss': 1.5681596994400024, 'train/bleu': 33.70677954684077, 'validation/accuracy': 0.6919758915901184, 'validation/loss': 1.5996581315994263, 'validation/bleu': 29.411841594614266, 'validation/num_examples': 3000, 'test/accuracy': 0.7064187526702881, 'test/loss': 1.505696177482605, 'test/bleu': 30.525565022949287, 'test/num_examples': 3003, 'score': 31947.136999607086, 'total_duration': 53038.14563202858, 'accumulated_submission_time': 31947.136999607086, 'accumulated_eval_time': 21084.915694475174, 'accumulated_logging_time': 0.8686017990112305, 'global_step': 92449, 'preemption_count': 0}), (94886, {'train/accuracy': 0.712945282459259, 'train/loss': 1.4973441362380981, 'train/bleu': 36.56059427749191, 'validation/accuracy': 0.691234290599823, 'validation/loss': 1.5975940227508545, 'validation/bleu': 30.73743738671405, 'validation/num_examples': 3000, 'test/accuracy': 0.7065925002098083, 'test/loss': 1.500529408454895, 'test/bleu': 30.544001278206462, 'test/num_examples': 3003, 'score': 32787.29338145256, 'total_duration': 54471.259895563126, 'accumulated_submission_time': 32787.29338145256, 'accumulated_eval_time': 21677.711508512497, 'accumulated_logging_time': 0.8960928916931152, 'global_step': 94886, 'preemption_count': 0}), (97327, {'train/accuracy': 0.7056571245193481, 'train/loss': 1.540151834487915, 'train/bleu': 36.06669453236822, 'validation/accuracy': 0.6928287148475647, 'validation/loss': 1.5963643789291382, 'validation/bleu': 30.697447607687916, 'validation/num_examples': 3000, 'test/accuracy': 0.7076815962791443, 'test/loss': 1.5003292560577393, 'test/bleu': 30.647556352681164, 'test/num_examples': 3003, 'score': 33627.28326129913, 'total_duration': 55834.35699033737, 'accumulated_submission_time': 33627.28326129913, 'accumulated_eval_time': 22200.655200004578, 'accumulated_logging_time': 0.9258041381835938, 'global_step': 97327, 'preemption_count': 0}), (99768, {'train/accuracy': 0.706973671913147, 'train/loss': 1.5277687311172485, 'train/bleu': 36.22577724462365, 'validation/accuracy': 0.6921118497848511, 'validation/loss': 1.5964674949645996, 'validation/bleu': 30.42248224816676, 'validation/num_examples': 3000, 'test/accuracy': 0.7088170647621155, 'test/loss': 1.4991157054901123, 'test/bleu': 30.425525234732874, 'test/num_examples': 3003, 'score': 34467.35411262512, 'total_duration': 57238.36360526085, 'accumulated_submission_time': 34467.35411262512, 'accumulated_eval_time': 22764.433198213577, 'accumulated_logging_time': 0.9539906978607178, 'global_step': 99768, 'preemption_count': 0}), (102205, {'train/accuracy': 0.7132712602615356, 'train/loss': 1.490501046180725, 'train/bleu': 36.99400538292106, 'validation/accuracy': 0.6925073862075806, 'validation/loss': 1.5926662683486938, 'validation/bleu': 30.66009220308909, 'validation/num_examples': 3000, 'test/accuracy': 0.7083999514579773, 'test/loss': 1.4959198236465454, 'test/bleu': 30.454271785550382, 'test/num_examples': 3003, 'score': 35307.30923914909, 'total_duration': 58621.15897846222, 'accumulated_submission_time': 35307.30923914909, 'accumulated_eval_time': 23307.114575624466, 'accumulated_logging_time': 0.9816803932189941, 'global_step': 102205, 'preemption_count': 0}), (104641, {'train/accuracy': 0.7109016180038452, 'train/loss': 1.5084280967712402, 'train/bleu': 36.867365187038665, 'validation/accuracy': 0.693038821220398, 'validation/loss': 1.5927646160125732, 'validation/bleu': 30.692969029625925, 'validation/num_examples': 3000, 'test/accuracy': 0.7092341780662537, 'test/loss': 1.4920985698699951, 'test/bleu': 30.7083934753596, 'test/num_examples': 3003, 'score': 36147.451006650925, 'total_duration': 59979.2313015461, 'accumulated_submission_time': 36147.451006650925, 'accumulated_eval_time': 23824.888671875, 'accumulated_logging_time': 1.0095503330230713, 'global_step': 104641, 'preemption_count': 0}), (107075, {'train/accuracy': 0.7198181748390198, 'train/loss': 1.464626669883728, 'train/bleu': 37.13337191162911, 'validation/accuracy': 0.6937928199768066, 'validation/loss': 1.5929930210113525, 'validation/bleu': 30.62094856894801, 'validation/num_examples': 3000, 'test/accuracy': 0.7091878056526184, 'test/loss': 1.4930214881896973, 'test/bleu': 30.48783987525846, 'test/num_examples': 3003, 'score': 36987.32492160797, 'total_duration': 61433.24531030655, 'accumulated_submission_time': 36987.32492160797, 'accumulated_eval_time': 24438.870349645615, 'accumulated_logging_time': 1.0374226570129395, 'global_step': 107075, 'preemption_count': 0}), (109510, {'train/accuracy': 0.7160464525222778, 'train/loss': 1.4835240840911865, 'train/bleu': 37.020364100600595, 'validation/accuracy': 0.6932119131088257, 'validation/loss': 1.5894633531570435, 'validation/bleu': 30.869406392541663, 'validation/num_examples': 3000, 'test/accuracy': 0.7097207903862, 'test/loss': 1.4902266263961792, 'test/bleu': 30.625667607350362, 'test/num_examples': 3003, 'score': 37827.364022016525, 'total_duration': 62927.7910630703, 'accumulated_submission_time': 37827.364022016525, 'accumulated_eval_time': 25093.217452287674, 'accumulated_logging_time': 1.0657727718353271, 'global_step': 109510, 'preemption_count': 0}), (111945, {'train/accuracy': 0.7171757817268372, 'train/loss': 1.4727190732955933, 'train/bleu': 37.249524917809964, 'validation/accuracy': 0.6934590935707092, 'validation/loss': 1.590230941772461, 'validation/bleu': 30.72372489582176, 'validation/num_examples': 3000, 'test/accuracy': 0.7098714113235474, 'test/loss': 1.4911916255950928, 'test/bleu': 30.531728809041763, 'test/num_examples': 3003, 'score': 38667.379287958145, 'total_duration': 64366.853920698166, 'accumulated_submission_time': 38667.379287958145, 'accumulated_eval_time': 25692.104635715485, 'accumulated_logging_time': 1.0947635173797607, 'global_step': 111945, 'preemption_count': 0}), (114384, {'train/accuracy': 0.721375048160553, 'train/loss': 1.4613536596298218, 'train/bleu': 37.160900060316926, 'validation/accuracy': 0.6936073899269104, 'validation/loss': 1.5902669429779053, 'validation/bleu': 30.69106878084488, 'validation/num_examples': 3000, 'test/accuracy': 0.7098829746246338, 'test/loss': 1.4899355173110962, 'test/bleu': 30.56318763413373, 'test/num_examples': 3003, 'score': 39507.539105415344, 'total_duration': 65814.84243154526, 'accumulated_submission_time': 39507.539105415344, 'accumulated_eval_time': 26299.772020578384, 'accumulated_logging_time': 1.124370813369751, 'global_step': 114384, 'preemption_count': 0}), (116825, {'train/accuracy': 0.7166643738746643, 'train/loss': 1.4843631982803345, 'train/bleu': 37.80544840602722, 'validation/accuracy': 0.6938793063163757, 'validation/loss': 1.5891107320785522, 'validation/bleu': 30.48159391912738, 'validation/num_examples': 3000, 'test/accuracy': 0.7102537155151367, 'test/loss': 1.4885772466659546, 'test/bleu': 30.69140672913999, 'test/num_examples': 3003, 'score': 40347.48541808128, 'total_duration': 67249.51229739189, 'accumulated_submission_time': 40347.48541808128, 'accumulated_eval_time': 26894.331008672714, 'accumulated_logging_time': 1.153806447982788, 'global_step': 116825, 'preemption_count': 0}), (119265, {'train/accuracy': 0.7212005853652954, 'train/loss': 1.4631843566894531, 'train/bleu': 37.47871857655585, 'validation/accuracy': 0.6936815977096558, 'validation/loss': 1.5897399187088013, 'validation/bleu': 30.697700256353706, 'validation/num_examples': 3000, 'test/accuracy': 0.7104275226593018, 'test/loss': 1.4894130229949951, 'test/bleu': 30.61822686843059, 'test/num_examples': 3003, 'score': 41187.52528619766, 'total_duration': 68689.93306279182, 'accumulated_submission_time': 41187.52528619766, 'accumulated_eval_time': 27494.548951864243, 'accumulated_logging_time': 1.1830172538757324, 'global_step': 119265, 'preemption_count': 0}), (121700, {'train/accuracy': 0.7197885513305664, 'train/loss': 1.4657936096191406, 'train/bleu': 37.07738726529021, 'validation/accuracy': 0.6935950517654419, 'validation/loss': 1.5897175073623657, 'validation/bleu': 30.69227093333282, 'validation/num_examples': 3000, 'test/accuracy': 0.7104159593582153, 'test/loss': 1.4894319772720337, 'test/bleu': 30.63635486067032, 'test/num_examples': 3003, 'score': 42027.54215502739, 'total_duration': 70126.71555447578, 'accumulated_submission_time': 42027.54215502739, 'accumulated_eval_time': 28091.14536499977, 'accumulated_logging_time': 1.2160611152648926, 'global_step': 121700, 'preemption_count': 0}), (124135, {'train/accuracy': 0.7191888093948364, 'train/loss': 1.4706958532333374, 'train/bleu': 37.77670475204979, 'validation/accuracy': 0.6935950517654419, 'validation/loss': 1.5897175073623657, 'validation/bleu': 30.69227093333282, 'validation/num_examples': 3000, 'test/accuracy': 0.7104159593582153, 'test/loss': 1.4894319772720337, 'test/bleu': 30.63635486067032, 'test/num_examples': 3003, 'score': 42867.45379781723, 'total_duration': 71557.35241913795, 'accumulated_submission_time': 42867.45379781723, 'accumulated_eval_time': 28681.700924873352, 'accumulated_logging_time': 1.2470524311065674, 'global_step': 124135, 'preemption_count': 0})], 'global_step': 126567}
I0306 15:20:10.267609 139728945038528 submission_runner.py:649] Timing: 43707.506381988525
I0306 15:20:10.267646 139728945038528 submission_runner.py:651] Total number of evals: 52
I0306 15:20:10.267674 139728945038528 submission_runner.py:652] ====================
I0306 15:20:10.267801 139728945038528 submission_runner.py:750] Final wmt score: 1
