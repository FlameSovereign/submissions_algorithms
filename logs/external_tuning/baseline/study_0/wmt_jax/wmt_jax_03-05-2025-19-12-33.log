python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=1002959722 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-33.log
2025-03-05 19:12:34.449027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201954.470421       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201954.477090       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:41.233788 139689492575424 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax.
I0305 19:12:42.029150 139689492575424 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:42.032296 139689492575424 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:42.034018 139689492575424 submission_runner.py:606] Using RNG seed 1002959722
I0305 19:12:42.600081 139689492575424 submission_runner.py:615] --- Tuning run 4/5 ---
I0305 19:12:42.600287 139689492575424 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_4.
I0305 19:12:42.600484 139689492575424 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_4/hparams.json.
I0305 19:12:42.828381 139689492575424 submission_runner.py:218] Initializing dataset.
I0305 19:12:43.028737 139689492575424 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:43.042864 139689492575424 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:43.118362 139689492575424 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:44.494969 139689492575424 submission_runner.py:229] Initializing model.
I0305 19:13:23.788649 139689492575424 submission_runner.py:272] Initializing optimizer.
I0305 19:13:24.636765 139689492575424 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:24.636980 139689492575424 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:24.638004 139689492575424 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_4 with prefix checkpoint_
I0305 19:13:24.638100 139689492575424 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_4/meta_data_0.json.
I0305 19:13:24.638285 139689492575424 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:24.638332 139689492575424 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:24.823014 139689492575424 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_4/flags_0.json.
I0305 19:13:24.870999 139689492575424 submission_runner.py:337] Starting training loop.
I0305 19:13:50.089531 139553360742144 logging_writer.py:48] [0] global_step=0, grad_norm=7.196130275726318, loss=11.120800971984863
I0305 19:13:50.144151 139689492575424 spec.py:321] Evaluating on the training split.
I0305 19:13:50.146118 139689492575424 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:50.149233 139689492575424 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:50.179621 139689492575424 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:56.137191 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 19:19:02.664275 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 19:19:02.675588 139689492575424 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:02.683207 139689492575424 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:02.715222 139689492575424 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:08.016534 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 19:24:08.835207 139689492575424 spec.py:349] Evaluating on the test split.
I0305 19:24:08.837621 139689492575424 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:08.841042 139689492575424 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:08.872503 139689492575424 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:11.641933 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 19:29:11.996681 139689492575424 submission_runner.py:469] Time since start: 947.13s, 	Step: 1, 	{'train/accuracy': 0.0006137479213066399, 'train/loss': 11.113200187683105, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.092617988586426, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.114619255065918, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.273067712783813, 'total_duration': 947.1256275177002, 'accumulated_submission_time': 25.273067712783813, 'accumulated_eval_time': 921.8524708747864, 'accumulated_logging_time': 0}
I0305 19:29:12.003523 139546439431936 logging_writer.py:48] [1] accumulated_eval_time=921.852, accumulated_logging_time=0, accumulated_submission_time=25.2731, global_step=1, preemption_count=0, score=25.2731, test/accuracy=0.000718341, test/bleu=0, test/loss=11.1146, test/num_examples=3003, total_duration=947.126, train/accuracy=0.000613748, train/bleu=0, train/loss=11.1132, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.0926, validation/num_examples=3000
I0305 19:29:46.505207 139546431039232 logging_writer.py:48] [100] global_step=100, grad_norm=0.9777467250823975, loss=7.537600517272949
I0305 19:30:21.047351 139546439431936 logging_writer.py:48] [200] global_step=200, grad_norm=0.4771346151828766, loss=6.514475345611572
I0305 19:30:55.618404 139546431039232 logging_writer.py:48] [300] global_step=300, grad_norm=0.5541947484016418, loss=5.855775833129883
I0305 19:31:30.208873 139546439431936 logging_writer.py:48] [400] global_step=400, grad_norm=0.5099092721939087, loss=5.4042067527771
I0305 19:32:04.785410 139546431039232 logging_writer.py:48] [500] global_step=500, grad_norm=0.4920094311237335, loss=4.951297283172607
I0305 19:32:39.343761 139546439431936 logging_writer.py:48] [600] global_step=600, grad_norm=0.48627766966819763, loss=4.743732452392578
I0305 19:33:13.909512 139546431039232 logging_writer.py:48] [700] global_step=700, grad_norm=0.43517640233039856, loss=4.326198577880859
I0305 19:33:48.461887 139546439431936 logging_writer.py:48] [800] global_step=800, grad_norm=0.4773003160953522, loss=4.1233062744140625
I0305 19:34:23.020732 139546431039232 logging_writer.py:48] [900] global_step=900, grad_norm=0.44719991087913513, loss=3.973940372467041
I0305 19:34:57.564355 139546439431936 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.49051347374916077, loss=3.831047296524048
I0305 19:35:32.079997 139546431039232 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4210731089115143, loss=3.695775032043457
I0305 19:36:06.706735 139546439431936 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3926016390323639, loss=3.5031802654266357
I0305 19:36:41.251347 139546431039232 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.40197670459747314, loss=3.525345802307129
I0305 19:37:15.770511 139546439431936 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.3136218786239624, loss=3.462054491043091
I0305 19:37:50.321156 139546431039232 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.2760206460952759, loss=3.3015236854553223
I0305 19:38:24.859003 139546439431936 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.2885584831237793, loss=3.146186351776123
I0305 19:38:59.378483 139546431039232 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.32701587677001953, loss=3.075805187225342
I0305 19:39:33.907550 139546439431936 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.2169514149427414, loss=3.035649538040161
I0305 19:40:08.449443 139546431039232 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.2606760561466217, loss=2.90146541595459
I0305 19:40:42.962669 139546439431936 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.24498453736305237, loss=2.8606936931610107
I0305 19:41:17.476915 139546431039232 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.15630044043064117, loss=2.7406322956085205
I0305 19:41:51.969481 139546439431936 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.20038336515426636, loss=2.7417163848876953
I0305 19:42:26.488533 139546431039232 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.21457046270370483, loss=2.7408647537231445
I0305 19:43:01.019629 139546439431936 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.24825674295425415, loss=2.588449716567993
I0305 19:43:12.081991 139689492575424 spec.py:321] Evaluating on the training split.
I0305 19:43:14.678678 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 19:46:08.772974 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 19:46:11.361766 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 19:48:59.229831 139689492575424 spec.py:349] Evaluating on the test split.
I0305 19:49:01.819190 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 19:51:32.817967 139689492575424 submission_runner.py:469] Time since start: 2287.95s, 	Step: 2433, 	{'train/accuracy': 0.5268543362617493, 'train/loss': 2.633565902709961, 'train/bleu': 23.59888208414684, 'validation/accuracy': 0.5368081331253052, 'validation/loss': 2.552730083465576, 'validation/bleu': 19.799683952039462, 'validation/num_examples': 3000, 'test/accuracy': 0.5373885035514832, 'test/loss': 2.565838575363159, 'test/bleu': 18.162308419097887, 'test/num_examples': 3003, 'score': 865.198891878128, 'total_duration': 2287.946869134903, 'accumulated_submission_time': 865.198891878128, 'accumulated_eval_time': 1422.5883588790894, 'accumulated_logging_time': 0.014955520629882812}
I0305 19:51:32.826554 139546431039232 logging_writer.py:48] [2433] accumulated_eval_time=1422.59, accumulated_logging_time=0.0149555, accumulated_submission_time=865.199, global_step=2433, preemption_count=0, score=865.199, test/accuracy=0.537389, test/bleu=18.1623, test/loss=2.56584, test/num_examples=3003, total_duration=2287.95, train/accuracy=0.526854, train/bleu=23.5989, train/loss=2.63357, validation/accuracy=0.536808, validation/bleu=19.7997, validation/loss=2.55273, validation/num_examples=3000
I0305 19:51:56.223643 139546439431936 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.4332502484321594, loss=2.6595206260681152
I0305 19:52:30.682104 139546431039232 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.21552106738090515, loss=2.532186508178711
I0305 19:53:05.165424 139546439431936 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.3159072995185852, loss=2.4731574058532715
I0305 19:53:39.658154 139546431039232 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.22291013598442078, loss=2.415489673614502
I0305 19:54:14.157193 139546439431936 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.22155669331550598, loss=2.4468436241149902
I0305 19:54:48.719224 139546431039232 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.3586294651031494, loss=2.384636402130127
I0305 19:55:23.242858 139546439431936 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.26085925102233887, loss=2.431658983230591
I0305 19:55:57.778967 139546431039232 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.22027762234210968, loss=2.416274309158325
I0305 19:56:32.310622 139546439431936 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.25236180424690247, loss=2.3032124042510986
I0305 19:57:06.861699 139546431039232 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.3033981919288635, loss=2.3437211513519287
I0305 19:57:41.387757 139546439431936 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.26340705156326294, loss=2.3059258460998535
I0305 19:58:15.944439 139546431039232 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.30875420570373535, loss=2.3271307945251465
I0305 19:58:50.519755 139546439431936 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4469691514968872, loss=2.2864363193511963
I0305 19:59:25.057492 139546431039232 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5370470285415649, loss=2.2392473220825195
I0305 19:59:59.609111 139546439431936 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.34811434149742126, loss=2.274559736251831
I0305 20:00:34.134110 139546431039232 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.41508132219314575, loss=2.2097535133361816
I0305 20:01:08.643149 139546439431936 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.2804245352745056, loss=2.223135232925415
I0305 20:01:43.159622 139546431039232 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4978496730327606, loss=2.2821106910705566
I0305 20:02:17.676890 139546439431936 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.31912165880203247, loss=2.3093528747558594
I0305 20:02:52.222185 139546431039232 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5623548030853271, loss=2.2735652923583984
I0305 20:03:26.764904 139546439431936 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5285129547119141, loss=2.3126373291015625
I0305 20:04:01.313100 139546431039232 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.23502382636070251, loss=2.2284188270568848
I0305 20:04:35.847070 139546439431936 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5824246406555176, loss=2.25663423538208
I0305 20:05:10.420038 139546431039232 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.4398212432861328, loss=2.192072629928589
I0305 20:05:32.860477 139689492575424 spec.py:321] Evaluating on the training split.
I0305 20:05:35.476248 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 20:09:10.059152 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 20:09:12.653674 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 20:11:59.149916 139689492575424 spec.py:349] Evaluating on the test split.
I0305 20:12:01.747509 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 20:14:21.722131 139689492575424 submission_runner.py:469] Time since start: 3656.85s, 	Step: 4866, 	{'train/accuracy': 0.5755481123924255, 'train/loss': 2.233250141143799, 'train/bleu': 27.06807370595582, 'validation/accuracy': 0.5934047102928162, 'validation/loss': 2.0908114910125732, 'validation/bleu': 23.52311379262754, 'validation/num_examples': 3000, 'test/accuracy': 0.5979492664337158, 'test/loss': 2.0481574535369873, 'test/bleu': 21.888933993197586, 'test/num_examples': 3003, 'score': 1705.0806679725647, 'total_duration': 3656.851088285446, 'accumulated_submission_time': 1705.0806679725647, 'accumulated_eval_time': 1951.4499690532684, 'accumulated_logging_time': 0.03181576728820801}
I0305 20:14:21.731521 139546439431936 logging_writer.py:48] [4866] accumulated_eval_time=1951.45, accumulated_logging_time=0.0318158, accumulated_submission_time=1705.08, global_step=4866, preemption_count=0, score=1705.08, test/accuracy=0.597949, test/bleu=21.8889, test/loss=2.04816, test/num_examples=3003, total_duration=3656.85, train/accuracy=0.575548, train/bleu=27.0681, train/loss=2.23325, validation/accuracy=0.593405, validation/bleu=23.5231, validation/loss=2.09081, validation/num_examples=3000
I0305 20:14:33.783186 139546431039232 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.2635383903980255, loss=2.2721779346466064
I0305 20:15:08.209012 139546439431936 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.2973305881023407, loss=2.2147374153137207
I0305 20:15:42.702991 139546431039232 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3697584867477417, loss=2.267176866531372
I0305 20:16:17.209062 139546439431936 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.3007146120071411, loss=2.1787476539611816
I0305 20:16:51.750077 139546431039232 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.34993231296539307, loss=2.1598052978515625
I0305 20:17:26.271762 139546439431936 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.3328894078731537, loss=2.1076858043670654
I0305 20:18:00.785123 139546431039232 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5021392703056335, loss=2.2544472217559814
I0305 20:18:35.305619 139546439431936 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.4743326008319855, loss=2.16068434715271
I0305 20:19:09.838094 139546431039232 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.3264217972755432, loss=2.162583827972412
I0305 20:19:44.363296 139546439431936 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5193892121315002, loss=2.1239192485809326
I0305 20:20:18.916457 139546431039232 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.3305259943008423, loss=2.1020336151123047
I0305 20:20:53.475739 139546439431936 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9697404503822327, loss=2.1901867389678955
I0305 20:21:28.020267 139546431039232 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.3575423061847687, loss=2.259932279586792
I0305 20:22:02.551747 139546439431936 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5102381110191345, loss=2.1382765769958496
I0305 20:22:37.048710 139545878185728 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.535503625869751, loss=2.2596020698547363
I0305 20:23:11.510684 139545869793024 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.2981681823730469, loss=2.1878623962402344
I0305 20:23:45.999815 139545878185728 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.26242101192474365, loss=2.1557726860046387
I0305 20:24:20.443269 139545869793024 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.27378568053245544, loss=2.2203311920166016
I0305 20:24:54.897119 139545878185728 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.9019509553909302, loss=2.2577006816864014
I0305 20:25:29.367228 139545869793024 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.31778860092163086, loss=2.22015118598938
I0305 20:26:03.817196 139545878185728 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.46670472621917725, loss=2.1831066608428955
I0305 20:26:38.282699 139545869793024 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5004008412361145, loss=2.149552583694458
I0305 20:27:12.737012 139545878185728 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.3164818584918976, loss=2.177957773208618
I0305 20:27:47.195237 139545869793024 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.3023875951766968, loss=2.0748178958892822
I0305 20:28:21.627105 139545878185728 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6795549392700195, loss=2.1654632091522217
I0305 20:28:21.978756 139689492575424 spec.py:321] Evaluating on the training split.
I0305 20:28:24.584163 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 20:32:46.556999 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 20:32:49.149653 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 20:36:42.736361 139689492575424 spec.py:349] Evaluating on the test split.
I0305 20:36:45.316738 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 20:39:57.408329 139689492575424 submission_runner.py:469] Time since start: 5192.54s, 	Step: 7302, 	{'train/accuracy': 0.5866950750350952, 'train/loss': 2.1409075260162354, 'train/bleu': 27.617000759658627, 'validation/accuracy': 0.6015252470970154, 'validation/loss': 2.0310425758361816, 'validation/bleu': 24.224764791205338, 'validation/num_examples': 3000, 'test/accuracy': 0.6064302921295166, 'test/loss': 1.9941027164459229, 'test/bleu': 22.91254225656282, 'test/num_examples': 3003, 'score': 2545.177567958832, 'total_duration': 5192.537220478058, 'accumulated_submission_time': 2545.177567958832, 'accumulated_eval_time': 2646.8794317245483, 'accumulated_logging_time': 0.05060625076293945}
I0305 20:39:57.417922 139545869793024 logging_writer.py:48] [7302] accumulated_eval_time=2646.88, accumulated_logging_time=0.0506063, accumulated_submission_time=2545.18, global_step=7302, preemption_count=0, score=2545.18, test/accuracy=0.60643, test/bleu=22.9125, test/loss=1.9941, test/num_examples=3003, total_duration=5192.54, train/accuracy=0.586695, train/bleu=27.617, train/loss=2.14091, validation/accuracy=0.601525, validation/bleu=24.2248, validation/loss=2.03104, validation/num_examples=3000
I0305 20:40:31.403345 139545878185728 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.39330944418907166, loss=2.185589551925659
I0305 20:41:05.734733 139545869793024 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3774589002132416, loss=2.1394734382629395
I0305 20:41:40.122139 139545878185728 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.0805290937423706, loss=2.1563992500305176
I0305 20:42:14.539661 139545869793024 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6728799939155579, loss=2.094538927078247
I0305 20:42:48.976651 139545878185728 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.6145601868629456, loss=2.1780025959014893
I0305 20:43:23.427165 139545869793024 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.48645347356796265, loss=2.1745009422302246
I0305 20:43:57.895014 139545878185728 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.40834420919418335, loss=2.118399143218994
I0305 20:44:32.323096 139545869793024 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.4980565011501312, loss=2.103644609451294
I0305 20:45:06.806515 139545878185728 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3974735140800476, loss=2.1338706016540527
I0305 20:45:41.243748 139545869793024 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3308066427707672, loss=1.9880086183547974
I0305 20:46:15.715573 139545878185728 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5629353523254395, loss=2.147040843963623
I0305 20:46:50.164247 139545869793024 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.40004023909568787, loss=2.1948583126068115
I0305 20:47:24.616941 139545878185728 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.38727763295173645, loss=2.153149127960205
I0305 20:47:59.061060 139545869793024 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.6092793345451355, loss=2.15667986869812
I0305 20:48:33.537835 139545878185728 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3966420292854309, loss=2.096308946609497
I0305 20:49:08.010355 139545869793024 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3467729687690735, loss=2.121478796005249
I0305 20:49:42.459880 139545878185728 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.39723116159439087, loss=2.164891242980957
I0305 20:50:16.893242 139545869793024 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3350902795791626, loss=2.147138833999634
I0305 20:50:51.343205 139545878185728 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.37386104464530945, loss=2.16912841796875
I0305 20:51:25.764130 139545869793024 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5989021062850952, loss=2.174917459487915
I0305 20:52:00.180883 139545878185728 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.38010555505752563, loss=2.1029980182647705
I0305 20:52:34.619744 139545869793024 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3344000279903412, loss=2.113095998764038
I0305 20:53:09.044745 139545878185728 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.47661957144737244, loss=2.093423843383789
I0305 20:53:43.480811 139545869793024 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.6338449120521545, loss=2.1644086837768555
I0305 20:53:57.614964 139689492575424 spec.py:321] Evaluating on the training split.
I0305 20:54:00.219086 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 20:57:11.543380 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 20:57:14.142956 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:00:02.907206 139689492575424 spec.py:349] Evaluating on the test split.
I0305 21:00:05.507188 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:02:46.783546 139689492575424 submission_runner.py:469] Time since start: 6561.91s, 	Step: 9742, 	{'train/accuracy': 0.5926209688186646, 'train/loss': 2.129615068435669, 'train/bleu': 28.122455950412665, 'validation/accuracy': 0.611845850944519, 'validation/loss': 1.9635041952133179, 'validation/bleu': 24.901812232626185, 'validation/num_examples': 3000, 'test/accuracy': 0.6167072057723999, 'test/loss': 1.9267637729644775, 'test/bleu': 23.346317025461424, 'test/num_examples': 3003, 'score': 3385.224435567856, 'total_duration': 6561.912496328354, 'accumulated_submission_time': 3385.224435567856, 'accumulated_eval_time': 3176.047967195511, 'accumulated_logging_time': 0.06971526145935059}
I0305 21:02:46.792100 139545878185728 logging_writer.py:48] [9742] accumulated_eval_time=3176.05, accumulated_logging_time=0.0697153, accumulated_submission_time=3385.22, global_step=9742, preemption_count=0, score=3385.22, test/accuracy=0.616707, test/bleu=23.3463, test/loss=1.92676, test/num_examples=3003, total_duration=6561.91, train/accuracy=0.592621, train/bleu=28.1225, train/loss=2.12962, validation/accuracy=0.611846, validation/bleu=24.9018, validation/loss=1.9635, validation/num_examples=3000
I0305 21:03:07.017814 139545869793024 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.5644939541816711, loss=2.109035015106201
I0305 21:03:41.320244 139545878185728 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.4658958613872528, loss=2.101017713546753
I0305 21:04:15.725428 139545869793024 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3083401918411255, loss=2.1442055702209473
I0305 21:04:50.133679 139545878185728 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3300953507423401, loss=2.1753222942352295
I0305 21:05:24.556431 139545869793024 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.34328776597976685, loss=2.1421284675598145
I0305 21:05:58.950955 139545878185728 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.2882692515850067, loss=2.182318925857544
I0305 21:06:33.379283 139545869793024 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3395366668701172, loss=2.208077907562256
I0305 21:07:07.803958 139545878185728 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3709968030452728, loss=2.132420539855957
I0305 21:07:42.227059 139545869793024 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.5855114459991455, loss=2.184185743331909
I0305 21:08:16.637607 139545878185728 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4214800000190735, loss=2.1397571563720703
I0305 21:08:51.073164 139545869793024 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.5339862108230591, loss=2.1473166942596436
I0305 21:09:25.499386 139545878185728 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.47513699531555176, loss=2.0822038650512695
I0305 21:09:59.940692 139545869793024 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.26740095019340515, loss=2.0960190296173096
I0305 21:10:34.475022 139545878185728 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.911545991897583, loss=2.1292600631713867
I0305 21:11:08.941079 139545869793024 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.35096582770347595, loss=2.14300537109375
I0305 21:11:43.380497 139545878185728 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.5055491924285889, loss=2.1329827308654785
I0305 21:12:17.825678 139545869793024 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.44779524207115173, loss=2.085732936859131
I0305 21:12:52.265688 139545878185728 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.24807342886924744, loss=2.0586695671081543
I0305 21:13:26.690123 139545869793024 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.43055370450019836, loss=2.067101240158081
I0305 21:14:01.155313 139545878185728 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3561052680015564, loss=2.106269359588623
I0305 21:14:35.578728 139545869793024 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5088129043579102, loss=2.2301130294799805
I0305 21:15:10.054271 139545878185728 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.4154844582080841, loss=2.062572956085205
I0305 21:15:44.476269 139545869793024 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3770299255847931, loss=2.1696088314056396
I0305 21:16:18.882478 139545878185728 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3273920714855194, loss=1.980087161064148
I0305 21:16:47.083968 139689492575424 spec.py:321] Evaluating on the training split.
I0305 21:16:49.685629 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:20:02.562222 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 21:20:05.145490 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:23:10.065793 139689492575424 spec.py:349] Evaluating on the test split.
I0305 21:23:12.658252 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:25:59.660320 139689492575424 submission_runner.py:469] Time since start: 7954.79s, 	Step: 12183, 	{'train/accuracy': 0.5957537293434143, 'train/loss': 2.109661340713501, 'train/bleu': 28.22868065607038, 'validation/accuracy': 0.6141448020935059, 'validation/loss': 1.95601224899292, 'validation/bleu': 24.81098728072508, 'validation/num_examples': 3000, 'test/accuracy': 0.6180627942085266, 'test/loss': 1.9053274393081665, 'test/bleu': 23.33940159463234, 'test/num_examples': 3003, 'score': 4225.374735355377, 'total_duration': 7954.78924202919, 'accumulated_submission_time': 4225.374735355377, 'accumulated_eval_time': 3728.624238014221, 'accumulated_logging_time': 0.08627080917358398}
I0305 21:25:59.669528 139545869793024 logging_writer.py:48] [12183] accumulated_eval_time=3728.62, accumulated_logging_time=0.0862708, accumulated_submission_time=4225.37, global_step=12183, preemption_count=0, score=4225.37, test/accuracy=0.618063, test/bleu=23.3394, test/loss=1.90533, test/num_examples=3003, total_duration=7954.79, train/accuracy=0.595754, train/bleu=28.2287, train/loss=2.10966, validation/accuracy=0.614145, validation/bleu=24.811, validation/loss=1.95601, validation/num_examples=3000
I0305 21:26:05.845865 139545878185728 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6817837953567505, loss=2.2082273960113525
I0305 21:26:40.144831 139545869793024 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3062169849872589, loss=2.041991710662842
I0305 21:27:14.442851 139545878185728 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.42112278938293457, loss=2.058960437774658
I0305 21:27:48.820963 139545869793024 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.49394428730010986, loss=2.115358829498291
I0305 21:28:23.260105 139545878185728 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7030904293060303, loss=2.1234495639801025
I0305 21:28:57.742532 139545869793024 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.27235403656959534, loss=2.1577603816986084
I0305 21:29:32.202461 139545878185728 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.4362771809101105, loss=2.1642043590545654
I0305 21:30:06.735141 139545869793024 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.32915323972702026, loss=2.0025572776794434
I0305 21:30:41.243273 139545878185728 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2700580060482025, loss=2.119302272796631
I0305 21:31:15.746926 139545869793024 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.5020824670791626, loss=2.051776170730591
I0305 21:31:50.221469 139545878185728 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.7090948224067688, loss=1.9994248151779175
I0305 21:32:24.792504 139545869793024 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.36212265491485596, loss=1.975650668144226
I0305 21:32:59.325719 139545878185728 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.2750857174396515, loss=2.202911138534546
I0305 21:33:33.816701 139545869793024 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7670659422874451, loss=2.1322271823883057
I0305 21:34:08.317201 139545878185728 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.34526246786117554, loss=2.080994129180908
I0305 21:34:42.833514 139545869793024 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.7620162963867188, loss=2.0312464237213135
I0305 21:35:17.334263 139545878185728 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.28353407979011536, loss=2.0378732681274414
I0305 21:35:51.823498 139545869793024 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.262359619140625, loss=2.0215542316436768
I0305 21:36:26.316628 139545878185728 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.384386271238327, loss=2.0847887992858887
I0305 21:37:00.798453 139545869793024 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.4162830710411072, loss=2.0946173667907715
I0305 21:37:35.311774 139545878185728 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.29426172375679016, loss=2.085963726043701
I0305 21:38:09.816771 139545869793024 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6430860757827759, loss=2.069438934326172
I0305 21:38:44.303619 139545878185728 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6159541606903076, loss=2.04712176322937
I0305 21:39:18.812993 139545869793024 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6907525658607483, loss=2.081814765930176
I0305 21:39:53.305494 139545878185728 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.69391930103302, loss=1.9701764583587646
I0305 21:39:59.866968 139689492575424 spec.py:321] Evaluating on the training split.
I0305 21:40:02.476230 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:43:51.534856 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 21:43:54.130904 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:47:07.353200 139689492575424 spec.py:349] Evaluating on the test split.
I0305 21:47:09.940687 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 21:50:17.087729 139689492575424 submission_runner.py:469] Time since start: 9412.22s, 	Step: 14620, 	{'train/accuracy': 0.5993795990943909, 'train/loss': 2.075961112976074, 'train/bleu': 27.6468726564483, 'validation/accuracy': 0.6140953898429871, 'validation/loss': 1.9391183853149414, 'validation/bleu': 24.631330065556224, 'validation/num_examples': 3000, 'test/accuracy': 0.6224076151847839, 'test/loss': 1.8961374759674072, 'test/bleu': 23.755933195601294, 'test/num_examples': 3003, 'score': 5065.436066389084, 'total_duration': 9412.216654777527, 'accumulated_submission_time': 5065.436066389084, 'accumulated_eval_time': 4345.8449177742, 'accumulated_logging_time': 0.10357880592346191}
I0305 21:50:17.098447 139545869793024 logging_writer.py:48] [14620] accumulated_eval_time=4345.84, accumulated_logging_time=0.103579, accumulated_submission_time=5065.44, global_step=14620, preemption_count=0, score=5065.44, test/accuracy=0.622408, test/bleu=23.7559, test/loss=1.89614, test/num_examples=3003, total_duration=9412.22, train/accuracy=0.59938, train/bleu=27.6469, train/loss=2.07596, validation/accuracy=0.614095, validation/bleu=24.6313, validation/loss=1.93912, validation/num_examples=3000
I0305 21:50:44.911528 139545878185728 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.32611075043678284, loss=2.1566691398620605
I0305 21:51:19.355005 139545869793024 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.3346836566925049, loss=2.046020030975342
I0305 21:51:53.846787 139545878185728 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.3900749981403351, loss=2.1278955936431885
I0305 21:52:28.424022 139545869793024 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5107609033584595, loss=2.070335865020752
I0305 21:53:02.948593 139545878185728 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2770520746707916, loss=1.9617176055908203
I0305 21:53:37.450225 139545869793024 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.37898167967796326, loss=2.0843443870544434
I0305 21:54:11.965471 139545878185728 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2779705822467804, loss=2.040562868118286
I0305 21:54:46.452091 139545869793024 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.5003114342689514, loss=2.097201347351074
I0305 21:55:20.932868 139545878185728 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.43360406160354614, loss=2.1500244140625
I0305 21:55:55.441773 139545869793024 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2814117968082428, loss=2.036853790283203
I0305 21:56:29.952664 139545878185728 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.5936218500137329, loss=2.045836925506592
I0305 21:57:04.459440 139545869793024 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.34682339429855347, loss=2.0771889686584473
I0305 21:57:38.993740 139545878185728 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.6220714449882507, loss=2.0296616554260254
I0305 21:58:13.494951 139545869793024 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.402496874332428, loss=2.0080554485321045
I0305 21:58:47.987665 139545878185728 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.42035597562789917, loss=2.022690534591675
I0305 21:59:22.508410 139545869793024 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.40826350450515747, loss=2.0612542629241943
I0305 21:59:57.005358 139545878185728 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3619847595691681, loss=2.0612621307373047
I0305 22:00:31.497315 139545869793024 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.5492734313011169, loss=2.1051063537597656
I0305 22:01:05.986751 139545878185728 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5789158940315247, loss=2.0652008056640625
I0305 22:01:40.501657 139545869793024 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3058389723300934, loss=2.0682857036590576
I0305 22:02:14.979858 139545878185728 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4373527467250824, loss=2.169254779815674
I0305 22:02:49.483564 139545869793024 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.4121883809566498, loss=2.061784267425537
I0305 22:03:23.996323 139545878185728 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5747699737548828, loss=2.0793418884277344
I0305 22:03:58.501745 139545869793024 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3050747811794281, loss=2.1208527088165283
I0305 22:04:17.125894 139689492575424 spec.py:321] Evaluating on the training split.
I0305 22:04:19.722855 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:07:23.073815 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 22:07:25.670517 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:10:15.960987 139689492575424 spec.py:349] Evaluating on the test split.
I0305 22:10:18.546880 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:13:02.692379 139689492575424 submission_runner.py:469] Time since start: 10777.82s, 	Step: 17055, 	{'train/accuracy': 0.5998297333717346, 'train/loss': 2.071913242340088, 'train/bleu': 28.266365049177512, 'validation/accuracy': 0.6177539229393005, 'validation/loss': 1.9214348793029785, 'validation/bleu': 24.703533923266356, 'validation/num_examples': 3000, 'test/accuracy': 0.627250611782074, 'test/loss': 1.8704904317855835, 'test/bleu': 23.91687603984399, 'test/num_examples': 3003, 'score': 5905.322413682938, 'total_duration': 10777.821339130402, 'accumulated_submission_time': 5905.322413682938, 'accumulated_eval_time': 4871.411356687546, 'accumulated_logging_time': 0.12322354316711426}
I0305 22:13:02.702114 139545878185728 logging_writer.py:48] [17055] accumulated_eval_time=4871.41, accumulated_logging_time=0.123224, accumulated_submission_time=5905.32, global_step=17055, preemption_count=0, score=5905.32, test/accuracy=0.627251, test/bleu=23.9169, test/loss=1.87049, test/num_examples=3003, total_duration=10777.8, train/accuracy=0.59983, train/bleu=28.2664, train/loss=2.07191, validation/accuracy=0.617754, validation/bleu=24.7035, validation/loss=1.92143, validation/num_examples=3000
I0305 22:13:18.488554 139545869793024 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3881966471672058, loss=2.140641927719116
I0305 22:13:52.876825 139545878185728 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.5239346623420715, loss=2.109884738922119
I0305 22:14:27.326508 139545869793024 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.4853914976119995, loss=2.0540008544921875
I0305 22:15:01.816315 139545878185728 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.49207764863967896, loss=2.068618059158325
I0305 22:15:36.374679 139545869793024 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.35631081461906433, loss=2.0221004486083984
I0305 22:16:10.887582 139545878185728 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.35730016231536865, loss=2.120619535446167
I0305 22:16:45.415338 139545869793024 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2879575788974762, loss=2.1113128662109375
I0305 22:17:19.961442 139545878185728 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.36827489733695984, loss=2.0758326053619385
I0305 22:17:54.498983 139545869793024 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6014298796653748, loss=1.998408317565918
I0305 22:18:29.017794 139545878185728 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.7458690404891968, loss=2.146822929382324
I0305 22:19:03.512790 139545869793024 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.5251063108444214, loss=2.041264772415161
I0305 22:19:37.992078 139545878185728 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3859233558177948, loss=2.0667543411254883
I0305 22:20:12.522818 139545869793024 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6203382015228271, loss=2.081357955932617
I0305 22:20:47.031103 139545878185728 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.43480926752090454, loss=2.063783645629883
I0305 22:21:21.527278 139545869793024 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.31831809878349304, loss=2.0911779403686523
I0305 22:21:56.017763 139545878185728 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.35837116837501526, loss=2.081820249557495
I0305 22:22:30.457702 139545869793024 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3349180221557617, loss=2.0831403732299805
I0305 22:23:04.986402 139545878185728 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.7141175866127014, loss=2.0724635124206543
I0305 22:23:39.733066 139545869793024 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.7616536617279053, loss=2.103095531463623
I0305 22:24:14.426257 139545878185728 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.8871753811836243, loss=2.101358413696289
I0305 22:24:49.075840 139545869793024 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.4245775640010834, loss=2.0674197673797607
I0305 22:25:23.747448 139545878185728 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3829388916492462, loss=2.085299491882324
I0305 22:25:58.370716 139545869793024 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.5839295983314514, loss=2.0328824520111084
I0305 22:26:33.017138 139545878185728 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3149098753929138, loss=2.091299057006836
I0305 22:27:02.812176 139689492575424 spec.py:321] Evaluating on the training split.
I0305 22:27:05.428098 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:30:04.951062 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 22:30:07.546370 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:33:00.187932 139689492575424 spec.py:349] Evaluating on the test split.
I0305 22:33:02.788282 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:35:44.872528 139689492575424 submission_runner.py:469] Time since start: 12140.00s, 	Step: 19487, 	{'train/accuracy': 0.6062939763069153, 'train/loss': 2.002042055130005, 'train/bleu': 28.480109655554514, 'validation/accuracy': 0.6182236075401306, 'validation/loss': 1.909186840057373, 'validation/bleu': 24.821472033894906, 'validation/num_examples': 3000, 'test/accuracy': 0.6248059272766113, 'test/loss': 1.8629462718963623, 'test/bleu': 24.132169736423464, 'test/num_examples': 3003, 'score': 6745.2925136089325, 'total_duration': 12140.001472711563, 'accumulated_submission_time': 6745.2925136089325, 'accumulated_eval_time': 5393.471658945084, 'accumulated_logging_time': 0.14071011543273926}
I0305 22:35:44.883172 139545869793024 logging_writer.py:48] [19487] accumulated_eval_time=5393.47, accumulated_logging_time=0.14071, accumulated_submission_time=6745.29, global_step=19487, preemption_count=0, score=6745.29, test/accuracy=0.624806, test/bleu=24.1322, test/loss=1.86295, test/num_examples=3003, total_duration=12140, train/accuracy=0.606294, train/bleu=28.4801, train/loss=2.00204, validation/accuracy=0.618224, validation/bleu=24.8215, validation/loss=1.90919, validation/num_examples=3000
I0305 22:35:49.715679 139545878185728 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.34918180108070374, loss=2.0617852210998535
I0305 22:36:24.209827 139545869793024 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5648018717765808, loss=1.9778927564620972
I0305 22:36:58.775147 139545878185728 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.44831645488739014, loss=2.0938754081726074
I0305 22:37:33.377283 139545869793024 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.5007485747337341, loss=2.0910696983337402
I0305 22:38:08.029680 139545878185728 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.31785130500793457, loss=2.0418405532836914
I0305 22:38:42.632458 139545869793024 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.48370257019996643, loss=2.068995952606201
I0305 22:39:17.220305 139545878185728 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3838905692100525, loss=1.9689451456069946
I0305 22:39:51.863307 139545869793024 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.33915969729423523, loss=2.0456771850585938
I0305 22:40:26.492241 139545878185728 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.32886993885040283, loss=2.0079798698425293
I0305 22:41:01.089039 139545869793024 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.270523339509964, loss=2.0196433067321777
I0305 22:41:35.736601 139545878185728 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.4106457829475403, loss=2.093815326690674
I0305 22:42:10.342370 139545869793024 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.31687313318252563, loss=2.081686496734619
I0305 22:42:45.001008 139545878185728 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.2883528470993042, loss=2.032902240753174
I0305 22:43:19.585458 139545869793024 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.2793515622615814, loss=2.0564699172973633
I0305 22:43:54.226446 139545878185728 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.41074079275131226, loss=2.048243761062622
I0305 22:44:28.826617 139545869793024 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.39308953285217285, loss=2.0130958557128906
I0305 22:45:03.484102 139545878185728 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.3730399012565613, loss=2.0086617469787598
I0305 22:45:38.094099 139545869793024 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4657335579395294, loss=2.0587868690490723
I0305 22:46:12.685644 139545878185728 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.3378733694553375, loss=1.9846307039260864
I0305 22:46:47.274732 139545869793024 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8134378790855408, loss=2.100098133087158
I0305 22:47:21.894863 139545878185728 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.3781360387802124, loss=2.13801908493042
I0305 22:47:56.492771 139545869793024 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3045229911804199, loss=2.117659330368042
I0305 22:48:31.156694 139545878185728 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.3999318480491638, loss=2.0402848720550537
I0305 22:49:05.790226 139545869793024 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.28963232040405273, loss=2.0514235496520996
I0305 22:49:40.406608 139545878185728 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.39888766407966614, loss=2.064091682434082
I0305 22:49:44.915797 139689492575424 spec.py:321] Evaluating on the training split.
I0305 22:49:47.529377 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:52:56.728666 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 22:52:59.330956 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:56:00.326981 139689492575424 spec.py:349] Evaluating on the test split.
I0305 22:56:02.914801 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 22:58:47.348601 139689492575424 submission_runner.py:469] Time since start: 13522.48s, 	Step: 21914, 	{'train/accuracy': 0.6007887125015259, 'train/loss': 2.0580198764801025, 'train/bleu': 28.323652909604206, 'validation/accuracy': 0.6190640926361084, 'validation/loss': 1.9009321928024292, 'validation/bleu': 24.85648089794779, 'validation/num_examples': 3000, 'test/accuracy': 0.6276909112930298, 'test/loss': 1.8486824035644531, 'test/bleu': 24.52536076890693, 'test/num_examples': 3003, 'score': 7585.193206548691, 'total_duration': 13522.477527618408, 'accumulated_submission_time': 7585.193206548691, 'accumulated_eval_time': 5935.904383182526, 'accumulated_logging_time': 0.15914463996887207}
I0305 22:58:47.359439 139545869793024 logging_writer.py:48] [21914] accumulated_eval_time=5935.9, accumulated_logging_time=0.159145, accumulated_submission_time=7585.19, global_step=21914, preemption_count=0, score=7585.19, test/accuracy=0.627691, test/bleu=24.5254, test/loss=1.84868, test/num_examples=3003, total_duration=13522.5, train/accuracy=0.600789, train/bleu=28.3237, train/loss=2.05802, validation/accuracy=0.619064, validation/bleu=24.8565, validation/loss=1.90093, validation/num_examples=3000
I0305 22:59:17.353820 139545878185728 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.32424280047416687, loss=1.9573487043380737
I0305 22:59:51.881614 139545869793024 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.47208172082901, loss=2.0337438583374023
I0305 23:00:26.477708 139545878185728 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3006652295589447, loss=2.112114429473877
I0305 23:01:01.080109 139545869793024 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3849273920059204, loss=2.055788993835449
I0305 23:01:35.689050 139545878185728 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.44038382172584534, loss=2.140986204147339
I0305 23:02:10.337751 139545869793024 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.49067169427871704, loss=2.125823974609375
I0305 23:02:44.927382 139545878185728 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.32265546917915344, loss=2.00665545463562
I0305 23:03:19.582233 139545869793024 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.3647295832633972, loss=2.0309441089630127
I0305 23:03:54.169521 139545878185728 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.4081047773361206, loss=2.02101993560791
I0305 23:04:28.788753 139545869793024 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3531024754047394, loss=2.044133186340332
I0305 23:05:03.419317 139545878185728 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.49026018381118774, loss=1.9964282512664795
I0305 23:05:38.026730 139545869793024 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.414816677570343, loss=2.1538054943084717
I0305 23:06:12.645697 139545878185728 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3351193368434906, loss=2.144094228744507
I0305 23:06:47.281884 139545869793024 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5146030187606812, loss=2.0289535522460938
I0305 23:07:21.892386 139545878185728 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.29119497537612915, loss=2.047410726547241
I0305 23:07:56.538363 139545869793024 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.2875566780567169, loss=1.9866385459899902
I0305 23:08:31.147191 139545878185728 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.45309919118881226, loss=2.017509698867798
I0305 23:09:05.795597 139545869793024 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.35973963141441345, loss=2.077238082885742
I0305 23:09:40.384177 139545878185728 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.4276405870914459, loss=1.9907869100570679
I0305 23:10:15.020293 139545869793024 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3292044401168823, loss=2.019566535949707
I0305 23:10:49.632164 139545878185728 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.31803157925605774, loss=2.0593178272247314
I0305 23:11:24.283918 139545869793024 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.38237911462783813, loss=2.0575695037841797
I0305 23:11:58.907662 139545878185728 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.24712032079696655, loss=1.936798095703125
I0305 23:12:33.537880 139545869793024 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.31203556060791016, loss=1.9322271347045898
I0305 23:12:47.365616 139689492575424 spec.py:321] Evaluating on the training split.
I0305 23:12:49.974092 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 23:16:08.973025 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 23:16:11.567636 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 23:19:12.485462 139689492575424 spec.py:349] Evaluating on the test split.
I0305 23:19:15.085900 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 23:22:09.652951 139689492575424 submission_runner.py:469] Time since start: 14924.78s, 	Step: 24341, 	{'train/accuracy': 0.603492021560669, 'train/loss': 2.0577852725982666, 'train/bleu': 29.115690771638018, 'validation/accuracy': 0.622005820274353, 'validation/loss': 1.890318751335144, 'validation/bleu': 25.613137879165905, 'validation/num_examples': 3000, 'test/accuracy': 0.630390465259552, 'test/loss': 1.8423603773117065, 'test/bleu': 24.8209793289967, 'test/num_examples': 3003, 'score': 8425.063936471939, 'total_duration': 14924.781909704208, 'accumulated_submission_time': 8425.063936471939, 'accumulated_eval_time': 6498.191673755646, 'accumulated_logging_time': 0.17865896224975586}
I0305 23:22:09.663933 139545878185728 logging_writer.py:48] [24341] accumulated_eval_time=6498.19, accumulated_logging_time=0.178659, accumulated_submission_time=8425.06, global_step=24341, preemption_count=0, score=8425.06, test/accuracy=0.63039, test/bleu=24.821, test/loss=1.84236, test/num_examples=3003, total_duration=14924.8, train/accuracy=0.603492, train/bleu=29.1157, train/loss=2.05779, validation/accuracy=0.622006, validation/bleu=25.6131, validation/loss=1.89032, validation/num_examples=3000
I0305 23:22:30.355918 139545869793024 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.42127445340156555, loss=1.9803392887115479
I0305 23:23:04.874033 139545878185728 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3506494164466858, loss=2.0629990100860596
I0305 23:23:39.460312 139545869793024 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4181574881076813, loss=2.1036784648895264
I0305 23:24:14.058895 139545878185728 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.353325754404068, loss=2.0953855514526367
I0305 23:24:48.641987 139545869793024 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3583256006240845, loss=2.1072678565979004
I0305 23:25:23.221149 139545878185728 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3997308313846588, loss=2.0735926628112793
I0305 23:25:57.821188 139545869793024 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.33708345890045166, loss=2.08657169342041
I0305 23:26:32.441035 139545878185728 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.37234875559806824, loss=2.1487836837768555
I0305 23:27:06.928724 139545869793024 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4318457543849945, loss=2.047520399093628
I0305 23:27:41.460359 139545878185728 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.42491114139556885, loss=2.0178542137145996
I0305 23:28:15.940069 139545869793024 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.31589028239250183, loss=2.1664462089538574
I0305 23:28:50.466136 139545878185728 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.283486545085907, loss=2.0115854740142822
I0305 23:29:24.973460 139545869793024 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2988247573375702, loss=2.0331640243530273
I0305 23:29:59.488842 139545878185728 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3562175929546356, loss=1.915700912475586
I0305 23:30:34.044493 139545869793024 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3224952816963196, loss=2.03259539604187
I0305 23:31:08.589138 139545878185728 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3739956021308899, loss=2.042045831680298
I0305 23:31:43.064735 139545869793024 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3226609528064728, loss=2.0561013221740723
I0305 23:32:17.605985 139545878185728 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3620177209377289, loss=2.057616949081421
I0305 23:32:52.184321 139545869793024 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.2922685444355011, loss=2.0531113147735596
I0305 23:33:26.671454 139545878185728 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.32342565059661865, loss=2.0785906314849854
I0305 23:34:01.201837 139545869793024 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.33196398615837097, loss=2.06186842918396
I0305 23:34:35.732280 139545878185728 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3100365996360779, loss=2.0318236351013184
I0305 23:35:10.288894 139545869793024 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.36705878376960754, loss=2.01357102394104
I0305 23:35:44.822442 139545878185728 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3241998553276062, loss=2.0313258171081543
I0305 23:36:09.690397 139689492575424 spec.py:321] Evaluating on the training split.
I0305 23:36:12.301612 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 23:39:21.444978 139689492575424 spec.py:333] Evaluating on the validation split.
I0305 23:39:24.046248 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 23:42:04.988820 139689492575424 spec.py:349] Evaluating on the test split.
I0305 23:42:07.588266 139689492575424 workload.py:181] Translating evaluation dataset.
I0305 23:44:46.205018 139689492575424 submission_runner.py:469] Time since start: 16281.33s, 	Step: 26773, 	{'train/accuracy': 0.6080901026725769, 'train/loss': 1.9998950958251953, 'train/bleu': 28.095064153509668, 'validation/accuracy': 0.6239339709281921, 'validation/loss': 1.8744738101959229, 'validation/bleu': 25.602918024913002, 'validation/num_examples': 3000, 'test/accuracy': 0.6316301822662354, 'test/loss': 1.8300033807754517, 'test/bleu': 24.760247311050563, 'test/num_examples': 3003, 'score': 9264.954586982727, 'total_duration': 16281.333972454071, 'accumulated_submission_time': 9264.954586982727, 'accumulated_eval_time': 7014.706246376038, 'accumulated_logging_time': 0.1974349021911621}
I0305 23:44:46.216422 139545869793024 logging_writer.py:48] [26773] accumulated_eval_time=7014.71, accumulated_logging_time=0.197435, accumulated_submission_time=9264.95, global_step=26773, preemption_count=0, score=9264.95, test/accuracy=0.63163, test/bleu=24.7602, test/loss=1.83, test/num_examples=3003, total_duration=16281.3, train/accuracy=0.60809, train/bleu=28.0951, train/loss=1.9999, validation/accuracy=0.623934, validation/bleu=25.6029, validation/loss=1.87447, validation/num_examples=3000
I0305 23:44:55.840862 139545878185728 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.41930732131004333, loss=2.0629634857177734
I0305 23:45:30.189114 139545869793024 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.3722921907901764, loss=2.0490188598632812
I0305 23:46:04.687259 139545878185728 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3209940493106842, loss=2.048290967941284
I0305 23:46:39.218898 139545869793024 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.2997109889984131, loss=1.9691433906555176
I0305 23:47:13.726381 139545878185728 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.32678139209747314, loss=2.030730962753296
I0305 23:47:48.272719 139545869793024 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.3208087682723999, loss=1.9434857368469238
I0305 23:48:22.794874 139545878185728 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2618558406829834, loss=1.9942632913589478
I0305 23:48:57.317917 139545869793024 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2889518737792969, loss=2.015286445617676
I0305 23:49:31.833787 139545878185728 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.3297938406467438, loss=2.097149610519409
I0305 23:50:06.367624 139545869793024 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.3417617380619049, loss=2.0209827423095703
I0305 23:50:40.877772 139545878185728 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.3302183449268341, loss=2.0920684337615967
I0305 23:51:15.428562 139545869793024 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.3888587951660156, loss=2.050989866256714
I0305 23:51:49.962403 139545878185728 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.29906320571899414, loss=2.0363779067993164
I0305 23:52:24.504052 139545869793024 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.39480462670326233, loss=2.112079381942749
I0305 23:52:59.006221 139545878185728 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.3629812002182007, loss=1.9746365547180176
I0305 23:53:33.505486 139545869793024 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.35514986515045166, loss=1.9949554204940796
I0305 23:54:08.054499 139545878185728 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.3831513524055481, loss=2.0341379642486572
I0305 23:54:42.520902 139545869793024 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.28282245993614197, loss=2.0653138160705566
I0305 23:55:17.044801 139545878185728 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.3315272629261017, loss=1.9153703451156616
I0305 23:55:51.581826 139545869793024 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.3425092399120331, loss=2.05823016166687
I0305 23:56:26.141476 139545878185728 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.280835896730423, loss=1.9566855430603027
I0305 23:57:00.640594 139545869793024 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.2577539384365082, loss=2.0191075801849365
I0305 23:57:35.149892 139545878185728 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.3818355202674866, loss=2.0670957565307617
I0305 23:58:09.675490 139545869793024 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.3353046774864197, loss=1.9482505321502686
I0305 23:58:44.192650 139545878185728 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4387473165988922, loss=2.066038131713867
I0305 23:58:46.276201 139689492575424 spec.py:321] Evaluating on the training split.
I0305 23:58:48.881783 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:02:11.172666 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 00:02:13.766123 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:05:29.176830 139689492575424 spec.py:349] Evaluating on the test split.
I0306 00:05:31.769855 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:08:23.644872 139689492575424 submission_runner.py:469] Time since start: 17698.77s, 	Step: 29207, 	{'train/accuracy': 0.6065573692321777, 'train/loss': 2.0197722911834717, 'train/bleu': 28.237094677986015, 'validation/accuracy': 0.6232665181159973, 'validation/loss': 1.8741369247436523, 'validation/bleu': 25.141865675721682, 'validation/num_examples': 3000, 'test/accuracy': 0.628953754901886, 'test/loss': 1.8296782970428467, 'test/bleu': 24.23731214332591, 'test/num_examples': 3003, 'score': 10104.879020929337, 'total_duration': 17698.773823976517, 'accumulated_submission_time': 10104.879020929337, 'accumulated_eval_time': 7592.074867486954, 'accumulated_logging_time': 0.21680188179016113}
I0306 00:08:23.655980 139545869793024 logging_writer.py:48] [29207] accumulated_eval_time=7592.07, accumulated_logging_time=0.216802, accumulated_submission_time=10104.9, global_step=29207, preemption_count=0, score=10104.9, test/accuracy=0.628954, test/bleu=24.2373, test/loss=1.82968, test/num_examples=3003, total_duration=17698.8, train/accuracy=0.606557, train/bleu=28.2371, train/loss=2.01977, validation/accuracy=0.623267, validation/bleu=25.1419, validation/loss=1.87414, validation/num_examples=3000
I0306 00:08:55.944196 139545878185728 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3777250051498413, loss=2.063659906387329
I0306 00:09:30.381755 139545869793024 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.334084153175354, loss=2.013518810272217
I0306 00:10:04.842197 139545878185728 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2677253782749176, loss=1.950679898262024
I0306 00:10:39.307966 139545869793024 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3480493128299713, loss=1.994413137435913
I0306 00:11:13.771266 139545878185728 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.3277587890625, loss=1.9738044738769531
I0306 00:11:48.261904 139545869793024 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.3006497323513031, loss=2.0655713081359863
I0306 00:12:22.725944 139545878185728 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.26017478108406067, loss=1.9895328283309937
I0306 00:12:57.201531 139545869793024 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2652435600757599, loss=1.9244825839996338
I0306 00:13:31.683770 139545878185728 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3807739019393921, loss=2.081613779067993
I0306 00:14:06.188506 139545869793024 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.2730720043182373, loss=1.9320992231369019
I0306 00:14:40.687441 139545878185728 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3714834749698639, loss=2.0125181674957275
I0306 00:15:15.189129 139545869793024 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.35139715671539307, loss=2.044731616973877
I0306 00:15:49.703104 139545878185728 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.26486819982528687, loss=2.0154128074645996
I0306 00:16:24.198737 139545869793024 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.2915111184120178, loss=1.9962712526321411
I0306 00:16:58.735255 139545878185728 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.38287022709846497, loss=1.964309811592102
I0306 00:17:33.224002 139545869793024 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.45575815439224243, loss=2.0434601306915283
I0306 00:18:07.751774 139545878185728 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.38347238302230835, loss=2.0882396697998047
I0306 00:18:42.249155 139545869793024 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.3283595144748688, loss=1.9799383878707886
I0306 00:19:16.806651 139545878185728 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.5739936828613281, loss=2.04193115234375
I0306 00:19:51.338846 139545869793024 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.28552567958831787, loss=1.9785845279693604
I0306 00:20:25.902533 139545878185728 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.34433579444885254, loss=2.02108097076416
I0306 00:21:00.485077 139545869793024 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.31471186876296997, loss=1.9358164072036743
I0306 00:21:35.130816 139545878185728 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.2779122591018677, loss=1.9607677459716797
I0306 00:22:09.815055 139545869793024 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.29349270462989807, loss=1.9546481370925903
I0306 00:22:23.662959 139689492575424 spec.py:321] Evaluating on the training split.
I0306 00:22:26.272915 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:26:11.202267 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 00:26:13.795759 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:29:13.160017 139689492575424 spec.py:349] Evaluating on the test split.
I0306 00:29:15.760177 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:32:20.747922 139689492575424 submission_runner.py:469] Time since start: 19135.88s, 	Step: 31641, 	{'train/accuracy': 0.6186055541038513, 'train/loss': 1.8835495710372925, 'train/bleu': 29.28565242711362, 'validation/accuracy': 0.6194719672203064, 'validation/loss': 1.89082670211792, 'validation/bleu': 25.13284595514229, 'validation/num_examples': 3000, 'test/accuracy': 0.6315374970436096, 'test/loss': 1.8259590864181519, 'test/bleu': 24.868472808984976, 'test/num_examples': 3003, 'score': 10944.750674247742, 'total_duration': 19135.87686753273, 'accumulated_submission_time': 10944.750674247742, 'accumulated_eval_time': 8189.159771203995, 'accumulated_logging_time': 0.2370615005493164}
I0306 00:32:20.760174 139545878185728 logging_writer.py:48] [31641] accumulated_eval_time=8189.16, accumulated_logging_time=0.237062, accumulated_submission_time=10944.8, global_step=31641, preemption_count=0, score=10944.8, test/accuracy=0.631537, test/bleu=24.8685, test/loss=1.82596, test/num_examples=3003, total_duration=19135.9, train/accuracy=0.618606, train/bleu=29.2857, train/loss=1.88355, validation/accuracy=0.619472, validation/bleu=25.1328, validation/loss=1.89083, validation/num_examples=3000
I0306 00:32:41.467323 139545869793024 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.293170690536499, loss=1.9519778490066528
I0306 00:33:16.026685 139545878185728 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.2818310260772705, loss=2.0415210723876953
I0306 00:33:50.669440 139545869793024 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.37039464712142944, loss=2.0657403469085693
I0306 00:34:25.282366 139545878185728 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3991096019744873, loss=1.9655847549438477
I0306 00:34:59.883659 139545869793024 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3790385127067566, loss=1.984895944595337
I0306 00:35:34.503455 139545878185728 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.28518518805503845, loss=1.9774857759475708
I0306 00:36:09.123006 139545869793024 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.374372273683548, loss=1.9798972606658936
I0306 00:36:43.722377 139545878185728 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2662234306335449, loss=2.051159143447876
I0306 00:37:18.332704 139545869793024 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.31837648153305054, loss=1.986515998840332
I0306 00:37:52.954282 139545878185728 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.31694573163986206, loss=1.972699761390686
I0306 00:38:27.622962 139545869793024 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.29170888662338257, loss=1.9695789813995361
I0306 00:39:02.271842 139545878185728 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.3422203063964844, loss=1.9789625406265259
I0306 00:39:36.908164 139545869793024 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.5008384585380554, loss=2.0145320892333984
I0306 00:40:11.495862 139545878185728 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3067247271537781, loss=1.941285252571106
I0306 00:40:46.073178 139545869793024 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.3196314871311188, loss=2.0239603519439697
I0306 00:41:20.727138 139545878185728 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.3696349561214447, loss=2.038858413696289
I0306 00:41:55.331940 139545869793024 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.34397706389427185, loss=2.0154078006744385
I0306 00:42:29.886393 139545878185728 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.43971821665763855, loss=1.9682824611663818
I0306 00:43:04.477988 139545869793024 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.4677187502384186, loss=2.041872501373291
I0306 00:43:39.066519 139545878185728 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.42089715600013733, loss=2.039095878601074
I0306 00:44:13.620436 139545869793024 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.3122505843639374, loss=2.0146737098693848
I0306 00:44:48.149797 139545878185728 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.27473098039627075, loss=1.9348522424697876
I0306 00:45:22.707499 139545869793024 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.28205540776252747, loss=1.9676507711410522
I0306 00:45:57.248866 139545878185728 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.4639367461204529, loss=1.9418431520462036
I0306 00:46:21.064814 139689492575424 spec.py:321] Evaluating on the training split.
I0306 00:46:23.675863 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:49:49.940111 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 00:49:52.523199 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:52:58.280431 139689492575424 spec.py:349] Evaluating on the test split.
I0306 00:53:00.866217 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 00:55:49.175565 139689492575424 submission_runner.py:469] Time since start: 20544.30s, 	Step: 34070, 	{'train/accuracy': 0.6033328175544739, 'train/loss': 2.0315353870391846, 'train/bleu': 28.374210626281904, 'validation/accuracy': 0.6257879734039307, 'validation/loss': 1.8602033853530884, 'validation/bleu': 25.092722468136138, 'validation/num_examples': 3000, 'test/accuracy': 0.6340053081512451, 'test/loss': 1.8100531101226807, 'test/bleu': 24.322860514866825, 'test/num_examples': 3003, 'score': 11784.921707630157, 'total_duration': 20544.304527759552, 'accumulated_submission_time': 11784.921707630157, 'accumulated_eval_time': 8757.27048420906, 'accumulated_logging_time': 0.2573122978210449}
I0306 00:55:49.187469 139545869793024 logging_writer.py:48] [34070] accumulated_eval_time=8757.27, accumulated_logging_time=0.257312, accumulated_submission_time=11784.9, global_step=34070, preemption_count=0, score=11784.9, test/accuracy=0.634005, test/bleu=24.3229, test/loss=1.81005, test/num_examples=3003, total_duration=20544.3, train/accuracy=0.603333, train/bleu=28.3742, train/loss=2.03154, validation/accuracy=0.625788, validation/bleu=25.0927, validation/loss=1.8602, validation/num_examples=3000
I0306 00:55:59.820068 139545878185728 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5413650274276733, loss=2.105147123336792
I0306 00:56:34.160824 139545869793024 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.3142370581626892, loss=2.010037899017334
I0306 00:57:08.596071 139545878185728 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.34934303164482117, loss=2.0791501998901367
I0306 00:57:43.080667 139545869793024 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.36850792169570923, loss=1.9761327505111694
I0306 00:58:17.579235 139545878185728 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.3893563747406006, loss=1.9995477199554443
I0306 00:58:52.059204 139545869793024 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.33813709020614624, loss=1.9618287086486816
I0306 00:59:26.552468 139545878185728 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.527113139629364, loss=2.076314926147461
I0306 01:00:01.061226 139545869793024 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.4942243695259094, loss=1.97879958152771
I0306 01:00:35.548003 139545878185728 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.26775020360946655, loss=1.8994629383087158
I0306 01:01:10.060342 139545869793024 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.29243868589401245, loss=2.0882792472839355
I0306 01:01:44.570730 139545878185728 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.37979841232299805, loss=1.9772193431854248
I0306 01:02:19.066575 139545869793024 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.26943516731262207, loss=2.033454179763794
I0306 01:02:53.577022 139545878185728 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.4745514690876007, loss=1.9672274589538574
I0306 01:03:28.149189 139545869793024 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.30834293365478516, loss=2.0030083656311035
I0306 01:04:02.645473 139545878185728 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.32045963406562805, loss=2.015094518661499
I0306 01:04:37.114311 139545869793024 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.29534006118774414, loss=2.0359041690826416
I0306 01:05:11.620599 139545878185728 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.463008850812912, loss=2.0609467029571533
I0306 01:05:46.144884 139545869793024 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.29317450523376465, loss=2.014962911605835
I0306 01:06:20.657042 139545878185728 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.494398832321167, loss=2.0506856441497803
I0306 01:06:55.165031 139545869793024 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3336544334888458, loss=1.9240223169326782
I0306 01:07:29.676413 139545878185728 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.2761383652687073, loss=1.9271841049194336
I0306 01:08:04.186623 139545869793024 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.29183605313301086, loss=2.054206132888794
I0306 01:08:38.674414 139545878185728 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.31675490736961365, loss=2.014808177947998
I0306 01:09:13.196282 139545869793024 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3083331882953644, loss=2.052933931350708
I0306 01:09:47.780509 139545878185728 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.2775746285915375, loss=1.993309497833252
I0306 01:09:49.510878 139689492575424 spec.py:321] Evaluating on the training split.
I0306 01:09:52.117519 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 01:13:23.288071 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 01:13:25.875171 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 01:17:03.951886 139689492575424 spec.py:349] Evaluating on the test split.
I0306 01:17:06.530616 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 01:20:06.074407 139689492575424 submission_runner.py:469] Time since start: 22001.20s, 	Step: 36506, 	{'train/accuracy': 0.6059224605560303, 'train/loss': 2.025665760040283, 'train/bleu': 28.93851010404595, 'validation/accuracy': 0.6288408637046814, 'validation/loss': 1.8471442461013794, 'validation/bleu': 25.76470886507192, 'validation/num_examples': 3000, 'test/accuracy': 0.6358938813209534, 'test/loss': 1.7881492376327515, 'test/bleu': 24.858251379464022, 'test/num_examples': 3003, 'score': 12625.111197710037, 'total_duration': 22001.20336818695, 'accumulated_submission_time': 12625.111197710037, 'accumulated_eval_time': 9373.833971738815, 'accumulated_logging_time': 0.277829647064209}
I0306 01:20:06.085890 139545869793024 logging_writer.py:48] [36506] accumulated_eval_time=9373.83, accumulated_logging_time=0.27783, accumulated_submission_time=12625.1, global_step=36506, preemption_count=0, score=12625.1, test/accuracy=0.635894, test/bleu=24.8583, test/loss=1.78815, test/num_examples=3003, total_duration=22001.2, train/accuracy=0.605922, train/bleu=28.9385, train/loss=2.02567, validation/accuracy=0.628841, validation/bleu=25.7647, validation/loss=1.84714, validation/num_examples=3000
I0306 01:20:38.743875 139545878185728 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.3652273416519165, loss=2.0340065956115723
I0306 01:21:13.165926 139545869793024 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.3036670982837677, loss=2.0031845569610596
I0306 01:21:47.641062 139545878185728 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.2845531404018402, loss=1.894092321395874
I0306 01:22:22.144317 139545869793024 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3294967710971832, loss=2.018906831741333
I0306 01:22:56.634133 139545878185728 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.32821401953697205, loss=2.0026371479034424
I0306 01:23:31.135399 139545869793024 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.339890718460083, loss=2.0835840702056885
I0306 01:24:05.607536 139545878185728 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.31411200761795044, loss=1.9887278079986572
I0306 01:24:40.095215 139545869793024 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.3916846513748169, loss=2.0069613456726074
I0306 01:25:14.592567 139545878185728 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3534700274467468, loss=1.955234169960022
I0306 01:25:49.089303 139545869793024 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.32786890864372253, loss=2.067919969558716
I0306 01:26:23.614154 139545878185728 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2829038202762604, loss=1.9422824382781982
I0306 01:26:58.117405 139545869793024 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.30747631192207336, loss=1.994718313217163
I0306 01:27:32.634510 139545878185728 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.35585466027259827, loss=2.139164686203003
I0306 01:28:07.111431 139545869793024 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.31410568952560425, loss=1.9850221872329712
I0306 01:28:41.615773 139545878185728 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.38513657450675964, loss=2.0192067623138428
I0306 01:29:16.112153 139545869793024 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.28256407380104065, loss=2.009753704071045
I0306 01:29:50.634668 139545878185728 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3346226215362549, loss=1.9860395193099976
I0306 01:30:25.116332 139545869793024 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.5094684362411499, loss=1.9729843139648438
I0306 01:30:59.601763 139545878185728 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.3795594871044159, loss=2.0086023807525635
I0306 01:31:34.135415 139545869793024 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.28400832414627075, loss=1.9570735692977905
I0306 01:32:08.604816 139545878185728 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2600972354412079, loss=1.8972251415252686
I0306 01:32:43.081696 139545869793024 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.28519561886787415, loss=1.9345086812973022
I0306 01:33:17.607839 139545878185728 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.41078320145606995, loss=1.9986567497253418
I0306 01:33:52.117015 139545869793024 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.40066608786582947, loss=2.036989688873291
I0306 01:34:06.268921 139689492575424 spec.py:321] Evaluating on the training split.
I0306 01:34:08.867026 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 01:37:32.862866 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 01:37:35.449074 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 01:40:36.606366 139689492575424 spec.py:349] Evaluating on the test split.
I0306 01:40:39.192445 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 01:43:23.474526 139689492575424 submission_runner.py:469] Time since start: 23398.60s, 	Step: 38942, 	{'train/accuracy': 0.6108911037445068, 'train/loss': 1.9686954021453857, 'train/bleu': 29.233891513405997, 'validation/accuracy': 0.6280498504638672, 'validation/loss': 1.8299354314804077, 'validation/bleu': 25.654858096249537, 'validation/num_examples': 3000, 'test/accuracy': 0.6367744207382202, 'test/loss': 1.7877776622772217, 'test/bleu': 25.24583476216284, 'test/num_examples': 3003, 'score': 13465.159498691559, 'total_duration': 23398.603474617004, 'accumulated_submission_time': 13465.159498691559, 'accumulated_eval_time': 9931.03953909874, 'accumulated_logging_time': 0.29782938957214355}
I0306 01:43:23.486521 139545878185728 logging_writer.py:48] [38942] accumulated_eval_time=9931.04, accumulated_logging_time=0.297829, accumulated_submission_time=13465.2, global_step=38942, preemption_count=0, score=13465.2, test/accuracy=0.636774, test/bleu=25.2458, test/loss=1.78778, test/num_examples=3003, total_duration=23398.6, train/accuracy=0.610891, train/bleu=29.2339, train/loss=1.9687, validation/accuracy=0.62805, validation/bleu=25.6549, validation/loss=1.82994, validation/num_examples=3000
I0306 01:43:43.733974 139545869793024 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.34703391790390015, loss=1.949717402458191
I0306 01:44:18.089277 139545878185728 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.34773480892181396, loss=2.0500192642211914
I0306 01:44:52.585792 139545869793024 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.27219244837760925, loss=1.9317576885223389
I0306 01:45:27.077837 139545878185728 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.3417815566062927, loss=2.006431818008423
I0306 01:46:01.556252 139545869793024 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4360893666744232, loss=2.0080337524414062
I0306 01:46:36.010160 139545878185728 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.26824915409088135, loss=2.0388705730438232
I0306 01:47:10.494605 139545869793024 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.26782113313674927, loss=2.0802736282348633
I0306 01:47:44.983541 139545878185728 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.30972757935523987, loss=2.0035934448242188
I0306 01:48:19.471181 139545869793024 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.31546977162361145, loss=1.9876662492752075
I0306 01:48:53.951308 139545878185728 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.2737346589565277, loss=1.8685303926467896
I0306 01:49:28.440163 139545869793024 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.43154633045196533, loss=2.0127553939819336
I0306 01:50:02.910482 139545878185728 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.29039090871810913, loss=1.9558749198913574
I0306 01:50:37.447690 139545869793024 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.33949294686317444, loss=2.018583297729492
I0306 01:51:11.929548 139545878185728 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.30997925996780396, loss=1.9638984203338623
I0306 01:51:46.429101 139545869793024 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.3718589246273041, loss=1.935853123664856
I0306 01:52:20.949999 139545878185728 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.33228763937950134, loss=1.9590914249420166
I0306 01:52:55.445079 139545869793024 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.27278605103492737, loss=2.049226999282837
I0306 01:53:30.007430 139545878185728 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.31367161870002747, loss=2.054607629776001
I0306 01:54:04.578989 139545869793024 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.37099844217300415, loss=1.9476776123046875
I0306 01:54:39.095972 139545878185728 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3627132773399353, loss=2.066028118133545
I0306 01:55:13.637375 139545869793024 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.31098708510398865, loss=1.9091682434082031
I0306 01:55:48.154631 139545878185728 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.30001887679100037, loss=2.015476942062378
I0306 01:56:22.629333 139545869793024 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.3697466254234314, loss=1.9143290519714355
I0306 01:56:57.136754 139545878185728 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3550715744495392, loss=2.042454719543457
I0306 01:57:23.664994 139689492575424 spec.py:321] Evaluating on the training split.
I0306 01:57:26.263452 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:00:38.167731 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 02:00:40.762496 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:03:12.915091 139689492575424 spec.py:349] Evaluating on the test split.
I0306 02:03:15.512844 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:05:54.497306 139689492575424 submission_runner.py:469] Time since start: 24749.63s, 	Step: 41378, 	{'train/accuracy': 0.6143617630004883, 'train/loss': 1.975722312927246, 'train/bleu': 29.167748830514626, 'validation/accuracy': 0.6329320669174194, 'validation/loss': 1.815677523612976, 'validation/bleu': 26.27517430804815, 'validation/num_examples': 3000, 'test/accuracy': 0.641478419303894, 'test/loss': 1.7617791891098022, 'test/bleu': 25.117817134580957, 'test/num_examples': 3003, 'score': 14305.20078587532, 'total_duration': 24749.626266479492, 'accumulated_submission_time': 14305.20078587532, 'accumulated_eval_time': 10441.871820926666, 'accumulated_logging_time': 0.3177821636199951}
I0306 02:05:54.509005 139545869793024 logging_writer.py:48] [41378] accumulated_eval_time=10441.9, accumulated_logging_time=0.317782, accumulated_submission_time=14305.2, global_step=41378, preemption_count=0, score=14305.2, test/accuracy=0.641478, test/bleu=25.1178, test/loss=1.76178, test/num_examples=3003, total_duration=24749.6, train/accuracy=0.614362, train/bleu=29.1677, train/loss=1.97572, validation/accuracy=0.632932, validation/bleu=26.2752, validation/loss=1.81568, validation/num_examples=3000
I0306 02:06:02.391443 139545878185728 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.30872976779937744, loss=2.03057599067688
I0306 02:06:36.728025 139545869793024 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.33720940351486206, loss=2.008281707763672
I0306 02:07:11.190487 139545878185728 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.36944493651390076, loss=1.9858595132827759
I0306 02:07:45.652283 139545869793024 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3423643708229065, loss=1.998939037322998
I0306 02:08:20.151282 139545878185728 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.30157092213630676, loss=1.92643404006958
I0306 02:08:54.657421 139545869793024 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.318299263715744, loss=1.966372013092041
I0306 02:09:29.104829 139545878185728 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.29755884408950806, loss=1.9837709665298462
I0306 02:10:03.582694 139545869793024 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2703709006309509, loss=1.9179613590240479
I0306 02:10:38.088828 139545878185728 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3647134304046631, loss=2.030233383178711
I0306 02:11:12.595570 139545869793024 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.31375372409820557, loss=2.0186591148376465
I0306 02:11:47.053138 139545878185728 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.41164299845695496, loss=1.980886459350586
I0306 02:12:21.510780 139545869793024 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3361901044845581, loss=1.9966771602630615
I0306 02:12:55.996370 139545878185728 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.28257808089256287, loss=1.94525146484375
I0306 02:13:30.496494 139545869793024 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2978610396385193, loss=1.9322257041931152
I0306 02:14:05.016936 139545878185728 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.39663994312286377, loss=1.9581527709960938
I0306 02:14:39.513549 139545869793024 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.4417559802532196, loss=2.040539026260376
I0306 02:15:14.036376 139545878185728 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.38591694831848145, loss=1.917087435722351
I0306 02:15:48.551876 139545869793024 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.336113840341568, loss=1.954010009765625
I0306 02:16:23.051185 139545878185728 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.29006627202033997, loss=1.9584087133407593
I0306 02:16:57.563184 139545869793024 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3904348909854889, loss=2.019157886505127
I0306 02:17:32.036042 139545878185728 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3146430253982544, loss=1.9918441772460938
I0306 02:18:06.513127 139545869793024 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.34432217478752136, loss=2.0126330852508545
I0306 02:18:40.988749 139545878185728 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.30255258083343506, loss=1.915354609489441
I0306 02:19:15.462726 139545869793024 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.31761348247528076, loss=1.991222620010376
I0306 02:19:49.941215 139545878185728 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.27674251794815063, loss=1.9702811241149902
I0306 02:19:54.801181 139689492575424 spec.py:321] Evaluating on the training split.
I0306 02:19:57.413564 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:23:04.210248 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 02:23:06.806306 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:25:56.316590 139689492575424 spec.py:349] Evaluating on the test split.
I0306 02:25:58.913943 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:28:30.928272 139689492575424 submission_runner.py:469] Time since start: 26106.06s, 	Step: 43815, 	{'train/accuracy': 0.6126684546470642, 'train/loss': 1.992365837097168, 'train/bleu': 28.83128440998963, 'validation/accuracy': 0.6336365938186646, 'validation/loss': 1.8152878284454346, 'validation/bleu': 26.517155892935076, 'validation/num_examples': 3000, 'test/accuracy': 0.6423705220222473, 'test/loss': 1.7523627281188965, 'test/bleu': 25.240188157574117, 'test/num_examples': 3003, 'score': 15145.361122608185, 'total_duration': 26106.057208776474, 'accumulated_submission_time': 15145.361122608185, 'accumulated_eval_time': 10957.99885392189, 'accumulated_logging_time': 0.3374199867248535}
I0306 02:28:30.940257 139545869793024 logging_writer.py:48] [43815] accumulated_eval_time=10958, accumulated_logging_time=0.33742, accumulated_submission_time=15145.4, global_step=43815, preemption_count=0, score=15145.4, test/accuracy=0.642371, test/bleu=25.2402, test/loss=1.75236, test/num_examples=3003, total_duration=26106.1, train/accuracy=0.612668, train/bleu=28.8313, train/loss=1.99237, validation/accuracy=0.633637, validation/bleu=26.5172, validation/loss=1.81529, validation/num_examples=3000
I0306 02:29:00.457431 139545878185728 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.2839990556240082, loss=1.9725569486618042
I0306 02:29:34.793777 139545869793024 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.36252930760383606, loss=1.9347922801971436
I0306 02:30:09.185768 139545878185728 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.29153820872306824, loss=1.9220762252807617
I0306 02:30:43.575808 139545869793024 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.3302707374095917, loss=2.0305519104003906
I0306 02:31:17.977470 139545878185728 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.29834088683128357, loss=2.0557823181152344
I0306 02:31:52.434863 139545869793024 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.27317413687705994, loss=1.9724209308624268
I0306 02:32:26.864346 139545878185728 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.36416277289390564, loss=1.949931263923645
I0306 02:33:01.274535 139545869793024 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.29816052317619324, loss=2.0281755924224854
I0306 02:33:35.663635 139545878185728 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.37067711353302, loss=1.9632004499435425
I0306 02:34:10.054228 139545869793024 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.3357510566711426, loss=2.0166115760803223
I0306 02:34:44.431133 139545878185728 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.325260728597641, loss=1.9746581315994263
I0306 02:35:18.838870 139545869793024 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3148994743824005, loss=2.008328676223755
I0306 02:35:53.228975 139545878185728 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.37162667512893677, loss=2.0133872032165527
I0306 02:36:27.652860 139545869793024 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3558577597141266, loss=1.9602633714675903
I0306 02:37:02.087083 139545878185728 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.24490167200565338, loss=1.8744889497756958
I0306 02:37:36.506436 139545869793024 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.4468458890914917, loss=1.9697184562683105
I0306 02:38:10.891067 139545878185728 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2929692566394806, loss=1.917479395866394
I0306 02:38:45.277792 139545869793024 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.30258017778396606, loss=1.973032832145691
I0306 02:39:19.729632 139545878185728 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.3094162344932556, loss=1.9335720539093018
I0306 02:39:54.131285 139545869793024 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.307569682598114, loss=1.9347354173660278
I0306 02:40:28.556601 139545878185728 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.35051777958869934, loss=1.986309289932251
I0306 02:41:02.984617 139545869793024 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3381866216659546, loss=2.042145252227783
I0306 02:41:37.389421 139545878185728 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3174677789211273, loss=1.8499438762664795
I0306 02:42:11.842153 139545869793024 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.3398423492908478, loss=1.9865338802337646
I0306 02:42:31.103922 139689492575424 spec.py:321] Evaluating on the training split.
I0306 02:42:33.703506 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:46:21.943824 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 02:46:24.533343 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:49:11.843502 139689492575424 spec.py:349] Evaluating on the test split.
I0306 02:49:14.437413 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 02:51:50.801821 139689492575424 submission_runner.py:469] Time since start: 27505.93s, 	Step: 46257, 	{'train/accuracy': 0.6152775883674622, 'train/loss': 1.953343152999878, 'train/bleu': 29.2012312583271, 'validation/accuracy': 0.6349838376045227, 'validation/loss': 1.8015406131744385, 'validation/bleu': 26.20738323965694, 'validation/num_examples': 3000, 'test/accuracy': 0.6414204835891724, 'test/loss': 1.7476232051849365, 'test/bleu': 25.088318590204942, 'test/num_examples': 3003, 'score': 15985.390161037445, 'total_duration': 27505.930781126022, 'accumulated_submission_time': 15985.390161037445, 'accumulated_eval_time': 11517.6967086792, 'accumulated_logging_time': 0.35872364044189453}
I0306 02:51:50.815259 139545878185728 logging_writer.py:48] [46257] accumulated_eval_time=11517.7, accumulated_logging_time=0.358724, accumulated_submission_time=15985.4, global_step=46257, preemption_count=0, score=15985.4, test/accuracy=0.64142, test/bleu=25.0883, test/loss=1.74762, test/num_examples=3003, total_duration=27505.9, train/accuracy=0.615278, train/bleu=29.2012, train/loss=1.95334, validation/accuracy=0.634984, validation/bleu=26.2074, validation/loss=1.80154, validation/num_examples=3000
I0306 02:52:05.894244 139545869793024 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.31762295961380005, loss=2.013227939605713
I0306 02:52:40.175355 139545878185728 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3814750611782074, loss=2.0449490547180176
I0306 02:53:14.599835 139545869793024 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.32666724920272827, loss=1.9738332033157349
I0306 02:53:48.986546 139545878185728 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3162606358528137, loss=1.9653006792068481
I0306 02:54:23.396898 139545869793024 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.4199163317680359, loss=2.0383241176605225
I0306 02:54:57.801933 139545878185728 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.3367200493812561, loss=1.9931187629699707
I0306 02:55:32.212765 139545869793024 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.3109396696090698, loss=1.982648491859436
I0306 02:56:06.637509 139545878185728 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.24709056317806244, loss=1.959342360496521
I0306 02:56:41.058614 139545869793024 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.3010391592979431, loss=2.056243658065796
I0306 02:57:15.443556 139545878185728 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.3594721555709839, loss=1.9274550676345825
I0306 02:57:49.852633 139545869793024 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.36190474033355713, loss=2.0104775428771973
I0306 02:58:24.274572 139545878185728 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3403516411781311, loss=1.9108810424804688
I0306 02:58:58.682664 139545869793024 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.33608829975128174, loss=2.012770891189575
I0306 02:59:33.081619 139545878185728 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.32068002223968506, loss=1.949430227279663
I0306 03:00:07.495191 139545869793024 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.394162118434906, loss=2.010221004486084
I0306 03:00:41.919291 139545878185728 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3392077684402466, loss=1.974557638168335
I0306 03:01:16.318128 139545869793024 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3112980127334595, loss=1.89387845993042
I0306 03:01:50.712262 139545878185728 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2806524336338043, loss=1.924582839012146
I0306 03:02:25.123413 139545869793024 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.4497658312320709, loss=1.9839818477630615
I0306 03:02:59.568128 139545878185728 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3156341314315796, loss=2.014063596725464
I0306 03:03:33.965244 139545869793024 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.3571299612522125, loss=2.0190374851226807
I0306 03:04:08.412930 139545878185728 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.4034627676010132, loss=2.0246708393096924
I0306 03:04:42.793449 139545869793024 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.30877768993377686, loss=1.8707935810089111
I0306 03:05:17.205027 139545878185728 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3140903115272522, loss=1.9279292821884155
I0306 03:05:50.936420 139689492575424 spec.py:321] Evaluating on the training split.
I0306 03:05:53.534407 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 03:10:01.939765 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 03:10:04.540505 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 03:13:00.144239 139689492575424 spec.py:349] Evaluating on the test split.
I0306 03:13:02.742780 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 03:15:57.477221 139689492575424 submission_runner.py:469] Time since start: 28952.61s, 	Step: 48699, 	{'train/accuracy': 0.6168846487998962, 'train/loss': 1.94062077999115, 'train/bleu': 29.500472430735, 'validation/accuracy': 0.6354905962944031, 'validation/loss': 1.7929277420043945, 'validation/bleu': 26.349236690840748, 'validation/num_examples': 3000, 'test/accuracy': 0.6444328427314758, 'test/loss': 1.7411733865737915, 'test/bleu': 25.414078304524136, 'test/num_examples': 3003, 'score': 16825.378594636917, 'total_duration': 28952.60617995262, 'accumulated_submission_time': 16825.378594636917, 'accumulated_eval_time': 12124.237494707108, 'accumulated_logging_time': 0.37989234924316406}
I0306 03:15:57.489651 139545869793024 logging_writer.py:48] [48699] accumulated_eval_time=12124.2, accumulated_logging_time=0.379892, accumulated_submission_time=16825.4, global_step=48699, preemption_count=0, score=16825.4, test/accuracy=0.644433, test/bleu=25.4141, test/loss=1.74117, test/num_examples=3003, total_duration=28952.6, train/accuracy=0.616885, train/bleu=29.5005, train/loss=1.94062, validation/accuracy=0.635491, validation/bleu=26.3492, validation/loss=1.79293, validation/num_examples=3000
I0306 03:15:58.189910 139545878185728 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3355506360530853, loss=1.8974629640579224
I0306 03:16:32.486600 139545869793024 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.27833226323127747, loss=1.877881407737732
I0306 03:17:06.856279 139545878185728 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2861669361591339, loss=1.869348168373108
I0306 03:17:41.295509 139545869793024 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3023585081100464, loss=1.9475260972976685
I0306 03:18:15.692887 139545878185728 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3075275421142578, loss=2.0505170822143555
I0306 03:18:50.118820 139545869793024 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.32253673672676086, loss=1.9073002338409424
I0306 03:19:24.486169 139545878185728 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.29325851798057556, loss=1.856101632118225
I0306 03:19:58.913330 139545869793024 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.34823253750801086, loss=1.999245285987854
I0306 03:20:33.343527 139545878185728 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2965930998325348, loss=1.9137094020843506
I0306 03:21:07.770662 139545869793024 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.26975125074386597, loss=1.8781596422195435
I0306 03:21:42.197371 139545878185728 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3896135091781616, loss=2.0025508403778076
I0306 03:22:16.614139 139545869793024 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.35264086723327637, loss=1.9311999082565308
I0306 03:22:51.035364 139545878185728 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3146962523460388, loss=1.9156808853149414
I0306 03:23:25.473193 139545869793024 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3057548403739929, loss=1.8916995525360107
I0306 03:23:59.878303 139545878185728 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.31101980805397034, loss=1.9066004753112793
I0306 03:24:34.285239 139545869793024 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.34249523282051086, loss=1.9894793033599854
I0306 03:25:08.793561 139545878185728 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.28167545795440674, loss=1.9407597780227661
I0306 03:25:43.256886 139545869793024 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.29333406686782837, loss=1.964365839958191
I0306 03:26:17.695453 139545878185728 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.30252593755722046, loss=1.9406574964523315
I0306 03:26:52.159406 139545869793024 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.34585970640182495, loss=1.9977459907531738
I0306 03:27:26.620616 139545878185728 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.297307550907135, loss=1.9716720581054688
I0306 03:28:01.144662 139545869793024 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.34059417247772217, loss=1.9188222885131836
I0306 03:28:35.635341 139545878185728 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.299811452627182, loss=1.8980613946914673
I0306 03:29:10.133700 139545869793024 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3724159300327301, loss=1.876470685005188
I0306 03:29:44.615642 139545878185728 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.2965411841869354, loss=1.9146106243133545
I0306 03:29:57.696313 139689492575424 spec.py:321] Evaluating on the training split.
I0306 03:30:00.303647 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 03:33:25.929087 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 03:33:28.518898 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 03:37:06.353759 139689492575424 spec.py:349] Evaluating on the test split.
I0306 03:37:08.933970 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 03:40:17.468046 139689492575424 submission_runner.py:469] Time since start: 30412.60s, 	Step: 51139, 	{'train/accuracy': 0.6234533190727234, 'train/loss': 1.880999207496643, 'train/bleu': 29.79921907715996, 'validation/accuracy': 0.6393716335296631, 'validation/loss': 1.774951457977295, 'validation/bleu': 25.177391084504883, 'validation/num_examples': 3000, 'test/accuracy': 0.6492410898208618, 'test/loss': 1.7134965658187866, 'test/bleu': 25.84408722158683, 'test/num_examples': 3003, 'score': 17665.453531742096, 'total_duration': 30412.59698700905, 'accumulated_submission_time': 17665.453531742096, 'accumulated_eval_time': 12744.009162187576, 'accumulated_logging_time': 0.40030622482299805}
I0306 03:40:17.481027 139545869793024 logging_writer.py:48] [51139] accumulated_eval_time=12744, accumulated_logging_time=0.400306, accumulated_submission_time=17665.5, global_step=51139, preemption_count=0, score=17665.5, test/accuracy=0.649241, test/bleu=25.8441, test/loss=1.7135, test/num_examples=3003, total_duration=30412.6, train/accuracy=0.623453, train/bleu=29.7992, train/loss=1.881, validation/accuracy=0.639372, validation/bleu=25.1774, validation/loss=1.77495, validation/num_examples=3000
I0306 03:40:38.763144 139545878185728 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3602502644062042, loss=2.0115602016448975
I0306 03:41:13.160820 139545869793024 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.35428744554519653, loss=1.8144209384918213
I0306 03:41:47.621887 139545878185728 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.29956960678100586, loss=1.865235686302185
I0306 03:42:22.097851 139545869793024 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.35155752301216125, loss=1.9797046184539795
I0306 03:42:56.582508 139545878185728 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.2876853048801422, loss=1.9342586994171143
I0306 03:43:31.087274 139545869793024 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.325349360704422, loss=1.9642434120178223
I0306 03:44:05.577624 139545878185728 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3830745220184326, loss=1.8674683570861816
I0306 03:44:40.067804 139545869793024 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.40895330905914307, loss=1.973128080368042
I0306 03:45:14.601541 139545878185728 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.29233837127685547, loss=2.0455496311187744
I0306 03:45:49.106246 139545869793024 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.2908515930175781, loss=1.8872590065002441
I0306 03:46:23.559385 139545878185728 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3495774269104004, loss=1.8486270904541016
I0306 03:46:58.046644 139545869793024 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.377892404794693, loss=1.9609003067016602
I0306 03:47:32.556171 139545878185728 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.32238614559173584, loss=1.9047397375106812
I0306 03:48:07.045810 139545869793024 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.34033674001693726, loss=1.9780089855194092
I0306 03:48:41.531868 139545878185728 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.34186655282974243, loss=2.0225982666015625
I0306 03:49:16.008285 139545869793024 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3642910420894623, loss=1.8820652961730957
I0306 03:49:50.523405 139545878185728 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.29244136810302734, loss=1.8929760456085205
I0306 03:50:25.029627 139545869793024 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.32250097393989563, loss=1.902663230895996
I0306 03:50:59.512910 139545878185728 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.2846013307571411, loss=1.9091014862060547
I0306 03:51:34.033802 139545869793024 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.37613147497177124, loss=1.8969640731811523
I0306 03:52:08.512546 139545878185728 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.36263740062713623, loss=1.8454073667526245
I0306 03:52:43.025440 139545869793024 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.30787795782089233, loss=1.911865234375
I0306 03:53:17.566230 139545878185728 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.29905110597610474, loss=1.88596510887146
I0306 03:53:52.058365 139545869793024 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3575654923915863, loss=1.9393246173858643
I0306 03:54:17.646615 139689492575424 spec.py:321] Evaluating on the training split.
I0306 03:54:20.252291 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 03:57:36.922030 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 03:57:39.530691 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:00:33.970653 139689492575424 spec.py:349] Evaluating on the test split.
I0306 04:00:36.565040 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:03:52.655990 139689492575424 submission_runner.py:469] Time since start: 31827.78s, 	Step: 53575, 	{'train/accuracy': 0.6167008876800537, 'train/loss': 1.9402761459350586, 'train/bleu': 29.189115394423293, 'validation/accuracy': 0.6389390230178833, 'validation/loss': 1.7629574537277222, 'validation/bleu': 26.3640744780536, 'validation/num_examples': 3000, 'test/accuracy': 0.6489050984382629, 'test/loss': 1.706540584564209, 'test/bleu': 25.750829699117823, 'test/num_examples': 3003, 'score': 18505.48460483551, 'total_duration': 31827.784942388535, 'accumulated_submission_time': 18505.48460483551, 'accumulated_eval_time': 13319.018489837646, 'accumulated_logging_time': 0.4210548400878906}
I0306 04:03:52.669497 139545878185728 logging_writer.py:48] [53575] accumulated_eval_time=13319, accumulated_logging_time=0.421055, accumulated_submission_time=18505.5, global_step=53575, preemption_count=0, score=18505.5, test/accuracy=0.648905, test/bleu=25.7508, test/loss=1.70654, test/num_examples=3003, total_duration=31827.8, train/accuracy=0.616701, train/bleu=29.1891, train/loss=1.94028, validation/accuracy=0.638939, validation/bleu=26.3641, validation/loss=1.76296, validation/num_examples=3000
I0306 04:04:01.606206 139545869793024 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2564854919910431, loss=1.9247548580169678
I0306 04:04:35.962723 139545878185728 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.29072871804237366, loss=1.9191405773162842
I0306 04:05:10.424382 139545869793024 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.30068665742874146, loss=1.8409353494644165
I0306 04:05:44.933526 139545878185728 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.27407005429267883, loss=1.841244101524353
I0306 04:06:19.418189 139545869793024 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.35976850986480713, loss=1.8091758489608765
I0306 04:06:53.888454 139545878185728 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.37310969829559326, loss=1.9310935735702515
I0306 04:07:28.471191 139545869793024 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.3056832253932953, loss=1.9191958904266357
I0306 04:08:02.935129 139545878185728 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3005354702472687, loss=1.9608079195022583
I0306 04:08:37.402808 139545869793024 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.31225770711898804, loss=1.9593191146850586
I0306 04:09:11.872773 139545878185728 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3164971172809601, loss=1.887001395225525
I0306 04:09:46.362238 139545869793024 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.31178373098373413, loss=1.9688425064086914
I0306 04:10:20.879353 139545878185728 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3177046775817871, loss=1.8632150888442993
I0306 04:10:55.401474 139545869793024 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2877562642097473, loss=1.906545639038086
I0306 04:11:29.901630 139545878185728 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.28204748034477234, loss=1.909501314163208
I0306 04:12:04.393355 139545869793024 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3684351146221161, loss=1.8454391956329346
I0306 04:12:38.854087 139545878185728 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.31405383348464966, loss=1.9630438089370728
I0306 04:13:13.325486 139545869793024 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.39217278361320496, loss=1.922615647315979
I0306 04:13:47.801483 139545878185728 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.30319783091545105, loss=1.9108314514160156
I0306 04:14:22.328072 139545869793024 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3154144585132599, loss=2.020214319229126
I0306 04:14:56.802730 139545878185728 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2946012020111084, loss=1.9079079627990723
I0306 04:15:31.294638 139545869793024 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.314711332321167, loss=1.919637680053711
I0306 04:16:05.786643 139545878185728 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3122439384460449, loss=1.8871456384658813
I0306 04:16:40.259872 139545869793024 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3899765610694885, loss=1.8770685195922852
I0306 04:17:14.743382 139545878185728 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.402102530002594, loss=1.9905813932418823
I0306 04:17:49.230529 139545869793024 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.3069711923599243, loss=1.881290316581726
I0306 04:17:52.678859 139689492575424 spec.py:321] Evaluating on the training split.
I0306 04:17:55.288049 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:21:31.762765 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 04:21:34.356965 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:24:24.300946 139689492575424 spec.py:349] Evaluating on the test split.
I0306 04:24:26.897731 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:27:16.205573 139689492575424 submission_runner.py:469] Time since start: 33231.33s, 	Step: 56011, 	{'train/accuracy': 0.6215807795524597, 'train/loss': 1.9044971466064453, 'train/bleu': 29.91973913062219, 'validation/accuracy': 0.6425852179527283, 'validation/loss': 1.7428629398345947, 'validation/bleu': 26.819695018013714, 'validation/num_examples': 3000, 'test/accuracy': 0.651326596736908, 'test/loss': 1.6833521127700806, 'test/bleu': 26.01054652126946, 'test/num_examples': 3003, 'score': 19345.36077594757, 'total_duration': 33231.33450436592, 'accumulated_submission_time': 19345.36077594757, 'accumulated_eval_time': 13882.54513001442, 'accumulated_logging_time': 0.44249820709228516}
I0306 04:27:16.219365 139545878185728 logging_writer.py:48] [56011] accumulated_eval_time=13882.5, accumulated_logging_time=0.442498, accumulated_submission_time=19345.4, global_step=56011, preemption_count=0, score=19345.4, test/accuracy=0.651327, test/bleu=26.0105, test/loss=1.68335, test/num_examples=3003, total_duration=33231.3, train/accuracy=0.621581, train/bleu=29.9197, train/loss=1.9045, validation/accuracy=0.642585, validation/bleu=26.8197, validation/loss=1.74286, validation/num_examples=3000
I0306 04:27:47.140667 139545869793024 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.301093727350235, loss=1.8925281763076782
I0306 04:28:21.544056 139545878185728 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.3001053035259247, loss=1.8437724113464355
I0306 04:28:56.058303 139545869793024 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.3139302134513855, loss=1.8932533264160156
I0306 04:29:30.528877 139545878185728 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3085480332374573, loss=1.8716365098953247
I0306 04:30:05.009035 139545869793024 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3144221007823944, loss=1.9035147428512573
I0306 04:30:39.502767 139545878185728 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.3279574513435364, loss=1.9097963571548462
I0306 04:31:13.982081 139545869793024 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.32804858684539795, loss=1.881248116493225
I0306 04:31:48.468227 139545878185728 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.28853586316108704, loss=1.8602465391159058
I0306 04:32:22.945631 139545869793024 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.34507161378860474, loss=1.8967949151992798
I0306 04:32:57.452460 139545878185728 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.30048668384552, loss=1.9464691877365112
I0306 04:33:31.922025 139545869793024 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.2901676893234253, loss=1.9002758264541626
I0306 04:34:06.439096 139545878185728 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3011658489704132, loss=1.9130796194076538
I0306 04:34:40.917375 139545869793024 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.323799192905426, loss=1.9332104921340942
I0306 04:35:15.417494 139545878185728 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3077007830142975, loss=1.7470964193344116
I0306 04:35:49.947199 139545869793024 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3375142514705658, loss=1.9816792011260986
I0306 04:36:24.406132 139545878185728 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2970390319824219, loss=1.9729374647140503
I0306 04:36:58.906892 139545869793024 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.33039188385009766, loss=1.8954393863677979
I0306 04:37:33.366856 139545878185728 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.32805609703063965, loss=1.8609206676483154
I0306 04:38:07.777244 139545869793024 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.35935476422309875, loss=1.8883126974105835
I0306 04:38:42.257620 139545878185728 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.34292280673980713, loss=1.8194879293441772
I0306 04:39:16.709154 139545869793024 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.2857123017311096, loss=1.9636924266815186
I0306 04:39:51.202538 139545878185728 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.25033581256866455, loss=1.9291821718215942
I0306 04:40:25.702507 139545869793024 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3532647490501404, loss=1.828072190284729
I0306 04:41:00.198101 139545878185728 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2782476544380188, loss=1.8908673524856567
I0306 04:41:16.408245 139689492575424 spec.py:321] Evaluating on the training split.
I0306 04:41:19.006712 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:45:50.282819 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 04:45:52.886841 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:49:14.697647 139689492575424 spec.py:349] Evaluating on the test split.
I0306 04:49:17.293358 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 04:52:29.047684 139689492575424 submission_runner.py:469] Time since start: 34744.18s, 	Step: 58448, 	{'train/accuracy': 0.6268053650856018, 'train/loss': 1.8684265613555908, 'train/bleu': 29.76866370597327, 'validation/accuracy': 0.6423874497413635, 'validation/loss': 1.7399617433547974, 'validation/bleu': 26.59567368946308, 'validation/num_examples': 3000, 'test/accuracy': 0.6539566516876221, 'test/loss': 1.6817491054534912, 'test/bleu': 26.413865931418783, 'test/num_examples': 3003, 'score': 20185.41586971283, 'total_duration': 34744.17664408684, 'accumulated_submission_time': 20185.41586971283, 'accumulated_eval_time': 14555.184524774551, 'accumulated_logging_time': 0.4643685817718506}
I0306 04:52:29.061563 139545869793024 logging_writer.py:48] [58448] accumulated_eval_time=14555.2, accumulated_logging_time=0.464369, accumulated_submission_time=20185.4, global_step=58448, preemption_count=0, score=20185.4, test/accuracy=0.653957, test/bleu=26.4139, test/loss=1.68175, test/num_examples=3003, total_duration=34744.2, train/accuracy=0.626805, train/bleu=29.7687, train/loss=1.86843, validation/accuracy=0.642387, validation/bleu=26.5957, validation/loss=1.73996, validation/num_examples=3000
I0306 04:52:47.270866 139545878185728 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.30077609419822693, loss=1.8588498830795288
I0306 04:53:21.740559 139545869793024 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.315792977809906, loss=1.8468434810638428
I0306 04:53:56.181638 139545878185728 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.31429874897003174, loss=1.9213007688522339
I0306 04:54:30.654243 139545869793024 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3200061023235321, loss=1.863013505935669
I0306 04:55:05.089405 139545878185728 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.29978296160697937, loss=1.977895975112915
I0306 04:55:39.552997 139545869793024 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3334328532218933, loss=1.8373668193817139
I0306 04:56:14.053774 139545878185728 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.29323044419288635, loss=1.9104173183441162
I0306 04:56:48.581240 139545869793024 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.31909289956092834, loss=1.933321475982666
I0306 04:57:23.065598 139545878185728 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.32409462332725525, loss=1.8616746664047241
I0306 04:57:57.549062 139545869793024 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2648353576660156, loss=1.8018420934677124
I0306 04:58:32.009804 139545878185728 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2868542969226837, loss=1.9238910675048828
I0306 04:59:06.468286 139545869793024 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3612480163574219, loss=1.974705457687378
I0306 04:59:40.963242 139545878185728 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.31206947565078735, loss=1.9755948781967163
I0306 05:00:15.461385 139545869793024 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.3211982846260071, loss=1.9120289087295532
I0306 05:00:49.954276 139545878185728 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3353211581707001, loss=1.913145661354065
I0306 05:01:24.439860 139545869793024 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.29140999913215637, loss=1.9174118041992188
I0306 05:01:58.904912 139545878185728 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.34074223041534424, loss=1.8782144784927368
I0306 05:02:33.429127 139545869793024 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3186561167240143, loss=1.9137051105499268
I0306 05:03:07.930774 139545878185728 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.3122187554836273, loss=1.9889004230499268
I0306 05:03:42.420258 139545869793024 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.316179096698761, loss=1.8512485027313232
I0306 05:04:16.869956 139545878185728 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2851245105266571, loss=1.7786694765090942
I0306 05:04:51.351013 139545869793024 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.34176158905029297, loss=1.872891902923584
I0306 05:05:25.825823 139545878185728 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.28919970989227295, loss=1.9284276962280273
I0306 05:06:00.292352 139545869793024 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2837335765361786, loss=1.8362549543380737
I0306 05:06:29.244680 139689492575424 spec.py:321] Evaluating on the training split.
I0306 05:06:31.848175 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:11:02.693634 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 05:11:05.281294 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:14:08.786240 139689492575424 spec.py:349] Evaluating on the test split.
I0306 05:14:11.373263 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:17:08.789466 139689492575424 submission_runner.py:469] Time since start: 36223.92s, 	Step: 60885, 	{'train/accuracy': 0.6256636381149292, 'train/loss': 1.8835506439208984, 'train/bleu': 29.694588496218937, 'validation/accuracy': 0.6439695358276367, 'validation/loss': 1.7309099435806274, 'validation/bleu': 26.745883438675794, 'validation/num_examples': 3000, 'test/accuracy': 0.6533194184303284, 'test/loss': 1.6685322523117065, 'test/bleu': 25.981176771521977, 'test/num_examples': 3003, 'score': 21025.46703028679, 'total_duration': 36223.918417692184, 'accumulated_submission_time': 21025.46703028679, 'accumulated_eval_time': 15194.729263067245, 'accumulated_logging_time': 0.48595476150512695}
I0306 05:17:08.803894 139545878185728 logging_writer.py:48] [60885] accumulated_eval_time=15194.7, accumulated_logging_time=0.485955, accumulated_submission_time=21025.5, global_step=60885, preemption_count=0, score=21025.5, test/accuracy=0.653319, test/bleu=25.9812, test/loss=1.66853, test/num_examples=3003, total_duration=36223.9, train/accuracy=0.625664, train/bleu=29.6946, train/loss=1.88355, validation/accuracy=0.64397, validation/bleu=26.7459, validation/loss=1.73091, validation/num_examples=3000
I0306 05:17:14.330659 139545869793024 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.30091550946235657, loss=1.8879704475402832
I0306 05:17:48.707736 139545878185728 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.27722591161727905, loss=1.9283502101898193
I0306 05:18:23.181539 139545869793024 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.32490280270576477, loss=1.908718228340149
I0306 05:18:57.648532 139545878185728 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.2771502137184143, loss=1.7987710237503052
I0306 05:19:32.093461 139545869793024 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2735234797000885, loss=1.855356216430664
I0306 05:20:06.550383 139545878185728 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.2911728024482727, loss=1.8971214294433594
I0306 05:20:41.032753 139545869793024 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.31039273738861084, loss=1.9779388904571533
I0306 05:21:15.540956 139545878185728 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2760855257511139, loss=1.8751130104064941
I0306 05:21:50.029186 139545869793024 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.31997376680374146, loss=1.9298325777053833
I0306 05:22:24.537396 139545878185728 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.28444230556488037, loss=1.832814335823059
I0306 05:22:59.046495 139545869793024 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.36170557141304016, loss=1.9104950428009033
I0306 05:23:33.546478 139545878185728 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.41822704672813416, loss=1.8456577062606812
I0306 05:24:08.054675 139545869793024 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.2808646559715271, loss=1.8968074321746826
I0306 05:24:42.523550 139545878185728 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.3120565712451935, loss=1.8733233213424683
I0306 05:25:17.017272 139545869793024 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.306334912776947, loss=1.8471063375473022
I0306 05:25:51.477807 139545878185728 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3186613619327545, loss=1.8149125576019287
I0306 05:26:25.967281 139545869793024 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.28910765051841736, loss=1.9141106605529785
I0306 05:27:00.480897 139545878185728 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.4014439582824707, loss=1.8779295682907104
I0306 05:27:34.933758 139545869793024 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3517841100692749, loss=1.8067898750305176
I0306 05:28:09.454603 139545878185728 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.28837382793426514, loss=1.9149656295776367
I0306 05:28:43.966572 139545869793024 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3115307688713074, loss=1.8531802892684937
I0306 05:29:18.426324 139545878185728 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.30688124895095825, loss=1.8234214782714844
I0306 05:29:52.888762 139545869793024 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.3104212284088135, loss=1.9627234935760498
I0306 05:30:27.352213 139545878185728 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2979523539543152, loss=1.819156527519226
I0306 05:31:01.840137 139545869793024 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.33959293365478516, loss=1.904439926147461
I0306 05:31:09.074114 139689492575424 spec.py:321] Evaluating on the training split.
I0306 05:31:11.678434 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:34:04.692780 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 05:34:07.292233 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:36:59.171180 139689492575424 spec.py:349] Evaluating on the test split.
I0306 05:37:01.765016 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:39:44.878851 139689492575424 submission_runner.py:469] Time since start: 37580.01s, 	Step: 63322, 	{'train/accuracy': 0.6351779699325562, 'train/loss': 1.7946828603744507, 'train/bleu': 30.574386314501613, 'validation/accuracy': 0.6473685503005981, 'validation/loss': 1.7055493593215942, 'validation/bleu': 26.95875875576747, 'validation/num_examples': 3000, 'test/accuracy': 0.6573282480239868, 'test/loss': 1.6436625719070435, 'test/bleu': 26.007848553564564, 'test/num_examples': 3003, 'score': 21865.603870153427, 'total_duration': 37580.00780367851, 'accumulated_submission_time': 21865.603870153427, 'accumulated_eval_time': 15710.533950567245, 'accumulated_logging_time': 0.5083463191986084}
I0306 05:39:44.892666 139545878185728 logging_writer.py:48] [63322] accumulated_eval_time=15710.5, accumulated_logging_time=0.508346, accumulated_submission_time=21865.6, global_step=63322, preemption_count=0, score=21865.6, test/accuracy=0.657328, test/bleu=26.0078, test/loss=1.64366, test/num_examples=3003, total_duration=37580, train/accuracy=0.635178, train/bleu=30.5744, train/loss=1.79468, validation/accuracy=0.647369, validation/bleu=26.9588, validation/loss=1.70555, validation/num_examples=3000
I0306 05:40:12.008985 139545869793024 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.30288583040237427, loss=1.8513906002044678
I0306 05:40:46.404410 139545878185728 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2927168905735016, loss=1.8226550817489624
I0306 05:41:20.869370 139545869793024 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.31499621272087097, loss=1.9404716491699219
I0306 05:41:55.308367 139545878185728 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.3136593997478485, loss=1.9072206020355225
I0306 05:42:29.791838 139545869793024 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.30580925941467285, loss=1.847485065460205
I0306 05:43:04.256646 139545878185728 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.31296640634536743, loss=1.7784959077835083
I0306 05:43:38.745755 139545869793024 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3070710599422455, loss=1.9314438104629517
I0306 05:44:13.228559 139545878185728 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3284524381160736, loss=1.8519585132598877
I0306 05:44:47.682107 139545869793024 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.27723902463912964, loss=1.791364073753357
I0306 05:45:22.158444 139545878185728 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.308829665184021, loss=1.7878531217575073
I0306 05:45:56.663880 139545869793024 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.335398405790329, loss=1.8720142841339111
I0306 05:46:31.143648 139545878185728 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.31603315472602844, loss=1.9206361770629883
I0306 05:47:05.600938 139545869793024 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3486008644104004, loss=1.7780253887176514
I0306 05:47:40.058725 139545878185728 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.32512158155441284, loss=1.8341654539108276
I0306 05:48:14.579134 139545869793024 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.27444979548454285, loss=1.8155548572540283
I0306 05:48:49.077923 139545878185728 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3194397985935211, loss=1.913102149963379
I0306 05:49:23.576021 139545869793024 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.29723531007766724, loss=1.8428739309310913
I0306 05:49:58.038259 139545878185728 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3686612844467163, loss=1.8913934230804443
I0306 05:50:32.588140 139545869793024 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.33198854327201843, loss=1.8762699365615845
I0306 05:51:07.030812 139545878185728 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2572536766529083, loss=1.817624807357788
I0306 05:51:41.492177 139545869793024 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.32961344718933105, loss=1.9078251123428345
I0306 05:52:15.980407 139545878185728 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.35735589265823364, loss=1.7996824979782104
I0306 05:52:50.476996 139545869793024 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.29000595211982727, loss=1.794906497001648
I0306 05:53:24.942778 139545878185728 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.3651839792728424, loss=1.906215786933899
I0306 05:53:44.961230 139689492575424 spec.py:321] Evaluating on the training split.
I0306 05:53:47.565998 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:56:58.634926 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 05:57:01.217574 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 05:59:51.767680 139689492575424 spec.py:349] Evaluating on the test split.
I0306 05:59:54.370723 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 06:02:53.893407 139689492575424 submission_runner.py:469] Time since start: 38969.02s, 	Step: 65759, 	{'train/accuracy': 0.628807008266449, 'train/loss': 1.8529696464538574, 'train/bleu': 30.53733152056961, 'validation/accuracy': 0.6511260271072388, 'validation/loss': 1.6819132566452026, 'validation/bleu': 27.34380213844386, 'validation/num_examples': 3000, 'test/accuracy': 0.6610126495361328, 'test/loss': 1.6236516237258911, 'test/bleu': 26.984343736185732, 'test/num_examples': 3003, 'score': 22705.53987812996, 'total_duration': 38969.022357702255, 'accumulated_submission_time': 22705.53987812996, 'accumulated_eval_time': 16259.46607542038, 'accumulated_logging_time': 0.5316293239593506}
I0306 06:02:53.908272 139545869793024 logging_writer.py:48] [65759] accumulated_eval_time=16259.5, accumulated_logging_time=0.531629, accumulated_submission_time=22705.5, global_step=65759, preemption_count=0, score=22705.5, test/accuracy=0.661013, test/bleu=26.9843, test/loss=1.62365, test/num_examples=3003, total_duration=38969, train/accuracy=0.628807, train/bleu=30.5373, train/loss=1.85297, validation/accuracy=0.651126, validation/bleu=27.3438, validation/loss=1.68191, validation/num_examples=3000
I0306 06:03:08.327243 139545878185728 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.310291588306427, loss=1.8397092819213867
I0306 06:03:42.670063 139545869793024 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2979048788547516, loss=1.8754900693893433
I0306 06:04:17.146779 139545878185728 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.35003751516342163, loss=1.8575078248977661
I0306 06:04:51.622208 139545869793024 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3684057295322418, loss=1.9039331674575806
I0306 06:05:26.079305 139545878185728 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3321593105792999, loss=1.9119539260864258
I0306 06:06:00.561551 139545869793024 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.33700647950172424, loss=1.8549998998641968
I0306 06:06:35.047244 139545878185728 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3187553584575653, loss=1.7577470541000366
I0306 06:07:09.535449 139545869793024 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.30508553981781006, loss=1.8651331663131714
I0306 06:07:44.032701 139545878185728 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.3272649645805359, loss=1.7896586656570435
I0306 06:08:18.542172 139545869793024 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3182092010974884, loss=1.8552170991897583
I0306 06:08:53.036042 139545878185728 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2888440787792206, loss=1.7768393754959106
I0306 06:09:27.562868 139545869793024 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.31950685381889343, loss=1.7838976383209229
I0306 06:10:02.061167 139545878185728 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3322010338306427, loss=1.9236963987350464
I0306 06:10:36.552539 139545869793024 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.34169378876686096, loss=1.8223694562911987
I0306 06:11:11.036628 139545878185728 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.318293958902359, loss=1.8656034469604492
I0306 06:11:45.564572 139545869793024 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.2807958126068115, loss=1.7903310060501099
I0306 06:12:20.049581 139545878185728 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.29089608788490295, loss=1.8492599725723267
I0306 06:12:54.508938 139545869793024 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3511505126953125, loss=1.8037090301513672
I0306 06:13:28.999275 139545878185728 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.27077919244766235, loss=1.8241580724716187
I0306 06:14:03.538306 139545869793024 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.31350472569465637, loss=1.784904956817627
I0306 06:14:38.066360 139545878185728 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.34617432951927185, loss=1.7775382995605469
I0306 06:15:12.551629 139545869793024 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.3040904402732849, loss=1.8641172647476196
I0306 06:15:47.021874 139545878185728 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.29426348209381104, loss=1.8804649114608765
I0306 06:16:21.509638 139545869793024 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3333364725112915, loss=1.8414160013198853
I0306 06:16:53.926984 139689492575424 spec.py:321] Evaluating on the training split.
I0306 06:16:56.535639 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 06:20:30.724235 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 06:20:33.313584 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 06:23:06.022198 139689492575424 spec.py:349] Evaluating on the test split.
I0306 06:23:08.617813 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 06:25:52.961414 139689492575424 submission_runner.py:469] Time since start: 40348.09s, 	Step: 68195, 	{'train/accuracy': 0.6305742263793945, 'train/loss': 1.8467528820037842, 'train/bleu': 30.146465612794493, 'validation/accuracy': 0.6538823246955872, 'validation/loss': 1.6715095043182373, 'validation/bleu': 27.59234478557179, 'validation/num_examples': 3000, 'test/accuracy': 0.6604912281036377, 'test/loss': 1.6091076135635376, 'test/bleu': 26.510615615856825, 'test/num_examples': 3003, 'score': 23545.424659967422, 'total_duration': 40348.090361595154, 'accumulated_submission_time': 23545.424659967422, 'accumulated_eval_time': 16798.50046467781, 'accumulated_logging_time': 0.5545899868011475}
I0306 06:25:52.975909 139545878185728 logging_writer.py:48] [68195] accumulated_eval_time=16798.5, accumulated_logging_time=0.55459, accumulated_submission_time=23545.4, global_step=68195, preemption_count=0, score=23545.4, test/accuracy=0.660491, test/bleu=26.5106, test/loss=1.60911, test/num_examples=3003, total_duration=40348.1, train/accuracy=0.630574, train/bleu=30.1465, train/loss=1.84675, validation/accuracy=0.653882, validation/bleu=27.5923, validation/loss=1.67151, validation/num_examples=3000
I0306 06:25:55.057141 139545869793024 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.3226904273033142, loss=1.7128055095672607
I0306 06:26:29.367169 139545878185728 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3388483226299286, loss=1.8334262371063232
I0306 06:27:03.763897 139545869793024 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3174607455730438, loss=1.8864367008209229
I0306 06:27:38.231513 139545878185728 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.30094972252845764, loss=1.8348746299743652
I0306 06:28:12.702684 139545869793024 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.30287081003189087, loss=1.8891222476959229
I0306 06:28:47.202392 139545878185728 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.3803345859050751, loss=1.831511378288269
I0306 06:29:21.667962 139545869793024 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2931888699531555, loss=1.8553783893585205
I0306 06:29:56.173919 139545878185728 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.2793326675891876, loss=1.900757908821106
I0306 06:30:30.609972 139545869793024 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.36588647961616516, loss=1.850945234298706
I0306 06:31:05.057784 139545878185728 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3667566478252411, loss=1.84286630153656
I0306 06:31:39.520484 139545869793024 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.30492737889289856, loss=1.7426332235336304
I0306 06:32:13.978508 139545878185728 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.35521748661994934, loss=1.7771289348602295
I0306 06:32:48.486085 139545869793024 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2926015853881836, loss=1.7484997510910034
I0306 06:33:22.948514 139545878185728 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.3342859745025635, loss=1.7925139665603638
I0306 06:33:57.375200 139545869793024 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.33520305156707764, loss=1.7462290525436401
I0306 06:34:31.834119 139545878185728 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2819126546382904, loss=1.7685885429382324
I0306 06:35:06.280943 139545869793024 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.34812110662460327, loss=1.7971569299697876
I0306 06:35:40.715880 139545878185728 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.33044862747192383, loss=1.8525174856185913
I0306 06:36:15.187576 139545869793024 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3086155951023102, loss=1.8763257265090942
I0306 06:36:49.644495 139545878185728 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.33043143153190613, loss=1.8783732652664185
I0306 06:37:24.060683 139545869793024 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2984807789325714, loss=1.7902629375457764
I0306 06:37:58.530749 139545878185728 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3039930760860443, loss=1.831688642501831
I0306 06:38:32.998533 139545869793024 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.32566288113594055, loss=1.837408423423767
I0306 06:39:07.437080 139545878185728 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.29980117082595825, loss=1.851956844329834
I0306 06:39:41.892197 139545869793024 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.28039655089378357, loss=1.8105155229568481
I0306 06:39:53.265232 139689492575424 spec.py:321] Evaluating on the training split.
I0306 06:39:55.874472 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 06:43:05.018836 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 06:43:07.613013 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 06:45:56.277314 139689492575424 spec.py:349] Evaluating on the test split.
I0306 06:45:58.865211 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 06:48:40.377259 139689492575424 submission_runner.py:469] Time since start: 41715.51s, 	Step: 70634, 	{'train/accuracy': 0.6382700204849243, 'train/loss': 1.790703296661377, 'train/bleu': 31.230143414995467, 'validation/accuracy': 0.6538575887680054, 'validation/loss': 1.6620250940322876, 'validation/bleu': 27.728363470018664, 'validation/num_examples': 3000, 'test/accuracy': 0.664940357208252, 'test/loss': 1.5942641496658325, 'test/bleu': 26.830956998584735, 'test/num_examples': 3003, 'score': 24385.58303141594, 'total_duration': 41715.506217479706, 'accumulated_submission_time': 24385.58303141594, 'accumulated_eval_time': 17325.61244392395, 'accumulated_logging_time': 0.5768651962280273}
I0306 06:48:40.392613 139545878185728 logging_writer.py:48] [70634] accumulated_eval_time=17325.6, accumulated_logging_time=0.576865, accumulated_submission_time=24385.6, global_step=70634, preemption_count=0, score=24385.6, test/accuracy=0.66494, test/bleu=26.831, test/loss=1.59426, test/num_examples=3003, total_duration=41715.5, train/accuracy=0.63827, train/bleu=31.2301, train/loss=1.7907, validation/accuracy=0.653858, validation/bleu=27.7284, validation/loss=1.66203, validation/num_examples=3000
I0306 06:49:03.359441 139545869793024 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3022196292877197, loss=1.7945327758789062
I0306 06:49:37.717226 139545878185728 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.31296294927597046, loss=1.8119356632232666
I0306 06:50:12.150114 139545869793024 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3059101998806, loss=1.8754578828811646
I0306 06:50:46.555666 139545878185728 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.27763381600379944, loss=1.8217196464538574
I0306 06:51:21.006985 139545869793024 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2965046167373657, loss=1.8268823623657227
I0306 06:51:55.445721 139545878185728 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.30090248584747314, loss=1.8240156173706055
I0306 06:52:29.877924 139545869793024 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.2870078384876251, loss=1.7799162864685059
I0306 06:53:04.328115 139545878185728 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3268204927444458, loss=1.80592942237854
I0306 06:53:38.798993 139545869793024 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3340378999710083, loss=1.866023063659668
I0306 06:54:13.245794 139545878185728 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.37353265285491943, loss=1.849875807762146
I0306 06:54:47.713572 139545869793024 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.31119826436042786, loss=1.846544623374939
I0306 06:55:22.189195 139545878185728 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.30930593609809875, loss=1.854689359664917
I0306 06:55:56.658929 139545869793024 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3173023760318756, loss=1.816487431526184
I0306 06:56:31.114128 139545878185728 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.30893266201019287, loss=1.894173502922058
I0306 06:57:05.575324 139545869793024 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.30837109684944153, loss=1.8449139595031738
I0306 06:57:40.055034 139545878185728 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.29297366738319397, loss=1.7522540092468262
I0306 06:58:14.497848 139545869793024 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.2791103422641754, loss=1.8067785501480103
I0306 06:58:49.142957 139545878185728 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.2907724380493164, loss=1.853683352470398
I0306 06:59:23.669949 139545869793024 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.32174918055534363, loss=1.863744854927063
I0306 06:59:58.250299 139545878185728 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.31424492597579956, loss=1.8007876873016357
I0306 07:00:32.828802 139545869793024 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.2593736946582794, loss=1.7590599060058594
I0306 07:01:07.367794 139545878185728 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.28617313504219055, loss=1.7764970064163208
I0306 07:01:41.933967 139545869793024 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3433306813240051, loss=1.8368926048278809
I0306 07:02:16.484472 139545878185728 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.35482293367385864, loss=1.7737478017807007
I0306 07:02:40.688332 139689492575424 spec.py:321] Evaluating on the training split.
I0306 07:02:43.298560 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 07:05:59.311802 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 07:06:01.922680 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 07:09:03.287977 139689492575424 spec.py:349] Evaluating on the test split.
I0306 07:09:05.899523 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 07:11:56.619082 139689492575424 submission_runner.py:469] Time since start: 43111.75s, 	Step: 73071, 	{'train/accuracy': 0.6343984007835388, 'train/loss': 1.8140376806259155, 'train/bleu': 31.09243345657882, 'validation/accuracy': 0.654166579246521, 'validation/loss': 1.653039574623108, 'validation/bleu': 27.596476200526222, 'validation/num_examples': 3000, 'test/accuracy': 0.6644073724746704, 'test/loss': 1.5911884307861328, 'test/bleu': 27.248072505544634, 'test/num_examples': 3003, 'score': 25225.748639583588, 'total_duration': 43111.74801707268, 'accumulated_submission_time': 25225.748639583588, 'accumulated_eval_time': 17881.54312467575, 'accumulated_logging_time': 0.5998735427856445}
I0306 07:11:56.638501 139545869793024 logging_writer.py:48] [73071] accumulated_eval_time=17881.5, accumulated_logging_time=0.599874, accumulated_submission_time=25225.7, global_step=73071, preemption_count=0, score=25225.7, test/accuracy=0.664407, test/bleu=27.2481, test/loss=1.59119, test/num_examples=3003, total_duration=43111.7, train/accuracy=0.634398, train/bleu=31.0924, train/loss=1.81404, validation/accuracy=0.654167, validation/bleu=27.5965, validation/loss=1.65304, validation/num_examples=3000
I0306 07:12:06.960747 139545878185728 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3027713894844055, loss=1.7939196825027466
I0306 07:12:41.402555 139545869793024 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.41304752230644226, loss=1.7940698862075806
I0306 07:13:15.982675 139545878185728 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.318783700466156, loss=1.8420538902282715
I0306 07:13:50.551362 139545869793024 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.30213984847068787, loss=1.7636890411376953
I0306 07:14:25.142677 139545878185728 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.34344467520713806, loss=1.7962373495101929
I0306 07:14:59.685499 139545869793024 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.3238483667373657, loss=1.7661556005477905
I0306 07:15:34.250219 139545878185728 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3299042284488678, loss=1.7268908023834229
I0306 07:16:08.815729 139545869793024 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3951716125011444, loss=1.823667049407959
I0306 07:16:43.395270 139545878185728 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3484141528606415, loss=1.8158866167068481
I0306 07:17:17.949297 139545869793024 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.32420048117637634, loss=1.7479116916656494
I0306 07:17:52.505880 139545878185728 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.32211023569107056, loss=1.838289737701416
I0306 07:18:27.024079 139545869793024 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.33432140946388245, loss=1.816166639328003
I0306 07:19:01.573627 139545878185728 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.32207414507865906, loss=1.7768218517303467
I0306 07:19:36.130643 139545869793024 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3128475248813629, loss=1.7667584419250488
I0306 07:20:10.673131 139545878185728 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3091807961463928, loss=1.8272455930709839
I0306 07:20:45.179301 139545869793024 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.3246881663799286, loss=1.8487917184829712
I0306 07:21:19.740150 139545878185728 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3217606842517853, loss=1.7329440116882324
I0306 07:21:54.252291 139545869793024 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.29426032304763794, loss=1.7603474855422974
I0306 07:22:28.809369 139545878185728 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3066548705101013, loss=1.8645118474960327
I0306 07:23:03.355942 139545869793024 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2863672077655792, loss=1.742842674255371
I0306 07:23:37.954417 139545878185728 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2819165885448456, loss=1.7892433404922485
I0306 07:24:12.474152 139545869793024 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3154568374156952, loss=1.7735093832015991
I0306 07:24:47.034692 139545878185728 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.3272036910057068, loss=1.824592113494873
I0306 07:25:21.682788 139545869793024 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.2909906208515167, loss=1.70909583568573
I0306 07:25:56.277867 139545878185728 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3303905427455902, loss=1.715842366218567
I0306 07:25:56.630913 139689492575424 spec.py:321] Evaluating on the training split.
I0306 07:25:59.246572 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 07:30:46.637206 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 07:30:49.234957 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 07:35:42.510560 139689492575424 spec.py:349] Evaluating on the test split.
I0306 07:35:45.110702 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 07:40:16.438459 139689492575424 submission_runner.py:469] Time since start: 44811.57s, 	Step: 75502, 	{'train/accuracy': 0.6624112129211426, 'train/loss': 1.6078808307647705, 'train/bleu': 32.12475020363412, 'validation/accuracy': 0.6585914492607117, 'validation/loss': 1.6327052116394043, 'validation/bleu': 23.73419533134501, 'validation/num_examples': 3000, 'test/accuracy': 0.6695632338523865, 'test/loss': 1.5611717700958252, 'test/bleu': 25.9650246937535, 'test/num_examples': 3003, 'score': 26065.610825777054, 'total_duration': 44811.56740927696, 'accumulated_submission_time': 26065.610825777054, 'accumulated_eval_time': 18741.350626707077, 'accumulated_logging_time': 0.6278650760650635}
I0306 07:40:16.454823 139545869793024 logging_writer.py:48] [75502] accumulated_eval_time=18741.4, accumulated_logging_time=0.627865, accumulated_submission_time=26065.6, global_step=75502, preemption_count=0, score=26065.6, test/accuracy=0.669563, test/bleu=25.965, test/loss=1.56117, test/num_examples=3003, total_duration=44811.6, train/accuracy=0.662411, train/bleu=32.1248, train/loss=1.60788, validation/accuracy=0.658591, validation/bleu=23.7342, validation/loss=1.63271, validation/num_examples=3000
I0306 07:40:50.562459 139545878185728 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.28113967180252075, loss=1.7850608825683594
I0306 07:41:25.162096 139545869793024 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.29710203409194946, loss=1.7753571271896362
I0306 07:41:59.760260 139545878185728 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3119196891784668, loss=1.7648773193359375
I0306 07:42:34.368983 139545869793024 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3394275903701782, loss=1.8195207118988037
I0306 07:43:08.982152 139545878185728 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.33116135001182556, loss=1.8682981729507446
I0306 07:43:43.542282 139545869793024 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.3059045076370239, loss=1.7679345607757568
I0306 07:44:18.146650 139545878185728 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.3194960653781891, loss=1.7090731859207153
I0306 07:44:52.766347 139545869793024 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.29430991411209106, loss=1.7425493001937866
I0306 07:45:27.342819 139545878185728 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3251769542694092, loss=1.8433977365493774
I0306 07:46:01.959987 139545869793024 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.27441713213920593, loss=1.783605694770813
I0306 07:46:36.562305 139545878185728 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3260670006275177, loss=1.847408652305603
I0306 07:47:11.183177 139545869793024 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3132013976573944, loss=1.7690049409866333
I0306 07:47:45.786437 139545878185728 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3651476502418518, loss=1.8117437362670898
I0306 07:48:20.412211 139545869793024 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.3349961042404175, loss=1.7790040969848633
I0306 07:48:55.053159 139545878185728 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.2786206007003784, loss=1.7129286527633667
I0306 07:49:29.657418 139545869793024 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.31831684708595276, loss=1.7647054195404053
I0306 07:50:04.264873 139545878185728 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.3124707043170929, loss=1.8120344877243042
I0306 07:50:38.844115 139545869793024 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.2919309735298157, loss=1.7147709131240845
I0306 07:51:13.407905 139545878185728 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.3188217878341675, loss=1.7996046543121338
I0306 07:51:48.000861 139545869793024 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.30042359232902527, loss=1.7474744319915771
I0306 07:52:22.594957 139545878185728 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.30069589614868164, loss=1.7867296934127808
I0306 07:52:57.209671 139545869793024 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3140924870967865, loss=1.8107702732086182
I0306 07:53:31.842704 139545878185728 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.34484100341796875, loss=1.746589183807373
I0306 07:54:06.428307 139545869793024 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.29786163568496704, loss=1.7096099853515625
I0306 07:54:16.466160 139689492575424 spec.py:321] Evaluating on the training split.
I0306 07:54:19.071730 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 07:59:02.907307 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 07:59:05.499680 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:02:28.666276 139689492575424 spec.py:349] Evaluating on the test split.
I0306 08:02:31.267293 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:05:42.414455 139689492575424 submission_runner.py:469] Time since start: 46337.54s, 	Step: 77930, 	{'train/accuracy': 0.6410998702049255, 'train/loss': 1.763219952583313, 'train/bleu': 31.804001039784108, 'validation/accuracy': 0.6607297658920288, 'validation/loss': 1.6128979921340942, 'validation/bleu': 28.17692700766994, 'validation/num_examples': 3000, 'test/accuracy': 0.673154890537262, 'test/loss': 1.5473706722259521, 'test/bleu': 28.02593175185072, 'test/num_examples': 3003, 'score': 26905.488362550735, 'total_duration': 46337.54341173172, 'accumulated_submission_time': 26905.488362550735, 'accumulated_eval_time': 19427.298873901367, 'accumulated_logging_time': 0.652367115020752}
I0306 08:05:42.431543 139545878185728 logging_writer.py:48] [77930] accumulated_eval_time=19427.3, accumulated_logging_time=0.652367, accumulated_submission_time=26905.5, global_step=77930, preemption_count=0, score=26905.5, test/accuracy=0.673155, test/bleu=28.0259, test/loss=1.54737, test/num_examples=3003, total_duration=46337.5, train/accuracy=0.6411, train/bleu=31.804, train/loss=1.76322, validation/accuracy=0.66073, validation/bleu=28.1769, validation/loss=1.6129, validation/num_examples=3000
I0306 08:06:06.907147 139545869793024 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.30936574935913086, loss=1.7305259704589844
I0306 08:06:41.395833 139545878185728 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.31269755959510803, loss=1.6992406845092773
I0306 08:07:16.001380 139545869793024 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.34355786442756653, loss=1.7401584386825562
I0306 08:07:50.559346 139545878185728 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.2924678921699524, loss=1.787695288658142
I0306 08:08:25.180598 139545869793024 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.35026857256889343, loss=1.717485785484314
I0306 08:08:59.742053 139545878185728 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3204418420791626, loss=1.7491660118103027
I0306 08:09:34.315130 139545869793024 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3289385735988617, loss=1.7343299388885498
I0306 08:10:08.912661 139545878185728 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.31095612049102783, loss=1.7362096309661865
I0306 08:10:43.538587 139545869793024 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3274458646774292, loss=1.658679485321045
I0306 08:11:18.164716 139545878185728 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.33244696259498596, loss=1.73514986038208
I0306 08:11:52.819343 139545869793024 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.29178372025489807, loss=1.747437596321106
I0306 08:12:27.419514 139545878185728 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.3240091800689697, loss=1.7000420093536377
I0306 08:13:02.075302 139545869793024 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.3562472462654114, loss=1.754305362701416
I0306 08:13:36.707132 139545878185728 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.3372645378112793, loss=1.8002322912216187
I0306 08:14:11.276445 139545869793024 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.31630685925483704, loss=1.7387646436691284
I0306 08:14:45.888028 139545878185728 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3118317723274231, loss=1.7851130962371826
I0306 08:15:20.474174 139545869793024 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.35244491696357727, loss=1.877436637878418
I0306 08:15:55.094239 139545878185728 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3072345554828644, loss=1.786749243736267
I0306 08:16:29.706593 139545869793024 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3202569782733917, loss=1.7339268922805786
I0306 08:17:04.332451 139545878185728 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.3669159412384033, loss=1.7055864334106445
I0306 08:17:38.955881 139545869793024 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3166419565677643, loss=1.7785260677337646
I0306 08:18:13.531129 139545878185728 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3388393521308899, loss=1.634564757347107
I0306 08:18:48.137688 139545869793024 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.31103450059890747, loss=1.799452781677246
I0306 08:19:22.744424 139545878185728 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.3165941834449768, loss=1.716722011566162
I0306 08:19:42.491186 139689492575424 spec.py:321] Evaluating on the training split.
I0306 08:19:45.111683 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:22:48.107028 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 08:22:50.707311 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:25:25.768769 139689492575424 spec.py:349] Evaluating on the test split.
I0306 08:25:28.363955 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:28:10.050486 139689492575424 submission_runner.py:469] Time since start: 47685.18s, 	Step: 80358, 	{'train/accuracy': 0.6419423818588257, 'train/loss': 1.7608380317687988, 'train/bleu': 31.83951738685453, 'validation/accuracy': 0.6632140874862671, 'validation/loss': 1.6009656190872192, 'validation/bleu': 28.122468114169163, 'validation/num_examples': 3000, 'test/accuracy': 0.6724481582641602, 'test/loss': 1.5342386960983276, 'test/bleu': 27.5907766150084, 'test/num_examples': 3003, 'score': 27745.4147837162, 'total_duration': 47685.179406404495, 'accumulated_submission_time': 27745.4147837162, 'accumulated_eval_time': 19934.85809111595, 'accumulated_logging_time': 0.6773777008056641}
I0306 08:28:10.070918 139545869793024 logging_writer.py:48] [80358] accumulated_eval_time=19934.9, accumulated_logging_time=0.677378, accumulated_submission_time=27745.4, global_step=80358, preemption_count=0, score=27745.4, test/accuracy=0.672448, test/bleu=27.5908, test/loss=1.53424, test/num_examples=3003, total_duration=47685.2, train/accuracy=0.641942, train/bleu=31.8395, train/loss=1.76084, validation/accuracy=0.663214, validation/bleu=28.1225, validation/loss=1.60097, validation/num_examples=3000
I0306 08:28:24.874257 139545878185728 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.30843645334243774, loss=1.7329868078231812
I0306 08:28:59.389662 139545869793024 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.318173348903656, loss=1.7457290887832642
I0306 08:29:33.947365 139545878185728 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.32229095697402954, loss=1.7568498849868774
I0306 08:30:08.534915 139545869793024 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.3500615954399109, loss=1.7565958499908447
I0306 08:30:43.128196 139545878185728 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.32214829325675964, loss=1.7576520442962646
I0306 08:31:17.676881 139545869793024 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.321748822927475, loss=1.749108076095581
I0306 08:31:52.243951 139545878185728 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3189143240451813, loss=1.8260802030563354
I0306 08:32:26.840017 139545869793024 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.2910381257534027, loss=1.6413973569869995
I0306 08:33:01.416634 139545878185728 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.32467368245124817, loss=1.770645022392273
I0306 08:33:36.017563 139545869793024 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.30672675371170044, loss=1.6241124868392944
I0306 08:34:10.595729 139545878185728 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.28938424587249756, loss=1.6747539043426514
I0306 08:34:45.195834 139545869793024 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.3742572069168091, loss=1.8175932168960571
I0306 08:35:19.782847 139545878185728 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.31911689043045044, loss=1.72401762008667
I0306 08:35:54.420248 139545869793024 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.326981782913208, loss=1.7530608177185059
I0306 08:36:29.111004 139545878185728 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.2958543002605438, loss=1.792654037475586
I0306 08:37:03.701540 139545869793024 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.3109934329986572, loss=1.7620338201522827
I0306 08:37:38.382709 139545878185728 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3095325529575348, loss=1.731109380722046
I0306 08:38:12.981833 139545869793024 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.30048811435699463, loss=1.7035307884216309
I0306 08:38:47.619902 139545878185728 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3156956732273102, loss=1.817049264907837
I0306 08:39:22.229286 139545869793024 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.29100388288497925, loss=1.6884245872497559
I0306 08:39:56.864877 139545878185728 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.31166231632232666, loss=1.7242248058319092
I0306 08:40:31.496683 139545869793024 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3005021810531616, loss=1.670337438583374
I0306 08:41:06.090980 139545878185728 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.3362143933773041, loss=1.695834755897522
I0306 08:41:40.721259 139545869793024 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.31827718019485474, loss=1.6603134870529175
I0306 08:42:10.117470 139689492575424 spec.py:321] Evaluating on the training split.
I0306 08:42:12.739223 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:45:35.332851 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 08:45:37.930373 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:48:25.193528 139689492575424 spec.py:349] Evaluating on the test split.
I0306 08:48:27.789491 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 08:50:57.759563 139689492575424 submission_runner.py:469] Time since start: 49052.89s, 	Step: 82786, 	{'train/accuracy': 0.6515135765075684, 'train/loss': 1.6875520944595337, 'train/bleu': 32.18715768853201, 'validation/accuracy': 0.6665266156196594, 'validation/loss': 1.5807284116744995, 'validation/bleu': 28.77804683738044, 'validation/num_examples': 3000, 'test/accuracy': 0.6780442595481873, 'test/loss': 1.5070613622665405, 'test/bleu': 28.153702852423912, 'test/num_examples': 3003, 'score': 28585.331007242203, 'total_duration': 49052.888520240784, 'accumulated_submission_time': 28585.331007242203, 'accumulated_eval_time': 20462.50013756752, 'accumulated_logging_time': 0.7066047191619873}
I0306 08:50:57.778281 139545878185728 logging_writer.py:48] [82786] accumulated_eval_time=20462.5, accumulated_logging_time=0.706605, accumulated_submission_time=28585.3, global_step=82786, preemption_count=0, score=28585.3, test/accuracy=0.678044, test/bleu=28.1537, test/loss=1.50706, test/num_examples=3003, total_duration=49052.9, train/accuracy=0.651514, train/bleu=32.1872, train/loss=1.68755, validation/accuracy=0.666527, validation/bleu=28.778, validation/loss=1.58073, validation/num_examples=3000
I0306 08:51:02.958453 139545869793024 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.30384328961372375, loss=1.777146339416504
I0306 08:51:37.429662 139545878185728 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.29585397243499756, loss=1.7299915552139282
I0306 08:52:12.035181 139545869793024 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.32752636075019836, loss=1.7442293167114258
I0306 08:52:46.655988 139545878185728 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.34677794575691223, loss=1.773814082145691
I0306 08:53:21.283440 139545869793024 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.3177662491798401, loss=1.7847819328308105
I0306 08:53:55.945771 139545878185728 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.3128316104412079, loss=1.7066222429275513
I0306 08:54:30.544688 139545869793024 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3184049725532532, loss=1.6797091960906982
I0306 08:55:05.189764 139545878185728 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3543657660484314, loss=1.799870252609253
I0306 08:55:39.805166 139545869793024 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.33131906390190125, loss=1.7443197965621948
I0306 08:56:14.436573 139545878185728 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3344031870365143, loss=1.7624413967132568
I0306 08:56:49.057635 139545869793024 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.32210686802864075, loss=1.5884088277816772
I0306 08:57:23.663898 139545878185728 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.30103063583374023, loss=1.7249946594238281
I0306 08:57:58.272955 139545869793024 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.35441336035728455, loss=1.6935611963272095
I0306 08:58:32.876145 139545878185728 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.33905088901519775, loss=1.7173197269439697
I0306 08:59:07.515191 139545869793024 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.32815176248550415, loss=1.6853454113006592
I0306 08:59:42.119247 139545878185728 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3225589692592621, loss=1.6735889911651611
I0306 09:00:16.735470 139545869793024 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.35378339886665344, loss=1.8624324798583984
I0306 09:00:51.373227 139545878185728 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.33857834339141846, loss=1.8242766857147217
I0306 09:01:25.995967 139545869793024 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.31323450803756714, loss=1.6710197925567627
I0306 09:02:00.611192 139545878185728 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.3445034623146057, loss=1.6955573558807373
I0306 09:02:35.228014 139545869793024 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.308417946100235, loss=1.7363247871398926
I0306 09:03:09.842885 139545878185728 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.31056874990463257, loss=1.6732364892959595
I0306 09:03:44.454611 139545869793024 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.31718388199806213, loss=1.745706558227539
I0306 09:04:19.082690 139545878185728 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.31036660075187683, loss=1.746590495109558
I0306 09:04:53.718678 139545869793024 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3109202980995178, loss=1.7605026960372925
I0306 09:04:57.870671 139689492575424 spec.py:321] Evaluating on the training split.
I0306 09:05:00.484962 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:08:19.347872 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 09:08:21.934805 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:11:15.670227 139689492575424 spec.py:349] Evaluating on the test split.
I0306 09:11:18.266110 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:13:45.619528 139689492575424 submission_runner.py:469] Time since start: 50420.75s, 	Step: 85213, 	{'train/accuracy': 0.6519049406051636, 'train/loss': 1.6945043802261353, 'train/bleu': 31.468646398352785, 'validation/accuracy': 0.6677131652832031, 'validation/loss': 1.571462869644165, 'validation/bleu': 28.55693722795598, 'validation/num_examples': 3000, 'test/accuracy': 0.6814737319946289, 'test/loss': 1.4955487251281738, 'test/bleu': 28.17801396797675, 'test/num_examples': 3003, 'score': 29425.292922735214, 'total_duration': 50420.74848651886, 'accumulated_submission_time': 29425.292922735214, 'accumulated_eval_time': 20990.248947143555, 'accumulated_logging_time': 0.7332005500793457}
I0306 09:13:45.636775 139545878185728 logging_writer.py:48] [85213] accumulated_eval_time=20990.2, accumulated_logging_time=0.733201, accumulated_submission_time=29425.3, global_step=85213, preemption_count=0, score=29425.3, test/accuracy=0.681474, test/bleu=28.178, test/loss=1.49555, test/num_examples=3003, total_duration=50420.7, train/accuracy=0.651905, train/bleu=31.4686, train/loss=1.6945, validation/accuracy=0.667713, validation/bleu=28.5569, validation/loss=1.57146, validation/num_examples=3000
I0306 09:14:15.928020 139545869793024 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.3192146420478821, loss=1.6806596517562866
I0306 09:14:50.399444 139545878185728 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3014103174209595, loss=1.709964394569397
I0306 09:15:24.936830 139545869793024 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.32020753622055054, loss=1.6959182024002075
I0306 09:15:59.467888 139545878185728 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.28951379656791687, loss=1.7529592514038086
I0306 09:16:34.011971 139545869793024 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.321492075920105, loss=1.7324706315994263
I0306 09:17:08.515698 139545878185728 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.31059688329696655, loss=1.7990403175354004
I0306 09:17:43.055414 139545869793024 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.33429446816444397, loss=1.7279717922210693
I0306 09:18:17.551184 139545878185728 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.31946367025375366, loss=1.6716598272323608
I0306 09:18:52.062686 139545869793024 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3468886613845825, loss=1.6762257814407349
I0306 09:19:26.559587 139545878185728 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.31998541951179504, loss=1.6815450191497803
I0306 09:20:01.050574 139545869793024 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.30760812759399414, loss=1.695556879043579
I0306 09:20:35.553287 139545878185728 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.31603747606277466, loss=1.6362093687057495
I0306 09:21:10.093202 139545869793024 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3118561804294586, loss=1.6775745153427124
I0306 09:21:44.571271 139545878185728 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.33099374175071716, loss=1.671659231185913
I0306 09:22:19.068037 139545869793024 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.30667486786842346, loss=1.6557084321975708
I0306 09:22:53.589380 139545878185728 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.34013450145721436, loss=1.6774122714996338
I0306 09:23:28.085472 139545869793024 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.320654958486557, loss=1.7133656740188599
I0306 09:24:02.574122 139545878185728 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.29868850111961365, loss=1.6256685256958008
I0306 09:24:37.086267 139545869793024 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.32063427567481995, loss=1.6419014930725098
I0306 09:25:11.563003 139545878185728 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3381345570087433, loss=1.745271921157837
I0306 09:25:46.110468 139545869793024 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.32361382246017456, loss=1.6935827732086182
I0306 09:26:20.640063 139545878185728 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3661710321903229, loss=1.7020771503448486
I0306 09:26:55.131528 139545869793024 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.30428317189216614, loss=1.7015751600265503
I0306 09:27:29.597267 139545878185728 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.31688886880874634, loss=1.7522480487823486
I0306 09:27:45.806366 139689492575424 spec.py:321] Evaluating on the training split.
I0306 09:27:48.414323 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:32:02.559900 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 09:32:05.162360 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:34:45.966926 139689492575424 spec.py:349] Evaluating on the test split.
I0306 09:34:48.552631 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:37:43.290764 139689492575424 submission_runner.py:469] Time since start: 51858.42s, 	Step: 87648, 	{'train/accuracy': 0.6497843861579895, 'train/loss': 1.7081503868103027, 'train/bleu': 32.03949079517517, 'validation/accuracy': 0.6713222861289978, 'validation/loss': 1.5528290271759033, 'validation/bleu': 28.78236413123116, 'validation/num_examples': 3000, 'test/accuracy': 0.6846020221710205, 'test/loss': 1.4713221788406372, 'test/bleu': 28.824666873911248, 'test/num_examples': 3003, 'score': 30265.327246904373, 'total_duration': 51858.419721364975, 'accumulated_submission_time': 30265.327246904373, 'accumulated_eval_time': 21587.73329925537, 'accumulated_logging_time': 0.7593300342559814}
I0306 09:37:43.307307 139545869793024 logging_writer.py:48] [87648] accumulated_eval_time=21587.7, accumulated_logging_time=0.75933, accumulated_submission_time=30265.3, global_step=87648, preemption_count=0, score=30265.3, test/accuracy=0.684602, test/bleu=28.8247, test/loss=1.47132, test/num_examples=3003, total_duration=51858.4, train/accuracy=0.649784, train/bleu=32.0395, train/loss=1.70815, validation/accuracy=0.671322, validation/bleu=28.7824, validation/loss=1.55283, validation/num_examples=3000
I0306 09:38:01.526821 139545878185728 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.3330381512641907, loss=1.7640788555145264
I0306 09:38:35.925057 139545869793024 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.29893678426742554, loss=1.651010513305664
I0306 09:39:10.331078 139545878185728 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3147985637187958, loss=1.7204313278198242
I0306 09:39:44.717259 139545869793024 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3382928669452667, loss=1.7109549045562744
I0306 09:40:19.139962 139545878185728 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3056180775165558, loss=1.7036001682281494
I0306 09:40:53.602411 139545869793024 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.37851861119270325, loss=1.6429353952407837
I0306 09:41:28.005328 139545878185728 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.31291279196739197, loss=1.6049425601959229
I0306 09:42:02.399744 139545869793024 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.3289583921432495, loss=1.6397815942764282
I0306 09:42:36.808104 139545878185728 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.32706648111343384, loss=1.714788556098938
I0306 09:43:11.204270 139545869793024 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.31441530585289, loss=1.7236593961715698
I0306 09:43:45.637716 139545878185728 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.31341278553009033, loss=1.7276060581207275
I0306 09:44:20.046051 139545869793024 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3483193814754486, loss=1.751968502998352
I0306 09:44:54.496007 139545878185728 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3194766342639923, loss=1.7545957565307617
I0306 09:45:28.894897 139545869793024 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3031269609928131, loss=1.6804180145263672
I0306 09:46:03.282594 139545878185728 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.31540533900260925, loss=1.6827616691589355
I0306 09:46:37.747374 139545869793024 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.34379610419273376, loss=1.6504237651824951
I0306 09:47:12.144902 139545878185728 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.3317670524120331, loss=1.6977037191390991
I0306 09:47:46.571249 139545869793024 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.31311362981796265, loss=1.7198964357376099
I0306 09:48:21.039835 139545878185728 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.339936763048172, loss=1.71343994140625
I0306 09:48:55.440883 139545869793024 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.3295333981513977, loss=1.7230781316757202
I0306 09:49:29.865682 139545878185728 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3296303451061249, loss=1.649348497390747
I0306 09:50:04.252943 139545869793024 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.33889585733413696, loss=1.6490461826324463
I0306 09:50:38.705647 139545878185728 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.3240515887737274, loss=1.6359715461730957
I0306 09:51:13.145579 139545869793024 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.3154851198196411, loss=1.6102440357208252
I0306 09:51:43.417734 139689492575424 spec.py:321] Evaluating on the training split.
I0306 09:51:46.028648 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:56:10.711284 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 09:56:13.310578 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 09:58:50.925197 139689492575424 spec.py:349] Evaluating on the test split.
I0306 09:58:53.517167 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 10:01:19.344667 139689492575424 submission_runner.py:469] Time since start: 53274.47s, 	Step: 90089, 	{'train/accuracy': 0.6603758335113525, 'train/loss': 1.6320950984954834, 'train/bleu': 33.022493754562475, 'validation/accuracy': 0.6735347509384155, 'validation/loss': 1.5366075038909912, 'validation/bleu': 29.04338025849897, 'validation/num_examples': 3000, 'test/accuracy': 0.687788188457489, 'test/loss': 1.4530447721481323, 'test/bleu': 29.20647557025608, 'test/num_examples': 3003, 'score': 31105.30047774315, 'total_duration': 53274.47362399101, 'accumulated_submission_time': 31105.30047774315, 'accumulated_eval_time': 22163.660198688507, 'accumulated_logging_time': 0.7838973999023438}
I0306 10:01:19.362433 139545878185728 logging_writer.py:48] [90089] accumulated_eval_time=22163.7, accumulated_logging_time=0.783897, accumulated_submission_time=31105.3, global_step=90089, preemption_count=0, score=31105.3, test/accuracy=0.687788, test/bleu=29.2065, test/loss=1.45304, test/num_examples=3003, total_duration=53274.5, train/accuracy=0.660376, train/bleu=33.0225, train/loss=1.6321, validation/accuracy=0.673535, validation/bleu=29.0434, validation/loss=1.53661, validation/num_examples=3000
I0306 10:01:23.477559 139545869793024 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3441649377346039, loss=1.7520502805709839
I0306 10:01:57.764562 139545878185728 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.36141735315322876, loss=1.7033004760742188
I0306 10:02:32.130890 139545869793024 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3355807065963745, loss=1.6428312063217163
I0306 10:03:06.501842 139545878185728 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3223036229610443, loss=1.7147297859191895
I0306 10:03:40.888257 139545869793024 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3403094708919525, loss=1.6668397188186646
I0306 10:04:15.342362 139545878185728 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.33924928307533264, loss=1.6827994585037231
I0306 10:04:49.735966 139545869793024 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.31758034229278564, loss=1.636620283126831
I0306 10:05:24.141608 139545878185728 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3544483780860901, loss=1.635407567024231
I0306 10:05:58.517264 139545869793024 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.33203309774398804, loss=1.6704254150390625
I0306 10:06:32.922013 139545878185728 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3103817403316498, loss=1.6012656688690186
I0306 10:07:07.335875 139545869793024 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.3484450578689575, loss=1.6662942171096802
I0306 10:07:41.765838 139545878185728 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3157874345779419, loss=1.6523188352584839
I0306 10:08:16.160921 139545869793024 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.32704731822013855, loss=1.648297667503357
I0306 10:08:50.596931 139545878185728 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.33775579929351807, loss=1.6743109226226807
I0306 10:09:25.028673 139545869793024 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.33462420105934143, loss=1.6274864673614502
I0306 10:09:59.440654 139545878185728 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.36249053478240967, loss=1.688854455947876
I0306 10:10:33.860053 139545869793024 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.3170050084590912, loss=1.5892689228057861
I0306 10:11:08.248079 139545878185728 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3218962848186493, loss=1.6486790180206299
I0306 10:11:42.672392 139545869793024 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.3382619321346283, loss=1.6018601655960083
I0306 10:12:17.073907 139545878185728 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3656679093837738, loss=1.5881661176681519
I0306 10:12:51.514875 139545869793024 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3224512040615082, loss=1.6965614557266235
I0306 10:13:25.924231 139545878185728 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.35842856764793396, loss=1.7164807319641113
I0306 10:14:00.309987 139545869793024 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3263353109359741, loss=1.6896039247512817
I0306 10:14:34.703262 139545878185728 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3598390221595764, loss=1.6924283504486084
I0306 10:15:09.125296 139545869793024 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.35576704144477844, loss=1.6948925256729126
I0306 10:15:19.441167 139689492575424 spec.py:321] Evaluating on the training split.
I0306 10:15:22.049309 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 10:18:47.723165 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 10:18:50.319803 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 10:21:33.712470 139689492575424 spec.py:349] Evaluating on the test split.
I0306 10:21:36.298908 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 10:24:07.346920 139689492575424 submission_runner.py:469] Time since start: 54642.48s, 	Step: 92531, 	{'train/accuracy': 0.659218430519104, 'train/loss': 1.6414427757263184, 'train/bleu': 33.0164677200164, 'validation/accuracy': 0.6762168407440186, 'validation/loss': 1.5189799070358276, 'validation/bleu': 29.464067011339953, 'validation/num_examples': 3000, 'test/accuracy': 0.6881241798400879, 'test/loss': 1.4379405975341797, 'test/bleu': 29.1639306192511, 'test/num_examples': 3003, 'score': 31945.24489593506, 'total_duration': 54642.475838661194, 'accumulated_submission_time': 31945.24489593506, 'accumulated_eval_time': 22691.565865516663, 'accumulated_logging_time': 0.8096988201141357}
I0306 10:24:07.364729 139545878185728 logging_writer.py:48] [92531] accumulated_eval_time=22691.6, accumulated_logging_time=0.809699, accumulated_submission_time=31945.2, global_step=92531, preemption_count=0, score=31945.2, test/accuracy=0.688124, test/bleu=29.1639, test/loss=1.43794, test/num_examples=3003, total_duration=54642.5, train/accuracy=0.659218, train/bleu=33.0165, train/loss=1.64144, validation/accuracy=0.676217, validation/bleu=29.4641, validation/loss=1.51898, validation/num_examples=3000
I0306 10:24:31.366748 139545869793024 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.3155688941478729, loss=1.5778939723968506
I0306 10:25:05.683178 139545878185728 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.3618752956390381, loss=1.7205899953842163
I0306 10:25:40.072531 139545869793024 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.324516624212265, loss=1.6069525480270386
I0306 10:26:14.482412 139545878185728 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.31677836179733276, loss=1.642201542854309
I0306 10:26:48.876752 139545869793024 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.35233569145202637, loss=1.6625921726226807
I0306 10:27:23.297817 139545878185728 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.3398246169090271, loss=1.5826369524002075
I0306 10:27:57.686941 139545869793024 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.3267557919025421, loss=1.5996602773666382
I0306 10:28:32.113267 139545878185728 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3352389633655548, loss=1.6639925241470337
I0306 10:29:06.543741 139545869793024 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3342438042163849, loss=1.6423274278640747
I0306 10:29:40.926691 139545878185728 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.327515572309494, loss=1.52812922000885
I0306 10:30:15.349569 139545869793024 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3433760404586792, loss=1.6102676391601562
I0306 10:30:49.731884 139545878185728 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.32076528668403625, loss=1.5819116830825806
I0306 10:31:24.184482 139545869793024 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.32625478506088257, loss=1.6375764608383179
I0306 10:31:58.570427 139545878185728 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3690851032733917, loss=1.6103622913360596
I0306 10:32:33.012680 139545869793024 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.314731240272522, loss=1.6318175792694092
I0306 10:33:07.447650 139545878185728 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.3481561541557312, loss=1.722084641456604
I0306 10:33:41.935137 139545869793024 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3239544928073883, loss=1.5921180248260498
I0306 10:34:16.431671 139545878185728 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.31770506501197815, loss=1.6223387718200684
I0306 10:34:50.913929 139545869793024 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3391137421131134, loss=1.6975535154342651
I0306 10:35:25.387523 139545878185728 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3318929374217987, loss=1.6158568859100342
I0306 10:35:59.877766 139545869793024 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.34348300099372864, loss=1.602745532989502
I0306 10:36:34.397052 139545878185728 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.35378098487854004, loss=1.5944585800170898
I0306 10:37:08.891445 139545869793024 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.30631351470947266, loss=1.6078776121139526
I0306 10:37:43.375956 139545878185728 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.3612954914569855, loss=1.5585737228393555
I0306 10:38:07.550856 139689492575424 spec.py:321] Evaluating on the training split.
I0306 10:38:10.160280 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 10:41:45.524698 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 10:41:48.111784 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 10:44:44.310351 139689492575424 spec.py:349] Evaluating on the test split.
I0306 10:44:46.907728 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 10:47:30.237023 139689492575424 submission_runner.py:469] Time since start: 56045.37s, 	Step: 94971, 	{'train/accuracy': 0.6745937466621399, 'train/loss': 1.538857102394104, 'train/bleu': 33.32081399737227, 'validation/accuracy': 0.6788248419761658, 'validation/loss': 1.5049623250961304, 'validation/bleu': 29.409977472835255, 'validation/num_examples': 3000, 'test/accuracy': 0.691785454750061, 'test/loss': 1.4231317043304443, 'test/bleu': 29.505565406079064, 'test/num_examples': 3003, 'score': 32785.29541873932, 'total_duration': 56045.3659696579, 'accumulated_submission_time': 32785.29541873932, 'accumulated_eval_time': 23254.251985549927, 'accumulated_logging_time': 0.8352725505828857}
I0306 10:47:30.254825 139545869793024 logging_writer.py:48] [94971] accumulated_eval_time=23254.3, accumulated_logging_time=0.835273, accumulated_submission_time=32785.3, global_step=94971, preemption_count=0, score=32785.3, test/accuracy=0.691785, test/bleu=29.5056, test/loss=1.42313, test/num_examples=3003, total_duration=56045.4, train/accuracy=0.674594, train/bleu=33.3208, train/loss=1.53886, validation/accuracy=0.678825, validation/bleu=29.41, validation/loss=1.50496, validation/num_examples=3000
I0306 10:47:40.555365 139545878185728 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.33788883686065674, loss=1.6795891523361206
I0306 10:48:14.903543 139545869793024 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.33508574962615967, loss=1.692482829093933
I0306 10:48:49.355302 139545878185728 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3570138216018677, loss=1.6770249605178833
I0306 10:49:23.841830 139545869793024 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.34683704376220703, loss=1.658726692199707
I0306 10:49:58.317423 139545878185728 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.31284117698669434, loss=1.674088478088379
I0306 10:50:32.810034 139545869793024 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.33979079127311707, loss=1.5740891695022583
I0306 10:51:07.319536 139545878185728 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3486424386501312, loss=1.636599063873291
I0306 10:51:41.757381 139545869793024 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.34203967452049255, loss=1.6367979049682617
I0306 10:52:16.273185 139545878185728 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.34806233644485474, loss=1.6828398704528809
I0306 10:52:50.782721 139545869793024 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.392068475484848, loss=1.5954557657241821
I0306 10:53:25.265276 139545878185728 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.33652517199516296, loss=1.622410774230957
I0306 10:53:59.767698 139545869793024 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.33278992772102356, loss=1.6021671295166016
I0306 10:54:34.372939 139545878185728 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.35997483134269714, loss=1.5907591581344604
I0306 10:55:08.876175 139545869793024 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.33743542432785034, loss=1.6674132347106934
I0306 10:55:43.336010 139545878185728 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3494853675365448, loss=1.6073740720748901
I0306 10:56:17.827405 139545869793024 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.33971214294433594, loss=1.654232144355774
I0306 10:56:52.331440 139545878185728 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.33809250593185425, loss=1.6179755926132202
I0306 10:57:26.825755 139545869793024 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3493284285068512, loss=1.5397371053695679
I0306 10:58:01.300691 139545878185728 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3639472424983978, loss=1.6750532388687134
I0306 10:58:35.750472 139545869793024 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3556675910949707, loss=1.6287761926651
I0306 10:59:10.218220 139545878185728 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.4315638244152069, loss=1.6069622039794922
I0306 10:59:44.715622 139545869793024 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3405759036540985, loss=1.6701467037200928
I0306 11:00:19.190342 139545878185728 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.33177053928375244, loss=1.5434637069702148
I0306 11:00:53.764428 139545869793024 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.33950895071029663, loss=1.5687611103057861
I0306 11:01:28.206051 139545878185728 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.31164783239364624, loss=1.5198585987091064
I0306 11:01:30.278621 139689492575424 spec.py:321] Evaluating on the training split.
I0306 11:01:32.883046 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:05:07.662194 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 11:05:10.251110 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:07:55.230184 139689492575424 spec.py:349] Evaluating on the test split.
I0306 11:07:57.824568 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:10:38.627571 139689492575424 submission_runner.py:469] Time since start: 57433.76s, 	Step: 97407, 	{'train/accuracy': 0.6671216487884521, 'train/loss': 1.5844563245773315, 'train/bleu': 32.968975707503155, 'validation/accuracy': 0.6801226139068604, 'validation/loss': 1.489696741104126, 'validation/bleu': 29.811164994494362, 'validation/num_examples': 3000, 'test/accuracy': 0.6946819424629211, 'test/loss': 1.4053953886032104, 'test/bleu': 29.80798658879084, 'test/num_examples': 3003, 'score': 33625.18694067001, 'total_duration': 57433.75652766228, 'accumulated_submission_time': 33625.18694067001, 'accumulated_eval_time': 23802.60088968277, 'accumulated_logging_time': 0.8610389232635498}
I0306 11:10:38.645541 139545869793024 logging_writer.py:48] [97407] accumulated_eval_time=23802.6, accumulated_logging_time=0.861039, accumulated_submission_time=33625.2, global_step=97407, preemption_count=0, score=33625.2, test/accuracy=0.694682, test/bleu=29.808, test/loss=1.4054, test/num_examples=3003, total_duration=57433.8, train/accuracy=0.667122, train/bleu=32.969, train/loss=1.58446, validation/accuracy=0.680123, validation/bleu=29.8112, validation/loss=1.4897, validation/num_examples=3000
I0306 11:11:10.910834 139545878185728 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.34293246269226074, loss=1.5850176811218262
I0306 11:11:45.335139 139545869793024 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.3277961015701294, loss=1.5548278093338013
I0306 11:12:19.822591 139545878185728 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.34946465492248535, loss=1.601649284362793
I0306 11:12:54.293506 139545869793024 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3144567310810089, loss=1.6080737113952637
I0306 11:13:28.779197 139545878185728 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.33041271567344666, loss=1.6044527292251587
I0306 11:14:03.245519 139545869793024 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.3385583758354187, loss=1.5495930910110474
I0306 11:14:37.780319 139545878185728 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3405701816082001, loss=1.6168638467788696
I0306 11:15:12.298314 139545869793024 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.3333321809768677, loss=1.5549354553222656
I0306 11:15:46.806626 139545878185728 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.33429813385009766, loss=1.6017186641693115
I0306 11:16:21.312691 139545869793024 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.32180550694465637, loss=1.5572419166564941
I0306 11:16:55.813014 139545878185728 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.35011476278305054, loss=1.653868556022644
I0306 11:17:30.312512 139545869793024 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3349630832672119, loss=1.621407389640808
I0306 11:18:04.833646 139545878185728 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.36238065361976624, loss=1.5906134843826294
I0306 11:18:39.361263 139545869793024 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3451579511165619, loss=1.5687432289123535
I0306 11:19:13.873668 139545878185728 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.3381258547306061, loss=1.5701984167099
I0306 11:19:48.366559 139545869793024 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.34661421179771423, loss=1.600639820098877
I0306 11:20:22.877056 139545878185728 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.3696095645427704, loss=1.6379034519195557
I0306 11:20:57.352493 139545869793024 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.36060044169425964, loss=1.6720592975616455
I0306 11:21:31.797768 139545878185728 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3298228979110718, loss=1.5243464708328247
I0306 11:22:06.295076 139545869793024 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.338556170463562, loss=1.595680832862854
I0306 11:22:40.806184 139545878185728 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.3646087348461151, loss=1.6553990840911865
I0306 11:23:15.309715 139545869793024 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3439437448978424, loss=1.6388684511184692
I0306 11:23:49.795190 139545878185728 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.33624914288520813, loss=1.5630953311920166
I0306 11:24:24.317440 139545869793024 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.35432249307632446, loss=1.5395480394363403
I0306 11:24:38.820550 139689492575424 spec.py:321] Evaluating on the training split.
I0306 11:24:41.426390 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:28:18.396741 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 11:28:21.002298 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:31:23.531675 139689492575424 spec.py:349] Evaluating on the test split.
I0306 11:31:26.132530 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:34:17.031445 139689492575424 submission_runner.py:469] Time since start: 58852.16s, 	Step: 99843, 	{'train/accuracy': 0.6677771806716919, 'train/loss': 1.5892866849899292, 'train/bleu': 33.31245207699641, 'validation/accuracy': 0.6835834383964539, 'validation/loss': 1.4707876443862915, 'validation/bleu': 29.856590665690337, 'validation/num_examples': 3000, 'test/accuracy': 0.6989688277244568, 'test/loss': 1.3837393522262573, 'test/bleu': 30.247295532347664, 'test/num_examples': 3003, 'score': 34465.22520542145, 'total_duration': 58852.160405635834, 'accumulated_submission_time': 34465.22520542145, 'accumulated_eval_time': 24380.81174635887, 'accumulated_logging_time': 0.8883967399597168}
I0306 11:34:17.049231 139545878185728 logging_writer.py:48] [99843] accumulated_eval_time=24380.8, accumulated_logging_time=0.888397, accumulated_submission_time=34465.2, global_step=99843, preemption_count=0, score=34465.2, test/accuracy=0.698969, test/bleu=30.2473, test/loss=1.38374, test/num_examples=3003, total_duration=58852.2, train/accuracy=0.667777, train/bleu=33.3125, train/loss=1.58929, validation/accuracy=0.683583, validation/bleu=29.8566, validation/loss=1.47079, validation/num_examples=3000
I0306 11:34:36.965384 139545869793024 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.3598947525024414, loss=1.6341469287872314
I0306 11:35:11.391767 139545878185728 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3548815846443176, loss=1.6150697469711304
I0306 11:35:45.917217 139545869793024 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.36480262875556946, loss=1.5106374025344849
I0306 11:36:20.404627 139545878185728 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3639792799949646, loss=1.5727447271347046
I0306 11:36:54.891495 139545869793024 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.33679771423339844, loss=1.5497767925262451
I0306 11:37:29.373318 139545878185728 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.3612896203994751, loss=1.581984519958496
I0306 11:38:03.841989 139545869793024 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.34160345792770386, loss=1.6041690111160278
I0306 11:38:38.330039 139545878185728 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3483457565307617, loss=1.5115580558776855
I0306 11:39:12.842100 139545869793024 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.336023211479187, loss=1.4758301973342896
I0306 11:39:47.334040 139545878185728 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.3271682858467102, loss=1.5487030744552612
I0306 11:40:21.829167 139545869793024 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.35504937171936035, loss=1.5292325019836426
I0306 11:40:56.361556 139545878185728 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3534354567527771, loss=1.5949790477752686
I0306 11:41:30.835355 139545869793024 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.34268587827682495, loss=1.5531129837036133
I0306 11:42:05.346568 139545878185728 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.33308276534080505, loss=1.5768245458602905
I0306 11:42:39.835176 139545869793024 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.3726527690887451, loss=1.5643913745880127
I0306 11:43:14.349910 139545878185728 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3633705675601959, loss=1.5885894298553467
I0306 11:43:48.830058 139545869793024 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.34949633479118347, loss=1.573121428489685
I0306 11:44:23.336909 139545878185728 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.3680739998817444, loss=1.6204683780670166
I0306 11:44:57.824668 139545869793024 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.3455600440502167, loss=1.5122982263565063
I0306 11:45:32.311480 139545878185728 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.3415319621562958, loss=1.6260632276535034
I0306 11:46:06.842691 139545869793024 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3348408043384552, loss=1.551041841506958
I0306 11:46:41.346323 139545878185728 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3405166268348694, loss=1.610768437385559
I0306 11:47:15.871678 139545869793024 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.33890047669410706, loss=1.5394113063812256
I0306 11:47:50.372603 139545878185728 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.37241706252098083, loss=1.547789216041565
I0306 11:48:17.328852 139689492575424 spec.py:321] Evaluating on the training split.
I0306 11:48:19.943781 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:52:37.737569 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 11:52:40.334423 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:55:48.021370 139689492575424 spec.py:349] Evaluating on the test split.
I0306 11:55:50.606671 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 11:59:10.080918 139689492575424 submission_runner.py:469] Time since start: 60345.21s, 	Step: 102279, 	{'train/accuracy': 0.6780392527580261, 'train/loss': 1.5182934999465942, 'train/bleu': 34.08348422046308, 'validation/accuracy': 0.685103714466095, 'validation/loss': 1.4602998495101929, 'validation/bleu': 30.211678343564657, 'validation/num_examples': 3000, 'test/accuracy': 0.7008225917816162, 'test/loss': 1.3738514184951782, 'test/bleu': 30.165329100553144, 'test/num_examples': 3003, 'score': 35305.37013506889, 'total_duration': 60345.209877491, 'accumulated_submission_time': 35305.37013506889, 'accumulated_eval_time': 25033.56376695633, 'accumulated_logging_time': 0.9141156673431396}
I0306 11:59:10.099339 139545869793024 logging_writer.py:48] [102279] accumulated_eval_time=25033.6, accumulated_logging_time=0.914116, accumulated_submission_time=35305.4, global_step=102279, preemption_count=0, score=35305.4, test/accuracy=0.700823, test/bleu=30.1653, test/loss=1.37385, test/num_examples=3003, total_duration=60345.2, train/accuracy=0.678039, train/bleu=34.0835, train/loss=1.51829, validation/accuracy=0.685104, validation/bleu=30.2117, validation/loss=1.4603, validation/num_examples=3000
I0306 11:59:17.625575 139545878185728 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3647654056549072, loss=1.618674635887146
I0306 11:59:52.005377 139545869793024 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.3708302974700928, loss=1.526810646057129
I0306 12:00:26.426328 139545878185728 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3732701539993286, loss=1.6152286529541016
I0306 12:01:00.927644 139545869793024 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.34577855467796326, loss=1.5034900903701782
I0306 12:01:35.397562 139545878185728 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.37474361062049866, loss=1.624760627746582
I0306 12:02:09.941929 139545869793024 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3524841368198395, loss=1.4624218940734863
I0306 12:02:44.407805 139545878185728 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.38169795274734497, loss=1.5064769983291626
I0306 12:03:18.893618 139545869793024 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.36011749505996704, loss=1.5074989795684814
I0306 12:03:53.413494 139545878185728 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.36787930130958557, loss=1.459210753440857
I0306 12:04:27.921486 139545869793024 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.36499255895614624, loss=1.5675997734069824
I0306 12:05:02.449900 139545878185728 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.3508875370025635, loss=1.5639880895614624
I0306 12:05:36.966438 139545869793024 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.3659631013870239, loss=1.554405689239502
I0306 12:06:11.447658 139545878185728 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.36564138531684875, loss=1.6611649990081787
I0306 12:06:45.966702 139545869793024 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3382554352283478, loss=1.5674492120742798
I0306 12:07:20.445140 139545878185728 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3472764790058136, loss=1.5160239934921265
I0306 12:07:54.954441 139545869793024 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3240659832954407, loss=1.5120439529418945
I0306 12:08:29.444910 139545878185728 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3624182939529419, loss=1.5385806560516357
I0306 12:09:03.973580 139545869793024 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.35975873470306396, loss=1.4912242889404297
I0306 12:09:38.442312 139545878185728 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.36694422364234924, loss=1.512723445892334
I0306 12:10:12.962748 139545869793024 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.39512670040130615, loss=1.611119031906128
I0306 12:10:47.481960 139545878185728 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.33844879269599915, loss=1.5009620189666748
I0306 12:11:21.957328 139545869793024 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.36773309111595154, loss=1.5722854137420654
I0306 12:11:56.472717 139545878185728 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.33933839201927185, loss=1.582321047782898
I0306 12:12:31.000669 139545869793024 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3727767765522003, loss=1.5370006561279297
I0306 12:13:05.525328 139545878185728 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.34298834204673767, loss=1.5399800539016724
I0306 12:13:10.370617 139689492575424 spec.py:321] Evaluating on the training split.
I0306 12:13:12.978565 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 12:17:20.150961 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 12:17:22.751241 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 12:20:22.294549 139689492575424 spec.py:349] Evaluating on the test split.
I0306 12:20:24.897196 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 12:23:20.688249 139689492575424 submission_runner.py:469] Time since start: 61795.82s, 	Step: 104715, 	{'train/accuracy': 0.6788626313209534, 'train/loss': 1.5193836688995361, 'train/bleu': 33.33430458049025, 'validation/accuracy': 0.6874274015426636, 'validation/loss': 1.4493452310562134, 'validation/bleu': 30.1511716810412, 'validation/num_examples': 3000, 'test/accuracy': 0.7026647925376892, 'test/loss': 1.3594906330108643, 'test/bleu': 30.611598211686623, 'test/num_examples': 3003, 'score': 36145.506828308105, 'total_duration': 61795.81718158722, 'accumulated_submission_time': 36145.506828308105, 'accumulated_eval_time': 25643.881331205368, 'accumulated_logging_time': 0.9414327144622803}
I0306 12:23:20.707511 139545869793024 logging_writer.py:48] [104715] accumulated_eval_time=25643.9, accumulated_logging_time=0.941433, accumulated_submission_time=36145.5, global_step=104715, preemption_count=0, score=36145.5, test/accuracy=0.702665, test/bleu=30.6116, test/loss=1.35949, test/num_examples=3003, total_duration=61795.8, train/accuracy=0.678863, train/bleu=33.3343, train/loss=1.51938, validation/accuracy=0.687427, validation/bleu=30.1512, validation/loss=1.44935, validation/num_examples=3000
I0306 12:23:50.258167 139545878185728 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.35624948143959045, loss=1.4707646369934082
I0306 12:24:24.716312 139545869793024 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.3453303575515747, loss=1.4988932609558105
I0306 12:24:59.235193 139545878185728 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.35738405585289, loss=1.5597797632217407
I0306 12:25:33.758933 139545869793024 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.398619145154953, loss=1.5627162456512451
I0306 12:26:08.297359 139545878185728 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.35617682337760925, loss=1.4493763446807861
I0306 12:26:42.770919 139545869793024 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.36110368371009827, loss=1.5452880859375
I0306 12:27:17.283767 139545878185728 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3639408349990845, loss=1.4682971239089966
I0306 12:27:51.838699 139545869793024 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.354106605052948, loss=1.4654960632324219
I0306 12:28:26.377131 139545878185728 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3536241054534912, loss=1.4727519750595093
I0306 12:29:00.904747 139545869793024 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.3594825565814972, loss=1.4746357202529907
I0306 12:29:35.374818 139545878185728 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.3816412687301636, loss=1.5084342956542969
I0306 12:30:09.879471 139545869793024 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.37728196382522583, loss=1.6080530881881714
I0306 12:30:44.414925 139545878185728 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.35114678740501404, loss=1.534621238708496
I0306 12:31:18.938071 139545869793024 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.3631376326084137, loss=1.4695357084274292
I0306 12:31:53.447889 139545878185728 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.36501380801200867, loss=1.4773871898651123
I0306 12:32:27.956979 139545869793024 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.37254929542541504, loss=1.5056605339050293
I0306 12:33:02.469359 139545878185728 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3724640905857086, loss=1.475778579711914
I0306 12:33:36.977294 139545869793024 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.3718160092830658, loss=1.5797168016433716
I0306 12:34:11.469325 139545878185728 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3816779851913452, loss=1.5048638582229614
I0306 12:34:45.975677 139545869793024 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.371082067489624, loss=1.5288481712341309
I0306 12:35:20.462988 139545878185728 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.37492695450782776, loss=1.5156883001327515
I0306 12:35:54.958756 139545869793024 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.37100470066070557, loss=1.415930151939392
I0306 12:36:29.474262 139545878185728 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3640141189098358, loss=1.4697104692459106
I0306 12:37:03.967762 139545869793024 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.35327619314193726, loss=1.4846673011779785
I0306 12:37:20.885787 139689492575424 spec.py:321] Evaluating on the training split.
I0306 12:37:23.501216 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 12:41:37.992587 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 12:41:40.590739 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 12:44:53.688270 139689492575424 spec.py:349] Evaluating on the test split.
I0306 12:44:56.279253 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 12:47:39.257714 139689492575424 submission_runner.py:469] Time since start: 63254.39s, 	Step: 107150, 	{'train/accuracy': 0.6933722496032715, 'train/loss': 1.4291377067565918, 'train/bleu': 35.39502832121977, 'validation/accuracy': 0.6889724135398865, 'validation/loss': 1.4430218935012817, 'validation/bleu': 30.320173706082663, 'validation/num_examples': 3000, 'test/accuracy': 0.7028385996818542, 'test/loss': 1.3506752252578735, 'test/bleu': 30.115848540168745, 'test/num_examples': 3003, 'score': 36985.54924941063, 'total_duration': 63254.38666534424, 'accumulated_submission_time': 36985.54924941063, 'accumulated_eval_time': 26262.25320839882, 'accumulated_logging_time': 0.9684188365936279}
I0306 12:47:39.276691 139545878185728 logging_writer.py:48] [107150] accumulated_eval_time=26262.3, accumulated_logging_time=0.968419, accumulated_submission_time=36985.5, global_step=107150, preemption_count=0, score=36985.5, test/accuracy=0.702839, test/bleu=30.1158, test/loss=1.35068, test/num_examples=3003, total_duration=63254.4, train/accuracy=0.693372, train/bleu=35.395, train/loss=1.42914, validation/accuracy=0.688972, validation/bleu=30.3202, validation/loss=1.44302, validation/num_examples=3000
I0306 12:47:56.808574 139545869793024 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.36913371086120605, loss=1.497504711151123
I0306 12:48:31.149083 139545878185728 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.36272427439689636, loss=1.4998329877853394
I0306 12:49:05.627133 139545869793024 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3691217005252838, loss=1.5226547718048096
I0306 12:49:40.077398 139545878185728 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3508828580379486, loss=1.4805430173873901
I0306 12:50:14.558015 139545869793024 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.37916675209999084, loss=1.4646713733673096
I0306 12:50:49.021183 139545878185728 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.37338775396347046, loss=1.530042052268982
I0306 12:51:23.501357 139545869793024 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.35519737005233765, loss=1.5164265632629395
I0306 12:51:58.004787 139545878185728 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.35487931966781616, loss=1.4688317775726318
I0306 12:52:32.441812 139545869793024 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.3852120041847229, loss=1.5088101625442505
I0306 12:53:06.967338 139545878185728 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.3681590259075165, loss=1.462541937828064
I0306 12:53:41.457419 139545869793024 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.3589879274368286, loss=1.4626550674438477
I0306 12:54:15.925316 139545878185728 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3659473955631256, loss=1.5018130540847778
I0306 12:54:50.403525 139545869793024 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3748049736022949, loss=1.3994861841201782
I0306 12:55:24.868310 139545878185728 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3778446316719055, loss=1.483256459236145
I0306 12:55:59.344394 139545869793024 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.37513482570648193, loss=1.5358165502548218
I0306 12:56:33.854511 139545878185728 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.38548457622528076, loss=1.4862003326416016
I0306 12:57:08.373734 139545869793024 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3656720519065857, loss=1.433108925819397
I0306 12:57:42.878618 139545878185728 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.3513181209564209, loss=1.4662429094314575
I0306 12:58:17.368889 139545869793024 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.37434765696525574, loss=1.456678867340088
I0306 12:58:51.852966 139545878185728 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3813539147377014, loss=1.4800001382827759
I0306 12:59:26.364145 139545869793024 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.3975529670715332, loss=1.4211596250534058
I0306 13:00:00.880525 139545878185728 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3799770772457123, loss=1.4413658380508423
I0306 13:00:35.368618 139545869793024 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3629738986492157, loss=1.5008362531661987
I0306 13:01:09.925284 139545878185728 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.36348193883895874, loss=1.4023252725601196
I0306 13:01:39.559989 139689492575424 spec.py:321] Evaluating on the training split.
I0306 13:01:42.161114 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:06:19.971209 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 13:06:22.565929 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:09:16.391537 139689492575424 spec.py:349] Evaluating on the test split.
I0306 13:09:18.984852 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:12:15.726336 139689492575424 submission_runner.py:469] Time since start: 64730.86s, 	Step: 109587, 	{'train/accuracy': 0.6888176202774048, 'train/loss': 1.456211805343628, 'train/bleu': 34.63510472800378, 'validation/accuracy': 0.689392626285553, 'validation/loss': 1.4324495792388916, 'validation/bleu': 30.383103394894057, 'validation/num_examples': 3000, 'test/accuracy': 0.7057119607925415, 'test/loss': 1.3385969400405884, 'test/bleu': 30.633743546084894, 'test/num_examples': 3003, 'score': 37825.69719719887, 'total_duration': 64730.85527205467, 'accumulated_submission_time': 37825.69719719887, 'accumulated_eval_time': 26898.419489860535, 'accumulated_logging_time': 0.9965364933013916}
I0306 13:12:15.747454 139545869793024 logging_writer.py:48] [109587] accumulated_eval_time=26898.4, accumulated_logging_time=0.996536, accumulated_submission_time=37825.7, global_step=109587, preemption_count=0, score=37825.7, test/accuracy=0.705712, test/bleu=30.6337, test/loss=1.3386, test/num_examples=3003, total_duration=64730.9, train/accuracy=0.688818, train/bleu=34.6351, train/loss=1.45621, validation/accuracy=0.689393, validation/bleu=30.3831, validation/loss=1.43245, validation/num_examples=3000
I0306 13:12:20.575715 139545878185728 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.35754555463790894, loss=1.5176845788955688
I0306 13:12:54.932511 139545869793024 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.36340540647506714, loss=1.4881435632705688
I0306 13:13:29.353034 139545878185728 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.40005746483802795, loss=1.504660725593567
I0306 13:14:03.829621 139545869793024 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.369128555059433, loss=1.4232336282730103
I0306 13:14:38.306721 139545878185728 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.37652888894081116, loss=1.507572889328003
I0306 13:15:12.786335 139545869793024 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.387743204832077, loss=1.475150465965271
I0306 13:15:47.236771 139545878185728 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.36412835121154785, loss=1.3933072090148926
I0306 13:16:21.725510 139545869793024 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.3802693486213684, loss=1.4369171857833862
I0306 13:16:56.232058 139545878185728 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3617088496685028, loss=1.43473482131958
I0306 13:17:30.680127 139545869793024 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3676096498966217, loss=1.4656509160995483
I0306 13:18:05.157485 139545878185728 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.34979739785194397, loss=1.420445442199707
I0306 13:18:39.636455 139545869793024 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.40521734952926636, loss=1.4287409782409668
I0306 13:19:14.161750 139545878185728 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.38675782084465027, loss=1.508785367012024
I0306 13:19:48.646430 139545869793024 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.36842402815818787, loss=1.4619759321212769
I0306 13:20:23.145074 139545878185728 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.3788088858127594, loss=1.493936538696289
I0306 13:20:57.611766 139545869793024 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3808649778366089, loss=1.4991581439971924
I0306 13:21:32.111944 139545878185728 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3762880563735962, loss=1.4541146755218506
I0306 13:22:06.613463 139545869793024 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.3866175413131714, loss=1.4839632511138916
I0306 13:22:41.055723 139545878185728 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.3734852373600006, loss=1.4945510625839233
I0306 13:23:15.510781 139545869793024 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.39491724967956543, loss=1.5026495456695557
I0306 13:23:49.998199 139545878185728 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.38874852657318115, loss=1.5114917755126953
I0306 13:24:24.464923 139545869793024 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3720175623893738, loss=1.4404933452606201
I0306 13:24:58.964648 139545878185728 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.37941843271255493, loss=1.4930346012115479
I0306 13:25:34.509639 139545869793024 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3888843059539795, loss=1.5316407680511475
I0306 13:26:08.972123 139545878185728 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.37924516201019287, loss=1.430329442024231
I0306 13:26:15.883661 139689492575424 spec.py:321] Evaluating on the training split.
I0306 13:26:18.485062 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:30:48.304600 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 13:30:50.897454 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:33:29.590648 139689492575424 spec.py:349] Evaluating on the test split.
I0306 13:33:32.193278 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:36:00.657833 139689492575424 submission_runner.py:469] Time since start: 66155.79s, 	Step: 112021, 	{'train/accuracy': 0.6880996227264404, 'train/loss': 1.4675003290176392, 'train/bleu': 34.87040125706429, 'validation/accuracy': 0.69106125831604, 'validation/loss': 1.4273666143417358, 'validation/bleu': 30.577266179227504, 'validation/num_examples': 3000, 'test/accuracy': 0.7083072662353516, 'test/loss': 1.3287723064422607, 'test/bleu': 30.795317804768935, 'test/num_examples': 3003, 'score': 38665.6983859539, 'total_duration': 66155.78678846359, 'accumulated_submission_time': 38665.6983859539, 'accumulated_eval_time': 27483.193616867065, 'accumulated_logging_time': 1.0259497165679932}
I0306 13:36:00.677364 139545869793024 logging_writer.py:48] [112021] accumulated_eval_time=27483.2, accumulated_logging_time=1.02595, accumulated_submission_time=38665.7, global_step=112021, preemption_count=0, score=38665.7, test/accuracy=0.708307, test/bleu=30.7953, test/loss=1.32877, test/num_examples=3003, total_duration=66155.8, train/accuracy=0.6881, train/bleu=34.8704, train/loss=1.4675, validation/accuracy=0.691061, validation/bleu=30.5773, validation/loss=1.42737, validation/num_examples=3000
I0306 13:36:28.213763 139545878185728 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.3866312503814697, loss=1.3969837427139282
I0306 13:37:02.656803 139545869793024 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.39020007848739624, loss=1.4275747537612915
I0306 13:37:37.123404 139545878185728 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.377682089805603, loss=1.4476341009140015
I0306 13:38:11.610667 139545869793024 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.36588168144226074, loss=1.4070041179656982
I0306 13:38:46.107299 139545878185728 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.3729066848754883, loss=1.4888359308242798
I0306 13:39:20.601099 139545869793024 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3703654408454895, loss=1.4240550994873047
I0306 13:39:55.123274 139545878185728 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.3730257451534271, loss=1.4403882026672363
I0306 13:40:29.733205 139545869793024 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3698700964450836, loss=1.414238452911377
I0306 13:41:04.219550 139545878185728 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.39096584916114807, loss=1.4217028617858887
I0306 13:41:38.688319 139545869793024 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.38830137252807617, loss=1.4360718727111816
I0306 13:42:13.187221 139545878185728 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.38160794973373413, loss=1.440977931022644
I0306 13:42:47.668545 139545869793024 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.3890421688556671, loss=1.4960435628890991
I0306 13:43:22.095213 139545878185728 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.3875103294849396, loss=1.3784910440444946
I0306 13:43:56.550978 139545869793024 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3930721580982208, loss=1.4582544565200806
I0306 13:44:30.977453 139545878185728 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.4023764133453369, loss=1.5172709226608276
I0306 13:45:05.460524 139545869793024 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.4096978008747101, loss=1.5442591905593872
I0306 13:45:39.895210 139545878185728 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.38260605931282043, loss=1.4522734880447388
I0306 13:46:14.377129 139545869793024 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3952958881855011, loss=1.5017058849334717
I0306 13:46:48.850669 139545878185728 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.42607319355010986, loss=1.5292285680770874
I0306 13:47:23.327550 139545869793024 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3733421266078949, loss=1.3947843313217163
I0306 13:47:57.755547 139545878185728 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.37492698431015015, loss=1.3916196823120117
I0306 13:48:32.195410 139545869793024 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.40116554498672485, loss=1.4480079412460327
I0306 13:49:06.675097 139545878185728 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3949705958366394, loss=1.4984441995620728
I0306 13:49:41.150024 139545869793024 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3882453441619873, loss=1.4798192977905273
I0306 13:50:00.814732 139689492575424 spec.py:321] Evaluating on the training split.
I0306 13:50:03.423934 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:54:36.285732 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 13:54:38.886142 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 13:57:35.049609 139689492575424 spec.py:349] Evaluating on the test split.
I0306 13:57:37.648093 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 14:00:36.221422 139689492575424 submission_runner.py:469] Time since start: 67631.35s, 	Step: 114458, 	{'train/accuracy': 0.693758487701416, 'train/loss': 1.437137484550476, 'train/bleu': 35.204630479308, 'validation/accuracy': 0.6919388175010681, 'validation/loss': 1.423449993133545, 'validation/bleu': 30.83663255069802, 'validation/num_examples': 3000, 'test/accuracy': 0.7082030177116394, 'test/loss': 1.3255749940872192, 'test/bleu': 31.043812284929025, 'test/num_examples': 3003, 'score': 39505.694428920746, 'total_duration': 67631.35035657883, 'accumulated_submission_time': 39505.694428920746, 'accumulated_eval_time': 28118.600239753723, 'accumulated_logging_time': 1.0543212890625}
I0306 14:00:36.244011 139545878185728 logging_writer.py:48] [114458] accumulated_eval_time=28118.6, accumulated_logging_time=1.05432, accumulated_submission_time=39505.7, global_step=114458, preemption_count=0, score=39505.7, test/accuracy=0.708203, test/bleu=31.0438, test/loss=1.32557, test/num_examples=3003, total_duration=67631.4, train/accuracy=0.693758, train/bleu=35.2046, train/loss=1.43714, validation/accuracy=0.691939, validation/bleu=30.8366, validation/loss=1.42345, validation/num_examples=3000
I0306 14:00:50.985534 139545869793024 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.3960638642311096, loss=1.4301908016204834
I0306 14:01:25.332140 139545878185728 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.40675339102745056, loss=1.4361379146575928
I0306 14:01:59.750491 139545869793024 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.38661980628967285, loss=1.4786465167999268
I0306 14:02:34.225411 139545878185728 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.39445385336875916, loss=1.555445671081543
I0306 14:03:08.696960 139545869793024 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.4008707106113434, loss=1.4440996646881104
I0306 14:03:43.192949 139545878185728 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.4124276041984558, loss=1.4334831237792969
I0306 14:04:17.654759 139545869793024 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.37902161478996277, loss=1.468198299407959
I0306 14:04:52.137795 139545878185728 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.388866662979126, loss=1.4143342971801758
I0306 14:05:26.640736 139545869793024 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.3946467638015747, loss=1.5191560983657837
I0306 14:06:01.124994 139545878185728 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.38136026263237, loss=1.4176026582717896
I0306 14:06:35.567692 139545869793024 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.40286707878112793, loss=1.3929686546325684
I0306 14:07:10.041239 139545878185728 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3928123414516449, loss=1.5096951723098755
I0306 14:07:44.526540 139545869793024 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.4119957983493805, loss=1.4494158029556274
I0306 14:08:18.969327 139545878185728 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.39084818959236145, loss=1.4675030708312988
I0306 14:08:53.443565 139545869793024 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.40497270226478577, loss=1.5478945970535278
I0306 14:09:27.926935 139545878185728 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.37518778443336487, loss=1.442177653312683
I0306 14:10:02.435280 139545869793024 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.3680013418197632, loss=1.418575644493103
I0306 14:10:36.863644 139545878185728 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.3701925277709961, loss=1.4460036754608154
I0306 14:11:11.361989 139545869793024 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.37790369987487793, loss=1.4246010780334473
I0306 14:11:45.843538 139545878185728 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.39652568101882935, loss=1.4088562726974487
I0306 14:12:20.301321 139545869793024 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.38092806935310364, loss=1.4227746725082397
I0306 14:12:54.753487 139545878185728 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.3876843750476837, loss=1.4823654890060425
I0306 14:13:29.235685 139545869793024 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.3823755979537964, loss=1.5252348184585571
I0306 14:14:03.734587 139545878185728 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.37926310300827026, loss=1.4924201965332031
I0306 14:14:36.490156 139689492575424 spec.py:321] Evaluating on the training split.
I0306 14:14:39.100164 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 14:19:12.328253 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 14:19:14.924337 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 14:21:49.708057 139689492575424 spec.py:349] Evaluating on the test split.
I0306 14:21:52.297591 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 14:24:37.299719 139689492575424 submission_runner.py:469] Time since start: 69072.43s, 	Step: 116896, 	{'train/accuracy': 0.6943850517272949, 'train/loss': 1.4304914474487305, 'train/bleu': 35.30651814736856, 'validation/accuracy': 0.6920253038406372, 'validation/loss': 1.4201182126998901, 'validation/bleu': 30.714690992036925, 'validation/num_examples': 3000, 'test/accuracy': 0.7088750004768372, 'test/loss': 1.3214093446731567, 'test/bleu': 31.178693493257015, 'test/num_examples': 3003, 'score': 40345.804631471634, 'total_duration': 69072.42863798141, 'accumulated_submission_time': 40345.804631471634, 'accumulated_eval_time': 28719.409719467163, 'accumulated_logging_time': 1.0852668285369873}
I0306 14:24:37.322108 139545869793024 logging_writer.py:48] [116896] accumulated_eval_time=28719.4, accumulated_logging_time=1.08527, accumulated_submission_time=40345.8, global_step=116896, preemption_count=0, score=40345.8, test/accuracy=0.708875, test/bleu=31.1787, test/loss=1.32141, test/num_examples=3003, total_duration=69072.4, train/accuracy=0.694385, train/bleu=35.3065, train/loss=1.43049, validation/accuracy=0.692025, validation/bleu=30.7147, validation/loss=1.42012, validation/num_examples=3000
I0306 14:24:39.049619 139545878185728 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.3784893751144409, loss=1.4437130689620972
I0306 14:25:13.373306 139545869793024 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.4057958722114563, loss=1.5138949155807495
I0306 14:25:47.772626 139545878185728 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.37772101163864136, loss=1.4157568216323853
I0306 14:26:22.228393 139545869793024 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.3889564871788025, loss=1.4452667236328125
I0306 14:26:56.643208 139545878185728 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.40528789162635803, loss=1.5254316329956055
I0306 14:27:31.110027 139545869793024 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.3848111629486084, loss=1.4541115760803223
I0306 14:28:05.584302 139545878185728 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3722013831138611, loss=1.4439430236816406
I0306 14:28:40.027988 139545869793024 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3859308362007141, loss=1.4899848699569702
I0306 14:29:14.510617 139545878185728 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.37697795033454895, loss=1.3842546939849854
I0306 14:29:48.947434 139545869793024 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.38706541061401367, loss=1.4220327138900757
I0306 14:30:23.376685 139545878185728 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.3811000883579254, loss=1.494683861732483
I0306 14:30:57.832416 139545869793024 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.3693622648715973, loss=1.4855704307556152
I0306 14:31:32.302932 139545878185728 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.3752673268318176, loss=1.4089746475219727
I0306 14:32:06.773856 139545869793024 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.3878929018974304, loss=1.4538793563842773
I0306 14:32:41.232932 139545878185728 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.37447789311408997, loss=1.4338140487670898
I0306 14:33:15.693754 139545869793024 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3792409598827362, loss=1.4093631505966187
I0306 14:33:50.143166 139545878185728 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3767871856689453, loss=1.4350429773330688
I0306 14:34:24.615763 139545869793024 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.3769090175628662, loss=1.3970575332641602
I0306 14:34:59.069666 139545878185728 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.3763735592365265, loss=1.4138004779815674
I0306 14:35:33.557830 139545869793024 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.3818623125553131, loss=1.4258365631103516
I0306 14:36:08.016545 139545878185728 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.38173431158065796, loss=1.4387288093566895
I0306 14:36:42.476804 139545869793024 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.37934958934783936, loss=1.401190161705017
I0306 14:37:16.956898 139545878185728 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.4001689851284027, loss=1.5167678594589233
I0306 14:37:51.451656 139545869793024 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.386669784784317, loss=1.4317705631256104
I0306 14:38:25.922402 139545878185728 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.39727771282196045, loss=1.448569655418396
I0306 14:38:37.644626 139689492575424 spec.py:321] Evaluating on the training split.
I0306 14:38:40.257953 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 14:43:01.223993 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 14:43:03.834411 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 14:45:43.930232 139689492575424 spec.py:349] Evaluating on the test split.
I0306 14:45:46.528280 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 14:48:25.622975 139689492575424 submission_runner.py:469] Time since start: 70500.75s, 	Step: 119335, 	{'train/accuracy': 0.6937611699104309, 'train/loss': 1.4346632957458496, 'train/bleu': 35.33694094470491, 'validation/accuracy': 0.6923590302467346, 'validation/loss': 1.4187566041946411, 'validation/bleu': 30.709309040437653, 'validation/num_examples': 3000, 'test/accuracy': 0.709176242351532, 'test/loss': 1.3205262422561646, 'test/bleu': 31.16010595361508, 'test/num_examples': 3003, 'score': 41185.99316048622, 'total_duration': 70500.75191736221, 'accumulated_submission_time': 41185.99316048622, 'accumulated_eval_time': 29307.38800597191, 'accumulated_logging_time': 1.1160492897033691}
I0306 14:48:25.642843 139545869793024 logging_writer.py:48] [119335] accumulated_eval_time=29307.4, accumulated_logging_time=1.11605, accumulated_submission_time=41186, global_step=119335, preemption_count=0, score=41186, test/accuracy=0.709176, test/bleu=31.1601, test/loss=1.32053, test/num_examples=3003, total_duration=70500.8, train/accuracy=0.693761, train/bleu=35.3369, train/loss=1.43466, validation/accuracy=0.692359, validation/bleu=30.7093, validation/loss=1.41876, validation/num_examples=3000
I0306 14:48:48.304095 139545878185728 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3872351050376892, loss=1.4671977758407593
I0306 14:49:22.725566 139545869793024 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.391170859336853, loss=1.459088921546936
I0306 14:49:57.178150 139545878185728 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.372528612613678, loss=1.3953008651733398
I0306 14:50:31.679380 139545869793024 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.36924251914024353, loss=1.3510152101516724
I0306 14:51:06.151494 139545878185728 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.3690873086452484, loss=1.4104334115982056
I0306 14:51:40.639304 139545869793024 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.3755474090576172, loss=1.3991643190383911
I0306 14:52:15.132270 139545878185728 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3631128966808319, loss=1.3995957374572754
I0306 14:52:49.612985 139545869793024 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.3926641047000885, loss=1.4425580501556396
I0306 14:53:24.081342 139545878185728 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.36729344725608826, loss=1.4075325727462769
I0306 14:53:58.600124 139545869793024 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.379528671503067, loss=1.4144923686981201
I0306 14:54:33.091485 139545878185728 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.3734237849712372, loss=1.4035391807556152
I0306 14:55:07.605918 139545869793024 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.37885382771492004, loss=1.4150769710540771
I0306 14:55:42.148662 139545878185728 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.37542450428009033, loss=1.4353253841400146
I0306 14:56:16.665544 139545869793024 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.36889320611953735, loss=1.3966214656829834
I0306 14:56:51.131417 139545878185728 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.3879864513874054, loss=1.444714903831482
I0306 14:57:25.650135 139545869793024 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.3814062476158142, loss=1.494569182395935
I0306 14:58:00.152915 139545878185728 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.38729748129844666, loss=1.452349305152893
I0306 14:58:34.644280 139545869793024 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3754616379737854, loss=1.4146672487258911
I0306 14:59:09.104701 139545878185728 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3697216212749481, loss=1.4430657625198364
I0306 14:59:43.577716 139545869793024 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.37176698446273804, loss=1.4512516260147095
I0306 15:00:18.095211 139545878185728 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.38306528329849243, loss=1.4072645902633667
I0306 15:00:52.580241 139545869793024 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.39635169506073, loss=1.4822970628738403
I0306 15:01:27.102270 139545878185728 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.39243391156196594, loss=1.3998675346374512
I0306 15:02:01.624959 139545869793024 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.3728583753108978, loss=1.3867673873901367
I0306 15:02:25.769497 139689492575424 spec.py:321] Evaluating on the training split.
I0306 15:02:28.374960 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 15:06:37.528834 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 15:06:40.146896 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 15:09:27.644929 139689492575424 spec.py:349] Evaluating on the test split.
I0306 15:09:30.236917 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 15:12:16.886146 139689492575424 submission_runner.py:469] Time since start: 71932.02s, 	Step: 121771, 	{'train/accuracy': 0.6943336725234985, 'train/loss': 1.4281764030456543, 'train/bleu': 35.800121114313306, 'validation/accuracy': 0.6923466920852661, 'validation/loss': 1.4188189506530762, 'validation/bleu': 30.68604794810933, 'validation/num_examples': 3000, 'test/accuracy': 0.7092341780662537, 'test/loss': 1.3205745220184326, 'test/bleu': 31.19492411570843, 'test/num_examples': 3003, 'score': 42025.983906030655, 'total_duration': 71932.01509547234, 'accumulated_submission_time': 42025.983906030655, 'accumulated_eval_time': 29898.50459933281, 'accumulated_logging_time': 1.1442885398864746}
I0306 15:12:16.906757 139545878185728 logging_writer.py:48] [121771] accumulated_eval_time=29898.5, accumulated_logging_time=1.14429, accumulated_submission_time=42026, global_step=121771, preemption_count=0, score=42026, test/accuracy=0.709234, test/bleu=31.1949, test/loss=1.32057, test/num_examples=3003, total_duration=71932, train/accuracy=0.694334, train/bleu=35.8001, train/loss=1.42818, validation/accuracy=0.692347, validation/bleu=30.686, validation/loss=1.41882, validation/num_examples=3000
I0306 15:12:27.197961 139545869793024 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.37717095017433167, loss=1.4720253944396973
I0306 15:13:01.555773 139545878185728 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.36009544134140015, loss=1.3845248222351074
I0306 15:13:36.044062 139545869793024 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.3706183135509491, loss=1.442112684249878
I0306 15:14:10.529415 139545878185728 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.4093858003616333, loss=1.4589557647705078
I0306 15:14:45.001142 139545869793024 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.3713485896587372, loss=1.3899054527282715
I0306 15:15:19.488861 139545878185728 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3755882978439331, loss=1.458524465560913
I0306 15:15:53.958736 139545869793024 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.37054121494293213, loss=1.4324790239334106
I0306 15:16:28.431160 139545878185728 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.37013155221939087, loss=1.3723490238189697
I0306 15:17:02.926577 139545869793024 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.3943422734737396, loss=1.4753462076187134
I0306 15:17:37.396685 139545878185728 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.3762074112892151, loss=1.3728086948394775
I0306 15:18:11.876864 139545869793024 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3900565207004547, loss=1.3824001550674438
I0306 15:18:46.337478 139545878185728 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.38382095098495483, loss=1.4414424896240234
I0306 15:19:20.834686 139545869793024 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.37301069498062134, loss=1.4783082008361816
I0306 15:19:55.343642 139545878185728 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.38156917691230774, loss=1.3915520906448364
I0306 15:20:29.838071 139545869793024 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3736236095428467, loss=1.4115537405014038
I0306 15:21:04.357080 139545878185728 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.3873355984687805, loss=1.407217264175415
I0306 15:21:38.816270 139545869793024 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3811948895454407, loss=1.3751929998397827
I0306 15:22:13.328676 139545878185728 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.39307308197021484, loss=1.436294674873352
I0306 15:22:47.797753 139545869793024 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.36740991473197937, loss=1.4298039674758911
I0306 15:23:22.266224 139545878185728 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3935302197933197, loss=1.487264633178711
I0306 15:23:56.761195 139545869793024 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.37119022011756897, loss=1.4333105087280273
I0306 15:24:31.221312 139545878185728 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.384216845035553, loss=1.472885251045227
I0306 15:25:05.725961 139545869793024 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.4007537066936493, loss=1.4894143342971802
I0306 15:25:40.241240 139545878185728 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.39593589305877686, loss=1.4745118618011475
I0306 15:26:14.747008 139545869793024 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.3563747704029083, loss=1.4474835395812988
I0306 15:26:17.165081 139689492575424 spec.py:321] Evaluating on the training split.
I0306 15:26:19.776358 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 15:30:33.849026 139689492575424 spec.py:333] Evaluating on the validation split.
I0306 15:30:36.458673 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 15:33:24.167927 139689492575424 spec.py:349] Evaluating on the test split.
I0306 15:33:26.770255 139689492575424 workload.py:181] Translating evaluation dataset.
I0306 15:36:13.650970 139689492575424 submission_runner.py:469] Time since start: 73368.78s, 	Step: 124208, 	{'train/accuracy': 0.6964361071586609, 'train/loss': 1.4138400554656982, 'train/bleu': 35.21291588737221, 'validation/accuracy': 0.6923466920852661, 'validation/loss': 1.4188189506530762, 'validation/bleu': 30.68604794810933, 'validation/num_examples': 3000, 'test/accuracy': 0.7092341780662537, 'test/loss': 1.3205745220184326, 'test/bleu': 31.19492411570843, 'test/num_examples': 3003, 'score': 42866.106605529785, 'total_duration': 73368.77992582321, 'accumulated_submission_time': 42866.106605529785, 'accumulated_eval_time': 30494.990439653397, 'accumulated_logging_time': 1.1726155281066895}
I0306 15:36:13.672073 139545878185728 logging_writer.py:48] [124208] accumulated_eval_time=30495, accumulated_logging_time=1.17262, accumulated_submission_time=42866.1, global_step=124208, preemption_count=0, score=42866.1, test/accuracy=0.709234, test/bleu=31.1949, test/loss=1.32057, test/num_examples=3003, total_duration=73368.8, train/accuracy=0.696436, train/bleu=35.2129, train/loss=1.41384, validation/accuracy=0.692347, validation/bleu=30.686, validation/loss=1.41882, validation/num_examples=3000
I0306 15:36:45.627023 139545869793024 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.3667313754558563, loss=1.4263134002685547
I0306 15:37:20.077654 139545878185728 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.40647026896476746, loss=1.4391411542892456
I0306 15:37:54.547686 139545869793024 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.37813928723335266, loss=1.4101228713989258
I0306 15:38:29.066797 139545878185728 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.40397271513938904, loss=1.4685124158859253
I0306 15:39:03.561482 139545869793024 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.38889747858047485, loss=1.483365774154663
I0306 15:39:38.048074 139545878185728 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.3678283393383026, loss=1.3618749380111694
I0306 15:40:12.563327 139545869793024 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.3748074173927307, loss=1.3997708559036255
I0306 15:40:47.069107 139545878185728 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3696802854537964, loss=1.4524717330932617
I0306 15:41:21.542255 139545869793024 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.36898553371429443, loss=1.5301717519760132
I0306 15:41:56.015154 139545878185728 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.36935222148895264, loss=1.342777132987976
I0306 15:42:30.535169 139545869793024 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3686271905899048, loss=1.3384864330291748
I0306 15:43:05.027566 139545878185728 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.383455365896225, loss=1.386189341545105
I0306 15:43:39.512133 139545869793024 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.3826994299888611, loss=1.4266515970230103
I0306 15:44:14.046221 139545878185728 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.37168487906455994, loss=1.4277678728103638
I0306 15:44:48.506787 139545869793024 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.37105241417884827, loss=1.4132397174835205
I0306 15:45:23.006699 139545878185728 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.3720376491546631, loss=1.3629450798034668
I0306 15:45:57.680549 139545869793024 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.3794667720794678, loss=1.422206163406372
I0306 15:46:32.318032 139545878185728 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.37593212723731995, loss=1.398158073425293
I0306 15:47:06.901942 139545869793024 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.3660198450088501, loss=1.440414309501648
I0306 15:47:41.498261 139545878185728 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.4109770357608795, loss=1.5327502489089966
I0306 15:48:16.155984 139545869793024 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.37773779034614563, loss=1.3554227352142334
I0306 15:48:50.751677 139545878185728 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.3725498616695404, loss=1.4757105112075806
I0306 15:49:25.387552 139545869793024 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.37114447355270386, loss=1.363916039466858
I0306 15:50:00.059201 139545878185728 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.3819703459739685, loss=1.459632396697998
I0306 15:50:13.951066 139545869793024 logging_writer.py:48] [126641] global_step=126641, preemption_count=0, score=43706.2
I0306 15:50:13.976360 139689492575424 submission_runner.py:646] Tuning trial 4/5
I0306 15:50:13.976512 139689492575424 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0306 15:50:13.980387 139689492575424 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006137479213066399, 'train/loss': 11.113200187683105, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.092617988586426, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.114619255065918, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.273067712783813, 'total_duration': 947.1256275177002, 'accumulated_submission_time': 25.273067712783813, 'accumulated_eval_time': 921.8524708747864, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2433, {'train/accuracy': 0.5268543362617493, 'train/loss': 2.633565902709961, 'train/bleu': 23.59888208414684, 'validation/accuracy': 0.5368081331253052, 'validation/loss': 2.552730083465576, 'validation/bleu': 19.799683952039462, 'validation/num_examples': 3000, 'test/accuracy': 0.5373885035514832, 'test/loss': 2.565838575363159, 'test/bleu': 18.162308419097887, 'test/num_examples': 3003, 'score': 865.198891878128, 'total_duration': 2287.946869134903, 'accumulated_submission_time': 865.198891878128, 'accumulated_eval_time': 1422.5883588790894, 'accumulated_logging_time': 0.014955520629882812, 'global_step': 2433, 'preemption_count': 0}), (4866, {'train/accuracy': 0.5755481123924255, 'train/loss': 2.233250141143799, 'train/bleu': 27.06807370595582, 'validation/accuracy': 0.5934047102928162, 'validation/loss': 2.0908114910125732, 'validation/bleu': 23.52311379262754, 'validation/num_examples': 3000, 'test/accuracy': 0.5979492664337158, 'test/loss': 2.0481574535369873, 'test/bleu': 21.888933993197586, 'test/num_examples': 3003, 'score': 1705.0806679725647, 'total_duration': 3656.851088285446, 'accumulated_submission_time': 1705.0806679725647, 'accumulated_eval_time': 1951.4499690532684, 'accumulated_logging_time': 0.03181576728820801, 'global_step': 4866, 'preemption_count': 0}), (7302, {'train/accuracy': 0.5866950750350952, 'train/loss': 2.1409075260162354, 'train/bleu': 27.617000759658627, 'validation/accuracy': 0.6015252470970154, 'validation/loss': 2.0310425758361816, 'validation/bleu': 24.224764791205338, 'validation/num_examples': 3000, 'test/accuracy': 0.6064302921295166, 'test/loss': 1.9941027164459229, 'test/bleu': 22.91254225656282, 'test/num_examples': 3003, 'score': 2545.177567958832, 'total_duration': 5192.537220478058, 'accumulated_submission_time': 2545.177567958832, 'accumulated_eval_time': 2646.8794317245483, 'accumulated_logging_time': 0.05060625076293945, 'global_step': 7302, 'preemption_count': 0}), (9742, {'train/accuracy': 0.5926209688186646, 'train/loss': 2.129615068435669, 'train/bleu': 28.122455950412665, 'validation/accuracy': 0.611845850944519, 'validation/loss': 1.9635041952133179, 'validation/bleu': 24.901812232626185, 'validation/num_examples': 3000, 'test/accuracy': 0.6167072057723999, 'test/loss': 1.9267637729644775, 'test/bleu': 23.346317025461424, 'test/num_examples': 3003, 'score': 3385.224435567856, 'total_duration': 6561.912496328354, 'accumulated_submission_time': 3385.224435567856, 'accumulated_eval_time': 3176.047967195511, 'accumulated_logging_time': 0.06971526145935059, 'global_step': 9742, 'preemption_count': 0}), (12183, {'train/accuracy': 0.5957537293434143, 'train/loss': 2.109661340713501, 'train/bleu': 28.22868065607038, 'validation/accuracy': 0.6141448020935059, 'validation/loss': 1.95601224899292, 'validation/bleu': 24.81098728072508, 'validation/num_examples': 3000, 'test/accuracy': 0.6180627942085266, 'test/loss': 1.9053274393081665, 'test/bleu': 23.33940159463234, 'test/num_examples': 3003, 'score': 4225.374735355377, 'total_duration': 7954.78924202919, 'accumulated_submission_time': 4225.374735355377, 'accumulated_eval_time': 3728.624238014221, 'accumulated_logging_time': 0.08627080917358398, 'global_step': 12183, 'preemption_count': 0}), (14620, {'train/accuracy': 0.5993795990943909, 'train/loss': 2.075961112976074, 'train/bleu': 27.6468726564483, 'validation/accuracy': 0.6140953898429871, 'validation/loss': 1.9391183853149414, 'validation/bleu': 24.631330065556224, 'validation/num_examples': 3000, 'test/accuracy': 0.6224076151847839, 'test/loss': 1.8961374759674072, 'test/bleu': 23.755933195601294, 'test/num_examples': 3003, 'score': 5065.436066389084, 'total_duration': 9412.216654777527, 'accumulated_submission_time': 5065.436066389084, 'accumulated_eval_time': 4345.8449177742, 'accumulated_logging_time': 0.10357880592346191, 'global_step': 14620, 'preemption_count': 0}), (17055, {'train/accuracy': 0.5998297333717346, 'train/loss': 2.071913242340088, 'train/bleu': 28.266365049177512, 'validation/accuracy': 0.6177539229393005, 'validation/loss': 1.9214348793029785, 'validation/bleu': 24.703533923266356, 'validation/num_examples': 3000, 'test/accuracy': 0.627250611782074, 'test/loss': 1.8704904317855835, 'test/bleu': 23.91687603984399, 'test/num_examples': 3003, 'score': 5905.322413682938, 'total_duration': 10777.821339130402, 'accumulated_submission_time': 5905.322413682938, 'accumulated_eval_time': 4871.411356687546, 'accumulated_logging_time': 0.12322354316711426, 'global_step': 17055, 'preemption_count': 0}), (19487, {'train/accuracy': 0.6062939763069153, 'train/loss': 2.002042055130005, 'train/bleu': 28.480109655554514, 'validation/accuracy': 0.6182236075401306, 'validation/loss': 1.909186840057373, 'validation/bleu': 24.821472033894906, 'validation/num_examples': 3000, 'test/accuracy': 0.6248059272766113, 'test/loss': 1.8629462718963623, 'test/bleu': 24.132169736423464, 'test/num_examples': 3003, 'score': 6745.2925136089325, 'total_duration': 12140.001472711563, 'accumulated_submission_time': 6745.2925136089325, 'accumulated_eval_time': 5393.471658945084, 'accumulated_logging_time': 0.14071011543273926, 'global_step': 19487, 'preemption_count': 0}), (21914, {'train/accuracy': 0.6007887125015259, 'train/loss': 2.0580198764801025, 'train/bleu': 28.323652909604206, 'validation/accuracy': 0.6190640926361084, 'validation/loss': 1.9009321928024292, 'validation/bleu': 24.85648089794779, 'validation/num_examples': 3000, 'test/accuracy': 0.6276909112930298, 'test/loss': 1.8486824035644531, 'test/bleu': 24.52536076890693, 'test/num_examples': 3003, 'score': 7585.193206548691, 'total_duration': 13522.477527618408, 'accumulated_submission_time': 7585.193206548691, 'accumulated_eval_time': 5935.904383182526, 'accumulated_logging_time': 0.15914463996887207, 'global_step': 21914, 'preemption_count': 0}), (24341, {'train/accuracy': 0.603492021560669, 'train/loss': 2.0577852725982666, 'train/bleu': 29.115690771638018, 'validation/accuracy': 0.622005820274353, 'validation/loss': 1.890318751335144, 'validation/bleu': 25.613137879165905, 'validation/num_examples': 3000, 'test/accuracy': 0.630390465259552, 'test/loss': 1.8423603773117065, 'test/bleu': 24.8209793289967, 'test/num_examples': 3003, 'score': 8425.063936471939, 'total_duration': 14924.781909704208, 'accumulated_submission_time': 8425.063936471939, 'accumulated_eval_time': 6498.191673755646, 'accumulated_logging_time': 0.17865896224975586, 'global_step': 24341, 'preemption_count': 0}), (26773, {'train/accuracy': 0.6080901026725769, 'train/loss': 1.9998950958251953, 'train/bleu': 28.095064153509668, 'validation/accuracy': 0.6239339709281921, 'validation/loss': 1.8744738101959229, 'validation/bleu': 25.602918024913002, 'validation/num_examples': 3000, 'test/accuracy': 0.6316301822662354, 'test/loss': 1.8300033807754517, 'test/bleu': 24.760247311050563, 'test/num_examples': 3003, 'score': 9264.954586982727, 'total_duration': 16281.333972454071, 'accumulated_submission_time': 9264.954586982727, 'accumulated_eval_time': 7014.706246376038, 'accumulated_logging_time': 0.1974349021911621, 'global_step': 26773, 'preemption_count': 0}), (29207, {'train/accuracy': 0.6065573692321777, 'train/loss': 2.0197722911834717, 'train/bleu': 28.237094677986015, 'validation/accuracy': 0.6232665181159973, 'validation/loss': 1.8741369247436523, 'validation/bleu': 25.141865675721682, 'validation/num_examples': 3000, 'test/accuracy': 0.628953754901886, 'test/loss': 1.8296782970428467, 'test/bleu': 24.23731214332591, 'test/num_examples': 3003, 'score': 10104.879020929337, 'total_duration': 17698.773823976517, 'accumulated_submission_time': 10104.879020929337, 'accumulated_eval_time': 7592.074867486954, 'accumulated_logging_time': 0.21680188179016113, 'global_step': 29207, 'preemption_count': 0}), (31641, {'train/accuracy': 0.6186055541038513, 'train/loss': 1.8835495710372925, 'train/bleu': 29.28565242711362, 'validation/accuracy': 0.6194719672203064, 'validation/loss': 1.89082670211792, 'validation/bleu': 25.13284595514229, 'validation/num_examples': 3000, 'test/accuracy': 0.6315374970436096, 'test/loss': 1.8259590864181519, 'test/bleu': 24.868472808984976, 'test/num_examples': 3003, 'score': 10944.750674247742, 'total_duration': 19135.87686753273, 'accumulated_submission_time': 10944.750674247742, 'accumulated_eval_time': 8189.159771203995, 'accumulated_logging_time': 0.2370615005493164, 'global_step': 31641, 'preemption_count': 0}), (34070, {'train/accuracy': 0.6033328175544739, 'train/loss': 2.0315353870391846, 'train/bleu': 28.374210626281904, 'validation/accuracy': 0.6257879734039307, 'validation/loss': 1.8602033853530884, 'validation/bleu': 25.092722468136138, 'validation/num_examples': 3000, 'test/accuracy': 0.6340053081512451, 'test/loss': 1.8100531101226807, 'test/bleu': 24.322860514866825, 'test/num_examples': 3003, 'score': 11784.921707630157, 'total_duration': 20544.304527759552, 'accumulated_submission_time': 11784.921707630157, 'accumulated_eval_time': 8757.27048420906, 'accumulated_logging_time': 0.2573122978210449, 'global_step': 34070, 'preemption_count': 0}), (36506, {'train/accuracy': 0.6059224605560303, 'train/loss': 2.025665760040283, 'train/bleu': 28.93851010404595, 'validation/accuracy': 0.6288408637046814, 'validation/loss': 1.8471442461013794, 'validation/bleu': 25.76470886507192, 'validation/num_examples': 3000, 'test/accuracy': 0.6358938813209534, 'test/loss': 1.7881492376327515, 'test/bleu': 24.858251379464022, 'test/num_examples': 3003, 'score': 12625.111197710037, 'total_duration': 22001.20336818695, 'accumulated_submission_time': 12625.111197710037, 'accumulated_eval_time': 9373.833971738815, 'accumulated_logging_time': 0.277829647064209, 'global_step': 36506, 'preemption_count': 0}), (38942, {'train/accuracy': 0.6108911037445068, 'train/loss': 1.9686954021453857, 'train/bleu': 29.233891513405997, 'validation/accuracy': 0.6280498504638672, 'validation/loss': 1.8299354314804077, 'validation/bleu': 25.654858096249537, 'validation/num_examples': 3000, 'test/accuracy': 0.6367744207382202, 'test/loss': 1.7877776622772217, 'test/bleu': 25.24583476216284, 'test/num_examples': 3003, 'score': 13465.159498691559, 'total_duration': 23398.603474617004, 'accumulated_submission_time': 13465.159498691559, 'accumulated_eval_time': 9931.03953909874, 'accumulated_logging_time': 0.29782938957214355, 'global_step': 38942, 'preemption_count': 0}), (41378, {'train/accuracy': 0.6143617630004883, 'train/loss': 1.975722312927246, 'train/bleu': 29.167748830514626, 'validation/accuracy': 0.6329320669174194, 'validation/loss': 1.815677523612976, 'validation/bleu': 26.27517430804815, 'validation/num_examples': 3000, 'test/accuracy': 0.641478419303894, 'test/loss': 1.7617791891098022, 'test/bleu': 25.117817134580957, 'test/num_examples': 3003, 'score': 14305.20078587532, 'total_duration': 24749.626266479492, 'accumulated_submission_time': 14305.20078587532, 'accumulated_eval_time': 10441.871820926666, 'accumulated_logging_time': 0.3177821636199951, 'global_step': 41378, 'preemption_count': 0}), (43815, {'train/accuracy': 0.6126684546470642, 'train/loss': 1.992365837097168, 'train/bleu': 28.83128440998963, 'validation/accuracy': 0.6336365938186646, 'validation/loss': 1.8152878284454346, 'validation/bleu': 26.517155892935076, 'validation/num_examples': 3000, 'test/accuracy': 0.6423705220222473, 'test/loss': 1.7523627281188965, 'test/bleu': 25.240188157574117, 'test/num_examples': 3003, 'score': 15145.361122608185, 'total_duration': 26106.057208776474, 'accumulated_submission_time': 15145.361122608185, 'accumulated_eval_time': 10957.99885392189, 'accumulated_logging_time': 0.3374199867248535, 'global_step': 43815, 'preemption_count': 0}), (46257, {'train/accuracy': 0.6152775883674622, 'train/loss': 1.953343152999878, 'train/bleu': 29.2012312583271, 'validation/accuracy': 0.6349838376045227, 'validation/loss': 1.8015406131744385, 'validation/bleu': 26.20738323965694, 'validation/num_examples': 3000, 'test/accuracy': 0.6414204835891724, 'test/loss': 1.7476232051849365, 'test/bleu': 25.088318590204942, 'test/num_examples': 3003, 'score': 15985.390161037445, 'total_duration': 27505.930781126022, 'accumulated_submission_time': 15985.390161037445, 'accumulated_eval_time': 11517.6967086792, 'accumulated_logging_time': 0.35872364044189453, 'global_step': 46257, 'preemption_count': 0}), (48699, {'train/accuracy': 0.6168846487998962, 'train/loss': 1.94062077999115, 'train/bleu': 29.500472430735, 'validation/accuracy': 0.6354905962944031, 'validation/loss': 1.7929277420043945, 'validation/bleu': 26.349236690840748, 'validation/num_examples': 3000, 'test/accuracy': 0.6444328427314758, 'test/loss': 1.7411733865737915, 'test/bleu': 25.414078304524136, 'test/num_examples': 3003, 'score': 16825.378594636917, 'total_duration': 28952.60617995262, 'accumulated_submission_time': 16825.378594636917, 'accumulated_eval_time': 12124.237494707108, 'accumulated_logging_time': 0.37989234924316406, 'global_step': 48699, 'preemption_count': 0}), (51139, {'train/accuracy': 0.6234533190727234, 'train/loss': 1.880999207496643, 'train/bleu': 29.79921907715996, 'validation/accuracy': 0.6393716335296631, 'validation/loss': 1.774951457977295, 'validation/bleu': 25.177391084504883, 'validation/num_examples': 3000, 'test/accuracy': 0.6492410898208618, 'test/loss': 1.7134965658187866, 'test/bleu': 25.84408722158683, 'test/num_examples': 3003, 'score': 17665.453531742096, 'total_duration': 30412.59698700905, 'accumulated_submission_time': 17665.453531742096, 'accumulated_eval_time': 12744.009162187576, 'accumulated_logging_time': 0.40030622482299805, 'global_step': 51139, 'preemption_count': 0}), (53575, {'train/accuracy': 0.6167008876800537, 'train/loss': 1.9402761459350586, 'train/bleu': 29.189115394423293, 'validation/accuracy': 0.6389390230178833, 'validation/loss': 1.7629574537277222, 'validation/bleu': 26.3640744780536, 'validation/num_examples': 3000, 'test/accuracy': 0.6489050984382629, 'test/loss': 1.706540584564209, 'test/bleu': 25.750829699117823, 'test/num_examples': 3003, 'score': 18505.48460483551, 'total_duration': 31827.784942388535, 'accumulated_submission_time': 18505.48460483551, 'accumulated_eval_time': 13319.018489837646, 'accumulated_logging_time': 0.4210548400878906, 'global_step': 53575, 'preemption_count': 0}), (56011, {'train/accuracy': 0.6215807795524597, 'train/loss': 1.9044971466064453, 'train/bleu': 29.91973913062219, 'validation/accuracy': 0.6425852179527283, 'validation/loss': 1.7428629398345947, 'validation/bleu': 26.819695018013714, 'validation/num_examples': 3000, 'test/accuracy': 0.651326596736908, 'test/loss': 1.6833521127700806, 'test/bleu': 26.01054652126946, 'test/num_examples': 3003, 'score': 19345.36077594757, 'total_duration': 33231.33450436592, 'accumulated_submission_time': 19345.36077594757, 'accumulated_eval_time': 13882.54513001442, 'accumulated_logging_time': 0.44249820709228516, 'global_step': 56011, 'preemption_count': 0}), (58448, {'train/accuracy': 0.6268053650856018, 'train/loss': 1.8684265613555908, 'train/bleu': 29.76866370597327, 'validation/accuracy': 0.6423874497413635, 'validation/loss': 1.7399617433547974, 'validation/bleu': 26.59567368946308, 'validation/num_examples': 3000, 'test/accuracy': 0.6539566516876221, 'test/loss': 1.6817491054534912, 'test/bleu': 26.413865931418783, 'test/num_examples': 3003, 'score': 20185.41586971283, 'total_duration': 34744.17664408684, 'accumulated_submission_time': 20185.41586971283, 'accumulated_eval_time': 14555.184524774551, 'accumulated_logging_time': 0.4643685817718506, 'global_step': 58448, 'preemption_count': 0}), (60885, {'train/accuracy': 0.6256636381149292, 'train/loss': 1.8835506439208984, 'train/bleu': 29.694588496218937, 'validation/accuracy': 0.6439695358276367, 'validation/loss': 1.7309099435806274, 'validation/bleu': 26.745883438675794, 'validation/num_examples': 3000, 'test/accuracy': 0.6533194184303284, 'test/loss': 1.6685322523117065, 'test/bleu': 25.981176771521977, 'test/num_examples': 3003, 'score': 21025.46703028679, 'total_duration': 36223.918417692184, 'accumulated_submission_time': 21025.46703028679, 'accumulated_eval_time': 15194.729263067245, 'accumulated_logging_time': 0.48595476150512695, 'global_step': 60885, 'preemption_count': 0}), (63322, {'train/accuracy': 0.6351779699325562, 'train/loss': 1.7946828603744507, 'train/bleu': 30.574386314501613, 'validation/accuracy': 0.6473685503005981, 'validation/loss': 1.7055493593215942, 'validation/bleu': 26.95875875576747, 'validation/num_examples': 3000, 'test/accuracy': 0.6573282480239868, 'test/loss': 1.6436625719070435, 'test/bleu': 26.007848553564564, 'test/num_examples': 3003, 'score': 21865.603870153427, 'total_duration': 37580.00780367851, 'accumulated_submission_time': 21865.603870153427, 'accumulated_eval_time': 15710.533950567245, 'accumulated_logging_time': 0.5083463191986084, 'global_step': 63322, 'preemption_count': 0}), (65759, {'train/accuracy': 0.628807008266449, 'train/loss': 1.8529696464538574, 'train/bleu': 30.53733152056961, 'validation/accuracy': 0.6511260271072388, 'validation/loss': 1.6819132566452026, 'validation/bleu': 27.34380213844386, 'validation/num_examples': 3000, 'test/accuracy': 0.6610126495361328, 'test/loss': 1.6236516237258911, 'test/bleu': 26.984343736185732, 'test/num_examples': 3003, 'score': 22705.53987812996, 'total_duration': 38969.022357702255, 'accumulated_submission_time': 22705.53987812996, 'accumulated_eval_time': 16259.46607542038, 'accumulated_logging_time': 0.5316293239593506, 'global_step': 65759, 'preemption_count': 0}), (68195, {'train/accuracy': 0.6305742263793945, 'train/loss': 1.8467528820037842, 'train/bleu': 30.146465612794493, 'validation/accuracy': 0.6538823246955872, 'validation/loss': 1.6715095043182373, 'validation/bleu': 27.59234478557179, 'validation/num_examples': 3000, 'test/accuracy': 0.6604912281036377, 'test/loss': 1.6091076135635376, 'test/bleu': 26.510615615856825, 'test/num_examples': 3003, 'score': 23545.424659967422, 'total_duration': 40348.090361595154, 'accumulated_submission_time': 23545.424659967422, 'accumulated_eval_time': 16798.50046467781, 'accumulated_logging_time': 0.5545899868011475, 'global_step': 68195, 'preemption_count': 0}), (70634, {'train/accuracy': 0.6382700204849243, 'train/loss': 1.790703296661377, 'train/bleu': 31.230143414995467, 'validation/accuracy': 0.6538575887680054, 'validation/loss': 1.6620250940322876, 'validation/bleu': 27.728363470018664, 'validation/num_examples': 3000, 'test/accuracy': 0.664940357208252, 'test/loss': 1.5942641496658325, 'test/bleu': 26.830956998584735, 'test/num_examples': 3003, 'score': 24385.58303141594, 'total_duration': 41715.506217479706, 'accumulated_submission_time': 24385.58303141594, 'accumulated_eval_time': 17325.61244392395, 'accumulated_logging_time': 0.5768651962280273, 'global_step': 70634, 'preemption_count': 0}), (73071, {'train/accuracy': 0.6343984007835388, 'train/loss': 1.8140376806259155, 'train/bleu': 31.09243345657882, 'validation/accuracy': 0.654166579246521, 'validation/loss': 1.653039574623108, 'validation/bleu': 27.596476200526222, 'validation/num_examples': 3000, 'test/accuracy': 0.6644073724746704, 'test/loss': 1.5911884307861328, 'test/bleu': 27.248072505544634, 'test/num_examples': 3003, 'score': 25225.748639583588, 'total_duration': 43111.74801707268, 'accumulated_submission_time': 25225.748639583588, 'accumulated_eval_time': 17881.54312467575, 'accumulated_logging_time': 0.5998735427856445, 'global_step': 73071, 'preemption_count': 0}), (75502, {'train/accuracy': 0.6624112129211426, 'train/loss': 1.6078808307647705, 'train/bleu': 32.12475020363412, 'validation/accuracy': 0.6585914492607117, 'validation/loss': 1.6327052116394043, 'validation/bleu': 23.73419533134501, 'validation/num_examples': 3000, 'test/accuracy': 0.6695632338523865, 'test/loss': 1.5611717700958252, 'test/bleu': 25.9650246937535, 'test/num_examples': 3003, 'score': 26065.610825777054, 'total_duration': 44811.56740927696, 'accumulated_submission_time': 26065.610825777054, 'accumulated_eval_time': 18741.350626707077, 'accumulated_logging_time': 0.6278650760650635, 'global_step': 75502, 'preemption_count': 0}), (77930, {'train/accuracy': 0.6410998702049255, 'train/loss': 1.763219952583313, 'train/bleu': 31.804001039784108, 'validation/accuracy': 0.6607297658920288, 'validation/loss': 1.6128979921340942, 'validation/bleu': 28.17692700766994, 'validation/num_examples': 3000, 'test/accuracy': 0.673154890537262, 'test/loss': 1.5473706722259521, 'test/bleu': 28.02593175185072, 'test/num_examples': 3003, 'score': 26905.488362550735, 'total_duration': 46337.54341173172, 'accumulated_submission_time': 26905.488362550735, 'accumulated_eval_time': 19427.298873901367, 'accumulated_logging_time': 0.652367115020752, 'global_step': 77930, 'preemption_count': 0}), (80358, {'train/accuracy': 0.6419423818588257, 'train/loss': 1.7608380317687988, 'train/bleu': 31.83951738685453, 'validation/accuracy': 0.6632140874862671, 'validation/loss': 1.6009656190872192, 'validation/bleu': 28.122468114169163, 'validation/num_examples': 3000, 'test/accuracy': 0.6724481582641602, 'test/loss': 1.5342386960983276, 'test/bleu': 27.5907766150084, 'test/num_examples': 3003, 'score': 27745.4147837162, 'total_duration': 47685.179406404495, 'accumulated_submission_time': 27745.4147837162, 'accumulated_eval_time': 19934.85809111595, 'accumulated_logging_time': 0.6773777008056641, 'global_step': 80358, 'preemption_count': 0}), (82786, {'train/accuracy': 0.6515135765075684, 'train/loss': 1.6875520944595337, 'train/bleu': 32.18715768853201, 'validation/accuracy': 0.6665266156196594, 'validation/loss': 1.5807284116744995, 'validation/bleu': 28.77804683738044, 'validation/num_examples': 3000, 'test/accuracy': 0.6780442595481873, 'test/loss': 1.5070613622665405, 'test/bleu': 28.153702852423912, 'test/num_examples': 3003, 'score': 28585.331007242203, 'total_duration': 49052.888520240784, 'accumulated_submission_time': 28585.331007242203, 'accumulated_eval_time': 20462.50013756752, 'accumulated_logging_time': 0.7066047191619873, 'global_step': 82786, 'preemption_count': 0}), (85213, {'train/accuracy': 0.6519049406051636, 'train/loss': 1.6945043802261353, 'train/bleu': 31.468646398352785, 'validation/accuracy': 0.6677131652832031, 'validation/loss': 1.571462869644165, 'validation/bleu': 28.55693722795598, 'validation/num_examples': 3000, 'test/accuracy': 0.6814737319946289, 'test/loss': 1.4955487251281738, 'test/bleu': 28.17801396797675, 'test/num_examples': 3003, 'score': 29425.292922735214, 'total_duration': 50420.74848651886, 'accumulated_submission_time': 29425.292922735214, 'accumulated_eval_time': 20990.248947143555, 'accumulated_logging_time': 0.7332005500793457, 'global_step': 85213, 'preemption_count': 0}), (87648, {'train/accuracy': 0.6497843861579895, 'train/loss': 1.7081503868103027, 'train/bleu': 32.03949079517517, 'validation/accuracy': 0.6713222861289978, 'validation/loss': 1.5528290271759033, 'validation/bleu': 28.78236413123116, 'validation/num_examples': 3000, 'test/accuracy': 0.6846020221710205, 'test/loss': 1.4713221788406372, 'test/bleu': 28.824666873911248, 'test/num_examples': 3003, 'score': 30265.327246904373, 'total_duration': 51858.419721364975, 'accumulated_submission_time': 30265.327246904373, 'accumulated_eval_time': 21587.73329925537, 'accumulated_logging_time': 0.7593300342559814, 'global_step': 87648, 'preemption_count': 0}), (90089, {'train/accuracy': 0.6603758335113525, 'train/loss': 1.6320950984954834, 'train/bleu': 33.022493754562475, 'validation/accuracy': 0.6735347509384155, 'validation/loss': 1.5366075038909912, 'validation/bleu': 29.04338025849897, 'validation/num_examples': 3000, 'test/accuracy': 0.687788188457489, 'test/loss': 1.4530447721481323, 'test/bleu': 29.20647557025608, 'test/num_examples': 3003, 'score': 31105.30047774315, 'total_duration': 53274.47362399101, 'accumulated_submission_time': 31105.30047774315, 'accumulated_eval_time': 22163.660198688507, 'accumulated_logging_time': 0.7838973999023438, 'global_step': 90089, 'preemption_count': 0}), (92531, {'train/accuracy': 0.659218430519104, 'train/loss': 1.6414427757263184, 'train/bleu': 33.0164677200164, 'validation/accuracy': 0.6762168407440186, 'validation/loss': 1.5189799070358276, 'validation/bleu': 29.464067011339953, 'validation/num_examples': 3000, 'test/accuracy': 0.6881241798400879, 'test/loss': 1.4379405975341797, 'test/bleu': 29.1639306192511, 'test/num_examples': 3003, 'score': 31945.24489593506, 'total_duration': 54642.475838661194, 'accumulated_submission_time': 31945.24489593506, 'accumulated_eval_time': 22691.565865516663, 'accumulated_logging_time': 0.8096988201141357, 'global_step': 92531, 'preemption_count': 0}), (94971, {'train/accuracy': 0.6745937466621399, 'train/loss': 1.538857102394104, 'train/bleu': 33.32081399737227, 'validation/accuracy': 0.6788248419761658, 'validation/loss': 1.5049623250961304, 'validation/bleu': 29.409977472835255, 'validation/num_examples': 3000, 'test/accuracy': 0.691785454750061, 'test/loss': 1.4231317043304443, 'test/bleu': 29.505565406079064, 'test/num_examples': 3003, 'score': 32785.29541873932, 'total_duration': 56045.3659696579, 'accumulated_submission_time': 32785.29541873932, 'accumulated_eval_time': 23254.251985549927, 'accumulated_logging_time': 0.8352725505828857, 'global_step': 94971, 'preemption_count': 0}), (97407, {'train/accuracy': 0.6671216487884521, 'train/loss': 1.5844563245773315, 'train/bleu': 32.968975707503155, 'validation/accuracy': 0.6801226139068604, 'validation/loss': 1.489696741104126, 'validation/bleu': 29.811164994494362, 'validation/num_examples': 3000, 'test/accuracy': 0.6946819424629211, 'test/loss': 1.4053953886032104, 'test/bleu': 29.80798658879084, 'test/num_examples': 3003, 'score': 33625.18694067001, 'total_duration': 57433.75652766228, 'accumulated_submission_time': 33625.18694067001, 'accumulated_eval_time': 23802.60088968277, 'accumulated_logging_time': 0.8610389232635498, 'global_step': 97407, 'preemption_count': 0}), (99843, {'train/accuracy': 0.6677771806716919, 'train/loss': 1.5892866849899292, 'train/bleu': 33.31245207699641, 'validation/accuracy': 0.6835834383964539, 'validation/loss': 1.4707876443862915, 'validation/bleu': 29.856590665690337, 'validation/num_examples': 3000, 'test/accuracy': 0.6989688277244568, 'test/loss': 1.3837393522262573, 'test/bleu': 30.247295532347664, 'test/num_examples': 3003, 'score': 34465.22520542145, 'total_duration': 58852.160405635834, 'accumulated_submission_time': 34465.22520542145, 'accumulated_eval_time': 24380.81174635887, 'accumulated_logging_time': 0.8883967399597168, 'global_step': 99843, 'preemption_count': 0}), (102279, {'train/accuracy': 0.6780392527580261, 'train/loss': 1.5182934999465942, 'train/bleu': 34.08348422046308, 'validation/accuracy': 0.685103714466095, 'validation/loss': 1.4602998495101929, 'validation/bleu': 30.211678343564657, 'validation/num_examples': 3000, 'test/accuracy': 0.7008225917816162, 'test/loss': 1.3738514184951782, 'test/bleu': 30.165329100553144, 'test/num_examples': 3003, 'score': 35305.37013506889, 'total_duration': 60345.209877491, 'accumulated_submission_time': 35305.37013506889, 'accumulated_eval_time': 25033.56376695633, 'accumulated_logging_time': 0.9141156673431396, 'global_step': 102279, 'preemption_count': 0}), (104715, {'train/accuracy': 0.6788626313209534, 'train/loss': 1.5193836688995361, 'train/bleu': 33.33430458049025, 'validation/accuracy': 0.6874274015426636, 'validation/loss': 1.4493452310562134, 'validation/bleu': 30.1511716810412, 'validation/num_examples': 3000, 'test/accuracy': 0.7026647925376892, 'test/loss': 1.3594906330108643, 'test/bleu': 30.611598211686623, 'test/num_examples': 3003, 'score': 36145.506828308105, 'total_duration': 61795.81718158722, 'accumulated_submission_time': 36145.506828308105, 'accumulated_eval_time': 25643.881331205368, 'accumulated_logging_time': 0.9414327144622803, 'global_step': 104715, 'preemption_count': 0}), (107150, {'train/accuracy': 0.6933722496032715, 'train/loss': 1.4291377067565918, 'train/bleu': 35.39502832121977, 'validation/accuracy': 0.6889724135398865, 'validation/loss': 1.4430218935012817, 'validation/bleu': 30.320173706082663, 'validation/num_examples': 3000, 'test/accuracy': 0.7028385996818542, 'test/loss': 1.3506752252578735, 'test/bleu': 30.115848540168745, 'test/num_examples': 3003, 'score': 36985.54924941063, 'total_duration': 63254.38666534424, 'accumulated_submission_time': 36985.54924941063, 'accumulated_eval_time': 26262.25320839882, 'accumulated_logging_time': 0.9684188365936279, 'global_step': 107150, 'preemption_count': 0}), (109587, {'train/accuracy': 0.6888176202774048, 'train/loss': 1.456211805343628, 'train/bleu': 34.63510472800378, 'validation/accuracy': 0.689392626285553, 'validation/loss': 1.4324495792388916, 'validation/bleu': 30.383103394894057, 'validation/num_examples': 3000, 'test/accuracy': 0.7057119607925415, 'test/loss': 1.3385969400405884, 'test/bleu': 30.633743546084894, 'test/num_examples': 3003, 'score': 37825.69719719887, 'total_duration': 64730.85527205467, 'accumulated_submission_time': 37825.69719719887, 'accumulated_eval_time': 26898.419489860535, 'accumulated_logging_time': 0.9965364933013916, 'global_step': 109587, 'preemption_count': 0}), (112021, {'train/accuracy': 0.6880996227264404, 'train/loss': 1.4675003290176392, 'train/bleu': 34.87040125706429, 'validation/accuracy': 0.69106125831604, 'validation/loss': 1.4273666143417358, 'validation/bleu': 30.577266179227504, 'validation/num_examples': 3000, 'test/accuracy': 0.7083072662353516, 'test/loss': 1.3287723064422607, 'test/bleu': 30.795317804768935, 'test/num_examples': 3003, 'score': 38665.6983859539, 'total_duration': 66155.78678846359, 'accumulated_submission_time': 38665.6983859539, 'accumulated_eval_time': 27483.193616867065, 'accumulated_logging_time': 1.0259497165679932, 'global_step': 112021, 'preemption_count': 0}), (114458, {'train/accuracy': 0.693758487701416, 'train/loss': 1.437137484550476, 'train/bleu': 35.204630479308, 'validation/accuracy': 0.6919388175010681, 'validation/loss': 1.423449993133545, 'validation/bleu': 30.83663255069802, 'validation/num_examples': 3000, 'test/accuracy': 0.7082030177116394, 'test/loss': 1.3255749940872192, 'test/bleu': 31.043812284929025, 'test/num_examples': 3003, 'score': 39505.694428920746, 'total_duration': 67631.35035657883, 'accumulated_submission_time': 39505.694428920746, 'accumulated_eval_time': 28118.600239753723, 'accumulated_logging_time': 1.0543212890625, 'global_step': 114458, 'preemption_count': 0}), (116896, {'train/accuracy': 0.6943850517272949, 'train/loss': 1.4304914474487305, 'train/bleu': 35.30651814736856, 'validation/accuracy': 0.6920253038406372, 'validation/loss': 1.4201182126998901, 'validation/bleu': 30.714690992036925, 'validation/num_examples': 3000, 'test/accuracy': 0.7088750004768372, 'test/loss': 1.3214093446731567, 'test/bleu': 31.178693493257015, 'test/num_examples': 3003, 'score': 40345.804631471634, 'total_duration': 69072.42863798141, 'accumulated_submission_time': 40345.804631471634, 'accumulated_eval_time': 28719.409719467163, 'accumulated_logging_time': 1.0852668285369873, 'global_step': 116896, 'preemption_count': 0}), (119335, {'train/accuracy': 0.6937611699104309, 'train/loss': 1.4346632957458496, 'train/bleu': 35.33694094470491, 'validation/accuracy': 0.6923590302467346, 'validation/loss': 1.4187566041946411, 'validation/bleu': 30.709309040437653, 'validation/num_examples': 3000, 'test/accuracy': 0.709176242351532, 'test/loss': 1.3205262422561646, 'test/bleu': 31.16010595361508, 'test/num_examples': 3003, 'score': 41185.99316048622, 'total_duration': 70500.75191736221, 'accumulated_submission_time': 41185.99316048622, 'accumulated_eval_time': 29307.38800597191, 'accumulated_logging_time': 1.1160492897033691, 'global_step': 119335, 'preemption_count': 0}), (121771, {'train/accuracy': 0.6943336725234985, 'train/loss': 1.4281764030456543, 'train/bleu': 35.800121114313306, 'validation/accuracy': 0.6923466920852661, 'validation/loss': 1.4188189506530762, 'validation/bleu': 30.68604794810933, 'validation/num_examples': 3000, 'test/accuracy': 0.7092341780662537, 'test/loss': 1.3205745220184326, 'test/bleu': 31.19492411570843, 'test/num_examples': 3003, 'score': 42025.983906030655, 'total_duration': 71932.01509547234, 'accumulated_submission_time': 42025.983906030655, 'accumulated_eval_time': 29898.50459933281, 'accumulated_logging_time': 1.1442885398864746, 'global_step': 121771, 'preemption_count': 0}), (124208, {'train/accuracy': 0.6964361071586609, 'train/loss': 1.4138400554656982, 'train/bleu': 35.21291588737221, 'validation/accuracy': 0.6923466920852661, 'validation/loss': 1.4188189506530762, 'validation/bleu': 30.68604794810933, 'validation/num_examples': 3000, 'test/accuracy': 0.7092341780662537, 'test/loss': 1.3205745220184326, 'test/bleu': 31.19492411570843, 'test/num_examples': 3003, 'score': 42866.106605529785, 'total_duration': 73368.77992582321, 'accumulated_submission_time': 42866.106605529785, 'accumulated_eval_time': 30494.990439653397, 'accumulated_logging_time': 1.1726155281066895, 'global_step': 124208, 'preemption_count': 0})], 'global_step': 126641}
I0306 15:50:13.980587 139689492575424 submission_runner.py:649] Timing: 43706.23016524315
I0306 15:50:13.980634 139689492575424 submission_runner.py:651] Total number of evals: 52
I0306 15:50:13.980670 139689492575424 submission_runner.py:652] ====================
I0306 15:50:13.980790 139689492575424 submission_runner.py:750] Final wmt score: 3
