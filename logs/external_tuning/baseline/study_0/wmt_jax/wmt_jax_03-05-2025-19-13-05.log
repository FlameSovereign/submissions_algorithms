python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-60001521 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-13-05.log
2025-03-05 19:13:06.816320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201986.838564       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201986.845408       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:13:13.694412 139996361643200 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax.
I0305 19:13:14.615112 139996361643200 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:13:14.618293 139996361643200 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:13:14.620061 139996361643200 submission_runner.py:606] Using RNG seed -60001521
I0305 19:13:15.165373 139996361643200 submission_runner.py:615] --- Tuning run 1/5 ---
I0305 19:13:15.165565 139996361643200 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_1.
I0305 19:13:15.165761 139996361643200 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_1/hparams.json.
I0305 19:13:15.393253 139996361643200 submission_runner.py:218] Initializing dataset.
I0305 19:13:15.571107 139996361643200 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:15.617555 139996361643200 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:15.686547 139996361643200 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:17.062600 139996361643200 submission_runner.py:229] Initializing model.
I0305 19:13:58.719890 139996361643200 submission_runner.py:272] Initializing optimizer.
I0305 19:13:59.589718 139996361643200 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:59.589965 139996361643200 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:59.590960 139996361643200 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_1 with prefix checkpoint_
I0305 19:13:59.591059 139996361643200 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_1/meta_data_0.json.
I0305 19:13:59.591242 139996361643200 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:59.591290 139996361643200 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:59.774852 139996361643200 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_1/flags_0.json.
I0305 19:13:59.814775 139996361643200 submission_runner.py:337] Starting training loop.
I0305 19:14:25.821302 139860216039168 logging_writer.py:48] [0] global_step=0, grad_norm=5.625347137451172, loss=11.09064769744873
I0305 19:14:25.879791 139996361643200 spec.py:321] Evaluating on the training split.
I0305 19:14:25.881949 139996361643200 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:25.885181 139996361643200 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:14:25.917896 139996361643200 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:32.001290 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 19:19:34.107677 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 19:19:34.143967 139996361643200 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:34.156028 139996361643200 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:34.191070 139996361643200 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:39.691489 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 19:24:35.778280 139996361643200 spec.py:349] Evaluating on the test split.
I0305 19:24:35.780647 139996361643200 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:35.784035 139996361643200 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:35.816557 139996361643200 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:38.583136 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 19:29:34.356802 139996361643200 submission_runner.py:469] Time since start: 934.54s, 	Step: 1, 	{'train/accuracy': 0.0005588631611317396, 'train/loss': 11.102604866027832, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.103591918945312, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.07886028289795, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.06490731239319, 'total_duration': 934.5419485569, 'accumulated_submission_time': 26.06490731239319, 'accumulated_eval_time': 908.4769337177277, 'accumulated_logging_time': 0}
I0305 19:29:34.364693 139853345134336 logging_writer.py:48] [1] accumulated_eval_time=908.477, accumulated_logging_time=0, accumulated_submission_time=26.0649, global_step=1, preemption_count=0, score=26.0649, test/accuracy=0.000718341, test/bleu=0, test/loss=11.0789, test/num_examples=3003, total_duration=934.542, train/accuracy=0.000558863, train/bleu=0, train/loss=11.1026, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.1036, validation/num_examples=3000
I0305 19:30:08.769829 139853336741632 logging_writer.py:48] [100] global_step=100, grad_norm=0.3749801218509674, loss=8.920063018798828
I0305 19:30:43.039652 139853345134336 logging_writer.py:48] [200] global_step=200, grad_norm=0.15958960354328156, loss=8.555222511291504
I0305 19:31:17.346085 139853336741632 logging_writer.py:48] [300] global_step=300, grad_norm=0.1657852679491043, loss=8.293856620788574
I0305 19:31:51.665106 139853345134336 logging_writer.py:48] [400] global_step=400, grad_norm=0.2645338773727417, loss=7.971022605895996
I0305 19:32:26.020163 139853336741632 logging_writer.py:48] [500] global_step=500, grad_norm=0.3402465581893921, loss=7.636048793792725
I0305 19:33:00.398994 139853345134336 logging_writer.py:48] [600] global_step=600, grad_norm=0.5179324150085449, loss=7.37168550491333
I0305 19:33:34.781709 139853336741632 logging_writer.py:48] [700] global_step=700, grad_norm=0.35840126872062683, loss=7.113553047180176
I0305 19:34:09.135304 139853345134336 logging_writer.py:48] [800] global_step=800, grad_norm=0.44703736901283264, loss=6.922713279724121
I0305 19:34:43.534768 139853336741632 logging_writer.py:48] [900] global_step=900, grad_norm=0.5724130272865295, loss=6.6938018798828125
I0305 19:35:17.927007 139853345134336 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7048343420028687, loss=6.476399898529053
I0305 19:35:52.318653 139853336741632 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6708053350448608, loss=6.294737815856934
I0305 19:36:26.719064 139853345134336 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7965804934501648, loss=6.0224289894104
I0305 19:37:01.103187 139853336741632 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6960968375205994, loss=5.995628833770752
I0305 19:37:35.520958 139853345134336 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.675888180732727, loss=5.9281439781188965
I0305 19:38:09.922855 139853336741632 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.8198838829994202, loss=5.740182876586914
I0305 19:38:44.324306 139853345134336 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.892192542552948, loss=5.586053848266602
I0305 19:39:18.727169 139853336741632 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5575610399246216, loss=5.442093372344971
I0305 19:39:53.141358 139853345134336 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1655921936035156, loss=5.460797309875488
I0305 19:40:27.560899 139853336741632 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8231369853019714, loss=5.2784295082092285
I0305 19:41:01.941683 139853345134336 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8635149002075195, loss=5.106506824493408
I0305 19:41:36.347042 139853336741632 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7901186943054199, loss=5.123759746551514
I0305 19:42:10.779036 139853345134336 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6220141053199768, loss=4.965320110321045
I0305 19:42:45.236299 139853336741632 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7989294528961182, loss=4.905203819274902
I0305 19:43:19.642641 139853345134336 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.64110267162323, loss=4.798548698425293
I0305 19:43:34.433942 139996361643200 spec.py:321] Evaluating on the training split.
I0305 19:43:37.042659 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 19:47:21.430299 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 19:47:24.014559 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 19:50:47.116224 139996361643200 spec.py:349] Evaluating on the test split.
I0305 19:50:49.703441 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 19:54:41.064514 139996361643200 submission_runner.py:469] Time since start: 2441.25s, 	Step: 2444, 	{'train/accuracy': 0.42766329646110535, 'train/loss': 3.819026470184326, 'train/bleu': 15.501693148832048, 'validation/accuracy': 0.41509902477264404, 'validation/loss': 3.926219940185547, 'validation/bleu': 10.91917970302653, 'validation/num_examples': 3000, 'test/accuracy': 0.4027111530303955, 'test/loss': 4.0999627113342285, 'test/bleu': 9.337357446358707, 'test/num_examples': 3003, 'score': 865.9629728794098, 'total_duration': 2441.249675989151, 'accumulated_submission_time': 865.9629728794098, 'accumulated_eval_time': 1575.1074559688568, 'accumulated_logging_time': 0.0168459415435791}
I0305 19:54:41.073774 139852750235392 logging_writer.py:48] [2444] accumulated_eval_time=1575.11, accumulated_logging_time=0.0168459, accumulated_submission_time=865.963, global_step=2444, preemption_count=0, score=865.963, test/accuracy=0.402711, test/bleu=9.33736, test/loss=4.09996, test/num_examples=3003, total_duration=2441.25, train/accuracy=0.427663, train/bleu=15.5017, train/loss=3.81903, validation/accuracy=0.415099, validation/bleu=10.9192, validation/loss=3.92622, validation/num_examples=3000
I0305 19:55:00.638589 139852741842688 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8827440142631531, loss=4.714670181274414
I0305 19:55:35.018140 139852750235392 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.09148371219635, loss=4.597919940948486
I0305 19:56:09.436115 139852741842688 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.9117668867111206, loss=4.481009483337402
I0305 19:56:43.857330 139852750235392 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7938345074653625, loss=4.513628959655762
I0305 19:57:18.293931 139852741842688 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9371092319488525, loss=4.403025150299072
I0305 19:57:52.752212 139852750235392 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8699654936790466, loss=4.363929748535156
I0305 19:58:27.236108 139852741842688 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7279072403907776, loss=4.370394229888916
I0305 19:59:01.667426 139852750235392 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.6708916425704956, loss=4.235239028930664
I0305 19:59:36.117888 139852741842688 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.7476233839988708, loss=4.114753723144531
I0305 20:00:10.562421 139852750235392 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5767311453819275, loss=4.103443145751953
I0305 20:00:45.017454 139852741842688 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6404235363006592, loss=4.125567436218262
I0305 20:01:19.427298 139852750235392 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6852831244468689, loss=4.084104537963867
I0305 20:01:53.879911 139852741842688 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8173683881759644, loss=3.9584383964538574
I0305 20:02:28.303123 139852750235392 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5511038303375244, loss=3.975205183029175
I0305 20:03:02.738714 139852741842688 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6857252717018127, loss=4.008138179779053
I0305 20:03:37.217576 139852750235392 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6301478743553162, loss=4.004017353057861
I0305 20:04:11.658233 139852741842688 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5507332682609558, loss=3.85345458984375
I0305 20:04:46.094120 139852750235392 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5736106038093567, loss=3.9264109134674072
I0305 20:05:20.518143 139852741842688 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6194254755973816, loss=3.8340210914611816
I0305 20:05:54.972255 139852750235392 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.578784704208374, loss=3.7757456302642822
I0305 20:06:29.401761 139852741842688 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5305565595626831, loss=3.8096742630004883
I0305 20:07:03.831041 139852750235392 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5327343344688416, loss=3.811847448348999
I0305 20:07:38.271469 139852741842688 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6704005599021912, loss=3.8065879344940186
I0305 20:08:12.692540 139852750235392 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5468725562095642, loss=3.688817262649536
I0305 20:08:41.274586 139996361643200 spec.py:321] Evaluating on the training split.
I0305 20:08:43.887874 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:11:35.261493 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 20:11:37.862563 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:14:32.456003 139996361643200 spec.py:349] Evaluating on the test split.
I0305 20:14:35.054876 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:17:15.204209 139996361643200 submission_runner.py:469] Time since start: 3795.39s, 	Step: 4884, 	{'train/accuracy': 0.5439789891242981, 'train/loss': 2.7240521907806396, 'train/bleu': 24.96267639215194, 'validation/accuracy': 0.5521840453147888, 'validation/loss': 2.6358182430267334, 'validation/bleu': 21.00371461439621, 'validation/num_examples': 3000, 'test/accuracy': 0.5547561049461365, 'test/loss': 2.6496100425720215, 'test/bleu': 19.70302729416108, 'test/num_examples': 3003, 'score': 1705.9944531917572, 'total_duration': 3795.3893852233887, 'accumulated_submission_time': 1705.9944531917572, 'accumulated_eval_time': 2089.0370349884033, 'accumulated_logging_time': 0.035944223403930664}
I0305 20:17:15.213707 139852741842688 logging_writer.py:48] [4884] accumulated_eval_time=2089.04, accumulated_logging_time=0.0359442, accumulated_submission_time=1705.99, global_step=4884, preemption_count=0, score=1705.99, test/accuracy=0.554756, test/bleu=19.703, test/loss=2.64961, test/num_examples=3003, total_duration=3795.39, train/accuracy=0.543979, train/bleu=24.9627, train/loss=2.72405, validation/accuracy=0.552184, validation/bleu=21.0037, validation/loss=2.63582, validation/num_examples=3000
I0305 20:17:21.081697 139852750235392 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6343522071838379, loss=3.678628921508789
I0305 20:17:55.406910 139852741842688 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.48947179317474365, loss=3.672438859939575
I0305 20:18:29.838849 139852750235392 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5049419403076172, loss=3.5940182209014893
I0305 20:19:04.319606 139852741842688 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5515735149383545, loss=3.6898248195648193
I0305 20:19:38.773022 139852750235392 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.46915197372436523, loss=3.682532548904419
I0305 20:20:13.219972 139852741842688 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.4624865651130676, loss=3.613119125366211
I0305 20:20:47.632997 139852750235392 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.4694349467754364, loss=3.5716564655303955
I0305 20:21:22.149382 139852741842688 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5156292915344238, loss=3.63678240776062
I0305 20:21:56.590025 139852750235392 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.4866979420185089, loss=3.548466205596924
I0305 20:22:31.004094 139852741842688 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5188256502151489, loss=3.575634717941284
I0305 20:23:05.416682 139852750235392 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4924018383026123, loss=3.5000252723693848
I0305 20:23:39.863337 139852741842688 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5164049863815308, loss=3.481431722640991
I0305 20:24:14.307437 139852750235392 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5002924203872681, loss=3.569223642349243
I0305 20:24:48.733863 139852741842688 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.4562351405620575, loss=3.555039167404175
I0305 20:25:23.094523 139852683093760 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4417337477207184, loss=3.5812110900878906
I0305 20:25:57.413527 139852674701056 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5506235957145691, loss=3.5709874629974365
I0305 20:26:31.742008 139852683093760 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.41247302293777466, loss=3.5235579013824463
I0305 20:27:06.023571 139852674701056 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5136677622795105, loss=3.5624618530273438
I0305 20:27:40.401973 139852683093760 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.397421270608902, loss=3.4242546558380127
I0305 20:28:14.719503 139852674701056 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.3897390365600586, loss=3.573861598968506
I0305 20:28:49.050638 139852683093760 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.39934566617012024, loss=3.524160146713257
I0305 20:29:23.405600 139852674701056 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.39237621426582336, loss=3.393467664718628
I0305 20:29:57.729463 139852683093760 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4504258930683136, loss=3.4668936729431152
I0305 20:30:32.031720 139852674701056 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.38787996768951416, loss=3.356389045715332
I0305 20:31:06.337528 139852683093760 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.40730729699134827, loss=3.4184834957122803
I0305 20:31:15.282994 139996361643200 spec.py:321] Evaluating on the training split.
I0305 20:31:17.885535 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:34:08.807296 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 20:34:11.405532 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:37:10.251848 139996361643200 spec.py:349] Evaluating on the test split.
I0305 20:37:12.863051 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:40:24.438277 139996361643200 submission_runner.py:469] Time since start: 5184.62s, 	Step: 7327, 	{'train/accuracy': 0.5850558280944824, 'train/loss': 2.3225369453430176, 'train/bleu': 27.38493782508135, 'validation/accuracy': 0.592292308807373, 'validation/loss': 2.2635257244110107, 'validation/bleu': 23.775843553381925, 'validation/num_examples': 3000, 'test/accuracy': 0.5948673486709595, 'test/loss': 2.259368419647217, 'test/bleu': 22.278679273909916, 'test/num_examples': 3003, 'score': 2545.8934302330017, 'total_duration': 5184.623443603516, 'accumulated_submission_time': 2545.8934302330017, 'accumulated_eval_time': 2638.192267894745, 'accumulated_logging_time': 0.05415701866149902}
I0305 20:40:24.448917 139852674701056 logging_writer.py:48] [7327] accumulated_eval_time=2638.19, accumulated_logging_time=0.054157, accumulated_submission_time=2545.89, global_step=7327, preemption_count=0, score=2545.89, test/accuracy=0.594867, test/bleu=22.2787, test/loss=2.25937, test/num_examples=3003, total_duration=5184.62, train/accuracy=0.585056, train/bleu=27.3849, train/loss=2.32254, validation/accuracy=0.592292, validation/bleu=23.7758, validation/loss=2.26353, validation/num_examples=3000
I0305 20:40:49.744672 139852683093760 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4908938407897949, loss=3.494650363922119
I0305 20:41:23.976389 139852674701056 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3680989444255829, loss=3.3863589763641357
I0305 20:41:58.264205 139852683093760 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4467593729496002, loss=3.465205192565918
I0305 20:42:32.537727 139852674701056 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.34918317198753357, loss=3.4280295372009277
I0305 20:43:06.871447 139852683093760 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.32933032512664795, loss=3.44262433052063
I0305 20:43:41.179630 139852674701056 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.370809942483902, loss=3.4229819774627686
I0305 20:44:15.487831 139852683093760 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.3418366611003876, loss=3.421191930770874
I0305 20:44:49.773517 139852674701056 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.33760619163513184, loss=3.3730719089508057
I0305 20:45:24.076865 139852683093760 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.33532771468162537, loss=3.409458637237549
I0305 20:45:58.387142 139852674701056 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.34690895676612854, loss=3.3535141944885254
I0305 20:46:32.665241 139852683093760 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.42371001839637756, loss=3.3792641162872314
I0305 20:47:06.974688 139852674701056 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.33723023533821106, loss=3.3461227416992188
I0305 20:47:41.282101 139852683093760 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3864924907684326, loss=3.3315582275390625
I0305 20:48:15.556286 139852674701056 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.30481064319610596, loss=3.3546335697174072
I0305 20:48:49.858043 139852683093760 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3057025372982025, loss=3.341555595397949
I0305 20:49:24.151634 139852674701056 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.30899232625961304, loss=3.3437082767486572
I0305 20:49:58.462633 139852683093760 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3392792046070099, loss=3.4224720001220703
I0305 20:50:32.749818 139852674701056 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.30025964975357056, loss=3.3548595905303955
I0305 20:51:07.042125 139852683093760 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3031369745731354, loss=3.349745988845825
I0305 20:51:41.320638 139852674701056 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3450208008289337, loss=3.366158962249756
I0305 20:52:15.623790 139852683093760 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3229736089706421, loss=3.2577414512634277
I0305 20:52:49.892725 139852674701056 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.29942813515663147, loss=3.2801051139831543
I0305 20:53:24.192272 139852683093760 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3354882299900055, loss=3.300668716430664
I0305 20:53:58.456838 139852674701056 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.27978792786598206, loss=3.297996997833252
I0305 20:54:24.509646 139996361643200 spec.py:321] Evaluating on the training split.
I0305 20:54:27.116845 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:57:14.843203 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 20:57:17.429290 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 20:59:51.010921 139996361643200 spec.py:349] Evaluating on the test split.
I0305 20:59:53.597618 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 21:02:13.091340 139996361643200 submission_runner.py:469] Time since start: 6493.28s, 	Step: 9777, 	{'train/accuracy': 0.5963719487190247, 'train/loss': 2.214189052581787, 'train/bleu': 28.566705599822033, 'validation/accuracy': 0.6121548414230347, 'validation/loss': 2.0879898071289062, 'validation/bleu': 24.896545524690787, 'validation/num_examples': 3000, 'test/accuracy': 0.6181322932243347, 'test/loss': 2.052985906600952, 'test/bleu': 24.018577984426827, 'test/num_examples': 3003, 'score': 3385.793024301529, 'total_duration': 6493.27650642395, 'accumulated_submission_time': 3385.793024301529, 'accumulated_eval_time': 3106.773915052414, 'accumulated_logging_time': 0.07386898994445801}
I0305 21:02:13.102265 139852683093760 logging_writer.py:48] [9777] accumulated_eval_time=3106.77, accumulated_logging_time=0.073869, accumulated_submission_time=3385.79, global_step=9777, preemption_count=0, score=3385.79, test/accuracy=0.618132, test/bleu=24.0186, test/loss=2.05299, test/num_examples=3003, total_duration=6493.28, train/accuracy=0.596372, train/bleu=28.5667, train/loss=2.21419, validation/accuracy=0.612155, validation/bleu=24.8965, validation/loss=2.08799, validation/num_examples=3000
I0305 21:02:21.302445 139852674701056 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.26348182559013367, loss=3.2534451484680176
I0305 21:02:55.408613 139852683093760 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.28758421540260315, loss=3.2888543605804443
I0305 21:03:29.674601 139852674701056 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.26636597514152527, loss=3.3043370246887207
I0305 21:04:03.918226 139852683093760 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.28984126448631287, loss=3.2437140941619873
I0305 21:04:38.177921 139852674701056 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.2576078772544861, loss=3.3199503421783447
I0305 21:05:12.394397 139852683093760 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.26592230796813965, loss=3.28240966796875
I0305 21:05:46.661504 139852674701056 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.3149937093257904, loss=3.2601613998413086
I0305 21:06:20.918230 139852683093760 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.25757983326911926, loss=3.235610008239746
I0305 21:06:55.178071 139852674701056 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.27635127305984497, loss=3.2420365810394287
I0305 21:07:29.441962 139852683093760 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.25462454557418823, loss=3.2501323223114014
I0305 21:08:03.693608 139852674701056 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.25209856033325195, loss=3.291652202606201
I0305 21:08:37.957957 139852683093760 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.24794121086597443, loss=3.1949546337127686
I0305 21:09:12.239498 139852674701056 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.2595963776111603, loss=3.2088565826416016
I0305 21:09:46.502404 139852683093760 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2771795392036438, loss=3.24715518951416
I0305 21:10:20.773945 139852674701056 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.2531231641769409, loss=3.2641146183013916
I0305 21:10:55.061588 139852683093760 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.27953192591667175, loss=3.2403321266174316
I0305 21:11:29.329229 139852674701056 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.2686053514480591, loss=3.2579691410064697
I0305 21:12:03.581005 139852683093760 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3177158534526825, loss=3.1618142127990723
I0305 21:12:37.837222 139852674701056 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.23492173850536346, loss=3.20041561126709
I0305 21:13:12.092277 139852683093760 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.24624593555927277, loss=3.2481188774108887
I0305 21:13:46.336356 139852674701056 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.23933111131191254, loss=3.236293315887451
I0305 21:14:20.597906 139852683093760 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.27357280254364014, loss=3.23703932762146
I0305 21:14:54.843133 139852674701056 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2288503497838974, loss=3.278777837753296
I0305 21:15:29.105875 139852683093760 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.23527486622333527, loss=3.1620020866394043
I0305 21:16:03.381692 139852674701056 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.25933775305747986, loss=3.2833445072174072
I0305 21:16:13.325554 139996361643200 spec.py:321] Evaluating on the training split.
I0305 21:16:15.928444 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 21:19:01.090731 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 21:19:03.686394 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 21:21:34.029481 139996361643200 spec.py:349] Evaluating on the test split.
I0305 21:21:36.624413 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 21:23:51.678313 139996361643200 submission_runner.py:469] Time since start: 7791.86s, 	Step: 12230, 	{'train/accuracy': 0.6070840358734131, 'train/loss': 2.116654634475708, 'train/bleu': 28.913669001751153, 'validation/accuracy': 0.6239463090896606, 'validation/loss': 1.9805448055267334, 'validation/bleu': 25.776371719076966, 'validation/num_examples': 3000, 'test/accuracy': 0.6332406401634216, 'test/loss': 1.9347119331359863, 'test/bleu': 25.131792310691463, 'test/num_examples': 3003, 'score': 4225.8612949848175, 'total_duration': 7791.86346578598, 'accumulated_submission_time': 4225.8612949848175, 'accumulated_eval_time': 3565.1266074180603, 'accumulated_logging_time': 0.09369301795959473}
I0305 21:23:51.688074 139852683093760 logging_writer.py:48] [12230] accumulated_eval_time=3565.13, accumulated_logging_time=0.093693, accumulated_submission_time=4225.86, global_step=12230, preemption_count=0, score=4225.86, test/accuracy=0.633241, test/bleu=25.1318, test/loss=1.93471, test/num_examples=3003, total_duration=7791.86, train/accuracy=0.607084, train/bleu=28.9137, train/loss=2.11665, validation/accuracy=0.623946, validation/bleu=25.7764, validation/loss=1.98054, validation/num_examples=3000
I0305 21:24:15.957385 139852674701056 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.22931896150112152, loss=3.1650612354278564
I0305 21:24:50.149232 139852683093760 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.3355349898338318, loss=3.1571171283721924
I0305 21:25:24.405490 139852674701056 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2663808763027191, loss=3.2069787979125977
I0305 21:25:58.728231 139852573988608 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.28762298822402954, loss=3.15037202835083
I0305 21:26:33.085979 139852565595904 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.2414822280406952, loss=3.178985118865967
I0305 21:27:07.405541 139852573988608 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.2376832365989685, loss=3.2104785442352295
I0305 21:27:41.731340 139852565595904 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.22702866792678833, loss=3.0997402667999268
I0305 21:28:16.066737 139852573988608 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2844560444355011, loss=3.2039542198181152
I0305 21:28:50.398415 139852565595904 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2563188672065735, loss=3.1094634532928467
I0305 21:29:24.716576 139852573988608 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.24280387163162231, loss=3.16715145111084
I0305 21:29:59.066414 139852565595904 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.23187363147735596, loss=3.127582550048828
I0305 21:30:33.386293 139852573988608 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.25546205043792725, loss=3.205138921737671
I0305 21:31:07.707783 139852565595904 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.34848204255104065, loss=3.1188464164733887
I0305 21:31:42.070575 139852573988608 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2431807518005371, loss=3.157071113586426
I0305 21:32:16.421240 139852565595904 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2382206916809082, loss=3.123873233795166
I0305 21:32:50.747804 139852573988608 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.3553166687488556, loss=3.1907453536987305
I0305 21:33:25.055726 139852565595904 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.23669594526290894, loss=3.120582342147827
I0305 21:33:59.388743 139852573988608 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.23473115265369415, loss=3.1197705268859863
I0305 21:34:33.710744 139852565595904 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.34969449043273926, loss=3.1697826385498047
I0305 21:35:08.020632 139852573988608 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2762588858604431, loss=3.19533371925354
I0305 21:35:42.320956 139852565595904 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.2638628482818604, loss=5.257995128631592
I0305 21:36:16.614193 139852573988608 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.4702301621437073, loss=3.5707204341888428
I0305 21:36:50.936438 139852565595904 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.4377744197845459, loss=3.2490103244781494
I0305 21:37:25.236151 139852573988608 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2495414912700653, loss=3.1118762493133545
I0305 21:37:52.012501 139996361643200 spec.py:321] Evaluating on the training split.
I0305 21:37:54.611152 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 21:40:47.251312 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 21:40:49.839669 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 21:43:43.047466 139996361643200 spec.py:349] Evaluating on the test split.
I0305 21:43:45.640022 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 21:46:50.113295 139996361643200 submission_runner.py:469] Time since start: 9170.30s, 	Step: 14679, 	{'train/accuracy': 0.6140821576118469, 'train/loss': 2.0441009998321533, 'train/bleu': 29.339519183401404, 'validation/accuracy': 0.6312881708145142, 'validation/loss': 1.9201366901397705, 'validation/bleu': 26.283284815228836, 'validation/num_examples': 3000, 'test/accuracy': 0.6383153796195984, 'test/loss': 1.8819077014923096, 'test/bleu': 25.38245347735306, 'test/num_examples': 3003, 'score': 5066.032030582428, 'total_duration': 9170.298466205597, 'accumulated_submission_time': 5066.032030582428, 'accumulated_eval_time': 4103.227353334427, 'accumulated_logging_time': 0.11328911781311035}
I0305 21:46:50.124047 139852565595904 logging_writer.py:48] [14679] accumulated_eval_time=4103.23, accumulated_logging_time=0.113289, accumulated_submission_time=5066.03, global_step=14679, preemption_count=0, score=5066.03, test/accuracy=0.638315, test/bleu=25.3825, test/loss=1.88191, test/num_examples=3003, total_duration=9170.3, train/accuracy=0.614082, train/bleu=29.3395, train/loss=2.0441, validation/accuracy=0.631288, validation/bleu=26.2833, validation/loss=1.92014, validation/num_examples=3000
I0305 21:46:57.661649 139852573988608 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.28904613852500916, loss=3.1350979804992676
I0305 21:47:31.923279 139852565595904 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2731741666793823, loss=3.010432243347168
I0305 21:48:06.237959 139852573988608 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.29705411195755005, loss=3.1571030616760254
I0305 21:48:40.557648 139852565595904 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.25571826100349426, loss=3.0657458305358887
I0305 21:49:14.878749 139852573988608 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.22054049372673035, loss=3.0711512565612793
I0305 21:49:49.220478 139852565595904 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.22561044991016388, loss=3.059750556945801
I0305 21:50:23.603148 139852573988608 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.24019768834114075, loss=3.055194139480591
I0305 21:50:57.899432 139852565595904 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.24020108580589294, loss=3.1557629108428955
I0305 21:51:32.231063 139852573988608 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.28323227167129517, loss=3.1851768493652344
I0305 21:52:06.575868 139852565595904 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.27090486884117126, loss=3.0749387741088867
I0305 21:52:40.904157 139852573988608 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.2546766400337219, loss=3.124504566192627
I0305 21:53:15.249115 139852565595904 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2243141382932663, loss=3.0500805377960205
I0305 21:53:49.593193 139852573988608 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.27187293767929077, loss=3.0502007007598877
I0305 21:54:23.912769 139852565595904 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.23166340589523315, loss=3.0585947036743164
I0305 21:54:58.261090 139852573988608 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.23415295779705048, loss=3.0851080417633057
I0305 21:55:32.621507 139852565595904 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.24795083701610565, loss=3.0692667961120605
I0305 21:56:06.959207 139852573988608 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.23779062926769257, loss=3.052320957183838
I0305 21:56:41.288935 139852565595904 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.23838657140731812, loss=3.1267619132995605
I0305 21:57:15.620849 139852573988608 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.33257853984832764, loss=3.1124930381774902
I0305 21:57:49.954117 139852565595904 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2504592537879944, loss=3.162245273590088
I0305 21:58:24.271408 139852573988608 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.2853558361530304, loss=3.0669491291046143
I0305 21:58:58.577521 139852565595904 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3270900845527649, loss=3.069329261779785
I0305 21:59:32.907165 139852573988608 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.28228992223739624, loss=3.0866739749908447
I0305 22:00:07.214421 139852565595904 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.32006365060806274, loss=3.086512565612793
I0305 22:00:41.525277 139852573988608 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2573437988758087, loss=3.101720094680786
I0305 22:00:50.452310 139996361643200 spec.py:321] Evaluating on the training split.
I0305 22:00:53.058421 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:03:33.762862 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 22:03:36.360267 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:06:06.228715 139996361643200 spec.py:349] Evaluating on the test split.
I0305 22:06:08.827217 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:08:27.273279 139996361643200 submission_runner.py:469] Time since start: 10467.46s, 	Step: 17127, 	{'train/accuracy': 0.6137308478355408, 'train/loss': 2.0295145511627197, 'train/bleu': 29.828357931834226, 'validation/accuracy': 0.6393221616744995, 'validation/loss': 1.8558814525604248, 'validation/bleu': 26.933973906139975, 'validation/num_examples': 3000, 'test/accuracy': 0.6484764218330383, 'test/loss': 1.803090214729309, 'test/bleu': 26.389998452299483, 'test/num_examples': 3003, 'score': 5906.204601764679, 'total_duration': 10467.458442687988, 'accumulated_submission_time': 5906.204601764679, 'accumulated_eval_time': 4560.048267841339, 'accumulated_logging_time': 0.1326441764831543}
I0305 22:08:27.284137 139852565595904 logging_writer.py:48] [17127] accumulated_eval_time=4560.05, accumulated_logging_time=0.132644, accumulated_submission_time=5906.2, global_step=17127, preemption_count=0, score=5906.2, test/accuracy=0.648476, test/bleu=26.39, test/loss=1.80309, test/num_examples=3003, total_duration=10467.5, train/accuracy=0.613731, train/bleu=29.8284, train/loss=2.02951, validation/accuracy=0.639322, validation/bleu=26.934, validation/loss=1.85588, validation/num_examples=3000
I0305 22:08:52.539766 139852573988608 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.26370880007743835, loss=3.1493802070617676
I0305 22:09:26.826640 139852565595904 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2532397210597992, loss=3.087372303009033
I0305 22:10:01.112087 139852573988608 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.28458544611930847, loss=3.0201973915100098
I0305 22:10:35.444813 139852565595904 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.2788929343223572, loss=3.0139999389648438
I0305 22:11:09.751427 139852573988608 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.25256016850471497, loss=3.07731294631958
I0305 22:11:44.060245 139852565595904 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.26789578795433044, loss=3.074018716812134
I0305 22:12:18.392479 139852573988608 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.29302290081977844, loss=3.0266499519348145
I0305 22:12:52.775724 139852565595904 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.2716939151287079, loss=3.0979292392730713
I0305 22:13:27.101608 139852573988608 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.26915720105171204, loss=3.1735141277313232
I0305 22:14:01.422532 139852565595904 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.37335094809532166, loss=3.146432399749756
I0305 22:14:35.719302 139852573988608 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.27170807123184204, loss=3.08551025390625
I0305 22:15:10.035074 139852565595904 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.28002944588661194, loss=3.079274892807007
I0305 22:15:44.341452 139852573988608 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.32408520579338074, loss=3.026665687561035
I0305 22:16:18.610791 139852565595904 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.27823835611343384, loss=3.0664255619049072
I0305 22:16:52.920548 139852573988608 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3103259801864624, loss=3.1320228576660156
I0305 22:17:27.211922 139852565595904 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3044435977935791, loss=3.0442535877227783
I0305 22:18:01.539614 139852573988608 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.31335359811782837, loss=3.0228428840637207
I0305 22:18:36.132781 139852565595904 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.26719802618026733, loss=3.129952907562256
I0305 22:19:10.562064 139852573988608 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.34917816519737244, loss=3.075613498687744
I0305 22:19:45.000051 139852565595904 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3011505603790283, loss=3.18327260017395
I0305 22:20:19.430590 139852573988608 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3367112874984741, loss=3.0190491676330566
I0305 22:20:53.862688 139852565595904 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.2761504650115967, loss=3.0480024814605713
I0305 22:21:28.310324 139852573988608 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.27083009481430054, loss=3.0338897705078125
I0305 22:22:02.746122 139852565595904 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3032505512237549, loss=3.014371633529663
I0305 22:22:27.539103 139996361643200 spec.py:321] Evaluating on the training split.
I0305 22:22:30.152038 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:25:15.941088 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 22:25:18.540492 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:27:52.809956 139996361643200 spec.py:349] Evaluating on the test split.
I0305 22:27:55.411727 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:30:23.393277 139996361643200 submission_runner.py:469] Time since start: 11783.58s, 	Step: 19573, 	{'train/accuracy': 0.631948709487915, 'train/loss': 1.8994444608688354, 'train/bleu': 31.340773498976425, 'validation/accuracy': 0.6457246541976929, 'validation/loss': 1.803261637687683, 'validation/bleu': 27.58544274783058, 'validation/num_examples': 3000, 'test/accuracy': 0.6546981930732727, 'test/loss': 1.749428153038025, 'test/bleu': 26.657551100160216, 'test/num_examples': 3003, 'score': 6746.311555862427, 'total_duration': 11783.578448057175, 'accumulated_submission_time': 6746.311555862427, 'accumulated_eval_time': 5035.902393817902, 'accumulated_logging_time': 0.1516437530517578}
I0305 22:30:23.404728 139852573988608 logging_writer.py:48] [19573] accumulated_eval_time=5035.9, accumulated_logging_time=0.151644, accumulated_submission_time=6746.31, global_step=19573, preemption_count=0, score=6746.31, test/accuracy=0.654698, test/bleu=26.6576, test/loss=1.74943, test/num_examples=3003, total_duration=11783.6, train/accuracy=0.631949, train/bleu=31.3408, train/loss=1.89944, validation/accuracy=0.645725, validation/bleu=27.5854, validation/loss=1.80326, validation/num_examples=3000
I0305 22:30:33.004230 139852565595904 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2921845614910126, loss=3.0140559673309326
I0305 22:31:07.366292 139852573988608 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.30759039521217346, loss=3.164687156677246
I0305 22:31:41.789776 139852565595904 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.4145500361919403, loss=3.114577531814575
I0305 22:32:16.237733 139852573988608 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.35581931471824646, loss=3.053900718688965
I0305 22:32:50.711017 139852565595904 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.30004069209098816, loss=3.0454766750335693
I0305 22:33:25.177483 139852573988608 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.4117470383644104, loss=3.017629384994507
I0305 22:33:59.587775 139852565595904 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3006747364997864, loss=3.0640270709991455
I0305 22:34:34.002973 139852573988608 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.343514084815979, loss=2.99104642868042
I0305 22:35:08.429170 139852565595904 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.31792232394218445, loss=3.024496078491211
I0305 22:35:42.879775 139852573988608 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.30277976393699646, loss=3.0753750801086426
I0305 22:36:17.283705 139852565595904 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.36094146966934204, loss=3.102940082550049
I0305 22:36:51.709821 139852573988608 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.40878623723983765, loss=3.0129876136779785
I0305 22:37:26.139113 139852565595904 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.3600655496120453, loss=3.112994432449341
I0305 22:38:00.549660 139852573988608 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3358861207962036, loss=2.984090566635132
I0305 22:38:34.963925 139852565595904 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6359291076660156, loss=3.127014636993408
I0305 22:39:09.324571 139852573988608 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.3595536947250366, loss=3.126575469970703
I0305 22:39:43.738265 139852565595904 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.292432963848114, loss=2.9947004318237305
I0305 22:40:18.157639 139852573988608 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.3238387405872345, loss=2.9673759937286377
I0305 22:40:52.578987 139852565595904 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.37614133954048157, loss=3.0600807666778564
I0305 22:41:27.020225 139852573988608 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.32535865902900696, loss=3.0027849674224854
I0305 22:42:01.480882 139852565595904 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3252565860748291, loss=3.030975103378296
I0305 22:42:35.932014 139852573988608 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.31443312764167786, loss=3.0552291870117188
I0305 22:43:10.414304 139852565595904 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.38038304448127747, loss=3.073378324508667
I0305 22:43:44.869826 139852573988608 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.32630467414855957, loss=2.9886491298675537
I0305 22:44:19.315141 139852565595904 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4425758421421051, loss=3.0365262031555176
I0305 22:44:23.445541 139996361643200 spec.py:321] Evaluating on the training split.
I0305 22:44:26.050393 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:47:21.377809 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 22:47:23.976841 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:49:59.236081 139996361643200 spec.py:349] Evaluating on the test split.
I0305 22:50:01.838760 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 22:52:27.246605 139996361643200 submission_runner.py:469] Time since start: 13107.43s, 	Step: 22013, 	{'train/accuracy': 0.6309760212898254, 'train/loss': 1.919883370399475, 'train/bleu': 29.972452552872173, 'validation/accuracy': 0.6479000449180603, 'validation/loss': 1.7862048149108887, 'validation/bleu': 27.616608391336992, 'validation/num_examples': 3000, 'test/accuracy': 0.6586490869522095, 'test/loss': 1.7273226976394653, 'test/bleu': 26.89697206658344, 'test/num_examples': 3003, 'score': 7586.205486297607, 'total_duration': 13107.431776285172, 'accumulated_submission_time': 7586.205486297607, 'accumulated_eval_time': 5519.703406572342, 'accumulated_logging_time': 0.17116641998291016}
I0305 22:52:27.258038 139852573988608 logging_writer.py:48] [22013] accumulated_eval_time=5519.7, accumulated_logging_time=0.171166, accumulated_submission_time=7586.21, global_step=22013, preemption_count=0, score=7586.21, test/accuracy=0.658649, test/bleu=26.897, test/loss=1.72732, test/num_examples=3003, total_duration=13107.4, train/accuracy=0.630976, train/bleu=29.9725, train/loss=1.91988, validation/accuracy=0.6479, validation/bleu=27.6166, validation/loss=1.7862, validation/num_examples=3000
I0305 22:52:57.462884 139852565595904 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.2936723530292511, loss=3.045551300048828
I0305 22:53:31.822426 139852573988608 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.36125144362449646, loss=3.0899758338928223
I0305 22:54:06.262469 139852565595904 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3365716338157654, loss=2.998434543609619
I0305 22:54:40.694785 139852573988608 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.376314252614975, loss=3.0943570137023926
I0305 22:55:15.136136 139852565595904 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.30685704946517944, loss=3.0115468502044678
I0305 22:55:49.573409 139852573988608 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3930727243423462, loss=3.0592052936553955
I0305 22:56:24.001620 139852565595904 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.39711371064186096, loss=2.972473621368408
I0305 22:56:58.442695 139852573988608 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.33161023259162903, loss=2.956249952316284
I0305 22:57:32.893275 139852565595904 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.386498361825943, loss=3.0341334342956543
I0305 22:58:07.311982 139852573988608 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.3399447500705719, loss=2.982642650604248
I0305 22:58:41.757822 139852565595904 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.31338581442832947, loss=3.049941062927246
I0305 22:59:16.224601 139852573988608 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.30635151267051697, loss=3.030308246612549
I0305 22:59:50.657222 139852565595904 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.31260743737220764, loss=2.9946632385253906
I0305 23:00:25.130284 139852573988608 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.37177878618240356, loss=3.0210111141204834
I0305 23:00:59.565430 139852565595904 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3336889445781708, loss=2.991710901260376
I0305 23:01:34.020523 139852573988608 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.33915975689888, loss=3.084585666656494
I0305 23:02:08.486833 139852565595904 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.554836094379425, loss=3.0434865951538086
I0305 23:02:42.933843 139852573988608 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.8776859045028687, loss=3.27347731590271
I0305 23:03:17.368227 139852565595904 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.45074263215065, loss=3.1000382900238037
I0305 23:03:51.799300 139852573988608 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.3420860469341278, loss=3.0982112884521484
I0305 23:04:26.215801 139852565595904 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3637710213661194, loss=3.021515130996704
I0305 23:05:00.678031 139852573988608 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3440054655075073, loss=2.9926345348358154
I0305 23:05:35.110625 139852565595904 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.32412561774253845, loss=2.9867351055145264
I0305 23:06:09.558444 139852573988608 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.36383000016212463, loss=3.047388792037964
I0305 23:06:27.474750 139996361643200 spec.py:321] Evaluating on the training split.
I0305 23:06:30.090181 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:09:47.871867 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 23:09:50.480858 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:12:26.152385 139996361643200 spec.py:349] Evaluating on the test split.
I0305 23:12:28.759701 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:14:52.601166 139996361643200 submission_runner.py:469] Time since start: 14452.79s, 	Step: 24453, 	{'train/accuracy': 0.630093514919281, 'train/loss': 1.9250246286392212, 'train/bleu': 30.745060661851227, 'validation/accuracy': 0.6523991227149963, 'validation/loss': 1.7541091442108154, 'validation/bleu': 27.896602015347828, 'validation/num_examples': 3000, 'test/accuracy': 0.6636310815811157, 'test/loss': 1.694322943687439, 'test/bleu': 27.200900934566015, 'test/num_examples': 3003, 'score': 8426.276630401611, 'total_duration': 14452.78630566597, 'accumulated_submission_time': 8426.276630401611, 'accumulated_eval_time': 6024.829742908478, 'accumulated_logging_time': 0.19096827507019043}
I0305 23:14:52.612950 139852565595904 logging_writer.py:48] [24453] accumulated_eval_time=6024.83, accumulated_logging_time=0.190968, accumulated_submission_time=8426.28, global_step=24453, preemption_count=0, score=8426.28, test/accuracy=0.663631, test/bleu=27.2009, test/loss=1.69432, test/num_examples=3003, total_duration=14452.8, train/accuracy=0.630094, train/bleu=30.7451, train/loss=1.92502, validation/accuracy=0.652399, validation/bleu=27.8966, validation/loss=1.75411, validation/num_examples=3000
I0305 23:15:09.091145 139852573988608 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3083285093307495, loss=2.9769482612609863
I0305 23:15:43.469892 139852565595904 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.2864457368850708, loss=2.973109483718872
I0305 23:16:17.952315 139852573988608 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.319679319858551, loss=3.0615975856781006
I0305 23:16:52.423060 139852565595904 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3623240888118744, loss=3.05806565284729
I0305 23:17:26.893658 139852573988608 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3491290211677551, loss=2.99216628074646
I0305 23:18:01.358694 139852565595904 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.292447030544281, loss=3.0217509269714355
I0305 23:18:35.831147 139852573988608 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3300141394138336, loss=3.0709307193756104
I0305 23:19:10.168720 139852565595904 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.32103386521339417, loss=3.0222930908203125
I0305 23:19:44.525734 139852573988608 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.31197068095207214, loss=2.935312509536743
I0305 23:20:18.892356 139852565595904 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.35460811853408813, loss=3.047985553741455
I0305 23:20:53.257590 139852573988608 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.33429035544395447, loss=2.999915361404419
I0305 23:21:27.623361 139852565595904 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.37353962659835815, loss=2.978193998336792
I0305 23:22:02.004164 139852573988608 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.37026211619377136, loss=2.9627320766448975
I0305 23:22:36.378469 139852565595904 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.29950401186943054, loss=3.013719320297241
I0305 23:23:10.712249 139852573988608 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.35716214776039124, loss=2.988422393798828
I0305 23:23:45.084113 139852565595904 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.32242926955223083, loss=3.0313923358917236
I0305 23:24:19.443982 139852573988608 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.34710070490837097, loss=3.0122673511505127
I0305 23:24:53.809872 139852565595904 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.34965118765830994, loss=3.108966112136841
I0305 23:25:28.178004 139852573988608 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.3106803894042969, loss=3.024596929550171
I0305 23:26:02.535851 139852565595904 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.3365650475025177, loss=2.997544288635254
I0305 23:26:36.905774 139852573988608 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3340075612068176, loss=3.006476879119873
I0305 23:27:11.239935 139852565595904 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.3613356351852417, loss=3.072263479232788
I0305 23:27:45.607112 139852573988608 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.2786954641342163, loss=2.9600799083709717
I0305 23:28:19.964664 139852565595904 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.31559136509895325, loss=3.120690107345581
I0305 23:28:52.919369 139996361643200 spec.py:321] Evaluating on the training split.
I0305 23:28:55.528677 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:31:48.757648 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 23:31:51.360468 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:34:28.193084 139996361643200 spec.py:349] Evaluating on the test split.
I0305 23:34:30.796177 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:37:01.055336 139996361643200 submission_runner.py:469] Time since start: 15781.24s, 	Step: 26897, 	{'train/accuracy': 0.6316536068916321, 'train/loss': 1.8966907262802124, 'train/bleu': 30.63339271877373, 'validation/accuracy': 0.6530294418334961, 'validation/loss': 1.7591471672058105, 'validation/bleu': 28.001520481179124, 'validation/num_examples': 3000, 'test/accuracy': 0.6632371544837952, 'test/loss': 1.695162296295166, 'test/bleu': 27.010411821341467, 'test/num_examples': 3003, 'score': 9266.43312907219, 'total_duration': 15781.240503549576, 'accumulated_submission_time': 9266.43312907219, 'accumulated_eval_time': 6512.965662956238, 'accumulated_logging_time': 0.21100878715515137}
I0305 23:37:01.068668 139852573988608 logging_writer.py:48] [26897] accumulated_eval_time=6512.97, accumulated_logging_time=0.211009, accumulated_submission_time=9266.43, global_step=26897, preemption_count=0, score=9266.43, test/accuracy=0.663237, test/bleu=27.0104, test/loss=1.69516, test/num_examples=3003, total_duration=15781.2, train/accuracy=0.631654, train/bleu=30.6334, train/loss=1.89669, validation/accuracy=0.653029, validation/bleu=28.0015, validation/loss=1.75915, validation/num_examples=3000
I0305 23:37:02.457151 139852565595904 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.3803703486919403, loss=3.015371322631836
I0305 23:37:36.760371 139852573988608 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3768327832221985, loss=3.045964479446411
I0305 23:38:11.135980 139852565595904 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.3368183374404907, loss=2.9247331619262695
I0305 23:38:45.476729 139852573988608 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.34831130504608154, loss=3.050760269165039
I0305 23:39:19.844863 139852565595904 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5528783798217773, loss=3.003842353820801
I0305 23:39:54.209835 139852573988608 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3270726501941681, loss=2.9217796325683594
I0305 23:40:28.590276 139852565595904 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.3317098021507263, loss=3.0217037200927734
I0305 23:41:02.995930 139852573988608 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.3334157168865204, loss=2.996901035308838
I0305 23:41:37.364703 139852565595904 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.3887633681297302, loss=3.1050004959106445
I0305 23:42:11.723008 139852573988608 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.38910335302352905, loss=3.028346538543701
I0305 23:42:46.097600 139852565595904 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.3233995735645294, loss=3.017838954925537
I0305 23:43:20.456035 139852573988608 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4137265384197235, loss=3.0148520469665527
I0305 23:43:54.800827 139852565595904 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.38453778624534607, loss=2.991960287094116
I0305 23:44:29.128406 139852573988608 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.31578323245048523, loss=2.951113224029541
I0305 23:45:03.446997 139852565595904 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.40845608711242676, loss=3.107396125793457
I0305 23:45:37.768796 139852573988608 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.34212908148765564, loss=2.9536941051483154
I0305 23:46:12.144486 139852565595904 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.34118854999542236, loss=2.9702162742614746
I0305 23:46:46.513616 139852573988608 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.3992234766483307, loss=2.9136922359466553
I0305 23:47:20.833018 139852565595904 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.35577908158302307, loss=2.986731767654419
I0305 23:47:55.200222 139852573988608 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3306821882724762, loss=3.0289626121520996
I0305 23:48:29.572214 139852565595904 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.9242308139801025, loss=3.036315679550171
I0305 23:49:03.944333 139852573988608 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.35009461641311646, loss=2.9984099864959717
I0305 23:49:38.313310 139852565595904 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.35294002294540405, loss=2.928133726119995
I0305 23:50:12.683766 139852573988608 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.29600948095321655, loss=2.992358922958374
I0305 23:50:47.011959 139852565595904 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3819996416568756, loss=2.95308256149292
I0305 23:51:01.110584 139996361643200 spec.py:321] Evaluating on the training split.
I0305 23:51:03.724486 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:53:52.121753 139996361643200 spec.py:333] Evaluating on the validation split.
I0305 23:53:54.719448 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:56:28.135331 139996361643200 spec.py:349] Evaluating on the test split.
I0305 23:56:30.737768 139996361643200 workload.py:181] Translating evaluation dataset.
I0305 23:58:58.303517 139996361643200 submission_runner.py:469] Time since start: 17098.49s, 	Step: 29342, 	{'train/accuracy': 0.6302140355110168, 'train/loss': 1.9148298501968384, 'train/bleu': 31.00624360944345, 'validation/accuracy': 0.6549205183982849, 'validation/loss': 1.7438840866088867, 'validation/bleu': 27.845483539843354, 'validation/num_examples': 3000, 'test/accuracy': 0.665554404258728, 'test/loss': 1.6838868856430054, 'test/bleu': 27.090925099516923, 'test/num_examples': 3003, 'score': 10106.327021360397, 'total_duration': 17098.488652944565, 'accumulated_submission_time': 10106.327021360397, 'accumulated_eval_time': 6990.158514261246, 'accumulated_logging_time': 0.23262643814086914}
I0305 23:58:58.315955 139852573988608 logging_writer.py:48] [29342] accumulated_eval_time=6990.16, accumulated_logging_time=0.232626, accumulated_submission_time=10106.3, global_step=29342, preemption_count=0, score=10106.3, test/accuracy=0.665554, test/bleu=27.0909, test/loss=1.68389, test/num_examples=3003, total_duration=17098.5, train/accuracy=0.630214, train/bleu=31.0062, train/loss=1.91483, validation/accuracy=0.654921, validation/bleu=27.8455, validation/loss=1.74388, validation/num_examples=3000
I0305 23:59:18.508791 139852565595904 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.3804631531238556, loss=2.9560768604278564
I0305 23:59:52.779311 139852573988608 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.34929442405700684, loss=3.0129013061523438
I0306 00:00:27.117455 139852565595904 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.4116222560405731, loss=3.024106025695801
I0306 00:01:01.498232 139852573988608 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.33555975556373596, loss=2.9435510635375977
I0306 00:01:35.875135 139852565595904 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.3308512568473816, loss=3.0412847995758057
I0306 00:02:10.232086 139852573988608 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.38326793909072876, loss=2.9725821018218994
I0306 00:02:44.595911 139852565595904 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.32309702038764954, loss=2.9389867782592773
I0306 00:03:18.912290 139852573988608 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.31922703981399536, loss=2.9951601028442383
I0306 00:03:53.293461 139852565595904 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.34477338194847107, loss=2.931375741958618
I0306 00:04:27.643954 139852573988608 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.48779523372650146, loss=3.060697078704834
I0306 00:05:01.990587 139852565595904 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.33654671907424927, loss=2.985382080078125
I0306 00:05:36.325355 139852573988608 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.30550888180732727, loss=2.9565534591674805
I0306 00:06:10.660011 139852565595904 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.31918540596961975, loss=2.9825804233551025
I0306 00:06:45.036417 139852573988608 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3421890437602997, loss=3.075819253921509
I0306 00:07:19.390169 139852565595904 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.3648001253604889, loss=2.9649269580841064
I0306 00:07:53.730674 139852573988608 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3328714072704315, loss=2.984358787536621
I0306 00:08:28.063035 139852565595904 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.3109854459762573, loss=2.9601690769195557
I0306 00:09:02.388861 139852573988608 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3290962874889374, loss=2.9999289512634277
I0306 00:09:36.728868 139852565595904 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.35386571288108826, loss=2.9801995754241943
I0306 00:10:11.077729 139852573988608 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3342181444168091, loss=2.9144043922424316
I0306 00:10:45.426335 139852565595904 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.4103406071662903, loss=2.9829678535461426
I0306 00:11:19.852186 139852573988608 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3543311357498169, loss=2.9465513229370117
I0306 00:11:54.307226 139852565595904 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.36020219326019287, loss=2.9550652503967285
I0306 00:12:28.738494 139852573988608 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.320843368768692, loss=3.025771379470825
I0306 00:12:58.330669 139996361643200 spec.py:321] Evaluating on the training split.
I0306 00:13:00.937994 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 00:15:49.825353 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 00:15:52.418516 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 00:18:37.059638 139996361643200 spec.py:349] Evaluating on the test split.
I0306 00:18:39.650513 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 00:21:29.134688 139996361643200 submission_runner.py:469] Time since start: 18449.32s, 	Step: 31787, 	{'train/accuracy': 0.6535369753837585, 'train/loss': 1.761945366859436, 'train/bleu': 31.64568627625226, 'validation/accuracy': 0.6559464335441589, 'validation/loss': 1.7315243482589722, 'validation/bleu': 28.34224495962689, 'validation/num_examples': 3000, 'test/accuracy': 0.6664465069770813, 'test/loss': 1.6702311038970947, 'test/bleu': 27.54304371803419, 'test/num_examples': 3003, 'score': 10946.194816350937, 'total_duration': 18449.31985449791, 'accumulated_submission_time': 10946.194816350937, 'accumulated_eval_time': 7500.962494134903, 'accumulated_logging_time': 0.2536015510559082}
I0306 00:21:29.148055 139852565595904 logging_writer.py:48] [31787] accumulated_eval_time=7500.96, accumulated_logging_time=0.253602, accumulated_submission_time=10946.2, global_step=31787, preemption_count=0, score=10946.2, test/accuracy=0.666447, test/bleu=27.543, test/loss=1.67023, test/num_examples=3003, total_duration=18449.3, train/accuracy=0.653537, train/bleu=31.6457, train/loss=1.76195, validation/accuracy=0.655946, validation/bleu=28.3422, validation/loss=1.73152, validation/num_examples=3000
I0306 00:21:33.965396 139852573988608 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3899119198322296, loss=2.984909772872925
I0306 00:22:08.303516 139852565595904 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3612368404865265, loss=2.959292411804199
I0306 00:22:42.717467 139852573988608 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3585548400878906, loss=2.9904539585113525
I0306 00:23:17.140267 139852565595904 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.35619258880615234, loss=2.924806833267212
I0306 00:23:51.579985 139852573988608 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.35068976879119873, loss=2.989635705947876
I0306 00:24:26.034658 139852565595904 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3738623559474945, loss=2.985769033432007
I0306 00:25:00.474826 139852573988608 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3864929974079132, loss=2.89920973777771
I0306 00:25:34.903520 139852565595904 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.40558215975761414, loss=2.951660633087158
I0306 00:26:09.319260 139852573988608 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.4157857298851013, loss=3.006521463394165
I0306 00:26:43.752314 139852565595904 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3689032196998596, loss=2.927043914794922
I0306 00:27:18.163079 139852573988608 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.32472318410873413, loss=2.9255802631378174
I0306 00:27:52.617298 139852565595904 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.29836252331733704, loss=2.979426383972168
I0306 00:28:27.018247 139852573988608 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3280528783798218, loss=2.9126548767089844
I0306 00:29:01.415623 139852565595904 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.35510143637657166, loss=2.8949053287506104
I0306 00:29:35.883316 139852573988608 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.3595644533634186, loss=3.0187742710113525
I0306 00:30:10.277836 139852565595904 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.31497833132743835, loss=2.92960262298584
I0306 00:30:44.651875 139852573988608 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2957890033721924, loss=2.8656420707702637
I0306 00:31:19.063056 139852565595904 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.391917884349823, loss=2.98266863822937
I0306 00:31:53.438436 139852573988608 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.37933802604675293, loss=2.9880924224853516
I0306 00:32:27.825627 139852565595904 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.3440961539745331, loss=2.9451470375061035
I0306 00:33:02.194660 139852573988608 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.39366060495376587, loss=2.9877614974975586
I0306 00:33:36.560438 139852565595904 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.35774362087249756, loss=2.9523959159851074
I0306 00:34:10.903430 139852573988608 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3828445076942444, loss=2.975701093673706
I0306 00:34:45.235550 139852565595904 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.35372647643089294, loss=2.998913526535034
I0306 00:35:19.522096 139852573988608 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.35417962074279785, loss=2.9879841804504395
I0306 00:35:29.469930 139996361643200 spec.py:321] Evaluating on the training split.
I0306 00:35:32.072734 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 00:38:40.669541 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 00:38:43.269321 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 00:41:15.413102 139996361643200 spec.py:349] Evaluating on the test split.
I0306 00:41:18.000283 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 00:43:43.470159 139996361643200 submission_runner.py:469] Time since start: 19783.66s, 	Step: 34230, 	{'train/accuracy': 0.6368748545646667, 'train/loss': 1.8621817827224731, 'train/bleu': 31.009493052839204, 'validation/accuracy': 0.6566632986068726, 'validation/loss': 1.7261155843734741, 'validation/bleu': 28.147480755086054, 'validation/num_examples': 3000, 'test/accuracy': 0.6686478853225708, 'test/loss': 1.6565425395965576, 'test/bleu': 27.698284666225053, 'test/num_examples': 3003, 'score': 11786.37015247345, 'total_duration': 19783.655319690704, 'accumulated_submission_time': 11786.37015247345, 'accumulated_eval_time': 7994.962661981583, 'accumulated_logging_time': 0.27576756477355957}
I0306 00:43:43.485242 139852565595904 logging_writer.py:48] [34230] accumulated_eval_time=7994.96, accumulated_logging_time=0.275768, accumulated_submission_time=11786.4, global_step=34230, preemption_count=0, score=11786.4, test/accuracy=0.668648, test/bleu=27.6983, test/loss=1.65654, test/num_examples=3003, total_duration=19783.7, train/accuracy=0.636875, train/bleu=31.0095, train/loss=1.86218, validation/accuracy=0.656663, validation/bleu=28.1475, validation/loss=1.72612, validation/num_examples=3000
I0306 00:44:07.753300 139852573988608 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.34005266427993774, loss=2.984539270401001
I0306 00:44:41.983089 139852565595904 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.40180328488349915, loss=2.9938788414001465
I0306 00:45:16.291978 139852573988608 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.3100228011608124, loss=2.970250368118286
I0306 00:45:50.636136 139852565595904 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.31981173157691956, loss=2.9982399940490723
I0306 00:46:24.943453 139852573988608 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3936971426010132, loss=2.9968080520629883
I0306 00:46:59.276124 139852565595904 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.35127440094947815, loss=2.9660356044769287
I0306 00:47:33.606189 139852573988608 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.36066848039627075, loss=2.924088478088379
I0306 00:48:07.920942 139852565595904 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3767944276332855, loss=3.069676399230957
I0306 00:48:42.249290 139852573988608 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.38313016295433044, loss=2.89107084274292
I0306 00:49:16.573846 139852565595904 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3323836624622345, loss=2.9598495960235596
I0306 00:49:50.863197 139852573988608 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.4285256266593933, loss=2.907907724380493
I0306 00:50:25.170523 139852565595904 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.34178590774536133, loss=2.9860541820526123
I0306 00:50:59.525461 139852573988608 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3571182191371918, loss=2.98858642578125
I0306 00:51:33.825963 139852565595904 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3735547661781311, loss=2.9558486938476562
I0306 00:52:08.113313 139852573988608 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.3544764816761017, loss=2.9114654064178467
I0306 00:52:42.444441 139852565595904 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.32707878947257996, loss=2.9934072494506836
I0306 00:53:16.767174 139852573988608 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.3482990562915802, loss=2.9850001335144043
I0306 00:53:51.100053 139852565595904 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3360948860645294, loss=2.928999900817871
I0306 00:54:25.385940 139852573988608 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.34874099493026733, loss=2.8830440044403076
I0306 00:54:59.688175 139852565595904 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.3282310962677002, loss=2.939037322998047
I0306 00:55:33.980058 139852573988608 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.37801483273506165, loss=2.9954607486724854
I0306 00:56:08.264034 139852565595904 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3243028521537781, loss=2.880033016204834
I0306 00:56:42.572100 139852573988608 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.3705441653728485, loss=2.925640821456909
I0306 00:57:16.895650 139852565595904 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.36966580152511597, loss=2.9571120738983154
I0306 00:57:43.684243 139996361643200 spec.py:321] Evaluating on the training split.
I0306 00:57:46.279695 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:00:42.818759 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 01:00:45.414411 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:03:18.767347 139996361643200 spec.py:349] Evaluating on the test split.
I0306 01:03:21.352710 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:05:46.312231 139996361643200 submission_runner.py:469] Time since start: 21106.50s, 	Step: 36679, 	{'train/accuracy': 0.6393715143203735, 'train/loss': 1.8610005378723145, 'train/bleu': 31.27685248618663, 'validation/accuracy': 0.6574667096138, 'validation/loss': 1.7112897634506226, 'validation/bleu': 28.119425215404167, 'validation/num_examples': 3000, 'test/accuracy': 0.6690070629119873, 'test/loss': 1.6529209613800049, 'test/bleu': 27.555001614043974, 'test/num_examples': 3003, 'score': 12626.421940803528, 'total_duration': 21106.497403621674, 'accumulated_submission_time': 12626.421940803528, 'accumulated_eval_time': 8477.59060049057, 'accumulated_logging_time': 0.29990553855895996}
I0306 01:05:46.325172 139852573988608 logging_writer.py:48] [36679] accumulated_eval_time=8477.59, accumulated_logging_time=0.299906, accumulated_submission_time=12626.4, global_step=36679, preemption_count=0, score=12626.4, test/accuracy=0.669007, test/bleu=27.555, test/loss=1.65292, test/num_examples=3003, total_duration=21106.5, train/accuracy=0.639372, train/bleu=31.2769, train/loss=1.861, validation/accuracy=0.657467, validation/bleu=28.1194, validation/loss=1.71129, validation/num_examples=3000
I0306 01:05:53.854793 139852565595904 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.370932936668396, loss=3.0089528560638428
I0306 01:06:28.068014 139852573988608 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.311258465051651, loss=2.942221164703369
I0306 01:07:02.373451 139852565595904 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3333262503147125, loss=2.9530529975891113
I0306 01:07:36.670382 139852573988608 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.36586591601371765, loss=2.9172658920288086
I0306 01:08:10.982562 139852565595904 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3410812318325043, loss=2.9281351566314697
I0306 01:08:45.296395 139852573988608 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.3551061153411865, loss=2.9830429553985596
I0306 01:09:19.593231 139852565595904 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.3348829746246338, loss=2.9281809329986572
I0306 01:09:53.902645 139852573988608 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3384472727775574, loss=2.953824043273926
I0306 01:10:28.180779 139852565595904 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.37553712725639343, loss=2.997404098510742
I0306 01:11:02.464780 139852573988608 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.37138327956199646, loss=3.0435116291046143
I0306 01:11:36.789239 139852565595904 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.3231467306613922, loss=2.929521322250366
I0306 01:12:11.052125 139852573988608 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3736482262611389, loss=3.0266001224517822
I0306 01:12:45.294327 139852565595904 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.36920660734176636, loss=2.9063048362731934
I0306 01:13:19.545214 139852573988608 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3392621874809265, loss=3.042170286178589
I0306 01:13:53.833686 139852565595904 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3546159565448761, loss=2.965481996536255
I0306 01:14:28.107686 139852573988608 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3655443489551544, loss=3.01809024810791
I0306 01:15:02.377957 139852565595904 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.345150351524353, loss=2.990812301635742
I0306 01:15:36.644112 139852573988608 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.3248050808906555, loss=2.977621555328369
I0306 01:16:10.897016 139852565595904 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.41205641627311707, loss=2.9388136863708496
I0306 01:16:45.156783 139852573988608 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.3667972683906555, loss=2.9321186542510986
I0306 01:17:19.460343 139852565595904 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3372921347618103, loss=2.947585344314575
I0306 01:17:53.777259 139852573988608 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.373003751039505, loss=2.9296469688415527
I0306 01:18:28.040954 139852565595904 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.3505295217037201, loss=2.9397029876708984
I0306 01:19:02.298767 139852573988608 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3787294030189514, loss=2.9740302562713623
I0306 01:19:36.570920 139852565595904 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.36327672004699707, loss=2.9886059761047363
I0306 01:19:46.502933 139996361643200 spec.py:321] Evaluating on the training split.
I0306 01:19:49.097260 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:22:31.520402 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 01:22:34.110825 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:25:09.896228 139996361643200 spec.py:349] Evaluating on the test split.
I0306 01:25:12.488168 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:27:46.561838 139996361643200 submission_runner.py:469] Time since start: 22426.75s, 	Step: 39130, 	{'train/accuracy': 0.6414465308189392, 'train/loss': 1.8317885398864746, 'train/bleu': 31.235375877789163, 'validation/accuracy': 0.6605443358421326, 'validation/loss': 1.6982654333114624, 'validation/bleu': 28.548513068567534, 'validation/num_examples': 3000, 'test/accuracy': 0.6736183762550354, 'test/loss': 1.632981777191162, 'test/bleu': 27.747848323361666, 'test/num_examples': 3003, 'score': 13466.453862190247, 'total_duration': 22426.746995210648, 'accumulated_submission_time': 13466.453862190247, 'accumulated_eval_time': 8957.649440050125, 'accumulated_logging_time': 0.3215906620025635}
I0306 01:27:46.574312 139852573988608 logging_writer.py:48] [39130] accumulated_eval_time=8957.65, accumulated_logging_time=0.321591, accumulated_submission_time=13466.5, global_step=39130, preemption_count=0, score=13466.5, test/accuracy=0.673618, test/bleu=27.7478, test/loss=1.63298, test/num_examples=3003, total_duration=22426.7, train/accuracy=0.641447, train/bleu=31.2354, train/loss=1.83179, validation/accuracy=0.660544, validation/bleu=28.5485, validation/loss=1.69827, validation/num_examples=3000
I0306 01:28:10.799340 139852565595904 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.35984817147254944, loss=2.931666374206543
I0306 01:28:44.998837 139852573988608 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.3945980966091156, loss=3.0337791442871094
I0306 01:29:19.241420 139852565595904 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.31087005138397217, loss=2.950753927230835
I0306 01:29:53.520816 139852573988608 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.3693709373474121, loss=2.9698143005371094
I0306 01:30:27.787425 139852565595904 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3995179831981659, loss=2.888890504837036
I0306 01:31:02.056197 139852573988608 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.3000963032245636, loss=2.9440133571624756
I0306 01:31:36.354378 139852565595904 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.3719664216041565, loss=2.9005208015441895
I0306 01:32:10.657922 139852573988608 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.32781335711479187, loss=2.9105637073516846
I0306 01:32:44.932856 139852565595904 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.3632642328739166, loss=2.9683849811553955
I0306 01:33:19.192260 139852573988608 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.37378889322280884, loss=2.9983150959014893
I0306 01:33:53.434542 139852565595904 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3827182352542877, loss=2.9945058822631836
I0306 01:34:27.678765 139852573988608 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.37681519985198975, loss=2.9835917949676514
I0306 01:35:01.957945 139852565595904 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.3537825644016266, loss=2.9614832401275635
I0306 01:35:36.183178 139852573988608 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3106129467487335, loss=2.8988900184631348
I0306 01:36:10.454821 139852565595904 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3715526759624481, loss=2.92429780960083
I0306 01:36:44.733619 139852573988608 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.34398767352104187, loss=2.971014976501465
I0306 01:37:18.988194 139852565595904 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.29954496026039124, loss=2.9097304344177246
I0306 01:37:53.239770 139852573988608 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3239282965660095, loss=2.9762473106384277
I0306 01:38:27.516499 139852565595904 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.35700833797454834, loss=2.8893117904663086
I0306 01:39:01.768231 139852573988608 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.31771373748779297, loss=2.9083125591278076
I0306 01:39:36.040237 139852565595904 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.34043338894844055, loss=2.907799005508423
I0306 01:40:10.304954 139852573988608 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3489179313182831, loss=2.9171218872070312
I0306 01:40:44.579834 139852565595904 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.34959110617637634, loss=2.947810173034668
I0306 01:41:18.848178 139852573988608 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.4209452271461487, loss=3.027055263519287
I0306 01:41:46.612060 139996361643200 spec.py:321] Evaluating on the training split.
I0306 01:41:49.204975 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:44:32.851307 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 01:44:35.442841 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:47:03.317960 139996361643200 spec.py:349] Evaluating on the test split.
I0306 01:47:05.923100 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 01:49:24.996565 139996361643200 submission_runner.py:469] Time since start: 23725.18s, 	Step: 41582, 	{'train/accuracy': 0.639885663986206, 'train/loss': 1.8503295183181763, 'train/bleu': 31.52679706986065, 'validation/accuracy': 0.6605443358421326, 'validation/loss': 1.6916389465332031, 'validation/bleu': 28.751247971351795, 'validation/num_examples': 3000, 'test/accuracy': 0.6760166883468628, 'test/loss': 1.6236522197723389, 'test/bleu': 28.27465753841795, 'test/num_examples': 3003, 'score': 14306.343038082123, 'total_duration': 23725.181736946106, 'accumulated_submission_time': 14306.343038082123, 'accumulated_eval_time': 9416.03389787674, 'accumulated_logging_time': 0.3431529998779297}
I0306 01:49:25.010524 139852565595904 logging_writer.py:48] [41582] accumulated_eval_time=9416.03, accumulated_logging_time=0.343153, accumulated_submission_time=14306.3, global_step=41582, preemption_count=0, score=14306.3, test/accuracy=0.676017, test/bleu=28.2747, test/loss=1.62365, test/num_examples=3003, total_duration=23725.2, train/accuracy=0.639886, train/bleu=31.5268, train/loss=1.85033, validation/accuracy=0.660544, validation/bleu=28.7512, validation/loss=1.69164, validation/num_examples=3000
I0306 01:49:31.516864 139852573988608 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.3563057482242584, loss=2.950437068939209
I0306 01:50:05.699057 139852565595904 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.34890174865722656, loss=2.9470577239990234
I0306 01:50:39.918499 139852573988608 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.3958984911441803, loss=2.982529401779175
I0306 01:51:14.187247 139852565595904 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.34945136308670044, loss=2.8506860733032227
I0306 01:51:48.440669 139852573988608 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.3363741338253021, loss=2.936690330505371
I0306 01:52:22.708021 139852565595904 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.3621861934661865, loss=2.9765567779541016
I0306 01:52:56.937627 139852573988608 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3227839767932892, loss=2.9374794960021973
I0306 01:53:31.213301 139852565595904 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3094176948070526, loss=2.9148008823394775
I0306 01:54:05.481766 139852573988608 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3476927578449249, loss=2.9478421211242676
I0306 01:54:39.724854 139852565595904 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.30681726336479187, loss=2.878380298614502
I0306 01:55:13.992416 139852573988608 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.37149810791015625, loss=2.9115984439849854
I0306 01:55:48.228144 139852565595904 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.35812488198280334, loss=2.9085683822631836
I0306 01:56:22.530557 139852573988608 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.34457308053970337, loss=2.9507815837860107
I0306 01:56:56.887752 139852565595904 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.3600955009460449, loss=2.951967716217041
I0306 01:57:31.200726 139852573988608 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3407314717769623, loss=2.9471487998962402
I0306 01:58:05.480152 139852565595904 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.31344321370124817, loss=2.984849214553833
I0306 01:58:39.790052 139852573988608 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.3370888829231262, loss=2.8910820484161377
I0306 01:59:14.046225 139852565595904 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3657327890396118, loss=2.9056456089019775
I0306 01:59:48.301495 139852573988608 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3542431890964508, loss=2.9286696910858154
I0306 02:00:22.545032 139852565595904 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.35925284028053284, loss=3.002546548843384
I0306 02:00:56.806363 139852573988608 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.3244019150733948, loss=2.8980445861816406
I0306 02:01:31.070605 139852565595904 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.3538114130496979, loss=2.9590516090393066
I0306 02:02:05.317446 139852573988608 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3721672296524048, loss=2.933936834335327
I0306 02:02:39.566312 139852565595904 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3478531539440155, loss=2.8950984477996826
I0306 02:03:13.875164 139852573988608 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.37282437086105347, loss=2.997310161590576
I0306 02:03:25.178612 139996361643200 spec.py:321] Evaluating on the training split.
I0306 02:03:27.777698 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:06:30.865222 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 02:06:33.458879 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:08:59.729717 139996361643200 spec.py:349] Evaluating on the test split.
I0306 02:09:02.323236 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:11:23.205003 139996361643200 submission_runner.py:469] Time since start: 25043.39s, 	Step: 44034, 	{'train/accuracy': 0.6724395751953125, 'train/loss': 1.6391425132751465, 'train/bleu': 33.943103371507874, 'validation/accuracy': 0.6622500419616699, 'validation/loss': 1.680512547492981, 'validation/bleu': 28.57360978446224, 'validation/num_examples': 3000, 'test/accuracy': 0.6769088506698608, 'test/loss': 1.6194344758987427, 'test/bleu': 28.12031173168794, 'test/num_examples': 3003, 'score': 15146.35967206955, 'total_duration': 25043.39017677307, 'accumulated_submission_time': 15146.35967206955, 'accumulated_eval_time': 9894.060240268707, 'accumulated_logging_time': 0.3657071590423584}
I0306 02:11:23.218074 139852565595904 logging_writer.py:48] [44034] accumulated_eval_time=9894.06, accumulated_logging_time=0.365707, accumulated_submission_time=15146.4, global_step=44034, preemption_count=0, score=15146.4, test/accuracy=0.676909, test/bleu=28.1203, test/loss=1.61943, test/num_examples=3003, total_duration=25043.4, train/accuracy=0.67244, train/bleu=33.9431, train/loss=1.63914, validation/accuracy=0.66225, validation/bleu=28.5736, validation/loss=1.68051, validation/num_examples=3000
I0306 02:11:46.105876 139852573988608 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.3378949761390686, loss=2.8924877643585205
I0306 02:12:20.337899 139852565595904 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.37393122911453247, loss=2.97188663482666
I0306 02:12:54.648080 139852573988608 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.34177711606025696, loss=2.8970508575439453
I0306 02:13:28.968788 139852565595904 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3512398898601532, loss=2.8556666374206543
I0306 02:14:03.240314 139852573988608 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.34308549761772156, loss=2.932666063308716
I0306 02:14:37.547544 139852565595904 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.3491489589214325, loss=2.9734580516815186
I0306 02:15:11.835543 139852573988608 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.30720871686935425, loss=2.852736711502075
I0306 02:15:46.159413 139852565595904 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.33354198932647705, loss=2.9938199520111084
I0306 02:16:20.465686 139852573988608 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.3591657876968384, loss=2.951273202896118
I0306 02:16:54.772833 139852565595904 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3378308117389679, loss=2.851552963256836
I0306 02:17:29.068953 139852573988608 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.32349124550819397, loss=2.8615543842315674
I0306 02:18:03.372689 139852565595904 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3381105065345764, loss=2.9094483852386475
I0306 02:18:37.652693 139852573988608 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3482424020767212, loss=3.0190422534942627
I0306 02:19:11.969148 139852565595904 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3485238254070282, loss=2.8959407806396484
I0306 02:19:46.282077 139852573988608 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.3667939007282257, loss=2.9809346199035645
I0306 02:20:20.596359 139852565595904 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3471410274505615, loss=2.910867214202881
I0306 02:20:54.882852 139852573988608 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.3646843433380127, loss=2.9201953411102295
I0306 02:21:29.205991 139852565595904 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.42635539174079895, loss=2.9856839179992676
I0306 02:22:03.501327 139852573988608 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.39356616139411926, loss=2.978156089782715
I0306 02:22:37.783492 139852565595904 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3343062400817871, loss=2.9244420528411865
I0306 02:23:12.099231 139852573988608 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3732849657535553, loss=2.9152419567108154
I0306 02:23:46.421954 139852565595904 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.35363489389419556, loss=3.049271821975708
I0306 02:24:20.743690 139852573988608 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.36596888303756714, loss=2.918179988861084
I0306 02:24:55.071126 139852565595904 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3526499569416046, loss=2.9227395057678223
I0306 02:25:23.545866 139996361643200 spec.py:321] Evaluating on the training split.
I0306 02:25:26.145001 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:28:18.826986 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 02:28:21.415546 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:30:49.285275 139996361643200 spec.py:349] Evaluating on the test split.
I0306 02:30:51.880556 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:33:23.360298 139996361643200 submission_runner.py:469] Time since start: 26363.55s, 	Step: 46484, 	{'train/accuracy': 0.6441134214401245, 'train/loss': 1.818184494972229, 'train/bleu': 31.529587493624923, 'validation/accuracy': 0.6635107398033142, 'validation/loss': 1.6775881052017212, 'validation/bleu': 28.565059684854546, 'validation/num_examples': 3000, 'test/accuracy': 0.6762484312057495, 'test/loss': 1.6067540645599365, 'test/bleu': 28.323112699366018, 'test/num_examples': 3003, 'score': 15986.541936635971, 'total_duration': 26363.545403957367, 'accumulated_submission_time': 15986.541936635971, 'accumulated_eval_time': 10373.874564409256, 'accumulated_logging_time': 0.3871452808380127}
I0306 02:33:23.373313 139852573988608 logging_writer.py:48] [46484] accumulated_eval_time=10373.9, accumulated_logging_time=0.387145, accumulated_submission_time=15986.5, global_step=46484, preemption_count=0, score=15986.5, test/accuracy=0.676248, test/bleu=28.3231, test/loss=1.60675, test/num_examples=3003, total_duration=26363.5, train/accuracy=0.644113, train/bleu=31.5296, train/loss=1.81818, validation/accuracy=0.663511, validation/bleu=28.5651, validation/loss=1.67759, validation/num_examples=3000
I0306 02:33:29.193407 139852565595904 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.37158405780792236, loss=2.9488587379455566
I0306 02:34:03.405022 139852573988608 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.34136778116226196, loss=2.9050815105438232
I0306 02:34:37.707030 139852565595904 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3679496645927429, loss=2.961477518081665
I0306 02:35:12.039814 139852573988608 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.36700090765953064, loss=2.944024085998535
I0306 02:35:46.354229 139852565595904 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.32958143949508667, loss=3.0066912174224854
I0306 02:36:20.693711 139852573988608 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.36405473947525024, loss=2.9738552570343018
I0306 02:36:55.007513 139852565595904 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.38007113337516785, loss=2.909823417663574
I0306 02:37:29.327512 139852573988608 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.39201071858406067, loss=2.9853804111480713
I0306 02:38:03.609079 139852565595904 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.38290920853614807, loss=2.9638664722442627
I0306 02:38:37.906377 139852573988608 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3371107578277588, loss=2.889453172683716
I0306 02:39:12.223175 139852565595904 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3381304442882538, loss=2.8904454708099365
I0306 02:39:46.539204 139852573988608 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.4767257869243622, loss=3.019026517868042
I0306 02:40:20.810436 139852565595904 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.321256160736084, loss=2.949441432952881
I0306 02:40:55.121166 139852573988608 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.329250305891037, loss=2.930997371673584
I0306 02:41:29.410781 139852565595904 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3741791546344757, loss=2.925264358520508
I0306 02:42:03.749625 139852573988608 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.3655683696269989, loss=2.9574170112609863
I0306 02:42:38.033027 139852565595904 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.34101349115371704, loss=2.909245729446411
I0306 02:43:12.361840 139852573988608 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.38560500741004944, loss=2.9776992797851562
I0306 02:43:46.743273 139852565595904 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.361515074968338, loss=2.988754987716675
I0306 02:44:21.050318 139852573988608 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.36444291472435, loss=2.943420886993408
I0306 02:44:55.356639 139852565595904 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.3460061550140381, loss=2.981062173843384
I0306 02:45:29.649094 139852573988608 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3265990614891052, loss=2.845505475997925
I0306 02:46:03.982634 139852565595904 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3326677680015564, loss=2.9428422451019287
I0306 02:46:38.274244 139852573988608 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3212966322898865, loss=2.842576742172241
I0306 02:47:12.576645 139852565595904 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3895505666732788, loss=2.852509021759033
I0306 02:47:23.578859 139996361643200 spec.py:321] Evaluating on the training split.
I0306 02:47:26.172862 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:50:17.627716 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 02:50:20.225368 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:52:48.688042 139996361643200 spec.py:349] Evaluating on the test split.
I0306 02:52:51.268911 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 02:55:15.141270 139996361643200 submission_runner.py:469] Time since start: 27675.33s, 	Step: 48933, 	{'train/accuracy': 0.643856406211853, 'train/loss': 1.8312432765960693, 'train/bleu': 31.67953308800338, 'validation/accuracy': 0.6646602153778076, 'validation/loss': 1.6712974309921265, 'validation/bleu': 28.853363415038007, 'validation/num_examples': 3000, 'test/accuracy': 0.6774881482124329, 'test/loss': 1.6015293598175049, 'test/bleu': 28.353558852972824, 'test/num_examples': 3003, 'score': 16826.603521585464, 'total_duration': 27675.32644534111, 'accumulated_submission_time': 16826.603521585464, 'accumulated_eval_time': 10845.43692946434, 'accumulated_logging_time': 0.40938735008239746}
I0306 02:55:15.154789 139852573988608 logging_writer.py:48] [48933] accumulated_eval_time=10845.4, accumulated_logging_time=0.409387, accumulated_submission_time=16826.6, global_step=48933, preemption_count=0, score=16826.6, test/accuracy=0.677488, test/bleu=28.3536, test/loss=1.60153, test/num_examples=3003, total_duration=27675.3, train/accuracy=0.643856, train/bleu=31.6795, train/loss=1.83124, validation/accuracy=0.66466, validation/bleu=28.8534, validation/loss=1.6713, validation/num_examples=3000
I0306 02:55:38.391919 139852565595904 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.33300888538360596, loss=2.9094479084014893
I0306 02:56:12.645247 139852573988608 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.40760016441345215, loss=2.919999122619629
I0306 02:56:46.983999 139852565595904 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3230830430984497, loss=2.932896375656128
I0306 02:57:21.325278 139852573988608 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.36987099051475525, loss=2.870697021484375
I0306 02:57:55.667545 139852565595904 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3495449721813202, loss=2.956637382507324
I0306 02:58:29.986576 139852573988608 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3520245850086212, loss=2.9540817737579346
I0306 02:59:04.321718 139852565595904 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.3676665425300598, loss=2.9346184730529785
I0306 02:59:38.644299 139852573988608 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.43814221024513245, loss=2.9383859634399414
I0306 03:00:12.987710 139852565595904 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.3781339228153229, loss=2.8924715518951416
I0306 03:00:47.276564 139852573988608 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.33755096793174744, loss=2.89084529876709
I0306 03:01:21.602758 139852565595904 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3449873626232147, loss=2.8802714347839355
I0306 03:01:55.929317 139852573988608 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.343465119600296, loss=2.9131717681884766
I0306 03:02:30.232799 139852565595904 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.31992870569229126, loss=2.9520604610443115
I0306 03:03:04.566952 139852573988608 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.3603661060333252, loss=2.8657891750335693
I0306 03:03:38.879757 139852565595904 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.39912933111190796, loss=2.8800978660583496
I0306 03:04:13.188170 139852573988608 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.35839715600013733, loss=2.922760009765625
I0306 03:04:47.462041 139852565595904 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.32734525203704834, loss=2.9221959114074707
I0306 03:05:21.794019 139852573988608 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.3254639208316803, loss=2.9280920028686523
I0306 03:05:56.074109 139852565595904 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3179723024368286, loss=2.884004831314087
I0306 03:06:30.392946 139852573988608 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.3693580627441406, loss=2.975032091140747
I0306 03:07:04.673962 139852565595904 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.34857866168022156, loss=2.8739724159240723
I0306 03:07:38.955505 139852573988608 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3535445034503937, loss=2.9426679611206055
I0306 03:08:13.259997 139852565595904 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.33736589550971985, loss=2.9047484397888184
I0306 03:08:47.529598 139852573988608 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.34622693061828613, loss=3.0255796909332275
I0306 03:09:15.318918 139996361643200 spec.py:321] Evaluating on the training split.
I0306 03:09:17.916920 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:12:08.579273 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 03:12:11.169109 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:14:39.284345 139996361643200 spec.py:349] Evaluating on the test split.
I0306 03:14:41.881925 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:17:13.231683 139996361643200 submission_runner.py:469] Time since start: 28993.42s, 	Step: 51382, 	{'train/accuracy': 0.6523168683052063, 'train/loss': 1.7708276510238647, 'train/bleu': 32.35050249495313, 'validation/accuracy': 0.6672928929328918, 'validation/loss': 1.661936640739441, 'validation/bleu': 28.82592007195202, 'validation/num_examples': 3000, 'test/accuracy': 0.6784845590591431, 'test/loss': 1.5940595865249634, 'test/bleu': 28.586904075021454, 'test/num_examples': 3003, 'score': 17666.622935533524, 'total_duration': 28993.416855096817, 'accumulated_submission_time': 17666.622935533524, 'accumulated_eval_time': 11323.349648237228, 'accumulated_logging_time': 0.4312119483947754}
I0306 03:17:13.246242 139852565595904 logging_writer.py:48] [51382] accumulated_eval_time=11323.3, accumulated_logging_time=0.431212, accumulated_submission_time=17666.6, global_step=51382, preemption_count=0, score=17666.6, test/accuracy=0.678485, test/bleu=28.5869, test/loss=1.59406, test/num_examples=3003, total_duration=28993.4, train/accuracy=0.652317, train/bleu=32.3505, train/loss=1.77083, validation/accuracy=0.667293, validation/bleu=28.8259, validation/loss=1.66194, validation/num_examples=3000
I0306 03:17:19.762658 139852573988608 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.3913504183292389, loss=2.931216239929199
I0306 03:17:53.946389 139852565595904 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.387250155210495, loss=2.878073215484619
I0306 03:18:28.228201 139852573988608 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.38988882303237915, loss=2.935194492340088
I0306 03:19:02.557833 139852565595904 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.3733307719230652, loss=2.9651827812194824
I0306 03:19:36.908986 139852573988608 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3290438652038574, loss=2.912111520767212
I0306 03:20:11.270068 139852565595904 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.33650025725364685, loss=2.9382076263427734
I0306 03:20:45.630049 139852573988608 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.426573246717453, loss=2.882143974304199
I0306 03:21:19.970940 139852565595904 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3413737416267395, loss=2.834207057952881
I0306 03:21:54.253733 139852573988608 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3593440055847168, loss=2.9517996311187744
I0306 03:22:28.567952 139852565595904 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.3350856900215149, loss=2.9069936275482178
I0306 03:23:02.883335 139852573988608 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.36157625913619995, loss=2.896411180496216
I0306 03:23:37.218175 139852565595904 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.34570398926734924, loss=2.96622896194458
I0306 03:24:11.533462 139852573988608 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3565661609172821, loss=2.920196056365967
I0306 03:24:45.861946 139852565595904 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.43666020035743713, loss=2.9263134002685547
I0306 03:25:20.199597 139852573988608 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3393879532814026, loss=2.9077537059783936
I0306 03:25:54.525124 139852565595904 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3857121467590332, loss=2.843066692352295
I0306 03:26:28.838706 139852573988608 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.3896959722042084, loss=2.9122395515441895
I0306 03:27:03.166791 139852565595904 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.3685615658760071, loss=2.9530789852142334
I0306 03:27:37.503417 139852573988608 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.38242700695991516, loss=2.969197988510132
I0306 03:28:11.832318 139852565595904 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.3799682855606079, loss=2.9156668186187744
I0306 03:28:46.186937 139852573988608 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.34405380487442017, loss=2.913830518722534
I0306 03:29:20.492458 139852565595904 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3630450665950775, loss=2.8755486011505127
I0306 03:29:54.810274 139852573988608 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.339459627866745, loss=2.8778560161590576
I0306 03:30:29.080135 139852565595904 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.32283955812454224, loss=2.8959288597106934
I0306 03:31:03.381573 139852573988608 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.3479407727718353, loss=2.868677854537964
I0306 03:31:13.343534 139996361643200 spec.py:321] Evaluating on the training split.
I0306 03:31:15.942724 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:34:04.242160 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 03:34:06.834503 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:36:31.825927 139996361643200 spec.py:349] Evaluating on the test split.
I0306 03:36:34.420580 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:38:57.167325 139996361643200 submission_runner.py:469] Time since start: 30297.35s, 	Step: 53830, 	{'train/accuracy': 0.6451072096824646, 'train/loss': 1.810860276222229, 'train/bleu': 31.246398789112245, 'validation/accuracy': 0.6674535870552063, 'validation/loss': 1.661716103553772, 'validation/bleu': 28.96608512135683, 'validation/num_examples': 3000, 'test/accuracy': 0.6782412528991699, 'test/loss': 1.5952285528182983, 'test/bleu': 28.181044192470285, 'test/num_examples': 3003, 'score': 18506.57123017311, 'total_duration': 30297.352501153946, 'accumulated_submission_time': 18506.57123017311, 'accumulated_eval_time': 11787.173393964767, 'accumulated_logging_time': 0.4540834426879883}
I0306 03:38:57.181742 139852565595904 logging_writer.py:48] [53830] accumulated_eval_time=11787.2, accumulated_logging_time=0.454083, accumulated_submission_time=18506.6, global_step=53830, preemption_count=0, score=18506.6, test/accuracy=0.678241, test/bleu=28.181, test/loss=1.59523, test/num_examples=3003, total_duration=30297.4, train/accuracy=0.645107, train/bleu=31.2464, train/loss=1.81086, validation/accuracy=0.667454, validation/bleu=28.9661, validation/loss=1.66172, validation/num_examples=3000
I0306 03:39:21.490784 139852573988608 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.33702391386032104, loss=2.9446210861206055
I0306 03:39:55.742666 139852565595904 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3426539897918701, loss=2.8262155055999756
I0306 03:40:30.083343 139852573988608 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3383292555809021, loss=2.9718668460845947
I0306 03:41:04.396692 139852565595904 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.36595380306243896, loss=2.927861213684082
I0306 03:41:38.698186 139852573988608 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3448397219181061, loss=2.9515044689178467
I0306 03:42:13.032609 139852565595904 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.33989569544792175, loss=2.885993003845215
I0306 03:42:47.344556 139852573988608 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.328893780708313, loss=2.9146392345428467
I0306 03:43:21.652604 139852565595904 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.37550440430641174, loss=2.89937424659729
I0306 03:43:55.958560 139852573988608 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.375501811504364, loss=2.9265787601470947
I0306 03:44:30.280532 139852565595904 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.33776429295539856, loss=2.885803461074829
I0306 03:45:04.566889 139852573988608 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.39155033230781555, loss=2.9533255100250244
I0306 03:45:38.866515 139852565595904 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.326963871717453, loss=2.911724090576172
I0306 03:46:13.170460 139852573988608 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.3486490547657013, loss=2.9100379943847656
I0306 03:46:47.486652 139852565595904 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.4213998019695282, loss=2.8913731575012207
I0306 03:47:21.836542 139852573988608 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3399447798728943, loss=2.8578693866729736
I0306 03:47:56.135838 139852565595904 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.32005882263183594, loss=2.884099245071411
I0306 03:48:30.436161 139852573988608 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.3431335985660553, loss=2.866150140762329
I0306 03:49:04.779829 139852565595904 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.32856646180152893, loss=2.8735435009002686
I0306 03:49:39.089241 139852573988608 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3466031849384308, loss=2.8629746437072754
I0306 03:50:13.386382 139852565595904 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3520095944404602, loss=2.855827569961548
I0306 03:50:47.684600 139852573988608 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.33386409282684326, loss=2.9349522590637207
I0306 03:51:21.987634 139852565595904 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.3548297584056854, loss=2.887589454650879
I0306 03:51:56.301928 139852573988608 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.34228864312171936, loss=2.9197821617126465
I0306 03:52:30.617181 139852565595904 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.32828372716903687, loss=2.893749713897705
I0306 03:52:57.379835 139996361643200 spec.py:321] Evaluating on the training split.
I0306 03:52:59.979030 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:55:43.838428 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 03:55:46.425974 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 03:58:21.873582 139996361643200 spec.py:349] Evaluating on the test split.
I0306 03:58:24.467214 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 04:00:41.090695 139996361643200 submission_runner.py:469] Time since start: 31601.28s, 	Step: 56279, 	{'train/accuracy': 0.6461771130561829, 'train/loss': 1.8151381015777588, 'train/bleu': 32.061809050393215, 'validation/accuracy': 0.6683682203292847, 'validation/loss': 1.650173544883728, 'validation/bleu': 28.98236802869139, 'validation/num_examples': 3000, 'test/accuracy': 0.6838257312774658, 'test/loss': 1.5725330114364624, 'test/bleu': 28.60594164670276, 'test/num_examples': 3003, 'score': 19346.625147104263, 'total_duration': 31601.275846481323, 'accumulated_submission_time': 19346.625147104263, 'accumulated_eval_time': 12250.884186029434, 'accumulated_logging_time': 0.4768850803375244}
I0306 04:00:41.105779 139852573988608 logging_writer.py:48] [56279] accumulated_eval_time=12250.9, accumulated_logging_time=0.476885, accumulated_submission_time=19346.6, global_step=56279, preemption_count=0, score=19346.6, test/accuracy=0.683826, test/bleu=28.6059, test/loss=1.57253, test/num_examples=3003, total_duration=31601.3, train/accuracy=0.646177, train/bleu=32.0618, train/loss=1.81514, validation/accuracy=0.668368, validation/bleu=28.9824, validation/loss=1.65017, validation/num_examples=3000
I0306 04:00:48.634965 139852565595904 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.34308934211730957, loss=2.871154308319092
I0306 04:01:22.806949 139852573988608 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3754664957523346, loss=2.8323986530303955
I0306 04:01:57.055042 139852565595904 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.337173193693161, loss=2.8263187408447266
I0306 04:02:31.278156 139852573988608 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.3318606913089752, loss=2.874481201171875
I0306 04:03:05.482372 139852565595904 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.3634815216064453, loss=2.907808780670166
I0306 04:03:39.688808 139852573988608 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3352925777435303, loss=2.8844690322875977
I0306 04:04:13.944021 139852565595904 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.3300218880176544, loss=2.8486416339874268
I0306 04:04:48.174450 139852573988608 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.33895716071128845, loss=2.8774337768554688
I0306 04:05:22.408939 139852565595904 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.3437660336494446, loss=2.8569447994232178
I0306 04:05:56.606302 139852573988608 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3440951704978943, loss=2.9301939010620117
I0306 04:06:30.815413 139852565595904 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3217768371105194, loss=2.9220829010009766
I0306 04:07:05.027881 139852573988608 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3680983781814575, loss=2.9196033477783203
I0306 04:07:39.214698 139852565595904 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3762081265449524, loss=2.8360862731933594
I0306 04:08:13.384083 139852573988608 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3875574767589569, loss=2.948657989501953
I0306 04:08:47.564716 139852565595904 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.34430935978889465, loss=2.8950185775756836
I0306 04:09:21.769466 139852573988608 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.3933303952217102, loss=2.834383010864258
I0306 04:09:56.016500 139852565595904 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.357219934463501, loss=2.833967685699463
I0306 04:10:30.259837 139852573988608 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3470996916294098, loss=2.9028165340423584
I0306 04:11:04.511617 139852565595904 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.3468116223812103, loss=2.884056806564331
I0306 04:11:38.773716 139852573988608 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3401648700237274, loss=2.893800973892212
I0306 04:12:13.002968 139852565595904 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3353103995323181, loss=2.9506962299346924
I0306 04:12:47.269680 139852573988608 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.34653180837631226, loss=2.833739757537842
I0306 04:13:21.500750 139852565595904 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.35918042063713074, loss=2.8762643337249756
I0306 04:13:55.736022 139852573988608 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.36463433504104614, loss=2.8051846027374268
I0306 04:14:29.954740 139852565595904 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3562231659889221, loss=2.8849406242370605
I0306 04:14:41.272905 139996361643200 spec.py:321] Evaluating on the training split.
I0306 04:14:43.876422 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 04:17:54.343986 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 04:17:56.946500 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 04:20:33.357380 139996361643200 spec.py:349] Evaluating on the test split.
I0306 04:20:35.942192 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 04:23:07.142698 139996361643200 submission_runner.py:469] Time since start: 32947.33s, 	Step: 58734, 	{'train/accuracy': 0.6509807109832764, 'train/loss': 1.7647664546966553, 'train/bleu': 32.33128945287037, 'validation/accuracy': 0.669431209564209, 'validation/loss': 1.6397720575332642, 'validation/bleu': 29.21710244818758, 'validation/num_examples': 3000, 'test/accuracy': 0.6824122667312622, 'test/loss': 1.5711911916732788, 'test/bleu': 28.769304351001292, 'test/num_examples': 3003, 'score': 20186.641088485718, 'total_duration': 32947.32786130905, 'accumulated_submission_time': 20186.641088485718, 'accumulated_eval_time': 12756.753923892975, 'accumulated_logging_time': 0.501591682434082}
I0306 04:23:07.157090 139852573988608 logging_writer.py:48] [58734] accumulated_eval_time=12756.8, accumulated_logging_time=0.501592, accumulated_submission_time=20186.6, global_step=58734, preemption_count=0, score=20186.6, test/accuracy=0.682412, test/bleu=28.7693, test/loss=1.57119, test/num_examples=3003, total_duration=32947.3, train/accuracy=0.650981, train/bleu=32.3313, train/loss=1.76477, validation/accuracy=0.669431, validation/bleu=29.2171, validation/loss=1.63977, validation/num_examples=3000
I0306 04:23:30.036128 139852565595904 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3628939986228943, loss=2.867635488510132
I0306 04:24:04.188810 139852573988608 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.33839136362075806, loss=2.9072015285491943
I0306 04:24:38.414896 139852565595904 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.32906192541122437, loss=2.945960521697998
I0306 04:25:12.666606 139852573988608 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.3715742826461792, loss=2.906209945678711
I0306 04:25:46.915802 139852565595904 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3399841785430908, loss=2.9185750484466553
I0306 04:26:21.153297 139852573988608 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.34375685453414917, loss=2.899693012237549
I0306 04:26:55.364588 139852565595904 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.35207024216651917, loss=2.8136796951293945
I0306 04:27:29.585968 139852573988608 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.34851011633872986, loss=2.96224045753479
I0306 04:28:03.816402 139852565595904 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.34188857674598694, loss=2.8732213973999023
I0306 04:28:38.014680 139852573988608 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3557142913341522, loss=2.9074485301971436
I0306 04:29:12.240125 139852565595904 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.3566984236240387, loss=2.8379459381103516
I0306 04:29:46.466438 139852573988608 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.34485483169555664, loss=2.8906519412994385
I0306 04:30:20.716309 139852565595904 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.3695168197154999, loss=2.8568906784057617
I0306 04:30:54.949280 139852573988608 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.3951215445995331, loss=2.9122283458709717
I0306 04:31:29.154204 139852565595904 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.33346202969551086, loss=2.8499770164489746
I0306 04:32:03.374349 139852573988608 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.33947494626045227, loss=2.855623722076416
I0306 04:32:37.585232 139852565595904 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.37318113446235657, loss=2.8910434246063232
I0306 04:33:11.816996 139852573988608 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3768031895160675, loss=2.9041786193847656
I0306 04:33:46.037778 139852565595904 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.35048818588256836, loss=2.8738558292388916
I0306 04:34:20.258139 139852573988608 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3193387985229492, loss=2.949636936187744
I0306 04:34:54.494729 139852565595904 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.35120314359664917, loss=2.905388832092285
I0306 04:35:28.704599 139852573988608 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.35140833258628845, loss=2.9516210556030273
I0306 04:36:02.971098 139852565595904 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3458196520805359, loss=2.906012535095215
I0306 04:36:37.161827 139852573988608 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.3488743007183075, loss=2.9163007736206055
I0306 04:37:07.263593 139996361643200 spec.py:321] Evaluating on the training split.
I0306 04:37:09.859330 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 04:40:03.389009 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 04:40:05.982608 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 04:42:38.022801 139996361643200 spec.py:349] Evaluating on the test split.
I0306 04:42:40.613367 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 04:45:04.620989 139996361643200 submission_runner.py:469] Time since start: 34264.81s, 	Step: 61189, 	{'train/accuracy': 0.6516432166099548, 'train/loss': 1.7733469009399414, 'train/bleu': 31.85245357652231, 'validation/accuracy': 0.671272873878479, 'validation/loss': 1.626589059829712, 'validation/bleu': 29.111893790024837, 'validation/num_examples': 3000, 'test/accuracy': 0.6855173110961914, 'test/loss': 1.5592955350875854, 'test/bleu': 29.376111506420216, 'test/num_examples': 3003, 'score': 21026.598375320435, 'total_duration': 34264.806159973145, 'accumulated_submission_time': 21026.598375320435, 'accumulated_eval_time': 13234.111276626587, 'accumulated_logging_time': 0.5244767665863037}
I0306 04:45:04.638277 139852565595904 logging_writer.py:48] [61189] accumulated_eval_time=13234.1, accumulated_logging_time=0.524477, accumulated_submission_time=21026.6, global_step=61189, preemption_count=0, score=21026.6, test/accuracy=0.685517, test/bleu=29.3761, test/loss=1.5593, test/num_examples=3003, total_duration=34264.8, train/accuracy=0.651643, train/bleu=31.8525, train/loss=1.77335, validation/accuracy=0.671273, validation/bleu=29.1119, validation/loss=1.62659, validation/num_examples=3000
I0306 04:45:08.747158 139852573988608 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3406848609447479, loss=2.8473238945007324
I0306 04:45:42.874146 139852565595904 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.3421206474304199, loss=2.9370551109313965
I0306 04:46:17.068656 139852573988608 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.32621219754219055, loss=2.836907386779785
I0306 04:46:51.258444 139852565595904 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.32394298911094666, loss=2.848348379135132
I0306 04:47:25.512694 139852573988608 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.3740867078304291, loss=2.913381576538086
I0306 04:47:59.738228 139852565595904 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3519485890865326, loss=2.8647429943084717
I0306 04:48:33.962230 139852573988608 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3408084809780121, loss=2.811368465423584
I0306 04:49:08.183836 139852565595904 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3373582661151886, loss=2.887632369995117
I0306 04:49:42.423462 139852573988608 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.376520574092865, loss=2.9262161254882812
I0306 04:50:16.648963 139852565595904 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.32787948846817017, loss=2.934340715408325
I0306 04:50:50.902633 139852573988608 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.3709700107574463, loss=2.8813159465789795
I0306 04:51:25.144157 139852565595904 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.3798220157623291, loss=2.9324233531951904
I0306 04:51:59.416789 139852573988608 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.33632367849349976, loss=2.927669048309326
I0306 04:52:33.628294 139852565595904 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3582761585712433, loss=2.9267196655273438
I0306 04:53:07.872112 139852573988608 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3334822952747345, loss=2.890685558319092
I0306 04:53:42.092856 139852565595904 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3431839346885681, loss=2.822652578353882
I0306 04:54:16.387102 139852573988608 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.35673901438713074, loss=2.882941961288452
I0306 04:54:50.675742 139852565595904 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3468175530433655, loss=2.83304500579834
I0306 04:55:24.998820 139852573988608 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3430350720882416, loss=2.829002618789673
I0306 04:55:59.320818 139852565595904 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.35162198543548584, loss=2.8793249130249023
I0306 04:56:33.617630 139852573988608 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.38709837198257446, loss=2.8864495754241943
I0306 04:57:07.944186 139852565595904 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.3485126793384552, loss=2.8417770862579346
I0306 04:57:42.244752 139852573988608 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.362179160118103, loss=2.8452506065368652
I0306 04:58:16.559243 139852565595904 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.37268179655075073, loss=2.819955348968506
I0306 04:58:50.880264 139852573988608 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.37048444151878357, loss=2.9070394039154053
I0306 04:59:04.931107 139996361643200 spec.py:321] Evaluating on the training split.
I0306 04:59:07.534965 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:02:00.173030 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 05:02:02.752055 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:04:38.175185 139996361643200 spec.py:349] Evaluating on the test split.
I0306 05:04:40.766281 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:07:14.054417 139996361643200 submission_runner.py:469] Time since start: 35594.24s, 	Step: 63642, 	{'train/accuracy': 0.6644970178604126, 'train/loss': 1.6864296197891235, 'train/bleu': 33.187818723648704, 'validation/accuracy': 0.6722493171691895, 'validation/loss': 1.6226963996887207, 'validation/bleu': 29.091850564155884, 'validation/num_examples': 3000, 'test/accuracy': 0.6871509552001953, 'test/loss': 1.5464812517166138, 'test/bleu': 28.915360424304996, 'test/num_examples': 3003, 'score': 21866.745727062225, 'total_duration': 35594.23958897591, 'accumulated_submission_time': 21866.745727062225, 'accumulated_eval_time': 13723.234539270401, 'accumulated_logging_time': 0.5500822067260742}
I0306 05:07:14.069555 139852565595904 logging_writer.py:48] [63642] accumulated_eval_time=13723.2, accumulated_logging_time=0.550082, accumulated_submission_time=21866.7, global_step=63642, preemption_count=0, score=21866.7, test/accuracy=0.687151, test/bleu=28.9154, test/loss=1.54648, test/num_examples=3003, total_duration=35594.2, train/accuracy=0.664497, train/bleu=33.1878, train/loss=1.68643, validation/accuracy=0.672249, validation/bleu=29.0919, validation/loss=1.6227, validation/num_examples=3000
I0306 05:07:34.247783 139852573988608 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.35402655601501465, loss=2.8342976570129395
I0306 05:08:08.491462 139852565595904 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3458077013492584, loss=2.8503830432891846
I0306 05:08:42.749632 139852573988608 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.35246142745018005, loss=2.83284330368042
I0306 05:09:17.056881 139852565595904 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3310539722442627, loss=2.7979185581207275
I0306 05:09:51.372453 139852573988608 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.33703452348709106, loss=2.8375203609466553
I0306 05:10:25.700880 139852565595904 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.34824222326278687, loss=2.881882667541504
I0306 05:10:59.997291 139852573988608 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3614238500595093, loss=2.8710649013519287
I0306 05:11:34.308675 139852565595904 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3509022891521454, loss=2.9025397300720215
I0306 05:12:08.618303 139852573988608 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.3407485783100128, loss=2.8143112659454346
I0306 05:12:42.921741 139852565595904 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3786889910697937, loss=2.8161511421203613
I0306 05:13:17.225546 139852573988608 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.35090693831443787, loss=2.859281063079834
I0306 05:13:51.504338 139852565595904 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.34300172328948975, loss=2.8387033939361572
I0306 05:14:25.790125 139852573988608 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.33668428659439087, loss=2.824333906173706
I0306 05:15:00.059788 139852565595904 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.37869471311569214, loss=2.834798812866211
I0306 05:15:34.302682 139852573988608 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3430461883544922, loss=2.8672051429748535
I0306 05:16:08.608410 139852565595904 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3405950963497162, loss=2.883782386779785
I0306 05:16:42.884394 139852573988608 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.34240537881851196, loss=2.7897732257843018
I0306 05:17:17.192500 139852565595904 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.35740265250205994, loss=2.8449361324310303
I0306 05:17:51.498286 139852573988608 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3358132839202881, loss=2.8166098594665527
I0306 05:18:25.787951 139852565595904 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3731189966201782, loss=2.8262743949890137
I0306 05:19:00.100027 139852573988608 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.34447628259658813, loss=2.8758230209350586
I0306 05:19:34.434095 139852565595904 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.3357918858528137, loss=2.8647916316986084
I0306 05:20:08.763293 139852573988608 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.36731842160224915, loss=2.9088082313537598
I0306 05:20:43.084125 139852565595904 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.3471736013889313, loss=2.8496615886688232
I0306 05:21:14.328570 139996361643200 spec.py:321] Evaluating on the training split.
I0306 05:21:16.941465 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:24:51.577030 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 05:24:54.164499 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:27:21.611900 139996361643200 spec.py:349] Evaluating on the test split.
I0306 05:27:24.199673 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:29:50.668722 139996361643200 submission_runner.py:469] Time since start: 36950.85s, 	Step: 66092, 	{'train/accuracy': 0.6554262042045593, 'train/loss': 1.7498739957809448, 'train/bleu': 32.483856194704494, 'validation/accuracy': 0.673312246799469, 'validation/loss': 1.6201781034469604, 'validation/bleu': 29.234932302328517, 'validation/num_examples': 3000, 'test/accuracy': 0.6884022951126099, 'test/loss': 1.5399383306503296, 'test/bleu': 28.93016399790172, 'test/num_examples': 3003, 'score': 22706.85374903679, 'total_duration': 36950.85386252403, 'accumulated_submission_time': 22706.85374903679, 'accumulated_eval_time': 14239.574616193771, 'accumulated_logging_time': 0.5749740600585938}
I0306 05:29:50.687736 139852573988608 logging_writer.py:48] [66092] accumulated_eval_time=14239.6, accumulated_logging_time=0.574974, accumulated_submission_time=22706.9, global_step=66092, preemption_count=0, score=22706.9, test/accuracy=0.688402, test/bleu=28.9302, test/loss=1.53994, test/num_examples=3003, total_duration=36950.9, train/accuracy=0.655426, train/bleu=32.4839, train/loss=1.74987, validation/accuracy=0.673312, validation/bleu=29.2349, validation/loss=1.62018, validation/num_examples=3000
I0306 05:29:53.777638 139852565595904 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.4454747140407562, loss=2.8411142826080322
I0306 05:30:27.932093 139852573988608 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3462945520877838, loss=2.790471076965332
I0306 05:31:02.172703 139852565595904 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.3300459682941437, loss=2.8715357780456543
I0306 05:31:36.497580 139852573988608 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3548387587070465, loss=2.9097607135772705
I0306 05:32:10.837089 139852565595904 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.37678325176239014, loss=2.8909645080566406
I0306 05:32:45.147191 139852573988608 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.3433191180229187, loss=2.841571807861328
I0306 05:33:19.453913 139852565595904 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3690794110298157, loss=2.8573386669158936
I0306 05:33:53.727789 139852573988608 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3503459692001343, loss=2.850637197494507
I0306 05:34:28.026268 139852565595904 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.34482860565185547, loss=2.8731627464294434
I0306 05:35:02.338459 139852573988608 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3770577907562256, loss=2.845453977584839
I0306 05:35:36.663442 139852565595904 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.35620859265327454, loss=2.862220287322998
I0306 05:36:10.946256 139852573988608 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.33444514870643616, loss=2.8111584186553955
I0306 05:36:45.240807 139852565595904 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.35534903407096863, loss=2.8390982151031494
I0306 05:37:19.524820 139852573988608 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3612063229084015, loss=2.880023956298828
I0306 05:37:53.801701 139852565595904 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3491409420967102, loss=2.812596082687378
I0306 05:38:28.110370 139852573988608 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.371844619512558, loss=2.829399824142456
I0306 05:39:02.401858 139852565595904 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3941575288772583, loss=2.924455165863037
I0306 05:39:36.729047 139852573988608 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3533570170402527, loss=2.838548183441162
I0306 05:40:11.026396 139852565595904 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.3471088707447052, loss=2.8568766117095947
I0306 05:40:45.341632 139852573988608 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.37527552247047424, loss=2.81965708732605
I0306 05:41:19.643083 139852565595904 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3533098101615906, loss=2.831997871398926
I0306 05:41:53.946989 139852573988608 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.38174551725387573, loss=2.9103753566741943
I0306 05:42:28.216462 139852565595904 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3753891885280609, loss=2.8150885105133057
I0306 05:43:02.520735 139852573988608 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3725190758705139, loss=2.8550145626068115
I0306 05:43:36.866938 139852565595904 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3601181209087372, loss=2.8016788959503174
I0306 05:43:50.935894 139996361643200 spec.py:321] Evaluating on the training split.
I0306 05:43:53.534669 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:47:40.800503 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 05:47:43.395245 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:50:23.834311 139996361643200 spec.py:349] Evaluating on the test split.
I0306 05:50:26.423370 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 05:53:07.936469 139996361643200 submission_runner.py:469] Time since start: 38348.12s, 	Step: 68542, 	{'train/accuracy': 0.653733491897583, 'train/loss': 1.7698416709899902, 'train/bleu': 32.42082237984671, 'validation/accuracy': 0.6751291751861572, 'validation/loss': 1.601650595664978, 'validation/bleu': 29.461086458437784, 'validation/num_examples': 3000, 'test/accuracy': 0.689190149307251, 'test/loss': 1.5305323600769043, 'test/bleu': 29.263862882613676, 'test/num_examples': 3003, 'score': 23546.953679561615, 'total_duration': 38348.12163424492, 'accumulated_submission_time': 23546.953679561615, 'accumulated_eval_time': 14796.575135231018, 'accumulated_logging_time': 0.6029472351074219}
I0306 05:53:07.952335 139852573988608 logging_writer.py:48] [68542] accumulated_eval_time=14796.6, accumulated_logging_time=0.602947, accumulated_submission_time=23547, global_step=68542, preemption_count=0, score=23547, test/accuracy=0.68919, test/bleu=29.2639, test/loss=1.53053, test/num_examples=3003, total_duration=38348.1, train/accuracy=0.653733, train/bleu=32.4208, train/loss=1.76984, validation/accuracy=0.675129, validation/bleu=29.4611, validation/loss=1.60165, validation/num_examples=3000
I0306 05:53:28.100308 139852565595904 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.36024951934814453, loss=2.84479022026062
I0306 05:54:02.279471 139852573988608 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.351794570684433, loss=2.8085546493530273
I0306 05:54:36.579020 139852565595904 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3456317186355591, loss=2.847632646560669
I0306 05:55:10.922664 139852573988608 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.33984801173210144, loss=2.835425615310669
I0306 05:55:45.257108 139852565595904 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.37011459469795227, loss=2.87811017036438
I0306 05:56:19.562245 139852573988608 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.37582188844680786, loss=2.8988029956817627
I0306 05:56:53.876894 139852565595904 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.34704551100730896, loss=2.860927104949951
I0306 05:57:28.153988 139852573988608 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.39648449420928955, loss=2.8901469707489014
I0306 05:58:02.471241 139852565595904 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3724121153354645, loss=2.777804374694824
I0306 05:58:36.783099 139852573988608 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.34555935859680176, loss=2.783127784729004
I0306 05:59:11.128399 139852565595904 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3838340938091278, loss=2.879112482070923
I0306 05:59:45.456392 139852573988608 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.37452638149261475, loss=2.8370916843414307
I0306 06:00:19.752060 139852565595904 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.36132365465164185, loss=2.8450005054473877
I0306 06:00:54.030801 139852573988608 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.3718845546245575, loss=2.901495933532715
I0306 06:01:28.313319 139852565595904 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3791970908641815, loss=2.8129804134368896
I0306 06:02:02.608966 139852573988608 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3464996814727783, loss=2.8186466693878174
I0306 06:02:36.908012 139852565595904 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.35332098603248596, loss=2.8231418132781982
I0306 06:03:11.186429 139852573988608 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.365018367767334, loss=2.859928846359253
I0306 06:03:45.490008 139852565595904 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.36216408014297485, loss=2.815615177154541
I0306 06:04:19.823785 139852573988608 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.36565151810646057, loss=2.7971103191375732
I0306 06:04:54.141553 139852565595904 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.40051785111427307, loss=2.9095962047576904
I0306 06:05:28.459746 139852573988608 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3778174817562103, loss=2.769275665283203
I0306 06:06:02.761549 139852565595904 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.40404999256134033, loss=2.8778510093688965
I0306 06:06:37.136526 139852573988608 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.4004991352558136, loss=2.826587438583374
I0306 06:07:08.110513 139996361643200 spec.py:321] Evaluating on the training split.
I0306 06:07:10.726961 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:11:05.398223 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 06:11:07.995118 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:13:42.617560 139996361643200 spec.py:349] Evaluating on the test split.
I0306 06:13:45.211707 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:16:23.623140 139996361643200 submission_runner.py:469] Time since start: 39743.81s, 	Step: 70991, 	{'train/accuracy': 0.6656088829040527, 'train/loss': 1.6856144666671753, 'train/bleu': 32.68839644071378, 'validation/accuracy': 0.6772550940513611, 'validation/loss': 1.5932538509368896, 'validation/bleu': 29.48452785909497, 'validation/num_examples': 3000, 'test/accuracy': 0.6920287609100342, 'test/loss': 1.5191024541854858, 'test/bleu': 29.350704851050587, 'test/num_examples': 3003, 'score': 24386.959141492844, 'total_duration': 39743.80830025673, 'accumulated_submission_time': 24386.959141492844, 'accumulated_eval_time': 15352.087708473206, 'accumulated_logging_time': 0.6285481452941895}
I0306 06:16:23.639193 139852565595904 logging_writer.py:48] [70991] accumulated_eval_time=15352.1, accumulated_logging_time=0.628548, accumulated_submission_time=24387, global_step=70991, preemption_count=0, score=24387, test/accuracy=0.692029, test/bleu=29.3507, test/loss=1.5191, test/num_examples=3003, total_duration=39743.8, train/accuracy=0.665609, train/bleu=32.6884, train/loss=1.68561, validation/accuracy=0.677255, validation/bleu=29.4845, validation/loss=1.59325, validation/num_examples=3000
I0306 06:16:27.076558 139852573988608 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.34402093291282654, loss=2.8765480518341064
I0306 06:17:01.255978 139852565595904 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.358642041683197, loss=2.8508238792419434
I0306 06:17:35.547792 139852573988608 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.3636946976184845, loss=2.8313140869140625
I0306 06:18:09.841517 139852565595904 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.399248331785202, loss=2.8382933139801025
I0306 06:18:44.137686 139852573988608 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3598105013370514, loss=2.864225387573242
I0306 06:19:18.405058 139852565595904 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.35991835594177246, loss=2.8727664947509766
I0306 06:19:52.748302 139852573988608 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.3774415850639343, loss=2.8193886280059814
I0306 06:20:27.034041 139852565595904 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.35890963673591614, loss=2.8223867416381836
I0306 06:21:01.356373 139852573988608 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.35645294189453125, loss=2.810570478439331
I0306 06:21:35.638593 139852565595904 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.38292598724365234, loss=2.8301749229431152
I0306 06:22:09.948059 139852573988608 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.36011502146720886, loss=2.7814011573791504
I0306 06:22:44.256271 139852565595904 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3826253116130829, loss=2.775764226913452
I0306 06:23:18.552219 139852573988608 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3716476857662201, loss=2.7961151599884033
I0306 06:23:52.851808 139852565595904 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.39805716276168823, loss=2.801323413848877
I0306 06:24:27.356266 139852573988608 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3888367712497711, loss=2.9008662700653076
I0306 06:25:01.800804 139852565595904 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.33817678689956665, loss=2.764954090118408
I0306 06:25:36.227126 139852573988608 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.371294766664505, loss=2.763836622238159
I0306 06:26:10.606491 139852565595904 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.37379515171051025, loss=2.7725186347961426
I0306 06:26:45.070776 139852573988608 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.37394407391548157, loss=2.7816529273986816
I0306 06:27:19.515167 139852565595904 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.382790744304657, loss=2.8852627277374268
I0306 06:27:53.918047 139852573988608 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.36091500520706177, loss=2.807474374771118
I0306 06:28:28.342253 139852565595904 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3594304025173187, loss=2.870591163635254
I0306 06:29:02.782836 139852573988608 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.37166914343833923, loss=2.8245792388916016
I0306 06:29:37.238269 139852565595904 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.3768821358680725, loss=2.8339242935180664
I0306 06:30:11.676871 139852573988608 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3498619496822357, loss=2.792202949523926
I0306 06:30:23.732416 139996361643200 spec.py:321] Evaluating on the training split.
I0306 06:30:26.338915 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:33:42.465833 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 06:33:45.067681 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:36:15.020275 139996361643200 spec.py:349] Evaluating on the test split.
I0306 06:36:17.622258 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:38:44.572201 139996361643200 submission_runner.py:469] Time since start: 41084.76s, 	Step: 73436, 	{'train/accuracy': 0.6601696014404297, 'train/loss': 1.7195792198181152, 'train/bleu': 32.34331833844137, 'validation/accuracy': 0.6761797666549683, 'validation/loss': 1.593739628791809, 'validation/bleu': 29.373874669258257, 'validation/num_examples': 3000, 'test/accuracy': 0.6914610266685486, 'test/loss': 1.5140252113342285, 'test/bleu': 29.288519856610918, 'test/num_examples': 3003, 'score': 25226.902934789658, 'total_duration': 41084.75736093521, 'accumulated_submission_time': 25226.902934789658, 'accumulated_eval_time': 15852.927433013916, 'accumulated_logging_time': 0.6539583206176758}
I0306 06:38:44.590882 139852565595904 logging_writer.py:48] [73436] accumulated_eval_time=15852.9, accumulated_logging_time=0.653958, accumulated_submission_time=25226.9, global_step=73436, preemption_count=0, score=25226.9, test/accuracy=0.691461, test/bleu=29.2885, test/loss=1.51403, test/num_examples=3003, total_duration=41084.8, train/accuracy=0.66017, train/bleu=32.3433, train/loss=1.71958, validation/accuracy=0.67618, validation/bleu=29.3739, validation/loss=1.59374, validation/num_examples=3000
I0306 06:39:06.919441 139852573988608 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.36082786321640015, loss=2.8292839527130127
I0306 06:39:41.294641 139852565595904 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.3458443582057953, loss=2.771104574203491
I0306 06:40:15.740441 139852573988608 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3725554645061493, loss=2.8587732315063477
I0306 06:40:50.197569 139852565595904 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3688637912273407, loss=2.840359926223755
I0306 06:41:24.634881 139852573988608 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.40527549386024475, loss=2.8441479206085205
I0306 06:41:59.067441 139852565595904 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.38100919127464294, loss=2.842414617538452
I0306 06:42:33.493623 139852573988608 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.36979514360427856, loss=2.820028781890869
I0306 06:43:07.919838 139852565595904 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.37192240357398987, loss=2.8129971027374268
I0306 06:43:42.362779 139852573988608 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3444086015224457, loss=2.8393967151641846
I0306 06:44:16.783499 139852565595904 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.36131760478019714, loss=2.7896928787231445
I0306 06:44:51.231779 139852573988608 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3740348219871521, loss=2.830618143081665
I0306 06:45:25.657944 139852565595904 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.37732982635498047, loss=2.877666473388672
I0306 06:46:00.103767 139852573988608 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.391130656003952, loss=2.8346400260925293
I0306 06:46:34.497452 139852565595904 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.38363122940063477, loss=2.7765915393829346
I0306 06:47:08.912842 139852573988608 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.39904269576072693, loss=2.8450098037719727
I0306 06:47:43.345255 139852565595904 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.39882391691207886, loss=2.8280563354492188
I0306 06:48:17.766321 139852573988608 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.37214723229408264, loss=2.814565658569336
I0306 06:48:52.175458 139852565595904 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3532165586948395, loss=2.836890697479248
I0306 06:49:26.580356 139852573988608 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.33686158061027527, loss=2.8051083087921143
I0306 06:50:01.014150 139852565595904 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.36368563771247864, loss=2.8558456897735596
I0306 06:50:35.460848 139852573988608 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3745885491371155, loss=2.7998464107513428
I0306 06:51:09.893787 139852565595904 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.382148802280426, loss=2.848517417907715
I0306 06:51:44.330154 139852573988608 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.36580890417099, loss=2.8294894695281982
I0306 06:52:18.743596 139852565595904 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.4057987332344055, loss=2.8111727237701416
I0306 06:52:44.901968 139996361643200 spec.py:321] Evaluating on the training split.
I0306 06:52:47.511186 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:55:21.777104 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 06:55:24.376905 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 06:57:51.385160 139996361643200 spec.py:349] Evaluating on the test split.
I0306 06:57:53.979692 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 07:00:15.697455 139996361643200 submission_runner.py:469] Time since start: 42375.88s, 	Step: 75877, 	{'train/accuracy': 0.677148699760437, 'train/loss': 1.6172224283218384, 'train/bleu': 33.74263148846099, 'validation/accuracy': 0.6797147393226624, 'validation/loss': 1.5837938785552979, 'validation/bleu': 29.711227934844704, 'validation/num_examples': 3000, 'test/accuracy': 0.6920750737190247, 'test/loss': 1.5132055282592773, 'test/bleu': 28.94981395555031, 'test/num_examples': 3003, 'score': 26067.066395998, 'total_duration': 42375.88261508942, 'accumulated_submission_time': 26067.066395998, 'accumulated_eval_time': 16303.722861289978, 'accumulated_logging_time': 0.6808102130889893}
I0306 07:00:15.715206 139852573988608 logging_writer.py:48] [75877] accumulated_eval_time=16303.7, accumulated_logging_time=0.68081, accumulated_submission_time=26067.1, global_step=75877, preemption_count=0, score=26067.1, test/accuracy=0.692075, test/bleu=28.9498, test/loss=1.51321, test/num_examples=3003, total_duration=42375.9, train/accuracy=0.677149, train/bleu=33.7426, train/loss=1.61722, validation/accuracy=0.679715, validation/bleu=29.7112, validation/loss=1.58379, validation/num_examples=3000
I0306 07:00:23.965152 139852565595904 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.37726137042045593, loss=2.839402914047241
I0306 07:00:58.294065 139852573988608 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.36523696780204773, loss=2.7734906673431396
I0306 07:01:32.712949 139852565595904 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.373079389333725, loss=2.8225131034851074
I0306 07:02:07.113885 139852573988608 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.4032043218612671, loss=2.784959316253662
I0306 07:02:41.519681 139852565595904 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.4200975000858307, loss=2.875178813934326
I0306 07:03:15.941489 139852573988608 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3892219364643097, loss=2.7982916831970215
I0306 07:03:50.365808 139852565595904 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.37731680274009705, loss=2.8656179904937744
I0306 07:04:24.767565 139852573988608 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.38097235560417175, loss=2.773369312286377
I0306 07:04:59.195060 139852565595904 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3780457079410553, loss=2.7682013511657715
I0306 07:05:33.597304 139852573988608 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.37993863224983215, loss=2.8378264904022217
I0306 07:06:07.992844 139852565595904 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.3674496114253998, loss=2.859257936477661
I0306 07:06:42.407384 139852573988608 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.3836108148097992, loss=2.769728660583496
I0306 07:07:16.823724 139852565595904 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.36437296867370605, loss=2.7923848628997803
I0306 07:07:51.228426 139852573988608 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.3896764814853668, loss=2.7930831909179688
I0306 07:08:25.678613 139852565595904 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.39465150237083435, loss=2.7799253463745117
I0306 07:09:00.097045 139852573988608 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.3791009187698364, loss=2.801766872406006
I0306 07:09:34.526844 139852565595904 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.3839521110057831, loss=2.843601703643799
I0306 07:10:08.959462 139852573988608 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.37508243322372437, loss=2.805697441101074
I0306 07:10:43.434566 139852565595904 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3748478293418884, loss=2.8096275329589844
I0306 07:11:17.869152 139852573988608 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.40178191661834717, loss=2.7755720615386963
I0306 07:11:52.330569 139852565595904 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.36847829818725586, loss=2.8977620601654053
I0306 07:12:26.766160 139852573988608 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3684222400188446, loss=2.7782270908355713
I0306 07:13:01.196129 139852565595904 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.39325255155563354, loss=2.710843324661255
I0306 07:13:35.607519 139852573988608 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.39187929034233093, loss=2.826544761657715
I0306 07:14:10.020549 139852565595904 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3835057020187378, loss=2.778707265853882
I0306 07:14:15.875215 139996361643200 spec.py:321] Evaluating on the training split.
I0306 07:14:18.482722 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 07:17:43.482672 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 07:17:46.086235 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 07:20:28.752939 139996361643200 spec.py:349] Evaluating on the test split.
I0306 07:20:31.351361 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 07:23:23.315731 139996361643200 submission_runner.py:469] Time since start: 43763.50s, 	Step: 78318, 	{'train/accuracy': 0.6633084416389465, 'train/loss': 1.7017958164215088, 'train/bleu': 33.23232868746256, 'validation/accuracy': 0.6793810129165649, 'validation/loss': 1.5754899978637695, 'validation/bleu': 29.998673149093527, 'validation/num_examples': 3000, 'test/accuracy': 0.6938361525535583, 'test/loss': 1.4985798597335815, 'test/bleu': 29.631323026778578, 'test/num_examples': 3003, 'score': 26907.07831954956, 'total_duration': 43763.500906705856, 'accumulated_submission_time': 26907.07831954956, 'accumulated_eval_time': 16851.163332223892, 'accumulated_logging_time': 0.70760178565979}
I0306 07:23:23.333853 139852573988608 logging_writer.py:48] [78318] accumulated_eval_time=16851.2, accumulated_logging_time=0.707602, accumulated_submission_time=26907.1, global_step=78318, preemption_count=0, score=26907.1, test/accuracy=0.693836, test/bleu=29.6313, test/loss=1.49858, test/num_examples=3003, total_duration=43763.5, train/accuracy=0.663308, train/bleu=33.2323, train/loss=1.7018, validation/accuracy=0.679381, validation/bleu=29.9987, validation/loss=1.57549, validation/num_examples=3000
I0306 07:23:51.810324 139852565595904 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.3607684075832367, loss=2.8167574405670166
I0306 07:24:26.209922 139852573988608 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.4152396023273468, loss=2.7835938930511475
I0306 07:25:00.655799 139852565595904 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3872350752353668, loss=2.800804853439331
I0306 07:25:35.080693 139852573988608 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4332137107849121, loss=2.7884633541107178
I0306 07:26:09.499793 139852565595904 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.37141427397727966, loss=2.7602410316467285
I0306 07:26:43.951841 139852573988608 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3978697657585144, loss=2.770620822906494
I0306 07:27:18.386882 139852565595904 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3896205425262451, loss=2.836944818496704
I0306 07:27:52.808233 139852573988608 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.40410879254341125, loss=2.7595860958099365
I0306 07:28:27.231169 139852565595904 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.4013653099536896, loss=2.8644967079162598
I0306 07:29:01.662022 139852573988608 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.3992396891117096, loss=2.7677383422851562
I0306 07:29:36.090841 139852565595904 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.42805033922195435, loss=2.7919952869415283
I0306 07:30:10.534210 139852573988608 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3912927508354187, loss=2.813979387283325
I0306 07:30:44.962698 139852565595904 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3839855194091797, loss=2.7294092178344727
I0306 07:31:19.343908 139852573988608 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.38854724168777466, loss=2.777137517929077
I0306 07:31:53.762090 139852565595904 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3983747661113739, loss=2.7977519035339355
I0306 07:32:28.172428 139852573988608 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.40549328923225403, loss=2.8012354373931885
I0306 07:33:02.598289 139852565595904 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3947729468345642, loss=2.8106865882873535
I0306 07:33:36.999350 139852573988608 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.41195541620254517, loss=2.8052194118499756
I0306 07:34:11.383820 139852565595904 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.40524375438690186, loss=2.791656494140625
I0306 07:34:45.801474 139852573988608 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.4021543562412262, loss=2.759812593460083
I0306 07:35:20.200415 139852565595904 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3878714144229889, loss=2.821314573287964
I0306 07:35:54.622024 139852573988608 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.4189029633998871, loss=2.833904981613159
I0306 07:36:29.017165 139852565595904 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.39944586157798767, loss=2.764563798904419
I0306 07:37:03.406810 139852573988608 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.3786851167678833, loss=2.7395994663238525
I0306 07:37:23.405321 139996361643200 spec.py:321] Evaluating on the training split.
I0306 07:37:26.014229 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 07:40:18.203345 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 07:40:20.803043 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 07:43:02.104800 139996361643200 spec.py:349] Evaluating on the test split.
I0306 07:43:04.701200 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 07:45:32.233344 139996361643200 submission_runner.py:469] Time since start: 45092.42s, 	Step: 80759, 	{'train/accuracy': 0.6657702326774597, 'train/loss': 1.6849085092544556, 'train/bleu': 33.050108297240335, 'validation/accuracy': 0.6818901300430298, 'validation/loss': 1.5678340196609497, 'validation/bleu': 29.92182243543931, 'validation/num_examples': 3000, 'test/accuracy': 0.6957131028175354, 'test/loss': 1.491607427597046, 'test/bleu': 29.467978542710963, 'test/num_examples': 3003, 'score': 27747.00184559822, 'total_duration': 45092.41849684715, 'accumulated_submission_time': 27747.00184559822, 'accumulated_eval_time': 17339.991286039352, 'accumulated_logging_time': 0.7359263896942139}
I0306 07:45:32.251731 139852565595904 logging_writer.py:48] [80759] accumulated_eval_time=17340, accumulated_logging_time=0.735926, accumulated_submission_time=27747, global_step=80759, preemption_count=0, score=27747, test/accuracy=0.695713, test/bleu=29.468, test/loss=1.49161, test/num_examples=3003, total_duration=45092.4, train/accuracy=0.66577, train/bleu=33.0501, train/loss=1.68491, validation/accuracy=0.68189, validation/bleu=29.9218, validation/loss=1.56783, validation/num_examples=3000
I0306 07:45:46.631742 139852573988608 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.3884650766849518, loss=2.8618950843811035
I0306 07:46:20.968291 139852565595904 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.36678966879844666, loss=2.7284903526306152
I0306 07:46:55.377195 139852573988608 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.39934709668159485, loss=2.8168084621429443
I0306 07:47:29.814945 139852565595904 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3909188508987427, loss=2.834864854812622
I0306 07:48:04.231819 139852573988608 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.42332127690315247, loss=2.8150572776794434
I0306 07:48:38.656534 139852565595904 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.44204646348953247, loss=2.7579352855682373
I0306 07:49:13.083096 139852573988608 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3964667320251465, loss=2.735703468322754
I0306 07:49:47.532735 139852565595904 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.40309834480285645, loss=2.7289369106292725
I0306 07:50:21.965916 139852573988608 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3995063900947571, loss=2.7879481315612793
I0306 07:50:56.370817 139852565595904 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.4329947531223297, loss=2.857571601867676
I0306 07:51:30.756460 139852573988608 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.40077799558639526, loss=2.8113770484924316
I0306 07:52:05.158216 139852565595904 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.4074961543083191, loss=2.807429075241089
I0306 07:52:39.554973 139852573988608 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4148409068584442, loss=2.819432020187378
I0306 07:53:13.976340 139852565595904 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3792497217655182, loss=2.7815845012664795
I0306 07:53:48.377220 139852573988608 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.39703863859176636, loss=2.8376212120056152
I0306 07:54:22.738200 139852565595904 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.3839806020259857, loss=2.77296781539917
I0306 07:54:57.152465 139852573988608 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.41240960359573364, loss=2.7321252822875977
I0306 07:55:31.493134 139852565595904 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.4207649827003479, loss=2.816002368927002
I0306 07:56:05.838442 139852573988608 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.39600640535354614, loss=2.7600221633911133
I0306 07:56:40.222952 139852565595904 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.40306419134140015, loss=2.7803876399993896
I0306 07:57:14.616422 139852573988608 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.40224748849868774, loss=2.7752230167388916
I0306 07:57:48.989678 139852565595904 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.401763916015625, loss=2.758690595626831
I0306 07:58:23.371397 139852573988608 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.4328360855579376, loss=2.836380958557129
I0306 07:58:57.765381 139852565595904 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.4245089888572693, loss=2.7694239616394043
I0306 07:59:32.147768 139852573988608 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.42999204993247986, loss=2.8253417015075684
I0306 07:59:32.502020 139996361643200 spec.py:321] Evaluating on the training split.
I0306 07:59:35.108628 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:02:23.939955 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 08:02:26.531410 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:05:04.459996 139996361643200 spec.py:349] Evaluating on the test split.
I0306 08:05:07.053582 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:07:40.051114 139996361643200 submission_runner.py:469] Time since start: 46420.24s, 	Step: 83202, 	{'train/accuracy': 0.6757742762565613, 'train/loss': 1.6251723766326904, 'train/bleu': 33.68281600775445, 'validation/accuracy': 0.6838430166244507, 'validation/loss': 1.5589230060577393, 'validation/bleu': 30.26013970469094, 'validation/num_examples': 3000, 'test/accuracy': 0.6983663439750671, 'test/loss': 1.4784269332885742, 'test/bleu': 30.072009301131, 'test/num_examples': 3003, 'score': 28587.103432178497, 'total_duration': 46420.23628664017, 'accumulated_submission_time': 28587.103432178497, 'accumulated_eval_time': 17827.540327310562, 'accumulated_logging_time': 0.76259446144104}
I0306 08:07:40.070325 139852565595904 logging_writer.py:48] [83202] accumulated_eval_time=17827.5, accumulated_logging_time=0.762594, accumulated_submission_time=28587.1, global_step=83202, preemption_count=0, score=28587.1, test/accuracy=0.698366, test/bleu=30.072, test/loss=1.47843, test/num_examples=3003, total_duration=46420.2, train/accuracy=0.675774, train/bleu=33.6828, train/loss=1.62517, validation/accuracy=0.683843, validation/bleu=30.2601, validation/loss=1.55892, validation/num_examples=3000
I0306 08:08:14.029901 139852573988608 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.40141016244888306, loss=2.7608835697174072
I0306 08:08:48.398550 139852565595904 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.4124930500984192, loss=2.811047315597534
I0306 08:09:22.743894 139852573988608 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.41796866059303284, loss=2.8013691902160645
I0306 08:09:57.151774 139852565595904 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.4324992597103119, loss=2.8772902488708496
I0306 08:10:31.514734 139852573988608 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.416097491979599, loss=2.801776885986328
I0306 08:11:05.910937 139852565595904 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.3966694474220276, loss=2.7654659748077393
I0306 08:11:40.274476 139852573988608 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.4052136540412903, loss=2.762603998184204
I0306 08:12:14.683778 139852565595904 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.4211319088935852, loss=2.74788236618042
I0306 08:12:49.109867 139852573988608 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.43225914239883423, loss=2.7552566528320312
I0306 08:13:23.521066 139852565595904 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.42171257734298706, loss=2.80104660987854
I0306 08:13:57.904006 139852573988608 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.4204293191432953, loss=2.7760400772094727
I0306 08:14:32.262457 139852565595904 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4024861752986908, loss=2.7778425216674805
I0306 08:15:06.612300 139852573988608 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.41406506299972534, loss=2.7852182388305664
I0306 08:15:40.969627 139852565595904 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.40349119901657104, loss=2.7763397693634033
I0306 08:16:15.356108 139852573988608 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.43573522567749023, loss=2.801278829574585
I0306 08:16:49.744036 139852565595904 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.4402138292789459, loss=2.786133289337158
I0306 08:17:24.139932 139852573988608 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4119672179222107, loss=2.7599759101867676
I0306 08:17:58.506334 139852565595904 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.43702057003974915, loss=2.758659839630127
I0306 08:18:32.844809 139852573988608 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4275924563407898, loss=2.7955942153930664
I0306 08:19:07.235661 139852565595904 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.41589492559432983, loss=2.7752952575683594
I0306 08:19:41.603955 139852573988608 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.41028711199760437, loss=2.74934720993042
I0306 08:20:15.993759 139852565595904 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.42397746443748474, loss=2.691981315612793
I0306 08:20:50.369910 139852573988608 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.42592471837997437, loss=2.779350519180298
I0306 08:21:24.719612 139852565595904 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.427731454372406, loss=2.7020082473754883
I0306 08:21:40.203565 139996361643200 spec.py:321] Evaluating on the training split.
I0306 08:21:42.819736 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:24:34.259114 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 08:24:36.863640 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:27:22.385059 139996361643200 spec.py:349] Evaluating on the test split.
I0306 08:27:24.976586 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:29:53.215305 139996361643200 submission_runner.py:469] Time since start: 47753.40s, 	Step: 85646, 	{'train/accuracy': 0.6707541346549988, 'train/loss': 1.6505005359649658, 'train/bleu': 33.528526463620025, 'validation/accuracy': 0.6846463680267334, 'validation/loss': 1.551901936531067, 'validation/bleu': 30.050078113742256, 'validation/num_examples': 3000, 'test/accuracy': 0.6988298296928406, 'test/loss': 1.4708362817764282, 'test/bleu': 30.01089694186151, 'test/num_examples': 3003, 'score': 29427.091165542603, 'total_duration': 47753.400482177734, 'accumulated_submission_time': 29427.091165542603, 'accumulated_eval_time': 18320.552021980286, 'accumulated_logging_time': 0.790024995803833}
I0306 08:29:53.234889 139852573988608 logging_writer.py:48] [85646] accumulated_eval_time=18320.6, accumulated_logging_time=0.790025, accumulated_submission_time=29427.1, global_step=85646, preemption_count=0, score=29427.1, test/accuracy=0.69883, test/bleu=30.0109, test/loss=1.47084, test/num_examples=3003, total_duration=47753.4, train/accuracy=0.670754, train/bleu=33.5285, train/loss=1.6505, validation/accuracy=0.684646, validation/bleu=30.0501, validation/loss=1.5519, validation/num_examples=3000
I0306 08:30:12.044012 139852565595904 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.432253897190094, loss=2.7889397144317627
I0306 08:30:46.365545 139852573988608 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.41178372502326965, loss=2.7666826248168945
I0306 08:31:20.722223 139852565595904 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.4336312711238861, loss=2.788325071334839
I0306 08:31:55.106131 139852573988608 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.41529202461242676, loss=2.7294509410858154
I0306 08:32:29.453884 139852565595904 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.42830073833465576, loss=2.7527456283569336
I0306 08:33:03.852064 139852573988608 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.4529076814651489, loss=2.7909042835235596
I0306 08:33:38.223868 139852565595904 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.44375506043434143, loss=2.7836403846740723
I0306 08:34:12.610949 139852573988608 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4482957124710083, loss=2.7171878814697266
I0306 08:34:46.980037 139852565595904 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.4172627031803131, loss=2.7315871715545654
I0306 08:35:21.347779 139852573988608 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4261343777179718, loss=2.7437028884887695
I0306 08:35:55.731341 139852565595904 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.419986367225647, loss=2.758901357650757
I0306 08:36:30.112981 139852573988608 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.43925774097442627, loss=2.717470407485962
I0306 08:37:04.452156 139852565595904 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4217606484889984, loss=2.789646625518799
I0306 08:37:38.824178 139852573988608 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.4434720575809479, loss=2.829094171524048
I0306 08:38:13.184252 139852565595904 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4569467008113861, loss=2.7560548782348633
I0306 08:38:47.569216 139852573988608 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.44120848178863525, loss=2.743483304977417
I0306 08:39:21.945864 139852565595904 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4417201578617096, loss=2.7754602432250977
I0306 08:39:56.346512 139852573988608 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.452248752117157, loss=2.732537269592285
I0306 08:40:30.705579 139852565595904 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4550945460796356, loss=2.7775564193725586
I0306 08:41:05.050411 139852573988608 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.4477251172065735, loss=2.724306106567383
I0306 08:41:39.411837 139852565595904 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.45178085565567017, loss=2.7878801822662354
I0306 08:42:13.768069 139852573988608 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.45239874720573425, loss=2.8476624488830566
I0306 08:42:48.154019 139852565595904 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.4463560879230499, loss=2.7215638160705566
I0306 08:43:22.572125 139852573988608 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.45444726943969727, loss=2.727297067642212
I0306 08:43:53.549930 139996361643200 spec.py:321] Evaluating on the training split.
I0306 08:43:56.160113 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:47:16.179100 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 08:47:18.783803 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:49:58.362112 139996361643200 spec.py:349] Evaluating on the test split.
I0306 08:50:00.968819 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 08:52:44.499013 139996361643200 submission_runner.py:469] Time since start: 49124.68s, 	Step: 88091, 	{'train/accuracy': 0.6947277188301086, 'train/loss': 1.5200045108795166, 'train/bleu': 35.46539763300043, 'validation/accuracy': 0.6865251064300537, 'validation/loss': 1.5389983654022217, 'validation/bleu': 30.180751053936746, 'validation/num_examples': 3000, 'test/accuracy': 0.7015756964683533, 'test/loss': 1.46199369430542, 'test/bleu': 30.182986855272343, 'test/num_examples': 3003, 'score': 30267.26272702217, 'total_duration': 49124.68418049812, 'accumulated_submission_time': 30267.26272702217, 'accumulated_eval_time': 18851.501060009003, 'accumulated_logging_time': 0.8179807662963867}
I0306 08:52:44.519354 139852565595904 logging_writer.py:48] [88091] accumulated_eval_time=18851.5, accumulated_logging_time=0.817981, accumulated_submission_time=30267.3, global_step=88091, preemption_count=0, score=30267.3, test/accuracy=0.701576, test/bleu=30.183, test/loss=1.46199, test/num_examples=3003, total_duration=49124.7, train/accuracy=0.694728, train/bleu=35.4654, train/loss=1.52, validation/accuracy=0.686525, validation/bleu=30.1808, validation/loss=1.539, validation/num_examples=3000
I0306 08:52:47.959981 139852573988608 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.4694686233997345, loss=2.7175278663635254
I0306 08:53:22.280161 139852565595904 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.43729448318481445, loss=2.7001874446868896
I0306 08:53:56.691767 139852573988608 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.4350086450576782, loss=2.7941200733184814
I0306 08:54:31.135917 139852565595904 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.452989786863327, loss=2.8302500247955322
I0306 08:55:05.582679 139852573988608 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.44835177063941956, loss=2.747305393218994
I0306 08:55:40.018400 139852565595904 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.45395416021347046, loss=2.754178524017334
I0306 08:56:14.462903 139852573988608 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.44162997603416443, loss=2.7117621898651123
I0306 08:56:48.903454 139852565595904 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4367307126522064, loss=2.7203309535980225
I0306 08:57:23.337796 139852573988608 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4613445997238159, loss=2.702194929122925
I0306 08:57:57.752298 139852565595904 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.4492223560810089, loss=2.731126308441162
I0306 08:58:32.161834 139852573988608 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.47361859679222107, loss=2.72152042388916
I0306 08:59:06.563904 139852565595904 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.481183797121048, loss=2.750854253768921
I0306 08:59:40.963226 139852573988608 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.4536949396133423, loss=2.7296345233917236
I0306 09:00:15.405801 139852565595904 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.46846532821655273, loss=2.723356008529663
I0306 09:00:49.800342 139852573988608 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4882982671260834, loss=2.8217520713806152
I0306 09:01:24.159632 139852565595904 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.4636777937412262, loss=2.7311432361602783
I0306 09:01:58.525635 139852573988608 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.47380292415618896, loss=2.767286777496338
I0306 09:02:32.891146 139852565595904 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.48091259598731995, loss=2.8312020301818848
I0306 09:03:07.251106 139852573988608 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.4579910635948181, loss=2.7202529907226562
I0306 09:03:41.586026 139852565595904 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.4647064208984375, loss=2.7218427658081055
I0306 09:04:15.942815 139852573988608 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.46876397728919983, loss=2.757361650466919
I0306 09:04:50.256016 139852565595904 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.45394349098205566, loss=2.7016756534576416
I0306 09:05:24.570344 139852573988608 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.477662593126297, loss=2.722947359085083
I0306 09:05:58.852840 139852565595904 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.4674016833305359, loss=2.7454562187194824
I0306 09:06:33.170654 139852573988608 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4864920973777771, loss=2.7943811416625977
I0306 09:06:44.502305 139996361643200 spec.py:321] Evaluating on the training split.
I0306 09:06:47.105068 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:09:54.584576 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 09:09:57.180361 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:12:28.391865 139996361643200 spec.py:349] Evaluating on the test split.
I0306 09:12:30.993002 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:14:56.714945 139996361643200 submission_runner.py:469] Time since start: 50456.90s, 	Step: 90534, 	{'train/accuracy': 0.6797850728034973, 'train/loss': 1.5971362590789795, 'train/bleu': 33.997540994858284, 'validation/accuracy': 0.6864756941795349, 'validation/loss': 1.5378836393356323, 'validation/bleu': 30.250776616789363, 'validation/num_examples': 3000, 'test/accuracy': 0.7011354565620422, 'test/loss': 1.4582113027572632, 'test/bleu': 29.979678669959416, 'test/num_examples': 3003, 'score': 31107.094529151917, 'total_duration': 50456.90011167526, 'accumulated_submission_time': 31107.094529151917, 'accumulated_eval_time': 19343.713647842407, 'accumulated_logging_time': 0.8466839790344238}
I0306 09:14:56.733541 139852565595904 logging_writer.py:48] [90534] accumulated_eval_time=19343.7, accumulated_logging_time=0.846684, accumulated_submission_time=31107.1, global_step=90534, preemption_count=0, score=31107.1, test/accuracy=0.701135, test/bleu=29.9797, test/loss=1.45821, test/num_examples=3003, total_duration=50456.9, train/accuracy=0.679785, train/bleu=33.9975, train/loss=1.59714, validation/accuracy=0.686476, validation/bleu=30.2508, validation/loss=1.53788, validation/num_examples=3000
I0306 09:15:19.634599 139852573988608 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.45895707607269287, loss=2.7447946071624756
I0306 09:15:53.870681 139852565595904 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.4752905070781708, loss=2.746094226837158
I0306 09:16:28.191058 139852573988608 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4737076759338379, loss=2.7585577964782715
I0306 09:17:02.497131 139852565595904 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4657595157623291, loss=2.7516629695892334
I0306 09:17:36.793290 139852573988608 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.4871848225593567, loss=2.7365758419036865
I0306 09:18:11.113404 139852565595904 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.4468039870262146, loss=2.699871301651001
I0306 09:18:45.405985 139852573988608 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.46371710300445557, loss=2.7408525943756104
I0306 09:19:19.730248 139852565595904 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.4717412292957306, loss=2.716013193130493
I0306 09:19:54.067350 139852573988608 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4555164873600006, loss=2.7426114082336426
I0306 09:20:28.387842 139852565595904 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.47218796610832214, loss=2.7525737285614014
I0306 09:21:02.704771 139852573988608 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4900258481502533, loss=2.753382444381714
I0306 09:21:36.992254 139852565595904 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.46687051653862, loss=2.7328734397888184
I0306 09:22:11.292917 139852573988608 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.44493934512138367, loss=2.6958703994750977
I0306 09:22:45.575256 139852565595904 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4756107032299042, loss=2.708599090576172
I0306 09:23:19.890146 139852573988608 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.5026495456695557, loss=2.7733304500579834
I0306 09:23:54.166996 139852565595904 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.48067373037338257, loss=2.6987557411193848
I0306 09:24:28.506245 139852573988608 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.4957001507282257, loss=2.7693047523498535
I0306 09:25:02.824985 139852565595904 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.5011151432991028, loss=2.7647571563720703
I0306 09:25:37.117386 139852573988608 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4660416841506958, loss=2.7310078144073486
I0306 09:26:11.444489 139852565595904 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.4798891544342041, loss=2.761991024017334
I0306 09:26:45.762212 139852573988608 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.47532182931900024, loss=2.6814093589782715
I0306 09:27:20.060005 139852565595904 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.4513755738735199, loss=2.738140106201172
I0306 09:27:54.352186 139852573988608 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.4732196629047394, loss=2.7484586238861084
I0306 09:28:28.634171 139852565595904 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.5074882507324219, loss=2.725037097930908
I0306 09:28:56.774823 139996361643200 spec.py:321] Evaluating on the training split.
I0306 09:28:59.378083 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:32:01.214945 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 09:32:03.812459 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:34:37.038863 139996361643200 spec.py:349] Evaluating on the test split.
I0306 09:34:39.625947 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:37:21.435396 139996361643200 submission_runner.py:469] Time since start: 51801.62s, 	Step: 92983, 	{'train/accuracy': 0.6829541921615601, 'train/loss': 1.5863165855407715, 'train/bleu': 33.982226695050755, 'validation/accuracy': 0.6878476142883301, 'validation/loss': 1.5304203033447266, 'validation/bleu': 30.361367291714725, 'validation/num_examples': 3000, 'test/accuracy': 0.70405513048172, 'test/loss': 1.449535608291626, 'test/bleu': 30.32133796016519, 'test/num_examples': 3003, 'score': 31946.98639678955, 'total_duration': 51801.62057065964, 'accumulated_submission_time': 31946.98639678955, 'accumulated_eval_time': 19848.374175786972, 'accumulated_logging_time': 0.8735888004302979}
I0306 09:37:21.453966 139852573988608 logging_writer.py:48] [92983] accumulated_eval_time=19848.4, accumulated_logging_time=0.873589, accumulated_submission_time=31947, global_step=92983, preemption_count=0, score=31947, test/accuracy=0.704055, test/bleu=30.3213, test/loss=1.44954, test/num_examples=3003, total_duration=51801.6, train/accuracy=0.682954, train/bleu=33.9822, train/loss=1.58632, validation/accuracy=0.687848, validation/bleu=30.3614, validation/loss=1.53042, validation/num_examples=3000
I0306 09:37:27.618658 139852565595904 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4904252886772156, loss=2.7293999195098877
I0306 09:38:01.872633 139852573988608 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.5186022520065308, loss=2.739652156829834
I0306 09:38:36.111117 139852565595904 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.49757757782936096, loss=2.773210048675537
I0306 09:39:10.407753 139852573988608 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4838208854198456, loss=2.7775049209594727
I0306 09:39:44.700788 139852565595904 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.46868857741355896, loss=2.7172834873199463
I0306 09:40:18.987718 139852573988608 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.4844859540462494, loss=2.7255189418792725
I0306 09:40:53.268259 139852565595904 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.48847296833992004, loss=2.7182538509368896
I0306 09:41:27.563160 139852573988608 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.4823911488056183, loss=2.676931381225586
I0306 09:42:01.890603 139852565595904 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.4800090193748474, loss=2.6603622436523438
I0306 09:42:36.178475 139852573988608 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.48355406522750854, loss=2.678192615509033
I0306 09:43:10.439348 139852565595904 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.4985860288143158, loss=2.684922695159912
I0306 09:43:44.722224 139852573988608 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.5049551725387573, loss=2.7798821926116943
I0306 09:44:18.991942 139852565595904 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.5089635848999023, loss=2.7430579662323
I0306 09:44:53.258963 139852573988608 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.5237524509429932, loss=2.8116610050201416
I0306 09:45:27.551120 139852565595904 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.5159705281257629, loss=2.7683770656585693
I0306 09:46:01.840001 139852573988608 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.5109836459159851, loss=2.723823070526123
I0306 09:46:36.137362 139852565595904 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.5004496574401855, loss=2.718698501586914
I0306 09:47:10.436460 139852573988608 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.5060166716575623, loss=2.6975882053375244
I0306 09:47:44.739460 139852565595904 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.47800570726394653, loss=2.6376147270202637
I0306 09:48:19.030275 139852573988608 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.5161920189857483, loss=2.6890273094177246
I0306 09:48:53.332034 139852565595904 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.5163915753364563, loss=2.711775302886963
I0306 09:49:27.658369 139852573988608 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.48386046290397644, loss=2.708390712738037
I0306 09:50:01.950613 139852565595904 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.5218601226806641, loss=2.665637493133545
I0306 09:50:36.230094 139852573988608 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.5069683194160461, loss=2.6769237518310547
I0306 09:51:10.494871 139852565595904 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.5335285663604736, loss=2.7262797355651855
I0306 09:51:21.470294 139996361643200 spec.py:321] Evaluating on the training split.
I0306 09:51:24.069602 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:54:50.188243 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 09:54:52.772018 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 09:57:42.703800 139996361643200 spec.py:349] Evaluating on the test split.
I0306 09:57:45.294265 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 10:00:15.514660 139996361643200 submission_runner.py:469] Time since start: 53175.70s, 	Step: 95433, 	{'train/accuracy': 0.6924535632133484, 'train/loss': 1.5221549272537231, 'train/bleu': 34.959723857890715, 'validation/accuracy': 0.6895285844802856, 'validation/loss': 1.523034691810608, 'validation/bleu': 30.494954424159523, 'validation/num_examples': 3000, 'test/accuracy': 0.7047155499458313, 'test/loss': 1.4444953203201294, 'test/bleu': 30.529157587202786, 'test/num_examples': 3003, 'score': 32786.85821843147, 'total_duration': 53175.69983482361, 'accumulated_submission_time': 32786.85821843147, 'accumulated_eval_time': 20382.418494939804, 'accumulated_logging_time': 0.900609016418457}
I0306 10:00:15.533900 139852573988608 logging_writer.py:48] [95433] accumulated_eval_time=20382.4, accumulated_logging_time=0.900609, accumulated_submission_time=32786.9, global_step=95433, preemption_count=0, score=32786.9, test/accuracy=0.704716, test/bleu=30.5292, test/loss=1.4445, test/num_examples=3003, total_duration=53175.7, train/accuracy=0.692454, train/bleu=34.9597, train/loss=1.52215, validation/accuracy=0.689529, validation/bleu=30.495, validation/loss=1.52303, validation/num_examples=3000
I0306 10:00:38.791578 139852565595904 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.5165660977363586, loss=2.7281551361083984
I0306 10:01:13.077856 139852573988608 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.5007778406143188, loss=2.7217679023742676
I0306 10:01:47.378935 139852565595904 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.5045705437660217, loss=2.654196262359619
I0306 10:02:21.683924 139852573988608 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.4941709339618683, loss=2.6435024738311768
I0306 10:02:55.984611 139852565595904 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.5112894773483276, loss=2.7546374797821045
I0306 10:03:30.276237 139852573988608 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5154537558555603, loss=2.6241326332092285
I0306 10:04:04.561181 139852565595904 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.5229110717773438, loss=2.6879584789276123
I0306 10:04:38.886730 139852573988608 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.5378326773643494, loss=2.744433879852295
I0306 10:05:13.173447 139852565595904 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.5143452286720276, loss=2.689091920852661
I0306 10:05:47.496129 139852573988608 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5151603817939758, loss=2.68104887008667
I0306 10:06:21.817129 139852565595904 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.5539551377296448, loss=2.760789155960083
I0306 10:06:56.128915 139852573988608 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.5144202709197998, loss=2.6383965015411377
I0306 10:07:30.451422 139852565595904 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.5190076231956482, loss=2.695387601852417
I0306 10:08:04.760498 139852573988608 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5177878737449646, loss=2.687293291091919
I0306 10:08:39.068893 139852565595904 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.5231188535690308, loss=2.651005744934082
I0306 10:09:13.401269 139852573988608 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.525432288646698, loss=2.7095367908477783
I0306 10:09:47.679550 139852565595904 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.5147656202316284, loss=2.6612308025360107
I0306 10:10:21.988702 139852573988608 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.5170369744300842, loss=2.6901004314422607
I0306 10:10:56.297032 139852565595904 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5210046768188477, loss=2.6733200550079346
I0306 10:11:30.619814 139852573988608 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5522646903991699, loss=2.6597046852111816
I0306 10:12:04.924474 139852565595904 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.558310329914093, loss=2.6745553016662598
I0306 10:12:39.218958 139852573988608 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.5387536287307739, loss=2.6349503993988037
I0306 10:13:13.493379 139852565595904 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.5259332060813904, loss=2.6714112758636475
I0306 10:13:47.771594 139852573988608 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.5324216485023499, loss=2.6783876419067383
I0306 10:14:15.564406 139996361643200 spec.py:321] Evaluating on the training split.
I0306 10:14:18.163940 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 10:17:40.268982 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 10:17:42.884266 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 10:20:25.004281 139996361643200 spec.py:349] Evaluating on the test split.
I0306 10:20:27.590426 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 10:22:54.367823 139996361643200 submission_runner.py:469] Time since start: 54534.55s, 	Step: 97882, 	{'train/accuracy': 0.6847188472747803, 'train/loss': 1.5716965198516846, 'train/bleu': 34.628114027600134, 'validation/accuracy': 0.6906286478042603, 'validation/loss': 1.5188223123550415, 'validation/bleu': 30.61172410487406, 'validation/num_examples': 3000, 'test/accuracy': 0.7072876691818237, 'test/loss': 1.433120608329773, 'test/bleu': 30.47436907166849, 'test/num_examples': 3003, 'score': 33626.742523908615, 'total_duration': 54534.552993535995, 'accumulated_submission_time': 33626.742523908615, 'accumulated_eval_time': 20901.221860170364, 'accumulated_logging_time': 0.9282701015472412}
I0306 10:22:54.386814 139852565595904 logging_writer.py:48] [97882] accumulated_eval_time=20901.2, accumulated_logging_time=0.92827, accumulated_submission_time=33626.7, global_step=97882, preemption_count=0, score=33626.7, test/accuracy=0.707288, test/bleu=30.4744, test/loss=1.43312, test/num_examples=3003, total_duration=54534.6, train/accuracy=0.684719, train/bleu=34.6281, train/loss=1.5717, validation/accuracy=0.690629, validation/bleu=30.6117, validation/loss=1.51882, validation/num_examples=3000
I0306 10:23:00.902559 139852573988608 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.5277943015098572, loss=2.6616599559783936
I0306 10:23:35.104307 139852565595904 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5376322269439697, loss=2.6822075843811035
I0306 10:24:09.384509 139852573988608 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5260922908782959, loss=2.7126121520996094
I0306 10:24:43.662352 139852565595904 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.5274918079376221, loss=2.669069290161133
I0306 10:25:17.964869 139852573988608 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5648252964019775, loss=2.645752429962158
I0306 10:25:52.330349 139852565595904 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.5395584106445312, loss=2.662719249725342
I0306 10:26:26.600453 139852573988608 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5679683089256287, loss=2.741978168487549
I0306 10:27:00.888447 139852565595904 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5388832688331604, loss=2.6804332733154297
I0306 10:27:35.168113 139852573988608 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5282337069511414, loss=2.7125022411346436
I0306 10:28:09.496713 139852565595904 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5488038659095764, loss=2.631618022918701
I0306 10:28:43.820669 139852573988608 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5418140888214111, loss=2.672940731048584
I0306 10:29:18.142973 139852565595904 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5541707873344421, loss=2.6935160160064697
I0306 10:29:52.456473 139852573988608 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5274971723556519, loss=2.658170700073242
I0306 10:30:26.742296 139852565595904 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5509220957756042, loss=2.5973026752471924
I0306 10:31:01.033881 139852573988608 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5632023215293884, loss=2.6828043460845947
I0306 10:31:35.345444 139852565595904 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5483781099319458, loss=2.7316064834594727
I0306 10:32:09.642209 139852573988608 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5508671998977661, loss=2.674211263656616
I0306 10:32:43.953577 139852565595904 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5332900285720825, loss=2.6552236080169678
I0306 10:33:18.282048 139852573988608 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5371191501617432, loss=2.6354572772979736
I0306 10:33:52.609324 139852565595904 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5774579644203186, loss=2.7098548412323
I0306 10:34:26.919353 139852573988608 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5649392604827881, loss=2.64947509765625
I0306 10:35:01.239203 139852565595904 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5436025857925415, loss=2.6857662200927734
I0306 10:35:35.554788 139852573988608 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5450744032859802, loss=2.6713526248931885
I0306 10:36:09.881946 139852565595904 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5479456782341003, loss=2.687777042388916
I0306 10:36:44.175077 139852573988608 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5512832403182983, loss=2.7013206481933594
I0306 10:36:54.491253 139996361643200 spec.py:321] Evaluating on the training split.
I0306 10:36:57.091448 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 10:40:28.051932 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 10:40:30.653663 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 10:43:05.269154 139996361643200 spec.py:349] Evaluating on the test split.
I0306 10:43:07.884243 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 10:45:51.165186 139996361643200 submission_runner.py:469] Time since start: 55911.35s, 	Step: 100331, 	{'train/accuracy': 0.6922535300254822, 'train/loss': 1.5347938537597656, 'train/bleu': 35.01068095870374, 'validation/accuracy': 0.6912219524383545, 'validation/loss': 1.5123759508132935, 'validation/bleu': 30.693625734771274, 'validation/num_examples': 3000, 'test/accuracy': 0.7085042595863342, 'test/loss': 1.4301732778549194, 'test/bleu': 30.658337950220538, 'test/num_examples': 3003, 'score': 34466.69800066948, 'total_duration': 55911.35035943985, 'accumulated_submission_time': 34466.69800066948, 'accumulated_eval_time': 21437.895745754242, 'accumulated_logging_time': 0.9557845592498779}
I0306 10:45:51.184682 139852565595904 logging_writer.py:48] [100331] accumulated_eval_time=21437.9, accumulated_logging_time=0.955785, accumulated_submission_time=34466.7, global_step=100331, preemption_count=0, score=34466.7, test/accuracy=0.708504, test/bleu=30.6583, test/loss=1.43017, test/num_examples=3003, total_duration=55911.4, train/accuracy=0.692254, train/bleu=35.0107, train/loss=1.53479, validation/accuracy=0.691222, validation/bleu=30.6936, validation/loss=1.51238, validation/num_examples=3000
I0306 10:46:15.105742 139852573988608 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5713636875152588, loss=2.6813130378723145
I0306 10:46:49.260048 139852565595904 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5743987560272217, loss=2.7119638919830322
I0306 10:47:23.502844 139852573988608 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5665829181671143, loss=2.68021559715271
I0306 10:47:57.691321 139852565595904 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5583618879318237, loss=2.7065823078155518
I0306 10:48:31.901584 139852573988608 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5638040900230408, loss=2.657999038696289
I0306 10:49:06.139305 139852565595904 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5611072182655334, loss=2.6561365127563477
I0306 10:49:40.337106 139852573988608 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5868416428565979, loss=2.6932883262634277
I0306 10:50:14.558972 139852565595904 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5877909064292908, loss=2.6773080825805664
I0306 10:50:48.797379 139852573988608 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5716767907142639, loss=2.6650583744049072
I0306 10:51:23.003633 139852565595904 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.5659831166267395, loss=2.592881917953491
I0306 10:51:57.239931 139852573988608 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.575935959815979, loss=2.696338176727295
I0306 10:52:31.447230 139852565595904 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5798486471176147, loss=2.7119903564453125
I0306 10:53:05.643151 139852573988608 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5737124681472778, loss=2.6763224601745605
I0306 10:53:39.857585 139852565595904 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5926870107650757, loss=2.6530022621154785
I0306 10:54:14.103500 139852573988608 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5805014967918396, loss=2.64833664894104
I0306 10:54:48.334053 139852565595904 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5626041293144226, loss=2.6710283756256104
I0306 10:55:22.553909 139852573988608 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.571474015712738, loss=2.7701070308685303
I0306 10:55:56.773851 139852565595904 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.623292088508606, loss=2.6402766704559326
I0306 10:56:31.007850 139852573988608 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5787621140480042, loss=2.5330471992492676
I0306 10:57:05.219388 139852565595904 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5799472332000732, loss=2.6422600746154785
I0306 10:57:39.416970 139852573988608 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5908923149108887, loss=2.603538990020752
I0306 10:58:13.629698 139852565595904 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5537464618682861, loss=2.640836477279663
I0306 10:58:47.863392 139852573988608 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5641140341758728, loss=2.682645797729492
I0306 10:59:22.084564 139852565595904 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5949535369873047, loss=2.640414237976074
I0306 10:59:51.189066 139996361643200 spec.py:321] Evaluating on the training split.
I0306 10:59:53.795057 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 11:03:32.049981 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 11:03:34.649696 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 11:06:25.966865 139996361643200 spec.py:349] Evaluating on the test split.
I0306 11:06:28.561703 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 11:09:16.092498 139996361643200 submission_runner.py:469] Time since start: 57316.28s, 	Step: 102786, 	{'train/accuracy': 0.696261465549469, 'train/loss': 1.5091254711151123, 'train/bleu': 35.16229026938933, 'validation/accuracy': 0.6916050910949707, 'validation/loss': 1.5091300010681152, 'validation/bleu': 30.837386780955292, 'validation/num_examples': 3000, 'test/accuracy': 0.7082725167274475, 'test/loss': 1.4248815774917603, 'test/bleu': 30.592381124465888, 'test/num_examples': 3003, 'score': 35306.553372621536, 'total_duration': 57316.277654886246, 'accumulated_submission_time': 35306.553372621536, 'accumulated_eval_time': 22002.799112796783, 'accumulated_logging_time': 0.9847755432128906}
I0306 11:09:16.111943 139852573988608 logging_writer.py:48] [102786] accumulated_eval_time=22002.8, accumulated_logging_time=0.984776, accumulated_submission_time=35306.6, global_step=102786, preemption_count=0, score=35306.6, test/accuracy=0.708273, test/bleu=30.5924, test/loss=1.42488, test/num_examples=3003, total_duration=57316.3, train/accuracy=0.696261, train/bleu=35.1623, train/loss=1.50913, validation/accuracy=0.691605, validation/bleu=30.8374, validation/loss=1.50913, validation/num_examples=3000
I0306 11:09:21.242965 139852565595904 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5993930101394653, loss=2.6176512241363525
I0306 11:09:55.361942 139852573988608 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5908929109573364, loss=2.637596845626831
I0306 11:10:29.574292 139852565595904 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.602773904800415, loss=2.623938798904419
I0306 11:11:03.752235 139852573988608 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5983986258506775, loss=2.625026226043701
I0306 11:11:37.969337 139852565595904 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5737934708595276, loss=2.6361749172210693
I0306 11:12:12.166585 139852573988608 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5946150422096252, loss=2.5627782344818115
I0306 11:12:46.394785 139852565595904 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.6468282341957092, loss=2.661217451095581
I0306 11:13:20.644445 139852573988608 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.6103827953338623, loss=2.6935677528381348
I0306 11:13:54.892923 139852565595904 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.6056515574455261, loss=2.7347025871276855
I0306 11:14:29.120901 139852573988608 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5859049558639526, loss=2.6622378826141357
I0306 11:15:03.339092 139852565595904 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.6220219731330872, loss=2.6293962001800537
I0306 11:15:37.570137 139852573988608 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.9004212617874146, loss=2.736161708831787
I0306 11:16:11.795720 139852565595904 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.6511304378509521, loss=2.641850709915161
I0306 11:16:46.019644 139852573988608 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5986688137054443, loss=2.708327054977417
I0306 11:17:20.255928 139852565595904 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.5969434380531311, loss=2.727111577987671
I0306 11:17:54.460278 139852573988608 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5998976826667786, loss=2.635467052459717
I0306 11:18:28.673893 139852565595904 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.6012135148048401, loss=2.631371021270752
I0306 11:19:02.887586 139852573988608 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.588539183139801, loss=2.6130106449127197
I0306 11:19:37.132405 139852565595904 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5794036388397217, loss=2.634861946105957
I0306 11:20:11.321919 139852573988608 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.6179893612861633, loss=2.662034511566162
I0306 11:20:45.537606 139852565595904 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.6109254956245422, loss=2.631042957305908
I0306 11:21:19.731998 139852573988608 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.6263746619224548, loss=2.603267192840576
I0306 11:21:53.974972 139852565595904 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.629578709602356, loss=2.6783056259155273
I0306 11:22:28.169356 139852573988608 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.6214820742607117, loss=2.6137993335723877
I0306 11:23:02.387852 139852565595904 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.6090496182441711, loss=2.640854835510254
I0306 11:23:16.416361 139996361643200 spec.py:321] Evaluating on the training split.
I0306 11:23:19.023133 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 11:26:35.584805 139996361643200 spec.py:333] Evaluating on the validation split.
I0306 11:26:38.177394 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 11:29:11.003258 139996361643200 spec.py:349] Evaluating on the test split.
I0306 11:29:13.590518 139996361643200 workload.py:181] Translating evaluation dataset.
I0306 11:31:37.757290 139996361643200 submission_runner.py:469] Time since start: 58657.94s, 	Step: 105242, 	{'train/accuracy': 0.6965323090553284, 'train/loss': 1.5030683279037476, 'train/bleu': 35.51545753319166, 'validation/accuracy': 0.6938298940658569, 'validation/loss': 1.505064606666565, 'validation/bleu': 30.979064226538934, 'validation/num_examples': 3000, 'test/accuracy': 0.7081218957901001, 'test/loss': 1.4200738668441772, 'test/bleu': 30.82045043625403, 'test/num_examples': 3003, 'score': 36146.710327625275, 'total_duration': 58657.94246172905, 'accumulated_submission_time': 36146.710327625275, 'accumulated_eval_time': 22504.14000272751, 'accumulated_logging_time': 1.0132761001586914}
I0306 11:31:37.777229 139852573988608 logging_writer.py:48] [105242] accumulated_eval_time=22504.1, accumulated_logging_time=1.01328, accumulated_submission_time=36146.7, global_step=105242, preemption_count=0, score=36146.7, test/accuracy=0.708122, test/bleu=30.8205, test/loss=1.42007, test/num_examples=3003, total_duration=58657.9, train/accuracy=0.696532, train/bleu=35.5155, train/loss=1.50307, validation/accuracy=0.69383, validation/bleu=30.9791, validation/loss=1.50506, validation/num_examples=3000
I0306 11:31:37.797655 139852565595904 logging_writer.py:48] [105242] global_step=105242, preemption_count=0, score=36146.7
I0306 11:31:37.826062 139996361643200 submission_runner.py:646] Tuning trial 1/5
I0306 11:31:37.826187 139996361643200 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0306 11:31:37.828067 139996361643200 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005588631611317396, 'train/loss': 11.102604866027832, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.103591918945312, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.07886028289795, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.06490731239319, 'total_duration': 934.5419485569, 'accumulated_submission_time': 26.06490731239319, 'accumulated_eval_time': 908.4769337177277, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2444, {'train/accuracy': 0.42766329646110535, 'train/loss': 3.819026470184326, 'train/bleu': 15.501693148832048, 'validation/accuracy': 0.41509902477264404, 'validation/loss': 3.926219940185547, 'validation/bleu': 10.91917970302653, 'validation/num_examples': 3000, 'test/accuracy': 0.4027111530303955, 'test/loss': 4.0999627113342285, 'test/bleu': 9.337357446358707, 'test/num_examples': 3003, 'score': 865.9629728794098, 'total_duration': 2441.249675989151, 'accumulated_submission_time': 865.9629728794098, 'accumulated_eval_time': 1575.1074559688568, 'accumulated_logging_time': 0.0168459415435791, 'global_step': 2444, 'preemption_count': 0}), (4884, {'train/accuracy': 0.5439789891242981, 'train/loss': 2.7240521907806396, 'train/bleu': 24.96267639215194, 'validation/accuracy': 0.5521840453147888, 'validation/loss': 2.6358182430267334, 'validation/bleu': 21.00371461439621, 'validation/num_examples': 3000, 'test/accuracy': 0.5547561049461365, 'test/loss': 2.6496100425720215, 'test/bleu': 19.70302729416108, 'test/num_examples': 3003, 'score': 1705.9944531917572, 'total_duration': 3795.3893852233887, 'accumulated_submission_time': 1705.9944531917572, 'accumulated_eval_time': 2089.0370349884033, 'accumulated_logging_time': 0.035944223403930664, 'global_step': 4884, 'preemption_count': 0}), (7327, {'train/accuracy': 0.5850558280944824, 'train/loss': 2.3225369453430176, 'train/bleu': 27.38493782508135, 'validation/accuracy': 0.592292308807373, 'validation/loss': 2.2635257244110107, 'validation/bleu': 23.775843553381925, 'validation/num_examples': 3000, 'test/accuracy': 0.5948673486709595, 'test/loss': 2.259368419647217, 'test/bleu': 22.278679273909916, 'test/num_examples': 3003, 'score': 2545.8934302330017, 'total_duration': 5184.623443603516, 'accumulated_submission_time': 2545.8934302330017, 'accumulated_eval_time': 2638.192267894745, 'accumulated_logging_time': 0.05415701866149902, 'global_step': 7327, 'preemption_count': 0}), (9777, {'train/accuracy': 0.5963719487190247, 'train/loss': 2.214189052581787, 'train/bleu': 28.566705599822033, 'validation/accuracy': 0.6121548414230347, 'validation/loss': 2.0879898071289062, 'validation/bleu': 24.896545524690787, 'validation/num_examples': 3000, 'test/accuracy': 0.6181322932243347, 'test/loss': 2.052985906600952, 'test/bleu': 24.018577984426827, 'test/num_examples': 3003, 'score': 3385.793024301529, 'total_duration': 6493.27650642395, 'accumulated_submission_time': 3385.793024301529, 'accumulated_eval_time': 3106.773915052414, 'accumulated_logging_time': 0.07386898994445801, 'global_step': 9777, 'preemption_count': 0}), (12230, {'train/accuracy': 0.6070840358734131, 'train/loss': 2.116654634475708, 'train/bleu': 28.913669001751153, 'validation/accuracy': 0.6239463090896606, 'validation/loss': 1.9805448055267334, 'validation/bleu': 25.776371719076966, 'validation/num_examples': 3000, 'test/accuracy': 0.6332406401634216, 'test/loss': 1.9347119331359863, 'test/bleu': 25.131792310691463, 'test/num_examples': 3003, 'score': 4225.8612949848175, 'total_duration': 7791.86346578598, 'accumulated_submission_time': 4225.8612949848175, 'accumulated_eval_time': 3565.1266074180603, 'accumulated_logging_time': 0.09369301795959473, 'global_step': 12230, 'preemption_count': 0}), (14679, {'train/accuracy': 0.6140821576118469, 'train/loss': 2.0441009998321533, 'train/bleu': 29.339519183401404, 'validation/accuracy': 0.6312881708145142, 'validation/loss': 1.9201366901397705, 'validation/bleu': 26.283284815228836, 'validation/num_examples': 3000, 'test/accuracy': 0.6383153796195984, 'test/loss': 1.8819077014923096, 'test/bleu': 25.38245347735306, 'test/num_examples': 3003, 'score': 5066.032030582428, 'total_duration': 9170.298466205597, 'accumulated_submission_time': 5066.032030582428, 'accumulated_eval_time': 4103.227353334427, 'accumulated_logging_time': 0.11328911781311035, 'global_step': 14679, 'preemption_count': 0}), (17127, {'train/accuracy': 0.6137308478355408, 'train/loss': 2.0295145511627197, 'train/bleu': 29.828357931834226, 'validation/accuracy': 0.6393221616744995, 'validation/loss': 1.8558814525604248, 'validation/bleu': 26.933973906139975, 'validation/num_examples': 3000, 'test/accuracy': 0.6484764218330383, 'test/loss': 1.803090214729309, 'test/bleu': 26.389998452299483, 'test/num_examples': 3003, 'score': 5906.204601764679, 'total_duration': 10467.458442687988, 'accumulated_submission_time': 5906.204601764679, 'accumulated_eval_time': 4560.048267841339, 'accumulated_logging_time': 0.1326441764831543, 'global_step': 17127, 'preemption_count': 0}), (19573, {'train/accuracy': 0.631948709487915, 'train/loss': 1.8994444608688354, 'train/bleu': 31.340773498976425, 'validation/accuracy': 0.6457246541976929, 'validation/loss': 1.803261637687683, 'validation/bleu': 27.58544274783058, 'validation/num_examples': 3000, 'test/accuracy': 0.6546981930732727, 'test/loss': 1.749428153038025, 'test/bleu': 26.657551100160216, 'test/num_examples': 3003, 'score': 6746.311555862427, 'total_duration': 11783.578448057175, 'accumulated_submission_time': 6746.311555862427, 'accumulated_eval_time': 5035.902393817902, 'accumulated_logging_time': 0.1516437530517578, 'global_step': 19573, 'preemption_count': 0}), (22013, {'train/accuracy': 0.6309760212898254, 'train/loss': 1.919883370399475, 'train/bleu': 29.972452552872173, 'validation/accuracy': 0.6479000449180603, 'validation/loss': 1.7862048149108887, 'validation/bleu': 27.616608391336992, 'validation/num_examples': 3000, 'test/accuracy': 0.6586490869522095, 'test/loss': 1.7273226976394653, 'test/bleu': 26.89697206658344, 'test/num_examples': 3003, 'score': 7586.205486297607, 'total_duration': 13107.431776285172, 'accumulated_submission_time': 7586.205486297607, 'accumulated_eval_time': 5519.703406572342, 'accumulated_logging_time': 0.17116641998291016, 'global_step': 22013, 'preemption_count': 0}), (24453, {'train/accuracy': 0.630093514919281, 'train/loss': 1.9250246286392212, 'train/bleu': 30.745060661851227, 'validation/accuracy': 0.6523991227149963, 'validation/loss': 1.7541091442108154, 'validation/bleu': 27.896602015347828, 'validation/num_examples': 3000, 'test/accuracy': 0.6636310815811157, 'test/loss': 1.694322943687439, 'test/bleu': 27.200900934566015, 'test/num_examples': 3003, 'score': 8426.276630401611, 'total_duration': 14452.78630566597, 'accumulated_submission_time': 8426.276630401611, 'accumulated_eval_time': 6024.829742908478, 'accumulated_logging_time': 0.19096827507019043, 'global_step': 24453, 'preemption_count': 0}), (26897, {'train/accuracy': 0.6316536068916321, 'train/loss': 1.8966907262802124, 'train/bleu': 30.63339271877373, 'validation/accuracy': 0.6530294418334961, 'validation/loss': 1.7591471672058105, 'validation/bleu': 28.001520481179124, 'validation/num_examples': 3000, 'test/accuracy': 0.6632371544837952, 'test/loss': 1.695162296295166, 'test/bleu': 27.010411821341467, 'test/num_examples': 3003, 'score': 9266.43312907219, 'total_duration': 15781.240503549576, 'accumulated_submission_time': 9266.43312907219, 'accumulated_eval_time': 6512.965662956238, 'accumulated_logging_time': 0.21100878715515137, 'global_step': 26897, 'preemption_count': 0}), (29342, {'train/accuracy': 0.6302140355110168, 'train/loss': 1.9148298501968384, 'train/bleu': 31.00624360944345, 'validation/accuracy': 0.6549205183982849, 'validation/loss': 1.7438840866088867, 'validation/bleu': 27.845483539843354, 'validation/num_examples': 3000, 'test/accuracy': 0.665554404258728, 'test/loss': 1.6838868856430054, 'test/bleu': 27.090925099516923, 'test/num_examples': 3003, 'score': 10106.327021360397, 'total_duration': 17098.488652944565, 'accumulated_submission_time': 10106.327021360397, 'accumulated_eval_time': 6990.158514261246, 'accumulated_logging_time': 0.23262643814086914, 'global_step': 29342, 'preemption_count': 0}), (31787, {'train/accuracy': 0.6535369753837585, 'train/loss': 1.761945366859436, 'train/bleu': 31.64568627625226, 'validation/accuracy': 0.6559464335441589, 'validation/loss': 1.7315243482589722, 'validation/bleu': 28.34224495962689, 'validation/num_examples': 3000, 'test/accuracy': 0.6664465069770813, 'test/loss': 1.6702311038970947, 'test/bleu': 27.54304371803419, 'test/num_examples': 3003, 'score': 10946.194816350937, 'total_duration': 18449.31985449791, 'accumulated_submission_time': 10946.194816350937, 'accumulated_eval_time': 7500.962494134903, 'accumulated_logging_time': 0.2536015510559082, 'global_step': 31787, 'preemption_count': 0}), (34230, {'train/accuracy': 0.6368748545646667, 'train/loss': 1.8621817827224731, 'train/bleu': 31.009493052839204, 'validation/accuracy': 0.6566632986068726, 'validation/loss': 1.7261155843734741, 'validation/bleu': 28.147480755086054, 'validation/num_examples': 3000, 'test/accuracy': 0.6686478853225708, 'test/loss': 1.6565425395965576, 'test/bleu': 27.698284666225053, 'test/num_examples': 3003, 'score': 11786.37015247345, 'total_duration': 19783.655319690704, 'accumulated_submission_time': 11786.37015247345, 'accumulated_eval_time': 7994.962661981583, 'accumulated_logging_time': 0.27576756477355957, 'global_step': 34230, 'preemption_count': 0}), (36679, {'train/accuracy': 0.6393715143203735, 'train/loss': 1.8610005378723145, 'train/bleu': 31.27685248618663, 'validation/accuracy': 0.6574667096138, 'validation/loss': 1.7112897634506226, 'validation/bleu': 28.119425215404167, 'validation/num_examples': 3000, 'test/accuracy': 0.6690070629119873, 'test/loss': 1.6529209613800049, 'test/bleu': 27.555001614043974, 'test/num_examples': 3003, 'score': 12626.421940803528, 'total_duration': 21106.497403621674, 'accumulated_submission_time': 12626.421940803528, 'accumulated_eval_time': 8477.59060049057, 'accumulated_logging_time': 0.29990553855895996, 'global_step': 36679, 'preemption_count': 0}), (39130, {'train/accuracy': 0.6414465308189392, 'train/loss': 1.8317885398864746, 'train/bleu': 31.235375877789163, 'validation/accuracy': 0.6605443358421326, 'validation/loss': 1.6982654333114624, 'validation/bleu': 28.548513068567534, 'validation/num_examples': 3000, 'test/accuracy': 0.6736183762550354, 'test/loss': 1.632981777191162, 'test/bleu': 27.747848323361666, 'test/num_examples': 3003, 'score': 13466.453862190247, 'total_duration': 22426.746995210648, 'accumulated_submission_time': 13466.453862190247, 'accumulated_eval_time': 8957.649440050125, 'accumulated_logging_time': 0.3215906620025635, 'global_step': 39130, 'preemption_count': 0}), (41582, {'train/accuracy': 0.639885663986206, 'train/loss': 1.8503295183181763, 'train/bleu': 31.52679706986065, 'validation/accuracy': 0.6605443358421326, 'validation/loss': 1.6916389465332031, 'validation/bleu': 28.751247971351795, 'validation/num_examples': 3000, 'test/accuracy': 0.6760166883468628, 'test/loss': 1.6236522197723389, 'test/bleu': 28.27465753841795, 'test/num_examples': 3003, 'score': 14306.343038082123, 'total_duration': 23725.181736946106, 'accumulated_submission_time': 14306.343038082123, 'accumulated_eval_time': 9416.03389787674, 'accumulated_logging_time': 0.3431529998779297, 'global_step': 41582, 'preemption_count': 0}), (44034, {'train/accuracy': 0.6724395751953125, 'train/loss': 1.6391425132751465, 'train/bleu': 33.943103371507874, 'validation/accuracy': 0.6622500419616699, 'validation/loss': 1.680512547492981, 'validation/bleu': 28.57360978446224, 'validation/num_examples': 3000, 'test/accuracy': 0.6769088506698608, 'test/loss': 1.6194344758987427, 'test/bleu': 28.12031173168794, 'test/num_examples': 3003, 'score': 15146.35967206955, 'total_duration': 25043.39017677307, 'accumulated_submission_time': 15146.35967206955, 'accumulated_eval_time': 9894.060240268707, 'accumulated_logging_time': 0.3657071590423584, 'global_step': 44034, 'preemption_count': 0}), (46484, {'train/accuracy': 0.6441134214401245, 'train/loss': 1.818184494972229, 'train/bleu': 31.529587493624923, 'validation/accuracy': 0.6635107398033142, 'validation/loss': 1.6775881052017212, 'validation/bleu': 28.565059684854546, 'validation/num_examples': 3000, 'test/accuracy': 0.6762484312057495, 'test/loss': 1.6067540645599365, 'test/bleu': 28.323112699366018, 'test/num_examples': 3003, 'score': 15986.541936635971, 'total_duration': 26363.545403957367, 'accumulated_submission_time': 15986.541936635971, 'accumulated_eval_time': 10373.874564409256, 'accumulated_logging_time': 0.3871452808380127, 'global_step': 46484, 'preemption_count': 0}), (48933, {'train/accuracy': 0.643856406211853, 'train/loss': 1.8312432765960693, 'train/bleu': 31.67953308800338, 'validation/accuracy': 0.6646602153778076, 'validation/loss': 1.6712974309921265, 'validation/bleu': 28.853363415038007, 'validation/num_examples': 3000, 'test/accuracy': 0.6774881482124329, 'test/loss': 1.6015293598175049, 'test/bleu': 28.353558852972824, 'test/num_examples': 3003, 'score': 16826.603521585464, 'total_duration': 27675.32644534111, 'accumulated_submission_time': 16826.603521585464, 'accumulated_eval_time': 10845.43692946434, 'accumulated_logging_time': 0.40938735008239746, 'global_step': 48933, 'preemption_count': 0}), (51382, {'train/accuracy': 0.6523168683052063, 'train/loss': 1.7708276510238647, 'train/bleu': 32.35050249495313, 'validation/accuracy': 0.6672928929328918, 'validation/loss': 1.661936640739441, 'validation/bleu': 28.82592007195202, 'validation/num_examples': 3000, 'test/accuracy': 0.6784845590591431, 'test/loss': 1.5940595865249634, 'test/bleu': 28.586904075021454, 'test/num_examples': 3003, 'score': 17666.622935533524, 'total_duration': 28993.416855096817, 'accumulated_submission_time': 17666.622935533524, 'accumulated_eval_time': 11323.349648237228, 'accumulated_logging_time': 0.4312119483947754, 'global_step': 51382, 'preemption_count': 0}), (53830, {'train/accuracy': 0.6451072096824646, 'train/loss': 1.810860276222229, 'train/bleu': 31.246398789112245, 'validation/accuracy': 0.6674535870552063, 'validation/loss': 1.661716103553772, 'validation/bleu': 28.96608512135683, 'validation/num_examples': 3000, 'test/accuracy': 0.6782412528991699, 'test/loss': 1.5952285528182983, 'test/bleu': 28.181044192470285, 'test/num_examples': 3003, 'score': 18506.57123017311, 'total_duration': 30297.352501153946, 'accumulated_submission_time': 18506.57123017311, 'accumulated_eval_time': 11787.173393964767, 'accumulated_logging_time': 0.4540834426879883, 'global_step': 53830, 'preemption_count': 0}), (56279, {'train/accuracy': 0.6461771130561829, 'train/loss': 1.8151381015777588, 'train/bleu': 32.061809050393215, 'validation/accuracy': 0.6683682203292847, 'validation/loss': 1.650173544883728, 'validation/bleu': 28.98236802869139, 'validation/num_examples': 3000, 'test/accuracy': 0.6838257312774658, 'test/loss': 1.5725330114364624, 'test/bleu': 28.60594164670276, 'test/num_examples': 3003, 'score': 19346.625147104263, 'total_duration': 31601.275846481323, 'accumulated_submission_time': 19346.625147104263, 'accumulated_eval_time': 12250.884186029434, 'accumulated_logging_time': 0.4768850803375244, 'global_step': 56279, 'preemption_count': 0}), (58734, {'train/accuracy': 0.6509807109832764, 'train/loss': 1.7647664546966553, 'train/bleu': 32.33128945287037, 'validation/accuracy': 0.669431209564209, 'validation/loss': 1.6397720575332642, 'validation/bleu': 29.21710244818758, 'validation/num_examples': 3000, 'test/accuracy': 0.6824122667312622, 'test/loss': 1.5711911916732788, 'test/bleu': 28.769304351001292, 'test/num_examples': 3003, 'score': 20186.641088485718, 'total_duration': 32947.32786130905, 'accumulated_submission_time': 20186.641088485718, 'accumulated_eval_time': 12756.753923892975, 'accumulated_logging_time': 0.501591682434082, 'global_step': 58734, 'preemption_count': 0}), (61189, {'train/accuracy': 0.6516432166099548, 'train/loss': 1.7733469009399414, 'train/bleu': 31.85245357652231, 'validation/accuracy': 0.671272873878479, 'validation/loss': 1.626589059829712, 'validation/bleu': 29.111893790024837, 'validation/num_examples': 3000, 'test/accuracy': 0.6855173110961914, 'test/loss': 1.5592955350875854, 'test/bleu': 29.376111506420216, 'test/num_examples': 3003, 'score': 21026.598375320435, 'total_duration': 34264.806159973145, 'accumulated_submission_time': 21026.598375320435, 'accumulated_eval_time': 13234.111276626587, 'accumulated_logging_time': 0.5244767665863037, 'global_step': 61189, 'preemption_count': 0}), (63642, {'train/accuracy': 0.6644970178604126, 'train/loss': 1.6864296197891235, 'train/bleu': 33.187818723648704, 'validation/accuracy': 0.6722493171691895, 'validation/loss': 1.6226963996887207, 'validation/bleu': 29.091850564155884, 'validation/num_examples': 3000, 'test/accuracy': 0.6871509552001953, 'test/loss': 1.5464812517166138, 'test/bleu': 28.915360424304996, 'test/num_examples': 3003, 'score': 21866.745727062225, 'total_duration': 35594.23958897591, 'accumulated_submission_time': 21866.745727062225, 'accumulated_eval_time': 13723.234539270401, 'accumulated_logging_time': 0.5500822067260742, 'global_step': 63642, 'preemption_count': 0}), (66092, {'train/accuracy': 0.6554262042045593, 'train/loss': 1.7498739957809448, 'train/bleu': 32.483856194704494, 'validation/accuracy': 0.673312246799469, 'validation/loss': 1.6201781034469604, 'validation/bleu': 29.234932302328517, 'validation/num_examples': 3000, 'test/accuracy': 0.6884022951126099, 'test/loss': 1.5399383306503296, 'test/bleu': 28.93016399790172, 'test/num_examples': 3003, 'score': 22706.85374903679, 'total_duration': 36950.85386252403, 'accumulated_submission_time': 22706.85374903679, 'accumulated_eval_time': 14239.574616193771, 'accumulated_logging_time': 0.5749740600585938, 'global_step': 66092, 'preemption_count': 0}), (68542, {'train/accuracy': 0.653733491897583, 'train/loss': 1.7698416709899902, 'train/bleu': 32.42082237984671, 'validation/accuracy': 0.6751291751861572, 'validation/loss': 1.601650595664978, 'validation/bleu': 29.461086458437784, 'validation/num_examples': 3000, 'test/accuracy': 0.689190149307251, 'test/loss': 1.5305323600769043, 'test/bleu': 29.263862882613676, 'test/num_examples': 3003, 'score': 23546.953679561615, 'total_duration': 38348.12163424492, 'accumulated_submission_time': 23546.953679561615, 'accumulated_eval_time': 14796.575135231018, 'accumulated_logging_time': 0.6029472351074219, 'global_step': 68542, 'preemption_count': 0}), (70991, {'train/accuracy': 0.6656088829040527, 'train/loss': 1.6856144666671753, 'train/bleu': 32.68839644071378, 'validation/accuracy': 0.6772550940513611, 'validation/loss': 1.5932538509368896, 'validation/bleu': 29.48452785909497, 'validation/num_examples': 3000, 'test/accuracy': 0.6920287609100342, 'test/loss': 1.5191024541854858, 'test/bleu': 29.350704851050587, 'test/num_examples': 3003, 'score': 24386.959141492844, 'total_duration': 39743.80830025673, 'accumulated_submission_time': 24386.959141492844, 'accumulated_eval_time': 15352.087708473206, 'accumulated_logging_time': 0.6285481452941895, 'global_step': 70991, 'preemption_count': 0}), (73436, {'train/accuracy': 0.6601696014404297, 'train/loss': 1.7195792198181152, 'train/bleu': 32.34331833844137, 'validation/accuracy': 0.6761797666549683, 'validation/loss': 1.593739628791809, 'validation/bleu': 29.373874669258257, 'validation/num_examples': 3000, 'test/accuracy': 0.6914610266685486, 'test/loss': 1.5140252113342285, 'test/bleu': 29.288519856610918, 'test/num_examples': 3003, 'score': 25226.902934789658, 'total_duration': 41084.75736093521, 'accumulated_submission_time': 25226.902934789658, 'accumulated_eval_time': 15852.927433013916, 'accumulated_logging_time': 0.6539583206176758, 'global_step': 73436, 'preemption_count': 0}), (75877, {'train/accuracy': 0.677148699760437, 'train/loss': 1.6172224283218384, 'train/bleu': 33.74263148846099, 'validation/accuracy': 0.6797147393226624, 'validation/loss': 1.5837938785552979, 'validation/bleu': 29.711227934844704, 'validation/num_examples': 3000, 'test/accuracy': 0.6920750737190247, 'test/loss': 1.5132055282592773, 'test/bleu': 28.94981395555031, 'test/num_examples': 3003, 'score': 26067.066395998, 'total_duration': 42375.88261508942, 'accumulated_submission_time': 26067.066395998, 'accumulated_eval_time': 16303.722861289978, 'accumulated_logging_time': 0.6808102130889893, 'global_step': 75877, 'preemption_count': 0}), (78318, {'train/accuracy': 0.6633084416389465, 'train/loss': 1.7017958164215088, 'train/bleu': 33.23232868746256, 'validation/accuracy': 0.6793810129165649, 'validation/loss': 1.5754899978637695, 'validation/bleu': 29.998673149093527, 'validation/num_examples': 3000, 'test/accuracy': 0.6938361525535583, 'test/loss': 1.4985798597335815, 'test/bleu': 29.631323026778578, 'test/num_examples': 3003, 'score': 26907.07831954956, 'total_duration': 43763.500906705856, 'accumulated_submission_time': 26907.07831954956, 'accumulated_eval_time': 16851.163332223892, 'accumulated_logging_time': 0.70760178565979, 'global_step': 78318, 'preemption_count': 0}), (80759, {'train/accuracy': 0.6657702326774597, 'train/loss': 1.6849085092544556, 'train/bleu': 33.050108297240335, 'validation/accuracy': 0.6818901300430298, 'validation/loss': 1.5678340196609497, 'validation/bleu': 29.92182243543931, 'validation/num_examples': 3000, 'test/accuracy': 0.6957131028175354, 'test/loss': 1.491607427597046, 'test/bleu': 29.467978542710963, 'test/num_examples': 3003, 'score': 27747.00184559822, 'total_duration': 45092.41849684715, 'accumulated_submission_time': 27747.00184559822, 'accumulated_eval_time': 17339.991286039352, 'accumulated_logging_time': 0.7359263896942139, 'global_step': 80759, 'preemption_count': 0}), (83202, {'train/accuracy': 0.6757742762565613, 'train/loss': 1.6251723766326904, 'train/bleu': 33.68281600775445, 'validation/accuracy': 0.6838430166244507, 'validation/loss': 1.5589230060577393, 'validation/bleu': 30.26013970469094, 'validation/num_examples': 3000, 'test/accuracy': 0.6983663439750671, 'test/loss': 1.4784269332885742, 'test/bleu': 30.072009301131, 'test/num_examples': 3003, 'score': 28587.103432178497, 'total_duration': 46420.23628664017, 'accumulated_submission_time': 28587.103432178497, 'accumulated_eval_time': 17827.540327310562, 'accumulated_logging_time': 0.76259446144104, 'global_step': 83202, 'preemption_count': 0}), (85646, {'train/accuracy': 0.6707541346549988, 'train/loss': 1.6505005359649658, 'train/bleu': 33.528526463620025, 'validation/accuracy': 0.6846463680267334, 'validation/loss': 1.551901936531067, 'validation/bleu': 30.050078113742256, 'validation/num_examples': 3000, 'test/accuracy': 0.6988298296928406, 'test/loss': 1.4708362817764282, 'test/bleu': 30.01089694186151, 'test/num_examples': 3003, 'score': 29427.091165542603, 'total_duration': 47753.400482177734, 'accumulated_submission_time': 29427.091165542603, 'accumulated_eval_time': 18320.552021980286, 'accumulated_logging_time': 0.790024995803833, 'global_step': 85646, 'preemption_count': 0}), (88091, {'train/accuracy': 0.6947277188301086, 'train/loss': 1.5200045108795166, 'train/bleu': 35.46539763300043, 'validation/accuracy': 0.6865251064300537, 'validation/loss': 1.5389983654022217, 'validation/bleu': 30.180751053936746, 'validation/num_examples': 3000, 'test/accuracy': 0.7015756964683533, 'test/loss': 1.46199369430542, 'test/bleu': 30.182986855272343, 'test/num_examples': 3003, 'score': 30267.26272702217, 'total_duration': 49124.68418049812, 'accumulated_submission_time': 30267.26272702217, 'accumulated_eval_time': 18851.501060009003, 'accumulated_logging_time': 0.8179807662963867, 'global_step': 88091, 'preemption_count': 0}), (90534, {'train/accuracy': 0.6797850728034973, 'train/loss': 1.5971362590789795, 'train/bleu': 33.997540994858284, 'validation/accuracy': 0.6864756941795349, 'validation/loss': 1.5378836393356323, 'validation/bleu': 30.250776616789363, 'validation/num_examples': 3000, 'test/accuracy': 0.7011354565620422, 'test/loss': 1.4582113027572632, 'test/bleu': 29.979678669959416, 'test/num_examples': 3003, 'score': 31107.094529151917, 'total_duration': 50456.90011167526, 'accumulated_submission_time': 31107.094529151917, 'accumulated_eval_time': 19343.713647842407, 'accumulated_logging_time': 0.8466839790344238, 'global_step': 90534, 'preemption_count': 0}), (92983, {'train/accuracy': 0.6829541921615601, 'train/loss': 1.5863165855407715, 'train/bleu': 33.982226695050755, 'validation/accuracy': 0.6878476142883301, 'validation/loss': 1.5304203033447266, 'validation/bleu': 30.361367291714725, 'validation/num_examples': 3000, 'test/accuracy': 0.70405513048172, 'test/loss': 1.449535608291626, 'test/bleu': 30.32133796016519, 'test/num_examples': 3003, 'score': 31946.98639678955, 'total_duration': 51801.62057065964, 'accumulated_submission_time': 31946.98639678955, 'accumulated_eval_time': 19848.374175786972, 'accumulated_logging_time': 0.8735888004302979, 'global_step': 92983, 'preemption_count': 0}), (95433, {'train/accuracy': 0.6924535632133484, 'train/loss': 1.5221549272537231, 'train/bleu': 34.959723857890715, 'validation/accuracy': 0.6895285844802856, 'validation/loss': 1.523034691810608, 'validation/bleu': 30.494954424159523, 'validation/num_examples': 3000, 'test/accuracy': 0.7047155499458313, 'test/loss': 1.4444953203201294, 'test/bleu': 30.529157587202786, 'test/num_examples': 3003, 'score': 32786.85821843147, 'total_duration': 53175.69983482361, 'accumulated_submission_time': 32786.85821843147, 'accumulated_eval_time': 20382.418494939804, 'accumulated_logging_time': 0.900609016418457, 'global_step': 95433, 'preemption_count': 0}), (97882, {'train/accuracy': 0.6847188472747803, 'train/loss': 1.5716965198516846, 'train/bleu': 34.628114027600134, 'validation/accuracy': 0.6906286478042603, 'validation/loss': 1.5188223123550415, 'validation/bleu': 30.61172410487406, 'validation/num_examples': 3000, 'test/accuracy': 0.7072876691818237, 'test/loss': 1.433120608329773, 'test/bleu': 30.47436907166849, 'test/num_examples': 3003, 'score': 33626.742523908615, 'total_duration': 54534.552993535995, 'accumulated_submission_time': 33626.742523908615, 'accumulated_eval_time': 20901.221860170364, 'accumulated_logging_time': 0.9282701015472412, 'global_step': 97882, 'preemption_count': 0}), (100331, {'train/accuracy': 0.6922535300254822, 'train/loss': 1.5347938537597656, 'train/bleu': 35.01068095870374, 'validation/accuracy': 0.6912219524383545, 'validation/loss': 1.5123759508132935, 'validation/bleu': 30.693625734771274, 'validation/num_examples': 3000, 'test/accuracy': 0.7085042595863342, 'test/loss': 1.4301732778549194, 'test/bleu': 30.658337950220538, 'test/num_examples': 3003, 'score': 34466.69800066948, 'total_duration': 55911.35035943985, 'accumulated_submission_time': 34466.69800066948, 'accumulated_eval_time': 21437.895745754242, 'accumulated_logging_time': 0.9557845592498779, 'global_step': 100331, 'preemption_count': 0}), (102786, {'train/accuracy': 0.696261465549469, 'train/loss': 1.5091254711151123, 'train/bleu': 35.16229026938933, 'validation/accuracy': 0.6916050910949707, 'validation/loss': 1.5091300010681152, 'validation/bleu': 30.837386780955292, 'validation/num_examples': 3000, 'test/accuracy': 0.7082725167274475, 'test/loss': 1.4248815774917603, 'test/bleu': 30.592381124465888, 'test/num_examples': 3003, 'score': 35306.553372621536, 'total_duration': 57316.277654886246, 'accumulated_submission_time': 35306.553372621536, 'accumulated_eval_time': 22002.799112796783, 'accumulated_logging_time': 0.9847755432128906, 'global_step': 102786, 'preemption_count': 0}), (105242, {'train/accuracy': 0.6965323090553284, 'train/loss': 1.5030683279037476, 'train/bleu': 35.51545753319166, 'validation/accuracy': 0.6938298940658569, 'validation/loss': 1.505064606666565, 'validation/bleu': 30.979064226538934, 'validation/num_examples': 3000, 'test/accuracy': 0.7081218957901001, 'test/loss': 1.4200738668441772, 'test/bleu': 30.82045043625403, 'test/num_examples': 3003, 'score': 36146.710327625275, 'total_duration': 58657.94246172905, 'accumulated_submission_time': 36146.710327625275, 'accumulated_eval_time': 22504.14000272751, 'accumulated_logging_time': 1.0132761001586914, 'global_step': 105242, 'preemption_count': 0})], 'global_step': 105242}
I0306 11:31:37.828178 139996361643200 submission_runner.py:649] Timing: 36146.710327625275
I0306 11:31:37.828229 139996361643200 submission_runner.py:651] Total number of evals: 44
I0306 11:31:37.828257 139996361643200 submission_runner.py:652] ====================
I0306 11:31:37.828370 139996361643200 submission_runner.py:750] Final wmt score: 0
