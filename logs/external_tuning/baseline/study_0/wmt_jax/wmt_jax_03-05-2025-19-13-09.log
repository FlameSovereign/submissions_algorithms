python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-1820741095 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-13-09.log
2025-03-05 19:13:10.256064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201990.278762       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201990.285671       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:13:17.049187 140453607589056 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax.
I0305 19:13:18.031285 140453607589056 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:13:18.034436 140453607589056 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:13:18.036230 140453607589056 submission_runner.py:606] Using RNG seed -1820741095
I0305 19:13:18.638106 140453607589056 submission_runner.py:615] --- Tuning run 3/5 ---
I0305 19:13:18.638324 140453607589056 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_3.
I0305 19:13:18.638525 140453607589056 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_3/hparams.json.
I0305 19:13:18.870142 140453607589056 submission_runner.py:218] Initializing dataset.
I0305 19:13:19.042192 140453607589056 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:19.049015 140453607589056 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:19.121302 140453607589056 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:20.517918 140453607589056 submission_runner.py:229] Initializing model.
I0305 19:14:02.402060 140453607589056 submission_runner.py:272] Initializing optimizer.
I0305 19:14:03.268234 140453607589056 submission_runner.py:279] Initializing metrics bundle.
I0305 19:14:03.268482 140453607589056 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:14:03.269613 140453607589056 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_3 with prefix checkpoint_
I0305 19:14:03.269724 140453607589056 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_3/meta_data_0.json.
I0305 19:14:03.269917 140453607589056 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:14:03.269969 140453607589056 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:14:03.460759 140453607589056 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_3/flags_0.json.
I0305 19:14:03.495758 140453607589056 submission_runner.py:337] Starting training loop.
I0305 19:14:29.737787 140317411976960 logging_writer.py:48] [0] global_step=0, grad_norm=6.003373146057129, loss=11.214605331420898
I0305 19:14:29.798330 140453607589056 spec.py:321] Evaluating on the training split.
I0305 19:14:29.800532 140453607589056 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:29.804092 140453607589056 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:14:29.836635 140453607589056 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:36.002470 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 19:19:43.302389 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 19:19:43.349025 140453607589056 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:43.371825 140453607589056 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:43.405097 140453607589056 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:48.557379 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 19:24:49.187819 140453607589056 spec.py:349] Evaluating on the test split.
I0305 19:24:49.190282 140453607589056 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:49.193425 140453607589056 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:49.226455 140453607589056 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:52.077148 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 19:29:52.487336 140453607589056 submission_runner.py:469] Time since start: 948.99s, 	Step: 1, 	{'train/accuracy': 0.0006720660021528602, 'train/loss': 11.23320198059082, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.248138427734375, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.245794296264648, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.302452564239502, 'total_duration': 948.991500377655, 'accumulated_submission_time': 26.302452564239502, 'accumulated_eval_time': 922.688939332962, 'accumulated_logging_time': 0}
I0305 19:29:52.494966 140310357153536 logging_writer.py:48] [1] accumulated_eval_time=922.689, accumulated_logging_time=0, accumulated_submission_time=26.3025, global_step=1, preemption_count=0, score=26.3025, test/accuracy=0.000718341, test/bleu=0, test/loss=11.2458, test/num_examples=3003, total_duration=948.992, train/accuracy=0.000672066, train/bleu=0, train/loss=11.2332, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.2481, validation/num_examples=3000
I0305 19:30:27.543072 140310348760832 logging_writer.py:48] [100] global_step=100, grad_norm=0.42057591676712036, loss=8.705294609069824
I0305 19:31:02.494137 140310357153536 logging_writer.py:48] [200] global_step=200, grad_norm=0.1771172732114792, loss=8.308656692504883
I0305 19:31:37.457994 140310348760832 logging_writer.py:48] [300] global_step=300, grad_norm=0.20521357655525208, loss=7.995695114135742
I0305 19:32:12.456444 140310357153536 logging_writer.py:48] [400] global_step=400, grad_norm=0.2931297719478607, loss=7.599889278411865
I0305 19:32:47.480348 140310348760832 logging_writer.py:48] [500] global_step=500, grad_norm=0.400884747505188, loss=7.225081920623779
I0305 19:33:22.536844 140310357153536 logging_writer.py:48] [600] global_step=600, grad_norm=0.6074106097221375, loss=6.923131465911865
I0305 19:33:57.588964 140310348760832 logging_writer.py:48] [700] global_step=700, grad_norm=0.589061975479126, loss=6.595981121063232
I0305 19:34:32.637988 140310357153536 logging_writer.py:48] [800] global_step=800, grad_norm=0.7666991353034973, loss=6.374453067779541
I0305 19:35:07.712122 140310348760832 logging_writer.py:48] [900] global_step=900, grad_norm=1.0057117938995361, loss=6.160004615783691
I0305 19:35:42.772578 140310357153536 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7804446220397949, loss=5.932745933532715
I0305 19:36:17.815062 140310348760832 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7229451537132263, loss=5.696985244750977
I0305 19:36:52.878848 140310357153536 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6513028144836426, loss=5.470172882080078
I0305 19:37:27.989930 140310348760832 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.804840624332428, loss=5.269440174102783
I0305 19:38:03.048313 140310357153536 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7443828582763672, loss=5.118338108062744
I0305 19:38:38.085217 140310348760832 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.48111093044281, loss=4.971184253692627
I0305 19:39:13.136685 140310357153536 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.63021320104599, loss=4.86700963973999
I0305 19:39:48.185693 140310348760832 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.2034012079238892, loss=4.724152565002441
I0305 19:40:23.261940 140310357153536 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9442091584205627, loss=4.615525245666504
I0305 19:40:58.315525 140310348760832 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9088292717933655, loss=4.480467319488525
I0305 19:41:33.397003 140310357153536 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8228359222412109, loss=4.331856727600098
I0305 19:42:08.429002 140310348760832 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7705788016319275, loss=4.186488151550293
I0305 19:42:43.490427 140310357153536 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8206204175949097, loss=4.120319843292236
I0305 19:43:18.515504 140310348760832 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.3869129419326782, loss=3.9271082878112793
I0305 19:43:52.545582 140453607589056 spec.py:321] Evaluating on the training split.
I0305 19:43:55.201349 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 19:48:04.125970 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 19:48:06.771797 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 19:52:12.365838 140453607589056 spec.py:349] Evaluating on the test split.
I0305 19:52:15.007163 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 19:56:28.209224 140453607589056 submission_runner.py:469] Time since start: 2544.71s, 	Step: 2398, 	{'train/accuracy': 0.41951772570610046, 'train/loss': 3.8381333351135254, 'train/bleu': 14.816741339202395, 'validation/accuracy': 0.4016018807888031, 'validation/loss': 3.978447675704956, 'validation/bleu': 9.499882996677812, 'validation/num_examples': 3000, 'test/accuracy': 0.388761430978775, 'test/loss': 4.154230117797852, 'test/bleu': 8.37801651737534, 'test/num_examples': 3003, 'score': 866.1766827106476, 'total_duration': 2544.7133824825287, 'accumulated_submission_time': 866.1766827106476, 'accumulated_eval_time': 1678.352516412735, 'accumulated_logging_time': 0.01751089096069336}
I0305 19:56:28.219081 140310357153536 logging_writer.py:48] [2398] accumulated_eval_time=1678.35, accumulated_logging_time=0.0175109, accumulated_submission_time=866.177, global_step=2398, preemption_count=0, score=866.177, test/accuracy=0.388761, test/bleu=8.37802, test/loss=4.15423, test/num_examples=3003, total_duration=2544.71, train/accuracy=0.419518, train/bleu=14.8167, train/loss=3.83813, validation/accuracy=0.401602, validation/bleu=9.49988, validation/loss=3.97845, validation/num_examples=3000
I0305 19:56:29.271513 140310348760832 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.8604695200920105, loss=3.888309955596924
I0305 19:57:04.202098 140310357153536 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.3705284595489502, loss=3.8072316646575928
I0305 19:57:39.194406 140310348760832 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9437748789787292, loss=3.607351303100586
I0305 19:58:14.218176 140310357153536 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.3337812423706055, loss=3.6132054328918457
I0305 19:58:49.298478 140310348760832 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.9061102271080017, loss=3.5370965003967285
I0305 19:59:24.380892 140310357153536 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.7245528101921082, loss=3.3767287731170654
I0305 19:59:59.441011 140310348760832 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8007560968399048, loss=3.295543670654297
I0305 20:00:34.515372 140310357153536 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7578179240226746, loss=3.283672571182251
I0305 20:01:09.594465 140310348760832 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.811455488204956, loss=3.177269220352173
I0305 20:01:44.665282 140310357153536 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8435758352279663, loss=3.0979366302490234
I0305 20:02:19.739554 140310348760832 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.7179044485092163, loss=3.1776282787323
I0305 20:02:54.840696 140310357153536 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6734153628349304, loss=3.1187427043914795
I0305 20:03:29.923541 140310348760832 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8614939451217651, loss=2.943425416946411
I0305 20:04:05.013995 140310357153536 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.9289244413375854, loss=2.917273759841919
I0305 20:04:40.100745 140310348760832 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8646461963653564, loss=2.88704776763916
I0305 20:05:15.165245 140310357153536 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6814162135124207, loss=2.862262487411499
I0305 20:05:50.238304 140310348760832 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6452642679214478, loss=2.8617570400238037
I0305 20:06:25.323075 140310357153536 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5931950211524963, loss=2.768113374710083
I0305 20:07:00.431067 140310348760832 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6482441425323486, loss=2.909043550491333
I0305 20:07:35.511986 140310357153536 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6535812616348267, loss=2.7998416423797607
I0305 20:08:10.603300 140310348760832 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.682777464389801, loss=2.661367893218994
I0305 20:08:45.693677 140310357153536 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.757331907749176, loss=2.737058162689209
I0305 20:09:20.827740 140310348760832 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8160983920097351, loss=2.670841932296753
I0305 20:09:55.900166 140310357153536 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6590198874473572, loss=2.656485080718994
I0305 20:10:28.547816 140453607589056 spec.py:321] Evaluating on the training split.
I0305 20:10:31.203077 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 20:13:24.361280 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 20:13:27.002912 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 20:16:11.059399 140453607589056 spec.py:349] Evaluating on the test split.
I0305 20:16:13.701871 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 20:18:45.550328 140453607589056 submission_runner.py:469] Time since start: 3882.05s, 	Step: 4794, 	{'train/accuracy': 0.5435895323753357, 'train/loss': 2.6274607181549072, 'train/bleu': 24.86587119835876, 'validation/accuracy': 0.5490569472312927, 'validation/loss': 2.574619770050049, 'validation/bleu': 20.6708920708382, 'validation/num_examples': 3000, 'test/accuracy': 0.5522998571395874, 'test/loss': 2.5946102142333984, 'test/bleu': 19.44563204619634, 'test/num_examples': 3003, 'score': 1706.3324828147888, 'total_duration': 3882.05451130867, 'accumulated_submission_time': 1706.3324828147888, 'accumulated_eval_time': 2175.3549778461456, 'accumulated_logging_time': 0.03632807731628418}
I0305 20:18:45.560936 140310315190016 logging_writer.py:48] [4794] accumulated_eval_time=2175.35, accumulated_logging_time=0.0363281, accumulated_submission_time=1706.33, global_step=4794, preemption_count=0, score=1706.33, test/accuracy=0.5523, test/bleu=19.4456, test/loss=2.59461, test/num_examples=3003, total_duration=3882.05, train/accuracy=0.54359, train/bleu=24.8659, train/loss=2.62746, validation/accuracy=0.549057, validation/bleu=20.6709, validation/loss=2.57462, validation/num_examples=3000
I0305 20:18:48.023061 140310306797312 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9744491577148438, loss=2.612539052963257
I0305 20:19:23.014257 140310315190016 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7428941130638123, loss=2.6653037071228027
I0305 20:19:58.027153 140310306797312 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6276283264160156, loss=2.578629970550537
I0305 20:20:33.122577 140310315190016 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.579907238483429, loss=2.4873220920562744
I0305 20:21:08.212958 140310306797312 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5537734627723694, loss=2.6347174644470215
I0305 20:21:43.305786 140310315190016 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5460699200630188, loss=2.510366916656494
I0305 20:22:18.363555 140310306797312 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5787529945373535, loss=2.4666125774383545
I0305 20:22:53.449387 140310315190016 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5952832102775574, loss=2.5519158840179443
I0305 20:23:28.514702 140310306797312 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6036610007286072, loss=2.4911129474639893
I0305 20:24:03.608175 140310315190016 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6082509160041809, loss=2.437924385070801
I0305 20:24:38.692028 140310306797312 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5088680982589722, loss=2.439732074737549
I0305 20:25:13.787894 140310315190016 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6232181787490845, loss=2.435053586959839
I0305 20:25:48.901488 140310306797312 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.557263195514679, loss=2.41174054145813
I0305 20:26:23.991969 140310315190016 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.6041142344474792, loss=2.482483148574829
I0305 20:26:59.101813 140310306797312 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5219735503196716, loss=2.4298834800720215
I0305 20:27:34.159114 140309904176896 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5159391760826111, loss=2.4816064834594727
I0305 20:28:09.195446 140309895784192 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.518493115901947, loss=2.3036599159240723
I0305 20:28:44.225230 140309904176896 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4724063277244568, loss=2.4827816486358643
I0305 20:29:19.240711 140309895784192 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4700937867164612, loss=2.3350319862365723
I0305 20:29:54.284278 140309904176896 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.47585949301719666, loss=2.3254170417785645
I0305 20:30:29.308826 140309895784192 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.5263258814811707, loss=2.336906671524048
I0305 20:31:04.317298 140309904176896 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4419781267642975, loss=2.332749128341675
I0305 20:31:39.337672 140309895784192 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4422905743122101, loss=2.366818904876709
I0305 20:32:14.328052 140309904176896 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5082208514213562, loss=2.3375704288482666
I0305 20:32:45.848238 140453607589056 spec.py:321] Evaluating on the training split.
I0305 20:32:48.505826 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 20:35:30.296728 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 20:35:32.934351 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 20:38:14.098204 140453607589056 spec.py:349] Evaluating on the test split.
I0305 20:38:16.736935 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 20:40:48.819086 140453607589056 submission_runner.py:469] Time since start: 5205.32s, 	Step: 7191, 	{'train/accuracy': 0.5866031646728516, 'train/loss': 2.2181344032287598, 'train/bleu': 27.59965013514339, 'validation/accuracy': 0.5883988738059998, 'validation/loss': 2.186305284500122, 'validation/bleu': 23.447596896426234, 'validation/num_examples': 3000, 'test/accuracy': 0.5920171737670898, 'test/loss': 2.1745574474334717, 'test/bleu': 22.014903552176293, 'test/num_examples': 3003, 'score': 2546.4510893821716, 'total_duration': 5205.323271512985, 'accumulated_submission_time': 2546.4510893821716, 'accumulated_eval_time': 2658.325782060623, 'accumulated_logging_time': 0.05561041831970215}
I0305 20:40:48.829182 140309895784192 logging_writer.py:48] [7191] accumulated_eval_time=2658.33, accumulated_logging_time=0.0556104, accumulated_submission_time=2546.45, global_step=7191, preemption_count=0, score=2546.45, test/accuracy=0.592017, test/bleu=22.0149, test/loss=2.17456, test/num_examples=3003, total_duration=5205.32, train/accuracy=0.586603, train/bleu=27.5997, train/loss=2.21813, validation/accuracy=0.588399, validation/bleu=23.4476, validation/loss=2.18631, validation/num_examples=3000
I0305 20:40:52.331603 140309904176896 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.45293402671813965, loss=2.218294858932495
I0305 20:41:27.192415 140309895784192 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.46774426102638245, loss=2.2707200050354004
I0305 20:42:02.105616 140309904176896 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.49800071120262146, loss=2.2405307292938232
I0305 20:42:37.057940 140309895784192 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.46160322427749634, loss=2.30525541305542
I0305 20:43:12.061382 140309904176896 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4145207405090332, loss=2.2080130577087402
I0305 20:43:47.065222 140309895784192 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4475167393684387, loss=2.2125537395477295
I0305 20:44:22.063672 140309904176896 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4765029847621918, loss=2.2485435009002686
I0305 20:44:57.076678 140309895784192 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.38605958223342896, loss=2.2499547004699707
I0305 20:45:32.093796 140309904176896 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.38455894589424133, loss=2.22712779045105
I0305 20:46:07.104247 140309895784192 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.5066066384315491, loss=2.3205113410949707
I0305 20:46:42.091711 140309904176896 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.39534997940063477, loss=2.1681437492370605
I0305 20:47:17.089585 140309895784192 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.49624931812286377, loss=2.1455228328704834
I0305 20:47:52.068813 140309904176896 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.39926254749298096, loss=2.1949429512023926
I0305 20:48:27.072084 140309895784192 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4122689664363861, loss=2.1801400184631348
I0305 20:49:02.066877 140309904176896 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3837951719760895, loss=2.0884854793548584
I0305 20:49:37.041899 140309895784192 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3463561534881592, loss=2.1693687438964844
I0305 20:50:12.046574 140309904176896 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.35882195830345154, loss=2.0903449058532715
I0305 20:50:47.062822 140309895784192 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3996155560016632, loss=2.146631956100464
I0305 20:51:22.048756 140309904176896 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.371618390083313, loss=2.142043113708496
I0305 20:51:57.025693 140309895784192 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3915189206600189, loss=2.14634108543396
I0305 20:52:32.016450 140309904176896 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3541688024997711, loss=2.1753525733947754
I0305 20:53:06.985852 140309895784192 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3680042624473572, loss=2.143127679824829
I0305 20:53:41.952270 140309904176896 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3369726836681366, loss=2.1765408515930176
I0305 20:54:16.942653 140309895784192 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3352765142917633, loss=2.0945188999176025
I0305 20:54:49.136769 140453607589056 spec.py:321] Evaluating on the training split.
I0305 20:54:51.791218 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 20:57:36.700889 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 20:57:39.336634 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:00:17.863314 140453607589056 spec.py:349] Evaluating on the test split.
I0305 21:00:20.500773 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:02:45.634054 140453607589056 submission_runner.py:469] Time since start: 6522.14s, 	Step: 9593, 	{'train/accuracy': 0.593389093875885, 'train/loss': 2.132319450378418, 'train/bleu': 28.316917201431533, 'validation/accuracy': 0.6066175699234009, 'validation/loss': 2.012321710586548, 'validation/bleu': 24.807958752701865, 'validation/num_examples': 3000, 'test/accuracy': 0.6139381527900696, 'test/loss': 1.9790500402450562, 'test/bleu': 23.720571622766467, 'test/num_examples': 3003, 'score': 3386.5949883461, 'total_duration': 6522.138231515884, 'accumulated_submission_time': 3386.5949883461, 'accumulated_eval_time': 3134.823011159897, 'accumulated_logging_time': 0.07428812980651855}
I0305 21:02:45.643785 140309904176896 logging_writer.py:48] [9593] accumulated_eval_time=3134.82, accumulated_logging_time=0.0742881, accumulated_submission_time=3386.59, global_step=9593, preemption_count=0, score=3386.59, test/accuracy=0.613938, test/bleu=23.7206, test/loss=1.97905, test/num_examples=3003, total_duration=6522.14, train/accuracy=0.593389, train/bleu=28.3169, train/loss=2.13232, validation/accuracy=0.606618, validation/bleu=24.808, validation/loss=2.01232, validation/num_examples=3000
I0305 21:02:48.439710 140309895784192 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3108406662940979, loss=2.0854551792144775
I0305 21:03:23.288647 140309904176896 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3283858597278595, loss=2.1058502197265625
I0305 21:03:58.192419 140309895784192 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3258375823497772, loss=2.067547559738159
I0305 21:04:33.098085 140309904176896 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3259041905403137, loss=2.0797982215881348
I0305 21:05:08.051660 140309895784192 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.30215394496917725, loss=2.1803090572357178
I0305 21:05:42.971574 140309904176896 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3215470016002655, loss=2.093998908996582
I0305 21:06:17.914999 140309895784192 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.3726901113986969, loss=2.1139469146728516
I0305 21:06:52.879813 140309904176896 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.32665982842445374, loss=2.089073657989502
I0305 21:07:27.833943 140309895784192 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.2925102710723877, loss=2.125941514968872
I0305 21:08:02.820591 140309904176896 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.34751972556114197, loss=2.061143398284912
I0305 21:08:37.778439 140309895784192 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3505080044269562, loss=2.1206815242767334
I0305 21:09:12.731210 140309904176896 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3231448531150818, loss=2.123594284057617
I0305 21:09:47.682067 140309895784192 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3296678066253662, loss=2.1369516849517822
I0305 21:10:22.652772 140309904176896 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.29771727323532104, loss=1.9770106077194214
I0305 21:10:57.631315 140309895784192 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.2980785369873047, loss=2.0379531383514404
I0305 21:11:32.583842 140309904176896 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2991737723350525, loss=2.082272529602051
I0305 21:12:07.621381 140309895784192 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3349155783653259, loss=2.0761489868164062
I0305 21:12:42.527762 140309904176896 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2905464470386505, loss=2.164525270462036
I0305 21:13:17.512228 140309895784192 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.28770312666893005, loss=2.0953283309936523
I0305 21:13:52.439632 140309904176896 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.29616579413414, loss=2.048778533935547
I0305 21:14:27.392768 140309895784192 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.3404702842235565, loss=2.170304775238037
I0305 21:15:02.352866 140309904176896 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.30515459179878235, loss=2.0003395080566406
I0305 21:15:37.314279 140309895784192 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.29044511914253235, loss=2.051953077316284
I0305 21:16:12.245030 140309904176896 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2652423083782196, loss=2.0311880111694336
I0305 21:16:45.792744 140453607589056 spec.py:321] Evaluating on the training split.
I0305 21:16:48.442095 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:21:46.607999 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 21:21:49.249588 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:26:39.414182 140453607589056 spec.py:349] Evaluating on the test split.
I0305 21:26:42.058749 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:31:31.865808 140453607589056 submission_runner.py:469] Time since start: 8248.37s, 	Step: 11997, 	{'train/accuracy': 0.6024340987205505, 'train/loss': 2.0452821254730225, 'train/bleu': 26.884426360051467, 'validation/accuracy': 0.6214372515678406, 'validation/loss': 1.8897901773452759, 'validation/bleu': 20.389571788168055, 'validation/num_examples': 3000, 'test/accuracy': 0.628409206867218, 'test/loss': 1.8486205339431763, 'test/bleu': 22.288411767668194, 'test/num_examples': 3003, 'score': 4226.5865046978, 'total_duration': 8248.369979143143, 'accumulated_submission_time': 4226.5865046978, 'accumulated_eval_time': 4020.8960151672363, 'accumulated_logging_time': 0.09292125701904297}
I0305 21:31:31.875838 140309895784192 logging_writer.py:48] [11997] accumulated_eval_time=4020.9, accumulated_logging_time=0.0929213, accumulated_submission_time=4226.59, global_step=11997, preemption_count=0, score=4226.59, test/accuracy=0.628409, test/bleu=22.2884, test/loss=1.84862, test/num_examples=3003, total_duration=8248.37, train/accuracy=0.602434, train/bleu=26.8844, train/loss=2.04528, validation/accuracy=0.621437, validation/bleu=20.3896, validation/loss=1.88979, validation/num_examples=3000
I0305 21:31:33.276460 140309904176896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.29677286744117737, loss=2.1482837200164795
I0305 21:32:08.080098 140309895784192 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.28448694944381714, loss=2.0201079845428467
I0305 21:32:42.977672 140309904176896 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.36289793252944946, loss=2.0289146900177
I0305 21:33:17.931981 140309895784192 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2586056888103485, loss=1.9224661588668823
I0305 21:33:52.862446 140309904176896 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.30139294266700745, loss=2.060215473175049
I0305 21:34:27.763611 140309895784192 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3151465654373169, loss=2.074122428894043
I0305 21:35:02.734421 140309904176896 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.32814210653305054, loss=2.0019681453704834
I0305 21:35:37.718941 140309895784192 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.2646549344062805, loss=2.052598714828491
I0305 21:36:12.704405 140309904176896 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.2632348835468292, loss=1.9886301755905151
I0305 21:36:47.740783 140309895784192 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.32191896438598633, loss=2.065380811691284
I0305 21:37:22.744577 140309904176896 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2571621835231781, loss=2.080430030822754
I0305 21:37:57.776642 140309895784192 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.273946613073349, loss=1.9125888347625732
I0305 21:38:32.798409 140309904176896 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.34271422028541565, loss=1.9243977069854736
I0305 21:39:07.844636 140309895784192 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2878621220588684, loss=2.0162267684936523
I0305 21:39:42.856229 140309904176896 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.28577548265457153, loss=2.092724084854126
I0305 21:40:17.916553 140309895784192 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2889653444290161, loss=1.9723619222640991
I0305 21:40:52.957842 140309904176896 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3383883237838745, loss=1.987694501876831
I0305 21:41:28.010945 140309895784192 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2684532403945923, loss=1.9431058168411255
I0305 21:42:03.071208 140309904176896 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.30293887853622437, loss=1.9358628988265991
I0305 21:42:38.113290 140309895784192 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3441818654537201, loss=2.012441635131836
I0305 21:43:13.155784 140309904176896 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2795453369617462, loss=2.048581838607788
I0305 21:43:48.169691 140309895784192 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2959672808647156, loss=2.0316555500030518
I0305 21:44:23.213396 140309904176896 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.30219170451164246, loss=1.9428735971450806
I0305 21:44:58.235310 140309895784192 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2997819185256958, loss=1.9493292570114136
I0305 21:45:32.188485 140453607589056 spec.py:321] Evaluating on the training split.
I0305 21:45:34.843159 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:48:14.398851 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 21:48:17.045372 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:50:53.444498 140453607589056 spec.py:349] Evaluating on the test split.
I0305 21:50:56.082672 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 21:53:15.491805 140453607589056 submission_runner.py:469] Time since start: 9552.00s, 	Step: 14398, 	{'train/accuracy': 0.6180242896080017, 'train/loss': 1.9136333465576172, 'train/bleu': 29.784373826342563, 'validation/accuracy': 0.6335129737854004, 'validation/loss': 1.7956960201263428, 'validation/bleu': 26.62909590109143, 'validation/num_examples': 3000, 'test/accuracy': 0.6435291171073914, 'test/loss': 1.7406089305877686, 'test/bleu': 25.452898851722505, 'test/num_examples': 3003, 'score': 5066.742300987244, 'total_duration': 9551.995981693268, 'accumulated_submission_time': 5066.742300987244, 'accumulated_eval_time': 4484.199281215668, 'accumulated_logging_time': 0.1129755973815918}
I0305 21:53:15.502254 140309904176896 logging_writer.py:48] [14398] accumulated_eval_time=4484.2, accumulated_logging_time=0.112976, accumulated_submission_time=5066.74, global_step=14398, preemption_count=0, score=5066.74, test/accuracy=0.643529, test/bleu=25.4529, test/loss=1.74061, test/num_examples=3003, total_duration=9552, train/accuracy=0.618024, train/bleu=29.7844, train/loss=1.91363, validation/accuracy=0.633513, validation/bleu=26.6291, validation/loss=1.7957, validation/num_examples=3000
I0305 21:53:16.556590 140309895784192 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.27807605266571045, loss=1.9707261323928833
I0305 21:53:51.486961 140309904176896 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.4036170244216919, loss=1.9558080434799194
I0305 21:54:26.485616 140309895784192 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2689113914966583, loss=1.9878990650177002
I0305 21:55:01.513555 140309904176896 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.30969128012657166, loss=1.9572527408599854
I0305 21:55:36.528484 140309895784192 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.32858285307884216, loss=1.926430344581604
I0305 21:56:11.538100 140309904176896 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.40215787291526794, loss=1.9235622882843018
I0305 21:56:46.543587 140309895784192 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3259570598602295, loss=1.8722294569015503
I0305 21:57:21.555172 140309904176896 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.342316210269928, loss=1.9357110261917114
I0305 21:57:56.566218 140309895784192 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5000997185707092, loss=1.9784830808639526
I0305 21:58:31.564292 140309904176896 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3192403018474579, loss=1.9612016677856445
I0305 21:59:06.553479 140309895784192 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.3006119132041931, loss=1.939974069595337
I0305 21:59:41.567007 140309904176896 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.30047860741615295, loss=2.018590211868286
I0305 22:00:16.574565 140309895784192 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.32692524790763855, loss=1.8147859573364258
I0305 22:00:51.622386 140309904176896 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.29395222663879395, loss=1.9157569408416748
I0305 22:01:26.675907 140309895784192 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.344092458486557, loss=1.9039385318756104
I0305 22:02:01.712306 140309904176896 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.32259607315063477, loss=1.9814505577087402
I0305 22:02:36.748075 140309895784192 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.342105895280838, loss=1.9323409795761108
I0305 22:03:11.798756 140309904176896 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.37470996379852295, loss=1.9068855047225952
I0305 22:03:46.826752 140309895784192 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3022829294204712, loss=1.9070403575897217
I0305 22:04:21.869255 140309904176896 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3389415442943573, loss=1.8994486331939697
I0305 22:04:56.917549 140309895784192 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.4494618773460388, loss=1.871086597442627
I0305 22:05:31.945247 140309904176896 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3206065595149994, loss=1.8882230520248413
I0305 22:06:06.975306 140309895784192 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3449822664260864, loss=1.908115267753601
I0305 22:06:41.983820 140309904176896 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4264320433139801, loss=1.9185688495635986
I0305 22:07:15.624018 140453607589056 spec.py:321] Evaluating on the training split.
I0305 22:07:18.279396 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:10:09.315066 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 22:10:11.958461 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:12:40.750023 140453607589056 spec.py:349] Evaluating on the test split.
I0305 22:12:43.388519 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:15:03.247279 140453607589056 submission_runner.py:469] Time since start: 10859.75s, 	Step: 16797, 	{'train/accuracy': 0.6221718788146973, 'train/loss': 1.8844703435897827, 'train/bleu': 30.573863066780767, 'validation/accuracy': 0.6428942084312439, 'validation/loss': 1.7411386966705322, 'validation/bleu': 27.16599045469385, 'validation/num_examples': 3000, 'test/accuracy': 0.6534005403518677, 'test/loss': 1.6806440353393555, 'test/bleu': 26.50965433659917, 'test/num_examples': 3003, 'score': 5906.7158052921295, 'total_duration': 10859.751459360123, 'accumulated_submission_time': 5906.7158052921295, 'accumulated_eval_time': 4951.822494506836, 'accumulated_logging_time': 0.13192129135131836}
I0305 22:15:03.259421 140309895784192 logging_writer.py:48] [16797] accumulated_eval_time=4951.82, accumulated_logging_time=0.131921, accumulated_submission_time=5906.72, global_step=16797, preemption_count=0, score=5906.72, test/accuracy=0.653401, test/bleu=26.5097, test/loss=1.68064, test/num_examples=3003, total_duration=10859.8, train/accuracy=0.622172, train/bleu=30.5739, train/loss=1.88447, validation/accuracy=0.642894, validation/bleu=27.166, validation/loss=1.74114, validation/num_examples=3000
I0305 22:15:04.670645 140309904176896 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3785392940044403, loss=1.9021633863449097
I0305 22:15:39.580993 140309895784192 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3513273596763611, loss=1.7994834184646606
I0305 22:16:14.542511 140309904176896 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3558410108089447, loss=1.990222454071045
I0305 22:16:49.551280 140309895784192 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.34177348017692566, loss=1.9349812269210815
I0305 22:17:24.550548 140309904176896 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.4147513806819916, loss=1.7980101108551025
I0305 22:17:59.581052 140309895784192 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.333671897649765, loss=1.8187962770462036
I0305 22:18:34.618187 140309904176896 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.336037814617157, loss=1.952890396118164
I0305 22:19:09.637792 140309895784192 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3864779770374298, loss=1.9389177560806274
I0305 22:19:44.644361 140309904176896 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.34598615765571594, loss=1.8968753814697266
I0305 22:20:19.680137 140309895784192 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3240118622779846, loss=1.9012409448623657
I0305 22:20:54.700590 140309904176896 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3168855309486389, loss=1.8735861778259277
I0305 22:21:29.724969 140309895784192 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3351825773715973, loss=1.8055957555770874
I0305 22:22:04.752289 140309904176896 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3783789873123169, loss=1.898888349533081
I0305 22:22:39.801011 140309895784192 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.36153116822242737, loss=1.9271278381347656
I0305 22:23:14.826681 140309904176896 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.40872621536254883, loss=1.9025436639785767
I0305 22:23:49.861693 140309895784192 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.48258697986602783, loss=1.7641053199768066
I0305 22:24:24.878969 140309904176896 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.41998714208602905, loss=1.924167275428772
I0305 22:24:59.908827 140309895784192 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.34146416187286377, loss=1.8823436498641968
I0305 22:25:34.909571 140309904176896 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3464573621749878, loss=1.8888086080551147
I0305 22:26:09.907353 140309895784192 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.32876014709472656, loss=1.8645505905151367
I0305 22:26:44.866949 140309904176896 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3609238862991333, loss=1.8895798921585083
I0305 22:27:20.059458 140309895784192 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3280298411846161, loss=1.8289722204208374
I0305 22:27:55.158474 140309904176896 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.46970972418785095, loss=1.9066596031188965
I0305 22:28:30.232052 140309895784192 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.32609033584594727, loss=1.8678117990493774
I0305 22:29:03.562939 140453607589056 spec.py:321] Evaluating on the training split.
I0305 22:29:06.223043 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:32:33.069753 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 22:32:35.711944 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:35:26.001180 140453607589056 spec.py:349] Evaluating on the test split.
I0305 22:35:28.648706 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:38:32.718520 140453607589056 submission_runner.py:469] Time since start: 12269.22s, 	Step: 19196, 	{'train/accuracy': 0.640593409538269, 'train/loss': 1.7329225540161133, 'train/bleu': 31.25223549350145, 'validation/accuracy': 0.6465528011322021, 'validation/loss': 1.7002032995224, 'validation/bleu': 27.56459137683218, 'validation/num_examples': 3000, 'test/accuracy': 0.6564245223999023, 'test/loss': 1.6423343420028687, 'test/bleu': 26.7979640584824, 'test/num_examples': 3003, 'score': 6746.866229534149, 'total_duration': 12269.222694158554, 'accumulated_submission_time': 6746.866229534149, 'accumulated_eval_time': 5520.978021621704, 'accumulated_logging_time': 0.15406394004821777}
I0305 22:38:32.730442 140309904176896 logging_writer.py:48] [19196] accumulated_eval_time=5520.98, accumulated_logging_time=0.154064, accumulated_submission_time=6746.87, global_step=19196, preemption_count=0, score=6746.87, test/accuracy=0.656425, test/bleu=26.798, test/loss=1.64233, test/num_examples=3003, total_duration=12269.2, train/accuracy=0.640593, train/bleu=31.2522, train/loss=1.73292, validation/accuracy=0.646553, validation/bleu=27.5646, validation/loss=1.7002, validation/num_examples=3000
I0305 22:38:34.495330 140309895784192 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3592424988746643, loss=1.8131022453308105
I0305 22:39:09.514712 140309904176896 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3625510036945343, loss=1.85903799533844
I0305 22:39:44.612729 140309895784192 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.4408659040927887, loss=1.8398306369781494
I0305 22:40:19.750650 140309904176896 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.36826616525650024, loss=1.808781623840332
I0305 22:40:54.899620 140309895784192 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.34353819489479065, loss=1.8365999460220337
I0305 22:41:30.089834 140309904176896 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.35536307096481323, loss=1.880391240119934
I0305 22:42:05.207535 140309895784192 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3778284192085266, loss=1.885114073753357
I0305 22:42:40.292467 140309904176896 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5655812621116638, loss=4.009682655334473
I0305 22:43:15.399657 140309895784192 logging_writer.py:48] [20000] global_step=20000, grad_norm=4.207786560058594, loss=3.429126501083374
I0305 22:43:50.523168 140309904176896 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.4973759949207306, loss=1.9920871257781982
I0305 22:44:25.615207 140309895784192 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.5371053814888, loss=1.9644496440887451
I0305 22:45:00.730080 140309904176896 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.4272021949291229, loss=1.8793681859970093
I0305 22:45:35.840610 140309895784192 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.4043349623680115, loss=1.9376953840255737
I0305 22:46:10.982804 140309904176896 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3604147732257843, loss=1.8737505674362183
I0305 22:46:46.062613 140309895784192 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.49794989824295044, loss=1.915079951286316
I0305 22:47:21.182987 140309904176896 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3420354127883911, loss=1.7947885990142822
I0305 22:47:56.260064 140309895784192 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.36798417568206787, loss=1.8823864459991455
I0305 22:48:31.376541 140309904176896 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.31827643513679504, loss=1.8389050960540771
I0305 22:49:06.506623 140309895784192 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.3281271457672119, loss=1.8204808235168457
I0305 22:49:41.634926 140309904176896 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.44644811749458313, loss=1.886657476425171
I0305 22:50:16.745723 140309895784192 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.3630632758140564, loss=1.8869802951812744
I0305 22:50:51.875832 140309904176896 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.4187564551830292, loss=1.806264042854309
I0305 22:51:27.015416 140309895784192 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.36743369698524475, loss=1.8383647203445435
I0305 22:52:02.150623 140309904176896 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.37410423159599304, loss=1.862730860710144
I0305 22:52:33.060345 140453607589056 spec.py:321] Evaluating on the training split.
I0305 22:52:35.717661 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:56:04.793249 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 22:56:07.435136 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 22:58:53.071408 140453607589056 spec.py:349] Evaluating on the test split.
I0305 22:58:55.713484 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 23:01:29.318753 140453607589056 submission_runner.py:469] Time since start: 13645.82s, 	Step: 21589, 	{'train/accuracy': 0.6294189691543579, 'train/loss': 1.8221043348312378, 'train/bleu': 30.46595577377374, 'validation/accuracy': 0.649642825126648, 'validation/loss': 1.678046703338623, 'validation/bleu': 27.51618627604794, 'validation/num_examples': 3000, 'test/accuracy': 0.6586606502532959, 'test/loss': 1.6265623569488525, 'test/bleu': 26.903761554144584, 'test/num_examples': 3003, 'score': 7587.050140380859, 'total_duration': 13645.822926521301, 'accumulated_submission_time': 7587.050140380859, 'accumulated_eval_time': 6057.236371517181, 'accumulated_logging_time': 0.1755530834197998}
I0305 23:01:29.331735 140309895784192 logging_writer.py:48] [21589] accumulated_eval_time=6057.24, accumulated_logging_time=0.175553, accumulated_submission_time=7587.05, global_step=21589, preemption_count=0, score=7587.05, test/accuracy=0.658661, test/bleu=26.9038, test/loss=1.62656, test/num_examples=3003, total_duration=13645.8, train/accuracy=0.629419, train/bleu=30.466, train/loss=1.8221, validation/accuracy=0.649643, validation/bleu=27.5162, validation/loss=1.67805, validation/num_examples=3000
I0305 23:01:33.533597 140309904176896 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.31307804584503174, loss=1.8189992904663086
I0305 23:02:08.554767 140309895784192 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.36317306756973267, loss=1.886529803276062
I0305 23:02:43.639539 140309904176896 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.32009148597717285, loss=1.8888155221939087
I0305 23:03:18.760463 140309895784192 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.36187273263931274, loss=1.7998095750808716
I0305 23:03:53.865597 140309904176896 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4046374261379242, loss=1.7413996458053589
I0305 23:04:28.961961 140309895784192 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.3492237627506256, loss=1.8671423196792603
I0305 23:05:04.106308 140309904176896 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3270741105079651, loss=1.853055715560913
I0305 23:05:39.228015 140309895784192 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.34805572032928467, loss=1.7969317436218262
I0305 23:06:14.397954 140309904176896 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.3538345396518707, loss=1.9467923641204834
I0305 23:06:49.464661 140309895784192 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3597443401813507, loss=1.800607442855835
I0305 23:07:24.616829 140309904176896 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.37344348430633545, loss=1.828263759613037
I0305 23:07:59.759626 140309895784192 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.38125184178352356, loss=1.9284740686416626
I0305 23:08:34.900463 140309904176896 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.37716934084892273, loss=1.8440676927566528
I0305 23:09:10.028571 140309895784192 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.41956281661987305, loss=1.8344688415527344
I0305 23:09:45.186658 140309904176896 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.3845982253551483, loss=1.8624873161315918
I0305 23:10:20.333235 140309895784192 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.3230581283569336, loss=1.8416842222213745
I0305 23:10:55.478040 140309904176896 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.4154109060764313, loss=1.861033320426941
I0305 23:11:30.627326 140309895784192 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.36223718523979187, loss=1.9163297414779663
I0305 23:12:05.771143 140309904176896 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.37437865138053894, loss=1.896323561668396
I0305 23:12:40.938532 140309895784192 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3812813460826874, loss=1.8804796934127808
I0305 23:13:16.077867 140309904176896 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3395906984806061, loss=1.8646513223648071
I0305 23:13:51.217382 140309895784192 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3652413785457611, loss=1.888502597808838
I0305 23:14:26.310171 140309904176896 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.33420243859291077, loss=1.7689034938812256
I0305 23:15:01.461593 140309895784192 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3687377870082855, loss=1.8062052726745605
I0305 23:15:29.544183 140453607589056 spec.py:321] Evaluating on the training split.
I0305 23:15:32.206762 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 23:19:44.178807 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 23:19:46.825043 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 23:24:31.912925 140453607589056 spec.py:349] Evaluating on the test split.
I0305 23:24:34.558010 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 23:28:35.569032 140453607589056 submission_runner.py:469] Time since start: 15272.07s, 	Step: 23981, 	{'train/accuracy': 0.630372941493988, 'train/loss': 1.822124719619751, 'train/bleu': 30.490199505040238, 'validation/accuracy': 0.6516945958137512, 'validation/loss': 1.6691845655441284, 'validation/bleu': 26.422340490767564, 'validation/num_examples': 3000, 'test/accuracy': 0.6602131724357605, 'test/loss': 1.613050937652588, 'test/bleu': 26.81878446308568, 'test/num_examples': 3003, 'score': 8427.116342067719, 'total_duration': 15272.073198795319, 'accumulated_submission_time': 8427.116342067719, 'accumulated_eval_time': 6843.261160135269, 'accumulated_logging_time': 0.19788551330566406}
I0305 23:28:35.582671 140309904176896 logging_writer.py:48] [23981] accumulated_eval_time=6843.26, accumulated_logging_time=0.197886, accumulated_submission_time=8427.12, global_step=23981, preemption_count=0, score=8427.12, test/accuracy=0.660213, test/bleu=26.8188, test/loss=1.61305, test/num_examples=3003, total_duration=15272.1, train/accuracy=0.630373, train/bleu=30.4902, train/loss=1.82212, validation/accuracy=0.651695, validation/bleu=26.4223, validation/loss=1.66918, validation/num_examples=3000
I0305 23:28:42.593823 140309895784192 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.3701142966747284, loss=1.8445652723312378
I0305 23:29:17.554902 140309904176896 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.4494298994541168, loss=1.8070857524871826
I0305 23:29:52.623769 140309895784192 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3515288233757019, loss=1.8554681539535522
I0305 23:30:27.719679 140309904176896 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3721418082714081, loss=1.7359704971313477
I0305 23:31:02.854734 140309895784192 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.36987030506134033, loss=1.8666589260101318
I0305 23:31:37.993439 140309904176896 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.40880024433135986, loss=1.8596222400665283
I0305 23:32:13.119563 140309895784192 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4304160177707672, loss=1.8120620250701904
I0305 23:32:48.301038 140309904176896 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.3869990408420563, loss=1.7474535703659058
I0305 23:33:23.460087 140309895784192 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.43254831433296204, loss=1.7876330614089966
I0305 23:33:58.607995 140309904176896 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3710346221923828, loss=1.8352278470993042
I0305 23:34:33.721704 140309895784192 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.42321744561195374, loss=1.925545334815979
I0305 23:35:08.869115 140309904176896 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.37656643986701965, loss=1.8600809574127197
I0305 23:35:43.890277 140309895784192 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.36976316571235657, loss=1.7769533395767212
I0305 23:36:18.884986 140309904176896 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.5242453813552856, loss=1.7800215482711792
I0305 23:36:53.896491 140309895784192 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.38626453280448914, loss=1.9572396278381348
I0305 23:37:28.903973 140309904176896 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3771130442619324, loss=1.8617030382156372
I0305 23:38:03.960022 140309895784192 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.3919209837913513, loss=1.8552730083465576
I0305 23:38:39.005939 140309904176896 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.37082764506340027, loss=1.8011651039123535
I0305 23:39:14.021874 140309895784192 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3482937812805176, loss=1.8591376543045044
I0305 23:39:49.066056 140309904176896 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.5700014233589172, loss=1.8513342142105103
I0305 23:40:24.128936 140309895784192 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4388394057750702, loss=1.8279215097427368
I0305 23:40:59.164222 140309904176896 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.49123308062553406, loss=1.797121524810791
I0305 23:41:34.185663 140309895784192 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.3660730719566345, loss=1.826358675956726
I0305 23:42:09.206999 140309904176896 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.5123249888420105, loss=1.9377318620681763
I0305 23:42:35.819994 140453607589056 spec.py:321] Evaluating on the training split.
I0305 23:42:38.468702 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 23:45:34.792383 140453607589056 spec.py:333] Evaluating on the validation split.
I0305 23:45:37.434000 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 23:48:01.563873 140453607589056 spec.py:349] Evaluating on the test split.
I0305 23:48:04.213852 140453607589056 workload.py:181] Translating evaluation dataset.
I0305 23:50:21.727227 140453607589056 submission_runner.py:469] Time since start: 16578.23s, 	Step: 26377, 	{'train/accuracy': 0.6365044713020325, 'train/loss': 1.7646968364715576, 'train/bleu': 30.66817966084646, 'validation/accuracy': 0.6517687439918518, 'validation/loss': 1.6645671129226685, 'validation/bleu': 27.56674809415129, 'validation/num_examples': 3000, 'test/accuracy': 0.6604217290878296, 'test/loss': 1.6079171895980835, 'test/bleu': 26.92458613124644, 'test/num_examples': 3003, 'score': 9267.204899787903, 'total_duration': 16578.23140025139, 'accumulated_submission_time': 9267.204899787903, 'accumulated_eval_time': 7309.168332099915, 'accumulated_logging_time': 0.22160744667053223}
I0305 23:50:21.740836 140309895784192 logging_writer.py:48] [26377] accumulated_eval_time=7309.17, accumulated_logging_time=0.221607, accumulated_submission_time=9267.2, global_step=26377, preemption_count=0, score=9267.2, test/accuracy=0.660422, test/bleu=26.9246, test/loss=1.60792, test/num_examples=3003, total_duration=16578.2, train/accuracy=0.636504, train/bleu=30.6682, train/loss=1.7647, validation/accuracy=0.651769, validation/bleu=27.5667, validation/loss=1.66457, validation/num_examples=3000
I0305 23:50:30.126255 140309904176896 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.42995017766952515, loss=1.7979518175125122
I0305 23:51:05.069308 140309895784192 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3929708003997803, loss=1.8339977264404297
I0305 23:51:40.054787 140309904176896 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.4283638000488281, loss=1.8285565376281738
I0305 23:52:15.092917 140309895784192 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3612642288208008, loss=1.8647433519363403
I0305 23:52:50.120630 140309904176896 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.3506595492362976, loss=1.8883568048477173
I0305 23:53:25.179240 140309895784192 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.5368412137031555, loss=1.9406118392944336
I0305 23:54:00.226726 140309904176896 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.35905611515045166, loss=1.7828874588012695
I0305 23:54:35.306691 140309895784192 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.41320598125457764, loss=1.805883765220642
I0305 23:55:10.370198 140309904176896 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.41040101647377014, loss=1.812587022781372
I0305 23:55:45.425722 140309895784192 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.40326017141342163, loss=1.8025680780410767
I0305 23:56:20.481089 140309904176896 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.38514113426208496, loss=1.8116967678070068
I0305 23:56:55.528052 140309895784192 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.3945334255695343, loss=1.833023190498352
I0305 23:57:30.565184 140309904176896 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.42726150155067444, loss=1.8962459564208984
I0305 23:58:05.574615 140309895784192 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4625566601753235, loss=1.7558910846710205
I0305 23:58:40.634943 140309904176896 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.37529927492141724, loss=1.8151086568832397
I0305 23:59:15.643070 140309895784192 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.42983478307724, loss=1.81177818775177
I0305 23:59:50.668013 140309904176896 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4739319980144501, loss=1.842923641204834
I0306 00:00:25.688292 140309895784192 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.34308043122291565, loss=1.868397831916809
I0306 00:01:00.730997 140309904176896 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.37112557888031006, loss=1.7245396375656128
I0306 00:01:35.757858 140309895784192 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.35772427916526794, loss=1.787480354309082
I0306 00:02:10.806593 140309904176896 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.37946662306785583, loss=1.7531410455703735
I0306 00:02:45.824121 140309895784192 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.4784209430217743, loss=1.8191505670547485
I0306 00:03:20.841699 140309904176896 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.42126014828681946, loss=1.763700008392334
I0306 00:03:55.896193 140309895784192 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.37574973702430725, loss=1.8175699710845947
I0306 00:04:21.825907 140453607589056 spec.py:321] Evaluating on the training split.
I0306 00:04:24.481587 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:07:31.949303 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 00:07:34.594650 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:10:32.103921 140453607589056 spec.py:349] Evaluating on the test split.
I0306 00:10:34.747382 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:13:30.722683 140453607589056 submission_runner.py:469] Time since start: 17967.23s, 	Step: 28775, 	{'train/accuracy': 0.6340263485908508, 'train/loss': 1.7928507328033447, 'train/bleu': 30.98873610480173, 'validation/accuracy': 0.6557610034942627, 'validation/loss': 1.6400070190429688, 'validation/bleu': 28.33064394454149, 'validation/num_examples': 3000, 'test/accuracy': 0.666238009929657, 'test/loss': 1.5779070854187012, 'test/bleu': 27.534685645164636, 'test/num_examples': 3003, 'score': 10107.142692804337, 'total_duration': 17967.226855278015, 'accumulated_submission_time': 10107.142692804337, 'accumulated_eval_time': 7858.0650470256805, 'accumulated_logging_time': 0.2455613613128662}
I0306 00:13:30.736691 140309904176896 logging_writer.py:48] [28775] accumulated_eval_time=7858.07, accumulated_logging_time=0.245561, accumulated_submission_time=10107.1, global_step=28775, preemption_count=0, score=10107.1, test/accuracy=0.666238, test/bleu=27.5347, test/loss=1.57791, test/num_examples=3003, total_duration=17967.2, train/accuracy=0.634026, train/bleu=30.9887, train/loss=1.79285, validation/accuracy=0.655761, validation/bleu=28.3306, validation/loss=1.64001, validation/num_examples=3000
I0306 00:13:39.805480 140309895784192 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.44787076115608215, loss=1.7663031816482544
I0306 00:14:14.724777 140309904176896 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.4634406268596649, loss=1.8258397579193115
I0306 00:14:49.711694 140309895784192 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.3945031762123108, loss=1.8407913446426392
I0306 00:15:24.741672 140309904176896 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5155010223388672, loss=1.854666829109192
I0306 00:15:59.768854 140309895784192 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.42483147978782654, loss=1.7947999238967896
I0306 00:16:34.806240 140309904176896 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3758485019207001, loss=1.6958770751953125
I0306 00:17:09.837602 140309895784192 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.41594791412353516, loss=1.7581974267959595
I0306 00:17:44.919429 140309904176896 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.3745383024215698, loss=1.7301591634750366
I0306 00:18:19.940397 140309895784192 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.44773948192596436, loss=1.7889952659606934
I0306 00:18:55.035816 140309904176896 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.5258563160896301, loss=1.6866093873977661
I0306 00:19:30.090909 140309895784192 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.4590250253677368, loss=1.7249996662139893
I0306 00:20:05.168942 140309904176896 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5116026997566223, loss=1.75855553150177
I0306 00:20:40.197788 140309895784192 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.39921092987060547, loss=1.698293685913086
I0306 00:21:15.249344 140309904176896 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.4445455074310303, loss=1.8047159910202026
I0306 00:21:50.312547 140309895784192 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.3539285659790039, loss=1.8050892353057861
I0306 00:22:25.334620 140309904176896 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.4012511968612671, loss=1.778470754623413
I0306 00:23:00.346287 140309895784192 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.39410489797592163, loss=1.8923159837722778
I0306 00:23:35.395749 140309904176896 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.4816170632839203, loss=1.8605307340621948
I0306 00:24:10.423860 140309895784192 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.41252338886260986, loss=1.8443989753723145
I0306 00:24:45.458923 140309904176896 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.4594733715057373, loss=1.9508934020996094
I0306 00:25:20.491403 140309895784192 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.41613468527793884, loss=1.7645174264907837
I0306 00:25:55.522225 140309904176896 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.37632566690444946, loss=1.7176121473312378
I0306 00:26:30.547789 140309895784192 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.40037721395492554, loss=1.7173603773117065
I0306 00:27:05.587950 140309904176896 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.494143009185791, loss=1.749211072921753
I0306 00:27:30.817745 140453607589056 spec.py:321] Evaluating on the training split.
I0306 00:27:33.469784 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:31:12.678515 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 00:31:15.312309 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:34:07.212757 140453607589056 spec.py:349] Evaluating on the test split.
I0306 00:34:09.843548 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:36:44.492355 140453607589056 submission_runner.py:469] Time since start: 19361.00s, 	Step: 31173, 	{'train/accuracy': 0.6328679323196411, 'train/loss': 1.8054627180099487, 'train/bleu': 30.898889922194606, 'validation/accuracy': 0.6550564765930176, 'validation/loss': 1.633868932723999, 'validation/bleu': 27.880600655196098, 'validation/num_examples': 3000, 'test/accuracy': 0.6678716540336609, 'test/loss': 1.5667964220046997, 'test/bleu': 27.789889315452434, 'test/num_examples': 3003, 'score': 10947.077741384506, 'total_duration': 19360.996530532837, 'accumulated_submission_time': 10947.077741384506, 'accumulated_eval_time': 8411.739597558975, 'accumulated_logging_time': 0.26898884773254395}
I0306 00:36:44.505083 140309895784192 logging_writer.py:48] [31173] accumulated_eval_time=8411.74, accumulated_logging_time=0.268989, accumulated_submission_time=10947.1, global_step=31173, preemption_count=0, score=10947.1, test/accuracy=0.667872, test/bleu=27.7899, test/loss=1.5668, test/num_examples=3003, total_duration=19361, train/accuracy=0.632868, train/bleu=30.8989, train/loss=1.80546, validation/accuracy=0.655056, validation/bleu=27.8806, validation/loss=1.63387, validation/num_examples=3000
I0306 00:36:54.267063 140309904176896 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.40525346994400024, loss=1.8269970417022705
I0306 00:37:29.138971 140309895784192 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.703295111656189, loss=1.732379674911499
I0306 00:38:04.132253 140309904176896 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.44931888580322266, loss=1.877947211265564
I0306 00:38:39.142332 140309895784192 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4032682478427887, loss=1.8239532709121704
I0306 00:39:14.210315 140309904176896 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.3938252627849579, loss=1.7005198001861572
I0306 00:39:49.227589 140309895784192 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.3846607506275177, loss=1.7367451190948486
I0306 00:40:24.254798 140309904176896 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.40296852588653564, loss=1.7924057245254517
I0306 00:40:59.279685 140309895784192 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.38634997606277466, loss=1.7859063148498535
I0306 00:41:34.274242 140309904176896 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3984653651714325, loss=1.78434419631958
I0306 00:42:09.273032 140309895784192 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.4117654860019684, loss=1.7071658372879028
I0306 00:42:44.284311 140309904176896 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.3958708643913269, loss=1.8475751876831055
I0306 00:43:19.297749 140309895784192 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3835862874984741, loss=1.760358214378357
I0306 00:43:54.331135 140309904176896 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.4698842763900757, loss=1.7993096113204956
I0306 00:44:29.387682 140309895784192 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.4923115670681, loss=1.7633975744247437
I0306 00:45:04.412215 140309904176896 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3873509466648102, loss=1.705828070640564
I0306 00:45:39.447623 140309895784192 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.38852959871292114, loss=1.8021289110183716
I0306 00:46:14.456055 140309904176896 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.3876667022705078, loss=1.8239736557006836
I0306 00:46:49.485837 140309895784192 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.3788696229457855, loss=1.8081356287002563
I0306 00:47:24.430055 140309904176896 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3980741798877716, loss=1.7235690355300903
I0306 00:47:59.432545 140309895784192 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.44156742095947266, loss=1.7532180547714233
I0306 00:48:34.435733 140309904176896 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.45073384046554565, loss=1.7926512956619263
I0306 00:49:09.445807 140309895784192 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.3907540440559387, loss=1.786597728729248
I0306 00:49:44.474995 140309904176896 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.389215350151062, loss=1.767133116722107
I0306 00:50:19.510301 140309895784192 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.41152793169021606, loss=1.7738151550292969
I0306 00:50:44.726677 140453607589056 spec.py:321] Evaluating on the training split.
I0306 00:50:47.375126 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:55:32.657632 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 00:55:35.296199 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 00:58:38.004384 140453607589056 spec.py:349] Evaluating on the test split.
I0306 00:58:40.633333 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 01:01:52.929356 140453607589056 submission_runner.py:469] Time since start: 20869.43s, 	Step: 33573, 	{'train/accuracy': 0.6378339529037476, 'train/loss': 1.766714096069336, 'train/bleu': 31.572037289892087, 'validation/accuracy': 0.6578498482704163, 'validation/loss': 1.6158266067504883, 'validation/bleu': 27.72155938962761, 'validation/num_examples': 3000, 'test/accuracy': 0.668624758720398, 'test/loss': 1.5609530210494995, 'test/bleu': 27.80415417962983, 'test/num_examples': 3003, 'score': 11787.156024217606, 'total_duration': 20869.433529615402, 'accumulated_submission_time': 11787.156024217606, 'accumulated_eval_time': 9079.942214250565, 'accumulated_logging_time': 0.290189266204834}
I0306 01:01:52.941370 140309904176896 logging_writer.py:48] [33573] accumulated_eval_time=9079.94, accumulated_logging_time=0.290189, accumulated_submission_time=11787.2, global_step=33573, preemption_count=0, score=11787.2, test/accuracy=0.668625, test/bleu=27.8042, test/loss=1.56095, test/num_examples=3003, total_duration=20869.4, train/accuracy=0.637834, train/bleu=31.572, train/loss=1.76671, validation/accuracy=0.65785, validation/bleu=27.7216, validation/loss=1.61583, validation/num_examples=3000
I0306 01:02:02.712215 140309895784192 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.4442988336086273, loss=1.8625894784927368
I0306 01:02:37.640978 140309904176896 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.4372578263282776, loss=1.734935998916626
I0306 01:03:12.596997 140309895784192 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.3842121660709381, loss=1.6697121858596802
I0306 01:03:47.625029 140309904176896 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.3662847578525543, loss=1.7129158973693848
I0306 01:04:22.627271 140309895784192 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.44335100054740906, loss=1.8378722667694092
I0306 01:04:57.647553 140309904176896 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.4099152684211731, loss=1.7508991956710815
I0306 01:05:32.664860 140309895784192 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.4171026945114136, loss=1.7979427576065063
I0306 01:06:07.688011 140309904176896 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.39947420358657837, loss=1.6930311918258667
I0306 01:06:42.733653 140309895784192 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.4285885691642761, loss=1.7924740314483643
I0306 01:07:17.749956 140309904176896 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4218834638595581, loss=1.811408519744873
I0306 01:07:52.762474 140309895784192 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.37242749333381653, loss=1.833113670349121
I0306 01:08:27.760528 140309904176896 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.36988094449043274, loss=1.7570899724960327
I0306 01:09:02.806980 140309895784192 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.3621942102909088, loss=1.7965025901794434
I0306 01:09:37.818910 140309904176896 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.4179489016532898, loss=1.7087777853012085
I0306 01:10:12.841554 140309895784192 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.39896562695503235, loss=1.76444411277771
I0306 01:10:47.840709 140309904176896 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.40923094749450684, loss=1.6815578937530518
I0306 01:11:22.852483 140309895784192 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3954019844532013, loss=1.838730812072754
I0306 01:11:57.898589 140309904176896 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.41472479701042175, loss=1.6812129020690918
I0306 01:12:32.934604 140309895784192 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.3913321793079376, loss=1.7196890115737915
I0306 01:13:07.945472 140309904176896 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.44124606251716614, loss=1.846590280532837
I0306 01:13:42.957955 140309895784192 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.46591541171073914, loss=1.8172250986099243
I0306 01:14:18.000872 140309904176896 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.37175455689430237, loss=1.8620284795761108
I0306 01:14:53.042254 140309895784192 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.4221181571483612, loss=1.692165732383728
I0306 01:15:28.074316 140309904176896 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.3547298312187195, loss=1.7529181241989136
I0306 01:15:52.946809 140453607589056 spec.py:321] Evaluating on the training split.
I0306 01:15:55.597505 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 01:19:21.367239 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 01:19:24.006306 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 01:21:58.850394 140453607589056 spec.py:349] Evaluating on the test split.
I0306 01:22:01.485914 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 01:24:26.018735 140453607589056 submission_runner.py:469] Time since start: 22222.52s, 	Step: 35972, 	{'train/accuracy': 0.6389380693435669, 'train/loss': 1.7561196088790894, 'train/bleu': 30.975432167725724, 'validation/accuracy': 0.6593825221061707, 'validation/loss': 1.614693522453308, 'validation/bleu': 28.40411547366733, 'validation/num_examples': 3000, 'test/accuracy': 0.6711852550506592, 'test/loss': 1.5476024150848389, 'test/bleu': 27.86889381072178, 'test/num_examples': 3003, 'score': 12627.011134147644, 'total_duration': 22222.522897958755, 'accumulated_submission_time': 12627.011134147644, 'accumulated_eval_time': 9593.014070510864, 'accumulated_logging_time': 0.3120300769805908}
I0306 01:24:26.031499 140309895784192 logging_writer.py:48] [35972] accumulated_eval_time=9593.01, accumulated_logging_time=0.31203, accumulated_submission_time=12627, global_step=35972, preemption_count=0, score=12627, test/accuracy=0.671185, test/bleu=27.8689, test/loss=1.5476, test/num_examples=3003, total_duration=22222.5, train/accuracy=0.638938, train/bleu=30.9754, train/loss=1.75612, validation/accuracy=0.659383, validation/bleu=28.4041, validation/loss=1.61469, validation/num_examples=3000
I0306 01:24:36.157624 140309904176896 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.45960766077041626, loss=1.8485654592514038
I0306 01:25:11.043125 140309895784192 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.42656615376472473, loss=1.76019287109375
I0306 01:25:46.006712 140309904176896 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.4062281548976898, loss=1.7629258632659912
I0306 01:26:20.987306 140309895784192 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.39310911297798157, loss=1.7675966024398804
I0306 01:26:56.007696 140309904176896 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.36745786666870117, loss=1.7066285610198975
I0306 01:27:31.010035 140309895784192 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.4347231090068817, loss=1.7095526456832886
I0306 01:28:06.026346 140309904176896 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.4359073042869568, loss=1.829262614250183
I0306 01:28:41.005224 140309895784192 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.48581328988075256, loss=1.6899113655090332
I0306 01:29:16.016888 140309904176896 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.38182222843170166, loss=1.7524930238723755
I0306 01:29:51.001473 140309895784192 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.45659682154655457, loss=1.6429575681686401
I0306 01:30:26.012460 140309904176896 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.3805886507034302, loss=1.7637602090835571
I0306 01:31:00.999395 140309895784192 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3893080949783325, loss=1.674098014831543
I0306 01:31:35.984677 140309904176896 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.39885959029197693, loss=1.8427555561065674
I0306 01:32:10.997348 140309895784192 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4265272915363312, loss=1.6840336322784424
I0306 01:32:45.993614 140309904176896 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3748786747455597, loss=1.7219926118850708
I0306 01:33:21.031980 140309895784192 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.41409730911254883, loss=1.8197845220565796
I0306 01:33:56.063981 140309904176896 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.45380085706710815, loss=1.843166470527649
I0306 01:34:31.063529 140309895784192 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.3972105383872986, loss=1.81160306930542
I0306 01:35:06.088693 140309904176896 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.39293110370635986, loss=1.7336037158966064
I0306 01:35:41.061436 140309895784192 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.430036723613739, loss=1.7257133722305298
I0306 01:36:16.053774 140309904176896 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.44854503870010376, loss=1.747335433959961
I0306 01:36:51.026784 140309895784192 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.4959561228752136, loss=1.7143031358718872
I0306 01:37:26.032711 140309904176896 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3999464511871338, loss=1.7332772016525269
I0306 01:38:01.015827 140309895784192 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.39906224608421326, loss=1.7779037952423096
I0306 01:38:26.226024 140453607589056 spec.py:321] Evaluating on the training split.
I0306 01:38:28.873775 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 01:42:00.189555 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 01:42:02.826412 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 01:44:39.067195 140453607589056 spec.py:349] Evaluating on the test split.
I0306 01:44:41.700782 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 01:47:05.109872 140453607589056 submission_runner.py:469] Time since start: 23581.61s, 	Step: 38373, 	{'train/accuracy': 0.6514740586280823, 'train/loss': 1.6701644659042358, 'train/bleu': 32.08197458359871, 'validation/accuracy': 0.6620893478393555, 'validation/loss': 1.596665859222412, 'validation/bleu': 28.55504342496179, 'validation/num_examples': 3000, 'test/accuracy': 0.6723902225494385, 'test/loss': 1.5324037075042725, 'test/bleu': 28.03651735656702, 'test/num_examples': 3003, 'score': 13467.057776212692, 'total_duration': 23581.614058494568, 'accumulated_submission_time': 13467.057776212692, 'accumulated_eval_time': 10111.897889614105, 'accumulated_logging_time': 0.33316993713378906}
I0306 01:47:05.123085 140309904176896 logging_writer.py:48] [38373] accumulated_eval_time=10111.9, accumulated_logging_time=0.33317, accumulated_submission_time=13467.1, global_step=38373, preemption_count=0, score=13467.1, test/accuracy=0.67239, test/bleu=28.0365, test/loss=1.5324, test/num_examples=3003, total_duration=23581.6, train/accuracy=0.651474, train/bleu=32.082, train/loss=1.67016, validation/accuracy=0.662089, validation/bleu=28.555, validation/loss=1.59667, validation/num_examples=3000
I0306 01:47:14.890489 140309895784192 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.419802188873291, loss=1.7393863201141357
I0306 01:47:49.804511 140309904176896 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.4229726195335388, loss=1.8035330772399902
I0306 01:48:24.756369 140309895784192 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.38620084524154663, loss=1.7084766626358032
I0306 01:48:59.739023 140309904176896 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.43754926323890686, loss=1.687608003616333
I0306 01:49:34.712949 140309895784192 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.43088382482528687, loss=1.7786521911621094
I0306 01:50:09.831402 140309904176896 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.44997772574424744, loss=1.7964909076690674
I0306 01:50:44.882070 140309895784192 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3940349817276001, loss=1.7755249738693237
I0306 01:51:19.892119 140309904176896 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.3914903402328491, loss=1.7274699211120605
I0306 01:51:54.898741 140309895784192 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.38337063789367676, loss=1.6902939081192017
I0306 01:52:29.901097 140309904176896 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.4327094852924347, loss=1.8148226737976074
I0306 01:53:04.936845 140309895784192 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.41228026151657104, loss=1.6700282096862793
I0306 01:53:39.938189 140309904176896 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.41970616579055786, loss=1.79957914352417
I0306 01:54:14.943382 140309895784192 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.4203745424747467, loss=1.900786280632019
I0306 01:54:49.953526 140309904176896 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.4014227092266083, loss=1.7982841730117798
I0306 01:55:24.953051 140309895784192 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.4557533860206604, loss=1.7622160911560059
I0306 01:55:59.936200 140309904176896 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.39177975058555603, loss=1.6883901357650757
I0306 01:56:34.957767 140309895784192 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.4123641848564148, loss=1.7035748958587646
I0306 01:57:09.951181 140309904176896 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.4101495146751404, loss=1.7905727624893188
I0306 01:57:44.953417 140309895784192 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.39788928627967834, loss=1.7038582563400269
I0306 01:58:19.982184 140309904176896 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.4183155298233032, loss=1.7469673156738281
I0306 01:58:54.986895 140309895784192 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.4387803077697754, loss=1.8314672708511353
I0306 01:59:30.025546 140309904176896 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.5365880131721497, loss=1.7413866519927979
I0306 02:00:05.004043 140309895784192 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.414885938167572, loss=1.7588011026382446
I0306 02:00:39.991509 140309904176896 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3710078299045563, loss=1.6870241165161133
I0306 02:01:05.203492 140453607589056 spec.py:321] Evaluating on the training split.
I0306 02:01:07.849815 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:05:23.929649 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 02:05:26.565625 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:08:12.720453 140453607589056 spec.py:349] Evaluating on the test split.
I0306 02:08:15.357448 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:10:56.530519 140453607589056 submission_runner.py:469] Time since start: 25013.03s, 	Step: 40773, 	{'train/accuracy': 0.6390610337257385, 'train/loss': 1.7670201063156128, 'train/bleu': 31.365826836187477, 'validation/accuracy': 0.6603589653968811, 'validation/loss': 1.5961488485336304, 'validation/bleu': 28.32218735784994, 'validation/num_examples': 3000, 'test/accuracy': 0.6730738282203674, 'test/loss': 1.5327777862548828, 'test/bleu': 28.060622916606548, 'test/num_examples': 3003, 'score': 14306.990105628967, 'total_duration': 25013.034703731537, 'accumulated_submission_time': 14306.990105628967, 'accumulated_eval_time': 10703.224868535995, 'accumulated_logging_time': 0.35448122024536133}
I0306 02:10:56.543629 140309895784192 logging_writer.py:48] [40773] accumulated_eval_time=10703.2, accumulated_logging_time=0.354481, accumulated_submission_time=14307, global_step=40773, preemption_count=0, score=14307, test/accuracy=0.673074, test/bleu=28.0606, test/loss=1.53278, test/num_examples=3003, total_duration=25013, train/accuracy=0.639061, train/bleu=31.3658, train/loss=1.76702, validation/accuracy=0.660359, validation/bleu=28.3222, validation/loss=1.59615, validation/num_examples=3000
I0306 02:11:06.331806 140309904176896 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3984891176223755, loss=1.753244161605835
I0306 02:11:41.228359 140309895784192 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.4588038921356201, loss=1.7827614545822144
I0306 02:12:16.225665 140309904176896 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.38025006651878357, loss=1.7560508251190186
I0306 02:12:51.228987 140309895784192 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.41112324595451355, loss=1.7445414066314697
I0306 02:13:26.203761 140309904176896 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.38196486234664917, loss=1.7494579553604126
I0306 02:14:01.212038 140309895784192 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.42778801918029785, loss=1.7151328325271606
I0306 02:14:36.205235 140309904176896 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.4317651391029358, loss=1.6845113039016724
I0306 02:15:11.209311 140309895784192 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3613228499889374, loss=1.722076177597046
I0306 02:15:46.230529 140309904176896 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.598033607006073, loss=1.7232118844985962
I0306 02:16:21.252983 140309895784192 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3833318054676056, loss=1.809569001197815
I0306 02:16:56.265129 140309904176896 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.42626962065696716, loss=1.727607011795044
I0306 02:17:31.282530 140309895784192 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.40733081102371216, loss=1.7052973508834839
I0306 02:18:06.274326 140309904176896 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.37740978598594666, loss=1.7257075309753418
I0306 02:18:41.271223 140309895784192 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.44406187534332275, loss=1.7654913663864136
I0306 02:19:16.248349 140309904176896 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4091651737689972, loss=1.7192885875701904
I0306 02:19:51.237917 140309895784192 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.4247599244117737, loss=1.6169325113296509
I0306 02:20:26.231317 140309904176896 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.4770065248012543, loss=1.808582067489624
I0306 02:21:01.215740 140309895784192 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.38567182421684265, loss=1.7712246179580688
I0306 02:21:36.204684 140309904176896 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.5286085605621338, loss=1.7496936321258545
I0306 02:22:11.183568 140309895784192 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.5452519059181213, loss=1.686163306236267
I0306 02:22:46.151604 140309904176896 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.4036044180393219, loss=1.7843658924102783
I0306 02:23:21.118752 140309895784192 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.43917566537857056, loss=1.7890756130218506
I0306 02:23:56.114283 140309904176896 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3972645401954651, loss=1.7485630512237549
I0306 02:24:31.074639 140309895784192 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.44730687141418457, loss=1.8290469646453857
I0306 02:24:56.603081 140453607589056 spec.py:321] Evaluating on the training split.
I0306 02:24:59.247916 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:28:03.137418 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 02:28:05.774024 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:30:39.632976 140453607589056 spec.py:349] Evaluating on the test split.
I0306 02:30:42.273250 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:33:07.000042 140453607589056 submission_runner.py:469] Time since start: 26343.50s, 	Step: 43174, 	{'train/accuracy': 0.640091061592102, 'train/loss': 1.7539657354354858, 'train/bleu': 31.510433459357863, 'validation/accuracy': 0.6599386930465698, 'validation/loss': 1.5912686586380005, 'validation/bleu': 28.09843418045749, 'validation/num_examples': 3000, 'test/accuracy': 0.6729695200920105, 'test/loss': 1.5272576808929443, 'test/bleu': 27.99592158423135, 'test/num_examples': 3003, 'score': 15146.903519630432, 'total_duration': 26343.5042219162, 'accumulated_submission_time': 15146.903519630432, 'accumulated_eval_time': 11193.621772766113, 'accumulated_logging_time': 0.37674784660339355}
I0306 02:33:07.012949 140309904176896 logging_writer.py:48] [43174] accumulated_eval_time=11193.6, accumulated_logging_time=0.376748, accumulated_submission_time=15146.9, global_step=43174, preemption_count=0, score=15146.9, test/accuracy=0.67297, test/bleu=27.9959, test/loss=1.52726, test/num_examples=3003, total_duration=26343.5, train/accuracy=0.640091, train/bleu=31.5104, train/loss=1.75397, validation/accuracy=0.659939, validation/bleu=28.0984, validation/loss=1.59127, validation/num_examples=3000
I0306 02:33:16.447370 140309895784192 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.41632089018821716, loss=1.7704887390136719
I0306 02:33:51.365637 140309904176896 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.41123539209365845, loss=1.7464396953582764
I0306 02:34:26.341727 140309895784192 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.4126184582710266, loss=1.6927874088287354
I0306 02:35:01.312444 140309904176896 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.4337749779224396, loss=1.734907627105713
I0306 02:35:36.313343 140309895784192 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.41471990942955017, loss=1.7933461666107178
I0306 02:36:11.295243 140309904176896 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.40768054127693176, loss=1.7440274953842163
I0306 02:36:46.319581 140309895784192 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.4948844313621521, loss=1.7608221769332886
I0306 02:37:21.327212 140309904176896 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.4002227485179901, loss=1.8047620058059692
I0306 02:37:56.299468 140309895784192 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3960472345352173, loss=1.8144742250442505
I0306 02:38:31.267885 140309904176896 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.37229645252227783, loss=1.7760108709335327
I0306 02:39:06.287119 140309895784192 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.4104091227054596, loss=1.7013535499572754
I0306 02:39:41.327553 140309904176896 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.409885048866272, loss=1.8083528280258179
I0306 02:40:16.348031 140309895784192 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3734109103679657, loss=1.7414630651474
I0306 02:40:51.356498 140309904176896 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.368446946144104, loss=1.7076066732406616
I0306 02:41:26.335237 140309895784192 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.41953399777412415, loss=1.6852611303329468
I0306 02:42:01.361793 140309904176896 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.3732164204120636, loss=1.7130290269851685
I0306 02:42:36.388476 140309895784192 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.4262954592704773, loss=1.7869046926498413
I0306 02:43:11.418911 140309904176896 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.3888280391693115, loss=1.801620602607727
I0306 02:43:46.433221 140309895784192 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3787772059440613, loss=1.724629521369934
I0306 02:44:21.467634 140309904176896 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.423772931098938, loss=1.7282993793487549
I0306 02:44:56.451033 140309895784192 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.4648343026638031, loss=1.733001470565796
I0306 02:45:31.472152 140309904176896 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.4087288975715637, loss=1.7483127117156982
I0306 02:46:06.483689 140309895784192 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.4272632598876953, loss=1.6388541460037231
I0306 02:46:41.500388 140309904176896 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.3885316550731659, loss=1.681051254272461
I0306 02:47:07.029672 140453607589056 spec.py:321] Evaluating on the training split.
I0306 02:47:09.676954 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:50:16.313784 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 02:50:18.951188 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:52:48.805301 140453607589056 spec.py:349] Evaluating on the test split.
I0306 02:52:51.449067 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 02:55:13.768469 140453607589056 submission_runner.py:469] Time since start: 27670.27s, 	Step: 45574, 	{'train/accuracy': 0.6470431685447693, 'train/loss': 1.6979851722717285, 'train/bleu': 31.634742612244903, 'validation/accuracy': 0.6632017493247986, 'validation/loss': 1.5809577703475952, 'validation/bleu': 28.69499530268323, 'validation/num_examples': 3000, 'test/accuracy': 0.6755532622337341, 'test/loss': 1.5120937824249268, 'test/bleu': 28.246344834546786, 'test/num_examples': 3003, 'score': 15986.774324178696, 'total_duration': 27670.272636175156, 'accumulated_submission_time': 15986.774324178696, 'accumulated_eval_time': 11680.36050772667, 'accumulated_logging_time': 0.398212194442749}
I0306 02:55:13.784940 140309895784192 logging_writer.py:48] [45574] accumulated_eval_time=11680.4, accumulated_logging_time=0.398212, accumulated_submission_time=15986.8, global_step=45574, preemption_count=0, score=15986.8, test/accuracy=0.675553, test/bleu=28.2463, test/loss=1.51209, test/num_examples=3003, total_duration=27670.3, train/accuracy=0.647043, train/bleu=31.6347, train/loss=1.69799, validation/accuracy=0.663202, validation/bleu=28.695, validation/loss=1.58096, validation/num_examples=3000
I0306 02:55:23.210503 140309904176896 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.4836368560791016, loss=1.8050575256347656
I0306 02:55:58.077288 140309895784192 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.4855950176715851, loss=1.6863553524017334
I0306 02:56:33.004578 140309904176896 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.41564103960990906, loss=1.689536213874817
I0306 02:57:08.005361 140309895784192 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.4001859128475189, loss=1.6433700323104858
I0306 02:57:42.996135 140309904176896 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.37441784143447876, loss=1.7549794912338257
I0306 02:58:17.980315 140309895784192 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.36626559495925903, loss=1.7372584342956543
I0306 02:58:52.992624 140309904176896 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.40335893630981445, loss=1.78323495388031
I0306 02:59:27.997575 140309895784192 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.41484010219573975, loss=1.759278655052185
I0306 03:00:03.000457 140309904176896 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.4056909680366516, loss=1.7528425455093384
I0306 03:00:37.989237 140309895784192 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.37750646471977234, loss=1.7062658071517944
I0306 03:01:13.023103 140309904176896 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.39808985590934753, loss=1.7371699810028076
I0306 03:01:48.047169 140309895784192 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.39619895815849304, loss=1.6332170963287354
I0306 03:02:23.057245 140309904176896 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.4118407070636749, loss=1.6856114864349365
I0306 03:02:58.090494 140309895784192 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.39026230573654175, loss=1.6919407844543457
I0306 03:03:33.095662 140309904176896 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.412733256816864, loss=1.7219250202178955
I0306 03:04:08.105796 140309895784192 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.4212893545627594, loss=1.6879048347473145
I0306 03:04:43.110337 140309904176896 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.3935537338256836, loss=1.7059667110443115
I0306 03:05:18.144546 140309895784192 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.3928520679473877, loss=1.6845604181289673
I0306 03:05:53.132081 140309904176896 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.4088512063026428, loss=1.7026559114456177
I0306 03:06:28.112844 140309895784192 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3905409872531891, loss=1.7263692617416382
I0306 03:07:03.124359 140309904176896 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.38883212208747864, loss=1.8340445756912231
I0306 03:07:38.119601 140309895784192 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.38758769631385803, loss=1.66849684715271
I0306 03:08:13.145878 140309904176896 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.375552773475647, loss=1.783799171447754
I0306 03:08:48.173587 140309895784192 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.40858104825019836, loss=1.6380908489227295
I0306 03:09:13.768795 140453607589056 spec.py:321] Evaluating on the training split.
I0306 03:09:16.415547 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 03:12:45.933009 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 03:12:48.564851 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 03:15:22.299934 140453607589056 spec.py:349] Evaluating on the test split.
I0306 03:15:24.937226 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 03:17:44.315123 140453607589056 submission_runner.py:469] Time since start: 29020.82s, 	Step: 47974, 	{'train/accuracy': 0.6438049077987671, 'train/loss': 1.7272613048553467, 'train/bleu': 31.116255338257915, 'validation/accuracy': 0.6634860634803772, 'validation/loss': 1.5731979608535767, 'validation/bleu': 28.738017957091454, 'validation/num_examples': 3000, 'test/accuracy': 0.6743598580360413, 'test/loss': 1.5088788270950317, 'test/bleu': 28.028513780507694, 'test/num_examples': 3003, 'score': 16826.6128783226, 'total_duration': 29020.819280147552, 'accumulated_submission_time': 16826.6128783226, 'accumulated_eval_time': 12190.90675854683, 'accumulated_logging_time': 0.424515962600708}
I0306 03:17:44.329726 140309904176896 logging_writer.py:48] [47974] accumulated_eval_time=12190.9, accumulated_logging_time=0.424516, accumulated_submission_time=16826.6, global_step=47974, preemption_count=0, score=16826.6, test/accuracy=0.67436, test/bleu=28.0285, test/loss=1.50888, test/num_examples=3003, total_duration=29020.8, train/accuracy=0.643805, train/bleu=31.1163, train/loss=1.72726, validation/accuracy=0.663486, validation/bleu=28.738, validation/loss=1.5732, validation/num_examples=3000
I0306 03:17:53.749150 140309895784192 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.37476903200149536, loss=1.6800811290740967
I0306 03:18:28.626184 140309904176896 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.4260047674179077, loss=1.8014329671859741
I0306 03:19:03.574301 140309895784192 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.38712963461875916, loss=1.8139288425445557
I0306 03:19:38.532622 140309904176896 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.3907822072505951, loss=1.7396944761276245
I0306 03:20:13.529957 140309895784192 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.4312371611595154, loss=1.7945687770843506
I0306 03:20:48.541541 140309904176896 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.43412724137306213, loss=1.70365309715271
I0306 03:21:23.559618 140309895784192 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3958812952041626, loss=1.6997015476226807
I0306 03:21:58.574435 140309904176896 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.4066672921180725, loss=1.747735857963562
I0306 03:22:33.575230 140309895784192 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.4226108193397522, loss=1.6562001705169678
I0306 03:23:08.546118 140309904176896 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.37873050570487976, loss=1.7550126314163208
I0306 03:23:43.545099 140309895784192 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.39033687114715576, loss=1.776749610900879
I0306 03:24:18.539932 140309904176896 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.40255093574523926, loss=1.6895819902420044
I0306 03:24:53.505008 140309895784192 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.39656558632850647, loss=1.7296710014343262
I0306 03:25:28.492496 140309904176896 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.4484449028968811, loss=1.7522165775299072
I0306 03:26:03.484984 140309895784192 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.4290775656700134, loss=1.6738466024398804
I0306 03:26:38.521669 140309904176896 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.45112335681915283, loss=1.7289365530014038
I0306 03:27:13.495931 140309895784192 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.3785017728805542, loss=1.6656453609466553
I0306 03:27:48.478776 140309904176896 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.36588573455810547, loss=1.6400097608566284
I0306 03:28:23.449381 140309895784192 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.35847538709640503, loss=1.7210545539855957
I0306 03:28:58.469436 140309904176896 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.45500555634498596, loss=1.78888738155365
I0306 03:29:33.474725 140309895784192 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.41433602571487427, loss=1.7408190965652466
I0306 03:30:08.465899 140309904176896 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.3973066806793213, loss=1.7188904285430908
I0306 03:30:43.446615 140309895784192 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.43093007802963257, loss=1.7017805576324463
I0306 03:31:18.389153 140309904176896 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.4356679320335388, loss=1.6514109373092651
I0306 03:31:44.609864 140453607589056 spec.py:321] Evaluating on the training split.
I0306 03:31:47.253746 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 03:35:01.975186 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 03:35:04.607355 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 03:37:41.319930 140453607589056 spec.py:349] Evaluating on the test split.
I0306 03:37:43.943303 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 03:40:09.526411 140453607589056 submission_runner.py:469] Time since start: 30366.03s, 	Step: 50376, 	{'train/accuracy': 0.674156904220581, 'train/loss': 1.5261074304580688, 'train/bleu': 33.58847072105214, 'validation/accuracy': 0.6658962368965149, 'validation/loss': 1.5646727085113525, 'validation/bleu': 28.56486944309737, 'validation/num_examples': 3000, 'test/accuracy': 0.6754373908042908, 'test/loss': 1.5047029256820679, 'test/bleu': 28.272951000066193, 'test/num_examples': 3003, 'score': 17666.74790906906, 'total_duration': 30366.03059363365, 'accumulated_submission_time': 17666.74790906906, 'accumulated_eval_time': 12695.823257684708, 'accumulated_logging_time': 0.44875073432922363}
I0306 03:40:09.540853 140309895784192 logging_writer.py:48] [50376] accumulated_eval_time=12695.8, accumulated_logging_time=0.448751, accumulated_submission_time=17666.7, global_step=50376, preemption_count=0, score=17666.7, test/accuracy=0.675437, test/bleu=28.273, test/loss=1.5047, test/num_examples=3003, total_duration=30366, train/accuracy=0.674157, train/bleu=33.5885, train/loss=1.52611, validation/accuracy=0.665896, validation/bleu=28.5649, validation/loss=1.56467, validation/num_examples=3000
I0306 03:40:18.252045 140309904176896 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.40107324719429016, loss=1.7024588584899902
I0306 03:40:53.054270 140309895784192 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.4136088192462921, loss=1.7404006719589233
I0306 03:41:27.968803 140309904176896 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.461866557598114, loss=1.6716004610061646
I0306 03:42:02.897124 140309895784192 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.3911500871181488, loss=1.6283401250839233
I0306 03:42:37.837596 140309904176896 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3715803623199463, loss=1.682259440422058
I0306 03:43:12.770379 140309895784192 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.36855554580688477, loss=1.7222729921340942
I0306 03:43:47.713793 140309904176896 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.4216999113559723, loss=1.7150553464889526
I0306 03:44:22.640954 140309895784192 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.40221884846687317, loss=1.707195520401001
I0306 03:44:57.570433 140309904176896 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.4009392559528351, loss=1.6622520685195923
I0306 03:45:32.505742 140309895784192 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.44679874181747437, loss=1.7465016841888428
I0306 03:46:07.446607 140309904176896 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.4714433252811432, loss=1.7906337976455688
I0306 03:46:42.374546 140309895784192 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3969900906085968, loss=1.7554153203964233
I0306 03:47:17.320651 140309904176896 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.40595027804374695, loss=1.6734639406204224
I0306 03:47:52.261086 140309895784192 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.40324345231056213, loss=1.6787070035934448
I0306 03:48:27.222074 140309904176896 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.386379599571228, loss=1.7409443855285645
I0306 03:49:02.172343 140309895784192 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.4126865863800049, loss=1.7175655364990234
I0306 03:49:37.134908 140309904176896 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.41963183879852295, loss=1.738008737564087
I0306 03:50:12.107993 140309895784192 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.45317357778549194, loss=1.685059666633606
I0306 03:50:47.065032 140309904176896 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3921428918838501, loss=1.6877427101135254
I0306 03:51:21.993228 140309895784192 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.3747134208679199, loss=1.6802338361740112
I0306 03:51:56.942800 140309904176896 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.39293742179870605, loss=1.7397390604019165
I0306 03:52:31.915910 140309895784192 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.43055880069732666, loss=1.7686378955841064
I0306 03:53:06.869249 140309904176896 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.384188175201416, loss=1.7113852500915527
I0306 03:53:41.844194 140309895784192 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.40432363748550415, loss=1.7208601236343384
I0306 03:54:09.834250 140453607589056 spec.py:321] Evaluating on the training split.
I0306 03:54:12.489253 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 03:58:19.733909 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 03:58:22.363057 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:01:07.238557 140453607589056 spec.py:349] Evaluating on the test split.
I0306 04:01:09.880386 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:03:47.782996 140453607589056 submission_runner.py:469] Time since start: 31784.29s, 	Step: 52781, 	{'train/accuracy': 0.6486982703208923, 'train/loss': 1.6904340982437134, 'train/bleu': 32.15732694962338, 'validation/accuracy': 0.6673176288604736, 'validation/loss': 1.5558617115020752, 'validation/bleu': 28.77861264876485, 'validation/num_examples': 3000, 'test/accuracy': 0.6812883615493774, 'test/loss': 1.4824622869491577, 'test/bleu': 28.723405265269353, 'test/num_examples': 3003, 'score': 18506.898864746094, 'total_duration': 31784.287172317505, 'accumulated_submission_time': 18506.898864746094, 'accumulated_eval_time': 13273.771958827972, 'accumulated_logging_time': 0.4718286991119385}
I0306 04:03:47.797496 140309904176896 logging_writer.py:48] [52781] accumulated_eval_time=13273.8, accumulated_logging_time=0.471829, accumulated_submission_time=18506.9, global_step=52781, preemption_count=0, score=18506.9, test/accuracy=0.681288, test/bleu=28.7234, test/loss=1.48246, test/num_examples=3003, total_duration=31784.3, train/accuracy=0.648698, train/bleu=32.1573, train/loss=1.69043, validation/accuracy=0.667318, validation/bleu=28.7786, validation/loss=1.55586, validation/num_examples=3000
I0306 04:03:54.780340 140309895784192 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.37806716561317444, loss=1.6862504482269287
I0306 04:04:29.644813 140309904176896 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.39612463116645813, loss=1.7590527534484863
I0306 04:05:04.579898 140309895784192 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.40600845217704773, loss=1.6959408521652222
I0306 04:05:39.514858 140309904176896 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.42548781633377075, loss=1.775060772895813
I0306 04:06:14.464930 140309895784192 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.36890843510627747, loss=1.7243646383285522
I0306 04:06:49.481179 140309904176896 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.36927658319473267, loss=1.6981205940246582
I0306 04:07:24.470574 140309895784192 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.4013308882713318, loss=1.6522536277770996
I0306 04:07:59.467607 140309904176896 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.42128679156303406, loss=1.7765754461288452
I0306 04:08:34.426151 140309895784192 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.39897212386131287, loss=1.629128098487854
I0306 04:09:09.414118 140309904176896 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.4406742751598358, loss=1.6854854822158813
I0306 04:09:44.352062 140309895784192 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.4062586724758148, loss=1.711012601852417
I0306 04:10:19.332150 140309904176896 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.4748809039592743, loss=1.727837324142456
I0306 04:10:54.301733 140309895784192 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3982212543487549, loss=1.6904680728912354
I0306 04:11:29.286563 140309904176896 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3962938189506531, loss=1.6711522340774536
I0306 04:12:04.253969 140309895784192 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.4063092768192291, loss=1.7035281658172607
I0306 04:12:39.261142 140309904176896 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.39108914136886597, loss=1.7209941148757935
I0306 04:13:14.201414 140309895784192 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.38034307956695557, loss=1.7536065578460693
I0306 04:13:49.172020 140309904176896 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.40857771039009094, loss=1.7129956483840942
I0306 04:14:24.132446 140309895784192 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3575406074523926, loss=1.7292946577072144
I0306 04:14:59.095088 140309904176896 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.44375985860824585, loss=1.7641947269439697
I0306 04:15:34.022974 140309895784192 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.4214887320995331, loss=1.7871956825256348
I0306 04:16:08.974163 140309904176896 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.4162088632583618, loss=1.7497777938842773
I0306 04:16:43.960800 140309895784192 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3717522621154785, loss=1.6826070547103882
I0306 04:17:18.920619 140309904176896 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.3826014995574951, loss=1.7212542295455933
I0306 04:17:47.965451 140453607589056 spec.py:321] Evaluating on the training split.
I0306 04:17:50.622273 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:21:16.998445 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 04:21:19.634283 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:24:05.298106 140453607589056 spec.py:349] Evaluating on the test split.
I0306 04:24:07.926778 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:26:33.374300 140453607589056 submission_runner.py:469] Time since start: 33149.88s, 	Step: 55184, 	{'train/accuracy': 0.6492453217506409, 'train/loss': 1.69197416305542, 'train/bleu': 31.692787530130385, 'validation/accuracy': 0.667243480682373, 'validation/loss': 1.5565459728240967, 'validation/bleu': 28.621042173289222, 'validation/num_examples': 3000, 'test/accuracy': 0.6807669997215271, 'test/loss': 1.4774938821792603, 'test/bleu': 28.647201093077218, 'test/num_examples': 3003, 'score': 19346.921707868576, 'total_duration': 33149.87847661972, 'accumulated_submission_time': 19346.921707868576, 'accumulated_eval_time': 13799.180753707886, 'accumulated_logging_time': 0.49449825286865234}
I0306 04:26:33.388655 140309895784192 logging_writer.py:48] [55184] accumulated_eval_time=13799.2, accumulated_logging_time=0.494498, accumulated_submission_time=19346.9, global_step=55184, preemption_count=0, score=19346.9, test/accuracy=0.680767, test/bleu=28.6472, test/loss=1.47749, test/num_examples=3003, total_duration=33149.9, train/accuracy=0.649245, train/bleu=31.6928, train/loss=1.69197, validation/accuracy=0.667243, validation/bleu=28.621, validation/loss=1.55655, validation/num_examples=3000
I0306 04:26:39.310184 140309904176896 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.39955055713653564, loss=1.6754502058029175
I0306 04:27:14.162253 140309895784192 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.41585150361061096, loss=1.728105902671814
I0306 04:27:49.086964 140309904176896 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.4102488160133362, loss=1.7292327880859375
I0306 04:28:24.034580 140309895784192 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.39867764711380005, loss=1.7122825384140015
I0306 04:28:58.967432 140309904176896 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.38939934968948364, loss=1.7382855415344238
I0306 04:29:33.926537 140309895784192 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3952900469303131, loss=1.7588202953338623
I0306 04:30:08.874874 140309904176896 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3869493305683136, loss=1.666271448135376
I0306 04:30:43.820678 140309895784192 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.502258837223053, loss=1.7040365934371948
I0306 04:31:18.787921 140309904176896 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.39705321192741394, loss=1.773685097694397
I0306 04:31:53.717255 140309895784192 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.42030227184295654, loss=1.6589407920837402
I0306 04:32:28.664789 140309904176896 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.399440199136734, loss=1.7661957740783691
I0306 04:33:03.631325 140309895784192 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.4112132489681244, loss=1.6879193782806396
I0306 04:33:38.582445 140309904176896 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.40375855565071106, loss=1.6516616344451904
I0306 04:34:13.592752 140309895784192 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.40379688143730164, loss=1.7454309463500977
I0306 04:34:48.593499 140309904176896 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.41283082962036133, loss=1.6863371133804321
I0306 04:35:23.601107 140309895784192 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.4452769458293915, loss=1.5769680738449097
I0306 04:35:58.582542 140309904176896 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3757309913635254, loss=1.6829863786697388
I0306 04:36:33.568169 140309895784192 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.44944852590560913, loss=1.7192968130111694
I0306 04:37:08.586444 140309904176896 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3876217007637024, loss=1.6921122074127197
I0306 04:37:43.568695 140309895784192 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.38846632838249207, loss=1.6465691328048706
I0306 04:38:18.571825 140309904176896 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.4004254639148712, loss=1.7289506196975708
I0306 04:38:53.590244 140309895784192 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.4183560609817505, loss=1.737821340560913
I0306 04:39:28.578993 140309904176896 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.37169310450553894, loss=1.6998059749603271
I0306 04:40:03.572752 140309895784192 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.39615270495414734, loss=1.5799338817596436
I0306 04:40:33.681880 140453607589056 spec.py:321] Evaluating on the training split.
I0306 04:40:36.328978 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:43:53.966910 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 04:43:56.603924 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:46:27.423894 140453607589056 spec.py:349] Evaluating on the test split.
I0306 04:46:30.073933 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 04:48:53.279698 140453607589056 submission_runner.py:469] Time since start: 34489.78s, 	Step: 57587, 	{'train/accuracy': 0.6559727191925049, 'train/loss': 1.6356278657913208, 'train/bleu': 31.691430250379028, 'validation/accuracy': 0.6697031259536743, 'validation/loss': 1.5434856414794922, 'validation/bleu': 29.144534450246965, 'validation/num_examples': 3000, 'test/accuracy': 0.6802108883857727, 'test/loss': 1.4796209335327148, 'test/bleu': 28.332969685306747, 'test/num_examples': 3003, 'score': 20187.067399024963, 'total_duration': 34489.783871889114, 'accumulated_submission_time': 20187.067399024963, 'accumulated_eval_time': 14298.7785115242, 'accumulated_logging_time': 0.5188720226287842}
I0306 04:48:53.295957 140309904176896 logging_writer.py:48] [57587] accumulated_eval_time=14298.8, accumulated_logging_time=0.518872, accumulated_submission_time=20187.1, global_step=57587, preemption_count=0, score=20187.1, test/accuracy=0.680211, test/bleu=28.333, test/loss=1.47962, test/num_examples=3003, total_duration=34489.8, train/accuracy=0.655973, train/bleu=31.6914, train/loss=1.63563, validation/accuracy=0.669703, validation/bleu=29.1445, validation/loss=1.54349, validation/num_examples=3000
I0306 04:48:58.182804 140309895784192 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.4006118178367615, loss=1.739355444908142
I0306 04:49:33.073102 140309904176896 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.38007867336273193, loss=1.6719398498535156
I0306 04:50:08.007925 140309895784192 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.39546358585357666, loss=1.7548174858093262
I0306 04:50:42.999141 140309904176896 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.39137810468673706, loss=1.6997138261795044
I0306 04:51:17.991370 140309895784192 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3884740173816681, loss=1.6966685056686401
I0306 04:51:53.007681 140309904176896 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.40819475054740906, loss=1.6843230724334717
I0306 04:52:27.983976 140309895784192 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.4337771236896515, loss=1.686506748199463
I0306 04:53:02.991531 140309904176896 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.40635085105895996, loss=1.695841908454895
I0306 04:53:37.970146 140309895784192 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.4670153260231018, loss=1.6423578262329102
I0306 04:54:12.966387 140309904176896 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.3847251236438751, loss=1.657732605934143
I0306 04:54:47.992475 140309895784192 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.4081992506980896, loss=1.6373023986816406
I0306 04:55:23.005974 140309904176896 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.378598153591156, loss=1.6999287605285645
I0306 04:55:58.069012 140309895784192 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.4020480811595917, loss=1.6934216022491455
I0306 04:56:33.048251 140309904176896 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3799927234649658, loss=1.68694269657135
I0306 04:57:08.001386 140309895784192 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.40894630551338196, loss=1.7376407384872437
I0306 04:57:42.982334 140309904176896 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.4191610515117645, loss=1.7003813982009888
I0306 04:58:17.963532 140309895784192 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.40114131569862366, loss=1.5720436573028564
I0306 04:58:52.980597 140309904176896 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.37314602732658386, loss=1.6665719747543335
I0306 04:59:28.006523 140309895784192 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3975156545639038, loss=1.6415456533432007
I0306 05:00:03.025816 140309904176896 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.39143863320350647, loss=1.7034897804260254
I0306 05:00:38.063444 140309895784192 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.40047353506088257, loss=1.5485845804214478
I0306 05:01:13.050363 140309904176896 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.38919880986213684, loss=1.6626129150390625
I0306 05:01:48.076982 140309895784192 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.3871849775314331, loss=1.7166565656661987
I0306 05:02:23.092958 140309904176896 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.4024503827095032, loss=1.6617180109024048
I0306 05:02:53.547651 140453607589056 spec.py:321] Evaluating on the training split.
I0306 05:02:56.201503 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:06:32.380475 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 05:06:35.008965 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:09:01.840036 140453607589056 spec.py:349] Evaluating on the test split.
I0306 05:09:04.475576 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:11:27.843261 140453607589056 submission_runner.py:469] Time since start: 35844.35s, 	Step: 59988, 	{'train/accuracy': 0.6522138714790344, 'train/loss': 1.6799737215042114, 'train/bleu': 31.940333164314833, 'validation/accuracy': 0.6708031892776489, 'validation/loss': 1.5347214937210083, 'validation/bleu': 28.853402565455895, 'validation/num_examples': 3000, 'test/accuracy': 0.6851581335067749, 'test/loss': 1.4620137214660645, 'test/bleu': 28.706303126128148, 'test/num_examples': 3003, 'score': 21027.170575380325, 'total_duration': 35844.34740138054, 'accumulated_submission_time': 21027.170575380325, 'accumulated_eval_time': 14813.074025392532, 'accumulated_logging_time': 0.5440225601196289}
I0306 05:11:27.858117 140309895784192 logging_writer.py:48] [59988] accumulated_eval_time=14813.1, accumulated_logging_time=0.544023, accumulated_submission_time=21027.2, global_step=59988, preemption_count=0, score=21027.2, test/accuracy=0.685158, test/bleu=28.7063, test/loss=1.46201, test/num_examples=3003, total_duration=35844.3, train/accuracy=0.652214, train/bleu=31.9403, train/loss=1.67997, validation/accuracy=0.670803, validation/bleu=28.8534, validation/loss=1.53472, validation/num_examples=3000
I0306 05:11:32.399616 140309904176896 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.3780767023563385, loss=1.6404926776885986
I0306 05:12:07.298847 140309895784192 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.3889426290988922, loss=1.6779509782791138
I0306 05:12:42.233025 140309904176896 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.39715373516082764, loss=1.625788927078247
I0306 05:13:17.226595 140309895784192 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.3924018442630768, loss=1.6936001777648926
I0306 05:13:52.230754 140309904176896 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.4545450210571289, loss=1.6937462091445923
I0306 05:14:27.231739 140309895784192 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.41171491146087646, loss=1.7021819353103638
I0306 05:15:02.251364 140309904176896 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.3681081533432007, loss=1.6535630226135254
I0306 05:15:37.253840 140309895784192 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.412781685590744, loss=1.6650429964065552
I0306 05:16:12.267869 140309904176896 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.391101211309433, loss=1.7130659818649292
I0306 05:16:47.270199 140309895784192 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3820960223674774, loss=1.6862225532531738
I0306 05:17:22.320253 140309904176896 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.41006678342819214, loss=1.6897428035736084
I0306 05:17:57.299854 140309895784192 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.39681774377822876, loss=1.6923147439956665
I0306 05:18:32.256084 140309904176896 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.42167431116104126, loss=1.6664680242538452
I0306 05:19:07.253157 140309895784192 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.387033075094223, loss=1.6690257787704468
I0306 05:19:42.217399 140309904176896 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.4198334515094757, loss=1.6060200929641724
I0306 05:20:17.194957 140309895784192 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.4530043303966522, loss=1.6423202753067017
I0306 05:20:52.176017 140309904176896 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.4140699803829193, loss=1.6357626914978027
I0306 05:21:27.124799 140309895784192 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.4195745289325714, loss=1.6322238445281982
I0306 05:22:02.104251 140309904176896 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.39936789870262146, loss=1.7273385524749756
I0306 05:22:37.061335 140309895784192 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3878009021282196, loss=1.6408246755599976
I0306 05:23:12.059252 140309904176896 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.43119239807128906, loss=1.687637448310852
I0306 05:23:47.022266 140309895784192 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.3657134771347046, loss=1.6423578262329102
I0306 05:24:22.009272 140309904176896 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.4355543255805969, loss=1.6865203380584717
I0306 05:24:56.986794 140309895784192 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.40800729393959045, loss=1.6754060983657837
I0306 05:25:28.127671 140453607589056 spec.py:321] Evaluating on the training split.
I0306 05:25:30.779515 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:29:06.963295 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 05:29:09.597885 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:31:50.465103 140453607589056 spec.py:349] Evaluating on the test split.
I0306 05:31:53.109548 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:34:10.230098 140453607589056 submission_runner.py:469] Time since start: 37206.73s, 	Step: 62390, 	{'train/accuracy': 0.651102602481842, 'train/loss': 1.6824616193771362, 'train/bleu': 32.226547921763384, 'validation/accuracy': 0.6733740568161011, 'validation/loss': 1.5258394479751587, 'validation/bleu': 29.09713585123688, 'validation/num_examples': 3000, 'test/accuracy': 0.6856447458267212, 'test/loss': 1.4494532346725464, 'test/bleu': 29.06448380937648, 'test/num_examples': 3003, 'score': 21867.291534662247, 'total_duration': 37206.734256505966, 'accumulated_submission_time': 21867.291534662247, 'accumulated_eval_time': 15335.17637681961, 'accumulated_logging_time': 0.5673058032989502}
I0306 05:34:10.246308 140309904176896 logging_writer.py:48] [62390] accumulated_eval_time=15335.2, accumulated_logging_time=0.567306, accumulated_submission_time=21867.3, global_step=62390, preemption_count=0, score=21867.3, test/accuracy=0.685645, test/bleu=29.0645, test/loss=1.44945, test/num_examples=3003, total_duration=37206.7, train/accuracy=0.651103, train/bleu=32.2265, train/loss=1.68246, validation/accuracy=0.673374, validation/bleu=29.0971, validation/loss=1.52584, validation/num_examples=3000
I0306 05:34:14.083922 140309895784192 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.4183565080165863, loss=1.821561336517334
I0306 05:34:49.017893 140309904176896 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.4030163884162903, loss=1.6122325658798218
I0306 05:35:23.960432 140309895784192 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.41292804479599, loss=1.7380189895629883
I0306 05:35:58.945073 140309904176896 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3889364004135132, loss=1.6411408185958862
I0306 05:36:33.941368 140309895784192 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.37932220101356506, loss=1.5990257263183594
I0306 05:37:08.941084 140309904176896 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3944774568080902, loss=1.6093326807022095
I0306 05:37:43.900057 140309895784192 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3954884707927704, loss=1.696233868598938
I0306 05:38:18.894376 140309904176896 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.42039117217063904, loss=1.6972004175186157
I0306 05:38:53.885631 140309895784192 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.38102537393569946, loss=1.6707252264022827
I0306 05:39:28.872543 140309904176896 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.3892641067504883, loss=1.7003779411315918
I0306 05:40:03.894832 140309895784192 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.405628502368927, loss=1.6727044582366943
I0306 05:40:38.906882 140309904176896 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.42465412616729736, loss=1.7114249467849731
I0306 05:41:13.915228 140309895784192 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3979911208152771, loss=1.655450463294983
I0306 05:41:48.869774 140309904176896 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.40883904695510864, loss=1.661042332649231
I0306 05:42:23.842683 140309895784192 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.38334769010543823, loss=1.7204538583755493
I0306 05:42:58.829396 140309904176896 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.41039201617240906, loss=1.663248062133789
I0306 05:43:33.798528 140309895784192 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.4171985983848572, loss=1.633046269416809
I0306 05:44:08.767994 140309904176896 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.41360902786254883, loss=1.677463412284851
I0306 05:44:43.743127 140309895784192 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.42477962374687195, loss=1.5973947048187256
I0306 05:45:18.717185 140309904176896 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.40865594148635864, loss=1.6477715969085693
I0306 05:45:53.692509 140309895784192 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.42494189739227295, loss=1.6983565092086792
I0306 05:46:28.667572 140309904176896 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.4064827263355255, loss=1.6591740846633911
I0306 05:47:03.633381 140309895784192 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.40786656737327576, loss=1.603918194770813
I0306 05:47:38.609928 140309904176896 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3862544000148773, loss=1.655678391456604
I0306 05:48:10.446204 140453607589056 spec.py:321] Evaluating on the training split.
I0306 05:48:13.095048 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:52:22.177067 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 05:52:24.805106 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:55:00.256175 140453607589056 spec.py:349] Evaluating on the test split.
I0306 05:55:02.879418 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 05:57:35.706558 140453607589056 submission_runner.py:469] Time since start: 38612.21s, 	Step: 64792, 	{'train/accuracy': 0.6555148363113403, 'train/loss': 1.6478312015533447, 'train/bleu': 32.878248224010875, 'validation/accuracy': 0.6724717617034912, 'validation/loss': 1.5215963125228882, 'validation/bleu': 29.098214727323537, 'validation/num_examples': 3000, 'test/accuracy': 0.6864789724349976, 'test/loss': 1.4471114873886108, 'test/bleu': 28.87790404441971, 'test/num_examples': 3003, 'score': 22707.34805560112, 'total_duration': 38612.21073055267, 'accumulated_submission_time': 22707.34805560112, 'accumulated_eval_time': 15900.436671257019, 'accumulated_logging_time': 0.5921285152435303}
I0306 05:57:35.722718 140309895784192 logging_writer.py:48] [64792] accumulated_eval_time=15900.4, accumulated_logging_time=0.592129, accumulated_submission_time=22707.3, global_step=64792, preemption_count=0, score=22707.3, test/accuracy=0.686479, test/bleu=28.8779, test/loss=1.44711, test/num_examples=3003, total_duration=38612.2, train/accuracy=0.655515, train/bleu=32.8782, train/loss=1.64783, validation/accuracy=0.672472, validation/bleu=29.0982, validation/loss=1.5216, validation/num_examples=3000
I0306 05:57:38.856126 140309904176896 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.3967199921607971, loss=1.6230298280715942
I0306 05:58:13.730865 140309895784192 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.4318660497665405, loss=1.6463909149169922
I0306 05:58:48.679352 140309904176896 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.42799216508865356, loss=1.602873682975769
I0306 05:59:23.647717 140309895784192 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.4581157863140106, loss=1.625949740409851
I0306 05:59:58.672892 140309904176896 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.44662174582481384, loss=1.6967693567276
I0306 06:00:33.673806 140309895784192 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.4102969765663147, loss=1.5982587337493896
I0306 06:01:08.703668 140309904176896 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.38491514325141907, loss=1.6622962951660156
I0306 06:01:43.719481 140309895784192 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3874044120311737, loss=1.6472569704055786
I0306 06:02:18.700181 140309904176896 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.4046170115470886, loss=1.6395809650421143
I0306 06:02:53.670958 140309895784192 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.4235374629497528, loss=1.67847740650177
I0306 06:03:28.656451 140309904176896 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.4030380845069885, loss=1.6308960914611816
I0306 06:04:03.664612 140309895784192 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.4127345383167267, loss=1.717156171798706
I0306 06:04:38.678389 140309904176896 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.40210771560668945, loss=1.584568977355957
I0306 06:05:13.711521 140309895784192 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.4206182062625885, loss=1.6329360008239746
I0306 06:05:48.719848 140309904176896 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.41802963614463806, loss=1.6566262245178223
I0306 06:06:23.696271 140309895784192 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.4361056387424469, loss=1.6186528205871582
I0306 06:06:58.678727 140309904176896 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.42120590806007385, loss=1.7148098945617676
I0306 06:07:33.651750 140309895784192 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.4355788230895996, loss=1.6548808813095093
I0306 06:08:08.712605 140309904176896 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.4053109884262085, loss=1.6573666334152222
I0306 06:08:43.769077 140309895784192 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.405272901058197, loss=1.6384814977645874
I0306 06:09:18.792546 140309904176896 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.4000200629234314, loss=1.6156891584396362
I0306 06:09:53.814177 140309895784192 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.48540329933166504, loss=1.7002832889556885
I0306 06:10:28.813908 140309904176896 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.4203718602657318, loss=1.6150189638137817
I0306 06:11:03.804881 140309895784192 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.4342731535434723, loss=1.6308488845825195
I0306 06:11:36.037722 140453607589056 spec.py:321] Evaluating on the training split.
I0306 06:11:38.690490 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 06:16:26.959809 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 06:16:29.595329 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 06:19:15.980776 140453607589056 spec.py:349] Evaluating on the test split.
I0306 06:19:18.610402 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 06:22:01.338754 140453607589056 submission_runner.py:469] Time since start: 40077.84s, 	Step: 67193, 	{'train/accuracy': 0.6565093994140625, 'train/loss': 1.6455990076065063, 'train/bleu': 32.42100988563706, 'validation/accuracy': 0.673954963684082, 'validation/loss': 1.5144321918487549, 'validation/bleu': 29.21163010832368, 'validation/num_examples': 3000, 'test/accuracy': 0.6876491904258728, 'test/loss': 1.4376064538955688, 'test/bleu': 28.975636482633906, 'test/num_examples': 3003, 'score': 23547.51508998871, 'total_duration': 40077.84293174744, 'accumulated_submission_time': 23547.51508998871, 'accumulated_eval_time': 16525.73765206337, 'accumulated_logging_time': 0.6165614128112793}
I0306 06:22:01.354646 140309904176896 logging_writer.py:48] [67193] accumulated_eval_time=16525.7, accumulated_logging_time=0.616561, accumulated_submission_time=23547.5, global_step=67193, preemption_count=0, score=23547.5, test/accuracy=0.687649, test/bleu=28.9756, test/loss=1.43761, test/num_examples=3003, total_duration=40077.8, train/accuracy=0.656509, train/bleu=32.421, train/loss=1.6456, validation/accuracy=0.673955, validation/bleu=29.2116, validation/loss=1.51443, validation/num_examples=3000
I0306 06:22:04.139720 140309895784192 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.39950403571128845, loss=1.6432514190673828
I0306 06:22:38.976945 140309904176896 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.4150850772857666, loss=1.7102407217025757
I0306 06:23:13.894032 140309895784192 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.39597782492637634, loss=1.57302725315094
I0306 06:23:48.844942 140309904176896 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.41225287318229675, loss=1.7098087072372437
I0306 06:24:23.821490 140309895784192 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.44055524468421936, loss=1.6688953638076782
I0306 06:24:58.808107 140309904176896 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.4070817530155182, loss=1.7177737951278687
I0306 06:25:33.777446 140309895784192 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.4273436963558197, loss=1.7298945188522339
I0306 06:26:08.781004 140309904176896 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.46908316016197205, loss=1.6106204986572266
I0306 06:26:43.767430 140309895784192 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.40292641520500183, loss=1.5852845907211304
I0306 06:27:18.776356 140309904176896 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.38969314098358154, loss=1.580214023590088
I0306 06:27:53.767305 140309895784192 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.4035818576812744, loss=1.7361263036727905
I0306 06:28:28.763014 140309904176896 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.40682822465896606, loss=1.5930278301239014
I0306 06:29:03.733007 140309895784192 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.4494529664516449, loss=1.6622722148895264
I0306 06:29:38.731444 140309904176896 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.39812326431274414, loss=1.6147150993347168
I0306 06:30:13.740079 140309895784192 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.5440966486930847, loss=1.6503185033798218
I0306 06:30:48.742246 140309904176896 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.4291887879371643, loss=1.6523783206939697
I0306 06:31:23.755933 140309895784192 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.4520954489707947, loss=1.580755352973938
I0306 06:31:58.758922 140309904176896 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.4236873388290405, loss=1.6683672666549683
I0306 06:32:33.797149 140309895784192 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.4610523283481598, loss=1.728196382522583
I0306 06:33:08.758625 140309904176896 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.47393667697906494, loss=1.672276496887207
I0306 06:33:43.726776 140309895784192 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.4112352728843689, loss=1.6603834629058838
I0306 06:34:18.631627 140309904176896 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3957013189792633, loss=1.6629981994628906
I0306 06:34:53.587573 140309895784192 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.39359813928604126, loss=1.5329896211624146
I0306 06:35:28.548795 140309904176896 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.4484408497810364, loss=1.6804497241973877
I0306 06:36:01.381590 140453607589056 spec.py:321] Evaluating on the training split.
I0306 06:36:04.027189 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 06:40:22.136488 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 06:40:24.768064 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 06:43:25.145685 140453607589056 spec.py:349] Evaluating on the test split.
I0306 06:43:27.785723 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 06:47:06.670161 140453607589056 submission_runner.py:469] Time since start: 41583.17s, 	Step: 69595, 	{'train/accuracy': 0.6725636124610901, 'train/loss': 1.5473986864089966, 'train/bleu': 33.509762381245224, 'validation/accuracy': 0.6759201884269714, 'validation/loss': 1.5030101537704468, 'validation/bleu': 29.617065789973818, 'validation/num_examples': 3000, 'test/accuracy': 0.6882632374763489, 'test/loss': 1.4309974908828735, 'test/bleu': 28.949633304848806, 'test/num_examples': 3003, 'score': 24387.395591020584, 'total_duration': 41583.17433524132, 'accumulated_submission_time': 24387.395591020584, 'accumulated_eval_time': 17191.026163101196, 'accumulated_logging_time': 0.6411175727844238}
I0306 06:47:06.686651 140309895784192 logging_writer.py:48] [69595] accumulated_eval_time=17191, accumulated_logging_time=0.641118, accumulated_submission_time=24387.4, global_step=69595, preemption_count=0, score=24387.4, test/accuracy=0.688263, test/bleu=28.9496, test/loss=1.431, test/num_examples=3003, total_duration=41583.2, train/accuracy=0.672564, train/bleu=33.5098, train/loss=1.5474, validation/accuracy=0.67592, validation/bleu=29.6171, validation/loss=1.50301, validation/num_examples=3000
I0306 06:47:08.776460 140309904176896 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.42198866605758667, loss=1.6805155277252197
I0306 06:47:43.614215 140309895784192 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.4090094566345215, loss=1.674100637435913
I0306 06:48:18.577941 140309904176896 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.39733588695526123, loss=1.5932111740112305
I0306 06:48:53.480670 140309895784192 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.42877939343452454, loss=1.6156022548675537
I0306 06:49:28.428916 140309904176896 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.41109761595726013, loss=1.619582176208496
I0306 06:50:03.383704 140309895784192 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.40699633955955505, loss=1.6771854162216187
I0306 06:50:38.347641 140309904176896 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.4110476076602936, loss=1.6463507413864136
I0306 06:51:13.269470 140309895784192 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3989856243133545, loss=1.569020390510559
I0306 06:51:48.215590 140309904176896 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.4158063232898712, loss=1.6494439840316772
I0306 06:52:23.152007 140309895784192 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.4140792787075043, loss=1.512845754623413
I0306 06:52:58.077394 140309904176896 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.41767799854278564, loss=1.6902211904525757
I0306 06:53:33.033824 140309895784192 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.4385320544242859, loss=1.6649755239486694
I0306 06:54:07.985859 140309904176896 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.4994220733642578, loss=1.6347037553787231
I0306 06:54:42.933803 140309895784192 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.4098007082939148, loss=1.713757038116455
I0306 06:55:17.900453 140309904176896 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.44726648926734924, loss=1.6699343919754028
I0306 06:55:52.882119 140309895784192 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.413521409034729, loss=1.5790085792541504
I0306 06:56:27.837006 140309904176896 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.4137879014015198, loss=1.59976065158844
I0306 06:57:02.783720 140309895784192 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.436166912317276, loss=1.6754019260406494
I0306 06:57:37.747421 140309904176896 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.4610599875450134, loss=1.7061138153076172
I0306 06:58:12.682512 140309895784192 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.43878182768821716, loss=1.641395092010498
I0306 06:58:47.640721 140309904176896 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.44725126028060913, loss=1.6499701738357544
I0306 06:59:22.604891 140309895784192 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.4100080728530884, loss=1.6936061382293701
I0306 06:59:57.546780 140309904176896 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.4339679479598999, loss=1.7160370349884033
I0306 07:00:32.486548 140309895784192 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.4164336025714874, loss=1.5795780420303345
I0306 07:01:06.705107 140453607589056 spec.py:321] Evaluating on the training split.
I0306 07:01:09.355955 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:05:55.234255 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 07:05:57.863665 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:08:48.742952 140453607589056 spec.py:349] Evaluating on the test split.
I0306 07:08:51.382143 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:11:36.038805 140453607589056 submission_runner.py:469] Time since start: 43052.54s, 	Step: 71999, 	{'train/accuracy': 0.6619178056716919, 'train/loss': 1.60077965259552, 'train/bleu': 32.96138151342419, 'validation/accuracy': 0.6758584380149841, 'validation/loss': 1.5000532865524292, 'validation/bleu': 29.492791567401795, 'validation/num_examples': 3000, 'test/accuracy': 0.6928397417068481, 'test/loss': 1.418620228767395, 'test/bleu': 29.582047222942496, 'test/num_examples': 3003, 'score': 25227.266050815582, 'total_duration': 43052.54297995567, 'accumulated_submission_time': 25227.266050815582, 'accumulated_eval_time': 17820.35981297493, 'accumulated_logging_time': 0.6657192707061768}
I0306 07:11:36.055145 140309904176896 logging_writer.py:48] [71999] accumulated_eval_time=17820.4, accumulated_logging_time=0.665719, accumulated_submission_time=25227.3, global_step=71999, preemption_count=0, score=25227.3, test/accuracy=0.69284, test/bleu=29.582, test/loss=1.41862, test/num_examples=3003, total_duration=43052.5, train/accuracy=0.661918, train/bleu=32.9614, train/loss=1.60078, validation/accuracy=0.675858, validation/bleu=29.4928, validation/loss=1.50005, validation/num_examples=3000
I0306 07:11:36.842705 140309895784192 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.45356202125549316, loss=1.656432867050171
I0306 07:12:11.791718 140309904176896 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.4133138656616211, loss=1.6458375453948975
I0306 07:12:46.742027 140309895784192 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.47610875964164734, loss=1.5419197082519531
I0306 07:13:21.760301 140309904176896 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.4263785183429718, loss=1.560174584388733
I0306 07:13:56.832339 140309895784192 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.45857635140419006, loss=1.6326345205307007
I0306 07:14:31.886730 140309904176896 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.4187898337841034, loss=1.6453404426574707
I0306 07:15:06.916768 140309895784192 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.4911767840385437, loss=1.5531312227249146
I0306 07:15:41.967254 140309904176896 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.42739176750183105, loss=1.5814954042434692
I0306 07:16:17.045936 140309895784192 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.4338657259941101, loss=1.5683727264404297
I0306 07:16:52.050108 140309904176896 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.4237564206123352, loss=1.616089105606079
I0306 07:17:27.079689 140309895784192 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.4287690818309784, loss=1.698110580444336
I0306 07:18:02.137567 140309904176896 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.42788323760032654, loss=1.6110759973526
I0306 07:18:37.173026 140309895784192 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.43970024585723877, loss=1.6261876821517944
I0306 07:19:12.186896 140309904176896 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.4269276559352875, loss=1.574450135231018
I0306 07:19:47.202500 140309895784192 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.40903550386428833, loss=1.6215571165084839
I0306 07:20:22.188562 140309904176896 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.4544191062450409, loss=1.6461200714111328
I0306 07:20:57.184973 140309895784192 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.4378863275051117, loss=1.7466065883636475
I0306 07:21:32.205579 140309904176896 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.43147990107536316, loss=1.5797628164291382
I0306 07:22:07.232985 140309895784192 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.42972317337989807, loss=1.5733834505081177
I0306 07:22:42.272764 140309904176896 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.4725363552570343, loss=1.7040045261383057
I0306 07:23:17.251880 140309895784192 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.5089051723480225, loss=1.6049550771713257
I0306 07:23:52.288658 140309904176896 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.41080188751220703, loss=1.611720085144043
I0306 07:24:27.282601 140309895784192 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.44292837381362915, loss=1.5787361860275269
I0306 07:25:02.297202 140309904176896 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.4672316014766693, loss=1.6248847246170044
I0306 07:25:36.259013 140453607589056 spec.py:321] Evaluating on the training split.
I0306 07:25:38.912546 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:29:17.768306 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 07:29:20.410827 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:32:05.614834 140453607589056 spec.py:349] Evaluating on the test split.
I0306 07:32:08.267980 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:34:58.962284 140453607589056 submission_runner.py:469] Time since start: 44455.47s, 	Step: 74398, 	{'train/accuracy': 0.6618347764015198, 'train/loss': 1.6120871305465698, 'train/bleu': 32.64953626723029, 'validation/accuracy': 0.6782810091972351, 'validation/loss': 1.4903374910354614, 'validation/bleu': 29.466284583754142, 'validation/num_examples': 3000, 'test/accuracy': 0.6940099596977234, 'test/loss': 1.4063055515289307, 'test/bleu': 29.672164920969, 'test/num_examples': 3003, 'score': 26067.250977516174, 'total_duration': 44455.46645307541, 'accumulated_submission_time': 26067.250977516174, 'accumulated_eval_time': 18383.06301856041, 'accumulated_logging_time': 0.7652857303619385}
I0306 07:34:58.981370 140309895784192 logging_writer.py:48] [74398] accumulated_eval_time=18383.1, accumulated_logging_time=0.765286, accumulated_submission_time=26067.3, global_step=74398, preemption_count=0, score=26067.3, test/accuracy=0.69401, test/bleu=29.6722, test/loss=1.40631, test/num_examples=3003, total_duration=44455.5, train/accuracy=0.661835, train/bleu=32.6495, train/loss=1.61209, validation/accuracy=0.678281, validation/bleu=29.4663, validation/loss=1.49034, validation/num_examples=3000
I0306 07:35:00.037848 140309904176896 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.42351600527763367, loss=1.5647199153900146
I0306 07:35:34.941366 140309895784192 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.4315563142299652, loss=1.6144025325775146
I0306 07:36:09.930531 140309904176896 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.4346620440483093, loss=1.6233733892440796
I0306 07:36:44.972910 140309895784192 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.45832329988479614, loss=1.5787513256072998
I0306 07:37:20.049754 140309904176896 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.4581793546676636, loss=1.6504231691360474
I0306 07:37:55.104792 140309895784192 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.4293025732040405, loss=1.635636806488037
I0306 07:38:30.216928 140309904176896 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.39508503675460815, loss=1.573035717010498
I0306 07:39:05.228712 140309895784192 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.40159714221954346, loss=1.569269061088562
I0306 07:39:40.317391 140309904176896 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.4106566607952118, loss=1.611366629600525
I0306 07:40:15.404104 140309895784192 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.42703959345817566, loss=1.6938334703445435
I0306 07:40:50.539274 140309904176896 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.4505006670951843, loss=1.6251779794692993
I0306 07:41:25.636625 140309895784192 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.4244050085544586, loss=1.564015507698059
I0306 07:42:00.753892 140309904176896 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.4388155937194824, loss=1.6019625663757324
I0306 07:42:35.857741 140309895784192 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.4800281226634979, loss=1.643807291984558
I0306 07:43:10.948282 140309904176896 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.44285848736763, loss=1.5784330368041992
I0306 07:43:46.028308 140309895784192 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.41821038722991943, loss=1.5826666355133057
I0306 07:44:21.124106 140309904176896 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.4507398307323456, loss=1.585994005203247
I0306 07:44:56.242680 140309895784192 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.4387678802013397, loss=1.609070062637329
I0306 07:45:31.347851 140309904176896 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.43764060735702515, loss=1.5666712522506714
I0306 07:46:06.460229 140309895784192 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.4547482132911682, loss=1.5923303365707397
I0306 07:46:41.585682 140309904176896 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.4505629241466522, loss=1.5918347835540771
I0306 07:47:16.689623 140309895784192 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.4320969879627228, loss=1.596313238143921
I0306 07:47:51.790744 140309904176896 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.4398975074291229, loss=1.562530755996704
I0306 07:48:26.892351 140309895784192 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.4369979500770569, loss=1.6158146858215332
I0306 07:48:59.199641 140453607589056 spec.py:321] Evaluating on the training split.
I0306 07:49:01.862554 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:52:56.573363 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 07:52:59.209953 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 07:56:51.760157 140453607589056 spec.py:349] Evaluating on the test split.
I0306 07:56:54.404479 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 08:00:28.547595 140453607589056 submission_runner.py:469] Time since start: 45985.05s, 	Step: 76793, 	{'train/accuracy': 0.6704612970352173, 'train/loss': 1.552795648574829, 'train/bleu': 33.30433664980367, 'validation/accuracy': 0.6792697906494141, 'validation/loss': 1.4802823066711426, 'validation/bleu': 29.293655392219645, 'validation/num_examples': 3000, 'test/accuracy': 0.6941837668418884, 'test/loss': 1.4015413522720337, 'test/bleu': 29.374207124294685, 'test/num_examples': 3003, 'score': 26907.325660467148, 'total_duration': 45985.05176591873, 'accumulated_submission_time': 26907.325660467148, 'accumulated_eval_time': 19072.410932064056, 'accumulated_logging_time': 0.7930440902709961}
I0306 08:00:28.566415 140309904176896 logging_writer.py:48] [76793] accumulated_eval_time=19072.4, accumulated_logging_time=0.793044, accumulated_submission_time=26907.3, global_step=76793, preemption_count=0, score=26907.3, test/accuracy=0.694184, test/bleu=29.3742, test/loss=1.40154, test/num_examples=3003, total_duration=45985.1, train/accuracy=0.670461, train/bleu=33.3043, train/loss=1.5528, validation/accuracy=0.67927, validation/bleu=29.2937, validation/loss=1.48028, validation/num_examples=3000
I0306 08:00:31.369884 140309895784192 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.44429704546928406, loss=1.6046644449234009
I0306 08:01:06.350018 140309904176896 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.4456574618816376, loss=1.5848181247711182
I0306 08:01:41.446985 140309895784192 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.4537167549133301, loss=1.5078414678573608
I0306 08:02:16.546959 140309904176896 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.42961981892585754, loss=1.6237821578979492
I0306 08:02:51.672052 140309895784192 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.4232982397079468, loss=1.5363720655441284
I0306 08:03:26.791929 140309904176896 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.4768410325050354, loss=1.6119143962860107
I0306 08:04:01.913397 140309895784192 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.4609946310520172, loss=1.5647547245025635
I0306 08:04:37.010344 140309904176896 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.48631516098976135, loss=1.5903629064559937
I0306 08:05:12.129946 140309895784192 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.43748360872268677, loss=1.6375317573547363
I0306 08:05:47.283873 140309904176896 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.44787830114364624, loss=1.5679547786712646
I0306 08:06:22.411931 140309895784192 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.4432278573513031, loss=1.5714235305786133
I0306 08:06:57.506007 140309904176896 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.42806360125541687, loss=1.5332834720611572
I0306 08:07:32.617326 140309895784192 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.455793559551239, loss=1.629347801208496
I0306 08:08:07.724749 140309904176896 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.460273802280426, loss=1.4741607904434204
I0306 08:08:42.851280 140309895784192 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.4504493772983551, loss=1.6897751092910767
I0306 08:09:17.955691 140309904176896 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.4596087634563446, loss=1.5536468029022217
I0306 08:09:53.061880 140309895784192 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.46859976649284363, loss=1.5695531368255615
I0306 08:10:28.160839 140309904176896 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.44914066791534424, loss=1.5959264039993286
I0306 08:11:03.334469 140309895784192 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.46951818466186523, loss=1.6575660705566406
I0306 08:11:38.461403 140309904176896 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4417928159236908, loss=1.5300458669662476
I0306 08:12:13.616076 140309895784192 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.4641065001487732, loss=1.571437954902649
I0306 08:12:48.720747 140309904176896 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.4799502491950989, loss=1.5679806470870972
I0306 08:13:23.850136 140309895784192 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.4472103416919708, loss=1.6234631538391113
I0306 08:13:58.994432 140309904176896 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.4611649513244629, loss=1.6072124242782593
I0306 08:14:28.859082 140453607589056 spec.py:321] Evaluating on the training split.
I0306 08:14:31.518227 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 08:17:46.141975 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 08:17:48.783519 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 08:20:31.003228 140453607589056 spec.py:349] Evaluating on the test split.
I0306 08:20:33.644942 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 08:23:18.202973 140453607589056 submission_runner.py:469] Time since start: 47354.71s, 	Step: 79186, 	{'train/accuracy': 0.6637898683547974, 'train/loss': 1.596150279045105, 'train/bleu': 32.89055536923566, 'validation/accuracy': 0.6819518804550171, 'validation/loss': 1.4719648361206055, 'validation/bleu': 29.9357743074103, 'validation/num_examples': 3000, 'test/accuracy': 0.6955972909927368, 'test/loss': 1.391950249671936, 'test/bleu': 29.776172154850713, 'test/num_examples': 3003, 'score': 27747.473903894424, 'total_duration': 47354.70715355873, 'accumulated_submission_time': 27747.473903894424, 'accumulated_eval_time': 19601.754769563675, 'accumulated_logging_time': 0.8204529285430908}
I0306 08:23:18.222836 140309895784192 logging_writer.py:48] [79186] accumulated_eval_time=19601.8, accumulated_logging_time=0.820453, accumulated_submission_time=27747.5, global_step=79186, preemption_count=0, score=27747.5, test/accuracy=0.695597, test/bleu=29.7762, test/loss=1.39195, test/num_examples=3003, total_duration=47354.7, train/accuracy=0.66379, train/bleu=32.8906, train/loss=1.59615, validation/accuracy=0.681952, validation/bleu=29.9358, validation/loss=1.47196, validation/num_examples=3000
I0306 08:23:23.480377 140309904176896 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.4603706896305084, loss=1.5898566246032715
I0306 08:23:58.476047 140309895784192 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.44435879588127136, loss=1.490934133529663
I0306 08:24:33.573598 140309904176896 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.4663313925266266, loss=1.5854698419570923
I0306 08:25:08.699487 140309895784192 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.43094882369041443, loss=1.5941967964172363
I0306 08:25:43.850228 140309904176896 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.45719489455223083, loss=1.6559269428253174
I0306 08:26:18.961257 140309895784192 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.46183300018310547, loss=1.632699966430664
I0306 08:26:54.089600 140309904176896 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.4516485929489136, loss=1.5993022918701172
I0306 08:27:29.228028 140309895784192 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.46900713443756104, loss=1.565014362335205
I0306 08:28:04.335867 140309904176896 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.4515600800514221, loss=1.5492677688598633
I0306 08:28:39.470517 140309895784192 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.48784199357032776, loss=1.6411702632904053
I0306 08:29:14.620211 140309904176896 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.44412457942962646, loss=1.5826231241226196
I0306 08:29:49.752879 140309895784192 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.4400559067726135, loss=1.5104680061340332
I0306 08:30:24.893218 140309904176896 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.4562660753726959, loss=1.5843257904052734
I0306 08:31:00.074672 140309895784192 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.4771137833595276, loss=1.596433401107788
I0306 08:31:35.171538 140309904176896 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.4479387402534485, loss=1.5267581939697266
I0306 08:32:10.286091 140309895784192 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.4746007025241852, loss=1.6056960821151733
I0306 08:32:45.379755 140309904176896 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.4584735929965973, loss=1.6717889308929443
I0306 08:33:20.536369 140309895784192 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.5033448934555054, loss=1.5682663917541504
I0306 08:33:55.661644 140309904176896 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.4660026729106903, loss=1.6381481885910034
I0306 08:34:30.787774 140309895784192 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.44378650188446045, loss=1.5527375936508179
I0306 08:35:05.891722 140309904176896 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.47332000732421875, loss=1.592619776725769
I0306 08:35:41.013827 140309895784192 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.46748271584510803, loss=1.6027510166168213
I0306 08:36:16.124028 140309904176896 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.4657467007637024, loss=1.5595207214355469
I0306 08:36:51.252327 140309895784192 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.4775608777999878, loss=1.5808244943618774
I0306 08:37:18.291558 140453607589056 spec.py:321] Evaluating on the training split.
I0306 08:37:20.943047 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 08:41:26.639125 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 08:41:29.281363 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 08:44:09.083104 140453607589056 spec.py:349] Evaluating on the test split.
I0306 08:44:11.721073 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 08:46:46.849170 140453607589056 submission_runner.py:469] Time since start: 48763.35s, 	Step: 81578, 	{'train/accuracy': 0.7066970467567444, 'train/loss': 1.3703205585479736, 'train/bleu': 35.93612648296777, 'validation/accuracy': 0.6836328506469727, 'validation/loss': 1.4616602659225464, 'validation/bleu': 30.14085761271446, 'validation/num_examples': 3000, 'test/accuracy': 0.6964430809020996, 'test/loss': 1.3863131999969482, 'test/bleu': 29.77283749742166, 'test/num_examples': 3003, 'score': 28587.399062395096, 'total_duration': 48763.35332083702, 'accumulated_submission_time': 28587.399062395096, 'accumulated_eval_time': 20170.312299251556, 'accumulated_logging_time': 0.8488447666168213}
I0306 08:46:46.869111 140309904176896 logging_writer.py:48] [81578] accumulated_eval_time=20170.3, accumulated_logging_time=0.848845, accumulated_submission_time=28587.4, global_step=81578, preemption_count=0, score=28587.4, test/accuracy=0.696443, test/bleu=29.7728, test/loss=1.38631, test/num_examples=3003, total_duration=48763.4, train/accuracy=0.706697, train/bleu=35.9361, train/loss=1.37032, validation/accuracy=0.683633, validation/bleu=30.1409, validation/loss=1.46166, validation/num_examples=3000
I0306 08:46:54.930792 140309895784192 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.4613969027996063, loss=1.6265182495117188
I0306 08:47:29.938297 140309904176896 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.46081429719924927, loss=1.5241594314575195
I0306 08:48:05.015664 140309895784192 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.4718622863292694, loss=1.5299845933914185
I0306 08:48:40.147641 140309904176896 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.4533345103263855, loss=1.5582756996154785
I0306 08:49:15.289805 140309895784192 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.45419374108314514, loss=1.5516021251678467
I0306 08:49:50.442770 140309904176896 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.45983582735061646, loss=1.5968502759933472
I0306 08:50:25.565934 140309895784192 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.45148754119873047, loss=1.5161744356155396
I0306 08:51:00.727089 140309904176896 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.497482031583786, loss=1.5519766807556152
I0306 08:51:35.822762 140309895784192 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4535558223724365, loss=1.5385444164276123
I0306 08:52:10.962604 140309904176896 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.4486958086490631, loss=1.5894885063171387
I0306 08:52:46.080643 140309895784192 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.48358091711997986, loss=1.6386783123016357
I0306 08:53:21.234698 140309904176896 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.46720919013023376, loss=1.529678463935852
I0306 08:53:56.358084 140309895784192 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.4845690131187439, loss=1.6574796438217163
I0306 08:54:31.486976 140309904176896 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.48447373509407043, loss=1.5382628440856934
I0306 08:55:06.626703 140309895784192 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.4892466962337494, loss=1.5928432941436768
I0306 08:55:41.751144 140309904176896 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.48748698830604553, loss=1.5161114931106567
I0306 08:56:16.867187 140309895784192 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.4541448950767517, loss=1.5780112743377686
I0306 08:56:51.958271 140309904176896 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.4909859597682953, loss=1.5670254230499268
I0306 08:57:27.039124 140309895784192 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.48678696155548096, loss=1.627029299736023
I0306 08:58:02.118625 140309904176896 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.47203463315963745, loss=1.5140315294265747
I0306 08:58:37.201512 140309895784192 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.5014155507087708, loss=1.655327320098877
I0306 08:59:12.263339 140309904176896 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.4783582389354706, loss=1.5568348169326782
I0306 08:59:47.308861 140309895784192 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.532073438167572, loss=1.542669653892517
I0306 09:00:22.368238 140309904176896 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.47790709137916565, loss=1.5448497533798218
I0306 09:00:46.892780 140453607589056 spec.py:321] Evaluating on the training split.
I0306 09:00:49.544202 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:04:10.256126 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 09:04:12.894916 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:07:06.525635 140453607589056 spec.py:349] Evaluating on the test split.
I0306 09:07:09.163089 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:10:12.505621 140453607589056 submission_runner.py:469] Time since start: 50169.01s, 	Step: 83971, 	{'train/accuracy': 0.6704986691474915, 'train/loss': 1.5535353422164917, 'train/bleu': 33.94902523137799, 'validation/accuracy': 0.6839419007301331, 'validation/loss': 1.4537330865859985, 'validation/bleu': 30.216417981249663, 'validation/num_examples': 3000, 'test/accuracy': 0.6974163055419922, 'test/loss': 1.3776614665985107, 'test/bleu': 29.986562110867396, 'test/num_examples': 3003, 'score': 29427.27654528618, 'total_duration': 50169.009808301926, 'accumulated_submission_time': 29427.27654528618, 'accumulated_eval_time': 20735.925092458725, 'accumulated_logging_time': 0.8774833679199219}
I0306 09:10:12.525151 140309895784192 logging_writer.py:48] [83971] accumulated_eval_time=20735.9, accumulated_logging_time=0.877483, accumulated_submission_time=29427.3, global_step=83971, preemption_count=0, score=29427.3, test/accuracy=0.697416, test/bleu=29.9866, test/loss=1.37766, test/num_examples=3003, total_duration=50169, train/accuracy=0.670499, train/bleu=33.949, train/loss=1.55354, validation/accuracy=0.683942, validation/bleu=30.2164, validation/loss=1.45373, validation/num_examples=3000
I0306 09:10:22.974781 140309904176896 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.5083447694778442, loss=1.5612480640411377
I0306 09:10:57.821529 140309895784192 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.48446714878082275, loss=1.591066837310791
I0306 09:11:32.828377 140309904176896 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.505265474319458, loss=1.5474554300308228
I0306 09:12:07.825729 140309895784192 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.4815726578235626, loss=1.5291165113449097
I0306 09:12:42.825950 140309904176896 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4652930200099945, loss=1.593414306640625
I0306 09:13:17.867125 140309895784192 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.5079922080039978, loss=1.574149250984192
I0306 09:13:52.861658 140309904176896 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.5076108574867249, loss=1.580012321472168
I0306 09:14:27.849753 140309895784192 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.5119491219520569, loss=1.5650302171707153
I0306 09:15:02.856713 140309904176896 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.51425701379776, loss=1.4636151790618896
I0306 09:15:37.881452 140309895784192 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4853702485561371, loss=1.502601981163025
I0306 09:16:12.890659 140309904176896 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.4820496439933777, loss=1.5058029890060425
I0306 09:16:47.879883 140309895784192 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4788435101509094, loss=1.608989953994751
I0306 09:17:22.899646 140309904176896 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.4697875380516052, loss=1.5393495559692383
I0306 09:17:57.894963 140309895784192 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.5159162282943726, loss=1.549082636833191
I0306 09:18:32.886185 140309904176896 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.5267965793609619, loss=1.5102664232254028
I0306 09:19:07.888947 140309895784192 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.5119471549987793, loss=1.561368703842163
I0306 09:19:42.882934 140309904176896 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.4823744595050812, loss=1.540343999862671
I0306 09:20:17.860394 140309895784192 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.46116265654563904, loss=1.4889971017837524
I0306 09:20:52.828636 140309904176896 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.49549877643585205, loss=1.5187880992889404
I0306 09:21:27.836237 140309895784192 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.5040302276611328, loss=1.5895711183547974
I0306 09:22:02.820048 140309904176896 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.5008125901222229, loss=1.6083709001541138
I0306 09:22:37.816547 140309895784192 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.5059815645217896, loss=1.5535351037979126
I0306 09:23:12.837294 140309904176896 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.5000403523445129, loss=1.6374263763427734
I0306 09:23:47.863953 140309895784192 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.4970637559890747, loss=1.5241525173187256
I0306 09:24:12.723773 140453607589056 spec.py:321] Evaluating on the training split.
I0306 09:24:15.376645 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:27:57.335198 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 09:27:59.974959 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:30:31.543269 140453607589056 spec.py:349] Evaluating on the test split.
I0306 09:30:34.179815 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:33:09.124147 140453607589056 submission_runner.py:469] Time since start: 51545.63s, 	Step: 86372, 	{'train/accuracy': 0.6722522377967834, 'train/loss': 1.5501307249069214, 'train/bleu': 33.62538084446093, 'validation/accuracy': 0.6846092939376831, 'validation/loss': 1.450387954711914, 'validation/bleu': 29.954275352387725, 'validation/num_examples': 3000, 'test/accuracy': 0.6993511915206909, 'test/loss': 1.3681271076202393, 'test/bleu': 29.83410800102765, 'test/num_examples': 3003, 'score': 30267.33128809929, 'total_duration': 51545.62833046913, 'accumulated_submission_time': 30267.33128809929, 'accumulated_eval_time': 21272.32541656494, 'accumulated_logging_time': 0.9054334163665771}
I0306 09:33:09.141982 140309904176896 logging_writer.py:48] [86372] accumulated_eval_time=21272.3, accumulated_logging_time=0.905433, accumulated_submission_time=30267.3, global_step=86372, preemption_count=0, score=30267.3, test/accuracy=0.699351, test/bleu=29.8341, test/loss=1.36813, test/num_examples=3003, total_duration=51545.6, train/accuracy=0.672252, train/bleu=33.6254, train/loss=1.55013, validation/accuracy=0.684609, validation/bleu=29.9543, validation/loss=1.45039, validation/num_examples=3000
I0306 09:33:19.273291 140309895784192 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.5167099833488464, loss=1.5541170835494995
I0306 09:33:54.170081 140309904176896 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.517778217792511, loss=1.5436159372329712
I0306 09:34:29.118344 140309895784192 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.5236575603485107, loss=1.6393885612487793
I0306 09:35:04.107198 140309904176896 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.5039480924606323, loss=1.5005524158477783
I0306 09:35:39.087656 140309895784192 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.49926596879959106, loss=1.522279977798462
I0306 09:36:14.094213 140309904176896 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.5285882353782654, loss=1.5446841716766357
I0306 09:36:49.115874 140309895784192 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.5326122045516968, loss=1.562100887298584
I0306 09:37:24.148841 140309904176896 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.5093916654586792, loss=1.563040852546692
I0306 09:37:59.161896 140309895784192 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.5368109941482544, loss=1.5998927354812622
I0306 09:38:34.182054 140309904176896 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.5093328952789307, loss=1.5137008428573608
I0306 09:39:09.215466 140309895784192 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.4904749095439911, loss=1.5001277923583984
I0306 09:39:44.222367 140309904176896 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.5321996808052063, loss=1.5803090333938599
I0306 09:40:19.238404 140309895784192 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.5290353298187256, loss=1.5942288637161255
I0306 09:40:54.261935 140309904176896 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.5181867480278015, loss=1.593785047531128
I0306 09:41:29.283426 140309895784192 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.5234349370002747, loss=1.5432511568069458
I0306 09:42:04.233260 140309904176896 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.49671873450279236, loss=1.4640876054763794
I0306 09:42:39.188575 140309895784192 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.49310392141342163, loss=1.4147471189498901
I0306 09:43:14.113550 140309904176896 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.5154478549957275, loss=1.5015941858291626
I0306 09:43:49.046938 140309895784192 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.5234308242797852, loss=1.6117324829101562
I0306 09:44:23.964683 140309904176896 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.5328779220581055, loss=1.5518137216567993
I0306 09:44:58.900668 140309895784192 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.5825040340423584, loss=1.5879844427108765
I0306 09:45:33.826206 140309904176896 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.5194519758224487, loss=1.5076919794082642
I0306 09:46:08.797646 140309895784192 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.5292759537696838, loss=1.6102864742279053
I0306 09:46:43.727357 140309904176896 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.5347124338150024, loss=1.546722173690796
I0306 09:47:09.256278 140453607589056 spec.py:321] Evaluating on the training split.
I0306 09:47:11.902721 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:50:18.824277 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 09:50:21.457433 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:53:17.604493 140453607589056 spec.py:349] Evaluating on the test split.
I0306 09:53:20.237893 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 09:55:53.337423 140453607589056 submission_runner.py:469] Time since start: 52909.84s, 	Step: 88774, 	{'train/accuracy': 0.6871306300163269, 'train/loss': 1.4589323997497559, 'train/bleu': 34.73349855115667, 'validation/accuracy': 0.6867599487304688, 'validation/loss': 1.4412500858306885, 'validation/bleu': 30.45087973487913, 'validation/num_examples': 3000, 'test/accuracy': 0.7011586427688599, 'test/loss': 1.362294316291809, 'test/bleu': 30.24114246069571, 'test/num_examples': 3003, 'score': 31107.301451921463, 'total_duration': 52909.84158372879, 'accumulated_submission_time': 31107.301451921463, 'accumulated_eval_time': 21796.406494617462, 'accumulated_logging_time': 0.9314451217651367}
I0306 09:55:53.356735 140309895784192 logging_writer.py:48] [88774] accumulated_eval_time=21796.4, accumulated_logging_time=0.931445, accumulated_submission_time=31107.3, global_step=88774, preemption_count=0, score=31107.3, test/accuracy=0.701159, test/bleu=30.2411, test/loss=1.36229, test/num_examples=3003, total_duration=52909.8, train/accuracy=0.687131, train/bleu=34.7335, train/loss=1.45893, validation/accuracy=0.68676, validation/bleu=30.4509, validation/loss=1.44125, validation/num_examples=3000
I0306 09:56:02.766223 140309904176896 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.5371699929237366, loss=1.4940203428268433
I0306 09:56:37.563059 140309895784192 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.5868621468544006, loss=1.591707468032837
I0306 09:57:12.458430 140309904176896 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.5476438403129578, loss=1.5473623275756836
I0306 09:57:47.389303 140309895784192 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.5495614409446716, loss=1.5047117471694946
I0306 09:58:22.375929 140309904176896 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.5333560109138489, loss=1.4734296798706055
I0306 09:58:57.337912 140309895784192 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.5408461093902588, loss=1.5155595541000366
I0306 09:59:32.290239 140309904176896 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.521167516708374, loss=1.5019222497940063
I0306 10:00:07.243552 140309895784192 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.5073668360710144, loss=1.4986367225646973
I0306 10:00:42.191835 140309904176896 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.5556541681289673, loss=1.5893828868865967
I0306 10:01:17.112975 140309895784192 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.5198593139648438, loss=1.4882011413574219
I0306 10:01:52.059670 140309904176896 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.5278880000114441, loss=1.495505690574646
I0306 10:02:27.002414 140309895784192 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.5302597284317017, loss=1.4840987920761108
I0306 10:03:01.963006 140309904176896 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.5489552617073059, loss=1.5522080659866333
I0306 10:03:36.881173 140309895784192 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.5396949052810669, loss=1.5519064664840698
I0306 10:04:11.831811 140309904176896 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.5830960273742676, loss=1.4936274290084839
I0306 10:04:46.789301 140309895784192 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.5429935455322266, loss=1.5141054391860962
I0306 10:05:21.750184 140309904176896 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.5683963894844055, loss=1.4757055044174194
I0306 10:05:56.691832 140309895784192 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.5280348062515259, loss=1.523113489151001
I0306 10:06:31.605282 140309904176896 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.5365918874740601, loss=1.502793550491333
I0306 10:07:06.586075 140309895784192 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.5505505204200745, loss=1.5615307092666626
I0306 10:07:41.549186 140309904176896 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.5407058000564575, loss=1.4847809076309204
I0306 10:08:16.547451 140309895784192 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.5505475997924805, loss=1.5315606594085693
I0306 10:08:51.492824 140309904176896 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.5949516296386719, loss=1.48561429977417
I0306 10:09:26.485957 140309895784192 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.5624127388000488, loss=1.509940505027771
I0306 10:09:53.431455 140453607589056 spec.py:321] Evaluating on the training split.
I0306 10:09:56.080803 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 10:13:33.839050 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 10:13:36.480046 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 10:16:18.844508 140453607589056 spec.py:349] Evaluating on the test split.
I0306 10:16:21.484259 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 10:19:07.428172 140453607589056 submission_runner.py:469] Time since start: 54303.93s, 	Step: 91178, 	{'train/accuracy': 0.6828172206878662, 'train/loss': 1.4830750226974487, 'train/bleu': 34.354220585509985, 'validation/accuracy': 0.6873285174369812, 'validation/loss': 1.4351505041122437, 'validation/bleu': 30.3717926548308, 'validation/num_examples': 3000, 'test/accuracy': 0.7021550536155701, 'test/loss': 1.354550838470459, 'test/bleu': 30.338084926189772, 'test/num_examples': 3003, 'score': 31947.22717356682, 'total_duration': 54303.932356357574, 'accumulated_submission_time': 31947.22717356682, 'accumulated_eval_time': 22350.403165578842, 'accumulated_logging_time': 0.9592108726501465}
I0306 10:19:07.446819 140309904176896 logging_writer.py:48] [91178] accumulated_eval_time=22350.4, accumulated_logging_time=0.959211, accumulated_submission_time=31947.2, global_step=91178, preemption_count=0, score=31947.2, test/accuracy=0.702155, test/bleu=30.3381, test/loss=1.35455, test/num_examples=3003, total_duration=54303.9, train/accuracy=0.682817, train/bleu=34.3542, train/loss=1.48308, validation/accuracy=0.687329, validation/bleu=30.3718, validation/loss=1.43515, validation/num_examples=3000
I0306 10:19:15.470828 140309895784192 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.5318620204925537, loss=1.4858676195144653
I0306 10:19:50.280663 140309904176896 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.5733394622802734, loss=1.503901720046997
I0306 10:20:25.174877 140309895784192 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.5707420110702515, loss=1.5549283027648926
I0306 10:21:00.131482 140309904176896 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.5440419912338257, loss=1.5415066480636597
I0306 10:21:35.079327 140309895784192 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.5513516068458557, loss=1.526953101158142
I0306 10:22:10.020619 140309904176896 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.5765278339385986, loss=1.5138481855392456
I0306 10:22:44.986800 140309895784192 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.5542168617248535, loss=1.4666177034378052
I0306 10:23:19.901667 140309904176896 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.5577682852745056, loss=1.5630801916122437
I0306 10:23:54.857046 140309895784192 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.5448735356330872, loss=1.564926266670227
I0306 10:24:29.780084 140309904176896 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.5683074593544006, loss=1.499564290046692
I0306 10:25:04.704732 140309895784192 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.5846574902534485, loss=1.5576095581054688
I0306 10:25:39.639644 140309904176896 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.5631648898124695, loss=1.51491379737854
I0306 10:26:14.560197 140309895784192 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.5745810866355896, loss=1.5609028339385986
I0306 10:26:49.545903 140309904176896 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.5586390495300293, loss=1.520654320716858
I0306 10:27:24.516658 140309895784192 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.5787516832351685, loss=1.4852571487426758
I0306 10:27:59.475929 140309904176896 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.5762881636619568, loss=1.590099811553955
I0306 10:28:34.422251 140309895784192 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.5758191347122192, loss=1.4707671403884888
I0306 10:29:09.373701 140309904176896 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.5766326189041138, loss=1.4810667037963867
I0306 10:29:44.325537 140309895784192 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.5691336393356323, loss=1.4740437269210815
I0306 10:30:19.252010 140309904176896 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.6013593077659607, loss=1.4963414669036865
I0306 10:30:54.220835 140309895784192 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.5632604360580444, loss=1.4781535863876343
I0306 10:31:29.196007 140309904176896 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.5803589820861816, loss=1.519616961479187
I0306 10:32:04.154426 140309895784192 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.599070131778717, loss=1.540414571762085
I0306 10:32:39.077257 140309904176896 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.5966956615447998, loss=1.4208065271377563
I0306 10:33:07.739459 140453607589056 spec.py:321] Evaluating on the training split.
I0306 10:33:10.387809 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 10:36:59.297003 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 10:37:01.939094 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 10:39:49.239638 140453607589056 spec.py:349] Evaluating on the test split.
I0306 10:39:51.881804 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 10:42:22.483427 140453607589056 submission_runner.py:469] Time since start: 55698.99s, 	Step: 93583, 	{'train/accuracy': 0.6788250803947449, 'train/loss': 1.5084667205810547, 'train/bleu': 34.57574675954446, 'validation/accuracy': 0.6886757612228394, 'validation/loss': 1.4312368631362915, 'validation/bleu': 30.539482233519827, 'validation/num_examples': 3000, 'test/accuracy': 0.7045417428016663, 'test/loss': 1.3455992937088013, 'test/bleu': 30.713214286426222, 'test/num_examples': 3003, 'score': 32787.37330365181, 'total_duration': 55698.98760342598, 'accumulated_submission_time': 32787.37330365181, 'accumulated_eval_time': 22905.147078752518, 'accumulated_logging_time': 0.9863340854644775}
I0306 10:42:22.502925 140309895784192 logging_writer.py:48] [93583] accumulated_eval_time=22905.1, accumulated_logging_time=0.986334, accumulated_submission_time=32787.4, global_step=93583, preemption_count=0, score=32787.4, test/accuracy=0.704542, test/bleu=30.7132, test/loss=1.3456, test/num_examples=3003, total_duration=55699, train/accuracy=0.678825, train/bleu=34.5757, train/loss=1.50847, validation/accuracy=0.688676, validation/bleu=30.5395, validation/loss=1.43124, validation/num_examples=3000
I0306 10:42:28.766624 140309904176896 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.5742653012275696, loss=1.484974980354309
I0306 10:43:03.603097 140309895784192 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.5932260751724243, loss=1.4364469051361084
I0306 10:43:38.462110 140309904176896 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.578698992729187, loss=1.4854702949523926
I0306 10:44:13.356234 140309895784192 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.5935256481170654, loss=1.5344010591506958
I0306 10:44:48.278447 140309904176896 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.6038515567779541, loss=1.5032519102096558
I0306 10:45:23.208421 140309895784192 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.593305230140686, loss=1.5097332000732422
I0306 10:45:58.216984 140309904176896 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.5972709059715271, loss=1.4310121536254883
I0306 10:46:33.198062 140309895784192 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.5907180905342102, loss=1.5063258409500122
I0306 10:47:08.182502 140309904176896 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.6115082502365112, loss=1.486505150794983
I0306 10:47:43.173628 140309895784192 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.5785806179046631, loss=1.492434024810791
I0306 10:48:18.162417 140309904176896 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.5975845456123352, loss=1.4929864406585693
I0306 10:48:53.146430 140309895784192 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.6054604649543762, loss=1.435678243637085
I0306 10:49:28.162677 140309904176896 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.5855399370193481, loss=1.4754846096038818
I0306 10:50:03.157130 140309895784192 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.6349493265151978, loss=1.4149394035339355
I0306 10:50:38.174916 140309904176896 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.5899519324302673, loss=1.411088466644287
I0306 10:51:13.178403 140309895784192 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.5787503719329834, loss=1.5099290609359741
I0306 10:51:48.194088 140309904176896 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.5954076647758484, loss=1.4225759506225586
I0306 10:52:23.192419 140309895784192 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.6043922305107117, loss=1.437425971031189
I0306 10:52:58.216060 140309904176896 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.6236191391944885, loss=1.4366159439086914
I0306 10:53:33.188507 140309895784192 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.639563798904419, loss=1.4636433124542236
I0306 10:54:08.164134 140309904176896 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.598553478717804, loss=1.4825124740600586
I0306 10:54:43.174519 140309895784192 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.6173408627510071, loss=1.447160243988037
I0306 10:55:18.155137 140309904176896 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.5992844104766846, loss=1.4118554592132568
I0306 10:55:53.117827 140309895784192 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.5975660085678101, loss=1.4242966175079346
I0306 10:56:22.499013 140453607589056 spec.py:321] Evaluating on the training split.
I0306 10:56:25.161903 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:00:16.149683 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 11:00:18.789759 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:03:22.281136 140453607589056 spec.py:349] Evaluating on the test split.
I0306 11:03:24.915623 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:06:15.103670 140453607589056 submission_runner.py:469] Time since start: 57131.61s, 	Step: 95985, 	{'train/accuracy': 0.6918825507164001, 'train/loss': 1.4285681247711182, 'train/bleu': 35.008627701949685, 'validation/accuracy': 0.6901466250419617, 'validation/loss': 1.424634337425232, 'validation/bleu': 30.525842301797034, 'validation/num_examples': 3000, 'test/accuracy': 0.7045996785163879, 'test/loss': 1.3450175523757935, 'test/bleu': 30.197763068621246, 'test/num_examples': 3003, 'score': 33627.22583436966, 'total_duration': 57131.60784983635, 'accumulated_submission_time': 33627.22583436966, 'accumulated_eval_time': 23497.751680135727, 'accumulated_logging_time': 1.0143377780914307}
I0306 11:06:15.122630 140309904176896 logging_writer.py:48] [95985] accumulated_eval_time=23497.8, accumulated_logging_time=1.01434, accumulated_submission_time=33627.2, global_step=95985, preemption_count=0, score=33627.2, test/accuracy=0.7046, test/bleu=30.1978, test/loss=1.34502, test/num_examples=3003, total_duration=57131.6, train/accuracy=0.691883, train/bleu=35.0086, train/loss=1.42857, validation/accuracy=0.690147, validation/bleu=30.5258, validation/loss=1.42463, validation/num_examples=3000
I0306 11:06:20.716224 140309895784192 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5978918075561523, loss=1.4949865341186523
I0306 11:06:55.631385 140309904176896 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.6185866594314575, loss=1.436235785484314
I0306 11:07:30.573364 140309895784192 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.5946570634841919, loss=1.5042285919189453
I0306 11:08:05.575419 140309904176896 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.6113095879554749, loss=1.4249919652938843
I0306 11:08:40.604645 140309895784192 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.6328452229499817, loss=1.4470405578613281
I0306 11:09:15.593531 140309904176896 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.6610157489776611, loss=1.4796003103256226
I0306 11:09:50.605827 140309895784192 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.6177150011062622, loss=1.4510538578033447
I0306 11:10:25.616853 140309904176896 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.5914668440818787, loss=1.4744316339492798
I0306 11:11:00.637949 140309895784192 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.6227424144744873, loss=1.4724366664886475
I0306 11:11:35.605984 140309904176896 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.625074028968811, loss=1.4156441688537598
I0306 11:12:10.638055 140309895784192 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.6340588927268982, loss=1.4605274200439453
I0306 11:12:45.621679 140309904176896 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.6051990985870361, loss=1.437067985534668
I0306 11:13:20.641024 140309895784192 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.6313794255256653, loss=1.4309173822402954
I0306 11:13:55.647618 140309904176896 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.6600735783576965, loss=1.4519016742706299
I0306 11:14:30.683533 140309895784192 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.6414823532104492, loss=1.4872304201126099
I0306 11:15:05.687205 140309904176896 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.696382999420166, loss=1.4770185947418213
I0306 11:15:40.676647 140309895784192 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.6322267055511475, loss=1.4664570093154907
I0306 11:16:15.664158 140309904176896 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.6198298931121826, loss=1.440944790840149
I0306 11:16:50.689287 140309895784192 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.6208722591400146, loss=1.3836251497268677
I0306 11:17:25.716167 140309904176896 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.653428316116333, loss=1.4053704738616943
I0306 11:18:00.735965 140309895784192 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.6577253937721252, loss=1.4476603269577026
I0306 11:18:35.717608 140309904176896 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.60764479637146, loss=1.4485969543457031
I0306 11:19:10.712793 140309895784192 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.6617876887321472, loss=1.436751127243042
I0306 11:19:45.683548 140309904176896 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.6262738108634949, loss=1.4907917976379395
I0306 11:20:15.117162 140453607589056 spec.py:321] Evaluating on the training split.
I0306 11:20:17.771965 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:23:20.173587 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 11:23:22.809887 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:26:08.127783 140453607589056 spec.py:349] Evaluating on the test split.
I0306 11:26:10.772188 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:28:54.083918 140453607589056 submission_runner.py:469] Time since start: 58490.59s, 	Step: 98385, 	{'train/accuracy': 0.687724232673645, 'train/loss': 1.4577997922897339, 'train/bleu': 35.33188848121079, 'validation/accuracy': 0.69059157371521, 'validation/loss': 1.419460415840149, 'validation/bleu': 30.736502522204596, 'validation/num_examples': 3000, 'test/accuracy': 0.707160234451294, 'test/loss': 1.3321189880371094, 'test/bleu': 30.61606226581012, 'test/num_examples': 3003, 'score': 34467.07561278343, 'total_duration': 58490.5881023407, 'accumulated_submission_time': 34467.07561278343, 'accumulated_eval_time': 24016.71838617325, 'accumulated_logging_time': 1.041715145111084}
I0306 11:28:54.103495 140309895784192 logging_writer.py:48] [98385] accumulated_eval_time=24016.7, accumulated_logging_time=1.04172, accumulated_submission_time=34467.1, global_step=98385, preemption_count=0, score=34467.1, test/accuracy=0.70716, test/bleu=30.6161, test/loss=1.33212, test/num_examples=3003, total_duration=58490.6, train/accuracy=0.687724, train/bleu=35.3319, train/loss=1.4578, validation/accuracy=0.690592, validation/bleu=30.7365, validation/loss=1.41946, validation/num_examples=3000
I0306 11:28:59.674905 140309904176896 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.6632023453712463, loss=1.5402847528457642
I0306 11:29:34.597892 140309895784192 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.6449688076972961, loss=1.4626312255859375
I0306 11:30:09.531312 140309904176896 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.6594032645225525, loss=1.4466215372085571
I0306 11:30:44.524044 140309895784192 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.6520929932594299, loss=1.4535311460494995
I0306 11:31:19.489590 140309904176896 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.6553928256034851, loss=1.420854926109314
I0306 11:31:54.476613 140309895784192 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.6817262768745422, loss=1.446337342262268
I0306 11:32:29.472609 140309904176896 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.6557333469390869, loss=1.4178111553192139
I0306 11:33:04.478764 140309895784192 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.6310693621635437, loss=1.4423342943191528
I0306 11:33:39.482414 140309904176896 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.6523917317390442, loss=1.4031991958618164
I0306 11:34:14.482748 140309895784192 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.6649998426437378, loss=1.51986825466156
I0306 11:34:49.490033 140309904176896 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.6365066766738892, loss=1.4717090129852295
I0306 11:35:24.466670 140309895784192 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.6374120116233826, loss=1.4475566148757935
I0306 11:35:59.447869 140309904176896 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.662045955657959, loss=1.442514419555664
I0306 11:36:34.441103 140309895784192 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.6500205993652344, loss=1.3639605045318604
I0306 11:37:09.428395 140309904176896 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.6729076504707336, loss=1.3970040082931519
I0306 11:37:44.423380 140309895784192 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.6661151051521301, loss=1.488458275794983
I0306 11:38:19.403826 140309904176896 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.6762652397155762, loss=1.4200403690338135
I0306 11:38:54.412379 140309895784192 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.6657963991165161, loss=1.4244153499603271
I0306 11:39:29.404861 140309904176896 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.687384843826294, loss=1.4494727849960327
I0306 11:40:04.401534 140309895784192 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.6975477337837219, loss=1.43584406375885
I0306 11:40:39.411068 140309904176896 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.6787967085838318, loss=1.4276490211486816
I0306 11:41:14.407401 140309895784192 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.6565836668014526, loss=1.4580997228622437
I0306 11:41:49.429284 140309904176896 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.6892632246017456, loss=1.439725399017334
I0306 11:42:24.452092 140309895784192 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.7016992568969727, loss=1.4202688932418823
I0306 11:42:54.219033 140453607589056 spec.py:321] Evaluating on the training split.
I0306 11:42:56.877956 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:47:13.450248 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 11:47:16.089683 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:50:18.072129 140453607589056 spec.py:349] Evaluating on the test split.
I0306 11:50:20.714293 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 11:53:20.351321 140453607589056 submission_runner.py:469] Time since start: 59956.86s, 	Step: 100786, 	{'train/accuracy': 0.7063007950782776, 'train/loss': 1.356135368347168, 'train/bleu': 36.11381278913436, 'validation/accuracy': 0.6916669011116028, 'validation/loss': 1.4140279293060303, 'validation/bleu': 30.633346457648827, 'validation/num_examples': 3000, 'test/accuracy': 0.7068821787834167, 'test/loss': 1.3313542604446411, 'test/bleu': 30.571785878005297, 'test/num_examples': 3003, 'score': 35307.050988435745, 'total_duration': 59956.855487823486, 'accumulated_submission_time': 35307.050988435745, 'accumulated_eval_time': 24642.850607156754, 'accumulated_logging_time': 1.069793939590454}
I0306 11:53:20.370737 140309904176896 logging_writer.py:48] [100786] accumulated_eval_time=24642.9, accumulated_logging_time=1.06979, accumulated_submission_time=35307.1, global_step=100786, preemption_count=0, score=35307.1, test/accuracy=0.706882, test/bleu=30.5718, test/loss=1.33135, test/num_examples=3003, total_duration=59956.9, train/accuracy=0.706301, train/bleu=36.1138, train/loss=1.35614, validation/accuracy=0.691667, validation/bleu=30.6333, validation/loss=1.41403, validation/num_examples=3000
I0306 11:53:25.601599 140309895784192 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.6636435389518738, loss=1.3697960376739502
I0306 11:54:00.496515 140309904176896 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.6482723355293274, loss=1.4494543075561523
I0306 11:54:35.457462 140309895784192 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.6598599553108215, loss=1.4152510166168213
I0306 11:55:10.442202 140309904176896 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.6545007824897766, loss=1.4005206823349
I0306 11:55:45.444848 140309895784192 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.6557070016860962, loss=1.472586989402771
I0306 11:56:20.417941 140309904176896 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.6679991483688354, loss=1.4674946069717407
I0306 11:56:55.404029 140309895784192 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.6990323662757874, loss=1.4754610061645508
I0306 11:57:30.359771 140309904176896 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.6862707734107971, loss=1.3891879320144653
I0306 11:58:05.345704 140309895784192 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.687896728515625, loss=1.5199002027511597
I0306 11:58:40.360781 140309904176896 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.662551999092102, loss=1.4088886976242065
I0306 11:59:15.348878 140309895784192 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.6953490376472473, loss=1.3227660655975342
I0306 11:59:50.366159 140309904176896 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.6821950078010559, loss=1.3935043811798096
I0306 12:00:25.354534 140309895784192 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.6826059222221375, loss=1.4588303565979004
I0306 12:01:00.399467 140309904176896 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.7004433870315552, loss=1.3732848167419434
I0306 12:01:35.402185 140309895784192 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.693006694316864, loss=1.3789550065994263
I0306 12:02:10.396747 140309904176896 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.6804875135421753, loss=1.427554965019226
I0306 12:02:45.390877 140309895784192 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.7143653631210327, loss=1.4577232599258423
I0306 12:03:20.388243 140309904176896 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.6950223445892334, loss=1.3997085094451904
I0306 12:03:55.404042 140309895784192 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.6933341026306152, loss=1.46003258228302
I0306 12:04:30.393552 140309904176896 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.686416506767273, loss=1.523844599723816
I0306 12:05:05.381313 140309895784192 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.7112377285957336, loss=1.4547761678695679
I0306 12:05:40.366856 140309904176896 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.7180590629577637, loss=1.4675731658935547
I0306 12:06:15.363604 140309895784192 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.69832444190979, loss=1.4841623306274414
I0306 12:06:50.356888 140309904176896 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.7327619791030884, loss=1.5439550876617432
I0306 12:07:20.451475 140453607589056 spec.py:321] Evaluating on the training split.
I0306 12:07:23.102471 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 12:11:42.259454 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 12:11:44.904871 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 12:14:47.502147 140453607589056 spec.py:349] Evaluating on the test split.
I0306 12:14:50.148427 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 12:18:00.798316 140453607589056 submission_runner.py:469] Time since start: 61437.30s, 	Step: 103187, 	{'train/accuracy': 0.6974995732307434, 'train/loss': 1.396683931350708, 'train/bleu': 35.45279725086385, 'validation/accuracy': 0.692692756652832, 'validation/loss': 1.4095714092254639, 'validation/bleu': 29.942548449907246, 'validation/num_examples': 3000, 'test/accuracy': 0.7077974677085876, 'test/loss': 1.325623631477356, 'test/bleu': 30.71331438052963, 'test/num_examples': 3003, 'score': 36146.98702502251, 'total_duration': 61437.302500486374, 'accumulated_submission_time': 36146.98702502251, 'accumulated_eval_time': 25283.197397232056, 'accumulated_logging_time': 1.0978243350982666}
I0306 12:18:00.818312 140309895784192 logging_writer.py:48] [103187] accumulated_eval_time=25283.2, accumulated_logging_time=1.09782, accumulated_submission_time=36147, global_step=103187, preemption_count=0, score=36147, test/accuracy=0.707797, test/bleu=30.7133, test/loss=1.32562, test/num_examples=3003, total_duration=61437.3, train/accuracy=0.6975, train/bleu=35.4528, train/loss=1.39668, validation/accuracy=0.692693, validation/bleu=29.9425, validation/loss=1.40957, validation/num_examples=3000
I0306 12:18:05.717939 140309904176896 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.6955626010894775, loss=1.4553027153015137
I0306 12:18:40.602390 140309895784192 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.703879177570343, loss=1.3971786499023438
I0306 12:19:15.586941 140309904176896 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.6967232823371887, loss=1.3986738920211792
I0306 12:19:50.590586 140309895784192 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.7031570672988892, loss=1.479712963104248
I0306 12:20:25.602545 140309904176896 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.7408649921417236, loss=1.4625755548477173
I0306 12:21:00.616664 140309895784192 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.7229158878326416, loss=1.4390666484832764
I0306 12:21:35.626596 140309904176896 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.7349981665611267, loss=1.418303370475769
I0306 12:22:10.618470 140309895784192 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.7283508777618408, loss=1.4847228527069092
I0306 12:22:45.612908 140309904176896 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.7340421080589294, loss=1.485228419303894
I0306 12:23:20.592256 140309895784192 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.7450032234191895, loss=1.4372745752334595
I0306 12:23:55.609712 140309904176896 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.7474822998046875, loss=1.4560800790786743
I0306 12:24:30.611955 140309895784192 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.7719364166259766, loss=1.4554955959320068
I0306 12:25:05.645910 140309904176896 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.6867803335189819, loss=1.4107402563095093
I0306 12:25:40.634015 140309895784192 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.7428687810897827, loss=1.4298481941223145
I0306 12:26:15.639575 140309904176896 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.7422313690185547, loss=1.4017446041107178
I0306 12:26:50.638534 140309895784192 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.7320898771286011, loss=1.402807354927063
I0306 12:27:25.641545 140309904176896 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.7487123012542725, loss=1.4425557851791382
I0306 12:28:00.640633 140309895784192 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.7269664406776428, loss=1.3732435703277588
I0306 12:28:35.644199 140309904176896 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.7303585410118103, loss=1.4214919805526733
I0306 12:29:10.673012 140309895784192 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.7413005232810974, loss=1.432760238647461
I0306 12:29:45.712608 140309904176896 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.7618105411529541, loss=1.4099513292312622
I0306 12:30:20.735114 140309895784192 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.7348107695579529, loss=1.3800626993179321
I0306 12:30:55.729553 140309904176896 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.7131360769271851, loss=1.423695683479309
I0306 12:31:30.725568 140309895784192 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.7300337553024292, loss=1.42951238155365
I0306 12:32:00.804998 140453607589056 spec.py:321] Evaluating on the training split.
I0306 12:32:03.454755 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 12:36:11.547962 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 12:36:14.184734 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 12:39:12.118796 140453607589056 spec.py:349] Evaluating on the test split.
I0306 12:39:14.759474 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 12:42:00.625647 140453607589056 submission_runner.py:469] Time since start: 62877.13s, 	Step: 105587, 	{'train/accuracy': 0.6921233534812927, 'train/loss': 1.4350160360336304, 'train/bleu': 35.7836417457067, 'validation/accuracy': 0.6933972835540771, 'validation/loss': 1.4069628715515137, 'validation/bleu': 30.83084041200125, 'validation/num_examples': 3000, 'test/accuracy': 0.708318829536438, 'test/loss': 1.3211647272109985, 'test/bleu': 30.810718286258798, 'test/num_examples': 3003, 'score': 36986.824026823044, 'total_duration': 62877.129821777344, 'accumulated_submission_time': 36986.824026823044, 'accumulated_eval_time': 25883.017988443375, 'accumulated_logging_time': 1.1272363662719727}
I0306 12:42:00.646698 140309904176896 logging_writer.py:48] [105587] accumulated_eval_time=25883, accumulated_logging_time=1.12724, accumulated_submission_time=36986.8, global_step=105587, preemption_count=0, score=36986.8, test/accuracy=0.708319, test/bleu=30.8107, test/loss=1.32116, test/num_examples=3003, total_duration=62877.1, train/accuracy=0.692123, train/bleu=35.7836, train/loss=1.43502, validation/accuracy=0.693397, validation/bleu=30.8308, validation/loss=1.40696, validation/num_examples=3000
I0306 12:42:05.531429 140309895784192 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.7367140054702759, loss=1.4347232580184937
I0306 12:42:40.396649 140309904176896 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.7524440288543701, loss=1.3351242542266846
I0306 12:43:15.354808 140309895784192 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.7339112162590027, loss=1.4122352600097656
I0306 12:43:50.349278 140309904176896 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.7513682246208191, loss=1.4011160135269165
I0306 12:44:25.381089 140309895784192 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.7648587226867676, loss=1.4825809001922607
I0306 12:45:00.381047 140309904176896 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.7385591864585876, loss=1.4585212469100952
I0306 12:45:35.418971 140309895784192 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.7243767976760864, loss=1.3738048076629639
I0306 12:46:10.449063 140309904176896 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.7328976392745972, loss=1.4451240301132202
I0306 12:46:45.487763 140309895784192 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.7213666439056396, loss=1.3780878782272339
I0306 12:47:20.524757 140309904176896 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.7396688461303711, loss=1.429739236831665
I0306 12:47:55.573742 140309895784192 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.7318254113197327, loss=1.3558706045150757
I0306 12:48:30.588979 140309904176896 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.7667940855026245, loss=1.376225233078003
I0306 12:49:05.607378 140309895784192 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.7791972160339355, loss=1.467728853225708
I0306 12:49:40.619677 140309904176896 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.756054162979126, loss=1.427827000617981
I0306 12:50:15.622877 140309895784192 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.7694599032402039, loss=1.3379193544387817
I0306 12:50:50.640028 140309904176896 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.7543722987174988, loss=1.3335751295089722
I0306 12:51:25.628527 140309895784192 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.772917628288269, loss=1.4321759939193726
I0306 12:52:00.628186 140309904176896 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.7929235696792603, loss=1.413185477256775
I0306 12:52:35.687125 140309895784192 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.762556791305542, loss=1.417602300643921
I0306 12:53:10.692371 140309904176896 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.770906925201416, loss=1.4550786018371582
I0306 12:53:45.684259 140309895784192 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.732795238494873, loss=1.4039065837860107
I0306 12:54:20.685218 140309904176896 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.7454525828361511, loss=1.3769184350967407
I0306 12:54:55.668635 140309895784192 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.7673905491828918, loss=1.4059407711029053
I0306 12:55:30.634660 140309904176896 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.7622703909873962, loss=1.4267079830169678
I0306 12:56:00.753808 140453607589056 spec.py:321] Evaluating on the training split.
I0306 12:56:03.405296 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 12:59:37.704814 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 12:59:40.353297 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:02:27.705239 140453607589056 spec.py:349] Evaluating on the test split.
I0306 13:02:30.348973 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:05:20.086642 140453607589056 submission_runner.py:469] Time since start: 64276.59s, 	Step: 107987, 	{'train/accuracy': 0.7081751823425293, 'train/loss': 1.3414620161056519, 'train/bleu': 36.02370807410332, 'validation/accuracy': 0.6940276622772217, 'validation/loss': 1.4024419784545898, 'validation/bleu': 30.688457008178812, 'validation/num_examples': 3000, 'test/accuracy': 0.7093963623046875, 'test/loss': 1.3161160945892334, 'test/bleu': 30.76632557151929, 'test/num_examples': 3003, 'score': 37826.78018832207, 'total_duration': 64276.590811252594, 'accumulated_submission_time': 37826.78018832207, 'accumulated_eval_time': 26442.350756406784, 'accumulated_logging_time': 1.156816005706787}
I0306 13:05:20.107424 140309895784192 logging_writer.py:48] [107987] accumulated_eval_time=26442.4, accumulated_logging_time=1.15682, accumulated_submission_time=37826.8, global_step=107987, preemption_count=0, score=37826.8, test/accuracy=0.709396, test/bleu=30.7663, test/loss=1.31612, test/num_examples=3003, total_duration=64276.6, train/accuracy=0.708175, train/bleu=36.0237, train/loss=1.34146, validation/accuracy=0.694028, validation/bleu=30.6885, validation/loss=1.40244, validation/num_examples=3000
I0306 13:05:24.996201 140309904176896 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.7214635014533997, loss=1.3466975688934326
I0306 13:05:59.883643 140309895784192 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.7959176301956177, loss=1.3243387937545776
I0306 13:06:34.829522 140309904176896 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.7792236804962158, loss=1.4280472993850708
I0306 13:07:09.834252 140309895784192 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.7680069208145142, loss=1.3525372743606567
I0306 13:07:44.808550 140309904176896 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.7716957926750183, loss=1.4666215181350708
I0306 13:08:19.796052 140309895784192 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.7758544683456421, loss=1.3675271272659302
I0306 13:08:54.780109 140309904176896 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.7450937032699585, loss=1.382010817527771
I0306 13:09:29.764141 140309895784192 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.7717284560203552, loss=1.5045002698898315
I0306 13:10:04.750214 140309904176896 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.7675719261169434, loss=1.3736048936843872
I0306 13:10:39.749721 140309895784192 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.7538645267486572, loss=1.4299449920654297
I0306 13:11:14.761144 140309904176896 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.7762669920921326, loss=1.4072777032852173
I0306 13:11:49.735381 140309895784192 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.7793282270431519, loss=1.3951802253723145
I0306 13:12:24.717036 140309904176896 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.784130334854126, loss=1.403941035270691
I0306 13:12:59.747455 140309895784192 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.7914627194404602, loss=1.4553098678588867
I0306 13:13:34.742927 140309904176896 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.7655019760131836, loss=1.415051817893982
I0306 13:14:09.724776 140309895784192 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.7658871412277222, loss=1.3508021831512451
I0306 13:14:44.750979 140309904176896 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.785714328289032, loss=1.331234097480774
I0306 13:15:19.729424 140309895784192 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.7861663699150085, loss=1.3299541473388672
I0306 13:15:54.697782 140309904176896 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.7975479364395142, loss=1.4065957069396973
I0306 13:16:29.686342 140309895784192 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.7741981148719788, loss=1.3708455562591553
I0306 13:17:04.665479 140309904176896 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.7963882088661194, loss=1.419404149055481
I0306 13:17:39.679810 140309895784192 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.7804471254348755, loss=1.4432116746902466
I0306 13:18:14.690516 140309904176896 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.7950028777122498, loss=1.3491145372390747
I0306 13:18:49.673621 140309895784192 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.7774683237075806, loss=1.3125393390655518
I0306 13:19:20.114769 140453607589056 spec.py:321] Evaluating on the training split.
I0306 13:19:22.771897 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:22:57.718350 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 13:23:00.364719 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:25:47.967365 140453607589056 spec.py:349] Evaluating on the test split.
I0306 13:25:50.614953 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:28:35.553669 140453607589056 submission_runner.py:469] Time since start: 65672.06s, 	Step: 110388, 	{'train/accuracy': 0.7045966982841492, 'train/loss': 1.362382411956787, 'train/bleu': 36.60497898867686, 'validation/accuracy': 0.6940523982048035, 'validation/loss': 1.403794765472412, 'validation/bleu': 30.830082859122726, 'validation/num_examples': 3000, 'test/accuracy': 0.7102305889129639, 'test/loss': 1.3172800540924072, 'test/bleu': 30.76776339724198, 'test/num_examples': 3003, 'score': 38666.64380669594, 'total_duration': 65672.05785250664, 'accumulated_submission_time': 38666.64380669594, 'accumulated_eval_time': 26997.789608955383, 'accumulated_logging_time': 1.186286449432373}
I0306 13:28:35.575157 140309904176896 logging_writer.py:48] [110388] accumulated_eval_time=26997.8, accumulated_logging_time=1.18629, accumulated_submission_time=38666.6, global_step=110388, preemption_count=0, score=38666.6, test/accuracy=0.710231, test/bleu=30.7678, test/loss=1.31728, test/num_examples=3003, total_duration=65672.1, train/accuracy=0.704597, train/bleu=36.605, train/loss=1.36238, validation/accuracy=0.694052, validation/bleu=30.8301, validation/loss=1.40379, validation/num_examples=3000
I0306 13:28:40.114490 140309895784192 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.7958611845970154, loss=1.3788498640060425
I0306 13:29:15.016349 140309904176896 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.8015305995941162, loss=1.4049566984176636
I0306 13:29:49.983991 140309895784192 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.7874795794487, loss=1.378970980644226
I0306 13:30:24.960325 140309904176896 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.766014575958252, loss=1.3736608028411865
I0306 13:30:59.990702 140309895784192 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.7750542759895325, loss=1.3395055532455444
I0306 13:31:34.983861 140309904176896 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.7885285019874573, loss=1.374118685722351
I0306 13:32:09.982813 140309895784192 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.7998654246330261, loss=1.3413293361663818
I0306 13:32:44.985570 140309904176896 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.7853355407714844, loss=1.4308898448944092
I0306 13:33:20.018029 140309895784192 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.7992229461669922, loss=1.3187932968139648
I0306 13:33:55.024950 140309904176896 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.7927642464637756, loss=1.3742595911026
I0306 13:34:30.070485 140309895784192 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.7844237685203552, loss=1.4389574527740479
I0306 13:35:05.091173 140309904176896 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.8039097189903259, loss=1.4529772996902466
I0306 13:35:40.120057 140309895784192 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.7783232927322388, loss=1.3644287586212158
I0306 13:36:15.143701 140309904176896 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.7964834570884705, loss=1.3203896284103394
I0306 13:36:50.170912 140309895784192 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.771686851978302, loss=1.4241071939468384
I0306 13:37:25.178313 140309904176896 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.8106324672698975, loss=1.4096721410751343
I0306 13:38:00.210593 140309895784192 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.8417339324951172, loss=1.4100971221923828
I0306 13:38:35.244552 140309904176896 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.7883358597755432, loss=1.3370546102523804
I0306 13:39:10.262335 140309895784192 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.8068541288375854, loss=1.436417818069458
I0306 13:39:45.281323 140309904176896 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.7925667762756348, loss=1.4131042957305908
I0306 13:40:20.265211 140309895784192 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.8002132177352905, loss=1.4009740352630615
I0306 13:40:55.307748 140309904176896 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.7708690762519836, loss=1.3466345071792603
I0306 13:41:30.304181 140309895784192 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.7940388321876526, loss=1.3868215084075928
I0306 13:42:05.329816 140309904176896 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.7938940525054932, loss=1.3840053081512451
I0306 13:42:35.762627 140453607589056 spec.py:321] Evaluating on the training split.
I0306 13:42:38.416228 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:46:03.231042 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 13:46:05.877724 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:48:50.666588 140453607589056 spec.py:349] Evaluating on the test split.
I0306 13:48:53.308071 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 13:51:28.337172 140453607589056 submission_runner.py:469] Time since start: 67044.84s, 	Step: 112788, 	{'train/accuracy': 0.7045066356658936, 'train/loss': 1.3604251146316528, 'train/bleu': 36.32073826341658, 'validation/accuracy': 0.6939535140991211, 'validation/loss': 1.3998583555221558, 'validation/bleu': 30.82202233521854, 'validation/num_examples': 3000, 'test/accuracy': 0.7100452184677124, 'test/loss': 1.314450740814209, 'test/bleu': 30.73716246455837, 'test/num_examples': 3003, 'score': 39506.68699860573, 'total_duration': 67044.84134840965, 'accumulated_submission_time': 39506.68699860573, 'accumulated_eval_time': 27530.364114522934, 'accumulated_logging_time': 1.216001272201538}
I0306 13:51:28.358093 140309895784192 logging_writer.py:48] [112788] accumulated_eval_time=27530.4, accumulated_logging_time=1.216, accumulated_submission_time=39506.7, global_step=112788, preemption_count=0, score=39506.7, test/accuracy=0.710045, test/bleu=30.7372, test/loss=1.31445, test/num_examples=3003, total_duration=67044.8, train/accuracy=0.704507, train/bleu=36.3207, train/loss=1.36043, validation/accuracy=0.693954, validation/bleu=30.822, validation/loss=1.39986, validation/num_examples=3000
I0306 13:51:32.892580 140309904176896 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.7954565286636353, loss=1.4364749193191528
I0306 13:52:07.773861 140309895784192 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.7553279399871826, loss=1.3063640594482422
I0306 13:52:42.656986 140309904176896 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.7835716605186462, loss=1.3796782493591309
I0306 13:53:17.622588 140309895784192 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.8042114973068237, loss=1.374182939529419
I0306 13:53:52.557111 140309904176896 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.7946878671646118, loss=1.420251727104187
I0306 13:54:27.523347 140309895784192 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.7821061015129089, loss=1.3597731590270996
I0306 13:55:02.504615 140309904176896 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.8117151260375977, loss=1.411159873008728
I0306 13:55:37.490948 140309895784192 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.7929512858390808, loss=1.4195854663848877
I0306 13:56:12.477961 140309904176896 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.7973098754882812, loss=1.3502588272094727
I0306 13:56:47.460633 140309895784192 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.7861545085906982, loss=1.3914624452590942
I0306 13:57:22.466991 140309904176896 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.8048352599143982, loss=1.3498483896255493
I0306 13:57:57.450668 140309895784192 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.7706055045127869, loss=1.3428303003311157
I0306 13:58:32.434589 140309904176896 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.8124637007713318, loss=1.3978265523910522
I0306 13:59:07.439156 140309895784192 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.8031951785087585, loss=1.380299687385559
I0306 13:59:42.432865 140309904176896 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.7977536916732788, loss=1.3587496280670166
I0306 14:00:17.453953 140309895784192 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.7799822688102722, loss=1.3989613056182861
I0306 14:00:52.438065 140309904176896 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.7749497294425964, loss=1.4030845165252686
I0306 14:01:27.452530 140309895784192 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.7719728946685791, loss=1.3479701280593872
I0306 14:02:02.443620 140309904176896 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.811161458492279, loss=1.3294051885604858
I0306 14:02:37.441699 140309895784192 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.8179014921188354, loss=1.3690619468688965
I0306 14:03:12.465732 140309904176896 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.8118534088134766, loss=1.3423594236373901
I0306 14:03:47.476297 140309895784192 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.7945650815963745, loss=1.399176001548767
I0306 14:04:22.486824 140309904176896 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.789131760597229, loss=1.3592028617858887
I0306 14:04:57.487824 140309895784192 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.7995474934577942, loss=1.3423590660095215
I0306 14:05:28.683334 140453607589056 spec.py:321] Evaluating on the training split.
I0306 14:05:31.343907 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 14:09:08.644094 140453607589056 spec.py:333] Evaluating on the validation split.
I0306 14:09:11.282517 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 14:11:54.631454 140453607589056 spec.py:349] Evaluating on the test split.
I0306 14:11:57.278025 140453607589056 workload.py:181] Translating evaluation dataset.
I0306 14:14:47.997478 140453607589056 submission_runner.py:469] Time since start: 68444.50s, 	Step: 115190, 	{'train/accuracy': 0.709965705871582, 'train/loss': 1.3361488580703735, 'train/bleu': 37.11630284453078, 'validation/accuracy': 0.6947816014289856, 'validation/loss': 1.3989111185073853, 'validation/bleu': 31.006010709498003, 'validation/num_examples': 3000, 'test/accuracy': 0.7106360793113708, 'test/loss': 1.3129425048828125, 'test/bleu': 30.715523117831665, 'test/num_examples': 3003, 'score': 40346.86899280548, 'total_duration': 68444.50165987015, 'accumulated_submission_time': 40346.86899280548, 'accumulated_eval_time': 28089.678209543228, 'accumulated_logging_time': 1.2451858520507812}
I0306 14:14:48.018946 140309904176896 logging_writer.py:48] [115190] accumulated_eval_time=28089.7, accumulated_logging_time=1.24519, accumulated_submission_time=40346.9, global_step=115190, preemption_count=0, score=40346.9, test/accuracy=0.710636, test/bleu=30.7155, test/loss=1.31294, test/num_examples=3003, total_duration=68444.5, train/accuracy=0.709966, train/bleu=37.1163, train/loss=1.33615, validation/accuracy=0.694782, validation/bleu=31.006, validation/loss=1.39891, validation/num_examples=3000
I0306 14:14:48.040795 140309895784192 logging_writer.py:48] [115190] global_step=115190, preemption_count=0, score=40346.9
I0306 14:14:48.069915 140453607589056 submission_runner.py:646] Tuning trial 3/5
I0306 14:14:48.070049 140453607589056 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0306 14:14:48.071907 140453607589056 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006720660021528602, 'train/loss': 11.23320198059082, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.248138427734375, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.245794296264648, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.302452564239502, 'total_duration': 948.991500377655, 'accumulated_submission_time': 26.302452564239502, 'accumulated_eval_time': 922.688939332962, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2398, {'train/accuracy': 0.41951772570610046, 'train/loss': 3.8381333351135254, 'train/bleu': 14.816741339202395, 'validation/accuracy': 0.4016018807888031, 'validation/loss': 3.978447675704956, 'validation/bleu': 9.499882996677812, 'validation/num_examples': 3000, 'test/accuracy': 0.388761430978775, 'test/loss': 4.154230117797852, 'test/bleu': 8.37801651737534, 'test/num_examples': 3003, 'score': 866.1766827106476, 'total_duration': 2544.7133824825287, 'accumulated_submission_time': 866.1766827106476, 'accumulated_eval_time': 1678.352516412735, 'accumulated_logging_time': 0.01751089096069336, 'global_step': 2398, 'preemption_count': 0}), (4794, {'train/accuracy': 0.5435895323753357, 'train/loss': 2.6274607181549072, 'train/bleu': 24.86587119835876, 'validation/accuracy': 0.5490569472312927, 'validation/loss': 2.574619770050049, 'validation/bleu': 20.6708920708382, 'validation/num_examples': 3000, 'test/accuracy': 0.5522998571395874, 'test/loss': 2.5946102142333984, 'test/bleu': 19.44563204619634, 'test/num_examples': 3003, 'score': 1706.3324828147888, 'total_duration': 3882.05451130867, 'accumulated_submission_time': 1706.3324828147888, 'accumulated_eval_time': 2175.3549778461456, 'accumulated_logging_time': 0.03632807731628418, 'global_step': 4794, 'preemption_count': 0}), (7191, {'train/accuracy': 0.5866031646728516, 'train/loss': 2.2181344032287598, 'train/bleu': 27.59965013514339, 'validation/accuracy': 0.5883988738059998, 'validation/loss': 2.186305284500122, 'validation/bleu': 23.447596896426234, 'validation/num_examples': 3000, 'test/accuracy': 0.5920171737670898, 'test/loss': 2.1745574474334717, 'test/bleu': 22.014903552176293, 'test/num_examples': 3003, 'score': 2546.4510893821716, 'total_duration': 5205.323271512985, 'accumulated_submission_time': 2546.4510893821716, 'accumulated_eval_time': 2658.325782060623, 'accumulated_logging_time': 0.05561041831970215, 'global_step': 7191, 'preemption_count': 0}), (9593, {'train/accuracy': 0.593389093875885, 'train/loss': 2.132319450378418, 'train/bleu': 28.316917201431533, 'validation/accuracy': 0.6066175699234009, 'validation/loss': 2.012321710586548, 'validation/bleu': 24.807958752701865, 'validation/num_examples': 3000, 'test/accuracy': 0.6139381527900696, 'test/loss': 1.9790500402450562, 'test/bleu': 23.720571622766467, 'test/num_examples': 3003, 'score': 3386.5949883461, 'total_duration': 6522.138231515884, 'accumulated_submission_time': 3386.5949883461, 'accumulated_eval_time': 3134.823011159897, 'accumulated_logging_time': 0.07428812980651855, 'global_step': 9593, 'preemption_count': 0}), (11997, {'train/accuracy': 0.6024340987205505, 'train/loss': 2.0452821254730225, 'train/bleu': 26.884426360051467, 'validation/accuracy': 0.6214372515678406, 'validation/loss': 1.8897901773452759, 'validation/bleu': 20.389571788168055, 'validation/num_examples': 3000, 'test/accuracy': 0.628409206867218, 'test/loss': 1.8486205339431763, 'test/bleu': 22.288411767668194, 'test/num_examples': 3003, 'score': 4226.5865046978, 'total_duration': 8248.369979143143, 'accumulated_submission_time': 4226.5865046978, 'accumulated_eval_time': 4020.8960151672363, 'accumulated_logging_time': 0.09292125701904297, 'global_step': 11997, 'preemption_count': 0}), (14398, {'train/accuracy': 0.6180242896080017, 'train/loss': 1.9136333465576172, 'train/bleu': 29.784373826342563, 'validation/accuracy': 0.6335129737854004, 'validation/loss': 1.7956960201263428, 'validation/bleu': 26.62909590109143, 'validation/num_examples': 3000, 'test/accuracy': 0.6435291171073914, 'test/loss': 1.7406089305877686, 'test/bleu': 25.452898851722505, 'test/num_examples': 3003, 'score': 5066.742300987244, 'total_duration': 9551.995981693268, 'accumulated_submission_time': 5066.742300987244, 'accumulated_eval_time': 4484.199281215668, 'accumulated_logging_time': 0.1129755973815918, 'global_step': 14398, 'preemption_count': 0}), (16797, {'train/accuracy': 0.6221718788146973, 'train/loss': 1.8844703435897827, 'train/bleu': 30.573863066780767, 'validation/accuracy': 0.6428942084312439, 'validation/loss': 1.7411386966705322, 'validation/bleu': 27.16599045469385, 'validation/num_examples': 3000, 'test/accuracy': 0.6534005403518677, 'test/loss': 1.6806440353393555, 'test/bleu': 26.50965433659917, 'test/num_examples': 3003, 'score': 5906.7158052921295, 'total_duration': 10859.751459360123, 'accumulated_submission_time': 5906.7158052921295, 'accumulated_eval_time': 4951.822494506836, 'accumulated_logging_time': 0.13192129135131836, 'global_step': 16797, 'preemption_count': 0}), (19196, {'train/accuracy': 0.640593409538269, 'train/loss': 1.7329225540161133, 'train/bleu': 31.25223549350145, 'validation/accuracy': 0.6465528011322021, 'validation/loss': 1.7002032995224, 'validation/bleu': 27.56459137683218, 'validation/num_examples': 3000, 'test/accuracy': 0.6564245223999023, 'test/loss': 1.6423343420028687, 'test/bleu': 26.7979640584824, 'test/num_examples': 3003, 'score': 6746.866229534149, 'total_duration': 12269.222694158554, 'accumulated_submission_time': 6746.866229534149, 'accumulated_eval_time': 5520.978021621704, 'accumulated_logging_time': 0.15406394004821777, 'global_step': 19196, 'preemption_count': 0}), (21589, {'train/accuracy': 0.6294189691543579, 'train/loss': 1.8221043348312378, 'train/bleu': 30.46595577377374, 'validation/accuracy': 0.649642825126648, 'validation/loss': 1.678046703338623, 'validation/bleu': 27.51618627604794, 'validation/num_examples': 3000, 'test/accuracy': 0.6586606502532959, 'test/loss': 1.6265623569488525, 'test/bleu': 26.903761554144584, 'test/num_examples': 3003, 'score': 7587.050140380859, 'total_duration': 13645.822926521301, 'accumulated_submission_time': 7587.050140380859, 'accumulated_eval_time': 6057.236371517181, 'accumulated_logging_time': 0.1755530834197998, 'global_step': 21589, 'preemption_count': 0}), (23981, {'train/accuracy': 0.630372941493988, 'train/loss': 1.822124719619751, 'train/bleu': 30.490199505040238, 'validation/accuracy': 0.6516945958137512, 'validation/loss': 1.6691845655441284, 'validation/bleu': 26.422340490767564, 'validation/num_examples': 3000, 'test/accuracy': 0.6602131724357605, 'test/loss': 1.613050937652588, 'test/bleu': 26.81878446308568, 'test/num_examples': 3003, 'score': 8427.116342067719, 'total_duration': 15272.073198795319, 'accumulated_submission_time': 8427.116342067719, 'accumulated_eval_time': 6843.261160135269, 'accumulated_logging_time': 0.19788551330566406, 'global_step': 23981, 'preemption_count': 0}), (26377, {'train/accuracy': 0.6365044713020325, 'train/loss': 1.7646968364715576, 'train/bleu': 30.66817966084646, 'validation/accuracy': 0.6517687439918518, 'validation/loss': 1.6645671129226685, 'validation/bleu': 27.56674809415129, 'validation/num_examples': 3000, 'test/accuracy': 0.6604217290878296, 'test/loss': 1.6079171895980835, 'test/bleu': 26.92458613124644, 'test/num_examples': 3003, 'score': 9267.204899787903, 'total_duration': 16578.23140025139, 'accumulated_submission_time': 9267.204899787903, 'accumulated_eval_time': 7309.168332099915, 'accumulated_logging_time': 0.22160744667053223, 'global_step': 26377, 'preemption_count': 0}), (28775, {'train/accuracy': 0.6340263485908508, 'train/loss': 1.7928507328033447, 'train/bleu': 30.98873610480173, 'validation/accuracy': 0.6557610034942627, 'validation/loss': 1.6400070190429688, 'validation/bleu': 28.33064394454149, 'validation/num_examples': 3000, 'test/accuracy': 0.666238009929657, 'test/loss': 1.5779070854187012, 'test/bleu': 27.534685645164636, 'test/num_examples': 3003, 'score': 10107.142692804337, 'total_duration': 17967.226855278015, 'accumulated_submission_time': 10107.142692804337, 'accumulated_eval_time': 7858.0650470256805, 'accumulated_logging_time': 0.2455613613128662, 'global_step': 28775, 'preemption_count': 0}), (31173, {'train/accuracy': 0.6328679323196411, 'train/loss': 1.8054627180099487, 'train/bleu': 30.898889922194606, 'validation/accuracy': 0.6550564765930176, 'validation/loss': 1.633868932723999, 'validation/bleu': 27.880600655196098, 'validation/num_examples': 3000, 'test/accuracy': 0.6678716540336609, 'test/loss': 1.5667964220046997, 'test/bleu': 27.789889315452434, 'test/num_examples': 3003, 'score': 10947.077741384506, 'total_duration': 19360.996530532837, 'accumulated_submission_time': 10947.077741384506, 'accumulated_eval_time': 8411.739597558975, 'accumulated_logging_time': 0.26898884773254395, 'global_step': 31173, 'preemption_count': 0}), (33573, {'train/accuracy': 0.6378339529037476, 'train/loss': 1.766714096069336, 'train/bleu': 31.572037289892087, 'validation/accuracy': 0.6578498482704163, 'validation/loss': 1.6158266067504883, 'validation/bleu': 27.72155938962761, 'validation/num_examples': 3000, 'test/accuracy': 0.668624758720398, 'test/loss': 1.5609530210494995, 'test/bleu': 27.80415417962983, 'test/num_examples': 3003, 'score': 11787.156024217606, 'total_duration': 20869.433529615402, 'accumulated_submission_time': 11787.156024217606, 'accumulated_eval_time': 9079.942214250565, 'accumulated_logging_time': 0.290189266204834, 'global_step': 33573, 'preemption_count': 0}), (35972, {'train/accuracy': 0.6389380693435669, 'train/loss': 1.7561196088790894, 'train/bleu': 30.975432167725724, 'validation/accuracy': 0.6593825221061707, 'validation/loss': 1.614693522453308, 'validation/bleu': 28.40411547366733, 'validation/num_examples': 3000, 'test/accuracy': 0.6711852550506592, 'test/loss': 1.5476024150848389, 'test/bleu': 27.86889381072178, 'test/num_examples': 3003, 'score': 12627.011134147644, 'total_duration': 22222.522897958755, 'accumulated_submission_time': 12627.011134147644, 'accumulated_eval_time': 9593.014070510864, 'accumulated_logging_time': 0.3120300769805908, 'global_step': 35972, 'preemption_count': 0}), (38373, {'train/accuracy': 0.6514740586280823, 'train/loss': 1.6701644659042358, 'train/bleu': 32.08197458359871, 'validation/accuracy': 0.6620893478393555, 'validation/loss': 1.596665859222412, 'validation/bleu': 28.55504342496179, 'validation/num_examples': 3000, 'test/accuracy': 0.6723902225494385, 'test/loss': 1.5324037075042725, 'test/bleu': 28.03651735656702, 'test/num_examples': 3003, 'score': 13467.057776212692, 'total_duration': 23581.614058494568, 'accumulated_submission_time': 13467.057776212692, 'accumulated_eval_time': 10111.897889614105, 'accumulated_logging_time': 0.33316993713378906, 'global_step': 38373, 'preemption_count': 0}), (40773, {'train/accuracy': 0.6390610337257385, 'train/loss': 1.7670201063156128, 'train/bleu': 31.365826836187477, 'validation/accuracy': 0.6603589653968811, 'validation/loss': 1.5961488485336304, 'validation/bleu': 28.32218735784994, 'validation/num_examples': 3000, 'test/accuracy': 0.6730738282203674, 'test/loss': 1.5327777862548828, 'test/bleu': 28.060622916606548, 'test/num_examples': 3003, 'score': 14306.990105628967, 'total_duration': 25013.034703731537, 'accumulated_submission_time': 14306.990105628967, 'accumulated_eval_time': 10703.224868535995, 'accumulated_logging_time': 0.35448122024536133, 'global_step': 40773, 'preemption_count': 0}), (43174, {'train/accuracy': 0.640091061592102, 'train/loss': 1.7539657354354858, 'train/bleu': 31.510433459357863, 'validation/accuracy': 0.6599386930465698, 'validation/loss': 1.5912686586380005, 'validation/bleu': 28.09843418045749, 'validation/num_examples': 3000, 'test/accuracy': 0.6729695200920105, 'test/loss': 1.5272576808929443, 'test/bleu': 27.99592158423135, 'test/num_examples': 3003, 'score': 15146.903519630432, 'total_duration': 26343.5042219162, 'accumulated_submission_time': 15146.903519630432, 'accumulated_eval_time': 11193.621772766113, 'accumulated_logging_time': 0.37674784660339355, 'global_step': 43174, 'preemption_count': 0}), (45574, {'train/accuracy': 0.6470431685447693, 'train/loss': 1.6979851722717285, 'train/bleu': 31.634742612244903, 'validation/accuracy': 0.6632017493247986, 'validation/loss': 1.5809577703475952, 'validation/bleu': 28.69499530268323, 'validation/num_examples': 3000, 'test/accuracy': 0.6755532622337341, 'test/loss': 1.5120937824249268, 'test/bleu': 28.246344834546786, 'test/num_examples': 3003, 'score': 15986.774324178696, 'total_duration': 27670.272636175156, 'accumulated_submission_time': 15986.774324178696, 'accumulated_eval_time': 11680.36050772667, 'accumulated_logging_time': 0.398212194442749, 'global_step': 45574, 'preemption_count': 0}), (47974, {'train/accuracy': 0.6438049077987671, 'train/loss': 1.7272613048553467, 'train/bleu': 31.116255338257915, 'validation/accuracy': 0.6634860634803772, 'validation/loss': 1.5731979608535767, 'validation/bleu': 28.738017957091454, 'validation/num_examples': 3000, 'test/accuracy': 0.6743598580360413, 'test/loss': 1.5088788270950317, 'test/bleu': 28.028513780507694, 'test/num_examples': 3003, 'score': 16826.6128783226, 'total_duration': 29020.819280147552, 'accumulated_submission_time': 16826.6128783226, 'accumulated_eval_time': 12190.90675854683, 'accumulated_logging_time': 0.424515962600708, 'global_step': 47974, 'preemption_count': 0}), (50376, {'train/accuracy': 0.674156904220581, 'train/loss': 1.5261074304580688, 'train/bleu': 33.58847072105214, 'validation/accuracy': 0.6658962368965149, 'validation/loss': 1.5646727085113525, 'validation/bleu': 28.56486944309737, 'validation/num_examples': 3000, 'test/accuracy': 0.6754373908042908, 'test/loss': 1.5047029256820679, 'test/bleu': 28.272951000066193, 'test/num_examples': 3003, 'score': 17666.74790906906, 'total_duration': 30366.03059363365, 'accumulated_submission_time': 17666.74790906906, 'accumulated_eval_time': 12695.823257684708, 'accumulated_logging_time': 0.44875073432922363, 'global_step': 50376, 'preemption_count': 0}), (52781, {'train/accuracy': 0.6486982703208923, 'train/loss': 1.6904340982437134, 'train/bleu': 32.15732694962338, 'validation/accuracy': 0.6673176288604736, 'validation/loss': 1.5558617115020752, 'validation/bleu': 28.77861264876485, 'validation/num_examples': 3000, 'test/accuracy': 0.6812883615493774, 'test/loss': 1.4824622869491577, 'test/bleu': 28.723405265269353, 'test/num_examples': 3003, 'score': 18506.898864746094, 'total_duration': 31784.287172317505, 'accumulated_submission_time': 18506.898864746094, 'accumulated_eval_time': 13273.771958827972, 'accumulated_logging_time': 0.4718286991119385, 'global_step': 52781, 'preemption_count': 0}), (55184, {'train/accuracy': 0.6492453217506409, 'train/loss': 1.69197416305542, 'train/bleu': 31.692787530130385, 'validation/accuracy': 0.667243480682373, 'validation/loss': 1.5565459728240967, 'validation/bleu': 28.621042173289222, 'validation/num_examples': 3000, 'test/accuracy': 0.6807669997215271, 'test/loss': 1.4774938821792603, 'test/bleu': 28.647201093077218, 'test/num_examples': 3003, 'score': 19346.921707868576, 'total_duration': 33149.87847661972, 'accumulated_submission_time': 19346.921707868576, 'accumulated_eval_time': 13799.180753707886, 'accumulated_logging_time': 0.49449825286865234, 'global_step': 55184, 'preemption_count': 0}), (57587, {'train/accuracy': 0.6559727191925049, 'train/loss': 1.6356278657913208, 'train/bleu': 31.691430250379028, 'validation/accuracy': 0.6697031259536743, 'validation/loss': 1.5434856414794922, 'validation/bleu': 29.144534450246965, 'validation/num_examples': 3000, 'test/accuracy': 0.6802108883857727, 'test/loss': 1.4796209335327148, 'test/bleu': 28.332969685306747, 'test/num_examples': 3003, 'score': 20187.067399024963, 'total_duration': 34489.783871889114, 'accumulated_submission_time': 20187.067399024963, 'accumulated_eval_time': 14298.7785115242, 'accumulated_logging_time': 0.5188720226287842, 'global_step': 57587, 'preemption_count': 0}), (59988, {'train/accuracy': 0.6522138714790344, 'train/loss': 1.6799737215042114, 'train/bleu': 31.940333164314833, 'validation/accuracy': 0.6708031892776489, 'validation/loss': 1.5347214937210083, 'validation/bleu': 28.853402565455895, 'validation/num_examples': 3000, 'test/accuracy': 0.6851581335067749, 'test/loss': 1.4620137214660645, 'test/bleu': 28.706303126128148, 'test/num_examples': 3003, 'score': 21027.170575380325, 'total_duration': 35844.34740138054, 'accumulated_submission_time': 21027.170575380325, 'accumulated_eval_time': 14813.074025392532, 'accumulated_logging_time': 0.5440225601196289, 'global_step': 59988, 'preemption_count': 0}), (62390, {'train/accuracy': 0.651102602481842, 'train/loss': 1.6824616193771362, 'train/bleu': 32.226547921763384, 'validation/accuracy': 0.6733740568161011, 'validation/loss': 1.5258394479751587, 'validation/bleu': 29.09713585123688, 'validation/num_examples': 3000, 'test/accuracy': 0.6856447458267212, 'test/loss': 1.4494532346725464, 'test/bleu': 29.06448380937648, 'test/num_examples': 3003, 'score': 21867.291534662247, 'total_duration': 37206.734256505966, 'accumulated_submission_time': 21867.291534662247, 'accumulated_eval_time': 15335.17637681961, 'accumulated_logging_time': 0.5673058032989502, 'global_step': 62390, 'preemption_count': 0}), (64792, {'train/accuracy': 0.6555148363113403, 'train/loss': 1.6478312015533447, 'train/bleu': 32.878248224010875, 'validation/accuracy': 0.6724717617034912, 'validation/loss': 1.5215963125228882, 'validation/bleu': 29.098214727323537, 'validation/num_examples': 3000, 'test/accuracy': 0.6864789724349976, 'test/loss': 1.4471114873886108, 'test/bleu': 28.87790404441971, 'test/num_examples': 3003, 'score': 22707.34805560112, 'total_duration': 38612.21073055267, 'accumulated_submission_time': 22707.34805560112, 'accumulated_eval_time': 15900.436671257019, 'accumulated_logging_time': 0.5921285152435303, 'global_step': 64792, 'preemption_count': 0}), (67193, {'train/accuracy': 0.6565093994140625, 'train/loss': 1.6455990076065063, 'train/bleu': 32.42100988563706, 'validation/accuracy': 0.673954963684082, 'validation/loss': 1.5144321918487549, 'validation/bleu': 29.21163010832368, 'validation/num_examples': 3000, 'test/accuracy': 0.6876491904258728, 'test/loss': 1.4376064538955688, 'test/bleu': 28.975636482633906, 'test/num_examples': 3003, 'score': 23547.51508998871, 'total_duration': 40077.84293174744, 'accumulated_submission_time': 23547.51508998871, 'accumulated_eval_time': 16525.73765206337, 'accumulated_logging_time': 0.6165614128112793, 'global_step': 67193, 'preemption_count': 0}), (69595, {'train/accuracy': 0.6725636124610901, 'train/loss': 1.5473986864089966, 'train/bleu': 33.509762381245224, 'validation/accuracy': 0.6759201884269714, 'validation/loss': 1.5030101537704468, 'validation/bleu': 29.617065789973818, 'validation/num_examples': 3000, 'test/accuracy': 0.6882632374763489, 'test/loss': 1.4309974908828735, 'test/bleu': 28.949633304848806, 'test/num_examples': 3003, 'score': 24387.395591020584, 'total_duration': 41583.17433524132, 'accumulated_submission_time': 24387.395591020584, 'accumulated_eval_time': 17191.026163101196, 'accumulated_logging_time': 0.6411175727844238, 'global_step': 69595, 'preemption_count': 0}), (71999, {'train/accuracy': 0.6619178056716919, 'train/loss': 1.60077965259552, 'train/bleu': 32.96138151342419, 'validation/accuracy': 0.6758584380149841, 'validation/loss': 1.5000532865524292, 'validation/bleu': 29.492791567401795, 'validation/num_examples': 3000, 'test/accuracy': 0.6928397417068481, 'test/loss': 1.418620228767395, 'test/bleu': 29.582047222942496, 'test/num_examples': 3003, 'score': 25227.266050815582, 'total_duration': 43052.54297995567, 'accumulated_submission_time': 25227.266050815582, 'accumulated_eval_time': 17820.35981297493, 'accumulated_logging_time': 0.6657192707061768, 'global_step': 71999, 'preemption_count': 0}), (74398, {'train/accuracy': 0.6618347764015198, 'train/loss': 1.6120871305465698, 'train/bleu': 32.64953626723029, 'validation/accuracy': 0.6782810091972351, 'validation/loss': 1.4903374910354614, 'validation/bleu': 29.466284583754142, 'validation/num_examples': 3000, 'test/accuracy': 0.6940099596977234, 'test/loss': 1.4063055515289307, 'test/bleu': 29.672164920969, 'test/num_examples': 3003, 'score': 26067.250977516174, 'total_duration': 44455.46645307541, 'accumulated_submission_time': 26067.250977516174, 'accumulated_eval_time': 18383.06301856041, 'accumulated_logging_time': 0.7652857303619385, 'global_step': 74398, 'preemption_count': 0}), (76793, {'train/accuracy': 0.6704612970352173, 'train/loss': 1.552795648574829, 'train/bleu': 33.30433664980367, 'validation/accuracy': 0.6792697906494141, 'validation/loss': 1.4802823066711426, 'validation/bleu': 29.293655392219645, 'validation/num_examples': 3000, 'test/accuracy': 0.6941837668418884, 'test/loss': 1.4015413522720337, 'test/bleu': 29.374207124294685, 'test/num_examples': 3003, 'score': 26907.325660467148, 'total_duration': 45985.05176591873, 'accumulated_submission_time': 26907.325660467148, 'accumulated_eval_time': 19072.410932064056, 'accumulated_logging_time': 0.7930440902709961, 'global_step': 76793, 'preemption_count': 0}), (79186, {'train/accuracy': 0.6637898683547974, 'train/loss': 1.596150279045105, 'train/bleu': 32.89055536923566, 'validation/accuracy': 0.6819518804550171, 'validation/loss': 1.4719648361206055, 'validation/bleu': 29.9357743074103, 'validation/num_examples': 3000, 'test/accuracy': 0.6955972909927368, 'test/loss': 1.391950249671936, 'test/bleu': 29.776172154850713, 'test/num_examples': 3003, 'score': 27747.473903894424, 'total_duration': 47354.70715355873, 'accumulated_submission_time': 27747.473903894424, 'accumulated_eval_time': 19601.754769563675, 'accumulated_logging_time': 0.8204529285430908, 'global_step': 79186, 'preemption_count': 0}), (81578, {'train/accuracy': 0.7066970467567444, 'train/loss': 1.3703205585479736, 'train/bleu': 35.93612648296777, 'validation/accuracy': 0.6836328506469727, 'validation/loss': 1.4616602659225464, 'validation/bleu': 30.14085761271446, 'validation/num_examples': 3000, 'test/accuracy': 0.6964430809020996, 'test/loss': 1.3863131999969482, 'test/bleu': 29.77283749742166, 'test/num_examples': 3003, 'score': 28587.399062395096, 'total_duration': 48763.35332083702, 'accumulated_submission_time': 28587.399062395096, 'accumulated_eval_time': 20170.312299251556, 'accumulated_logging_time': 0.8488447666168213, 'global_step': 81578, 'preemption_count': 0}), (83971, {'train/accuracy': 0.6704986691474915, 'train/loss': 1.5535353422164917, 'train/bleu': 33.94902523137799, 'validation/accuracy': 0.6839419007301331, 'validation/loss': 1.4537330865859985, 'validation/bleu': 30.216417981249663, 'validation/num_examples': 3000, 'test/accuracy': 0.6974163055419922, 'test/loss': 1.3776614665985107, 'test/bleu': 29.986562110867396, 'test/num_examples': 3003, 'score': 29427.27654528618, 'total_duration': 50169.009808301926, 'accumulated_submission_time': 29427.27654528618, 'accumulated_eval_time': 20735.925092458725, 'accumulated_logging_time': 0.8774833679199219, 'global_step': 83971, 'preemption_count': 0}), (86372, {'train/accuracy': 0.6722522377967834, 'train/loss': 1.5501307249069214, 'train/bleu': 33.62538084446093, 'validation/accuracy': 0.6846092939376831, 'validation/loss': 1.450387954711914, 'validation/bleu': 29.954275352387725, 'validation/num_examples': 3000, 'test/accuracy': 0.6993511915206909, 'test/loss': 1.3681271076202393, 'test/bleu': 29.83410800102765, 'test/num_examples': 3003, 'score': 30267.33128809929, 'total_duration': 51545.62833046913, 'accumulated_submission_time': 30267.33128809929, 'accumulated_eval_time': 21272.32541656494, 'accumulated_logging_time': 0.9054334163665771, 'global_step': 86372, 'preemption_count': 0}), (88774, {'train/accuracy': 0.6871306300163269, 'train/loss': 1.4589323997497559, 'train/bleu': 34.73349855115667, 'validation/accuracy': 0.6867599487304688, 'validation/loss': 1.4412500858306885, 'validation/bleu': 30.45087973487913, 'validation/num_examples': 3000, 'test/accuracy': 0.7011586427688599, 'test/loss': 1.362294316291809, 'test/bleu': 30.24114246069571, 'test/num_examples': 3003, 'score': 31107.301451921463, 'total_duration': 52909.84158372879, 'accumulated_submission_time': 31107.301451921463, 'accumulated_eval_time': 21796.406494617462, 'accumulated_logging_time': 0.9314451217651367, 'global_step': 88774, 'preemption_count': 0}), (91178, {'train/accuracy': 0.6828172206878662, 'train/loss': 1.4830750226974487, 'train/bleu': 34.354220585509985, 'validation/accuracy': 0.6873285174369812, 'validation/loss': 1.4351505041122437, 'validation/bleu': 30.3717926548308, 'validation/num_examples': 3000, 'test/accuracy': 0.7021550536155701, 'test/loss': 1.354550838470459, 'test/bleu': 30.338084926189772, 'test/num_examples': 3003, 'score': 31947.22717356682, 'total_duration': 54303.932356357574, 'accumulated_submission_time': 31947.22717356682, 'accumulated_eval_time': 22350.403165578842, 'accumulated_logging_time': 0.9592108726501465, 'global_step': 91178, 'preemption_count': 0}), (93583, {'train/accuracy': 0.6788250803947449, 'train/loss': 1.5084667205810547, 'train/bleu': 34.57574675954446, 'validation/accuracy': 0.6886757612228394, 'validation/loss': 1.4312368631362915, 'validation/bleu': 30.539482233519827, 'validation/num_examples': 3000, 'test/accuracy': 0.7045417428016663, 'test/loss': 1.3455992937088013, 'test/bleu': 30.713214286426222, 'test/num_examples': 3003, 'score': 32787.37330365181, 'total_duration': 55698.98760342598, 'accumulated_submission_time': 32787.37330365181, 'accumulated_eval_time': 22905.147078752518, 'accumulated_logging_time': 0.9863340854644775, 'global_step': 93583, 'preemption_count': 0}), (95985, {'train/accuracy': 0.6918825507164001, 'train/loss': 1.4285681247711182, 'train/bleu': 35.008627701949685, 'validation/accuracy': 0.6901466250419617, 'validation/loss': 1.424634337425232, 'validation/bleu': 30.525842301797034, 'validation/num_examples': 3000, 'test/accuracy': 0.7045996785163879, 'test/loss': 1.3450175523757935, 'test/bleu': 30.197763068621246, 'test/num_examples': 3003, 'score': 33627.22583436966, 'total_duration': 57131.60784983635, 'accumulated_submission_time': 33627.22583436966, 'accumulated_eval_time': 23497.751680135727, 'accumulated_logging_time': 1.0143377780914307, 'global_step': 95985, 'preemption_count': 0}), (98385, {'train/accuracy': 0.687724232673645, 'train/loss': 1.4577997922897339, 'train/bleu': 35.33188848121079, 'validation/accuracy': 0.69059157371521, 'validation/loss': 1.419460415840149, 'validation/bleu': 30.736502522204596, 'validation/num_examples': 3000, 'test/accuracy': 0.707160234451294, 'test/loss': 1.3321189880371094, 'test/bleu': 30.61606226581012, 'test/num_examples': 3003, 'score': 34467.07561278343, 'total_duration': 58490.5881023407, 'accumulated_submission_time': 34467.07561278343, 'accumulated_eval_time': 24016.71838617325, 'accumulated_logging_time': 1.041715145111084, 'global_step': 98385, 'preemption_count': 0}), (100786, {'train/accuracy': 0.7063007950782776, 'train/loss': 1.356135368347168, 'train/bleu': 36.11381278913436, 'validation/accuracy': 0.6916669011116028, 'validation/loss': 1.4140279293060303, 'validation/bleu': 30.633346457648827, 'validation/num_examples': 3000, 'test/accuracy': 0.7068821787834167, 'test/loss': 1.3313542604446411, 'test/bleu': 30.571785878005297, 'test/num_examples': 3003, 'score': 35307.050988435745, 'total_duration': 59956.855487823486, 'accumulated_submission_time': 35307.050988435745, 'accumulated_eval_time': 24642.850607156754, 'accumulated_logging_time': 1.069793939590454, 'global_step': 100786, 'preemption_count': 0}), (103187, {'train/accuracy': 0.6974995732307434, 'train/loss': 1.396683931350708, 'train/bleu': 35.45279725086385, 'validation/accuracy': 0.692692756652832, 'validation/loss': 1.4095714092254639, 'validation/bleu': 29.942548449907246, 'validation/num_examples': 3000, 'test/accuracy': 0.7077974677085876, 'test/loss': 1.325623631477356, 'test/bleu': 30.71331438052963, 'test/num_examples': 3003, 'score': 36146.98702502251, 'total_duration': 61437.302500486374, 'accumulated_submission_time': 36146.98702502251, 'accumulated_eval_time': 25283.197397232056, 'accumulated_logging_time': 1.0978243350982666, 'global_step': 103187, 'preemption_count': 0}), (105587, {'train/accuracy': 0.6921233534812927, 'train/loss': 1.4350160360336304, 'train/bleu': 35.7836417457067, 'validation/accuracy': 0.6933972835540771, 'validation/loss': 1.4069628715515137, 'validation/bleu': 30.83084041200125, 'validation/num_examples': 3000, 'test/accuracy': 0.708318829536438, 'test/loss': 1.3211647272109985, 'test/bleu': 30.810718286258798, 'test/num_examples': 3003, 'score': 36986.824026823044, 'total_duration': 62877.129821777344, 'accumulated_submission_time': 36986.824026823044, 'accumulated_eval_time': 25883.017988443375, 'accumulated_logging_time': 1.1272363662719727, 'global_step': 105587, 'preemption_count': 0}), (107987, {'train/accuracy': 0.7081751823425293, 'train/loss': 1.3414620161056519, 'train/bleu': 36.02370807410332, 'validation/accuracy': 0.6940276622772217, 'validation/loss': 1.4024419784545898, 'validation/bleu': 30.688457008178812, 'validation/num_examples': 3000, 'test/accuracy': 0.7093963623046875, 'test/loss': 1.3161160945892334, 'test/bleu': 30.76632557151929, 'test/num_examples': 3003, 'score': 37826.78018832207, 'total_duration': 64276.590811252594, 'accumulated_submission_time': 37826.78018832207, 'accumulated_eval_time': 26442.350756406784, 'accumulated_logging_time': 1.156816005706787, 'global_step': 107987, 'preemption_count': 0}), (110388, {'train/accuracy': 0.7045966982841492, 'train/loss': 1.362382411956787, 'train/bleu': 36.60497898867686, 'validation/accuracy': 0.6940523982048035, 'validation/loss': 1.403794765472412, 'validation/bleu': 30.830082859122726, 'validation/num_examples': 3000, 'test/accuracy': 0.7102305889129639, 'test/loss': 1.3172800540924072, 'test/bleu': 30.76776339724198, 'test/num_examples': 3003, 'score': 38666.64380669594, 'total_duration': 65672.05785250664, 'accumulated_submission_time': 38666.64380669594, 'accumulated_eval_time': 26997.789608955383, 'accumulated_logging_time': 1.186286449432373, 'global_step': 110388, 'preemption_count': 0}), (112788, {'train/accuracy': 0.7045066356658936, 'train/loss': 1.3604251146316528, 'train/bleu': 36.32073826341658, 'validation/accuracy': 0.6939535140991211, 'validation/loss': 1.3998583555221558, 'validation/bleu': 30.82202233521854, 'validation/num_examples': 3000, 'test/accuracy': 0.7100452184677124, 'test/loss': 1.314450740814209, 'test/bleu': 30.73716246455837, 'test/num_examples': 3003, 'score': 39506.68699860573, 'total_duration': 67044.84134840965, 'accumulated_submission_time': 39506.68699860573, 'accumulated_eval_time': 27530.364114522934, 'accumulated_logging_time': 1.216001272201538, 'global_step': 112788, 'preemption_count': 0}), (115190, {'train/accuracy': 0.709965705871582, 'train/loss': 1.3361488580703735, 'train/bleu': 37.11630284453078, 'validation/accuracy': 0.6947816014289856, 'validation/loss': 1.3989111185073853, 'validation/bleu': 31.006010709498003, 'validation/num_examples': 3000, 'test/accuracy': 0.7106360793113708, 'test/loss': 1.3129425048828125, 'test/bleu': 30.715523117831665, 'test/num_examples': 3003, 'score': 40346.86899280548, 'total_duration': 68444.50165987015, 'accumulated_submission_time': 40346.86899280548, 'accumulated_eval_time': 28089.678209543228, 'accumulated_logging_time': 1.2451858520507812, 'global_step': 115190, 'preemption_count': 0})], 'global_step': 115190}
I0306 14:14:48.072026 140453607589056 submission_runner.py:649] Timing: 40346.86899280548
I0306 14:14:48.072062 140453607589056 submission_runner.py:651] Total number of evals: 49
I0306 14:14:48.072089 140453607589056 submission_runner.py:652] ====================
I0306 14:14:48.072211 140453607589056 submission_runner.py:750] Final wmt score: 2
