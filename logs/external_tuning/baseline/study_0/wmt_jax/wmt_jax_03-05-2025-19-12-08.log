python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-2039330123 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-08.log
2025-03-05 19:12:09.072752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201929.095454       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201929.102377       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:16.025044 140212307657920 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax.
I0305 19:12:17.052300 140212307657920 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:17.055216 140212307657920 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:17.056802 140212307657920 submission_runner.py:606] Using RNG seed -2039330123
I0305 19:12:17.606725 140212307657920 submission_runner.py:615] --- Tuning run 5/5 ---
I0305 19:12:17.606905 140212307657920 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_5.
I0305 19:12:17.607092 140212307657920 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_5/hparams.json.
I0305 19:12:17.832158 140212307657920 submission_runner.py:218] Initializing dataset.
I0305 19:12:17.999850 140212307657920 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:18.023995 140212307657920 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:18.096181 140212307657920 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:19.474010 140212307657920 submission_runner.py:229] Initializing model.
I0305 19:13:01.993825 140212307657920 submission_runner.py:272] Initializing optimizer.
I0305 19:13:02.835864 140212307657920 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:02.836126 140212307657920 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:02.837176 140212307657920 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_5 with prefix checkpoint_
I0305 19:13:02.837292 140212307657920 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_5/meta_data_0.json.
I0305 19:13:02.837487 140212307657920 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:02.837540 140212307657920 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:03.031767 140212307657920 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/wmt_jax/trial_5/flags_0.json.
I0305 19:13:03.069403 140212307657920 submission_runner.py:337] Starting training loop.
I0305 19:13:42.060884 140076004509440 logging_writer.py:48] [0] global_step=0, grad_norm=4.834344863891602, loss=11.053056716918945
I0305 19:13:42.127007 140212307657920 spec.py:321] Evaluating on the training split.
I0305 19:13:42.129558 140212307657920 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:42.133583 140212307657920 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:42.168213 140212307657920 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:48.556493 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 19:18:55.851781 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 19:18:55.949631 140212307657920 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:18:55.975166 140212307657920 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:18:56.007780 140212307657920 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:01.320121 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 19:24:01.558026 140212307657920 spec.py:349] Evaluating on the test split.
I0305 19:24:01.560495 140212307657920 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:01.563822 140212307657920 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:01.598361 140212307657920 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:04.432282 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 19:29:04.073610 140212307657920 submission_runner.py:469] Time since start: 961.00s, 	Step: 1, 	{'train/accuracy': 0.0006943100597709417, 'train/loss': 11.047662734985352, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.026718139648438, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.049092292785645, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 39.057475090026855, 'total_duration': 961.0041332244873, 'accumulated_submission_time': 39.057475090026855, 'accumulated_eval_time': 921.9465339183807, 'accumulated_logging_time': 0}
I0305 19:29:04.081148 140068949456640 logging_writer.py:48] [1] accumulated_eval_time=921.947, accumulated_logging_time=0, accumulated_submission_time=39.0575, global_step=1, preemption_count=0, score=39.0575, test/accuracy=0.000718341, test/bleu=0, test/loss=11.0491, test/num_examples=3003, total_duration=961.004, train/accuracy=0.00069431, train/bleu=0, train/loss=11.0477, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.0267, validation/num_examples=3000
I0305 19:29:39.527040 140068941063936 logging_writer.py:48] [100] global_step=100, grad_norm=0.17781797051429749, loss=8.249842643737793
I0305 19:30:14.869943 140068949456640 logging_writer.py:48] [200] global_step=200, grad_norm=0.39105308055877686, loss=7.404726505279541
I0305 19:30:50.227297 140068941063936 logging_writer.py:48] [300] global_step=300, grad_norm=0.4622707664966583, loss=6.813743591308594
I0305 19:31:25.581730 140068949456640 logging_writer.py:48] [400] global_step=400, grad_norm=0.4788748025894165, loss=6.184110641479492
I0305 19:32:00.949311 140068941063936 logging_writer.py:48] [500] global_step=500, grad_norm=0.3604300618171692, loss=5.755644798278809
I0305 19:32:36.303011 140068949456640 logging_writer.py:48] [600] global_step=600, grad_norm=0.48156705498695374, loss=5.496782302856445
I0305 19:33:11.651410 140068941063936 logging_writer.py:48] [700] global_step=700, grad_norm=0.535912811756134, loss=5.125786304473877
I0305 19:33:47.040975 140068949456640 logging_writer.py:48] [800] global_step=800, grad_norm=0.8422676920890808, loss=4.979851722717285
I0305 19:34:22.425637 140068941063936 logging_writer.py:48] [900] global_step=900, grad_norm=0.45704105496406555, loss=4.734888553619385
I0305 19:34:57.853681 140068949456640 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5219687819480896, loss=4.43784761428833
I0305 19:35:33.256197 140068941063936 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.4952901303768158, loss=4.159289360046387
I0305 19:36:08.686989 140068949456640 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5127984881401062, loss=4.003512859344482
I0305 19:36:44.095148 140068941063936 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5188016891479492, loss=3.8538596630096436
I0305 19:37:19.497146 140068949456640 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6054437756538391, loss=3.6417295932769775
I0305 19:37:54.916272 140068941063936 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.451663076877594, loss=3.5597527027130127
I0305 19:38:30.332288 140068949456640 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.44358009099960327, loss=3.4103341102600098
I0305 19:39:05.745486 140068941063936 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.45425739884376526, loss=3.3996810913085938
I0305 19:39:41.166726 140068949456640 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.4619463384151459, loss=3.2842156887054443
I0305 19:40:16.574686 140068941063936 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4432598054409027, loss=3.164773464202881
I0305 19:40:51.959289 140068949456640 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5171319842338562, loss=3.060487985610962
I0305 19:41:27.371844 140068941063936 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6611594557762146, loss=3.1222310066223145
I0305 19:42:02.764811 140068949456640 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.5121316313743591, loss=3.063678503036499
I0305 19:42:38.171457 140068941063936 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.32242974638938904, loss=2.968350410461426
I0305 19:43:04.372356 140212307657920 spec.py:321] Evaluating on the training split.
I0305 19:43:07.010452 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 19:46:06.077493 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 19:46:08.704943 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 19:48:56.087408 140212307657920 spec.py:349] Evaluating on the test split.
I0305 19:48:58.723673 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 19:51:26.895146 140212307657920 submission_runner.py:469] Time since start: 2303.83s, 	Step: 2375, 	{'train/accuracy': 0.5152161121368408, 'train/loss': 2.8282525539398193, 'train/bleu': 22.492903780599978, 'validation/accuracy': 0.5163276195526123, 'validation/loss': 2.8092308044433594, 'validation/bleu': 18.058623869254703, 'validation/num_examples': 3000, 'test/accuracy': 0.5181207060813904, 'test/loss': 2.8514907360076904, 'test/bleu': 16.871162337151773, 'test/num_examples': 3003, 'score': 879.1771328449249, 'total_duration': 2303.825659275055, 'accumulated_submission_time': 879.1771328449249, 'accumulated_eval_time': 1424.4692523479462, 'accumulated_logging_time': 0.01697993278503418}
I0305 19:51:26.903807 140068949456640 logging_writer.py:48] [2375] accumulated_eval_time=1424.47, accumulated_logging_time=0.0169799, accumulated_submission_time=879.177, global_step=2375, preemption_count=0, score=879.177, test/accuracy=0.518121, test/bleu=16.8712, test/loss=2.85149, test/num_examples=3003, total_duration=2303.83, train/accuracy=0.515216, train/bleu=22.4929, train/loss=2.82825, validation/accuracy=0.516328, validation/bleu=18.0586, validation/loss=2.80923, validation/num_examples=3000
I0305 19:51:36.121748 140068941063936 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3662632703781128, loss=3.0219309329986572
I0305 19:52:11.534806 140068949456640 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.3540424406528473, loss=3.006533622741699
I0305 19:52:46.954572 140068941063936 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.37958618998527527, loss=2.9641711711883545
I0305 19:53:22.380885 140068949456640 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.2930077910423279, loss=2.741100549697876
I0305 19:53:57.795372 140068941063936 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.29362744092941284, loss=2.7683441638946533
I0305 19:54:33.229041 140068949456640 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2283399999141693, loss=2.7166128158569336
I0305 19:55:08.607113 140068941063936 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2139100879430771, loss=2.666072130203247
I0305 19:55:44.036094 140068949456640 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.22640854120254517, loss=2.6437108516693115
I0305 19:56:19.441035 140068941063936 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.21526166796684265, loss=2.6490538120269775
I0305 19:56:54.832626 140068949456640 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.22729012370109558, loss=2.564885377883911
I0305 19:57:30.265639 140068941063936 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.19397534430027008, loss=2.6087756156921387
I0305 19:58:05.682698 140068949456640 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.3018565773963928, loss=2.5499963760375977
I0305 19:58:41.084262 140068941063936 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.24138674139976501, loss=2.5501034259796143
I0305 19:59:16.447085 140068949456640 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.19464313983917236, loss=2.396085500717163
I0305 19:59:51.846894 140068941063936 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.19962497055530548, loss=2.5041391849517822
I0305 20:00:27.250159 140068949456640 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.18325455486774445, loss=2.4439783096313477
I0305 20:01:02.671071 140068941063936 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.21042443811893463, loss=2.4970908164978027
I0305 20:01:38.105452 140068949456640 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.2474217414855957, loss=2.397583246231079
I0305 20:02:13.507894 140068941063936 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.1835065633058548, loss=2.4645116329193115
I0305 20:02:48.938014 140068949456640 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.19053278863430023, loss=2.3510687351226807
I0305 20:03:24.383480 140068941063936 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.17003019154071808, loss=2.3077924251556396
I0305 20:03:59.823572 140068949456640 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.17668017745018005, loss=2.4145267009735107
I0305 20:04:35.255672 140068941063936 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.16294634342193604, loss=2.3167037963867188
I0305 20:05:10.694428 140068949456640 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.18385207653045654, loss=2.3460257053375244
I0305 20:05:27.010560 140212307657920 spec.py:321] Evaluating on the training split.
I0305 20:05:29.644877 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:10:24.926450 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 20:10:27.558552 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:13:34.986762 140212307657920 spec.py:349] Evaluating on the test split.
I0305 20:13:37.621110 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:16:04.801088 140212307657920 submission_runner.py:469] Time since start: 3781.73s, 	Step: 4747, 	{'train/accuracy': 0.5804626941680908, 'train/loss': 2.2196571826934814, 'train/bleu': 27.14476854415835, 'validation/accuracy': 0.5936271548271179, 'validation/loss': 2.1162376403808594, 'validation/bleu': 23.233468092640987, 'validation/num_examples': 3000, 'test/accuracy': 0.597648024559021, 'test/loss': 2.099470376968384, 'test/bleu': 22.057978389027344, 'test/num_examples': 3003, 'score': 1719.1184754371643, 'total_duration': 3781.73161816597, 'accumulated_submission_time': 1719.1184754371643, 'accumulated_eval_time': 2062.25971698761, 'accumulated_logging_time': 0.034064292907714844}
I0305 20:16:04.811343 140068941063936 logging_writer.py:48] [4747] accumulated_eval_time=2062.26, accumulated_logging_time=0.0340643, accumulated_submission_time=1719.12, global_step=4747, preemption_count=0, score=1719.12, test/accuracy=0.597648, test/bleu=22.058, test/loss=2.09947, test/num_examples=3003, total_duration=3781.73, train/accuracy=0.580463, train/bleu=27.1448, train/loss=2.21966, validation/accuracy=0.593627, validation/bleu=23.2335, validation/loss=2.11624, validation/num_examples=3000
I0305 20:16:23.964874 140068949456640 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1471816748380661, loss=2.2784817218780518
I0305 20:16:59.381486 140068941063936 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.15438950061798096, loss=2.2697696685791016
I0305 20:17:34.779772 140068949456640 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1727169007062912, loss=2.2531769275665283
I0305 20:18:10.181464 140068941063936 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.16874855756759644, loss=2.2472333908081055
I0305 20:18:45.622491 140068949456640 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.16376365721225739, loss=2.3045248985290527
I0305 20:19:21.018427 140068941063936 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.15920673310756683, loss=2.259084939956665
I0305 20:19:56.431115 140068949456640 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.1917116791009903, loss=2.2385499477386475
I0305 20:20:31.857421 140068941063936 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.14768676459789276, loss=2.2268004417419434
I0305 20:21:07.268137 140068949456640 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.14496760070323944, loss=2.203781843185425
I0305 20:21:42.680489 140068941063936 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.1453324854373932, loss=2.165820360183716
I0305 20:22:18.070309 140068949456640 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.17418402433395386, loss=2.164487600326538
I0305 20:22:53.478374 140068941063936 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.1507270634174347, loss=2.150679588317871
I0305 20:23:28.865766 140068949456640 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.16472186148166656, loss=2.193103075027466
I0305 20:24:04.248697 140068941063936 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.16899004578590393, loss=2.176701784133911
I0305 20:24:39.622576 140068949456640 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.1405773013830185, loss=2.1572012901306152
I0305 20:25:14.968282 140068907493120 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.14149972796440125, loss=2.2079851627349854
I0305 20:25:50.249023 140068899100416 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.1916256546974182, loss=2.194162130355835
I0305 20:26:25.540159 140068907493120 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.14416831731796265, loss=2.0820603370666504
I0305 20:27:00.820601 140068899100416 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.21680042147636414, loss=2.120896100997925
I0305 20:27:36.088501 140068907493120 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.18174400925636292, loss=2.234977960586548
I0305 20:28:11.372231 140068899100416 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.1403769552707672, loss=2.167205572128296
I0305 20:28:46.668040 140068907493120 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.14738991856575012, loss=2.1431984901428223
I0305 20:29:21.950829 140068899100416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.1440151482820511, loss=2.1719913482666016
I0305 20:29:57.229901 140068907493120 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.13559651374816895, loss=2.1399168968200684
I0305 20:30:05.001143 140212307657920 spec.py:321] Evaluating on the training split.
I0305 20:30:07.633784 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:32:53.124304 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 20:32:55.747048 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:35:36.047763 140212307657920 spec.py:349] Evaluating on the test split.
I0305 20:35:38.674454 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:38:01.917856 140212307657920 submission_runner.py:469] Time since start: 5098.85s, 	Step: 7123, 	{'train/accuracy': 0.6062124967575073, 'train/loss': 1.999056100845337, 'train/bleu': 29.061814986945432, 'validation/accuracy': 0.6180135011672974, 'validation/loss': 1.9137722253799438, 'validation/bleu': 25.37362160859294, 'validation/num_examples': 3000, 'test/accuracy': 0.6240644454956055, 'test/loss': 1.870509147644043, 'test/bleu': 24.115369376860922, 'test/num_examples': 3003, 'score': 2559.144184112549, 'total_duration': 5098.848392724991, 'accumulated_submission_time': 2559.144184112549, 'accumulated_eval_time': 2539.176375389099, 'accumulated_logging_time': 0.05314207077026367}
I0305 20:38:01.927646 140068899100416 logging_writer.py:48] [7123] accumulated_eval_time=2539.18, accumulated_logging_time=0.0531421, accumulated_submission_time=2559.14, global_step=7123, preemption_count=0, score=2559.14, test/accuracy=0.624064, test/bleu=24.1154, test/loss=1.87051, test/num_examples=3003, total_duration=5098.85, train/accuracy=0.606212, train/bleu=29.0618, train/loss=1.99906, validation/accuracy=0.618014, validation/bleu=25.3736, validation/loss=1.91377, validation/num_examples=3000
I0305 20:38:29.462684 140068907493120 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.19760487973690033, loss=2.1061742305755615
I0305 20:39:04.771761 140068899100416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.16559477150440216, loss=2.0823159217834473
I0305 20:39:40.054460 140068907493120 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.16018961369991302, loss=2.128386974334717
I0305 20:40:15.343272 140068899100416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.18762433528900146, loss=2.1625938415527344
I0305 20:40:50.648293 140068907493120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.1678374707698822, loss=2.145045280456543
I0305 20:41:25.991282 140068899100416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.16106723248958588, loss=2.079159736633301
I0305 20:42:01.339249 140068907493120 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.16324657201766968, loss=2.1495203971862793
I0305 20:42:36.691103 140068899100416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.23614472150802612, loss=2.0940210819244385
I0305 20:43:12.048273 140068907493120 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.23230797052383423, loss=2.1025922298431396
I0305 20:43:47.378403 140068899100416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.14377591013908386, loss=2.029007911682129
I0305 20:44:22.703051 140068907493120 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.17164991796016693, loss=2.019213914871216
I0305 20:44:58.078862 140068899100416 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.20156803727149963, loss=2.025655508041382
I0305 20:45:33.430083 140068907493120 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.1504889577627182, loss=2.0259127616882324
I0305 20:46:08.754685 140068899100416 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1524040848016739, loss=2.007868528366089
I0305 20:46:44.076915 140068907493120 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16986286640167236, loss=2.07983136177063
I0305 20:47:19.394015 140068899100416 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.14543607831001282, loss=2.0702264308929443
I0305 20:47:54.747884 140068907493120 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.2488303929567337, loss=2.0473721027374268
I0305 20:48:30.101303 140068899100416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.15420375764369965, loss=2.064784288406372
I0305 20:49:05.408930 140068907493120 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.2529769241809845, loss=2.0974113941192627
I0305 20:49:40.744298 140068899100416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.17394492030143738, loss=2.1495954990386963
I0305 20:50:16.094040 140068907493120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.1845950186252594, loss=2.065145969390869
I0305 20:50:51.424954 140068899100416 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.16703641414642334, loss=2.047929048538208
I0305 20:51:26.773487 140068907493120 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.18927660584449768, loss=2.0198023319244385
I0305 20:52:02.117079 140068899100416 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.15967389941215515, loss=1.9768863916397095
I0305 20:52:02.125865 140212307657920 spec.py:321] Evaluating on the training split.
I0305 20:52:04.756555 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:55:00.908499 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 20:55:03.547277 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 20:57:46.444290 140212307657920 spec.py:349] Evaluating on the test split.
I0305 20:57:49.070174 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 21:00:02.107440 140212307657920 submission_runner.py:469] Time since start: 6419.04s, 	Step: 9501, 	{'train/accuracy': 0.6145806312561035, 'train/loss': 1.941043734550476, 'train/bleu': 29.941794157353346, 'validation/accuracy': 0.6330556869506836, 'validation/loss': 1.8025121688842773, 'validation/bleu': 26.34033600151328, 'validation/num_examples': 3000, 'test/accuracy': 0.6381531953811646, 'test/loss': 1.749673843383789, 'test/bleu': 25.449653639054095, 'test/num_examples': 3003, 'score': 3399.178116798401, 'total_duration': 6419.037971735001, 'accumulated_submission_time': 3399.178116798401, 'accumulated_eval_time': 3019.157881498337, 'accumulated_logging_time': 0.07168149948120117}
I0305 21:00:02.117761 140068907493120 logging_writer.py:48] [9501] accumulated_eval_time=3019.16, accumulated_logging_time=0.0716815, accumulated_submission_time=3399.18, global_step=9501, preemption_count=0, score=3399.18, test/accuracy=0.638153, test/bleu=25.4497, test/loss=1.74967, test/num_examples=3003, total_duration=6419.04, train/accuracy=0.614581, train/bleu=29.9418, train/loss=1.94104, validation/accuracy=0.633056, validation/bleu=26.3403, validation/loss=1.80251, validation/num_examples=3000
I0305 21:00:37.391640 140068899100416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.17276650667190552, loss=2.01131534576416
I0305 21:01:12.651054 140068907493120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.16254903376102448, loss=2.033005952835083
I0305 21:01:47.918276 140068899100416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1663174033164978, loss=2.0575642585754395
I0305 21:02:23.199686 140068907493120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.18767321109771729, loss=2.069812536239624
I0305 21:02:58.475888 140068899100416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.15572276711463928, loss=1.9420133829116821
I0305 21:03:33.747939 140068907493120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.15029798448085785, loss=2.061304807662964
I0305 21:04:08.990631 140068899100416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.16178672015666962, loss=1.988242268562317
I0305 21:04:44.235188 140068907493120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.2900106906890869, loss=2.0810489654541016
I0305 21:05:19.514517 140068899100416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17019908130168915, loss=2.0101845264434814
I0305 21:05:54.774783 140068907493120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.15795579552650452, loss=1.9813013076782227
I0305 21:06:30.025122 140068899100416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.20246364176273346, loss=1.9450016021728516
I0305 21:07:05.294817 140068907493120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.18400613963603973, loss=2.003340244293213
I0305 21:07:40.557995 140068899100416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.19822975993156433, loss=1.9565554857254028
I0305 21:08:15.853164 140068907493120 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.16195061802864075, loss=1.9132194519042969
I0305 21:08:51.129944 140068899100416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.15311279892921448, loss=2.0084540843963623
I0305 21:09:26.398199 140068907493120 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.1579052358865738, loss=1.9916136264801025
I0305 21:10:01.695622 140068899100416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3236156702041626, loss=1.9971823692321777
I0305 21:10:37.017761 140068907493120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.19650772213935852, loss=1.9240654706954956
I0305 21:11:12.332430 140068899100416 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.20269586145877838, loss=1.9475781917572021
I0305 21:11:47.647838 140068907493120 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.21916770935058594, loss=1.961310625076294
I0305 21:12:22.936208 140068899100416 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.20353126525878906, loss=1.975042462348938
I0305 21:12:58.252567 140068907493120 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.17179182171821594, loss=1.9541350603103638
I0305 21:13:33.572094 140068899100416 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.16348797082901, loss=1.9941211938858032
I0305 21:14:02.167020 140212307657920 spec.py:321] Evaluating on the training split.
I0305 21:14:04.786798 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 21:17:07.485534 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 21:17:10.099010 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 21:19:33.457550 140212307657920 spec.py:349] Evaluating on the test split.
I0305 21:19:36.080269 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 21:21:47.594030 140212307657920 submission_runner.py:469] Time since start: 7724.52s, 	Step: 11882, 	{'train/accuracy': 0.6196687817573547, 'train/loss': 1.8990176916122437, 'train/bleu': 30.187858044177897, 'validation/accuracy': 0.6392850875854492, 'validation/loss': 1.7413069009780884, 'validation/bleu': 26.86661004510521, 'validation/num_examples': 3000, 'test/accuracy': 0.6497393250465393, 'test/loss': 1.6813669204711914, 'test/bleu': 25.719035987612042, 'test/num_examples': 3003, 'score': 4239.077433109283, 'total_duration': 7724.524552106857, 'accumulated_submission_time': 4239.077433109283, 'accumulated_eval_time': 3484.58482503891, 'accumulated_logging_time': 0.09088730812072754}
I0305 21:21:47.604762 140068907493120 logging_writer.py:48] [11882] accumulated_eval_time=3484.58, accumulated_logging_time=0.0908873, accumulated_submission_time=4239.08, global_step=11882, preemption_count=0, score=4239.08, test/accuracy=0.649739, test/bleu=25.719, test/loss=1.68137, test/num_examples=3003, total_duration=7724.52, train/accuracy=0.619669, train/bleu=30.1879, train/loss=1.89902, validation/accuracy=0.639285, validation/bleu=26.8666, validation/loss=1.74131, validation/num_examples=3000
I0305 21:21:54.311388 140068899100416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.18956701457500458, loss=1.959944486618042
I0305 21:22:29.573583 140068907493120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.19461289048194885, loss=1.967038869857788
I0305 21:23:04.840057 140068899100416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.16633300483226776, loss=1.9321402311325073
I0305 21:23:40.130756 140068907493120 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.19153892993927002, loss=1.9496971368789673
I0305 21:24:15.398193 140068899100416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2655723989009857, loss=1.9340410232543945
I0305 21:24:50.648076 140068907493120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.1775224506855011, loss=1.90847647190094
I0305 21:25:25.924990 140068899100416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.23441487550735474, loss=1.9340883493423462
I0305 21:26:01.267119 140068907493120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.1657562106847763, loss=1.9328114986419678
I0305 21:26:36.662136 140068899100416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1571732759475708, loss=1.9291456937789917
I0305 21:27:12.040131 140068907493120 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.17761178314685822, loss=1.9477555751800537
I0305 21:27:47.447197 140068899100416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.18938931822776794, loss=1.977922797203064
I0305 21:28:22.821907 140068907493120 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.336360365152359, loss=2.0279197692871094
I0305 21:28:58.206884 140068899100416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.16670793294906616, loss=1.8856542110443115
I0305 21:29:33.568208 140068907493120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.23289993405342102, loss=1.942887544631958
I0305 21:30:08.917810 140068899100416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.20078136026859283, loss=1.945769190788269
I0305 21:30:44.319439 140068907493120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1641768366098404, loss=1.9848564863204956
I0305 21:31:19.712096 140068899100416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.17828381061553955, loss=1.9576621055603027
I0305 21:31:55.111423 140068907493120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.17314669489860535, loss=1.9397028684616089
I0305 21:32:30.507743 140068899100416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.18641427159309387, loss=1.9301215410232544
I0305 21:33:05.914089 140068907493120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.16567498445510864, loss=1.9125211238861084
I0305 21:33:41.310691 140068899100416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.17930804193019867, loss=1.9397523403167725
I0305 21:34:16.703248 140068907493120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.1820795089006424, loss=1.9151026010513306
I0305 21:34:52.132524 140068899100416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.15738743543624878, loss=1.8795655965805054
I0305 21:35:27.508772 140068907493120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.15419195592403412, loss=1.8811683654785156
I0305 21:35:47.691933 140212307657920 spec.py:321] Evaluating on the training split.
I0305 21:35:50.318584 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 21:39:31.284476 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 21:39:33.903881 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 21:42:11.508457 140212307657920 spec.py:349] Evaluating on the test split.
I0305 21:42:14.138699 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 21:44:43.808705 140212307657920 submission_runner.py:469] Time since start: 9100.74s, 	Step: 14258, 	{'train/accuracy': 0.6268925666809082, 'train/loss': 1.8272523880004883, 'train/bleu': 30.39317097433503, 'validation/accuracy': 0.6446987986564636, 'validation/loss': 1.697914481163025, 'validation/bleu': 27.259572065622542, 'validation/num_examples': 3000, 'test/accuracy': 0.6521492600440979, 'test/loss': 1.6408313512802124, 'test/bleu': 26.0815832816221, 'test/num_examples': 3003, 'score': 5079.01932144165, 'total_duration': 9100.739243507385, 'accumulated_submission_time': 5079.01932144165, 'accumulated_eval_time': 4020.7015478610992, 'accumulated_logging_time': 0.11070585250854492}
I0305 21:44:43.819306 140068899100416 logging_writer.py:48] [14258] accumulated_eval_time=4020.7, accumulated_logging_time=0.110706, accumulated_submission_time=5079.02, global_step=14258, preemption_count=0, score=5079.02, test/accuracy=0.652149, test/bleu=26.0816, test/loss=1.64083, test/num_examples=3003, total_duration=9100.74, train/accuracy=0.626893, train/bleu=30.3932, train/loss=1.82725, validation/accuracy=0.644699, validation/bleu=27.2596, validation/loss=1.69791, validation/num_examples=3000
I0305 21:44:59.056244 140068907493120 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.21550673246383667, loss=1.9576257467269897
I0305 21:45:34.421302 140068899100416 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2272559404373169, loss=2.0528056621551514
I0305 21:46:09.767393 140068907493120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.17979960143566132, loss=1.9328216314315796
I0305 21:46:45.116216 140068899100416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.19330939650535583, loss=1.8787143230438232
I0305 21:47:20.487247 140068907493120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.17848500609397888, loss=1.9178928136825562
I0305 21:47:55.851367 140068899100416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.17467954754829407, loss=1.8527145385742188
I0305 21:48:31.244786 140068907493120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.19257904589176178, loss=1.9813209772109985
I0305 21:49:06.610601 140068899100416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18160296976566315, loss=1.8489841222763062
I0305 21:49:41.993871 140068907493120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.20319008827209473, loss=1.9307903051376343
I0305 21:50:17.358433 140068899100416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.17422066628932953, loss=1.8686169385910034
I0305 21:50:52.723217 140068907493120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.16787321865558624, loss=1.9217379093170166
I0305 21:51:28.096544 140068899100416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.22362565994262695, loss=1.9514856338500977
I0305 21:52:03.447615 140068907493120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.23118798434734344, loss=1.9105141162872314
I0305 21:52:38.815899 140068899100416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.1893770694732666, loss=1.941786766052246
I0305 21:53:14.159431 140068907493120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.2116784304380417, loss=1.9506821632385254
I0305 21:53:49.551334 140068899100416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.16486085951328278, loss=1.8694097995758057
I0305 21:54:24.879534 140068907493120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.2365860491991043, loss=1.8822377920150757
I0305 21:55:00.268216 140068899100416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.20296117663383484, loss=1.8845340013504028
I0305 21:55:35.621715 140068907493120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.18091340363025665, loss=1.928486943244934
I0305 21:56:10.972632 140068899100416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.1667749583721161, loss=1.851575255393982
I0305 21:56:46.360746 140068907493120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.17430973052978516, loss=1.8879566192626953
I0305 21:57:21.757924 140068899100416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.16859102249145508, loss=1.850777506828308
I0305 21:57:57.140305 140068907493120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.19595973193645477, loss=1.906978964805603
I0305 21:58:32.522736 140068899100416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.16621030867099762, loss=1.947494626045227
I0305 21:58:43.860496 140212307657920 spec.py:321] Evaluating on the training split.
I0305 21:58:46.482187 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:02:30.597953 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 22:02:33.228668 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:05:17.678334 140212307657920 spec.py:349] Evaluating on the test split.
I0305 22:05:20.305487 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:07:45.032036 140212307657920 submission_runner.py:469] Time since start: 10481.96s, 	Step: 16633, 	{'train/accuracy': 0.6254225969314575, 'train/loss': 1.8350229263305664, 'train/bleu': 29.973115088127216, 'validation/accuracy': 0.6459594964981079, 'validation/loss': 1.6814725399017334, 'validation/bleu': 26.789727809717796, 'validation/num_examples': 3000, 'test/accuracy': 0.6574209332466125, 'test/loss': 1.614414930343628, 'test/bleu': 26.50551211497311, 'test/num_examples': 3003, 'score': 5918.914885520935, 'total_duration': 10481.96255993843, 'accumulated_submission_time': 5918.914885520935, 'accumulated_eval_time': 4561.873019695282, 'accumulated_logging_time': 0.1305081844329834}
I0305 22:07:45.042752 140068907493120 logging_writer.py:48] [16633] accumulated_eval_time=4561.87, accumulated_logging_time=0.130508, accumulated_submission_time=5918.91, global_step=16633, preemption_count=0, score=5918.91, test/accuracy=0.657421, test/bleu=26.5055, test/loss=1.61441, test/num_examples=3003, total_duration=10482, train/accuracy=0.625423, train/bleu=29.9731, train/loss=1.83502, validation/accuracy=0.645959, validation/bleu=26.7897, validation/loss=1.68147, validation/num_examples=3000
I0305 22:08:09.082145 140068899100416 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.18444600701332092, loss=1.9605491161346436
I0305 22:08:44.463798 140068907493120 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.19218511879444122, loss=1.9916439056396484
I0305 22:09:19.828057 140068899100416 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.17071282863616943, loss=1.9358453750610352
I0305 22:09:55.197082 140068907493120 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.1908968687057495, loss=1.8678085803985596
I0305 22:10:30.601418 140068899100416 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.22075492143630981, loss=1.9056928157806396
I0305 22:11:05.957839 140068907493120 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.1828552484512329, loss=1.8368384838104248
I0305 22:11:41.351066 140068899100416 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2894700765609741, loss=1.8515770435333252
I0305 22:12:16.767586 140068907493120 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.18094892799854279, loss=1.8416318893432617
I0305 22:12:52.167301 140068899100416 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.17969216406345367, loss=1.8489315509796143
I0305 22:13:27.565276 140068907493120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.18175354599952698, loss=1.9273960590362549
I0305 22:14:03.004062 140068899100416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.17203249037265778, loss=1.8221100568771362
I0305 22:14:38.412512 140068907493120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2736593186855316, loss=1.9143264293670654
I0305 22:15:13.832075 140068899100416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.22825606167316437, loss=1.8975476026535034
I0305 22:15:49.249257 140068907493120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.2055649608373642, loss=1.8924243450164795
I0305 22:16:24.676755 140068899100416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3002328872680664, loss=1.9520107507705688
I0305 22:17:00.049694 140068907493120 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.18543194234371185, loss=1.9364434480667114
I0305 22:17:35.452776 140068899100416 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.1871357560157776, loss=1.88856041431427
I0305 22:18:10.864234 140068907493120 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2010892927646637, loss=1.8563193082809448
I0305 22:18:46.238450 140068899100416 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.23926015198230743, loss=1.9192607402801514
I0305 22:19:21.618167 140068907493120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.26568546891212463, loss=1.8920108079910278
I0305 22:19:57.021787 140068899100416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.1787135899066925, loss=1.9309077262878418
I0305 22:20:32.419237 140068907493120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.19361327588558197, loss=1.8034589290618896
I0305 22:21:08.077682 140068899100416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.1834634244441986, loss=1.8900257349014282
I0305 22:21:43.664239 140068907493120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.18044772744178772, loss=1.8135384321212769
I0305 22:21:45.092710 140212307657920 spec.py:321] Evaluating on the training split.
I0305 22:21:47.725912 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:24:26.869761 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 22:24:29.501727 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:26:54.877099 140212307657920 spec.py:349] Evaluating on the test split.
I0305 22:26:57.511247 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:29:06.170342 140212307657920 submission_runner.py:469] Time since start: 11763.10s, 	Step: 19005, 	{'train/accuracy': 0.6518511772155762, 'train/loss': 1.6419318914413452, 'train/bleu': 31.83436671719162, 'validation/accuracy': 0.6497911214828491, 'validation/loss': 1.655680537223816, 'validation/bleu': 27.587279723158062, 'validation/num_examples': 3000, 'test/accuracy': 0.6607577204704285, 'test/loss': 1.5895891189575195, 'test/bleu': 26.79997617457195, 'test/num_examples': 3003, 'score': 6758.8208882808685, 'total_duration': 11763.100868701935, 'accumulated_submission_time': 6758.8208882808685, 'accumulated_eval_time': 5002.95058465004, 'accumulated_logging_time': 0.15057754516601562}
I0305 22:29:06.182110 140068899100416 logging_writer.py:48] [19005] accumulated_eval_time=5002.95, accumulated_logging_time=0.150578, accumulated_submission_time=6758.82, global_step=19005, preemption_count=0, score=6758.82, test/accuracy=0.660758, test/bleu=26.8, test/loss=1.58959, test/num_examples=3003, total_duration=11763.1, train/accuracy=0.651851, train/bleu=31.8344, train/loss=1.64193, validation/accuracy=0.649791, validation/bleu=27.5873, validation/loss=1.65568, validation/num_examples=3000
I0305 22:29:40.295664 140068907493120 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.18870341777801514, loss=1.8955698013305664
I0305 22:30:15.845634 140068899100416 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.19411709904670715, loss=1.890533685684204
I0305 22:30:51.384008 140068907493120 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.1934002786874771, loss=1.7787725925445557
I0305 22:31:26.908957 140068899100416 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1864539384841919, loss=1.8100414276123047
I0305 22:32:02.428808 140068907493120 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.18267549574375153, loss=1.9118188619613647
I0305 22:32:37.948967 140068899100416 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.18963295221328735, loss=1.7932136058807373
I0305 22:33:13.480475 140068907493120 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.21749815344810486, loss=1.7799687385559082
I0305 22:33:49.025443 140068899100416 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.20967252552509308, loss=1.8230931758880615
I0305 22:34:24.571794 140068907493120 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.19057704508304596, loss=1.7989507913589478
I0305 22:35:00.108758 140068899100416 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.2168429046869278, loss=1.8740975856781006
I0305 22:35:35.635622 140068907493120 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.1701851338148117, loss=1.8254899978637695
I0305 22:36:11.170881 140068899100416 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.21875788271427155, loss=1.8323289155960083
I0305 22:36:46.698300 140068907493120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.1838446855545044, loss=1.8687541484832764
I0305 22:37:22.228062 140068899100416 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.20461972057819366, loss=1.8234999179840088
I0305 22:37:57.759062 140068907493120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.1628316342830658, loss=1.7452259063720703
I0305 22:38:33.340805 140068899100416 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.1876714527606964, loss=1.7783483266830444
I0305 22:39:08.881539 140068907493120 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.16687962412834167, loss=1.868960976600647
I0305 22:39:44.424012 140068899100416 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.1859561800956726, loss=1.9012001752853394
I0305 22:40:19.989245 140068907493120 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.22402948141098022, loss=1.8728868961334229
I0305 22:40:55.527421 140068899100416 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.2676883935928345, loss=1.8584586381912231
I0305 22:41:31.051078 140068907493120 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.1996580958366394, loss=1.9018903970718384
I0305 22:42:06.630257 140068899100416 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.20858889818191528, loss=1.8497956991195679
I0305 22:42:42.145862 140068907493120 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2485334277153015, loss=1.8894010782241821
I0305 22:43:06.301812 140212307657920 spec.py:321] Evaluating on the training split.
I0305 22:43:08.938555 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:46:58.034577 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 22:47:00.674241 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:49:50.184168 140212307657920 spec.py:349] Evaluating on the test split.
I0305 22:49:52.810857 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 22:52:39.997343 140212307657920 submission_runner.py:469] Time since start: 13176.93s, 	Step: 21369, 	{'train/accuracy': 0.6354414820671082, 'train/loss': 1.7713111639022827, 'train/bleu': 31.200686184709465, 'validation/accuracy': 0.6550070643424988, 'validation/loss': 1.6310099363327026, 'validation/bleu': 27.8168476649093, 'validation/num_examples': 3000, 'test/accuracy': 0.6656702756881714, 'test/loss': 1.562355399131775, 'test/bleu': 27.06585832127, 'test/num_examples': 3003, 'score': 7598.7966096401215, 'total_duration': 13176.927866697311, 'accumulated_submission_time': 7598.7966096401215, 'accumulated_eval_time': 5576.646045207977, 'accumulated_logging_time': 0.17189979553222656}
I0305 22:52:40.009484 140068899100416 logging_writer.py:48] [21369] accumulated_eval_time=5576.65, accumulated_logging_time=0.1719, accumulated_submission_time=7598.8, global_step=21369, preemption_count=0, score=7598.8, test/accuracy=0.66567, test/bleu=27.0659, test/loss=1.56236, test/num_examples=3003, total_duration=13176.9, train/accuracy=0.635441, train/bleu=31.2007, train/loss=1.77131, validation/accuracy=0.655007, validation/bleu=27.8168, validation/loss=1.63101, validation/num_examples=3000
I0305 22:52:51.394949 140068907493120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.24701116979122162, loss=1.899266004562378
I0305 22:53:26.962634 140068899100416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.17995549738407135, loss=1.8825218677520752
I0305 22:54:02.527147 140068907493120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.1860627830028534, loss=1.8246511220932007
I0305 22:54:38.108868 140068899100416 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.1908452957868576, loss=1.8914114236831665
I0305 22:55:13.663687 140068907493120 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.19232702255249023, loss=1.7741566896438599
I0305 22:55:49.211732 140068899100416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.2224586308002472, loss=1.899049997329712
I0305 22:56:24.780885 140068907493120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.1941940039396286, loss=1.7998968362808228
I0305 22:57:00.346130 140068899100416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.24600005149841309, loss=1.905855417251587
I0305 22:57:35.910707 140068907493120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.19328922033309937, loss=1.8224917650222778
I0305 22:58:11.469870 140068899100416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.24234135448932648, loss=1.7838726043701172
I0305 22:58:47.059944 140068907493120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.17271344363689423, loss=1.9056915044784546
I0305 22:59:22.611613 140068899100416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.24696066975593567, loss=1.7958821058273315
I0305 22:59:58.166046 140068907493120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3170558214187622, loss=1.8463642597198486
I0305 23:00:33.726579 140068899100416 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.1826486736536026, loss=1.8080980777740479
I0305 23:01:09.282978 140068907493120 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.2568812072277069, loss=1.7995282411575317
I0305 23:01:44.859228 140068899100416 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.18928080797195435, loss=1.8690588474273682
I0305 23:02:20.425101 140068907493120 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.21078741550445557, loss=1.8336684703826904
I0305 23:02:55.987067 140068899100416 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.19121809303760529, loss=1.8306961059570312
I0305 23:03:31.573291 140068907493120 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.2640909254550934, loss=1.832319974899292
I0305 23:04:07.122487 140068899100416 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.2508868873119354, loss=1.8839281797409058
I0305 23:04:42.684579 140068907493120 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.2020009309053421, loss=1.7952076196670532
I0305 23:05:18.252434 140068899100416 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.21485909819602966, loss=1.8426942825317383
I0305 23:05:53.827117 140068907493120 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2245786339044571, loss=1.7780836820602417
I0305 23:06:29.392348 140068899100416 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2133440375328064, loss=1.8902333974838257
I0305 23:06:40.064545 140212307657920 spec.py:321] Evaluating on the training split.
I0305 23:06:42.696418 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:11:08.889597 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 23:11:11.510143 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:13:53.774641 140212307657920 spec.py:349] Evaluating on the test split.
I0305 23:13:56.394567 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:16:38.890640 140212307657920 submission_runner.py:469] Time since start: 14615.82s, 	Step: 23731, 	{'train/accuracy': 0.6333337426185608, 'train/loss': 1.78093683719635, 'train/bleu': 31.079124359439554, 'validation/accuracy': 0.6556868553161621, 'validation/loss': 1.6233105659484863, 'validation/bleu': 27.793874029321984, 'validation/num_examples': 3000, 'test/accuracy': 0.6649519205093384, 'test/loss': 1.557727575302124, 'test/bleu': 26.895749148653014, 'test/num_examples': 3003, 'score': 8438.711206912994, 'total_duration': 14615.821152448654, 'accumulated_submission_time': 8438.711206912994, 'accumulated_eval_time': 6175.4720640182495, 'accumulated_logging_time': 0.19208908081054688}
I0305 23:16:38.902752 140068907493120 logging_writer.py:48] [23731] accumulated_eval_time=6175.47, accumulated_logging_time=0.192089, accumulated_submission_time=8438.71, global_step=23731, preemption_count=0, score=8438.71, test/accuracy=0.664952, test/bleu=26.8957, test/loss=1.55773, test/num_examples=3003, total_duration=14615.8, train/accuracy=0.633334, train/bleu=31.0791, train/loss=1.78094, validation/accuracy=0.655687, validation/bleu=27.7939, validation/loss=1.62331, validation/num_examples=3000
I0305 23:17:03.742809 140068899100416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.20355616509914398, loss=1.820555329322815
I0305 23:17:39.278120 140068907493120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.21333901584148407, loss=1.8821427822113037
I0305 23:18:14.857091 140068899100416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.2114737182855606, loss=1.864778757095337
I0305 23:18:50.391407 140068907493120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.22861868143081665, loss=1.8205604553222656
I0305 23:19:25.951142 140068899100416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.20576511323451996, loss=1.8053340911865234
I0305 23:20:01.508149 140068907493120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.21126028895378113, loss=1.834395170211792
I0305 23:20:37.067502 140068899100416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.20040041208267212, loss=1.7455039024353027
I0305 23:21:12.629510 140068907493120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.21558009088039398, loss=1.7709242105484009
I0305 23:21:48.176897 140068899100416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.22886869311332703, loss=1.8725183010101318
I0305 23:22:23.731736 140068907493120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2342355102300644, loss=1.8219958543777466
I0305 23:22:59.294361 140068899100416 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.2060900628566742, loss=1.7950854301452637
I0305 23:23:34.841354 140068907493120 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.18887223303318024, loss=1.8732548952102661
I0305 23:24:10.399972 140068899100416 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.1904255896806717, loss=1.9024592638015747
I0305 23:24:45.920701 140068907493120 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.19300374388694763, loss=1.7428511381149292
I0305 23:25:21.489352 140068899100416 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.24094310402870178, loss=1.8841427564620972
I0305 23:25:57.021815 140068907493120 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.32363882660865784, loss=1.7471243143081665
I0305 23:26:32.559527 140068899100416 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.19673560559749603, loss=1.8607232570648193
I0305 23:27:08.113839 140068907493120 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.20654025673866272, loss=1.7697980403900146
I0305 23:27:43.661285 140068899100416 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.23528870940208435, loss=1.8011401891708374
I0305 23:28:19.252047 140068907493120 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.23419278860092163, loss=1.8649723529815674
I0305 23:28:54.793576 140068899100416 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.2019561380147934, loss=1.8659369945526123
I0305 23:29:30.362070 140068907493120 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.17826342582702637, loss=1.7791242599487305
I0305 23:30:05.935272 140068899100416 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2153463363647461, loss=1.8146021366119385
I0305 23:30:38.989840 140212307657920 spec.py:321] Evaluating on the training split.
I0305 23:30:41.635445 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:34:10.366972 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 23:34:12.995639 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:36:41.519676 140212307657920 spec.py:349] Evaluating on the test split.
I0305 23:36:44.145835 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:39:17.775235 140212307657920 submission_runner.py:469] Time since start: 15974.71s, 	Step: 26094, 	{'train/accuracy': 0.6397547721862793, 'train/loss': 1.709077000617981, 'train/bleu': 30.908126263162664, 'validation/accuracy': 0.6570340991020203, 'validation/loss': 1.6055611371994019, 'validation/bleu': 27.978284611913267, 'validation/num_examples': 3000, 'test/accuracy': 0.6680454015731812, 'test/loss': 1.538719654083252, 'test/bleu': 26.939348026519493, 'test/num_examples': 3003, 'score': 9278.656661510468, 'total_duration': 15974.705775499344, 'accumulated_submission_time': 9278.656661510468, 'accumulated_eval_time': 6694.25741314888, 'accumulated_logging_time': 0.21350789070129395}
I0305 23:39:17.788100 140068907493120 logging_writer.py:48] [26094] accumulated_eval_time=6694.26, accumulated_logging_time=0.213508, accumulated_submission_time=9278.66, global_step=26094, preemption_count=0, score=9278.66, test/accuracy=0.668045, test/bleu=26.9393, test/loss=1.53872, test/num_examples=3003, total_duration=15974.7, train/accuracy=0.639755, train/bleu=30.9081, train/loss=1.70908, validation/accuracy=0.657034, validation/bleu=27.9783, validation/loss=1.60556, validation/num_examples=3000
I0305 23:39:20.281402 140068899100416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.18573340773582458, loss=1.8394055366516113
I0305 23:39:55.828952 140068907493120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.18279308080673218, loss=1.795942783355713
I0305 23:40:31.443382 140068899100416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.19794896245002747, loss=1.831600546836853
I0305 23:41:06.989602 140068907493120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.24092744290828705, loss=1.8501802682876587
I0305 23:41:42.551589 140068899100416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.19192521274089813, loss=1.8266842365264893
I0305 23:42:18.085625 140068907493120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.19844898581504822, loss=1.87868070602417
I0305 23:42:53.641660 140068899100416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.20856018364429474, loss=1.8058363199234009
I0305 23:43:29.212306 140068907493120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.1661086082458496, loss=1.8476316928863525
I0305 23:44:04.756313 140068899100416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2138424664735794, loss=1.8155381679534912
I0305 23:44:40.328700 140068907493120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.19300080835819244, loss=1.8194725513458252
I0305 23:45:15.889883 140068899100416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.16690181195735931, loss=1.7246997356414795
I0305 23:45:51.458452 140068907493120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.21948948502540588, loss=1.8142889738082886
I0305 23:46:27.015997 140068899100416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.17591099441051483, loss=1.7596968412399292
I0305 23:47:02.582216 140068907493120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2001042366027832, loss=1.8278818130493164
I0305 23:47:38.155784 140068899100416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.19233673810958862, loss=1.8000707626342773
I0305 23:48:13.722371 140068907493120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.2415473908185959, loss=1.75819730758667
I0305 23:48:49.290261 140068899100416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.20268698036670685, loss=1.8619403839111328
I0305 23:49:24.831937 140068907493120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.21059446036815643, loss=1.732713222503662
I0305 23:50:00.396019 140068899100416 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.19534316658973694, loss=1.8067281246185303
I0305 23:50:35.986993 140068907493120 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.19763803482055664, loss=1.839363932609558
I0305 23:51:11.548537 140068899100416 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.23128414154052734, loss=1.8552706241607666
I0305 23:51:47.117784 140068907493120 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.19075095653533936, loss=1.6734448671340942
I0305 23:52:22.687579 140068899100416 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.19769977033138275, loss=1.8109480142593384
I0305 23:52:58.211199 140068907493120 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.2191847264766693, loss=1.8822638988494873
I0305 23:53:17.782845 140212307657920 spec.py:321] Evaluating on the training split.
I0305 23:53:20.425573 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:56:14.238236 140212307657920 spec.py:333] Evaluating on the validation split.
I0305 23:56:16.881796 140212307657920 workload.py:181] Translating evaluation dataset.
I0305 23:58:54.047275 140212307657920 spec.py:349] Evaluating on the test split.
I0305 23:58:56.677088 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 00:01:10.860052 140212307657920 submission_runner.py:469] Time since start: 17287.79s, 	Step: 28456, 	{'train/accuracy': 0.6388267278671265, 'train/loss': 1.7438158988952637, 'train/bleu': 31.855825170936452, 'validation/accuracy': 0.6595555543899536, 'validation/loss': 1.6053845882415771, 'validation/bleu': 28.255604165039742, 'validation/num_examples': 3000, 'test/accuracy': 0.6694473624229431, 'test/loss': 1.5254756212234497, 'test/bleu': 27.38211647510358, 'test/num_examples': 3003, 'score': 10118.513062000275, 'total_duration': 17287.790566682816, 'accumulated_submission_time': 10118.513062000275, 'accumulated_eval_time': 7167.334541559219, 'accumulated_logging_time': 0.23447227478027344}
I0306 00:01:10.872687 140068899100416 logging_writer.py:48] [28456] accumulated_eval_time=7167.33, accumulated_logging_time=0.234472, accumulated_submission_time=10118.5, global_step=28456, preemption_count=0, score=10118.5, test/accuracy=0.669447, test/bleu=27.3821, test/loss=1.52548, test/num_examples=3003, total_duration=17287.8, train/accuracy=0.638827, train/bleu=31.8558, train/loss=1.74382, validation/accuracy=0.659556, validation/bleu=28.2556, validation/loss=1.60538, validation/num_examples=3000
I0306 00:01:26.875426 140068907493120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.19403629004955292, loss=1.820174217224121
I0306 00:02:02.406685 140068899100416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18794508278369904, loss=1.7242555618286133
I0306 00:02:37.931527 140068907493120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.2022055983543396, loss=1.7051275968551636
I0306 00:03:13.439572 140068899100416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.19485971331596375, loss=1.7948917150497437
I0306 00:03:48.935176 140068907493120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.18590758740901947, loss=1.8011897802352905
I0306 00:04:24.440812 140068899100416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.198334202170372, loss=1.7728177309036255
I0306 00:04:59.919931 140068907493120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.17871302366256714, loss=1.9242483377456665
I0306 00:05:35.419878 140068899100416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.20686188340187073, loss=1.8047798871994019
I0306 00:06:10.933423 140068907493120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.26627856492996216, loss=1.840829849243164
I0306 00:06:46.414834 140068899100416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.1903037577867508, loss=1.754702091217041
I0306 00:07:21.912507 140068907493120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2266683727502823, loss=1.8326082229614258
I0306 00:07:57.399020 140068899100416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.2025178223848343, loss=1.7825602293014526
I0306 00:08:32.883074 140068907493120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2297372817993164, loss=1.7701367139816284
I0306 00:09:08.372317 140068899100416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.2431272715330124, loss=1.7348958253860474
I0306 00:09:43.899745 140068907493120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.18310169875621796, loss=1.7703404426574707
I0306 00:10:19.386300 140068899100416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.20235775411128998, loss=1.8245232105255127
I0306 00:10:54.869761 140068907493120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.180159792304039, loss=1.82485830783844
I0306 00:11:30.404471 140068899100416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.26609867811203003, loss=1.8064210414886475
I0306 00:12:05.957115 140068907493120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.17437945306301117, loss=1.8358582258224487
I0306 00:12:41.482942 140068899100416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.1871492862701416, loss=1.8176406621932983
I0306 00:13:17.003623 140068907493120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.1990726888179779, loss=1.7552263736724854
I0306 00:13:52.523426 140068899100416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.191007599234581, loss=1.82408607006073
I0306 00:14:28.035126 140068907493120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.20126400887966156, loss=1.7504158020019531
I0306 00:15:03.531264 140068899100416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.17032389342784882, loss=1.8178082704544067
I0306 00:15:10.998172 140212307657920 spec.py:321] Evaluating on the training split.
I0306 00:15:13.632571 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 00:19:45.697109 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 00:19:48.322352 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 00:24:27.192059 140212307657920 spec.py:349] Evaluating on the test split.
I0306 00:24:29.818318 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 00:28:25.339808 140212307657920 submission_runner.py:469] Time since start: 18922.27s, 	Step: 30822, 	{'train/accuracy': 0.6412522792816162, 'train/loss': 1.7266671657562256, 'train/bleu': 31.18596722785369, 'validation/accuracy': 0.6601241230964661, 'validation/loss': 1.5894215106964111, 'validation/bleu': 26.685167083769347, 'validation/num_examples': 3000, 'test/accuracy': 0.6721584796905518, 'test/loss': 1.515637993812561, 'test/bleu': 27.384455182666894, 'test/num_examples': 3003, 'score': 10958.499197721481, 'total_duration': 18922.270339250565, 'accumulated_submission_time': 10958.499197721481, 'accumulated_eval_time': 7961.676118373871, 'accumulated_logging_time': 0.25522375106811523}
I0306 00:28:25.353443 140068907493120 logging_writer.py:48] [30822] accumulated_eval_time=7961.68, accumulated_logging_time=0.255224, accumulated_submission_time=10958.5, global_step=30822, preemption_count=0, score=10958.5, test/accuracy=0.672158, test/bleu=27.3845, test/loss=1.51564, test/num_examples=3003, total_duration=18922.3, train/accuracy=0.641252, train/bleu=31.186, train/loss=1.72667, validation/accuracy=0.660124, validation/bleu=26.6852, validation/loss=1.58942, validation/num_examples=3000
I0306 00:28:53.435235 140068899100416 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.20451894402503967, loss=1.7688642740249634
I0306 00:29:28.996641 140068907493120 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.18735940754413605, loss=1.780436635017395
I0306 00:30:04.536565 140068899100416 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.22771763801574707, loss=1.919138789176941
I0306 00:30:40.072618 140068907493120 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.23228038847446442, loss=1.8730543851852417
I0306 00:31:15.583248 140068899100416 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.18056388199329376, loss=1.7464629411697388
I0306 00:31:51.064448 140068907493120 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.18805043399333954, loss=1.8062098026275635
I0306 00:32:26.461212 140068899100416 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.2031158208847046, loss=1.8036935329437256
I0306 00:33:01.847279 140068907493120 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1806548684835434, loss=1.76419198513031
I0306 00:33:37.223179 140068899100416 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.20458364486694336, loss=1.8122130632400513
I0306 00:34:12.556769 140068907493120 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.22044995427131653, loss=1.7072560787200928
I0306 00:34:47.865826 140068899100416 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.20732204616069794, loss=1.7874504327774048
I0306 00:35:23.175757 140068907493120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.1726016253232956, loss=1.7655212879180908
I0306 00:35:58.479161 140068899100416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.23396451771259308, loss=1.7673492431640625
I0306 00:36:33.807046 140068907493120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.18182633817195892, loss=1.8334400653839111
I0306 00:37:09.123935 140068899100416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.20298422873020172, loss=1.830186128616333
I0306 00:37:44.416599 140068907493120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2071107178926468, loss=1.841750144958496
I0306 00:38:19.702911 140068899100416 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.298433780670166, loss=1.8982055187225342
I0306 00:38:54.994029 140068907493120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.2061605453491211, loss=1.7675775289535522
I0306 00:39:30.257860 140068899100416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.22591674327850342, loss=1.757776141166687
I0306 00:40:05.546721 140068907493120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.1900644451379776, loss=1.818738341331482
I0306 00:40:40.833212 140068899100416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.18361148238182068, loss=1.8794809579849243
I0306 00:41:16.135723 140068907493120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.22847001254558563, loss=1.7839525938034058
I0306 00:41:51.434944 140068899100416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.18371325731277466, loss=1.8316608667373657
I0306 00:42:25.350027 140212307657920 spec.py:321] Evaluating on the training split.
I0306 00:42:27.971273 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 00:46:24.626477 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 00:46:27.253741 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 00:49:13.396418 140212307657920 spec.py:349] Evaluating on the test split.
I0306 00:49:16.014879 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 00:51:46.685074 140212307657920 submission_runner.py:469] Time since start: 20323.62s, 	Step: 33197, 	{'train/accuracy': 0.6480333209037781, 'train/loss': 1.6811453104019165, 'train/bleu': 32.090352114838176, 'validation/accuracy': 0.6626084446907043, 'validation/loss': 1.5778529644012451, 'validation/bleu': 28.40613126178176, 'validation/num_examples': 3000, 'test/accuracy': 0.6720194816589355, 'test/loss': 1.5063328742980957, 'test/bleu': 27.51479866753613, 'test/num_examples': 3003, 'score': 11798.352121591568, 'total_duration': 20323.615604162216, 'accumulated_submission_time': 11798.352121591568, 'accumulated_eval_time': 8523.011108875275, 'accumulated_logging_time': 0.27739477157592773}
I0306 00:51:46.698977 140068907493120 logging_writer.py:48] [33197] accumulated_eval_time=8523.01, accumulated_logging_time=0.277395, accumulated_submission_time=11798.4, global_step=33197, preemption_count=0, score=11798.4, test/accuracy=0.672019, test/bleu=27.5148, test/loss=1.50633, test/num_examples=3003, total_duration=20323.6, train/accuracy=0.648033, train/bleu=32.0904, train/loss=1.68115, validation/accuracy=0.662608, validation/bleu=28.4061, validation/loss=1.57785, validation/num_examples=3000
I0306 00:51:48.125455 140068899100416 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.19475999474525452, loss=1.7727044820785522
I0306 00:52:23.374128 140068907493120 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.1973094791173935, loss=1.802201509475708
I0306 00:52:58.619799 140068899100416 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2044668197631836, loss=1.7252966165542603
I0306 00:53:33.917392 140068907493120 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.19705156981945038, loss=1.9092179536819458
I0306 00:54:09.195946 140068899100416 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.18177522718906403, loss=1.8286744356155396
I0306 00:54:44.490146 140068907493120 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.18905095756053925, loss=1.8093798160552979
I0306 00:55:19.786946 140068899100416 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.2524299621582031, loss=1.7496092319488525
I0306 00:55:55.083367 140068907493120 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.17882072925567627, loss=1.7735159397125244
I0306 00:56:30.390292 140068899100416 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3163144886493683, loss=1.8249703645706177
I0306 00:57:05.696717 140068907493120 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.20617124438285828, loss=1.7930575609207153
I0306 00:57:40.991692 140068899100416 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.19078390300273895, loss=1.7798625230789185
I0306 00:58:16.273712 140068907493120 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.21023860573768616, loss=1.7497596740722656
I0306 00:58:51.552521 140068899100416 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.18511798977851868, loss=1.740026831626892
I0306 00:59:26.836332 140068907493120 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1954115629196167, loss=1.7355492115020752
I0306 01:00:02.107186 140068899100416 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.19312554597854614, loss=1.8520587682724
I0306 01:00:37.397988 140068907493120 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.2024310678243637, loss=1.8779969215393066
I0306 01:01:12.669586 140068899100416 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.21586370468139648, loss=1.703782320022583
I0306 01:01:47.945881 140068907493120 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1746119260787964, loss=1.7562199831008911
I0306 01:02:23.220048 140068899100416 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.20214812457561493, loss=1.7185181379318237
I0306 01:02:58.513145 140068907493120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.1960369050502777, loss=1.7770946025848389
I0306 01:03:33.820497 140068899100416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.21676115691661835, loss=1.8043290376663208
I0306 01:04:09.080354 140068907493120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.19004498422145844, loss=1.791762113571167
I0306 01:04:44.364326 140068899100416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.26075711846351624, loss=1.8723992109298706
I0306 01:05:19.638751 140068907493120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.17981362342834473, loss=1.7738927602767944
I0306 01:05:46.785802 140212307657920 spec.py:321] Evaluating on the training split.
I0306 01:05:49.408873 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 01:09:53.121811 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 01:09:55.744930 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 01:13:23.234645 140212307657920 spec.py:349] Evaluating on the test split.
I0306 01:13:25.851115 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 01:16:44.814244 140212307657920 submission_runner.py:469] Time since start: 21821.74s, 	Step: 35578, 	{'train/accuracy': 0.6439980268478394, 'train/loss': 1.7123889923095703, 'train/bleu': 31.733237402899025, 'validation/accuracy': 0.6621758937835693, 'validation/loss': 1.5727108716964722, 'validation/bleu': 28.253470886345244, 'validation/num_examples': 3000, 'test/accuracy': 0.6741744875907898, 'test/loss': 1.4931344985961914, 'test/bleu': 27.698417551604745, 'test/num_examples': 3003, 'score': 12638.300065755844, 'total_duration': 21821.744757413864, 'accumulated_submission_time': 12638.300065755844, 'accumulated_eval_time': 9181.039473295212, 'accumulated_logging_time': 0.30121660232543945}
I0306 01:16:44.826822 140068899100416 logging_writer.py:48] [35578] accumulated_eval_time=9181.04, accumulated_logging_time=0.301217, accumulated_submission_time=12638.3, global_step=35578, preemption_count=0, score=12638.3, test/accuracy=0.674174, test/bleu=27.6984, test/loss=1.49313, test/num_examples=3003, total_duration=21821.7, train/accuracy=0.643998, train/bleu=31.7332, train/loss=1.71239, validation/accuracy=0.662176, validation/bleu=28.2535, validation/loss=1.57271, validation/num_examples=3000
I0306 01:16:52.932892 140068907493120 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.19861747324466705, loss=1.7511895895004272
I0306 01:17:28.197291 140068899100416 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2427733987569809, loss=1.748871088027954
I0306 01:18:03.494429 140068907493120 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.1900140345096588, loss=1.7652345895767212
I0306 01:18:38.793088 140068899100416 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.24647223949432373, loss=1.7269344329833984
I0306 01:19:14.125512 140068907493120 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.18885329365730286, loss=1.7788238525390625
I0306 01:19:49.431047 140068899100416 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.27170881628990173, loss=1.8267314434051514
I0306 01:20:24.753060 140068907493120 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.1958017349243164, loss=1.6772451400756836
I0306 01:21:00.072787 140068899100416 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.18614865839481354, loss=1.744670033454895
I0306 01:21:35.382294 140068907493120 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.2511121332645416, loss=1.8043959140777588
I0306 01:22:10.698078 140068899100416 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.2632169723510742, loss=1.7504380941390991
I0306 01:22:45.992148 140068907493120 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.23842909932136536, loss=1.8155102729797363
I0306 01:23:21.284572 140068899100416 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.19242143630981445, loss=1.7743761539459229
I0306 01:23:56.576865 140068907493120 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.22860142588615417, loss=1.8351693153381348
I0306 01:24:31.904372 140068899100416 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.1814521998167038, loss=1.7411354780197144
I0306 01:25:07.195023 140068907493120 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.21288813650608063, loss=1.7855360507965088
I0306 01:25:42.502902 140068899100416 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.19764919579029083, loss=1.7320661544799805
I0306 01:26:17.810890 140068907493120 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.22512638568878174, loss=1.775479793548584
I0306 01:26:53.152010 140068899100416 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.19106537103652954, loss=1.7984546422958374
I0306 01:27:28.469437 140068907493120 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.21027277410030365, loss=1.8162423372268677
I0306 01:28:03.763682 140068899100416 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.19091971218585968, loss=1.8464421033859253
I0306 01:28:39.070011 140068907493120 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.19715286791324615, loss=1.8142623901367188
I0306 01:29:14.457956 140068899100416 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.19239476323127747, loss=1.7804558277130127
I0306 01:29:49.844038 140068907493120 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.2185230851173401, loss=1.8310240507125854
I0306 01:30:25.263659 140068899100416 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.19836224615573883, loss=1.687323808670044
I0306 01:30:45.076745 140212307657920 spec.py:321] Evaluating on the training split.
I0306 01:30:47.703285 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 01:34:20.874166 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 01:34:23.491931 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 01:37:19.757755 140212307657920 spec.py:349] Evaluating on the test split.
I0306 01:37:22.382644 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 01:40:04.027584 140212307657920 submission_runner.py:469] Time since start: 23220.96s, 	Step: 37957, 	{'train/accuracy': 0.6634432077407837, 'train/loss': 1.564178466796875, 'train/bleu': 32.66177202404907, 'validation/accuracy': 0.6647220253944397, 'validation/loss': 1.5614858865737915, 'validation/bleu': 28.896349372408686, 'validation/num_examples': 3000, 'test/accuracy': 0.6754953265190125, 'test/loss': 1.4828883409500122, 'test/bleu': 27.764955148381297, 'test/num_examples': 3003, 'score': 13478.408727169037, 'total_duration': 23220.958114624023, 'accumulated_submission_time': 13478.408727169037, 'accumulated_eval_time': 9739.99025440216, 'accumulated_logging_time': 0.3221702575683594}
I0306 01:40:04.040644 140068907493120 logging_writer.py:48] [37957] accumulated_eval_time=9739.99, accumulated_logging_time=0.32217, accumulated_submission_time=13478.4, global_step=37957, preemption_count=0, score=13478.4, test/accuracy=0.675495, test/bleu=27.765, test/loss=1.48289, test/num_examples=3003, total_duration=23221, train/accuracy=0.663443, train/bleu=32.6618, train/loss=1.56418, validation/accuracy=0.664722, validation/bleu=28.8963, validation/loss=1.56149, validation/num_examples=3000
I0306 01:40:19.605716 140068899100416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2122516632080078, loss=1.745692491531372
I0306 01:40:54.974055 140068907493120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.20057131350040436, loss=1.746172308921814
I0306 01:41:30.309454 140068899100416 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1809067726135254, loss=1.7085869312286377
I0306 01:42:05.690358 140068907493120 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.1954038441181183, loss=1.814194679260254
I0306 01:42:41.077892 140068899100416 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.2468036562204361, loss=1.7830630540847778
I0306 01:43:16.463674 140068907493120 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.18821735680103302, loss=1.744615077972412
I0306 01:43:51.860687 140068899100416 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.20820097625255585, loss=1.717907190322876
I0306 01:44:27.299229 140068907493120 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2973771393299103, loss=1.7677143812179565
I0306 01:45:02.706256 140068899100416 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.20284968614578247, loss=1.8011924028396606
I0306 01:45:38.121664 140068907493120 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.22767728567123413, loss=1.771517038345337
I0306 01:46:13.502531 140068899100416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.20466981828212738, loss=1.7209786176681519
I0306 01:46:48.923035 140068907493120 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.18454039096832275, loss=1.6795552968978882
I0306 01:47:24.330166 140068899100416 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.20048683881759644, loss=1.7588921785354614
I0306 01:47:59.735428 140068907493120 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.19611628353595734, loss=1.7321832180023193
I0306 01:48:35.150342 140068899100416 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1829228550195694, loss=1.7940301895141602
I0306 01:49:10.552855 140068907493120 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1779409795999527, loss=1.7676382064819336
I0306 01:49:45.950592 140068899100416 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.2144603729248047, loss=1.8328452110290527
I0306 01:50:21.353829 140068907493120 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.21093176305294037, loss=1.7873032093048096
I0306 01:50:56.734509 140068899100416 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.20641662180423737, loss=1.7577422857284546
I0306 01:51:32.120212 140068907493120 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.1882413774728775, loss=1.7558112144470215
I0306 01:52:07.518143 140068899100416 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.19317542016506195, loss=1.7126469612121582
I0306 01:52:42.916426 140068907493120 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.1834258735179901, loss=1.832288384437561
I0306 01:53:18.331561 140068899100416 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2313966304063797, loss=1.813912034034729
I0306 01:53:53.753898 140068907493120 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.1909784972667694, loss=1.6901578903198242
I0306 01:54:04.381616 140212307657920 spec.py:321] Evaluating on the training split.
I0306 01:54:07.008554 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 01:58:31.646874 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 01:58:34.266834 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:01:19.314211 140212307657920 spec.py:349] Evaluating on the test split.
I0306 02:01:21.935783 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:04:05.061307 140212307657920 submission_runner.py:469] Time since start: 24661.99s, 	Step: 40331, 	{'train/accuracy': 0.6500967144966125, 'train/loss': 1.6688239574432373, 'train/bleu': 31.690167398975976, 'validation/accuracy': 0.6662052273750305, 'validation/loss': 1.553770899772644, 'validation/bleu': 28.50183298191134, 'validation/num_examples': 3000, 'test/accuracy': 0.6776271462440491, 'test/loss': 1.4742405414581299, 'test/bleu': 27.950927840810635, 'test/num_examples': 3003, 'score': 14318.608921289444, 'total_duration': 24661.99183654785, 'accumulated_submission_time': 14318.608921289444, 'accumulated_eval_time': 10340.669889450073, 'accumulated_logging_time': 0.34351253509521484}
I0306 02:04:05.074479 140068899100416 logging_writer.py:48] [40331] accumulated_eval_time=10340.7, accumulated_logging_time=0.343513, accumulated_submission_time=14318.6, global_step=40331, preemption_count=0, score=14318.6, test/accuracy=0.677627, test/bleu=27.9509, test/loss=1.47424, test/num_examples=3003, total_duration=24662, train/accuracy=0.650097, train/bleu=31.6902, train/loss=1.66882, validation/accuracy=0.666205, validation/bleu=28.5018, validation/loss=1.55377, validation/num_examples=3000
I0306 02:04:29.855856 140068907493120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.18394945561885834, loss=1.810743808746338
I0306 02:05:05.267108 140068899100416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.1849297285079956, loss=1.6695886850357056
I0306 02:05:40.662135 140068907493120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.2515322268009186, loss=1.7860757112503052
I0306 02:06:16.065743 140068899100416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.19949743151664734, loss=1.795487880706787
I0306 02:06:51.470765 140068907493120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.21914684772491455, loss=1.8290374279022217
I0306 02:07:26.861827 140068899100416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.18889585137367249, loss=1.7899188995361328
I0306 02:08:02.247192 140068907493120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2010616660118103, loss=1.724191665649414
I0306 02:08:37.651151 140068899100416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.202775239944458, loss=1.728699803352356
I0306 02:09:13.040939 140068907493120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.18075819313526154, loss=1.7334749698638916
I0306 02:09:48.467610 140068899100416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.1861095428466797, loss=1.7293732166290283
I0306 02:10:23.911998 140068907493120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.22659051418304443, loss=1.593932032585144
I0306 02:10:59.283369 140068899100416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.20154647529125214, loss=1.7499498128890991
I0306 02:11:34.701659 140068907493120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2014053910970688, loss=1.788766622543335
I0306 02:12:10.098811 140068899100416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2159150093793869, loss=1.8186169862747192
I0306 02:12:45.514093 140068907493120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.19073137640953064, loss=1.777519702911377
I0306 02:13:20.879623 140068899100416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.43595004081726074, loss=1.788740634918213
I0306 02:13:56.285587 140068907493120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.20858623087406158, loss=1.6887022256851196
I0306 02:14:31.679893 140068899100416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.19027845561504364, loss=1.7461717128753662
I0306 02:15:07.073216 140068907493120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1854744255542755, loss=1.7435352802276611
I0306 02:15:42.452846 140068899100416 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.19542355835437775, loss=1.7769396305084229
I0306 02:16:17.861177 140068907493120 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2684801518917084, loss=1.7614408731460571
I0306 02:16:53.257456 140068899100416 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.21224911510944366, loss=1.8916971683502197
I0306 02:17:28.647442 140068907493120 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.1966056376695633, loss=1.7953944206237793
I0306 02:18:04.026424 140068899100416 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.19696186482906342, loss=1.7325247526168823
I0306 02:18:05.092808 140212307657920 spec.py:321] Evaluating on the training split.
I0306 02:18:07.722521 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:21:15.127299 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 02:21:17.744135 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:23:48.135868 140212307657920 spec.py:349] Evaluating on the test split.
I0306 02:23:50.760523 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:26:14.429332 140212307657920 submission_runner.py:469] Time since start: 25991.36s, 	Step: 42704, 	{'train/accuracy': 0.647441029548645, 'train/loss': 1.6936265230178833, 'train/bleu': 31.69901578903699, 'validation/accuracy': 0.6674906611442566, 'validation/loss': 1.5491188764572144, 'validation/bleu': 28.82032954574636, 'validation/num_examples': 3000, 'test/accuracy': 0.6796084046363831, 'test/loss': 1.4663722515106201, 'test/bleu': 28.378934223926457, 'test/num_examples': 3003, 'score': 15158.48569059372, 'total_duration': 25991.35985803604, 'accumulated_submission_time': 15158.48569059372, 'accumulated_eval_time': 10830.006344556808, 'accumulated_logging_time': 0.36525917053222656}
I0306 02:26:14.442625 140068907493120 logging_writer.py:48] [42704] accumulated_eval_time=10830, accumulated_logging_time=0.365259, accumulated_submission_time=15158.5, global_step=42704, preemption_count=0, score=15158.5, test/accuracy=0.679608, test/bleu=28.3789, test/loss=1.46637, test/num_examples=3003, total_duration=25991.4, train/accuracy=0.647441, train/bleu=31.699, train/loss=1.69363, validation/accuracy=0.667491, validation/bleu=28.8203, validation/loss=1.54912, validation/num_examples=3000
I0306 02:26:48.787157 140068899100416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.18437454104423523, loss=1.7499098777770996
I0306 02:27:24.210702 140068907493120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.21332672238349915, loss=1.776318907737732
I0306 02:27:59.607184 140068899100416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9720668792724609, loss=1.8300626277923584
I0306 02:28:35.022796 140068907493120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.19964416325092316, loss=1.8264726400375366
I0306 02:29:10.414629 140068899100416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1891738325357437, loss=1.7654072046279907
I0306 02:29:45.817395 140068907493120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.19655853509902954, loss=1.7688679695129395
I0306 02:30:21.209371 140068899100416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.19927319884300232, loss=1.7546982765197754
I0306 02:30:56.574879 140068907493120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.19778817892074585, loss=1.687433123588562
I0306 02:31:31.989871 140068899100416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.21762409806251526, loss=1.7743242979049683
I0306 02:32:07.380254 140068907493120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.18938615918159485, loss=1.7675375938415527
I0306 02:32:42.760593 140068899100416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2085571140050888, loss=1.7409605979919434
I0306 02:33:18.117359 140068907493120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19811853766441345, loss=1.6923290491104126
I0306 02:33:53.497253 140068899100416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.18440783023834229, loss=1.7512891292572021
I0306 02:34:28.882174 140068907493120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.19925731420516968, loss=1.74917471408844
I0306 02:35:04.238045 140068899100416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.18759161233901978, loss=1.6621750593185425
I0306 02:35:39.595432 140068907493120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.20426571369171143, loss=1.6696321964263916
I0306 02:36:14.956992 140068899100416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.19689986109733582, loss=1.7636860609054565
I0306 02:36:50.281704 140068907493120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.1910853087902069, loss=1.7419898509979248
I0306 02:37:25.642052 140068899100416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.20132535696029663, loss=1.7190263271331787
I0306 02:38:01.004329 140068907493120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1993543952703476, loss=1.7691082954406738
I0306 02:38:36.371222 140068899100416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.19412212073802948, loss=1.699534296989441
I0306 02:39:11.745588 140068907493120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.19845165312290192, loss=1.79744291305542
I0306 02:39:47.070021 140068899100416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.19436725974082947, loss=1.6950503587722778
I0306 02:40:14.672665 140212307657920 spec.py:321] Evaluating on the training split.
I0306 02:40:17.297838 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:43:36.418041 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 02:43:39.041379 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:46:08.786980 140212307657920 spec.py:349] Evaluating on the test split.
I0306 02:46:11.418926 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 02:48:38.461573 140212307657920 submission_runner.py:469] Time since start: 27335.39s, 	Step: 45079, 	{'train/accuracy': 0.6565155982971191, 'train/loss': 1.611972689628601, 'train/bleu': 32.076985214795315, 'validation/accuracy': 0.6656613945960999, 'validation/loss': 1.5430052280426025, 'validation/bleu': 28.755135554479228, 'validation/num_examples': 3000, 'test/accuracy': 0.6801645159721375, 'test/loss': 1.4603431224822998, 'test/bleu': 28.538505871044222, 'test/num_examples': 3003, 'score': 15998.578137397766, 'total_duration': 27335.392095565796, 'accumulated_submission_time': 15998.578137397766, 'accumulated_eval_time': 11333.7951836586, 'accumulated_logging_time': 0.38661861419677734}
I0306 02:48:38.475575 140068907493120 logging_writer.py:48] [45079] accumulated_eval_time=11333.8, accumulated_logging_time=0.386619, accumulated_submission_time=15998.6, global_step=45079, preemption_count=0, score=15998.6, test/accuracy=0.680165, test/bleu=28.5385, test/loss=1.46034, test/num_examples=3003, total_duration=27335.4, train/accuracy=0.656516, train/bleu=32.077, train/loss=1.61197, validation/accuracy=0.665661, validation/bleu=28.7551, validation/loss=1.54301, validation/num_examples=3000
I0306 02:48:46.266018 140068899100416 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.19364961981773376, loss=1.806895136833191
I0306 02:49:21.654957 140068907493120 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.19425177574157715, loss=1.7686799764633179
I0306 02:49:57.063073 140068899100416 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.19019989669322968, loss=1.785925269126892
I0306 02:50:32.427686 140068907493120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.2376219928264618, loss=1.7873477935791016
I0306 02:51:07.800593 140068899100416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.20329640805721283, loss=1.7518625259399414
I0306 02:51:43.200125 140068907493120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.20210491120815277, loss=1.77128267288208
I0306 02:52:18.622365 140068899100416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.199000284075737, loss=1.72508704662323
I0306 02:52:53.991430 140068907493120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.21773448586463928, loss=1.7771673202514648
I0306 02:53:29.379424 140068899100416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.19795961678028107, loss=1.8019834756851196
I0306 02:54:04.781291 140068907493120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.19585633277893066, loss=1.7309917211532593
I0306 02:54:40.148707 140068899100416 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.18009242415428162, loss=1.7074060440063477
I0306 02:55:15.579517 140068907493120 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.19869031012058258, loss=1.7172397375106812
I0306 02:55:50.969317 140068899100416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19548673927783966, loss=1.785801649093628
I0306 02:56:26.347321 140068907493120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.1763380765914917, loss=1.6314140558242798
I0306 02:57:01.731084 140068899100416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.19249272346496582, loss=1.7659857273101807
I0306 02:57:37.112613 140068907493120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.40339720249176025, loss=1.6845394372940063
I0306 02:58:12.494026 140068899100416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.18712595105171204, loss=1.8095543384552002
I0306 02:58:47.830914 140068907493120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.18501059710979462, loss=1.7805243730545044
I0306 02:59:23.173223 140068899100416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.211324542760849, loss=1.7408549785614014
I0306 02:59:58.530531 140068907493120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.19661648571491241, loss=1.7942827939987183
I0306 03:00:33.897475 140068899100416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.24323901534080505, loss=1.7992603778839111
I0306 03:01:09.249477 140068907493120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.20348075032234192, loss=1.8213895559310913
I0306 03:01:44.614180 140068899100416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.187271848320961, loss=1.7607711553573608
I0306 03:02:19.998716 140068907493120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.1946960687637329, loss=1.7263880968093872
I0306 03:02:38.736571 140212307657920 spec.py:321] Evaluating on the training split.
I0306 03:02:41.365812 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:07:19.812860 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 03:07:22.428347 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:10:44.186555 140212307657920 spec.py:349] Evaluating on the test split.
I0306 03:10:46.812509 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:13:59.927016 140212307657920 submission_runner.py:469] Time since start: 28856.86s, 	Step: 47454, 	{'train/accuracy': 0.6510302424430847, 'train/loss': 1.6474436521530151, 'train/bleu': 31.691456998677253, 'validation/accuracy': 0.6682199239730835, 'validation/loss': 1.530137300491333, 'validation/bleu': 28.558814169703766, 'validation/num_examples': 3000, 'test/accuracy': 0.6830378770828247, 'test/loss': 1.445305585861206, 'test/bleu': 28.48279242792457, 'test/num_examples': 3003, 'score': 16838.701078653336, 'total_duration': 28856.857549905777, 'accumulated_submission_time': 16838.701078653336, 'accumulated_eval_time': 12014.985574483871, 'accumulated_logging_time': 0.4089033603668213}
I0306 03:13:59.942005 140068899100416 logging_writer.py:48] [47454] accumulated_eval_time=12015, accumulated_logging_time=0.408903, accumulated_submission_time=16838.7, global_step=47454, preemption_count=0, score=16838.7, test/accuracy=0.683038, test/bleu=28.4828, test/loss=1.44531, test/num_examples=3003, total_duration=28856.9, train/accuracy=0.65103, train/bleu=31.6915, train/loss=1.64744, validation/accuracy=0.66822, validation/bleu=28.5588, validation/loss=1.53014, validation/num_examples=3000
I0306 03:14:16.600453 140068907493120 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.20066621899604797, loss=1.7888381481170654
I0306 03:14:52.000856 140068899100416 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.183808833360672, loss=1.6908355951309204
I0306 03:15:27.420845 140068907493120 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.19937162101268768, loss=1.7168707847595215
I0306 03:16:02.830319 140068899100416 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.20132851600646973, loss=1.7086185216903687
I0306 03:16:38.265398 140068907493120 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2134941816329956, loss=1.7903128862380981
I0306 03:17:13.734342 140068899100416 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.19664044678211212, loss=1.7150261402130127
I0306 03:17:49.127870 140068907493120 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.21385693550109863, loss=1.8246490955352783
I0306 03:18:24.506898 140068899100416 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.19682061672210693, loss=1.7333608865737915
I0306 03:18:59.887783 140068907493120 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.18704275786876678, loss=1.72711980342865
I0306 03:19:35.259555 140068899100416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.2128552347421646, loss=1.7029114961624146
I0306 03:20:10.649824 140068907493120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.20892786979675293, loss=1.654555082321167
I0306 03:20:46.016234 140068899100416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.19644604623317719, loss=1.7003016471862793
I0306 03:21:21.412695 140068907493120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.18785041570663452, loss=1.7727773189544678
I0306 03:21:56.799060 140068899100416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.23951300978660583, loss=1.6992359161376953
I0306 03:22:32.185340 140068907493120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.19684238731861115, loss=1.745545744895935
I0306 03:23:07.536944 140068899100416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.21745219826698303, loss=1.7217062711715698
I0306 03:23:42.909606 140068907493120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.20284996926784515, loss=1.6581453084945679
I0306 03:24:18.316898 140068899100416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.20514436066150665, loss=1.6773760318756104
I0306 03:24:53.709763 140068907493120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.19376254081726074, loss=1.6948680877685547
I0306 03:25:29.118651 140068899100416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.18833789229393005, loss=1.647614598274231
I0306 03:26:04.495809 140068907493120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.21066273748874664, loss=1.679771065711975
I0306 03:26:39.895470 140068899100416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.18681740760803223, loss=1.702572226524353
I0306 03:27:15.297356 140068907493120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.2010812759399414, loss=1.759901523590088
I0306 03:27:50.703240 140068899100416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.19991378486156464, loss=1.7229187488555908
I0306 03:28:00.259270 140212307657920 spec.py:321] Evaluating on the training split.
I0306 03:28:02.885678 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:31:42.984288 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 03:31:45.608170 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:34:19.408740 140212307657920 spec.py:349] Evaluating on the test split.
I0306 03:34:22.036986 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:36:34.918341 140212307657920 submission_runner.py:469] Time since start: 30211.85s, 	Step: 49828, 	{'train/accuracy': 0.6485689282417297, 'train/loss': 1.6717698574066162, 'train/bleu': 31.90736627337915, 'validation/accuracy': 0.6708278656005859, 'validation/loss': 1.5216649770736694, 'validation/bleu': 28.911351289783067, 'validation/num_examples': 3000, 'test/accuracy': 0.6841617226600647, 'test/loss': 1.4405287504196167, 'test/bleu': 28.726340262530012, 'test/num_examples': 3003, 'score': 17678.878575086594, 'total_duration': 30211.84886264801, 'accumulated_submission_time': 17678.878575086594, 'accumulated_eval_time': 12529.644575119019, 'accumulated_logging_time': 0.43222999572753906}
I0306 03:36:34.932632 140068907493120 logging_writer.py:48] [49828] accumulated_eval_time=12529.6, accumulated_logging_time=0.43223, accumulated_submission_time=17678.9, global_step=49828, preemption_count=0, score=17678.9, test/accuracy=0.684162, test/bleu=28.7263, test/loss=1.44053, test/num_examples=3003, total_duration=30211.8, train/accuracy=0.648569, train/bleu=31.9074, train/loss=1.67177, validation/accuracy=0.670828, validation/bleu=28.9114, validation/loss=1.52166, validation/num_examples=3000
I0306 03:37:00.786744 140068899100416 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.19612473249435425, loss=1.726936936378479
I0306 03:37:36.195527 140068907493120 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.21942180395126343, loss=1.6787776947021484
I0306 03:38:11.606215 140068899100416 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1927727311849594, loss=1.671745777130127
I0306 03:38:46.960142 140068907493120 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.23015594482421875, loss=1.7590116262435913
I0306 03:39:22.259202 140068899100416 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.21448324620723724, loss=1.6897953748703003
I0306 03:39:57.561683 140068907493120 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.1824570745229721, loss=1.695590853691101
I0306 03:40:32.865524 140068899100416 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21458248794078827, loss=1.6943930387496948
I0306 03:41:08.191080 140068907493120 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.22285374999046326, loss=1.7238247394561768
I0306 03:41:43.484501 140068899100416 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.19945214688777924, loss=1.7004437446594238
I0306 03:42:18.803614 140068907493120 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.19160054624080658, loss=1.7078800201416016
I0306 03:42:54.103431 140068899100416 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2166489213705063, loss=1.6767431497573853
I0306 03:43:29.385774 140068907493120 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.24365419149398804, loss=1.7329983711242676
I0306 03:44:04.682300 140068899100416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.23379161953926086, loss=1.6738535165786743
I0306 03:44:39.988040 140068907493120 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.2006053477525711, loss=1.736453652381897
I0306 03:45:15.329066 140068899100416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.1848383992910385, loss=1.6862314939498901
I0306 03:45:50.602210 140068907493120 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.19539682567119598, loss=1.6339962482452393
I0306 03:46:25.911111 140068899100416 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.19735799729824066, loss=1.7985172271728516
I0306 03:47:01.214346 140068907493120 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.20052503049373627, loss=1.7176038026809692
I0306 03:47:36.491959 140068899100416 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2326943576335907, loss=1.7367680072784424
I0306 03:48:11.747567 140068907493120 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.23382408916950226, loss=1.7186044454574585
I0306 03:48:47.026091 140068899100416 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.20362113416194916, loss=1.6454906463623047
I0306 03:49:22.303716 140068907493120 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2133057713508606, loss=1.7685140371322632
I0306 03:49:57.585819 140068899100416 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.19357742369174957, loss=1.7396185398101807
I0306 03:50:32.889783 140068907493120 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.20913778245449066, loss=1.7098970413208008
I0306 03:50:35.012401 140212307657920 spec.py:321] Evaluating on the training split.
I0306 03:50:37.642096 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:54:16.738374 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 03:54:19.366522 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:57:05.628396 140212307657920 spec.py:349] Evaluating on the test split.
I0306 03:57:08.253212 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 03:59:44.860262 140212307657920 submission_runner.py:469] Time since start: 31601.79s, 	Step: 52207, 	{'train/accuracy': 0.6551029086112976, 'train/loss': 1.6269837617874146, 'train/bleu': 32.00939499052922, 'validation/accuracy': 0.6701357364654541, 'validation/loss': 1.5202046632766724, 'validation/bleu': 28.859004666853203, 'validation/num_examples': 3000, 'test/accuracy': 0.6831769347190857, 'test/loss': 1.4323288202285767, 'test/bleu': 28.430479771263705, 'test/num_examples': 3003, 'score': 18518.817111730576, 'total_duration': 31601.790792942047, 'accumulated_submission_time': 18518.817111730576, 'accumulated_eval_time': 13079.492374420166, 'accumulated_logging_time': 0.45488524436950684}
I0306 03:59:44.877390 140068899100416 logging_writer.py:48] [52207] accumulated_eval_time=13079.5, accumulated_logging_time=0.454885, accumulated_submission_time=18518.8, global_step=52207, preemption_count=0, score=18518.8, test/accuracy=0.683177, test/bleu=28.4305, test/loss=1.43233, test/num_examples=3003, total_duration=31601.8, train/accuracy=0.655103, train/bleu=32.0094, train/loss=1.62698, validation/accuracy=0.670136, validation/bleu=28.859, validation/loss=1.5202, validation/num_examples=3000
I0306 04:00:18.049243 140068907493120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1750677078962326, loss=1.7147650718688965
I0306 04:00:53.348455 140068899100416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1953778713941574, loss=1.6066240072250366
I0306 04:01:28.641206 140068907493120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.24666190147399902, loss=1.7331757545471191
I0306 04:02:03.944834 140068899100416 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.2296060174703598, loss=1.7541553974151611
I0306 04:02:39.219443 140068907493120 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.21739734709262848, loss=1.7583482265472412
I0306 04:03:14.502134 140068899100416 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1925077736377716, loss=1.680622935295105
I0306 04:03:49.789807 140068907493120 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.19469140470027924, loss=1.6968276500701904
I0306 04:04:25.092665 140068899100416 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.20920272171497345, loss=1.7551559209823608
I0306 04:05:00.345500 140068907493120 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.22046244144439697, loss=1.766367793083191
I0306 04:05:35.643192 140068899100416 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.20234797894954681, loss=1.7554452419281006
I0306 04:06:10.942642 140068907493120 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2136935293674469, loss=1.6647342443466187
I0306 04:06:46.219918 140068899100416 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.1953415870666504, loss=1.7018004655838013
I0306 04:07:21.489793 140068907493120 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.2047322690486908, loss=1.6745833158493042
I0306 04:07:56.772243 140068899100416 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.18475472927093506, loss=1.6685460805892944
I0306 04:08:32.032063 140068907493120 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.19990243017673492, loss=1.6618341207504272
I0306 04:09:07.284641 140068899100416 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.19657346606254578, loss=1.6732293367385864
I0306 04:09:42.551138 140068907493120 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.19883522391319275, loss=1.7787911891937256
I0306 04:10:17.778045 140068899100416 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.18237455189228058, loss=1.6543718576431274
I0306 04:10:53.026983 140068907493120 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1936643272638321, loss=1.756876826286316
I0306 04:11:28.273416 140068899100416 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.1838967651128769, loss=1.776745080947876
I0306 04:12:03.523361 140068907493120 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.23376774787902832, loss=1.6766772270202637
I0306 04:12:38.762942 140068899100416 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.17960737645626068, loss=1.6119532585144043
I0306 04:13:14.005943 140068907493120 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2296251803636551, loss=1.71016263961792
I0306 04:13:45.022778 140212307657920 spec.py:321] Evaluating on the training split.
I0306 04:13:47.644691 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 04:17:49.057885 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 04:17:51.675503 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 04:20:50.014454 140212307657920 spec.py:349] Evaluating on the test split.
I0306 04:20:52.631590 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 04:23:13.836182 140212307657920 submission_runner.py:469] Time since start: 33010.77s, 	Step: 54589, 	{'train/accuracy': 0.6548044681549072, 'train/loss': 1.6401519775390625, 'train/bleu': 32.02586460235268, 'validation/accuracy': 0.671940267086029, 'validation/loss': 1.5146325826644897, 'validation/bleu': 29.08512569436522, 'validation/num_examples': 3000, 'test/accuracy': 0.6848453283309937, 'test/loss': 1.4275223016738892, 'test/bleu': 28.42736669655433, 'test/num_examples': 3003, 'score': 19358.822951555252, 'total_duration': 33010.76670432091, 'accumulated_submission_time': 19358.822951555252, 'accumulated_eval_time': 13648.30570936203, 'accumulated_logging_time': 0.48189401626586914}
I0306 04:23:13.852481 140068899100416 logging_writer.py:48] [54589] accumulated_eval_time=13648.3, accumulated_logging_time=0.481894, accumulated_submission_time=19358.8, global_step=54589, preemption_count=0, score=19358.8, test/accuracy=0.684845, test/bleu=28.4274, test/loss=1.42752, test/num_examples=3003, total_duration=33010.8, train/accuracy=0.654804, train/bleu=32.0259, train/loss=1.64015, validation/accuracy=0.67194, validation/bleu=29.0851, validation/loss=1.51463, validation/num_examples=3000
I0306 04:23:18.098358 140068907493120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.2164125293493271, loss=1.6882882118225098
I0306 04:23:53.365307 140068899100416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.19294553995132446, loss=1.6870017051696777
I0306 04:24:28.638169 140068907493120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.18868650496006012, loss=1.7764860391616821
I0306 04:25:03.937144 140068899100416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.19134849309921265, loss=1.7097954750061035
I0306 04:25:39.237883 140068907493120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.19113227725028992, loss=1.6174697875976562
I0306 04:26:14.513232 140068899100416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.20391961932182312, loss=1.7497506141662598
I0306 04:26:49.774871 140068907493120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.19465501606464386, loss=1.7096933126449585
I0306 04:27:25.022325 140068899100416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.22069698572158813, loss=1.7442760467529297
I0306 04:28:00.298422 140068907493120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.1836925745010376, loss=1.6746649742126465
I0306 04:28:35.582376 140068899100416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.1966577172279358, loss=1.6782323122024536
I0306 04:29:10.833621 140068907493120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.20511288940906525, loss=1.7185810804367065
I0306 04:29:46.093987 140068899100416 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.19222284853458405, loss=1.6639540195465088
I0306 04:30:21.367585 140068907493120 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.19244752824306488, loss=1.6589744091033936
I0306 04:30:56.634913 140068899100416 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2080392688512802, loss=1.722842812538147
I0306 04:31:31.919344 140068907493120 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.20429088175296783, loss=1.724812388420105
I0306 04:32:07.218220 140068899100416 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2014080286026001, loss=1.7326408624649048
I0306 04:32:42.521736 140068907493120 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.21523845195770264, loss=1.7510703802108765
I0306 04:33:17.793796 140068899100416 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.2049730122089386, loss=1.7181504964828491
I0306 04:33:53.071769 140068907493120 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.22816486656665802, loss=1.7232095003128052
I0306 04:34:28.397213 140068899100416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.18377351760864258, loss=1.6466978788375854
I0306 04:35:03.741364 140068907493120 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.21497008204460144, loss=1.7068264484405518
I0306 04:35:39.079515 140068899100416 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.2035187929868698, loss=1.7250311374664307
I0306 04:36:14.438305 140068907493120 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.21859297156333923, loss=1.754472017288208
I0306 04:36:49.811501 140068899100416 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.21808107197284698, loss=1.719472050666809
I0306 04:37:13.862783 140212307657920 spec.py:321] Evaluating on the training split.
I0306 04:37:16.492537 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 04:41:17.504088 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 04:41:20.122555 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 04:44:09.667641 140212307657920 spec.py:349] Evaluating on the test split.
I0306 04:44:12.283710 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 04:46:57.745076 140212307657920 submission_runner.py:469] Time since start: 34434.68s, 	Step: 56969, 	{'train/accuracy': 0.6689996123313904, 'train/loss': 1.5259759426116943, 'train/bleu': 33.03018508121132, 'validation/accuracy': 0.6735965609550476, 'validation/loss': 1.5059330463409424, 'validation/bleu': 28.974533009461226, 'validation/num_examples': 3000, 'test/accuracy': 0.6851465702056885, 'test/loss': 1.4180183410644531, 'test/bleu': 28.619870151252766, 'test/num_examples': 3003, 'score': 20198.696849822998, 'total_duration': 34434.67561483383, 'accumulated_submission_time': 20198.696849822998, 'accumulated_eval_time': 14232.18794798851, 'accumulated_logging_time': 0.5070185661315918}
I0306 04:46:57.759965 140068907493120 logging_writer.py:48] [56969] accumulated_eval_time=14232.2, accumulated_logging_time=0.507019, accumulated_submission_time=20198.7, global_step=56969, preemption_count=0, score=20198.7, test/accuracy=0.685147, test/bleu=28.6199, test/loss=1.41802, test/num_examples=3003, total_duration=34434.7, train/accuracy=0.669, train/bleu=33.0302, train/loss=1.52598, validation/accuracy=0.673597, validation/bleu=28.9745, validation/loss=1.50593, validation/num_examples=3000
I0306 04:47:09.066989 140068899100416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2017158567905426, loss=1.7530388832092285
I0306 04:47:44.393004 140068907493120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.1898156851530075, loss=1.6327528953552246
I0306 04:48:19.734973 140068899100416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.18460673093795776, loss=1.6828370094299316
I0306 04:48:55.111772 140068907493120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.20442268252372742, loss=1.7957651615142822
I0306 04:49:30.479300 140068899100416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.17848554253578186, loss=1.6651016473770142
I0306 04:50:05.879791 140068907493120 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.21534381806850433, loss=1.6540603637695312
I0306 04:50:41.257743 140068899100416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.21382984519004822, loss=1.7053723335266113
I0306 04:51:16.665354 140068907493120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.20263172686100006, loss=1.71506929397583
I0306 04:51:52.079929 140068899100416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.21170182526111603, loss=1.6877598762512207
I0306 04:52:27.472969 140068907493120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.19209447503089905, loss=1.6344528198242188
I0306 04:53:02.896675 140068899100416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.18395842611789703, loss=1.6717605590820312
I0306 04:53:38.281805 140068907493120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.20029956102371216, loss=1.6740306615829468
I0306 04:54:13.683938 140068899100416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19763006269931793, loss=1.7428534030914307
I0306 04:54:49.070996 140068907493120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2578886151313782, loss=1.7050814628601074
I0306 04:55:24.466736 140068899100416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.23698367178440094, loss=1.7653005123138428
I0306 04:55:59.841439 140068907493120 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.20176361501216888, loss=1.739681601524353
I0306 04:56:35.240955 140068899100416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.204763263463974, loss=1.6946163177490234
I0306 04:57:10.617844 140068907493120 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2056833803653717, loss=1.7042455673217773
I0306 04:57:45.961353 140068899100416 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.19208426773548126, loss=1.6831341981887817
I0306 04:58:21.325387 140068907493120 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.19263267517089844, loss=1.6810959577560425
I0306 04:58:56.710311 140068899100416 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.21259398758411407, loss=1.6593730449676514
I0306 04:59:32.072062 140068907493120 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19541704654693604, loss=1.6836923360824585
I0306 05:00:07.483471 140068899100416 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.1831628531217575, loss=1.6440685987472534
I0306 05:00:42.852030 140068907493120 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.19657400250434875, loss=1.7668043375015259
I0306 05:00:58.058059 140212307657920 spec.py:321] Evaluating on the training split.
I0306 05:01:00.688021 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:05:26.776265 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 05:05:29.392995 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:08:32.064138 140212307657920 spec.py:349] Evaluating on the test split.
I0306 05:08:34.686901 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:11:32.217586 140212307657920 submission_runner.py:469] Time since start: 35909.15s, 	Step: 59344, 	{'train/accuracy': 0.6552916169166565, 'train/loss': 1.6171928644180298, 'train/bleu': 32.23854304887016, 'validation/accuracy': 0.6758337020874023, 'validation/loss': 1.493086576461792, 'validation/bleu': 29.244896091433304, 'validation/num_examples': 3000, 'test/accuracy': 0.6894218325614929, 'test/loss': 1.4055259227752686, 'test/bleu': 28.838429229625458, 'test/num_examples': 3003, 'score': 21038.860293149948, 'total_duration': 35909.1481013298, 'accumulated_submission_time': 21038.860293149948, 'accumulated_eval_time': 14866.34739613533, 'accumulated_logging_time': 0.5300712585449219}
I0306 05:11:32.233660 140068899100416 logging_writer.py:48] [59344] accumulated_eval_time=14866.3, accumulated_logging_time=0.530071, accumulated_submission_time=21038.9, global_step=59344, preemption_count=0, score=21038.9, test/accuracy=0.689422, test/bleu=28.8384, test/loss=1.40553, test/num_examples=3003, total_duration=35909.1, train/accuracy=0.655292, train/bleu=32.2385, train/loss=1.61719, validation/accuracy=0.675834, validation/bleu=29.2449, validation/loss=1.49309, validation/num_examples=3000
I0306 05:11:52.399765 140068907493120 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.19157853722572327, loss=1.7081811428070068
I0306 05:12:27.781182 140068899100416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.20612186193466187, loss=1.7581838369369507
I0306 05:13:03.146254 140068907493120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.21829994022846222, loss=1.644087791442871
I0306 05:13:38.495274 140068899100416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.1905921846628189, loss=1.7306532859802246
I0306 05:14:13.857532 140068907493120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.19642971456050873, loss=1.7143819332122803
I0306 05:14:49.223081 140068899100416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.1832658052444458, loss=1.7029868364334106
I0306 05:15:24.608147 140068907493120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.20350009202957153, loss=1.7006040811538696
I0306 05:16:00.015743 140068899100416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.2002672702074051, loss=1.7387003898620605
I0306 05:16:35.446727 140068907493120 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.30627283453941345, loss=1.6516495943069458
I0306 05:17:10.851324 140068899100416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.19347816705703735, loss=1.7600045204162598
I0306 05:17:46.264992 140068907493120 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.19928917288780212, loss=1.7044285535812378
I0306 05:18:21.660453 140068899100416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.19555191695690155, loss=1.6705564260482788
I0306 05:18:57.045231 140068907493120 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.18660634756088257, loss=1.7050174474716187
I0306 05:19:32.446886 140068899100416 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.19749853014945984, loss=1.6860136985778809
I0306 05:20:07.841324 140068907493120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.19796451926231384, loss=1.6017111539840698
I0306 05:20:43.267316 140068899100416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.21132144331932068, loss=1.7125017642974854
I0306 05:21:18.664148 140068907493120 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2055836319923401, loss=1.7132488489151
I0306 05:21:54.086677 140068899100416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.19403964281082153, loss=1.6235288381576538
I0306 05:22:29.486899 140068907493120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.196647047996521, loss=1.6826785802841187
I0306 05:23:04.870065 140068899100416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2221054881811142, loss=1.6594226360321045
I0306 05:23:40.266656 140068907493120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.18772299587726593, loss=1.7091641426086426
I0306 05:24:15.660219 140068899100416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2025178074836731, loss=1.7067850828170776
I0306 05:24:51.041567 140068907493120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.22184579074382782, loss=1.7670397758483887
I0306 05:25:26.458688 140068899100416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.20023174583911896, loss=1.7273647785186768
I0306 05:25:32.472520 140212307657920 spec.py:321] Evaluating on the training split.
I0306 05:25:35.093819 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:29:32.732763 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 05:29:35.349565 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:32:19.863417 140212307657920 spec.py:349] Evaluating on the test split.
I0306 05:32:22.492322 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:34:47.761295 140212307657920 submission_runner.py:469] Time since start: 37304.69s, 	Step: 61718, 	{'train/accuracy': 0.6550386548042297, 'train/loss': 1.6222668886184692, 'train/bleu': 32.55484795016553, 'validation/accuracy': 0.6756482720375061, 'validation/loss': 1.4868499040603638, 'validation/bleu': 29.190480798550205, 'validation/num_examples': 3000, 'test/accuracy': 0.686977207660675, 'test/loss': 1.4051361083984375, 'test/bleu': 28.715630456715825, 'test/num_examples': 3003, 'score': 21878.960909605026, 'total_duration': 37304.69179344177, 'accumulated_submission_time': 21878.960909605026, 'accumulated_eval_time': 15421.636085510254, 'accumulated_logging_time': 0.5542142391204834}
I0306 05:34:47.777320 140068907493120 logging_writer.py:48] [61718] accumulated_eval_time=15421.6, accumulated_logging_time=0.554214, accumulated_submission_time=21879, global_step=61718, preemption_count=0, score=21879, test/accuracy=0.686977, test/bleu=28.7156, test/loss=1.40514, test/num_examples=3003, total_duration=37304.7, train/accuracy=0.655039, train/bleu=32.5548, train/loss=1.62227, validation/accuracy=0.675648, validation/bleu=29.1905, validation/loss=1.48685, validation/num_examples=3000
I0306 05:35:17.164962 140068899100416 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.18304692208766937, loss=1.6804732084274292
I0306 05:35:52.594345 140068907493120 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.1992429494857788, loss=1.6930967569351196
I0306 05:36:28.004845 140068899100416 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.19516795873641968, loss=1.5951610803604126
I0306 05:37:03.390619 140068907493120 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.21176354587078094, loss=1.6836352348327637
I0306 05:37:38.779446 140068899100416 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.19563259184360504, loss=1.6183514595031738
I0306 05:38:14.134782 140068907493120 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.20260386168956757, loss=1.7028090953826904
I0306 05:38:49.486371 140068899100416 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.2052067071199417, loss=1.6674495935440063
I0306 05:39:24.856919 140068907493120 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.1933121234178543, loss=1.6894019842147827
I0306 05:40:00.243749 140068899100416 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.20972226560115814, loss=1.6225132942199707
I0306 05:40:35.668790 140068907493120 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.20169559121131897, loss=1.5943374633789062
I0306 05:41:11.111322 140068899100416 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.21576043963432312, loss=1.563078761100769
I0306 05:41:46.503232 140068907493120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2047315239906311, loss=1.704257845878601
I0306 05:42:21.864137 140068899100416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.201690211892128, loss=1.5766654014587402
I0306 05:42:57.265725 140068907493120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.19803105294704437, loss=1.6851452589035034
I0306 05:43:32.638088 140068899100416 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.20110733807086945, loss=1.5936092138290405
I0306 05:44:08.016422 140068907493120 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.20957466959953308, loss=1.6409205198287964
I0306 05:44:43.360655 140068899100416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.20259499549865723, loss=1.651824712753296
I0306 05:45:18.694587 140068907493120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.19888070225715637, loss=1.6143248081207275
I0306 05:45:54.075528 140068899100416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.19116699695587158, loss=1.7258388996124268
I0306 05:46:29.447388 140068907493120 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.20468664169311523, loss=1.632733941078186
I0306 05:47:04.818555 140068899100416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.21372325718402863, loss=1.589250922203064
I0306 05:47:40.217703 140068907493120 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.1853667050600052, loss=1.6675143241882324
I0306 05:48:15.621738 140068899100416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.19476436078548431, loss=1.7252544164657593
I0306 05:48:47.828935 140212307657920 spec.py:321] Evaluating on the training split.
I0306 05:48:50.459765 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:53:31.458991 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 05:53:34.080656 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 05:57:02.262249 140212307657920 spec.py:349] Evaluating on the test split.
I0306 05:57:04.891073 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 06:00:16.355668 140212307657920 submission_runner.py:469] Time since start: 38833.29s, 	Step: 64092, 	{'train/accuracy': 0.6666781902313232, 'train/loss': 1.5514503717422485, 'train/bleu': 32.91918021189076, 'validation/accuracy': 0.6763404607772827, 'validation/loss': 1.481407642364502, 'validation/bleu': 29.467292144743723, 'validation/num_examples': 3000, 'test/accuracy': 0.6918549537658691, 'test/loss': 1.3882687091827393, 'test/bleu': 29.156801779212287, 'test/num_examples': 3003, 'score': 22718.87263226509, 'total_duration': 38833.28619623184, 'accumulated_submission_time': 22718.87263226509, 'accumulated_eval_time': 16110.162761211395, 'accumulated_logging_time': 0.578355073928833}
I0306 06:00:16.371646 140068907493120 logging_writer.py:48] [64092] accumulated_eval_time=16110.2, accumulated_logging_time=0.578355, accumulated_submission_time=22718.9, global_step=64092, preemption_count=0, score=22718.9, test/accuracy=0.691855, test/bleu=29.1568, test/loss=1.38827, test/num_examples=3003, total_duration=38833.3, train/accuracy=0.666678, train/bleu=32.9192, train/loss=1.55145, validation/accuracy=0.67634, validation/bleu=29.4673, validation/loss=1.48141, validation/num_examples=3000
I0306 06:00:19.564485 140068899100416 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.18743422627449036, loss=1.6514970064163208
I0306 06:00:54.899332 140068907493120 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.19775664806365967, loss=1.5977028608322144
I0306 06:01:30.301940 140068899100416 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.2113051414489746, loss=1.5771578550338745
I0306 06:02:05.684402 140068907493120 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.20476533472537994, loss=1.7577507495880127
I0306 06:02:41.068266 140068899100416 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2078077346086502, loss=1.6900614500045776
I0306 06:03:16.441024 140068907493120 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2003142386674881, loss=1.702765703201294
I0306 06:03:51.901511 140068899100416 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.20549549162387848, loss=1.719914197921753
I0306 06:04:27.321189 140068907493120 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.19267001748085022, loss=1.633837342262268
I0306 06:05:02.701209 140068899100416 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.20572729408740997, loss=1.6403591632843018
I0306 06:05:38.106129 140068907493120 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.1926356852054596, loss=1.6136618852615356
I0306 06:06:13.508303 140068899100416 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.1999993920326233, loss=1.669586181640625
I0306 06:06:48.901417 140068907493120 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.1934097856283188, loss=1.647445559501648
I0306 06:07:24.298644 140068899100416 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.19085417687892914, loss=1.7139530181884766
I0306 06:07:59.682870 140068907493120 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.20342598855495453, loss=1.7395341396331787
I0306 06:08:35.096199 140068899100416 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.20722058415412903, loss=1.7002209424972534
I0306 06:09:10.474314 140068907493120 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.19530239701271057, loss=1.6215009689331055
I0306 06:09:45.876488 140068899100416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.23435497283935547, loss=1.7323555946350098
I0306 06:10:21.262708 140068907493120 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.20198379456996918, loss=1.6438227891921997
I0306 06:10:56.631845 140068899100416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.1924067586660385, loss=1.6536428928375244
I0306 06:11:31.993299 140068907493120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.19458620250225067, loss=1.5910862684249878
I0306 06:12:07.407273 140068899100416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.20433098077774048, loss=1.6959800720214844
I0306 06:12:42.770679 140068907493120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.20855842530727386, loss=1.656350016593933
I0306 06:13:18.161101 140068899100416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.1965717375278473, loss=1.6370469331741333
I0306 06:13:53.555018 140068907493120 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2085634171962738, loss=1.6988039016723633
I0306 06:14:16.575841 140212307657920 spec.py:321] Evaluating on the training split.
I0306 06:14:19.207464 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 06:18:18.962131 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 06:18:21.578976 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 06:21:13.490478 140212307657920 spec.py:349] Evaluating on the test split.
I0306 06:21:16.122884 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 06:24:06.039985 140212307657920 submission_runner.py:469] Time since start: 40262.97s, 	Step: 66466, 	{'train/accuracy': 0.6642294526100159, 'train/loss': 1.5680408477783203, 'train/bleu': 32.673107220138746, 'validation/accuracy': 0.6779472827911377, 'validation/loss': 1.4741722345352173, 'validation/bleu': 29.43328690697816, 'validation/num_examples': 3000, 'test/accuracy': 0.693627655506134, 'test/loss': 1.3810653686523438, 'test/bleu': 29.14719277040466, 'test/num_examples': 3003, 'score': 23558.94005560875, 'total_duration': 40262.97049665451, 'accumulated_submission_time': 23558.94005560875, 'accumulated_eval_time': 16699.626826047897, 'accumulated_logging_time': 0.6023757457733154}
I0306 06:24:06.056648 140068899100416 logging_writer.py:48] [66466] accumulated_eval_time=16699.6, accumulated_logging_time=0.602376, accumulated_submission_time=23558.9, global_step=66466, preemption_count=0, score=23558.9, test/accuracy=0.693628, test/bleu=29.1472, test/loss=1.38107, test/num_examples=3003, total_duration=40263, train/accuracy=0.664229, train/bleu=32.6731, train/loss=1.56804, validation/accuracy=0.677947, validation/bleu=29.4333, validation/loss=1.47417, validation/num_examples=3000
I0306 06:24:18.442331 140068907493120 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.20461001992225647, loss=1.6878657341003418
I0306 06:24:53.832599 140068899100416 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.21592462062835693, loss=1.6976292133331299
I0306 06:25:29.226613 140068907493120 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.20001903176307678, loss=1.7373219728469849
I0306 06:26:04.595969 140068899100416 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.21417011320590973, loss=1.6606206893920898
I0306 06:26:40.039441 140068907493120 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.19593074917793274, loss=1.6459673643112183
I0306 06:27:15.464134 140068899100416 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.21259616315364838, loss=1.7326693534851074
I0306 06:27:50.872290 140068907493120 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.20615176856517792, loss=1.6570113897323608
I0306 06:28:26.271193 140068899100416 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.19631269574165344, loss=1.7107982635498047
I0306 06:29:01.701755 140068907493120 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.18936005234718323, loss=1.632299542427063
I0306 06:29:37.122937 140068899100416 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.23320640623569489, loss=1.6547034978866577
I0306 06:30:12.524580 140068907493120 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.20927166938781738, loss=1.6660103797912598
I0306 06:30:47.899904 140068899100416 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.20373789966106415, loss=1.7400968074798584
I0306 06:31:23.304818 140068907493120 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.21131137013435364, loss=1.6650071144104004
I0306 06:31:58.711024 140068899100416 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.1991080939769745, loss=1.5749150514602661
I0306 06:32:34.161746 140068907493120 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2027774453163147, loss=1.6056914329528809
I0306 06:33:09.567862 140068899100416 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.21651871502399445, loss=1.6464484930038452
I0306 06:33:44.985864 140068907493120 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.19794529676437378, loss=1.614098310470581
I0306 06:34:20.378427 140068899100416 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.20347028970718384, loss=1.6509225368499756
I0306 06:34:55.771497 140068907493120 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.20194301009178162, loss=1.7399293184280396
I0306 06:35:31.163006 140068899100416 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.19065214693546295, loss=1.6439664363861084
I0306 06:36:06.547555 140068907493120 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.1900983601808548, loss=1.6533527374267578
I0306 06:36:41.946561 140068899100416 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2109704613685608, loss=1.6728547811508179
I0306 06:37:17.349776 140068907493120 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.20709121227264404, loss=1.6814970970153809
I0306 06:37:52.747383 140068899100416 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.20959392189979553, loss=1.6635788679122925
I0306 06:38:06.198528 140212307657920 spec.py:321] Evaluating on the training split.
I0306 06:38:08.834155 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 06:41:41.804533 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 06:41:44.422733 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 06:44:40.632920 140212307657920 spec.py:349] Evaluating on the test split.
I0306 06:44:43.250694 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 06:47:36.009871 140212307657920 submission_runner.py:469] Time since start: 41672.94s, 	Step: 68839, 	{'train/accuracy': 0.6594803929328918, 'train/loss': 1.6030139923095703, 'train/bleu': 32.515804644469455, 'validation/accuracy': 0.6791585683822632, 'validation/loss': 1.4622377157211304, 'validation/bleu': 29.559927351146165, 'validation/num_examples': 3000, 'test/accuracy': 0.6944155097007751, 'test/loss': 1.3685837984085083, 'test/bleu': 29.490603562303292, 'test/num_examples': 3003, 'score': 24398.943358659744, 'total_duration': 41672.940413713455, 'accumulated_submission_time': 24398.943358659744, 'accumulated_eval_time': 17269.438121318817, 'accumulated_logging_time': 0.6271536350250244}
I0306 06:47:36.026027 140068907493120 logging_writer.py:48] [68839] accumulated_eval_time=17269.4, accumulated_logging_time=0.627154, accumulated_submission_time=24398.9, global_step=68839, preemption_count=0, score=24398.9, test/accuracy=0.694416, test/bleu=29.4906, test/loss=1.36858, test/num_examples=3003, total_duration=41672.9, train/accuracy=0.65948, train/bleu=32.5158, train/loss=1.60301, validation/accuracy=0.679159, validation/bleu=29.5599, validation/loss=1.46224, validation/num_examples=3000
I0306 06:47:57.988508 140068899100416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3856219947338104, loss=1.6765552759170532
I0306 06:48:33.369472 140068907493120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.21110916137695312, loss=1.7276705503463745
I0306 06:49:08.766227 140068899100416 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2006620168685913, loss=1.6018248796463013
I0306 06:49:44.150283 140068907493120 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.21871431171894073, loss=1.6240158081054688
I0306 06:50:19.555245 140068899100416 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.19491776823997498, loss=1.7005491256713867
I0306 06:50:54.932740 140068907493120 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.20996122062206268, loss=1.630298376083374
I0306 06:51:30.342643 140068899100416 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.2092095911502838, loss=1.5964865684509277
I0306 06:52:05.735677 140068907493120 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.20438829064369202, loss=1.6335402727127075
I0306 06:52:41.145321 140068899100416 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.18594640493392944, loss=1.6579535007476807
I0306 06:53:16.544831 140068907493120 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.19119884073734283, loss=1.5311354398727417
I0306 06:53:51.925136 140068899100416 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.19829171895980835, loss=1.6635589599609375
I0306 06:54:27.304869 140068907493120 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.1999795287847519, loss=1.63166081905365
I0306 06:55:02.667190 140068899100416 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.20776817202568054, loss=1.6889173984527588
I0306 06:55:38.078535 140068907493120 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2050667554140091, loss=1.5657497644424438
I0306 06:56:13.517900 140068899100416 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.19589686393737793, loss=1.6838582754135132
I0306 06:56:48.917694 140068907493120 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.20812492072582245, loss=1.6779223680496216
I0306 06:57:24.336573 140068899100416 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.20294839143753052, loss=1.6360116004943848
I0306 06:57:59.711756 140068907493120 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.19265474379062653, loss=1.6339224576950073
I0306 06:58:35.101351 140068899100416 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2200949341058731, loss=1.6439567804336548
I0306 06:59:10.490243 140068907493120 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.21367275714874268, loss=1.6205757856369019
I0306 06:59:45.889105 140068899100416 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.19505105912685394, loss=1.5950453281402588
I0306 07:00:21.285239 140068907493120 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20673257112503052, loss=1.607464075088501
I0306 07:00:56.648093 140068899100416 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2125604897737503, loss=1.6913349628448486
I0306 07:01:32.041808 140068907493120 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.20018787682056427, loss=1.6689516305923462
I0306 07:01:36.292689 140212307657920 spec.py:321] Evaluating on the training split.
I0306 07:01:38.923434 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:06:19.414508 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 07:06:22.033194 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:10:01.406721 140212307657920 spec.py:349] Evaluating on the test split.
I0306 07:10:04.028779 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:13:50.332572 140212307657920 submission_runner.py:469] Time since start: 43247.26s, 	Step: 71213, 	{'train/accuracy': 0.6687206625938416, 'train/loss': 1.5341448783874512, 'train/bleu': 33.19223553033539, 'validation/accuracy': 0.6823598146438599, 'validation/loss': 1.4484879970550537, 'validation/bleu': 29.65569232106284, 'validation/num_examples': 3000, 'test/accuracy': 0.6948789358139038, 'test/loss': 1.362641453742981, 'test/bleu': 29.401666866615503, 'test/num_examples': 3003, 'score': 25239.074939727783, 'total_duration': 43247.263063430786, 'accumulated_submission_time': 25239.074939727783, 'accumulated_eval_time': 18003.477901935577, 'accumulated_logging_time': 0.6511471271514893}
I0306 07:13:50.348855 140068899100416 logging_writer.py:48] [71213] accumulated_eval_time=18003.5, accumulated_logging_time=0.651147, accumulated_submission_time=25239.1, global_step=71213, preemption_count=0, score=25239.1, test/accuracy=0.694879, test/bleu=29.4017, test/loss=1.36264, test/num_examples=3003, total_duration=43247.3, train/accuracy=0.668721, train/bleu=33.1922, train/loss=1.53414, validation/accuracy=0.68236, validation/bleu=29.6557, validation/loss=1.44849, validation/num_examples=3000
I0306 07:14:21.498777 140068907493120 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.23130261898040771, loss=1.5759751796722412
I0306 07:14:56.892557 140068899100416 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.19316807389259338, loss=1.6991465091705322
I0306 07:15:32.272608 140068907493120 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2097175121307373, loss=1.6434460878372192
I0306 07:16:07.626785 140068899100416 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.20257925987243652, loss=1.5393917560577393
I0306 07:16:43.015737 140068907493120 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2087825983762741, loss=1.6186604499816895
I0306 07:17:18.685406 140068899100416 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.19796310365200043, loss=1.6448442935943604
I0306 07:17:54.191951 140068907493120 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.21078456938266754, loss=1.6992288827896118
I0306 07:18:29.734989 140068899100416 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.20236466825008392, loss=1.6888244152069092
I0306 07:19:05.255575 140068907493120 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.18863841891288757, loss=1.5647326707839966
I0306 07:19:40.792937 140068899100416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.19582633674144745, loss=1.6176683902740479
I0306 07:20:16.319057 140068907493120 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.19942058622837067, loss=1.6104505062103271
I0306 07:20:51.856797 140068899100416 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.18523971736431122, loss=1.620281457901001
I0306 07:21:27.409453 140068907493120 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.19733883440494537, loss=1.6703745126724243
I0306 07:22:02.970082 140068899100416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.17377763986587524, loss=1.5919603109359741
I0306 07:22:38.540831 140068907493120 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.18782950937747955, loss=1.5209543704986572
I0306 07:23:14.089848 140068899100416 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.19537846744060516, loss=1.56929349899292
I0306 07:23:49.637013 140068907493120 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2510056793689728, loss=1.6259678602218628
I0306 07:24:25.188772 140068899100416 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.5406410694122314, loss=1.6330944299697876
I0306 07:25:00.695136 140068907493120 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2022985816001892, loss=1.663923740386963
I0306 07:25:36.281396 140068899100416 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.19629549980163574, loss=1.658970594406128
I0306 07:26:11.814078 140068907493120 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.20603710412979126, loss=1.5909682512283325
I0306 07:26:47.357434 140068899100416 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.20540474355220795, loss=1.6411261558532715
I0306 07:27:22.890055 140068907493120 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.192745640873909, loss=1.571657419204712
I0306 07:27:50.599071 140212307657920 spec.py:321] Evaluating on the training split.
I0306 07:27:53.235244 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:31:52.597523 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 07:31:55.223878 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:34:31.058110 140212307657920 spec.py:349] Evaluating on the test split.
I0306 07:34:33.697671 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:36:53.812773 140212307657920 submission_runner.py:469] Time since start: 44630.74s, 	Step: 73579, 	{'train/accuracy': 0.6632655262947083, 'train/loss': 1.568823218345642, 'train/bleu': 33.0778286724886, 'validation/accuracy': 0.6829036474227905, 'validation/loss': 1.4487764835357666, 'validation/bleu': 29.829407099608986, 'validation/num_examples': 3000, 'test/accuracy': 0.6966516375541687, 'test/loss': 1.3561928272247314, 'test/bleu': 29.814798412250695, 'test/num_examples': 3003, 'score': 26079.187994003296, 'total_duration': 44630.74330019951, 'accumulated_submission_time': 26079.187994003296, 'accumulated_eval_time': 18546.691540956497, 'accumulated_logging_time': 0.6764729022979736}
I0306 07:36:53.831490 140068899100416 logging_writer.py:48] [73579] accumulated_eval_time=18546.7, accumulated_logging_time=0.676473, accumulated_submission_time=26079.2, global_step=73579, preemption_count=0, score=26079.2, test/accuracy=0.696652, test/bleu=29.8148, test/loss=1.35619, test/num_examples=3003, total_duration=44630.7, train/accuracy=0.663266, train/bleu=33.0778, train/loss=1.56882, validation/accuracy=0.682904, validation/bleu=29.8294, validation/loss=1.44878, validation/num_examples=3000
I0306 07:37:01.646615 140068907493120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.21130207180976868, loss=1.5987272262573242
I0306 07:37:37.164919 140068899100416 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.19356396794319153, loss=1.654278039932251
I0306 07:38:12.668373 140068907493120 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.19217462837696075, loss=1.7111928462982178
I0306 07:38:48.178471 140068899100416 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.2157192975282669, loss=1.5702406167984009
I0306 07:39:23.698171 140068907493120 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.22439062595367432, loss=1.6660457849502563
I0306 07:39:59.239681 140068899100416 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.21097137033939362, loss=1.6188504695892334
I0306 07:40:34.793664 140068907493120 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.20156264305114746, loss=1.6052011251449585
I0306 07:41:10.323305 140068899100416 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.20615704357624054, loss=1.68136727809906
I0306 07:41:45.860042 140068907493120 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.206672802567482, loss=1.676700234413147
I0306 07:42:21.400190 140068899100416 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.19975951313972473, loss=1.5935513973236084
I0306 07:42:56.946970 140068907493120 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.22726823389530182, loss=1.6768285036087036
I0306 07:43:32.499604 140068899100416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.20384472608566284, loss=1.6466094255447388
I0306 07:44:08.017128 140068907493120 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.215849831700325, loss=1.6995790004730225
I0306 07:44:43.571139 140068899100416 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.2153216004371643, loss=1.6525272130966187
I0306 07:45:19.113996 140068907493120 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.21095260977745056, loss=1.6489593982696533
I0306 07:45:54.646373 140068899100416 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.20833410322666168, loss=1.644685983657837
I0306 07:46:30.185037 140068907493120 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.20180974900722504, loss=1.582383394241333
I0306 07:47:05.689442 140068899100416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.23124678432941437, loss=1.5459458827972412
I0306 07:47:41.168857 140068907493120 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.1960456669330597, loss=1.704359531402588
I0306 07:48:16.651084 140068899100416 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.23050734400749207, loss=1.6069774627685547
I0306 07:48:52.152067 140068907493120 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.20754311978816986, loss=1.6913328170776367
I0306 07:49:27.628427 140068899100416 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.19730620086193085, loss=1.5138945579528809
I0306 07:50:03.133092 140068907493120 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.19007746875286102, loss=1.5896321535110474
I0306 07:50:38.605378 140068899100416 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.20468957722187042, loss=1.5966086387634277
I0306 07:50:53.874238 140212307657920 spec.py:321] Evaluating on the training split.
I0306 07:50:56.502999 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:54:47.724452 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 07:54:50.358213 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:57:16.235574 140212307657920 spec.py:349] Evaluating on the test split.
I0306 07:57:18.867057 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 07:59:57.241266 140212307657920 submission_runner.py:469] Time since start: 46014.17s, 	Step: 75944, 	{'train/accuracy': 0.675548255443573, 'train/loss': 1.4768906831741333, 'train/bleu': 34.2841374195302, 'validation/accuracy': 0.6844115257263184, 'validation/loss': 1.4405282735824585, 'validation/bleu': 30.479519921572283, 'validation/num_examples': 3000, 'test/accuracy': 0.6983779668807983, 'test/loss': 1.3479682207107544, 'test/bleu': 29.42071740690849, 'test/num_examples': 3003, 'score': 26919.094887018204, 'total_duration': 46014.17180156708, 'accumulated_submission_time': 26919.094887018204, 'accumulated_eval_time': 19090.05850839615, 'accumulated_logging_time': 0.7037150859832764}
I0306 07:59:57.260381 140068907493120 logging_writer.py:48] [75944] accumulated_eval_time=19090.1, accumulated_logging_time=0.703715, accumulated_submission_time=26919.1, global_step=75944, preemption_count=0, score=26919.1, test/accuracy=0.698378, test/bleu=29.4207, test/loss=1.34797, test/num_examples=3003, total_duration=46014.2, train/accuracy=0.675548, train/bleu=34.2841, train/loss=1.47689, validation/accuracy=0.684412, validation/bleu=30.4795, validation/loss=1.44053, validation/num_examples=3000
I0306 08:00:17.507161 140068899100416 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.18720319867134094, loss=1.605338454246521
I0306 08:00:53.014055 140068907493120 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.20840241014957428, loss=1.5846651792526245
I0306 08:01:28.492960 140068899100416 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.23548729717731476, loss=1.6930437088012695
I0306 08:02:03.994669 140068907493120 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.19531908631324768, loss=1.5488990545272827
I0306 08:02:39.496334 140068899100416 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.19585156440734863, loss=1.6191179752349854
I0306 08:03:14.961339 140068907493120 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.20254597067832947, loss=1.6726669073104858
I0306 08:03:50.462671 140068899100416 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.19616246223449707, loss=1.6596035957336426
I0306 08:04:25.948823 140068907493120 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.20564836263656616, loss=1.6598596572875977
I0306 08:05:01.438015 140068899100416 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.21266186237335205, loss=1.5902329683303833
I0306 08:05:36.953247 140068907493120 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.1964891105890274, loss=1.5208024978637695
I0306 08:06:12.418977 140068899100416 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.20120304822921753, loss=1.678884506225586
I0306 08:06:47.811303 140068907493120 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.20912428200244904, loss=1.5878419876098633
I0306 08:07:23.255689 140068899100416 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2098531424999237, loss=1.6856741905212402
I0306 08:07:58.697527 140068907493120 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.20724940299987793, loss=1.5712476968765259
I0306 08:08:34.161443 140068899100416 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.2154918611049652, loss=1.545148491859436
I0306 08:09:09.595887 140068907493120 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.19779710471630096, loss=1.5804301500320435
I0306 08:09:45.015831 140068899100416 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.20269055664539337, loss=1.6021846532821655
I0306 08:10:20.459228 140068907493120 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.19274140894412994, loss=1.6209769248962402
I0306 08:10:55.916814 140068899100416 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.19935216009616852, loss=1.560874581336975
I0306 08:11:31.369289 140068907493120 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.2198757380247116, loss=1.5333833694458008
I0306 08:12:06.819644 140068899100416 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.22098471224308014, loss=1.660685420036316
I0306 08:12:42.307400 140068907493120 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.21056725084781647, loss=1.6172983646392822
I0306 08:13:17.760738 140068899100416 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.21664845943450928, loss=1.615851879119873
I0306 08:13:53.232605 140068907493120 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.21423986554145813, loss=1.627863883972168
I0306 08:13:57.498146 140212307657920 spec.py:321] Evaluating on the training split.
I0306 08:14:00.127691 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 08:18:24.814757 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 08:18:27.435354 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 08:21:40.902187 140212307657920 spec.py:349] Evaluating on the test split.
I0306 08:21:43.532493 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 08:24:41.198514 140212307657920 submission_runner.py:469] Time since start: 47498.13s, 	Step: 78313, 	{'train/accuracy': 0.6693249940872192, 'train/loss': 1.521856427192688, 'train/bleu': 33.909759531176604, 'validation/accuracy': 0.6844239234924316, 'validation/loss': 1.4351258277893066, 'validation/bleu': 29.979010461316015, 'validation/num_examples': 3000, 'test/accuracy': 0.6992700695991516, 'test/loss': 1.3383339643478394, 'test/bleu': 29.69807100837037, 'test/num_examples': 3003, 'score': 27759.19494485855, 'total_duration': 47498.129028081894, 'accumulated_submission_time': 27759.19494485855, 'accumulated_eval_time': 19733.758798122406, 'accumulated_logging_time': 0.7311797142028809}
I0306 08:24:41.217925 140068899100416 logging_writer.py:48] [78313] accumulated_eval_time=19733.8, accumulated_logging_time=0.73118, accumulated_submission_time=27759.2, global_step=78313, preemption_count=0, score=27759.2, test/accuracy=0.69927, test/bleu=29.6981, test/loss=1.33833, test/num_examples=3003, total_duration=47498.1, train/accuracy=0.669325, train/bleu=33.9098, train/loss=1.52186, validation/accuracy=0.684424, validation/bleu=29.979, validation/loss=1.43513, validation/num_examples=3000
I0306 08:25:12.426872 140068907493120 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20335420966148376, loss=1.5724830627441406
I0306 08:25:47.926205 140068899100416 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.20825567841529846, loss=1.550734043121338
I0306 08:26:23.415640 140068907493120 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.20484980940818787, loss=1.5231183767318726
I0306 08:26:58.885333 140068899100416 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.21448183059692383, loss=1.6451809406280518
I0306 08:27:34.356048 140068907493120 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.19122248888015747, loss=1.5836728811264038
I0306 08:28:09.815142 140068899100416 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.21867682039737701, loss=1.6188362836837769
I0306 08:28:45.278992 140068907493120 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.22950413823127747, loss=1.6135151386260986
I0306 08:29:20.739553 140068899100416 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.20308174192905426, loss=1.5263936519622803
I0306 08:29:56.259984 140068907493120 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.212653249502182, loss=1.648959755897522
I0306 08:30:31.727990 140068899100416 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2147032618522644, loss=1.550498366355896
I0306 08:31:07.146524 140068907493120 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.2213912010192871, loss=1.6146841049194336
I0306 08:31:42.619931 140068899100416 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.20110341906547546, loss=1.6429327726364136
I0306 08:32:18.056970 140068907493120 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.19053302705287933, loss=1.5747706890106201
I0306 08:32:53.491944 140068899100416 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2104218602180481, loss=1.680827021598816
I0306 08:33:28.951541 140068907493120 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2030392438173294, loss=1.613972544670105
I0306 08:34:04.411792 140068899100416 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.20242150127887726, loss=1.5211504697799683
I0306 08:34:39.858626 140068907493120 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2123773843050003, loss=1.6493381261825562
I0306 08:35:15.296294 140068899100416 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.21896085143089294, loss=1.5807271003723145
I0306 08:35:50.747895 140068907493120 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.20110134780406952, loss=1.6359142065048218
I0306 08:36:26.242385 140068899100416 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.20233669877052307, loss=1.5506041049957275
I0306 08:37:01.694541 140068907493120 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20532475411891937, loss=1.5715752840042114
I0306 08:37:37.174720 140068899100416 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.21296930313110352, loss=1.5869048833847046
I0306 08:38:12.660292 140068907493120 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.20705263316631317, loss=1.5872129201889038
I0306 08:38:41.383975 140212307657920 spec.py:321] Evaluating on the training split.
I0306 08:38:44.022197 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 08:42:32.666472 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 08:42:35.297110 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 08:45:05.222465 140212307657920 spec.py:349] Evaluating on the test split.
I0306 08:45:07.867924 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 08:47:38.864955 140212307657920 submission_runner.py:469] Time since start: 48875.80s, 	Step: 80682, 	{'train/accuracy': 0.668994128704071, 'train/loss': 1.535681128501892, 'train/bleu': 33.428632903543345, 'validation/accuracy': 0.6851531267166138, 'validation/loss': 1.4262844324111938, 'validation/bleu': 29.94134797835924, 'validation/num_examples': 3000, 'test/accuracy': 0.6996524333953857, 'test/loss': 1.3295632600784302, 'test/bleu': 29.765955901502444, 'test/num_examples': 3003, 'score': 28599.22697019577, 'total_duration': 48875.79549431801, 'accumulated_submission_time': 28599.22697019577, 'accumulated_eval_time': 20271.23972916603, 'accumulated_logging_time': 0.7588772773742676}
I0306 08:47:38.883407 140068899100416 logging_writer.py:48] [80682] accumulated_eval_time=20271.2, accumulated_logging_time=0.758877, accumulated_submission_time=28599.2, global_step=80682, preemption_count=0, score=28599.2, test/accuracy=0.699652, test/bleu=29.766, test/loss=1.32956, test/num_examples=3003, total_duration=48875.8, train/accuracy=0.668994, train/bleu=33.4286, train/loss=1.53568, validation/accuracy=0.685153, validation/bleu=29.9413, validation/loss=1.42628, validation/num_examples=3000
I0306 08:47:45.627567 140068907493120 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.2106935679912567, loss=1.5020943880081177
I0306 08:48:21.006054 140068899100416 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.2168288677930832, loss=1.562169075012207
I0306 08:48:56.424835 140068907493120 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.21083801984786987, loss=1.5012093782424927
I0306 08:49:31.812337 140068899100416 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.2004203200340271, loss=1.5772112607955933
I0306 08:50:07.184181 140068907493120 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.2075667828321457, loss=1.681510329246521
I0306 08:50:42.554569 140068899100416 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.19658708572387695, loss=1.5385760068893433
I0306 08:51:17.964421 140068907493120 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.22319082915782928, loss=1.6635804176330566
I0306 08:51:53.364317 140068899100416 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.20469678938388824, loss=1.5891276597976685
I0306 08:52:28.791640 140068907493120 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.21430829167366028, loss=1.615249514579773
I0306 08:53:04.225953 140068899100416 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.22239533066749573, loss=1.6487503051757812
I0306 08:53:39.683909 140068907493120 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2059067338705063, loss=1.6157344579696655
I0306 08:54:15.132154 140068899100416 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.21887601912021637, loss=1.6911050081253052
I0306 08:54:50.580629 140068907493120 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.20308007299900055, loss=1.5699704885482788
I0306 08:55:26.014038 140068899100416 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.22427311539649963, loss=1.6200333833694458
I0306 08:56:01.449340 140068907493120 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.19952023029327393, loss=1.5770829916000366
I0306 08:56:36.896345 140068899100416 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.21735702455043793, loss=1.6770381927490234
I0306 08:57:12.319613 140068907493120 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.20701545476913452, loss=1.5666965246200562
I0306 08:57:47.758490 140068899100416 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.21491602063179016, loss=1.5144259929656982
I0306 08:58:23.159987 140068907493120 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.21020054817199707, loss=1.6205540895462036
I0306 08:58:58.598401 140068899100416 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.2160554826259613, loss=1.5413438081741333
I0306 08:59:33.979002 140068907493120 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.22439023852348328, loss=1.6888240575790405
I0306 09:00:09.393753 140068899100416 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.21284116804599762, loss=1.6015347242355347
I0306 09:00:44.788283 140068907493120 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2090035080909729, loss=1.5104132890701294
I0306 09:01:20.184999 140068899100416 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.20129714906215668, loss=1.5802335739135742
I0306 09:01:38.946500 140212307657920 spec.py:321] Evaluating on the training split.
I0306 09:01:41.577454 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 09:06:02.133012 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 09:06:04.761418 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 09:09:16.437283 140212307657920 spec.py:349] Evaluating on the test split.
I0306 09:09:19.054412 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 09:12:27.275155 140212307657920 submission_runner.py:469] Time since start: 50364.21s, 	Step: 83054, 	{'train/accuracy': 0.6757895946502686, 'train/loss': 1.4848592281341553, 'train/bleu': 34.272951930540636, 'validation/accuracy': 0.6879218220710754, 'validation/loss': 1.4152507781982422, 'validation/bleu': 30.203432645750027, 'validation/num_examples': 3000, 'test/accuracy': 0.7012860774993896, 'test/loss': 1.3231533765792847, 'test/bleu': 29.945020352973515, 'test/num_examples': 3003, 'score': 29439.155190467834, 'total_duration': 50364.20569181442, 'accumulated_submission_time': 29439.155190467834, 'accumulated_eval_time': 20919.568329572678, 'accumulated_logging_time': 0.7854514122009277}
I0306 09:12:27.293322 140068907493120 logging_writer.py:48] [83054] accumulated_eval_time=20919.6, accumulated_logging_time=0.785451, accumulated_submission_time=29439.2, global_step=83054, preemption_count=0, score=29439.2, test/accuracy=0.701286, test/bleu=29.945, test/loss=1.32315, test/num_examples=3003, total_duration=50364.2, train/accuracy=0.67579, train/bleu=34.273, train/loss=1.48486, validation/accuracy=0.687922, validation/bleu=30.2034, validation/loss=1.41525, validation/num_examples=3000
I0306 09:12:43.914055 140068899100416 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.21409007906913757, loss=1.6000429391860962
I0306 09:13:19.342500 140068907493120 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.21182958781719208, loss=1.5345518589019775
I0306 09:13:54.762261 140068899100416 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.19888532161712646, loss=1.5838249921798706
I0306 09:14:30.178480 140068907493120 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.21215538680553436, loss=1.5927445888519287
I0306 09:15:05.594427 140068899100416 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.20229315757751465, loss=1.5582828521728516
I0306 09:15:40.993268 140068907493120 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.21821409463882446, loss=1.62045419216156
I0306 09:16:16.442650 140068899100416 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.2064819484949112, loss=1.5321751832962036
I0306 09:16:51.848624 140068907493120 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.21756389737129211, loss=1.6428343057632446
I0306 09:17:27.246501 140068899100416 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.2063751071691513, loss=1.539145588874817
I0306 09:18:02.661262 140068907493120 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21542306244373322, loss=1.582831621170044
I0306 09:18:38.045601 140068899100416 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.22655601799488068, loss=1.5686779022216797
I0306 09:19:13.423046 140068907493120 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.20716945827007294, loss=1.6134274005889893
I0306 09:19:48.810975 140068899100416 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3131581246852875, loss=1.528767704963684
I0306 09:20:24.173914 140068907493120 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.21481753885746002, loss=1.517033576965332
I0306 09:20:59.570796 140068899100416 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.20521287620067596, loss=1.6559511423110962
I0306 09:21:34.984201 140068907493120 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.2101544290781021, loss=1.6021486520767212
I0306 09:22:10.362471 140068899100416 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.2289855182170868, loss=1.6163768768310547
I0306 09:22:45.728553 140068907493120 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.21064798533916473, loss=1.572458028793335
I0306 09:23:21.106750 140068899100416 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.20631611347198486, loss=1.5461790561676025
I0306 09:23:56.450424 140068907493120 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2167583405971527, loss=1.5939668416976929
I0306 09:24:31.808996 140068899100416 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.21268697082996368, loss=1.533760666847229
I0306 09:25:07.165396 140068907493120 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.21325479447841644, loss=1.5347201824188232
I0306 09:25:42.528441 140068899100416 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.22150664031505585, loss=1.61820387840271
I0306 09:26:17.876276 140068907493120 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.21856054663658142, loss=1.5418251752853394
I0306 09:26:27.436425 140212307657920 spec.py:321] Evaluating on the training split.
I0306 09:26:30.062394 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 09:30:57.167544 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 09:30:59.786921 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 09:35:19.586389 140212307657920 spec.py:349] Evaluating on the test split.
I0306 09:35:22.201931 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 09:39:50.838391 140212307657920 submission_runner.py:469] Time since start: 52007.77s, 	Step: 85428, 	{'train/accuracy': 0.6747319102287292, 'train/loss': 1.496392011642456, 'train/bleu': 33.8617690708556, 'validation/accuracy': 0.6870813369750977, 'validation/loss': 1.4138641357421875, 'validation/bleu': 29.384605890812352, 'validation/num_examples': 3000, 'test/accuracy': 0.7026764154434204, 'test/loss': 1.3169686794281006, 'test/bleu': 29.960615040979388, 'test/num_examples': 3003, 'score': 30279.161264896393, 'total_duration': 52007.76891565323, 'accumulated_submission_time': 30279.161264896393, 'accumulated_eval_time': 21722.970227003098, 'accumulated_logging_time': 0.8118948936462402}
I0306 09:39:50.856911 140068899100416 logging_writer.py:48] [85428] accumulated_eval_time=21723, accumulated_logging_time=0.811895, accumulated_submission_time=30279.2, global_step=85428, preemption_count=0, score=30279.2, test/accuracy=0.702676, test/bleu=29.9606, test/loss=1.31697, test/num_examples=3003, total_duration=52007.8, train/accuracy=0.674732, train/bleu=33.8618, train/loss=1.49639, validation/accuracy=0.687081, validation/bleu=29.3846, validation/loss=1.41386, validation/num_examples=3000
I0306 09:40:16.650183 140068907493120 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.21029338240623474, loss=1.6132467985153198
I0306 09:40:52.015487 140068899100416 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.21753980219364166, loss=1.6768897771835327
I0306 09:41:27.368239 140068907493120 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.22531776130199432, loss=1.5339021682739258
I0306 09:42:02.722030 140068899100416 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2087327390909195, loss=1.4902520179748535
I0306 09:42:38.090481 140068907493120 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.22053124010562897, loss=1.597251534461975
I0306 09:43:13.459386 140068899100416 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.21186049282550812, loss=1.522041916847229
I0306 09:43:48.843060 140068907493120 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.2218681424856186, loss=1.6154361963272095
I0306 09:44:24.230728 140068899100416 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.20595043897628784, loss=1.567218542098999
I0306 09:44:59.593648 140068907493120 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.21414056420326233, loss=1.5937541723251343
I0306 09:45:34.951276 140068899100416 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.2182927131652832, loss=1.5476889610290527
I0306 09:46:10.346383 140068907493120 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2099090963602066, loss=1.5262196063995361
I0306 09:46:45.748002 140068899100416 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.21104975044727325, loss=1.6217827796936035
I0306 09:47:21.147165 140068907493120 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.2105272263288498, loss=1.505714774131775
I0306 09:47:56.553320 140068899100416 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.2135486900806427, loss=1.599302053451538
I0306 09:48:31.933386 140068907493120 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.19691063463687897, loss=1.5416029691696167
I0306 09:49:07.326935 140068899100416 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.21301211416721344, loss=1.6010111570358276
I0306 09:49:42.734909 140068907493120 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.1991622895002365, loss=1.5085060596466064
I0306 09:50:18.150007 140068899100416 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.21577049791812897, loss=1.5218496322631836
I0306 09:50:53.579850 140068907493120 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.21925878524780273, loss=1.598352313041687
I0306 09:51:29.092179 140068899100416 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.2174030840396881, loss=1.5659981966018677
I0306 09:52:04.533984 140068907493120 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.20933778584003448, loss=1.5331577062606812
I0306 09:52:39.952592 140068899100416 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.920418918132782, loss=1.5206458568572998
I0306 09:53:15.384770 140068907493120 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.20865903794765472, loss=1.5215952396392822
I0306 09:53:50.827157 140068899100416 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.21707352995872498, loss=1.5814719200134277
I0306 09:53:51.190885 140212307657920 spec.py:321] Evaluating on the training split.
I0306 09:53:53.820989 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 09:58:25.583891 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 09:58:28.210453 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:02:37.877125 140212307657920 spec.py:349] Evaluating on the test split.
I0306 10:02:40.502268 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:06:30.596738 140212307657920 submission_runner.py:469] Time since start: 53607.53s, 	Step: 87802, 	{'train/accuracy': 0.6762893795967102, 'train/loss': 1.489400029182434, 'train/bleu': 34.030359888003574, 'validation/accuracy': 0.6888982653617859, 'validation/loss': 1.4010565280914307, 'validation/bleu': 30.398251631579015, 'validation/num_examples': 3000, 'test/accuracy': 0.7041130661964417, 'test/loss': 1.306091547012329, 'test/bleu': 29.98516937652068, 'test/num_examples': 3003, 'score': 31119.351791620255, 'total_duration': 53607.5272526741, 'accumulated_submission_time': 31119.351791620255, 'accumulated_eval_time': 22482.375999689102, 'accumulated_logging_time': 0.8386707305908203}
I0306 10:06:30.616488 140068907493120 logging_writer.py:48] [87802] accumulated_eval_time=22482.4, accumulated_logging_time=0.838671, accumulated_submission_time=31119.4, global_step=87802, preemption_count=0, score=31119.4, test/accuracy=0.704113, test/bleu=29.9852, test/loss=1.30609, test/num_examples=3003, total_duration=53607.5, train/accuracy=0.676289, train/bleu=34.0304, train/loss=1.4894, validation/accuracy=0.688898, validation/bleu=30.3983, validation/loss=1.40106, validation/num_examples=3000
I0306 10:07:05.603298 140068899100416 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.2130918800830841, loss=1.5238473415374756
I0306 10:07:40.966415 140068907493120 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.22063757479190826, loss=1.5085581541061401
I0306 10:08:16.313272 140068899100416 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.2189759612083435, loss=1.5313743352890015
I0306 10:08:51.692176 140068907493120 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21304012835025787, loss=1.5239845514297485
I0306 10:09:27.062681 140068899100416 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.21084067225456238, loss=1.5148690938949585
I0306 10:10:02.463137 140068907493120 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.21940024197101593, loss=1.6354860067367554
I0306 10:10:37.866513 140068899100416 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.2229328602552414, loss=1.6331843137741089
I0306 10:11:13.208290 140068907493120 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.20450982451438904, loss=1.559683084487915
I0306 10:11:48.559317 140068899100416 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21005097031593323, loss=1.63784658908844
I0306 10:12:23.936303 140068907493120 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.2144259810447693, loss=1.4988315105438232
I0306 10:12:59.278476 140068899100416 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2117849588394165, loss=1.5513362884521484
I0306 10:13:34.641527 140068907493120 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.21940737962722778, loss=1.5583242177963257
I0306 10:14:10.028539 140068899100416 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.2256973534822464, loss=1.6101590394973755
I0306 10:14:45.377820 140068907493120 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.22431974112987518, loss=1.4958375692367554
I0306 10:15:20.775257 140068899100416 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.21088659763336182, loss=1.5182815790176392
I0306 10:15:56.153138 140068907493120 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.21390344202518463, loss=1.4965733289718628
I0306 10:16:31.476769 140068899100416 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.22290578484535217, loss=1.5607025623321533
I0306 10:17:06.846853 140068907493120 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.21721258759498596, loss=1.5048989057540894
I0306 10:17:42.212565 140068899100416 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.214020773768425, loss=1.5390254259109497
I0306 10:18:17.601704 140068907493120 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.2163895070552826, loss=1.6008021831512451
I0306 10:18:52.969842 140068899100416 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2173556536436081, loss=1.5459003448486328
I0306 10:19:28.388881 140068907493120 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.20620419085025787, loss=1.562552571296692
I0306 10:20:03.789443 140068899100416 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.22066402435302734, loss=1.524361252784729
I0306 10:20:30.703555 140212307657920 spec.py:321] Evaluating on the training split.
I0306 10:20:33.329036 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:25:11.179755 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 10:25:13.800149 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:29:37.532661 140212307657920 spec.py:349] Evaluating on the test split.
I0306 10:29:40.153417 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:34:26.262952 140212307657920 submission_runner.py:469] Time since start: 55283.19s, 	Step: 90177, 	{'train/accuracy': 0.6828116178512573, 'train/loss': 1.446681261062622, 'train/bleu': 34.69519178939945, 'validation/accuracy': 0.6897140145301819, 'validation/loss': 1.3971505165100098, 'validation/bleu': 29.603136007416598, 'validation/num_examples': 3000, 'test/accuracy': 0.7053296566009521, 'test/loss': 1.302403211593628, 'test/bleu': 30.127322176394287, 'test/num_examples': 3003, 'score': 31959.29849600792, 'total_duration': 55283.19348669052, 'accumulated_submission_time': 31959.29849600792, 'accumulated_eval_time': 23317.93534207344, 'accumulated_logging_time': 0.8673756122589111}
I0306 10:34:26.281695 140068907493120 logging_writer.py:48] [90177] accumulated_eval_time=23317.9, accumulated_logging_time=0.867376, accumulated_submission_time=31959.3, global_step=90177, preemption_count=0, score=31959.3, test/accuracy=0.70533, test/bleu=30.1273, test/loss=1.3024, test/num_examples=3003, total_duration=55283.2, train/accuracy=0.682812, train/bleu=34.6952, train/loss=1.44668, validation/accuracy=0.689714, validation/bleu=29.6031, validation/loss=1.39715, validation/num_examples=3000
I0306 10:34:34.790401 140068899100416 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.22840330004692078, loss=1.5757708549499512
I0306 10:35:10.162124 140068907493120 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.21880660951137543, loss=1.5834927558898926
I0306 10:35:45.606062 140068899100416 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.21300199627876282, loss=1.4811315536499023
I0306 10:36:21.004250 140068907493120 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.20724643766880035, loss=1.559232234954834
I0306 10:36:56.395385 140068899100416 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.21963045001029968, loss=1.5668025016784668
I0306 10:37:31.812386 140068907493120 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.21524713933467865, loss=1.5366748571395874
I0306 10:38:07.196779 140068899100416 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.2191888391971588, loss=1.6486085653305054
I0306 10:38:42.639522 140068907493120 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.2772844433784485, loss=1.6378971338272095
I0306 10:39:18.046781 140068899100416 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.22775551676750183, loss=1.5356533527374268
I0306 10:39:53.484779 140068907493120 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.2044602334499359, loss=1.5331836938858032
I0306 10:40:28.894237 140068899100416 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.22675301134586334, loss=1.5449621677398682
I0306 10:41:04.316123 140068907493120 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.2145330309867859, loss=1.428122878074646
I0306 10:41:39.760665 140068899100416 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.2138756960630417, loss=1.597404956817627
I0306 10:42:15.148108 140068907493120 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.20969517529010773, loss=1.5545753240585327
I0306 10:42:50.518051 140068899100416 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.2241603434085846, loss=1.5786523818969727
I0306 10:43:25.903904 140068907493120 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.2067224085330963, loss=1.4958796501159668
I0306 10:44:01.284091 140068899100416 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.2100774049758911, loss=1.5648388862609863
I0306 10:44:36.662084 140068907493120 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2223510444164276, loss=1.5836158990859985
I0306 10:45:12.041903 140068899100416 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2286437749862671, loss=1.5805087089538574
I0306 10:45:47.425222 140068907493120 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.22973471879959106, loss=1.5495094060897827
I0306 10:46:22.807818 140068899100416 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.22340929508209229, loss=1.5281920433044434
I0306 10:46:58.176097 140068907493120 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21108005940914154, loss=1.5209468603134155
I0306 10:47:33.565214 140068899100416 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.2290055900812149, loss=1.5619394779205322
I0306 10:48:08.954028 140068907493120 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.22713595628738403, loss=1.558119535446167
I0306 10:48:26.304932 140212307657920 spec.py:321] Evaluating on the training split.
I0306 10:48:28.936829 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:52:33.508779 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 10:52:36.123026 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:55:15.171113 140212307657920 spec.py:349] Evaluating on the test split.
I0306 10:55:17.787684 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 10:57:55.933737 140212307657920 submission_runner.py:469] Time since start: 56692.86s, 	Step: 92550, 	{'train/accuracy': 0.6782653331756592, 'train/loss': 1.4729794263839722, 'train/bleu': 34.232219951615896, 'validation/accuracy': 0.6924208402633667, 'validation/loss': 1.3908897638320923, 'validation/bleu': 30.822220372766996, 'validation/num_examples': 3000, 'test/accuracy': 0.7065925002098083, 'test/loss': 1.292184591293335, 'test/bleu': 30.284079897015598, 'test/num_examples': 3003, 'score': 32799.182674884796, 'total_duration': 56692.86427307129, 'accumulated_submission_time': 32799.182674884796, 'accumulated_eval_time': 23887.56408762932, 'accumulated_logging_time': 0.8943343162536621}
I0306 10:57:55.952577 140068899100416 logging_writer.py:48] [92550] accumulated_eval_time=23887.6, accumulated_logging_time=0.894334, accumulated_submission_time=32799.2, global_step=92550, preemption_count=0, score=32799.2, test/accuracy=0.706593, test/bleu=30.2841, test/loss=1.29218, test/num_examples=3003, total_duration=56692.9, train/accuracy=0.678265, train/bleu=34.2322, train/loss=1.47298, validation/accuracy=0.692421, validation/bleu=30.8222, validation/loss=1.39089, validation/num_examples=3000
I0306 10:58:13.998348 140068907493120 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.21261726319789886, loss=1.510024070739746
I0306 10:58:49.388868 140068899100416 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.21976947784423828, loss=1.4591699838638306
I0306 10:59:24.787685 140068907493120 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.22661226987838745, loss=1.5496773719787598
I0306 11:00:00.177833 140068899100416 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2222577929496765, loss=1.5114446878433228
I0306 11:00:35.574457 140068907493120 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.21954697370529175, loss=1.5284308195114136
I0306 11:01:10.956217 140068899100416 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.24589452147483826, loss=1.5051671266555786
I0306 11:01:46.372859 140068907493120 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.2318720668554306, loss=1.5453969240188599
I0306 11:02:21.780536 140068899100416 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.2196286916732788, loss=1.589389443397522
I0306 11:02:57.200900 140068907493120 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21022918820381165, loss=1.5092930793762207
I0306 11:03:32.600832 140068899100416 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.22246310114860535, loss=1.494342565536499
I0306 11:04:08.020803 140068907493120 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.2230272889137268, loss=1.5547864437103271
I0306 11:04:43.407297 140068899100416 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.20379850268363953, loss=1.5697132349014282
I0306 11:05:18.800350 140068907493120 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.20266638696193695, loss=1.4638315439224243
I0306 11:05:54.232514 140068899100416 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.22088287770748138, loss=1.530064582824707
I0306 11:06:29.649334 140068907493120 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.2129378318786621, loss=1.4785268306732178
I0306 11:07:05.066483 140068899100416 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.22290921211242676, loss=1.6274362802505493
I0306 11:07:40.382133 140068907493120 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.2196519374847412, loss=1.4641287326812744
I0306 11:08:15.690578 140068899100416 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.22786779701709747, loss=1.5280835628509521
I0306 11:08:50.980630 140068907493120 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.21635913848876953, loss=1.515058159828186
I0306 11:09:26.292404 140068899100416 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21797366440296173, loss=1.5364024639129639
I0306 11:10:01.604833 140068907493120 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.21821001172065735, loss=1.5079277753829956
I0306 11:10:36.904733 140068899100416 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.21715429425239563, loss=1.4942522048950195
I0306 11:11:12.212956 140068907493120 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.2100067436695099, loss=1.490067481994629
I0306 11:11:47.527475 140068899100416 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.21587541699409485, loss=1.5017141103744507
I0306 11:11:56.014472 140212307657920 spec.py:321] Evaluating on the training split.
I0306 11:11:58.645687 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 11:16:29.931411 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 11:16:32.548742 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 11:20:11.652546 140212307657920 spec.py:349] Evaluating on the test split.
I0306 11:20:14.268111 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 11:24:24.336509 140212307657920 submission_runner.py:469] Time since start: 58281.27s, 	Step: 94925, 	{'train/accuracy': 0.6925462484359741, 'train/loss': 1.3874192237854004, 'train/bleu': 35.19283306012468, 'validation/accuracy': 0.6925444602966309, 'validation/loss': 1.3840805292129517, 'validation/bleu': 30.026964831872164, 'validation/num_examples': 3000, 'test/accuracy': 0.706210196018219, 'test/loss': 1.2886738777160645, 'test/bleu': 30.027783356627435, 'test/num_examples': 3003, 'score': 33639.1058473587, 'total_duration': 58281.26703095436, 'accumulated_submission_time': 33639.1058473587, 'accumulated_eval_time': 24635.88607430458, 'accumulated_logging_time': 0.9212605953216553}
I0306 11:24:24.356117 140068907493120 logging_writer.py:48] [94925] accumulated_eval_time=24635.9, accumulated_logging_time=0.921261, accumulated_submission_time=33639.1, global_step=94925, preemption_count=0, score=33639.1, test/accuracy=0.70621, test/bleu=30.0278, test/loss=1.28867, test/num_examples=3003, total_duration=58281.3, train/accuracy=0.692546, train/bleu=35.1928, train/loss=1.38742, validation/accuracy=0.692544, validation/bleu=30.027, validation/loss=1.38408, validation/num_examples=3000
I0306 11:24:51.128843 140068899100416 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.21733364462852478, loss=1.5769561529159546
I0306 11:25:26.408444 140068907493120 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22474367916584015, loss=1.5811777114868164
I0306 11:26:01.687094 140068899100416 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.2185877412557602, loss=1.488730788230896
I0306 11:26:36.986390 140068907493120 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.23021987080574036, loss=1.5435702800750732
I0306 11:27:12.303079 140068899100416 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.22564560174942017, loss=1.560484528541565
I0306 11:27:47.609409 140068907493120 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.23689521849155426, loss=1.6262378692626953
I0306 11:28:22.903594 140068899100416 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22193415462970734, loss=1.5568678379058838
I0306 11:28:58.191070 140068907493120 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.22038094699382782, loss=1.527387261390686
I0306 11:29:33.473350 140068899100416 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.2205507755279541, loss=1.514986276626587
I0306 11:30:08.778971 140068907493120 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.22099831700325012, loss=1.555957555770874
I0306 11:30:44.090481 140068899100416 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.21264766156673431, loss=1.4363672733306885
I0306 11:31:19.407755 140068907493120 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.20751695334911346, loss=1.459024429321289
I0306 11:31:54.717676 140068899100416 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.21506397426128387, loss=1.5150436162948608
I0306 11:32:30.040396 140068907493120 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.2321852445602417, loss=1.5344901084899902
I0306 11:33:05.346622 140068899100416 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.22607934474945068, loss=1.462694525718689
I0306 11:33:40.660225 140068907493120 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.22926561534404755, loss=1.552344560623169
I0306 11:34:16.001443 140068899100416 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.22234487533569336, loss=1.481118083000183
I0306 11:34:51.320493 140068907493120 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.22811293601989746, loss=1.5930969715118408
I0306 11:35:26.617748 140068899100416 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.2123989313840866, loss=1.5164541006088257
I0306 11:36:01.943744 140068907493120 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.2449519783258438, loss=1.4325766563415527
I0306 11:36:37.252644 140068899100416 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.22393856942653656, loss=1.5479103326797485
I0306 11:37:12.575712 140068907493120 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.22592613101005554, loss=1.5258742570877075
I0306 11:37:47.890094 140068899100416 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.22743208706378937, loss=1.514106273651123
I0306 11:38:23.215767 140068907493120 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.22848807275295258, loss=1.5014886856079102
I0306 11:38:24.639226 140212307657920 spec.py:321] Evaluating on the training split.
I0306 11:38:27.264034 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 11:42:39.682569 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 11:42:42.300221 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 11:45:58.543692 140212307657920 spec.py:349] Evaluating on the test split.
I0306 11:46:01.174795 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 11:49:10.197367 140212307657920 submission_runner.py:469] Time since start: 59767.13s, 	Step: 97305, 	{'train/accuracy': 0.685263454914093, 'train/loss': 1.4279803037643433, 'train/bleu': 34.81829855470339, 'validation/accuracy': 0.6918893456459045, 'validation/loss': 1.3828574419021606, 'validation/bleu': 30.57669769314072, 'validation/num_examples': 3000, 'test/accuracy': 0.7081798315048218, 'test/loss': 1.281714916229248, 'test/bleu': 30.446962898453915, 'test/num_examples': 3003, 'score': 34479.24953746796, 'total_duration': 59767.127900123596, 'accumulated_submission_time': 34479.24953746796, 'accumulated_eval_time': 25281.44415807724, 'accumulated_logging_time': 0.9491422176361084}
I0306 11:49:10.217023 140068899100416 logging_writer.py:48] [97305] accumulated_eval_time=25281.4, accumulated_logging_time=0.949142, accumulated_submission_time=34479.2, global_step=97305, preemption_count=0, score=34479.2, test/accuracy=0.70818, test/bleu=30.447, test/loss=1.28171, test/num_examples=3003, total_duration=59767.1, train/accuracy=0.685263, train/bleu=34.8183, train/loss=1.42798, validation/accuracy=0.691889, validation/bleu=30.5767, validation/loss=1.38286, validation/num_examples=3000
I0306 11:49:44.074821 140068907493120 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5077148675918579, loss=1.5815913677215576
I0306 11:50:19.367633 140068899100416 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.22531455755233765, loss=1.5437450408935547
I0306 11:50:54.699178 140068907493120 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.21729084849357605, loss=1.5327345132827759
I0306 11:51:30.004863 140068899100416 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.214082270860672, loss=1.4878625869750977
I0306 11:52:05.272632 140068907493120 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.21855182945728302, loss=1.4444773197174072
I0306 11:52:40.555005 140068899100416 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.22299905121326447, loss=1.5046509504318237
I0306 11:53:15.825080 140068907493120 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.22582529485225677, loss=1.5394835472106934
I0306 11:53:51.131510 140068899100416 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.22339953482151031, loss=1.526361346244812
I0306 11:54:26.414613 140068907493120 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.21327362954616547, loss=1.5188158750534058
I0306 11:55:01.679887 140068899100416 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.21605515480041504, loss=1.4649477005004883
I0306 11:55:36.964181 140068907493120 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21378803253173828, loss=1.4932894706726074
I0306 11:56:12.232181 140068899100416 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.21664464473724365, loss=1.5113004446029663
I0306 11:56:47.490235 140068907493120 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.21159112453460693, loss=1.4248089790344238
I0306 11:57:22.781139 140068899100416 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.21722576022148132, loss=1.4410655498504639
I0306 11:57:58.082844 140068907493120 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.2100917547941208, loss=1.4349110126495361
I0306 11:58:33.351709 140068899100416 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.23337914049625397, loss=1.4990640878677368
I0306 11:59:08.665184 140068907493120 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.219783216714859, loss=1.5075362920761108
I0306 11:59:43.952951 140068899100416 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.21969637274742126, loss=1.4835788011550903
I0306 12:00:19.267629 140068907493120 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.21598102152347565, loss=1.5193347930908203
I0306 12:00:54.579134 140068899100416 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21866139769554138, loss=1.5143332481384277
I0306 12:01:29.908945 140068907493120 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.22450833022594452, loss=1.5135526657104492
I0306 12:02:05.187994 140068899100416 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.2282354086637497, loss=1.528195858001709
I0306 12:02:40.503735 140068907493120 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.23225992918014526, loss=1.5460045337677002
I0306 12:03:10.533506 140212307657920 spec.py:321] Evaluating on the training split.
I0306 12:03:13.166164 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 12:07:53.739023 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 12:07:56.359215 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 12:11:08.536537 140212307657920 spec.py:349] Evaluating on the test split.
I0306 12:11:11.152856 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 12:14:00.008288 140212307657920 submission_runner.py:469] Time since start: 61256.94s, 	Step: 99686, 	{'train/accuracy': 0.6867350935935974, 'train/loss': 1.424070119857788, 'train/bleu': 34.27068467528398, 'validation/accuracy': 0.6941512227058411, 'validation/loss': 1.3777809143066406, 'validation/bleu': 30.87112692038172, 'validation/num_examples': 3000, 'test/accuracy': 0.7099988460540771, 'test/loss': 1.274711012840271, 'test/bleu': 30.39367112554895, 'test/num_examples': 3003, 'score': 35319.428525447845, 'total_duration': 61256.93881416321, 'accumulated_submission_time': 35319.428525447845, 'accumulated_eval_time': 25930.918875932693, 'accumulated_logging_time': 0.9772729873657227}
I0306 12:14:00.028179 140068899100416 logging_writer.py:48] [99686] accumulated_eval_time=25930.9, accumulated_logging_time=0.977273, accumulated_submission_time=35319.4, global_step=99686, preemption_count=0, score=35319.4, test/accuracy=0.709999, test/bleu=30.3937, test/loss=1.27471, test/num_examples=3003, total_duration=61256.9, train/accuracy=0.686735, train/bleu=34.2707, train/loss=1.42407, validation/accuracy=0.694151, validation/bleu=30.8711, validation/loss=1.37778, validation/num_examples=3000
I0306 12:14:05.336927 140068907493120 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22605381906032562, loss=1.5455838441848755
I0306 12:14:40.682279 140068899100416 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.21109071373939514, loss=1.4465337991714478
I0306 12:15:15.983702 140068907493120 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.20634910464286804, loss=1.4822465181350708
I0306 12:15:51.309676 140068899100416 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.22738629579544067, loss=1.5188456773757935
I0306 12:16:26.636222 140068907493120 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.2133042961359024, loss=1.5039784908294678
I0306 12:17:01.929072 140068899100416 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.2237195372581482, loss=1.4548618793487549
I0306 12:17:37.249908 140068907493120 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.21779489517211914, loss=1.46171236038208
I0306 12:18:12.627660 140068899100416 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.22987130284309387, loss=1.4909189939498901
I0306 12:18:48.036201 140068907493120 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.24252483248710632, loss=1.473118543624878
I0306 12:19:23.459154 140068899100416 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.22543998062610626, loss=1.4406846761703491
I0306 12:19:58.879065 140068907493120 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.21800857782363892, loss=1.4244791269302368
I0306 12:20:34.302284 140068899100416 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.22431521117687225, loss=1.5129879713058472
I0306 12:21:09.684310 140068907493120 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.2224731594324112, loss=1.481417179107666
I0306 12:21:45.100149 140068899100416 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.22394989430904388, loss=1.5382933616638184
I0306 12:22:20.521268 140068907493120 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.23205122351646423, loss=1.501587152481079
I0306 12:22:55.926681 140068899100416 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.3094739615917206, loss=1.5115598440170288
I0306 12:23:31.340314 140068907493120 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.21709252893924713, loss=1.4169217348098755
I0306 12:24:06.754712 140068899100416 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.2325975000858307, loss=1.5788859128952026
I0306 12:24:42.212926 140068907493120 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.23659609258174896, loss=1.6120173931121826
I0306 12:25:17.610120 140068899100416 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.2214241921901703, loss=1.532008409500122
I0306 12:25:52.998895 140068907493120 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.22963182628154755, loss=1.4861966371536255
I0306 12:26:28.338330 140068899100416 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.22852706909179688, loss=1.4444324970245361
I0306 12:27:03.675333 140068907493120 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.22283773124217987, loss=1.466352105140686
I0306 12:27:39.041596 140068899100416 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.22950050234794617, loss=1.5761613845825195
I0306 12:28:00.236751 140212307657920 spec.py:321] Evaluating on the training split.
I0306 12:28:02.863545 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 12:32:22.875717 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 12:32:25.504603 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 12:35:29.947187 140212307657920 spec.py:349] Evaluating on the test split.
I0306 12:35:32.575726 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 12:38:06.548752 140212307657920 submission_runner.py:469] Time since start: 62703.48s, 	Step: 102061, 	{'train/accuracy': 0.6881170272827148, 'train/loss': 1.4109420776367188, 'train/bleu': 35.044263571116645, 'validation/accuracy': 0.6933602094650269, 'validation/loss': 1.3702937364578247, 'validation/bleu': 30.64121882147161, 'validation/num_examples': 3000, 'test/accuracy': 0.7099409103393555, 'test/loss': 1.2696092128753662, 'test/bleu': 30.55360830788501, 'test/num_examples': 3003, 'score': 36159.49967432022, 'total_duration': 62703.47927594185, 'accumulated_submission_time': 36159.49967432022, 'accumulated_eval_time': 26537.23081469536, 'accumulated_logging_time': 1.0055463314056396}
I0306 12:38:06.568879 140068907493120 logging_writer.py:48] [102061] accumulated_eval_time=26537.2, accumulated_logging_time=1.00555, accumulated_submission_time=36159.5, global_step=102061, preemption_count=0, score=36159.5, test/accuracy=0.709941, test/bleu=30.5536, test/loss=1.26961, test/num_examples=3003, total_duration=62703.5, train/accuracy=0.688117, train/bleu=35.0443, train/loss=1.41094, validation/accuracy=0.69336, validation/bleu=30.6412, validation/loss=1.37029, validation/num_examples=3000
I0306 12:38:20.733953 140068899100416 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.23232819139957428, loss=1.513121247291565
I0306 12:38:56.142614 140068907493120 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.2260797768831253, loss=1.3861782550811768
I0306 12:39:31.523713 140068899100416 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22626593708992004, loss=1.4683343172073364
I0306 12:40:06.917578 140068907493120 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.24395400285720825, loss=1.533861517906189
I0306 12:40:42.319402 140068899100416 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.22212663292884827, loss=1.4651753902435303
I0306 12:41:17.719748 140068907493120 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.23451447486877441, loss=1.4988652467727661
I0306 12:41:53.140332 140068899100416 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.22748489677906036, loss=1.4578337669372559
I0306 12:42:28.554781 140068907493120 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.23597566783428192, loss=1.5331132411956787
I0306 12:43:03.948331 140068899100416 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.22710362076759338, loss=1.541386604309082
I0306 12:43:39.376453 140068907493120 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.22810214757919312, loss=1.4555978775024414
I0306 12:44:14.772374 140068899100416 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.21887175738811493, loss=1.4770748615264893
I0306 12:44:50.181086 140068907493120 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.21745046973228455, loss=1.4539176225662231
I0306 12:45:25.580689 140068899100416 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.22302313148975372, loss=1.4300291538238525
I0306 12:46:00.986113 140068907493120 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.22653110325336456, loss=1.5293010473251343
I0306 12:46:36.414350 140068899100416 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.23888520896434784, loss=1.5408101081848145
I0306 12:47:11.833982 140068907493120 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.23471768200397491, loss=1.587679386138916
I0306 12:47:47.215322 140068899100416 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.23979902267456055, loss=1.551568627357483
I0306 12:48:22.637849 140068907493120 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22981901466846466, loss=1.4754399061203003
I0306 12:48:58.062380 140068899100416 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.25155511498451233, loss=1.5793002843856812
I0306 12:49:33.476494 140068907493120 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.22523528337478638, loss=1.516284465789795
I0306 12:50:08.883649 140068899100416 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.2222377061843872, loss=1.5146046876907349
I0306 12:50:44.283128 140068907493120 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.22585135698318481, loss=1.504012107849121
I0306 12:51:19.647055 140068899100416 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.22953209280967712, loss=1.4596410989761353
I0306 12:51:54.999043 140068907493120 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.22112911939620972, loss=1.429107904434204
I0306 12:52:06.669214 140212307657920 spec.py:321] Evaluating on the training split.
I0306 12:52:09.290678 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 12:56:44.559560 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 12:56:47.177783 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 13:00:04.572440 140212307657920 spec.py:349] Evaluating on the test split.
I0306 13:00:07.188040 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 13:03:09.149276 140212307657920 submission_runner.py:469] Time since start: 64206.08s, 	Step: 104434, 	{'train/accuracy': 0.6902491450309753, 'train/loss': 1.3958348035812378, 'train/bleu': 35.05885941492424, 'validation/accuracy': 0.6941883563995361, 'validation/loss': 1.368359088897705, 'validation/bleu': 30.675479833071535, 'validation/num_examples': 3000, 'test/accuracy': 0.7118178606033325, 'test/loss': 1.2646790742874146, 'test/bleu': 30.641655233846457, 'test/num_examples': 3003, 'score': 36999.465198516846, 'total_duration': 64206.07980656624, 'accumulated_submission_time': 36999.465198516846, 'accumulated_eval_time': 27199.7108168602, 'accumulated_logging_time': 1.0340840816497803}
I0306 13:03:09.168974 140068899100416 logging_writer.py:48] [104434] accumulated_eval_time=27199.7, accumulated_logging_time=1.03408, accumulated_submission_time=36999.5, global_step=104434, preemption_count=0, score=36999.5, test/accuracy=0.711818, test/bleu=30.6417, test/loss=1.26468, test/num_examples=3003, total_duration=64206.1, train/accuracy=0.690249, train/bleu=35.0589, train/loss=1.39583, validation/accuracy=0.694188, validation/bleu=30.6755, validation/loss=1.36836, validation/num_examples=3000
I0306 13:03:32.919402 140068907493120 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.22486598789691925, loss=1.4479330778121948
I0306 13:04:08.297872 140068899100416 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.23368927836418152, loss=1.4789620637893677
I0306 13:04:43.683187 140068907493120 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.2321804314851761, loss=1.4660130739212036
I0306 13:05:19.060962 140068899100416 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.22470009326934814, loss=1.5329718589782715
I0306 13:05:54.424267 140068907493120 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.2261112928390503, loss=1.4598511457443237
I0306 13:06:29.835955 140068899100416 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.229728102684021, loss=1.4736382961273193
I0306 13:07:05.213705 140068907493120 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.21763867139816284, loss=1.4397913217544556
I0306 13:07:40.587124 140068899100416 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.22825361788272858, loss=1.4180465936660767
I0306 13:08:16.001500 140068907493120 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.220542311668396, loss=1.472476840019226
I0306 13:08:51.401382 140068899100416 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.2265513688325882, loss=1.4838781356811523
I0306 13:09:26.829290 140068907493120 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.23295548558235168, loss=1.5536428689956665
I0306 13:10:02.254697 140068899100416 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.2345782220363617, loss=1.4694851636886597
I0306 13:10:37.676237 140068907493120 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.21931156516075134, loss=1.4238957166671753
I0306 13:11:13.069590 140068899100416 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.23474818468093872, loss=1.5050281286239624
I0306 13:11:48.478634 140068907493120 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.22514818608760834, loss=1.4569709300994873
I0306 13:12:23.900269 140068899100416 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.24461276829242706, loss=1.477988362312317
I0306 13:12:59.305115 140068907493120 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.22379936277866364, loss=1.4661552906036377
I0306 13:13:34.706763 140068899100416 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.2217731475830078, loss=1.4093071222305298
I0306 13:14:10.126073 140068907493120 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.22730033099651337, loss=1.4451743364334106
I0306 13:14:45.508975 140068899100416 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.2349357306957245, loss=1.5699708461761475
I0306 13:15:20.913341 140068907493120 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.22048451006412506, loss=1.4705437421798706
I0306 13:15:56.299068 140068899100416 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.4963317811489105, loss=1.4669204950332642
I0306 13:16:31.704818 140068907493120 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.2380228340625763, loss=1.513563871383667
I0306 13:17:07.123288 140068899100416 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.2280157208442688, loss=1.4868831634521484
I0306 13:17:09.258097 140212307657920 spec.py:321] Evaluating on the training split.
I0306 13:17:11.885995 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 13:21:22.179739 140212307657920 spec.py:333] Evaluating on the validation split.
I0306 13:21:24.804236 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 13:24:03.090511 140212307657920 spec.py:349] Evaluating on the test split.
I0306 13:24:05.718873 140212307657920 workload.py:181] Translating evaluation dataset.
I0306 13:26:24.018145 140212307657920 submission_runner.py:469] Time since start: 65600.95s, 	Step: 106807, 	{'train/accuracy': 0.6967753767967224, 'train/loss': 1.3586703538894653, 'train/bleu': 35.84086083569249, 'validation/accuracy': 0.695288360118866, 'validation/loss': 1.36423659324646, 'validation/bleu': 31.043810605436786, 'validation/num_examples': 3000, 'test/accuracy': 0.7109952569007874, 'test/loss': 1.2635352611541748, 'test/bleu': 30.74456091890457, 'test/num_examples': 3003, 'score': 37839.41935944557, 'total_duration': 65600.94868159294, 'accumulated_submission_time': 37839.41935944557, 'accumulated_eval_time': 27754.470810890198, 'accumulated_logging_time': 1.0620830059051514}
I0306 13:26:24.038049 140068907493120 logging_writer.py:48] [106807] accumulated_eval_time=27754.5, accumulated_logging_time=1.06208, accumulated_submission_time=37839.4, global_step=106807, preemption_count=0, score=37839.4, test/accuracy=0.710995, test/bleu=30.7446, test/loss=1.26354, test/num_examples=3003, total_duration=65600.9, train/accuracy=0.696775, train/bleu=35.8409, train/loss=1.35867, validation/accuracy=0.695288, validation/bleu=31.0438, validation/loss=1.36424, validation/num_examples=3000
I0306 13:26:24.058816 140068899100416 logging_writer.py:48] [106807] global_step=106807, preemption_count=0, score=37839.4
I0306 13:26:24.088854 140212307657920 submission_runner.py:646] Tuning trial 5/5
I0306 13:26:24.088966 140212307657920 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0306 13:26:24.091552 140212307657920 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006943100597709417, 'train/loss': 11.047662734985352, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.026718139648438, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.049092292785645, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 39.057475090026855, 'total_duration': 961.0041332244873, 'accumulated_submission_time': 39.057475090026855, 'accumulated_eval_time': 921.9465339183807, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2375, {'train/accuracy': 0.5152161121368408, 'train/loss': 2.8282525539398193, 'train/bleu': 22.492903780599978, 'validation/accuracy': 0.5163276195526123, 'validation/loss': 2.8092308044433594, 'validation/bleu': 18.058623869254703, 'validation/num_examples': 3000, 'test/accuracy': 0.5181207060813904, 'test/loss': 2.8514907360076904, 'test/bleu': 16.871162337151773, 'test/num_examples': 3003, 'score': 879.1771328449249, 'total_duration': 2303.825659275055, 'accumulated_submission_time': 879.1771328449249, 'accumulated_eval_time': 1424.4692523479462, 'accumulated_logging_time': 0.01697993278503418, 'global_step': 2375, 'preemption_count': 0}), (4747, {'train/accuracy': 0.5804626941680908, 'train/loss': 2.2196571826934814, 'train/bleu': 27.14476854415835, 'validation/accuracy': 0.5936271548271179, 'validation/loss': 2.1162376403808594, 'validation/bleu': 23.233468092640987, 'validation/num_examples': 3000, 'test/accuracy': 0.597648024559021, 'test/loss': 2.099470376968384, 'test/bleu': 22.057978389027344, 'test/num_examples': 3003, 'score': 1719.1184754371643, 'total_duration': 3781.73161816597, 'accumulated_submission_time': 1719.1184754371643, 'accumulated_eval_time': 2062.25971698761, 'accumulated_logging_time': 0.034064292907714844, 'global_step': 4747, 'preemption_count': 0}), (7123, {'train/accuracy': 0.6062124967575073, 'train/loss': 1.999056100845337, 'train/bleu': 29.061814986945432, 'validation/accuracy': 0.6180135011672974, 'validation/loss': 1.9137722253799438, 'validation/bleu': 25.37362160859294, 'validation/num_examples': 3000, 'test/accuracy': 0.6240644454956055, 'test/loss': 1.870509147644043, 'test/bleu': 24.115369376860922, 'test/num_examples': 3003, 'score': 2559.144184112549, 'total_duration': 5098.848392724991, 'accumulated_submission_time': 2559.144184112549, 'accumulated_eval_time': 2539.176375389099, 'accumulated_logging_time': 0.05314207077026367, 'global_step': 7123, 'preemption_count': 0}), (9501, {'train/accuracy': 0.6145806312561035, 'train/loss': 1.941043734550476, 'train/bleu': 29.941794157353346, 'validation/accuracy': 0.6330556869506836, 'validation/loss': 1.8025121688842773, 'validation/bleu': 26.34033600151328, 'validation/num_examples': 3000, 'test/accuracy': 0.6381531953811646, 'test/loss': 1.749673843383789, 'test/bleu': 25.449653639054095, 'test/num_examples': 3003, 'score': 3399.178116798401, 'total_duration': 6419.037971735001, 'accumulated_submission_time': 3399.178116798401, 'accumulated_eval_time': 3019.157881498337, 'accumulated_logging_time': 0.07168149948120117, 'global_step': 9501, 'preemption_count': 0}), (11882, {'train/accuracy': 0.6196687817573547, 'train/loss': 1.8990176916122437, 'train/bleu': 30.187858044177897, 'validation/accuracy': 0.6392850875854492, 'validation/loss': 1.7413069009780884, 'validation/bleu': 26.86661004510521, 'validation/num_examples': 3000, 'test/accuracy': 0.6497393250465393, 'test/loss': 1.6813669204711914, 'test/bleu': 25.719035987612042, 'test/num_examples': 3003, 'score': 4239.077433109283, 'total_duration': 7724.524552106857, 'accumulated_submission_time': 4239.077433109283, 'accumulated_eval_time': 3484.58482503891, 'accumulated_logging_time': 0.09088730812072754, 'global_step': 11882, 'preemption_count': 0}), (14258, {'train/accuracy': 0.6268925666809082, 'train/loss': 1.8272523880004883, 'train/bleu': 30.39317097433503, 'validation/accuracy': 0.6446987986564636, 'validation/loss': 1.697914481163025, 'validation/bleu': 27.259572065622542, 'validation/num_examples': 3000, 'test/accuracy': 0.6521492600440979, 'test/loss': 1.6408313512802124, 'test/bleu': 26.0815832816221, 'test/num_examples': 3003, 'score': 5079.01932144165, 'total_duration': 9100.739243507385, 'accumulated_submission_time': 5079.01932144165, 'accumulated_eval_time': 4020.7015478610992, 'accumulated_logging_time': 0.11070585250854492, 'global_step': 14258, 'preemption_count': 0}), (16633, {'train/accuracy': 0.6254225969314575, 'train/loss': 1.8350229263305664, 'train/bleu': 29.973115088127216, 'validation/accuracy': 0.6459594964981079, 'validation/loss': 1.6814725399017334, 'validation/bleu': 26.789727809717796, 'validation/num_examples': 3000, 'test/accuracy': 0.6574209332466125, 'test/loss': 1.614414930343628, 'test/bleu': 26.50551211497311, 'test/num_examples': 3003, 'score': 5918.914885520935, 'total_duration': 10481.96255993843, 'accumulated_submission_time': 5918.914885520935, 'accumulated_eval_time': 4561.873019695282, 'accumulated_logging_time': 0.1305081844329834, 'global_step': 16633, 'preemption_count': 0}), (19005, {'train/accuracy': 0.6518511772155762, 'train/loss': 1.6419318914413452, 'train/bleu': 31.83436671719162, 'validation/accuracy': 0.6497911214828491, 'validation/loss': 1.655680537223816, 'validation/bleu': 27.587279723158062, 'validation/num_examples': 3000, 'test/accuracy': 0.6607577204704285, 'test/loss': 1.5895891189575195, 'test/bleu': 26.79997617457195, 'test/num_examples': 3003, 'score': 6758.8208882808685, 'total_duration': 11763.100868701935, 'accumulated_submission_time': 6758.8208882808685, 'accumulated_eval_time': 5002.95058465004, 'accumulated_logging_time': 0.15057754516601562, 'global_step': 19005, 'preemption_count': 0}), (21369, {'train/accuracy': 0.6354414820671082, 'train/loss': 1.7713111639022827, 'train/bleu': 31.200686184709465, 'validation/accuracy': 0.6550070643424988, 'validation/loss': 1.6310099363327026, 'validation/bleu': 27.8168476649093, 'validation/num_examples': 3000, 'test/accuracy': 0.6656702756881714, 'test/loss': 1.562355399131775, 'test/bleu': 27.06585832127, 'test/num_examples': 3003, 'score': 7598.7966096401215, 'total_duration': 13176.927866697311, 'accumulated_submission_time': 7598.7966096401215, 'accumulated_eval_time': 5576.646045207977, 'accumulated_logging_time': 0.17189979553222656, 'global_step': 21369, 'preemption_count': 0}), (23731, {'train/accuracy': 0.6333337426185608, 'train/loss': 1.78093683719635, 'train/bleu': 31.079124359439554, 'validation/accuracy': 0.6556868553161621, 'validation/loss': 1.6233105659484863, 'validation/bleu': 27.793874029321984, 'validation/num_examples': 3000, 'test/accuracy': 0.6649519205093384, 'test/loss': 1.557727575302124, 'test/bleu': 26.895749148653014, 'test/num_examples': 3003, 'score': 8438.711206912994, 'total_duration': 14615.821152448654, 'accumulated_submission_time': 8438.711206912994, 'accumulated_eval_time': 6175.4720640182495, 'accumulated_logging_time': 0.19208908081054688, 'global_step': 23731, 'preemption_count': 0}), (26094, {'train/accuracy': 0.6397547721862793, 'train/loss': 1.709077000617981, 'train/bleu': 30.908126263162664, 'validation/accuracy': 0.6570340991020203, 'validation/loss': 1.6055611371994019, 'validation/bleu': 27.978284611913267, 'validation/num_examples': 3000, 'test/accuracy': 0.6680454015731812, 'test/loss': 1.538719654083252, 'test/bleu': 26.939348026519493, 'test/num_examples': 3003, 'score': 9278.656661510468, 'total_duration': 15974.705775499344, 'accumulated_submission_time': 9278.656661510468, 'accumulated_eval_time': 6694.25741314888, 'accumulated_logging_time': 0.21350789070129395, 'global_step': 26094, 'preemption_count': 0}), (28456, {'train/accuracy': 0.6388267278671265, 'train/loss': 1.7438158988952637, 'train/bleu': 31.855825170936452, 'validation/accuracy': 0.6595555543899536, 'validation/loss': 1.6053845882415771, 'validation/bleu': 28.255604165039742, 'validation/num_examples': 3000, 'test/accuracy': 0.6694473624229431, 'test/loss': 1.5254756212234497, 'test/bleu': 27.38211647510358, 'test/num_examples': 3003, 'score': 10118.513062000275, 'total_duration': 17287.790566682816, 'accumulated_submission_time': 10118.513062000275, 'accumulated_eval_time': 7167.334541559219, 'accumulated_logging_time': 0.23447227478027344, 'global_step': 28456, 'preemption_count': 0}), (30822, {'train/accuracy': 0.6412522792816162, 'train/loss': 1.7266671657562256, 'train/bleu': 31.18596722785369, 'validation/accuracy': 0.6601241230964661, 'validation/loss': 1.5894215106964111, 'validation/bleu': 26.685167083769347, 'validation/num_examples': 3000, 'test/accuracy': 0.6721584796905518, 'test/loss': 1.515637993812561, 'test/bleu': 27.384455182666894, 'test/num_examples': 3003, 'score': 10958.499197721481, 'total_duration': 18922.270339250565, 'accumulated_submission_time': 10958.499197721481, 'accumulated_eval_time': 7961.676118373871, 'accumulated_logging_time': 0.25522375106811523, 'global_step': 30822, 'preemption_count': 0}), (33197, {'train/accuracy': 0.6480333209037781, 'train/loss': 1.6811453104019165, 'train/bleu': 32.090352114838176, 'validation/accuracy': 0.6626084446907043, 'validation/loss': 1.5778529644012451, 'validation/bleu': 28.40613126178176, 'validation/num_examples': 3000, 'test/accuracy': 0.6720194816589355, 'test/loss': 1.5063328742980957, 'test/bleu': 27.51479866753613, 'test/num_examples': 3003, 'score': 11798.352121591568, 'total_duration': 20323.615604162216, 'accumulated_submission_time': 11798.352121591568, 'accumulated_eval_time': 8523.011108875275, 'accumulated_logging_time': 0.27739477157592773, 'global_step': 33197, 'preemption_count': 0}), (35578, {'train/accuracy': 0.6439980268478394, 'train/loss': 1.7123889923095703, 'train/bleu': 31.733237402899025, 'validation/accuracy': 0.6621758937835693, 'validation/loss': 1.5727108716964722, 'validation/bleu': 28.253470886345244, 'validation/num_examples': 3000, 'test/accuracy': 0.6741744875907898, 'test/loss': 1.4931344985961914, 'test/bleu': 27.698417551604745, 'test/num_examples': 3003, 'score': 12638.300065755844, 'total_duration': 21821.744757413864, 'accumulated_submission_time': 12638.300065755844, 'accumulated_eval_time': 9181.039473295212, 'accumulated_logging_time': 0.30121660232543945, 'global_step': 35578, 'preemption_count': 0}), (37957, {'train/accuracy': 0.6634432077407837, 'train/loss': 1.564178466796875, 'train/bleu': 32.66177202404907, 'validation/accuracy': 0.6647220253944397, 'validation/loss': 1.5614858865737915, 'validation/bleu': 28.896349372408686, 'validation/num_examples': 3000, 'test/accuracy': 0.6754953265190125, 'test/loss': 1.4828883409500122, 'test/bleu': 27.764955148381297, 'test/num_examples': 3003, 'score': 13478.408727169037, 'total_duration': 23220.958114624023, 'accumulated_submission_time': 13478.408727169037, 'accumulated_eval_time': 9739.99025440216, 'accumulated_logging_time': 0.3221702575683594, 'global_step': 37957, 'preemption_count': 0}), (40331, {'train/accuracy': 0.6500967144966125, 'train/loss': 1.6688239574432373, 'train/bleu': 31.690167398975976, 'validation/accuracy': 0.6662052273750305, 'validation/loss': 1.553770899772644, 'validation/bleu': 28.50183298191134, 'validation/num_examples': 3000, 'test/accuracy': 0.6776271462440491, 'test/loss': 1.4742405414581299, 'test/bleu': 27.950927840810635, 'test/num_examples': 3003, 'score': 14318.608921289444, 'total_duration': 24661.99183654785, 'accumulated_submission_time': 14318.608921289444, 'accumulated_eval_time': 10340.669889450073, 'accumulated_logging_time': 0.34351253509521484, 'global_step': 40331, 'preemption_count': 0}), (42704, {'train/accuracy': 0.647441029548645, 'train/loss': 1.6936265230178833, 'train/bleu': 31.69901578903699, 'validation/accuracy': 0.6674906611442566, 'validation/loss': 1.5491188764572144, 'validation/bleu': 28.82032954574636, 'validation/num_examples': 3000, 'test/accuracy': 0.6796084046363831, 'test/loss': 1.4663722515106201, 'test/bleu': 28.378934223926457, 'test/num_examples': 3003, 'score': 15158.48569059372, 'total_duration': 25991.35985803604, 'accumulated_submission_time': 15158.48569059372, 'accumulated_eval_time': 10830.006344556808, 'accumulated_logging_time': 0.36525917053222656, 'global_step': 42704, 'preemption_count': 0}), (45079, {'train/accuracy': 0.6565155982971191, 'train/loss': 1.611972689628601, 'train/bleu': 32.076985214795315, 'validation/accuracy': 0.6656613945960999, 'validation/loss': 1.5430052280426025, 'validation/bleu': 28.755135554479228, 'validation/num_examples': 3000, 'test/accuracy': 0.6801645159721375, 'test/loss': 1.4603431224822998, 'test/bleu': 28.538505871044222, 'test/num_examples': 3003, 'score': 15998.578137397766, 'total_duration': 27335.392095565796, 'accumulated_submission_time': 15998.578137397766, 'accumulated_eval_time': 11333.7951836586, 'accumulated_logging_time': 0.38661861419677734, 'global_step': 45079, 'preemption_count': 0}), (47454, {'train/accuracy': 0.6510302424430847, 'train/loss': 1.6474436521530151, 'train/bleu': 31.691456998677253, 'validation/accuracy': 0.6682199239730835, 'validation/loss': 1.530137300491333, 'validation/bleu': 28.558814169703766, 'validation/num_examples': 3000, 'test/accuracy': 0.6830378770828247, 'test/loss': 1.445305585861206, 'test/bleu': 28.48279242792457, 'test/num_examples': 3003, 'score': 16838.701078653336, 'total_duration': 28856.857549905777, 'accumulated_submission_time': 16838.701078653336, 'accumulated_eval_time': 12014.985574483871, 'accumulated_logging_time': 0.4089033603668213, 'global_step': 47454, 'preemption_count': 0}), (49828, {'train/accuracy': 0.6485689282417297, 'train/loss': 1.6717698574066162, 'train/bleu': 31.90736627337915, 'validation/accuracy': 0.6708278656005859, 'validation/loss': 1.5216649770736694, 'validation/bleu': 28.911351289783067, 'validation/num_examples': 3000, 'test/accuracy': 0.6841617226600647, 'test/loss': 1.4405287504196167, 'test/bleu': 28.726340262530012, 'test/num_examples': 3003, 'score': 17678.878575086594, 'total_duration': 30211.84886264801, 'accumulated_submission_time': 17678.878575086594, 'accumulated_eval_time': 12529.644575119019, 'accumulated_logging_time': 0.43222999572753906, 'global_step': 49828, 'preemption_count': 0}), (52207, {'train/accuracy': 0.6551029086112976, 'train/loss': 1.6269837617874146, 'train/bleu': 32.00939499052922, 'validation/accuracy': 0.6701357364654541, 'validation/loss': 1.5202046632766724, 'validation/bleu': 28.859004666853203, 'validation/num_examples': 3000, 'test/accuracy': 0.6831769347190857, 'test/loss': 1.4323288202285767, 'test/bleu': 28.430479771263705, 'test/num_examples': 3003, 'score': 18518.817111730576, 'total_duration': 31601.790792942047, 'accumulated_submission_time': 18518.817111730576, 'accumulated_eval_time': 13079.492374420166, 'accumulated_logging_time': 0.45488524436950684, 'global_step': 52207, 'preemption_count': 0}), (54589, {'train/accuracy': 0.6548044681549072, 'train/loss': 1.6401519775390625, 'train/bleu': 32.02586460235268, 'validation/accuracy': 0.671940267086029, 'validation/loss': 1.5146325826644897, 'validation/bleu': 29.08512569436522, 'validation/num_examples': 3000, 'test/accuracy': 0.6848453283309937, 'test/loss': 1.4275223016738892, 'test/bleu': 28.42736669655433, 'test/num_examples': 3003, 'score': 19358.822951555252, 'total_duration': 33010.76670432091, 'accumulated_submission_time': 19358.822951555252, 'accumulated_eval_time': 13648.30570936203, 'accumulated_logging_time': 0.48189401626586914, 'global_step': 54589, 'preemption_count': 0}), (56969, {'train/accuracy': 0.6689996123313904, 'train/loss': 1.5259759426116943, 'train/bleu': 33.03018508121132, 'validation/accuracy': 0.6735965609550476, 'validation/loss': 1.5059330463409424, 'validation/bleu': 28.974533009461226, 'validation/num_examples': 3000, 'test/accuracy': 0.6851465702056885, 'test/loss': 1.4180183410644531, 'test/bleu': 28.619870151252766, 'test/num_examples': 3003, 'score': 20198.696849822998, 'total_duration': 34434.67561483383, 'accumulated_submission_time': 20198.696849822998, 'accumulated_eval_time': 14232.18794798851, 'accumulated_logging_time': 0.5070185661315918, 'global_step': 56969, 'preemption_count': 0}), (59344, {'train/accuracy': 0.6552916169166565, 'train/loss': 1.6171928644180298, 'train/bleu': 32.23854304887016, 'validation/accuracy': 0.6758337020874023, 'validation/loss': 1.493086576461792, 'validation/bleu': 29.244896091433304, 'validation/num_examples': 3000, 'test/accuracy': 0.6894218325614929, 'test/loss': 1.4055259227752686, 'test/bleu': 28.838429229625458, 'test/num_examples': 3003, 'score': 21038.860293149948, 'total_duration': 35909.1481013298, 'accumulated_submission_time': 21038.860293149948, 'accumulated_eval_time': 14866.34739613533, 'accumulated_logging_time': 0.5300712585449219, 'global_step': 59344, 'preemption_count': 0}), (61718, {'train/accuracy': 0.6550386548042297, 'train/loss': 1.6222668886184692, 'train/bleu': 32.55484795016553, 'validation/accuracy': 0.6756482720375061, 'validation/loss': 1.4868499040603638, 'validation/bleu': 29.190480798550205, 'validation/num_examples': 3000, 'test/accuracy': 0.686977207660675, 'test/loss': 1.4051361083984375, 'test/bleu': 28.715630456715825, 'test/num_examples': 3003, 'score': 21878.960909605026, 'total_duration': 37304.69179344177, 'accumulated_submission_time': 21878.960909605026, 'accumulated_eval_time': 15421.636085510254, 'accumulated_logging_time': 0.5542142391204834, 'global_step': 61718, 'preemption_count': 0}), (64092, {'train/accuracy': 0.6666781902313232, 'train/loss': 1.5514503717422485, 'train/bleu': 32.91918021189076, 'validation/accuracy': 0.6763404607772827, 'validation/loss': 1.481407642364502, 'validation/bleu': 29.467292144743723, 'validation/num_examples': 3000, 'test/accuracy': 0.6918549537658691, 'test/loss': 1.3882687091827393, 'test/bleu': 29.156801779212287, 'test/num_examples': 3003, 'score': 22718.87263226509, 'total_duration': 38833.28619623184, 'accumulated_submission_time': 22718.87263226509, 'accumulated_eval_time': 16110.162761211395, 'accumulated_logging_time': 0.578355073928833, 'global_step': 64092, 'preemption_count': 0}), (66466, {'train/accuracy': 0.6642294526100159, 'train/loss': 1.5680408477783203, 'train/bleu': 32.673107220138746, 'validation/accuracy': 0.6779472827911377, 'validation/loss': 1.4741722345352173, 'validation/bleu': 29.43328690697816, 'validation/num_examples': 3000, 'test/accuracy': 0.693627655506134, 'test/loss': 1.3810653686523438, 'test/bleu': 29.14719277040466, 'test/num_examples': 3003, 'score': 23558.94005560875, 'total_duration': 40262.97049665451, 'accumulated_submission_time': 23558.94005560875, 'accumulated_eval_time': 16699.626826047897, 'accumulated_logging_time': 0.6023757457733154, 'global_step': 66466, 'preemption_count': 0}), (68839, {'train/accuracy': 0.6594803929328918, 'train/loss': 1.6030139923095703, 'train/bleu': 32.515804644469455, 'validation/accuracy': 0.6791585683822632, 'validation/loss': 1.4622377157211304, 'validation/bleu': 29.559927351146165, 'validation/num_examples': 3000, 'test/accuracy': 0.6944155097007751, 'test/loss': 1.3685837984085083, 'test/bleu': 29.490603562303292, 'test/num_examples': 3003, 'score': 24398.943358659744, 'total_duration': 41672.940413713455, 'accumulated_submission_time': 24398.943358659744, 'accumulated_eval_time': 17269.438121318817, 'accumulated_logging_time': 0.6271536350250244, 'global_step': 68839, 'preemption_count': 0}), (71213, {'train/accuracy': 0.6687206625938416, 'train/loss': 1.5341448783874512, 'train/bleu': 33.19223553033539, 'validation/accuracy': 0.6823598146438599, 'validation/loss': 1.4484879970550537, 'validation/bleu': 29.65569232106284, 'validation/num_examples': 3000, 'test/accuracy': 0.6948789358139038, 'test/loss': 1.362641453742981, 'test/bleu': 29.401666866615503, 'test/num_examples': 3003, 'score': 25239.074939727783, 'total_duration': 43247.263063430786, 'accumulated_submission_time': 25239.074939727783, 'accumulated_eval_time': 18003.477901935577, 'accumulated_logging_time': 0.6511471271514893, 'global_step': 71213, 'preemption_count': 0}), (73579, {'train/accuracy': 0.6632655262947083, 'train/loss': 1.568823218345642, 'train/bleu': 33.0778286724886, 'validation/accuracy': 0.6829036474227905, 'validation/loss': 1.4487764835357666, 'validation/bleu': 29.829407099608986, 'validation/num_examples': 3000, 'test/accuracy': 0.6966516375541687, 'test/loss': 1.3561928272247314, 'test/bleu': 29.814798412250695, 'test/num_examples': 3003, 'score': 26079.187994003296, 'total_duration': 44630.74330019951, 'accumulated_submission_time': 26079.187994003296, 'accumulated_eval_time': 18546.691540956497, 'accumulated_logging_time': 0.6764729022979736, 'global_step': 73579, 'preemption_count': 0}), (75944, {'train/accuracy': 0.675548255443573, 'train/loss': 1.4768906831741333, 'train/bleu': 34.2841374195302, 'validation/accuracy': 0.6844115257263184, 'validation/loss': 1.4405282735824585, 'validation/bleu': 30.479519921572283, 'validation/num_examples': 3000, 'test/accuracy': 0.6983779668807983, 'test/loss': 1.3479682207107544, 'test/bleu': 29.42071740690849, 'test/num_examples': 3003, 'score': 26919.094887018204, 'total_duration': 46014.17180156708, 'accumulated_submission_time': 26919.094887018204, 'accumulated_eval_time': 19090.05850839615, 'accumulated_logging_time': 0.7037150859832764, 'global_step': 75944, 'preemption_count': 0}), (78313, {'train/accuracy': 0.6693249940872192, 'train/loss': 1.521856427192688, 'train/bleu': 33.909759531176604, 'validation/accuracy': 0.6844239234924316, 'validation/loss': 1.4351258277893066, 'validation/bleu': 29.979010461316015, 'validation/num_examples': 3000, 'test/accuracy': 0.6992700695991516, 'test/loss': 1.3383339643478394, 'test/bleu': 29.69807100837037, 'test/num_examples': 3003, 'score': 27759.19494485855, 'total_duration': 47498.129028081894, 'accumulated_submission_time': 27759.19494485855, 'accumulated_eval_time': 19733.758798122406, 'accumulated_logging_time': 0.7311797142028809, 'global_step': 78313, 'preemption_count': 0}), (80682, {'train/accuracy': 0.668994128704071, 'train/loss': 1.535681128501892, 'train/bleu': 33.428632903543345, 'validation/accuracy': 0.6851531267166138, 'validation/loss': 1.4262844324111938, 'validation/bleu': 29.94134797835924, 'validation/num_examples': 3000, 'test/accuracy': 0.6996524333953857, 'test/loss': 1.3295632600784302, 'test/bleu': 29.765955901502444, 'test/num_examples': 3003, 'score': 28599.22697019577, 'total_duration': 48875.79549431801, 'accumulated_submission_time': 28599.22697019577, 'accumulated_eval_time': 20271.23972916603, 'accumulated_logging_time': 0.7588772773742676, 'global_step': 80682, 'preemption_count': 0}), (83054, {'train/accuracy': 0.6757895946502686, 'train/loss': 1.4848592281341553, 'train/bleu': 34.272951930540636, 'validation/accuracy': 0.6879218220710754, 'validation/loss': 1.4152507781982422, 'validation/bleu': 30.203432645750027, 'validation/num_examples': 3000, 'test/accuracy': 0.7012860774993896, 'test/loss': 1.3231533765792847, 'test/bleu': 29.945020352973515, 'test/num_examples': 3003, 'score': 29439.155190467834, 'total_duration': 50364.20569181442, 'accumulated_submission_time': 29439.155190467834, 'accumulated_eval_time': 20919.568329572678, 'accumulated_logging_time': 0.7854514122009277, 'global_step': 83054, 'preemption_count': 0}), (85428, {'train/accuracy': 0.6747319102287292, 'train/loss': 1.496392011642456, 'train/bleu': 33.8617690708556, 'validation/accuracy': 0.6870813369750977, 'validation/loss': 1.4138641357421875, 'validation/bleu': 29.384605890812352, 'validation/num_examples': 3000, 'test/accuracy': 0.7026764154434204, 'test/loss': 1.3169686794281006, 'test/bleu': 29.960615040979388, 'test/num_examples': 3003, 'score': 30279.161264896393, 'total_duration': 52007.76891565323, 'accumulated_submission_time': 30279.161264896393, 'accumulated_eval_time': 21722.970227003098, 'accumulated_logging_time': 0.8118948936462402, 'global_step': 85428, 'preemption_count': 0}), (87802, {'train/accuracy': 0.6762893795967102, 'train/loss': 1.489400029182434, 'train/bleu': 34.030359888003574, 'validation/accuracy': 0.6888982653617859, 'validation/loss': 1.4010565280914307, 'validation/bleu': 30.398251631579015, 'validation/num_examples': 3000, 'test/accuracy': 0.7041130661964417, 'test/loss': 1.306091547012329, 'test/bleu': 29.98516937652068, 'test/num_examples': 3003, 'score': 31119.351791620255, 'total_duration': 53607.5272526741, 'accumulated_submission_time': 31119.351791620255, 'accumulated_eval_time': 22482.375999689102, 'accumulated_logging_time': 0.8386707305908203, 'global_step': 87802, 'preemption_count': 0}), (90177, {'train/accuracy': 0.6828116178512573, 'train/loss': 1.446681261062622, 'train/bleu': 34.69519178939945, 'validation/accuracy': 0.6897140145301819, 'validation/loss': 1.3971505165100098, 'validation/bleu': 29.603136007416598, 'validation/num_examples': 3000, 'test/accuracy': 0.7053296566009521, 'test/loss': 1.302403211593628, 'test/bleu': 30.127322176394287, 'test/num_examples': 3003, 'score': 31959.29849600792, 'total_duration': 55283.19348669052, 'accumulated_submission_time': 31959.29849600792, 'accumulated_eval_time': 23317.93534207344, 'accumulated_logging_time': 0.8673756122589111, 'global_step': 90177, 'preemption_count': 0}), (92550, {'train/accuracy': 0.6782653331756592, 'train/loss': 1.4729794263839722, 'train/bleu': 34.232219951615896, 'validation/accuracy': 0.6924208402633667, 'validation/loss': 1.3908897638320923, 'validation/bleu': 30.822220372766996, 'validation/num_examples': 3000, 'test/accuracy': 0.7065925002098083, 'test/loss': 1.292184591293335, 'test/bleu': 30.284079897015598, 'test/num_examples': 3003, 'score': 32799.182674884796, 'total_duration': 56692.86427307129, 'accumulated_submission_time': 32799.182674884796, 'accumulated_eval_time': 23887.56408762932, 'accumulated_logging_time': 0.8943343162536621, 'global_step': 92550, 'preemption_count': 0}), (94925, {'train/accuracy': 0.6925462484359741, 'train/loss': 1.3874192237854004, 'train/bleu': 35.19283306012468, 'validation/accuracy': 0.6925444602966309, 'validation/loss': 1.3840805292129517, 'validation/bleu': 30.026964831872164, 'validation/num_examples': 3000, 'test/accuracy': 0.706210196018219, 'test/loss': 1.2886738777160645, 'test/bleu': 30.027783356627435, 'test/num_examples': 3003, 'score': 33639.1058473587, 'total_duration': 58281.26703095436, 'accumulated_submission_time': 33639.1058473587, 'accumulated_eval_time': 24635.88607430458, 'accumulated_logging_time': 0.9212605953216553, 'global_step': 94925, 'preemption_count': 0}), (97305, {'train/accuracy': 0.685263454914093, 'train/loss': 1.4279803037643433, 'train/bleu': 34.81829855470339, 'validation/accuracy': 0.6918893456459045, 'validation/loss': 1.3828574419021606, 'validation/bleu': 30.57669769314072, 'validation/num_examples': 3000, 'test/accuracy': 0.7081798315048218, 'test/loss': 1.281714916229248, 'test/bleu': 30.446962898453915, 'test/num_examples': 3003, 'score': 34479.24953746796, 'total_duration': 59767.127900123596, 'accumulated_submission_time': 34479.24953746796, 'accumulated_eval_time': 25281.44415807724, 'accumulated_logging_time': 0.9491422176361084, 'global_step': 97305, 'preemption_count': 0}), (99686, {'train/accuracy': 0.6867350935935974, 'train/loss': 1.424070119857788, 'train/bleu': 34.27068467528398, 'validation/accuracy': 0.6941512227058411, 'validation/loss': 1.3777809143066406, 'validation/bleu': 30.87112692038172, 'validation/num_examples': 3000, 'test/accuracy': 0.7099988460540771, 'test/loss': 1.274711012840271, 'test/bleu': 30.39367112554895, 'test/num_examples': 3003, 'score': 35319.428525447845, 'total_duration': 61256.93881416321, 'accumulated_submission_time': 35319.428525447845, 'accumulated_eval_time': 25930.918875932693, 'accumulated_logging_time': 0.9772729873657227, 'global_step': 99686, 'preemption_count': 0}), (102061, {'train/accuracy': 0.6881170272827148, 'train/loss': 1.4109420776367188, 'train/bleu': 35.044263571116645, 'validation/accuracy': 0.6933602094650269, 'validation/loss': 1.3702937364578247, 'validation/bleu': 30.64121882147161, 'validation/num_examples': 3000, 'test/accuracy': 0.7099409103393555, 'test/loss': 1.2696092128753662, 'test/bleu': 30.55360830788501, 'test/num_examples': 3003, 'score': 36159.49967432022, 'total_duration': 62703.47927594185, 'accumulated_submission_time': 36159.49967432022, 'accumulated_eval_time': 26537.23081469536, 'accumulated_logging_time': 1.0055463314056396, 'global_step': 102061, 'preemption_count': 0}), (104434, {'train/accuracy': 0.6902491450309753, 'train/loss': 1.3958348035812378, 'train/bleu': 35.05885941492424, 'validation/accuracy': 0.6941883563995361, 'validation/loss': 1.368359088897705, 'validation/bleu': 30.675479833071535, 'validation/num_examples': 3000, 'test/accuracy': 0.7118178606033325, 'test/loss': 1.2646790742874146, 'test/bleu': 30.641655233846457, 'test/num_examples': 3003, 'score': 36999.465198516846, 'total_duration': 64206.07980656624, 'accumulated_submission_time': 36999.465198516846, 'accumulated_eval_time': 27199.7108168602, 'accumulated_logging_time': 1.0340840816497803, 'global_step': 104434, 'preemption_count': 0}), (106807, {'train/accuracy': 0.6967753767967224, 'train/loss': 1.3586703538894653, 'train/bleu': 35.84086083569249, 'validation/accuracy': 0.695288360118866, 'validation/loss': 1.36423659324646, 'validation/bleu': 31.043810605436786, 'validation/num_examples': 3000, 'test/accuracy': 0.7109952569007874, 'test/loss': 1.2635352611541748, 'test/bleu': 30.74456091890457, 'test/num_examples': 3003, 'score': 37839.41935944557, 'total_duration': 65600.94868159294, 'accumulated_submission_time': 37839.41935944557, 'accumulated_eval_time': 27754.470810890198, 'accumulated_logging_time': 1.0620830059051514, 'global_step': 106807, 'preemption_count': 0})], 'global_step': 106807}
I0306 13:26:24.091696 140212307657920 submission_runner.py:649] Timing: 37839.41935944557
I0306 13:26:24.091739 140212307657920 submission_runner.py:651] Total number of evals: 46
I0306 13:26:24.091772 140212307657920 submission_runner.py:652] ====================
I0306 13:26:24.091897 140212307657920 submission_runner.py:750] Final wmt score: 4
