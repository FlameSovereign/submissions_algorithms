python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-1181127603 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/ogbg_jax_03-07-2025-16-37-37.log
2025-03-07 16:37:56.052275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741365476.813349       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741365476.938541       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:38:49.546695 140681127527616 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax.
I0307 16:38:52.290034 140681127527616 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:38:52.293168 140681127527616 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:38:52.310019 140681127527616 submission_runner.py:606] Using RNG seed -1181127603
I0307 16:38:55.707222 140681127527616 submission_runner.py:615] --- Tuning run 2/5 ---
I0307 16:38:55.707417 140681127527616 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_2.
I0307 16:38:55.707601 140681127527616 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_2/hparams.json.
I0307 16:38:55.965347 140681127527616 submission_runner.py:218] Initializing dataset.
I0307 16:38:57.613517 140681127527616 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:38:57.636269 140681127527616 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0307 16:38:58.697261 140681127527616 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0307 16:38:58.852222 140681127527616 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:38:59.193268 140681127527616 submission_runner.py:229] Initializing model.
I0307 16:39:07.694705 140681127527616 submission_runner.py:272] Initializing optimizer.
I0307 16:39:08.123709 140681127527616 submission_runner.py:279] Initializing metrics bundle.
I0307 16:39:08.123956 140681127527616 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:39:08.124727 140681127527616 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_2 with prefix checkpoint_
I0307 16:39:08.124846 140681127527616 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_2/meta_data_0.json.
I0307 16:39:08.125021 140681127527616 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:39:08.125091 140681127527616 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:39:08.550425 140681127527616 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_2/flags_0.json.
I0307 16:39:08.879531 140681127527616 submission_runner.py:337] Starting training loop.
I0307 16:39:22.694324 140544984045312 logging_writer.py:48] [0] global_step=0, grad_norm=2.7589685916900635, loss=0.7204422950744629
I0307 16:39:22.889266 140681127527616 spec.py:321] Evaluating on the training split.
I0307 16:39:22.892776 140681127527616 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:39:22.896484 140681127527616 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:39:22.957783 140681127527616 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:40:43.865860 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 16:40:43.868441 140681127527616 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:40:43.872231 140681127527616 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:40:43.932786 140681127527616 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:41:50.766997 140681127527616 spec.py:349] Evaluating on the test split.
I0307 16:41:50.769560 140681127527616 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:41:50.773129 140681127527616 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:41:50.836316 140681127527616 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:42:58.568914 140681127527616 submission_runner.py:469] Time since start: 229.69s, 	Step: 1, 	{'train/accuracy': 0.5716273188591003, 'train/loss': 0.7195250391960144, 'train/mean_average_precision': 0.022255541417572446, 'validation/accuracy': 0.5684533715248108, 'validation/loss': 0.7225921154022217, 'validation/mean_average_precision': 0.0271315223376292, 'validation/num_examples': 43793, 'test/accuracy': 0.5673698782920837, 'test/loss': 0.7226720452308655, 'test/mean_average_precision': 0.02830521083488609, 'test/num_examples': 43793, 'score': 14.009626865386963, 'total_duration': 229.68927121162415, 'accumulated_submission_time': 14.009626865386963, 'accumulated_eval_time': 215.6795220375061, 'accumulated_logging_time': 0}
I0307 16:42:58.576512 140539315664640 logging_writer.py:48] [1] accumulated_eval_time=215.68, accumulated_logging_time=0, accumulated_submission_time=14.0096, global_step=1, preemption_count=0, score=14.0096, test/accuracy=0.56737, test/loss=0.722672, test/mean_average_precision=0.0283052, test/num_examples=43793, total_duration=229.689, train/accuracy=0.571627, train/loss=0.719525, train/mean_average_precision=0.0222555, validation/accuracy=0.568453, validation/loss=0.722592, validation/mean_average_precision=0.0271315, validation/num_examples=43793
I0307 16:43:20.800665 140539324057344 logging_writer.py:48] [100] global_step=100, grad_norm=0.4243624210357666, loss=0.37964579463005066
I0307 16:43:43.094168 140539315664640 logging_writer.py:48] [200] global_step=200, grad_norm=0.30159851908683777, loss=0.26136907935142517
I0307 16:44:05.493079 140539324057344 logging_writer.py:48] [300] global_step=300, grad_norm=0.1788652390241623, loss=0.15499447286128998
I0307 16:44:27.796325 140539315664640 logging_writer.py:48] [400] global_step=400, grad_norm=0.09882980585098267, loss=0.10084529221057892
I0307 16:44:50.401330 140539324057344 logging_writer.py:48] [500] global_step=500, grad_norm=0.06176292523741722, loss=0.07546679675579071
I0307 16:45:12.825587 140539315664640 logging_writer.py:48] [600] global_step=600, grad_norm=0.09119164943695068, loss=0.06164444983005524
I0307 16:45:34.985284 140539869484800 logging_writer.py:48] [700] global_step=700, grad_norm=0.07248619198799133, loss=0.05921722203493118
I0307 16:45:57.052571 140539861092096 logging_writer.py:48] [800] global_step=800, grad_norm=0.03172104060649872, loss=0.05850118398666382
I0307 16:46:18.683075 140539869484800 logging_writer.py:48] [900] global_step=900, grad_norm=0.036049045622348785, loss=0.05705166608095169
I0307 16:46:40.890535 140539861092096 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.02406769059598446, loss=0.051454633474349976
I0307 16:46:58.640974 140681127527616 spec.py:321] Evaluating on the training split.
I0307 16:48:16.903159 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 16:48:18.944922 140681127527616 spec.py:349] Evaluating on the test split.
I0307 16:48:21.006001 140681127527616 submission_runner.py:469] Time since start: 552.13s, 	Step: 1082, 	{'train/accuracy': 0.986789882183075, 'train/loss': 0.05068320408463478, 'train/mean_average_precision': 0.06520019054850393, 'validation/accuracy': 0.9842275381088257, 'validation/loss': 0.05962624028325081, 'validation/mean_average_precision': 0.07007967402926334, 'validation/num_examples': 43793, 'test/accuracy': 0.9832583069801331, 'test/loss': 0.06295129656791687, 'test/mean_average_precision': 0.06607542075288701, 'test/num_examples': 43793, 'score': 254.0324490070343, 'total_duration': 552.1264216899872, 'accumulated_submission_time': 254.0324490070343, 'accumulated_eval_time': 298.04449582099915, 'accumulated_logging_time': 0.017806291580200195}
I0307 16:48:21.016350 140539869484800 logging_writer.py:48] [1082] accumulated_eval_time=298.044, accumulated_logging_time=0.0178063, accumulated_submission_time=254.032, global_step=1082, preemption_count=0, score=254.032, test/accuracy=0.983258, test/loss=0.0629513, test/mean_average_precision=0.0660754, test/num_examples=43793, total_duration=552.126, train/accuracy=0.98679, train/loss=0.0506832, train/mean_average_precision=0.0652002, validation/accuracy=0.984228, validation/loss=0.0596262, validation/mean_average_precision=0.0700797, validation/num_examples=43793
I0307 16:48:25.095939 140539861092096 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.045213352888822556, loss=0.05110105499625206
I0307 16:48:46.933919 140539869484800 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.017078956589102745, loss=0.052290961146354675
I0307 16:49:08.854189 140539861092096 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.04790523648262024, loss=0.05589059740304947
I0307 16:49:30.561680 140539869484800 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.03769344463944435, loss=0.05052642151713371
I0307 16:49:52.530557 140539861092096 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0459691621363163, loss=0.047166407108306885
I0307 16:50:14.299196 140539869484800 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.06200235337018967, loss=0.04460495710372925
I0307 16:50:36.014798 140539861092096 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.03899478539824486, loss=0.04641665890812874
I0307 16:50:57.629350 140539869484800 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.05983637273311615, loss=0.047322481870651245
I0307 16:51:19.319358 140539861092096 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.06189798563718796, loss=0.04853714630007744
I0307 16:51:41.247339 140539869484800 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.027828337624669075, loss=0.04739151895046234
I0307 16:52:03.532147 140539861092096 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.027981506660580635, loss=0.04785721004009247
I0307 16:52:21.169505 140681127527616 spec.py:321] Evaluating on the training split.
I0307 16:53:37.060513 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 16:53:39.117221 140681127527616 spec.py:349] Evaluating on the test split.
I0307 16:53:41.119060 140681127527616 submission_runner.py:469] Time since start: 872.24s, 	Step: 2182, 	{'train/accuracy': 0.9873363971710205, 'train/loss': 0.045514464378356934, 'train/mean_average_precision': 0.13427337334202727, 'validation/accuracy': 0.9846927523612976, 'validation/loss': 0.054520756006240845, 'validation/mean_average_precision': 0.12973835427495983, 'validation/num_examples': 43793, 'test/accuracy': 0.9836757183074951, 'test/loss': 0.05759686976671219, 'test/mean_average_precision': 0.13045619218234186, 'test/num_examples': 43793, 'score': 494.1472702026367, 'total_duration': 872.2394938468933, 'accumulated_submission_time': 494.1472702026367, 'accumulated_eval_time': 377.9940023422241, 'accumulated_logging_time': 0.03851509094238281}
I0307 16:53:41.128932 140539869484800 logging_writer.py:48] [2182] accumulated_eval_time=377.994, accumulated_logging_time=0.0385151, accumulated_submission_time=494.147, global_step=2182, preemption_count=0, score=494.147, test/accuracy=0.983676, test/loss=0.0575969, test/mean_average_precision=0.130456, test/num_examples=43793, total_duration=872.239, train/accuracy=0.987336, train/loss=0.0455145, train/mean_average_precision=0.134273, validation/accuracy=0.984693, validation/loss=0.0545208, validation/mean_average_precision=0.129738, validation/num_examples=43793
I0307 16:53:45.382131 140539861092096 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.03334673494100571, loss=0.05121464654803276
I0307 16:54:06.877605 140539869484800 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.02532196417450905, loss=0.04560978710651398
I0307 16:54:28.641841 140539861092096 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.020609458908438683, loss=0.048138681799173355
I0307 16:54:50.768160 140539869484800 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.04989881068468094, loss=0.04508965462446213
I0307 16:55:12.388311 140539861092096 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.032373275607824326, loss=0.046851594001054764
I0307 16:55:34.069643 140539869484800 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.02978278137743473, loss=0.05030226707458496
I0307 16:55:55.540977 140539861092096 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.07360012829303741, loss=0.04599202796816826
I0307 16:56:16.993844 140539869484800 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.04942471161484718, loss=0.04965750500559807
I0307 16:56:38.552830 140539861092096 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.04440228268504143, loss=0.050539594143629074
I0307 16:57:00.032489 140539869484800 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.020938068628311157, loss=0.04408673942089081
I0307 16:57:21.426801 140539861092096 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0456230454146862, loss=0.04849870502948761
I0307 16:57:41.126968 140681127527616 spec.py:321] Evaluating on the training split.
I0307 16:58:56.053191 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 16:58:58.082321 140681127527616 spec.py:349] Evaluating on the test split.
I0307 16:59:00.072434 140681127527616 submission_runner.py:469] Time since start: 1191.19s, 	Step: 3293, 	{'train/accuracy': 0.988227903842926, 'train/loss': 0.04125930368900299, 'train/mean_average_precision': 0.19321183888667243, 'validation/accuracy': 0.9853317141532898, 'validation/loss': 0.050942905247211456, 'validation/mean_average_precision': 0.16239212056515664, 'validation/num_examples': 43793, 'test/accuracy': 0.9843505024909973, 'test/loss': 0.05353173986077309, 'test/mean_average_precision': 0.15985406621486176, 'test/num_examples': 43793, 'score': 734.105954170227, 'total_duration': 1191.1928668022156, 'accumulated_submission_time': 734.105954170227, 'accumulated_eval_time': 456.9394247531891, 'accumulated_logging_time': 0.05840802192687988}
I0307 16:59:00.081549 140539869484800 logging_writer.py:48] [3293] accumulated_eval_time=456.939, accumulated_logging_time=0.058408, accumulated_submission_time=734.106, global_step=3293, preemption_count=0, score=734.106, test/accuracy=0.984351, test/loss=0.0535317, test/mean_average_precision=0.159854, test/num_examples=43793, total_duration=1191.19, train/accuracy=0.988228, train/loss=0.0412593, train/mean_average_precision=0.193212, validation/accuracy=0.985332, validation/loss=0.0509429, validation/mean_average_precision=0.162392, validation/num_examples=43793
I0307 16:59:01.782322 140539861092096 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.017102709040045738, loss=0.04917285963892937
I0307 16:59:23.301909 140539869484800 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.026492087170481682, loss=0.04676453396677971
I0307 16:59:44.874398 140539861092096 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.05378986895084381, loss=0.04546719416975975
I0307 17:00:06.603040 140539869484800 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03324919193983078, loss=0.048042938113212585
I0307 17:00:28.267963 140539861092096 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.018669193610548973, loss=0.04522579535841942
I0307 17:00:49.712540 140539869484800 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.019153183326125145, loss=0.0445636548101902
I0307 17:01:11.226037 140539861092096 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.028185633942484856, loss=0.04273432493209839
I0307 17:01:32.554168 140539869484800 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.023447565734386444, loss=0.0483773835003376
I0307 17:01:54.085474 140539861092096 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.026467466726899147, loss=0.043879419565200806
I0307 17:02:15.959631 140539869484800 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.016260741278529167, loss=0.04316265881061554
I0307 17:02:37.218359 140539861092096 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.02295750007033348, loss=0.043084435164928436
I0307 17:02:58.796948 140539869484800 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.020784366875886917, loss=0.041672468185424805
I0307 17:03:00.110214 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:04:16.389875 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:04:18.426518 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:04:20.390497 140681127527616 submission_runner.py:469] Time since start: 1511.51s, 	Step: 4407, 	{'train/accuracy': 0.9883833527565002, 'train/loss': 0.03985724598169327, 'train/mean_average_precision': 0.2151241399838424, 'validation/accuracy': 0.9855647087097168, 'validation/loss': 0.04890182986855507, 'validation/mean_average_precision': 0.18830423295859752, 'validation/num_examples': 43793, 'test/accuracy': 0.9846385717391968, 'test/loss': 0.05160592496395111, 'test/mean_average_precision': 0.18754837387013282, 'test/num_examples': 43793, 'score': 974.0946433544159, 'total_duration': 1511.5109298229218, 'accumulated_submission_time': 974.0946433544159, 'accumulated_eval_time': 537.2196614742279, 'accumulated_logging_time': 0.07704663276672363}
I0307 17:04:20.462103 140539379742464 logging_writer.py:48] [4407] accumulated_eval_time=537.22, accumulated_logging_time=0.0770466, accumulated_submission_time=974.095, global_step=4407, preemption_count=0, score=974.095, test/accuracy=0.984639, test/loss=0.0516059, test/mean_average_precision=0.187548, test/num_examples=43793, total_duration=1511.51, train/accuracy=0.988383, train/loss=0.0398572, train/mean_average_precision=0.215124, validation/accuracy=0.985565, validation/loss=0.0489018, validation/mean_average_precision=0.188304, validation/num_examples=43793
I0307 17:04:40.818122 140539371349760 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.017030920833349228, loss=0.044003237038850784
I0307 17:05:02.562385 140539379742464 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.023227538913488388, loss=0.045394789427518845
I0307 17:05:24.206598 140539371349760 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.018951483070850372, loss=0.04252540320158005
I0307 17:05:45.722465 140539379742464 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.027042092755436897, loss=0.049104537814855576
I0307 17:06:07.137614 140539371349760 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.013648061081767082, loss=0.041402071714401245
I0307 17:06:28.686453 140539379742464 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.031059546396136284, loss=0.04147355630993843
I0307 17:06:49.888418 140539371349760 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.018429484218358994, loss=0.04321235045790672
I0307 17:07:11.233182 140539379742464 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.020619448274374008, loss=0.042059846222400665
I0307 17:07:32.508459 140539371349760 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.028638694435358047, loss=0.04075343534350395
I0307 17:07:54.113245 140539379742464 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.01605224609375, loss=0.04296272620558739
I0307 17:08:15.925913 140539371349760 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.020394766703248024, loss=0.046974558383226395
I0307 17:08:20.465936 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:09:34.568903 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:09:36.654992 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:09:38.687137 140681127527616 submission_runner.py:469] Time since start: 1829.81s, 	Step: 5522, 	{'train/accuracy': 0.9887790083885193, 'train/loss': 0.03839410841464996, 'train/mean_average_precision': 0.25577890577791085, 'validation/accuracy': 0.9858001470565796, 'validation/loss': 0.04812321066856384, 'validation/mean_average_precision': 0.21073301512008588, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05073074251413345, 'test/mean_average_precision': 0.20799545431461458, 'test/num_examples': 43793, 'score': 1214.0543098449707, 'total_duration': 1829.8075709342957, 'accumulated_submission_time': 1214.0543098449707, 'accumulated_eval_time': 615.4408128261566, 'accumulated_logging_time': 0.15913128852844238}
I0307 17:09:38.697470 140539379742464 logging_writer.py:48] [5522] accumulated_eval_time=615.441, accumulated_logging_time=0.159131, accumulated_submission_time=1214.05, global_step=5522, preemption_count=0, score=1214.05, test/accuracy=0.984913, test/loss=0.0507307, test/mean_average_precision=0.207995, test/num_examples=43793, total_duration=1829.81, train/accuracy=0.988779, train/loss=0.0383941, train/mean_average_precision=0.255779, validation/accuracy=0.9858, validation/loss=0.0481232, validation/mean_average_precision=0.210733, validation/num_examples=43793
I0307 17:09:55.730767 140539371349760 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.01878504268825054, loss=0.04154876619577408
I0307 17:10:16.928946 140539379742464 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.014547732658684254, loss=0.0444943867623806
I0307 17:10:38.080539 140539371349760 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01534056942909956, loss=0.04238716512918472
I0307 17:10:59.159238 140539379742464 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.012559894472360611, loss=0.04072918742895126
I0307 17:11:20.321749 140539371349760 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01569860242307186, loss=0.04139813780784607
I0307 17:11:41.434103 140539379742464 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.018402226269245148, loss=0.041772231459617615
I0307 17:12:02.952101 140539371349760 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.013539218343794346, loss=0.041031453758478165
I0307 17:12:24.195710 140539379742464 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.016549572348594666, loss=0.043902669101953506
I0307 17:12:45.703255 140539371349760 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.018920965492725372, loss=0.041850607842206955
I0307 17:13:07.393604 140539379742464 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.01772145740687847, loss=0.04019249230623245
I0307 17:13:29.261599 140539371349760 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.013234574347734451, loss=0.03645165264606476
I0307 17:13:38.828411 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:14:53.653221 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:14:55.624947 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:14:57.628952 140681127527616 submission_runner.py:469] Time since start: 2148.75s, 	Step: 6645, 	{'train/accuracy': 0.9889899492263794, 'train/loss': 0.03749311715364456, 'train/mean_average_precision': 0.26813544641535814, 'validation/accuracy': 0.9861054420471191, 'validation/loss': 0.047119785100221634, 'validation/mean_average_precision': 0.21628643495975602, 'validation/num_examples': 43793, 'test/accuracy': 0.9852088689804077, 'test/loss': 0.04979196563363075, 'test/mean_average_precision': 0.21585612349535419, 'test/num_examples': 43793, 'score': 1454.148190498352, 'total_duration': 2148.749235868454, 'accumulated_submission_time': 1454.148190498352, 'accumulated_eval_time': 694.2411503791809, 'accumulated_logging_time': 0.17903685569763184}
I0307 17:14:57.639137 140539379742464 logging_writer.py:48] [6645] accumulated_eval_time=694.241, accumulated_logging_time=0.179037, accumulated_submission_time=1454.15, global_step=6645, preemption_count=0, score=1454.15, test/accuracy=0.985209, test/loss=0.049792, test/mean_average_precision=0.215856, test/num_examples=43793, total_duration=2148.75, train/accuracy=0.98899, train/loss=0.0374931, train/mean_average_precision=0.268135, validation/accuracy=0.986105, validation/loss=0.0471198, validation/mean_average_precision=0.216286, validation/num_examples=43793
I0307 17:15:09.817895 140539371349760 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.02265975810587406, loss=0.04211808368563652
I0307 17:15:30.821727 140539379742464 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.012726095505058765, loss=0.04029635712504387
I0307 17:15:52.197693 140539371349760 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.01369395013898611, loss=0.04054294154047966
I0307 17:16:13.772283 140539379742464 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.014837314374744892, loss=0.041185081005096436
I0307 17:16:35.168192 140539371349760 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.01998182386159897, loss=0.04248719662427902
I0307 17:16:56.631696 140539379742464 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.011250064708292484, loss=0.03869251534342766
I0307 17:17:18.416695 140539371349760 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.017487550154328346, loss=0.0391978994011879
I0307 17:17:40.343870 140539379742464 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.017817331477999687, loss=0.04137145355343819
I0307 17:18:02.167942 140539371349760 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.025724202394485474, loss=0.046876177191734314
I0307 17:18:23.732103 140539379742464 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01578347384929657, loss=0.042766593396663666
I0307 17:18:44.787610 140539371349760 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.021711785346269608, loss=0.03994409367442131
I0307 17:18:57.729511 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:20:14.180401 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:20:16.168489 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:20:18.149013 140681127527616 submission_runner.py:469] Time since start: 2469.27s, 	Step: 7762, 	{'train/accuracy': 0.9889519214630127, 'train/loss': 0.0369044728577137, 'train/mean_average_precision': 0.30611968214716434, 'validation/accuracy': 0.9859174489974976, 'validation/loss': 0.047410693019628525, 'validation/mean_average_precision': 0.2350091741449841, 'validation/num_examples': 43793, 'test/accuracy': 0.9850820899009705, 'test/loss': 0.050203725695610046, 'test/mean_average_precision': 0.2307180195891641, 'test/num_examples': 43793, 'score': 1694.1986210346222, 'total_duration': 2469.2692592144012, 'accumulated_submission_time': 1694.1986210346222, 'accumulated_eval_time': 774.6604170799255, 'accumulated_logging_time': 0.19954681396484375}
I0307 17:20:18.159098 140539379742464 logging_writer.py:48] [7762] accumulated_eval_time=774.66, accumulated_logging_time=0.199547, accumulated_submission_time=1694.2, global_step=7762, preemption_count=0, score=1694.2, test/accuracy=0.985082, test/loss=0.0502037, test/mean_average_precision=0.230718, test/num_examples=43793, total_duration=2469.27, train/accuracy=0.988952, train/loss=0.0369045, train/mean_average_precision=0.30612, validation/accuracy=0.985917, validation/loss=0.0474107, validation/mean_average_precision=0.235009, validation/num_examples=43793
I0307 17:20:26.703754 140539371349760 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.018761727958917618, loss=0.041703738272190094
I0307 17:20:48.553694 140539379742464 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.018429340794682503, loss=0.034561216831207275
I0307 17:21:10.178521 140539371349760 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.01583065651357174, loss=0.03961179777979851
I0307 17:21:31.928706 140539379742464 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.013129870407283306, loss=0.039481375366449356
I0307 17:21:53.774412 140539371349760 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.016853585839271545, loss=0.03963445872068405
I0307 17:22:15.364608 140539379742464 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.016546012833714485, loss=0.04095396772027016
I0307 17:22:37.159717 140539371349760 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.016151288524270058, loss=0.04552155360579491
I0307 17:22:58.729358 140539379742464 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.014038649387657642, loss=0.037471625953912735
I0307 17:23:20.693965 140539371349760 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.01625022105872631, loss=0.04203736409544945
I0307 17:23:42.073621 140539379742464 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.022390153259038925, loss=0.042691584676504135
I0307 17:24:03.485217 140539371349760 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.018013782799243927, loss=0.03955920413136482
I0307 17:24:18.332303 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:25:32.210937 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:25:34.355860 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:25:36.430957 140681127527616 submission_runner.py:469] Time since start: 2787.55s, 	Step: 8869, 	{'train/accuracy': 0.9895359873771667, 'train/loss': 0.03476869687438011, 'train/mean_average_precision': 0.3336378839560209, 'validation/accuracy': 0.9862109422683716, 'validation/loss': 0.04657946899533272, 'validation/mean_average_precision': 0.23641985568612103, 'validation/num_examples': 43793, 'test/accuracy': 0.985342800617218, 'test/loss': 0.049263693392276764, 'test/mean_average_precision': 0.23012571803707949, 'test/num_examples': 43793, 'score': 1934.3325681686401, 'total_duration': 2787.551236152649, 'accumulated_submission_time': 1934.3325681686401, 'accumulated_eval_time': 852.7588722705841, 'accumulated_logging_time': 0.21915745735168457}
I0307 17:25:36.441087 140539379742464 logging_writer.py:48] [8869] accumulated_eval_time=852.759, accumulated_logging_time=0.219157, accumulated_submission_time=1934.33, global_step=8869, preemption_count=0, score=1934.33, test/accuracy=0.985343, test/loss=0.0492637, test/mean_average_precision=0.230126, test/num_examples=43793, total_duration=2787.55, train/accuracy=0.989536, train/loss=0.0347687, train/mean_average_precision=0.333638, validation/accuracy=0.986211, validation/loss=0.0465795, validation/mean_average_precision=0.23642, validation/num_examples=43793
I0307 17:25:43.184703 140539371349760 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.020595213398337364, loss=0.04294634610414505
I0307 17:26:04.436665 140539379742464 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.022888870909810066, loss=0.03918452933430672
I0307 17:26:25.895703 140539371349760 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.018756717443466187, loss=0.03942333161830902
I0307 17:26:47.804077 140539379742464 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.017231466248631477, loss=0.04320474714040756
I0307 17:27:09.646511 140539371349760 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.014740402810275555, loss=0.03817497566342354
I0307 17:27:31.253791 140539379742464 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.014573992229998112, loss=0.03854268789291382
I0307 17:27:53.298943 140539371349760 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.016689307987689972, loss=0.04027978330850601
I0307 17:28:14.891568 140539379742464 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01823078840970993, loss=0.03887861594557762
I0307 17:28:36.722410 140539371349760 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.025944679975509644, loss=0.03875940293073654
I0307 17:28:58.456740 140539379742464 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.016597939655184746, loss=0.039714399725198746
I0307 17:29:20.362041 140539371349760 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.01886318437755108, loss=0.03747750446200371
I0307 17:29:36.440315 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:30:49.182987 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:30:51.341534 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:30:53.409611 140681127527616 submission_runner.py:469] Time since start: 3104.53s, 	Step: 9974, 	{'train/accuracy': 0.9900067448616028, 'train/loss': 0.033745765686035156, 'train/mean_average_precision': 0.35276618447730945, 'validation/accuracy': 0.9865276217460632, 'validation/loss': 0.04516411945223808, 'validation/mean_average_precision': 0.24660534268412235, 'validation/num_examples': 43793, 'test/accuracy': 0.9856818914413452, 'test/loss': 0.04798480123281479, 'test/mean_average_precision': 0.24649667254919383, 'test/num_examples': 43793, 'score': 2174.292731523514, 'total_duration': 3104.5298507213593, 'accumulated_submission_time': 2174.292731523514, 'accumulated_eval_time': 929.7279224395752, 'accumulated_logging_time': 0.23972439765930176}
I0307 17:30:53.420896 140539379742464 logging_writer.py:48] [9974] accumulated_eval_time=929.728, accumulated_logging_time=0.239724, accumulated_submission_time=2174.29, global_step=9974, preemption_count=0, score=2174.29, test/accuracy=0.985682, test/loss=0.0479848, test/mean_average_precision=0.246497, test/num_examples=43793, total_duration=3104.53, train/accuracy=0.990007, train/loss=0.0337458, train/mean_average_precision=0.352766, validation/accuracy=0.986528, validation/loss=0.0451641, validation/mean_average_precision=0.246605, validation/num_examples=43793
I0307 17:30:59.160033 140539371349760 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01566050387918949, loss=0.03987317904829979
I0307 17:31:20.455853 140539379742464 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.01815716177225113, loss=0.039154086261987686
I0307 17:31:41.954025 140539371349760 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.01563708856701851, loss=0.0393681675195694
I0307 17:32:03.654844 140539379742464 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.01848760060966015, loss=0.03975838050246239
I0307 17:32:25.357089 140539371349760 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.019169924780726433, loss=0.03892207890748978
I0307 17:32:47.121741 140539379742464 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.016713347285985947, loss=0.03792181983590126
I0307 17:33:08.735126 140539371349760 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.028183355927467346, loss=0.03955398499965668
I0307 17:33:30.296503 140539379742464 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.015444698743522167, loss=0.03743615373969078
I0307 17:33:51.822139 140539371349760 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.016707774251699448, loss=0.038292866200208664
I0307 17:34:13.179532 140539379742464 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.020068565383553505, loss=0.039773061871528625
I0307 17:34:34.682285 140539371349760 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.016768217086791992, loss=0.03834105655550957
I0307 17:34:53.432627 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:36:07.625320 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:36:09.635071 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:36:11.647804 140681127527616 submission_runner.py:469] Time since start: 3422.77s, 	Step: 11087, 	{'train/accuracy': 0.9906244277954102, 'train/loss': 0.03141099214553833, 'train/mean_average_precision': 0.39621151412709577, 'validation/accuracy': 0.9866433143615723, 'validation/loss': 0.04498683661222458, 'validation/mean_average_precision': 0.25042331057778094, 'validation/num_examples': 43793, 'test/accuracy': 0.985772430896759, 'test/loss': 0.047772228717803955, 'test/mean_average_precision': 0.24775094628990368, 'test/num_examples': 43793, 'score': 2414.2670624256134, 'total_duration': 3422.7681024074554, 'accumulated_submission_time': 2414.2670624256134, 'accumulated_eval_time': 1007.9429130554199, 'accumulated_logging_time': 0.26107168197631836}
I0307 17:36:11.658440 140539379742464 logging_writer.py:48] [11087] accumulated_eval_time=1007.94, accumulated_logging_time=0.261072, accumulated_submission_time=2414.27, global_step=11087, preemption_count=0, score=2414.27, test/accuracy=0.985772, test/loss=0.0477722, test/mean_average_precision=0.247751, test/num_examples=43793, total_duration=3422.77, train/accuracy=0.990624, train/loss=0.031411, train/mean_average_precision=0.396212, validation/accuracy=0.986643, validation/loss=0.0449868, validation/mean_average_precision=0.250423, validation/num_examples=43793
I0307 17:36:14.657306 140539371349760 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.025376712903380394, loss=0.04284956678748131
I0307 17:36:36.294387 140539379742464 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.02573731727898121, loss=0.039087530225515366
I0307 17:36:57.539484 140539371349760 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.01808088645339012, loss=0.03725658357143402
I0307 17:37:19.124638 140539379742464 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.019427182152867317, loss=0.0376281775534153
I0307 17:37:40.539110 140539371349760 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02013605460524559, loss=0.039821889251470566
I0307 17:38:02.149349 140539379742464 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.02330804243683815, loss=0.03738037496805191
I0307 17:38:23.678358 140539371349760 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.018394652754068375, loss=0.03667610138654709
I0307 17:38:45.175731 140539379742464 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.02747030183672905, loss=0.043305691331624985
I0307 17:39:06.789029 140539371349760 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.021806802600622177, loss=0.040146756917238235
I0307 17:39:28.672977 140539379742464 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.02208370342850685, loss=0.04112691059708595
I0307 17:39:49.974099 140539371349760 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.02025534398853779, loss=0.03993748500943184
I0307 17:40:11.472421 140539379742464 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.020619483664631844, loss=0.039796266704797745
I0307 17:40:11.692167 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:41:27.364051 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:41:29.429054 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:41:31.455318 140681127527616 submission_runner.py:469] Time since start: 3742.58s, 	Step: 12202, 	{'train/accuracy': 0.9905134439468384, 'train/loss': 0.031510163098573685, 'train/mean_average_precision': 0.4019224485779094, 'validation/accuracy': 0.9866319298744202, 'validation/loss': 0.04499562457203865, 'validation/mean_average_precision': 0.25606791041604826, 'validation/num_examples': 43793, 'test/accuracy': 0.985729455947876, 'test/loss': 0.047853659838438034, 'test/mean_average_precision': 0.25563159401159624, 'test/num_examples': 43793, 'score': 2654.2603480815887, 'total_duration': 3742.5755701065063, 'accumulated_submission_time': 2654.2603480815887, 'accumulated_eval_time': 1087.7058262825012, 'accumulated_logging_time': 0.28218579292297363}
I0307 17:41:31.466100 140539371349760 logging_writer.py:48] [12202] accumulated_eval_time=1087.71, accumulated_logging_time=0.282186, accumulated_submission_time=2654.26, global_step=12202, preemption_count=0, score=2654.26, test/accuracy=0.985729, test/loss=0.0478537, test/mean_average_precision=0.255632, test/num_examples=43793, total_duration=3742.58, train/accuracy=0.990513, train/loss=0.0315102, train/mean_average_precision=0.401922, validation/accuracy=0.986632, validation/loss=0.0449956, validation/mean_average_precision=0.256068, validation/num_examples=43793
I0307 17:41:52.679426 140539379742464 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.017879579216241837, loss=0.03827729448676109
I0307 17:42:14.319609 140539371349760 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.02086091972887516, loss=0.035794034600257874
I0307 17:42:35.786329 140539379742464 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.023235857486724854, loss=0.03692299872636795
I0307 17:42:57.440345 140539371349760 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.02471976913511753, loss=0.03912419453263283
I0307 17:43:19.227734 140539379742464 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.020299946889281273, loss=0.03662751987576485
I0307 17:43:40.909383 140539371349760 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.021265599876642227, loss=0.03781934082508087
I0307 17:44:02.398670 140539379742464 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.01855047605931759, loss=0.03740193694829941
I0307 17:44:24.143908 140539371349760 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.01607268862426281, loss=0.03554516285657883
I0307 17:44:45.788797 140539379742464 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.019429149106144905, loss=0.03878598287701607
I0307 17:45:07.511131 140539371349760 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.02457515150308609, loss=0.03976597264409065
I0307 17:45:29.340552 140539379742464 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.020195571705698967, loss=0.03740477189421654
I0307 17:45:31.512288 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:46:47.539786 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:46:49.620777 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:46:51.548369 140681127527616 submission_runner.py:469] Time since start: 4062.67s, 	Step: 13311, 	{'train/accuracy': 0.9907165169715881, 'train/loss': 0.030867191031575203, 'train/mean_average_precision': 0.3966914693686757, 'validation/accuracy': 0.9867135286331177, 'validation/loss': 0.04499348998069763, 'validation/mean_average_precision': 0.2562457729471811, 'validation/num_examples': 43793, 'test/accuracy': 0.9858511686325073, 'test/loss': 0.047732334583997726, 'test/mean_average_precision': 0.25553321923247074, 'test/num_examples': 43793, 'score': 2894.267228126526, 'total_duration': 4062.66877245903, 'accumulated_submission_time': 2894.267228126526, 'accumulated_eval_time': 1167.741821527481, 'accumulated_logging_time': 0.30239343643188477}
I0307 17:46:51.558808 140539371349760 logging_writer.py:48] [13311] accumulated_eval_time=1167.74, accumulated_logging_time=0.302393, accumulated_submission_time=2894.27, global_step=13311, preemption_count=0, score=2894.27, test/accuracy=0.985851, test/loss=0.0477323, test/mean_average_precision=0.255533, test/num_examples=43793, total_duration=4062.67, train/accuracy=0.990717, train/loss=0.0308672, train/mean_average_precision=0.396691, validation/accuracy=0.986714, validation/loss=0.0449935, validation/mean_average_precision=0.256246, validation/num_examples=43793
I0307 17:47:11.037263 140539379742464 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02082682214677334, loss=0.036054544150829315
I0307 17:47:32.689950 140539371349760 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.01796294003725052, loss=0.034818775951862335
I0307 17:47:54.406263 140539379742464 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0219899769872427, loss=0.038322076201438904
I0307 17:48:16.219578 140539371349760 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.025079114362597466, loss=0.03675360977649689
I0307 17:48:37.692008 140539379742464 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.022994408383965492, loss=0.03530960530042648
I0307 17:48:59.185249 140539371349760 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.027159132063388824, loss=0.037251219153404236
I0307 17:49:20.656172 140539379742464 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.020205577835440636, loss=0.03575469180941582
I0307 17:49:42.123437 140539371349760 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.022469589486718178, loss=0.036894239485263824
I0307 17:50:03.661494 140539379742464 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.020015938207507133, loss=0.03749260678887367
I0307 17:50:25.318508 140539371349760 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.019731231033802032, loss=0.03525557741522789
I0307 17:50:46.978635 140539379742464 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.025333410128951073, loss=0.03602273762226105
I0307 17:50:51.718912 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:52:06.886945 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:52:08.968034 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:52:10.994514 140681127527616 submission_runner.py:469] Time since start: 4382.11s, 	Step: 14423, 	{'train/accuracy': 0.9913085103034973, 'train/loss': 0.02904149517416954, 'train/mean_average_precision': 0.4597103301805182, 'validation/accuracy': 0.986660361289978, 'validation/loss': 0.044762022793293, 'validation/mean_average_precision': 0.2594895337665095, 'validation/num_examples': 43793, 'test/accuracy': 0.9857934713363647, 'test/loss': 0.04752175882458687, 'test/mean_average_precision': 0.2489293268632724, 'test/num_examples': 43793, 'score': 3134.3860597610474, 'total_duration': 4382.11479473114, 'accumulated_submission_time': 3134.3860597610474, 'accumulated_eval_time': 1247.0172290802002, 'accumulated_logging_time': 0.3221871852874756}
I0307 17:52:11.004791 140539371349760 logging_writer.py:48] [14423] accumulated_eval_time=1247.02, accumulated_logging_time=0.322187, accumulated_submission_time=3134.39, global_step=14423, preemption_count=0, score=3134.39, test/accuracy=0.985793, test/loss=0.0475218, test/mean_average_precision=0.248929, test/num_examples=43793, total_duration=4382.11, train/accuracy=0.991309, train/loss=0.0290415, train/mean_average_precision=0.45971, validation/accuracy=0.98666, validation/loss=0.044762, validation/mean_average_precision=0.25949, validation/num_examples=43793
I0307 17:52:27.921405 140539379742464 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.02198052778840065, loss=0.03830369561910629
I0307 17:52:49.713439 140539371349760 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0231514573097229, loss=0.037249427288770676
I0307 17:53:11.257601 140539379742464 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.021795935928821564, loss=0.03461989760398865
I0307 17:53:33.015413 140539371349760 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.02427467703819275, loss=0.036391060799360275
I0307 17:53:54.874773 140539379742464 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.02457994408905506, loss=0.03683085739612579
I0307 17:54:16.679567 140539371349760 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.020311133936047554, loss=0.036114439368247986
I0307 17:54:38.509168 140539379742464 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.023780783638358116, loss=0.03581361472606659
I0307 17:55:00.356168 140539371349760 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.018440118059515953, loss=0.03298402205109596
I0307 17:55:21.970958 140539379742464 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.018918363377451897, loss=0.03428184613585472
I0307 17:55:43.994571 140539371349760 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.019556362181901932, loss=0.03284922614693642
I0307 17:56:05.638072 140539379742464 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.019636739045381546, loss=0.03452032059431076
I0307 17:56:11.098224 140681127527616 spec.py:321] Evaluating on the training split.
I0307 17:57:24.046551 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 17:57:26.037085 140681127527616 spec.py:349] Evaluating on the test split.
I0307 17:57:28.035888 140681127527616 submission_runner.py:469] Time since start: 4699.16s, 	Step: 15526, 	{'train/accuracy': 0.99116450548172, 'train/loss': 0.02930135279893875, 'train/mean_average_precision': 0.4596503570921001, 'validation/accuracy': 0.9867017269134521, 'validation/loss': 0.04537179321050644, 'validation/mean_average_precision': 0.2587348912957092, 'validation/num_examples': 43793, 'test/accuracy': 0.9857943058013916, 'test/loss': 0.04824480041861534, 'test/mean_average_precision': 0.2553885637590089, 'test/num_examples': 43793, 'score': 3374.4397547245026, 'total_duration': 4699.156151533127, 'accumulated_submission_time': 3374.4397547245026, 'accumulated_eval_time': 1323.9546794891357, 'accumulated_logging_time': 0.3423612117767334}
I0307 17:57:28.047243 140539371349760 logging_writer.py:48] [15526] accumulated_eval_time=1323.95, accumulated_logging_time=0.342361, accumulated_submission_time=3374.44, global_step=15526, preemption_count=0, score=3374.44, test/accuracy=0.985794, test/loss=0.0482448, test/mean_average_precision=0.255389, test/num_examples=43793, total_duration=4699.16, train/accuracy=0.991165, train/loss=0.0293014, train/mean_average_precision=0.45965, validation/accuracy=0.986702, validation/loss=0.0453718, validation/mean_average_precision=0.258735, validation/num_examples=43793
I0307 17:57:44.157445 140539379742464 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.019468732178211212, loss=0.032746631652116776
I0307 17:58:05.570822 140539371349760 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.019957534968852997, loss=0.034791089594364166
I0307 17:58:26.780769 140539379742464 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.026794735342264175, loss=0.03691398724913597
I0307 17:58:48.229494 140539371349760 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.02972864918410778, loss=0.03682871162891388
I0307 17:59:09.758369 140539379742464 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.025241121649742126, loss=0.037555135786533356
I0307 17:59:31.341946 140539371349760 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.02320636808872223, loss=0.0344601534307003
I0307 17:59:52.953958 140539379742464 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.02299266867339611, loss=0.03533357381820679
I0307 18:00:14.302934 140539371349760 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.022841118276119232, loss=0.03485526144504547
I0307 18:00:36.035784 140539379742464 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.02915121056139469, loss=0.03407977521419525
I0307 18:00:57.915482 140539371349760 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.023942727595567703, loss=0.03962470591068268
I0307 18:01:19.858776 140539379742464 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.022745901718735695, loss=0.034151386469602585
I0307 18:01:28.038697 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:02:44.514853 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:02:46.526763 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:02:48.459308 140681127527616 submission_runner.py:469] Time since start: 5019.58s, 	Step: 16639, 	{'train/accuracy': 0.9917422533035278, 'train/loss': 0.027018975466489792, 'train/mean_average_precision': 0.5054917883613397, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.04532735049724579, 'validation/mean_average_precision': 0.26429787677699873, 'validation/num_examples': 43793, 'test/accuracy': 0.9859703779220581, 'test/loss': 0.048137325793504715, 'test/mean_average_precision': 0.2572786200951081, 'test/num_examples': 43793, 'score': 3614.3927824497223, 'total_duration': 5019.579539299011, 'accumulated_submission_time': 3614.3927824497223, 'accumulated_eval_time': 1404.3750398159027, 'accumulated_logging_time': 0.3638455867767334}
I0307 18:02:48.470744 140539371349760 logging_writer.py:48] [16639] accumulated_eval_time=1404.38, accumulated_logging_time=0.363846, accumulated_submission_time=3614.39, global_step=16639, preemption_count=0, score=3614.39, test/accuracy=0.98597, test/loss=0.0481373, test/mean_average_precision=0.257279, test/num_examples=43793, total_duration=5019.58, train/accuracy=0.991742, train/loss=0.027019, train/mean_average_precision=0.505492, validation/accuracy=0.986758, validation/loss=0.0453274, validation/mean_average_precision=0.264298, validation/num_examples=43793
I0307 18:03:02.147726 140539379742464 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.02753974311053753, loss=0.0334356352686882
I0307 18:03:24.006142 140539371349760 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.02393743209540844, loss=0.03409048169851303
I0307 18:03:46.011282 140539379742464 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.02050813101232052, loss=0.035492368042469025
I0307 18:04:07.849418 140539371349760 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.023973992094397545, loss=0.03524553403258324
I0307 18:04:29.803875 140539379742464 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.023767272010445595, loss=0.034485381096601486
I0307 18:04:51.786578 140539371349760 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.02388111501932144, loss=0.03860687091946602
I0307 18:05:13.833508 140539379742464 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.024035340175032616, loss=0.03503647446632385
I0307 18:05:35.627672 140539371349760 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.031029751524329185, loss=0.03680635243654251
I0307 18:05:57.689540 140539379742464 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.02452760376036167, loss=0.03418631851673126
I0307 18:06:19.739604 140539371349760 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02270030602812767, loss=0.03317270055413246
I0307 18:06:41.156263 140539379742464 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.024211199954152107, loss=0.0342230424284935
I0307 18:06:48.495585 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:08:05.505889 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:08:07.551807 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:08:09.491886 140681127527616 submission_runner.py:469] Time since start: 5340.61s, 	Step: 17735, 	{'train/accuracy': 0.9912893772125244, 'train/loss': 0.027928806841373444, 'train/mean_average_precision': 0.49817851882015035, 'validation/accuracy': 0.9866729378700256, 'validation/loss': 0.04598692059516907, 'validation/mean_average_precision': 0.258333338531433, 'validation/num_examples': 43793, 'test/accuracy': 0.9858827590942383, 'test/loss': 0.04875091463327408, 'test/mean_average_precision': 0.25343527495760393, 'test/num_examples': 43793, 'score': 3854.378380537033, 'total_duration': 5340.612284898758, 'accumulated_submission_time': 3854.378380537033, 'accumulated_eval_time': 1485.371256828308, 'accumulated_logging_time': 0.3858203887939453}
I0307 18:08:09.503058 140539371349760 logging_writer.py:48] [17735] accumulated_eval_time=1485.37, accumulated_logging_time=0.38582, accumulated_submission_time=3854.38, global_step=17735, preemption_count=0, score=3854.38, test/accuracy=0.985883, test/loss=0.0487509, test/mean_average_precision=0.253435, test/num_examples=43793, total_duration=5340.61, train/accuracy=0.991289, train/loss=0.0279288, train/mean_average_precision=0.498179, validation/accuracy=0.986673, validation/loss=0.0459869, validation/mean_average_precision=0.258333, validation/num_examples=43793
I0307 18:08:23.873544 140539379742464 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.02203407883644104, loss=0.03040621615946293
I0307 18:08:45.679556 140539371349760 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.029622962698340416, loss=0.03633299842476845
I0307 18:09:07.789981 140539379742464 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.027781333774328232, loss=0.034964244812726974
I0307 18:09:29.269399 140539371349760 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.022997358813881874, loss=0.03465864062309265
I0307 18:09:50.932048 140539379742464 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.027069546282291412, loss=0.03665873780846596
I0307 18:10:12.510458 140539371349760 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.02826337330043316, loss=0.03729286044836044
I0307 18:10:34.479431 140539379742464 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.024457449093461037, loss=0.03254314884543419
I0307 18:10:55.901168 140539371349760 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.028720080852508545, loss=0.03679021820425987
I0307 18:11:17.783655 140539379742464 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.028764313086867332, loss=0.03553842753171921
I0307 18:11:39.730710 140539371349760 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.02574065886437893, loss=0.03512869030237198
I0307 18:12:01.558221 140539379742464 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.02476719580590725, loss=0.03623522073030472
I0307 18:12:09.641623 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:13:24.210714 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:13:26.188471 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:13:28.149595 140681127527616 submission_runner.py:469] Time since start: 5659.27s, 	Step: 18838, 	{'train/accuracy': 0.9920688271522522, 'train/loss': 0.026050804182887077, 'train/mean_average_precision': 0.5213555031762197, 'validation/accuracy': 0.9867825508117676, 'validation/loss': 0.045983366668224335, 'validation/mean_average_precision': 0.2612213705168048, 'validation/num_examples': 43793, 'test/accuracy': 0.98590087890625, 'test/loss': 0.04899316281080246, 'test/mean_average_precision': 0.2531019784688823, 'test/num_examples': 43793, 'score': 4094.4802825450897, 'total_duration': 5659.26993393898, 'accumulated_submission_time': 4094.4802825450897, 'accumulated_eval_time': 1563.8790833950043, 'accumulated_logging_time': 0.40639328956604004}
I0307 18:13:28.160732 140539371349760 logging_writer.py:48] [18838] accumulated_eval_time=1563.88, accumulated_logging_time=0.406393, accumulated_submission_time=4094.48, global_step=18838, preemption_count=0, score=4094.48, test/accuracy=0.985901, test/loss=0.0489932, test/mean_average_precision=0.253102, test/num_examples=43793, total_duration=5659.27, train/accuracy=0.992069, train/loss=0.0260508, train/mean_average_precision=0.521356, validation/accuracy=0.986783, validation/loss=0.0459834, validation/mean_average_precision=0.261221, validation/num_examples=43793
I0307 18:13:41.618293 140539379742464 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.029082681983709335, loss=0.03499743342399597
I0307 18:14:03.000540 140539371349760 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.023324182257056236, loss=0.033290982246398926
I0307 18:14:24.106412 140539379742464 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.028695575892925262, loss=0.03618137910962105
I0307 18:14:45.942426 140539371349760 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.02650504931807518, loss=0.03356403857469559
I0307 18:15:07.630808 140539379742464 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.02845011092722416, loss=0.03324822708964348
I0307 18:15:29.567260 140539371349760 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.029632365331053734, loss=0.03342725336551666
I0307 18:15:51.608710 140539379742464 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.02931991033256054, loss=0.03422047570347786
I0307 18:16:13.048649 140539371349760 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.025758637115359306, loss=0.034340355545282364
I0307 18:16:34.168235 140539379742464 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.02754008024930954, loss=0.033810392022132874
I0307 18:16:55.655401 140539371349760 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.02506248652935028, loss=0.031335584819316864
I0307 18:17:17.360087 140539379742464 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.03501899540424347, loss=0.03339381515979767
I0307 18:17:28.307481 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:18:43.738985 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:18:45.759091 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:18:47.760038 140681127527616 submission_runner.py:469] Time since start: 5978.88s, 	Step: 19952, 	{'train/accuracy': 0.9921428561210632, 'train/loss': 0.025559471920132637, 'train/mean_average_precision': 0.5327124947685367, 'validation/accuracy': 0.9868085384368896, 'validation/loss': 0.04626746475696564, 'validation/mean_average_precision': 0.258900247415377, 'validation/num_examples': 43793, 'test/accuracy': 0.9857665300369263, 'test/loss': 0.04968218132853508, 'test/mean_average_precision': 0.2493889945445301, 'test/num_examples': 43793, 'score': 4334.588676691055, 'total_duration': 5978.880343914032, 'accumulated_submission_time': 4334.588676691055, 'accumulated_eval_time': 1643.3314793109894, 'accumulated_logging_time': 0.4276409149169922}
I0307 18:18:47.771132 140539371349760 logging_writer.py:48] [19952] accumulated_eval_time=1643.33, accumulated_logging_time=0.427641, accumulated_submission_time=4334.59, global_step=19952, preemption_count=0, score=4334.59, test/accuracy=0.985767, test/loss=0.0496822, test/mean_average_precision=0.249389, test/num_examples=43793, total_duration=5978.88, train/accuracy=0.992143, train/loss=0.0255595, train/mean_average_precision=0.532712, validation/accuracy=0.986809, validation/loss=0.0462675, validation/mean_average_precision=0.2589, validation/num_examples=43793
I0307 18:18:58.593978 140539379742464 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.024328688159585, loss=0.03454359248280525
I0307 18:19:19.977893 140539371349760 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.029909934848546982, loss=0.034322384744882584
I0307 18:19:41.561365 140539379742464 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.027373896911740303, loss=0.0327129140496254
I0307 18:20:03.001545 140539371349760 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.026301121339201927, loss=0.03372538834810257
I0307 18:20:24.351293 140539379742464 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.02777210809290409, loss=0.03290426358580589
I0307 18:20:45.998772 140539371349760 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.035347871482372284, loss=0.035411521792411804
I0307 18:21:07.623504 140539379742464 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.027423985302448273, loss=0.030616186559200287
I0307 18:21:29.109144 140539371349760 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.02694934606552124, loss=0.03136282414197922
I0307 18:21:51.017520 140539379742464 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.02635267749428749, loss=0.03317510709166527
I0307 18:22:12.524442 140539371349760 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.02789689227938652, loss=0.03052275814116001
I0307 18:22:34.290421 140539379742464 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.02889515459537506, loss=0.03122139535844326
I0307 18:22:47.774048 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:24:05.010140 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:24:07.054780 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:24:09.004698 140681127527616 submission_runner.py:469] Time since start: 6300.13s, 	Step: 21063, 	{'train/accuracy': 0.9922109246253967, 'train/loss': 0.025429103523492813, 'train/mean_average_precision': 0.5364263126685468, 'validation/accuracy': 0.9866505861282349, 'validation/loss': 0.04653804004192352, 'validation/mean_average_precision': 0.263310892088798, 'validation/num_examples': 43793, 'test/accuracy': 0.9858238101005554, 'test/loss': 0.04970753937959671, 'test/mean_average_precision': 0.25144894776296034, 'test/num_examples': 43793, 'score': 4574.5525159835815, 'total_duration': 6300.125139474869, 'accumulated_submission_time': 4574.5525159835815, 'accumulated_eval_time': 1724.562082529068, 'accumulated_logging_time': 0.4483628273010254}
I0307 18:24:09.016034 140539371349760 logging_writer.py:48] [21063] accumulated_eval_time=1724.56, accumulated_logging_time=0.448363, accumulated_submission_time=4574.55, global_step=21063, preemption_count=0, score=4574.55, test/accuracy=0.985824, test/loss=0.0497075, test/mean_average_precision=0.251449, test/num_examples=43793, total_duration=6300.13, train/accuracy=0.992211, train/loss=0.0254291, train/mean_average_precision=0.536426, validation/accuracy=0.986651, validation/loss=0.046538, validation/mean_average_precision=0.263311, validation/num_examples=43793
I0307 18:24:17.124768 140539379742464 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.027615662664175034, loss=0.034801241010427475
I0307 18:24:38.790904 140539371349760 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.026023991405963898, loss=0.03131066635251045
I0307 18:25:00.140758 140539379742464 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.030843442305922508, loss=0.03220014274120331
I0307 18:25:21.935323 140539371349760 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.04154127836227417, loss=0.033144645392894745
I0307 18:25:43.339670 140539379742464 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.028881965205073357, loss=0.03200709819793701
I0307 18:26:05.008888 140539371349760 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.025096362456679344, loss=0.03107309341430664
I0307 18:26:26.842061 140539379742464 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.030409032478928566, loss=0.03305419161915779
I0307 18:26:48.914295 140539371349760 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.030560607090592384, loss=0.03354158625006676
I0307 18:27:10.800895 140539379742464 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.03273934870958328, loss=0.03467687591910362
I0307 18:27:32.440998 140539371349760 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.02851169928908348, loss=0.03151564672589302
I0307 18:27:54.354880 140539379742464 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.029300445690751076, loss=0.032865703105926514
I0307 18:28:09.113518 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:29:24.324224 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:29:26.339369 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:29:28.311159 140681127527616 submission_runner.py:469] Time since start: 6619.43s, 	Step: 22169, 	{'train/accuracy': 0.992899477481842, 'train/loss': 0.023096101358532906, 'train/mean_average_precision': 0.5920535574588823, 'validation/accuracy': 0.9866822361946106, 'validation/loss': 0.046731531620025635, 'validation/mean_average_precision': 0.26172669389736536, 'validation/num_examples': 43793, 'test/accuracy': 0.9858840703964233, 'test/loss': 0.04983590543270111, 'test/mean_average_precision': 0.251740907075593, 'test/num_examples': 43793, 'score': 4814.611516237259, 'total_duration': 6619.431487083435, 'accumulated_submission_time': 4814.611516237259, 'accumulated_eval_time': 1803.7595672607422, 'accumulated_logging_time': 0.46943092346191406}
I0307 18:29:28.322887 140539371349760 logging_writer.py:48] [22169] accumulated_eval_time=1803.76, accumulated_logging_time=0.469431, accumulated_submission_time=4814.61, global_step=22169, preemption_count=0, score=4814.61, test/accuracy=0.985884, test/loss=0.0498359, test/mean_average_precision=0.251741, test/num_examples=43793, total_duration=6619.43, train/accuracy=0.992899, train/loss=0.0230961, train/mean_average_precision=0.592054, validation/accuracy=0.986682, validation/loss=0.0467315, validation/mean_average_precision=0.261727, validation/num_examples=43793
I0307 18:29:35.262569 140539379742464 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.028146814554929733, loss=0.030820392072200775
I0307 18:29:57.021519 140539371349760 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.030322959646582603, loss=0.031626105308532715
I0307 18:30:18.537513 140539379742464 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.030888499692082405, loss=0.03220922872424126
I0307 18:30:40.627588 140539371349760 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.030589519068598747, loss=0.033944614231586456
I0307 18:31:02.333559 140539379742464 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.031984005123376846, loss=0.034088414162397385
I0307 18:31:23.904952 140539371349760 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.027966203168034554, loss=0.030391816049814224
I0307 18:31:45.474612 140539379742464 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.02874010056257248, loss=0.03142248094081879
I0307 18:32:07.473210 140539371349760 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.032609786838293076, loss=0.03162728622555733
I0307 18:32:28.710052 140539379742464 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.03173365071415901, loss=0.033564094454050064
I0307 18:32:49.975891 140539371349760 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.037896979600191116, loss=0.030125487595796585
I0307 18:33:11.524224 140539379742464 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.030525751411914825, loss=0.03058236464858055
I0307 18:33:28.370638 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:34:42.680025 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:34:44.779926 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:34:46.781578 140681127527616 submission_runner.py:469] Time since start: 6937.90s, 	Step: 23278, 	{'train/accuracy': 0.9925416111946106, 'train/loss': 0.024167874827980995, 'train/mean_average_precision': 0.5664100288876779, 'validation/accuracy': 0.9866615533828735, 'validation/loss': 0.04728492721915245, 'validation/mean_average_precision': 0.2533365939874336, 'validation/num_examples': 43793, 'test/accuracy': 0.9858158230781555, 'test/loss': 0.050241753458976746, 'test/mean_average_precision': 0.25312532452599984, 'test/num_examples': 43793, 'score': 5054.620993614197, 'total_duration': 6937.901864051819, 'accumulated_submission_time': 5054.620993614197, 'accumulated_eval_time': 1882.1703062057495, 'accumulated_logging_time': 0.49088525772094727}
I0307 18:34:46.793161 140539371349760 logging_writer.py:48] [23278] accumulated_eval_time=1882.17, accumulated_logging_time=0.490885, accumulated_submission_time=5054.62, global_step=23278, preemption_count=0, score=5054.62, test/accuracy=0.985816, test/loss=0.0502418, test/mean_average_precision=0.253125, test/num_examples=43793, total_duration=6937.9, train/accuracy=0.992542, train/loss=0.0241679, train/mean_average_precision=0.56641, validation/accuracy=0.986662, validation/loss=0.0472849, validation/mean_average_precision=0.253337, validation/num_examples=43793
I0307 18:34:51.802576 140539379742464 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.03661227971315384, loss=0.034489087760448456
I0307 18:35:13.424995 140539371349760 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.0370522066950798, loss=0.033285316079854965
I0307 18:35:35.108203 140539379742464 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.031221535056829453, loss=0.031361304223537445
I0307 18:35:56.848626 140539371349760 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.03519930690526962, loss=0.030033307150006294
I0307 18:36:18.656221 140539379742464 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.032028328627347946, loss=0.031205914914608
I0307 18:36:40.576431 140539371349760 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.033225078135728836, loss=0.031573858112096786
I0307 18:37:02.492737 140539379742464 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.0339411124587059, loss=0.03112471103668213
I0307 18:37:24.407635 140539371349760 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.0368528887629509, loss=0.032305166125297546
I0307 18:37:46.360823 140539379742464 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.030180424451828003, loss=0.028475020080804825
I0307 18:38:08.274258 140539371349760 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.034354645758867264, loss=0.03155214339494705
I0307 18:38:29.746608 140539379742464 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03805559128522873, loss=0.033555060625076294
I0307 18:38:46.900019 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:40:01.917765 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:40:04.042099 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:40:06.077346 140681127527616 submission_runner.py:469] Time since start: 7257.20s, 	Step: 24380, 	{'train/accuracy': 0.9934418201446533, 'train/loss': 0.021381603553891182, 'train/mean_average_precision': 0.6230545571224955, 'validation/accuracy': 0.9867167472839355, 'validation/loss': 0.04735250025987625, 'validation/mean_average_precision': 0.25955633650647425, 'validation/num_examples': 43793, 'test/accuracy': 0.9858099222183228, 'test/loss': 0.050629694014787674, 'test/mean_average_precision': 0.24780152889570692, 'test/num_examples': 43793, 'score': 5294.686759233475, 'total_duration': 7257.19761967659, 'accumulated_submission_time': 5294.686759233475, 'accumulated_eval_time': 1961.3474252223969, 'accumulated_logging_time': 0.5124292373657227}
I0307 18:40:06.089649 140539371349760 logging_writer.py:48] [24380] accumulated_eval_time=1961.35, accumulated_logging_time=0.512429, accumulated_submission_time=5294.69, global_step=24380, preemption_count=0, score=5294.69, test/accuracy=0.98581, test/loss=0.0506297, test/mean_average_precision=0.247802, test/num_examples=43793, total_duration=7257.2, train/accuracy=0.993442, train/loss=0.0213816, train/mean_average_precision=0.623055, validation/accuracy=0.986717, validation/loss=0.0473525, validation/mean_average_precision=0.259556, validation/num_examples=43793
I0307 18:40:10.595231 140539379742464 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.03913487121462822, loss=0.03116781823337078
I0307 18:40:32.018416 140539371349760 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.04006268084049225, loss=0.03378579765558243
I0307 18:40:53.224666 140539379742464 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.035006310790777206, loss=0.030483394861221313
I0307 18:41:14.589277 140539371349760 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.03507004305720329, loss=0.02857833355665207
I0307 18:41:36.169473 140539379742464 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.033015698194503784, loss=0.03248609974980354
I0307 18:41:57.864242 140539371349760 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.033559806644916534, loss=0.02958115004003048
I0307 18:42:19.484992 140539379742464 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.03708112984895706, loss=0.031842947006225586
I0307 18:42:41.370016 140539371349760 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.03491552174091339, loss=0.03298380970954895
I0307 18:43:03.452499 140539379742464 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.035512104630470276, loss=0.03313930332660675
I0307 18:43:25.134727 140539371349760 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.03670848533511162, loss=0.029505064710974693
I0307 18:43:46.833300 140539379742464 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.03729938715696335, loss=0.03023063763976097
I0307 18:44:06.153698 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:45:22.144302 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:45:24.218823 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:45:26.245184 140681127527616 submission_runner.py:469] Time since start: 7577.37s, 	Step: 25491, 	{'train/accuracy': 0.9931038022041321, 'train/loss': 0.022402552887797356, 'train/mean_average_precision': 0.6004998552857195, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.04819728434085846, 'validation/mean_average_precision': 0.2592291029051286, 'validation/num_examples': 43793, 'test/accuracy': 0.9857050180435181, 'test/loss': 0.05148175358772278, 'test/mean_average_precision': 0.2496276009985486, 'test/num_examples': 43793, 'score': 5534.713684558868, 'total_duration': 7577.365520000458, 'accumulated_submission_time': 5534.713684558868, 'accumulated_eval_time': 2041.4387638568878, 'accumulated_logging_time': 0.5348010063171387}
I0307 18:45:26.256910 140539371349760 logging_writer.py:48] [25491] accumulated_eval_time=2041.44, accumulated_logging_time=0.534801, accumulated_submission_time=5534.71, global_step=25491, preemption_count=0, score=5534.71, test/accuracy=0.985705, test/loss=0.0514818, test/mean_average_precision=0.249628, test/num_examples=43793, total_duration=7577.37, train/accuracy=0.993104, train/loss=0.0224026, train/mean_average_precision=0.6005, validation/accuracy=0.986703, validation/loss=0.0481973, validation/mean_average_precision=0.259229, validation/num_examples=43793
I0307 18:45:28.509779 140539379742464 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.03933396935462952, loss=0.03187249228358269
I0307 18:45:50.255103 140539371349760 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.03492994233965874, loss=0.027353588491678238
I0307 18:46:11.401605 140539379742464 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.044948335736989975, loss=0.03101835586130619
I0307 18:46:32.921924 140539371349760 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.041458748281002045, loss=0.030268115922808647
I0307 18:46:54.603758 140539379742464 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.03797392174601555, loss=0.029183194041252136
I0307 18:47:16.770851 140539371349760 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.038537003099918365, loss=0.03169361129403114
I0307 18:47:38.581581 140539379742464 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.04146483913064003, loss=0.03046526573598385
I0307 18:48:00.371252 140539371349760 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.036356184631586075, loss=0.028820691630244255
I0307 18:48:22.210068 140539379742464 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.04436206817626953, loss=0.030617069453001022
I0307 18:48:43.942157 140539371349760 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.03596197068691254, loss=0.028756212443113327
I0307 18:49:05.785428 140539379742464 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.040760308504104614, loss=0.03017689287662506
I0307 18:49:26.248391 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:50:43.894846 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:50:46.103038 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:50:48.043446 140681127527616 submission_runner.py:469] Time since start: 7899.16s, 	Step: 26595, 	{'train/accuracy': 0.9934980869293213, 'train/loss': 0.021209150552749634, 'train/mean_average_precision': 0.613647568288411, 'validation/accuracy': 0.9864910840988159, 'validation/loss': 0.04872254654765129, 'validation/mean_average_precision': 0.2555855673491779, 'validation/num_examples': 43793, 'test/accuracy': 0.9856199622154236, 'test/loss': 0.052097950130701065, 'test/mean_average_precision': 0.24300745410759514, 'test/num_examples': 43793, 'score': 5774.6672995090485, 'total_duration': 7899.163873434067, 'accumulated_submission_time': 5774.6672995090485, 'accumulated_eval_time': 2123.23375916481, 'accumulated_logging_time': 0.5574033260345459}
I0307 18:50:48.055633 140539371349760 logging_writer.py:48] [26595] accumulated_eval_time=2123.23, accumulated_logging_time=0.557403, accumulated_submission_time=5774.67, global_step=26595, preemption_count=0, score=5774.67, test/accuracy=0.98562, test/loss=0.052098, test/mean_average_precision=0.243007, test/num_examples=43793, total_duration=7899.16, train/accuracy=0.993498, train/loss=0.0212092, train/mean_average_precision=0.613648, validation/accuracy=0.986491, validation/loss=0.0487225, validation/mean_average_precision=0.255586, validation/num_examples=43793
I0307 18:50:49.382299 140539379742464 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.039518147706985474, loss=0.031310249119997025
I0307 18:51:11.235528 140539371349760 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.04015999659895897, loss=0.03137150779366493
I0307 18:51:32.909161 140539379742464 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.04091096669435501, loss=0.029276804998517036
I0307 18:51:54.752882 140539371349760 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.04438960179686546, loss=0.03041876293718815
I0307 18:52:16.644495 140539379742464 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.03707565367221832, loss=0.03087208792567253
I0307 18:52:38.457131 140539371349760 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.033163607120513916, loss=0.028278162702918053
I0307 18:53:00.626551 140539379742464 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.037581346929073334, loss=0.028523020446300507
I0307 18:53:22.438629 140539371349760 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.045306991785764694, loss=0.03132098913192749
I0307 18:53:44.343478 140539379742464 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.043327633291482925, loss=0.030585549771785736
I0307 18:54:06.393830 140539371349760 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.04238997399806976, loss=0.03025921620428562
I0307 18:54:28.637547 140539379742464 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.04145869240164757, loss=0.03069343790411949
I0307 18:54:48.249618 140681127527616 spec.py:321] Evaluating on the training split.
I0307 18:56:05.364182 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 18:56:07.322284 140681127527616 spec.py:349] Evaluating on the test split.
I0307 18:56:09.260611 140681127527616 submission_runner.py:469] Time since start: 8220.38s, 	Step: 27691, 	{'train/accuracy': 0.9940281510353088, 'train/loss': 0.01947796903550625, 'train/mean_average_precision': 0.6682530387046626, 'validation/accuracy': 0.9865698218345642, 'validation/loss': 0.04949478060007095, 'validation/mean_average_precision': 0.2528564819855809, 'validation/num_examples': 43793, 'test/accuracy': 0.9856932163238525, 'test/loss': 0.052731096744537354, 'test/mean_average_precision': 0.24326898764642005, 'test/num_examples': 43793, 'score': 6014.823639392853, 'total_duration': 8220.381026268005, 'accumulated_submission_time': 6014.823639392853, 'accumulated_eval_time': 2204.2446784973145, 'accumulated_logging_time': 0.5796847343444824}
I0307 18:56:09.273836 140539371349760 logging_writer.py:48] [27691] accumulated_eval_time=2204.24, accumulated_logging_time=0.579685, accumulated_submission_time=6014.82, global_step=27691, preemption_count=0, score=6014.82, test/accuracy=0.985693, test/loss=0.0527311, test/mean_average_precision=0.243269, test/num_examples=43793, total_duration=8220.38, train/accuracy=0.994028, train/loss=0.019478, train/mean_average_precision=0.668253, validation/accuracy=0.98657, validation/loss=0.0494948, validation/mean_average_precision=0.252856, validation/num_examples=43793
I0307 18:56:11.460990 140539379742464 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.04028622806072235, loss=0.028167355805635452
I0307 18:56:33.781278 140539371349760 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.04164353758096695, loss=0.03224024176597595
I0307 18:56:55.761993 140539379742464 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.046764787286520004, loss=0.031894225627183914
I0307 18:57:17.507119 140539371349760 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.04591088742017746, loss=0.030737457796931267
I0307 18:57:39.625129 140539379742464 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.04014047235250473, loss=0.030026264488697052
I0307 18:58:01.464136 140539371349760 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.03935324028134346, loss=0.02757491171360016
I0307 18:58:22.991604 140539379742464 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.037321869283914566, loss=0.02957957796752453
I0307 18:58:44.541707 140539371349760 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.04018808528780937, loss=0.028932325541973114
I0307 18:59:06.597484 140539379742464 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.04146352782845497, loss=0.02861430123448372
I0307 18:59:28.570223 140539371349760 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.03991229087114334, loss=0.029436921700835228
I0307 18:59:50.535653 140539379742464 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.05003809928894043, loss=0.028399964794516563
I0307 19:00:09.341952 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:01:25.407958 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:01:27.345327 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:01:29.427625 140681127527616 submission_runner.py:469] Time since start: 8540.55s, 	Step: 28787, 	{'train/accuracy': 0.993468701839447, 'train/loss': 0.020785164088010788, 'train/mean_average_precision': 0.6354571079446321, 'validation/accuracy': 0.986544668674469, 'validation/loss': 0.05049121379852295, 'validation/mean_average_precision': 0.24725478867541148, 'validation/num_examples': 43793, 'test/accuracy': 0.9856536388397217, 'test/loss': 0.05412588268518448, 'test/mean_average_precision': 0.23914086872056928, 'test/num_examples': 43793, 'score': 6254.85346198082, 'total_duration': 8540.547902822495, 'accumulated_submission_time': 6254.85346198082, 'accumulated_eval_time': 2284.330146074295, 'accumulated_logging_time': 0.602679967880249}
I0307 19:01:29.441231 140539371349760 logging_writer.py:48] [28787] accumulated_eval_time=2284.33, accumulated_logging_time=0.60268, accumulated_submission_time=6254.85, global_step=28787, preemption_count=0, score=6254.85, test/accuracy=0.985654, test/loss=0.0541259, test/mean_average_precision=0.239141, test/num_examples=43793, total_duration=8540.55, train/accuracy=0.993469, train/loss=0.0207852, train/mean_average_precision=0.635457, validation/accuracy=0.986545, validation/loss=0.0504912, validation/mean_average_precision=0.247255, validation/num_examples=43793
I0307 19:01:32.489262 140539379742464 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.0487656332552433, loss=0.02921343594789505
I0307 19:01:54.597925 140539371349760 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.052898064255714417, loss=0.028740886598825455
I0307 19:02:16.439868 140539379742464 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.04558292031288147, loss=0.02954144775867462
I0307 19:02:38.390732 140539371349760 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05058905482292175, loss=0.029603952541947365
I0307 19:03:00.625431 140539379742464 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04417634382843971, loss=0.03071296215057373
I0307 19:03:22.350727 140539371349760 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.037510208785533905, loss=0.02766990289092064
I0307 19:03:44.222669 140539379742464 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.03950303792953491, loss=0.02671743929386139
I0307 19:04:05.979509 140539371349760 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.04520080238580704, loss=0.0286569744348526
I0307 19:04:27.299861 140539379742464 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.042983528226614, loss=0.02870544232428074
I0307 19:04:48.536439 140539371349760 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.04534810781478882, loss=0.02872118167579174
I0307 19:05:10.132142 140539379742464 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.04531392082571983, loss=0.027532853186130524
I0307 19:05:29.552121 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:06:43.852400 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:06:45.778370 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:06:47.656320 140681127527616 submission_runner.py:469] Time since start: 8858.78s, 	Step: 29890, 	{'train/accuracy': 0.9950157999992371, 'train/loss': 0.01692265272140503, 'train/mean_average_precision': 0.7184164979935426, 'validation/accuracy': 0.9864171743392944, 'validation/loss': 0.0504840724170208, 'validation/mean_average_precision': 0.24648669029315962, 'validation/num_examples': 43793, 'test/accuracy': 0.9855079054832458, 'test/loss': 0.05403723567724228, 'test/mean_average_precision': 0.23776723358315677, 'test/num_examples': 43793, 'score': 6494.924700021744, 'total_duration': 8858.77674627304, 'accumulated_submission_time': 6494.924700021744, 'accumulated_eval_time': 2362.4342861175537, 'accumulated_logging_time': 0.6283829212188721}
I0307 19:06:47.668628 140539371349760 logging_writer.py:48] [29890] accumulated_eval_time=2362.43, accumulated_logging_time=0.628383, accumulated_submission_time=6494.92, global_step=29890, preemption_count=0, score=6494.92, test/accuracy=0.985508, test/loss=0.0540372, test/mean_average_precision=0.237767, test/num_examples=43793, total_duration=8858.78, train/accuracy=0.995016, train/loss=0.0169227, train/mean_average_precision=0.718416, validation/accuracy=0.986417, validation/loss=0.0504841, validation/mean_average_precision=0.246487, validation/num_examples=43793
I0307 19:06:50.093801 140539379742464 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.04533951357007027, loss=0.028150461614131927
I0307 19:07:11.981090 140539371349760 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.04523424059152603, loss=0.027674539014697075
I0307 19:07:33.754904 140539379742464 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.040995631366968155, loss=0.0287613682448864
I0307 19:07:55.783543 140539371349760 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.04358872398734093, loss=0.028237121179699898
I0307 19:08:17.832862 140539379742464 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.055001020431518555, loss=0.028692137449979782
I0307 19:08:40.045564 140539371349760 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.04824360832571983, loss=0.029444681480526924
I0307 19:09:02.105872 140539379742464 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.04732039198279381, loss=0.028151972219347954
I0307 19:09:24.021298 140539371349760 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.0522293820977211, loss=0.028859920799732208
I0307 19:09:45.692294 140539379742464 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.04538360983133316, loss=0.028963452205061913
I0307 19:10:07.193168 140539371349760 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05121685191988945, loss=0.028943568468093872
I0307 19:10:28.900852 140539379742464 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.04022511467337608, loss=0.02638385444879532
I0307 19:10:47.704589 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:12:03.215670 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:12:05.213493 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:12:07.292032 140681127527616 submission_runner.py:469] Time since start: 9178.41s, 	Step: 30986, 	{'train/accuracy': 0.9939271211624146, 'train/loss': 0.019201230257749557, 'train/mean_average_precision': 0.6621570320546166, 'validation/accuracy': 0.9864354729652405, 'validation/loss': 0.051746681332588196, 'validation/mean_average_precision': 0.2444029919437009, 'validation/num_examples': 43793, 'test/accuracy': 0.9855096340179443, 'test/loss': 0.05538325011730194, 'test/mean_average_precision': 0.23533579322350964, 'test/num_examples': 43793, 'score': 6734.922221422195, 'total_duration': 9178.412378787994, 'accumulated_submission_time': 6734.922221422195, 'accumulated_eval_time': 2442.021591901779, 'accumulated_logging_time': 0.6507487297058105}
I0307 19:12:07.304929 140539371349760 logging_writer.py:48] [30986] accumulated_eval_time=2442.02, accumulated_logging_time=0.650749, accumulated_submission_time=6734.92, global_step=30986, preemption_count=0, score=6734.92, test/accuracy=0.98551, test/loss=0.0553833, test/mean_average_precision=0.235336, test/num_examples=43793, total_duration=9178.41, train/accuracy=0.993927, train/loss=0.0192012, train/mean_average_precision=0.662157, validation/accuracy=0.986435, validation/loss=0.0517467, validation/mean_average_precision=0.244403, validation/num_examples=43793
I0307 19:12:10.468141 140539379742464 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.046503178775310516, loss=0.026064952835440636
I0307 19:12:32.556529 140539371349760 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.04251446574926376, loss=0.024732014164328575
I0307 19:12:54.311205 140539379742464 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.04702065512537956, loss=0.02825506217777729
I0307 19:13:16.046563 140539371349760 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.05096755549311638, loss=0.028766220435500145
I0307 19:13:37.854077 140539379742464 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.059340596199035645, loss=0.029015667736530304
I0307 19:13:59.811357 140539371349760 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.04834559187293053, loss=0.026801470667123795
I0307 19:14:21.345998 140539379742464 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.05304620787501335, loss=0.02739160880446434
I0307 19:14:42.797844 140539371349760 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04722004756331444, loss=0.027584971860051155
I0307 19:15:04.497735 140539379742464 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.046386074274778366, loss=0.028731120750308037
I0307 19:15:26.281089 140539371349760 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.04814017191529274, loss=0.02855365164577961
I0307 19:15:48.016119 140539379742464 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05103738605976105, loss=0.02691492810845375
I0307 19:16:07.325507 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:17:20.738147 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:17:22.875163 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:17:24.852514 140681127527616 submission_runner.py:469] Time since start: 9495.97s, 	Step: 32090, 	{'train/accuracy': 0.9947919249534607, 'train/loss': 0.017432861030101776, 'train/mean_average_precision': 0.6972565675726514, 'validation/accuracy': 0.9862750768661499, 'validation/loss': 0.05191296711564064, 'validation/mean_average_precision': 0.24740458532398987, 'validation/num_examples': 43793, 'test/accuracy': 0.9853023886680603, 'test/loss': 0.055560316890478134, 'test/mean_average_precision': 0.23129149749969194, 'test/num_examples': 43793, 'score': 6974.904255151749, 'total_duration': 9495.972897529602, 'accumulated_submission_time': 6974.904255151749, 'accumulated_eval_time': 2519.548498392105, 'accumulated_logging_time': 0.6738758087158203}
I0307 19:17:24.865982 140539371349760 logging_writer.py:48] [32090] accumulated_eval_time=2519.55, accumulated_logging_time=0.673876, accumulated_submission_time=6974.9, global_step=32090, preemption_count=0, score=6974.9, test/accuracy=0.985302, test/loss=0.0555603, test/mean_average_precision=0.231291, test/num_examples=43793, total_duration=9495.97, train/accuracy=0.994792, train/loss=0.0174329, train/mean_average_precision=0.697257, validation/accuracy=0.986275, validation/loss=0.051913, validation/mean_average_precision=0.247405, validation/num_examples=43793
I0307 19:17:27.343185 140539379742464 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.056064631789922714, loss=0.02841072715818882
I0307 19:17:49.085591 140539371349760 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.054591115564107895, loss=0.027231091633439064
I0307 19:18:10.923974 140539379742464 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.053222138434648514, loss=0.028063936159014702
I0307 19:18:32.771146 140539371349760 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.05066962540149689, loss=0.027939576655626297
I0307 19:18:54.587153 140539379742464 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0523652508854866, loss=0.025504548102617264
I0307 19:19:16.599329 140539371349760 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.05020535737276077, loss=0.02736920863389969
I0307 19:19:38.434038 140539379742464 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.0464383065700531, loss=0.026271497830748558
I0307 19:20:00.765374 140539371349760 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.050216346979141235, loss=0.027194933965802193
I0307 19:20:22.860400 140539379742464 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05196871608495712, loss=0.026989882811903954
I0307 19:20:45.260133 140539371349760 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05388326197862625, loss=0.027859820052981377
I0307 19:21:07.281620 140539379742464 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05151939392089844, loss=0.025676313787698746
I0307 19:21:24.861418 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:22:42.000039 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:22:46.460062 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:22:48.519494 140681127527616 submission_runner.py:469] Time since start: 9819.64s, 	Step: 33181, 	{'train/accuracy': 0.9951968193054199, 'train/loss': 0.0158045906573534, 'train/mean_average_precision': 0.7449554460251785, 'validation/accuracy': 0.9863039255142212, 'validation/loss': 0.05318564176559448, 'validation/mean_average_precision': 0.24087646253364822, 'validation/num_examples': 43793, 'test/accuracy': 0.985432505607605, 'test/loss': 0.05694403126835823, 'test/mean_average_precision': 0.23087480566131038, 'test/num_examples': 43793, 'score': 7214.861590623856, 'total_duration': 9819.639766931534, 'accumulated_submission_time': 7214.861590623856, 'accumulated_eval_time': 2603.2063603401184, 'accumulated_logging_time': 0.6973221302032471}
I0307 19:22:48.533413 140539371349760 logging_writer.py:48] [33181] accumulated_eval_time=2603.21, accumulated_logging_time=0.697322, accumulated_submission_time=7214.86, global_step=33181, preemption_count=0, score=7214.86, test/accuracy=0.985433, test/loss=0.056944, test/mean_average_precision=0.230875, test/num_examples=43793, total_duration=9819.64, train/accuracy=0.995197, train/loss=0.0158046, train/mean_average_precision=0.744955, validation/accuracy=0.986304, validation/loss=0.0531856, validation/mean_average_precision=0.240876, validation/num_examples=43793
I0307 19:22:52.991566 140539379742464 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.057291120290756226, loss=0.027658727020025253
I0307 19:23:14.797899 140539371349760 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.04851198568940163, loss=0.026290064677596092
I0307 19:23:36.789755 140539379742464 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.04952272027730942, loss=0.024898642674088478
I0307 19:23:58.973422 140539371349760 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.051581259816884995, loss=0.02812414988875389
I0307 19:24:21.208142 140539379742464 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.054842542856931686, loss=0.026927974075078964
I0307 19:24:43.024180 140539371349760 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.04915565624833107, loss=0.025790253654122353
I0307 19:25:04.773381 140539379742464 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05326637998223305, loss=0.026151062920689583
I0307 19:25:26.662743 140539371349760 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.05240162834525108, loss=0.027746085077524185
I0307 19:25:48.969391 140539379742464 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.05247291550040245, loss=0.027044665068387985
I0307 19:26:11.026995 140539371349760 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.05204470455646515, loss=0.025859886780381203
I0307 19:26:32.764245 140539379742464 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.05501113086938858, loss=0.02684352919459343
I0307 19:26:48.602085 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:28:03.232818 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:28:05.300040 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:28:07.270590 140681127527616 submission_runner.py:469] Time since start: 10138.39s, 	Step: 34274, 	{'train/accuracy': 0.9943652153015137, 'train/loss': 0.017712391912937164, 'train/mean_average_precision': 0.7002198268441346, 'validation/accuracy': 0.9862844347953796, 'validation/loss': 0.05365937575697899, 'validation/mean_average_precision': 0.24209437638856285, 'validation/num_examples': 43793, 'test/accuracy': 0.9853491187095642, 'test/loss': 0.057420093566179276, 'test/mean_average_precision': 0.22993860785917022, 'test/num_examples': 43793, 'score': 7454.888871192932, 'total_duration': 10138.390893697739, 'accumulated_submission_time': 7454.888871192932, 'accumulated_eval_time': 2681.874681711197, 'accumulated_logging_time': 0.721172571182251}
I0307 19:28:07.284316 140539371349760 logging_writer.py:48] [34274] accumulated_eval_time=2681.87, accumulated_logging_time=0.721173, accumulated_submission_time=7454.89, global_step=34274, preemption_count=0, score=7454.89, test/accuracy=0.985349, test/loss=0.0574201, test/mean_average_precision=0.229939, test/num_examples=43793, total_duration=10138.4, train/accuracy=0.994365, train/loss=0.0177124, train/mean_average_precision=0.70022, validation/accuracy=0.986284, validation/loss=0.0536594, validation/mean_average_precision=0.242094, validation/num_examples=43793
I0307 19:28:13.329388 140539379742464 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.05572057515382767, loss=0.02722543105483055
I0307 19:28:35.354647 140539371349760 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.05596545338630676, loss=0.027103789150714874
I0307 19:28:57.582152 140539379742464 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.05161289870738983, loss=0.025528356432914734
I0307 19:29:19.661989 140539371349760 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.05042850226163864, loss=0.02621155045926571
I0307 19:29:42.135107 140539379742464 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.05264623463153839, loss=0.024933742359280586
I0307 19:30:03.827747 140539371349760 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.050137873739004135, loss=0.024878598749637604
I0307 19:30:25.198066 140539379742464 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.05284598097205162, loss=0.026100868359208107
I0307 19:30:47.033291 140539371349760 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.057997941970825195, loss=0.02709994837641716
I0307 19:31:09.086010 140539379742464 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.04900340735912323, loss=0.02521635591983795
I0307 19:31:30.921469 140539371349760 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05175166204571724, loss=0.02521471306681633
I0307 19:31:52.824075 140539379742464 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06127079203724861, loss=0.026997150853276253
I0307 19:32:07.341605 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:33:21.426848 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:33:23.488619 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:33:25.513632 140681127527616 submission_runner.py:469] Time since start: 10456.63s, 	Step: 35367, 	{'train/accuracy': 0.9962413311004639, 'train/loss': 0.013400845229625702, 'train/mean_average_precision': 0.7820493120596349, 'validation/accuracy': 0.9862003922462463, 'validation/loss': 0.05483304709196091, 'validation/mean_average_precision': 0.24495549713178533, 'validation/num_examples': 43793, 'test/accuracy': 0.9852880835533142, 'test/loss': 0.05877852067351341, 'test/mean_average_precision': 0.22590880596937812, 'test/num_examples': 43793, 'score': 7694.906746149063, 'total_duration': 10456.633985996246, 'accumulated_submission_time': 7694.906746149063, 'accumulated_eval_time': 2760.0465755462646, 'accumulated_logging_time': 0.7447490692138672}
I0307 19:33:25.527729 140539371349760 logging_writer.py:48] [35367] accumulated_eval_time=2760.05, accumulated_logging_time=0.744749, accumulated_submission_time=7694.91, global_step=35367, preemption_count=0, score=7694.91, test/accuracy=0.985288, test/loss=0.0587785, test/mean_average_precision=0.225909, test/num_examples=43793, total_duration=10456.6, train/accuracy=0.996241, train/loss=0.0134008, train/mean_average_precision=0.782049, validation/accuracy=0.9862, validation/loss=0.054833, validation/mean_average_precision=0.244955, validation/num_examples=43793
I0307 19:33:32.910819 140539379742464 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.048729293048381805, loss=0.025499699637293816
I0307 19:33:54.875007 140539371349760 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0518285296857357, loss=0.026321973651647568
I0307 19:34:17.289311 140539379742464 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.052070073783397675, loss=0.025615345686674118
I0307 19:34:39.099359 140539371349760 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.05104782059788704, loss=0.024940570816397667
I0307 19:35:01.194683 140539379742464 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.04882318153977394, loss=0.02578788995742798
I0307 19:35:23.074679 140539371349760 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05409451574087143, loss=0.025827936828136444
I0307 19:35:45.382475 140539379742464 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06011611968278885, loss=0.027114970609545708
I0307 19:36:07.501656 140539371349760 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.04633163660764694, loss=0.02409098856151104
I0307 19:36:29.251752 140539379742464 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.05383482947945595, loss=0.025348294526338577
I0307 19:36:51.188382 140539371349760 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.05005689710378647, loss=0.02473493479192257
I0307 19:37:13.437129 140539379742464 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.0570252388715744, loss=0.025253748521208763
I0307 19:37:25.623677 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:38:41.184870 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:38:43.159861 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:38:45.117463 140681127527616 submission_runner.py:469] Time since start: 10776.24s, 	Step: 36457, 	{'train/accuracy': 0.994905412197113, 'train/loss': 0.01600693352520466, 'train/mean_average_precision': 0.7346263165754756, 'validation/accuracy': 0.9861776828765869, 'validation/loss': 0.05546123906970024, 'validation/mean_average_precision': 0.2342912991630984, 'validation/num_examples': 43793, 'test/accuracy': 0.9852594137191772, 'test/loss': 0.05940711125731468, 'test/mean_average_precision': 0.22573166413569928, 'test/num_examples': 43793, 'score': 7934.964791297913, 'total_duration': 10776.23775434494, 'accumulated_submission_time': 7934.964791297913, 'accumulated_eval_time': 2839.5401697158813, 'accumulated_logging_time': 0.769134521484375}
I0307 19:38:45.131597 140539371349760 logging_writer.py:48] [36457] accumulated_eval_time=2839.54, accumulated_logging_time=0.769135, accumulated_submission_time=7934.96, global_step=36457, preemption_count=0, score=7934.96, test/accuracy=0.985259, test/loss=0.0594071, test/mean_average_precision=0.225732, test/num_examples=43793, total_duration=10776.2, train/accuracy=0.994905, train/loss=0.0160069, train/mean_average_precision=0.734626, validation/accuracy=0.986178, validation/loss=0.0554612, validation/mean_average_precision=0.234291, validation/num_examples=43793
I0307 19:38:54.761887 140539379742464 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0560922846198082, loss=0.024678483605384827
I0307 19:39:16.949238 140539371349760 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06012643873691559, loss=0.026215603575110435
I0307 19:39:38.813641 140539379742464 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.057086993008852005, loss=0.02365635707974434
I0307 19:40:00.494824 140539371349760 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.057105161249637604, loss=0.02392091415822506
I0307 19:40:22.064605 140539379742464 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.04660140350461006, loss=0.02322850190103054
I0307 19:40:43.796724 140539371349760 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.04671334847807884, loss=0.024639667943120003
I0307 19:41:06.019596 140539379742464 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.05472203344106674, loss=0.02511673793196678
I0307 19:41:27.929945 140539371349760 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.05243461951613426, loss=0.02484341338276863
I0307 19:41:49.774379 140539379742464 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.0547882542014122, loss=0.025659341365098953
I0307 19:42:11.435431 140539371349760 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.0526348352432251, loss=0.02447616308927536
I0307 19:42:33.242734 140539379742464 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.05268725007772446, loss=0.026223525404930115
I0307 19:42:45.274991 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:44:00.830222 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:44:02.940707 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:44:04.991966 140681127527616 submission_runner.py:469] Time since start: 11096.11s, 	Step: 37557, 	{'train/accuracy': 0.9956643581390381, 'train/loss': 0.014182097278535366, 'train/mean_average_precision': 0.7695155245881993, 'validation/accuracy': 0.986148476600647, 'validation/loss': 0.05685465782880783, 'validation/mean_average_precision': 0.2360836863060357, 'validation/num_examples': 43793, 'test/accuracy': 0.985171377658844, 'test/loss': 0.06084699183702469, 'test/mean_average_precision': 0.22139841830616547, 'test/num_examples': 43793, 'score': 8175.070830345154, 'total_duration': 11096.11225438118, 'accumulated_submission_time': 8175.070830345154, 'accumulated_eval_time': 2919.2569456100464, 'accumulated_logging_time': 0.7927367687225342}
I0307 19:44:05.005427 140539371349760 logging_writer.py:48] [37557] accumulated_eval_time=2919.26, accumulated_logging_time=0.792737, accumulated_submission_time=8175.07, global_step=37557, preemption_count=0, score=8175.07, test/accuracy=0.985171, test/loss=0.060847, test/mean_average_precision=0.221398, test/num_examples=43793, total_duration=11096.1, train/accuracy=0.995664, train/loss=0.0141821, train/mean_average_precision=0.769516, validation/accuracy=0.986148, validation/loss=0.0568547, validation/mean_average_precision=0.236084, validation/num_examples=43793
I0307 19:44:14.522078 140539379742464 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.05649067461490631, loss=0.02474714070558548
I0307 19:44:36.299443 140539371349760 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.0534033365547657, loss=0.02345883473753929
I0307 19:44:58.358827 140539379742464 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.052915193140506744, loss=0.022888677194714546
I0307 19:45:20.262302 140539371349760 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06040079891681671, loss=0.024336418136954308
I0307 19:45:42.160916 140539379742464 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.054602380841970444, loss=0.024591300636529922
I0307 19:46:03.928662 140539371349760 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.05964064598083496, loss=0.024440951645374298
I0307 19:46:26.048729 140539379742464 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.05591467022895813, loss=0.024200772866606712
I0307 19:46:48.259378 140539371349760 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.053874336183071136, loss=0.022836793214082718
I0307 19:47:10.440345 140539379742464 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06065627560019493, loss=0.02457364648580551
I0307 19:47:32.295987 140539371349760 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.050769783556461334, loss=0.024605456739664078
I0307 19:47:54.286901 140539379742464 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.05151383578777313, loss=0.02310863323509693
I0307 19:48:05.051647 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:49:22.513431 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:49:24.523714 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:49:26.547703 140681127527616 submission_runner.py:469] Time since start: 11417.67s, 	Step: 38650, 	{'train/accuracy': 0.9966215491294861, 'train/loss': 0.01235384214669466, 'train/mean_average_precision': 0.8127454760132536, 'validation/accuracy': 0.9860749840736389, 'validation/loss': 0.057182252407073975, 'validation/mean_average_precision': 0.23225317108569346, 'validation/num_examples': 43793, 'test/accuracy': 0.9851073622703552, 'test/loss': 0.06137857958674431, 'test/mean_average_precision': 0.2184106255203696, 'test/num_examples': 43793, 'score': 8415.077866792679, 'total_duration': 11417.668135643005, 'accumulated_submission_time': 8415.077866792679, 'accumulated_eval_time': 3000.7529504299164, 'accumulated_logging_time': 0.8158724308013916}
I0307 19:49:26.562488 140539371349760 logging_writer.py:48] [38650] accumulated_eval_time=3000.75, accumulated_logging_time=0.815872, accumulated_submission_time=8415.08, global_step=38650, preemption_count=0, score=8415.08, test/accuracy=0.985107, test/loss=0.0613786, test/mean_average_precision=0.218411, test/num_examples=43793, total_duration=11417.7, train/accuracy=0.996622, train/loss=0.0123538, train/mean_average_precision=0.812745, validation/accuracy=0.986075, validation/loss=0.0571823, validation/mean_average_precision=0.232253, validation/num_examples=43793
I0307 19:49:37.721590 140539379742464 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.04672025516629219, loss=0.023077912628650665
I0307 19:49:59.415074 140539371349760 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.04851389676332474, loss=0.022762876003980637
I0307 19:50:21.427409 140539379742464 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.04910193383693695, loss=0.02364610694348812
I0307 19:50:44.306579 140539371349760 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.05543627589941025, loss=0.024614771828055382
I0307 19:51:06.971867 140539379742464 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.054071344435214996, loss=0.023865211755037308
I0307 19:51:28.922433 140539371349760 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.06431557238101959, loss=0.026106925681233406
I0307 19:51:50.931720 140539379742464 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.054905787110328674, loss=0.025079194456338882
I0307 19:52:12.746812 140539371349760 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.04997444525361061, loss=0.023021573200821877
I0307 19:52:34.510173 140539379742464 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.04923063516616821, loss=0.02389521151781082
I0307 19:52:56.196860 140539371349760 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.053605660796165466, loss=0.022113164886832237
I0307 19:53:18.304550 140539379742464 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.052774518728256226, loss=0.02432127669453621
I0307 19:53:26.731664 140681127527616 spec.py:321] Evaluating on the training split.
I0307 19:54:38.909213 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 19:54:41.021361 140681127527616 spec.py:349] Evaluating on the test split.
I0307 19:54:43.127299 140681127527616 submission_runner.py:469] Time since start: 11734.25s, 	Step: 39740, 	{'train/accuracy': 0.9950730800628662, 'train/loss': 0.01540119294077158, 'train/mean_average_precision': 0.7420471551511711, 'validation/accuracy': 0.9860648512840271, 'validation/loss': 0.05806157365441322, 'validation/mean_average_precision': 0.23402133550235252, 'validation/num_examples': 43793, 'test/accuracy': 0.9850812554359436, 'test/loss': 0.06228027120232582, 'test/mean_average_precision': 0.221019696144846, 'test/num_examples': 43793, 'score': 8655.207736492157, 'total_duration': 11734.24772143364, 'accumulated_submission_time': 8655.207736492157, 'accumulated_eval_time': 3077.14851975441, 'accumulated_logging_time': 0.8406710624694824}
I0307 19:54:43.141280 140539371349760 logging_writer.py:48] [39740] accumulated_eval_time=3077.15, accumulated_logging_time=0.840671, accumulated_submission_time=8655.21, global_step=39740, preemption_count=0, score=8655.21, test/accuracy=0.985081, test/loss=0.0622803, test/mean_average_precision=0.22102, test/num_examples=43793, total_duration=11734.2, train/accuracy=0.995073, train/loss=0.0154012, train/mean_average_precision=0.742047, validation/accuracy=0.986065, validation/loss=0.0580616, validation/mean_average_precision=0.234021, validation/num_examples=43793
I0307 19:54:59.185206 140539379742464 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.061682991683483124, loss=0.023774772882461548
I0307 19:55:21.086529 140539371349760 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.05202379450201988, loss=0.024285530671477318
I0307 19:55:44.111697 140539379742464 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.06099183112382889, loss=0.02400147169828415
I0307 19:56:07.493796 140539371349760 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.04620738700032234, loss=0.02207118272781372
I0307 19:56:29.485605 140539379742464 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.05802328884601593, loss=0.02375887520611286
I0307 19:56:52.506591 140539371349760 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.047894448041915894, loss=0.021492650732398033
I0307 19:57:15.944656 140539379742464 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.05403647571802139, loss=0.022908197715878487
I0307 19:57:37.683187 140539371349760 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.05678185448050499, loss=0.023223359137773514
I0307 19:58:00.837630 140539379742464 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.05418456345796585, loss=0.02324676886200905
I0307 19:58:23.927335 140539371349760 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.050322163850069046, loss=0.021935636177659035
I0307 19:58:43.292859 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:00:02.417547 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:00:05.917787 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:00:07.997460 140681127527616 submission_runner.py:469] Time since start: 12059.12s, 	Step: 40791, 	{'train/accuracy': 0.9962680339813232, 'train/loss': 0.012959683313965797, 'train/mean_average_precision': 0.8086942489551621, 'validation/accuracy': 0.9860067963600159, 'validation/loss': 0.05914413556456566, 'validation/mean_average_precision': 0.22812119340760142, 'validation/num_examples': 43793, 'test/accuracy': 0.985074520111084, 'test/loss': 0.0632818192243576, 'test/mean_average_precision': 0.21857273568098864, 'test/num_examples': 43793, 'score': 8895.314485311508, 'total_duration': 12059.117860078812, 'accumulated_submission_time': 8895.314485311508, 'accumulated_eval_time': 3161.8530378341675, 'accumulated_logging_time': 0.8682987689971924}
I0307 20:00:08.011515 140539379742464 logging_writer.py:48] [40791] accumulated_eval_time=3161.85, accumulated_logging_time=0.868299, accumulated_submission_time=8895.31, global_step=40791, preemption_count=0, score=8895.31, test/accuracy=0.985075, test/loss=0.0632818, test/mean_average_precision=0.218573, test/num_examples=43793, total_duration=12059.1, train/accuracy=0.996268, train/loss=0.0129597, train/mean_average_precision=0.808694, validation/accuracy=0.986007, validation/loss=0.0591441, validation/mean_average_precision=0.228121, validation/num_examples=43793
I0307 20:00:10.251470 140539371349760 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.04812902584671974, loss=0.02245410718023777
I0307 20:00:32.301821 140539379742464 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.05488208308815956, loss=0.024805948138237
I0307 20:00:55.564573 140539371349760 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.04605688527226448, loss=0.022161759436130524
I0307 20:01:18.949103 140539379742464 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.0496058352291584, loss=0.02278743125498295
I0307 20:01:40.885509 140539371349760 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.049591269344091415, loss=0.02232150360941887
I0307 20:02:04.295819 140539379742464 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.05434098467230797, loss=0.022862624377012253
I0307 20:02:27.574889 140539371349760 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.05365714058279991, loss=0.02378946729004383
I0307 20:02:49.374806 140539379742464 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.059098903089761734, loss=0.023620963096618652
I0307 20:03:12.360225 140539371349760 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.04641404747962952, loss=0.02297394908964634
I0307 20:03:34.054573 140539379742464 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.047743286937475204, loss=0.02133161574602127
I0307 20:03:57.371529 140539371349760 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.05350374057888985, loss=0.022151829674839973
I0307 20:04:08.004894 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:05:28.243298 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:05:30.342767 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:05:32.411405 140681127527616 submission_runner.py:469] Time since start: 12383.53s, 	Step: 41850, 	{'train/accuracy': 0.9967944025993347, 'train/loss': 0.011492342688143253, 'train/mean_average_precision': 0.8188840064696002, 'validation/accuracy': 0.9860250353813171, 'validation/loss': 0.05982394888997078, 'validation/mean_average_precision': 0.22915862541401752, 'validation/num_examples': 43793, 'test/accuracy': 0.9851305484771729, 'test/loss': 0.06414381414651871, 'test/mean_average_precision': 0.21542256282912498, 'test/num_examples': 43793, 'score': 9135.268005132675, 'total_duration': 12383.531695842743, 'accumulated_submission_time': 9135.268005132675, 'accumulated_eval_time': 3246.259354829788, 'accumulated_logging_time': 0.8931386470794678}
I0307 20:05:32.427785 140539379742464 logging_writer.py:48] [41850] accumulated_eval_time=3246.26, accumulated_logging_time=0.893139, accumulated_submission_time=9135.27, global_step=41850, preemption_count=0, score=9135.27, test/accuracy=0.985131, test/loss=0.0641438, test/mean_average_precision=0.215423, test/num_examples=43793, total_duration=12383.5, train/accuracy=0.996794, train/loss=0.0114923, train/mean_average_precision=0.818884, validation/accuracy=0.986025, validation/loss=0.0598239, validation/mean_average_precision=0.229159, validation/num_examples=43793
I0307 20:05:43.573235 140539371349760 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.05219883844256401, loss=0.023093998432159424
I0307 20:06:07.263950 140539379742464 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.05747650936245918, loss=0.024090057238936424
I0307 20:06:29.204594 140539371349760 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.047948434948921204, loss=0.0235408004373312
I0307 20:06:52.910118 140539379742464 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.04668876528739929, loss=0.02252401039004326
I0307 20:07:16.392446 140539371349760 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.04090757295489311, loss=0.020785311236977577
I0307 20:07:38.674445 140539379742464 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.0428432822227478, loss=0.02075241319835186
I0307 20:08:02.042444 140539371349760 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.05497422441840172, loss=0.023219730705022812
I0307 20:08:25.368164 140539379742464 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.05204644426703453, loss=0.02261275425553322
I0307 20:08:47.641022 140539371349760 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.047809261828660965, loss=0.021431971341371536
I0307 20:09:11.260017 140539379742464 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.05055532231926918, loss=0.02317395992577076
I0307 20:09:32.555266 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:10:52.474296 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:10:54.582484 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:10:56.608660 140681127527616 submission_runner.py:469] Time since start: 12707.73s, 	Step: 42891, 	{'train/accuracy': 0.9952070116996765, 'train/loss': 0.014795399270951748, 'train/mean_average_precision': 0.7647963971867565, 'validation/accuracy': 0.9859657883644104, 'validation/loss': 0.060548730194568634, 'validation/mean_average_precision': 0.22370711043783192, 'validation/num_examples': 43793, 'test/accuracy': 0.9850673675537109, 'test/loss': 0.06475042551755905, 'test/mean_average_precision': 0.21328248732083535, 'test/num_examples': 43793, 'score': 9375.35702419281, 'total_duration': 12707.729056835175, 'accumulated_submission_time': 9375.35702419281, 'accumulated_eval_time': 3330.3126599788666, 'accumulated_logging_time': 0.920159101486206}
I0307 20:10:56.623267 140539371349760 logging_writer.py:48] [42891] accumulated_eval_time=3330.31, accumulated_logging_time=0.920159, accumulated_submission_time=9375.36, global_step=42891, preemption_count=0, score=9375.36, test/accuracy=0.985067, test/loss=0.0647504, test/mean_average_precision=0.213282, test/num_examples=43793, total_duration=12707.7, train/accuracy=0.995207, train/loss=0.0147954, train/mean_average_precision=0.764796, validation/accuracy=0.985966, validation/loss=0.0605487, validation/mean_average_precision=0.223707, validation/num_examples=43793
I0307 20:10:58.765850 140539379742464 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.052171334624290466, loss=0.02386000193655491
I0307 20:11:21.741297 140539371349760 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.043653588742017746, loss=0.02252049185335636
I0307 20:11:44.142189 140539379742464 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.04602520167827606, loss=0.021494561806321144
I0307 20:12:07.824189 140539371349760 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.05202077701687813, loss=0.02428724244236946
I0307 20:12:31.158561 140539379742464 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.04803686961531639, loss=0.022998202592134476
I0307 20:12:53.333732 140539371349760 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.04723099619150162, loss=0.02257566712796688
I0307 20:13:16.772596 140539379742464 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.04756767302751541, loss=0.022175291553139687
I0307 20:13:40.420663 140539371349760 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.10014329105615616, loss=0.02534460835158825
I0307 20:14:02.957050 140539379742464 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.05000387877225876, loss=0.023081423714756966
I0307 20:14:26.356215 140539371349760 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.04624646529555321, loss=0.022583644837141037
I0307 20:14:49.663723 140539379742464 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.05498539283871651, loss=0.023543326184153557
I0307 20:14:56.620234 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:16:13.484213 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:16:15.560035 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:16:17.606339 140681127527616 submission_runner.py:469] Time since start: 13028.73s, 	Step: 43933, 	{'train/accuracy': 0.9973065853118896, 'train/loss': 0.010488463565707207, 'train/mean_average_precision': 0.8563660520742207, 'validation/accuracy': 0.9860116243362427, 'validation/loss': 0.061013076454401016, 'validation/mean_average_precision': 0.22908297287198628, 'validation/num_examples': 43793, 'test/accuracy': 0.9850635528564453, 'test/loss': 0.06536463648080826, 'test/mean_average_precision': 0.21398566318342374, 'test/num_examples': 43793, 'score': 9615.314539194107, 'total_duration': 13028.726650714874, 'accumulated_submission_time': 9615.314539194107, 'accumulated_eval_time': 3411.298595905304, 'accumulated_logging_time': 0.9459872245788574}
I0307 20:16:17.620802 140539371349760 logging_writer.py:48] [43933] accumulated_eval_time=3411.3, accumulated_logging_time=0.945987, accumulated_submission_time=9615.31, global_step=43933, preemption_count=0, score=9615.31, test/accuracy=0.985064, test/loss=0.0653646, test/mean_average_precision=0.213986, test/num_examples=43793, total_duration=13028.7, train/accuracy=0.997307, train/loss=0.0104885, train/mean_average_precision=0.856366, validation/accuracy=0.986012, validation/loss=0.0610131, validation/mean_average_precision=0.229083, validation/num_examples=43793
I0307 20:16:34.463272 140539379742464 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.054100193083286285, loss=0.02290535531938076
I0307 20:16:56.493626 140539371349760 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.046431686729192734, loss=0.022297237068414688
I0307 20:17:20.253311 140539379742464 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.04854883626103401, loss=0.022512879222631454
I0307 20:17:44.036112 140539371349760 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.059161070734262466, loss=0.023266272619366646
I0307 20:18:05.874938 140539379742464 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.04601503908634186, loss=0.022611413151025772
I0307 20:18:29.385322 140539371349760 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.05151557922363281, loss=0.024592388421297073
I0307 20:18:52.728135 140539379742464 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.05231871083378792, loss=0.022978968918323517
I0307 20:19:14.594000 140539371349760 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.048477355390787125, loss=0.022809643298387527
I0307 20:19:37.626944 140539379742464 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.04986433684825897, loss=0.022965170443058014
I0307 20:19:59.540231 140539371349760 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.048488881438970566, loss=0.023177849128842354
I0307 20:20:17.608570 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:21:38.227593 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:21:40.239231 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:21:42.256920 140681127527616 submission_runner.py:469] Time since start: 13353.38s, 	Step: 44975, 	{'train/accuracy': 0.9977905750274658, 'train/loss': 0.009721540845930576, 'train/mean_average_precision': 0.8649640297539942, 'validation/accuracy': 0.9859353303909302, 'validation/loss': 0.06185228005051613, 'validation/mean_average_precision': 0.22119776795887428, 'validation/num_examples': 43793, 'test/accuracy': 0.9850159883499146, 'test/loss': 0.06618864834308624, 'test/mean_average_precision': 0.21143031014602137, 'test/num_examples': 43793, 'score': 9855.26269030571, 'total_duration': 13353.377353429794, 'accumulated_submission_time': 9855.26269030571, 'accumulated_eval_time': 3495.9469141960144, 'accumulated_logging_time': 0.9704456329345703}
I0307 20:21:42.271293 140539379742464 logging_writer.py:48] [44975] accumulated_eval_time=3495.95, accumulated_logging_time=0.970446, accumulated_submission_time=9855.26, global_step=44975, preemption_count=0, score=9855.26, test/accuracy=0.985016, test/loss=0.0661886, test/mean_average_precision=0.21143, test/num_examples=43793, total_duration=13353.4, train/accuracy=0.997791, train/loss=0.00972154, train/mean_average_precision=0.864964, validation/accuracy=0.985935, validation/loss=0.0618523, validation/mean_average_precision=0.221198, validation/num_examples=43793
I0307 20:21:49.432620 140539371349760 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.05132899060845375, loss=0.022412635385990143
I0307 20:22:11.392295 140539379742464 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.05276917293667793, loss=0.022286714985966682
I0307 20:22:34.575916 140539371349760 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.04707619175314903, loss=0.022367332130670547
I0307 20:22:57.274928 140539379742464 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.04512740299105644, loss=0.02017947845160961
I0307 20:23:19.530167 140539371349760 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.04363308101892471, loss=0.022367794066667557
I0307 20:23:42.587967 140539379742464 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.041386526077985764, loss=0.022053489461541176
I0307 20:24:04.742696 140539371349760 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.043058544397354126, loss=0.022251851856708527
I0307 20:24:27.822641 140539379742464 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.048795152455568314, loss=0.021278437227010727
I0307 20:24:50.924474 140539371349760 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.04285891354084015, loss=0.022496270015835762
I0307 20:25:12.784428 140539379742464 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.05215330421924591, loss=0.023730069398880005
I0307 20:25:36.132951 140539371349760 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.04633944109082222, loss=0.02243245393037796
I0307 20:25:42.362961 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:26:59.093862 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:27:01.031081 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:27:03.755107 140681127527616 submission_runner.py:469] Time since start: 13674.88s, 	Step: 46029, 	{'train/accuracy': 0.99576735496521, 'train/loss': 0.013160244561731815, 'train/mean_average_precision': 0.8003886349363453, 'validation/accuracy': 0.9859138131141663, 'validation/loss': 0.061825551092624664, 'validation/mean_average_precision': 0.22507051291977662, 'validation/num_examples': 43793, 'test/accuracy': 0.9849889874458313, 'test/loss': 0.0662682056427002, 'test/mean_average_precision': 0.21024573095020252, 'test/num_examples': 43793, 'score': 10095.315613031387, 'total_duration': 13674.875415086746, 'accumulated_submission_time': 10095.315613031387, 'accumulated_eval_time': 3577.3388850688934, 'accumulated_logging_time': 0.9947977066040039}
I0307 20:27:03.769524 140539379742464 logging_writer.py:48] [46029] accumulated_eval_time=3577.34, accumulated_logging_time=0.994798, accumulated_submission_time=10095.3, global_step=46029, preemption_count=0, score=10095.3, test/accuracy=0.984989, test/loss=0.0662682, test/mean_average_precision=0.210246, test/num_examples=43793, total_duration=13674.9, train/accuracy=0.995767, train/loss=0.0131602, train/mean_average_precision=0.800389, validation/accuracy=0.985914, validation/loss=0.0618256, validation/mean_average_precision=0.225071, validation/num_examples=43793
I0307 20:27:19.601814 140539371349760 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.047901589423418045, loss=0.02279878780245781
I0307 20:27:42.833781 140539379742464 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.046326156705617905, loss=0.022731365635991096
I0307 20:28:04.354083 140539371349760 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.03629206120967865, loss=0.021226614713668823
I0307 20:28:27.563559 140539379742464 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.0418345108628273, loss=0.02260684221982956
I0307 20:28:50.681664 140539371349760 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.04712975025177002, loss=0.021431582048535347
I0307 20:29:12.550373 140539379742464 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.045156072825193405, loss=0.022258980199694633
I0307 20:29:35.691907 140539371349760 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.04307723417878151, loss=0.022289112210273743
I0307 20:29:58.934299 140539379742464 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.04498168081045151, loss=0.02172149159014225
I0307 20:30:20.590611 140539371349760 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.041384413838386536, loss=0.02270103059709072
I0307 20:30:43.475452 140539379742464 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.04212936386466026, loss=0.022769998759031296
I0307 20:31:03.790896 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:32:21.192627 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:32:23.480943 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:32:25.504463 140681127527616 submission_runner.py:469] Time since start: 13996.62s, 	Step: 47092, 	{'train/accuracy': 0.9975412487983704, 'train/loss': 0.009990371763706207, 'train/mean_average_precision': 0.8561627072996958, 'validation/accuracy': 0.9859129786491394, 'validation/loss': 0.0621907077729702, 'validation/mean_average_precision': 0.2238166018853482, 'validation/num_examples': 43793, 'test/accuracy': 0.9849873185157776, 'test/loss': 0.06659504771232605, 'test/mean_average_precision': 0.21038231333144, 'test/num_examples': 43793, 'score': 10335.297533035278, 'total_duration': 13996.62484908104, 'accumulated_submission_time': 10335.297533035278, 'accumulated_eval_time': 3659.0523521900177, 'accumulated_logging_time': 1.0190553665161133}
I0307 20:32:25.519142 140539371349760 logging_writer.py:48] [47092] accumulated_eval_time=3659.05, accumulated_logging_time=1.01906, accumulated_submission_time=10335.3, global_step=47092, preemption_count=0, score=10335.3, test/accuracy=0.984987, test/loss=0.066595, test/mean_average_precision=0.210382, test/num_examples=43793, total_duration=13996.6, train/accuracy=0.997541, train/loss=0.00999037, train/mean_average_precision=0.856163, validation/accuracy=0.985913, validation/loss=0.0621907, validation/mean_average_precision=0.223817, validation/num_examples=43793
I0307 20:32:27.562077 140539379742464 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.04217098653316498, loss=0.021454287692904472
I0307 20:32:49.320912 140539371349760 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.04142775759100914, loss=0.02225576527416706
I0307 20:33:14.826955 140539379742464 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.040289171040058136, loss=0.022460397332906723
I0307 20:33:38.004765 140539371349760 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.0450453907251358, loss=0.02175634726881981
I0307 20:33:59.843454 140539379742464 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.039620980620384216, loss=0.020079711452126503
I0307 20:34:23.369082 140539371349760 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.04162622615695, loss=0.021319100633263588
I0307 20:34:46.604302 140539379742464 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.044016387313604355, loss=0.022213781252503395
I0307 20:35:08.722506 140539371349760 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.0423320047557354, loss=0.022775588557124138
I0307 20:35:33.244854 140539379742464 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.04192283749580383, loss=0.021372584626078606
I0307 20:35:55.136619 140539371349760 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.041509225964546204, loss=0.021410765126347542
I0307 20:36:19.091412 140539379742464 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.04291505739092827, loss=0.021657882258296013
I0307 20:36:25.605928 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:38:06.999614 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:38:09.008751 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:38:11.141745 140681127527616 submission_runner.py:469] Time since start: 14342.26s, 	Step: 48130, 	{'train/accuracy': 0.9976902008056641, 'train/loss': 0.009740324690937996, 'train/mean_average_precision': 0.8675106947623336, 'validation/accuracy': 0.9859369397163391, 'validation/loss': 0.062404219061136246, 'validation/mean_average_precision': 0.22509001635530793, 'validation/num_examples': 43793, 'test/accuracy': 0.9850117564201355, 'test/loss': 0.06687269359827042, 'test/mean_average_precision': 0.2089665159698329, 'test/num_examples': 43793, 'score': 10575.345266342163, 'total_duration': 14342.262172937393, 'accumulated_submission_time': 10575.345266342163, 'accumulated_eval_time': 3764.5881111621857, 'accumulated_logging_time': 1.0442917346954346}
I0307 20:38:11.158161 140539371349760 logging_writer.py:48] [48130] accumulated_eval_time=3764.59, accumulated_logging_time=1.04429, accumulated_submission_time=10575.3, global_step=48130, preemption_count=0, score=10575.3, test/accuracy=0.985012, test/loss=0.0668727, test/mean_average_precision=0.208967, test/num_examples=43793, total_duration=14342.3, train/accuracy=0.99769, train/loss=0.00974032, train/mean_average_precision=0.867511, validation/accuracy=0.985937, validation/loss=0.0624042, validation/mean_average_precision=0.22509, validation/num_examples=43793
I0307 20:38:29.188281 140539379742464 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.04439606890082359, loss=0.022736351937055588
I0307 20:38:51.207575 140539371349760 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.044340696185827255, loss=0.02218799479305744
I0307 20:39:15.541345 140539379742464 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.03609813377261162, loss=0.02151421643793583
I0307 20:39:37.279377 140539371349760 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.04671679064631462, loss=0.021498829126358032
I0307 20:40:01.278169 140539379742464 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.04323871061205864, loss=0.02193029783666134
I0307 20:40:25.281582 140539371349760 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.046453624963760376, loss=0.021512355655431747
I0307 20:40:47.148396 140539379742464 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.04608980938792229, loss=0.022294677793979645
I0307 20:41:13.462506 140539371349760 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.04433756321668625, loss=0.021227173507213593
I0307 20:41:37.167787 140539379742464 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.0446234829723835, loss=0.022494543343782425
I0307 20:41:59.181351 140539371349760 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.04182173311710358, loss=0.021129954606294632
I0307 20:42:11.253420 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:44:05.479447 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:44:15.240301 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:44:36.461488 140681127527616 submission_runner.py:469] Time since start: 14727.58s, 	Step: 49148, 	{'train/accuracy': 0.9969300627708435, 'train/loss': 0.011011132970452309, 'train/mean_average_precision': 0.841815208447237, 'validation/accuracy': 0.9858037829399109, 'validation/loss': 0.06214151903986931, 'validation/mean_average_precision': 0.2205536472826599, 'validation/num_examples': 43793, 'test/accuracy': 0.984874427318573, 'test/loss': 0.06647491455078125, 'test/mean_average_precision': 0.20727917184459968, 'test/num_examples': 43793, 'score': 10815.399083852768, 'total_duration': 14727.581475019455, 'accumulated_submission_time': 10815.399083852768, 'accumulated_eval_time': 3909.795681476593, 'accumulated_logging_time': 1.0719881057739258}
I0307 20:44:36.542300 140539379742464 logging_writer.py:48] [49148] accumulated_eval_time=3909.8, accumulated_logging_time=1.07199, accumulated_submission_time=10815.4, global_step=49148, preemption_count=0, score=10815.4, test/accuracy=0.984874, test/loss=0.0664749, test/mean_average_precision=0.207279, test/num_examples=43793, total_duration=14727.6, train/accuracy=0.99693, train/loss=0.0110111, train/mean_average_precision=0.841815, validation/accuracy=0.985804, validation/loss=0.0621415, validation/mean_average_precision=0.220554, validation/num_examples=43793
I0307 20:44:50.006985 140539371349760 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.04879630729556084, loss=0.021878432482481003
I0307 20:45:12.048951 140539379742464 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.038104098290205, loss=0.02182883769273758
I0307 20:45:36.214485 140539371349760 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.041136473417282104, loss=0.021109815686941147
I0307 20:46:00.689150 140539379742464 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.04239307716488838, loss=0.02239588275551796
I0307 20:46:22.828035 140539371349760 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.048324789851903915, loss=0.023050149902701378
I0307 20:46:47.406985 140539379742464 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.05170785263180733, loss=0.023104172199964523
I0307 20:47:11.782946 140539371349760 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.039489079266786575, loss=0.022622788324952126
I0307 20:47:33.423290 140539379742464 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.05526278167963028, loss=0.02034219354391098
I0307 20:47:57.858301 140539371349760 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.04593171924352646, loss=0.023389875888824463
I0307 20:48:22.194925 140539379742464 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.048352763056755066, loss=0.021572167053818703
I0307 20:48:36.463512 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:50:29.036095 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:50:45.618949 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:50:50.474907 140681127527616 submission_runner.py:469] Time since start: 15101.60s, 	Step: 50165, 	{'train/accuracy': 0.996914267539978, 'train/loss': 0.010785517282783985, 'train/mean_average_precision': 0.8531941909615431, 'validation/accuracy': 0.9858890771865845, 'validation/loss': 0.06255883723497391, 'validation/mean_average_precision': 0.2242452881051335, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.06695663928985596, 'test/mean_average_precision': 0.2087962214743045, 'test/num_examples': 43793, 'score': 11055.229188919067, 'total_duration': 15101.595242023468, 'accumulated_submission_time': 11055.229188919067, 'accumulated_eval_time': 4043.806925058365, 'accumulated_logging_time': 1.214524269104004}
I0307 20:50:50.495305 140539371349760 logging_writer.py:48] [50165] accumulated_eval_time=4043.81, accumulated_logging_time=1.21452, accumulated_submission_time=11055.2, global_step=50165, preemption_count=0, score=11055.2, test/accuracy=0.984976, test/loss=0.0669566, test/mean_average_precision=0.208796, test/num_examples=43793, total_duration=15101.6, train/accuracy=0.996914, train/loss=0.0107855, train/mean_average_precision=0.853194, validation/accuracy=0.985889, validation/loss=0.0625588, validation/mean_average_precision=0.224245, validation/num_examples=43793
I0307 20:51:01.020689 140539379742464 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.050960421562194824, loss=0.021216981112957
I0307 20:51:23.271898 140539371349760 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.04580659046769142, loss=0.021952925249934196
I0307 20:51:48.089570 140539379742464 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.051712535321712494, loss=0.022892283275723457
I0307 20:52:12.526973 140539371349760 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.051558442413806915, loss=0.021981505677103996
I0307 20:52:34.730165 140539379742464 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.0667848065495491, loss=0.022757600992918015
I0307 20:52:59.470564 140539371349760 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.050912801176309586, loss=0.02158932387828827
I0307 20:53:24.113810 140539379742464 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.06095161661505699, loss=0.023366698995232582
I0307 20:53:47.146322 140539371349760 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.05992892012000084, loss=0.023116476833820343
I0307 20:54:12.114906 140539379742464 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.06279552727937698, loss=0.02324862591922283
I0307 20:54:36.352103 140539371349760 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.06559205800294876, loss=0.022572055459022522
I0307 20:54:50.512128 140681127527616 spec.py:321] Evaluating on the training split.
I0307 20:56:48.154560 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 20:57:12.318625 140681127527616 spec.py:349] Evaluating on the test split.
I0307 20:57:16.455461 140681127527616 submission_runner.py:469] Time since start: 15487.58s, 	Step: 51155, 	{'train/accuracy': 0.9976332783699036, 'train/loss': 0.0098845474421978, 'train/mean_average_precision': 0.8672128549457234, 'validation/accuracy': 0.9858216643333435, 'validation/loss': 0.06224546581506729, 'validation/mean_average_precision': 0.22321332898618293, 'validation/num_examples': 43793, 'test/accuracy': 0.9849334359169006, 'test/loss': 0.06662486493587494, 'test/mean_average_precision': 0.2072129423416019, 'test/num_examples': 43793, 'score': 11295.203172922134, 'total_duration': 15487.575869321823, 'accumulated_submission_time': 11295.203172922134, 'accumulated_eval_time': 4189.750177621841, 'accumulated_logging_time': 1.2471799850463867}
I0307 20:57:16.470691 140539379742464 logging_writer.py:48] [51155] accumulated_eval_time=4189.75, accumulated_logging_time=1.24718, accumulated_submission_time=11295.2, global_step=51155, preemption_count=0, score=11295.2, test/accuracy=0.984933, test/loss=0.0666249, test/mean_average_precision=0.207213, test/num_examples=43793, total_duration=15487.6, train/accuracy=0.997633, train/loss=0.00988455, train/mean_average_precision=0.867213, validation/accuracy=0.985822, validation/loss=0.0622455, validation/mean_average_precision=0.223213, validation/num_examples=43793
I0307 20:57:29.724569 140539371349760 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.06397393345832825, loss=0.021935265511274338
I0307 20:57:51.759900 140539379742464 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.09627176821231842, loss=0.02314605563879013
I0307 20:58:17.136582 140539371349760 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.08415383845567703, loss=0.023190833628177643
I0307 20:58:42.441920 140539379742464 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.053249213844537735, loss=0.02094084955751896
I0307 20:59:04.718924 140539371349760 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.06258446723222733, loss=0.022052550688385963
I0307 20:59:29.987824 140539379742464 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.06432868540287018, loss=0.021847795695066452
I0307 20:59:55.217355 140539371349760 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.08213537186384201, loss=0.02242864854633808
I0307 21:00:17.287024 140539379742464 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.0873083546757698, loss=0.022897707298398018
I0307 21:00:49.000933 140539371349760 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.08228351920843124, loss=0.021609844639897346
I0307 21:01:14.061336 140539379742464 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.07951314002275467, loss=0.02387327328324318
I0307 21:01:16.469401 140681127527616 spec.py:321] Evaluating on the training split.
I0307 21:03:23.445966 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 21:03:37.127413 140681127527616 spec.py:349] Evaluating on the test split.
I0307 21:03:39.199260 140681127527616 submission_runner.py:469] Time since start: 15870.32s, 	Step: 52112, 	{'train/accuracy': 0.9975487589836121, 'train/loss': 0.009948467835783958, 'train/mean_average_precision': 0.8602134859313437, 'validation/accuracy': 0.985877275466919, 'validation/loss': 0.06221054866909981, 'validation/mean_average_precision': 0.22375578237176247, 'validation/num_examples': 43793, 'test/accuracy': 0.9849380254745483, 'test/loss': 0.06661153584718704, 'test/mean_average_precision': 0.2081028431010148, 'test/num_examples': 43793, 'score': 11535.160253286362, 'total_duration': 15870.319672107697, 'accumulated_submission_time': 11535.160253286362, 'accumulated_eval_time': 4332.479960203171, 'accumulated_logging_time': 1.274245262145996}
I0307 21:03:39.214953 140539371349760 logging_writer.py:48] [52112] accumulated_eval_time=4332.48, accumulated_logging_time=1.27425, accumulated_submission_time=11535.2, global_step=52112, preemption_count=0, score=11535.2, test/accuracy=0.984938, test/loss=0.0666115, test/mean_average_precision=0.208103, test/num_examples=43793, total_duration=15870.3, train/accuracy=0.997549, train/loss=0.00994847, train/mean_average_precision=0.860213, validation/accuracy=0.985877, validation/loss=0.0622105, validation/mean_average_precision=0.223756, validation/num_examples=43793
I0307 21:04:03.002706 140539379742464 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.06474734097719193, loss=0.02280159294605255
I0307 21:04:25.088907 140539371349760 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.061443231999874115, loss=0.022982625290751457
I0307 21:04:51.233447 140539379742464 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.06594974547624588, loss=0.021620480343699455
I0307 21:05:17.415983 140539371349760 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.08013708144426346, loss=0.021629592403769493
I0307 21:05:39.562107 140539379742464 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.06415288150310516, loss=0.02261635847389698
I0307 21:06:05.845885 140539371349760 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.08486389368772507, loss=0.021365275606513023
I0307 21:06:32.152482 140539379742464 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.07079136371612549, loss=0.022159313783049583
I0307 21:06:54.119910 140539371349760 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.054924026131629944, loss=0.02261410281062126
I0307 21:07:20.756696 140539379742464 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.060645684599876404, loss=0.022955646738409996
I0307 21:07:39.281433 140681127527616 spec.py:321] Evaluating on the training split.
I0307 21:09:41.477624 140681127527616 spec.py:333] Evaluating on the validation split.
I0307 21:09:49.250032 140681127527616 spec.py:349] Evaluating on the test split.
I0307 21:09:53.027564 140681127527616 submission_runner.py:469] Time since start: 16244.15s, 	Step: 53085, 	{'train/accuracy': 0.997531533241272, 'train/loss': 0.00994553230702877, 'train/mean_average_precision': 0.8513676537016667, 'validation/accuracy': 0.985877275466919, 'validation/loss': 0.06221054866909981, 'validation/mean_average_precision': 0.22376590147394174, 'validation/num_examples': 43793, 'test/accuracy': 0.9849380254745483, 'test/loss': 0.06661153584718704, 'test/mean_average_precision': 0.20806403867485468, 'test/num_examples': 43793, 'score': 11775.186443328857, 'total_duration': 16244.147909641266, 'accumulated_submission_time': 11775.186443328857, 'accumulated_eval_time': 4466.225948810577, 'accumulated_logging_time': 1.300379991531372}
I0307 21:09:53.044097 140539371349760 logging_writer.py:48] [53085] accumulated_eval_time=4466.23, accumulated_logging_time=1.30038, accumulated_submission_time=11775.2, global_step=53085, preemption_count=0, score=11775.2, test/accuracy=0.984938, test/loss=0.0666115, test/mean_average_precision=0.208064, test/num_examples=43793, total_duration=16244.1, train/accuracy=0.997532, train/loss=0.00994553, train/mean_average_precision=0.851368, validation/accuracy=0.985877, validation/loss=0.0622105, validation/mean_average_precision=0.223766, validation/num_examples=43793
I0307 21:09:56.536756 140539379742464 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.07177790254354477, loss=0.02300886996090412
I0307 21:10:18.975323 140539371349760 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09369908273220062, loss=0.021735958755016327
I0307 21:10:44.905600 140539379742464 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.069820836186409, loss=0.022980960085988045
I0307 21:11:10.511771 140539371349760 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.07814576476812363, loss=0.02285831607878208
I0307 21:11:32.812801 140539379742464 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.07170962542295456, loss=0.02217647060751915
I0307 21:11:58.364005 140539371349760 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.05774528905749321, loss=0.022019725292921066
I0307 21:12:24.040172 140539379742464 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.05719566345214844, loss=0.02072230912744999
I0307 21:12:46.109461 140539371349760 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.06317126005887985, loss=0.021514860913157463
I0307 21:13:11.727195 140539379742464 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.07859790325164795, loss=0.021302077919244766
I0307 21:13:33.537910 140539371349760 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.07334905862808228, loss=0.02211591601371765
I0307 21:13:53.161024 140539379742464 logging_writer.py:48] [54074] global_step=54074, preemption_count=0, score=12015.3
I0307 21:13:53.302769 140681127527616 submission_runner.py:646] Tuning trial 2/5
I0307 21:13:53.302951 140681127527616 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0307 21:13:53.305871 140681127527616 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5716273188591003, 'train/loss': 0.7195250391960144, 'train/mean_average_precision': 0.022255541417572446, 'validation/accuracy': 0.5684533715248108, 'validation/loss': 0.7225921154022217, 'validation/mean_average_precision': 0.0271315223376292, 'validation/num_examples': 43793, 'test/accuracy': 0.5673698782920837, 'test/loss': 0.7226720452308655, 'test/mean_average_precision': 0.02830521083488609, 'test/num_examples': 43793, 'score': 14.009626865386963, 'total_duration': 229.68927121162415, 'accumulated_submission_time': 14.009626865386963, 'accumulated_eval_time': 215.6795220375061, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1082, {'train/accuracy': 0.986789882183075, 'train/loss': 0.05068320408463478, 'train/mean_average_precision': 0.06520019054850393, 'validation/accuracy': 0.9842275381088257, 'validation/loss': 0.05962624028325081, 'validation/mean_average_precision': 0.07007967402926334, 'validation/num_examples': 43793, 'test/accuracy': 0.9832583069801331, 'test/loss': 0.06295129656791687, 'test/mean_average_precision': 0.06607542075288701, 'test/num_examples': 43793, 'score': 254.0324490070343, 'total_duration': 552.1264216899872, 'accumulated_submission_time': 254.0324490070343, 'accumulated_eval_time': 298.04449582099915, 'accumulated_logging_time': 0.017806291580200195, 'global_step': 1082, 'preemption_count': 0}), (2182, {'train/accuracy': 0.9873363971710205, 'train/loss': 0.045514464378356934, 'train/mean_average_precision': 0.13427337334202727, 'validation/accuracy': 0.9846927523612976, 'validation/loss': 0.054520756006240845, 'validation/mean_average_precision': 0.12973835427495983, 'validation/num_examples': 43793, 'test/accuracy': 0.9836757183074951, 'test/loss': 0.05759686976671219, 'test/mean_average_precision': 0.13045619218234186, 'test/num_examples': 43793, 'score': 494.1472702026367, 'total_duration': 872.2394938468933, 'accumulated_submission_time': 494.1472702026367, 'accumulated_eval_time': 377.9940023422241, 'accumulated_logging_time': 0.03851509094238281, 'global_step': 2182, 'preemption_count': 0}), (3293, {'train/accuracy': 0.988227903842926, 'train/loss': 0.04125930368900299, 'train/mean_average_precision': 0.19321183888667243, 'validation/accuracy': 0.9853317141532898, 'validation/loss': 0.050942905247211456, 'validation/mean_average_precision': 0.16239212056515664, 'validation/num_examples': 43793, 'test/accuracy': 0.9843505024909973, 'test/loss': 0.05353173986077309, 'test/mean_average_precision': 0.15985406621486176, 'test/num_examples': 43793, 'score': 734.105954170227, 'total_duration': 1191.1928668022156, 'accumulated_submission_time': 734.105954170227, 'accumulated_eval_time': 456.9394247531891, 'accumulated_logging_time': 0.05840802192687988, 'global_step': 3293, 'preemption_count': 0}), (4407, {'train/accuracy': 0.9883833527565002, 'train/loss': 0.03985724598169327, 'train/mean_average_precision': 0.2151241399838424, 'validation/accuracy': 0.9855647087097168, 'validation/loss': 0.04890182986855507, 'validation/mean_average_precision': 0.18830423295859752, 'validation/num_examples': 43793, 'test/accuracy': 0.9846385717391968, 'test/loss': 0.05160592496395111, 'test/mean_average_precision': 0.18754837387013282, 'test/num_examples': 43793, 'score': 974.0946433544159, 'total_duration': 1511.5109298229218, 'accumulated_submission_time': 974.0946433544159, 'accumulated_eval_time': 537.2196614742279, 'accumulated_logging_time': 0.07704663276672363, 'global_step': 4407, 'preemption_count': 0}), (5522, {'train/accuracy': 0.9887790083885193, 'train/loss': 0.03839410841464996, 'train/mean_average_precision': 0.25577890577791085, 'validation/accuracy': 0.9858001470565796, 'validation/loss': 0.04812321066856384, 'validation/mean_average_precision': 0.21073301512008588, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05073074251413345, 'test/mean_average_precision': 0.20799545431461458, 'test/num_examples': 43793, 'score': 1214.0543098449707, 'total_duration': 1829.8075709342957, 'accumulated_submission_time': 1214.0543098449707, 'accumulated_eval_time': 615.4408128261566, 'accumulated_logging_time': 0.15913128852844238, 'global_step': 5522, 'preemption_count': 0}), (6645, {'train/accuracy': 0.9889899492263794, 'train/loss': 0.03749311715364456, 'train/mean_average_precision': 0.26813544641535814, 'validation/accuracy': 0.9861054420471191, 'validation/loss': 0.047119785100221634, 'validation/mean_average_precision': 0.21628643495975602, 'validation/num_examples': 43793, 'test/accuracy': 0.9852088689804077, 'test/loss': 0.04979196563363075, 'test/mean_average_precision': 0.21585612349535419, 'test/num_examples': 43793, 'score': 1454.148190498352, 'total_duration': 2148.749235868454, 'accumulated_submission_time': 1454.148190498352, 'accumulated_eval_time': 694.2411503791809, 'accumulated_logging_time': 0.17903685569763184, 'global_step': 6645, 'preemption_count': 0}), (7762, {'train/accuracy': 0.9889519214630127, 'train/loss': 0.0369044728577137, 'train/mean_average_precision': 0.30611968214716434, 'validation/accuracy': 0.9859174489974976, 'validation/loss': 0.047410693019628525, 'validation/mean_average_precision': 0.2350091741449841, 'validation/num_examples': 43793, 'test/accuracy': 0.9850820899009705, 'test/loss': 0.050203725695610046, 'test/mean_average_precision': 0.2307180195891641, 'test/num_examples': 43793, 'score': 1694.1986210346222, 'total_duration': 2469.2692592144012, 'accumulated_submission_time': 1694.1986210346222, 'accumulated_eval_time': 774.6604170799255, 'accumulated_logging_time': 0.19954681396484375, 'global_step': 7762, 'preemption_count': 0}), (8869, {'train/accuracy': 0.9895359873771667, 'train/loss': 0.03476869687438011, 'train/mean_average_precision': 0.3336378839560209, 'validation/accuracy': 0.9862109422683716, 'validation/loss': 0.04657946899533272, 'validation/mean_average_precision': 0.23641985568612103, 'validation/num_examples': 43793, 'test/accuracy': 0.985342800617218, 'test/loss': 0.049263693392276764, 'test/mean_average_precision': 0.23012571803707949, 'test/num_examples': 43793, 'score': 1934.3325681686401, 'total_duration': 2787.551236152649, 'accumulated_submission_time': 1934.3325681686401, 'accumulated_eval_time': 852.7588722705841, 'accumulated_logging_time': 0.21915745735168457, 'global_step': 8869, 'preemption_count': 0}), (9974, {'train/accuracy': 0.9900067448616028, 'train/loss': 0.033745765686035156, 'train/mean_average_precision': 0.35276618447730945, 'validation/accuracy': 0.9865276217460632, 'validation/loss': 0.04516411945223808, 'validation/mean_average_precision': 0.24660534268412235, 'validation/num_examples': 43793, 'test/accuracy': 0.9856818914413452, 'test/loss': 0.04798480123281479, 'test/mean_average_precision': 0.24649667254919383, 'test/num_examples': 43793, 'score': 2174.292731523514, 'total_duration': 3104.5298507213593, 'accumulated_submission_time': 2174.292731523514, 'accumulated_eval_time': 929.7279224395752, 'accumulated_logging_time': 0.23972439765930176, 'global_step': 9974, 'preemption_count': 0}), (11087, {'train/accuracy': 0.9906244277954102, 'train/loss': 0.03141099214553833, 'train/mean_average_precision': 0.39621151412709577, 'validation/accuracy': 0.9866433143615723, 'validation/loss': 0.04498683661222458, 'validation/mean_average_precision': 0.25042331057778094, 'validation/num_examples': 43793, 'test/accuracy': 0.985772430896759, 'test/loss': 0.047772228717803955, 'test/mean_average_precision': 0.24775094628990368, 'test/num_examples': 43793, 'score': 2414.2670624256134, 'total_duration': 3422.7681024074554, 'accumulated_submission_time': 2414.2670624256134, 'accumulated_eval_time': 1007.9429130554199, 'accumulated_logging_time': 0.26107168197631836, 'global_step': 11087, 'preemption_count': 0}), (12202, {'train/accuracy': 0.9905134439468384, 'train/loss': 0.031510163098573685, 'train/mean_average_precision': 0.4019224485779094, 'validation/accuracy': 0.9866319298744202, 'validation/loss': 0.04499562457203865, 'validation/mean_average_precision': 0.25606791041604826, 'validation/num_examples': 43793, 'test/accuracy': 0.985729455947876, 'test/loss': 0.047853659838438034, 'test/mean_average_precision': 0.25563159401159624, 'test/num_examples': 43793, 'score': 2654.2603480815887, 'total_duration': 3742.5755701065063, 'accumulated_submission_time': 2654.2603480815887, 'accumulated_eval_time': 1087.7058262825012, 'accumulated_logging_time': 0.28218579292297363, 'global_step': 12202, 'preemption_count': 0}), (13311, {'train/accuracy': 0.9907165169715881, 'train/loss': 0.030867191031575203, 'train/mean_average_precision': 0.3966914693686757, 'validation/accuracy': 0.9867135286331177, 'validation/loss': 0.04499348998069763, 'validation/mean_average_precision': 0.2562457729471811, 'validation/num_examples': 43793, 'test/accuracy': 0.9858511686325073, 'test/loss': 0.047732334583997726, 'test/mean_average_precision': 0.25553321923247074, 'test/num_examples': 43793, 'score': 2894.267228126526, 'total_duration': 4062.66877245903, 'accumulated_submission_time': 2894.267228126526, 'accumulated_eval_time': 1167.741821527481, 'accumulated_logging_time': 0.30239343643188477, 'global_step': 13311, 'preemption_count': 0}), (14423, {'train/accuracy': 0.9913085103034973, 'train/loss': 0.02904149517416954, 'train/mean_average_precision': 0.4597103301805182, 'validation/accuracy': 0.986660361289978, 'validation/loss': 0.044762022793293, 'validation/mean_average_precision': 0.2594895337665095, 'validation/num_examples': 43793, 'test/accuracy': 0.9857934713363647, 'test/loss': 0.04752175882458687, 'test/mean_average_precision': 0.2489293268632724, 'test/num_examples': 43793, 'score': 3134.3860597610474, 'total_duration': 4382.11479473114, 'accumulated_submission_time': 3134.3860597610474, 'accumulated_eval_time': 1247.0172290802002, 'accumulated_logging_time': 0.3221871852874756, 'global_step': 14423, 'preemption_count': 0}), (15526, {'train/accuracy': 0.99116450548172, 'train/loss': 0.02930135279893875, 'train/mean_average_precision': 0.4596503570921001, 'validation/accuracy': 0.9867017269134521, 'validation/loss': 0.04537179321050644, 'validation/mean_average_precision': 0.2587348912957092, 'validation/num_examples': 43793, 'test/accuracy': 0.9857943058013916, 'test/loss': 0.04824480041861534, 'test/mean_average_precision': 0.2553885637590089, 'test/num_examples': 43793, 'score': 3374.4397547245026, 'total_duration': 4699.156151533127, 'accumulated_submission_time': 3374.4397547245026, 'accumulated_eval_time': 1323.9546794891357, 'accumulated_logging_time': 0.3423612117767334, 'global_step': 15526, 'preemption_count': 0}), (16639, {'train/accuracy': 0.9917422533035278, 'train/loss': 0.027018975466489792, 'train/mean_average_precision': 0.5054917883613397, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.04532735049724579, 'validation/mean_average_precision': 0.26429787677699873, 'validation/num_examples': 43793, 'test/accuracy': 0.9859703779220581, 'test/loss': 0.048137325793504715, 'test/mean_average_precision': 0.2572786200951081, 'test/num_examples': 43793, 'score': 3614.3927824497223, 'total_duration': 5019.579539299011, 'accumulated_submission_time': 3614.3927824497223, 'accumulated_eval_time': 1404.3750398159027, 'accumulated_logging_time': 0.3638455867767334, 'global_step': 16639, 'preemption_count': 0}), (17735, {'train/accuracy': 0.9912893772125244, 'train/loss': 0.027928806841373444, 'train/mean_average_precision': 0.49817851882015035, 'validation/accuracy': 0.9866729378700256, 'validation/loss': 0.04598692059516907, 'validation/mean_average_precision': 0.258333338531433, 'validation/num_examples': 43793, 'test/accuracy': 0.9858827590942383, 'test/loss': 0.04875091463327408, 'test/mean_average_precision': 0.25343527495760393, 'test/num_examples': 43793, 'score': 3854.378380537033, 'total_duration': 5340.612284898758, 'accumulated_submission_time': 3854.378380537033, 'accumulated_eval_time': 1485.371256828308, 'accumulated_logging_time': 0.3858203887939453, 'global_step': 17735, 'preemption_count': 0}), (18838, {'train/accuracy': 0.9920688271522522, 'train/loss': 0.026050804182887077, 'train/mean_average_precision': 0.5213555031762197, 'validation/accuracy': 0.9867825508117676, 'validation/loss': 0.045983366668224335, 'validation/mean_average_precision': 0.2612213705168048, 'validation/num_examples': 43793, 'test/accuracy': 0.98590087890625, 'test/loss': 0.04899316281080246, 'test/mean_average_precision': 0.2531019784688823, 'test/num_examples': 43793, 'score': 4094.4802825450897, 'total_duration': 5659.26993393898, 'accumulated_submission_time': 4094.4802825450897, 'accumulated_eval_time': 1563.8790833950043, 'accumulated_logging_time': 0.40639328956604004, 'global_step': 18838, 'preemption_count': 0}), (19952, {'train/accuracy': 0.9921428561210632, 'train/loss': 0.025559471920132637, 'train/mean_average_precision': 0.5327124947685367, 'validation/accuracy': 0.9868085384368896, 'validation/loss': 0.04626746475696564, 'validation/mean_average_precision': 0.258900247415377, 'validation/num_examples': 43793, 'test/accuracy': 0.9857665300369263, 'test/loss': 0.04968218132853508, 'test/mean_average_precision': 0.2493889945445301, 'test/num_examples': 43793, 'score': 4334.588676691055, 'total_duration': 5978.880343914032, 'accumulated_submission_time': 4334.588676691055, 'accumulated_eval_time': 1643.3314793109894, 'accumulated_logging_time': 0.4276409149169922, 'global_step': 19952, 'preemption_count': 0}), (21063, {'train/accuracy': 0.9922109246253967, 'train/loss': 0.025429103523492813, 'train/mean_average_precision': 0.5364263126685468, 'validation/accuracy': 0.9866505861282349, 'validation/loss': 0.04653804004192352, 'validation/mean_average_precision': 0.263310892088798, 'validation/num_examples': 43793, 'test/accuracy': 0.9858238101005554, 'test/loss': 0.04970753937959671, 'test/mean_average_precision': 0.25144894776296034, 'test/num_examples': 43793, 'score': 4574.5525159835815, 'total_duration': 6300.125139474869, 'accumulated_submission_time': 4574.5525159835815, 'accumulated_eval_time': 1724.562082529068, 'accumulated_logging_time': 0.4483628273010254, 'global_step': 21063, 'preemption_count': 0}), (22169, {'train/accuracy': 0.992899477481842, 'train/loss': 0.023096101358532906, 'train/mean_average_precision': 0.5920535574588823, 'validation/accuracy': 0.9866822361946106, 'validation/loss': 0.046731531620025635, 'validation/mean_average_precision': 0.26172669389736536, 'validation/num_examples': 43793, 'test/accuracy': 0.9858840703964233, 'test/loss': 0.04983590543270111, 'test/mean_average_precision': 0.251740907075593, 'test/num_examples': 43793, 'score': 4814.611516237259, 'total_duration': 6619.431487083435, 'accumulated_submission_time': 4814.611516237259, 'accumulated_eval_time': 1803.7595672607422, 'accumulated_logging_time': 0.46943092346191406, 'global_step': 22169, 'preemption_count': 0}), (23278, {'train/accuracy': 0.9925416111946106, 'train/loss': 0.024167874827980995, 'train/mean_average_precision': 0.5664100288876779, 'validation/accuracy': 0.9866615533828735, 'validation/loss': 0.04728492721915245, 'validation/mean_average_precision': 0.2533365939874336, 'validation/num_examples': 43793, 'test/accuracy': 0.9858158230781555, 'test/loss': 0.050241753458976746, 'test/mean_average_precision': 0.25312532452599984, 'test/num_examples': 43793, 'score': 5054.620993614197, 'total_duration': 6937.901864051819, 'accumulated_submission_time': 5054.620993614197, 'accumulated_eval_time': 1882.1703062057495, 'accumulated_logging_time': 0.49088525772094727, 'global_step': 23278, 'preemption_count': 0}), (24380, {'train/accuracy': 0.9934418201446533, 'train/loss': 0.021381603553891182, 'train/mean_average_precision': 0.6230545571224955, 'validation/accuracy': 0.9867167472839355, 'validation/loss': 0.04735250025987625, 'validation/mean_average_precision': 0.25955633650647425, 'validation/num_examples': 43793, 'test/accuracy': 0.9858099222183228, 'test/loss': 0.050629694014787674, 'test/mean_average_precision': 0.24780152889570692, 'test/num_examples': 43793, 'score': 5294.686759233475, 'total_duration': 7257.19761967659, 'accumulated_submission_time': 5294.686759233475, 'accumulated_eval_time': 1961.3474252223969, 'accumulated_logging_time': 0.5124292373657227, 'global_step': 24380, 'preemption_count': 0}), (25491, {'train/accuracy': 0.9931038022041321, 'train/loss': 0.022402552887797356, 'train/mean_average_precision': 0.6004998552857195, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.04819728434085846, 'validation/mean_average_precision': 0.2592291029051286, 'validation/num_examples': 43793, 'test/accuracy': 0.9857050180435181, 'test/loss': 0.05148175358772278, 'test/mean_average_precision': 0.2496276009985486, 'test/num_examples': 43793, 'score': 5534.713684558868, 'total_duration': 7577.365520000458, 'accumulated_submission_time': 5534.713684558868, 'accumulated_eval_time': 2041.4387638568878, 'accumulated_logging_time': 0.5348010063171387, 'global_step': 25491, 'preemption_count': 0}), (26595, {'train/accuracy': 0.9934980869293213, 'train/loss': 0.021209150552749634, 'train/mean_average_precision': 0.613647568288411, 'validation/accuracy': 0.9864910840988159, 'validation/loss': 0.04872254654765129, 'validation/mean_average_precision': 0.2555855673491779, 'validation/num_examples': 43793, 'test/accuracy': 0.9856199622154236, 'test/loss': 0.052097950130701065, 'test/mean_average_precision': 0.24300745410759514, 'test/num_examples': 43793, 'score': 5774.6672995090485, 'total_duration': 7899.163873434067, 'accumulated_submission_time': 5774.6672995090485, 'accumulated_eval_time': 2123.23375916481, 'accumulated_logging_time': 0.5574033260345459, 'global_step': 26595, 'preemption_count': 0}), (27691, {'train/accuracy': 0.9940281510353088, 'train/loss': 0.01947796903550625, 'train/mean_average_precision': 0.6682530387046626, 'validation/accuracy': 0.9865698218345642, 'validation/loss': 0.04949478060007095, 'validation/mean_average_precision': 0.2528564819855809, 'validation/num_examples': 43793, 'test/accuracy': 0.9856932163238525, 'test/loss': 0.052731096744537354, 'test/mean_average_precision': 0.24326898764642005, 'test/num_examples': 43793, 'score': 6014.823639392853, 'total_duration': 8220.381026268005, 'accumulated_submission_time': 6014.823639392853, 'accumulated_eval_time': 2204.2446784973145, 'accumulated_logging_time': 0.5796847343444824, 'global_step': 27691, 'preemption_count': 0}), (28787, {'train/accuracy': 0.993468701839447, 'train/loss': 0.020785164088010788, 'train/mean_average_precision': 0.6354571079446321, 'validation/accuracy': 0.986544668674469, 'validation/loss': 0.05049121379852295, 'validation/mean_average_precision': 0.24725478867541148, 'validation/num_examples': 43793, 'test/accuracy': 0.9856536388397217, 'test/loss': 0.05412588268518448, 'test/mean_average_precision': 0.23914086872056928, 'test/num_examples': 43793, 'score': 6254.85346198082, 'total_duration': 8540.547902822495, 'accumulated_submission_time': 6254.85346198082, 'accumulated_eval_time': 2284.330146074295, 'accumulated_logging_time': 0.602679967880249, 'global_step': 28787, 'preemption_count': 0}), (29890, {'train/accuracy': 0.9950157999992371, 'train/loss': 0.01692265272140503, 'train/mean_average_precision': 0.7184164979935426, 'validation/accuracy': 0.9864171743392944, 'validation/loss': 0.0504840724170208, 'validation/mean_average_precision': 0.24648669029315962, 'validation/num_examples': 43793, 'test/accuracy': 0.9855079054832458, 'test/loss': 0.05403723567724228, 'test/mean_average_precision': 0.23776723358315677, 'test/num_examples': 43793, 'score': 6494.924700021744, 'total_duration': 8858.77674627304, 'accumulated_submission_time': 6494.924700021744, 'accumulated_eval_time': 2362.4342861175537, 'accumulated_logging_time': 0.6283829212188721, 'global_step': 29890, 'preemption_count': 0}), (30986, {'train/accuracy': 0.9939271211624146, 'train/loss': 0.019201230257749557, 'train/mean_average_precision': 0.6621570320546166, 'validation/accuracy': 0.9864354729652405, 'validation/loss': 0.051746681332588196, 'validation/mean_average_precision': 0.2444029919437009, 'validation/num_examples': 43793, 'test/accuracy': 0.9855096340179443, 'test/loss': 0.05538325011730194, 'test/mean_average_precision': 0.23533579322350964, 'test/num_examples': 43793, 'score': 6734.922221422195, 'total_duration': 9178.412378787994, 'accumulated_submission_time': 6734.922221422195, 'accumulated_eval_time': 2442.021591901779, 'accumulated_logging_time': 0.6507487297058105, 'global_step': 30986, 'preemption_count': 0}), (32090, {'train/accuracy': 0.9947919249534607, 'train/loss': 0.017432861030101776, 'train/mean_average_precision': 0.6972565675726514, 'validation/accuracy': 0.9862750768661499, 'validation/loss': 0.05191296711564064, 'validation/mean_average_precision': 0.24740458532398987, 'validation/num_examples': 43793, 'test/accuracy': 0.9853023886680603, 'test/loss': 0.055560316890478134, 'test/mean_average_precision': 0.23129149749969194, 'test/num_examples': 43793, 'score': 6974.904255151749, 'total_duration': 9495.972897529602, 'accumulated_submission_time': 6974.904255151749, 'accumulated_eval_time': 2519.548498392105, 'accumulated_logging_time': 0.6738758087158203, 'global_step': 32090, 'preemption_count': 0}), (33181, {'train/accuracy': 0.9951968193054199, 'train/loss': 0.0158045906573534, 'train/mean_average_precision': 0.7449554460251785, 'validation/accuracy': 0.9863039255142212, 'validation/loss': 0.05318564176559448, 'validation/mean_average_precision': 0.24087646253364822, 'validation/num_examples': 43793, 'test/accuracy': 0.985432505607605, 'test/loss': 0.05694403126835823, 'test/mean_average_precision': 0.23087480566131038, 'test/num_examples': 43793, 'score': 7214.861590623856, 'total_duration': 9819.639766931534, 'accumulated_submission_time': 7214.861590623856, 'accumulated_eval_time': 2603.2063603401184, 'accumulated_logging_time': 0.6973221302032471, 'global_step': 33181, 'preemption_count': 0}), (34274, {'train/accuracy': 0.9943652153015137, 'train/loss': 0.017712391912937164, 'train/mean_average_precision': 0.7002198268441346, 'validation/accuracy': 0.9862844347953796, 'validation/loss': 0.05365937575697899, 'validation/mean_average_precision': 0.24209437638856285, 'validation/num_examples': 43793, 'test/accuracy': 0.9853491187095642, 'test/loss': 0.057420093566179276, 'test/mean_average_precision': 0.22993860785917022, 'test/num_examples': 43793, 'score': 7454.888871192932, 'total_duration': 10138.390893697739, 'accumulated_submission_time': 7454.888871192932, 'accumulated_eval_time': 2681.874681711197, 'accumulated_logging_time': 0.721172571182251, 'global_step': 34274, 'preemption_count': 0}), (35367, {'train/accuracy': 0.9962413311004639, 'train/loss': 0.013400845229625702, 'train/mean_average_precision': 0.7820493120596349, 'validation/accuracy': 0.9862003922462463, 'validation/loss': 0.05483304709196091, 'validation/mean_average_precision': 0.24495549713178533, 'validation/num_examples': 43793, 'test/accuracy': 0.9852880835533142, 'test/loss': 0.05877852067351341, 'test/mean_average_precision': 0.22590880596937812, 'test/num_examples': 43793, 'score': 7694.906746149063, 'total_duration': 10456.633985996246, 'accumulated_submission_time': 7694.906746149063, 'accumulated_eval_time': 2760.0465755462646, 'accumulated_logging_time': 0.7447490692138672, 'global_step': 35367, 'preemption_count': 0}), (36457, {'train/accuracy': 0.994905412197113, 'train/loss': 0.01600693352520466, 'train/mean_average_precision': 0.7346263165754756, 'validation/accuracy': 0.9861776828765869, 'validation/loss': 0.05546123906970024, 'validation/mean_average_precision': 0.2342912991630984, 'validation/num_examples': 43793, 'test/accuracy': 0.9852594137191772, 'test/loss': 0.05940711125731468, 'test/mean_average_precision': 0.22573166413569928, 'test/num_examples': 43793, 'score': 7934.964791297913, 'total_duration': 10776.23775434494, 'accumulated_submission_time': 7934.964791297913, 'accumulated_eval_time': 2839.5401697158813, 'accumulated_logging_time': 0.769134521484375, 'global_step': 36457, 'preemption_count': 0}), (37557, {'train/accuracy': 0.9956643581390381, 'train/loss': 0.014182097278535366, 'train/mean_average_precision': 0.7695155245881993, 'validation/accuracy': 0.986148476600647, 'validation/loss': 0.05685465782880783, 'validation/mean_average_precision': 0.2360836863060357, 'validation/num_examples': 43793, 'test/accuracy': 0.985171377658844, 'test/loss': 0.06084699183702469, 'test/mean_average_precision': 0.22139841830616547, 'test/num_examples': 43793, 'score': 8175.070830345154, 'total_duration': 11096.11225438118, 'accumulated_submission_time': 8175.070830345154, 'accumulated_eval_time': 2919.2569456100464, 'accumulated_logging_time': 0.7927367687225342, 'global_step': 37557, 'preemption_count': 0}), (38650, {'train/accuracy': 0.9966215491294861, 'train/loss': 0.01235384214669466, 'train/mean_average_precision': 0.8127454760132536, 'validation/accuracy': 0.9860749840736389, 'validation/loss': 0.057182252407073975, 'validation/mean_average_precision': 0.23225317108569346, 'validation/num_examples': 43793, 'test/accuracy': 0.9851073622703552, 'test/loss': 0.06137857958674431, 'test/mean_average_precision': 0.2184106255203696, 'test/num_examples': 43793, 'score': 8415.077866792679, 'total_duration': 11417.668135643005, 'accumulated_submission_time': 8415.077866792679, 'accumulated_eval_time': 3000.7529504299164, 'accumulated_logging_time': 0.8158724308013916, 'global_step': 38650, 'preemption_count': 0}), (39740, {'train/accuracy': 0.9950730800628662, 'train/loss': 0.01540119294077158, 'train/mean_average_precision': 0.7420471551511711, 'validation/accuracy': 0.9860648512840271, 'validation/loss': 0.05806157365441322, 'validation/mean_average_precision': 0.23402133550235252, 'validation/num_examples': 43793, 'test/accuracy': 0.9850812554359436, 'test/loss': 0.06228027120232582, 'test/mean_average_precision': 0.221019696144846, 'test/num_examples': 43793, 'score': 8655.207736492157, 'total_duration': 11734.24772143364, 'accumulated_submission_time': 8655.207736492157, 'accumulated_eval_time': 3077.14851975441, 'accumulated_logging_time': 0.8406710624694824, 'global_step': 39740, 'preemption_count': 0}), (40791, {'train/accuracy': 0.9962680339813232, 'train/loss': 0.012959683313965797, 'train/mean_average_precision': 0.8086942489551621, 'validation/accuracy': 0.9860067963600159, 'validation/loss': 0.05914413556456566, 'validation/mean_average_precision': 0.22812119340760142, 'validation/num_examples': 43793, 'test/accuracy': 0.985074520111084, 'test/loss': 0.0632818192243576, 'test/mean_average_precision': 0.21857273568098864, 'test/num_examples': 43793, 'score': 8895.314485311508, 'total_duration': 12059.117860078812, 'accumulated_submission_time': 8895.314485311508, 'accumulated_eval_time': 3161.8530378341675, 'accumulated_logging_time': 0.8682987689971924, 'global_step': 40791, 'preemption_count': 0}), (41850, {'train/accuracy': 0.9967944025993347, 'train/loss': 0.011492342688143253, 'train/mean_average_precision': 0.8188840064696002, 'validation/accuracy': 0.9860250353813171, 'validation/loss': 0.05982394888997078, 'validation/mean_average_precision': 0.22915862541401752, 'validation/num_examples': 43793, 'test/accuracy': 0.9851305484771729, 'test/loss': 0.06414381414651871, 'test/mean_average_precision': 0.21542256282912498, 'test/num_examples': 43793, 'score': 9135.268005132675, 'total_duration': 12383.531695842743, 'accumulated_submission_time': 9135.268005132675, 'accumulated_eval_time': 3246.259354829788, 'accumulated_logging_time': 0.8931386470794678, 'global_step': 41850, 'preemption_count': 0}), (42891, {'train/accuracy': 0.9952070116996765, 'train/loss': 0.014795399270951748, 'train/mean_average_precision': 0.7647963971867565, 'validation/accuracy': 0.9859657883644104, 'validation/loss': 0.060548730194568634, 'validation/mean_average_precision': 0.22370711043783192, 'validation/num_examples': 43793, 'test/accuracy': 0.9850673675537109, 'test/loss': 0.06475042551755905, 'test/mean_average_precision': 0.21328248732083535, 'test/num_examples': 43793, 'score': 9375.35702419281, 'total_duration': 12707.729056835175, 'accumulated_submission_time': 9375.35702419281, 'accumulated_eval_time': 3330.3126599788666, 'accumulated_logging_time': 0.920159101486206, 'global_step': 42891, 'preemption_count': 0}), (43933, {'train/accuracy': 0.9973065853118896, 'train/loss': 0.010488463565707207, 'train/mean_average_precision': 0.8563660520742207, 'validation/accuracy': 0.9860116243362427, 'validation/loss': 0.061013076454401016, 'validation/mean_average_precision': 0.22908297287198628, 'validation/num_examples': 43793, 'test/accuracy': 0.9850635528564453, 'test/loss': 0.06536463648080826, 'test/mean_average_precision': 0.21398566318342374, 'test/num_examples': 43793, 'score': 9615.314539194107, 'total_duration': 13028.726650714874, 'accumulated_submission_time': 9615.314539194107, 'accumulated_eval_time': 3411.298595905304, 'accumulated_logging_time': 0.9459872245788574, 'global_step': 43933, 'preemption_count': 0}), (44975, {'train/accuracy': 0.9977905750274658, 'train/loss': 0.009721540845930576, 'train/mean_average_precision': 0.8649640297539942, 'validation/accuracy': 0.9859353303909302, 'validation/loss': 0.06185228005051613, 'validation/mean_average_precision': 0.22119776795887428, 'validation/num_examples': 43793, 'test/accuracy': 0.9850159883499146, 'test/loss': 0.06618864834308624, 'test/mean_average_precision': 0.21143031014602137, 'test/num_examples': 43793, 'score': 9855.26269030571, 'total_duration': 13353.377353429794, 'accumulated_submission_time': 9855.26269030571, 'accumulated_eval_time': 3495.9469141960144, 'accumulated_logging_time': 0.9704456329345703, 'global_step': 44975, 'preemption_count': 0}), (46029, {'train/accuracy': 0.99576735496521, 'train/loss': 0.013160244561731815, 'train/mean_average_precision': 0.8003886349363453, 'validation/accuracy': 0.9859138131141663, 'validation/loss': 0.061825551092624664, 'validation/mean_average_precision': 0.22507051291977662, 'validation/num_examples': 43793, 'test/accuracy': 0.9849889874458313, 'test/loss': 0.0662682056427002, 'test/mean_average_precision': 0.21024573095020252, 'test/num_examples': 43793, 'score': 10095.315613031387, 'total_duration': 13674.875415086746, 'accumulated_submission_time': 10095.315613031387, 'accumulated_eval_time': 3577.3388850688934, 'accumulated_logging_time': 0.9947977066040039, 'global_step': 46029, 'preemption_count': 0}), (47092, {'train/accuracy': 0.9975412487983704, 'train/loss': 0.009990371763706207, 'train/mean_average_precision': 0.8561627072996958, 'validation/accuracy': 0.9859129786491394, 'validation/loss': 0.0621907077729702, 'validation/mean_average_precision': 0.2238166018853482, 'validation/num_examples': 43793, 'test/accuracy': 0.9849873185157776, 'test/loss': 0.06659504771232605, 'test/mean_average_precision': 0.21038231333144, 'test/num_examples': 43793, 'score': 10335.297533035278, 'total_duration': 13996.62484908104, 'accumulated_submission_time': 10335.297533035278, 'accumulated_eval_time': 3659.0523521900177, 'accumulated_logging_time': 1.0190553665161133, 'global_step': 47092, 'preemption_count': 0}), (48130, {'train/accuracy': 0.9976902008056641, 'train/loss': 0.009740324690937996, 'train/mean_average_precision': 0.8675106947623336, 'validation/accuracy': 0.9859369397163391, 'validation/loss': 0.062404219061136246, 'validation/mean_average_precision': 0.22509001635530793, 'validation/num_examples': 43793, 'test/accuracy': 0.9850117564201355, 'test/loss': 0.06687269359827042, 'test/mean_average_precision': 0.2089665159698329, 'test/num_examples': 43793, 'score': 10575.345266342163, 'total_duration': 14342.262172937393, 'accumulated_submission_time': 10575.345266342163, 'accumulated_eval_time': 3764.5881111621857, 'accumulated_logging_time': 1.0442917346954346, 'global_step': 48130, 'preemption_count': 0}), (49148, {'train/accuracy': 0.9969300627708435, 'train/loss': 0.011011132970452309, 'train/mean_average_precision': 0.841815208447237, 'validation/accuracy': 0.9858037829399109, 'validation/loss': 0.06214151903986931, 'validation/mean_average_precision': 0.2205536472826599, 'validation/num_examples': 43793, 'test/accuracy': 0.984874427318573, 'test/loss': 0.06647491455078125, 'test/mean_average_precision': 0.20727917184459968, 'test/num_examples': 43793, 'score': 10815.399083852768, 'total_duration': 14727.581475019455, 'accumulated_submission_time': 10815.399083852768, 'accumulated_eval_time': 3909.795681476593, 'accumulated_logging_time': 1.0719881057739258, 'global_step': 49148, 'preemption_count': 0}), (50165, {'train/accuracy': 0.996914267539978, 'train/loss': 0.010785517282783985, 'train/mean_average_precision': 0.8531941909615431, 'validation/accuracy': 0.9858890771865845, 'validation/loss': 0.06255883723497391, 'validation/mean_average_precision': 0.2242452881051335, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.06695663928985596, 'test/mean_average_precision': 0.2087962214743045, 'test/num_examples': 43793, 'score': 11055.229188919067, 'total_duration': 15101.595242023468, 'accumulated_submission_time': 11055.229188919067, 'accumulated_eval_time': 4043.806925058365, 'accumulated_logging_time': 1.214524269104004, 'global_step': 50165, 'preemption_count': 0}), (51155, {'train/accuracy': 0.9976332783699036, 'train/loss': 0.0098845474421978, 'train/mean_average_precision': 0.8672128549457234, 'validation/accuracy': 0.9858216643333435, 'validation/loss': 0.06224546581506729, 'validation/mean_average_precision': 0.22321332898618293, 'validation/num_examples': 43793, 'test/accuracy': 0.9849334359169006, 'test/loss': 0.06662486493587494, 'test/mean_average_precision': 0.2072129423416019, 'test/num_examples': 43793, 'score': 11295.203172922134, 'total_duration': 15487.575869321823, 'accumulated_submission_time': 11295.203172922134, 'accumulated_eval_time': 4189.750177621841, 'accumulated_logging_time': 1.2471799850463867, 'global_step': 51155, 'preemption_count': 0}), (52112, {'train/accuracy': 0.9975487589836121, 'train/loss': 0.009948467835783958, 'train/mean_average_precision': 0.8602134859313437, 'validation/accuracy': 0.985877275466919, 'validation/loss': 0.06221054866909981, 'validation/mean_average_precision': 0.22375578237176247, 'validation/num_examples': 43793, 'test/accuracy': 0.9849380254745483, 'test/loss': 0.06661153584718704, 'test/mean_average_precision': 0.2081028431010148, 'test/num_examples': 43793, 'score': 11535.160253286362, 'total_duration': 15870.319672107697, 'accumulated_submission_time': 11535.160253286362, 'accumulated_eval_time': 4332.479960203171, 'accumulated_logging_time': 1.274245262145996, 'global_step': 52112, 'preemption_count': 0}), (53085, {'train/accuracy': 0.997531533241272, 'train/loss': 0.00994553230702877, 'train/mean_average_precision': 0.8513676537016667, 'validation/accuracy': 0.985877275466919, 'validation/loss': 0.06221054866909981, 'validation/mean_average_precision': 0.22376590147394174, 'validation/num_examples': 43793, 'test/accuracy': 0.9849380254745483, 'test/loss': 0.06661153584718704, 'test/mean_average_precision': 0.20806403867485468, 'test/num_examples': 43793, 'score': 11775.186443328857, 'total_duration': 16244.147909641266, 'accumulated_submission_time': 11775.186443328857, 'accumulated_eval_time': 4466.225948810577, 'accumulated_logging_time': 1.300379991531372, 'global_step': 53085, 'preemption_count': 0})], 'global_step': 54074}
I0307 21:13:53.305994 140681127527616 submission_runner.py:649] Timing: 12015.253298282623
I0307 21:13:53.306039 140681127527616 submission_runner.py:651] Total number of evals: 50
I0307 21:13:53.306073 140681127527616 submission_runner.py:652] ====================
I0307 21:13:53.306535 140681127527616 submission_runner.py:750] Final ogbg score: 1
