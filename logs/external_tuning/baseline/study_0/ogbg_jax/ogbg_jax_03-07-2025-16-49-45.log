python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=801834307 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/ogbg_jax_03-07-2025-16-49-45.log
2025-03-07 16:50:00.502666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741366200.548090       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741366200.560824       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:50:49.831985 140599079560384 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax.
I0307 16:50:51.680251 140599079560384 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:50:51.683481 140599079560384 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:50:51.698385 140599079560384 submission_runner.py:606] Using RNG seed 801834307
I0307 16:50:52.775883 140599079560384 submission_runner.py:615] --- Tuning run 5/5 ---
I0307 16:50:52.776091 140599079560384 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_5.
I0307 16:50:52.776301 140599079560384 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_5/hparams.json.
I0307 16:50:53.021916 140599079560384 submission_runner.py:218] Initializing dataset.
I0307 16:50:54.866690 140599079560384 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:50:54.963373 140599079560384 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0307 16:50:55.356817 140599079560384 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0307 16:50:55.414085 140599079560384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:50:55.508339 140599079560384 submission_runner.py:229] Initializing model.
I0307 16:51:04.172638 140599079560384 submission_runner.py:272] Initializing optimizer.
I0307 16:51:04.612359 140599079560384 submission_runner.py:279] Initializing metrics bundle.
I0307 16:51:04.612617 140599079560384 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:51:04.613483 140599079560384 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_5 with prefix checkpoint_
I0307 16:51:04.613606 140599079560384 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_5/meta_data_0.json.
I0307 16:51:04.613785 140599079560384 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:51:04.613838 140599079560384 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:51:05.066693 140599079560384 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_5/flags_0.json.
I0307 16:51:05.298779 140599079560384 submission_runner.py:337] Starting training loop.
I0307 16:51:23.733808 140462903961344 logging_writer.py:48] [0] global_step=0, grad_norm=2.3304905891418457, loss=0.7415458559989929
I0307 16:51:24.063740 140599079560384 spec.py:321] Evaluating on the training split.
I0307 16:51:24.068176 140599079560384 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:51:24.072381 140599079560384 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:51:24.140164 140599079560384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:52:45.797410 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 16:52:45.800170 140599079560384 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:52:45.804198 140599079560384 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:52:45.870316 140599079560384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:53:52.108869 140599079560384 spec.py:349] Evaluating on the test split.
I0307 16:53:52.111382 140599079560384 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:53:52.115122 140599079560384 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:53:52.179094 140599079560384 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:54:59.229835 140599079560384 submission_runner.py:469] Time since start: 233.93s, 	Step: 1, 	{'train/accuracy': 0.48007529973983765, 'train/loss': 0.7292938232421875, 'train/mean_average_precision': 0.02283410630454976, 'validation/accuracy': 0.4905015826225281, 'validation/loss': 0.7234783172607422, 'validation/mean_average_precision': 0.02555354870908746, 'validation/num_examples': 43793, 'test/accuracy': 0.4925242066383362, 'test/loss': 0.7222917079925537, 'test/mean_average_precision': 0.026829375439128535, 'test/num_examples': 43793, 'score': 18.764833211898804, 'total_duration': 233.93102025985718, 'accumulated_submission_time': 18.764833211898804, 'accumulated_eval_time': 215.16605591773987, 'accumulated_logging_time': 0}
I0307 16:54:59.236723 140457135232768 logging_writer.py:48] [1] accumulated_eval_time=215.166, accumulated_logging_time=0, accumulated_submission_time=18.7648, global_step=1, preemption_count=0, score=18.7648, test/accuracy=0.492524, test/loss=0.722292, test/mean_average_precision=0.0268294, test/num_examples=43793, total_duration=233.931, train/accuracy=0.480075, train/loss=0.729294, train/mean_average_precision=0.0228341, validation/accuracy=0.490502, validation/loss=0.723478, validation/mean_average_precision=0.0255535, validation/num_examples=43793
I0307 16:55:21.213050 140457143625472 logging_writer.py:48] [100] global_step=100, grad_norm=0.227621391415596, loss=0.208104208111763
I0307 16:55:43.590034 140457135232768 logging_writer.py:48] [200] global_step=200, grad_norm=0.04858352616429329, loss=0.08449473232030869
I0307 16:56:05.619142 140457143625472 logging_writer.py:48] [300] global_step=300, grad_norm=0.017965281382203102, loss=0.057029008865356445
I0307 16:56:28.038529 140457135232768 logging_writer.py:48] [400] global_step=400, grad_norm=0.014917089603841305, loss=0.058092355728149414
I0307 16:56:50.046314 140457143625472 logging_writer.py:48] [500] global_step=500, grad_norm=0.0236698966473341, loss=0.053568460047245026
I0307 16:57:12.139911 140457135232768 logging_writer.py:48] [600] global_step=600, grad_norm=0.028787316754460335, loss=0.05385703593492508
I0307 16:57:34.465233 140458930353920 logging_writer.py:48] [700] global_step=700, grad_norm=0.02178095653653145, loss=0.05576879531145096
I0307 16:57:56.272827 140457845716736 logging_writer.py:48] [800] global_step=800, grad_norm=0.017080022022128105, loss=0.05117148905992508
I0307 16:58:18.355749 140458930353920 logging_writer.py:48] [900] global_step=900, grad_norm=0.015531959012150764, loss=0.04987175390124321
I0307 16:58:40.567004 140457845716736 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.01903644949197769, loss=0.05455807223916054
I0307 16:58:59.277103 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:00:15.083186 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:00:17.078833 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:00:19.070895 140599079560384 submission_runner.py:469] Time since start: 553.77s, 	Step: 1088, 	{'train/accuracy': 0.9870059490203857, 'train/loss': 0.048422180116176605, 'train/mean_average_precision': 0.08479339055811896, 'validation/accuracy': 0.9843679666519165, 'validation/loss': 0.05816641449928284, 'validation/mean_average_precision': 0.0849512830954789, 'validation/num_examples': 43793, 'test/accuracy': 0.9833509922027588, 'test/loss': 0.061555877327919006, 'test/mean_average_precision': 0.07919301991733778, 'test/num_examples': 43793, 'score': 258.76359963417053, 'total_duration': 553.7720704078674, 'accumulated_submission_time': 258.76359963417053, 'accumulated_eval_time': 294.9598033428192, 'accumulated_logging_time': 0.016712427139282227}
I0307 17:00:19.079664 140458930353920 logging_writer.py:48] [1088] accumulated_eval_time=294.96, accumulated_logging_time=0.0167124, accumulated_submission_time=258.764, global_step=1088, preemption_count=0, score=258.764, test/accuracy=0.983351, test/loss=0.0615559, test/mean_average_precision=0.079193, test/num_examples=43793, total_duration=553.772, train/accuracy=0.987006, train/loss=0.0484222, train/mean_average_precision=0.0847934, validation/accuracy=0.984368, validation/loss=0.0581664, validation/mean_average_precision=0.0849513, validation/num_examples=43793
I0307 17:00:21.882024 140457845716736 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.010722249746322632, loss=0.04797288030385971
I0307 17:00:43.728852 140458930353920 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.016323966905474663, loss=0.04807857796549797
I0307 17:01:05.989391 140457845716736 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.015641773119568825, loss=0.04920952022075653
I0307 17:01:27.959989 140458930353920 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.016492370516061783, loss=0.055911801755428314
I0307 17:01:50.050030 140457845716736 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.017771480605006218, loss=0.045684508979320526
I0307 17:02:11.795700 140458930353920 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.01780296489596367, loss=0.05211349576711655
I0307 17:02:34.033459 140457845716736 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.014431728050112724, loss=0.047497935593128204
I0307 17:02:56.306351 140458930353920 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.010949920862913132, loss=0.04684266820549965
I0307 17:03:18.509123 140457845716736 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.019880739971995354, loss=0.04381389170885086
I0307 17:03:40.498770 140458930353920 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.014295147731900215, loss=0.04650949686765671
I0307 17:04:02.418030 140457845716736 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.014090688899159431, loss=0.04383528232574463
I0307 17:04:19.253281 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:05:34.650042 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:05:36.672282 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:05:38.644316 140599079560384 submission_runner.py:469] Time since start: 873.35s, 	Step: 2176, 	{'train/accuracy': 0.987819254398346, 'train/loss': 0.043525632470846176, 'train/mean_average_precision': 0.14257660419287255, 'validation/accuracy': 0.9850130081176758, 'validation/loss': 0.052820298820734024, 'validation/mean_average_precision': 0.13431356962689756, 'validation/num_examples': 43793, 'test/accuracy': 0.984071671962738, 'test/loss': 0.05582745000720024, 'test/mean_average_precision': 0.1318445734594856, 'test/num_examples': 43793, 'score': 498.8953061103821, 'total_duration': 873.3454933166504, 'accumulated_submission_time': 498.8953061103821, 'accumulated_eval_time': 374.35080075263977, 'accumulated_logging_time': 0.03506875038146973}
I0307 17:05:38.652874 140458930353920 logging_writer.py:48] [2176] accumulated_eval_time=374.351, accumulated_logging_time=0.0350688, accumulated_submission_time=498.895, global_step=2176, preemption_count=0, score=498.895, test/accuracy=0.984072, test/loss=0.0558275, test/mean_average_precision=0.131845, test/num_examples=43793, total_duration=873.345, train/accuracy=0.987819, train/loss=0.0435256, train/mean_average_precision=0.142577, validation/accuracy=0.985013, validation/loss=0.0528203, validation/mean_average_precision=0.134314, validation/num_examples=43793
I0307 17:05:44.262091 140457845716736 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.012026562355458736, loss=0.046146210283041
I0307 17:06:06.065289 140458930353920 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.010092755779623985, loss=0.041457030922174454
I0307 17:06:28.163316 140457845716736 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0145877068862319, loss=0.042938232421875
I0307 17:06:49.696014 140458930353920 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.011081977747380733, loss=0.04738318547606468
I0307 17:07:11.943916 140457845716736 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.02313452959060669, loss=0.04052736237645149
I0307 17:07:34.000662 140458930353920 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.017128119245171547, loss=0.03913716599345207
I0307 17:07:56.081152 140457845716736 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.01809421367943287, loss=0.045386653393507004
I0307 17:08:17.916874 140458930353920 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0115688880905509, loss=0.04167411848902702
I0307 17:08:39.895123 140457845716736 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.010502444580197334, loss=0.04346418008208275
I0307 17:09:01.879018 140458930353920 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.010251255705952644, loss=0.04118688032031059
I0307 17:09:23.864654 140457845716736 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.01084227953106165, loss=0.04233410954475403
I0307 17:09:38.760076 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:10:54.473378 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:10:56.512812 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:10:58.469164 140599079560384 submission_runner.py:469] Time since start: 1193.17s, 	Step: 3268, 	{'train/accuracy': 0.9882214069366455, 'train/loss': 0.04083537310361862, 'train/mean_average_precision': 0.18236941732909004, 'validation/accuracy': 0.9853994846343994, 'validation/loss': 0.05042979121208191, 'validation/mean_average_precision': 0.1553376895724147, 'validation/num_examples': 43793, 'test/accuracy': 0.9844818711280823, 'test/loss': 0.05319877341389656, 'test/mean_average_precision': 0.15897204050035846, 'test/num_examples': 43793, 'score': 738.9628694057465, 'total_duration': 1193.1703388690948, 'accumulated_submission_time': 738.9628694057465, 'accumulated_eval_time': 454.05983114242554, 'accumulated_logging_time': 0.05324888229370117}
I0307 17:10:58.478493 140458930353920 logging_writer.py:48] [3268] accumulated_eval_time=454.06, accumulated_logging_time=0.0532489, accumulated_submission_time=738.963, global_step=3268, preemption_count=0, score=738.963, test/accuracy=0.984482, test/loss=0.0531988, test/mean_average_precision=0.158972, test/num_examples=43793, total_duration=1193.17, train/accuracy=0.988221, train/loss=0.0408354, train/mean_average_precision=0.182369, validation/accuracy=0.985399, validation/loss=0.0504298, validation/mean_average_precision=0.155338, validation/num_examples=43793
I0307 17:11:05.876317 140457845716736 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.014605269767343998, loss=0.04142124950885773
I0307 17:11:28.040458 140458930353920 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.009826288558542728, loss=0.042547110468149185
I0307 17:11:49.990714 140457845716736 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0192519910633564, loss=0.037471309304237366
I0307 17:12:12.022939 140458930353920 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.018207695335149765, loss=0.04310142621397972
I0307 17:12:34.555336 140457845716736 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.013978243805468082, loss=0.04057501256465912
I0307 17:12:56.323152 140458930353920 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.009274902753531933, loss=0.04056190326809883
I0307 17:13:17.933110 140457845716736 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.012867490760982037, loss=0.04242913797497749
I0307 17:13:39.680227 140458930353920 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.011589707806706429, loss=0.04112377390265465
I0307 17:14:01.252066 140457845716736 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.012728456407785416, loss=0.03635448217391968
I0307 17:14:23.389447 140458930353920 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.01335511077195406, loss=0.04108147323131561
I0307 17:14:45.156054 140457845716736 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.011915276758372784, loss=0.04104878008365631
I0307 17:14:58.666246 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:16:15.658313 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:16:17.608889 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:16:19.513766 140599079560384 submission_runner.py:469] Time since start: 1514.21s, 	Step: 4363, 	{'train/accuracy': 0.9887198805809021, 'train/loss': 0.03877720236778259, 'train/mean_average_precision': 0.20975646864817155, 'validation/accuracy': 0.9857136607170105, 'validation/loss': 0.048355501145124435, 'validation/mean_average_precision': 0.18880914599759863, 'validation/num_examples': 43793, 'test/accuracy': 0.9848297834396362, 'test/loss': 0.05119384080171585, 'test/mean_average_precision': 0.19210166610070203, 'test/num_examples': 43793, 'score': 979.1117026805878, 'total_duration': 1514.2149381637573, 'accumulated_submission_time': 979.1117026805878, 'accumulated_eval_time': 534.9072921276093, 'accumulated_logging_time': 0.07212185859680176}
I0307 17:16:19.522557 140457571948288 logging_writer.py:48] [4363] accumulated_eval_time=534.907, accumulated_logging_time=0.0721219, accumulated_submission_time=979.112, global_step=4363, preemption_count=0, score=979.112, test/accuracy=0.98483, test/loss=0.0511938, test/mean_average_precision=0.192102, test/num_examples=43793, total_duration=1514.21, train/accuracy=0.98872, train/loss=0.0387772, train/mean_average_precision=0.209756, validation/accuracy=0.985714, validation/loss=0.0483555, validation/mean_average_precision=0.188809, validation/num_examples=43793
I0307 17:16:28.022258 140457563555584 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.010446161031723022, loss=0.03707500174641609
I0307 17:16:50.018055 140457571948288 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013931449502706528, loss=0.03839359059929848
I0307 17:17:11.799461 140457563555584 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.015386003069579601, loss=0.04082014784216881
I0307 17:17:33.581275 140457571948288 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.015710171312093735, loss=0.039924152195453644
I0307 17:17:55.714933 140457563555584 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.014708965085446835, loss=0.040504343807697296
I0307 17:18:17.537850 140457571948288 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.012307090684771538, loss=0.0395551435649395
I0307 17:18:39.386610 140457563555584 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.013216780498623848, loss=0.04015061631798744
I0307 17:19:00.996463 140457571948288 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012021788395941257, loss=0.03783135488629341
I0307 17:19:23.069700 140457563555584 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.012821074575185776, loss=0.034985583275556564
I0307 17:19:45.186906 140457571948288 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.01194238942116499, loss=0.04285707324743271
I0307 17:20:07.304607 140457563555584 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.013134758919477463, loss=0.03907396271824837
I0307 17:20:19.547876 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:21:35.994218 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:21:37.959588 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:21:39.857828 140599079560384 submission_runner.py:469] Time since start: 1834.56s, 	Step: 5457, 	{'train/accuracy': 0.9890140891075134, 'train/loss': 0.03742047771811485, 'train/mean_average_precision': 0.24820763483645697, 'validation/accuracy': 0.9860498309135437, 'validation/loss': 0.047213517129421234, 'validation/mean_average_precision': 0.2152637398390373, 'validation/num_examples': 43793, 'test/accuracy': 0.9851452708244324, 'test/loss': 0.04988021403551102, 'test/mean_average_precision': 0.21482940856492982, 'test/num_examples': 43793, 'score': 1219.0983471870422, 'total_duration': 1834.559006690979, 'accumulated_submission_time': 1219.0983471870422, 'accumulated_eval_time': 615.2171878814697, 'accumulated_logging_time': 0.09035277366638184}
I0307 17:21:39.866871 140457571948288 logging_writer.py:48] [5457] accumulated_eval_time=615.217, accumulated_logging_time=0.0903528, accumulated_submission_time=1219.1, global_step=5457, preemption_count=0, score=1219.1, test/accuracy=0.985145, test/loss=0.0498802, test/mean_average_precision=0.214829, test/num_examples=43793, total_duration=1834.56, train/accuracy=0.989014, train/loss=0.0374205, train/mean_average_precision=0.248208, validation/accuracy=0.98605, validation/loss=0.0472135, validation/mean_average_precision=0.215264, validation/num_examples=43793
I0307 17:21:49.242982 140457563555584 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.013442917726933956, loss=0.03743099421262741
I0307 17:22:10.679300 140457571948288 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.02171468734741211, loss=0.042394377291202545
I0307 17:22:32.478933 140457563555584 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.013393801636993885, loss=0.04024305194616318
I0307 17:22:54.094455 140457571948288 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.013283525593578815, loss=0.03980148211121559
I0307 17:23:15.500966 140457563555584 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.0133505305275321, loss=0.038088589906692505
I0307 17:23:37.488710 140457571948288 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.014727575704455376, loss=0.03741768002510071
I0307 17:23:59.327248 140457563555584 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.012472727335989475, loss=0.038175005465745926
I0307 17:24:21.593061 140457571948288 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.018120642751455307, loss=0.0359068363904953
I0307 17:24:44.242448 140457563555584 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.011641389690339565, loss=0.03968753293156624
I0307 17:25:06.334045 140457571948288 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.01793583482503891, loss=0.03532881662249565
I0307 17:25:28.414251 140457563555584 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.02104843780398369, loss=0.04006035253405571
I0307 17:25:39.890917 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:27:05.017538 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:27:06.978943 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:27:08.910638 140599079560384 submission_runner.py:469] Time since start: 2163.61s, 	Step: 6553, 	{'train/accuracy': 0.989457368850708, 'train/loss': 0.03533061221241951, 'train/mean_average_precision': 0.2873491300239006, 'validation/accuracy': 0.9862543940544128, 'validation/loss': 0.04603609815239906, 'validation/mean_average_precision': 0.22877868137675786, 'validation/num_examples': 43793, 'test/accuracy': 0.9854017496109009, 'test/loss': 0.04862169921398163, 'test/mean_average_precision': 0.22937052014241854, 'test/num_examples': 43793, 'score': 1459.0844507217407, 'total_duration': 2163.6116647720337, 'accumulated_submission_time': 1459.0844507217407, 'accumulated_eval_time': 704.2366998195648, 'accumulated_logging_time': 0.1095733642578125}
I0307 17:27:08.919926 140457571948288 logging_writer.py:48] [6553] accumulated_eval_time=704.237, accumulated_logging_time=0.109573, accumulated_submission_time=1459.08, global_step=6553, preemption_count=0, score=1459.08, test/accuracy=0.985402, test/loss=0.0486217, test/mean_average_precision=0.229371, test/num_examples=43793, total_duration=2163.61, train/accuracy=0.989457, train/loss=0.0353306, train/mean_average_precision=0.287349, validation/accuracy=0.986254, validation/loss=0.0460361, validation/mean_average_precision=0.228779, validation/num_examples=43793
I0307 17:27:19.463047 140457563555584 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.011568336747586727, loss=0.03350342810153961
I0307 17:27:41.418234 140457571948288 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.013050763867795467, loss=0.03620067238807678
I0307 17:28:03.060471 140457563555584 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.016120923683047295, loss=0.034539736807346344
I0307 17:28:24.776749 140457571948288 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.021954961121082306, loss=0.03571197763085365
I0307 17:28:46.538275 140457563555584 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.017890043556690216, loss=0.03966275230050087
I0307 17:29:08.241339 140457571948288 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.015390467830002308, loss=0.03594886139035225
I0307 17:29:29.810325 140457563555584 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.014920602552592754, loss=0.03703664615750313
I0307 17:29:51.753069 140457571948288 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.01866433210670948, loss=0.03432727977633476
I0307 17:30:13.959850 140457563555584 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.012740927748382092, loss=0.035533931106328964
I0307 17:30:35.731044 140457571948288 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.01565513014793396, loss=0.037843529134988785
I0307 17:30:57.718704 140457563555584 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.023135961964726448, loss=0.03530370071530342
I0307 17:31:08.972281 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:32:26.330945 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:32:28.278094 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:32:30.163400 140599079560384 submission_runner.py:469] Time since start: 2484.86s, 	Step: 7652, 	{'train/accuracy': 0.9895524978637695, 'train/loss': 0.03532123938202858, 'train/mean_average_precision': 0.2968694208852885, 'validation/accuracy': 0.9862657785415649, 'validation/loss': 0.04557625576853752, 'validation/mean_average_precision': 0.23296524540126679, 'validation/num_examples': 43793, 'test/accuracy': 0.9854401350021362, 'test/loss': 0.04819291830062866, 'test/mean_average_precision': 0.23364066459334326, 'test/num_examples': 43793, 'score': 1699.0924091339111, 'total_duration': 2484.86448431015, 'accumulated_submission_time': 1699.0924091339111, 'accumulated_eval_time': 785.4276740550995, 'accumulated_logging_time': 0.13152289390563965}
I0307 17:32:30.172923 140457571948288 logging_writer.py:48] [7652] accumulated_eval_time=785.428, accumulated_logging_time=0.131523, accumulated_submission_time=1699.09, global_step=7652, preemption_count=0, score=1699.09, test/accuracy=0.98544, test/loss=0.0481929, test/mean_average_precision=0.233641, test/num_examples=43793, total_duration=2484.86, train/accuracy=0.989552, train/loss=0.0353212, train/mean_average_precision=0.296869, validation/accuracy=0.986266, validation/loss=0.0455763, validation/mean_average_precision=0.232965, validation/num_examples=43793
I0307 17:32:40.895092 140457563555584 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.02386372722685337, loss=0.035504017025232315
I0307 17:33:02.561047 140457571948288 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.02389962412416935, loss=0.03691380098462105
I0307 17:33:23.897694 140457563555584 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.01681913621723652, loss=0.03488556295633316
I0307 17:33:45.712752 140457571948288 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.017174748703837395, loss=0.0338301807641983
I0307 17:34:08.071538 140457563555584 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.015542835928499699, loss=0.0376412533223629
I0307 17:34:30.384692 140457571948288 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.01834259182214737, loss=0.04053269699215889
I0307 17:34:52.371434 140457563555584 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.019887082278728485, loss=0.03732070326805115
I0307 17:35:14.296077 140457571948288 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.01538445707410574, loss=0.03549639508128166
I0307 17:35:36.332912 140457563555584 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.014631373807787895, loss=0.03819294273853302
I0307 17:35:58.044301 140457571948288 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03937065601348877, loss=0.03397335857152939
I0307 17:36:19.315409 140457563555584 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.017959000542759895, loss=0.034669000655412674
I0307 17:36:30.203280 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:37:47.421195 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:37:49.409397 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:37:51.347981 140599079560384 submission_runner.py:469] Time since start: 2806.05s, 	Step: 8751, 	{'train/accuracy': 0.9899919629096985, 'train/loss': 0.0334242582321167, 'train/mean_average_precision': 0.32886826535202107, 'validation/accuracy': 0.9862824082374573, 'validation/loss': 0.04539112746715546, 'validation/mean_average_precision': 0.24188441221917778, 'validation/num_examples': 43793, 'test/accuracy': 0.9854556918144226, 'test/loss': 0.04789965599775314, 'test/mean_average_precision': 0.23819103737125583, 'test/num_examples': 43793, 'score': 1939.0843923091888, 'total_duration': 2806.049070596695, 'accumulated_submission_time': 1939.0843923091888, 'accumulated_eval_time': 866.5722324848175, 'accumulated_logging_time': 0.15059161186218262}
I0307 17:37:51.358152 140457571948288 logging_writer.py:48] [8751] accumulated_eval_time=866.572, accumulated_logging_time=0.150592, accumulated_submission_time=1939.08, global_step=8751, preemption_count=0, score=1939.08, test/accuracy=0.985456, test/loss=0.0478997, test/mean_average_precision=0.238191, test/num_examples=43793, total_duration=2806.05, train/accuracy=0.989992, train/loss=0.0334243, train/mean_average_precision=0.328868, validation/accuracy=0.986282, validation/loss=0.0453911, validation/mean_average_precision=0.241884, validation/num_examples=43793
I0307 17:38:02.336553 140457563555584 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.019825894385576248, loss=0.034441087394952774
I0307 17:38:23.995289 140457571948288 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.01945934072136879, loss=0.040613532066345215
I0307 17:38:45.187494 140457563555584 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0270463265478611, loss=0.03387514129281044
I0307 17:39:06.513696 140457571948288 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.029855962842702866, loss=0.0310332253575325
I0307 17:39:28.090335 140457563555584 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.016143355518579483, loss=0.03803068771958351
I0307 17:39:49.391620 140457571948288 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.022451553493738174, loss=0.0356484092772007
I0307 17:40:10.999390 140457563555584 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.01652776449918747, loss=0.031346552073955536
I0307 17:40:32.622688 140457571948288 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.015825608745217323, loss=0.033545274287462234
I0307 17:40:54.262998 140457563555584 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01864919811487198, loss=0.02949102409183979
I0307 17:41:15.935337 140457571948288 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.02597295120358467, loss=0.033911630511283875
I0307 17:41:37.316524 140457563555584 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.018130987882614136, loss=0.030539242550730705
I0307 17:41:51.479496 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:43:05.568515 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:43:08.823580 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:43:10.834121 140599079560384 submission_runner.py:469] Time since start: 3125.54s, 	Step: 9867, 	{'train/accuracy': 0.9899487495422363, 'train/loss': 0.033588752150535583, 'train/mean_average_precision': 0.34148947858789175, 'validation/accuracy': 0.9862747192382812, 'validation/loss': 0.04532691836357117, 'validation/mean_average_precision': 0.24617711319956007, 'validation/num_examples': 43793, 'test/accuracy': 0.9854148626327515, 'test/loss': 0.048002731055021286, 'test/mean_average_precision': 0.23690137486416762, 'test/num_examples': 43793, 'score': 2179.166847705841, 'total_duration': 3125.5352046489716, 'accumulated_submission_time': 2179.166847705841, 'accumulated_eval_time': 945.9267082214355, 'accumulated_logging_time': 0.17134642601013184}
I0307 17:43:10.844893 140457571948288 logging_writer.py:48] [9867] accumulated_eval_time=945.927, accumulated_logging_time=0.171346, accumulated_submission_time=2179.17, global_step=9867, preemption_count=0, score=2179.17, test/accuracy=0.985415, test/loss=0.0480027, test/mean_average_precision=0.236901, test/num_examples=43793, total_duration=3125.54, train/accuracy=0.989949, train/loss=0.0335888, train/mean_average_precision=0.341489, validation/accuracy=0.986275, validation/loss=0.0453269, validation/mean_average_precision=0.246177, validation/num_examples=43793
I0307 17:43:18.333318 140457563555584 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.037125445902347565, loss=0.036204058676958084
I0307 17:43:40.276984 140457571948288 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.02494066394865513, loss=0.03852741792798042
I0307 17:44:01.206885 140457563555584 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.020376505330204964, loss=0.029970023781061172
I0307 17:44:22.841972 140457571948288 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.022937944158911705, loss=0.03796934336423874
I0307 17:44:44.296592 140457563555584 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.029416128993034363, loss=0.032890524715185165
I0307 17:45:06.010938 140457571948288 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.02414156310260296, loss=0.03224656358361244
I0307 17:45:27.972474 140457563555584 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0261470265686512, loss=0.033295270055532455
I0307 17:45:49.543760 140457571948288 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.02081800438463688, loss=0.035839665681123734
I0307 17:46:11.191682 140457563555584 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.02453344687819481, loss=0.032803624868392944
I0307 17:46:32.597269 140457571948288 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.024536864832043648, loss=0.033342454582452774
I0307 17:46:54.286298 140457563555584 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0242860596626997, loss=0.033670663833618164
I0307 17:47:11.002360 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:48:26.930144 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:48:28.894394 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:48:30.803680 140599079560384 submission_runner.py:469] Time since start: 3445.50s, 	Step: 10978, 	{'train/accuracy': 0.9903349876403809, 'train/loss': 0.03216930851340294, 'train/mean_average_precision': 0.3591861510115374, 'validation/accuracy': 0.986579179763794, 'validation/loss': 0.0447654165327549, 'validation/mean_average_precision': 0.25091364789921167, 'validation/num_examples': 43793, 'test/accuracy': 0.9858284592628479, 'test/loss': 0.04724174365401268, 'test/mean_average_precision': 0.2514152074257688, 'test/num_examples': 43793, 'score': 2419.285927772522, 'total_duration': 3445.504778146744, 'accumulated_submission_time': 2419.285927772522, 'accumulated_eval_time': 1025.7278971672058, 'accumulated_logging_time': 0.1920156478881836}
I0307 17:48:30.814038 140457571948288 logging_writer.py:48] [10978] accumulated_eval_time=1025.73, accumulated_logging_time=0.192016, accumulated_submission_time=2419.29, global_step=10978, preemption_count=0, score=2419.29, test/accuracy=0.985828, test/loss=0.0472417, test/mean_average_precision=0.251415, test/num_examples=43793, total_duration=3445.5, train/accuracy=0.990335, train/loss=0.0321693, train/mean_average_precision=0.359186, validation/accuracy=0.986579, validation/loss=0.0447654, validation/mean_average_precision=0.250914, validation/num_examples=43793
I0307 17:48:35.912340 140457563555584 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.028812523931264877, loss=0.03794584795832634
I0307 17:48:57.617193 140457571948288 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.02051086723804474, loss=0.03051740862429142
I0307 17:49:18.778815 140457563555584 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.026265563443303108, loss=0.030930833891034126
I0307 17:49:40.083030 140457571948288 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.02184048853814602, loss=0.031219352036714554
I0307 17:50:02.022083 140457563555584 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.02318849414587021, loss=0.03410141542553902
I0307 17:50:23.931668 140457571948288 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02934981882572174, loss=0.03288578987121582
I0307 17:50:45.631387 140457563555584 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03408249840140343, loss=0.03793106973171234
I0307 17:51:07.284230 140457571948288 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.024476272985339165, loss=0.03541601821780205
I0307 17:51:28.759714 140457563555584 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.02765507809817791, loss=0.032716020941734314
I0307 17:51:50.726975 140457571948288 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.030049527063965797, loss=0.030346283689141273
I0307 17:52:12.544364 140457563555584 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.026465939357876778, loss=0.032624028623104095
I0307 17:52:30.919273 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:53:46.923639 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:53:48.940716 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:53:50.870552 140599079560384 submission_runner.py:469] Time since start: 3765.57s, 	Step: 12086, 	{'train/accuracy': 0.990524411201477, 'train/loss': 0.03106624074280262, 'train/mean_average_precision': 0.3829854208253379, 'validation/accuracy': 0.9867752194404602, 'validation/loss': 0.04414454475045204, 'validation/mean_average_precision': 0.26460933646628576, 'validation/num_examples': 43793, 'test/accuracy': 0.9858684539794922, 'test/loss': 0.04679103568196297, 'test/mean_average_precision': 0.2622607391192419, 'test/num_examples': 43793, 'score': 2659.3496923446655, 'total_duration': 3765.5716207027435, 'accumulated_submission_time': 2659.3496923446655, 'accumulated_eval_time': 1105.6790163516998, 'accumulated_logging_time': 0.2120954990386963}
I0307 17:53:50.880762 140457571948288 logging_writer.py:48] [12086] accumulated_eval_time=1105.68, accumulated_logging_time=0.212095, accumulated_submission_time=2659.35, global_step=12086, preemption_count=0, score=2659.35, test/accuracy=0.985868, test/loss=0.046791, test/mean_average_precision=0.262261, test/num_examples=43793, total_duration=3765.57, train/accuracy=0.990524, train/loss=0.0310662, train/mean_average_precision=0.382985, validation/accuracy=0.986775, validation/loss=0.0441445, validation/mean_average_precision=0.264609, validation/num_examples=43793
I0307 17:53:54.225778 140457563555584 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.02176814340054989, loss=0.03424597531557083
I0307 17:54:15.929710 140457571948288 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03386532515287399, loss=0.035713598132133484
I0307 17:54:37.690404 140457563555584 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.028326964005827904, loss=0.03301604837179184
I0307 17:54:59.296290 140457571948288 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.028173018246889114, loss=0.035598937422037125
I0307 17:55:21.271446 140457563555584 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.050167664885520935, loss=0.033642396330833435
I0307 17:55:43.456673 140457571948288 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0304267518222332, loss=0.036833398044109344
I0307 17:56:05.497055 140457563555584 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.0301522146910429, loss=0.0319465696811676
I0307 17:56:27.481629 140457571948288 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03571820631623268, loss=0.031201282516121864
I0307 17:56:48.534994 140457563555584 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.02757832780480385, loss=0.034938618540763855
I0307 17:57:09.804861 140457571948288 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0226757675409317, loss=0.032837968319654465
I0307 17:57:31.334967 140457563555584 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.02945512905716896, loss=0.030600538477301598
I0307 17:57:51.064897 140599079560384 spec.py:321] Evaluating on the training split.
I0307 17:59:08.035942 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 17:59:09.995097 140599079560384 spec.py:349] Evaluating on the test split.
I0307 17:59:11.950004 140599079560384 submission_runner.py:469] Time since start: 4086.65s, 	Step: 13191, 	{'train/accuracy': 0.9904653429985046, 'train/loss': 0.03153182193636894, 'train/mean_average_precision': 0.3860227645360099, 'validation/accuracy': 0.9866944551467896, 'validation/loss': 0.04395931586623192, 'validation/mean_average_precision': 0.26564403707665724, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.046528302133083344, 'test/mean_average_precision': 0.26038152585922486, 'test/num_examples': 43793, 'score': 2899.4923284053802, 'total_duration': 4086.6511096954346, 'accumulated_submission_time': 2899.4923284053802, 'accumulated_eval_time': 1186.5640168190002, 'accumulated_logging_time': 0.2315518856048584}
I0307 17:59:11.960141 140457571948288 logging_writer.py:48] [13191] accumulated_eval_time=1186.56, accumulated_logging_time=0.231552, accumulated_submission_time=2899.49, global_step=13191, preemption_count=0, score=2899.49, test/accuracy=0.985895, test/loss=0.0465283, test/mean_average_precision=0.260382, test/num_examples=43793, total_duration=4086.65, train/accuracy=0.990465, train/loss=0.0315318, train/mean_average_precision=0.386023, validation/accuracy=0.986694, validation/loss=0.0439593, validation/mean_average_precision=0.265644, validation/num_examples=43793
I0307 17:59:14.134721 140457563555584 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.03840876743197441, loss=0.033635567873716354
I0307 17:59:35.680083 140457571948288 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.045118775218725204, loss=0.030364997684955597
I0307 17:59:57.433697 140457563555584 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02882414497435093, loss=0.031002556905150414
I0307 18:00:19.485317 140457571948288 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.034055765718221664, loss=0.03312212973833084
I0307 18:00:41.373148 140457563555584 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.03598003089427948, loss=0.033975888043642044
I0307 18:01:03.211838 140457571948288 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.031293030828237534, loss=0.03396159037947655
I0307 18:01:25.207970 140457563555584 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.027360158041119576, loss=0.03233492001891136
I0307 18:01:47.114847 140457571948288 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0462813638150692, loss=0.03217103332281113
I0307 18:02:08.579893 140457563555584 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.04083043336868286, loss=0.03179672732949257
I0307 18:02:30.708477 140457571948288 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.042478565126657486, loss=0.03324078395962715
I0307 18:02:52.243336 140457563555584 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03372577205300331, loss=0.0365467444062233
I0307 18:03:12.072362 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:04:26.608539 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:04:28.545114 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:04:30.423696 140599079560384 submission_runner.py:469] Time since start: 4405.12s, 	Step: 14291, 	{'train/accuracy': 0.9910811185836792, 'train/loss': 0.029043609276413918, 'train/mean_average_precision': 0.426462640196047, 'validation/accuracy': 0.9867675304412842, 'validation/loss': 0.04401632398366928, 'validation/mean_average_precision': 0.25779969491847393, 'validation/num_examples': 43793, 'test/accuracy': 0.9859813451766968, 'test/loss': 0.046731673181056976, 'test/mean_average_precision': 0.26130555567149877, 'test/num_examples': 43793, 'score': 3139.5650029182434, 'total_duration': 4405.124881744385, 'accumulated_submission_time': 3139.5650029182434, 'accumulated_eval_time': 1264.9153034687042, 'accumulated_logging_time': 0.2512028217315674}
I0307 18:04:30.434078 140457571948288 logging_writer.py:48] [14291] accumulated_eval_time=1264.92, accumulated_logging_time=0.251203, accumulated_submission_time=3139.57, global_step=14291, preemption_count=0, score=3139.57, test/accuracy=0.985981, test/loss=0.0467317, test/mean_average_precision=0.261306, test/num_examples=43793, total_duration=4405.12, train/accuracy=0.991081, train/loss=0.0290436, train/mean_average_precision=0.426463, validation/accuracy=0.986768, validation/loss=0.0440163, validation/mean_average_precision=0.2578, validation/num_examples=43793
I0307 18:04:32.577249 140457563555584 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.029570339247584343, loss=0.03227469325065613
I0307 18:04:54.168366 140457571948288 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.03196904435753822, loss=0.03515513241291046
I0307 18:05:15.704085 140457563555584 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.033199988305568695, loss=0.03311331197619438
I0307 18:05:37.400127 140457571948288 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.03577098250389099, loss=0.03543618321418762
I0307 18:05:59.047439 140457563555584 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.037535957992076874, loss=0.03171258792281151
I0307 18:06:20.725734 140457571948288 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03198344632983208, loss=0.033414289355278015
I0307 18:06:42.457497 140457563555584 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.03923793509602547, loss=0.031073076650500298
I0307 18:07:04.351719 140457571948288 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03585118427872658, loss=0.034925393760204315
I0307 18:07:26.079444 140457563555584 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.042433708906173706, loss=0.03262057155370712
I0307 18:07:47.968673 140457571948288 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.032725654542446136, loss=0.035267334431409836
I0307 18:08:09.993354 140457563555584 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.031337056308984756, loss=0.03442533314228058
I0307 18:08:30.611564 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:09:45.177638 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:09:47.357967 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:09:49.432422 140599079560384 submission_runner.py:469] Time since start: 4724.13s, 	Step: 15396, 	{'train/accuracy': 0.9908100962638855, 'train/loss': 0.0302596315741539, 'train/mean_average_precision': 0.4056472952079579, 'validation/accuracy': 0.986656665802002, 'validation/loss': 0.04412321373820305, 'validation/mean_average_precision': 0.27360263397159096, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.046523600816726685, 'test/mean_average_precision': 0.26494671357332417, 'test/num_examples': 43793, 'score': 3379.7036340236664, 'total_duration': 4724.133488178253, 'accumulated_submission_time': 3379.7036340236664, 'accumulated_eval_time': 1343.7360126972198, 'accumulated_logging_time': 0.27190303802490234}
I0307 18:09:49.443522 140457571948288 logging_writer.py:48] [15396] accumulated_eval_time=1343.74, accumulated_logging_time=0.271903, accumulated_submission_time=3379.7, global_step=15396, preemption_count=0, score=3379.7, test/accuracy=0.985906, test/loss=0.0465236, test/mean_average_precision=0.264947, test/num_examples=43793, total_duration=4724.13, train/accuracy=0.99081, train/loss=0.0302596, train/mean_average_precision=0.405647, validation/accuracy=0.986657, validation/loss=0.0441232, validation/mean_average_precision=0.273603, validation/num_examples=43793
I0307 18:09:50.494139 140457563555584 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.04860764369368553, loss=0.034451618790626526
I0307 18:10:12.283232 140457571948288 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.02778584137558937, loss=0.032159749418497086
I0307 18:10:34.138934 140457563555584 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.03724728897213936, loss=0.031007414683699608
I0307 18:10:56.129634 140457571948288 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.04693860933184624, loss=0.0290212519466877
I0307 18:11:17.965902 140457563555584 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.03619706258177757, loss=0.030502676963806152
I0307 18:11:39.787833 140457571948288 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.04462646320462227, loss=0.03202119097113609
I0307 18:12:01.808905 140457563555584 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.045025166124105453, loss=0.03362061455845833
I0307 18:12:23.637548 140457571948288 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.05061648413538933, loss=0.03563534840941429
I0307 18:12:45.313197 140457563555584 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.041863009333610535, loss=0.032879266887903214
I0307 18:13:07.134032 140457571948288 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.04124513268470764, loss=0.03152896463871002
I0307 18:13:29.168641 140457563555584 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.056748002767562866, loss=0.031016627326607704
I0307 18:13:49.635756 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:15:05.728125 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:15:07.752285 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:15:09.750927 140599079560384 submission_runner.py:469] Time since start: 5044.45s, 	Step: 16498, 	{'train/accuracy': 0.9911425113677979, 'train/loss': 0.02906337007880211, 'train/mean_average_precision': 0.44068499839785924, 'validation/accuracy': 0.986907958984375, 'validation/loss': 0.043933477252721786, 'validation/mean_average_precision': 0.27722143746873806, 'validation/num_examples': 43793, 'test/accuracy': 0.9861085414886475, 'test/loss': 0.04667659476399422, 'test/mean_average_precision': 0.26375793058228075, 'test/num_examples': 43793, 'score': 3619.8550128936768, 'total_duration': 5044.452063322067, 'accumulated_submission_time': 3619.8550128936768, 'accumulated_eval_time': 1423.8510928153992, 'accumulated_logging_time': 0.29395031929016113}
I0307 18:15:09.761559 140457571948288 logging_writer.py:48] [16498] accumulated_eval_time=1423.85, accumulated_logging_time=0.29395, accumulated_submission_time=3619.86, global_step=16498, preemption_count=0, score=3619.86, test/accuracy=0.986109, test/loss=0.0466766, test/mean_average_precision=0.263758, test/num_examples=43793, total_duration=5044.45, train/accuracy=0.991143, train/loss=0.0290634, train/mean_average_precision=0.440685, validation/accuracy=0.986908, validation/loss=0.0439335, validation/mean_average_precision=0.277221, validation/num_examples=43793
I0307 18:15:10.449901 140457563555584 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0383332259953022, loss=0.029773961752653122
I0307 18:15:32.223658 140457571948288 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03441959619522095, loss=0.03007832169532776
I0307 18:15:54.324385 140457563555584 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.0401446558535099, loss=0.032411765307188034
I0307 18:16:16.290411 140457571948288 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.03879309073090553, loss=0.032813817262649536
I0307 18:16:38.206141 140457563555584 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.0466318242251873, loss=0.03334394469857216
I0307 18:16:59.633893 140457571948288 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03669673949480057, loss=0.031717315316200256
I0307 18:17:21.021618 140457563555584 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.045192889869213104, loss=0.034055329859256744
I0307 18:17:42.355711 140457571948288 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.04124459996819496, loss=0.02907666563987732
I0307 18:18:04.297850 140457563555584 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.04335399344563484, loss=0.0326143242418766
I0307 18:18:26.326259 140457571948288 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.04222509637475014, loss=0.032019004225730896
I0307 18:18:48.071372 140457563555584 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.03747865557670593, loss=0.030761435627937317
I0307 18:19:09.850382 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:20:25.456305 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:20:27.432904 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:20:29.307478 140599079560384 submission_runner.py:469] Time since start: 5364.01s, 	Step: 17600, 	{'train/accuracy': 0.9911929965019226, 'train/loss': 0.02869012951850891, 'train/mean_average_precision': 0.46597715755924074, 'validation/accuracy': 0.9868779182434082, 'validation/loss': 0.04366927966475487, 'validation/mean_average_precision': 0.279304769705352, 'validation/num_examples': 43793, 'test/accuracy': 0.9860579967498779, 'test/loss': 0.04661668837070465, 'test/mean_average_precision': 0.2691008665177276, 'test/num_examples': 43793, 'score': 3859.9057545661926, 'total_duration': 5364.008668661118, 'accumulated_submission_time': 3859.9057545661926, 'accumulated_eval_time': 1503.308146238327, 'accumulated_logging_time': 0.31415510177612305}
I0307 18:20:29.318004 140457571948288 logging_writer.py:48] [17600] accumulated_eval_time=1503.31, accumulated_logging_time=0.314155, accumulated_submission_time=3859.91, global_step=17600, preemption_count=0, score=3859.91, test/accuracy=0.986058, test/loss=0.0466167, test/mean_average_precision=0.269101, test/num_examples=43793, total_duration=5364.01, train/accuracy=0.991193, train/loss=0.0286901, train/mean_average_precision=0.465977, validation/accuracy=0.986878, validation/loss=0.0436693, validation/mean_average_precision=0.279305, validation/num_examples=43793
I0307 18:20:29.546659 140457563555584 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.03830457106232643, loss=0.030978601425886154
I0307 18:20:51.494840 140457571948288 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0736846849322319, loss=0.03271261230111122
I0307 18:21:13.356373 140457563555584 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.04362116754055023, loss=0.029459070414304733
I0307 18:21:35.601036 140457571948288 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.04602300003170967, loss=0.031506456434726715
I0307 18:21:58.188625 140457563555584 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.04665865749120712, loss=0.03238154575228691
I0307 18:22:20.714815 140457571948288 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.04060458019375801, loss=0.03309827297925949
I0307 18:22:43.282118 140457563555584 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04738333821296692, loss=0.032274648547172546
I0307 18:23:06.085998 140457571948288 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.04360813647508621, loss=0.029642390087246895
I0307 18:23:28.488839 140457563555584 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.04640042409300804, loss=0.0341954231262207
I0307 18:23:50.440346 140457571948288 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.04758724942803383, loss=0.030460836365818977
I0307 18:24:12.606510 140457563555584 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.03900955989956856, loss=0.030479369685053825
I0307 18:24:29.467267 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:25:44.927789 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:25:46.879009 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:25:48.786190 140599079560384 submission_runner.py:469] Time since start: 5683.49s, 	Step: 18677, 	{'train/accuracy': 0.9911320209503174, 'train/loss': 0.028838112950325012, 'train/mean_average_precision': 0.4315994234809823, 'validation/accuracy': 0.986940860748291, 'validation/loss': 0.043792419135570526, 'validation/mean_average_precision': 0.27110268295108964, 'validation/num_examples': 43793, 'test/accuracy': 0.9861359000205994, 'test/loss': 0.046408139169216156, 'test/mean_average_precision': 0.2711829242780989, 'test/num_examples': 43793, 'score': 4100.013939380646, 'total_duration': 5683.487295627594, 'accumulated_submission_time': 4100.013939380646, 'accumulated_eval_time': 1582.6269409656525, 'accumulated_logging_time': 0.3342301845550537}
I0307 18:25:48.797565 140457571948288 logging_writer.py:48] [18677] accumulated_eval_time=1582.63, accumulated_logging_time=0.33423, accumulated_submission_time=4100.01, global_step=18677, preemption_count=0, score=4100.01, test/accuracy=0.986136, test/loss=0.0464081, test/mean_average_precision=0.271183, test/num_examples=43793, total_duration=5683.49, train/accuracy=0.991132, train/loss=0.0288381, train/mean_average_precision=0.431599, validation/accuracy=0.986941, validation/loss=0.0437924, validation/mean_average_precision=0.271103, validation/num_examples=43793
I0307 18:25:54.094333 140457563555584 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.048527684062719345, loss=0.02809767797589302
I0307 18:26:16.090455 140457571948288 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.045166198164224625, loss=0.029762979596853256
I0307 18:26:37.912388 140457563555584 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.05138551443815231, loss=0.03234317898750305
I0307 18:27:00.010625 140457571948288 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.03834300488233566, loss=0.03125223517417908
I0307 18:27:21.921579 140457563555584 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.050336360931396484, loss=0.0314939022064209
I0307 18:27:44.428440 140457571948288 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.04458446055650711, loss=0.0252855084836483
I0307 18:28:06.501095 140457563555584 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.04711104556918144, loss=0.03144611045718193
I0307 18:28:28.741427 140457571948288 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.047669440507888794, loss=0.027845531702041626
I0307 18:28:50.489197 140457563555584 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.04153621569275856, loss=0.03108956664800644
I0307 18:29:12.337373 140457571948288 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04418381676077843, loss=0.029478956013917923
I0307 18:29:34.625565 140457563555584 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.05475187301635742, loss=0.03064979799091816
I0307 18:29:48.828821 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:31:05.175156 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:31:07.114326 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:31:09.092131 140599079560384 submission_runner.py:469] Time since start: 6003.79s, 	Step: 19766, 	{'train/accuracy': 0.9915907382965088, 'train/loss': 0.027355369180440903, 'train/mean_average_precision': 0.47479867412456644, 'validation/accuracy': 0.9869996905326843, 'validation/loss': 0.043903157114982605, 'validation/mean_average_precision': 0.27861525636780815, 'validation/num_examples': 43793, 'test/accuracy': 0.986042857170105, 'test/loss': 0.046836670488119125, 'test/mean_average_precision': 0.2715348981717626, 'test/num_examples': 43793, 'score': 4340.0025770664215, 'total_duration': 6003.793316602707, 'accumulated_submission_time': 4340.0025770664215, 'accumulated_eval_time': 1662.8902022838593, 'accumulated_logging_time': 0.3551747798919678}
I0307 18:31:09.103547 140457571948288 logging_writer.py:48] [19766] accumulated_eval_time=1662.89, accumulated_logging_time=0.355175, accumulated_submission_time=4340, global_step=19766, preemption_count=0, score=4340, test/accuracy=0.986043, test/loss=0.0468367, test/mean_average_precision=0.271535, test/num_examples=43793, total_duration=6003.79, train/accuracy=0.991591, train/loss=0.0273554, train/mean_average_precision=0.474799, validation/accuracy=0.987, validation/loss=0.0439032, validation/mean_average_precision=0.278615, validation/num_examples=43793
I0307 18:31:16.687035 140457563555584 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.038372572511434555, loss=0.02805795520544052
I0307 18:31:38.405210 140457571948288 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.04411584883928299, loss=0.029340986162424088
I0307 18:32:00.293025 140457563555584 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.05178392305970192, loss=0.028020203113555908
I0307 18:32:22.432165 140457571948288 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04942837357521057, loss=0.030130330473184586
I0307 18:32:44.479762 140457563555584 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.044541895389556885, loss=0.03241156414151192
I0307 18:33:06.243970 140457571948288 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.049373917281627655, loss=0.025623152032494545
I0307 18:33:28.385292 140457563555584 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.04907684400677681, loss=0.03052637353539467
I0307 18:33:50.493415 140457571948288 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.04511591047048569, loss=0.029001250863075256
I0307 18:34:12.203639 140457563555584 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.05042612552642822, loss=0.028354596346616745
I0307 18:34:34.103657 140457571948288 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.05113479122519493, loss=0.030034586787223816
I0307 18:34:55.918783 140457563555584 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.047778114676475525, loss=0.03276408091187477
I0307 18:35:09.214564 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:36:24.914264 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:36:26.845695 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:36:28.778424 140599079560384 submission_runner.py:469] Time since start: 6323.48s, 	Step: 20863, 	{'train/accuracy': 0.9913347363471985, 'train/loss': 0.02805819921195507, 'train/mean_average_precision': 0.4600625930185884, 'validation/accuracy': 0.986845850944519, 'validation/loss': 0.04403992369771004, 'validation/mean_average_precision': 0.2755472894965234, 'validation/num_examples': 43793, 'test/accuracy': 0.9861056208610535, 'test/loss': 0.046770524233579636, 'test/mean_average_precision': 0.27163194059058016, 'test/num_examples': 43793, 'score': 4580.072528600693, 'total_duration': 6323.4795463085175, 'accumulated_submission_time': 4580.072528600693, 'accumulated_eval_time': 1742.4539473056793, 'accumulated_logging_time': 0.3776204586029053}
I0307 18:36:28.790059 140457571948288 logging_writer.py:48] [20863] accumulated_eval_time=1742.45, accumulated_logging_time=0.37762, accumulated_submission_time=4580.07, global_step=20863, preemption_count=0, score=4580.07, test/accuracy=0.986106, test/loss=0.0467705, test/mean_average_precision=0.271632, test/num_examples=43793, total_duration=6323.48, train/accuracy=0.991335, train/loss=0.0280582, train/mean_average_precision=0.460063, validation/accuracy=0.986846, validation/loss=0.0440399, validation/mean_average_precision=0.275547, validation/num_examples=43793
I0307 18:36:37.064494 140457563555584 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.05406711623072624, loss=0.02784186601638794
I0307 18:36:58.754160 140457571948288 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0471980907022953, loss=0.027400830760598183
I0307 18:37:20.449884 140457563555584 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.049629516899585724, loss=0.03170778229832649
I0307 18:37:42.027975 140457571948288 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.04844643920660019, loss=0.02935020811855793
I0307 18:38:03.486754 140457563555584 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.05875227972865105, loss=0.03079189918935299
I0307 18:38:25.365810 140457571948288 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.04873165488243103, loss=0.029651416465640068
I0307 18:38:47.280194 140457563555584 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.05195673182606697, loss=0.028658637776970863
I0307 18:39:09.245973 140457571948288 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.06047772243618965, loss=0.031133204698562622
I0307 18:39:31.217244 140457563555584 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.05200278013944626, loss=0.03002551943063736
I0307 18:39:52.322838 140457571948288 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.062357109040021896, loss=0.033391911536455154
I0307 18:40:13.715738 140457563555584 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.05611168220639229, loss=0.026330482214689255
I0307 18:40:28.911163 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:41:41.597249 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:41:43.640366 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:41:45.739703 140599079560384 submission_runner.py:469] Time since start: 6640.44s, 	Step: 21972, 	{'train/accuracy': 0.9915658235549927, 'train/loss': 0.027395011857151985, 'train/mean_average_precision': 0.4685489623012838, 'validation/accuracy': 0.9869627952575684, 'validation/loss': 0.04418163746595383, 'validation/mean_average_precision': 0.2761416568358736, 'validation/num_examples': 43793, 'test/accuracy': 0.9862201809883118, 'test/loss': 0.046943556517362595, 'test/mean_average_precision': 0.2717343747849372, 'test/num_examples': 43793, 'score': 4820.15309047699, 'total_duration': 6640.440789699554, 'accumulated_submission_time': 4820.15309047699, 'accumulated_eval_time': 1819.2823376655579, 'accumulated_logging_time': 0.3996124267578125}
I0307 18:41:45.752310 140457571948288 logging_writer.py:48] [21972] accumulated_eval_time=1819.28, accumulated_logging_time=0.399612, accumulated_submission_time=4820.15, global_step=21972, preemption_count=0, score=4820.15, test/accuracy=0.98622, test/loss=0.0469436, test/mean_average_precision=0.271734, test/num_examples=43793, total_duration=6640.44, train/accuracy=0.991566, train/loss=0.027395, train/mean_average_precision=0.468549, validation/accuracy=0.986963, validation/loss=0.0441816, validation/mean_average_precision=0.276142, validation/num_examples=43793
I0307 18:41:51.869712 140457563555584 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.046031154692173004, loss=0.027449490502476692
I0307 18:42:13.444542 140457571948288 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.05107789859175682, loss=0.02802659384906292
I0307 18:42:35.191413 140457563555584 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.05815490335226059, loss=0.029878253117203712
I0307 18:42:57.036080 140457571948288 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.046958740800619125, loss=0.026316655799746513
I0307 18:43:18.890688 140457563555584 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.05995461717247963, loss=0.03398704156279564
I0307 18:43:40.311158 140457571948288 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.05931101366877556, loss=0.030872594565153122
I0307 18:44:01.755301 140457563555584 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.04845907911658287, loss=0.02807491458952427
I0307 18:44:23.466202 140457571948288 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.045136697590351105, loss=0.02610696665942669
I0307 18:44:45.013056 140457563555584 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.057469937950372696, loss=0.027318641543388367
I0307 18:45:06.658385 140457571948288 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.05602928251028061, loss=0.03138025104999542
I0307 18:45:28.364715 140457563555584 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.05309796333312988, loss=0.030467111617326736
I0307 18:45:45.818064 140599079560384 spec.py:321] Evaluating on the training split.
I0307 18:47:01.934976 140599079560384 spec.py:333] Evaluating on the validation split.
I0307 18:47:03.870802 140599079560384 spec.py:349] Evaluating on the test split.
I0307 18:47:05.763208 140599079560384 submission_runner.py:469] Time since start: 6960.46s, 	Step: 23080, 	{'train/accuracy': 0.9917892813682556, 'train/loss': 0.026538562029600143, 'train/mean_average_precision': 0.5012284759907096, 'validation/accuracy': 0.9870865941047668, 'validation/loss': 0.04390565678477287, 'validation/mean_average_precision': 0.284518941011064, 'validation/num_examples': 43793, 'test/accuracy': 0.9862774610519409, 'test/loss': 0.04663080349564552, 'test/mean_average_precision': 0.2724634284710311, 'test/num_examples': 43793, 'score': 5060.175243616104, 'total_duration': 6960.464319705963, 'accumulated_submission_time': 5060.175243616104, 'accumulated_eval_time': 1899.2273635864258, 'accumulated_logging_time': 0.4254477024078369}
I0307 18:47:05.775122 140457571948288 logging_writer.py:48] [23080] accumulated_eval_time=1899.23, accumulated_logging_time=0.425448, accumulated_submission_time=5060.18, global_step=23080, preemption_count=0, score=5060.18, test/accuracy=0.986277, test/loss=0.0466308, test/mean_average_precision=0.272463, test/num_examples=43793, total_duration=6960.46, train/accuracy=0.991789, train/loss=0.0265386, train/mean_average_precision=0.501228, validation/accuracy=0.987087, validation/loss=0.0439057, validation/mean_average_precision=0.284519, validation/num_examples=43793
I0307 18:47:05.790545 140457563555584 logging_writer.py:48] [23080] global_step=23080, preemption_count=0, score=5060.18
I0307 18:47:05.935755 140599079560384 submission_runner.py:646] Tuning trial 5/5
I0307 18:47:05.935951 140599079560384 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0307 18:47:05.937190 140599079560384 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.48007529973983765, 'train/loss': 0.7292938232421875, 'train/mean_average_precision': 0.02283410630454976, 'validation/accuracy': 0.4905015826225281, 'validation/loss': 0.7234783172607422, 'validation/mean_average_precision': 0.02555354870908746, 'validation/num_examples': 43793, 'test/accuracy': 0.4925242066383362, 'test/loss': 0.7222917079925537, 'test/mean_average_precision': 0.026829375439128535, 'test/num_examples': 43793, 'score': 18.764833211898804, 'total_duration': 233.93102025985718, 'accumulated_submission_time': 18.764833211898804, 'accumulated_eval_time': 215.16605591773987, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1088, {'train/accuracy': 0.9870059490203857, 'train/loss': 0.048422180116176605, 'train/mean_average_precision': 0.08479339055811896, 'validation/accuracy': 0.9843679666519165, 'validation/loss': 0.05816641449928284, 'validation/mean_average_precision': 0.0849512830954789, 'validation/num_examples': 43793, 'test/accuracy': 0.9833509922027588, 'test/loss': 0.061555877327919006, 'test/mean_average_precision': 0.07919301991733778, 'test/num_examples': 43793, 'score': 258.76359963417053, 'total_duration': 553.7720704078674, 'accumulated_submission_time': 258.76359963417053, 'accumulated_eval_time': 294.9598033428192, 'accumulated_logging_time': 0.016712427139282227, 'global_step': 1088, 'preemption_count': 0}), (2176, {'train/accuracy': 0.987819254398346, 'train/loss': 0.043525632470846176, 'train/mean_average_precision': 0.14257660419287255, 'validation/accuracy': 0.9850130081176758, 'validation/loss': 0.052820298820734024, 'validation/mean_average_precision': 0.13431356962689756, 'validation/num_examples': 43793, 'test/accuracy': 0.984071671962738, 'test/loss': 0.05582745000720024, 'test/mean_average_precision': 0.1318445734594856, 'test/num_examples': 43793, 'score': 498.8953061103821, 'total_duration': 873.3454933166504, 'accumulated_submission_time': 498.8953061103821, 'accumulated_eval_time': 374.35080075263977, 'accumulated_logging_time': 0.03506875038146973, 'global_step': 2176, 'preemption_count': 0}), (3268, {'train/accuracy': 0.9882214069366455, 'train/loss': 0.04083537310361862, 'train/mean_average_precision': 0.18236941732909004, 'validation/accuracy': 0.9853994846343994, 'validation/loss': 0.05042979121208191, 'validation/mean_average_precision': 0.1553376895724147, 'validation/num_examples': 43793, 'test/accuracy': 0.9844818711280823, 'test/loss': 0.05319877341389656, 'test/mean_average_precision': 0.15897204050035846, 'test/num_examples': 43793, 'score': 738.9628694057465, 'total_duration': 1193.1703388690948, 'accumulated_submission_time': 738.9628694057465, 'accumulated_eval_time': 454.05983114242554, 'accumulated_logging_time': 0.05324888229370117, 'global_step': 3268, 'preemption_count': 0}), (4363, {'train/accuracy': 0.9887198805809021, 'train/loss': 0.03877720236778259, 'train/mean_average_precision': 0.20975646864817155, 'validation/accuracy': 0.9857136607170105, 'validation/loss': 0.048355501145124435, 'validation/mean_average_precision': 0.18880914599759863, 'validation/num_examples': 43793, 'test/accuracy': 0.9848297834396362, 'test/loss': 0.05119384080171585, 'test/mean_average_precision': 0.19210166610070203, 'test/num_examples': 43793, 'score': 979.1117026805878, 'total_duration': 1514.2149381637573, 'accumulated_submission_time': 979.1117026805878, 'accumulated_eval_time': 534.9072921276093, 'accumulated_logging_time': 0.07212185859680176, 'global_step': 4363, 'preemption_count': 0}), (5457, {'train/accuracy': 0.9890140891075134, 'train/loss': 0.03742047771811485, 'train/mean_average_precision': 0.24820763483645697, 'validation/accuracy': 0.9860498309135437, 'validation/loss': 0.047213517129421234, 'validation/mean_average_precision': 0.2152637398390373, 'validation/num_examples': 43793, 'test/accuracy': 0.9851452708244324, 'test/loss': 0.04988021403551102, 'test/mean_average_precision': 0.21482940856492982, 'test/num_examples': 43793, 'score': 1219.0983471870422, 'total_duration': 1834.559006690979, 'accumulated_submission_time': 1219.0983471870422, 'accumulated_eval_time': 615.2171878814697, 'accumulated_logging_time': 0.09035277366638184, 'global_step': 5457, 'preemption_count': 0}), (6553, {'train/accuracy': 0.989457368850708, 'train/loss': 0.03533061221241951, 'train/mean_average_precision': 0.2873491300239006, 'validation/accuracy': 0.9862543940544128, 'validation/loss': 0.04603609815239906, 'validation/mean_average_precision': 0.22877868137675786, 'validation/num_examples': 43793, 'test/accuracy': 0.9854017496109009, 'test/loss': 0.04862169921398163, 'test/mean_average_precision': 0.22937052014241854, 'test/num_examples': 43793, 'score': 1459.0844507217407, 'total_duration': 2163.6116647720337, 'accumulated_submission_time': 1459.0844507217407, 'accumulated_eval_time': 704.2366998195648, 'accumulated_logging_time': 0.1095733642578125, 'global_step': 6553, 'preemption_count': 0}), (7652, {'train/accuracy': 0.9895524978637695, 'train/loss': 0.03532123938202858, 'train/mean_average_precision': 0.2968694208852885, 'validation/accuracy': 0.9862657785415649, 'validation/loss': 0.04557625576853752, 'validation/mean_average_precision': 0.23296524540126679, 'validation/num_examples': 43793, 'test/accuracy': 0.9854401350021362, 'test/loss': 0.04819291830062866, 'test/mean_average_precision': 0.23364066459334326, 'test/num_examples': 43793, 'score': 1699.0924091339111, 'total_duration': 2484.86448431015, 'accumulated_submission_time': 1699.0924091339111, 'accumulated_eval_time': 785.4276740550995, 'accumulated_logging_time': 0.13152289390563965, 'global_step': 7652, 'preemption_count': 0}), (8751, {'train/accuracy': 0.9899919629096985, 'train/loss': 0.0334242582321167, 'train/mean_average_precision': 0.32886826535202107, 'validation/accuracy': 0.9862824082374573, 'validation/loss': 0.04539112746715546, 'validation/mean_average_precision': 0.24188441221917778, 'validation/num_examples': 43793, 'test/accuracy': 0.9854556918144226, 'test/loss': 0.04789965599775314, 'test/mean_average_precision': 0.23819103737125583, 'test/num_examples': 43793, 'score': 1939.0843923091888, 'total_duration': 2806.049070596695, 'accumulated_submission_time': 1939.0843923091888, 'accumulated_eval_time': 866.5722324848175, 'accumulated_logging_time': 0.15059161186218262, 'global_step': 8751, 'preemption_count': 0}), (9867, {'train/accuracy': 0.9899487495422363, 'train/loss': 0.033588752150535583, 'train/mean_average_precision': 0.34148947858789175, 'validation/accuracy': 0.9862747192382812, 'validation/loss': 0.04532691836357117, 'validation/mean_average_precision': 0.24617711319956007, 'validation/num_examples': 43793, 'test/accuracy': 0.9854148626327515, 'test/loss': 0.048002731055021286, 'test/mean_average_precision': 0.23690137486416762, 'test/num_examples': 43793, 'score': 2179.166847705841, 'total_duration': 3125.5352046489716, 'accumulated_submission_time': 2179.166847705841, 'accumulated_eval_time': 945.9267082214355, 'accumulated_logging_time': 0.17134642601013184, 'global_step': 9867, 'preemption_count': 0}), (10978, {'train/accuracy': 0.9903349876403809, 'train/loss': 0.03216930851340294, 'train/mean_average_precision': 0.3591861510115374, 'validation/accuracy': 0.986579179763794, 'validation/loss': 0.0447654165327549, 'validation/mean_average_precision': 0.25091364789921167, 'validation/num_examples': 43793, 'test/accuracy': 0.9858284592628479, 'test/loss': 0.04724174365401268, 'test/mean_average_precision': 0.2514152074257688, 'test/num_examples': 43793, 'score': 2419.285927772522, 'total_duration': 3445.504778146744, 'accumulated_submission_time': 2419.285927772522, 'accumulated_eval_time': 1025.7278971672058, 'accumulated_logging_time': 0.1920156478881836, 'global_step': 10978, 'preemption_count': 0}), (12086, {'train/accuracy': 0.990524411201477, 'train/loss': 0.03106624074280262, 'train/mean_average_precision': 0.3829854208253379, 'validation/accuracy': 0.9867752194404602, 'validation/loss': 0.04414454475045204, 'validation/mean_average_precision': 0.26460933646628576, 'validation/num_examples': 43793, 'test/accuracy': 0.9858684539794922, 'test/loss': 0.04679103568196297, 'test/mean_average_precision': 0.2622607391192419, 'test/num_examples': 43793, 'score': 2659.3496923446655, 'total_duration': 3765.5716207027435, 'accumulated_submission_time': 2659.3496923446655, 'accumulated_eval_time': 1105.6790163516998, 'accumulated_logging_time': 0.2120954990386963, 'global_step': 12086, 'preemption_count': 0}), (13191, {'train/accuracy': 0.9904653429985046, 'train/loss': 0.03153182193636894, 'train/mean_average_precision': 0.3860227645360099, 'validation/accuracy': 0.9866944551467896, 'validation/loss': 0.04395931586623192, 'validation/mean_average_precision': 0.26564403707665724, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.046528302133083344, 'test/mean_average_precision': 0.26038152585922486, 'test/num_examples': 43793, 'score': 2899.4923284053802, 'total_duration': 4086.6511096954346, 'accumulated_submission_time': 2899.4923284053802, 'accumulated_eval_time': 1186.5640168190002, 'accumulated_logging_time': 0.2315518856048584, 'global_step': 13191, 'preemption_count': 0}), (14291, {'train/accuracy': 0.9910811185836792, 'train/loss': 0.029043609276413918, 'train/mean_average_precision': 0.426462640196047, 'validation/accuracy': 0.9867675304412842, 'validation/loss': 0.04401632398366928, 'validation/mean_average_precision': 0.25779969491847393, 'validation/num_examples': 43793, 'test/accuracy': 0.9859813451766968, 'test/loss': 0.046731673181056976, 'test/mean_average_precision': 0.26130555567149877, 'test/num_examples': 43793, 'score': 3139.5650029182434, 'total_duration': 4405.124881744385, 'accumulated_submission_time': 3139.5650029182434, 'accumulated_eval_time': 1264.9153034687042, 'accumulated_logging_time': 0.2512028217315674, 'global_step': 14291, 'preemption_count': 0}), (15396, {'train/accuracy': 0.9908100962638855, 'train/loss': 0.0302596315741539, 'train/mean_average_precision': 0.4056472952079579, 'validation/accuracy': 0.986656665802002, 'validation/loss': 0.04412321373820305, 'validation/mean_average_precision': 0.27360263397159096, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.046523600816726685, 'test/mean_average_precision': 0.26494671357332417, 'test/num_examples': 43793, 'score': 3379.7036340236664, 'total_duration': 4724.133488178253, 'accumulated_submission_time': 3379.7036340236664, 'accumulated_eval_time': 1343.7360126972198, 'accumulated_logging_time': 0.27190303802490234, 'global_step': 15396, 'preemption_count': 0}), (16498, {'train/accuracy': 0.9911425113677979, 'train/loss': 0.02906337007880211, 'train/mean_average_precision': 0.44068499839785924, 'validation/accuracy': 0.986907958984375, 'validation/loss': 0.043933477252721786, 'validation/mean_average_precision': 0.27722143746873806, 'validation/num_examples': 43793, 'test/accuracy': 0.9861085414886475, 'test/loss': 0.04667659476399422, 'test/mean_average_precision': 0.26375793058228075, 'test/num_examples': 43793, 'score': 3619.8550128936768, 'total_duration': 5044.452063322067, 'accumulated_submission_time': 3619.8550128936768, 'accumulated_eval_time': 1423.8510928153992, 'accumulated_logging_time': 0.29395031929016113, 'global_step': 16498, 'preemption_count': 0}), (17600, {'train/accuracy': 0.9911929965019226, 'train/loss': 0.02869012951850891, 'train/mean_average_precision': 0.46597715755924074, 'validation/accuracy': 0.9868779182434082, 'validation/loss': 0.04366927966475487, 'validation/mean_average_precision': 0.279304769705352, 'validation/num_examples': 43793, 'test/accuracy': 0.9860579967498779, 'test/loss': 0.04661668837070465, 'test/mean_average_precision': 0.2691008665177276, 'test/num_examples': 43793, 'score': 3859.9057545661926, 'total_duration': 5364.008668661118, 'accumulated_submission_time': 3859.9057545661926, 'accumulated_eval_time': 1503.308146238327, 'accumulated_logging_time': 0.31415510177612305, 'global_step': 17600, 'preemption_count': 0}), (18677, {'train/accuracy': 0.9911320209503174, 'train/loss': 0.028838112950325012, 'train/mean_average_precision': 0.4315994234809823, 'validation/accuracy': 0.986940860748291, 'validation/loss': 0.043792419135570526, 'validation/mean_average_precision': 0.27110268295108964, 'validation/num_examples': 43793, 'test/accuracy': 0.9861359000205994, 'test/loss': 0.046408139169216156, 'test/mean_average_precision': 0.2711829242780989, 'test/num_examples': 43793, 'score': 4100.013939380646, 'total_duration': 5683.487295627594, 'accumulated_submission_time': 4100.013939380646, 'accumulated_eval_time': 1582.6269409656525, 'accumulated_logging_time': 0.3342301845550537, 'global_step': 18677, 'preemption_count': 0}), (19766, {'train/accuracy': 0.9915907382965088, 'train/loss': 0.027355369180440903, 'train/mean_average_precision': 0.47479867412456644, 'validation/accuracy': 0.9869996905326843, 'validation/loss': 0.043903157114982605, 'validation/mean_average_precision': 0.27861525636780815, 'validation/num_examples': 43793, 'test/accuracy': 0.986042857170105, 'test/loss': 0.046836670488119125, 'test/mean_average_precision': 0.2715348981717626, 'test/num_examples': 43793, 'score': 4340.0025770664215, 'total_duration': 6003.793316602707, 'accumulated_submission_time': 4340.0025770664215, 'accumulated_eval_time': 1662.8902022838593, 'accumulated_logging_time': 0.3551747798919678, 'global_step': 19766, 'preemption_count': 0}), (20863, {'train/accuracy': 0.9913347363471985, 'train/loss': 0.02805819921195507, 'train/mean_average_precision': 0.4600625930185884, 'validation/accuracy': 0.986845850944519, 'validation/loss': 0.04403992369771004, 'validation/mean_average_precision': 0.2755472894965234, 'validation/num_examples': 43793, 'test/accuracy': 0.9861056208610535, 'test/loss': 0.046770524233579636, 'test/mean_average_precision': 0.27163194059058016, 'test/num_examples': 43793, 'score': 4580.072528600693, 'total_duration': 6323.4795463085175, 'accumulated_submission_time': 4580.072528600693, 'accumulated_eval_time': 1742.4539473056793, 'accumulated_logging_time': 0.3776204586029053, 'global_step': 20863, 'preemption_count': 0}), (21972, {'train/accuracy': 0.9915658235549927, 'train/loss': 0.027395011857151985, 'train/mean_average_precision': 0.4685489623012838, 'validation/accuracy': 0.9869627952575684, 'validation/loss': 0.04418163746595383, 'validation/mean_average_precision': 0.2761416568358736, 'validation/num_examples': 43793, 'test/accuracy': 0.9862201809883118, 'test/loss': 0.046943556517362595, 'test/mean_average_precision': 0.2717343747849372, 'test/num_examples': 43793, 'score': 4820.15309047699, 'total_duration': 6640.440789699554, 'accumulated_submission_time': 4820.15309047699, 'accumulated_eval_time': 1819.2823376655579, 'accumulated_logging_time': 0.3996124267578125, 'global_step': 21972, 'preemption_count': 0}), (23080, {'train/accuracy': 0.9917892813682556, 'train/loss': 0.026538562029600143, 'train/mean_average_precision': 0.5012284759907096, 'validation/accuracy': 0.9870865941047668, 'validation/loss': 0.04390565678477287, 'validation/mean_average_precision': 0.284518941011064, 'validation/num_examples': 43793, 'test/accuracy': 0.9862774610519409, 'test/loss': 0.04663080349564552, 'test/mean_average_precision': 0.2724634284710311, 'test/num_examples': 43793, 'score': 5060.175243616104, 'total_duration': 6960.464319705963, 'accumulated_submission_time': 5060.175243616104, 'accumulated_eval_time': 1899.2273635864258, 'accumulated_logging_time': 0.4254477024078369, 'global_step': 23080, 'preemption_count': 0})], 'global_step': 23080}
I0307 18:47:05.937314 140599079560384 submission_runner.py:649] Timing: 5060.175243616104
I0307 18:47:05.937360 140599079560384 submission_runner.py:651] Total number of evals: 22
I0307 18:47:05.937411 140599079560384 submission_runner.py:652] ====================
I0307 18:47:05.937606 140599079560384 submission_runner.py:750] Final ogbg score: 4
