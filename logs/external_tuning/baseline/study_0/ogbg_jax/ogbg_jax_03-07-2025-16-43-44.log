python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-237046077 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/ogbg_jax_03-07-2025-16-43-44.log
2025-03-07 16:43:45.720565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741365825.742157       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741365825.748680       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:43:54.328564 140351869203648 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax.
I0307 16:43:55.137489 140351869203648 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:43:55.140397 140351869203648 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:43:55.142204 140351869203648 submission_runner.py:606] Using RNG seed -237046077
I0307 16:43:55.732311 140351869203648 submission_runner.py:615] --- Tuning run 4/5 ---
I0307 16:43:55.732569 140351869203648 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_4.
I0307 16:43:55.732795 140351869203648 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_4/hparams.json.
I0307 16:43:55.985928 140351869203648 submission_runner.py:218] Initializing dataset.
I0307 16:43:57.559480 140351869203648 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:43:57.603099 140351869203648 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0307 16:43:57.823254 140351869203648 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0307 16:43:58.013874 140351869203648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:43:58.040145 140351869203648 submission_runner.py:229] Initializing model.
I0307 16:44:05.194575 140351869203648 submission_runner.py:272] Initializing optimizer.
I0307 16:44:05.602249 140351869203648 submission_runner.py:279] Initializing metrics bundle.
I0307 16:44:05.602460 140351869203648 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:44:05.603187 140351869203648 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_4 with prefix checkpoint_
I0307 16:44:05.603296 140351869203648 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_4/meta_data_0.json.
I0307 16:44:05.603466 140351869203648 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:44:05.603519 140351869203648 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:44:05.760109 140351869203648 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_4/flags_0.json.
I0307 16:44:05.792668 140351869203648 submission_runner.py:337] Starting training loop.
I0307 16:44:17.562802 140215798281984 logging_writer.py:48] [0] global_step=0, grad_norm=2.8412489891052246, loss=0.7866222858428955
I0307 16:44:17.612575 140351869203648 spec.py:321] Evaluating on the training split.
I0307 16:44:17.616071 140351869203648 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:44:17.619667 140351869203648 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:44:17.683237 140351869203648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:45:33.518845 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 16:45:33.521464 140351869203648 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:45:33.525058 140351869203648 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:45:33.584845 140351869203648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:46:36.774749 140351869203648 spec.py:349] Evaluating on the test split.
I0307 16:46:36.777162 140351869203648 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:46:36.780649 140351869203648 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:46:36.839760 140351869203648 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:47:41.336801 140351869203648 submission_runner.py:469] Time since start: 215.54s, 	Step: 1, 	{'train/accuracy': 0.4847404658794403, 'train/loss': 0.7871220111846924, 'train/mean_average_precision': 0.021790056954124008, 'validation/accuracy': 0.4927646815776825, 'validation/loss': 0.7793631553649902, 'validation/mean_average_precision': 0.026076991706914007, 'validation/num_examples': 43793, 'test/accuracy': 0.49434229731559753, 'test/loss': 0.7786306738853455, 'test/mean_average_precision': 0.02722064855864404, 'test/num_examples': 43793, 'score': 11.819800615310669, 'total_duration': 215.54408311843872, 'accumulated_submission_time': 11.819800615310669, 'accumulated_eval_time': 203.72417569160461, 'accumulated_logging_time': 0}
I0307 16:47:41.343592 140209844664064 logging_writer.py:48] [1] accumulated_eval_time=203.724, accumulated_logging_time=0, accumulated_submission_time=11.8198, global_step=1, preemption_count=0, score=11.8198, test/accuracy=0.494342, test/loss=0.778631, test/mean_average_precision=0.0272206, test/num_examples=43793, total_duration=215.544, train/accuracy=0.48474, train/loss=0.787122, train/mean_average_precision=0.0217901, validation/accuracy=0.492765, validation/loss=0.779363, validation/mean_average_precision=0.026077, validation/num_examples=43793
I0307 16:48:03.050824 140209853056768 logging_writer.py:48] [100] global_step=100, grad_norm=0.03362370282411575, loss=0.062137242406606674
I0307 16:48:24.717417 140209844664064 logging_writer.py:48] [200] global_step=200, grad_norm=0.009476537816226482, loss=0.05339285358786583
I0307 16:48:46.449767 140209853056768 logging_writer.py:48] [300] global_step=300, grad_norm=0.00702971126884222, loss=0.054203301668167114
I0307 16:49:08.132195 140209844664064 logging_writer.py:48] [400] global_step=400, grad_norm=0.0069394176825881, loss=0.05095928534865379
I0307 16:49:29.859258 140209853056768 logging_writer.py:48] [500] global_step=500, grad_norm=0.009842704981565475, loss=0.05940970033407211
I0307 16:49:51.683647 140209844664064 logging_writer.py:48] [600] global_step=600, grad_norm=0.023505233228206635, loss=0.05395900458097458
I0307 16:50:13.201118 140210306262784 logging_writer.py:48] [700] global_step=700, grad_norm=0.0339030921459198, loss=0.051070597022771835
I0307 16:50:34.560108 140210297870080 logging_writer.py:48] [800] global_step=800, grad_norm=0.006705282721668482, loss=0.05774838477373123
I0307 16:50:55.814672 140210306262784 logging_writer.py:48] [900] global_step=900, grad_norm=0.01122849527746439, loss=0.05635802447795868
I0307 16:51:17.130432 140210297870080 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.009389210492372513, loss=0.04870585724711418
I0307 16:51:38.205980 140210306262784 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.004074200056493282, loss=0.05326427146792412
I0307 16:51:41.377856 140351869203648 spec.py:321] Evaluating on the training split.
I0307 16:52:54.679521 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 16:52:56.579576 140351869203648 spec.py:349] Evaluating on the test split.
I0307 16:52:58.417380 140351869203648 submission_runner.py:469] Time since start: 532.62s, 	Step: 1116, 	{'train/accuracy': 0.986709475517273, 'train/loss': 0.05399135872721672, 'train/mean_average_precision': 0.04022380471433991, 'validation/accuracy': 0.9841905832290649, 'validation/loss': 0.06377651542425156, 'validation/mean_average_precision': 0.04044888522409357, 'validation/num_examples': 43793, 'test/accuracy': 0.9832048416137695, 'test/loss': 0.06695762276649475, 'test/mean_average_precision': 0.04210862281502998, 'test/num_examples': 43793, 'score': 251.812185049057, 'total_duration': 532.6246609687805, 'accumulated_submission_time': 251.812185049057, 'accumulated_eval_time': 280.76365876197815, 'accumulated_logging_time': 0.016812801361083984}
I0307 16:52:58.426431 140210297870080 logging_writer.py:48] [1116] accumulated_eval_time=280.764, accumulated_logging_time=0.0168128, accumulated_submission_time=251.812, global_step=1116, preemption_count=0, score=251.812, test/accuracy=0.983205, test/loss=0.0669576, test/mean_average_precision=0.0421086, test/num_examples=43793, total_duration=532.625, train/accuracy=0.986709, train/loss=0.0539914, train/mean_average_precision=0.0402238, validation/accuracy=0.984191, validation/loss=0.0637765, validation/mean_average_precision=0.0404489, validation/num_examples=43793
I0307 16:53:16.316210 140210306262784 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.003141176188364625, loss=0.05754349008202553
I0307 16:53:37.720950 140210297870080 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.006626824848353863, loss=0.06112947687506676
I0307 16:53:59.004830 140210306262784 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0069172740913927555, loss=0.051357071846723557
I0307 16:54:20.169304 140210297870080 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.006632940843701363, loss=0.05342138931155205
I0307 16:54:41.374608 140210306262784 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.011870714835822582, loss=0.05052580684423447
I0307 16:55:02.588839 140210297870080 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.012620129622519016, loss=0.058300770819187164
I0307 16:55:23.726363 140210306262784 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.006263101007789373, loss=0.06267299503087997
I0307 16:55:44.674959 140210297870080 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0031837995629757643, loss=0.04780270904302597
I0307 16:56:05.922410 140210306262784 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.00448902091011405, loss=0.05787930637598038
I0307 16:56:27.329941 140210297870080 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.014172558672726154, loss=0.056188587099313736
I0307 16:56:48.386815 140210306262784 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.009918353520333767, loss=0.045981574803590775
I0307 16:56:58.510330 140351869203648 spec.py:321] Evaluating on the training split.
I0307 16:58:08.286818 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 16:58:10.513447 140351869203648 spec.py:349] Evaluating on the test split.
I0307 16:58:12.379692 140351869203648 submission_runner.py:469] Time since start: 846.59s, 	Step: 2249, 	{'train/accuracy': 0.9868757128715515, 'train/loss': 0.050838906317949295, 'train/mean_average_precision': 0.06279027247495665, 'validation/accuracy': 0.9842352271080017, 'validation/loss': 0.06066789850592613, 'validation/mean_average_precision': 0.061190772326033586, 'validation/num_examples': 43793, 'test/accuracy': 0.9832726120948792, 'test/loss': 0.06394655257463455, 'test/mean_average_precision': 0.05942017070438365, 'test/num_examples': 43793, 'score': 491.85641956329346, 'total_duration': 846.5869677066803, 'accumulated_submission_time': 491.85641956329346, 'accumulated_eval_time': 354.63296937942505, 'accumulated_logging_time': 0.035170793533325195}
I0307 16:58:12.388423 140210297870080 logging_writer.py:48] [2249] accumulated_eval_time=354.633, accumulated_logging_time=0.0351708, accumulated_submission_time=491.856, global_step=2249, preemption_count=0, score=491.856, test/accuracy=0.983273, test/loss=0.0639466, test/mean_average_precision=0.0594202, test/num_examples=43793, total_duration=846.587, train/accuracy=0.986876, train/loss=0.0508389, train/mean_average_precision=0.0627903, validation/accuracy=0.984235, validation/loss=0.0606679, validation/mean_average_precision=0.0611908, validation/num_examples=43793
I0307 16:58:23.436752 140210306262784 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.01129054930061102, loss=0.05661566182971001
I0307 16:58:44.539515 140210297870080 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.009645218029618263, loss=0.049288902431726456
I0307 16:59:05.874308 140210306262784 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.011347559280693531, loss=0.06171644106507301
I0307 16:59:26.916941 140210297870080 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.011419265531003475, loss=0.045488256961107254
I0307 16:59:48.217164 140210306262784 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.009554802440106869, loss=0.058264534920454025
I0307 17:00:09.313964 140210297870080 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.007646531797945499, loss=0.04321279004216194
I0307 17:00:30.505688 140210306262784 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.030163854360580444, loss=0.04697157442569733
I0307 17:00:51.433950 140210297870080 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.025847824290394783, loss=0.044243697077035904
I0307 17:01:12.153694 140210306262784 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01555337943136692, loss=0.048942822962999344
I0307 17:01:32.836261 140210297870080 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.018285103142261505, loss=0.051349200308322906
I0307 17:01:53.567465 140210306262784 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.030747435986995697, loss=0.049058496952056885
I0307 17:02:12.543770 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:03:23.774472 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:03:25.685689 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:03:27.551278 140351869203648 submission_runner.py:469] Time since start: 1161.76s, 	Step: 3393, 	{'train/accuracy': 0.9872559309005737, 'train/loss': 0.047611381858587265, 'train/mean_average_precision': 0.09043060441627604, 'validation/accuracy': 0.9843655824661255, 'validation/loss': 0.05751677602529526, 'validation/mean_average_precision': 0.08779165491311472, 'validation/num_examples': 43793, 'test/accuracy': 0.9833977222442627, 'test/loss': 0.0607469342648983, 'test/mean_average_precision': 0.08593121905638043, 'test/num_examples': 43793, 'score': 731.9744083881378, 'total_duration': 1161.758546113968, 'accumulated_submission_time': 731.9744083881378, 'accumulated_eval_time': 429.6404175758362, 'accumulated_logging_time': 0.053038597106933594}
I0307 17:03:27.559956 140210297870080 logging_writer.py:48] [3393] accumulated_eval_time=429.64, accumulated_logging_time=0.0530386, accumulated_submission_time=731.974, global_step=3393, preemption_count=0, score=731.974, test/accuracy=0.983398, test/loss=0.0607469, test/mean_average_precision=0.0859312, test/num_examples=43793, total_duration=1161.76, train/accuracy=0.987256, train/loss=0.0476114, train/mean_average_precision=0.0904306, validation/accuracy=0.984366, validation/loss=0.0575168, validation/mean_average_precision=0.0877917, validation/num_examples=43793
I0307 17:03:29.240088 140210306262784 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.021328741684556007, loss=0.04810043424367905
I0307 17:03:49.763630 140210297870080 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.046336423605680466, loss=0.05501570180058479
I0307 17:04:10.333830 140210306262784 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.017386402934789658, loss=0.054587364196777344
I0307 17:04:30.831835 140210297870080 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.022259941324591637, loss=0.04689917340874672
I0307 17:04:51.107319 140210306262784 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.0439501591026783, loss=0.04706312716007233
I0307 17:05:11.755176 140210297870080 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.04694291949272156, loss=0.04429580271244049
I0307 17:05:32.258869 140210306262784 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.018693536520004272, loss=0.05162504315376282
I0307 17:05:52.844245 140210297870080 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.019549893215298653, loss=0.04556674882769585
I0307 17:06:13.559131 140210306262784 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.027572544291615486, loss=0.05007137730717659
I0307 17:06:34.349303 140210297870080 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.05988077074289322, loss=0.046790819615125656
I0307 17:06:55.074583 140210306262784 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.03769993036985397, loss=0.040699899196624756
I0307 17:07:15.684667 140210297870080 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.02890513278543949, loss=0.046595703810453415
I0307 17:07:27.720762 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:08:38.040013 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:08:39.944020 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:08:41.811044 140351869203648 submission_runner.py:469] Time since start: 1476.02s, 	Step: 4559, 	{'train/accuracy': 0.987697184085846, 'train/loss': 0.04486807435750961, 'train/mean_average_precision': 0.11786624348268035, 'validation/accuracy': 0.9848851561546326, 'validation/loss': 0.054417744278907776, 'validation/mean_average_precision': 0.11528918522697759, 'validation/num_examples': 43793, 'test/accuracy': 0.9838892817497253, 'test/loss': 0.05748297646641731, 'test/mean_average_precision': 0.11277707008183724, 'test/num_examples': 43793, 'score': 972.0975506305695, 'total_duration': 1476.0183260440826, 'accumulated_submission_time': 972.0975506305695, 'accumulated_eval_time': 503.73065161705017, 'accumulated_logging_time': 0.07184791564941406}
I0307 17:08:41.820042 140209761548032 logging_writer.py:48] [4559] accumulated_eval_time=503.731, accumulated_logging_time=0.0718479, accumulated_submission_time=972.098, global_step=4559, preemption_count=0, score=972.098, test/accuracy=0.983889, test/loss=0.057483, test/mean_average_precision=0.112777, test/num_examples=43793, total_duration=1476.02, train/accuracy=0.987697, train/loss=0.0448681, train/mean_average_precision=0.117866, validation/accuracy=0.984885, validation/loss=0.0544177, validation/mean_average_precision=0.115289, validation/num_examples=43793
I0307 17:08:50.468418 140209753155328 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.020373279228806496, loss=0.0482146292924881
I0307 17:09:11.046992 140209761548032 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.015128678642213345, loss=0.0441412478685379
I0307 17:09:31.722158 140209753155328 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01690535992383957, loss=0.04239732027053833
I0307 17:09:52.408596 140209761548032 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.016884801909327507, loss=0.03905203193426132
I0307 17:10:13.162732 140209753155328 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.04221532866358757, loss=0.04303273931145668
I0307 17:10:33.796454 140209761548032 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.05047101154923439, loss=0.048133499920368195
I0307 17:10:54.390457 140209753155328 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.017035672441124916, loss=0.04331475496292114
I0307 17:11:15.099796 140209761548032 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.05315569415688515, loss=0.04614296182990074
I0307 17:11:35.744946 140209753155328 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.05131502449512482, loss=0.043025244027376175
I0307 17:11:56.472290 140209761548032 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.06251709908246994, loss=0.04526016116142273
I0307 17:12:17.099767 140209753155328 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.05502091348171234, loss=0.041020654141902924
I0307 17:12:37.617381 140209761548032 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.048856765031814575, loss=0.042962297797203064
I0307 17:12:41.964800 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:13:52.957446 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:13:54.862466 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:13:56.730924 140351869203648 submission_runner.py:469] Time since start: 1790.94s, 	Step: 5722, 	{'train/accuracy': 0.9877718091011047, 'train/loss': 0.04345260187983513, 'train/mean_average_precision': 0.14823155518071335, 'validation/accuracy': 0.9849667549133301, 'validation/loss': 0.05403447151184082, 'validation/mean_average_precision': 0.13694550827480725, 'validation/num_examples': 43793, 'test/accuracy': 0.9840265512466431, 'test/loss': 0.0570068284869194, 'test/mean_average_precision': 0.13345553758974338, 'test/num_examples': 43793, 'score': 1212.2034177780151, 'total_duration': 1790.9381802082062, 'accumulated_submission_time': 1212.2034177780151, 'accumulated_eval_time': 578.4967081546783, 'accumulated_logging_time': 0.08995842933654785}
I0307 17:13:56.739444 140209753155328 logging_writer.py:48] [5722] accumulated_eval_time=578.497, accumulated_logging_time=0.0899584, accumulated_submission_time=1212.2, global_step=5722, preemption_count=0, score=1212.2, test/accuracy=0.984027, test/loss=0.0570068, test/mean_average_precision=0.133456, test/num_examples=43793, total_duration=1790.94, train/accuracy=0.987772, train/loss=0.0434526, train/mean_average_precision=0.148232, validation/accuracy=0.984967, validation/loss=0.0540345, validation/mean_average_precision=0.136946, validation/num_examples=43793
I0307 17:14:13.049750 140209761548032 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.03623287379741669, loss=0.04050609841942787
I0307 17:14:33.644514 140209753155328 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.09950675815343857, loss=0.052629102021455765
I0307 17:14:54.572752 140209761548032 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.022199800238013268, loss=0.0427505262196064
I0307 17:15:15.284802 140209753155328 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.03190697729587555, loss=0.046948812901973724
I0307 17:15:36.091685 140209761548032 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0999116450548172, loss=0.038701124489307404
I0307 17:15:56.881816 140209753155328 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.04468094930052757, loss=0.04172108694911003
I0307 17:16:17.486288 140209761548032 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.025923701003193855, loss=0.04379840940237045
I0307 17:16:38.315311 140209753155328 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.055289216339588165, loss=0.044455260038375854
I0307 17:16:59.226780 140209761548032 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.05784459039568901, loss=0.04026564210653305
I0307 17:17:19.914888 140209753155328 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.06244833767414093, loss=0.04190998524427414
I0307 17:17:40.445337 140209761548032 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0415387898683548, loss=0.04121944308280945
I0307 17:17:56.734251 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:19:07.128561 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:19:09.016136 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:19:10.874170 140351869203648 submission_runner.py:469] Time since start: 2105.08s, 	Step: 6879, 	{'train/accuracy': 0.9878882169723511, 'train/loss': 0.04271654039621353, 'train/mean_average_precision': 0.14986085118311837, 'validation/accuracy': 0.9851145148277283, 'validation/loss': 0.05216933414340019, 'validation/mean_average_precision': 0.13985625315307748, 'validation/num_examples': 43793, 'test/accuracy': 0.9842308759689331, 'test/loss': 0.054933276027441025, 'test/mean_average_precision': 0.1428439539342359, 'test/num_examples': 43793, 'score': 1452.1587362289429, 'total_duration': 2105.0814514160156, 'accumulated_submission_time': 1452.1587362289429, 'accumulated_eval_time': 652.6365842819214, 'accumulated_logging_time': 0.10740351676940918}
I0307 17:19:10.883177 140209753155328 logging_writer.py:48] [6879] accumulated_eval_time=652.637, accumulated_logging_time=0.107404, accumulated_submission_time=1452.16, global_step=6879, preemption_count=0, score=1452.16, test/accuracy=0.984231, test/loss=0.0549333, test/mean_average_precision=0.142844, test/num_examples=43793, total_duration=2105.08, train/accuracy=0.987888, train/loss=0.0427165, train/mean_average_precision=0.149861, validation/accuracy=0.985115, validation/loss=0.0521693, validation/mean_average_precision=0.139856, validation/num_examples=43793
I0307 17:19:15.447336 140209761548032 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.019353223964571953, loss=0.04053499177098274
I0307 17:19:36.022002 140209753155328 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.08214495331048965, loss=0.041189707815647125
I0307 17:19:56.537011 140209761548032 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.08480597287416458, loss=0.04467872157692909
I0307 17:20:17.115682 140209753155328 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.026870686560869217, loss=0.04341818392276764
I0307 17:20:37.785369 140209761548032 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03399261087179184, loss=0.04225219413638115
I0307 17:20:58.427157 140209753155328 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.08633973449468613, loss=0.04497424513101578
I0307 17:21:18.915335 140209761548032 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.03912699222564697, loss=0.04100619629025459
I0307 17:21:39.537479 140209753155328 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.036402009427547455, loss=0.044963616877794266
I0307 17:22:00.171772 140209761548032 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03714371100068092, loss=0.04508799687027931
I0307 17:22:20.895894 140209753155328 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.06933368742465973, loss=0.037640348076820374
I0307 17:22:41.450174 140209761548032 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.04247114807367325, loss=0.05126482993364334
I0307 17:23:01.937167 140209753155328 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03830815851688385, loss=0.0425664484500885
I0307 17:23:11.027856 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:24:22.211909 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:24:24.094389 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:24:25.937728 140351869203648 submission_runner.py:469] Time since start: 2420.14s, 	Step: 8045, 	{'train/accuracy': 0.9882065653800964, 'train/loss': 0.041298069059848785, 'train/mean_average_precision': 0.17088348205827078, 'validation/accuracy': 0.9853150844573975, 'validation/loss': 0.05207788571715355, 'validation/mean_average_precision': 0.14811786168059773, 'validation/num_examples': 43793, 'test/accuracy': 0.9843605756759644, 'test/loss': 0.05522608384490013, 'test/mean_average_precision': 0.14924636050688517, 'test/num_examples': 43793, 'score': 1692.2665588855743, 'total_duration': 2420.1448788642883, 'accumulated_submission_time': 1692.2665588855743, 'accumulated_eval_time': 727.546293258667, 'accumulated_logging_time': 0.12540745735168457}
I0307 17:24:25.947694 140209761548032 logging_writer.py:48] [8045] accumulated_eval_time=727.546, accumulated_logging_time=0.125407, accumulated_submission_time=1692.27, global_step=8045, preemption_count=0, score=1692.27, test/accuracy=0.984361, test/loss=0.0552261, test/mean_average_precision=0.149246, test/num_examples=43793, total_duration=2420.14, train/accuracy=0.988207, train/loss=0.0412981, train/mean_average_precision=0.170883, validation/accuracy=0.985315, validation/loss=0.0520779, validation/mean_average_precision=0.148118, validation/num_examples=43793
I0307 17:24:37.408316 140209753155328 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.06367916613817215, loss=0.0403275303542614
I0307 17:24:57.921805 140209761548032 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.027166781947016716, loss=0.04254448786377907
I0307 17:25:18.603117 140209753155328 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.07501690089702606, loss=0.04494990408420563
I0307 17:25:39.191580 140209761548032 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.05568212643265724, loss=0.03869900852441788
I0307 17:25:59.846021 140209753155328 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.05343962833285332, loss=0.04031706973910332
I0307 17:26:20.467216 140209761548032 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.12252097576856613, loss=0.03771001845598221
I0307 17:26:40.982754 140209753155328 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.09633656591176987, loss=0.04187578707933426
I0307 17:27:01.410728 140209761548032 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.08542729914188385, loss=0.04103410243988037
I0307 17:27:21.951054 140209753155328 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.037586476653814316, loss=0.04387800022959709
I0307 17:27:42.622158 140209761548032 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.05756925046443939, loss=0.0369553305208683
I0307 17:28:03.283467 140209753155328 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.023978445678949356, loss=0.0403272844851017
I0307 17:28:23.798096 140209761548032 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.033183906227350235, loss=0.04123252257704735
I0307 17:28:26.073266 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:29:34.713844 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:29:36.649829 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:29:38.517076 140351869203648 submission_runner.py:469] Time since start: 2732.72s, 	Step: 9212, 	{'train/accuracy': 0.9882962107658386, 'train/loss': 0.04099811613559723, 'train/mean_average_precision': 0.1733771070231122, 'validation/accuracy': 0.9853734970092773, 'validation/loss': 0.05058133229613304, 'validation/mean_average_precision': 0.15342083824876565, 'validation/num_examples': 43793, 'test/accuracy': 0.9845025539398193, 'test/loss': 0.053506091237068176, 'test/mean_average_precision': 0.1539502200776052, 'test/num_examples': 43793, 'score': 1932.355211019516, 'total_duration': 2732.724241733551, 'accumulated_submission_time': 1932.355211019516, 'accumulated_eval_time': 799.9899609088898, 'accumulated_logging_time': 0.14403653144836426}
I0307 17:29:38.526334 140209753155328 logging_writer.py:48] [9212] accumulated_eval_time=799.99, accumulated_logging_time=0.144037, accumulated_submission_time=1932.36, global_step=9212, preemption_count=0, score=1932.36, test/accuracy=0.984503, test/loss=0.0535061, test/mean_average_precision=0.15395, test/num_examples=43793, total_duration=2732.72, train/accuracy=0.988296, train/loss=0.0409981, train/mean_average_precision=0.173377, validation/accuracy=0.985373, validation/loss=0.0505813, validation/mean_average_precision=0.153421, validation/num_examples=43793
I0307 17:29:56.978996 140209761548032 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.05443119257688522, loss=0.04325830936431885
I0307 17:30:17.581031 140209753155328 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.048285968601703644, loss=0.03987427055835724
I0307 17:30:38.105762 140209761548032 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.039291150867938995, loss=0.04370005428791046
I0307 17:30:58.589897 140209753155328 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.044703513383865356, loss=0.03935304284095764
I0307 17:31:18.973796 140209761548032 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1350412517786026, loss=0.04094519466161728
I0307 17:31:39.518792 140209753155328 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.02706051804125309, loss=0.03865973651409149
I0307 17:32:00.075864 140209761548032 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.057927608489990234, loss=0.04081200435757637
I0307 17:32:20.660184 140209753155328 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.09815449267625809, loss=0.04267772659659386
I0307 17:32:41.346801 140209761548032 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.03829991817474365, loss=0.04891384392976761
I0307 17:33:02.037577 140209753155328 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.055652718991041183, loss=0.040160708129405975
I0307 17:33:22.732806 140209761548032 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.04645358398556709, loss=0.037347424775362015
I0307 17:33:38.519144 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:34:50.400528 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:34:52.298844 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:34:54.114595 140351869203648 submission_runner.py:469] Time since start: 3048.32s, 	Step: 10377, 	{'train/accuracy': 0.9884138107299805, 'train/loss': 0.04086827486753464, 'train/mean_average_precision': 0.17238742067007534, 'validation/accuracy': 0.9853199124336243, 'validation/loss': 0.05074239522218704, 'validation/mean_average_precision': 0.15440124586248596, 'validation/num_examples': 43793, 'test/accuracy': 0.984449028968811, 'test/loss': 0.0533616729080677, 'test/mean_average_precision': 0.15328489942817614, 'test/num_examples': 43793, 'score': 2172.311248064041, 'total_duration': 3048.321879386902, 'accumulated_submission_time': 2172.311248064041, 'accumulated_eval_time': 875.5853695869446, 'accumulated_logging_time': 0.16205096244812012}
I0307 17:34:54.123433 140209753155328 logging_writer.py:48] [10377] accumulated_eval_time=875.585, accumulated_logging_time=0.162051, accumulated_submission_time=2172.31, global_step=10377, preemption_count=0, score=2172.31, test/accuracy=0.984449, test/loss=0.0533617, test/mean_average_precision=0.153285, test/num_examples=43793, total_duration=3048.32, train/accuracy=0.988414, train/loss=0.0408683, train/mean_average_precision=0.172387, validation/accuracy=0.98532, validation/loss=0.0507424, validation/mean_average_precision=0.154401, validation/num_examples=43793
I0307 17:34:59.073542 140209761548032 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.034774426370859146, loss=0.04291818290948868
I0307 17:35:19.695892 140209753155328 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.045329511165618896, loss=0.03696977347135544
I0307 17:35:40.216984 140209761548032 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.026671670377254486, loss=0.03683938458561897
I0307 17:36:00.826696 140209753155328 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.04782193526625633, loss=0.04445625841617584
I0307 17:36:21.381635 140209761548032 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.057198066264390945, loss=0.042900048196315765
I0307 17:36:41.903158 140209753155328 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.04846276715397835, loss=0.03915058821439743
I0307 17:37:02.501147 140209761548032 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.02920311689376831, loss=0.04282889887690544
I0307 17:37:23.028493 140209753155328 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03180839121341705, loss=0.03906737640500069
I0307 17:37:43.771198 140209761548032 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.11507491022348404, loss=0.04095619544386864
I0307 17:38:04.429755 140209753155328 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.07498420774936676, loss=0.03892830014228821
I0307 17:38:24.963950 140209761548032 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.06367267668247223, loss=0.042494311928749084
I0307 17:38:45.393223 140209753155328 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.10523663461208344, loss=0.03834030032157898
I0307 17:38:54.237195 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:40:05.520701 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:40:07.403393 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:40:09.250391 140351869203648 submission_runner.py:469] Time since start: 3363.46s, 	Step: 11544, 	{'train/accuracy': 0.9884788990020752, 'train/loss': 0.04015427082777023, 'train/mean_average_precision': 0.19291017048389753, 'validation/accuracy': 0.9854737520217896, 'validation/loss': 0.04974060133099556, 'validation/mean_average_precision': 0.16543146904235276, 'validation/num_examples': 43793, 'test/accuracy': 0.9845825433731079, 'test/loss': 0.05253992974758148, 'test/mean_average_precision': 0.16440582742435628, 'test/num_examples': 43793, 'score': 2412.3876707553864, 'total_duration': 3363.4575414657593, 'accumulated_submission_time': 2412.3876707553864, 'accumulated_eval_time': 950.5983846187592, 'accumulated_logging_time': 0.18010711669921875}
I0307 17:40:09.259472 140209761548032 logging_writer.py:48] [11544] accumulated_eval_time=950.598, accumulated_logging_time=0.180107, accumulated_submission_time=2412.39, global_step=11544, preemption_count=0, score=2412.39, test/accuracy=0.984583, test/loss=0.0525399, test/mean_average_precision=0.164406, test/num_examples=43793, total_duration=3363.46, train/accuracy=0.988479, train/loss=0.0401543, train/mean_average_precision=0.19291, validation/accuracy=0.985474, validation/loss=0.0497406, validation/mean_average_precision=0.165431, validation/num_examples=43793
I0307 17:40:20.821997 140209753155328 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.027695106342434883, loss=0.04190005362033844
I0307 17:40:41.404083 140209761548032 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.058074701577425, loss=0.04465712979435921
I0307 17:41:02.236458 140209753155328 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.10088912397623062, loss=0.03983389586210251
I0307 17:41:22.857088 140209761548032 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.03269747644662857, loss=0.03747664391994476
I0307 17:41:43.599043 140209753155328 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.032448235899209976, loss=0.0395178459584713
I0307 17:42:04.322394 140209761548032 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03541744872927666, loss=0.036569129675626755
I0307 17:42:25.099293 140209753155328 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.07775938510894775, loss=0.04283621907234192
I0307 17:42:45.806777 140209761548032 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.028107503429055214, loss=0.04453500732779503
I0307 17:43:06.620617 140209753155328 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.047019630670547485, loss=0.0431043803691864
I0307 17:43:27.115302 140209761548032 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.031094206497073174, loss=0.039481133222579956
I0307 17:43:47.755763 140209753155328 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.02575736492872238, loss=0.036864712834358215
I0307 17:44:08.330883 140209761548032 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.07221443951129913, loss=0.03848503902554512
I0307 17:44:09.364850 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:45:17.779216 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:45:19.676556 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:45:21.554108 140351869203648 submission_runner.py:469] Time since start: 3675.76s, 	Step: 12706, 	{'train/accuracy': 0.9884593486785889, 'train/loss': 0.03976178914308548, 'train/mean_average_precision': 0.18501394544647362, 'validation/accuracy': 0.9854717254638672, 'validation/loss': 0.04980354756116867, 'validation/mean_average_precision': 0.16518487080471425, 'validation/num_examples': 43793, 'test/accuracy': 0.9845328330993652, 'test/loss': 0.0525357611477375, 'test/mean_average_precision': 0.16741551064468646, 'test/num_examples': 43793, 'score': 2652.454883813858, 'total_duration': 3675.7612776756287, 'accumulated_submission_time': 2652.454883813858, 'accumulated_eval_time': 1022.7874805927277, 'accumulated_logging_time': 0.1981043815612793}
I0307 17:45:21.563451 140209753155328 logging_writer.py:48] [12706] accumulated_eval_time=1022.79, accumulated_logging_time=0.198104, accumulated_submission_time=2652.45, global_step=12706, preemption_count=0, score=2652.45, test/accuracy=0.984533, test/loss=0.0525358, test/mean_average_precision=0.167416, test/num_examples=43793, total_duration=3675.76, train/accuracy=0.988459, train/loss=0.0397618, train/mean_average_precision=0.185014, validation/accuracy=0.985472, validation/loss=0.0498035, validation/mean_average_precision=0.165185, validation/num_examples=43793
I0307 17:45:41.255747 140209761548032 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.037407878786325455, loss=0.04444176331162453
I0307 17:46:01.888729 140209753155328 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.09979337453842163, loss=0.03742830082774162
I0307 17:46:22.630808 140209761548032 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.09376147389411926, loss=0.041518040001392365
I0307 17:46:43.249010 140209753155328 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.037918634712696075, loss=0.034822579473257065
I0307 17:47:03.881758 140209761548032 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.032817061990499496, loss=0.041226230561733246
I0307 17:47:24.411137 140209753155328 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.06367255002260208, loss=0.04356126859784126
I0307 17:47:45.032608 140209761548032 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.059061262756586075, loss=0.035599470138549805
I0307 17:48:05.581770 140209753155328 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0482616052031517, loss=0.04168935865163803
I0307 17:48:26.203987 140209761548032 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.04331696406006813, loss=0.04166387394070625
I0307 17:48:46.889658 140209753155328 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.05312202498316765, loss=0.04193892702460289
I0307 17:49:07.532934 140209761548032 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.02451350726187229, loss=0.037421081215143204
I0307 17:49:21.644115 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:50:31.634558 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:50:33.521553 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:50:35.355467 140351869203648 submission_runner.py:469] Time since start: 3989.56s, 	Step: 13869, 	{'train/accuracy': 0.9884340167045593, 'train/loss': 0.04030493274331093, 'train/mean_average_precision': 0.18717147998983497, 'validation/accuracy': 0.9855358600616455, 'validation/loss': 0.04973352327942848, 'validation/mean_average_precision': 0.16017044762353935, 'validation/num_examples': 43793, 'test/accuracy': 0.9846495389938354, 'test/loss': 0.05236131697893143, 'test/mean_average_precision': 0.15937123704930567, 'test/num_examples': 43793, 'score': 2892.4982256889343, 'total_duration': 3989.5626220703125, 'accumulated_submission_time': 2892.4982256889343, 'accumulated_eval_time': 1096.4986581802368, 'accumulated_logging_time': 0.21637892723083496}
I0307 17:50:35.365037 140209753155328 logging_writer.py:48] [13869] accumulated_eval_time=1096.5, accumulated_logging_time=0.216379, accumulated_submission_time=2892.5, global_step=13869, preemption_count=0, score=2892.5, test/accuracy=0.98465, test/loss=0.0523613, test/mean_average_precision=0.159371, test/num_examples=43793, total_duration=3989.56, train/accuracy=0.988434, train/loss=0.0403049, train/mean_average_precision=0.187171, validation/accuracy=0.985536, validation/loss=0.0497335, validation/mean_average_precision=0.16017, validation/num_examples=43793
I0307 17:50:41.975288 140209761548032 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.07494104653596878, loss=0.04203468933701515
I0307 17:51:02.523725 140209753155328 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.08874831348657608, loss=0.04283967241644859
I0307 17:51:23.052687 140209761548032 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.04897260293364525, loss=0.04217664524912834
I0307 17:51:43.599438 140209753155328 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03504836559295654, loss=0.04152035713195801
I0307 17:52:04.246883 140209761548032 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.046350058168172836, loss=0.04381334409117699
I0307 17:52:24.989901 140209753155328 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.027688436210155487, loss=0.035536203533411026
I0307 17:52:45.718347 140209761548032 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.08146210014820099, loss=0.03893667459487915
I0307 17:53:06.591751 140209753155328 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.028662610799074173, loss=0.03773900866508484
I0307 17:53:27.546406 140209761548032 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.07834773510694504, loss=0.036212578415870667
I0307 17:53:48.219524 140209753155328 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0391947366297245, loss=0.04393894970417023
I0307 17:54:09.044824 140209761548032 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.066929392516613, loss=0.04122025519609451
I0307 17:54:29.764159 140209753155328 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03109714575111866, loss=0.03832479938864708
I0307 17:54:35.530314 140351869203648 spec.py:321] Evaluating on the training split.
I0307 17:55:47.320734 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 17:55:49.213103 140351869203648 spec.py:349] Evaluating on the test split.
I0307 17:55:51.071381 140351869203648 submission_runner.py:469] Time since start: 4305.28s, 	Step: 15029, 	{'train/accuracy': 0.9886447787284851, 'train/loss': 0.03944523632526398, 'train/mean_average_precision': 0.19421323151798875, 'validation/accuracy': 0.985591471195221, 'validation/loss': 0.05005280300974846, 'validation/mean_average_precision': 0.17215742046903945, 'validation/num_examples': 43793, 'test/accuracy': 0.9846053123474121, 'test/loss': 0.052922144532203674, 'test/mean_average_precision': 0.17317978512399076, 'test/num_examples': 43793, 'score': 3132.62256360054, 'total_duration': 4305.278651237488, 'accumulated_submission_time': 3132.62256360054, 'accumulated_eval_time': 1172.0396654605865, 'accumulated_logging_time': 0.23593854904174805}
I0307 17:55:51.081345 140209761548032 logging_writer.py:48] [15029] accumulated_eval_time=1172.04, accumulated_logging_time=0.235939, accumulated_submission_time=3132.62, global_step=15029, preemption_count=0, score=3132.62, test/accuracy=0.984605, test/loss=0.0529221, test/mean_average_precision=0.17318, test/num_examples=43793, total_duration=4305.28, train/accuracy=0.988645, train/loss=0.0394452, train/mean_average_precision=0.194213, validation/accuracy=0.985591, validation/loss=0.0500528, validation/mean_average_precision=0.172157, validation/num_examples=43793
I0307 17:56:05.821684 140209753155328 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.03395433723926544, loss=0.03711456060409546
I0307 17:56:26.375305 140209761548032 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.07885915040969849, loss=0.04305858910083771
I0307 17:56:46.840821 140209753155328 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.03131464868783951, loss=0.03998000919818878
I0307 17:57:07.483325 140209761548032 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.11366932839155197, loss=0.04295005649328232
I0307 17:57:28.201327 140209753155328 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.03161245957016945, loss=0.034587156027555466
I0307 17:57:49.056437 140209761548032 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.059684328734874725, loss=0.0401887483894825
I0307 17:58:09.835578 140209753155328 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0776495635509491, loss=0.041470274329185486
I0307 17:58:30.770899 140209761548032 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.07030414789915085, loss=0.044277340173721313
I0307 17:58:51.662294 140209753155328 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.12092404812574387, loss=0.04025459662079811
I0307 17:59:12.289791 140209761548032 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.13362766802310944, loss=0.04066895321011543
I0307 17:59:32.788800 140209753155328 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.042255278676748276, loss=0.039301879703998566
I0307 17:59:51.101134 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:01:01.370651 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:01:03.267576 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:01:05.123253 140351869203648 submission_runner.py:469] Time since start: 4619.33s, 	Step: 16190, 	{'train/accuracy': 0.9886651635169983, 'train/loss': 0.03922230005264282, 'train/mean_average_precision': 0.19650189395122658, 'validation/accuracy': 0.985602855682373, 'validation/loss': 0.04910341650247574, 'validation/mean_average_precision': 0.17631554403992677, 'validation/num_examples': 43793, 'test/accuracy': 0.9847005009651184, 'test/loss': 0.05187113955616951, 'test/mean_average_precision': 0.1722503737343848, 'test/num_examples': 43793, 'score': 3372.6036059856415, 'total_duration': 4619.330516576767, 'accumulated_submission_time': 3372.6036059856415, 'accumulated_eval_time': 1246.0617234706879, 'accumulated_logging_time': 0.2565314769744873}
I0307 18:01:05.135121 140209761548032 logging_writer.py:48] [16190] accumulated_eval_time=1246.06, accumulated_logging_time=0.256531, accumulated_submission_time=3372.6, global_step=16190, preemption_count=0, score=3372.6, test/accuracy=0.984701, test/loss=0.0518711, test/mean_average_precision=0.17225, test/num_examples=43793, total_duration=4619.33, train/accuracy=0.988665, train/loss=0.0392223, train/mean_average_precision=0.196502, validation/accuracy=0.985603, validation/loss=0.0491034, validation/mean_average_precision=0.176316, validation/num_examples=43793
I0307 18:01:07.433525 140209753155328 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.07081180810928345, loss=0.04331720247864723
I0307 18:01:28.100564 140209761548032 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.028982039541006088, loss=0.03765080124139786
I0307 18:01:48.691314 140209753155328 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.028300289064645767, loss=0.03846140205860138
I0307 18:02:09.333104 140209761548032 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.05624939128756523, loss=0.03939814493060112
I0307 18:02:30.073660 140209753155328 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.059264522045850754, loss=0.03333405777812004
I0307 18:02:50.721885 140209761548032 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.042254488915205, loss=0.036207035183906555
I0307 18:03:11.199316 140209753155328 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.07850883901119232, loss=0.04288428649306297
I0307 18:03:31.940386 140209761548032 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.0849715918302536, loss=0.03531361371278763
I0307 18:03:52.883691 140209753155328 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.025358712300658226, loss=0.03992658108472824
I0307 18:04:13.655339 140209761548032 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04409702494740486, loss=0.03841029852628708
I0307 18:04:34.332439 140209753155328 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.043223533779382706, loss=0.04036210849881172
I0307 18:04:55.044109 140209761548032 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.08617746829986572, loss=0.0372830294072628
I0307 18:05:05.201594 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:06:14.845352 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:06:16.746749 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:06:18.587765 140351869203648 submission_runner.py:469] Time since start: 4932.79s, 	Step: 17350, 	{'train/accuracy': 0.988707423210144, 'train/loss': 0.039025574922561646, 'train/mean_average_precision': 0.20166152038629817, 'validation/accuracy': 0.9855687618255615, 'validation/loss': 0.04920481890439987, 'validation/mean_average_precision': 0.175239581794696, 'validation/num_examples': 43793, 'test/accuracy': 0.984625518321991, 'test/loss': 0.05206214264035225, 'test/mean_average_precision': 0.17419328626335087, 'test/num_examples': 43793, 'score': 3612.6325249671936, 'total_duration': 4932.7949895858765, 'accumulated_submission_time': 3612.6325249671936, 'accumulated_eval_time': 1319.4477875232697, 'accumulated_logging_time': 0.27706313133239746}
I0307 18:06:18.597749 140209753155328 logging_writer.py:48] [17350] accumulated_eval_time=1319.45, accumulated_logging_time=0.277063, accumulated_submission_time=3612.63, global_step=17350, preemption_count=0, score=3612.63, test/accuracy=0.984626, test/loss=0.0520621, test/mean_average_precision=0.174193, test/num_examples=43793, total_duration=4932.79, train/accuracy=0.988707, train/loss=0.0390256, train/mean_average_precision=0.201662, validation/accuracy=0.985569, validation/loss=0.0492048, validation/mean_average_precision=0.17524, validation/num_examples=43793
I0307 18:06:29.062768 140209761548032 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.06601541489362717, loss=0.04337901994585991
I0307 18:06:49.680500 140209753155328 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.038666464388370514, loss=0.03344358503818512
I0307 18:07:10.574999 140209761548032 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.03331836685538292, loss=0.04158896207809448
I0307 18:07:31.018839 140209753155328 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.06792762130498886, loss=0.04039434716105461
I0307 18:07:51.647810 140209761548032 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.06111185625195503, loss=0.038272738456726074
I0307 18:08:12.501079 140209753155328 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.030436217784881592, loss=0.0399334616959095
I0307 18:08:33.167288 140209761548032 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.12366172671318054, loss=0.03860526904463768
I0307 18:08:54.062826 140209753155328 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.14277349412441254, loss=0.04220735654234886
I0307 18:09:14.710267 140209761548032 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.046928152441978455, loss=0.041110388934612274
I0307 18:09:35.431583 140209753155328 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.059489332139492035, loss=0.03820682689547539
I0307 18:09:56.242861 140209761548032 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.04640042781829834, loss=0.03906949236989021
I0307 18:10:16.978316 140209753155328 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.06428682059049606, loss=0.03808911517262459
I0307 18:10:18.622400 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:11:29.889264 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:11:31.794935 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:11:33.660211 140351869203648 submission_runner.py:469] Time since start: 5247.87s, 	Step: 18509, 	{'train/accuracy': 0.9885414838790894, 'train/loss': 0.039603255689144135, 'train/mean_average_precision': 0.2001174406660332, 'validation/accuracy': 0.9854839444160461, 'validation/loss': 0.04976131021976471, 'validation/mean_average_precision': 0.17054034717041244, 'validation/num_examples': 43793, 'test/accuracy': 0.9846280217170715, 'test/loss': 0.05254702642560005, 'test/mean_average_precision': 0.17498074070585454, 'test/num_examples': 43793, 'score': 3852.619345664978, 'total_duration': 5247.867392778397, 'accumulated_submission_time': 3852.619345664978, 'accumulated_eval_time': 1394.4854485988617, 'accumulated_logging_time': 0.2968254089355469}
I0307 18:11:33.670249 140209761548032 logging_writer.py:48] [18509] accumulated_eval_time=1394.49, accumulated_logging_time=0.296825, accumulated_submission_time=3852.62, global_step=18509, preemption_count=0, score=3852.62, test/accuracy=0.984628, test/loss=0.052547, test/mean_average_precision=0.174981, test/num_examples=43793, total_duration=5247.87, train/accuracy=0.988541, train/loss=0.0396033, train/mean_average_precision=0.200117, validation/accuracy=0.985484, validation/loss=0.0497613, validation/mean_average_precision=0.17054, validation/num_examples=43793
I0307 18:11:52.749284 140209753155328 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.037970323115587234, loss=0.037801384925842285
I0307 18:12:13.376624 140209761548032 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.09208512306213379, loss=0.0424996018409729
I0307 18:12:34.249374 140209753155328 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.04958149790763855, loss=0.04185625910758972
I0307 18:12:55.044282 140209761548032 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.04455520585179329, loss=0.03674203157424927
I0307 18:13:15.734208 140209753155328 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.07179951667785645, loss=0.04513462260365486
I0307 18:13:36.559329 140209761548032 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.03967548534274101, loss=0.03487864136695862
I0307 18:13:57.548289 140209753155328 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0932631716132164, loss=0.041814688593149185
I0307 18:14:18.316620 140209761548032 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.05778223276138306, loss=0.04119648039340973
I0307 18:14:39.113801 140209753155328 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.09982679784297943, loss=0.0364530012011528
I0307 18:14:59.997674 140209761548032 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.04537782818078995, loss=0.03610850125551224
I0307 18:15:20.598760 140209753155328 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.03875849023461342, loss=0.039946358650922775
I0307 18:15:33.667426 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:16:44.236558 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:16:46.126629 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:16:47.990095 140351869203648 submission_runner.py:469] Time since start: 5562.20s, 	Step: 19664, 	{'train/accuracy': 0.988696813583374, 'train/loss': 0.03887319564819336, 'train/mean_average_precision': 0.20316929639262585, 'validation/accuracy': 0.9856483340263367, 'validation/loss': 0.04885517805814743, 'validation/mean_average_precision': 0.17416494030168003, 'validation/num_examples': 43793, 'test/accuracy': 0.9847674369812012, 'test/loss': 0.05152552202343941, 'test/mean_average_precision': 0.1710801320322232, 'test/num_examples': 43793, 'score': 4092.5756454467773, 'total_duration': 5562.197319507599, 'accumulated_submission_time': 4092.5756454467773, 'accumulated_eval_time': 1468.8080174922943, 'accumulated_logging_time': 0.31752586364746094}
I0307 18:16:48.001563 140209761548032 logging_writer.py:48] [19664] accumulated_eval_time=1468.81, accumulated_logging_time=0.317526, accumulated_submission_time=4092.58, global_step=19664, preemption_count=0, score=4092.58, test/accuracy=0.984767, test/loss=0.0515255, test/mean_average_precision=0.17108, test/num_examples=43793, total_duration=5562.2, train/accuracy=0.988697, train/loss=0.0388732, train/mean_average_precision=0.203169, validation/accuracy=0.985648, validation/loss=0.0488552, validation/mean_average_precision=0.174165, validation/num_examples=43793
I0307 18:16:55.716682 140209753155328 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.04686421528458595, loss=0.04030599817633629
I0307 18:17:16.420251 140209761548032 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.04713444039225578, loss=0.0356983058154583
I0307 18:17:37.282717 140209753155328 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.07912280410528183, loss=0.03867349401116371
I0307 18:17:58.047765 140209761548032 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.06589236855506897, loss=0.03641925007104874
I0307 18:18:18.957211 140209753155328 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.046551477164030075, loss=0.037040092051029205
I0307 18:18:39.706149 140209761548032 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.04274753853678703, loss=0.03861095383763313
I0307 18:19:00.347362 140209753155328 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.10352768748998642, loss=0.03991730138659477
I0307 18:19:21.184655 140209761548032 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.05694714933633804, loss=0.03626307472586632
I0307 18:19:41.789561 140209753155328 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.05340943858027458, loss=0.038807328790426254
I0307 18:20:02.441601 140209761548032 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.06794259697198868, loss=0.03557277098298073
I0307 18:20:23.102574 140209753155328 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.04014366865158081, loss=0.040039870887994766
I0307 18:20:43.809615 140209761548032 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.03733191266655922, loss=0.036320142447948456
I0307 18:20:48.138477 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:22:01.481838 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:22:03.384957 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:22:05.225903 140351869203648 submission_runner.py:469] Time since start: 5879.43s, 	Step: 20822, 	{'train/accuracy': 0.9888809323310852, 'train/loss': 0.038621727377176285, 'train/mean_average_precision': 0.20557050492761572, 'validation/accuracy': 0.9858106970787048, 'validation/loss': 0.04889046028256416, 'validation/mean_average_precision': 0.18067202379380873, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05161231756210327, 'test/mean_average_precision': 0.18131597758312887, 'test/num_examples': 43793, 'score': 4332.675221681595, 'total_duration': 5879.433055400848, 'accumulated_submission_time': 4332.675221681595, 'accumulated_eval_time': 1545.8952639102936, 'accumulated_logging_time': 0.33777904510498047}
I0307 18:22:05.236268 140209753155328 logging_writer.py:48] [20822] accumulated_eval_time=1545.9, accumulated_logging_time=0.337779, accumulated_submission_time=4332.68, global_step=20822, preemption_count=0, score=4332.68, test/accuracy=0.984913, test/loss=0.0516123, test/mean_average_precision=0.181316, test/num_examples=43793, total_duration=5879.43, train/accuracy=0.988881, train/loss=0.0386217, train/mean_average_precision=0.205571, validation/accuracy=0.985811, validation/loss=0.0488905, validation/mean_average_precision=0.180672, validation/num_examples=43793
I0307 18:22:21.538003 140209761548032 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.04565334692597389, loss=0.039706166833639145
I0307 18:22:42.308106 140209753155328 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.05377354472875595, loss=0.03971489518880844
I0307 18:23:03.087670 140209761548032 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.07121410220861435, loss=0.03791731223464012
I0307 18:23:23.752159 140209753155328 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.03196321055293083, loss=0.03742741048336029
I0307 18:23:44.381119 140209761548032 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.08356241881847382, loss=0.03603601083159447
I0307 18:24:05.126173 140209753155328 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.07640457898378372, loss=0.03798230364918709
I0307 18:24:25.990271 140209761548032 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.10995125025510788, loss=0.03852544724941254
I0307 18:24:46.847786 140209753155328 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.04277392476797104, loss=0.03623049333691597
I0307 18:25:07.588503 140209761548032 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.06861868500709534, loss=0.04056499898433685
I0307 18:25:28.260926 140209753155328 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.07906445860862732, loss=0.03754471242427826
I0307 18:25:49.064815 140209761548032 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.0524139367043972, loss=0.03853679448366165
I0307 18:26:05.411403 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:27:15.455442 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:27:17.323667 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:27:19.161333 140351869203648 submission_runner.py:469] Time since start: 6193.37s, 	Step: 21979, 	{'train/accuracy': 0.988983690738678, 'train/loss': 0.03811206296086311, 'train/mean_average_precision': 0.22067338071597997, 'validation/accuracy': 0.9858646988868713, 'validation/loss': 0.04842767491936684, 'validation/mean_average_precision': 0.1831553759296441, 'validation/num_examples': 43793, 'test/accuracy': 0.9848554730415344, 'test/loss': 0.0510706901550293, 'test/mean_average_precision': 0.18809981352745814, 'test/num_examples': 43793, 'score': 4572.812266111374, 'total_duration': 6193.368548870087, 'accumulated_submission_time': 4572.812266111374, 'accumulated_eval_time': 1619.6450974941254, 'accumulated_logging_time': 0.3579246997833252}
I0307 18:27:19.171843 140209753155328 logging_writer.py:48] [21979] accumulated_eval_time=1619.65, accumulated_logging_time=0.357925, accumulated_submission_time=4572.81, global_step=21979, preemption_count=0, score=4572.81, test/accuracy=0.984855, test/loss=0.0510707, test/mean_average_precision=0.1881, test/num_examples=43793, total_duration=6193.37, train/accuracy=0.988984, train/loss=0.0381121, train/mean_average_precision=0.220673, validation/accuracy=0.985865, validation/loss=0.0484277, validation/mean_average_precision=0.183155, validation/num_examples=43793
I0307 18:27:23.733280 140209761548032 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.08060389012098312, loss=0.038912441581487656
I0307 18:27:44.368159 140209753155328 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.03883480653166771, loss=0.036817584186792374
I0307 18:28:05.078945 140209761548032 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.04363494738936424, loss=0.0383511558175087
I0307 18:28:25.904063 140209753155328 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.04404238238930702, loss=0.041077155619859695
I0307 18:28:46.750370 140209761548032 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.038896799087524414, loss=0.03722074255347252
I0307 18:29:07.506575 140209753155328 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.05611409619450569, loss=0.04405286908149719
I0307 18:29:28.245093 140209761548032 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.09675305336713791, loss=0.03949183225631714
I0307 18:29:48.944871 140209753155328 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.07574953138828278, loss=0.03981580585241318
I0307 18:30:09.783561 140209761548032 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.08339980244636536, loss=0.037643469870090485
I0307 18:30:30.472829 140209753155328 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.03898007050156593, loss=0.03894215449690819
I0307 18:30:51.274009 140209761548032 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.06565822660923004, loss=0.03648271784186363
I0307 18:31:11.866305 140209753155328 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.0677386000752449, loss=0.03864683955907822
I0307 18:31:19.339319 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:32:30.927123 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:32:32.812949 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:32:34.656278 140351869203648 submission_runner.py:469] Time since start: 6508.86s, 	Step: 23137, 	{'train/accuracy': 0.9888655543327332, 'train/loss': 0.038585416972637177, 'train/mean_average_precision': 0.21180715934351815, 'validation/accuracy': 0.9858070611953735, 'validation/loss': 0.04839330539107323, 'validation/mean_average_precision': 0.18108191048319328, 'validation/num_examples': 43793, 'test/accuracy': 0.9848892092704773, 'test/loss': 0.05119654908776283, 'test/mean_average_precision': 0.17652973770941913, 'test/num_examples': 43793, 'score': 4812.941241502762, 'total_duration': 6508.863561630249, 'accumulated_submission_time': 4812.941241502762, 'accumulated_eval_time': 1694.9620282649994, 'accumulated_logging_time': 0.3785266876220703}
I0307 18:32:34.667148 140209761548032 logging_writer.py:48] [23137] accumulated_eval_time=1694.96, accumulated_logging_time=0.378527, accumulated_submission_time=4812.94, global_step=23137, preemption_count=0, score=4812.94, test/accuracy=0.984889, test/loss=0.0511965, test/mean_average_precision=0.17653, test/num_examples=43793, total_duration=6508.86, train/accuracy=0.988866, train/loss=0.0385854, train/mean_average_precision=0.211807, validation/accuracy=0.985807, validation/loss=0.0483933, validation/mean_average_precision=0.181082, validation/num_examples=43793
I0307 18:32:47.914912 140209753155328 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.06049400195479393, loss=0.040166422724723816
I0307 18:33:08.484983 140209761548032 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.04679648205637932, loss=0.03966076672077179
I0307 18:33:29.073840 140209753155328 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.10852892696857452, loss=0.03786332905292511
I0307 18:33:49.731473 140209761548032 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.12768159806728363, loss=0.03827492147684097
I0307 18:34:10.458639 140209753155328 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.040921639651060104, loss=0.03685038909316063
I0307 18:34:31.235775 140209761548032 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.08380633592605591, loss=0.038216136395931244
I0307 18:34:51.809194 140209753155328 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.03867241367697716, loss=0.03858404606580734
I0307 18:35:12.375179 140209761548032 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.04811646789312363, loss=0.03696971759200096
I0307 18:35:33.020838 140209753155328 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.11533063650131226, loss=0.04032700136303902
I0307 18:35:53.746968 140209761548032 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.04656067118048668, loss=0.040751367807388306
I0307 18:36:14.385262 140209753155328 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.06857781857252121, loss=0.04225873202085495
I0307 18:36:34.711619 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:37:43.788871 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:37:45.696932 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:37:47.553835 140351869203648 submission_runner.py:469] Time since start: 6821.76s, 	Step: 24300, 	{'train/accuracy': 0.9888185262680054, 'train/loss': 0.038224633783102036, 'train/mean_average_precision': 0.21241110801793645, 'validation/accuracy': 0.9857120513916016, 'validation/loss': 0.04928415268659592, 'validation/mean_average_precision': 0.1868081023850515, 'validation/num_examples': 43793, 'test/accuracy': 0.9847948551177979, 'test/loss': 0.05222317948937416, 'test/mean_average_precision': 0.17864170464025428, 'test/num_examples': 43793, 'score': 5052.947694063187, 'total_duration': 6821.761011123657, 'accumulated_submission_time': 5052.947694063187, 'accumulated_eval_time': 1767.8041067123413, 'accumulated_logging_time': 0.39826226234436035}
I0307 18:37:47.564873 140209761548032 logging_writer.py:48] [24300] accumulated_eval_time=1767.8, accumulated_logging_time=0.398262, accumulated_submission_time=5052.95, global_step=24300, preemption_count=0, score=5052.95, test/accuracy=0.984795, test/loss=0.0522232, test/mean_average_precision=0.178642, test/num_examples=43793, total_duration=6821.76, train/accuracy=0.988819, train/loss=0.0382246, train/mean_average_precision=0.212411, validation/accuracy=0.985712, validation/loss=0.0492842, validation/mean_average_precision=0.186808, validation/num_examples=43793
I0307 18:37:47.778119 140209753155328 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.09276076406240463, loss=0.039228904992341995
I0307 18:38:08.515963 140209761548032 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.0593668557703495, loss=0.03922676295042038
I0307 18:38:29.074465 140209753155328 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.04945354163646698, loss=0.037404343485832214
I0307 18:38:49.736937 140209761548032 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.054040782153606415, loss=0.032832372933626175
I0307 18:39:10.387591 140209753155328 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.06831984966993332, loss=0.03772380203008652
I0307 18:39:31.137933 140209761548032 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.07381785660982132, loss=0.03690866380929947
I0307 18:39:51.984090 140209753155328 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.03079954721033573, loss=0.03623709827661514
I0307 18:40:12.686332 140209761548032 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.05774062126874924, loss=0.03486735373735428
I0307 18:40:33.287174 140209753155328 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.06851093471050262, loss=0.03908839821815491
I0307 18:40:54.132827 140209761548032 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.05352029949426651, loss=0.03867480531334877
I0307 18:41:14.926249 140209753155328 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.05126669630408287, loss=0.03754603862762451
I0307 18:41:35.561641 140209761548032 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.06525275111198425, loss=0.03803149610757828
I0307 18:41:47.590868 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:42:57.677706 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:42:59.586995 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:43:01.432320 140351869203648 submission_runner.py:469] Time since start: 7135.64s, 	Step: 25459, 	{'train/accuracy': 0.9888516068458557, 'train/loss': 0.03825634717941284, 'train/mean_average_precision': 0.22276084297089577, 'validation/accuracy': 0.9858630895614624, 'validation/loss': 0.04819132015109062, 'validation/mean_average_precision': 0.18829974770583538, 'validation/num_examples': 43793, 'test/accuracy': 0.9849330186843872, 'test/loss': 0.05104750767350197, 'test/mean_average_precision': 0.1837583811216438, 'test/num_examples': 43793, 'score': 5292.934512615204, 'total_duration': 7135.639479398727, 'accumulated_submission_time': 5292.934512615204, 'accumulated_eval_time': 1841.645385980606, 'accumulated_logging_time': 0.41911959648132324}
I0307 18:43:01.443626 140209753155328 logging_writer.py:48] [25459] accumulated_eval_time=1841.65, accumulated_logging_time=0.41912, accumulated_submission_time=5292.93, global_step=25459, preemption_count=0, score=5292.93, test/accuracy=0.984933, test/loss=0.0510475, test/mean_average_precision=0.183758, test/num_examples=43793, total_duration=7135.64, train/accuracy=0.988852, train/loss=0.0382563, train/mean_average_precision=0.222761, validation/accuracy=0.985863, validation/loss=0.0481913, validation/mean_average_precision=0.1883, validation/num_examples=43793
I0307 18:43:10.113203 140209761548032 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.06332847476005554, loss=0.03555025905370712
I0307 18:43:30.834042 140209753155328 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.08601433038711548, loss=0.0421479232609272
I0307 18:43:51.576052 140209761548032 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.06916172802448273, loss=0.03545158728957176
I0307 18:44:12.155185 140209753155328 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.05061504244804382, loss=0.04125996679067612
I0307 18:44:32.832828 140209761548032 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.12711091339588165, loss=0.03821481019258499
I0307 18:44:53.517142 140209753155328 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.047273725271224976, loss=0.03413549065589905
I0307 18:45:14.194685 140209761548032 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.04723687469959259, loss=0.039053115993738174
I0307 18:45:34.780381 140209753155328 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.06378043442964554, loss=0.03569665551185608
I0307 18:45:55.556673 140209761548032 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.07722368836402893, loss=0.04004743695259094
I0307 18:46:16.271812 140209753155328 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.05776818469166756, loss=0.037998445332050323
I0307 18:46:36.781374 140209761548032 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.053962428122758865, loss=0.039476778358221054
I0307 18:46:57.441773 140209753155328 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.058457206934690475, loss=0.03630447015166283
I0307 18:47:01.551864 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:48:10.613858 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:48:12.479164 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:48:14.321072 140351869203648 submission_runner.py:469] Time since start: 7448.53s, 	Step: 26621, 	{'train/accuracy': 0.9890813231468201, 'train/loss': 0.03750619664788246, 'train/mean_average_precision': 0.2346812202733523, 'validation/accuracy': 0.985808253288269, 'validation/loss': 0.048034921288490295, 'validation/mean_average_precision': 0.19128254877612466, 'validation/num_examples': 43793, 'test/accuracy': 0.9848365187644958, 'test/loss': 0.051006611436605453, 'test/mean_average_precision': 0.1851048637558814, 'test/num_examples': 43793, 'score': 5533.005326271057, 'total_duration': 7448.528254508972, 'accumulated_submission_time': 5533.005326271057, 'accumulated_eval_time': 1914.4144451618195, 'accumulated_logging_time': 0.4396181106567383}
I0307 18:48:14.332254 140209761548032 logging_writer.py:48] [26621] accumulated_eval_time=1914.41, accumulated_logging_time=0.439618, accumulated_submission_time=5533.01, global_step=26621, preemption_count=0, score=5533.01, test/accuracy=0.984837, test/loss=0.0510066, test/mean_average_precision=0.185105, test/num_examples=43793, total_duration=7448.53, train/accuracy=0.989081, train/loss=0.0375062, train/mean_average_precision=0.234681, validation/accuracy=0.985808, validation/loss=0.0480349, validation/mean_average_precision=0.191283, validation/num_examples=43793
I0307 18:48:31.016670 140209753155328 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.03789038211107254, loss=0.035380445420742035
I0307 18:48:51.746375 140209761548032 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.05353396013379097, loss=0.04044778272509575
I0307 18:49:12.663513 140209753155328 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.07775015383958817, loss=0.03915218636393547
I0307 18:49:33.521428 140209761548032 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.06346531212329865, loss=0.039871469140052795
I0307 18:49:54.275508 140209753155328 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.1253635287284851, loss=0.034415777772665024
I0307 18:50:15.467706 140209761548032 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.05623651295900345, loss=0.03802298754453659
I0307 18:50:36.306564 140209753155328 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.0881875529885292, loss=0.03699285164475441
I0307 18:50:57.227773 140209761548032 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.13273288309574127, loss=0.037401266396045685
I0307 18:51:18.004708 140209753155328 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.07513368874788284, loss=0.035433512181043625
I0307 18:51:38.852499 140209761548032 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.04502319544553757, loss=0.035979218780994415
I0307 18:51:59.715324 140209753155328 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.14020825922489166, loss=0.03645652160048485
I0307 18:52:14.411188 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:53:24.379587 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:53:26.272354 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:53:28.126407 140351869203648 submission_runner.py:469] Time since start: 7762.33s, 	Step: 27771, 	{'train/accuracy': 0.9891672134399414, 'train/loss': 0.0374494269490242, 'train/mean_average_precision': 0.22403331339853977, 'validation/accuracy': 0.9859174489974976, 'validation/loss': 0.04801275208592415, 'validation/mean_average_precision': 0.1881024160748155, 'validation/num_examples': 43793, 'test/accuracy': 0.9849502444267273, 'test/loss': 0.05093797296285629, 'test/mean_average_precision': 0.18914909470560715, 'test/num_examples': 43793, 'score': 5773.045235872269, 'total_duration': 7762.333683252335, 'accumulated_submission_time': 5773.045235872269, 'accumulated_eval_time': 1988.1296277046204, 'accumulated_logging_time': 0.4603147506713867}
I0307 18:53:28.137987 140209761548032 logging_writer.py:48] [27771] accumulated_eval_time=1988.13, accumulated_logging_time=0.460315, accumulated_submission_time=5773.05, global_step=27771, preemption_count=0, score=5773.05, test/accuracy=0.98495, test/loss=0.050938, test/mean_average_precision=0.189149, test/num_examples=43793, total_duration=7762.33, train/accuracy=0.989167, train/loss=0.0374494, train/mean_average_precision=0.224033, validation/accuracy=0.985917, validation/loss=0.0480128, validation/mean_average_precision=0.188102, validation/num_examples=43793
I0307 18:53:34.700317 140209753155328 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.11838283389806747, loss=0.039668601006269455
I0307 18:53:55.593561 140209761548032 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04788009077310562, loss=0.03846299648284912
I0307 18:54:16.515686 140209753155328 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.09527329355478287, loss=0.044335395097732544
I0307 18:54:37.423154 140209761548032 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.09231217950582504, loss=0.04292496666312218
I0307 18:54:58.436351 140209753155328 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.08696980029344559, loss=0.0412568598985672
I0307 18:55:19.401046 140209761548032 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.1130865141749382, loss=0.038813795894384384
I0307 18:55:40.577326 140209753155328 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.058230891823768616, loss=0.03979264572262764
I0307 18:56:01.592951 140209761548032 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.07309897243976593, loss=0.03736986592411995
I0307 18:56:22.478468 140209753155328 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.04772225767374039, loss=0.03895888477563858
I0307 18:56:43.506650 140209761548032 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.06160995364189148, loss=0.038004305213689804
I0307 18:57:04.426751 140209753155328 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.060736946761608124, loss=0.035838328301906586
I0307 18:57:25.290297 140209761548032 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.0504535436630249, loss=0.03894753009080887
I0307 18:57:28.271349 140351869203648 spec.py:321] Evaluating on the training split.
I0307 18:58:35.660631 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 18:58:37.552223 140351869203648 spec.py:349] Evaluating on the test split.
I0307 18:58:39.395013 140351869203648 submission_runner.py:469] Time since start: 8073.60s, 	Step: 28915, 	{'train/accuracy': 0.9893405437469482, 'train/loss': 0.036619193851947784, 'train/mean_average_precision': 0.23933924858268354, 'validation/accuracy': 0.9859998822212219, 'validation/loss': 0.047850728034973145, 'validation/mean_average_precision': 0.20013137241244977, 'validation/num_examples': 43793, 'test/accuracy': 0.9850930571556091, 'test/loss': 0.050854094326496124, 'test/mean_average_precision': 0.18676501947533358, 'test/num_examples': 43793, 'score': 6012.941381931305, 'total_duration': 8073.602168798447, 'accumulated_submission_time': 6012.941381931305, 'accumulated_eval_time': 2059.2531156539917, 'accumulated_logging_time': 0.6800205707550049}
I0307 18:58:39.407337 140209753155328 logging_writer.py:48] [28915] accumulated_eval_time=2059.25, accumulated_logging_time=0.680021, accumulated_submission_time=6012.94, global_step=28915, preemption_count=0, score=6012.94, test/accuracy=0.985093, test/loss=0.0508541, test/mean_average_precision=0.186765, test/num_examples=43793, total_duration=8073.6, train/accuracy=0.989341, train/loss=0.0366192, train/mean_average_precision=0.239339, validation/accuracy=0.986, validation/loss=0.0478507, validation/mean_average_precision=0.200131, validation/num_examples=43793
I0307 18:58:57.313273 140209761548032 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.07461729645729065, loss=0.03517282009124756
I0307 18:59:18.119421 140209753155328 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05956830829381943, loss=0.037191398441791534
I0307 18:59:39.042027 140209761548032 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.0647260919213295, loss=0.0360276885330677
I0307 19:00:00.101761 140209753155328 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.05155344679951668, loss=0.04043695703148842
I0307 19:00:20.978925 140209761548032 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.13201838731765747, loss=0.040547315031290054
I0307 19:00:41.998540 140209753155328 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.09074918925762177, loss=0.03939102217555046
I0307 19:01:03.031213 140209761548032 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.12914833426475525, loss=0.04150845482945442
I0307 19:01:23.917983 140209753155328 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.07645963132381439, loss=0.03752274438738823
I0307 19:01:44.616139 140209761548032 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.056442372500896454, loss=0.038848038762807846
I0307 19:02:05.272006 140209753155328 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.13867336511611938, loss=0.03713247552514076
I0307 19:02:26.102551 140209761548032 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.055160216987133026, loss=0.03437749668955803
I0307 19:02:39.580893 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:03:51.460948 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:03:53.388289 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:03:55.262040 140351869203648 submission_runner.py:469] Time since start: 8389.47s, 	Step: 30066, 	{'train/accuracy': 0.9892280101776123, 'train/loss': 0.037156715989112854, 'train/mean_average_precision': 0.23033424398207608, 'validation/accuracy': 0.985937774181366, 'validation/loss': 0.04770900681614876, 'validation/mean_average_precision': 0.1910839485901341, 'validation/num_examples': 43793, 'test/accuracy': 0.9849987030029297, 'test/loss': 0.0503774993121624, 'test/mean_average_precision': 0.1866270660368374, 'test/num_examples': 43793, 'score': 6253.078722000122, 'total_duration': 8389.469281435013, 'accumulated_submission_time': 6253.078722000122, 'accumulated_eval_time': 2134.9341747760773, 'accumulated_logging_time': 0.7013907432556152}
I0307 19:03:55.274454 140209753155328 logging_writer.py:48] [30066] accumulated_eval_time=2134.93, accumulated_logging_time=0.701391, accumulated_submission_time=6253.08, global_step=30066, preemption_count=0, score=6253.08, test/accuracy=0.984999, test/loss=0.0503775, test/mean_average_precision=0.186627, test/num_examples=43793, total_duration=8389.47, train/accuracy=0.989228, train/loss=0.0371567, train/mean_average_precision=0.230334, validation/accuracy=0.985938, validation/loss=0.047709, validation/mean_average_precision=0.191084, validation/num_examples=43793
I0307 19:04:02.649649 140209761548032 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.07299186289310455, loss=0.03928260877728462
I0307 19:04:23.680567 140209753155328 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.1388200968503952, loss=0.036382466554641724
I0307 19:04:44.515928 140209761548032 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.05145027115941048, loss=0.036552898585796356
I0307 19:05:05.590541 140209753155328 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.18127135932445526, loss=0.04328317940235138
I0307 19:05:26.473442 140209761548032 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.06298252195119858, loss=0.03655961528420448
I0307 19:05:47.436085 140209753155328 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.05949515476822853, loss=0.037348672747612
I0307 19:06:08.273081 140209761548032 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.06436207890510559, loss=0.035111624747514725
I0307 19:06:29.155393 140209753155328 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.1324288696050644, loss=0.04072947055101395
I0307 19:06:50.149676 140209761548032 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.049718353897333145, loss=0.037242840975522995
I0307 19:07:11.031791 140209753155328 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.11236684769392014, loss=0.038750648498535156
I0307 19:07:31.978597 140209761548032 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.11115242540836334, loss=0.03430832177400589
I0307 19:07:52.847546 140209753155328 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.07638590782880783, loss=0.034644998610019684
I0307 19:07:55.403762 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:09:04.184756 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:09:06.097801 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:09:07.960314 140351869203648 submission_runner.py:469] Time since start: 8702.17s, 	Step: 31213, 	{'train/accuracy': 0.9892027378082275, 'train/loss': 0.03682052716612816, 'train/mean_average_precision': 0.24121918876150683, 'validation/accuracy': 0.9859734773635864, 'validation/loss': 0.047756846994161606, 'validation/mean_average_precision': 0.20211730274003983, 'validation/num_examples': 43793, 'test/accuracy': 0.9851199984550476, 'test/loss': 0.05059695243835449, 'test/mean_average_precision': 0.1995309502367599, 'test/num_examples': 43793, 'score': 6493.169812440872, 'total_duration': 8702.167463302612, 'accumulated_submission_time': 6493.169812440872, 'accumulated_eval_time': 2207.4905438423157, 'accumulated_logging_time': 0.7227873802185059}
I0307 19:09:07.973350 140209761548032 logging_writer.py:48] [31213] accumulated_eval_time=2207.49, accumulated_logging_time=0.722787, accumulated_submission_time=6493.17, global_step=31213, preemption_count=0, score=6493.17, test/accuracy=0.98512, test/loss=0.050597, test/mean_average_precision=0.199531, test/num_examples=43793, total_duration=8702.17, train/accuracy=0.989203, train/loss=0.0368205, train/mean_average_precision=0.241219, validation/accuracy=0.985973, validation/loss=0.0477568, validation/mean_average_precision=0.202117, validation/num_examples=43793
I0307 19:09:26.285476 140209753155328 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.10652255266904831, loss=0.03663138300180435
I0307 19:09:46.916815 140209761548032 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.060179512947797775, loss=0.03615733981132507
I0307 19:10:07.738034 140209753155328 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.06199580430984497, loss=0.034873008728027344
I0307 19:10:28.672987 140209761548032 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.05878833308815956, loss=0.036919739097356796
I0307 19:10:49.531949 140209753155328 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.08014118671417236, loss=0.038084495812654495
I0307 19:11:10.702207 140209761548032 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.08830612897872925, loss=0.0336146242916584
I0307 19:11:31.563905 140209753155328 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.10704728215932846, loss=0.03735119849443436
I0307 19:11:52.291280 140209761548032 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.06512254476547241, loss=0.03474041819572449
I0307 19:12:12.969840 140209753155328 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.07075612992048264, loss=0.037398386746644974
I0307 19:12:33.790571 140209761548032 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.08240252733230591, loss=0.03544878959655762
I0307 19:12:54.653648 140209753155328 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.09175626188516617, loss=0.03996865078806877
I0307 19:13:07.996969 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:14:20.125071 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:14:22.007254 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:14:23.861473 140351869203648 submission_runner.py:469] Time since start: 9018.07s, 	Step: 32365, 	{'train/accuracy': 0.9891340136528015, 'train/loss': 0.037064630538225174, 'train/mean_average_precision': 0.23819130063880567, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.0477922149002552, 'validation/mean_average_precision': 0.20301761629443574, 'validation/num_examples': 43793, 'test/accuracy': 0.9851102828979492, 'test/loss': 0.05069917440414429, 'test/mean_average_precision': 0.19386314089916906, 'test/num_examples': 43793, 'score': 6733.153298854828, 'total_duration': 9018.068637371063, 'accumulated_submission_time': 6733.153298854828, 'accumulated_eval_time': 2283.3548877239227, 'accumulated_logging_time': 0.7472352981567383}
I0307 19:14:23.873415 140209761548032 logging_writer.py:48] [32365] accumulated_eval_time=2283.35, accumulated_logging_time=0.747235, accumulated_submission_time=6733.15, global_step=32365, preemption_count=0, score=6733.15, test/accuracy=0.98511, test/loss=0.0506992, test/mean_average_precision=0.193863, test/num_examples=43793, total_duration=9018.07, train/accuracy=0.989134, train/loss=0.0370646, train/mean_average_precision=0.238191, validation/accuracy=0.98609, validation/loss=0.0477922, validation/mean_average_precision=0.203018, validation/num_examples=43793
I0307 19:14:31.417487 140209753155328 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.0579027496278286, loss=0.03742328658699989
I0307 19:14:52.408740 140209761548032 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.06826664507389069, loss=0.03718695417046547
I0307 19:15:13.270123 140209753155328 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.06654783338308334, loss=0.03134337440133095
I0307 19:15:34.253391 140209761548032 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.08314984291791916, loss=0.03734162822365761
I0307 19:15:55.260157 140209753155328 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.12596319615840912, loss=0.03595202788710594
I0307 19:16:16.232160 140209761548032 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.04233855754137039, loss=0.03212437778711319
I0307 19:16:37.076665 140209753155328 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.13181012868881226, loss=0.03452422097325325
I0307 19:16:57.780881 140209761548032 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.07449027895927429, loss=0.039392322301864624
I0307 19:17:18.845685 140209753155328 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.09442326426506042, loss=0.03506303206086159
I0307 19:17:39.735752 140209761548032 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.11982585489749908, loss=0.044533904641866684
I0307 19:18:00.551261 140209753155328 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.10419366508722305, loss=0.04192196577787399
I0307 19:18:21.448243 140209761548032 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05844497308135033, loss=0.034642454236745834
I0307 19:18:23.972398 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:19:34.383555 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:19:36.291639 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:19:38.130606 140351869203648 submission_runner.py:469] Time since start: 9332.34s, 	Step: 33513, 	{'train/accuracy': 0.9894510507583618, 'train/loss': 0.03624799847602844, 'train/mean_average_precision': 0.2579487555854764, 'validation/accuracy': 0.9860088229179382, 'validation/loss': 0.04735192283987999, 'validation/mean_average_precision': 0.19850637259685747, 'validation/num_examples': 43793, 'test/accuracy': 0.9851102828979492, 'test/loss': 0.05001460388302803, 'test/mean_average_precision': 0.19865388504752293, 'test/num_examples': 43793, 'score': 6973.213913679123, 'total_duration': 9332.337782621384, 'accumulated_submission_time': 6973.213913679123, 'accumulated_eval_time': 2357.512940645218, 'accumulated_logging_time': 0.7679746150970459}
I0307 19:19:38.142411 140209753155328 logging_writer.py:48] [33513] accumulated_eval_time=2357.51, accumulated_logging_time=0.767975, accumulated_submission_time=6973.21, global_step=33513, preemption_count=0, score=6973.21, test/accuracy=0.98511, test/loss=0.0500146, test/mean_average_precision=0.198654, test/num_examples=43793, total_duration=9332.34, train/accuracy=0.989451, train/loss=0.036248, train/mean_average_precision=0.257949, validation/accuracy=0.986009, validation/loss=0.0473519, validation/mean_average_precision=0.198506, validation/num_examples=43793
I0307 19:19:56.242873 140209761548032 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.06908097863197327, loss=0.0362376943230629
I0307 19:20:17.190331 140209753155328 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.14810711145401, loss=0.03618486225605011
I0307 19:20:38.154839 140209761548032 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.09454472362995148, loss=0.0330854170024395
I0307 19:20:58.974810 140209753155328 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.07548761367797852, loss=0.041289642453193665
I0307 19:21:19.860714 140209761548032 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.1263469159603119, loss=0.03731605410575867
I0307 19:21:40.575320 140209753155328 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.0772518441081047, loss=0.03648162633180618
I0307 19:22:01.386485 140209761548032 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.06618689000606537, loss=0.037952546030282974
I0307 19:22:22.254225 140209753155328 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.17696741223335266, loss=0.03751475363969803
I0307 19:22:43.147192 140209761548032 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.08948256820440292, loss=0.03567260876297951
I0307 19:23:03.950406 140209753155328 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1327693611383438, loss=0.033499304205179214
I0307 19:23:24.682323 140209761548032 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.10807584226131439, loss=0.04117380827665329
I0307 19:23:38.244072 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:24:48.136500 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:24:50.294636 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:24:52.350426 140351869203648 submission_runner.py:469] Time since start: 9646.56s, 	Step: 34666, 	{'train/accuracy': 0.989586353302002, 'train/loss': 0.035799890756607056, 'train/mean_average_precision': 0.25355789654876926, 'validation/accuracy': 0.9861427545547485, 'validation/loss': 0.04750558361411095, 'validation/mean_average_precision': 0.20385740887934536, 'validation/num_examples': 43793, 'test/accuracy': 0.9852290749549866, 'test/loss': 0.05044238641858101, 'test/mean_average_precision': 0.20116868983594, 'test/num_examples': 43793, 'score': 7213.277344226837, 'total_duration': 9646.557542324066, 'accumulated_submission_time': 7213.277344226837, 'accumulated_eval_time': 2431.619081735611, 'accumulated_logging_time': 0.7885448932647705}
I0307 19:24:52.363867 140209753155328 logging_writer.py:48] [34666] accumulated_eval_time=2431.62, accumulated_logging_time=0.788545, accumulated_submission_time=7213.28, global_step=34666, preemption_count=0, score=7213.28, test/accuracy=0.985229, test/loss=0.0504424, test/mean_average_precision=0.201169, test/num_examples=43793, total_duration=9646.56, train/accuracy=0.989586, train/loss=0.0357999, train/mean_average_precision=0.253558, validation/accuracy=0.986143, validation/loss=0.0475056, validation/mean_average_precision=0.203857, validation/num_examples=43793
I0307 19:24:59.510311 140209761548032 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.12137525528669357, loss=0.03342791274189949
I0307 19:25:20.237631 140209753155328 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.07168164104223251, loss=0.037268299609422684
I0307 19:25:41.041931 140209761548032 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.12000171840190887, loss=0.036754585802555084
I0307 19:26:01.906582 140209753155328 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.10096866637468338, loss=0.03539586812257767
I0307 19:26:22.615410 140209761548032 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.09964320808649063, loss=0.03342737257480621
I0307 19:26:43.624236 140209753155328 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.12123707681894302, loss=0.03556983172893524
I0307 19:27:04.424742 140209761548032 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.07501518726348877, loss=0.03398429974913597
I0307 19:27:25.162035 140209753155328 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.0713716670870781, loss=0.03281771391630173
I0307 19:27:45.949339 140209761548032 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.09273480623960495, loss=0.03593131899833679
I0307 19:28:06.811864 140209753155328 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.07911018282175064, loss=0.03800095245242119
I0307 19:28:27.509400 140209761548032 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.09539229422807693, loss=0.03536955267190933
I0307 19:28:48.043714 140209753155328 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.0910533219575882, loss=0.033170636743307114
I0307 19:28:52.543497 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:30:01.410184 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:30:03.324689 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:30:05.170772 140351869203648 submission_runner.py:469] Time since start: 9959.38s, 	Step: 35823, 	{'train/accuracy': 0.989521861076355, 'train/loss': 0.03579404950141907, 'train/mean_average_precision': 0.2557826457070283, 'validation/accuracy': 0.9860323667526245, 'validation/loss': 0.04730730131268501, 'validation/mean_average_precision': 0.2058503194729604, 'validation/num_examples': 43793, 'test/accuracy': 0.9851216673851013, 'test/loss': 0.0500328429043293, 'test/mean_average_precision': 0.19924270958323567, 'test/num_examples': 43793, 'score': 7453.420997619629, 'total_duration': 9959.37802362442, 'accumulated_submission_time': 7453.420997619629, 'accumulated_eval_time': 2504.2462828159332, 'accumulated_logging_time': 0.8110270500183105}
I0307 19:30:05.183284 140209761548032 logging_writer.py:48] [35823] accumulated_eval_time=2504.25, accumulated_logging_time=0.811027, accumulated_submission_time=7453.42, global_step=35823, preemption_count=0, score=7453.42, test/accuracy=0.985122, test/loss=0.0500328, test/mean_average_precision=0.199243, test/num_examples=43793, total_duration=9959.38, train/accuracy=0.989522, train/loss=0.035794, train/mean_average_precision=0.255783, validation/accuracy=0.986032, validation/loss=0.0473073, validation/mean_average_precision=0.20585, validation/num_examples=43793
I0307 19:30:21.329198 140209753155328 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.08132058382034302, loss=0.03823813423514366
I0307 19:30:41.909805 140209761548032 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.12420438230037689, loss=0.035773634910583496
I0307 19:31:02.768600 140209753155328 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.0904993787407875, loss=0.034669000655412674
I0307 19:31:23.364013 140209761548032 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.09856891632080078, loss=0.03753373771905899
I0307 19:31:44.201589 140209753155328 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.085527703166008, loss=0.037284933030605316
I0307 19:32:05.079566 140209761548032 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.08648733794689178, loss=0.03937158361077309
I0307 19:32:26.048906 140209753155328 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.09521226584911346, loss=0.03769689425826073
I0307 19:32:46.892916 140209761548032 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.09110035747289658, loss=0.03741772100329399
I0307 19:33:07.689984 140209753155328 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.10908252745866776, loss=0.03784005716443062
I0307 19:33:28.394709 140209761548032 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.13091763854026794, loss=0.03759924694895744
I0307 19:33:49.060701 140209753155328 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.1451886147260666, loss=0.03406901657581329
I0307 19:34:05.294153 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:35:14.898121 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:35:16.791156 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:35:18.648457 140351869203648 submission_runner.py:469] Time since start: 10272.86s, 	Step: 36979, 	{'train/accuracy': 0.9897124767303467, 'train/loss': 0.03508268669247627, 'train/mean_average_precision': 0.26355136790526, 'validation/accuracy': 0.9860916137695312, 'validation/loss': 0.046970706433057785, 'validation/mean_average_precision': 0.20741455077996354, 'validation/num_examples': 43793, 'test/accuracy': 0.985194981098175, 'test/loss': 0.04981720447540283, 'test/mean_average_precision': 0.20410978208178363, 'test/num_examples': 43793, 'score': 7693.493750572205, 'total_duration': 10272.855638742447, 'accumulated_submission_time': 7693.493750572205, 'accumulated_eval_time': 2577.6004457473755, 'accumulated_logging_time': 0.8326258659362793}
I0307 19:35:18.660809 140209761548032 logging_writer.py:48] [36979] accumulated_eval_time=2577.6, accumulated_logging_time=0.832626, accumulated_submission_time=7693.49, global_step=36979, preemption_count=0, score=7693.49, test/accuracy=0.985195, test/loss=0.0498172, test/mean_average_precision=0.20411, test/num_examples=43793, total_duration=10272.9, train/accuracy=0.989712, train/loss=0.0350827, train/mean_average_precision=0.263551, validation/accuracy=0.986092, validation/loss=0.0469707, validation/mean_average_precision=0.207415, validation/num_examples=43793
I0307 19:35:23.252310 140209753155328 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.11865460127592087, loss=0.03485840931534767
I0307 19:35:44.016588 140209761548032 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.07150421291589737, loss=0.03606298565864563
I0307 19:36:04.914625 140209753155328 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.07021261006593704, loss=0.03775779902935028
I0307 19:36:25.749558 140209761548032 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.07411856949329376, loss=0.034029800444841385
I0307 19:36:46.592620 140209753155328 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.10153068602085114, loss=0.03269423916935921
I0307 19:37:07.477102 140209761548032 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.08457846194505692, loss=0.03637178614735603
I0307 19:37:28.292492 140209753155328 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.08342397212982178, loss=0.03728030249476433
I0307 19:37:49.090517 140209761548032 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.1492498368024826, loss=0.03275451064109802
I0307 19:38:09.723136 140209753155328 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.14195095002651215, loss=0.035261038690805435
I0307 19:38:30.446104 140209761548032 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.1049942895770073, loss=0.04031836986541748
I0307 19:38:51.340020 140209753155328 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.13895545899868011, loss=0.03295060992240906
I0307 19:39:12.155626 140209761548032 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.09455817937850952, loss=0.03911582753062248
I0307 19:39:18.706552 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:40:27.985792 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:40:29.886950 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:40:31.719010 140351869203648 submission_runner.py:469] Time since start: 10585.93s, 	Step: 38133, 	{'train/accuracy': 0.9895079731941223, 'train/loss': 0.035500165075063705, 'train/mean_average_precision': 0.27171960602742073, 'validation/accuracy': 0.9861817359924316, 'validation/loss': 0.04681684449315071, 'validation/mean_average_precision': 0.20469671082952884, 'validation/num_examples': 43793, 'test/accuracy': 0.9852312207221985, 'test/loss': 0.049536366015672684, 'test/mean_average_precision': 0.2060034951271204, 'test/num_examples': 43793, 'score': 7933.499976873398, 'total_duration': 10585.926189422607, 'accumulated_submission_time': 7933.499976873398, 'accumulated_eval_time': 2650.612752199173, 'accumulated_logging_time': 0.8538093566894531}
I0307 19:40:31.733075 140209753155328 logging_writer.py:48] [38133] accumulated_eval_time=2650.61, accumulated_logging_time=0.853809, accumulated_submission_time=7933.5, global_step=38133, preemption_count=0, score=7933.5, test/accuracy=0.985231, test/loss=0.0495364, test/mean_average_precision=0.206003, test/num_examples=43793, total_duration=10585.9, train/accuracy=0.989508, train/loss=0.0355002, train/mean_average_precision=0.27172, validation/accuracy=0.986182, validation/loss=0.0468168, validation/mean_average_precision=0.204697, validation/num_examples=43793
I0307 19:40:45.800091 140209761548032 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.12606650590896606, loss=0.035798635333776474
I0307 19:41:06.738208 140209753155328 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.12422310560941696, loss=0.03858135640621185
I0307 19:41:27.477546 140209761548032 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.10412696748971939, loss=0.033963724970817566
I0307 19:41:48.280234 140209753155328 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.09693161398172379, loss=0.03772544488310814
I0307 19:42:09.066039 140209761548032 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.09271099418401718, loss=0.03446892276406288
I0307 19:42:29.914103 140209753155328 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.11133834719657898, loss=0.034982744604349136
I0307 19:42:50.737594 140209761548032 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.11171726882457733, loss=0.03325308859348297
I0307 19:43:11.480148 140209753155328 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.11605937778949738, loss=0.031138718128204346
I0307 19:43:32.161160 140209761548032 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07504718750715256, loss=0.03559780493378639
I0307 19:43:52.759622 140209753155328 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.08721663802862167, loss=0.032244522124528885
I0307 19:44:13.304291 140209761548032 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.11324448883533478, loss=0.03275994583964348
I0307 19:44:31.791808 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:45:41.750319 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:45:43.627671 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:45:45.458866 140351869203648 submission_runner.py:469] Time since start: 10899.67s, 	Step: 39291, 	{'train/accuracy': 0.989795982837677, 'train/loss': 0.03474171459674835, 'train/mean_average_precision': 0.2755681563232172, 'validation/accuracy': 0.9862251877784729, 'validation/loss': 0.04683874920010567, 'validation/mean_average_precision': 0.21659267534488377, 'validation/num_examples': 43793, 'test/accuracy': 0.9852825999259949, 'test/loss': 0.049716152250766754, 'test/mean_average_precision': 0.20870617503729172, 'test/num_examples': 43793, 'score': 8173.519993543625, 'total_duration': 10899.666026592255, 'accumulated_submission_time': 8173.519993543625, 'accumulated_eval_time': 2724.279644012451, 'accumulated_logging_time': 0.876816987991333}
I0307 19:45:45.472523 140209753155328 logging_writer.py:48] [39291] accumulated_eval_time=2724.28, accumulated_logging_time=0.876817, accumulated_submission_time=8173.52, global_step=39291, preemption_count=0, score=8173.52, test/accuracy=0.985283, test/loss=0.0497162, test/mean_average_precision=0.208706, test/num_examples=43793, total_duration=10899.7, train/accuracy=0.989796, train/loss=0.0347417, train/mean_average_precision=0.275568, validation/accuracy=0.986225, validation/loss=0.0468387, validation/mean_average_precision=0.216593, validation/num_examples=43793
I0307 19:45:47.533442 140209761548032 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.16105882823467255, loss=0.03202419728040695
I0307 19:46:08.126621 140209753155328 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.0895313248038292, loss=0.03377282992005348
I0307 19:46:28.746658 140209761548032 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.11846272647380829, loss=0.03180558979511261
I0307 19:46:49.353075 140209753155328 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.17219050228595734, loss=0.03759223595261574
I0307 19:47:10.230431 140209761548032 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.09967238456010818, loss=0.03435036167502403
I0307 19:47:30.909859 140209753155328 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1576732099056244, loss=0.03632824495434761
I0307 19:47:51.555567 140209761548032 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.08892636746168137, loss=0.038432251662015915
I0307 19:48:12.190268 140209753155328 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.15413644909858704, loss=0.03578890115022659
I0307 19:48:32.777592 140209761548032 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.12584257125854492, loss=0.03352043777704239
I0307 19:48:53.393017 140209753155328 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.1161474883556366, loss=0.038320355117321014
I0307 19:49:14.143508 140209761548032 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.11635978519916534, loss=0.038400717079639435
I0307 19:49:34.756013 140209753155328 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.09780023247003555, loss=0.031083473935723305
I0307 19:49:45.552265 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:50:55.572301 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:50:57.532943 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:50:59.437835 140351869203648 submission_runner.py:469] Time since start: 11213.65s, 	Step: 40453, 	{'train/accuracy': 0.9897791147232056, 'train/loss': 0.034540314227342606, 'train/mean_average_precision': 0.28691357560113906, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.0467817597091198, 'validation/mean_average_precision': 0.21798785023808673, 'validation/num_examples': 43793, 'test/accuracy': 0.9852745532989502, 'test/loss': 0.04970575496554375, 'test/mean_average_precision': 0.2024124321898261, 'test/num_examples': 43793, 'score': 8413.562614440918, 'total_duration': 11213.645117759705, 'accumulated_submission_time': 8413.562614440918, 'accumulated_eval_time': 2798.1651699543, 'accumulated_logging_time': 0.899620532989502}
I0307 19:50:59.450970 140209761548032 logging_writer.py:48] [40453] accumulated_eval_time=2798.17, accumulated_logging_time=0.899621, accumulated_submission_time=8413.56, global_step=40453, preemption_count=0, score=8413.56, test/accuracy=0.985275, test/loss=0.0497058, test/mean_average_precision=0.202412, test/num_examples=43793, total_duration=11213.6, train/accuracy=0.989779, train/loss=0.0345403, train/mean_average_precision=0.286914, validation/accuracy=0.986249, validation/loss=0.0467818, validation/mean_average_precision=0.217988, validation/num_examples=43793
I0307 19:51:09.432537 140209753155328 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.1817106157541275, loss=0.028169766068458557
I0307 19:51:30.072388 140209761548032 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.16517958045005798, loss=0.031912170350551605
I0307 19:51:50.820575 140209753155328 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.11320662498474121, loss=0.03383747860789299
I0307 19:52:11.763675 140209761548032 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.11472007632255554, loss=0.03496113047003746
I0307 19:52:32.656435 140209753155328 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.11796137690544128, loss=0.034735240042209625
I0307 19:52:53.249857 140209761548032 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.15285900235176086, loss=0.03683093935251236
I0307 19:53:13.964228 140209753155328 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.15272080898284912, loss=0.03777837008237839
I0307 19:53:34.641763 140209761548032 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.0995030626654625, loss=0.03439990058541298
I0307 19:53:55.317459 140209753155328 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.11067157238721848, loss=0.03296026587486267
I0307 19:54:15.806002 140209761548032 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.0767756775021553, loss=0.03498066961765289
I0307 19:54:37.861099 140209753155328 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.10652640461921692, loss=0.03450090065598488
I0307 19:54:59.367668 140209761548032 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.17326784133911133, loss=0.0357927642762661
I0307 19:54:59.587345 140351869203648 spec.py:321] Evaluating on the training split.
I0307 19:56:14.612530 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 19:56:16.514503 140351869203648 spec.py:349] Evaluating on the test split.
I0307 19:56:18.368084 140351869203648 submission_runner.py:469] Time since start: 11532.58s, 	Step: 41602, 	{'train/accuracy': 0.9901075959205627, 'train/loss': 0.03372436389327049, 'train/mean_average_precision': 0.2837075829450806, 'validation/accuracy': 0.9863619804382324, 'validation/loss': 0.04636257141828537, 'validation/mean_average_precision': 0.21937410659615195, 'validation/num_examples': 43793, 'test/accuracy': 0.9853718876838684, 'test/loss': 0.04914310574531555, 'test/mean_average_precision': 0.20795128086621745, 'test/num_examples': 43793, 'score': 8653.662235498428, 'total_duration': 11532.575370788574, 'accumulated_submission_time': 8653.662235498428, 'accumulated_eval_time': 2876.9458661079407, 'accumulated_logging_time': 0.9214963912963867}
I0307 19:56:18.381932 140209753155328 logging_writer.py:48] [41602] accumulated_eval_time=2876.95, accumulated_logging_time=0.921496, accumulated_submission_time=8653.66, global_step=41602, preemption_count=0, score=8653.66, test/accuracy=0.985372, test/loss=0.0491431, test/mean_average_precision=0.207951, test/num_examples=43793, total_duration=11532.6, train/accuracy=0.990108, train/loss=0.0337244, train/mean_average_precision=0.283708, validation/accuracy=0.986362, validation/loss=0.0463626, validation/mean_average_precision=0.219374, validation/num_examples=43793
I0307 19:56:38.890775 140209761548032 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.15530990064144135, loss=0.033509593456983566
I0307 19:56:59.715159 140209753155328 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.1564105898141861, loss=0.03408196195960045
I0307 19:57:20.487672 140209761548032 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.15136656165122986, loss=0.03173105791211128
I0307 19:57:41.484461 140209753155328 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.19672368466854095, loss=0.03334268927574158
I0307 19:58:02.288710 140209761548032 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.18704023957252502, loss=0.03690817952156067
I0307 19:58:23.158504 140209753155328 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1152496263384819, loss=0.030538367107510567
I0307 19:58:43.765092 140209761548032 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.15625342726707458, loss=0.035326797515153885
I0307 19:59:04.340301 140209753155328 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.13330426812171936, loss=0.03273935988545418
I0307 19:59:25.123705 140209761548032 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.12661252915859222, loss=0.03658232465386391
I0307 19:59:45.900063 140209753155328 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.13735045492649078, loss=0.03461981564760208
I0307 20:00:06.632380 140209761548032 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.12142828106880188, loss=0.03171428665518761
I0307 20:00:18.530990 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:01:26.721447 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:01:28.620387 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:01:30.512285 140351869203648 submission_runner.py:469] Time since start: 11844.72s, 	Step: 42758, 	{'train/accuracy': 0.9900380373001099, 'train/loss': 0.03372671827673912, 'train/mean_average_precision': 0.29619519718321297, 'validation/accuracy': 0.9864281415939331, 'validation/loss': 0.04625309258699417, 'validation/mean_average_precision': 0.2174568302061361, 'validation/num_examples': 43793, 'test/accuracy': 0.9854792952537537, 'test/loss': 0.04896895959973335, 'test/mean_average_precision': 0.21018299992038633, 'test/num_examples': 43793, 'score': 8893.772742033005, 'total_duration': 11844.719472408295, 'accumulated_submission_time': 8893.772742033005, 'accumulated_eval_time': 2948.927017688751, 'accumulated_logging_time': 0.945145845413208}
I0307 20:01:30.525742 140209753155328 logging_writer.py:48] [42758] accumulated_eval_time=2948.93, accumulated_logging_time=0.945146, accumulated_submission_time=8893.77, global_step=42758, preemption_count=0, score=8893.77, test/accuracy=0.985479, test/loss=0.048969, test/mean_average_precision=0.210183, test/num_examples=43793, total_duration=11844.7, train/accuracy=0.990038, train/loss=0.0337267, train/mean_average_precision=0.296195, validation/accuracy=0.986428, validation/loss=0.0462531, validation/mean_average_precision=0.217457, validation/num_examples=43793
I0307 20:01:39.461732 140209761548032 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.20895570516586304, loss=0.03572476655244827
I0307 20:02:00.249901 140209753155328 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.2784683108329773, loss=0.034665148705244064
I0307 20:02:21.129404 140209761548032 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.24037021398544312, loss=0.032063230872154236
I0307 20:02:42.102856 140209753155328 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.10255067050457001, loss=0.0344872809946537
I0307 20:03:03.126131 140209761548032 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.18965895473957062, loss=0.03554066643118858
I0307 20:03:23.969966 140209753155328 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.10708778351545334, loss=0.03315076231956482
I0307 20:03:44.884280 140209761548032 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.18633154034614563, loss=0.030189035460352898
I0307 20:04:05.732340 140209753155328 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.11962837725877762, loss=0.03479669988155365
I0307 20:04:26.625640 140209761548032 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.229407399892807, loss=0.033033452928066254
I0307 20:04:47.515447 140209753155328 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.17081381380558014, loss=0.03082568757236004
I0307 20:05:08.419907 140209761548032 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.14451518654823303, loss=0.033711232244968414
I0307 20:05:29.235221 140209753155328 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.12341585755348206, loss=0.03345504030585289
I0307 20:05:30.675565 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:06:41.101904 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:06:43.007780 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:06:44.872918 140351869203648 submission_runner.py:469] Time since start: 12159.08s, 	Step: 43908, 	{'train/accuracy': 0.9901311993598938, 'train/loss': 0.03343880921602249, 'train/mean_average_precision': 0.2988139218257717, 'validation/accuracy': 0.9863713383674622, 'validation/loss': 0.04617256298661232, 'validation/mean_average_precision': 0.22275679949269492, 'validation/num_examples': 43793, 'test/accuracy': 0.9854017496109009, 'test/loss': 0.049057602882385254, 'test/mean_average_precision': 0.20994188273278738, 'test/num_examples': 43793, 'score': 9133.886105775833, 'total_duration': 12159.080085039139, 'accumulated_submission_time': 9133.886105775833, 'accumulated_eval_time': 3023.1242043972015, 'accumulated_logging_time': 0.9676332473754883}
I0307 20:06:44.886112 140209761548032 logging_writer.py:48] [43908] accumulated_eval_time=3023.12, accumulated_logging_time=0.967633, accumulated_submission_time=9133.89, global_step=43908, preemption_count=0, score=9133.89, test/accuracy=0.985402, test/loss=0.0490576, test/mean_average_precision=0.209942, test/num_examples=43793, total_duration=12159.1, train/accuracy=0.990131, train/loss=0.0334388, train/mean_average_precision=0.298814, validation/accuracy=0.986371, validation/loss=0.0461726, validation/mean_average_precision=0.222757, validation/num_examples=43793
I0307 20:07:04.262523 140209753155328 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08858262747526169, loss=0.031019754707813263
I0307 20:07:25.033872 140209761548032 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.2203959822654724, loss=0.03373827785253525
I0307 20:07:45.891116 140209753155328 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1254281848669052, loss=0.03433205932378769
I0307 20:08:06.701975 140209761548032 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.12043639272451401, loss=0.03172631934285164
I0307 20:08:27.686070 140209753155328 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.14075733721256256, loss=0.031721822917461395
I0307 20:08:48.413096 140209761548032 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.14221377670764923, loss=0.0287272147834301
I0307 20:09:18.724661 140209753155328 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.16788619756698608, loss=0.031760141253471375
I0307 20:09:39.386436 140209761548032 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.14164890348911285, loss=0.0324687696993351
I0307 20:10:00.079562 140209753155328 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.10987206548452377, loss=0.032975777983665466
I0307 20:10:20.885007 140209761548032 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2015814185142517, loss=0.03653552010655403
I0307 20:10:41.766383 140209753155328 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2099573016166687, loss=0.03461631387472153
I0307 20:10:44.915930 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:11:51.902815 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:11:53.823330 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:11:55.876966 140351869203648 submission_runner.py:469] Time since start: 12470.08s, 	Step: 45016, 	{'train/accuracy': 0.9901768565177917, 'train/loss': 0.0333244726061821, 'train/mean_average_precision': 0.30025291677084154, 'validation/accuracy': 0.9864252805709839, 'validation/loss': 0.046283572912216187, 'validation/mean_average_precision': 0.22170927743064406, 'validation/num_examples': 43793, 'test/accuracy': 0.9854506254196167, 'test/loss': 0.04914785176515579, 'test/mean_average_precision': 0.21485691694600836, 'test/num_examples': 43793, 'score': 9373.878672361374, 'total_duration': 12470.084194898605, 'accumulated_submission_time': 9373.878672361374, 'accumulated_eval_time': 3094.08514380455, 'accumulated_logging_time': 0.9895884990692139}
I0307 20:11:55.892761 140209761548032 logging_writer.py:48] [45016] accumulated_eval_time=3094.09, accumulated_logging_time=0.989588, accumulated_submission_time=9373.88, global_step=45016, preemption_count=0, score=9373.88, test/accuracy=0.985451, test/loss=0.0491479, test/mean_average_precision=0.214857, test/num_examples=43793, total_duration=12470.1, train/accuracy=0.990177, train/loss=0.0333245, train/mean_average_precision=0.300253, validation/accuracy=0.986425, validation/loss=0.0462836, validation/mean_average_precision=0.221709, validation/num_examples=43793
I0307 20:12:13.448832 140209753155328 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.14557135105133057, loss=0.03198275342583656
I0307 20:12:34.395533 140209761548032 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.10390932857990265, loss=0.03185587003827095
I0307 20:12:55.423403 140209753155328 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.10118960589170456, loss=0.033048566430807114
I0307 20:13:16.584115 140209761548032 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.127475768327713, loss=0.030051449313759804
I0307 20:13:37.559733 140209753155328 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.16419494152069092, loss=0.03154514729976654
I0307 20:13:58.563158 140209761548032 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.12885309755802155, loss=0.03496849536895752
I0307 20:14:19.707036 140209753155328 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.13862575590610504, loss=0.036072973161935806
I0307 20:14:40.494878 140209761548032 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.1724468320608139, loss=0.033786121755838394
I0307 20:15:01.374531 140209753155328 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.20984016358852386, loss=0.030883004888892174
I0307 20:15:22.080270 140209761548032 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.13546305894851685, loss=0.03202717751264572
I0307 20:15:42.858154 140209753155328 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.19119322299957275, loss=0.03202693909406662
I0307 20:15:55.973711 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:17:05.222026 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:17:07.102866 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:17:08.969678 140351869203648 submission_runner.py:469] Time since start: 12783.18s, 	Step: 46164, 	{'train/accuracy': 0.9904270768165588, 'train/loss': 0.03239098936319351, 'train/mean_average_precision': 0.31444661593216083, 'validation/accuracy': 0.986456573009491, 'validation/loss': 0.046119190752506256, 'validation/mean_average_precision': 0.2242544431840841, 'validation/num_examples': 43793, 'test/accuracy': 0.9854746460914612, 'test/loss': 0.048969294875860214, 'test/mean_average_precision': 0.21388230741047265, 'test/num_examples': 43793, 'score': 9613.918910264969, 'total_duration': 12783.176902532578, 'accumulated_submission_time': 9613.918910264969, 'accumulated_eval_time': 3167.0810120105743, 'accumulated_logging_time': 1.015930414199829}
I0307 20:17:08.983850 140209761548032 logging_writer.py:48] [46164] accumulated_eval_time=3167.08, accumulated_logging_time=1.01593, accumulated_submission_time=9613.92, global_step=46164, preemption_count=0, score=9613.92, test/accuracy=0.985475, test/loss=0.0489693, test/mean_average_precision=0.213882, test/num_examples=43793, total_duration=12783.2, train/accuracy=0.990427, train/loss=0.032391, train/mean_average_precision=0.314447, validation/accuracy=0.986457, validation/loss=0.0461192, validation/mean_average_precision=0.224254, validation/num_examples=43793
I0307 20:17:16.714306 140209753155328 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.12006396800279617, loss=0.03424280136823654
I0307 20:17:37.434808 140209761548032 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.16911277174949646, loss=0.03327671438455582
I0307 20:17:58.216201 140209753155328 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.14692239463329315, loss=0.03216734528541565
I0307 20:18:18.955436 140209761548032 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.14850416779518127, loss=0.0335882268846035
I0307 20:18:39.624430 140209753155328 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.15403932332992554, loss=0.030712876468896866
I0307 20:19:00.222861 140209761548032 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.18992401659488678, loss=0.03866901993751526
I0307 20:19:21.059867 140209753155328 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2882380783557892, loss=0.029705485329031944
I0307 20:19:41.687873 140209761548032 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.1542443335056305, loss=0.03143726661801338
I0307 20:20:02.435002 140209753155328 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.16006895899772644, loss=0.03191087394952774
I0307 20:20:23.092051 140209761548032 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.14589713513851166, loss=0.03222697228193283
I0307 20:20:43.623490 140209753155328 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.13424760103225708, loss=0.030486639589071274
I0307 20:21:04.248764 140209761548032 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.16132034361362457, loss=0.03406903147697449
I0307 20:21:09.047563 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:22:14.429432 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:22:16.378170 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:22:18.242170 140351869203648 submission_runner.py:469] Time since start: 13092.45s, 	Step: 47324, 	{'train/accuracy': 0.9905426502227783, 'train/loss': 0.032299917191267014, 'train/mean_average_precision': 0.32580895224616346, 'validation/accuracy': 0.9864476323127747, 'validation/loss': 0.04621846601366997, 'validation/mean_average_precision': 0.2234323594824429, 'validation/num_examples': 43793, 'test/accuracy': 0.9854418039321899, 'test/loss': 0.049184225499629974, 'test/mean_average_precision': 0.21353102769842158, 'test/num_examples': 43793, 'score': 9853.94199180603, 'total_duration': 13092.449440956116, 'accumulated_submission_time': 9853.94199180603, 'accumulated_eval_time': 3236.2755579948425, 'accumulated_logging_time': 1.0418851375579834}
I0307 20:22:18.255977 140209753155328 logging_writer.py:48] [47324] accumulated_eval_time=3236.28, accumulated_logging_time=1.04189, accumulated_submission_time=9853.94, global_step=47324, preemption_count=0, score=9853.94, test/accuracy=0.985442, test/loss=0.0491842, test/mean_average_precision=0.213531, test/num_examples=43793, total_duration=13092.4, train/accuracy=0.990543, train/loss=0.0322999, train/mean_average_precision=0.325809, validation/accuracy=0.986448, validation/loss=0.0462185, validation/mean_average_precision=0.223432, validation/num_examples=43793
I0307 20:22:34.532614 140209761548032 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.16437405347824097, loss=0.031981270760297775
I0307 20:22:55.348781 140209753155328 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.22455111145973206, loss=0.0346003882586956
I0307 20:23:16.247341 140209761548032 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.1663125902414322, loss=0.03428506851196289
I0307 20:23:37.204584 140209753155328 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.13761429488658905, loss=0.02745766006410122
I0307 20:23:57.744145 140209761548032 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.12706582248210907, loss=0.032338231801986694
I0307 20:24:18.272764 140209753155328 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3387617766857147, loss=0.03619543835520744
I0307 20:24:39.258983 140209761548032 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.17945809662342072, loss=0.02926919423043728
I0307 20:25:00.065199 140209753155328 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.18922677636146545, loss=0.03500966727733612
I0307 20:25:20.961940 140209761548032 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.21204422414302826, loss=0.029774492606520653
I0307 20:25:41.859155 140209753155328 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.18810856342315674, loss=0.03083515167236328
I0307 20:26:02.652820 140209761548032 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.19090278446674347, loss=0.03515580669045448
I0307 20:26:18.307150 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:27:24.977561 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:27:26.874503 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:27:28.759137 140351869203648 submission_runner.py:469] Time since start: 13402.97s, 	Step: 48476, 	{'train/accuracy': 0.9905759692192078, 'train/loss': 0.032080601900815964, 'train/mean_average_precision': 0.33100188454993706, 'validation/accuracy': 0.9864391088485718, 'validation/loss': 0.046355508267879486, 'validation/mean_average_precision': 0.223311767892547, 'validation/num_examples': 43793, 'test/accuracy': 0.9854628443717957, 'test/loss': 0.04927074536681175, 'test/mean_average_precision': 0.21563779915128894, 'test/num_examples': 43793, 'score': 10093.95628786087, 'total_duration': 13402.966351270676, 'accumulated_submission_time': 10093.95628786087, 'accumulated_eval_time': 3306.7274553775787, 'accumulated_logging_time': 1.0646114349365234}
I0307 20:27:28.773597 140209753155328 logging_writer.py:48] [48476] accumulated_eval_time=3306.73, accumulated_logging_time=1.06461, accumulated_submission_time=10094, global_step=48476, preemption_count=0, score=10094, test/accuracy=0.985463, test/loss=0.0492707, test/mean_average_precision=0.215638, test/num_examples=43793, total_duration=13403, train/accuracy=0.990576, train/loss=0.0320806, train/mean_average_precision=0.331002, validation/accuracy=0.986439, validation/loss=0.0463555, validation/mean_average_precision=0.223312, validation/num_examples=43793
I0307 20:27:34.019827 140209761548032 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.15423688292503357, loss=0.030918153002858162
I0307 20:27:54.842536 140209753155328 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.27395445108413696, loss=0.031464867293834686
I0307 20:28:15.695127 140209761548032 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.14197313785552979, loss=0.03257114812731743
I0307 20:28:36.762506 140209753155328 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.17184753715991974, loss=0.03716779872775078
I0307 20:28:57.740454 140209761548032 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2880084812641144, loss=0.03353280574083328
I0307 20:29:18.540603 140209753155328 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.15574149787425995, loss=0.03365786001086235
I0307 20:29:39.334917 140209761548032 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.20627735555171967, loss=0.03157652169466019
I0307 20:30:00.160661 140209753155328 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.1871340274810791, loss=0.03454120084643364
I0307 20:30:20.881892 140209761548032 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.1299661099910736, loss=0.027821555733680725
I0307 20:30:41.773248 140209753155328 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.11024662852287292, loss=0.029385630041360855
I0307 20:31:02.769092 140209761548032 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.241977721452713, loss=0.03230800852179527
I0307 20:31:23.767467 140209753155328 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.2801172435283661, loss=0.031167129054665565
I0307 20:31:28.800054 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:32:37.295887 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:32:39.208091 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:32:41.062287 140351869203648 submission_runner.py:469] Time since start: 13715.27s, 	Step: 49625, 	{'train/accuracy': 0.9904268980026245, 'train/loss': 0.03226373717188835, 'train/mean_average_precision': 0.32131254392298836, 'validation/accuracy': 0.9864395260810852, 'validation/loss': 0.04622156172990799, 'validation/mean_average_precision': 0.2240481773380816, 'validation/num_examples': 43793, 'test/accuracy': 0.985448956489563, 'test/loss': 0.049159303307533264, 'test/mean_average_precision': 0.21449660035030782, 'test/num_examples': 43793, 'score': 10333.94580411911, 'total_duration': 13715.269563674927, 'accumulated_submission_time': 10333.94580411911, 'accumulated_eval_time': 3378.9896388053894, 'accumulated_logging_time': 1.088163137435913}
I0307 20:32:41.076677 140209761548032 logging_writer.py:48] [49625] accumulated_eval_time=3378.99, accumulated_logging_time=1.08816, accumulated_submission_time=10333.9, global_step=49625, preemption_count=0, score=10333.9, test/accuracy=0.985449, test/loss=0.0491593, test/mean_average_precision=0.214497, test/num_examples=43793, total_duration=13715.3, train/accuracy=0.990427, train/loss=0.0322637, train/mean_average_precision=0.321313, validation/accuracy=0.98644, validation/loss=0.0462216, validation/mean_average_precision=0.224048, validation/num_examples=43793
I0307 20:32:56.986600 140209753155328 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1635507047176361, loss=0.028397180140018463
I0307 20:33:17.803550 140209761548032 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.14258047938346863, loss=0.029374785721302032
I0307 20:33:38.853401 140209753155328 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.20315739512443542, loss=0.03329339623451233
I0307 20:33:59.771680 140209761548032 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.19740267097949982, loss=0.0302627794444561
I0307 20:34:20.658864 140209753155328 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.12969397008419037, loss=0.032902222126722336
I0307 20:34:41.771628 140209761548032 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.27755898237228394, loss=0.03371895104646683
I0307 20:35:02.677487 140209753155328 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.22587387263774872, loss=0.03065534122288227
I0307 20:35:23.480419 140209761548032 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.13289503753185272, loss=0.03362230211496353
I0307 20:35:44.273967 140209753155328 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.17064642906188965, loss=0.030222972854971886
I0307 20:36:05.087034 140209761548032 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.1529976725578308, loss=0.029540684074163437
I0307 20:36:26.081117 140209753155328 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.15074752271175385, loss=0.034833889454603195
I0307 20:36:41.140095 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:37:48.663542 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:37:50.587453 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:37:52.450527 140351869203648 submission_runner.py:469] Time since start: 14026.66s, 	Step: 50773, 	{'train/accuracy': 0.990505039691925, 'train/loss': 0.03223705664277077, 'train/mean_average_precision': 0.3223540279759599, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.04625045508146286, 'validation/mean_average_precision': 0.22361738231162714, 'validation/num_examples': 43793, 'test/accuracy': 0.985484778881073, 'test/loss': 0.04918171837925911, 'test/mean_average_precision': 0.21543042999790354, 'test/num_examples': 43793, 'score': 10573.970704555511, 'total_duration': 14026.657806396484, 'accumulated_submission_time': 10573.970704555511, 'accumulated_eval_time': 3450.300018310547, 'accumulated_logging_time': 1.1114203929901123}
I0307 20:37:52.467453 140209761548032 logging_writer.py:48] [50773] accumulated_eval_time=3450.3, accumulated_logging_time=1.11142, accumulated_submission_time=10574, global_step=50773, preemption_count=0, score=10574, test/accuracy=0.985485, test/loss=0.0491817, test/mean_average_precision=0.21543, test/num_examples=43793, total_duration=14026.7, train/accuracy=0.990505, train/loss=0.0322371, train/mean_average_precision=0.322354, validation/accuracy=0.986456, validation/loss=0.0462505, validation/mean_average_precision=0.223617, validation/num_examples=43793
I0307 20:37:58.354415 140209753155328 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.18508440256118774, loss=0.0337521992623806
I0307 20:38:19.324868 140209761548032 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.18253764510154724, loss=0.02916286326944828
I0307 20:38:40.330485 140209753155328 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.1616944819688797, loss=0.03282675892114639
I0307 20:39:01.336162 140209761548032 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.12018781155347824, loss=0.02951274812221527
I0307 20:39:22.173610 140209753155328 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.1557668298482895, loss=0.03337978199124336
I0307 20:39:42.934843 140209761548032 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.1635148823261261, loss=0.030276425182819366
I0307 20:40:03.974464 140209753155328 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.17053931951522827, loss=0.03559393808245659
I0307 20:40:24.936532 140209761548032 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1815907508134842, loss=0.03277016431093216
I0307 20:40:45.925289 140209753155328 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.17688846588134766, loss=0.032596539705991745
I0307 20:41:06.620654 140209761548032 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2973876893520355, loss=0.03567836061120033
I0307 20:41:27.560635 140209753155328 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.1468563824892044, loss=0.034220676869153976
I0307 20:41:48.300008 140209761548032 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.1163460984826088, loss=0.032924894243478775
I0307 20:41:52.495731 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:43:03.870479 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:43:05.774177 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:43:07.621397 140351869203648 submission_runner.py:469] Time since start: 14341.83s, 	Step: 51921, 	{'train/accuracy': 0.9905572533607483, 'train/loss': 0.031936999410390854, 'train/mean_average_precision': 0.32155789589291284, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223267912864685, 'validation/mean_average_precision': 0.2235231142345789, 'validation/num_examples': 43793, 'test/accuracy': 0.9854809641838074, 'test/loss': 0.0491524375975132, 'test/mean_average_precision': 0.21520907982753493, 'test/num_examples': 43793, 'score': 10813.960614681244, 'total_duration': 14341.828605413437, 'accumulated_submission_time': 10813.960614681244, 'accumulated_eval_time': 3525.4255764484406, 'accumulated_logging_time': 1.1389245986938477}
I0307 20:43:07.635734 140209753155328 logging_writer.py:48] [51921] accumulated_eval_time=3525.43, accumulated_logging_time=1.13892, accumulated_submission_time=10814, global_step=51921, preemption_count=0, score=10814, test/accuracy=0.985481, test/loss=0.0491524, test/mean_average_precision=0.215209, test/num_examples=43793, total_duration=14341.8, train/accuracy=0.990557, train/loss=0.031937, train/mean_average_precision=0.321558, validation/accuracy=0.986448, validation/loss=0.0462233, validation/mean_average_precision=0.223523, validation/num_examples=43793
I0307 20:43:24.271647 140209761548032 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.18475426733493805, loss=0.03525177389383316
I0307 20:43:45.072629 140209753155328 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.1616148203611374, loss=0.0319245271384716
I0307 20:44:05.995925 140209761548032 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.1722978949546814, loss=0.03259726241230965
I0307 20:44:26.957045 140209753155328 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1540122628211975, loss=0.0321122445166111
I0307 20:44:47.723545 140209761548032 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.13742423057556152, loss=0.033215995877981186
I0307 20:45:08.454995 140209753155328 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.16240307688713074, loss=0.03346668556332588
I0307 20:45:29.148208 140209761548032 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.1067841574549675, loss=0.03058687224984169
I0307 20:45:49.933492 140209753155328 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.22876247763633728, loss=0.028293702751398087
I0307 20:46:10.728943 140209761548032 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.12064026296138763, loss=0.028609411790966988
I0307 20:46:31.435946 140209753155328 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.15981942415237427, loss=0.03783230483531952
I0307 20:46:52.383271 140209761548032 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.18274705111980438, loss=0.034875333309173584
I0307 20:47:07.623341 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:48:14.763095 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:48:16.675918 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:48:18.506844 140351869203648 submission_runner.py:469] Time since start: 14652.71s, 	Step: 53075, 	{'train/accuracy': 0.9906442761421204, 'train/loss': 0.03184876590967178, 'train/mean_average_precision': 0.3380970114306732, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.04622327908873558, 'validation/mean_average_precision': 0.2235652563420821, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04915238544344902, 'test/mean_average_precision': 0.21520626804487206, 'test/num_examples': 43793, 'score': 11053.910799741745, 'total_duration': 14652.714066267014, 'accumulated_submission_time': 11053.910799741745, 'accumulated_eval_time': 3596.308970451355, 'accumulated_logging_time': 1.1629300117492676}
I0307 20:48:18.521329 140209753155328 logging_writer.py:48] [53075] accumulated_eval_time=3596.31, accumulated_logging_time=1.16293, accumulated_submission_time=11053.9, global_step=53075, preemption_count=0, score=11053.9, test/accuracy=0.985481, test/loss=0.0491524, test/mean_average_precision=0.215206, test/num_examples=43793, total_duration=14652.7, train/accuracy=0.990644, train/loss=0.0318488, train/mean_average_precision=0.338097, validation/accuracy=0.986448, validation/loss=0.0462233, validation/mean_average_precision=0.223565, validation/num_examples=43793
I0307 20:48:23.917421 140209761548032 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.24132747948169708, loss=0.03353099524974823
I0307 20:48:44.592461 140209753155328 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.15507176518440247, loss=0.032910626381635666
I0307 20:49:05.375509 140209761548032 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.186062291264534, loss=0.033244356513023376
I0307 20:49:26.310989 140209753155328 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.16873392462730408, loss=0.03109288029372692
I0307 20:49:47.154233 140209761548032 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.15794791281223297, loss=0.031086906790733337
I0307 20:50:08.379017 140209753155328 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.17676734924316406, loss=0.031097853556275368
I0307 20:50:29.044502 140209761548032 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1246914342045784, loss=0.03171968087553978
I0307 20:50:49.875353 140209753155328 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.16551274061203003, loss=0.031111665070056915
I0307 20:51:10.771086 140209761548032 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.36868715286254883, loss=0.03136075660586357
I0307 20:51:31.673017 140209753155328 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.16603030264377594, loss=0.034558407962322235
I0307 20:51:52.320747 140209761548032 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.182750403881073, loss=0.03719828650355339
I0307 20:52:13.083255 140209753155328 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.14118419587612152, loss=0.03335823118686676
I0307 20:52:18.589764 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:53:28.463898 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:53:30.653656 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:53:32.732904 140351869203648 submission_runner.py:469] Time since start: 14966.94s, 	Step: 54227, 	{'train/accuracy': 0.9906573295593262, 'train/loss': 0.031983815133571625, 'train/mean_average_precision': 0.3225101048744034, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223271638154984, 'validation/mean_average_precision': 0.22350641771105728, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04915237799286842, 'test/mean_average_precision': 0.21518936979830983, 'test/num_examples': 43793, 'score': 11293.942175149918, 'total_duration': 14966.940074920654, 'accumulated_submission_time': 11293.942175149918, 'accumulated_eval_time': 3670.4519674777985, 'accumulated_logging_time': 1.18721342086792}
I0307 20:53:32.750070 140209761548032 logging_writer.py:48] [54227] accumulated_eval_time=3670.45, accumulated_logging_time=1.18721, accumulated_submission_time=11293.9, global_step=54227, preemption_count=0, score=11293.9, test/accuracy=0.985481, test/loss=0.0491524, test/mean_average_precision=0.215189, test/num_examples=43793, total_duration=14966.9, train/accuracy=0.990657, train/loss=0.0319838, train/mean_average_precision=0.32251, validation/accuracy=0.986448, validation/loss=0.0462233, validation/mean_average_precision=0.223506, validation/num_examples=43793
I0307 20:53:48.100325 140209753155328 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.1554236114025116, loss=0.03262494131922722
I0307 20:54:08.741688 140209761548032 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.11382029950618744, loss=0.029422862455248833
I0307 20:54:29.539360 140209753155328 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.12371055781841278, loss=0.02828720398247242
I0307 20:54:50.392297 140209761548032 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.12115728110074997, loss=0.032097481191158295
I0307 20:55:11.473440 140209753155328 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.11289136111736298, loss=0.02922448143362999
I0307 20:55:32.482268 140209761548032 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.22696958482265472, loss=0.03524283319711685
I0307 20:55:53.295912 140209753155328 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.15212206542491913, loss=0.033028777688741684
I0307 20:56:14.253117 140209761548032 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.17063648998737335, loss=0.034888774156570435
I0307 20:56:35.255202 140209753155328 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.17120331525802612, loss=0.03231608495116234
I0307 20:56:56.138124 140209761548032 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1208181381225586, loss=0.030632086098194122
I0307 20:57:17.134541 140209753155328 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.19238027930259705, loss=0.031165778636932373
I0307 20:57:32.820579 140351869203648 spec.py:321] Evaluating on the training split.
I0307 20:58:46.991855 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 20:58:48.923676 140351869203648 spec.py:349] Evaluating on the test split.
I0307 20:58:50.768927 140351869203648 submission_runner.py:469] Time since start: 15284.98s, 	Step: 55376, 	{'train/accuracy': 0.9904732704162598, 'train/loss': 0.03218638524413109, 'train/mean_average_precision': 0.3344232933598673, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223271638154984, 'validation/mean_average_precision': 0.2235740259247442, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04915238916873932, 'test/mean_average_precision': 0.21519319260729491, 'test/num_examples': 43793, 'score': 11533.973967075348, 'total_duration': 15284.97614622116, 'accumulated_submission_time': 11533.973967075348, 'accumulated_eval_time': 3748.4002027511597, 'accumulated_logging_time': 1.214712142944336}
I0307 20:58:50.785114 140209761548032 logging_writer.py:48] [55376] accumulated_eval_time=3748.4, accumulated_logging_time=1.21471, accumulated_submission_time=11534, global_step=55376, preemption_count=0, score=11534, test/accuracy=0.985481, test/loss=0.0491524, test/mean_average_precision=0.215193, test/num_examples=43793, total_duration=15285, train/accuracy=0.990473, train/loss=0.0321864, train/mean_average_precision=0.334423, validation/accuracy=0.986448, validation/loss=0.0462233, validation/mean_average_precision=0.223574, validation/num_examples=43793
I0307 20:58:56.023984 140209753155328 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.13523772358894348, loss=0.03301919624209404
I0307 20:59:17.037704 140209761548032 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.16435018181800842, loss=0.033078212291002274
I0307 20:59:37.954736 140209753155328 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.14107973873615265, loss=0.02983052097260952
I0307 20:59:58.925588 140209761548032 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.13652366399765015, loss=0.030679382383823395
I0307 21:00:19.712586 140209753155328 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.1495211273431778, loss=0.03614817187190056
I0307 21:00:40.528469 140209761548032 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.11837409436702728, loss=0.033172156661748886
I0307 21:01:01.332011 140209753155328 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.13341926038265228, loss=0.027968760579824448
I0307 21:01:22.141866 140209761548032 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.13161122798919678, loss=0.03375091403722763
I0307 21:01:42.939233 140209753155328 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.15771421790122986, loss=0.03267303854227066
I0307 21:02:03.740989 140209761548032 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.15930016338825226, loss=0.03362679481506348
I0307 21:02:24.676243 140209753155328 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.12768962979316711, loss=0.03146577626466751
I0307 21:02:45.573999 140209761548032 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.1347469538450241, loss=0.03249330073595047
I0307 21:02:50.873395 140351869203648 spec.py:321] Evaluating on the training split.
I0307 21:03:58.543505 140351869203648 spec.py:333] Evaluating on the validation split.
I0307 21:04:00.460554 140351869203648 spec.py:349] Evaluating on the test split.
I0307 21:04:02.291690 140351869203648 submission_runner.py:469] Time since start: 15596.50s, 	Step: 56526, 	{'train/accuracy': 0.9906229972839355, 'train/loss': 0.03179965913295746, 'train/mean_average_precision': 0.3274818239418065, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223267912864685, 'validation/mean_average_precision': 0.22361817177369125, 'validation/num_examples': 43793, 'test/accuracy': 0.9854812026023865, 'test/loss': 0.04915237054228783, 'test/mean_average_precision': 0.2152145977415101, 'test/num_examples': 43793, 'score': 11774.025574207306, 'total_duration': 15596.49895977974, 'accumulated_submission_time': 11774.025574207306, 'accumulated_eval_time': 3819.81845164299, 'accumulated_logging_time': 1.2394392490386963}
I0307 21:04:02.307590 140209753155328 logging_writer.py:48] [56526] accumulated_eval_time=3819.82, accumulated_logging_time=1.23944, accumulated_submission_time=11774, global_step=56526, preemption_count=0, score=11774, test/accuracy=0.985481, test/loss=0.0491524, test/mean_average_precision=0.215215, test/num_examples=43793, total_duration=15596.5, train/accuracy=0.990623, train/loss=0.0317997, train/mean_average_precision=0.327482, validation/accuracy=0.986448, validation/loss=0.0462233, validation/mean_average_precision=0.223618, validation/num_examples=43793
I0307 21:04:17.980906 140209761548032 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.23939576745033264, loss=0.029948627576231956
I0307 21:04:38.943111 140209753155328 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.14200806617736816, loss=0.03021046333014965
I0307 21:05:00.029228 140209761548032 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.1844448745250702, loss=0.03734135627746582
I0307 21:05:20.881600 140209753155328 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1410304754972458, loss=0.034923359751701355
I0307 21:05:41.666840 140209761548032 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.13102516531944275, loss=0.035347528755664825
I0307 21:06:02.679975 140209753155328 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.11701700836420059, loss=0.03165170177817345
I0307 21:06:23.543508 140209761548032 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.16453446447849274, loss=0.032114267349243164
I0307 21:06:44.442478 140209753155328 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.1896345317363739, loss=0.025009525939822197
I0307 21:07:05.363758 140209761548032 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.2054779827594757, loss=0.030997607856988907
I0307 21:07:26.149711 140209753155328 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.1459808647632599, loss=0.02927447110414505
I0307 21:07:46.961398 140209761548032 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2793049216270447, loss=0.03272249549627304
I0307 21:08:02.465001 140209753155328 logging_writer.py:48] [57676] global_step=57676, preemption_count=0, score=12014.1
I0307 21:08:02.594970 140351869203648 submission_runner.py:646] Tuning trial 4/5
I0307 21:08:02.595139 140351869203648 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0307 21:08:02.597316 140351869203648 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4847404658794403, 'train/loss': 0.7871220111846924, 'train/mean_average_precision': 0.021790056954124008, 'validation/accuracy': 0.4927646815776825, 'validation/loss': 0.7793631553649902, 'validation/mean_average_precision': 0.026076991706914007, 'validation/num_examples': 43793, 'test/accuracy': 0.49434229731559753, 'test/loss': 0.7786306738853455, 'test/mean_average_precision': 0.02722064855864404, 'test/num_examples': 43793, 'score': 11.819800615310669, 'total_duration': 215.54408311843872, 'accumulated_submission_time': 11.819800615310669, 'accumulated_eval_time': 203.72417569160461, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1116, {'train/accuracy': 0.986709475517273, 'train/loss': 0.05399135872721672, 'train/mean_average_precision': 0.04022380471433991, 'validation/accuracy': 0.9841905832290649, 'validation/loss': 0.06377651542425156, 'validation/mean_average_precision': 0.04044888522409357, 'validation/num_examples': 43793, 'test/accuracy': 0.9832048416137695, 'test/loss': 0.06695762276649475, 'test/mean_average_precision': 0.04210862281502998, 'test/num_examples': 43793, 'score': 251.812185049057, 'total_duration': 532.6246609687805, 'accumulated_submission_time': 251.812185049057, 'accumulated_eval_time': 280.76365876197815, 'accumulated_logging_time': 0.016812801361083984, 'global_step': 1116, 'preemption_count': 0}), (2249, {'train/accuracy': 0.9868757128715515, 'train/loss': 0.050838906317949295, 'train/mean_average_precision': 0.06279027247495665, 'validation/accuracy': 0.9842352271080017, 'validation/loss': 0.06066789850592613, 'validation/mean_average_precision': 0.061190772326033586, 'validation/num_examples': 43793, 'test/accuracy': 0.9832726120948792, 'test/loss': 0.06394655257463455, 'test/mean_average_precision': 0.05942017070438365, 'test/num_examples': 43793, 'score': 491.85641956329346, 'total_duration': 846.5869677066803, 'accumulated_submission_time': 491.85641956329346, 'accumulated_eval_time': 354.63296937942505, 'accumulated_logging_time': 0.035170793533325195, 'global_step': 2249, 'preemption_count': 0}), (3393, {'train/accuracy': 0.9872559309005737, 'train/loss': 0.047611381858587265, 'train/mean_average_precision': 0.09043060441627604, 'validation/accuracy': 0.9843655824661255, 'validation/loss': 0.05751677602529526, 'validation/mean_average_precision': 0.08779165491311472, 'validation/num_examples': 43793, 'test/accuracy': 0.9833977222442627, 'test/loss': 0.0607469342648983, 'test/mean_average_precision': 0.08593121905638043, 'test/num_examples': 43793, 'score': 731.9744083881378, 'total_duration': 1161.758546113968, 'accumulated_submission_time': 731.9744083881378, 'accumulated_eval_time': 429.6404175758362, 'accumulated_logging_time': 0.053038597106933594, 'global_step': 3393, 'preemption_count': 0}), (4559, {'train/accuracy': 0.987697184085846, 'train/loss': 0.04486807435750961, 'train/mean_average_precision': 0.11786624348268035, 'validation/accuracy': 0.9848851561546326, 'validation/loss': 0.054417744278907776, 'validation/mean_average_precision': 0.11528918522697759, 'validation/num_examples': 43793, 'test/accuracy': 0.9838892817497253, 'test/loss': 0.05748297646641731, 'test/mean_average_precision': 0.11277707008183724, 'test/num_examples': 43793, 'score': 972.0975506305695, 'total_duration': 1476.0183260440826, 'accumulated_submission_time': 972.0975506305695, 'accumulated_eval_time': 503.73065161705017, 'accumulated_logging_time': 0.07184791564941406, 'global_step': 4559, 'preemption_count': 0}), (5722, {'train/accuracy': 0.9877718091011047, 'train/loss': 0.04345260187983513, 'train/mean_average_precision': 0.14823155518071335, 'validation/accuracy': 0.9849667549133301, 'validation/loss': 0.05403447151184082, 'validation/mean_average_precision': 0.13694550827480725, 'validation/num_examples': 43793, 'test/accuracy': 0.9840265512466431, 'test/loss': 0.0570068284869194, 'test/mean_average_precision': 0.13345553758974338, 'test/num_examples': 43793, 'score': 1212.2034177780151, 'total_duration': 1790.9381802082062, 'accumulated_submission_time': 1212.2034177780151, 'accumulated_eval_time': 578.4967081546783, 'accumulated_logging_time': 0.08995842933654785, 'global_step': 5722, 'preemption_count': 0}), (6879, {'train/accuracy': 0.9878882169723511, 'train/loss': 0.04271654039621353, 'train/mean_average_precision': 0.14986085118311837, 'validation/accuracy': 0.9851145148277283, 'validation/loss': 0.05216933414340019, 'validation/mean_average_precision': 0.13985625315307748, 'validation/num_examples': 43793, 'test/accuracy': 0.9842308759689331, 'test/loss': 0.054933276027441025, 'test/mean_average_precision': 0.1428439539342359, 'test/num_examples': 43793, 'score': 1452.1587362289429, 'total_duration': 2105.0814514160156, 'accumulated_submission_time': 1452.1587362289429, 'accumulated_eval_time': 652.6365842819214, 'accumulated_logging_time': 0.10740351676940918, 'global_step': 6879, 'preemption_count': 0}), (8045, {'train/accuracy': 0.9882065653800964, 'train/loss': 0.041298069059848785, 'train/mean_average_precision': 0.17088348205827078, 'validation/accuracy': 0.9853150844573975, 'validation/loss': 0.05207788571715355, 'validation/mean_average_precision': 0.14811786168059773, 'validation/num_examples': 43793, 'test/accuracy': 0.9843605756759644, 'test/loss': 0.05522608384490013, 'test/mean_average_precision': 0.14924636050688517, 'test/num_examples': 43793, 'score': 1692.2665588855743, 'total_duration': 2420.1448788642883, 'accumulated_submission_time': 1692.2665588855743, 'accumulated_eval_time': 727.546293258667, 'accumulated_logging_time': 0.12540745735168457, 'global_step': 8045, 'preemption_count': 0}), (9212, {'train/accuracy': 0.9882962107658386, 'train/loss': 0.04099811613559723, 'train/mean_average_precision': 0.1733771070231122, 'validation/accuracy': 0.9853734970092773, 'validation/loss': 0.05058133229613304, 'validation/mean_average_precision': 0.15342083824876565, 'validation/num_examples': 43793, 'test/accuracy': 0.9845025539398193, 'test/loss': 0.053506091237068176, 'test/mean_average_precision': 0.1539502200776052, 'test/num_examples': 43793, 'score': 1932.355211019516, 'total_duration': 2732.724241733551, 'accumulated_submission_time': 1932.355211019516, 'accumulated_eval_time': 799.9899609088898, 'accumulated_logging_time': 0.14403653144836426, 'global_step': 9212, 'preemption_count': 0}), (10377, {'train/accuracy': 0.9884138107299805, 'train/loss': 0.04086827486753464, 'train/mean_average_precision': 0.17238742067007534, 'validation/accuracy': 0.9853199124336243, 'validation/loss': 0.05074239522218704, 'validation/mean_average_precision': 0.15440124586248596, 'validation/num_examples': 43793, 'test/accuracy': 0.984449028968811, 'test/loss': 0.0533616729080677, 'test/mean_average_precision': 0.15328489942817614, 'test/num_examples': 43793, 'score': 2172.311248064041, 'total_duration': 3048.321879386902, 'accumulated_submission_time': 2172.311248064041, 'accumulated_eval_time': 875.5853695869446, 'accumulated_logging_time': 0.16205096244812012, 'global_step': 10377, 'preemption_count': 0}), (11544, {'train/accuracy': 0.9884788990020752, 'train/loss': 0.04015427082777023, 'train/mean_average_precision': 0.19291017048389753, 'validation/accuracy': 0.9854737520217896, 'validation/loss': 0.04974060133099556, 'validation/mean_average_precision': 0.16543146904235276, 'validation/num_examples': 43793, 'test/accuracy': 0.9845825433731079, 'test/loss': 0.05253992974758148, 'test/mean_average_precision': 0.16440582742435628, 'test/num_examples': 43793, 'score': 2412.3876707553864, 'total_duration': 3363.4575414657593, 'accumulated_submission_time': 2412.3876707553864, 'accumulated_eval_time': 950.5983846187592, 'accumulated_logging_time': 0.18010711669921875, 'global_step': 11544, 'preemption_count': 0}), (12706, {'train/accuracy': 0.9884593486785889, 'train/loss': 0.03976178914308548, 'train/mean_average_precision': 0.18501394544647362, 'validation/accuracy': 0.9854717254638672, 'validation/loss': 0.04980354756116867, 'validation/mean_average_precision': 0.16518487080471425, 'validation/num_examples': 43793, 'test/accuracy': 0.9845328330993652, 'test/loss': 0.0525357611477375, 'test/mean_average_precision': 0.16741551064468646, 'test/num_examples': 43793, 'score': 2652.454883813858, 'total_duration': 3675.7612776756287, 'accumulated_submission_time': 2652.454883813858, 'accumulated_eval_time': 1022.7874805927277, 'accumulated_logging_time': 0.1981043815612793, 'global_step': 12706, 'preemption_count': 0}), (13869, {'train/accuracy': 0.9884340167045593, 'train/loss': 0.04030493274331093, 'train/mean_average_precision': 0.18717147998983497, 'validation/accuracy': 0.9855358600616455, 'validation/loss': 0.04973352327942848, 'validation/mean_average_precision': 0.16017044762353935, 'validation/num_examples': 43793, 'test/accuracy': 0.9846495389938354, 'test/loss': 0.05236131697893143, 'test/mean_average_precision': 0.15937123704930567, 'test/num_examples': 43793, 'score': 2892.4982256889343, 'total_duration': 3989.5626220703125, 'accumulated_submission_time': 2892.4982256889343, 'accumulated_eval_time': 1096.4986581802368, 'accumulated_logging_time': 0.21637892723083496, 'global_step': 13869, 'preemption_count': 0}), (15029, {'train/accuracy': 0.9886447787284851, 'train/loss': 0.03944523632526398, 'train/mean_average_precision': 0.19421323151798875, 'validation/accuracy': 0.985591471195221, 'validation/loss': 0.05005280300974846, 'validation/mean_average_precision': 0.17215742046903945, 'validation/num_examples': 43793, 'test/accuracy': 0.9846053123474121, 'test/loss': 0.052922144532203674, 'test/mean_average_precision': 0.17317978512399076, 'test/num_examples': 43793, 'score': 3132.62256360054, 'total_duration': 4305.278651237488, 'accumulated_submission_time': 3132.62256360054, 'accumulated_eval_time': 1172.0396654605865, 'accumulated_logging_time': 0.23593854904174805, 'global_step': 15029, 'preemption_count': 0}), (16190, {'train/accuracy': 0.9886651635169983, 'train/loss': 0.03922230005264282, 'train/mean_average_precision': 0.19650189395122658, 'validation/accuracy': 0.985602855682373, 'validation/loss': 0.04910341650247574, 'validation/mean_average_precision': 0.17631554403992677, 'validation/num_examples': 43793, 'test/accuracy': 0.9847005009651184, 'test/loss': 0.05187113955616951, 'test/mean_average_precision': 0.1722503737343848, 'test/num_examples': 43793, 'score': 3372.6036059856415, 'total_duration': 4619.330516576767, 'accumulated_submission_time': 3372.6036059856415, 'accumulated_eval_time': 1246.0617234706879, 'accumulated_logging_time': 0.2565314769744873, 'global_step': 16190, 'preemption_count': 0}), (17350, {'train/accuracy': 0.988707423210144, 'train/loss': 0.039025574922561646, 'train/mean_average_precision': 0.20166152038629817, 'validation/accuracy': 0.9855687618255615, 'validation/loss': 0.04920481890439987, 'validation/mean_average_precision': 0.175239581794696, 'validation/num_examples': 43793, 'test/accuracy': 0.984625518321991, 'test/loss': 0.05206214264035225, 'test/mean_average_precision': 0.17419328626335087, 'test/num_examples': 43793, 'score': 3612.6325249671936, 'total_duration': 4932.7949895858765, 'accumulated_submission_time': 3612.6325249671936, 'accumulated_eval_time': 1319.4477875232697, 'accumulated_logging_time': 0.27706313133239746, 'global_step': 17350, 'preemption_count': 0}), (18509, {'train/accuracy': 0.9885414838790894, 'train/loss': 0.039603255689144135, 'train/mean_average_precision': 0.2001174406660332, 'validation/accuracy': 0.9854839444160461, 'validation/loss': 0.04976131021976471, 'validation/mean_average_precision': 0.17054034717041244, 'validation/num_examples': 43793, 'test/accuracy': 0.9846280217170715, 'test/loss': 0.05254702642560005, 'test/mean_average_precision': 0.17498074070585454, 'test/num_examples': 43793, 'score': 3852.619345664978, 'total_duration': 5247.867392778397, 'accumulated_submission_time': 3852.619345664978, 'accumulated_eval_time': 1394.4854485988617, 'accumulated_logging_time': 0.2968254089355469, 'global_step': 18509, 'preemption_count': 0}), (19664, {'train/accuracy': 0.988696813583374, 'train/loss': 0.03887319564819336, 'train/mean_average_precision': 0.20316929639262585, 'validation/accuracy': 0.9856483340263367, 'validation/loss': 0.04885517805814743, 'validation/mean_average_precision': 0.17416494030168003, 'validation/num_examples': 43793, 'test/accuracy': 0.9847674369812012, 'test/loss': 0.05152552202343941, 'test/mean_average_precision': 0.1710801320322232, 'test/num_examples': 43793, 'score': 4092.5756454467773, 'total_duration': 5562.197319507599, 'accumulated_submission_time': 4092.5756454467773, 'accumulated_eval_time': 1468.8080174922943, 'accumulated_logging_time': 0.31752586364746094, 'global_step': 19664, 'preemption_count': 0}), (20822, {'train/accuracy': 0.9888809323310852, 'train/loss': 0.038621727377176285, 'train/mean_average_precision': 0.20557050492761572, 'validation/accuracy': 0.9858106970787048, 'validation/loss': 0.04889046028256416, 'validation/mean_average_precision': 0.18067202379380873, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05161231756210327, 'test/mean_average_precision': 0.18131597758312887, 'test/num_examples': 43793, 'score': 4332.675221681595, 'total_duration': 5879.433055400848, 'accumulated_submission_time': 4332.675221681595, 'accumulated_eval_time': 1545.8952639102936, 'accumulated_logging_time': 0.33777904510498047, 'global_step': 20822, 'preemption_count': 0}), (21979, {'train/accuracy': 0.988983690738678, 'train/loss': 0.03811206296086311, 'train/mean_average_precision': 0.22067338071597997, 'validation/accuracy': 0.9858646988868713, 'validation/loss': 0.04842767491936684, 'validation/mean_average_precision': 0.1831553759296441, 'validation/num_examples': 43793, 'test/accuracy': 0.9848554730415344, 'test/loss': 0.0510706901550293, 'test/mean_average_precision': 0.18809981352745814, 'test/num_examples': 43793, 'score': 4572.812266111374, 'total_duration': 6193.368548870087, 'accumulated_submission_time': 4572.812266111374, 'accumulated_eval_time': 1619.6450974941254, 'accumulated_logging_time': 0.3579246997833252, 'global_step': 21979, 'preemption_count': 0}), (23137, {'train/accuracy': 0.9888655543327332, 'train/loss': 0.038585416972637177, 'train/mean_average_precision': 0.21180715934351815, 'validation/accuracy': 0.9858070611953735, 'validation/loss': 0.04839330539107323, 'validation/mean_average_precision': 0.18108191048319328, 'validation/num_examples': 43793, 'test/accuracy': 0.9848892092704773, 'test/loss': 0.05119654908776283, 'test/mean_average_precision': 0.17652973770941913, 'test/num_examples': 43793, 'score': 4812.941241502762, 'total_duration': 6508.863561630249, 'accumulated_submission_time': 4812.941241502762, 'accumulated_eval_time': 1694.9620282649994, 'accumulated_logging_time': 0.3785266876220703, 'global_step': 23137, 'preemption_count': 0}), (24300, {'train/accuracy': 0.9888185262680054, 'train/loss': 0.038224633783102036, 'train/mean_average_precision': 0.21241110801793645, 'validation/accuracy': 0.9857120513916016, 'validation/loss': 0.04928415268659592, 'validation/mean_average_precision': 0.1868081023850515, 'validation/num_examples': 43793, 'test/accuracy': 0.9847948551177979, 'test/loss': 0.05222317948937416, 'test/mean_average_precision': 0.17864170464025428, 'test/num_examples': 43793, 'score': 5052.947694063187, 'total_duration': 6821.761011123657, 'accumulated_submission_time': 5052.947694063187, 'accumulated_eval_time': 1767.8041067123413, 'accumulated_logging_time': 0.39826226234436035, 'global_step': 24300, 'preemption_count': 0}), (25459, {'train/accuracy': 0.9888516068458557, 'train/loss': 0.03825634717941284, 'train/mean_average_precision': 0.22276084297089577, 'validation/accuracy': 0.9858630895614624, 'validation/loss': 0.04819132015109062, 'validation/mean_average_precision': 0.18829974770583538, 'validation/num_examples': 43793, 'test/accuracy': 0.9849330186843872, 'test/loss': 0.05104750767350197, 'test/mean_average_precision': 0.1837583811216438, 'test/num_examples': 43793, 'score': 5292.934512615204, 'total_duration': 7135.639479398727, 'accumulated_submission_time': 5292.934512615204, 'accumulated_eval_time': 1841.645385980606, 'accumulated_logging_time': 0.41911959648132324, 'global_step': 25459, 'preemption_count': 0}), (26621, {'train/accuracy': 0.9890813231468201, 'train/loss': 0.03750619664788246, 'train/mean_average_precision': 0.2346812202733523, 'validation/accuracy': 0.985808253288269, 'validation/loss': 0.048034921288490295, 'validation/mean_average_precision': 0.19128254877612466, 'validation/num_examples': 43793, 'test/accuracy': 0.9848365187644958, 'test/loss': 0.051006611436605453, 'test/mean_average_precision': 0.1851048637558814, 'test/num_examples': 43793, 'score': 5533.005326271057, 'total_duration': 7448.528254508972, 'accumulated_submission_time': 5533.005326271057, 'accumulated_eval_time': 1914.4144451618195, 'accumulated_logging_time': 0.4396181106567383, 'global_step': 26621, 'preemption_count': 0}), (27771, {'train/accuracy': 0.9891672134399414, 'train/loss': 0.0374494269490242, 'train/mean_average_precision': 0.22403331339853977, 'validation/accuracy': 0.9859174489974976, 'validation/loss': 0.04801275208592415, 'validation/mean_average_precision': 0.1881024160748155, 'validation/num_examples': 43793, 'test/accuracy': 0.9849502444267273, 'test/loss': 0.05093797296285629, 'test/mean_average_precision': 0.18914909470560715, 'test/num_examples': 43793, 'score': 5773.045235872269, 'total_duration': 7762.333683252335, 'accumulated_submission_time': 5773.045235872269, 'accumulated_eval_time': 1988.1296277046204, 'accumulated_logging_time': 0.4603147506713867, 'global_step': 27771, 'preemption_count': 0}), (28915, {'train/accuracy': 0.9893405437469482, 'train/loss': 0.036619193851947784, 'train/mean_average_precision': 0.23933924858268354, 'validation/accuracy': 0.9859998822212219, 'validation/loss': 0.047850728034973145, 'validation/mean_average_precision': 0.20013137241244977, 'validation/num_examples': 43793, 'test/accuracy': 0.9850930571556091, 'test/loss': 0.050854094326496124, 'test/mean_average_precision': 0.18676501947533358, 'test/num_examples': 43793, 'score': 6012.941381931305, 'total_duration': 8073.602168798447, 'accumulated_submission_time': 6012.941381931305, 'accumulated_eval_time': 2059.2531156539917, 'accumulated_logging_time': 0.6800205707550049, 'global_step': 28915, 'preemption_count': 0}), (30066, {'train/accuracy': 0.9892280101776123, 'train/loss': 0.037156715989112854, 'train/mean_average_precision': 0.23033424398207608, 'validation/accuracy': 0.985937774181366, 'validation/loss': 0.04770900681614876, 'validation/mean_average_precision': 0.1910839485901341, 'validation/num_examples': 43793, 'test/accuracy': 0.9849987030029297, 'test/loss': 0.0503774993121624, 'test/mean_average_precision': 0.1866270660368374, 'test/num_examples': 43793, 'score': 6253.078722000122, 'total_duration': 8389.469281435013, 'accumulated_submission_time': 6253.078722000122, 'accumulated_eval_time': 2134.9341747760773, 'accumulated_logging_time': 0.7013907432556152, 'global_step': 30066, 'preemption_count': 0}), (31213, {'train/accuracy': 0.9892027378082275, 'train/loss': 0.03682052716612816, 'train/mean_average_precision': 0.24121918876150683, 'validation/accuracy': 0.9859734773635864, 'validation/loss': 0.047756846994161606, 'validation/mean_average_precision': 0.20211730274003983, 'validation/num_examples': 43793, 'test/accuracy': 0.9851199984550476, 'test/loss': 0.05059695243835449, 'test/mean_average_precision': 0.1995309502367599, 'test/num_examples': 43793, 'score': 6493.169812440872, 'total_duration': 8702.167463302612, 'accumulated_submission_time': 6493.169812440872, 'accumulated_eval_time': 2207.4905438423157, 'accumulated_logging_time': 0.7227873802185059, 'global_step': 31213, 'preemption_count': 0}), (32365, {'train/accuracy': 0.9891340136528015, 'train/loss': 0.037064630538225174, 'train/mean_average_precision': 0.23819130063880567, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.0477922149002552, 'validation/mean_average_precision': 0.20301761629443574, 'validation/num_examples': 43793, 'test/accuracy': 0.9851102828979492, 'test/loss': 0.05069917440414429, 'test/mean_average_precision': 0.19386314089916906, 'test/num_examples': 43793, 'score': 6733.153298854828, 'total_duration': 9018.068637371063, 'accumulated_submission_time': 6733.153298854828, 'accumulated_eval_time': 2283.3548877239227, 'accumulated_logging_time': 0.7472352981567383, 'global_step': 32365, 'preemption_count': 0}), (33513, {'train/accuracy': 0.9894510507583618, 'train/loss': 0.03624799847602844, 'train/mean_average_precision': 0.2579487555854764, 'validation/accuracy': 0.9860088229179382, 'validation/loss': 0.04735192283987999, 'validation/mean_average_precision': 0.19850637259685747, 'validation/num_examples': 43793, 'test/accuracy': 0.9851102828979492, 'test/loss': 0.05001460388302803, 'test/mean_average_precision': 0.19865388504752293, 'test/num_examples': 43793, 'score': 6973.213913679123, 'total_duration': 9332.337782621384, 'accumulated_submission_time': 6973.213913679123, 'accumulated_eval_time': 2357.512940645218, 'accumulated_logging_time': 0.7679746150970459, 'global_step': 33513, 'preemption_count': 0}), (34666, {'train/accuracy': 0.989586353302002, 'train/loss': 0.035799890756607056, 'train/mean_average_precision': 0.25355789654876926, 'validation/accuracy': 0.9861427545547485, 'validation/loss': 0.04750558361411095, 'validation/mean_average_precision': 0.20385740887934536, 'validation/num_examples': 43793, 'test/accuracy': 0.9852290749549866, 'test/loss': 0.05044238641858101, 'test/mean_average_precision': 0.20116868983594, 'test/num_examples': 43793, 'score': 7213.277344226837, 'total_duration': 9646.557542324066, 'accumulated_submission_time': 7213.277344226837, 'accumulated_eval_time': 2431.619081735611, 'accumulated_logging_time': 0.7885448932647705, 'global_step': 34666, 'preemption_count': 0}), (35823, {'train/accuracy': 0.989521861076355, 'train/loss': 0.03579404950141907, 'train/mean_average_precision': 0.2557826457070283, 'validation/accuracy': 0.9860323667526245, 'validation/loss': 0.04730730131268501, 'validation/mean_average_precision': 0.2058503194729604, 'validation/num_examples': 43793, 'test/accuracy': 0.9851216673851013, 'test/loss': 0.0500328429043293, 'test/mean_average_precision': 0.19924270958323567, 'test/num_examples': 43793, 'score': 7453.420997619629, 'total_duration': 9959.37802362442, 'accumulated_submission_time': 7453.420997619629, 'accumulated_eval_time': 2504.2462828159332, 'accumulated_logging_time': 0.8110270500183105, 'global_step': 35823, 'preemption_count': 0}), (36979, {'train/accuracy': 0.9897124767303467, 'train/loss': 0.03508268669247627, 'train/mean_average_precision': 0.26355136790526, 'validation/accuracy': 0.9860916137695312, 'validation/loss': 0.046970706433057785, 'validation/mean_average_precision': 0.20741455077996354, 'validation/num_examples': 43793, 'test/accuracy': 0.985194981098175, 'test/loss': 0.04981720447540283, 'test/mean_average_precision': 0.20410978208178363, 'test/num_examples': 43793, 'score': 7693.493750572205, 'total_duration': 10272.855638742447, 'accumulated_submission_time': 7693.493750572205, 'accumulated_eval_time': 2577.6004457473755, 'accumulated_logging_time': 0.8326258659362793, 'global_step': 36979, 'preemption_count': 0}), (38133, {'train/accuracy': 0.9895079731941223, 'train/loss': 0.035500165075063705, 'train/mean_average_precision': 0.27171960602742073, 'validation/accuracy': 0.9861817359924316, 'validation/loss': 0.04681684449315071, 'validation/mean_average_precision': 0.20469671082952884, 'validation/num_examples': 43793, 'test/accuracy': 0.9852312207221985, 'test/loss': 0.049536366015672684, 'test/mean_average_precision': 0.2060034951271204, 'test/num_examples': 43793, 'score': 7933.499976873398, 'total_duration': 10585.926189422607, 'accumulated_submission_time': 7933.499976873398, 'accumulated_eval_time': 2650.612752199173, 'accumulated_logging_time': 0.8538093566894531, 'global_step': 38133, 'preemption_count': 0}), (39291, {'train/accuracy': 0.989795982837677, 'train/loss': 0.03474171459674835, 'train/mean_average_precision': 0.2755681563232172, 'validation/accuracy': 0.9862251877784729, 'validation/loss': 0.04683874920010567, 'validation/mean_average_precision': 0.21659267534488377, 'validation/num_examples': 43793, 'test/accuracy': 0.9852825999259949, 'test/loss': 0.049716152250766754, 'test/mean_average_precision': 0.20870617503729172, 'test/num_examples': 43793, 'score': 8173.519993543625, 'total_duration': 10899.666026592255, 'accumulated_submission_time': 8173.519993543625, 'accumulated_eval_time': 2724.279644012451, 'accumulated_logging_time': 0.876816987991333, 'global_step': 39291, 'preemption_count': 0}), (40453, {'train/accuracy': 0.9897791147232056, 'train/loss': 0.034540314227342606, 'train/mean_average_precision': 0.28691357560113906, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.0467817597091198, 'validation/mean_average_precision': 0.21798785023808673, 'validation/num_examples': 43793, 'test/accuracy': 0.9852745532989502, 'test/loss': 0.04970575496554375, 'test/mean_average_precision': 0.2024124321898261, 'test/num_examples': 43793, 'score': 8413.562614440918, 'total_duration': 11213.645117759705, 'accumulated_submission_time': 8413.562614440918, 'accumulated_eval_time': 2798.1651699543, 'accumulated_logging_time': 0.899620532989502, 'global_step': 40453, 'preemption_count': 0}), (41602, {'train/accuracy': 0.9901075959205627, 'train/loss': 0.03372436389327049, 'train/mean_average_precision': 0.2837075829450806, 'validation/accuracy': 0.9863619804382324, 'validation/loss': 0.04636257141828537, 'validation/mean_average_precision': 0.21937410659615195, 'validation/num_examples': 43793, 'test/accuracy': 0.9853718876838684, 'test/loss': 0.04914310574531555, 'test/mean_average_precision': 0.20795128086621745, 'test/num_examples': 43793, 'score': 8653.662235498428, 'total_duration': 11532.575370788574, 'accumulated_submission_time': 8653.662235498428, 'accumulated_eval_time': 2876.9458661079407, 'accumulated_logging_time': 0.9214963912963867, 'global_step': 41602, 'preemption_count': 0}), (42758, {'train/accuracy': 0.9900380373001099, 'train/loss': 0.03372671827673912, 'train/mean_average_precision': 0.29619519718321297, 'validation/accuracy': 0.9864281415939331, 'validation/loss': 0.04625309258699417, 'validation/mean_average_precision': 0.2174568302061361, 'validation/num_examples': 43793, 'test/accuracy': 0.9854792952537537, 'test/loss': 0.04896895959973335, 'test/mean_average_precision': 0.21018299992038633, 'test/num_examples': 43793, 'score': 8893.772742033005, 'total_duration': 11844.719472408295, 'accumulated_submission_time': 8893.772742033005, 'accumulated_eval_time': 2948.927017688751, 'accumulated_logging_time': 0.945145845413208, 'global_step': 42758, 'preemption_count': 0}), (43908, {'train/accuracy': 0.9901311993598938, 'train/loss': 0.03343880921602249, 'train/mean_average_precision': 0.2988139218257717, 'validation/accuracy': 0.9863713383674622, 'validation/loss': 0.04617256298661232, 'validation/mean_average_precision': 0.22275679949269492, 'validation/num_examples': 43793, 'test/accuracy': 0.9854017496109009, 'test/loss': 0.049057602882385254, 'test/mean_average_precision': 0.20994188273278738, 'test/num_examples': 43793, 'score': 9133.886105775833, 'total_duration': 12159.080085039139, 'accumulated_submission_time': 9133.886105775833, 'accumulated_eval_time': 3023.1242043972015, 'accumulated_logging_time': 0.9676332473754883, 'global_step': 43908, 'preemption_count': 0}), (45016, {'train/accuracy': 0.9901768565177917, 'train/loss': 0.0333244726061821, 'train/mean_average_precision': 0.30025291677084154, 'validation/accuracy': 0.9864252805709839, 'validation/loss': 0.046283572912216187, 'validation/mean_average_precision': 0.22170927743064406, 'validation/num_examples': 43793, 'test/accuracy': 0.9854506254196167, 'test/loss': 0.04914785176515579, 'test/mean_average_precision': 0.21485691694600836, 'test/num_examples': 43793, 'score': 9373.878672361374, 'total_duration': 12470.084194898605, 'accumulated_submission_time': 9373.878672361374, 'accumulated_eval_time': 3094.08514380455, 'accumulated_logging_time': 0.9895884990692139, 'global_step': 45016, 'preemption_count': 0}), (46164, {'train/accuracy': 0.9904270768165588, 'train/loss': 0.03239098936319351, 'train/mean_average_precision': 0.31444661593216083, 'validation/accuracy': 0.986456573009491, 'validation/loss': 0.046119190752506256, 'validation/mean_average_precision': 0.2242544431840841, 'validation/num_examples': 43793, 'test/accuracy': 0.9854746460914612, 'test/loss': 0.048969294875860214, 'test/mean_average_precision': 0.21388230741047265, 'test/num_examples': 43793, 'score': 9613.918910264969, 'total_duration': 12783.176902532578, 'accumulated_submission_time': 9613.918910264969, 'accumulated_eval_time': 3167.0810120105743, 'accumulated_logging_time': 1.015930414199829, 'global_step': 46164, 'preemption_count': 0}), (47324, {'train/accuracy': 0.9905426502227783, 'train/loss': 0.032299917191267014, 'train/mean_average_precision': 0.32580895224616346, 'validation/accuracy': 0.9864476323127747, 'validation/loss': 0.04621846601366997, 'validation/mean_average_precision': 0.2234323594824429, 'validation/num_examples': 43793, 'test/accuracy': 0.9854418039321899, 'test/loss': 0.049184225499629974, 'test/mean_average_precision': 0.21353102769842158, 'test/num_examples': 43793, 'score': 9853.94199180603, 'total_duration': 13092.449440956116, 'accumulated_submission_time': 9853.94199180603, 'accumulated_eval_time': 3236.2755579948425, 'accumulated_logging_time': 1.0418851375579834, 'global_step': 47324, 'preemption_count': 0}), (48476, {'train/accuracy': 0.9905759692192078, 'train/loss': 0.032080601900815964, 'train/mean_average_precision': 0.33100188454993706, 'validation/accuracy': 0.9864391088485718, 'validation/loss': 0.046355508267879486, 'validation/mean_average_precision': 0.223311767892547, 'validation/num_examples': 43793, 'test/accuracy': 0.9854628443717957, 'test/loss': 0.04927074536681175, 'test/mean_average_precision': 0.21563779915128894, 'test/num_examples': 43793, 'score': 10093.95628786087, 'total_duration': 13402.966351270676, 'accumulated_submission_time': 10093.95628786087, 'accumulated_eval_time': 3306.7274553775787, 'accumulated_logging_time': 1.0646114349365234, 'global_step': 48476, 'preemption_count': 0}), (49625, {'train/accuracy': 0.9904268980026245, 'train/loss': 0.03226373717188835, 'train/mean_average_precision': 0.32131254392298836, 'validation/accuracy': 0.9864395260810852, 'validation/loss': 0.04622156172990799, 'validation/mean_average_precision': 0.2240481773380816, 'validation/num_examples': 43793, 'test/accuracy': 0.985448956489563, 'test/loss': 0.049159303307533264, 'test/mean_average_precision': 0.21449660035030782, 'test/num_examples': 43793, 'score': 10333.94580411911, 'total_duration': 13715.269563674927, 'accumulated_submission_time': 10333.94580411911, 'accumulated_eval_time': 3378.9896388053894, 'accumulated_logging_time': 1.088163137435913, 'global_step': 49625, 'preemption_count': 0}), (50773, {'train/accuracy': 0.990505039691925, 'train/loss': 0.03223705664277077, 'train/mean_average_precision': 0.3223540279759599, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.04625045508146286, 'validation/mean_average_precision': 0.22361738231162714, 'validation/num_examples': 43793, 'test/accuracy': 0.985484778881073, 'test/loss': 0.04918171837925911, 'test/mean_average_precision': 0.21543042999790354, 'test/num_examples': 43793, 'score': 10573.970704555511, 'total_duration': 14026.657806396484, 'accumulated_submission_time': 10573.970704555511, 'accumulated_eval_time': 3450.300018310547, 'accumulated_logging_time': 1.1114203929901123, 'global_step': 50773, 'preemption_count': 0}), (51921, {'train/accuracy': 0.9905572533607483, 'train/loss': 0.031936999410390854, 'train/mean_average_precision': 0.32155789589291284, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223267912864685, 'validation/mean_average_precision': 0.2235231142345789, 'validation/num_examples': 43793, 'test/accuracy': 0.9854809641838074, 'test/loss': 0.0491524375975132, 'test/mean_average_precision': 0.21520907982753493, 'test/num_examples': 43793, 'score': 10813.960614681244, 'total_duration': 14341.828605413437, 'accumulated_submission_time': 10813.960614681244, 'accumulated_eval_time': 3525.4255764484406, 'accumulated_logging_time': 1.1389245986938477, 'global_step': 51921, 'preemption_count': 0}), (53075, {'train/accuracy': 0.9906442761421204, 'train/loss': 0.03184876590967178, 'train/mean_average_precision': 0.3380970114306732, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.04622327908873558, 'validation/mean_average_precision': 0.2235652563420821, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04915238544344902, 'test/mean_average_precision': 0.21520626804487206, 'test/num_examples': 43793, 'score': 11053.910799741745, 'total_duration': 14652.714066267014, 'accumulated_submission_time': 11053.910799741745, 'accumulated_eval_time': 3596.308970451355, 'accumulated_logging_time': 1.1629300117492676, 'global_step': 53075, 'preemption_count': 0}), (54227, {'train/accuracy': 0.9906573295593262, 'train/loss': 0.031983815133571625, 'train/mean_average_precision': 0.3225101048744034, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223271638154984, 'validation/mean_average_precision': 0.22350641771105728, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04915237799286842, 'test/mean_average_precision': 0.21518936979830983, 'test/num_examples': 43793, 'score': 11293.942175149918, 'total_duration': 14966.940074920654, 'accumulated_submission_time': 11293.942175149918, 'accumulated_eval_time': 3670.4519674777985, 'accumulated_logging_time': 1.18721342086792, 'global_step': 54227, 'preemption_count': 0}), (55376, {'train/accuracy': 0.9904732704162598, 'train/loss': 0.03218638524413109, 'train/mean_average_precision': 0.3344232933598673, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223271638154984, 'validation/mean_average_precision': 0.2235740259247442, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04915238916873932, 'test/mean_average_precision': 0.21519319260729491, 'test/num_examples': 43793, 'score': 11533.973967075348, 'total_duration': 15284.97614622116, 'accumulated_submission_time': 11533.973967075348, 'accumulated_eval_time': 3748.4002027511597, 'accumulated_logging_time': 1.214712142944336, 'global_step': 55376, 'preemption_count': 0}), (56526, {'train/accuracy': 0.9906229972839355, 'train/loss': 0.03179965913295746, 'train/mean_average_precision': 0.3274818239418065, 'validation/accuracy': 0.9864480495452881, 'validation/loss': 0.046223267912864685, 'validation/mean_average_precision': 0.22361817177369125, 'validation/num_examples': 43793, 'test/accuracy': 0.9854812026023865, 'test/loss': 0.04915237054228783, 'test/mean_average_precision': 0.2152145977415101, 'test/num_examples': 43793, 'score': 11774.025574207306, 'total_duration': 15596.49895977974, 'accumulated_submission_time': 11774.025574207306, 'accumulated_eval_time': 3819.81845164299, 'accumulated_logging_time': 1.2394392490386963, 'global_step': 56526, 'preemption_count': 0})], 'global_step': 57676}
I0307 21:08:02.597425 140351869203648 submission_runner.py:649] Timing: 12014.134752035141
I0307 21:08:02.597468 140351869203648 submission_runner.py:651] Total number of evals: 50
I0307 21:08:02.597503 140351869203648 submission_runner.py:652] ====================
I0307 21:08:02.597843 140351869203648 submission_runner.py:750] Final ogbg score: 3
