python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=211739446 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/ogbg_jax_03-07-2025-16-39-21.log
2025-03-07 16:39:39.544861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741365580.056291       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741365580.210420       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:40:31.527418 139843824272576 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax.
I0307 16:40:33.939097 139843824272576 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:40:33.942161 139843824272576 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:40:33.959817 139843824272576 submission_runner.py:606] Using RNG seed 211739446
I0307 16:40:38.062254 139843824272576 submission_runner.py:615] --- Tuning run 3/5 ---
I0307 16:40:38.062448 139843824272576 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_3.
I0307 16:40:38.062680 139843824272576 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_3/hparams.json.
I0307 16:40:38.300774 139843824272576 submission_runner.py:218] Initializing dataset.
I0307 16:40:40.040623 139843824272576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:40:40.085560 139843824272576 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0307 16:40:41.323680 139843824272576 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0307 16:40:41.427755 139843824272576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:40:41.803566 139843824272576 submission_runner.py:229] Initializing model.
I0307 16:40:50.604433 139843824272576 submission_runner.py:272] Initializing optimizer.
I0307 16:40:51.037403 139843824272576 submission_runner.py:279] Initializing metrics bundle.
I0307 16:40:51.037642 139843824272576 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:40:51.038571 139843824272576 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_3 with prefix checkpoint_
I0307 16:40:51.038691 139843824272576 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_3/meta_data_0.json.
I0307 16:40:51.038869 139843824272576 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:40:51.038918 139843824272576 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:40:51.521116 139843824272576 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_3/flags_0.json.
I0307 16:40:51.889936 139843824272576 submission_runner.py:337] Starting training loop.
I0307 16:41:06.123760 139707658356480 logging_writer.py:48] [0] global_step=0, grad_norm=2.789571523666382, loss=0.8002952933311462
I0307 16:41:06.418745 139843824272576 spec.py:321] Evaluating on the training split.
I0307 16:41:06.422523 139843824272576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:41:06.426431 139843824272576 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:41:06.489389 139843824272576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:42:27.729824 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 16:42:27.732818 139843824272576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:42:27.736817 139843824272576 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:42:27.798971 139843824272576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:43:34.935175 139843824272576 spec.py:349] Evaluating on the test split.
I0307 16:43:34.937809 139843824272576 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:43:34.941396 139843824272576 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:43:35.002444 139843824272576 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:44:43.436709 139843824272576 submission_runner.py:469] Time since start: 231.55s, 	Step: 1, 	{'train/accuracy': 0.4888734519481659, 'train/loss': 0.7987210750579834, 'train/mean_average_precision': 0.02035999178161005, 'validation/accuracy': 0.48879316449165344, 'validation/loss': 0.7953991889953613, 'validation/mean_average_precision': 0.023025011304165587, 'validation/num_examples': 43793, 'test/accuracy': 0.4907718300819397, 'test/loss': 0.7937217950820923, 'test/mean_average_precision': 0.0255982929472327, 'test/num_examples': 43793, 'score': 14.52869701385498, 'total_duration': 231.54671955108643, 'accumulated_submission_time': 14.52869701385498, 'accumulated_eval_time': 217.01790595054626, 'accumulated_logging_time': 0}
I0307 16:44:43.444323 139701914441472 logging_writer.py:48] [1] accumulated_eval_time=217.018, accumulated_logging_time=0, accumulated_submission_time=14.5287, global_step=1, preemption_count=0, score=14.5287, test/accuracy=0.490772, test/loss=0.793722, test/mean_average_precision=0.0255983, test/num_examples=43793, total_duration=231.547, train/accuracy=0.488873, train/loss=0.798721, train/mean_average_precision=0.02036, validation/accuracy=0.488793, validation/loss=0.795399, validation/mean_average_precision=0.023025, validation/num_examples=43793
I0307 16:45:05.525492 139701922834176 logging_writer.py:48] [100] global_step=100, grad_norm=0.5321171283721924, loss=0.44779059290885925
I0307 16:45:27.826757 139701914441472 logging_writer.py:48] [200] global_step=200, grad_norm=0.3533207178115845, loss=0.31198346614837646
I0307 16:45:50.051030 139701922834176 logging_writer.py:48] [300] global_step=300, grad_norm=0.21781601011753082, loss=0.1944790631532669
I0307 16:46:12.455201 139701914441472 logging_writer.py:48] [400] global_step=400, grad_norm=0.12308365106582642, loss=0.11972680687904358
I0307 16:46:35.086581 139701922834176 logging_writer.py:48] [500] global_step=500, grad_norm=0.0669383779168129, loss=0.08841653168201447
I0307 16:46:56.985002 139701914441472 logging_writer.py:48] [600] global_step=600, grad_norm=0.04317999631166458, loss=0.07135672867298126
I0307 16:47:19.137082 139702527026944 logging_writer.py:48] [700] global_step=700, grad_norm=0.16855035722255707, loss=0.05781752988696098
I0307 16:47:41.668845 139702518634240 logging_writer.py:48] [800] global_step=800, grad_norm=0.21723484992980957, loss=0.06180889531970024
I0307 16:48:04.315611 139702527026944 logging_writer.py:48] [900] global_step=900, grad_norm=0.020314550027251244, loss=0.0575142577290535
I0307 16:48:26.907314 139702518634240 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.045613016933202744, loss=0.05423547700047493
I0307 16:48:43.613670 139843824272576 spec.py:321] Evaluating on the training split.
I0307 16:50:00.760072 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 16:50:02.732027 139843824272576 spec.py:349] Evaluating on the test split.
I0307 16:50:04.774523 139843824272576 submission_runner.py:469] Time since start: 552.88s, 	Step: 1075, 	{'train/accuracy': 0.9866853952407837, 'train/loss': 0.054289259016513824, 'train/mean_average_precision': 0.046729288398821366, 'validation/accuracy': 0.9841580986976624, 'validation/loss': 0.06318428367376328, 'validation/mean_average_precision': 0.048025281764093555, 'validation/num_examples': 43793, 'test/accuracy': 0.9831863045692444, 'test/loss': 0.06626905500888824, 'test/mean_average_precision': 0.04927335139616927, 'test/num_examples': 43793, 'score': 254.65742588043213, 'total_duration': 552.8845226764679, 'accumulated_submission_time': 254.65742588043213, 'accumulated_eval_time': 298.1787006855011, 'accumulated_logging_time': 0.01788043975830078}
I0307 16:50:04.783630 139702527026944 logging_writer.py:48] [1075] accumulated_eval_time=298.179, accumulated_logging_time=0.0178804, accumulated_submission_time=254.657, global_step=1075, preemption_count=0, score=254.657, test/accuracy=0.983186, test/loss=0.0662691, test/mean_average_precision=0.0492734, test/num_examples=43793, total_duration=552.885, train/accuracy=0.986685, train/loss=0.0542893, train/mean_average_precision=0.0467293, validation/accuracy=0.984158, validation/loss=0.0631843, validation/mean_average_precision=0.0480253, validation/num_examples=43793
I0307 16:50:10.538994 139702518634240 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.1308603733778, loss=0.05455845966935158
I0307 16:50:33.112553 139702527026944 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.09408362954854965, loss=0.05165867507457733
I0307 16:50:55.259895 139702518634240 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.04739208519458771, loss=0.04981796070933342
I0307 16:51:17.942942 139702527026944 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.055840350687503815, loss=0.04861525073647499
I0307 16:51:40.791287 139702518634240 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.14752118289470673, loss=0.05098803713917732
I0307 16:52:03.205222 139702527026944 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.17059160768985748, loss=0.04848621413111687
I0307 16:52:25.425472 139702518634240 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.09637746959924698, loss=0.046914953738451004
I0307 16:52:47.086612 139702527026944 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0832056775689125, loss=0.03923776373267174
I0307 16:53:08.383347 139702518634240 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.07674894481897354, loss=0.04210374876856804
I0307 16:53:29.695364 139702527026944 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.05656108260154724, loss=0.04410358890891075
I0307 16:53:51.657134 139702518634240 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.16151155531406403, loss=0.04077640548348427
I0307 16:54:04.840752 139843824272576 spec.py:321] Evaluating on the training split.
I0307 16:55:21.037243 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 16:55:23.075654 139843824272576 spec.py:349] Evaluating on the test split.
I0307 16:55:25.039386 139843824272576 submission_runner.py:469] Time since start: 873.15s, 	Step: 2161, 	{'train/accuracy': 0.9877825975418091, 'train/loss': 0.04426362365484238, 'train/mean_average_precision': 0.13192660961437241, 'validation/accuracy': 0.9849594235420227, 'validation/loss': 0.053551796823740005, 'validation/mean_average_precision': 0.13107382257029848, 'validation/num_examples': 43793, 'test/accuracy': 0.9840021729469299, 'test/loss': 0.0566747784614563, 'test/mean_average_precision': 0.12891195384055912, 'test/num_examples': 43793, 'score': 494.6748118400574, 'total_duration': 873.149251461029, 'accumulated_submission_time': 494.6748118400574, 'accumulated_eval_time': 378.377158164978, 'accumulated_logging_time': 0.03690958023071289}
I0307 16:55:25.048473 139702527026944 logging_writer.py:48] [2161] accumulated_eval_time=378.377, accumulated_logging_time=0.0369096, accumulated_submission_time=494.675, global_step=2161, preemption_count=0, score=494.675, test/accuracy=0.984002, test/loss=0.0566748, test/mean_average_precision=0.128912, test/num_examples=43793, total_duration=873.149, train/accuracy=0.987783, train/loss=0.0442636, train/mean_average_precision=0.131927, validation/accuracy=0.984959, validation/loss=0.0535518, validation/mean_average_precision=0.131074, validation/num_examples=43793
I0307 16:55:33.894467 139702518634240 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.12909048795700073, loss=0.04770197719335556
I0307 16:55:55.758854 139702527026944 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.09614517539739609, loss=0.04247511550784111
I0307 16:56:17.524922 139702518634240 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.12097632139921188, loss=0.04123702645301819
I0307 16:56:39.651509 139702527026944 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.05712701007723808, loss=0.04356396943330765
I0307 16:57:01.557311 139702518634240 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.055615320801734924, loss=0.0406927689909935
I0307 16:57:23.203978 139702527026944 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.043951615691185, loss=0.04170828312635422
I0307 16:57:45.021249 139702518634240 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.09465601295232773, loss=0.0459103025496006
I0307 16:58:06.806375 139702527026944 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.10763100534677505, loss=0.04267255589365959
I0307 16:58:28.728486 139702518634240 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.06382561475038528, loss=0.04048093408346176
I0307 16:58:50.463287 139702527026944 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.05956590920686722, loss=0.040362320840358734
I0307 16:59:12.198430 139702518634240 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0374983549118042, loss=0.04134250432252884
I0307 16:59:25.206110 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:00:41.686356 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:00:43.805270 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:00:45.794831 139843824272576 submission_runner.py:469] Time since start: 1193.90s, 	Step: 3260, 	{'train/accuracy': 0.9883559942245483, 'train/loss': 0.040617696940898895, 'train/mean_average_precision': 0.18824316633658297, 'validation/accuracy': 0.9852334856987, 'validation/loss': 0.0507977195084095, 'validation/mean_average_precision': 0.15431984260257706, 'validation/num_examples': 43793, 'test/accuracy': 0.98442542552948, 'test/loss': 0.05346004292368889, 'test/mean_average_precision': 0.15982154157004205, 'test/num_examples': 43793, 'score': 734.7927660942078, 'total_duration': 1193.9048264026642, 'accumulated_submission_time': 734.7927660942078, 'accumulated_eval_time': 458.96581196784973, 'accumulated_logging_time': 0.05552506446838379}
I0307 17:00:45.803624 139702527026944 logging_writer.py:48] [3260] accumulated_eval_time=458.966, accumulated_logging_time=0.0555251, accumulated_submission_time=734.793, global_step=3260, preemption_count=0, score=734.793, test/accuracy=0.984425, test/loss=0.05346, test/mean_average_precision=0.159822, test/num_examples=43793, total_duration=1193.9, train/accuracy=0.988356, train/loss=0.0406177, train/mean_average_precision=0.188243, validation/accuracy=0.985233, validation/loss=0.0507977, validation/mean_average_precision=0.15432, validation/num_examples=43793
I0307 17:00:54.568555 139702518634240 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.08354496210813522, loss=0.03948238492012024
I0307 17:01:16.805282 139702527026944 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.08988915383815765, loss=0.05021556094288826
I0307 17:01:38.577057 139702518634240 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.08580738306045532, loss=0.04034585878252983
I0307 17:02:00.676768 139702527026944 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.090421661734581, loss=0.04203977808356285
I0307 17:02:22.590704 139702518634240 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.04743369668722153, loss=0.04037002846598625
I0307 17:02:44.367993 139702527026944 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.045707978308200836, loss=0.04610389098525047
I0307 17:03:06.227936 139702518634240 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.04473784193396568, loss=0.03578060492873192
I0307 17:03:28.023890 139702527026944 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.06849760562181473, loss=0.0380454920232296
I0307 17:03:49.983408 139702518634240 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.03674676641821861, loss=0.03977543115615845
I0307 17:04:11.804291 139702527026944 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.05334301292896271, loss=0.043016429990530014
I0307 17:04:33.741706 139702518634240 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04796024411916733, loss=0.0395650751888752
I0307 17:04:45.880246 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:06:03.373571 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:06:05.410430 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:06:07.383640 139843824272576 submission_runner.py:469] Time since start: 1515.49s, 	Step: 4357, 	{'train/accuracy': 0.9884837865829468, 'train/loss': 0.03977229446172714, 'train/mean_average_precision': 0.20432664485055496, 'validation/accuracy': 0.9856292605400085, 'validation/loss': 0.04886684566736221, 'validation/mean_average_precision': 0.18680530843930473, 'validation/num_examples': 43793, 'test/accuracy': 0.9847333431243896, 'test/loss': 0.051688507199287415, 'test/mean_average_precision': 0.18614424673459568, 'test/num_examples': 43793, 'score': 974.8287355899811, 'total_duration': 1515.4936537742615, 'accumulated_submission_time': 974.8287355899811, 'accumulated_eval_time': 540.4691524505615, 'accumulated_logging_time': 0.07383108139038086}
I0307 17:06:07.392960 139702104409856 logging_writer.py:48] [4357] accumulated_eval_time=540.469, accumulated_logging_time=0.0738311, accumulated_submission_time=974.829, global_step=4357, preemption_count=0, score=974.829, test/accuracy=0.984733, test/loss=0.0516885, test/mean_average_precision=0.186144, test/num_examples=43793, total_duration=1515.49, train/accuracy=0.988484, train/loss=0.0397723, train/mean_average_precision=0.204327, validation/accuracy=0.985629, validation/loss=0.0488668, validation/mean_average_precision=0.186805, validation/num_examples=43793
I0307 17:06:16.961374 139702096017152 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.054786454886198044, loss=0.044591907411813736
I0307 17:06:38.622417 139702104409856 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.03548826649785042, loss=0.038416530936956406
I0307 17:06:59.874933 139702096017152 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.0963597223162651, loss=0.04283427447080612
I0307 17:07:21.718012 139702104409856 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.04788690805435181, loss=0.03505893051624298
I0307 17:07:43.596972 139702096017152 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.024910572916269302, loss=0.039720114320516586
I0307 17:08:05.461012 139702104409856 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.054100751876831055, loss=0.040486790239810944
I0307 17:08:27.317234 139702096017152 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.027863232418894768, loss=0.03565000370144844
I0307 17:08:49.329397 139702104409856 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.08954765647649765, loss=0.04350190609693527
I0307 17:09:11.246574 139702096017152 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.028604241088032722, loss=0.03648948669433594
I0307 17:09:33.042799 139702104409856 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.024541648104786873, loss=0.03826943412423134
I0307 17:09:54.760619 139702096017152 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.04715760052204132, loss=0.04144763574004173
I0307 17:10:07.428680 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:11:24.350679 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:11:26.374045 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:11:28.303220 139843824272576 submission_runner.py:469] Time since start: 1836.41s, 	Step: 5459, 	{'train/accuracy': 0.9887811541557312, 'train/loss': 0.038167599588632584, 'train/mean_average_precision': 0.23686243035877524, 'validation/accuracy': 0.9857709407806396, 'validation/loss': 0.04812463000416756, 'validation/mean_average_precision': 0.20061241729478005, 'validation/num_examples': 43793, 'test/accuracy': 0.9847227931022644, 'test/loss': 0.05108640715479851, 'test/mean_average_precision': 0.1900487215808086, 'test/num_examples': 43793, 'score': 1214.827980518341, 'total_duration': 1836.413230895996, 'accumulated_submission_time': 1214.827980518341, 'accumulated_eval_time': 621.3436448574066, 'accumulated_logging_time': 0.0921177864074707}
I0307 17:11:28.312484 139702104409856 logging_writer.py:48] [5459] accumulated_eval_time=621.344, accumulated_logging_time=0.0921178, accumulated_submission_time=1214.83, global_step=5459, preemption_count=0, score=1214.83, test/accuracy=0.984723, test/loss=0.0510864, test/mean_average_precision=0.190049, test/num_examples=43793, total_duration=1836.41, train/accuracy=0.988781, train/loss=0.0381676, train/mean_average_precision=0.236862, validation/accuracy=0.985771, validation/loss=0.0481246, validation/mean_average_precision=0.200612, validation/num_examples=43793
I0307 17:11:37.661145 139702096017152 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.03040502406656742, loss=0.036908168345689774
I0307 17:11:59.831325 139702104409856 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.02851017378270626, loss=0.035531118512153625
I0307 17:12:21.726935 139702096017152 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.02608063444495201, loss=0.038744837045669556
I0307 17:12:43.551542 139702104409856 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.03516365587711334, loss=0.039741598069667816
I0307 17:13:05.479375 139702096017152 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.025932947173714638, loss=0.03253383934497833
I0307 17:13:27.462727 139702104409856 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.02989218384027481, loss=0.04042494297027588
I0307 17:13:48.465703 139702096017152 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.03442791476845741, loss=0.04060684144496918
I0307 17:14:09.489602 139702104409856 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0527239665389061, loss=0.03964167460799217
I0307 17:14:30.664473 139702096017152 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.031342752277851105, loss=0.03751690313220024
I0307 17:14:52.295661 139702104409856 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.029872138053178787, loss=0.04163627326488495
I0307 17:15:13.184711 139702096017152 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.04139226675033569, loss=0.03855470195412636
I0307 17:15:28.428495 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:16:44.117822 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:16:46.084407 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:16:48.021262 139843824272576 submission_runner.py:469] Time since start: 2156.13s, 	Step: 6574, 	{'train/accuracy': 0.9890382885932922, 'train/loss': 0.036975350230932236, 'train/mean_average_precision': 0.2600357684134873, 'validation/accuracy': 0.9860140681266785, 'validation/loss': 0.04712377488613129, 'validation/mean_average_precision': 0.20786160573019172, 'validation/num_examples': 43793, 'test/accuracy': 0.9852316379547119, 'test/loss': 0.04958077892661095, 'test/mean_average_precision': 0.2134764043731595, 'test/num_examples': 43793, 'score': 1454.9046506881714, 'total_duration': 2156.131096124649, 'accumulated_submission_time': 1454.9046506881714, 'accumulated_eval_time': 700.9361793994904, 'accumulated_logging_time': 0.11029505729675293}
I0307 17:16:48.030872 139702104409856 logging_writer.py:48] [6574] accumulated_eval_time=700.936, accumulated_logging_time=0.110295, accumulated_submission_time=1454.9, global_step=6574, preemption_count=0, score=1454.9, test/accuracy=0.985232, test/loss=0.0495808, test/mean_average_precision=0.213476, test/num_examples=43793, total_duration=2156.13, train/accuracy=0.989038, train/loss=0.0369754, train/mean_average_precision=0.260036, validation/accuracy=0.986014, validation/loss=0.0471238, validation/mean_average_precision=0.207862, validation/num_examples=43793
I0307 17:16:53.932025 139702096017152 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.03397214040160179, loss=0.03737328574061394
I0307 17:17:15.696526 139702104409856 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.04200111702084541, loss=0.0423191674053669
I0307 17:17:37.508528 139702096017152 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.028145797550678253, loss=0.03730903938412666
I0307 17:17:59.260916 139702104409856 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.03329585865139961, loss=0.03778856247663498
I0307 17:18:21.033506 139702096017152 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0273459292948246, loss=0.03660854697227478
I0307 17:18:42.715874 139702104409856 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.03209599107503891, loss=0.03328397125005722
I0307 17:19:04.333837 139702096017152 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.02947717159986496, loss=0.038270916789770126
I0307 17:19:26.417066 139702104409856 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.029559798538684845, loss=0.03506489843130112
I0307 17:19:47.500997 139702096017152 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.02616436779499054, loss=0.039080217480659485
I0307 17:20:08.585384 139702104409856 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.02522600069642067, loss=0.03389754518866539
I0307 17:20:30.144855 139702096017152 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.02389388531446457, loss=0.03259143605828285
I0307 17:20:48.204356 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:22:04.166866 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:22:06.112413 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:22:08.002093 139843824272576 submission_runner.py:469] Time since start: 2476.11s, 	Step: 7685, 	{'train/accuracy': 0.9893725514411926, 'train/loss': 0.035915616899728775, 'train/mean_average_precision': 0.27092566175358246, 'validation/accuracy': 0.9861992001533508, 'validation/loss': 0.04646225646138191, 'validation/mean_average_precision': 0.2163233596838837, 'validation/num_examples': 43793, 'test/accuracy': 0.985359251499176, 'test/loss': 0.04879593849182129, 'test/mean_average_precision': 0.2240232027142597, 'test/num_examples': 43793, 'score': 1695.039249420166, 'total_duration': 2476.1119663715363, 'accumulated_submission_time': 1695.039249420166, 'accumulated_eval_time': 780.7337276935577, 'accumulated_logging_time': 0.12884211540222168}
I0307 17:22:08.011962 139702104409856 logging_writer.py:48] [7685] accumulated_eval_time=780.734, accumulated_logging_time=0.128842, accumulated_submission_time=1695.04, global_step=7685, preemption_count=0, score=1695.04, test/accuracy=0.985359, test/loss=0.0487959, test/mean_average_precision=0.224023, test/num_examples=43793, total_duration=2476.11, train/accuracy=0.989373, train/loss=0.0359156, train/mean_average_precision=0.270926, validation/accuracy=0.986199, validation/loss=0.0464623, validation/mean_average_precision=0.216323, validation/num_examples=43793
I0307 17:22:11.500142 139702096017152 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.04486243054270744, loss=0.03906366974115372
I0307 17:22:33.416401 139702104409856 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.026456961408257484, loss=0.03663260117173195
I0307 17:22:55.265935 139702096017152 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.03223670646548271, loss=0.03678061440587044
I0307 17:23:16.886962 139702104409856 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.025201397016644478, loss=0.0354015938937664
I0307 17:23:38.564421 139702096017152 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.04306609183549881, loss=0.03548897057771683
I0307 17:24:00.016656 139702104409856 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.030405431985855103, loss=0.032549209892749786
I0307 17:24:21.635006 139702096017152 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.03164221718907356, loss=0.037506286054849625
I0307 17:24:43.284392 139702104409856 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.039103347808122635, loss=0.031958580017089844
I0307 17:25:04.788373 139702096017152 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03267007693648338, loss=0.040919408202171326
I0307 17:25:26.525119 139702104409856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.02576327696442604, loss=0.038542963564395905
I0307 17:25:48.402072 139702096017152 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.027652092278003693, loss=0.03313463553786278
I0307 17:26:08.093648 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:27:23.076519 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:27:25.073884 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:27:27.154540 139843824272576 submission_runner.py:469] Time since start: 2795.26s, 	Step: 8792, 	{'train/accuracy': 0.9899721145629883, 'train/loss': 0.033684421330690384, 'train/mean_average_precision': 0.3288627040656614, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.0454062893986702, 'validation/mean_average_precision': 0.23219470481186744, 'validation/num_examples': 43793, 'test/accuracy': 0.985499918460846, 'test/loss': 0.04803035035729408, 'test/mean_average_precision': 0.22858181276034606, 'test/num_examples': 43793, 'score': 1935.0809762477875, 'total_duration': 2795.2644288539886, 'accumulated_submission_time': 1935.0809762477875, 'accumulated_eval_time': 859.7944459915161, 'accumulated_logging_time': 0.149428129196167}
I0307 17:27:27.165025 139702104409856 logging_writer.py:48] [8792] accumulated_eval_time=859.794, accumulated_logging_time=0.149428, accumulated_submission_time=1935.08, global_step=8792, preemption_count=0, score=1935.08, test/accuracy=0.9855, test/loss=0.0480304, test/mean_average_precision=0.228582, test/num_examples=43793, total_duration=2795.26, train/accuracy=0.989972, train/loss=0.0336844, train/mean_average_precision=0.328863, validation/accuracy=0.986351, validation/loss=0.0454063, validation/mean_average_precision=0.232195, validation/num_examples=43793
I0307 17:27:29.052799 139702096017152 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.02643059566617012, loss=0.035010404884815216
I0307 17:27:50.102796 139702104409856 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.032922159880399704, loss=0.03595052286982536
I0307 17:28:12.086198 139702096017152 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03528893366456032, loss=0.037322938442230225
I0307 17:28:33.849969 139702104409856 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.04919637367129326, loss=0.03430034965276718
I0307 17:28:55.775472 139702096017152 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.02910417877137661, loss=0.03518735244870186
I0307 17:29:17.651278 139702104409856 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.022750793024897575, loss=0.032003507018089294
I0307 17:29:39.625074 139702096017152 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.025730637833476067, loss=0.038632214069366455
I0307 17:30:01.489775 139702104409856 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.04417383670806885, loss=0.03302853927016258
I0307 17:30:23.278727 139702096017152 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.037255316972732544, loss=0.03536921739578247
I0307 17:30:45.159934 139702104409856 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.03282207250595093, loss=0.03726974502205849
I0307 17:31:06.250282 139702096017152 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.033216845244169235, loss=0.03342820331454277
I0307 17:31:27.265050 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:32:43.896213 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:32:45.862454 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:32:47.765704 139843824272576 submission_runner.py:469] Time since start: 3115.88s, 	Step: 9896, 	{'train/accuracy': 0.9899682998657227, 'train/loss': 0.03352784737944603, 'train/mean_average_precision': 0.3195062179556214, 'validation/accuracy': 0.9865978360176086, 'validation/loss': 0.04521291330456734, 'validation/mean_average_precision': 0.245911743780941, 'validation/num_examples': 43793, 'test/accuracy': 0.9856582880020142, 'test/loss': 0.04790876433253288, 'test/mean_average_precision': 0.23827438733183215, 'test/num_examples': 43793, 'score': 2175.1418731212616, 'total_duration': 3115.875633239746, 'accumulated_submission_time': 2175.1418731212616, 'accumulated_eval_time': 940.2949657440186, 'accumulated_logging_time': 0.17014241218566895}
I0307 17:32:47.775592 139702104409856 logging_writer.py:48] [9896] accumulated_eval_time=940.295, accumulated_logging_time=0.170142, accumulated_submission_time=2175.14, global_step=9896, preemption_count=0, score=2175.14, test/accuracy=0.985658, test/loss=0.0479088, test/mean_average_precision=0.238274, test/num_examples=43793, total_duration=3115.88, train/accuracy=0.989968, train/loss=0.0335278, train/mean_average_precision=0.319506, validation/accuracy=0.986598, validation/loss=0.0452129, validation/mean_average_precision=0.245912, validation/num_examples=43793
I0307 17:32:48.840607 139702096017152 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.04575169458985329, loss=0.036553941667079926
I0307 17:33:10.837209 139702104409856 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.06292399764060974, loss=0.0347890630364418
I0307 17:33:32.861763 139702096017152 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.03789418563246727, loss=0.03628204017877579
I0307 17:33:54.786567 139702104409856 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.028365368023514748, loss=0.031047016382217407
I0307 17:34:17.185863 139702096017152 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.05423624441027641, loss=0.03115694783627987
I0307 17:34:39.187021 139702104409856 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03969455882906914, loss=0.03494107723236084
I0307 17:35:01.446960 139702096017152 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.03878292068839073, loss=0.03439828008413315
I0307 17:35:22.810326 139702104409856 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.035442277789115906, loss=0.032388970255851746
I0307 17:35:43.789896 139702096017152 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.04147503897547722, loss=0.033810827881097794
I0307 17:36:05.195904 139702104409856 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05304490774869919, loss=0.03501524403691292
I0307 17:36:26.526101 139702096017152 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.046739429235458374, loss=0.03035171888768673
I0307 17:36:47.797654 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:38:04.540020 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:38:06.451620 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:38:08.512492 139843824272576 submission_runner.py:469] Time since start: 3436.62s, 	Step: 10999, 	{'train/accuracy': 0.9902253746986389, 'train/loss': 0.03255723789334297, 'train/mean_average_precision': 0.36059278516434967, 'validation/accuracy': 0.9865081310272217, 'validation/loss': 0.04516172409057617, 'validation/mean_average_precision': 0.2468036560906089, 'validation/num_examples': 43793, 'test/accuracy': 0.9856081604957581, 'test/loss': 0.047820981591939926, 'test/mean_average_precision': 0.2407608601714161, 'test/num_examples': 43793, 'score': 2415.1224250793457, 'total_duration': 3436.622328042984, 'accumulated_submission_time': 2415.1224250793457, 'accumulated_eval_time': 1021.0095772743225, 'accumulated_logging_time': 0.1892995834350586}
I0307 17:38:08.523353 139702104409856 logging_writer.py:48] [10999] accumulated_eval_time=1021.01, accumulated_logging_time=0.1893, accumulated_submission_time=2415.12, global_step=10999, preemption_count=0, score=2415.12, test/accuracy=0.985608, test/loss=0.047821, test/mean_average_precision=0.240761, test/num_examples=43793, total_duration=3436.62, train/accuracy=0.990225, train/loss=0.0325572, train/mean_average_precision=0.360593, validation/accuracy=0.986508, validation/loss=0.0451617, validation/mean_average_precision=0.246804, validation/num_examples=43793
I0307 17:38:08.947571 139702096017152 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0490146242082119, loss=0.031193582341074944
I0307 17:38:29.944148 139702104409856 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03615637496113777, loss=0.03051094524562359
I0307 17:38:51.127327 139702096017152 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.05022241175174713, loss=0.030966589227318764
I0307 17:39:12.807551 139702104409856 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.03591509908437729, loss=0.028930820524692535
I0307 17:39:34.248639 139702096017152 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.05485586076974869, loss=0.028524484485387802
I0307 17:39:55.999821 139702104409856 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03801532834768295, loss=0.03215445578098297
I0307 17:40:17.621635 139702096017152 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.05304932966828346, loss=0.03348835930228233
I0307 17:40:39.381542 139702104409856 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.07003964483737946, loss=0.03384530544281006
I0307 17:41:00.589802 139702096017152 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.04886979982256889, loss=0.03455037996172905
I0307 17:41:21.897509 139702104409856 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.05356704816222191, loss=0.033812470734119415
I0307 17:41:43.463939 139702096017152 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.044174980372190475, loss=0.029752865433692932
I0307 17:42:05.130205 139702104409856 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.05552663654088974, loss=0.03360344097018242
I0307 17:42:08.514868 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:43:24.301324 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:43:26.267863 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:43:28.171472 139843824272576 submission_runner.py:469] Time since start: 3756.28s, 	Step: 12117, 	{'train/accuracy': 0.9902422428131104, 'train/loss': 0.03200624883174896, 'train/mean_average_precision': 0.36505012629759903, 'validation/accuracy': 0.986553966999054, 'validation/loss': 0.04547463729977608, 'validation/mean_average_precision': 0.2499866526674444, 'validation/num_examples': 43793, 'test/accuracy': 0.9856384992599487, 'test/loss': 0.04826817288994789, 'test/mean_average_precision': 0.2434509874460612, 'test/num_examples': 43793, 'score': 2655.077239751816, 'total_duration': 3756.2813346385956, 'accumulated_submission_time': 2655.077239751816, 'accumulated_eval_time': 1100.6659760475159, 'accumulated_logging_time': 0.2096247673034668}
I0307 17:43:28.181385 139702096017152 logging_writer.py:48] [12117] accumulated_eval_time=1100.67, accumulated_logging_time=0.209625, accumulated_submission_time=2655.08, global_step=12117, preemption_count=0, score=2655.08, test/accuracy=0.985638, test/loss=0.0482682, test/mean_average_precision=0.243451, test/num_examples=43793, total_duration=3756.28, train/accuracy=0.990242, train/loss=0.0320062, train/mean_average_precision=0.36505, validation/accuracy=0.986554, validation/loss=0.0454746, validation/mean_average_precision=0.249987, validation/num_examples=43793
I0307 17:43:46.299108 139702104409856 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.05115385726094246, loss=0.03353886306285858
I0307 17:44:07.926920 139702096017152 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.06545684486627579, loss=0.03486284613609314
I0307 17:44:30.161532 139702104409856 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.06408694386482239, loss=0.031360842287540436
I0307 17:44:51.851915 139702096017152 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.06110408529639244, loss=0.036592260003089905
I0307 17:45:13.344586 139702104409856 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.06224199756979942, loss=0.02914552018046379
I0307 17:45:34.445687 139702096017152 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.060440804809331894, loss=0.03477874770760536
I0307 17:45:55.592889 139702104409856 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.07703056186437607, loss=0.03068208135664463
I0307 17:46:17.085500 139702096017152 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.05757235363125801, loss=0.03281194344162941
I0307 17:46:38.463395 139702104409856 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.05487724766135216, loss=0.03091452457010746
I0307 17:46:59.890271 139702096017152 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.055182185024023056, loss=0.02940627932548523
I0307 17:47:21.441840 139702104409856 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.07526717334985733, loss=0.03011636808514595
I0307 17:47:28.230043 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:48:42.067378 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:48:44.021625 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:48:45.914119 139843824272576 submission_runner.py:469] Time since start: 4074.02s, 	Step: 13233, 	{'train/accuracy': 0.99032062292099, 'train/loss': 0.03178824111819267, 'train/mean_average_precision': 0.3758721021864116, 'validation/accuracy': 0.9866928458213806, 'validation/loss': 0.044864654541015625, 'validation/mean_average_precision': 0.2594236899803729, 'validation/num_examples': 43793, 'test/accuracy': 0.9858676195144653, 'test/loss': 0.047269709408283234, 'test/mean_average_precision': 0.2508124205066057, 'test/num_examples': 43793, 'score': 2895.0889592170715, 'total_duration': 4074.0239820480347, 'accumulated_submission_time': 2895.0889592170715, 'accumulated_eval_time': 1178.3498539924622, 'accumulated_logging_time': 0.22901105880737305}
I0307 17:48:45.924426 139702096017152 logging_writer.py:48] [13233] accumulated_eval_time=1178.35, accumulated_logging_time=0.229011, accumulated_submission_time=2895.09, global_step=13233, preemption_count=0, score=2895.09, test/accuracy=0.985868, test/loss=0.0472697, test/mean_average_precision=0.250812, test/num_examples=43793, total_duration=4074.02, train/accuracy=0.990321, train/loss=0.0317882, train/mean_average_precision=0.375872, validation/accuracy=0.986693, validation/loss=0.0448647, validation/mean_average_precision=0.259424, validation/num_examples=43793
I0307 17:49:00.560686 139702104409856 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.05724643915891647, loss=0.0310983844101429
I0307 17:49:22.263349 139702096017152 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.05155806243419647, loss=0.03251301124691963
I0307 17:49:43.775978 139702104409856 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.047508154064416885, loss=0.029726622626185417
I0307 17:50:05.371109 139702096017152 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.054028432816267014, loss=0.03508703038096428
I0307 17:50:26.912639 139702104409856 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.04595639556646347, loss=0.029120607301592827
I0307 17:50:48.350035 139702096017152 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0806843638420105, loss=0.03025357611477375
I0307 17:51:10.146902 139702104409856 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.07615044713020325, loss=0.03162591531872749
I0307 17:51:31.597009 139702096017152 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.08419374376535416, loss=0.03000538796186447
I0307 17:51:52.991012 139702104409856 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.047341492027044296, loss=0.030257446691393852
I0307 17:52:14.499584 139702096017152 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.051747847348451614, loss=0.03256852924823761
I0307 17:52:35.964953 139702104409856 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.07510235160589218, loss=0.028484823182225227
I0307 17:52:45.940092 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:53:59.817983 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:54:01.789344 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:54:03.720563 139843824272576 submission_runner.py:469] Time since start: 4391.83s, 	Step: 14348, 	{'train/accuracy': 0.9909123778343201, 'train/loss': 0.029886435717344284, 'train/mean_average_precision': 0.4046758114023122, 'validation/accuracy': 0.986763060092926, 'validation/loss': 0.04458058252930641, 'validation/mean_average_precision': 0.26890911444858634, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.047543738037347794, 'test/mean_average_precision': 0.2539929108283998, 'test/num_examples': 43793, 'score': 3135.0638337135315, 'total_duration': 4391.830424070358, 'accumulated_submission_time': 3135.0638337135315, 'accumulated_eval_time': 1256.1301290988922, 'accumulated_logging_time': 0.24869632720947266}
I0307 17:54:03.731852 139702096017152 logging_writer.py:48] [14348] accumulated_eval_time=1256.13, accumulated_logging_time=0.248696, accumulated_submission_time=3135.06, global_step=14348, preemption_count=0, score=3135.06, test/accuracy=0.985881, test/loss=0.0475437, test/mean_average_precision=0.253993, test/num_examples=43793, total_duration=4391.83, train/accuracy=0.990912, train/loss=0.0298864, train/mean_average_precision=0.404676, validation/accuracy=0.986763, validation/loss=0.0445806, validation/mean_average_precision=0.268909, validation/num_examples=43793
I0307 17:54:15.270681 139702104409856 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0643581673502922, loss=0.0305387731641531
I0307 17:54:36.965404 139702096017152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.06230514124035835, loss=0.030076367780566216
I0307 17:54:58.326780 139702104409856 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.06115089729428291, loss=0.03156599402427673
I0307 17:55:19.828350 139702096017152 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.058923352509737015, loss=0.032831549644470215
I0307 17:55:41.275500 139702104409856 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.0943106934428215, loss=0.03319651260972023
I0307 17:56:02.599255 139702096017152 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.06967855989933014, loss=0.03823721781373024
I0307 17:56:23.952696 139702104409856 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.068354532122612, loss=0.02927786111831665
I0307 17:56:45.531930 139702096017152 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0704028531908989, loss=0.0316675528883934
I0307 17:57:07.377879 139702104409856 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05989542603492737, loss=0.030980395153164864
I0307 17:57:28.818748 139702096017152 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.09872149676084518, loss=0.031075388193130493
I0307 17:57:50.363296 139702104409856 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.06406252831220627, loss=0.03086456097662449
I0307 17:58:03.818253 139843824272576 spec.py:321] Evaluating on the training split.
I0307 17:59:22.652028 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 17:59:24.656073 139843824272576 spec.py:349] Evaluating on the test split.
I0307 17:59:26.592825 139843824272576 submission_runner.py:469] Time since start: 4714.70s, 	Step: 15464, 	{'train/accuracy': 0.9907843470573425, 'train/loss': 0.030587248504161835, 'train/mean_average_precision': 0.38778817703076657, 'validation/accuracy': 0.9866079688072205, 'validation/loss': 0.044562287628650665, 'validation/mean_average_precision': 0.2592255149954002, 'validation/num_examples': 43793, 'test/accuracy': 0.9858006238937378, 'test/loss': 0.04721960052847862, 'test/mean_average_precision': 0.2562299253963582, 'test/num_examples': 43793, 'score': 3375.1132776737213, 'total_duration': 4714.702702045441, 'accumulated_submission_time': 3375.1132776737213, 'accumulated_eval_time': 1338.9045150279999, 'accumulated_logging_time': 0.2691154479980469}
I0307 17:59:26.603229 139702096017152 logging_writer.py:48] [15464] accumulated_eval_time=1338.9, accumulated_logging_time=0.269115, accumulated_submission_time=3375.11, global_step=15464, preemption_count=0, score=3375.11, test/accuracy=0.985801, test/loss=0.0472196, test/mean_average_precision=0.25623, test/num_examples=43793, total_duration=4714.7, train/accuracy=0.990784, train/loss=0.0305872, train/mean_average_precision=0.387788, validation/accuracy=0.986608, validation/loss=0.0445623, validation/mean_average_precision=0.259226, validation/num_examples=43793
I0307 17:59:34.594424 139702104409856 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.06808991730213165, loss=0.03452266752719879
I0307 17:59:56.304373 139702096017152 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.08913502842187881, loss=0.03432556241750717
I0307 18:00:18.170847 139702104409856 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.10128887742757797, loss=0.02938450686633587
I0307 18:00:40.002163 139702096017152 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.06380172818899155, loss=0.029832063242793083
I0307 18:01:01.600970 139702104409856 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.08275156468153, loss=0.03497476875782013
I0307 18:01:23.000866 139702096017152 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.07073599100112915, loss=0.031840287148952484
I0307 18:01:44.547817 139702104409856 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.06858920305967331, loss=0.027756812050938606
I0307 18:02:06.305805 139702096017152 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.0849403515458107, loss=0.03148571401834488
I0307 18:02:27.844109 139702104409856 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.08729307353496552, loss=0.027560295537114143
I0307 18:02:49.393931 139702096017152 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0846165344119072, loss=0.03236371651291847
I0307 18:03:11.073125 139702104409856 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.06535691022872925, loss=0.028471767902374268
I0307 18:03:26.740993 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:04:39.534138 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:04:41.533379 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:04:43.582669 139843824272576 submission_runner.py:469] Time since start: 5031.69s, 	Step: 16572, 	{'train/accuracy': 0.9912809133529663, 'train/loss': 0.028598172590136528, 'train/mean_average_precision': 0.4450888484206308, 'validation/accuracy': 0.9867825508117676, 'validation/loss': 0.04463914781808853, 'validation/mean_average_precision': 0.2656126200153692, 'validation/num_examples': 43793, 'test/accuracy': 0.9859177470207214, 'test/loss': 0.047120094299316406, 'test/mean_average_precision': 0.2642979796032486, 'test/num_examples': 43793, 'score': 3615.212069272995, 'total_duration': 5031.692683458328, 'accumulated_submission_time': 3615.212069272995, 'accumulated_eval_time': 1415.7461564540863, 'accumulated_logging_time': 0.29024839401245117}
I0307 18:04:43.593956 139702096017152 logging_writer.py:48] [16572] accumulated_eval_time=1415.75, accumulated_logging_time=0.290248, accumulated_submission_time=3615.21, global_step=16572, preemption_count=0, score=3615.21, test/accuracy=0.985918, test/loss=0.0471201, test/mean_average_precision=0.264298, test/num_examples=43793, total_duration=5031.69, train/accuracy=0.991281, train/loss=0.0285982, train/mean_average_precision=0.445089, validation/accuracy=0.986783, validation/loss=0.0446391, validation/mean_average_precision=0.265613, validation/num_examples=43793
I0307 18:04:49.945203 139702104409856 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.09614710509777069, loss=0.03329971805214882
I0307 18:05:11.321202 139702096017152 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.06540782004594803, loss=0.02879238687455654
I0307 18:05:32.911947 139702104409856 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.0904017835855484, loss=0.03533589839935303
I0307 18:05:54.876294 139702096017152 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.07717245817184448, loss=0.028154397383332253
I0307 18:06:17.075583 139702104409856 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0761275440454483, loss=0.02853134088218212
I0307 18:06:39.112125 139702096017152 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.07537886500358582, loss=0.030283045023679733
I0307 18:07:00.597249 139702104409856 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.0779375210404396, loss=0.02975035272538662
I0307 18:07:22.196113 139702096017152 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.062036436051130295, loss=0.02946729026734829
I0307 18:07:44.286848 139702104409856 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.07954126596450806, loss=0.03266655653715134
I0307 18:08:06.109283 139702096017152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.07966578006744385, loss=0.025849396362900734
I0307 18:08:27.871083 139702104409856 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.11192452907562256, loss=0.02809280902147293
I0307 18:08:43.721393 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:10:00.956932 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:10:03.116653 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:10:05.011501 139843824272576 submission_runner.py:469] Time since start: 5353.12s, 	Step: 17676, 	{'train/accuracy': 0.9909286499023438, 'train/loss': 0.029505448415875435, 'train/mean_average_precision': 0.42648156214677246, 'validation/accuracy': 0.986790657043457, 'validation/loss': 0.04442321136593819, 'validation/mean_average_precision': 0.26463555099577357, 'validation/num_examples': 43793, 'test/accuracy': 0.9859021306037903, 'test/loss': 0.04707392305135727, 'test/mean_average_precision': 0.26354958654526683, 'test/num_examples': 43793, 'score': 3855.302666902542, 'total_duration': 5353.121511936188, 'accumulated_submission_time': 3855.302666902542, 'accumulated_eval_time': 1497.0362107753754, 'accumulated_logging_time': 0.31109070777893066}
I0307 18:10:05.024499 139702096017152 logging_writer.py:48] [17676] accumulated_eval_time=1497.04, accumulated_logging_time=0.311091, accumulated_submission_time=3855.3, global_step=17676, preemption_count=0, score=3855.3, test/accuracy=0.985902, test/loss=0.0470739, test/mean_average_precision=0.26355, test/num_examples=43793, total_duration=5353.12, train/accuracy=0.990929, train/loss=0.0295054, train/mean_average_precision=0.426482, validation/accuracy=0.986791, validation/loss=0.0444232, validation/mean_average_precision=0.264636, validation/num_examples=43793
I0307 18:10:10.471506 139702104409856 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.11921067535877228, loss=0.03226345777511597
I0307 18:10:32.571433 139702096017152 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.08778098225593567, loss=0.03052266128361225
I0307 18:10:54.605780 139702104409856 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.08679263293743134, loss=0.030603686347603798
I0307 18:11:16.566335 139702096017152 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.12669946253299713, loss=0.030508223921060562
I0307 18:11:38.464981 139702104409856 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.08101274073123932, loss=0.030942216515541077
I0307 18:12:00.067690 139702096017152 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.08493052423000336, loss=0.029889656230807304
I0307 18:12:21.793820 139702104409856 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.11722862720489502, loss=0.03051629848778248
I0307 18:12:43.148328 139702096017152 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.08572504669427872, loss=0.031834039837121964
I0307 18:13:04.566775 139702104409856 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.09327982366085052, loss=0.028678864240646362
I0307 18:13:26.015549 139702096017152 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.09792803227901459, loss=0.028250547125935555
I0307 18:13:48.007921 139702104409856 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.08226186782121658, loss=0.028484424576163292
I0307 18:14:05.145427 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:15:20.988602 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:15:23.009839 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:15:24.900846 139843824272576 submission_runner.py:469] Time since start: 5673.01s, 	Step: 18780, 	{'train/accuracy': 0.9910614490509033, 'train/loss': 0.029216507449746132, 'train/mean_average_precision': 0.42080688829826324, 'validation/accuracy': 0.9866063594818115, 'validation/loss': 0.04475586488842964, 'validation/mean_average_precision': 0.26023441091744975, 'validation/num_examples': 43793, 'test/accuracy': 0.9858258962631226, 'test/loss': 0.047403931617736816, 'test/mean_average_precision': 0.2607284701018005, 'test/num_examples': 43793, 'score': 4095.385016679764, 'total_duration': 5673.010764122009, 'accumulated_submission_time': 4095.385016679764, 'accumulated_eval_time': 1576.7914912700653, 'accumulated_logging_time': 0.33441829681396484}
I0307 18:15:24.912935 139702096017152 logging_writer.py:48] [18780] accumulated_eval_time=1576.79, accumulated_logging_time=0.334418, accumulated_submission_time=4095.39, global_step=18780, preemption_count=0, score=4095.39, test/accuracy=0.985826, test/loss=0.0474039, test/mean_average_precision=0.260728, test/num_examples=43793, total_duration=5673.01, train/accuracy=0.991061, train/loss=0.0292165, train/mean_average_precision=0.420807, validation/accuracy=0.986606, validation/loss=0.0447559, validation/mean_average_precision=0.260234, validation/num_examples=43793
I0307 18:15:29.439220 139702104409856 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.08005020767450333, loss=0.027139512822031975
I0307 18:15:50.496546 139702096017152 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.08024077117443085, loss=0.02779977396130562
I0307 18:16:12.406708 139702104409856 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.08412104845046997, loss=0.03189154714345932
I0307 18:16:34.167835 139702096017152 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.09700414538383484, loss=0.027918880805373192
I0307 18:16:56.172482 139702104409856 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.10187353938817978, loss=0.029134195297956467
I0307 18:17:17.683581 139702096017152 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.10525639355182648, loss=0.029213881120085716
I0307 18:17:39.672994 139702104409856 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07691086083650589, loss=0.02682952582836151
I0307 18:18:01.444140 139702096017152 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.10863295197486877, loss=0.028745057061314583
I0307 18:18:23.379008 139702104409856 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.11530071496963501, loss=0.0327664390206337
I0307 18:18:45.036840 139702096017152 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.15240679681301117, loss=0.03087499551475048
I0307 18:19:06.519210 139702104409856 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.08740916103124619, loss=0.031836144626140594
I0307 18:19:25.099584 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:20:37.433885 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:20:39.416783 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:20:41.350844 139843824272576 submission_runner.py:469] Time since start: 5989.46s, 	Step: 19886, 	{'train/accuracy': 0.9914048314094543, 'train/loss': 0.028049286454916, 'train/mean_average_precision': 0.4539812954122756, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.044580765068531036, 'validation/mean_average_precision': 0.2738403266025462, 'validation/num_examples': 43793, 'test/accuracy': 0.9860074520111084, 'test/loss': 0.04707479849457741, 'test/mean_average_precision': 0.26638148722373917, 'test/num_examples': 43793, 'score': 4335.533841133118, 'total_duration': 5989.460833787918, 'accumulated_submission_time': 4335.533841133118, 'accumulated_eval_time': 1653.0426771640778, 'accumulated_logging_time': 0.3563382625579834}
I0307 18:20:41.363169 139702096017152 logging_writer.py:48] [19886] accumulated_eval_time=1653.04, accumulated_logging_time=0.356338, accumulated_submission_time=4335.53, global_step=19886, preemption_count=0, score=4335.53, test/accuracy=0.986007, test/loss=0.0470748, test/mean_average_precision=0.266381, test/num_examples=43793, total_duration=5989.46, train/accuracy=0.991405, train/loss=0.0280493, train/mean_average_precision=0.453981, validation/accuracy=0.986742, validation/loss=0.0445808, validation/mean_average_precision=0.27384, validation/num_examples=43793
I0307 18:20:44.650564 139702104409856 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.07379547506570816, loss=0.028721585869789124
I0307 18:21:06.644653 139702096017152 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.08432828634977341, loss=0.029931649565696716
I0307 18:21:28.460316 139702104409856 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.09495475888252258, loss=0.0293793473392725
I0307 18:21:50.064579 139702096017152 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.10570455342531204, loss=0.029840655624866486
I0307 18:22:11.954578 139702104409856 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.07959794998168945, loss=0.029308583587408066
I0307 18:22:33.788353 139702096017152 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.11267176270484924, loss=0.03402852267026901
I0307 18:22:55.456540 139702104409856 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.07268746942281723, loss=0.02652682177722454
I0307 18:23:17.063550 139702096017152 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.07186998426914215, loss=0.02818262204527855
I0307 18:23:38.987846 139702104409856 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.10778078436851501, loss=0.02789897657930851
I0307 18:24:00.719624 139702096017152 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.12070855498313904, loss=0.028432250022888184
I0307 18:24:22.355259 139702104409856 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.08177661895751953, loss=0.027860790491104126
I0307 18:24:41.387938 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:25:59.975715 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:26:02.096386 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:26:04.130722 139843824272576 submission_runner.py:469] Time since start: 6312.24s, 	Step: 20988, 	{'train/accuracy': 0.9912294745445251, 'train/loss': 0.028580479323863983, 'train/mean_average_precision': 0.4343371213016963, 'validation/accuracy': 0.9867683053016663, 'validation/loss': 0.04479736462235451, 'validation/mean_average_precision': 0.2746105046784574, 'validation/num_examples': 43793, 'test/accuracy': 0.9859354496002197, 'test/loss': 0.04769391939043999, 'test/mean_average_precision': 0.26374140606351926, 'test/num_examples': 43793, 'score': 4575.517915248871, 'total_duration': 6312.240740060806, 'accumulated_submission_time': 4575.517915248871, 'accumulated_eval_time': 1735.7854135036469, 'accumulated_logging_time': 0.3794248104095459}
I0307 18:26:04.142180 139702096017152 logging_writer.py:48] [20988] accumulated_eval_time=1735.79, accumulated_logging_time=0.379425, accumulated_submission_time=4575.52, global_step=20988, preemption_count=0, score=4575.52, test/accuracy=0.985935, test/loss=0.0476939, test/mean_average_precision=0.263741, test/num_examples=43793, total_duration=6312.24, train/accuracy=0.991229, train/loss=0.0285805, train/mean_average_precision=0.434337, validation/accuracy=0.986768, validation/loss=0.0447974, validation/mean_average_precision=0.274611, validation/num_examples=43793
I0307 18:26:06.947746 139702104409856 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.07924086600542068, loss=0.028467366471886635
I0307 18:26:30.597833 139702096017152 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.09663570672273636, loss=0.030010361224412918
I0307 18:26:52.265366 139702104409856 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.0806419625878334, loss=0.02905876561999321
I0307 18:27:15.707585 139702096017152 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.14623962342739105, loss=0.032510772347450256
I0307 18:27:39.506956 139702104409856 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.0950462594628334, loss=0.026966815814375877
I0307 18:28:01.154943 139702096017152 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.10624679177999496, loss=0.027846848592162132
I0307 18:28:24.668585 139702104409856 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.09205082803964615, loss=0.028719153255224228
I0307 18:28:48.302626 139702096017152 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.10134486109018326, loss=0.03192673996090889
I0307 18:29:10.106784 139702104409856 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.0892510861158371, loss=0.027826981619000435
I0307 18:29:33.440505 139702096017152 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.10528688132762909, loss=0.028744781389832497
I0307 18:29:55.190540 139702104409856 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.09671374410390854, loss=0.028195152059197426
I0307 18:30:04.200596 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:31:22.309026 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:31:24.492616 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:31:26.533308 139843824272576 submission_runner.py:469] Time since start: 6634.64s, 	Step: 22034, 	{'train/accuracy': 0.9917982816696167, 'train/loss': 0.02672702819108963, 'train/mean_average_precision': 0.48281166940679643, 'validation/accuracy': 0.9868012070655823, 'validation/loss': 0.04453675076365471, 'validation/mean_average_precision': 0.27392685859045196, 'validation/num_examples': 43793, 'test/accuracy': 0.9859341979026794, 'test/loss': 0.047351229935884476, 'test/mean_average_precision': 0.26296631713252744, 'test/num_examples': 43793, 'score': 4815.536692857742, 'total_duration': 6634.643180608749, 'accumulated_submission_time': 4815.536692857742, 'accumulated_eval_time': 1818.117932319641, 'accumulated_logging_time': 0.4002974033355713}
I0307 18:31:26.544826 139702096017152 logging_writer.py:48] [22034] accumulated_eval_time=1818.12, accumulated_logging_time=0.400297, accumulated_submission_time=4815.54, global_step=22034, preemption_count=0, score=4815.54, test/accuracy=0.985934, test/loss=0.0473512, test/mean_average_precision=0.262966, test/num_examples=43793, total_duration=6634.64, train/accuracy=0.991798, train/loss=0.026727, train/mean_average_precision=0.482812, validation/accuracy=0.986801, validation/loss=0.0445368, validation/mean_average_precision=0.273927, validation/num_examples=43793
I0307 18:31:43.800777 139702104409856 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.16494455933570862, loss=0.03403187915682793
I0307 18:32:06.120493 139702096017152 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.09505452960729599, loss=0.026939405128359795
I0307 18:32:29.751742 139702104409856 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.10346967726945877, loss=0.027592331171035767
I0307 18:32:50.960156 139702096017152 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.0904887244105339, loss=0.02806766703724861
I0307 18:33:14.299510 139702104409856 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.11364371329545975, loss=0.026544075459241867
I0307 18:33:38.340832 139702096017152 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.0937146544456482, loss=0.028755275532603264
I0307 18:34:00.220041 139702104409856 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.19239716231822968, loss=0.030282821506261826
I0307 18:34:23.840078 139702096017152 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.09502088278532028, loss=0.027615400031208992
I0307 18:34:47.610818 139702104409856 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.10279650241136551, loss=0.027580777183175087
I0307 18:35:09.217032 139702096017152 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.10555977374315262, loss=0.026961008086800575
I0307 18:35:26.542416 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:36:42.658892 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:36:44.775471 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:36:46.811244 139843824272576 submission_runner.py:469] Time since start: 6954.92s, 	Step: 23072, 	{'train/accuracy': 0.9917907118797302, 'train/loss': 0.026596948504447937, 'train/mean_average_precision': 0.48982699993218104, 'validation/accuracy': 0.9867756366729736, 'validation/loss': 0.04485158994793892, 'validation/mean_average_precision': 0.2710567277612556, 'validation/num_examples': 43793, 'test/accuracy': 0.9859097599983215, 'test/loss': 0.047569870948791504, 'test/mean_average_precision': 0.25911274933743883, 'test/num_examples': 43793, 'score': 5055.495976686478, 'total_duration': 6954.921161651611, 'accumulated_submission_time': 5055.495976686478, 'accumulated_eval_time': 1898.386614561081, 'accumulated_logging_time': 0.42129945755004883}
I0307 18:36:46.823801 139702104409856 logging_writer.py:48] [23072] accumulated_eval_time=1898.39, accumulated_logging_time=0.421299, accumulated_submission_time=5055.5, global_step=23072, preemption_count=0, score=5055.5, test/accuracy=0.98591, test/loss=0.0475699, test/mean_average_precision=0.259113, test/num_examples=43793, total_duration=6954.92, train/accuracy=0.991791, train/loss=0.0265969, train/mean_average_precision=0.489827, validation/accuracy=0.986776, validation/loss=0.0448516, validation/mean_average_precision=0.271057, validation/num_examples=43793
I0307 18:36:53.103561 139702096017152 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.1449577808380127, loss=0.029340002685785294
I0307 18:37:16.573557 139702104409856 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.1123233288526535, loss=0.02871537394821644
I0307 18:37:37.918986 139702096017152 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.08403579145669937, loss=0.02574160508811474
I0307 18:38:01.718742 139702104409856 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.11716905236244202, loss=0.030422469601035118
I0307 18:38:25.216984 139702096017152 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.11887944489717484, loss=0.030970409512519836
I0307 18:38:46.770111 139702104409856 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.09824775904417038, loss=0.028401006013154984
I0307 18:39:10.193575 139702096017152 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.10790243744850159, loss=0.03188541531562805
I0307 18:39:33.638693 139702104409856 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.12148396670818329, loss=0.029563723132014275
I0307 18:39:55.595257 139702096017152 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.09196916967630386, loss=0.026017777621746063
I0307 18:40:19.129209 139702104409856 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.10198734700679779, loss=0.02748056687414646
I0307 18:40:41.187143 139702096017152 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.11640357226133347, loss=0.02566506713628769
I0307 18:40:46.880314 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:42:05.225927 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:42:07.339210 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:42:09.431206 139843824272576 submission_runner.py:469] Time since start: 7277.54s, 	Step: 24119, 	{'train/accuracy': 0.991434633731842, 'train/loss': 0.027600200846791267, 'train/mean_average_precision': 0.45611483435552574, 'validation/accuracy': 0.986748456954956, 'validation/loss': 0.0447673499584198, 'validation/mean_average_precision': 0.2722016577472177, 'validation/num_examples': 43793, 'test/accuracy': 0.9859842658042908, 'test/loss': 0.04744948819279671, 'test/mean_average_precision': 0.26159257253742085, 'test/num_examples': 43793, 'score': 5295.512573003769, 'total_duration': 7277.541084766388, 'accumulated_submission_time': 5295.512573003769, 'accumulated_eval_time': 1980.937319278717, 'accumulated_logging_time': 0.4444100856781006}
I0307 18:42:09.442598 139702104409856 logging_writer.py:48] [24119] accumulated_eval_time=1980.94, accumulated_logging_time=0.44441, accumulated_submission_time=5295.51, global_step=24119, preemption_count=0, score=5295.51, test/accuracy=0.985984, test/loss=0.0474495, test/mean_average_precision=0.261593, test/num_examples=43793, total_duration=7277.54, train/accuracy=0.991435, train/loss=0.0276002, train/mean_average_precision=0.456115, validation/accuracy=0.986748, validation/loss=0.0447673, validation/mean_average_precision=0.272202, validation/num_examples=43793
I0307 18:42:27.176585 139702096017152 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.09193596243858337, loss=0.025257911533117294
I0307 18:42:51.015881 139702104409856 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.10809050500392914, loss=0.028975678607821465
I0307 18:43:14.725073 139702096017152 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.10054013878107071, loss=0.02961055189371109
I0307 18:43:36.676140 139702104409856 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.14088164269924164, loss=0.02602616511285305
I0307 18:44:00.283015 139702096017152 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.10509389638900757, loss=0.02803780511021614
I0307 18:44:23.829683 139702104409856 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.17727218568325043, loss=0.030840003862977028
I0307 18:44:45.773812 139702096017152 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.15004153549671173, loss=0.026860984042286873
I0307 18:45:09.357451 139702104409856 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.0925077274441719, loss=0.025116752833127975
I0307 18:45:32.946449 139702096017152 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.10240287333726883, loss=0.03046499192714691
I0307 18:45:54.798969 139702104409856 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.11195962131023407, loss=0.024046137928962708
I0307 18:46:09.596455 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:47:25.264305 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:47:27.212379 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:47:29.125372 139843824272576 submission_runner.py:469] Time since start: 7597.24s, 	Step: 25157, 	{'train/accuracy': 0.9919379353523254, 'train/loss': 0.02624361589550972, 'train/mean_average_precision': 0.489644977739022, 'validation/accuracy': 0.9867597818374634, 'validation/loss': 0.04496688023209572, 'validation/mean_average_precision': 0.2719349561106214, 'validation/num_examples': 43793, 'test/accuracy': 0.985878586769104, 'test/loss': 0.04765566438436508, 'test/mean_average_precision': 0.2586091683531267, 'test/num_examples': 43793, 'score': 5535.626770973206, 'total_duration': 7597.235291481018, 'accumulated_submission_time': 5535.626770973206, 'accumulated_eval_time': 2060.4661214351654, 'accumulated_logging_time': 0.46613121032714844}
I0307 18:47:29.137771 139702096017152 logging_writer.py:48] [25157] accumulated_eval_time=2060.47, accumulated_logging_time=0.466131, accumulated_submission_time=5535.63, global_step=25157, preemption_count=0, score=5535.63, test/accuracy=0.985879, test/loss=0.0476557, test/mean_average_precision=0.258609, test/num_examples=43793, total_duration=7597.24, train/accuracy=0.991938, train/loss=0.0262436, train/mean_average_precision=0.489645, validation/accuracy=0.98676, validation/loss=0.0449669, validation/mean_average_precision=0.271935, validation/num_examples=43793
I0307 18:47:38.757910 139702104409856 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.1972413808107376, loss=0.029311776161193848
I0307 18:48:03.017594 139702096017152 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.12712617218494415, loss=0.02987186424434185
I0307 18:48:25.037175 139702104409856 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.11088310182094574, loss=0.02601170353591442
I0307 18:48:49.358159 139702096017152 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.13271859288215637, loss=0.02744240313768387
I0307 18:49:13.640933 139702104409856 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.11715814471244812, loss=0.028221528977155685
I0307 18:49:35.256222 139702096017152 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.10866302996873856, loss=0.02503116987645626
I0307 18:49:59.593019 139702104409856 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.10874079912900925, loss=0.025364883244037628
I0307 18:50:23.630683 139702096017152 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.12435388565063477, loss=0.02644495666027069
I0307 18:50:45.902588 139702104409856 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.1030021607875824, loss=0.028580743819475174
I0307 18:51:10.234745 139702096017152 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.11721112579107285, loss=0.025551889091730118
I0307 18:51:29.276970 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:53:06.336684 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:53:11.745791 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:53:19.438887 139843824272576 submission_runner.py:469] Time since start: 7947.55s, 	Step: 26189, 	{'train/accuracy': 0.9922596216201782, 'train/loss': 0.0248671043664217, 'train/mean_average_precision': 0.5262551192935045, 'validation/accuracy': 0.9868000149726868, 'validation/loss': 0.04518637806177139, 'validation/mean_average_precision': 0.2760459917216366, 'validation/num_examples': 43793, 'test/accuracy': 0.9858697056770325, 'test/loss': 0.04804248735308647, 'test/mean_average_precision': 0.2641128503355187, 'test/num_examples': 43793, 'score': 5775.725816726685, 'total_duration': 7947.548739433289, 'accumulated_submission_time': 5775.725816726685, 'accumulated_eval_time': 2170.6278228759766, 'accumulated_logging_time': 0.48775815963745117}
I0307 18:53:19.451038 139702104409856 logging_writer.py:48] [26189] accumulated_eval_time=2170.63, accumulated_logging_time=0.487758, accumulated_submission_time=5775.73, global_step=26189, preemption_count=0, score=5775.73, test/accuracy=0.98587, test/loss=0.0480425, test/mean_average_precision=0.264113, test/num_examples=43793, total_duration=7947.55, train/accuracy=0.99226, train/loss=0.0248671, train/mean_average_precision=0.526255, validation/accuracy=0.9868, validation/loss=0.0451864, validation/mean_average_precision=0.276046, validation/num_examples=43793
I0307 18:53:22.126640 139702096017152 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.09966807067394257, loss=0.02753647416830063
I0307 18:53:46.321820 139702104409856 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.13256457448005676, loss=0.029762089252471924
I0307 18:54:10.358966 139702096017152 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.10524196922779083, loss=0.025368405506014824
I0307 18:54:32.080473 139702104409856 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.1136823371052742, loss=0.02844051830470562
I0307 18:54:56.149111 139702096017152 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.11646987497806549, loss=0.024785995483398438
I0307 18:55:19.878000 139702104409856 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.11106225848197937, loss=0.026159251108765602
I0307 18:55:41.862041 139702096017152 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.10084255784749985, loss=0.025086700916290283
I0307 18:56:05.762354 139702104409856 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.10640814900398254, loss=0.0252988301217556
I0307 18:56:29.413941 139702096017152 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.11700501292943954, loss=0.027878237888216972
I0307 18:56:53.750783 139702104409856 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.13063614070415497, loss=0.025103859603405
I0307 18:57:17.936205 139702096017152 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.10321928560733795, loss=0.024252522736787796
I0307 18:57:19.453072 139843824272576 spec.py:321] Evaluating on the training split.
I0307 18:59:20.375331 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 18:59:27.501430 139843824272576 spec.py:349] Evaluating on the test split.
I0307 18:59:45.651233 139843824272576 submission_runner.py:469] Time since start: 8333.76s, 	Step: 27208, 	{'train/accuracy': 0.9919833540916443, 'train/loss': 0.025941427797079086, 'train/mean_average_precision': 0.5034283641119102, 'validation/accuracy': 0.9866704940795898, 'validation/loss': 0.045347534120082855, 'validation/mean_average_precision': 0.2740679644359419, 'validation/num_examples': 43793, 'test/accuracy': 0.9858124256134033, 'test/loss': 0.048293180763721466, 'test/mean_average_precision': 0.26285946522114834, 'test/num_examples': 43793, 'score': 6015.687978506088, 'total_duration': 8333.761028766632, 'accumulated_submission_time': 6015.687978506088, 'accumulated_eval_time': 2316.8257219791412, 'accumulated_logging_time': 0.509462833404541}
I0307 18:59:45.666812 139702104409856 logging_writer.py:48] [27208] accumulated_eval_time=2316.83, accumulated_logging_time=0.509463, accumulated_submission_time=6015.69, global_step=27208, preemption_count=0, score=6015.69, test/accuracy=0.985812, test/loss=0.0482932, test/mean_average_precision=0.262859, test/num_examples=43793, total_duration=8333.76, train/accuracy=0.991983, train/loss=0.0259414, train/mean_average_precision=0.503428, validation/accuracy=0.98667, validation/loss=0.0453475, validation/mean_average_precision=0.274068, validation/num_examples=43793
I0307 19:00:09.878720 139702096017152 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.1333095282316208, loss=0.0277533158659935
I0307 19:00:34.985871 139702104409856 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.13730627298355103, loss=0.022999832406640053
I0307 19:00:56.871830 139702096017152 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.10679630935192108, loss=0.022863540798425674
I0307 19:01:21.816160 139702104409856 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.11275244504213333, loss=0.023659702390432358
I0307 19:01:47.070891 139702096017152 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.14863309264183044, loss=0.027856430038809776
I0307 19:02:09.105536 139702104409856 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.16342364251613617, loss=0.031259965151548386
I0307 19:02:34.133843 139702096017152 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.11398401111364365, loss=0.02775285206735134
I0307 19:02:59.512612 139702104409856 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.114339180290699, loss=0.02429201826453209
I0307 19:03:20.925800 139702096017152 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.12938468158245087, loss=0.025343183428049088
I0307 19:03:45.863776 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:05:04.888944 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:05:06.909519 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:05:08.901710 139843824272576 submission_runner.py:469] Time since start: 8657.01s, 	Step: 28199, 	{'train/accuracy': 0.9921227097511292, 'train/loss': 0.025318870320916176, 'train/mean_average_precision': 0.5220592412870835, 'validation/accuracy': 0.9866359829902649, 'validation/loss': 0.04564915597438812, 'validation/mean_average_precision': 0.2725394367112026, 'validation/num_examples': 43793, 'test/accuracy': 0.9858288764953613, 'test/loss': 0.04845212399959564, 'test/mean_average_precision': 0.2653679971134227, 'test/num_examples': 43793, 'score': 6255.508816003799, 'total_duration': 8657.011657238007, 'accumulated_submission_time': 6255.508816003799, 'accumulated_eval_time': 2399.86354136467, 'accumulated_logging_time': 0.8720173835754395}
I0307 19:05:08.915060 139702104409856 logging_writer.py:48] [28199] accumulated_eval_time=2399.86, accumulated_logging_time=0.872017, accumulated_submission_time=6255.51, global_step=28199, preemption_count=0, score=6255.51, test/accuracy=0.985829, test/loss=0.0484521, test/mean_average_precision=0.265368, test/num_examples=43793, total_duration=8657.01, train/accuracy=0.992123, train/loss=0.0253189, train/mean_average_precision=0.522059, validation/accuracy=0.986636, validation/loss=0.0456492, validation/mean_average_precision=0.272539, validation/num_examples=43793
I0307 19:05:09.373147 139702096017152 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.13650332391262054, loss=0.024803148582577705
I0307 19:05:34.598262 139702104409856 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.11183812469244003, loss=0.02426151931285858
I0307 19:05:56.642274 139702096017152 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.12355989962816238, loss=0.022282861173152924
I0307 19:06:21.290852 139702104409856 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.14585886895656586, loss=0.025637896731495857
I0307 19:06:46.322346 139702096017152 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.10582858324050903, loss=0.026681754738092422
I0307 19:07:08.123016 139702104409856 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.14342820644378662, loss=0.025166548788547516
I0307 19:07:32.922961 139702096017152 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1326112598180771, loss=0.02858024276793003
I0307 19:07:54.321433 139702104409856 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.150364950299263, loss=0.025245284661650658
I0307 19:08:19.707518 139702096017152 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.14597372710704803, loss=0.028669925406575203
I0307 19:08:44.787922 139702104409856 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.141643688082695, loss=0.02581642009317875
I0307 19:09:06.828175 139702096017152 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.12775617837905884, loss=0.022821668535470963
I0307 19:09:09.015598 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:10:49.615418 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:11:11.783855 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:11:17.387520 139843824272576 submission_runner.py:469] Time since start: 9025.50s, 	Step: 29211, 	{'train/accuracy': 0.9929994344711304, 'train/loss': 0.022669773548841476, 'train/mean_average_precision': 0.5693031603866031, 'validation/accuracy': 0.9866693019866943, 'validation/loss': 0.04578140377998352, 'validation/mean_average_precision': 0.27938112841132345, 'validation/num_examples': 43793, 'test/accuracy': 0.9858431816101074, 'test/loss': 0.04852947220206261, 'test/mean_average_precision': 0.2695027912116471, 'test/num_examples': 43793, 'score': 6495.568902730942, 'total_duration': 9025.497288227081, 'accumulated_submission_time': 6495.568902730942, 'accumulated_eval_time': 2528.2351665496826, 'accumulated_logging_time': 0.896010160446167}
I0307 19:11:17.458119 139702104409856 logging_writer.py:48] [29211] accumulated_eval_time=2528.24, accumulated_logging_time=0.89601, accumulated_submission_time=6495.57, global_step=29211, preemption_count=0, score=6495.57, test/accuracy=0.985843, test/loss=0.0485295, test/mean_average_precision=0.269503, test/num_examples=43793, total_duration=9025.5, train/accuracy=0.992999, train/loss=0.0226698, train/mean_average_precision=0.569303, validation/accuracy=0.986669, validation/loss=0.0457814, validation/mean_average_precision=0.279381, validation/num_examples=43793
I0307 19:11:40.298984 139702096017152 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1251262128353119, loss=0.025660639628767967
I0307 19:12:06.500043 139702104409856 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.21940693259239197, loss=0.027982674539089203
I0307 19:12:28.739701 139702096017152 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.12223861366510391, loss=0.02390003576874733
I0307 19:12:54.976029 139702104409856 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.14100952446460724, loss=0.025905059650540352
I0307 19:13:21.384641 139702096017152 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.1180843859910965, loss=0.024549532681703568
I0307 19:13:43.809577 139702104409856 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.12168006598949432, loss=0.024263927713036537
I0307 19:14:09.879224 139702096017152 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.12975536286830902, loss=0.02183975838124752
I0307 19:14:31.817253 139702104409856 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.13428281247615814, loss=0.02520115301012993
I0307 19:14:58.147813 139702096017152 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.11818140000104904, loss=0.022976644337177277
I0307 19:15:17.468473 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:16:49.795427 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:16:52.557835 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:16:54.655994 139843824272576 submission_runner.py:469] Time since start: 9362.77s, 	Step: 30171, 	{'train/accuracy': 0.9926010966300964, 'train/loss': 0.02342505007982254, 'train/mean_average_precision': 0.5577519849501482, 'validation/accuracy': 0.9865056872367859, 'validation/loss': 0.046440642327070236, 'validation/mean_average_precision': 0.2673776522013648, 'validation/num_examples': 43793, 'test/accuracy': 0.9856102466583252, 'test/loss': 0.04942326992750168, 'test/mean_average_precision': 0.25640913745024424, 'test/num_examples': 43793, 'score': 6735.489598035812, 'total_duration': 9362.765830516815, 'accumulated_submission_time': 6735.489598035812, 'accumulated_eval_time': 2625.422542333603, 'accumulated_logging_time': 1.023122787475586}
I0307 19:16:54.668518 139702104409856 logging_writer.py:48] [30171] accumulated_eval_time=2625.42, accumulated_logging_time=1.02312, accumulated_submission_time=6735.49, global_step=30171, preemption_count=0, score=6735.49, test/accuracy=0.98561, test/loss=0.0494233, test/mean_average_precision=0.256409, test/num_examples=43793, total_duration=9362.77, train/accuracy=0.992601, train/loss=0.0234251, train/mean_average_precision=0.557752, validation/accuracy=0.986506, validation/loss=0.0464406, validation/mean_average_precision=0.267378, validation/num_examples=43793
I0307 19:17:01.267221 139702096017152 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.1343299001455307, loss=0.022593842819333076
I0307 19:17:27.141930 139702104409856 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.13707919418811798, loss=0.02253393828868866
I0307 19:17:49.506130 139702096017152 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.16209319233894348, loss=0.026654595509171486
I0307 19:18:15.362568 139702104409856 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.16078533232212067, loss=0.026128901168704033
I0307 19:18:41.335434 139702096017152 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.1447688788175583, loss=0.02269802615046501
I0307 19:19:03.003882 139702104409856 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.12194442003965378, loss=0.02168254740536213
I0307 19:19:28.337240 139702096017152 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.13983307778835297, loss=0.023524941876530647
I0307 19:19:51.701544 139702104409856 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.17178775370121002, loss=0.02419152669608593
I0307 19:20:15.707229 139702096017152 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.13152022659778595, loss=0.023331692442297935
I0307 19:20:41.262207 139702104409856 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.16168074309825897, loss=0.023689597845077515
I0307 19:20:54.775428 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:22:46.052537 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:22:50.172037 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:23:11.693470 139843824272576 submission_runner.py:469] Time since start: 9739.80s, 	Step: 31163, 	{'train/accuracy': 0.9926149249076843, 'train/loss': 0.023582249879837036, 'train/mean_average_precision': 0.5420363427953263, 'validation/accuracy': 0.9865077137947083, 'validation/loss': 0.04630015417933464, 'validation/mean_average_precision': 0.2710621432951711, 'validation/num_examples': 43793, 'test/accuracy': 0.9858199954032898, 'test/loss': 0.04889128357172012, 'test/mean_average_precision': 0.268126563513086, 'test/num_examples': 43793, 'score': 6975.553866386414, 'total_duration': 9739.803375959396, 'accumulated_submission_time': 6975.553866386414, 'accumulated_eval_time': 2762.340423822403, 'accumulated_logging_time': 1.0473568439483643}
I0307 19:23:11.707172 139702096017152 logging_writer.py:48] [31163] accumulated_eval_time=2762.34, accumulated_logging_time=1.04736, accumulated_submission_time=6975.55, global_step=31163, preemption_count=0, score=6975.55, test/accuracy=0.98582, test/loss=0.0488913, test/mean_average_precision=0.268127, test/num_examples=43793, total_duration=9739.8, train/accuracy=0.992615, train/loss=0.0235822, train/mean_average_precision=0.542036, validation/accuracy=0.986508, validation/loss=0.0463002, validation/mean_average_precision=0.271062, validation/num_examples=43793
I0307 19:23:24.210976 139702104409856 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.13262967765331268, loss=0.02189919911324978
I0307 19:23:46.292156 139702096017152 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.12399952858686447, loss=0.023950913920998573
I0307 19:24:12.411454 139702104409856 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.19237735867500305, loss=0.025324717164039612
I0307 19:24:38.416308 139702096017152 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.1276092529296875, loss=0.021744247525930405
I0307 19:25:00.239026 139702104409856 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.14146000146865845, loss=0.021309146657586098
I0307 19:25:26.365694 139702096017152 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.15179502964019775, loss=0.023211374878883362
I0307 19:25:49.740269 139702104409856 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.15014943480491638, loss=0.022613318637013435
I0307 19:26:14.549135 139702096017152 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.1288067251443863, loss=0.0221555233001709
I0307 19:26:40.193023 139702104409856 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.14486059546470642, loss=0.022594358772039413
I0307 19:27:02.389577 139702096017152 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.133043110370636, loss=0.023431051522493362
I0307 19:27:11.847093 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:29:20.140765 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:29:22.341072 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:29:48.550346 139843824272576 submission_runner.py:469] Time since start: 10136.66s, 	Step: 32126, 	{'train/accuracy': 0.9926468133926392, 'train/loss': 0.02340000867843628, 'train/mean_average_precision': 0.5606597311428207, 'validation/accuracy': 0.9866757392883301, 'validation/loss': 0.04663236066699028, 'validation/mean_average_precision': 0.2687272938360035, 'validation/num_examples': 43793, 'test/accuracy': 0.9857951998710632, 'test/loss': 0.04956445470452309, 'test/mean_average_precision': 0.26287872559364395, 'test/num_examples': 43793, 'score': 7215.65358710289, 'total_duration': 10136.660241127014, 'accumulated_submission_time': 7215.65358710289, 'accumulated_eval_time': 2919.0435349941254, 'accumulated_logging_time': 1.0708155632019043}
I0307 19:29:48.564042 139702104409856 logging_writer.py:48] [32126] accumulated_eval_time=2919.04, accumulated_logging_time=1.07082, accumulated_submission_time=7215.65, global_step=32126, preemption_count=0, score=7215.65, test/accuracy=0.985795, test/loss=0.0495645, test/mean_average_precision=0.262879, test/num_examples=43793, total_duration=10136.7, train/accuracy=0.992647, train/loss=0.0234, train/mean_average_precision=0.56066, validation/accuracy=0.986676, validation/loss=0.0466324, validation/mean_average_precision=0.268727, validation/num_examples=43793
I0307 19:30:09.353111 139702096017152 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.14841459691524506, loss=0.02328716591000557
I0307 19:30:36.067113 139702104409856 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.14472460746765137, loss=0.023737065494060516
I0307 19:30:58.367713 139702096017152 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.16454964876174927, loss=0.02290468104183674
I0307 19:31:25.482214 139702104409856 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.1434888392686844, loss=0.022075191140174866
I0307 19:31:47.595791 139702096017152 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.17489002645015717, loss=0.023299483582377434
I0307 19:32:14.215342 139702104409856 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.16777515411376953, loss=0.023188428953289986
I0307 19:32:42.690133 139702096017152 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.14915955066680908, loss=0.024653056636452675
I0307 19:33:05.231804 139702104409856 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.14481830596923828, loss=0.022056149318814278
I0307 19:33:33.382570 139702096017152 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.14693699777126312, loss=0.023983050137758255
I0307 19:33:48.697400 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:35:32.820593 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:35:34.799389 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:35:36.771017 139843824272576 submission_runner.py:469] Time since start: 10484.88s, 	Step: 33070, 	{'train/accuracy': 0.9931266903877258, 'train/loss': 0.02186143770813942, 'train/mean_average_precision': 0.5854248971690507, 'validation/accuracy': 0.9865105748176575, 'validation/loss': 0.0472368486225605, 'validation/mean_average_precision': 0.2727369982932136, 'validation/num_examples': 43793, 'test/accuracy': 0.9856507182121277, 'test/loss': 0.05016958341002464, 'test/mean_average_precision': 0.25712994198602157, 'test/num_examples': 43793, 'score': 7455.74582695961, 'total_duration': 10484.880962371826, 'accumulated_submission_time': 7455.74582695961, 'accumulated_eval_time': 3027.117034673691, 'accumulated_logging_time': 1.09440016746521}
I0307 19:35:36.783905 139702104409856 logging_writer.py:48] [33070] accumulated_eval_time=3027.12, accumulated_logging_time=1.0944, accumulated_submission_time=7455.75, global_step=33070, preemption_count=0, score=7455.75, test/accuracy=0.985651, test/loss=0.0501696, test/mean_average_precision=0.25713, test/num_examples=43793, total_duration=10484.9, train/accuracy=0.993127, train/loss=0.0218614, train/mean_average_precision=0.585425, validation/accuracy=0.986511, validation/loss=0.0472368, validation/mean_average_precision=0.272737, validation/num_examples=43793
I0307 19:35:43.619707 139702096017152 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.13852128386497498, loss=0.022936666384339333
I0307 19:36:10.333472 139702104409856 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.17382141947746277, loss=0.021074218675494194
I0307 19:36:35.995487 139702096017152 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.15022195875644684, loss=0.023218553513288498
I0307 19:37:00.075030 139702104409856 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.13912153244018555, loss=0.01963767223060131
I0307 19:37:27.135658 139702096017152 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.17458130419254303, loss=0.023543309420347214
I0307 19:37:49.169461 139702104409856 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.19616049528121948, loss=0.02213571034371853
I0307 19:38:16.396328 139702096017152 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.16651096940040588, loss=0.021419713273644447
I0307 19:38:43.378338 139702104409856 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.13903313875198364, loss=0.020045621320605278
I0307 19:39:05.647363 139702096017152 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.18787981569766998, loss=0.024709299206733704
I0307 19:39:32.482970 139702104409856 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.16418857872486115, loss=0.024715792387723923
I0307 19:39:36.774529 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:41:32.647884 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:41:43.494189 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:42:01.550132 139843824272576 submission_runner.py:469] Time since start: 10869.66s, 	Step: 34020, 	{'train/accuracy': 0.9937687516212463, 'train/loss': 0.01991601660847664, 'train/mean_average_precision': 0.6384030764002606, 'validation/accuracy': 0.9866607189178467, 'validation/loss': 0.04756362363696098, 'validation/mean_average_precision': 0.27655691076652733, 'validation/num_examples': 43793, 'test/accuracy': 0.9858170747756958, 'test/loss': 0.050600532442331314, 'test/mean_average_precision': 0.2722631267531079, 'test/num_examples': 43793, 'score': 7695.694377422333, 'total_duration': 10869.659365177155, 'accumulated_submission_time': 7695.694377422333, 'accumulated_eval_time': 3171.8918023109436, 'accumulated_logging_time': 1.1169514656066895}
I0307 19:42:01.644424 139702096017152 logging_writer.py:48] [34020] accumulated_eval_time=3171.89, accumulated_logging_time=1.11695, accumulated_submission_time=7695.69, global_step=34020, preemption_count=0, score=7695.69, test/accuracy=0.985817, test/loss=0.0506005, test/mean_average_precision=0.272263, test/num_examples=43793, total_duration=10869.7, train/accuracy=0.993769, train/loss=0.019916, train/mean_average_precision=0.638403, validation/accuracy=0.986661, validation/loss=0.0475636, validation/mean_average_precision=0.276557, validation/num_examples=43793
I0307 19:42:24.404846 139702104409856 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.2067417949438095, loss=0.021804431453347206
I0307 19:42:51.686483 139702096017152 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.1721215397119522, loss=0.023351503536105156
I0307 19:43:13.851112 139702104409856 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.17387114465236664, loss=0.019306499511003494
I0307 19:43:41.531920 139702096017152 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.1566636562347412, loss=0.023427342996001244
I0307 19:44:05.068037 139702104409856 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1797877848148346, loss=0.022510917857289314
I0307 19:44:30.990984 139702096017152 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.1760759800672531, loss=0.02357807196676731
I0307 19:44:58.596888 139702104409856 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.13789325952529907, loss=0.01944494992494583
I0307 19:45:20.695457 139702096017152 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.15504947304725647, loss=0.021443840116262436
I0307 19:45:48.012048 139702104409856 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.15790143609046936, loss=0.021838635206222534
I0307 19:46:01.761001 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:48:18.205073 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:48:25.079416 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:48:55.790248 139843824272576 submission_runner.py:469] Time since start: 11283.90s, 	Step: 34963, 	{'train/accuracy': 0.993643045425415, 'train/loss': 0.0199844129383564, 'train/mean_average_precision': 0.6165708895013072, 'validation/accuracy': 0.9866185188293457, 'validation/loss': 0.04814017936587334, 'validation/mean_average_precision': 0.27742874992882666, 'validation/num_examples': 43793, 'test/accuracy': 0.9856843948364258, 'test/loss': 0.05150644853711128, 'test/mean_average_precision': 0.25915414157150624, 'test/num_examples': 43793, 'score': 7935.719910144806, 'total_duration': 11283.90010547638, 'accumulated_submission_time': 7935.719910144806, 'accumulated_eval_time': 3345.9208443164825, 'accumulated_logging_time': 1.27052640914917}
I0307 19:48:55.803789 139702096017152 logging_writer.py:48] [34963] accumulated_eval_time=3345.92, accumulated_logging_time=1.27053, accumulated_submission_time=7935.72, global_step=34963, preemption_count=0, score=7935.72, test/accuracy=0.985684, test/loss=0.0515064, test/mean_average_precision=0.259154, test/num_examples=43793, total_duration=11283.9, train/accuracy=0.993643, train/loss=0.0199844, train/mean_average_precision=0.616571, validation/accuracy=0.986619, validation/loss=0.0481402, validation/mean_average_precision=0.277429, validation/num_examples=43793
I0307 19:49:09.936688 139702104409856 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.1493251621723175, loss=0.01899581402540207
I0307 19:49:32.207034 139702096017152 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.15925483405590057, loss=0.022286774590611458
I0307 19:49:59.881553 139702104409856 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.19672301411628723, loss=0.020771436393260956
I0307 19:50:27.817102 139702096017152 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.20583820343017578, loss=0.022208981215953827
I0307 19:50:49.812730 139702104409856 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.18258288502693176, loss=0.020303523167967796
I0307 19:51:17.459917 139702096017152 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.14364129304885864, loss=0.020197749137878418
I0307 19:51:39.606255 139702104409856 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.16056035459041595, loss=0.020563818514347076
I0307 19:52:07.530924 139702096017152 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.16190481185913086, loss=0.021410463377833366
I0307 19:52:35.325774 139702104409856 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.14779286086559296, loss=0.020381273701786995
I0307 19:52:55.877699 139843824272576 spec.py:321] Evaluating on the training split.
I0307 19:54:48.022533 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 19:55:16.665450 139843824272576 spec.py:349] Evaluating on the test split.
I0307 19:55:26.028556 139843824272576 submission_runner.py:469] Time since start: 11674.14s, 	Step: 35894, 	{'train/accuracy': 0.9933398365974426, 'train/loss': 0.0208708718419075, 'train/mean_average_precision': 0.609296342599606, 'validation/accuracy': 0.9865304231643677, 'validation/loss': 0.04874027147889137, 'validation/mean_average_precision': 0.26971469409770044, 'validation/num_examples': 43793, 'test/accuracy': 0.9856528043746948, 'test/loss': 0.051855143159627914, 'test/mean_average_precision': 0.26619169287074146, 'test/num_examples': 43793, 'score': 8175.753685235977, 'total_duration': 11674.138521909714, 'accumulated_submission_time': 8175.753685235977, 'accumulated_eval_time': 3496.071601629257, 'accumulated_logging_time': 1.2942588329315186}
I0307 19:55:26.043825 139702096017152 logging_writer.py:48] [35894] accumulated_eval_time=3496.07, accumulated_logging_time=1.29426, accumulated_submission_time=8175.75, global_step=35894, preemption_count=0, score=8175.75, test/accuracy=0.985653, test/loss=0.0518551, test/mean_average_precision=0.266192, test/num_examples=43793, total_duration=11674.1, train/accuracy=0.99334, train/loss=0.0208709, train/mean_average_precision=0.609296, validation/accuracy=0.98653, validation/loss=0.0487403, validation/mean_average_precision=0.269715, validation/num_examples=43793
I0307 19:55:27.569542 139702104409856 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.16292963922023773, loss=0.02055094763636589
I0307 19:55:49.890799 139702096017152 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.16968154907226562, loss=0.02137223817408085
I0307 19:56:17.655011 139702104409856 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.17549429833889008, loss=0.020553024485707283
I0307 19:56:39.959204 139702096017152 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.21354131400585175, loss=0.0178553257137537
I0307 19:57:08.304941 139702104409856 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.17867493629455566, loss=0.02070128358900547
I0307 19:57:36.132029 139702096017152 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.20663784444332123, loss=0.02272374927997589
I0307 19:57:58.152326 139702104409856 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.19340336322784424, loss=0.0197332464158535
I0307 19:58:26.011276 139702096017152 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.21951769292354584, loss=0.020505426451563835
I0307 19:58:51.825967 139702104409856 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.22909951210021973, loss=0.01966346800327301
I0307 19:59:24.782821 139702096017152 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.18837901949882507, loss=0.01885179989039898
I0307 19:59:26.085955 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:02:24.686905 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:02:35.036561 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:02:39.928903 139843824272576 submission_runner.py:469] Time since start: 12108.04s, 	Step: 36807, 	{'train/accuracy': 0.9934282898902893, 'train/loss': 0.020509324967861176, 'train/mean_average_precision': 0.6145964473428487, 'validation/accuracy': 0.9863453507423401, 'validation/loss': 0.049681417644023895, 'validation/mean_average_precision': 0.2604879474346703, 'validation/num_examples': 43793, 'test/accuracy': 0.9855390787124634, 'test/loss': 0.05275654420256615, 'test/mean_average_precision': 0.25872547916827, 'test/num_examples': 43793, 'score': 8415.754895925522, 'total_duration': 12108.038880348206, 'accumulated_submission_time': 8415.754895925522, 'accumulated_eval_time': 3689.9144699573517, 'accumulated_logging_time': 1.319704532623291}
I0307 20:02:39.942383 139702104409856 logging_writer.py:48] [36807] accumulated_eval_time=3689.91, accumulated_logging_time=1.3197, accumulated_submission_time=8415.75, global_step=36807, preemption_count=0, score=8415.75, test/accuracy=0.985539, test/loss=0.0527565, test/mean_average_precision=0.258725, test/num_examples=43793, total_duration=12108, train/accuracy=0.993428, train/loss=0.0205093, train/mean_average_precision=0.614596, validation/accuracy=0.986345, validation/loss=0.0496814, validation/mean_average_precision=0.260488, validation/num_examples=43793
I0307 20:03:10.263300 139702096017152 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.1809489130973816, loss=0.018027344718575478
I0307 20:03:32.478049 139702104409856 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.17595435678958893, loss=0.021414976567029953
I0307 20:04:01.027123 139702096017152 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.18610964715480804, loss=0.019130056723952293
I0307 20:04:27.030277 139702104409856 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2446473091840744, loss=0.018583187833428383
I0307 20:04:51.816130 139702096017152 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.16923001408576965, loss=0.015996836125850677
I0307 20:05:27.498048 139702104409856 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.1779075562953949, loss=0.019776223227381706
I0307 20:05:50.027959 139702096017152 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.18414181470870972, loss=0.017388995736837387
I0307 20:06:18.591165 139702104409856 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.1751515120267868, loss=0.01930207945406437
I0307 20:06:40.081414 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:09:08.613255 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:09:37.871008 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:09:41.018949 139843824272576 submission_runner.py:469] Time since start: 12529.13s, 	Step: 37691, 	{'train/accuracy': 0.9934500455856323, 'train/loss': 0.020330805331468582, 'train/mean_average_precision': 0.6181393482559931, 'validation/accuracy': 0.9865032434463501, 'validation/loss': 0.05014101415872574, 'validation/mean_average_precision': 0.26814789675695905, 'validation/num_examples': 43793, 'test/accuracy': 0.9856207966804504, 'test/loss': 0.05352601036429405, 'test/mean_average_precision': 0.2591445806767985, 'test/num_examples': 43793, 'score': 8655.852705955505, 'total_duration': 12529.12860250473, 'accumulated_submission_time': 8655.852705955505, 'accumulated_eval_time': 3870.851680994034, 'accumulated_logging_time': 1.3429534435272217}
I0307 20:09:41.097088 139702096017152 logging_writer.py:48] [37691] accumulated_eval_time=3870.85, accumulated_logging_time=1.34295, accumulated_submission_time=8655.85, global_step=37691, preemption_count=0, score=8655.85, test/accuracy=0.985621, test/loss=0.053526, test/mean_average_precision=0.259145, test/num_examples=43793, total_duration=12529.1, train/accuracy=0.99345, train/loss=0.0203308, train/mean_average_precision=0.618139, validation/accuracy=0.986503, validation/loss=0.050141, validation/mean_average_precision=0.268148, validation/num_examples=43793
I0307 20:09:49.247704 139702104409856 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.20326906442642212, loss=0.019077418372035027
I0307 20:10:12.478611 139702096017152 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.220037043094635, loss=0.021923717111349106
I0307 20:10:42.091855 139702104409856 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.198950856924057, loss=0.017523720860481262
I0307 20:11:04.204187 139702096017152 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.20632734894752502, loss=0.01971311680972576
I0307 20:11:33.196938 139702104409856 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.1710883527994156, loss=0.01774786412715912
I0307 20:12:02.592751 139702096017152 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1996753066778183, loss=0.01661221869289875
I0307 20:12:25.052537 139702104409856 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.18588890135288239, loss=0.01848609931766987
I0307 20:12:54.264560 139702096017152 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.18674035370349884, loss=0.018003560602664948
I0307 20:13:29.518819 139702104409856 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.20330367982387543, loss=0.020177733153104782
I0307 20:13:41.154176 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:15:54.424504 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:16:28.567023 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:16:34.132383 139843824272576 submission_runner.py:469] Time since start: 12942.24s, 	Step: 38553, 	{'train/accuracy': 0.9939141869544983, 'train/loss': 0.01909920759499073, 'train/mean_average_precision': 0.644408953255529, 'validation/accuracy': 0.9863400459289551, 'validation/loss': 0.05091005563735962, 'validation/mean_average_precision': 0.2639433104015775, 'validation/num_examples': 43793, 'test/accuracy': 0.9854497909545898, 'test/loss': 0.054305531084537506, 'test/mean_average_precision': 0.25520256352965703, 'test/num_examples': 43793, 'score': 8895.822674751282, 'total_duration': 12942.242048025131, 'accumulated_submission_time': 8895.822674751282, 'accumulated_eval_time': 4043.8294870853424, 'accumulated_logging_time': 1.476332187652588}
I0307 20:16:34.204265 139702096017152 logging_writer.py:48] [38553] accumulated_eval_time=4043.83, accumulated_logging_time=1.47633, accumulated_submission_time=8895.82, global_step=38553, preemption_count=0, score=8895.82, test/accuracy=0.98545, test/loss=0.0543055, test/mean_average_precision=0.255203, test/num_examples=43793, total_duration=12942.2, train/accuracy=0.993914, train/loss=0.0190992, train/mean_average_precision=0.644409, validation/accuracy=0.98634, validation/loss=0.0509101, validation/mean_average_precision=0.263943, validation/num_examples=43793
I0307 20:16:49.394925 139702104409856 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.20258334279060364, loss=0.018449625000357628
I0307 20:17:11.222734 139702096017152 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.19887331128120422, loss=0.017099393531680107
I0307 20:17:40.916770 139702104409856 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.19385622441768646, loss=0.016212012618780136
I0307 20:18:10.403975 139702096017152 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.19221466779708862, loss=0.01777825504541397
I0307 20:18:32.642522 139702104409856 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.21016840636730194, loss=0.017117664217948914
I0307 20:19:02.130601 139702096017152 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.2109672576189041, loss=0.019150089472532272
I0307 20:19:24.299710 139702104409856 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.22203625738620758, loss=0.021039387211203575
I0307 20:19:57.042015 139702096017152 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.20687107741832733, loss=0.016198109835386276
I0307 20:20:29.489736 139702104409856 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.19262565672397614, loss=0.015384444035589695
I0307 20:20:34.260371 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:22:58.044431 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:23:05.485195 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:23:07.674867 139843824272576 submission_runner.py:469] Time since start: 13335.78s, 	Step: 39422, 	{'train/accuracy': 0.9935669302940369, 'train/loss': 0.01988809183239937, 'train/mean_average_precision': 0.6251321413531297, 'validation/accuracy': 0.9864175915718079, 'validation/loss': 0.0521615631878376, 'validation/mean_average_precision': 0.2610725248257678, 'validation/num_examples': 43793, 'test/accuracy': 0.9855833053588867, 'test/loss': 0.05540802702307701, 'test/mean_average_precision': 0.25717389173247834, 'test/num_examples': 43793, 'score': 9135.771723985672, 'total_duration': 13335.784710407257, 'accumulated_submission_time': 9135.771723985672, 'accumulated_eval_time': 4197.2437608242035, 'accumulated_logging_time': 1.6233999729156494}
I0307 20:23:07.689961 139702096017152 logging_writer.py:48] [39422] accumulated_eval_time=4197.24, accumulated_logging_time=1.6234, accumulated_submission_time=9135.77, global_step=39422, preemption_count=0, score=9135.77, test/accuracy=0.985583, test/loss=0.055408, test/mean_average_precision=0.257174, test/num_examples=43793, total_duration=13335.8, train/accuracy=0.993567, train/loss=0.0198881, train/mean_average_precision=0.625132, validation/accuracy=0.986418, validation/loss=0.0521616, validation/mean_average_precision=0.261073, validation/num_examples=43793
I0307 20:23:32.199856 139702104409856 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2263123244047165, loss=0.018360212445259094
I0307 20:23:54.797424 139702096017152 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.22700348496437073, loss=0.016651879996061325
I0307 20:24:27.218453 139702104409856 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.26044732332229614, loss=0.01829126849770546
I0307 20:24:49.317060 139702096017152 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.20222207903862, loss=0.018927065655589104
I0307 20:25:21.986768 139702104409856 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.20067144930362701, loss=0.01558816246688366
I0307 20:25:49.137842 139702096017152 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.21840721368789673, loss=0.015753526240587234
I0307 20:26:16.612149 139702104409856 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.22953876852989197, loss=0.01757732406258583
I0307 20:26:48.891059 139702096017152 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.22263506054878235, loss=0.017618445679545403
I0307 20:27:07.859764 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:29:59.496913 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:30:32.882703 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:30:36.096440 139843824272576 submission_runner.py:469] Time since start: 13784.21s, 	Step: 40286, 	{'train/accuracy': 0.9943349361419678, 'train/loss': 0.017571382224559784, 'train/mean_average_precision': 0.6772662046876651, 'validation/accuracy': 0.9862921833992004, 'validation/loss': 0.052747711539268494, 'validation/mean_average_precision': 0.25928499892049306, 'validation/num_examples': 43793, 'test/accuracy': 0.98539799451828, 'test/loss': 0.056186046451330185, 'test/mean_average_precision': 0.2585284110205704, 'test/num_examples': 43793, 'score': 9375.897370576859, 'total_duration': 13784.2063434124, 'accumulated_submission_time': 9375.897370576859, 'accumulated_eval_time': 4405.480292797089, 'accumulated_logging_time': 1.648796796798706}
I0307 20:30:36.110822 139702104409856 logging_writer.py:48] [40286] accumulated_eval_time=4405.48, accumulated_logging_time=1.6488, accumulated_submission_time=9375.9, global_step=40286, preemption_count=0, score=9375.9, test/accuracy=0.985398, test/loss=0.056186, test/mean_average_precision=0.258528, test/num_examples=43793, total_duration=13784.2, train/accuracy=0.994335, train/loss=0.0175714, train/mean_average_precision=0.677266, validation/accuracy=0.986292, validation/loss=0.0527477, validation/mean_average_precision=0.259285, validation/num_examples=43793
I0307 20:30:48.233981 139702096017152 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.22185583412647247, loss=0.01738588884472847
I0307 20:31:12.975320 139702104409856 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.22808124125003815, loss=0.016724219545722008
I0307 20:31:46.345023 139702096017152 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.22934357821941376, loss=0.017276544123888016
I0307 20:32:09.127687 139702104409856 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.22486630082130432, loss=0.01751730404794216
I0307 20:32:39.190701 139702096017152 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.20382966101169586, loss=0.01582683064043522
I0307 20:33:07.914187 139702104409856 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.2234964370727539, loss=0.014855536632239819
I0307 20:33:31.833973 139702096017152 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.23031148314476013, loss=0.015125468373298645
I0307 20:34:02.429789 139702104409856 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.24815063178539276, loss=0.016896944493055344
I0307 20:34:24.841653 139702096017152 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.21027430891990662, loss=0.014981212094426155
I0307 20:34:36.148608 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:37:10.192421 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:37:39.395011 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:37:57.023212 139843824272576 submission_runner.py:469] Time since start: 14225.13s, 	Step: 41125, 	{'train/accuracy': 0.9947344660758972, 'train/loss': 0.016399336978793144, 'train/mean_average_precision': 0.6997507287158085, 'validation/accuracy': 0.9861768484115601, 'validation/loss': 0.0537332147359848, 'validation/mean_average_precision': 0.25899301959266624, 'validation/num_examples': 43793, 'test/accuracy': 0.9852463603019714, 'test/loss': 0.05744802951812744, 'test/mean_average_precision': 0.25201856053062216, 'test/num_examples': 43793, 'score': 9615.874061346054, 'total_duration': 14225.13319349289, 'accumulated_submission_time': 9615.874061346054, 'accumulated_eval_time': 4606.3549337387085, 'accumulated_logging_time': 1.6909832954406738}
I0307 20:37:57.037837 139702104409856 logging_writer.py:48] [41125] accumulated_eval_time=4606.35, accumulated_logging_time=1.69098, accumulated_submission_time=9615.87, global_step=41125, preemption_count=0, score=9615.87, test/accuracy=0.985246, test/loss=0.057448, test/mean_average_precision=0.252019, test/num_examples=43793, total_duration=14225.1, train/accuracy=0.994734, train/loss=0.0163993, train/mean_average_precision=0.699751, validation/accuracy=0.986177, validation/loss=0.0537332, validation/mean_average_precision=0.258993, validation/num_examples=43793
I0307 20:38:14.348684 139702096017152 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.229548379778862, loss=0.01634811796247959
I0307 20:38:45.554977 139702104409856 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.24402756989002228, loss=0.016679847612977028
I0307 20:39:07.443765 139702096017152 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2374674528837204, loss=0.015242252498865128
I0307 20:39:38.782816 139702104409856 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.21724288165569305, loss=0.014555969275534153
I0307 20:40:09.899915 139702096017152 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2363218516111374, loss=0.016093743965029716
I0307 20:40:32.755469 139702104409856 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.23478546738624573, loss=0.016882052645087242
I0307 20:41:03.995045 139702096017152 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.24184294044971466, loss=0.01596206985414028
I0307 20:41:30.171604 139702104409856 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.22918815910816193, loss=0.01737714372575283
I0307 20:41:56.384976 139702096017152 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.21080094575881958, loss=0.013939127326011658
I0307 20:41:57.053265 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:45:41.433540 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:46:04.455509 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:46:42.709419 139843824272576 submission_runner.py:469] Time since start: 14750.82s, 	Step: 42004, 	{'train/accuracy': 0.9952811598777771, 'train/loss': 0.014881030656397343, 'train/mean_average_precision': 0.7382278069221647, 'validation/accuracy': 0.9861959218978882, 'validation/loss': 0.054622769355773926, 'validation/mean_average_precision': 0.26028120222327283, 'validation/num_examples': 43793, 'test/accuracy': 0.9852411150932312, 'test/loss': 0.05846964567899704, 'test/mean_average_precision': 0.2500642277622693, 'test/num_examples': 43793, 'score': 9855.845724344254, 'total_duration': 14750.819407701492, 'accumulated_submission_time': 9855.845724344254, 'accumulated_eval_time': 4892.011008024216, 'accumulated_logging_time': 1.715456485748291}
I0307 20:46:42.781294 139702104409856 logging_writer.py:48] [42004] accumulated_eval_time=4892.01, accumulated_logging_time=1.71546, accumulated_submission_time=9855.85, global_step=42004, preemption_count=0, score=9855.85, test/accuracy=0.985241, test/loss=0.0584696, test/mean_average_precision=0.250064, test/num_examples=43793, total_duration=14750.8, train/accuracy=0.995281, train/loss=0.014881, train/mean_average_precision=0.738228, validation/accuracy=0.986196, validation/loss=0.0546228, validation/mean_average_precision=0.260281, validation/num_examples=43793
I0307 20:47:13.177670 139702096017152 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.24538245797157288, loss=0.017363399267196655
I0307 20:47:35.352905 139702104409856 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.21465496718883514, loss=0.015074052847921848
I0307 20:48:09.109961 139702096017152 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.22518393397331238, loss=0.013781966641545296
I0307 20:48:42.767468 139702104409856 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.23465192317962646, loss=0.014468129724264145
I0307 20:49:06.504385 139702096017152 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2307044416666031, loss=0.01308493223041296
I0307 20:49:37.906850 139702104409856 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.2815839648246765, loss=0.015243712812662125
I0307 20:50:00.561269 139702096017152 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.22952936589717865, loss=0.013961472548544407
I0307 20:50:31.992926 139702104409856 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.23702934384346008, loss=0.014597557485103607
I0307 20:50:42.796246 139843824272576 spec.py:321] Evaluating on the training split.
I0307 20:52:42.154284 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 20:53:13.108923 139843824272576 spec.py:349] Evaluating on the test split.
I0307 20:53:59.954376 139843824272576 submission_runner.py:469] Time since start: 15188.06s, 	Step: 42849, 	{'train/accuracy': 0.995642900466919, 'train/loss': 0.013742235489189625, 'train/mean_average_precision': 0.753501697560599, 'validation/accuracy': 0.9861090779304504, 'validation/loss': 0.056181859225034714, 'validation/mean_average_precision': 0.254918822718795, 'validation/num_examples': 43793, 'test/accuracy': 0.9853225946426392, 'test/loss': 0.059770505875349045, 'test/mean_average_precision': 0.2513521919183122, 'test/num_examples': 43793, 'score': 10095.815959692001, 'total_duration': 15188.063847780228, 'accumulated_submission_time': 10095.815959692001, 'accumulated_eval_time': 5089.168545722961, 'accumulated_logging_time': 1.7977545261383057}
I0307 20:54:00.037493 139702096017152 logging_writer.py:48] [42849] accumulated_eval_time=5089.17, accumulated_logging_time=1.79775, accumulated_submission_time=10095.8, global_step=42849, preemption_count=0, score=10095.8, test/accuracy=0.985323, test/loss=0.0597705, test/mean_average_precision=0.251352, test/num_examples=43793, total_duration=15188.1, train/accuracy=0.995643, train/loss=0.0137422, train/mean_average_precision=0.753502, validation/accuracy=0.986109, validation/loss=0.0561819, validation/mean_average_precision=0.254919, validation/num_examples=43793
I0307 20:54:20.162695 139702104409856 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.24070128798484802, loss=0.014147141017019749
I0307 20:54:42.548221 139702096017152 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.28609541058540344, loss=0.013605567626655102
I0307 20:55:14.507531 139702104409856 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2305813431739807, loss=0.013305917382240295
I0307 20:55:46.043710 139702096017152 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.23318932950496674, loss=0.013670999556779861
I0307 20:56:08.495732 139702104409856 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.25631237030029297, loss=0.015350966714322567
I0307 20:56:40.058653 139702096017152 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.24336804449558258, loss=0.015020102262496948
I0307 20:57:02.765975 139702104409856 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.23435957729816437, loss=0.013566483743488789
I0307 20:57:34.450495 139702096017152 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.23063993453979492, loss=0.012897316366434097
I0307 20:58:00.499070 139843824272576 spec.py:321] Evaluating on the training split.
I0307 21:00:42.683111 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 21:01:44.777729 139843824272576 spec.py:349] Evaluating on the test split.
I0307 21:02:02.822384 139843824272576 submission_runner.py:469] Time since start: 15670.93s, 	Step: 43682, 	{'train/accuracy': 0.9958009719848633, 'train/loss': 0.013466255739331245, 'train/mean_average_precision': 0.7552365720972756, 'validation/accuracy': 0.9860116243362427, 'validation/loss': 0.056640807539224625, 'validation/mean_average_precision': 0.2538939584771832, 'validation/num_examples': 43793, 'test/accuracy': 0.9851263165473938, 'test/loss': 0.06039847433567047, 'test/mean_average_precision': 0.24841706991387588, 'test/num_examples': 43793, 'score': 10336.181158542633, 'total_duration': 15670.932301282883, 'accumulated_submission_time': 10336.181158542633, 'accumulated_eval_time': 5331.491797208786, 'accumulated_logging_time': 1.9403514862060547}
I0307 21:02:02.837329 139702104409856 logging_writer.py:48] [43682] accumulated_eval_time=5331.49, accumulated_logging_time=1.94035, accumulated_submission_time=10336.2, global_step=43682, preemption_count=0, score=10336.2, test/accuracy=0.985126, test/loss=0.0603985, test/mean_average_precision=0.248417, test/num_examples=43793, total_duration=15670.9, train/accuracy=0.995801, train/loss=0.0134663, train/mean_average_precision=0.755237, validation/accuracy=0.986012, validation/loss=0.0566408, validation/mean_average_precision=0.253894, validation/num_examples=43793
I0307 21:02:07.108573 139702096017152 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.22975094616413116, loss=0.012818153016269207
I0307 21:02:29.637038 139702104409856 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.310720294713974, loss=0.013343858532607555
I0307 21:03:01.723351 139702096017152 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.24974863231182098, loss=0.013072862289845943
I0307 21:03:33.789432 139702104409856 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.22562798857688904, loss=0.013168207369744778
I0307 21:03:56.438839 139702096017152 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.24403901398181915, loss=0.014313483610749245
I0307 21:04:28.310065 139702104409856 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.23640000820159912, loss=0.013792919926345348
I0307 21:05:08.438326 139702096017152 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2813827395439148, loss=0.013718479312956333
I0307 21:05:31.263972 139702104409856 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2584197223186493, loss=0.014679169282317162
I0307 21:06:03.567192 139843824272576 spec.py:321] Evaluating on the training split.
I0307 21:09:03.799116 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 21:10:06.679141 139843824272576 spec.py:349] Evaluating on the test split.
I0307 21:10:20.844983 139843824272576 submission_runner.py:469] Time since start: 16168.95s, 	Step: 44482, 	{'train/accuracy': 0.9959072470664978, 'train/loss': 0.01304843183606863, 'train/mean_average_precision': 0.7722820084387875, 'validation/accuracy': 0.9858528971672058, 'validation/loss': 0.05783652514219284, 'validation/mean_average_precision': 0.25041395809068845, 'validation/num_examples': 43793, 'test/accuracy': 0.9850125908851624, 'test/loss': 0.06157413125038147, 'test/mean_average_precision': 0.250204750012491, 'test/num_examples': 43793, 'score': 10576.863844633102, 'total_duration': 16168.954593896866, 'accumulated_submission_time': 10576.863844633102, 'accumulated_eval_time': 5588.769352674484, 'accumulated_logging_time': 1.9652602672576904}
I0307 21:10:20.927185 139702096017152 logging_writer.py:48] [44482] accumulated_eval_time=5588.77, accumulated_logging_time=1.96526, accumulated_submission_time=10576.9, global_step=44482, preemption_count=0, score=10576.9, test/accuracy=0.985013, test/loss=0.0615741, test/mean_average_precision=0.250205, test/num_examples=43793, total_duration=16169, train/accuracy=0.995907, train/loss=0.0130484, train/mean_average_precision=0.772282, validation/accuracy=0.985853, validation/loss=0.0578365, validation/mean_average_precision=0.250414, validation/num_examples=43793
I0307 21:10:33.580449 139702104409856 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2537364959716797, loss=0.014661215245723724
I0307 21:10:56.044335 139702096017152 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.24105525016784668, loss=0.01278741005808115
I0307 21:11:28.748622 139702104409856 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.25078731775283813, loss=0.014661913737654686
I0307 21:11:51.677929 139702096017152 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.22348712384700775, loss=0.013121458701789379
I0307 21:12:24.572235 139702104409856 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.22608907520771027, loss=0.012343275360763073
I0307 21:12:57.473918 139702096017152 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2945707142353058, loss=0.015068985521793365
I0307 21:13:20.151093 139702104409856 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.24347415566444397, loss=0.01426865253597498
I0307 21:13:52.531274 139702096017152 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2442757934331894, loss=0.013445560820400715
I0307 21:14:15.254159 139702104409856 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.2486668825149536, loss=0.011744431219995022
I0307 21:14:20.965090 139843824272576 spec.py:321] Evaluating on the training split.
I0307 21:18:47.087001 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 21:19:13.726042 139843824272576 spec.py:349] Evaluating on the test split.
I0307 21:19:24.435049 139843824272576 submission_runner.py:469] Time since start: 16712.54s, 	Step: 45309, 	{'train/accuracy': 0.9961453676223755, 'train/loss': 0.012363744899630547, 'train/mean_average_precision': 0.7793193345344225, 'validation/accuracy': 0.9858058094978333, 'validation/loss': 0.05860166624188423, 'validation/mean_average_precision': 0.2544785951342224, 'validation/num_examples': 43793, 'test/accuracy': 0.9848681092262268, 'test/loss': 0.06258011609315872, 'test/mean_average_precision': 0.24811656616712924, 'test/num_examples': 43793, 'score': 10816.806696891785, 'total_duration': 16712.54499554634, 'accumulated_submission_time': 10816.806696891785, 'accumulated_eval_time': 5892.239295959473, 'accumulated_logging_time': 2.1094772815704346}
I0307 21:19:24.450277 139702096017152 logging_writer.py:48] [45309] accumulated_eval_time=5892.24, accumulated_logging_time=2.10948, accumulated_submission_time=10816.8, global_step=45309, preemption_count=0, score=10816.8, test/accuracy=0.984868, test/loss=0.0625801, test/mean_average_precision=0.248117, test/num_examples=43793, total_duration=16712.5, train/accuracy=0.996145, train/loss=0.0123637, train/mean_average_precision=0.779319, validation/accuracy=0.985806, validation/loss=0.0586017, validation/mean_average_precision=0.254479, validation/num_examples=43793
I0307 21:19:45.590040 139702104409856 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.2369290441274643, loss=0.012054537422955036
I0307 21:20:18.542317 139702096017152 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.24970188736915588, loss=0.013738829642534256
I0307 21:20:41.000118 139702104409856 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.2183961421251297, loss=0.011428884230554104
I0307 21:21:23.842572 139702096017152 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.314709335565567, loss=0.01442908775061369
I0307 21:21:46.048511 139702104409856 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2333417683839798, loss=0.013297496363520622
I0307 21:22:18.450704 139702096017152 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2439999282360077, loss=0.01163568440824747
I0307 21:22:51.102355 139702104409856 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.28105461597442627, loss=0.013045357540249825
I0307 21:23:13.635947 139702096017152 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.22074076533317566, loss=0.010737343691289425
I0307 21:23:25.183971 139843824272576 spec.py:321] Evaluating on the training split.
I0307 21:26:24.093832 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 21:27:02.060630 139843824272576 spec.py:349] Evaluating on the test split.
I0307 21:27:07.710715 139843824272576 submission_runner.py:469] Time since start: 17175.82s, 	Step: 46143, 	{'train/accuracy': 0.9963669180870056, 'train/loss': 0.011866775341331959, 'train/mean_average_precision': 0.7887896196002202, 'validation/accuracy': 0.9856812357902527, 'validation/loss': 0.059525396674871445, 'validation/mean_average_precision': 0.25578924493201816, 'validation/num_examples': 43793, 'test/accuracy': 0.9847535490989685, 'test/loss': 0.06360175460577011, 'test/mean_average_precision': 0.2454862852846679, 'test/num_examples': 43793, 'score': 11057.497724294662, 'total_duration': 17175.820520162582, 'accumulated_submission_time': 11057.497724294662, 'accumulated_eval_time': 6114.765877008438, 'accumulated_logging_time': 2.1347782611846924}
I0307 21:27:07.786718 139702104409856 logging_writer.py:48] [46143] accumulated_eval_time=6114.77, accumulated_logging_time=2.13478, accumulated_submission_time=11057.5, global_step=46143, preemption_count=0, score=11057.5, test/accuracy=0.984754, test/loss=0.0636018, test/mean_average_precision=0.245486, test/num_examples=43793, total_duration=17175.8, train/accuracy=0.996367, train/loss=0.0118668, train/mean_average_precision=0.78879, validation/accuracy=0.985681, validation/loss=0.0595254, validation/mean_average_precision=0.255789, validation/num_examples=43793
I0307 21:27:43.252928 139702096017152 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.20896807312965393, loss=0.01113158743828535
I0307 21:28:05.524039 139702104409856 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.24167677760124207, loss=0.012249594554305077
I0307 21:28:38.365507 139702096017152 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.2772148847579956, loss=0.0130397267639637
I0307 21:29:00.807036 139702104409856 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.2538132965564728, loss=0.011828083544969559
I0307 21:29:33.825072 139702096017152 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.21993394196033478, loss=0.011838439851999283
I0307 21:30:06.929946 139702104409856 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2414173036813736, loss=0.012449457310140133
I0307 21:30:29.418409 139702096017152 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2985341250896454, loss=0.011720262467861176
I0307 21:31:07.713391 139843824272576 spec.py:321] Evaluating on the training split.
I0307 21:35:57.934439 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 21:36:35.693509 139843824272576 spec.py:349] Evaluating on the test split.
I0307 21:36:52.492548 139843824272576 submission_runner.py:469] Time since start: 17760.60s, 	Step: 46881, 	{'train/accuracy': 0.9959153532981873, 'train/loss': 0.012638011947274208, 'train/mean_average_precision': 0.775536081396346, 'validation/accuracy': 0.9857904314994812, 'validation/loss': 0.06019804626703262, 'validation/mean_average_precision': 0.25509967275415013, 'validation/num_examples': 43793, 'test/accuracy': 0.9848432540893555, 'test/loss': 0.06444653123617172, 'test/mean_average_precision': 0.24479387503080025, 'test/num_examples': 43793, 'score': 11297.303407669067, 'total_duration': 17760.602556705475, 'accumulated_submission_time': 11297.303407669067, 'accumulated_eval_time': 6459.545073747635, 'accumulated_logging_time': 2.2961666584014893}
I0307 21:36:52.507973 139702104409856 logging_writer.py:48] [46881] accumulated_eval_time=6459.55, accumulated_logging_time=2.29617, accumulated_submission_time=11297.3, global_step=46881, preemption_count=0, score=11297.3, test/accuracy=0.984843, test/loss=0.0644465, test/mean_average_precision=0.244794, test/num_examples=43793, total_duration=17760.6, train/accuracy=0.995915, train/loss=0.012638, train/mean_average_precision=0.775536, validation/accuracy=0.98579, validation/loss=0.060198, validation/mean_average_precision=0.2551, validation/num_examples=43793
I0307 21:36:57.085179 139702096017152 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.27112123370170593, loss=0.012724766507744789
I0307 21:37:19.883696 139702104409856 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.24505136907100677, loss=0.009415369480848312
I0307 21:37:57.024663 139702096017152 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.27139943838119507, loss=0.010952462442219257
I0307 21:38:23.133873 139702104409856 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.25254225730895996, loss=0.012239673174917698
I0307 21:38:57.478677 139702096017152 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.27459466457366943, loss=0.013681228272616863
I0307 21:39:34.202765 139702104409856 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.2677009105682373, loss=0.01225818321108818
I0307 21:40:05.291682 139702096017152 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2752950191497803, loss=0.012651410885155201
I0307 21:40:52.523217 139843824272576 spec.py:321] Evaluating on the training split.
I0307 21:44:45.255014 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 21:45:21.275399 139843824272576 spec.py:349] Evaluating on the test split.
I0307 21:46:04.964619 139843824272576 submission_runner.py:469] Time since start: 18313.07s, 	Step: 47599, 	{'train/accuracy': 0.9953779578208923, 'train/loss': 0.014015729539096355, 'train/mean_average_precision': 0.7507341103664186, 'validation/accuracy': 0.985800564289093, 'validation/loss': 0.060831695795059204, 'validation/mean_average_precision': 0.25154888792827207, 'validation/num_examples': 43793, 'test/accuracy': 0.9848458170890808, 'test/loss': 0.06503494828939438, 'test/mean_average_precision': 0.24457584547622235, 'test/num_examples': 43793, 'score': 11537.270168542862, 'total_duration': 18313.07462143898, 'accumulated_submission_time': 11537.270168542862, 'accumulated_eval_time': 6771.98642373085, 'accumulated_logging_time': 2.323227643966675}
I0307 21:46:04.981645 139702104409856 logging_writer.py:48] [47599] accumulated_eval_time=6771.99, accumulated_logging_time=2.32323, accumulated_submission_time=11537.3, global_step=47599, preemption_count=0, score=11537.3, test/accuracy=0.984846, test/loss=0.0650349, test/mean_average_precision=0.244576, test/num_examples=43793, total_duration=18313.1, train/accuracy=0.995378, train/loss=0.0140157, train/mean_average_precision=0.750734, validation/accuracy=0.985801, validation/loss=0.0608317, validation/mean_average_precision=0.251549, validation/num_examples=43793
I0307 21:46:05.442571 139702096017152 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.263593852519989, loss=0.011474226601421833
I0307 21:46:39.365663 139702104409856 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.24668990075588226, loss=0.010311907157301903
I0307 21:47:12.150386 139702096017152 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.31016185879707336, loss=0.011289427988231182
I0307 21:47:35.680581 139702104409856 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2643445134162903, loss=0.012323947623372078
I0307 21:48:09.594167 139702096017152 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2482057511806488, loss=0.01088035199791193
I0307 21:48:32.505022 139702104409856 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.2783466577529907, loss=0.01288334559649229
I0307 21:49:06.197924 139702096017152 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.24973924458026886, loss=0.010067316703498363
I0307 21:49:48.863973 139702104409856 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.20917260646820068, loss=0.009257782250642776
I0307 21:50:05.108795 139843824272576 spec.py:321] Evaluating on the training split.
I0307 21:54:15.873573 139843824272576 spec.py:333] Evaluating on the validation split.
I0307 21:54:46.292931 139843824272576 spec.py:349] Evaluating on the test split.
I0307 21:55:40.721733 139843824272576 submission_runner.py:469] Time since start: 18888.83s, 	Step: 48373, 	{'train/accuracy': 0.9951593279838562, 'train/loss': 0.014557230286300182, 'train/mean_average_precision': 0.7289603587544338, 'validation/accuracy': 0.9855918884277344, 'validation/loss': 0.0613689087331295, 'validation/mean_average_precision': 0.24973453223171174, 'validation/num_examples': 43793, 'test/accuracy': 0.9847699999809265, 'test/loss': 0.06537631154060364, 'test/mean_average_precision': 0.24401842668726562, 'test/num_examples': 43793, 'score': 11777.349622964859, 'total_duration': 18888.831733942032, 'accumulated_submission_time': 11777.349622964859, 'accumulated_eval_time': 7107.599301815033, 'accumulated_logging_time': 2.35101580619812}
I0307 21:55:40.770211 139702096017152 logging_writer.py:48] [48373] accumulated_eval_time=7107.6, accumulated_logging_time=2.35102, accumulated_submission_time=11777.3, global_step=48373, preemption_count=0, score=11777.3, test/accuracy=0.98477, test/loss=0.0653763, test/mean_average_precision=0.244018, test/num_examples=43793, total_duration=18888.8, train/accuracy=0.995159, train/loss=0.0145572, train/mean_average_precision=0.72896, validation/accuracy=0.985592, validation/loss=0.0613689, validation/mean_average_precision=0.249735, validation/num_examples=43793
I0307 21:55:47.085832 139702104409856 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.2513975203037262, loss=0.012204197235405445
I0307 21:56:09.734630 139702096017152 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.27368366718292236, loss=0.011219815351068974
I0307 21:56:43.682518 139702104409856 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3039725422859192, loss=0.01208850834518671
I0307 21:57:16.640490 139702096017152 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.24590492248535156, loss=0.010231757536530495
I0307 21:57:52.518381 139702104409856 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.24937601387500763, loss=0.01214849017560482
I0307 21:58:26.489360 139702096017152 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.27590563893318176, loss=0.010819010436534882
I0307 21:58:49.222268 139702104409856 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.22034254670143127, loss=0.009056353941559792
I0307 21:59:23.209390 139702096017152 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.22487056255340576, loss=0.009880940429866314
I0307 21:59:40.753302 139702104409856 logging_writer.py:48] [49179] global_step=49179, preemption_count=0, score=12017.3
I0307 21:59:41.078757 139843824272576 submission_runner.py:646] Tuning trial 3/5
I0307 21:59:41.078955 139843824272576 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 21:59:41.081505 139843824272576 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.4888734519481659, 'train/loss': 0.7987210750579834, 'train/mean_average_precision': 0.02035999178161005, 'validation/accuracy': 0.48879316449165344, 'validation/loss': 0.7953991889953613, 'validation/mean_average_precision': 0.023025011304165587, 'validation/num_examples': 43793, 'test/accuracy': 0.4907718300819397, 'test/loss': 0.7937217950820923, 'test/mean_average_precision': 0.0255982929472327, 'test/num_examples': 43793, 'score': 14.52869701385498, 'total_duration': 231.54671955108643, 'accumulated_submission_time': 14.52869701385498, 'accumulated_eval_time': 217.01790595054626, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1075, {'train/accuracy': 0.9866853952407837, 'train/loss': 0.054289259016513824, 'train/mean_average_precision': 0.046729288398821366, 'validation/accuracy': 0.9841580986976624, 'validation/loss': 0.06318428367376328, 'validation/mean_average_precision': 0.048025281764093555, 'validation/num_examples': 43793, 'test/accuracy': 0.9831863045692444, 'test/loss': 0.06626905500888824, 'test/mean_average_precision': 0.04927335139616927, 'test/num_examples': 43793, 'score': 254.65742588043213, 'total_duration': 552.8845226764679, 'accumulated_submission_time': 254.65742588043213, 'accumulated_eval_time': 298.1787006855011, 'accumulated_logging_time': 0.01788043975830078, 'global_step': 1075, 'preemption_count': 0}), (2161, {'train/accuracy': 0.9877825975418091, 'train/loss': 0.04426362365484238, 'train/mean_average_precision': 0.13192660961437241, 'validation/accuracy': 0.9849594235420227, 'validation/loss': 0.053551796823740005, 'validation/mean_average_precision': 0.13107382257029848, 'validation/num_examples': 43793, 'test/accuracy': 0.9840021729469299, 'test/loss': 0.0566747784614563, 'test/mean_average_precision': 0.12891195384055912, 'test/num_examples': 43793, 'score': 494.6748118400574, 'total_duration': 873.149251461029, 'accumulated_submission_time': 494.6748118400574, 'accumulated_eval_time': 378.377158164978, 'accumulated_logging_time': 0.03690958023071289, 'global_step': 2161, 'preemption_count': 0}), (3260, {'train/accuracy': 0.9883559942245483, 'train/loss': 0.040617696940898895, 'train/mean_average_precision': 0.18824316633658297, 'validation/accuracy': 0.9852334856987, 'validation/loss': 0.0507977195084095, 'validation/mean_average_precision': 0.15431984260257706, 'validation/num_examples': 43793, 'test/accuracy': 0.98442542552948, 'test/loss': 0.05346004292368889, 'test/mean_average_precision': 0.15982154157004205, 'test/num_examples': 43793, 'score': 734.7927660942078, 'total_duration': 1193.9048264026642, 'accumulated_submission_time': 734.7927660942078, 'accumulated_eval_time': 458.96581196784973, 'accumulated_logging_time': 0.05552506446838379, 'global_step': 3260, 'preemption_count': 0}), (4357, {'train/accuracy': 0.9884837865829468, 'train/loss': 0.03977229446172714, 'train/mean_average_precision': 0.20432664485055496, 'validation/accuracy': 0.9856292605400085, 'validation/loss': 0.04886684566736221, 'validation/mean_average_precision': 0.18680530843930473, 'validation/num_examples': 43793, 'test/accuracy': 0.9847333431243896, 'test/loss': 0.051688507199287415, 'test/mean_average_precision': 0.18614424673459568, 'test/num_examples': 43793, 'score': 974.8287355899811, 'total_duration': 1515.4936537742615, 'accumulated_submission_time': 974.8287355899811, 'accumulated_eval_time': 540.4691524505615, 'accumulated_logging_time': 0.07383108139038086, 'global_step': 4357, 'preemption_count': 0}), (5459, {'train/accuracy': 0.9887811541557312, 'train/loss': 0.038167599588632584, 'train/mean_average_precision': 0.23686243035877524, 'validation/accuracy': 0.9857709407806396, 'validation/loss': 0.04812463000416756, 'validation/mean_average_precision': 0.20061241729478005, 'validation/num_examples': 43793, 'test/accuracy': 0.9847227931022644, 'test/loss': 0.05108640715479851, 'test/mean_average_precision': 0.1900487215808086, 'test/num_examples': 43793, 'score': 1214.827980518341, 'total_duration': 1836.413230895996, 'accumulated_submission_time': 1214.827980518341, 'accumulated_eval_time': 621.3436448574066, 'accumulated_logging_time': 0.0921177864074707, 'global_step': 5459, 'preemption_count': 0}), (6574, {'train/accuracy': 0.9890382885932922, 'train/loss': 0.036975350230932236, 'train/mean_average_precision': 0.2600357684134873, 'validation/accuracy': 0.9860140681266785, 'validation/loss': 0.04712377488613129, 'validation/mean_average_precision': 0.20786160573019172, 'validation/num_examples': 43793, 'test/accuracy': 0.9852316379547119, 'test/loss': 0.04958077892661095, 'test/mean_average_precision': 0.2134764043731595, 'test/num_examples': 43793, 'score': 1454.9046506881714, 'total_duration': 2156.131096124649, 'accumulated_submission_time': 1454.9046506881714, 'accumulated_eval_time': 700.9361793994904, 'accumulated_logging_time': 0.11029505729675293, 'global_step': 6574, 'preemption_count': 0}), (7685, {'train/accuracy': 0.9893725514411926, 'train/loss': 0.035915616899728775, 'train/mean_average_precision': 0.27092566175358246, 'validation/accuracy': 0.9861992001533508, 'validation/loss': 0.04646225646138191, 'validation/mean_average_precision': 0.2163233596838837, 'validation/num_examples': 43793, 'test/accuracy': 0.985359251499176, 'test/loss': 0.04879593849182129, 'test/mean_average_precision': 0.2240232027142597, 'test/num_examples': 43793, 'score': 1695.039249420166, 'total_duration': 2476.1119663715363, 'accumulated_submission_time': 1695.039249420166, 'accumulated_eval_time': 780.7337276935577, 'accumulated_logging_time': 0.12884211540222168, 'global_step': 7685, 'preemption_count': 0}), (8792, {'train/accuracy': 0.9899721145629883, 'train/loss': 0.033684421330690384, 'train/mean_average_precision': 0.3288627040656614, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.0454062893986702, 'validation/mean_average_precision': 0.23219470481186744, 'validation/num_examples': 43793, 'test/accuracy': 0.985499918460846, 'test/loss': 0.04803035035729408, 'test/mean_average_precision': 0.22858181276034606, 'test/num_examples': 43793, 'score': 1935.0809762477875, 'total_duration': 2795.2644288539886, 'accumulated_submission_time': 1935.0809762477875, 'accumulated_eval_time': 859.7944459915161, 'accumulated_logging_time': 0.149428129196167, 'global_step': 8792, 'preemption_count': 0}), (9896, {'train/accuracy': 0.9899682998657227, 'train/loss': 0.03352784737944603, 'train/mean_average_precision': 0.3195062179556214, 'validation/accuracy': 0.9865978360176086, 'validation/loss': 0.04521291330456734, 'validation/mean_average_precision': 0.245911743780941, 'validation/num_examples': 43793, 'test/accuracy': 0.9856582880020142, 'test/loss': 0.04790876433253288, 'test/mean_average_precision': 0.23827438733183215, 'test/num_examples': 43793, 'score': 2175.1418731212616, 'total_duration': 3115.875633239746, 'accumulated_submission_time': 2175.1418731212616, 'accumulated_eval_time': 940.2949657440186, 'accumulated_logging_time': 0.17014241218566895, 'global_step': 9896, 'preemption_count': 0}), (10999, {'train/accuracy': 0.9902253746986389, 'train/loss': 0.03255723789334297, 'train/mean_average_precision': 0.36059278516434967, 'validation/accuracy': 0.9865081310272217, 'validation/loss': 0.04516172409057617, 'validation/mean_average_precision': 0.2468036560906089, 'validation/num_examples': 43793, 'test/accuracy': 0.9856081604957581, 'test/loss': 0.047820981591939926, 'test/mean_average_precision': 0.2407608601714161, 'test/num_examples': 43793, 'score': 2415.1224250793457, 'total_duration': 3436.622328042984, 'accumulated_submission_time': 2415.1224250793457, 'accumulated_eval_time': 1021.0095772743225, 'accumulated_logging_time': 0.1892995834350586, 'global_step': 10999, 'preemption_count': 0}), (12117, {'train/accuracy': 0.9902422428131104, 'train/loss': 0.03200624883174896, 'train/mean_average_precision': 0.36505012629759903, 'validation/accuracy': 0.986553966999054, 'validation/loss': 0.04547463729977608, 'validation/mean_average_precision': 0.2499866526674444, 'validation/num_examples': 43793, 'test/accuracy': 0.9856384992599487, 'test/loss': 0.04826817288994789, 'test/mean_average_precision': 0.2434509874460612, 'test/num_examples': 43793, 'score': 2655.077239751816, 'total_duration': 3756.2813346385956, 'accumulated_submission_time': 2655.077239751816, 'accumulated_eval_time': 1100.6659760475159, 'accumulated_logging_time': 0.2096247673034668, 'global_step': 12117, 'preemption_count': 0}), (13233, {'train/accuracy': 0.99032062292099, 'train/loss': 0.03178824111819267, 'train/mean_average_precision': 0.3758721021864116, 'validation/accuracy': 0.9866928458213806, 'validation/loss': 0.044864654541015625, 'validation/mean_average_precision': 0.2594236899803729, 'validation/num_examples': 43793, 'test/accuracy': 0.9858676195144653, 'test/loss': 0.047269709408283234, 'test/mean_average_precision': 0.2508124205066057, 'test/num_examples': 43793, 'score': 2895.0889592170715, 'total_duration': 4074.0239820480347, 'accumulated_submission_time': 2895.0889592170715, 'accumulated_eval_time': 1178.3498539924622, 'accumulated_logging_time': 0.22901105880737305, 'global_step': 13233, 'preemption_count': 0}), (14348, {'train/accuracy': 0.9909123778343201, 'train/loss': 0.029886435717344284, 'train/mean_average_precision': 0.4046758114023122, 'validation/accuracy': 0.986763060092926, 'validation/loss': 0.04458058252930641, 'validation/mean_average_precision': 0.26890911444858634, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.047543738037347794, 'test/mean_average_precision': 0.2539929108283998, 'test/num_examples': 43793, 'score': 3135.0638337135315, 'total_duration': 4391.830424070358, 'accumulated_submission_time': 3135.0638337135315, 'accumulated_eval_time': 1256.1301290988922, 'accumulated_logging_time': 0.24869632720947266, 'global_step': 14348, 'preemption_count': 0}), (15464, {'train/accuracy': 0.9907843470573425, 'train/loss': 0.030587248504161835, 'train/mean_average_precision': 0.38778817703076657, 'validation/accuracy': 0.9866079688072205, 'validation/loss': 0.044562287628650665, 'validation/mean_average_precision': 0.2592255149954002, 'validation/num_examples': 43793, 'test/accuracy': 0.9858006238937378, 'test/loss': 0.04721960052847862, 'test/mean_average_precision': 0.2562299253963582, 'test/num_examples': 43793, 'score': 3375.1132776737213, 'total_duration': 4714.702702045441, 'accumulated_submission_time': 3375.1132776737213, 'accumulated_eval_time': 1338.9045150279999, 'accumulated_logging_time': 0.2691154479980469, 'global_step': 15464, 'preemption_count': 0}), (16572, {'train/accuracy': 0.9912809133529663, 'train/loss': 0.028598172590136528, 'train/mean_average_precision': 0.4450888484206308, 'validation/accuracy': 0.9867825508117676, 'validation/loss': 0.04463914781808853, 'validation/mean_average_precision': 0.2656126200153692, 'validation/num_examples': 43793, 'test/accuracy': 0.9859177470207214, 'test/loss': 0.047120094299316406, 'test/mean_average_precision': 0.2642979796032486, 'test/num_examples': 43793, 'score': 3615.212069272995, 'total_duration': 5031.692683458328, 'accumulated_submission_time': 3615.212069272995, 'accumulated_eval_time': 1415.7461564540863, 'accumulated_logging_time': 0.29024839401245117, 'global_step': 16572, 'preemption_count': 0}), (17676, {'train/accuracy': 0.9909286499023438, 'train/loss': 0.029505448415875435, 'train/mean_average_precision': 0.42648156214677246, 'validation/accuracy': 0.986790657043457, 'validation/loss': 0.04442321136593819, 'validation/mean_average_precision': 0.26463555099577357, 'validation/num_examples': 43793, 'test/accuracy': 0.9859021306037903, 'test/loss': 0.04707392305135727, 'test/mean_average_precision': 0.26354958654526683, 'test/num_examples': 43793, 'score': 3855.302666902542, 'total_duration': 5353.121511936188, 'accumulated_submission_time': 3855.302666902542, 'accumulated_eval_time': 1497.0362107753754, 'accumulated_logging_time': 0.31109070777893066, 'global_step': 17676, 'preemption_count': 0}), (18780, {'train/accuracy': 0.9910614490509033, 'train/loss': 0.029216507449746132, 'train/mean_average_precision': 0.42080688829826324, 'validation/accuracy': 0.9866063594818115, 'validation/loss': 0.04475586488842964, 'validation/mean_average_precision': 0.26023441091744975, 'validation/num_examples': 43793, 'test/accuracy': 0.9858258962631226, 'test/loss': 0.047403931617736816, 'test/mean_average_precision': 0.2607284701018005, 'test/num_examples': 43793, 'score': 4095.385016679764, 'total_duration': 5673.010764122009, 'accumulated_submission_time': 4095.385016679764, 'accumulated_eval_time': 1576.7914912700653, 'accumulated_logging_time': 0.33441829681396484, 'global_step': 18780, 'preemption_count': 0}), (19886, {'train/accuracy': 0.9914048314094543, 'train/loss': 0.028049286454916, 'train/mean_average_precision': 0.4539812954122756, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.044580765068531036, 'validation/mean_average_precision': 0.2738403266025462, 'validation/num_examples': 43793, 'test/accuracy': 0.9860074520111084, 'test/loss': 0.04707479849457741, 'test/mean_average_precision': 0.26638148722373917, 'test/num_examples': 43793, 'score': 4335.533841133118, 'total_duration': 5989.460833787918, 'accumulated_submission_time': 4335.533841133118, 'accumulated_eval_time': 1653.0426771640778, 'accumulated_logging_time': 0.3563382625579834, 'global_step': 19886, 'preemption_count': 0}), (20988, {'train/accuracy': 0.9912294745445251, 'train/loss': 0.028580479323863983, 'train/mean_average_precision': 0.4343371213016963, 'validation/accuracy': 0.9867683053016663, 'validation/loss': 0.04479736462235451, 'validation/mean_average_precision': 0.2746105046784574, 'validation/num_examples': 43793, 'test/accuracy': 0.9859354496002197, 'test/loss': 0.04769391939043999, 'test/mean_average_precision': 0.26374140606351926, 'test/num_examples': 43793, 'score': 4575.517915248871, 'total_duration': 6312.240740060806, 'accumulated_submission_time': 4575.517915248871, 'accumulated_eval_time': 1735.7854135036469, 'accumulated_logging_time': 0.3794248104095459, 'global_step': 20988, 'preemption_count': 0}), (22034, {'train/accuracy': 0.9917982816696167, 'train/loss': 0.02672702819108963, 'train/mean_average_precision': 0.48281166940679643, 'validation/accuracy': 0.9868012070655823, 'validation/loss': 0.04453675076365471, 'validation/mean_average_precision': 0.27392685859045196, 'validation/num_examples': 43793, 'test/accuracy': 0.9859341979026794, 'test/loss': 0.047351229935884476, 'test/mean_average_precision': 0.26296631713252744, 'test/num_examples': 43793, 'score': 4815.536692857742, 'total_duration': 6634.643180608749, 'accumulated_submission_time': 4815.536692857742, 'accumulated_eval_time': 1818.117932319641, 'accumulated_logging_time': 0.4002974033355713, 'global_step': 22034, 'preemption_count': 0}), (23072, {'train/accuracy': 0.9917907118797302, 'train/loss': 0.026596948504447937, 'train/mean_average_precision': 0.48982699993218104, 'validation/accuracy': 0.9867756366729736, 'validation/loss': 0.04485158994793892, 'validation/mean_average_precision': 0.2710567277612556, 'validation/num_examples': 43793, 'test/accuracy': 0.9859097599983215, 'test/loss': 0.047569870948791504, 'test/mean_average_precision': 0.25911274933743883, 'test/num_examples': 43793, 'score': 5055.495976686478, 'total_duration': 6954.921161651611, 'accumulated_submission_time': 5055.495976686478, 'accumulated_eval_time': 1898.386614561081, 'accumulated_logging_time': 0.42129945755004883, 'global_step': 23072, 'preemption_count': 0}), (24119, {'train/accuracy': 0.991434633731842, 'train/loss': 0.027600200846791267, 'train/mean_average_precision': 0.45611483435552574, 'validation/accuracy': 0.986748456954956, 'validation/loss': 0.0447673499584198, 'validation/mean_average_precision': 0.2722016577472177, 'validation/num_examples': 43793, 'test/accuracy': 0.9859842658042908, 'test/loss': 0.04744948819279671, 'test/mean_average_precision': 0.26159257253742085, 'test/num_examples': 43793, 'score': 5295.512573003769, 'total_duration': 7277.541084766388, 'accumulated_submission_time': 5295.512573003769, 'accumulated_eval_time': 1980.937319278717, 'accumulated_logging_time': 0.4444100856781006, 'global_step': 24119, 'preemption_count': 0}), (25157, {'train/accuracy': 0.9919379353523254, 'train/loss': 0.02624361589550972, 'train/mean_average_precision': 0.489644977739022, 'validation/accuracy': 0.9867597818374634, 'validation/loss': 0.04496688023209572, 'validation/mean_average_precision': 0.2719349561106214, 'validation/num_examples': 43793, 'test/accuracy': 0.985878586769104, 'test/loss': 0.04765566438436508, 'test/mean_average_precision': 0.2586091683531267, 'test/num_examples': 43793, 'score': 5535.626770973206, 'total_duration': 7597.235291481018, 'accumulated_submission_time': 5535.626770973206, 'accumulated_eval_time': 2060.4661214351654, 'accumulated_logging_time': 0.46613121032714844, 'global_step': 25157, 'preemption_count': 0}), (26189, {'train/accuracy': 0.9922596216201782, 'train/loss': 0.0248671043664217, 'train/mean_average_precision': 0.5262551192935045, 'validation/accuracy': 0.9868000149726868, 'validation/loss': 0.04518637806177139, 'validation/mean_average_precision': 0.2760459917216366, 'validation/num_examples': 43793, 'test/accuracy': 0.9858697056770325, 'test/loss': 0.04804248735308647, 'test/mean_average_precision': 0.2641128503355187, 'test/num_examples': 43793, 'score': 5775.725816726685, 'total_duration': 7947.548739433289, 'accumulated_submission_time': 5775.725816726685, 'accumulated_eval_time': 2170.6278228759766, 'accumulated_logging_time': 0.48775815963745117, 'global_step': 26189, 'preemption_count': 0}), (27208, {'train/accuracy': 0.9919833540916443, 'train/loss': 0.025941427797079086, 'train/mean_average_precision': 0.5034283641119102, 'validation/accuracy': 0.9866704940795898, 'validation/loss': 0.045347534120082855, 'validation/mean_average_precision': 0.2740679644359419, 'validation/num_examples': 43793, 'test/accuracy': 0.9858124256134033, 'test/loss': 0.048293180763721466, 'test/mean_average_precision': 0.26285946522114834, 'test/num_examples': 43793, 'score': 6015.687978506088, 'total_duration': 8333.761028766632, 'accumulated_submission_time': 6015.687978506088, 'accumulated_eval_time': 2316.8257219791412, 'accumulated_logging_time': 0.509462833404541, 'global_step': 27208, 'preemption_count': 0}), (28199, {'train/accuracy': 0.9921227097511292, 'train/loss': 0.025318870320916176, 'train/mean_average_precision': 0.5220592412870835, 'validation/accuracy': 0.9866359829902649, 'validation/loss': 0.04564915597438812, 'validation/mean_average_precision': 0.2725394367112026, 'validation/num_examples': 43793, 'test/accuracy': 0.9858288764953613, 'test/loss': 0.04845212399959564, 'test/mean_average_precision': 0.2653679971134227, 'test/num_examples': 43793, 'score': 6255.508816003799, 'total_duration': 8657.011657238007, 'accumulated_submission_time': 6255.508816003799, 'accumulated_eval_time': 2399.86354136467, 'accumulated_logging_time': 0.8720173835754395, 'global_step': 28199, 'preemption_count': 0}), (29211, {'train/accuracy': 0.9929994344711304, 'train/loss': 0.022669773548841476, 'train/mean_average_precision': 0.5693031603866031, 'validation/accuracy': 0.9866693019866943, 'validation/loss': 0.04578140377998352, 'validation/mean_average_precision': 0.27938112841132345, 'validation/num_examples': 43793, 'test/accuracy': 0.9858431816101074, 'test/loss': 0.04852947220206261, 'test/mean_average_precision': 0.2695027912116471, 'test/num_examples': 43793, 'score': 6495.568902730942, 'total_duration': 9025.497288227081, 'accumulated_submission_time': 6495.568902730942, 'accumulated_eval_time': 2528.2351665496826, 'accumulated_logging_time': 0.896010160446167, 'global_step': 29211, 'preemption_count': 0}), (30171, {'train/accuracy': 0.9926010966300964, 'train/loss': 0.02342505007982254, 'train/mean_average_precision': 0.5577519849501482, 'validation/accuracy': 0.9865056872367859, 'validation/loss': 0.046440642327070236, 'validation/mean_average_precision': 0.2673776522013648, 'validation/num_examples': 43793, 'test/accuracy': 0.9856102466583252, 'test/loss': 0.04942326992750168, 'test/mean_average_precision': 0.25640913745024424, 'test/num_examples': 43793, 'score': 6735.489598035812, 'total_duration': 9362.765830516815, 'accumulated_submission_time': 6735.489598035812, 'accumulated_eval_time': 2625.422542333603, 'accumulated_logging_time': 1.023122787475586, 'global_step': 30171, 'preemption_count': 0}), (31163, {'train/accuracy': 0.9926149249076843, 'train/loss': 0.023582249879837036, 'train/mean_average_precision': 0.5420363427953263, 'validation/accuracy': 0.9865077137947083, 'validation/loss': 0.04630015417933464, 'validation/mean_average_precision': 0.2710621432951711, 'validation/num_examples': 43793, 'test/accuracy': 0.9858199954032898, 'test/loss': 0.04889128357172012, 'test/mean_average_precision': 0.268126563513086, 'test/num_examples': 43793, 'score': 6975.553866386414, 'total_duration': 9739.803375959396, 'accumulated_submission_time': 6975.553866386414, 'accumulated_eval_time': 2762.340423822403, 'accumulated_logging_time': 1.0473568439483643, 'global_step': 31163, 'preemption_count': 0}), (32126, {'train/accuracy': 0.9926468133926392, 'train/loss': 0.02340000867843628, 'train/mean_average_precision': 0.5606597311428207, 'validation/accuracy': 0.9866757392883301, 'validation/loss': 0.04663236066699028, 'validation/mean_average_precision': 0.2687272938360035, 'validation/num_examples': 43793, 'test/accuracy': 0.9857951998710632, 'test/loss': 0.04956445470452309, 'test/mean_average_precision': 0.26287872559364395, 'test/num_examples': 43793, 'score': 7215.65358710289, 'total_duration': 10136.660241127014, 'accumulated_submission_time': 7215.65358710289, 'accumulated_eval_time': 2919.0435349941254, 'accumulated_logging_time': 1.0708155632019043, 'global_step': 32126, 'preemption_count': 0}), (33070, {'train/accuracy': 0.9931266903877258, 'train/loss': 0.02186143770813942, 'train/mean_average_precision': 0.5854248971690507, 'validation/accuracy': 0.9865105748176575, 'validation/loss': 0.0472368486225605, 'validation/mean_average_precision': 0.2727369982932136, 'validation/num_examples': 43793, 'test/accuracy': 0.9856507182121277, 'test/loss': 0.05016958341002464, 'test/mean_average_precision': 0.25712994198602157, 'test/num_examples': 43793, 'score': 7455.74582695961, 'total_duration': 10484.880962371826, 'accumulated_submission_time': 7455.74582695961, 'accumulated_eval_time': 3027.117034673691, 'accumulated_logging_time': 1.09440016746521, 'global_step': 33070, 'preemption_count': 0}), (34020, {'train/accuracy': 0.9937687516212463, 'train/loss': 0.01991601660847664, 'train/mean_average_precision': 0.6384030764002606, 'validation/accuracy': 0.9866607189178467, 'validation/loss': 0.04756362363696098, 'validation/mean_average_precision': 0.27655691076652733, 'validation/num_examples': 43793, 'test/accuracy': 0.9858170747756958, 'test/loss': 0.050600532442331314, 'test/mean_average_precision': 0.2722631267531079, 'test/num_examples': 43793, 'score': 7695.694377422333, 'total_duration': 10869.659365177155, 'accumulated_submission_time': 7695.694377422333, 'accumulated_eval_time': 3171.8918023109436, 'accumulated_logging_time': 1.1169514656066895, 'global_step': 34020, 'preemption_count': 0}), (34963, {'train/accuracy': 0.993643045425415, 'train/loss': 0.0199844129383564, 'train/mean_average_precision': 0.6165708895013072, 'validation/accuracy': 0.9866185188293457, 'validation/loss': 0.04814017936587334, 'validation/mean_average_precision': 0.27742874992882666, 'validation/num_examples': 43793, 'test/accuracy': 0.9856843948364258, 'test/loss': 0.05150644853711128, 'test/mean_average_precision': 0.25915414157150624, 'test/num_examples': 43793, 'score': 7935.719910144806, 'total_duration': 11283.90010547638, 'accumulated_submission_time': 7935.719910144806, 'accumulated_eval_time': 3345.9208443164825, 'accumulated_logging_time': 1.27052640914917, 'global_step': 34963, 'preemption_count': 0}), (35894, {'train/accuracy': 0.9933398365974426, 'train/loss': 0.0208708718419075, 'train/mean_average_precision': 0.609296342599606, 'validation/accuracy': 0.9865304231643677, 'validation/loss': 0.04874027147889137, 'validation/mean_average_precision': 0.26971469409770044, 'validation/num_examples': 43793, 'test/accuracy': 0.9856528043746948, 'test/loss': 0.051855143159627914, 'test/mean_average_precision': 0.26619169287074146, 'test/num_examples': 43793, 'score': 8175.753685235977, 'total_duration': 11674.138521909714, 'accumulated_submission_time': 8175.753685235977, 'accumulated_eval_time': 3496.071601629257, 'accumulated_logging_time': 1.2942588329315186, 'global_step': 35894, 'preemption_count': 0}), (36807, {'train/accuracy': 0.9934282898902893, 'train/loss': 0.020509324967861176, 'train/mean_average_precision': 0.6145964473428487, 'validation/accuracy': 0.9863453507423401, 'validation/loss': 0.049681417644023895, 'validation/mean_average_precision': 0.2604879474346703, 'validation/num_examples': 43793, 'test/accuracy': 0.9855390787124634, 'test/loss': 0.05275654420256615, 'test/mean_average_precision': 0.25872547916827, 'test/num_examples': 43793, 'score': 8415.754895925522, 'total_duration': 12108.038880348206, 'accumulated_submission_time': 8415.754895925522, 'accumulated_eval_time': 3689.9144699573517, 'accumulated_logging_time': 1.319704532623291, 'global_step': 36807, 'preemption_count': 0}), (37691, {'train/accuracy': 0.9934500455856323, 'train/loss': 0.020330805331468582, 'train/mean_average_precision': 0.6181393482559931, 'validation/accuracy': 0.9865032434463501, 'validation/loss': 0.05014101415872574, 'validation/mean_average_precision': 0.26814789675695905, 'validation/num_examples': 43793, 'test/accuracy': 0.9856207966804504, 'test/loss': 0.05352601036429405, 'test/mean_average_precision': 0.2591445806767985, 'test/num_examples': 43793, 'score': 8655.852705955505, 'total_duration': 12529.12860250473, 'accumulated_submission_time': 8655.852705955505, 'accumulated_eval_time': 3870.851680994034, 'accumulated_logging_time': 1.3429534435272217, 'global_step': 37691, 'preemption_count': 0}), (38553, {'train/accuracy': 0.9939141869544983, 'train/loss': 0.01909920759499073, 'train/mean_average_precision': 0.644408953255529, 'validation/accuracy': 0.9863400459289551, 'validation/loss': 0.05091005563735962, 'validation/mean_average_precision': 0.2639433104015775, 'validation/num_examples': 43793, 'test/accuracy': 0.9854497909545898, 'test/loss': 0.054305531084537506, 'test/mean_average_precision': 0.25520256352965703, 'test/num_examples': 43793, 'score': 8895.822674751282, 'total_duration': 12942.242048025131, 'accumulated_submission_time': 8895.822674751282, 'accumulated_eval_time': 4043.8294870853424, 'accumulated_logging_time': 1.476332187652588, 'global_step': 38553, 'preemption_count': 0}), (39422, {'train/accuracy': 0.9935669302940369, 'train/loss': 0.01988809183239937, 'train/mean_average_precision': 0.6251321413531297, 'validation/accuracy': 0.9864175915718079, 'validation/loss': 0.0521615631878376, 'validation/mean_average_precision': 0.2610725248257678, 'validation/num_examples': 43793, 'test/accuracy': 0.9855833053588867, 'test/loss': 0.05540802702307701, 'test/mean_average_precision': 0.25717389173247834, 'test/num_examples': 43793, 'score': 9135.771723985672, 'total_duration': 13335.784710407257, 'accumulated_submission_time': 9135.771723985672, 'accumulated_eval_time': 4197.2437608242035, 'accumulated_logging_time': 1.6233999729156494, 'global_step': 39422, 'preemption_count': 0}), (40286, {'train/accuracy': 0.9943349361419678, 'train/loss': 0.017571382224559784, 'train/mean_average_precision': 0.6772662046876651, 'validation/accuracy': 0.9862921833992004, 'validation/loss': 0.052747711539268494, 'validation/mean_average_precision': 0.25928499892049306, 'validation/num_examples': 43793, 'test/accuracy': 0.98539799451828, 'test/loss': 0.056186046451330185, 'test/mean_average_precision': 0.2585284110205704, 'test/num_examples': 43793, 'score': 9375.897370576859, 'total_duration': 13784.2063434124, 'accumulated_submission_time': 9375.897370576859, 'accumulated_eval_time': 4405.480292797089, 'accumulated_logging_time': 1.648796796798706, 'global_step': 40286, 'preemption_count': 0}), (41125, {'train/accuracy': 0.9947344660758972, 'train/loss': 0.016399336978793144, 'train/mean_average_precision': 0.6997507287158085, 'validation/accuracy': 0.9861768484115601, 'validation/loss': 0.0537332147359848, 'validation/mean_average_precision': 0.25899301959266624, 'validation/num_examples': 43793, 'test/accuracy': 0.9852463603019714, 'test/loss': 0.05744802951812744, 'test/mean_average_precision': 0.25201856053062216, 'test/num_examples': 43793, 'score': 9615.874061346054, 'total_duration': 14225.13319349289, 'accumulated_submission_time': 9615.874061346054, 'accumulated_eval_time': 4606.3549337387085, 'accumulated_logging_time': 1.6909832954406738, 'global_step': 41125, 'preemption_count': 0}), (42004, {'train/accuracy': 0.9952811598777771, 'train/loss': 0.014881030656397343, 'train/mean_average_precision': 0.7382278069221647, 'validation/accuracy': 0.9861959218978882, 'validation/loss': 0.054622769355773926, 'validation/mean_average_precision': 0.26028120222327283, 'validation/num_examples': 43793, 'test/accuracy': 0.9852411150932312, 'test/loss': 0.05846964567899704, 'test/mean_average_precision': 0.2500642277622693, 'test/num_examples': 43793, 'score': 9855.845724344254, 'total_duration': 14750.819407701492, 'accumulated_submission_time': 9855.845724344254, 'accumulated_eval_time': 4892.011008024216, 'accumulated_logging_time': 1.715456485748291, 'global_step': 42004, 'preemption_count': 0}), (42849, {'train/accuracy': 0.995642900466919, 'train/loss': 0.013742235489189625, 'train/mean_average_precision': 0.753501697560599, 'validation/accuracy': 0.9861090779304504, 'validation/loss': 0.056181859225034714, 'validation/mean_average_precision': 0.254918822718795, 'validation/num_examples': 43793, 'test/accuracy': 0.9853225946426392, 'test/loss': 0.059770505875349045, 'test/mean_average_precision': 0.2513521919183122, 'test/num_examples': 43793, 'score': 10095.815959692001, 'total_duration': 15188.063847780228, 'accumulated_submission_time': 10095.815959692001, 'accumulated_eval_time': 5089.168545722961, 'accumulated_logging_time': 1.7977545261383057, 'global_step': 42849, 'preemption_count': 0}), (43682, {'train/accuracy': 0.9958009719848633, 'train/loss': 0.013466255739331245, 'train/mean_average_precision': 0.7552365720972756, 'validation/accuracy': 0.9860116243362427, 'validation/loss': 0.056640807539224625, 'validation/mean_average_precision': 0.2538939584771832, 'validation/num_examples': 43793, 'test/accuracy': 0.9851263165473938, 'test/loss': 0.06039847433567047, 'test/mean_average_precision': 0.24841706991387588, 'test/num_examples': 43793, 'score': 10336.181158542633, 'total_duration': 15670.932301282883, 'accumulated_submission_time': 10336.181158542633, 'accumulated_eval_time': 5331.491797208786, 'accumulated_logging_time': 1.9403514862060547, 'global_step': 43682, 'preemption_count': 0}), (44482, {'train/accuracy': 0.9959072470664978, 'train/loss': 0.01304843183606863, 'train/mean_average_precision': 0.7722820084387875, 'validation/accuracy': 0.9858528971672058, 'validation/loss': 0.05783652514219284, 'validation/mean_average_precision': 0.25041395809068845, 'validation/num_examples': 43793, 'test/accuracy': 0.9850125908851624, 'test/loss': 0.06157413125038147, 'test/mean_average_precision': 0.250204750012491, 'test/num_examples': 43793, 'score': 10576.863844633102, 'total_duration': 16168.954593896866, 'accumulated_submission_time': 10576.863844633102, 'accumulated_eval_time': 5588.769352674484, 'accumulated_logging_time': 1.9652602672576904, 'global_step': 44482, 'preemption_count': 0}), (45309, {'train/accuracy': 0.9961453676223755, 'train/loss': 0.012363744899630547, 'train/mean_average_precision': 0.7793193345344225, 'validation/accuracy': 0.9858058094978333, 'validation/loss': 0.05860166624188423, 'validation/mean_average_precision': 0.2544785951342224, 'validation/num_examples': 43793, 'test/accuracy': 0.9848681092262268, 'test/loss': 0.06258011609315872, 'test/mean_average_precision': 0.24811656616712924, 'test/num_examples': 43793, 'score': 10816.806696891785, 'total_duration': 16712.54499554634, 'accumulated_submission_time': 10816.806696891785, 'accumulated_eval_time': 5892.239295959473, 'accumulated_logging_time': 2.1094772815704346, 'global_step': 45309, 'preemption_count': 0}), (46143, {'train/accuracy': 0.9963669180870056, 'train/loss': 0.011866775341331959, 'train/mean_average_precision': 0.7887896196002202, 'validation/accuracy': 0.9856812357902527, 'validation/loss': 0.059525396674871445, 'validation/mean_average_precision': 0.25578924493201816, 'validation/num_examples': 43793, 'test/accuracy': 0.9847535490989685, 'test/loss': 0.06360175460577011, 'test/mean_average_precision': 0.2454862852846679, 'test/num_examples': 43793, 'score': 11057.497724294662, 'total_duration': 17175.820520162582, 'accumulated_submission_time': 11057.497724294662, 'accumulated_eval_time': 6114.765877008438, 'accumulated_logging_time': 2.1347782611846924, 'global_step': 46143, 'preemption_count': 0}), (46881, {'train/accuracy': 0.9959153532981873, 'train/loss': 0.012638011947274208, 'train/mean_average_precision': 0.775536081396346, 'validation/accuracy': 0.9857904314994812, 'validation/loss': 0.06019804626703262, 'validation/mean_average_precision': 0.25509967275415013, 'validation/num_examples': 43793, 'test/accuracy': 0.9848432540893555, 'test/loss': 0.06444653123617172, 'test/mean_average_precision': 0.24479387503080025, 'test/num_examples': 43793, 'score': 11297.303407669067, 'total_duration': 17760.602556705475, 'accumulated_submission_time': 11297.303407669067, 'accumulated_eval_time': 6459.545073747635, 'accumulated_logging_time': 2.2961666584014893, 'global_step': 46881, 'preemption_count': 0}), (47599, {'train/accuracy': 0.9953779578208923, 'train/loss': 0.014015729539096355, 'train/mean_average_precision': 0.7507341103664186, 'validation/accuracy': 0.985800564289093, 'validation/loss': 0.060831695795059204, 'validation/mean_average_precision': 0.25154888792827207, 'validation/num_examples': 43793, 'test/accuracy': 0.9848458170890808, 'test/loss': 0.06503494828939438, 'test/mean_average_precision': 0.24457584547622235, 'test/num_examples': 43793, 'score': 11537.270168542862, 'total_duration': 18313.07462143898, 'accumulated_submission_time': 11537.270168542862, 'accumulated_eval_time': 6771.98642373085, 'accumulated_logging_time': 2.323227643966675, 'global_step': 47599, 'preemption_count': 0}), (48373, {'train/accuracy': 0.9951593279838562, 'train/loss': 0.014557230286300182, 'train/mean_average_precision': 0.7289603587544338, 'validation/accuracy': 0.9855918884277344, 'validation/loss': 0.0613689087331295, 'validation/mean_average_precision': 0.24973453223171174, 'validation/num_examples': 43793, 'test/accuracy': 0.9847699999809265, 'test/loss': 0.06537631154060364, 'test/mean_average_precision': 0.24401842668726562, 'test/num_examples': 43793, 'score': 11777.349622964859, 'total_duration': 18888.831733942032, 'accumulated_submission_time': 11777.349622964859, 'accumulated_eval_time': 7107.599301815033, 'accumulated_logging_time': 2.35101580619812, 'global_step': 48373, 'preemption_count': 0})], 'global_step': 49179}
I0307 21:59:41.081631 139843824272576 submission_runner.py:649] Timing: 12017.279103517532
I0307 21:59:41.081671 139843824272576 submission_runner.py:651] Total number of evals: 50
I0307 21:59:41.081699 139843824272576 submission_runner.py:652] ====================
I0307 21:59:41.082154 139843824272576 submission_runner.py:750] Final ogbg score: 2
