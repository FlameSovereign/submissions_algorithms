python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-844482572 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/ogbg_jax_03-07-2025-16-24-31.log
2025-03-07 16:24:35.350717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741364675.375659       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741364675.383303       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 16:24:44.002871 139659327427776 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax.
I0307 16:24:44.905795 139659327427776 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 16:24:44.908645 139659327427776 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 16:24:44.910444 139659327427776 submission_runner.py:606] Using RNG seed -844482572
I0307 16:24:45.507772 139659327427776 submission_runner.py:615] --- Tuning run 1/5 ---
I0307 16:24:45.507976 139659327427776 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_1.
I0307 16:24:45.508182 139659327427776 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_1/hparams.json.
I0307 16:24:45.744998 139659327427776 submission_runner.py:218] Initializing dataset.
I0307 16:24:46.424192 139659327427776 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:24:46.466001 139659327427776 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0307 16:24:46.711798 139659327427776 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0307 16:24:46.903244 139659327427776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:24:46.933929 139659327427776 submission_runner.py:229] Initializing model.
I0307 16:24:54.739508 139659327427776 submission_runner.py:272] Initializing optimizer.
I0307 16:24:55.184189 139659327427776 submission_runner.py:279] Initializing metrics bundle.
I0307 16:24:55.184434 139659327427776 submission_runner.py:301] Initializing checkpoint and logger.
I0307 16:24:55.185316 139659327427776 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_1 with prefix checkpoint_
I0307 16:24:55.185423 139659327427776 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_1/meta_data_0.json.
I0307 16:24:55.185602 139659327427776 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 16:24:55.185651 139659327427776 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 16:24:55.348797 139659327427776 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_0/ogbg_jax/trial_1/flags_0.json.
I0307 16:24:55.389439 139659327427776 submission_runner.py:337] Starting training loop.
I0307 16:25:07.141520 139523175642880 logging_writer.py:48] [0] global_step=0, grad_norm=2.5407731533050537, loss=0.7521843314170837
I0307 16:25:07.194356 139659327427776 spec.py:321] Evaluating on the training split.
I0307 16:25:07.197936 139659327427776 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:25:07.201754 139659327427776 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:25:07.263881 139659327427776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:26:26.086515 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 16:26:26.089282 139659327427776 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:26:26.093179 139659327427776 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:26:26.154846 139659327427776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:27:31.832605 139659327427776 spec.py:349] Evaluating on the test split.
I0307 16:27:31.835103 139659327427776 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:27:31.838528 139659327427776 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0307 16:27:31.899540 139659327427776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0307 16:28:37.957937 139659327427776 submission_runner.py:469] Time since start: 222.57s, 	Step: 1, 	{'train/accuracy': 0.48254281282424927, 'train/loss': 0.7525402903556824, 'train/mean_average_precision': 0.024656325580167525, 'validation/accuracy': 0.48500412702560425, 'validation/loss': 0.749235987663269, 'validation/mean_average_precision': 0.02684926652430975, 'validation/num_examples': 43793, 'test/accuracy': 0.4849407970905304, 'test/loss': 0.7482256889343262, 'test/mean_average_precision': 0.02805860381982226, 'test/num_examples': 43793, 'score': 11.804836511611938, 'total_duration': 222.5684370994568, 'accumulated_submission_time': 11.804836511611938, 'accumulated_eval_time': 210.76350450515747, 'accumulated_logging_time': 0}
I0307 16:28:37.965557 139517331461888 logging_writer.py:48] [1] accumulated_eval_time=210.764, accumulated_logging_time=0, accumulated_submission_time=11.8048, global_step=1, preemption_count=0, score=11.8048, test/accuracy=0.484941, test/loss=0.748226, test/mean_average_precision=0.0280586, test/num_examples=43793, total_duration=222.568, train/accuracy=0.482543, train/loss=0.75254, train/mean_average_precision=0.0246563, validation/accuracy=0.485004, validation/loss=0.749236, validation/mean_average_precision=0.0268493, validation/num_examples=43793
I0307 16:28:59.102585 139517339854592 logging_writer.py:48] [100] global_step=100, grad_norm=0.5242807865142822, loss=0.46526262164115906
I0307 16:29:20.298911 139517331461888 logging_writer.py:48] [200] global_step=200, grad_norm=0.34307432174682617, loss=0.31113600730895996
I0307 16:29:42.072744 139517339854592 logging_writer.py:48] [300] global_step=300, grad_norm=0.2108452469110489, loss=0.19182159006595612
I0307 16:30:04.165662 139517331461888 logging_writer.py:48] [400] global_step=400, grad_norm=0.11578585207462311, loss=0.12078792601823807
I0307 16:30:25.705276 139517339854592 logging_writer.py:48] [500] global_step=500, grad_norm=0.06283096969127655, loss=0.08240244537591934
I0307 16:30:47.273980 139517331461888 logging_writer.py:48] [600] global_step=600, grad_norm=0.035672374069690704, loss=0.07172853499650955
I0307 16:31:08.807807 139517893773056 logging_writer.py:48] [700] global_step=700, grad_norm=0.0830407589673996, loss=0.05942590907216072
I0307 16:31:30.316679 139517885380352 logging_writer.py:48] [800] global_step=800, grad_norm=0.03610505163669586, loss=0.05682499706745148
I0307 16:31:51.924406 139517893773056 logging_writer.py:48] [900] global_step=900, grad_norm=0.09496696293354034, loss=0.055605143308639526
I0307 16:32:13.354939 139517885380352 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.10180708020925522, loss=0.050355684012174606
I0307 16:32:34.724312 139517893773056 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.1076698750257492, loss=0.04909694567322731
I0307 16:32:38.113790 139659327427776 spec.py:321] Evaluating on the training split.
I0307 16:33:54.080604 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 16:33:56.034794 139659327427776 spec.py:349] Evaluating on the test split.
I0307 16:33:57.953722 139659327427776 submission_runner.py:469] Time since start: 542.56s, 	Step: 1117, 	{'train/accuracy': 0.9869800806045532, 'train/loss': 0.05193205550312996, 'train/mean_average_precision': 0.0589090869406095, 'validation/accuracy': 0.9843980073928833, 'validation/loss': 0.06138761714100838, 'validation/mean_average_precision': 0.05816697287384551, 'validation/num_examples': 43793, 'test/accuracy': 0.9834192395210266, 'test/loss': 0.06457491219043732, 'test/mean_average_precision': 0.05911169910342183, 'test/num_examples': 43793, 'score': 251.91265845298767, 'total_duration': 542.5642328262329, 'accumulated_submission_time': 251.91265845298767, 'accumulated_eval_time': 290.60338377952576, 'accumulated_logging_time': 0.017836570739746094}
I0307 16:33:57.962577 139517885380352 logging_writer.py:48] [1117] accumulated_eval_time=290.603, accumulated_logging_time=0.0178366, accumulated_submission_time=251.913, global_step=1117, preemption_count=0, score=251.913, test/accuracy=0.983419, test/loss=0.0645749, test/mean_average_precision=0.0591117, test/num_examples=43793, total_duration=542.564, train/accuracy=0.98698, train/loss=0.0519321, train/mean_average_precision=0.0589091, validation/accuracy=0.984398, validation/loss=0.0613876, validation/mean_average_precision=0.058167, validation/num_examples=43793
I0307 16:34:16.176101 139517893773056 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.13059541583061218, loss=0.04763401672244072
I0307 16:34:37.757109 139517885380352 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.12143786251544952, loss=0.051134027540683746
I0307 16:34:59.382266 139517893773056 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.10690055787563324, loss=0.04748322442173958
I0307 16:35:20.916674 139517885380352 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.1802825629711151, loss=0.047034069895744324
I0307 16:35:42.784813 139517893773056 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.06893397867679596, loss=0.04702973738312721
I0307 16:36:04.795416 139517885380352 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.07660961151123047, loss=0.04753277078270912
I0307 16:36:26.601943 139517893773056 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.1006498634815216, loss=0.043991800397634506
I0307 16:36:48.112232 139517885380352 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.13371439278125763, loss=0.04584908485412598
I0307 16:37:09.511004 139517893773056 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.08756063878536224, loss=0.047435227781534195
I0307 16:37:31.062175 139517885380352 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.24568508565425873, loss=0.04800507426261902
I0307 16:37:52.712630 139517893773056 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.10472749173641205, loss=0.04330598562955856
I0307 16:37:58.069601 139659327427776 spec.py:321] Evaluating on the training split.
I0307 16:39:12.767950 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 16:39:14.733036 139659327427776 spec.py:349] Evaluating on the test split.
I0307 16:39:16.651112 139659327427776 submission_runner.py:469] Time since start: 861.26s, 	Step: 2226, 	{'train/accuracy': 0.9877414703369141, 'train/loss': 0.044081032276153564, 'train/mean_average_precision': 0.14362420047860947, 'validation/accuracy': 0.9849979877471924, 'validation/loss': 0.05317061021924019, 'validation/mean_average_precision': 0.13290089444939004, 'validation/num_examples': 43793, 'test/accuracy': 0.9840046763420105, 'test/loss': 0.05624566972255707, 'test/mean_average_precision': 0.13035579983754264, 'test/num_examples': 43793, 'score': 491.98144578933716, 'total_duration': 861.2616300582886, 'accumulated_submission_time': 491.98144578933716, 'accumulated_eval_time': 369.1848440170288, 'accumulated_logging_time': 0.0359649658203125}
I0307 16:39:16.659914 139517885380352 logging_writer.py:48] [2226] accumulated_eval_time=369.185, accumulated_logging_time=0.035965, accumulated_submission_time=491.981, global_step=2226, preemption_count=0, score=491.981, test/accuracy=0.984005, test/loss=0.0562457, test/mean_average_precision=0.130356, test/num_examples=43793, total_duration=861.262, train/accuracy=0.987741, train/loss=0.044081, train/mean_average_precision=0.143624, validation/accuracy=0.984998, validation/loss=0.0531706, validation/mean_average_precision=0.132901, validation/num_examples=43793
I0307 16:39:32.896520 139517893773056 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.09841184318065643, loss=0.046019259840250015
I0307 16:39:54.656752 139517885380352 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.08267620950937271, loss=0.0485493466258049
I0307 16:40:16.414495 139517893773056 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.060474179685115814, loss=0.04182620719075203
I0307 16:40:37.828794 139517885380352 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.041346266865730286, loss=0.04017406329512596
I0307 16:40:59.432850 139517893773056 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.10706928372383118, loss=0.04289456084370613
I0307 16:41:21.089947 139517885380352 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.06346990168094635, loss=0.042948562651872635
I0307 16:41:42.604197 139517893773056 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.09940371662378311, loss=0.04924766719341278
I0307 16:42:04.186155 139517885380352 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.050215944647789, loss=0.04481571912765503
I0307 16:42:25.753648 139517893773056 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.043717142194509506, loss=0.04357491061091423
I0307 16:42:47.636553 139517885380352 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.061264198273420334, loss=0.04489726945757866
I0307 16:43:09.416424 139517893773056 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.07847393304109573, loss=0.04336711764335632
I0307 16:43:16.849897 139659327427776 spec.py:321] Evaluating on the training split.
I0307 16:44:32.829588 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 16:44:34.779182 139659327427776 spec.py:349] Evaluating on the test split.
I0307 16:44:36.664913 139659327427776 submission_runner.py:469] Time since start: 1181.28s, 	Step: 3335, 	{'train/accuracy': 0.9882873296737671, 'train/loss': 0.04083382710814476, 'train/mean_average_precision': 0.1849430265389228, 'validation/accuracy': 0.9853743314743042, 'validation/loss': 0.05053276941180229, 'validation/mean_average_precision': 0.16109067960310847, 'validation/num_examples': 43793, 'test/accuracy': 0.9844532608985901, 'test/loss': 0.05324564129114151, 'test/mean_average_precision': 0.1621799465838432, 'test/num_examples': 43793, 'score': 732.132963180542, 'total_duration': 1181.2754306793213, 'accumulated_submission_time': 732.132963180542, 'accumulated_eval_time': 448.99981331825256, 'accumulated_logging_time': 0.05407524108886719}
I0307 16:44:36.673936 139517885380352 logging_writer.py:48] [3335] accumulated_eval_time=449, accumulated_logging_time=0.0540752, accumulated_submission_time=732.133, global_step=3335, preemption_count=0, score=732.133, test/accuracy=0.984453, test/loss=0.0532456, test/mean_average_precision=0.16218, test/num_examples=43793, total_duration=1181.28, train/accuracy=0.988287, train/loss=0.0408338, train/mean_average_precision=0.184943, validation/accuracy=0.985374, validation/loss=0.0505328, validation/mean_average_precision=0.161091, validation/num_examples=43793
I0307 16:44:50.732137 139517893773056 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.09675946831703186, loss=0.04604562371969223
I0307 16:45:12.147414 139517885380352 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0422552190721035, loss=0.044897425919771194
I0307 16:45:33.668030 139517893773056 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.07389388233423233, loss=0.044130802154541016
I0307 16:45:55.236330 139517885380352 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.06631822139024734, loss=0.04152291640639305
I0307 16:46:16.558262 139517893773056 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.04805351048707962, loss=0.041557520627975464
I0307 16:46:38.048820 139517885380352 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.06067516282200813, loss=0.04614808410406113
I0307 16:46:59.511018 139517893773056 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.04751773551106453, loss=0.04165180027484894
I0307 16:47:20.945247 139517885380352 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.06882641464471817, loss=0.044401392340660095
I0307 16:47:42.546344 139517893773056 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.056313417851924896, loss=0.046563368290662766
I0307 16:48:04.208261 139517885380352 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.03515150770545006, loss=0.04428786039352417
I0307 16:48:25.540176 139517893773056 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0638825073838234, loss=0.043348561972379684
I0307 16:48:36.851142 139659327427776 spec.py:321] Evaluating on the training split.
I0307 16:49:53.284948 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 16:49:55.210063 139659327427776 spec.py:349] Evaluating on the test split.
I0307 16:49:57.105865 139659327427776 submission_runner.py:469] Time since start: 1501.72s, 	Step: 4454, 	{'train/accuracy': 0.9884161949157715, 'train/loss': 0.040067408233881, 'train/mean_average_precision': 0.21110601972482995, 'validation/accuracy': 0.9855407476425171, 'validation/loss': 0.04931637644767761, 'validation/mean_average_precision': 0.18409452381887398, 'validation/num_examples': 43793, 'test/accuracy': 0.9846756458282471, 'test/loss': 0.05195789784193039, 'test/mean_average_precision': 0.18511094464359082, 'test/num_examples': 43793, 'score': 972.2698929309845, 'total_duration': 1501.7163844108582, 'accumulated_submission_time': 972.2698929309845, 'accumulated_eval_time': 529.2544910907745, 'accumulated_logging_time': 0.07394766807556152}
I0307 16:49:57.115543 139517471074048 logging_writer.py:48] [4454] accumulated_eval_time=529.254, accumulated_logging_time=0.0739477, accumulated_submission_time=972.27, global_step=4454, preemption_count=0, score=972.27, test/accuracy=0.984676, test/loss=0.0519579, test/mean_average_precision=0.185111, test/num_examples=43793, total_duration=1501.72, train/accuracy=0.988416, train/loss=0.0400674, train/mean_average_precision=0.211106, validation/accuracy=0.985541, validation/loss=0.0493164, validation/mean_average_precision=0.184095, validation/num_examples=43793
I0307 16:50:07.022980 139517462681344 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.028573961928486824, loss=0.03786810487508774
I0307 16:50:28.217695 139517471074048 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.047732897102832794, loss=0.040698956698179245
I0307 16:50:49.448000 139517462681344 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.030175363644957542, loss=0.04084314405918121
I0307 16:51:10.623000 139517471074048 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.065416119992733, loss=0.044979263097047806
I0307 16:51:31.932676 139517462681344 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.050071634352207184, loss=0.04150239750742912
I0307 16:51:53.258402 139517471074048 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.03377052769064903, loss=0.04311803728342056
I0307 16:52:14.498857 139517462681344 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.039464741945266724, loss=0.043817680329084396
I0307 16:52:35.724764 139517471074048 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.04376262053847313, loss=0.041323862969875336
I0307 16:52:56.756916 139517462681344 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.07404548674821854, loss=0.039822664111852646
I0307 16:53:17.900518 139517471074048 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.039408713579177856, loss=0.04593353718519211
I0307 16:53:38.957591 139517462681344 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.03458670899271965, loss=0.04308909922838211
I0307 16:53:57.215665 139659327427776 spec.py:321] Evaluating on the training split.
I0307 16:55:11.673906 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 16:55:13.640070 139659327427776 spec.py:349] Evaluating on the test split.
I0307 16:55:15.565547 139659327427776 submission_runner.py:469] Time since start: 1820.18s, 	Step: 5587, 	{'train/accuracy': 0.9885985255241394, 'train/loss': 0.03893957659602165, 'train/mean_average_precision': 0.23378070698116138, 'validation/accuracy': 0.9856057167053223, 'validation/loss': 0.04935465753078461, 'validation/mean_average_precision': 0.19008496069046132, 'validation/num_examples': 43793, 'test/accuracy': 0.9847164750099182, 'test/loss': 0.052222754806280136, 'test/mean_average_precision': 0.19325779178273966, 'test/num_examples': 43793, 'score': 1212.3269619941711, 'total_duration': 1820.1760606765747, 'accumulated_submission_time': 1212.3269619941711, 'accumulated_eval_time': 607.6043236255646, 'accumulated_logging_time': 0.09311032295227051}
I0307 16:55:15.574773 139517471074048 logging_writer.py:48] [5587] accumulated_eval_time=607.604, accumulated_logging_time=0.0931103, accumulated_submission_time=1212.33, global_step=5587, preemption_count=0, score=1212.33, test/accuracy=0.984716, test/loss=0.0522228, test/mean_average_precision=0.193258, test/num_examples=43793, total_duration=1820.18, train/accuracy=0.988599, train/loss=0.0389396, train/mean_average_precision=0.233781, validation/accuracy=0.985606, validation/loss=0.0493547, validation/mean_average_precision=0.190085, validation/num_examples=43793
I0307 16:55:18.516531 139517462681344 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.025800757110118866, loss=0.042901039123535156
I0307 16:55:39.688797 139517471074048 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.022936467081308365, loss=0.03904237970709801
I0307 16:56:01.015018 139517462681344 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.03767213970422745, loss=0.03847029060125351
I0307 16:56:22.323998 139517471074048 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.019837966188788414, loss=0.038151852786540985
I0307 16:56:43.428710 139517462681344 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.029873952269554138, loss=0.04324198514223099
I0307 16:57:04.436881 139517471074048 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.02790825255215168, loss=0.041072968393564224
I0307 16:57:25.606317 139517462681344 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0278709027916193, loss=0.040049001574516296
I0307 16:57:46.844112 139517471074048 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.029037930071353912, loss=0.039886053651571274
I0307 16:58:08.177871 139517462681344 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.02453061006963253, loss=0.0444222129881382
I0307 16:58:29.564090 139517471074048 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.027853628620505333, loss=0.040585312992334366
I0307 16:58:50.642907 139517462681344 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.04517557844519615, loss=0.037836357951164246
I0307 16:59:11.893536 139517471074048 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.05391949042677879, loss=0.042921032756567
I0307 16:59:15.744627 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:00:29.677759 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:00:31.643101 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:00:33.540754 139659327427776 submission_runner.py:469] Time since start: 2138.15s, 	Step: 6719, 	{'train/accuracy': 0.988862931728363, 'train/loss': 0.03792678192257881, 'train/mean_average_precision': 0.24610911905029084, 'validation/accuracy': 0.9859678149223328, 'validation/loss': 0.047702472656965256, 'validation/mean_average_precision': 0.20919302285404998, 'validation/num_examples': 43793, 'test/accuracy': 0.9850631356239319, 'test/loss': 0.05034596472978592, 'test/mean_average_precision': 0.21048278278916685, 'test/num_examples': 43793, 'score': 1452.4611961841583, 'total_duration': 2138.1511075496674, 'accumulated_submission_time': 1452.4611961841583, 'accumulated_eval_time': 685.4002315998077, 'accumulated_logging_time': 0.11127686500549316}
I0307 17:00:33.549829 139517462681344 logging_writer.py:48] [6719] accumulated_eval_time=685.4, accumulated_logging_time=0.111277, accumulated_submission_time=1452.46, global_step=6719, preemption_count=0, score=1452.46, test/accuracy=0.985063, test/loss=0.050346, test/mean_average_precision=0.210483, test/num_examples=43793, total_duration=2138.15, train/accuracy=0.988863, train/loss=0.0379268, train/mean_average_precision=0.246109, validation/accuracy=0.985968, validation/loss=0.0477025, validation/mean_average_precision=0.209193, validation/num_examples=43793
I0307 17:00:50.881668 139517471074048 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.026389073580503464, loss=0.040428806096315384
I0307 17:01:11.744015 139517462681344 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.024004578590393066, loss=0.04068537428975105
I0307 17:01:32.889018 139517471074048 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.023139644414186478, loss=0.03457944095134735
I0307 17:01:54.089767 139517462681344 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.029270797967910767, loss=0.03980249539017677
I0307 17:02:15.091839 139517471074048 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.02179167978465557, loss=0.03924928978085518
I0307 17:02:36.046817 139517462681344 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03618781268596649, loss=0.03851576894521713
I0307 17:02:56.869817 139517471074048 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.027138782665133476, loss=0.04222935065627098
I0307 17:03:17.820574 139517462681344 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.018766136839985847, loss=0.037882786244153976
I0307 17:03:38.766984 139517471074048 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01883552223443985, loss=0.03787463530898094
I0307 17:03:59.939153 139517462681344 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.020575975999236107, loss=0.037599124014377594
I0307 17:04:20.911938 139517471074048 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.0336511954665184, loss=0.035670313984155655
I0307 17:04:33.737374 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:05:48.589725 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:05:50.541305 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:05:52.484063 139659327427776 submission_runner.py:469] Time since start: 2457.09s, 	Step: 7862, 	{'train/accuracy': 0.9893061518669128, 'train/loss': 0.03617340326309204, 'train/mean_average_precision': 0.28092856857354503, 'validation/accuracy': 0.9862747192382812, 'validation/loss': 0.04634670913219452, 'validation/mean_average_precision': 0.2295277817267843, 'validation/num_examples': 43793, 'test/accuracy': 0.9853272438049316, 'test/loss': 0.04914823919534683, 'test/mean_average_precision': 0.22062230402788743, 'test/num_examples': 43793, 'score': 1692.6092796325684, 'total_duration': 2457.0944299697876, 'accumulated_submission_time': 1692.6092796325684, 'accumulated_eval_time': 764.1467208862305, 'accumulated_logging_time': 0.1309497356414795}
I0307 17:05:52.493458 139517462681344 logging_writer.py:48] [7862] accumulated_eval_time=764.147, accumulated_logging_time=0.13095, accumulated_submission_time=1692.61, global_step=7862, preemption_count=0, score=1692.61, test/accuracy=0.985327, test/loss=0.0491482, test/mean_average_precision=0.220622, test/num_examples=43793, total_duration=2457.09, train/accuracy=0.989306, train/loss=0.0361734, train/mean_average_precision=0.280929, validation/accuracy=0.986275, validation/loss=0.0463467, validation/mean_average_precision=0.229528, validation/num_examples=43793
I0307 17:06:00.850122 139517471074048 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.017050404101610184, loss=0.03813367336988449
I0307 17:06:22.086987 139517462681344 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.019148942083120346, loss=0.03832190856337547
I0307 17:06:43.277544 139517471074048 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.02844819240272045, loss=0.03908413648605347
I0307 17:07:04.339143 139517462681344 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.025690101087093353, loss=0.04241943359375
I0307 17:07:25.676938 139517471074048 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.026813169941306114, loss=0.03991414234042168
I0307 17:07:46.914895 139517462681344 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.027449125424027443, loss=0.03940492868423462
I0307 17:08:08.218997 139517471074048 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.028046265244483948, loss=0.04134378954768181
I0307 17:08:29.240613 139517462681344 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.022665871307253838, loss=0.03770657628774643
I0307 17:08:50.223459 139517471074048 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.01813272014260292, loss=0.03740474954247475
I0307 17:09:11.378735 139517462681344 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.018420793116092682, loss=0.03575467690825462
I0307 17:09:32.360929 139517471074048 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.03958091512322426, loss=0.039963237941265106
I0307 17:09:52.565664 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:11:05.423876 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:11:07.443090 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:11:09.402918 139659327427776 submission_runner.py:469] Time since start: 2774.01s, 	Step: 8997, 	{'train/accuracy': 0.9898136854171753, 'train/loss': 0.034795619547367096, 'train/mean_average_precision': 0.31071371611029064, 'validation/accuracy': 0.9863429069519043, 'validation/loss': 0.04595512896776199, 'validation/mean_average_precision': 0.23279257542281742, 'validation/num_examples': 43793, 'test/accuracy': 0.9855369925498962, 'test/loss': 0.0485963337123394, 'test/mean_average_precision': 0.22497764709640183, 'test/num_examples': 43793, 'score': 1932.6417162418365, 'total_duration': 2774.0133028030396, 'accumulated_submission_time': 1932.6417162418365, 'accumulated_eval_time': 840.9837901592255, 'accumulated_logging_time': 0.15025901794433594}
I0307 17:11:09.412455 139517462681344 logging_writer.py:48] [8997] accumulated_eval_time=840.984, accumulated_logging_time=0.150259, accumulated_submission_time=1932.64, global_step=8997, preemption_count=0, score=1932.64, test/accuracy=0.985537, test/loss=0.0485963, test/mean_average_precision=0.224978, test/num_examples=43793, total_duration=2774.01, train/accuracy=0.989814, train/loss=0.0347956, train/mean_average_precision=0.310714, validation/accuracy=0.986343, validation/loss=0.0459551, validation/mean_average_precision=0.232793, validation/num_examples=43793
I0307 17:11:10.264240 139517471074048 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.036386460065841675, loss=0.036597754806280136
I0307 17:11:31.287107 139517462681344 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.041174013167619705, loss=0.03914503753185272
I0307 17:11:52.388518 139517471074048 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.030505916103720665, loss=0.03871984779834747
I0307 17:12:13.542267 139517462681344 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.03228543698787689, loss=0.036837976425886154
I0307 17:12:34.513690 139517471074048 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.025765525177121162, loss=0.03998003154993057
I0307 17:12:55.575956 139517462681344 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02641974948346615, loss=0.037196092307567596
I0307 17:13:16.560038 139517471074048 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.059600830078125, loss=0.04004977270960808
I0307 17:13:37.817461 139517462681344 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.029360109940171242, loss=0.04127157852053642
I0307 17:13:59.026668 139517471074048 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.029073266312479973, loss=0.03569289296865463
I0307 17:14:20.141690 139517462681344 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03413056954741478, loss=0.03415656462311745
I0307 17:14:41.351821 139517471074048 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.034575849771499634, loss=0.03932763636112213
I0307 17:15:02.572476 139517462681344 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.024404408410191536, loss=0.038112081587314606
I0307 17:15:09.459263 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:16:22.287956 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:16:24.273707 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:16:26.182108 139659327427776 submission_runner.py:469] Time since start: 3090.79s, 	Step: 10134, 	{'train/accuracy': 0.989830493927002, 'train/loss': 0.03414575010538101, 'train/mean_average_precision': 0.3157925279331685, 'validation/accuracy': 0.9864922761917114, 'validation/loss': 0.04527546837925911, 'validation/mean_average_precision': 0.24112616647087454, 'validation/num_examples': 43793, 'test/accuracy': 0.9856991171836853, 'test/loss': 0.047831762582063675, 'test/mean_average_precision': 0.23618721068908818, 'test/num_examples': 43793, 'score': 2172.648035287857, 'total_duration': 3090.792464494705, 'accumulated_submission_time': 2172.648035287857, 'accumulated_eval_time': 917.7064299583435, 'accumulated_logging_time': 0.16934490203857422}
I0307 17:16:26.191607 139517471074048 logging_writer.py:48] [10134] accumulated_eval_time=917.706, accumulated_logging_time=0.169345, accumulated_submission_time=2172.65, global_step=10134, preemption_count=0, score=2172.65, test/accuracy=0.985699, test/loss=0.0478318, test/mean_average_precision=0.236187, test/num_examples=43793, total_duration=3090.79, train/accuracy=0.98983, train/loss=0.0341458, train/mean_average_precision=0.315793, validation/accuracy=0.986492, validation/loss=0.0452755, validation/mean_average_precision=0.241126, validation/num_examples=43793
I0307 17:16:40.247334 139517462681344 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.028596607968211174, loss=0.03518989309668541
I0307 17:17:01.405424 139517471074048 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.03202873095870018, loss=0.03766320273280144
I0307 17:17:22.395869 139517462681344 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03021450713276863, loss=0.03805382549762726
I0307 17:17:43.527516 139517471074048 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.042122308164834976, loss=0.03897210210561752
I0307 17:18:04.571852 139517462681344 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.03065074048936367, loss=0.03606278821825981
I0307 17:18:25.680914 139517471074048 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.030707864090800285, loss=0.03756122291088104
I0307 17:18:46.824835 139517462681344 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.027639217674732208, loss=0.03637145087122917
I0307 17:19:08.100661 139517471074048 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.037398356944322586, loss=0.0368829108774662
I0307 17:19:29.231395 139517462681344 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.036971565335989, loss=0.04019813612103462
I0307 17:19:50.300499 139517471074048 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.030560221523046494, loss=0.036048680543899536
I0307 17:20:11.354958 139517462681344 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.046253520995378494, loss=0.03836200013756752
I0307 17:20:26.269172 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:21:40.196659 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:21:42.153810 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:21:44.086014 139659327427776 submission_runner.py:469] Time since start: 3408.70s, 	Step: 11272, 	{'train/accuracy': 0.9901042580604553, 'train/loss': 0.03306565806269646, 'train/mean_average_precision': 0.3439462078911937, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.045292507857084274, 'validation/mean_average_precision': 0.24053192754892036, 'validation/num_examples': 43793, 'test/accuracy': 0.9855281114578247, 'test/loss': 0.04801567643880844, 'test/mean_average_precision': 0.2323789082099413, 'test/num_examples': 43793, 'score': 2412.687742948532, 'total_duration': 3408.6963777542114, 'accumulated_submission_time': 2412.687742948532, 'accumulated_eval_time': 995.523065328598, 'accumulated_logging_time': 0.18798589706420898}
I0307 17:21:44.095893 139517471074048 logging_writer.py:48] [11272] accumulated_eval_time=995.523, accumulated_logging_time=0.187986, accumulated_submission_time=2412.69, global_step=11272, preemption_count=0, score=2412.69, test/accuracy=0.985528, test/loss=0.0480157, test/mean_average_precision=0.232379, test/num_examples=43793, total_duration=3408.7, train/accuracy=0.990104, train/loss=0.0330657, train/mean_average_precision=0.343946, validation/accuracy=0.98636, validation/loss=0.0452925, validation/mean_average_precision=0.240532, validation/num_examples=43793
I0307 17:21:50.207592 139517462681344 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.03419962152838707, loss=0.0363505557179451
I0307 17:22:11.272917 139517471074048 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.033487554639577866, loss=0.03629186376929283
I0307 17:22:32.256412 139517462681344 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0334235280752182, loss=0.03879009932279587
I0307 17:22:53.208838 139517471074048 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.043529439717531204, loss=0.03947562351822853
I0307 17:23:14.198887 139517462681344 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.041794098913669586, loss=0.04031610116362572
I0307 17:23:35.243037 139517471074048 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.04411526769399643, loss=0.033764760941267014
I0307 17:23:56.315636 139517462681344 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04035336524248123, loss=0.03441669046878815
I0307 17:24:17.287373 139517471074048 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.04947582632303238, loss=0.03522524982690811
I0307 17:24:38.237514 139517462681344 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03145245090126991, loss=0.03540849685668945
I0307 17:24:59.440786 139517471074048 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03094126097857952, loss=0.03465403616428375
I0307 17:25:20.620059 139517462681344 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.03975645452737808, loss=0.03439491614699364
I0307 17:25:42.064433 139517471074048 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.049164388328790665, loss=0.03535519540309906
I0307 17:25:44.177329 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:26:57.381674 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:26:59.333350 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:27:01.264101 139659327427776 submission_runner.py:469] Time since start: 3725.87s, 	Step: 12411, 	{'train/accuracy': 0.9901197552680969, 'train/loss': 0.0328025259077549, 'train/mean_average_precision': 0.3511143515888282, 'validation/accuracy': 0.9867009520530701, 'validation/loss': 0.04495331272482872, 'validation/mean_average_precision': 0.2558747943500725, 'validation/num_examples': 43793, 'test/accuracy': 0.9858735203742981, 'test/loss': 0.04774082452058792, 'test/mean_average_precision': 0.24560777151153387, 'test/num_examples': 43793, 'score': 2652.730261325836, 'total_duration': 3725.874457836151, 'accumulated_submission_time': 2652.730261325836, 'accumulated_eval_time': 1072.6096420288086, 'accumulated_logging_time': 0.20878291130065918}
I0307 17:27:01.274400 139517462681344 logging_writer.py:48] [12411] accumulated_eval_time=1072.61, accumulated_logging_time=0.208783, accumulated_submission_time=2652.73, global_step=12411, preemption_count=0, score=2652.73, test/accuracy=0.985874, test/loss=0.0477408, test/mean_average_precision=0.245608, test/num_examples=43793, total_duration=3725.87, train/accuracy=0.99012, train/loss=0.0328025, train/mean_average_precision=0.351114, validation/accuracy=0.986701, validation/loss=0.0449533, validation/mean_average_precision=0.255875, validation/num_examples=43793
I0307 17:27:20.385629 139517471074048 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03404669463634491, loss=0.033557772636413574
I0307 17:27:41.304369 139517462681344 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.037047129124403, loss=0.03496137261390686
I0307 17:28:02.429368 139517471074048 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.05271722003817558, loss=0.03518826141953468
I0307 17:28:23.545795 139517462681344 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.0658319741487503, loss=0.03246369957923889
I0307 17:28:44.520582 139517471074048 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.03889355808496475, loss=0.03667895495891571
I0307 17:29:05.646788 139517462681344 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.055840812623500824, loss=0.037854887545108795
I0307 17:29:26.921509 139517471074048 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.03631375730037689, loss=0.03295369818806648
I0307 17:29:48.132134 139517462681344 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.04028884321451187, loss=0.03718424215912819
I0307 17:30:09.313495 139517471074048 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.05627346783876419, loss=0.03494931384921074
I0307 17:30:30.342710 139517462681344 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.04251670464873314, loss=0.037637267261743546
I0307 17:30:51.449740 139517471074048 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.043572425842285156, loss=0.035386551171541214
I0307 17:31:01.406818 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:32:16.149218 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:32:18.082600 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:32:20.000984 139659327427776 submission_runner.py:469] Time since start: 4044.61s, 	Step: 13548, 	{'train/accuracy': 0.9903571605682373, 'train/loss': 0.031637243926525116, 'train/mean_average_precision': 0.3866782287342651, 'validation/accuracy': 0.9867476224899292, 'validation/loss': 0.04467985779047012, 'validation/mean_average_precision': 0.2572974737341221, 'validation/num_examples': 43793, 'test/accuracy': 0.9857951998710632, 'test/loss': 0.04761962592601776, 'test/mean_average_precision': 0.24620493812266, 'test/num_examples': 43793, 'score': 2892.8241715431213, 'total_duration': 4044.611449956894, 'accumulated_submission_time': 2892.8241715431213, 'accumulated_eval_time': 1151.20370221138, 'accumulated_logging_time': 0.22852706909179688}
I0307 17:32:20.011452 139517462681344 logging_writer.py:48] [13548] accumulated_eval_time=1151.2, accumulated_logging_time=0.228527, accumulated_submission_time=2892.82, global_step=13548, preemption_count=0, score=2892.82, test/accuracy=0.985795, test/loss=0.0476196, test/mean_average_precision=0.246205, test/num_examples=43793, total_duration=4044.61, train/accuracy=0.990357, train/loss=0.0316372, train/mean_average_precision=0.386678, validation/accuracy=0.986748, validation/loss=0.0446799, validation/mean_average_precision=0.257297, validation/num_examples=43793
I0307 17:32:31.058297 139517471074048 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.04999415948987007, loss=0.03555816039443016
I0307 17:32:52.062763 139517462681344 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.03923351317644119, loss=0.0369003601372242
I0307 17:33:13.359306 139517471074048 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.04990595579147339, loss=0.03784168139100075
I0307 17:33:34.464547 139517462681344 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.05694838985800743, loss=0.03602650389075279
I0307 17:33:55.504376 139517471074048 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.04981916770339012, loss=0.03779979050159454
I0307 17:34:16.518415 139517462681344 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.04339861497282982, loss=0.034767162054777145
I0307 17:34:37.784117 139517471074048 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.040924154222011566, loss=0.03378991410136223
I0307 17:34:59.083471 139517462681344 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.05721072852611542, loss=0.03312404081225395
I0307 17:35:20.498249 139517471074048 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.048301972448825836, loss=0.0356406643986702
I0307 17:35:41.734123 139517462681344 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04309089109301567, loss=0.0360453724861145
I0307 17:36:02.921672 139517471074048 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.054102715104818344, loss=0.03672797605395317
I0307 17:36:20.163156 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:37:33.864949 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:37:35.852300 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:37:37.974987 139659327427776 submission_runner.py:469] Time since start: 4362.59s, 	Step: 14683, 	{'train/accuracy': 0.990379273891449, 'train/loss': 0.03158879280090332, 'train/mean_average_precision': 0.37781407800689526, 'validation/accuracy': 0.9867516756057739, 'validation/loss': 0.04452819004654884, 'validation/mean_average_precision': 0.25664048218291374, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.04751362279057503, 'test/mean_average_precision': 0.24661634877420657, 'test/num_examples': 43793, 'score': 3132.937895298004, 'total_duration': 4362.585327625275, 'accumulated_submission_time': 3132.937895298004, 'accumulated_eval_time': 1229.015308856964, 'accumulated_logging_time': 0.24912166595458984}
I0307 17:37:37.986838 139517462681344 logging_writer.py:48] [14683] accumulated_eval_time=1229.02, accumulated_logging_time=0.249122, accumulated_submission_time=3132.94, global_step=14683, preemption_count=0, score=3132.94, test/accuracy=0.985853, test/loss=0.0475136, test/mean_average_precision=0.246616, test/num_examples=43793, total_duration=4362.59, train/accuracy=0.990379, train/loss=0.0315888, train/mean_average_precision=0.377814, validation/accuracy=0.986752, validation/loss=0.0445282, validation/mean_average_precision=0.25664, validation/num_examples=43793
I0307 17:37:41.815213 139517471074048 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.059639301151037216, loss=0.03462173417210579
I0307 17:38:03.122195 139517462681344 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.05757055804133415, loss=0.03702225908637047
I0307 17:38:24.319519 139517471074048 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.05120197683572769, loss=0.036292966455221176
I0307 17:38:45.198144 139517462681344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.039434634149074554, loss=0.033732395619153976
I0307 17:39:06.201068 139517471074048 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.04965348169207573, loss=0.03396645933389664
I0307 17:39:27.407887 139517462681344 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.054761577397584915, loss=0.04009202867746353
I0307 17:39:48.447057 139517471074048 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.05259883776307106, loss=0.036340851336717606
I0307 17:40:09.622527 139517462681344 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.13281887769699097, loss=0.03684673458337784
I0307 17:40:30.845587 139517471074048 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.09503058344125748, loss=0.037465646862983704
I0307 17:40:51.943027 139517462681344 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.054279714822769165, loss=0.03422446548938751
I0307 17:41:13.107041 139517471074048 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.05068197846412659, loss=0.03514000028371811
I0307 17:41:34.126869 139517462681344 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04657226800918579, loss=0.03175133466720581
I0307 17:41:38.150199 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:42:50.254256 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:42:52.189947 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:42:54.297434 139659327427776 submission_runner.py:469] Time since start: 4678.91s, 	Step: 15820, 	{'train/accuracy': 0.9910165071487427, 'train/loss': 0.02982884831726551, 'train/mean_average_precision': 0.41986058026756756, 'validation/accuracy': 0.9867488145828247, 'validation/loss': 0.04419844597578049, 'validation/mean_average_precision': 0.26172257204319543, 'validation/num_examples': 43793, 'test/accuracy': 0.985920250415802, 'test/loss': 0.047168903052806854, 'test/mean_average_precision': 0.2473847450603414, 'test/num_examples': 43793, 'score': 3373.0578005313873, 'total_duration': 4678.907758235931, 'accumulated_submission_time': 3373.0578005313873, 'accumulated_eval_time': 1305.1622958183289, 'accumulated_logging_time': 0.27553749084472656}
I0307 17:42:54.311488 139517471074048 logging_writer.py:48] [15820] accumulated_eval_time=1305.16, accumulated_logging_time=0.275537, accumulated_submission_time=3373.06, global_step=15820, preemption_count=0, score=3373.06, test/accuracy=0.98592, test/loss=0.0471689, test/mean_average_precision=0.247385, test/num_examples=43793, total_duration=4678.91, train/accuracy=0.991017, train/loss=0.0298288, train/mean_average_precision=0.419861, validation/accuracy=0.986749, validation/loss=0.0441984, validation/mean_average_precision=0.261723, validation/num_examples=43793
I0307 17:43:11.467996 139517462681344 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.05656180530786514, loss=0.03370023891329765
I0307 17:43:32.405534 139517471074048 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.08058147877454758, loss=0.0345686599612236
I0307 17:43:53.523879 139517462681344 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.06870979815721512, loss=0.03486812114715576
I0307 17:44:14.600561 139517471074048 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.06610967963933945, loss=0.036866918206214905
I0307 17:44:35.618496 139517462681344 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.057116419076919556, loss=0.03263453394174576
I0307 17:44:56.636309 139517471074048 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.059232644736766815, loss=0.03375723958015442
I0307 17:45:17.778105 139517462681344 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.06696496158838272, loss=0.033861007541418076
I0307 17:45:39.031678 139517471074048 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.057310253381729126, loss=0.03381400182843208
I0307 17:46:00.186939 139517462681344 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.04405196011066437, loss=0.03183427453041077
I0307 17:46:21.359867 139517471074048 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.05819593742489815, loss=0.03481652960181236
I0307 17:46:42.396095 139517462681344 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.07400421053171158, loss=0.03539596498012543
I0307 17:46:54.366153 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:48:07.164919 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:48:09.095454 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:48:10.967014 139659327427776 submission_runner.py:469] Time since start: 4995.58s, 	Step: 16958, 	{'train/accuracy': 0.990696907043457, 'train/loss': 0.030731679871678352, 'train/mean_average_precision': 0.4010825200729872, 'validation/accuracy': 0.9867163896560669, 'validation/loss': 0.04472581669688225, 'validation/mean_average_precision': 0.2593270907578323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857121706008911, 'test/loss': 0.04772987961769104, 'test/mean_average_precision': 0.24526947222641993, 'test/num_examples': 43793, 'score': 3613.074487924576, 'total_duration': 4995.577400684357, 'accumulated_submission_time': 3613.074487924576, 'accumulated_eval_time': 1381.7629730701447, 'accumulated_logging_time': 0.29963135719299316}
I0307 17:48:10.978927 139517471074048 logging_writer.py:48] [16958] accumulated_eval_time=1381.76, accumulated_logging_time=0.299631, accumulated_submission_time=3613.07, global_step=16958, preemption_count=0, score=3613.07, test/accuracy=0.985712, test/loss=0.0477299, test/mean_average_precision=0.245269, test/num_examples=43793, total_duration=4995.58, train/accuracy=0.990697, train/loss=0.0307317, train/mean_average_precision=0.401083, validation/accuracy=0.986716, validation/loss=0.0447258, validation/mean_average_precision=0.259327, validation/num_examples=43793
I0307 17:48:20.105619 139517462681344 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.05769335851073265, loss=0.03521358594298363
I0307 17:48:41.214321 139517471074048 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.058174844831228256, loss=0.03266657888889313
I0307 17:49:02.181984 139517462681344 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.04553139954805374, loss=0.03251266852021217
I0307 17:49:23.431068 139517471074048 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.06560532748699188, loss=0.03360579162836075
I0307 17:49:44.642328 139517462681344 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.05934004485607147, loss=0.03280775994062424
I0307 17:50:05.773211 139517471074048 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.07071462273597717, loss=0.036194272339344025
I0307 17:50:26.881002 139517462681344 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.07751809060573578, loss=0.03557281568646431
I0307 17:50:48.060277 139517471074048 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0886530876159668, loss=0.03412602096796036
I0307 17:51:09.109499 139517462681344 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.061268389225006104, loss=0.032183028757572174
I0307 17:51:30.141452 139517471074048 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.08541475981473923, loss=0.03504020720720291
I0307 17:51:51.244950 139517462681344 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.054524797946214676, loss=0.035711869597435
I0307 17:52:11.065825 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:53:24.492401 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:53:26.438799 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:53:28.337739 139659327427776 submission_runner.py:469] Time since start: 5312.95s, 	Step: 18095, 	{'train/accuracy': 0.9913542866706848, 'train/loss': 0.028638113290071487, 'train/mean_average_precision': 0.45954159622254487, 'validation/accuracy': 0.9868413805961609, 'validation/loss': 0.04417752847075462, 'validation/mean_average_precision': 0.26452343290676844, 'validation/num_examples': 43793, 'test/accuracy': 0.986014187335968, 'test/loss': 0.04686443880200386, 'test/mean_average_precision': 0.2605700218770182, 'test/num_examples': 43793, 'score': 3853.121359348297, 'total_duration': 5312.948255300522, 'accumulated_submission_time': 3853.121359348297, 'accumulated_eval_time': 1459.0348358154297, 'accumulated_logging_time': 0.3217177391052246}
I0307 17:53:28.349273 139517471074048 logging_writer.py:48] [18095] accumulated_eval_time=1459.03, accumulated_logging_time=0.321718, accumulated_submission_time=3853.12, global_step=18095, preemption_count=0, score=3853.12, test/accuracy=0.986014, test/loss=0.0468644, test/mean_average_precision=0.26057, test/num_examples=43793, total_duration=5312.95, train/accuracy=0.991354, train/loss=0.0286381, train/mean_average_precision=0.459542, validation/accuracy=0.986841, validation/loss=0.0441775, validation/mean_average_precision=0.264523, validation/num_examples=43793
I0307 17:53:29.627119 139517462681344 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.0668274536728859, loss=0.031334418803453445
I0307 17:53:50.750727 139517471074048 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.065187007188797, loss=0.03210487961769104
I0307 17:54:12.007869 139517462681344 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.08552073687314987, loss=0.037547335028648376
I0307 17:54:33.223199 139517471074048 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.05959427356719971, loss=0.032406169921159744
I0307 17:54:54.434204 139517462681344 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.07659521698951721, loss=0.03277329355478287
I0307 17:55:15.642185 139517471074048 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.07387858629226685, loss=0.03516010567545891
I0307 17:55:37.015480 139517462681344 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.05873185396194458, loss=0.030292700976133347
I0307 17:55:58.289425 139517471074048 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.08478956669569016, loss=0.03320755809545517
I0307 17:56:19.565061 139517462681344 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.1309167593717575, loss=0.03614785522222519
I0307 17:56:40.679867 139517471074048 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.083368681371212, loss=0.03393875062465668
I0307 17:57:01.752275 139517462681344 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.06542015075683594, loss=0.03502156585454941
I0307 17:57:22.939802 139517471074048 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.06178091838955879, loss=0.0317348837852478
I0307 17:57:28.402418 139659327427776 spec.py:321] Evaluating on the training split.
I0307 17:58:42.210528 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 17:58:44.350111 139659327427776 spec.py:349] Evaluating on the test split.
I0307 17:58:46.455679 139659327427776 submission_runner.py:469] Time since start: 5631.07s, 	Step: 19227, 	{'train/accuracy': 0.9910323619842529, 'train/loss': 0.029413660988211632, 'train/mean_average_precision': 0.41896966487057685, 'validation/accuracy': 0.9868364930152893, 'validation/loss': 0.04398559406399727, 'validation/mean_average_precision': 0.2708583521190506, 'validation/num_examples': 43793, 'test/accuracy': 0.9859935641288757, 'test/loss': 0.04672582820057869, 'test/mean_average_precision': 0.26307612537913017, 'test/num_examples': 43793, 'score': 4093.1363949775696, 'total_duration': 5631.066077470779, 'accumulated_submission_time': 4093.1363949775696, 'accumulated_eval_time': 1537.0879225730896, 'accumulated_logging_time': 0.34250545501708984}
I0307 17:58:46.467536 139517462681344 logging_writer.py:48] [19227] accumulated_eval_time=1537.09, accumulated_logging_time=0.342505, accumulated_submission_time=4093.14, global_step=19227, preemption_count=0, score=4093.14, test/accuracy=0.985994, test/loss=0.0467258, test/mean_average_precision=0.263076, test/num_examples=43793, total_duration=5631.07, train/accuracy=0.991032, train/loss=0.0294137, train/mean_average_precision=0.41897, validation/accuracy=0.986836, validation/loss=0.0439856, validation/mean_average_precision=0.270858, validation/num_examples=43793
I0307 17:59:02.206146 139517471074048 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.08123505115509033, loss=0.036464475095272064
I0307 17:59:23.525378 139517462681344 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.10237675905227661, loss=0.03658898547291756
I0307 17:59:45.091240 139517471074048 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.07143130153417587, loss=0.03397300839424133
I0307 18:00:06.558066 139517462681344 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.07927023619413376, loss=0.03386370837688446
I0307 18:00:27.889438 139517471074048 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.08227275311946869, loss=0.035433150827884674
I0307 18:00:49.163760 139517462681344 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0706111490726471, loss=0.033418942242860794
I0307 18:01:10.381196 139517471074048 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.10248108208179474, loss=0.036688875406980515
I0307 18:01:31.838277 139517462681344 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0627431869506836, loss=0.029157044366002083
I0307 18:01:53.351458 139517471074048 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.07137256115674973, loss=0.03233390673995018
I0307 18:02:14.772879 139517462681344 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.06581321358680725, loss=0.03307746350765228
I0307 18:02:36.108290 139517471074048 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.07885535061359406, loss=0.033777788281440735
I0307 18:02:46.598283 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:04:00.253960 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:04:02.213258 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:04:04.149486 139659327427776 submission_runner.py:469] Time since start: 5948.76s, 	Step: 20350, 	{'train/accuracy': 0.9910579323768616, 'train/loss': 0.029218640178442, 'train/mean_average_precision': 0.440907992438448, 'validation/accuracy': 0.986901044845581, 'validation/loss': 0.044548410922288895, 'validation/mean_average_precision': 0.26575967266384554, 'validation/num_examples': 43793, 'test/accuracy': 0.9859480857849121, 'test/loss': 0.047507863491773605, 'test/mean_average_precision': 0.25504836256985625, 'test/num_examples': 43793, 'score': 4333.230062246323, 'total_duration': 5948.759950399399, 'accumulated_submission_time': 4333.230062246323, 'accumulated_eval_time': 1614.6390228271484, 'accumulated_logging_time': 0.3642137050628662}
I0307 18:04:04.160463 139517462681344 logging_writer.py:48] [20350] accumulated_eval_time=1614.64, accumulated_logging_time=0.364214, accumulated_submission_time=4333.23, global_step=20350, preemption_count=0, score=4333.23, test/accuracy=0.985948, test/loss=0.0475079, test/mean_average_precision=0.255048, test/num_examples=43793, total_duration=5948.76, train/accuracy=0.991058, train/loss=0.0292186, train/mean_average_precision=0.440908, validation/accuracy=0.986901, validation/loss=0.0445484, validation/mean_average_precision=0.26576, validation/num_examples=43793
I0307 18:04:15.131453 139517471074048 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.06398849934339523, loss=0.035675469785928726
I0307 18:04:36.464535 139517462681344 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.12406396865844727, loss=0.030743690207600594
I0307 18:04:57.727068 139517471074048 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.09401509165763855, loss=0.03688178211450577
I0307 18:05:18.985380 139517462681344 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.06837392598390579, loss=0.029304562136530876
I0307 18:05:40.277547 139517471074048 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.0825129896402359, loss=0.030220577493309975
I0307 18:06:01.363888 139517462681344 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.09294145554304123, loss=0.03550438955426216
I0307 18:06:22.626114 139517471074048 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.06190783530473709, loss=0.0314856618642807
I0307 18:06:43.775347 139517462681344 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.06620854139328003, loss=0.033893052488565445
I0307 18:07:04.967231 139517471074048 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.11437346041202545, loss=0.03682561591267586
I0307 18:07:26.031609 139517462681344 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.07989901304244995, loss=0.032654840499162674
I0307 18:07:47.218712 139517471074048 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.11900226771831512, loss=0.03247218579053879
I0307 18:08:04.330549 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:09:18.831905 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:09:20.748589 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:09:22.721627 139659327427776 submission_runner.py:469] Time since start: 6267.33s, 	Step: 21481, 	{'train/accuracy': 0.991349995136261, 'train/loss': 0.02845444716513157, 'train/mean_average_precision': 0.4583176141692592, 'validation/accuracy': 0.9868364930152893, 'validation/loss': 0.0442361980676651, 'validation/mean_average_precision': 0.2664014460740803, 'validation/num_examples': 43793, 'test/accuracy': 0.9859383702278137, 'test/loss': 0.04711903631687164, 'test/mean_average_precision': 0.2527427512009371, 'test/num_examples': 43793, 'score': 4573.3611352443695, 'total_duration': 6267.332014083862, 'accumulated_submission_time': 4573.3611352443695, 'accumulated_eval_time': 1693.0299170017242, 'accumulated_logging_time': 0.38426899909973145}
I0307 18:09:22.732418 139517462681344 logging_writer.py:48] [21481] accumulated_eval_time=1693.03, accumulated_logging_time=0.384269, accumulated_submission_time=4573.36, global_step=21481, preemption_count=0, score=4573.36, test/accuracy=0.985938, test/loss=0.047119, test/mean_average_precision=0.252743, test/num_examples=43793, total_duration=6267.33, train/accuracy=0.99135, train/loss=0.0284544, train/mean_average_precision=0.458318, validation/accuracy=0.986836, validation/loss=0.0442362, validation/mean_average_precision=0.266401, validation/num_examples=43793
I0307 18:09:27.003398 139517471074048 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.07309014350175858, loss=0.035742923617362976
I0307 18:09:48.252705 139517462681344 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.07717166841030121, loss=0.034503500908613205
I0307 18:10:09.602263 139517471074048 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.11864408850669861, loss=0.030139748007059097
I0307 18:10:30.926620 139517462681344 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.06506207585334778, loss=0.034084733575582504
I0307 18:10:52.158300 139517471074048 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.07255231589078903, loss=0.03278307616710663
I0307 18:11:13.433910 139517462681344 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.09099718928337097, loss=0.03178605064749718
I0307 18:11:34.628764 139517471074048 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.10577207058668137, loss=0.031835053116083145
I0307 18:11:55.799378 139517462681344 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.08981110155582428, loss=0.03573955222964287
I0307 18:12:17.015080 139517471074048 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.09208223968744278, loss=0.032179299741983414
I0307 18:12:38.149210 139517462681344 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.0780014619231224, loss=0.03616726025938988
I0307 18:12:59.159608 139517471074048 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.08644548058509827, loss=0.03461167588829994
I0307 18:13:20.453461 139517462681344 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.08306024223566055, loss=0.03437967225909233
I0307 18:13:22.788003 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:14:34.630632 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:14:36.725240 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:14:38.828392 139659327427776 submission_runner.py:469] Time since start: 6583.44s, 	Step: 22612, 	{'train/accuracy': 0.9913470149040222, 'train/loss': 0.02843548357486725, 'train/mean_average_precision': 0.4482000966798827, 'validation/accuracy': 0.9869250059127808, 'validation/loss': 0.04414811730384827, 'validation/mean_average_precision': 0.2728610464994278, 'validation/num_examples': 43793, 'test/accuracy': 0.9859678745269775, 'test/loss': 0.04713272675871849, 'test/mean_average_precision': 0.25413614597329665, 'test/num_examples': 43793, 'score': 4813.378000736237, 'total_duration': 6583.438756942749, 'accumulated_submission_time': 4813.378000736237, 'accumulated_eval_time': 1769.070101261139, 'accumulated_logging_time': 0.4041714668273926}
I0307 18:14:38.840990 139517471074048 logging_writer.py:48] [22612] accumulated_eval_time=1769.07, accumulated_logging_time=0.404171, accumulated_submission_time=4813.38, global_step=22612, preemption_count=0, score=4813.38, test/accuracy=0.985968, test/loss=0.0471327, test/mean_average_precision=0.254136, test/num_examples=43793, total_duration=6583.44, train/accuracy=0.991347, train/loss=0.0284355, train/mean_average_precision=0.4482, validation/accuracy=0.986925, validation/loss=0.0441481, validation/mean_average_precision=0.272861, validation/num_examples=43793
I0307 18:14:57.711943 139517462681344 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.1012270599603653, loss=0.030275775119662285
I0307 18:15:18.861374 139517471074048 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.09480089694261551, loss=0.03036104142665863
I0307 18:15:40.067681 139517462681344 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.11170724779367447, loss=0.03266633301973343
I0307 18:16:01.449384 139517471074048 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.06711216270923615, loss=0.03300473093986511
I0307 18:16:22.737716 139517462681344 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.078973688185215, loss=0.029988901689648628
I0307 18:16:43.923118 139517471074048 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.09173694252967834, loss=0.03411243483424187
I0307 18:17:05.210930 139517462681344 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.07116018235683441, loss=0.03350788727402687
I0307 18:17:26.459975 139517471074048 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.08417569100856781, loss=0.034300629049539566
I0307 18:17:47.716039 139517462681344 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.08708292245864868, loss=0.031090499833226204
I0307 18:18:08.984811 139517471074048 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.09614580869674683, loss=0.028929660096764565
I0307 18:18:30.317343 139517462681344 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.11841987818479538, loss=0.033720627427101135
I0307 18:18:38.938183 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:19:52.675181 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:19:54.668092 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:19:56.620831 139659327427776 submission_runner.py:469] Time since start: 6901.23s, 	Step: 23741, 	{'train/accuracy': 0.9915456175804138, 'train/loss': 0.027390239760279655, 'train/mean_average_precision': 0.47999502630236, 'validation/accuracy': 0.9867910742759705, 'validation/loss': 0.04482853040099144, 'validation/mean_average_precision': 0.2655767803293215, 'validation/num_examples': 43793, 'test/accuracy': 0.9857779145240784, 'test/loss': 0.0479108989238739, 'test/mean_average_precision': 0.24697440228104617, 'test/num_examples': 43793, 'score': 5053.436358690262, 'total_duration': 6901.231354236603, 'accumulated_submission_time': 5053.436358690262, 'accumulated_eval_time': 1846.7526998519897, 'accumulated_logging_time': 0.42779111862182617}
I0307 18:19:56.632249 139517471074048 logging_writer.py:48] [23741] accumulated_eval_time=1846.75, accumulated_logging_time=0.427791, accumulated_submission_time=5053.44, global_step=23741, preemption_count=0, score=5053.44, test/accuracy=0.985778, test/loss=0.0479109, test/mean_average_precision=0.246974, test/num_examples=43793, total_duration=6901.23, train/accuracy=0.991546, train/loss=0.0273902, train/mean_average_precision=0.479995, validation/accuracy=0.986791, validation/loss=0.0448285, validation/mean_average_precision=0.265577, validation/num_examples=43793
I0307 18:20:09.359532 139517462681344 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.08481301367282867, loss=0.03143185004591942
I0307 18:20:30.683690 139517471074048 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.06995800882577896, loss=0.030308891087770462
I0307 18:20:51.994473 139517462681344 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.07904570549726486, loss=0.031862903386354446
I0307 18:21:13.396127 139517471074048 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.0794750526547432, loss=0.03332746401429176
I0307 18:21:34.686505 139517462681344 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.07653944939374924, loss=0.03118707425892353
I0307 18:21:55.906688 139517471074048 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.10484989732503891, loss=0.032574716955423355
I0307 18:22:17.297577 139517462681344 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.08097788691520691, loss=0.030470283702015877
I0307 18:22:38.672102 139517471074048 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.13552036881446838, loss=0.03265967592597008
I0307 18:22:59.811479 139517462681344 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.10716171562671661, loss=0.0321541391313076
I0307 18:23:21.144215 139517471074048 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.10104992240667343, loss=0.034399814903736115
I0307 18:23:42.569454 139517462681344 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.0781826376914978, loss=0.030300727114081383
I0307 18:23:56.718500 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:25:11.441635 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:25:13.418525 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:25:15.479368 139659327427776 submission_runner.py:469] Time since start: 7220.09s, 	Step: 24867, 	{'train/accuracy': 0.9913817644119263, 'train/loss': 0.028227442875504494, 'train/mean_average_precision': 0.45525468313761785, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04429924488067627, 'validation/mean_average_precision': 0.27070838177985534, 'validation/num_examples': 43793, 'test/accuracy': 0.9860563278198242, 'test/loss': 0.047134868800640106, 'test/mean_average_precision': 0.2606326773671783, 'test/num_examples': 43793, 'score': 5293.4833998680115, 'total_duration': 7220.089718103409, 'accumulated_submission_time': 5293.4833998680115, 'accumulated_eval_time': 1925.5133485794067, 'accumulated_logging_time': 0.44914865493774414}
I0307 18:25:15.491752 139517471074048 logging_writer.py:48] [24867] accumulated_eval_time=1925.51, accumulated_logging_time=0.449149, accumulated_submission_time=5293.48, global_step=24867, preemption_count=0, score=5293.48, test/accuracy=0.986056, test/loss=0.0471349, test/mean_average_precision=0.260633, test/num_examples=43793, total_duration=7220.09, train/accuracy=0.991382, train/loss=0.0282274, train/mean_average_precision=0.455255, validation/accuracy=0.986971, validation/loss=0.0442992, validation/mean_average_precision=0.270708, validation/num_examples=43793
I0307 18:25:22.757334 139517462681344 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.09035378694534302, loss=0.03200628608465195
I0307 18:25:44.047315 139517471074048 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.13644205033779144, loss=0.02931968867778778
I0307 18:26:05.526968 139517462681344 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.08765467256307602, loss=0.030482184141874313
I0307 18:26:26.865265 139517471074048 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.09591007977724075, loss=0.032655660063028336
I0307 18:26:48.155317 139517462681344 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.08226881176233292, loss=0.031276457011699677
I0307 18:27:09.550321 139517471074048 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.08028959482908249, loss=0.03203069046139717
I0307 18:27:30.885008 139517462681344 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.08581617474555969, loss=0.03153863176703453
I0307 18:27:52.312537 139517471074048 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.093656025826931, loss=0.031807880848646164
I0307 18:28:13.803281 139517462681344 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.09307890385389328, loss=0.031826552003622055
I0307 18:28:34.945131 139517471074048 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.08149373531341553, loss=0.03185896947979927
I0307 18:28:56.407233 139517462681344 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.08407510071992874, loss=0.03001369908452034
I0307 18:29:15.613863 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:30:29.653371 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:30:31.614562 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:30:33.534607 139659327427776 submission_runner.py:469] Time since start: 7538.15s, 	Step: 25991, 	{'train/accuracy': 0.9920634031295776, 'train/loss': 0.025894856080412865, 'train/mean_average_precision': 0.506709979263162, 'validation/accuracy': 0.9868572354316711, 'validation/loss': 0.04447468742728233, 'validation/mean_average_precision': 0.26789646633195857, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.047472622245550156, 'test/mean_average_precision': 0.2560846180210622, 'test/num_examples': 43793, 'score': 5533.565752744675, 'total_duration': 7538.145130634308, 'accumulated_submission_time': 5533.565752744675, 'accumulated_eval_time': 2003.4340472221375, 'accumulated_logging_time': 0.4715595245361328}
I0307 18:30:33.546529 139517471074048 logging_writer.py:48] [25991] accumulated_eval_time=2003.43, accumulated_logging_time=0.47156, accumulated_submission_time=5533.57, global_step=25991, preemption_count=0, score=5533.57, test/accuracy=0.985906, test/loss=0.0474726, test/mean_average_precision=0.256085, test/num_examples=43793, total_duration=7538.15, train/accuracy=0.992063, train/loss=0.0258949, train/mean_average_precision=0.50671, validation/accuracy=0.986857, validation/loss=0.0444747, validation/mean_average_precision=0.267896, validation/num_examples=43793
I0307 18:30:35.714906 139517462681344 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.14171135425567627, loss=0.0306362546980381
I0307 18:30:57.162050 139517471074048 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.12785013020038605, loss=0.0292444359511137
I0307 18:31:18.685033 139517462681344 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.09396158158779144, loss=0.031504206359386444
I0307 18:31:40.029552 139517471074048 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.11879914253950119, loss=0.03302410989999771
I0307 18:32:01.384592 139517462681344 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.11019822210073471, loss=0.031306516379117966
I0307 18:32:22.878597 139517471074048 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.09844684600830078, loss=0.03455613553524017
I0307 18:32:44.385428 139517462681344 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.10577897727489471, loss=0.029575664550065994
I0307 18:33:05.866697 139517471074048 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.09727286547422409, loss=0.030859362334012985
I0307 18:33:27.183541 139517462681344 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.08013850450515747, loss=0.029511980712413788
I0307 18:33:48.563238 139517471074048 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.08906050771474838, loss=0.0284937284886837
I0307 18:34:10.066847 139517462681344 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.10223785787820816, loss=0.03077799454331398
I0307 18:34:31.456978 139517471074048 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.11220621317625046, loss=0.033356498926877975
I0307 18:34:33.643974 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:35:48.185770 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:35:50.126183 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:35:52.045778 139659327427776 submission_runner.py:469] Time since start: 7856.66s, 	Step: 27111, 	{'train/accuracy': 0.9916271567344666, 'train/loss': 0.026868902146816254, 'train/mean_average_precision': 0.4790074681747368, 'validation/accuracy': 0.9869075417518616, 'validation/loss': 0.04508747532963753, 'validation/mean_average_precision': 0.2726442952403928, 'validation/num_examples': 43793, 'test/accuracy': 0.9860550761222839, 'test/loss': 0.048241425305604935, 'test/mean_average_precision': 0.25946120585306226, 'test/num_examples': 43793, 'score': 5773.625909566879, 'total_duration': 7856.656160593033, 'accumulated_submission_time': 5773.625909566879, 'accumulated_eval_time': 2081.835661172867, 'accumulated_logging_time': 0.49282169342041016}
I0307 18:35:52.057915 139517462681344 logging_writer.py:48] [27111] accumulated_eval_time=2081.84, accumulated_logging_time=0.492822, accumulated_submission_time=5773.63, global_step=27111, preemption_count=0, score=5773.63, test/accuracy=0.986055, test/loss=0.0482414, test/mean_average_precision=0.259461, test/num_examples=43793, total_duration=7856.66, train/accuracy=0.991627, train/loss=0.0268689, train/mean_average_precision=0.479007, validation/accuracy=0.986908, validation/loss=0.0450875, validation/mean_average_precision=0.272644, validation/num_examples=43793
I0307 18:36:11.512786 139517471074048 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.09521744400262833, loss=0.03349708393216133
I0307 18:36:32.967925 139517462681344 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.10472067445516586, loss=0.02759760431945324
I0307 18:36:54.407717 139517471074048 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.14690113067626953, loss=0.03093368373811245
I0307 18:37:15.928677 139517462681344 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.09052480757236481, loss=0.02945891208946705
I0307 18:37:37.707345 139517471074048 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.09197793155908585, loss=0.029025182127952576
I0307 18:37:59.129014 139517462681344 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.09124281257390976, loss=0.030851013958454132
I0307 18:38:20.582387 139517471074048 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.10216475278139114, loss=0.029377499595284462
I0307 18:38:42.052165 139517462681344 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.10178933292627335, loss=0.03059268929064274
I0307 18:39:03.476330 139517471074048 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.08582514524459839, loss=0.030935153365135193
I0307 18:39:25.012723 139517462681344 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.09263896197080612, loss=0.03010229952633381
I0307 18:39:46.321059 139517471074048 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.1016119197010994, loss=0.03178850933909416
I0307 18:39:52.170810 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:41:07.205986 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:41:09.404127 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:41:11.542967 139659327427776 submission_runner.py:469] Time since start: 8176.15s, 	Step: 28228, 	{'train/accuracy': 0.9924265742301941, 'train/loss': 0.024819646030664444, 'train/mean_average_precision': 0.53403962775716, 'validation/accuracy': 0.9869489669799805, 'validation/loss': 0.04451194405555725, 'validation/mean_average_precision': 0.27481056311260504, 'validation/num_examples': 43793, 'test/accuracy': 0.9859973192214966, 'test/loss': 0.047571323812007904, 'test/mean_average_precision': 0.2589014093001228, 'test/num_examples': 43793, 'score': 6013.698362588882, 'total_duration': 8176.153395414352, 'accumulated_submission_time': 6013.698362588882, 'accumulated_eval_time': 2161.2076761722565, 'accumulated_logging_time': 0.5143706798553467}
I0307 18:41:11.556555 139517462681344 logging_writer.py:48] [28228] accumulated_eval_time=2161.21, accumulated_logging_time=0.514371, accumulated_submission_time=6013.7, global_step=28228, preemption_count=0, score=6013.7, test/accuracy=0.985997, test/loss=0.0475713, test/mean_average_precision=0.258901, test/num_examples=43793, total_duration=8176.15, train/accuracy=0.992427, train/loss=0.0248196, train/mean_average_precision=0.53404, validation/accuracy=0.986949, validation/loss=0.0445119, validation/mean_average_precision=0.274811, validation/num_examples=43793
I0307 18:41:27.155720 139517471074048 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.10148030519485474, loss=0.02860039658844471
I0307 18:41:48.395964 139517462681344 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.09592016786336899, loss=0.0281580351293087
I0307 18:42:09.826896 139517471074048 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.11893033981323242, loss=0.030861102044582367
I0307 18:42:31.159611 139517462681344 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.11419030278921127, loss=0.029072875156998634
I0307 18:42:52.526329 139517471074048 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.09579247236251831, loss=0.028145460411906242
I0307 18:43:14.115233 139517462681344 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.10411125421524048, loss=0.028567474335432053
I0307 18:43:35.633311 139517471074048 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.08321281522512436, loss=0.02910131774842739
I0307 18:43:57.130178 139517462681344 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0902845486998558, loss=0.027379797771573067
I0307 18:44:18.490791 139517471074048 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.10457918047904968, loss=0.029667764902114868
I0307 18:44:40.259852 139517462681344 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.11054177582263947, loss=0.029122022911906242
I0307 18:45:01.879897 139517471074048 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.12717588245868683, loss=0.03156019747257233
I0307 18:45:11.645821 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:46:26.099091 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:46:28.038633 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:46:29.951735 139659327427776 submission_runner.py:469] Time since start: 8494.56s, 	Step: 29347, 	{'train/accuracy': 0.9921352863311768, 'train/loss': 0.025597229599952698, 'train/mean_average_precision': 0.5200810427492805, 'validation/accuracy': 0.986847460269928, 'validation/loss': 0.04491814225912094, 'validation/mean_average_precision': 0.2664187236608896, 'validation/num_examples': 43793, 'test/accuracy': 0.985929548740387, 'test/loss': 0.0479218065738678, 'test/mean_average_precision': 0.2528803977516929, 'test/num_examples': 43793, 'score': 6253.750548362732, 'total_duration': 8494.56211233139, 'accumulated_submission_time': 6253.750548362732, 'accumulated_eval_time': 2239.5133967399597, 'accumulated_logging_time': 0.5379643440246582}
I0307 18:46:29.963874 139517462681344 logging_writer.py:48] [29347] accumulated_eval_time=2239.51, accumulated_logging_time=0.537964, accumulated_submission_time=6253.75, global_step=29347, preemption_count=0, score=6253.75, test/accuracy=0.98593, test/loss=0.0479218, test/mean_average_precision=0.25288, test/num_examples=43793, total_duration=8494.56, train/accuracy=0.992135, train/loss=0.0255972, train/mean_average_precision=0.520081, validation/accuracy=0.986847, validation/loss=0.0449181, validation/mean_average_precision=0.266419, validation/num_examples=43793
I0307 18:46:41.468162 139517471074048 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.10548972338438034, loss=0.029543446376919746
I0307 18:47:02.764991 139517462681344 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.09513813257217407, loss=0.03062664344906807
I0307 18:47:24.193524 139517471074048 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.09132208675146103, loss=0.030750662088394165
I0307 18:47:45.581865 139517462681344 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.1156025305390358, loss=0.028821034356951714
I0307 18:48:07.017922 139517471074048 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.10609416663646698, loss=0.029451092705130577
I0307 18:48:28.316699 139517462681344 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.09975306689739227, loss=0.026003936305642128
I0307 18:48:49.670676 139517471074048 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.1529543250799179, loss=0.02849021926522255
I0307 18:49:11.102942 139517462681344 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.09626354277133942, loss=0.029341934248805046
I0307 18:49:32.660206 139517471074048 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.12011989206075668, loss=0.028636842966079712
I0307 18:49:54.296257 139517462681344 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.1456218808889389, loss=0.030133428052067757
I0307 18:50:15.667540 139517471074048 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.11843442171812057, loss=0.03267671540379524
I0307 18:50:30.030011 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:51:41.444768 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:51:43.387707 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:51:45.347224 139659327427776 submission_runner.py:469] Time since start: 8809.96s, 	Step: 30468, 	{'train/accuracy': 0.9923573732376099, 'train/loss': 0.025053629651665688, 'train/mean_average_precision': 0.5168561382590843, 'validation/accuracy': 0.9869363903999329, 'validation/loss': 0.0449998676776886, 'validation/mean_average_precision': 0.27093491677922665, 'validation/num_examples': 43793, 'test/accuracy': 0.9860504269599915, 'test/loss': 0.048078544437885284, 'test/mean_average_precision': 0.2592491103125053, 'test/num_examples': 43793, 'score': 6493.779803276062, 'total_duration': 8809.95774102211, 'accumulated_submission_time': 6493.779803276062, 'accumulated_eval_time': 2314.8305563926697, 'accumulated_logging_time': 0.5593171119689941}
I0307 18:51:45.359784 139517462681344 logging_writer.py:48] [30468] accumulated_eval_time=2314.83, accumulated_logging_time=0.559317, accumulated_submission_time=6493.78, global_step=30468, preemption_count=0, score=6493.78, test/accuracy=0.98605, test/loss=0.0480785, test/mean_average_precision=0.259249, test/num_examples=43793, total_duration=8809.96, train/accuracy=0.992357, train/loss=0.0250536, train/mean_average_precision=0.516856, validation/accuracy=0.986936, validation/loss=0.0449999, validation/mean_average_precision=0.270935, validation/num_examples=43793
I0307 18:51:52.448914 139517471074048 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.08879382908344269, loss=0.027639377862215042
I0307 18:52:13.928677 139517462681344 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.09910423308610916, loss=0.030019696801900864
I0307 18:52:35.366076 139517471074048 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.10689158737659454, loss=0.02660309709608555
I0307 18:52:56.839239 139517462681344 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.1416001170873642, loss=0.029639188200235367
I0307 18:53:18.392680 139517471074048 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.11563822627067566, loss=0.031125027686357498
I0307 18:53:39.772103 139517462681344 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.1130010262131691, loss=0.027071325108408928
I0307 18:54:01.066729 139517471074048 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.10560130327939987, loss=0.027679573744535446
I0307 18:54:22.615761 139517462681344 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.12452041357755661, loss=0.031732868403196335
I0307 18:54:44.140547 139517471074048 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.1387391984462738, loss=0.029880588874220848
I0307 18:55:05.605465 139517462681344 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.09697240591049194, loss=0.027370529249310493
I0307 18:55:27.050006 139517471074048 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.12879648804664612, loss=0.02476794645190239
I0307 18:55:45.455653 139659327427776 spec.py:321] Evaluating on the training split.
I0307 18:57:00.741995 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 18:57:02.727411 139659327427776 spec.py:349] Evaluating on the test split.
I0307 18:57:04.655169 139659327427776 submission_runner.py:469] Time since start: 9129.27s, 	Step: 31587, 	{'train/accuracy': 0.9926300644874573, 'train/loss': 0.02374393120408058, 'train/mean_average_precision': 0.5493526286592454, 'validation/accuracy': 0.9870033860206604, 'validation/loss': 0.045271873474121094, 'validation/mean_average_precision': 0.2751791614452758, 'validation/num_examples': 43793, 'test/accuracy': 0.9860584139823914, 'test/loss': 0.04844682291150093, 'test/mean_average_precision': 0.2597082628577487, 'test/num_examples': 43793, 'score': 6733.836859226227, 'total_duration': 9129.265543699265, 'accumulated_submission_time': 6733.836859226227, 'accumulated_eval_time': 2394.029878616333, 'accumulated_logging_time': 0.5814833641052246}
I0307 18:57:04.667750 139517462681344 logging_writer.py:48] [31587] accumulated_eval_time=2394.03, accumulated_logging_time=0.581483, accumulated_submission_time=6733.84, global_step=31587, preemption_count=0, score=6733.84, test/accuracy=0.986058, test/loss=0.0484468, test/mean_average_precision=0.259708, test/num_examples=43793, total_duration=9129.27, train/accuracy=0.99263, train/loss=0.0237439, train/mean_average_precision=0.549353, validation/accuracy=0.987003, validation/loss=0.0452719, validation/mean_average_precision=0.275179, validation/num_examples=43793
I0307 18:57:07.725691 139517471074048 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.113131083548069, loss=0.03085308149456978
I0307 18:57:29.119559 139517462681344 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.11099318414926529, loss=0.028566783294081688
I0307 18:57:50.573900 139517471074048 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.10578620433807373, loss=0.030404724180698395
I0307 18:58:11.928499 139517462681344 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.14431197941303253, loss=0.02878282032907009
I0307 18:58:33.346059 139517471074048 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.11708861589431763, loss=0.027684388682246208
I0307 18:58:54.526777 139517462681344 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.10011156648397446, loss=0.029325788840651512
I0307 18:59:15.864991 139517471074048 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.09655354917049408, loss=0.029764842242002487
I0307 18:59:37.145206 139517462681344 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.11629969626665115, loss=0.030750516802072525
I0307 18:59:58.688755 139517471074048 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.11831054091453552, loss=0.03021661378443241
I0307 19:00:20.022360 139517462681344 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.10606569051742554, loss=0.028604108840227127
I0307 19:00:41.316384 139517471074048 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.12018263339996338, loss=0.028392836451530457
I0307 19:01:02.585387 139517462681344 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.11495304852724075, loss=0.026277532801032066
I0307 19:01:04.727179 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:02:14.938081 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:02:16.929584 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:02:18.910203 139659327427776 submission_runner.py:469] Time since start: 9443.52s, 	Step: 32711, 	{'train/accuracy': 0.9925702214241028, 'train/loss': 0.023990873247385025, 'train/mean_average_precision': 0.545553500948556, 'validation/accuracy': 0.9869193434715271, 'validation/loss': 0.0455615371465683, 'validation/mean_average_precision': 0.27216154715818497, 'validation/num_examples': 43793, 'test/accuracy': 0.9860382080078125, 'test/loss': 0.048560917377471924, 'test/mean_average_precision': 0.2574208522597094, 'test/num_examples': 43793, 'score': 6973.859038114548, 'total_duration': 9443.520685434341, 'accumulated_submission_time': 6973.859038114548, 'accumulated_eval_time': 2468.2128133773804, 'accumulated_logging_time': 0.6035270690917969}
I0307 19:02:18.923182 139517471074048 logging_writer.py:48] [32711] accumulated_eval_time=2468.21, accumulated_logging_time=0.603527, accumulated_submission_time=6973.86, global_step=32711, preemption_count=0, score=6973.86, test/accuracy=0.986038, test/loss=0.0485609, test/mean_average_precision=0.257421, test/num_examples=43793, total_duration=9443.52, train/accuracy=0.99257, train/loss=0.0239909, train/mean_average_precision=0.545554, validation/accuracy=0.986919, validation/loss=0.0455615, validation/mean_average_precision=0.272162, validation/num_examples=43793
I0307 19:02:38.173030 139517462681344 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.11767658591270447, loss=0.02856685221195221
I0307 19:02:59.566516 139517471074048 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.12153948098421097, loss=0.02668491005897522
I0307 19:03:30.531879 139517462681344 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.1277998983860016, loss=0.028086334466934204
I0307 19:03:52.015038 139517471074048 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.14826594293117523, loss=0.028873877599835396
I0307 19:04:13.455026 139517462681344 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.11036206781864166, loss=0.02628331258893013
I0307 19:04:34.842486 139517471074048 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.10345027595758438, loss=0.029030898585915565
I0307 19:04:56.149574 139517462681344 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.13074025511741638, loss=0.027830006554722786
I0307 19:05:17.700481 139517471074048 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.11460082232952118, loss=0.027367951348423958
I0307 19:05:39.166179 139517462681344 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.16765715181827545, loss=0.02808292582631111
I0307 19:06:00.414205 139517471074048 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.11696215718984604, loss=0.026545166969299316
I0307 19:06:19.089735 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:07:34.512407 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:07:36.483169 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:07:38.509608 139659327427776 submission_runner.py:469] Time since start: 9763.12s, 	Step: 33789, 	{'train/accuracy': 0.9934369325637817, 'train/loss': 0.021386293694376945, 'train/mean_average_precision': 0.6039116568987395, 'validation/accuracy': 0.9869063496589661, 'validation/loss': 0.04584026709198952, 'validation/mean_average_precision': 0.27104542879251936, 'validation/num_examples': 43793, 'test/accuracy': 0.9859910011291504, 'test/loss': 0.048934489488601685, 'test/mean_average_precision': 0.2596579320773102, 'test/num_examples': 43793, 'score': 7213.986223697662, 'total_duration': 9763.120081424713, 'accumulated_submission_time': 7213.986223697662, 'accumulated_eval_time': 2547.632602930069, 'accumulated_logging_time': 0.6295428276062012}
I0307 19:07:38.522668 139517462681344 logging_writer.py:48] [33789] accumulated_eval_time=2547.63, accumulated_logging_time=0.629543, accumulated_submission_time=7213.99, global_step=33789, preemption_count=0, score=7213.99, test/accuracy=0.985991, test/loss=0.0489345, test/mean_average_precision=0.259658, test/num_examples=43793, total_duration=9763.12, train/accuracy=0.993437, train/loss=0.0213863, train/mean_average_precision=0.603912, validation/accuracy=0.986906, validation/loss=0.0458403, validation/mean_average_precision=0.271045, validation/num_examples=43793
I0307 19:07:41.069056 139517471074048 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.1321977823972702, loss=0.028700117021799088
I0307 19:08:02.433583 139517462681344 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.13130982220172882, loss=0.029395345598459244
I0307 19:08:23.708052 139517471074048 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.12358690053224564, loss=0.025917213410139084
I0307 19:08:44.973211 139517462681344 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.13813413679599762, loss=0.02918827161192894
I0307 19:09:06.330710 139517471074048 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.14187213778495789, loss=0.029810458421707153
I0307 19:09:27.616863 139517462681344 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.17249849438667297, loss=0.03079855814576149
I0307 19:09:49.081043 139517471074048 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.14359316229820251, loss=0.029144391417503357
I0307 19:10:10.549399 139517462681344 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.13539062440395355, loss=0.028112996369600296
I0307 19:10:31.975715 139517471074048 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.13018105924129486, loss=0.027546294033527374
I0307 19:10:53.319541 139517462681344 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.11928441375494003, loss=0.028680777177214622
I0307 19:11:14.657614 139517471074048 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.1322656273841858, loss=0.027767809107899666
I0307 19:11:36.022842 139517462681344 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.13668027520179749, loss=0.03090757504105568
I0307 19:11:38.603676 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:12:51.102670 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:12:53.121432 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:12:55.057374 139659327427776 submission_runner.py:469] Time since start: 10079.67s, 	Step: 34913, 	{'train/accuracy': 0.9927451610565186, 'train/loss': 0.023204749450087547, 'train/mean_average_precision': 0.5658095586527297, 'validation/accuracy': 0.9868612885475159, 'validation/loss': 0.04628916457295418, 'validation/mean_average_precision': 0.2696627607899645, 'validation/num_examples': 43793, 'test/accuracy': 0.985939621925354, 'test/loss': 0.049364764243364334, 'test/mean_average_precision': 0.2539269310041063, 'test/num_examples': 43793, 'score': 7454.029727220535, 'total_duration': 10079.66783618927, 'accumulated_submission_time': 7454.029727220535, 'accumulated_eval_time': 2624.0861914157867, 'accumulated_logging_time': 0.6527044773101807}
I0307 19:12:55.071234 139517471074048 logging_writer.py:48] [34913] accumulated_eval_time=2624.09, accumulated_logging_time=0.652704, accumulated_submission_time=7454.03, global_step=34913, preemption_count=0, score=7454.03, test/accuracy=0.98594, test/loss=0.0493648, test/mean_average_precision=0.253927, test/num_examples=43793, total_duration=10079.7, train/accuracy=0.992745, train/loss=0.0232047, train/mean_average_precision=0.56581, validation/accuracy=0.986861, validation/loss=0.0462892, validation/mean_average_precision=0.269663, validation/num_examples=43793
I0307 19:13:13.590436 139517462681344 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.13000236451625824, loss=0.02802375704050064
I0307 19:13:35.019439 139517471074048 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.11807821691036224, loss=0.025516660884022713
I0307 19:13:56.449312 139517462681344 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.14681048691272736, loss=0.027732515707612038
I0307 19:14:17.955346 139517471074048 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.12718334794044495, loss=0.029082665219902992
I0307 19:14:39.199557 139517462681344 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.13965223729610443, loss=0.028210071846842766
I0307 19:15:00.562489 139517471074048 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.1268387883901596, loss=0.027328742668032646
I0307 19:15:21.975489 139517462681344 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.1368096023797989, loss=0.02778911590576172
I0307 19:15:43.290387 139517471074048 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.12522169947624207, loss=0.028038229793310165
I0307 19:16:04.610396 139517462681344 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.12480147182941437, loss=0.02816031314432621
I0307 19:16:26.218654 139517471074048 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.123233363032341, loss=0.026461385190486908
I0307 19:16:47.715965 139517462681344 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.14802414178848267, loss=0.027088051661849022
I0307 19:16:55.139904 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:18:10.313823 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:18:12.284624 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:18:14.227051 139659327427776 submission_runner.py:469] Time since start: 10398.84s, 	Step: 36036, 	{'train/accuracy': 0.993628203868866, 'train/loss': 0.020763562992215157, 'train/mean_average_precision': 0.6191475711928272, 'validation/accuracy': 0.9868194460868835, 'validation/loss': 0.046570561826229095, 'validation/mean_average_precision': 0.2700427519962571, 'validation/num_examples': 43793, 'test/accuracy': 0.9858313798904419, 'test/loss': 0.0498359315097332, 'test/mean_average_precision': 0.25920728306436336, 'test/num_examples': 43793, 'score': 7694.060019254684, 'total_duration': 10398.837435245514, 'accumulated_submission_time': 7694.060019254684, 'accumulated_eval_time': 2703.173148870468, 'accumulated_logging_time': 0.6759274005889893}
I0307 19:18:14.240120 139517471074048 logging_writer.py:48] [36036] accumulated_eval_time=2703.17, accumulated_logging_time=0.675927, accumulated_submission_time=7694.06, global_step=36036, preemption_count=0, score=7694.06, test/accuracy=0.985831, test/loss=0.0498359, test/mean_average_precision=0.259207, test/num_examples=43793, total_duration=10398.8, train/accuracy=0.993628, train/loss=0.0207636, train/mean_average_precision=0.619148, validation/accuracy=0.986819, validation/loss=0.0465706, validation/mean_average_precision=0.270043, validation/num_examples=43793
I0307 19:18:28.228380 139517462681344 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.12142094224691391, loss=0.024587413296103477
I0307 19:18:49.796322 139517471074048 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.16314588487148285, loss=0.025390584021806717
I0307 19:19:11.132811 139517462681344 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.16112589836120605, loss=0.02840791456401348
I0307 19:19:32.596013 139517471074048 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.1382877379655838, loss=0.0259756650775671
I0307 19:19:54.100785 139517462681344 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.12313804030418396, loss=0.02610287442803383
I0307 19:20:15.452990 139517471074048 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.13224747776985168, loss=0.025922320783138275
I0307 19:20:36.942879 139517462681344 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.17778517305850983, loss=0.02918071672320366
I0307 19:20:58.336686 139517471074048 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.142441064119339, loss=0.023957882076501846
I0307 19:21:19.765942 139517462681344 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.14726977050304413, loss=0.026315541937947273
I0307 19:21:41.399651 139517471074048 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.16414763033390045, loss=0.027055595070123672
I0307 19:22:02.824684 139517462681344 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.14768089354038239, loss=0.023703666403889656
I0307 19:22:14.294360 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:23:28.032467 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:23:30.015360 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:23:32.081782 139659327427776 submission_runner.py:469] Time since start: 10716.69s, 	Step: 37154, 	{'train/accuracy': 0.993422269821167, 'train/loss': 0.02095273695886135, 'train/mean_average_precision': 0.598515751992627, 'validation/accuracy': 0.9868576526641846, 'validation/loss': 0.04717548191547394, 'validation/mean_average_precision': 0.27039233755447034, 'validation/num_examples': 43793, 'test/accuracy': 0.9859105944633484, 'test/loss': 0.0504155308008194, 'test/mean_average_precision': 0.2550039953521819, 'test/num_examples': 43793, 'score': 7934.0755879879, 'total_duration': 10716.692224264145, 'accumulated_submission_time': 7934.0755879879, 'accumulated_eval_time': 2780.9604473114014, 'accumulated_logging_time': 0.6981160640716553}
I0307 19:23:32.095992 139517471074048 logging_writer.py:48] [37154] accumulated_eval_time=2780.96, accumulated_logging_time=0.698116, accumulated_submission_time=7934.08, global_step=37154, preemption_count=0, score=7934.08, test/accuracy=0.985911, test/loss=0.0504155, test/mean_average_precision=0.255004, test/num_examples=43793, total_duration=10716.7, train/accuracy=0.993422, train/loss=0.0209527, train/mean_average_precision=0.598516, validation/accuracy=0.986858, validation/loss=0.0471755, validation/mean_average_precision=0.270392, validation/num_examples=43793
I0307 19:23:42.071219 139517462681344 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.16656436026096344, loss=0.027414951473474503
I0307 19:24:03.278620 139517471074048 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.15391187369823456, loss=0.02797090820968151
I0307 19:24:25.031078 139517462681344 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.14400805532932281, loss=0.024741636589169502
I0307 19:24:46.425860 139517471074048 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.1555795967578888, loss=0.028011620044708252
I0307 19:25:07.881785 139517462681344 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.14661642909049988, loss=0.025656117126345634
I0307 19:25:29.432614 139517471074048 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.16459113359451294, loss=0.02701915055513382
I0307 19:25:50.971651 139517462681344 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.15999849140644073, loss=0.025097457692027092
I0307 19:26:12.519518 139517471074048 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.18874873220920563, loss=0.025825515389442444
I0307 19:26:33.806917 139517462681344 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.13573040068149567, loss=0.024542702361941338
I0307 19:26:55.213591 139517471074048 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.15140362083911896, loss=0.023641588166356087
I0307 19:27:16.705681 139517462681344 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.17350240051746368, loss=0.025562996044754982
I0307 19:27:32.132109 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:28:48.702929 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:28:50.681652 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:28:52.618976 139659327427776 submission_runner.py:469] Time since start: 11037.23s, 	Step: 38273, 	{'train/accuracy': 0.993793249130249, 'train/loss': 0.019994808360934258, 'train/mean_average_precision': 0.6234364863375629, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.04793078452348709, 'validation/mean_average_precision': 0.2625957496125843, 'validation/num_examples': 43793, 'test/accuracy': 0.9859004616737366, 'test/loss': 0.051143474876880646, 'test/mean_average_precision': 0.25116897686096334, 'test/num_examples': 43793, 'score': 8174.071074962616, 'total_duration': 11037.229346990585, 'accumulated_submission_time': 8174.071074962616, 'accumulated_eval_time': 2861.4471204280853, 'accumulated_logging_time': 0.7253048419952393}
I0307 19:28:52.632094 139517471074048 logging_writer.py:48] [38273] accumulated_eval_time=2861.45, accumulated_logging_time=0.725305, accumulated_submission_time=8174.07, global_step=38273, preemption_count=0, score=8174.07, test/accuracy=0.9859, test/loss=0.0511435, test/mean_average_precision=0.251169, test/num_examples=43793, total_duration=11037.2, train/accuracy=0.993793, train/loss=0.0199948, train/mean_average_precision=0.623436, validation/accuracy=0.986692, validation/loss=0.0479308, validation/mean_average_precision=0.262596, validation/num_examples=43793
I0307 19:28:58.748297 139517462681344 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.16112951934337616, loss=0.024844471365213394
I0307 19:29:20.306656 139517471074048 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.15765610337257385, loss=0.025006337091326714
I0307 19:29:41.823629 139517462681344 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.18868295848369598, loss=0.026237063109874725
I0307 19:30:03.335543 139517471074048 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.15849915146827698, loss=0.022663842886686325
I0307 19:30:24.878786 139517462681344 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.1361626833677292, loss=0.02451271191239357
I0307 19:30:46.399209 139517471074048 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.19772516191005707, loss=0.027649618685245514
I0307 19:31:07.969901 139517462681344 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.19150125980377197, loss=0.027025185525417328
I0307 19:31:29.359870 139517471074048 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.1572161316871643, loss=0.024846509099006653
I0307 19:31:50.753025 139517462681344 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.1504659801721573, loss=0.02378019504249096
I0307 19:32:12.218111 139517471074048 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.1461867392063141, loss=0.022393153980374336
I0307 19:32:33.560594 139517462681344 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.1746867150068283, loss=0.02515522576868534
I0307 19:32:52.691512 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:34:08.638537 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:34:10.627873 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:34:12.570053 139659327427776 submission_runner.py:469] Time since start: 11357.18s, 	Step: 39390, 	{'train/accuracy': 0.9942801594734192, 'train/loss': 0.01863309182226658, 'train/mean_average_precision': 0.6550697077654688, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.048125702887773514, 'validation/mean_average_precision': 0.2655068313219917, 'validation/num_examples': 43793, 'test/accuracy': 0.9857551455497742, 'test/loss': 0.051581624895334244, 'test/mean_average_precision': 0.2515382030447509, 'test/num_examples': 43793, 'score': 8414.090274810791, 'total_duration': 11357.180511713028, 'accumulated_submission_time': 8414.090274810791, 'accumulated_eval_time': 2941.3255622386932, 'accumulated_logging_time': 0.7496404647827148}
I0307 19:34:12.584113 139517471074048 logging_writer.py:48] [39390] accumulated_eval_time=2941.33, accumulated_logging_time=0.74964, accumulated_submission_time=8414.09, global_step=39390, preemption_count=0, score=8414.09, test/accuracy=0.985755, test/loss=0.0515816, test/mean_average_precision=0.251538, test/num_examples=43793, total_duration=11357.2, train/accuracy=0.99428, train/loss=0.0186331, train/mean_average_precision=0.65507, validation/accuracy=0.986681, validation/loss=0.0481257, validation/mean_average_precision=0.265507, validation/num_examples=43793
I0307 19:34:15.002561 139517462681344 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1813444048166275, loss=0.025098543614149094
I0307 19:34:36.413298 139517471074048 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.16571779549121857, loss=0.024916432797908783
I0307 19:34:57.863346 139517462681344 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.1550271362066269, loss=0.025437338277697563
I0307 19:35:19.398358 139517471074048 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.17656268179416656, loss=0.02510738931596279
I0307 19:35:40.738552 139517462681344 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.17718449234962463, loss=0.023107152432203293
I0307 19:36:02.214791 139517471074048 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.16320393979549408, loss=0.02477414160966873
I0307 19:36:23.933988 139517462681344 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.16762138903141022, loss=0.024071883410215378
I0307 19:36:45.332240 139517471074048 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.17406263947486877, loss=0.026496117934584618
I0307 19:37:06.634010 139517462681344 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.17039956152439117, loss=0.023443466052412987
I0307 19:37:27.985059 139517471074048 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.1792910248041153, loss=0.023689057677984238
I0307 19:37:49.321648 139517462681344 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.15607477724552155, loss=0.02248997800052166
I0307 19:38:10.895886 139517471074048 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.18456265330314636, loss=0.024550342932343483
I0307 19:38:12.606653 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:39:31.741062 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:39:33.689679 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:39:35.594187 139659327427776 submission_runner.py:469] Time since start: 11680.20s, 	Step: 40509, 	{'train/accuracy': 0.9939073324203491, 'train/loss': 0.01933678798377514, 'train/mean_average_precision': 0.64784043836471, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.04882986098527908, 'validation/mean_average_precision': 0.26305309566465684, 'validation/num_examples': 43793, 'test/accuracy': 0.9857977032661438, 'test/loss': 0.05236607417464256, 'test/mean_average_precision': 0.24949630422558006, 'test/num_examples': 43793, 'score': 8654.076290130615, 'total_duration': 11680.204643249512, 'accumulated_submission_time': 8654.076290130615, 'accumulated_eval_time': 3024.312996149063, 'accumulated_logging_time': 0.773061990737915}
I0307 19:39:35.607914 139517462681344 logging_writer.py:48] [40509] accumulated_eval_time=3024.31, accumulated_logging_time=0.773062, accumulated_submission_time=8654.08, global_step=40509, preemption_count=0, score=8654.08, test/accuracy=0.985798, test/loss=0.0523661, test/mean_average_precision=0.249496, test/num_examples=43793, total_duration=11680.2, train/accuracy=0.993907, train/loss=0.0193368, train/mean_average_precision=0.64784, validation/accuracy=0.98672, validation/loss=0.0488299, validation/mean_average_precision=0.263053, validation/num_examples=43793
I0307 19:39:55.154473 139517471074048 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.16693784296512604, loss=0.02496776543557644
I0307 19:40:16.557703 139517462681344 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.1599491983652115, loss=0.024016719311475754
I0307 19:40:38.107495 139517471074048 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.17060044407844543, loss=0.024425465613603592
I0307 19:40:59.669412 139517462681344 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.18847611546516418, loss=0.02598024718463421
I0307 19:41:21.035417 139517471074048 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.20488372445106506, loss=0.022179463878273964
I0307 19:41:42.207483 139517462681344 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.14602449536323547, loss=0.022456897422671318
I0307 19:42:03.725612 139517471074048 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.18486791849136353, loss=0.024541189894080162
I0307 19:42:25.182836 139517462681344 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.1587485373020172, loss=0.023941755294799805
I0307 19:42:46.540175 139517471074048 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.19277609884738922, loss=0.021663259714841843
I0307 19:43:07.976877 139517462681344 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.16935044527053833, loss=0.02139691635966301
I0307 19:43:29.421310 139517471074048 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.18416495621204376, loss=0.02297348715364933
I0307 19:43:35.659649 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:44:49.027379 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:44:51.101307 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:44:53.134786 139659327427776 submission_runner.py:469] Time since start: 11997.75s, 	Step: 41630, 	{'train/accuracy': 0.994847297668457, 'train/loss': 0.016862019896507263, 'train/mean_average_precision': 0.694057481408302, 'validation/accuracy': 0.9865572452545166, 'validation/loss': 0.04959559813141823, 'validation/mean_average_precision': 0.25781379753699946, 'validation/num_examples': 43793, 'test/accuracy': 0.985562264919281, 'test/loss': 0.05327992141246796, 'test/mean_average_precision': 0.24537853381887362, 'test/num_examples': 43793, 'score': 8894.088335990906, 'total_duration': 11997.745305538177, 'accumulated_submission_time': 8894.088335990906, 'accumulated_eval_time': 3101.7880811691284, 'accumulated_logging_time': 0.7954974174499512}
I0307 19:44:53.148885 139517462681344 logging_writer.py:48] [41630] accumulated_eval_time=3101.79, accumulated_logging_time=0.795497, accumulated_submission_time=8894.09, global_step=41630, preemption_count=0, score=8894.09, test/accuracy=0.985562, test/loss=0.0532799, test/mean_average_precision=0.245379, test/num_examples=43793, total_duration=11997.7, train/accuracy=0.994847, train/loss=0.016862, train/mean_average_precision=0.694057, validation/accuracy=0.986557, validation/loss=0.0495956, validation/mean_average_precision=0.257814, validation/num_examples=43793
I0307 19:45:08.357698 139517471074048 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2141389697790146, loss=0.023061756044626236
I0307 19:45:29.877461 139517462681344 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.18710428476333618, loss=0.02506420761346817
I0307 19:45:51.439551 139517471074048 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.18573731184005737, loss=0.023684127256274223
I0307 19:46:12.944133 139517462681344 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.19738440215587616, loss=0.023881928995251656
I0307 19:46:34.527444 139517471074048 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.1760580688714981, loss=0.022605154663324356
I0307 19:46:55.873554 139517462681344 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1640121042728424, loss=0.022246168926358223
I0307 19:47:17.218816 139517471074048 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.18807291984558105, loss=0.02247878536581993
I0307 19:47:38.560383 139517462681344 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.21368077397346497, loss=0.024447111412882805
I0307 19:48:00.776104 139517471074048 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.1783483624458313, loss=0.0217340886592865
I0307 19:48:23.570975 139517462681344 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.19074486196041107, loss=0.02160303294658661
I0307 19:48:45.724702 139517471074048 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2110057771205902, loss=0.024202346801757812
I0307 19:48:53.305441 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:50:07.451761 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:50:12.284978 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:50:14.242269 139659327427776 submission_runner.py:469] Time since start: 12318.85s, 	Step: 42736, 	{'train/accuracy': 0.9941988587379456, 'train/loss': 0.018283061683177948, 'train/mean_average_precision': 0.6564854197468704, 'validation/accuracy': 0.9865552186965942, 'validation/loss': 0.05036221072077751, 'validation/mean_average_precision': 0.2599248903636448, 'validation/num_examples': 43793, 'test/accuracy': 0.9856275320053101, 'test/loss': 0.05392998084425926, 'test/mean_average_precision': 0.24534255280443062, 'test/num_examples': 43793, 'score': 9134.207527399063, 'total_duration': 12318.852712869644, 'accumulated_submission_time': 9134.207527399063, 'accumulated_eval_time': 3182.724787712097, 'accumulated_logging_time': 0.8188595771789551}
I0307 19:50:14.283133 139517462681344 logging_writer.py:48] [42736] accumulated_eval_time=3182.72, accumulated_logging_time=0.81886, accumulated_submission_time=9134.21, global_step=42736, preemption_count=0, score=9134.21, test/accuracy=0.985628, test/loss=0.05393, test/mean_average_precision=0.245343, test/num_examples=43793, total_duration=12318.9, train/accuracy=0.994199, train/loss=0.0182831, train/mean_average_precision=0.656485, validation/accuracy=0.986555, validation/loss=0.0503622, validation/mean_average_precision=0.259925, validation/num_examples=43793
I0307 19:50:28.135521 139517471074048 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.2111096829175949, loss=0.023597806692123413
I0307 19:50:49.523405 139517462681344 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.17027907073497772, loss=0.02157287858426571
I0307 19:51:10.701542 139517471074048 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.17393898963928223, loss=0.02170509658753872
I0307 19:51:32.182678 139517462681344 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.17602893710136414, loss=0.02387559786438942
I0307 19:51:53.804424 139517471074048 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.17061163485050201, loss=0.02138088084757328
I0307 19:52:15.303645 139517462681344 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.20710712671279907, loss=0.02214054763317108
I0307 19:52:36.864826 139517471074048 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.19876307249069214, loss=0.021646013483405113
I0307 19:52:58.305264 139517462681344 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.23796552419662476, loss=0.0214410200715065
I0307 19:53:19.883083 139517471074048 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.1832004189491272, loss=0.022133952006697655
I0307 19:53:41.333409 139517462681344 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.19000421464443207, loss=0.021283168345689774
I0307 19:54:02.700487 139517471074048 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.19658243656158447, loss=0.021908452734351158
I0307 19:54:14.368127 139659327427776 spec.py:321] Evaluating on the training split.
I0307 19:55:26.885103 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 19:55:28.843331 139659327427776 spec.py:349] Evaluating on the test split.
I0307 19:55:30.957394 139659327427776 submission_runner.py:469] Time since start: 12635.57s, 	Step: 43855, 	{'train/accuracy': 0.9953716397285461, 'train/loss': 0.015282107517123222, 'train/mean_average_precision': 0.7246460761293376, 'validation/accuracy': 0.9865056872367859, 'validation/loss': 0.05144544690847397, 'validation/mean_average_precision': 0.2547158524794639, 'validation/num_examples': 43793, 'test/accuracy': 0.9855795502662659, 'test/loss': 0.05520850419998169, 'test/mean_average_precision': 0.24428296618916157, 'test/num_examples': 43793, 'score': 9374.253158330917, 'total_duration': 12635.567828655243, 'accumulated_submission_time': 9374.253158330917, 'accumulated_eval_time': 3259.3139259815216, 'accumulated_logging_time': 0.8692541122436523}
I0307 19:55:30.973181 139517462681344 logging_writer.py:48] [43855] accumulated_eval_time=3259.31, accumulated_logging_time=0.869254, accumulated_submission_time=9374.25, global_step=43855, preemption_count=0, score=9374.25, test/accuracy=0.98558, test/loss=0.0552085, test/mean_average_precision=0.244283, test/num_examples=43793, total_duration=12635.6, train/accuracy=0.995372, train/loss=0.0152821, train/mean_average_precision=0.724646, validation/accuracy=0.986506, validation/loss=0.0514454, validation/mean_average_precision=0.254716, validation/num_examples=43793
I0307 19:55:40.869924 139517471074048 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.17138002812862396, loss=0.020813394337892532
I0307 19:56:02.200638 139517462681344 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2037108689546585, loss=0.0219089575111866
I0307 19:56:23.594589 139517471074048 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1852208375930786, loss=0.02342258207499981
I0307 19:56:45.030508 139517462681344 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1708085536956787, loss=0.020687928423285484
I0307 19:57:06.593332 139517471074048 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.20409171283245087, loss=0.021741457283496857
I0307 19:57:27.979960 139517462681344 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.1727493405342102, loss=0.019894840195775032
I0307 19:57:49.381871 139517471074048 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.1959792524576187, loss=0.02282804250717163
I0307 19:58:10.698724 139517462681344 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.188926100730896, loss=0.023155804723501205
I0307 19:58:32.136627 139517471074048 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1994001418352127, loss=0.02132575586438179
I0307 19:58:53.521331 139517462681344 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.18097473680973053, loss=0.020242126658558846
I0307 19:59:14.857790 139517471074048 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.18377768993377686, loss=0.022076036781072617
I0307 19:59:31.080447 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:00:44.890370 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:00:46.840153 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:00:48.730486 139659327427776 submission_runner.py:469] Time since start: 12953.34s, 	Step: 44977, 	{'train/accuracy': 0.9948879480361938, 'train/loss': 0.01627352088689804, 'train/mean_average_precision': 0.6932686653719271, 'validation/accuracy': 0.986436665058136, 'validation/loss': 0.052207138389348984, 'validation/mean_average_precision': 0.25352518680269764, 'validation/num_examples': 43793, 'test/accuracy': 0.9855159521102905, 'test/loss': 0.05578320100903511, 'test/mean_average_precision': 0.24101867055998927, 'test/num_examples': 43793, 'score': 9614.322991847992, 'total_duration': 12953.340996742249, 'accumulated_submission_time': 9614.322991847992, 'accumulated_eval_time': 3336.963909626007, 'accumulated_logging_time': 0.8951094150543213}
I0307 20:00:48.744936 139517462681344 logging_writer.py:48] [44977] accumulated_eval_time=3336.96, accumulated_logging_time=0.895109, accumulated_submission_time=9614.32, global_step=44977, preemption_count=0, score=9614.32, test/accuracy=0.985516, test/loss=0.0557832, test/mean_average_precision=0.241019, test/num_examples=43793, total_duration=12953.3, train/accuracy=0.994888, train/loss=0.0162735, train/mean_average_precision=0.693269, validation/accuracy=0.986437, validation/loss=0.0522071, validation/mean_average_precision=0.253525, validation/num_examples=43793
I0307 20:00:53.806098 139517471074048 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.20418080687522888, loss=0.021014750003814697
I0307 20:01:14.956188 139517462681344 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.17622463405132294, loss=0.021784545853734016
I0307 20:01:36.359896 139517471074048 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.19984853267669678, loss=0.021228620782494545
I0307 20:01:57.895269 139517462681344 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.18562331795692444, loss=0.02043292112648487
I0307 20:02:19.602025 139517471074048 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.18739186227321625, loss=0.02369941771030426
I0307 20:02:41.142605 139517462681344 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.21758630871772766, loss=0.02019045129418373
I0307 20:03:02.784167 139517471074048 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.17328114807605743, loss=0.02123490907251835
I0307 20:03:24.331989 139517462681344 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.21530760824680328, loss=0.020755859091877937
I0307 20:03:45.859171 139517471074048 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.18165510892868042, loss=0.021862974390387535
I0307 20:04:07.379854 139517462681344 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.18897424638271332, loss=0.020166592672467232
I0307 20:04:28.729736 139517471074048 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.18668243288993835, loss=0.0204001534730196
I0307 20:04:48.874120 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:06:03.806554 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:06:05.755516 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:06:07.649385 139659327427776 submission_runner.py:469] Time since start: 13272.26s, 	Step: 46095, 	{'train/accuracy': 0.9952924847602844, 'train/loss': 0.015211712568998337, 'train/mean_average_precision': 0.7254823150809586, 'validation/accuracy': 0.9863481521606445, 'validation/loss': 0.05264874920248985, 'validation/mean_average_precision': 0.2506719634003946, 'validation/num_examples': 43793, 'test/accuracy': 0.9854135513305664, 'test/loss': 0.05640161409974098, 'test/mean_average_precision': 0.23969290206991867, 'test/num_examples': 43793, 'score': 9854.41327881813, 'total_duration': 13272.259805679321, 'accumulated_submission_time': 9854.41327881813, 'accumulated_eval_time': 3415.7390475273132, 'accumulated_logging_time': 0.9191961288452148}
I0307 20:06:07.663834 139517462681344 logging_writer.py:48] [46095] accumulated_eval_time=3415.74, accumulated_logging_time=0.919196, accumulated_submission_time=9854.41, global_step=46095, preemption_count=0, score=9854.41, test/accuracy=0.985414, test/loss=0.0564016, test/mean_average_precision=0.239693, test/num_examples=43793, total_duration=13272.3, train/accuracy=0.995292, train/loss=0.0152117, train/mean_average_precision=0.725482, validation/accuracy=0.986348, validation/loss=0.0526487, validation/mean_average_precision=0.250672, validation/num_examples=43793
I0307 20:06:08.982822 139517471074048 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.18730668723583221, loss=0.02079831063747406
I0307 20:06:30.227293 139517462681344 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.21858781576156616, loss=0.02083020657300949
I0307 20:06:51.916645 139517471074048 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.20420531928539276, loss=0.019733576104044914
I0307 20:07:13.416570 139517462681344 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.20099565386772156, loss=0.021067487075924873
I0307 20:07:35.021195 139517471074048 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.18517103791236877, loss=0.01885831542313099
I0307 20:07:56.459544 139517462681344 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.17159192264080048, loss=0.020011471584439278
I0307 20:08:17.814218 139517471074048 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.1867903172969818, loss=0.01892850361764431
I0307 20:08:39.321447 139517462681344 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.17897412180900574, loss=0.020195992663502693
I0307 20:09:00.835854 139517471074048 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.17731350660324097, loss=0.019915100187063217
I0307 20:09:22.456556 139517462681344 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.19163373112678528, loss=0.018933789804577827
I0307 20:09:43.881707 139517471074048 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.20397348701953888, loss=0.0200494471937418
I0307 20:10:05.286511 139517462681344 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.19683456420898438, loss=0.021729471161961555
I0307 20:10:07.829451 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:11:20.809419 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:11:22.806163 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:11:24.715071 139659327427776 submission_runner.py:469] Time since start: 13589.33s, 	Step: 47213, 	{'train/accuracy': 0.9951796531677246, 'train/loss': 0.015382890589535236, 'train/mean_average_precision': 0.713931345592469, 'validation/accuracy': 0.9863579273223877, 'validation/loss': 0.05329151824116707, 'validation/mean_average_precision': 0.24951376141884868, 'validation/num_examples': 43793, 'test/accuracy': 0.9854022264480591, 'test/loss': 0.057217709720134735, 'test/mean_average_precision': 0.23853706032703315, 'test/num_examples': 43793, 'score': 10094.540465831757, 'total_duration': 13589.325511932373, 'accumulated_submission_time': 10094.540465831757, 'accumulated_eval_time': 3492.624537229538, 'accumulated_logging_time': 0.9445290565490723}
I0307 20:11:24.730749 139517471074048 logging_writer.py:48] [47213] accumulated_eval_time=3492.62, accumulated_logging_time=0.944529, accumulated_submission_time=10094.5, global_step=47213, preemption_count=0, score=10094.5, test/accuracy=0.985402, test/loss=0.0572177, test/mean_average_precision=0.238537, test/num_examples=43793, total_duration=13589.3, train/accuracy=0.99518, train/loss=0.0153829, train/mean_average_precision=0.713931, validation/accuracy=0.986358, validation/loss=0.0532915, validation/mean_average_precision=0.249514, validation/num_examples=43793
I0307 20:11:43.529767 139517462681344 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.18510691821575165, loss=0.021958032622933388
I0307 20:12:05.026375 139517471074048 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20831267535686493, loss=0.021850358694791794
I0307 20:12:26.495115 139517462681344 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.20319795608520508, loss=0.021771440282464027
I0307 20:12:48.143805 139517471074048 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.1727549284696579, loss=0.019801825284957886
I0307 20:13:09.667721 139517462681344 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.20590409636497498, loss=0.020512936636805534
I0307 20:13:31.174239 139517471074048 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.20226508378982544, loss=0.021758001297712326
I0307 20:13:52.594451 139517462681344 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.20812977850437164, loss=0.01822398230433464
I0307 20:14:14.350838 139517471074048 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.174007847905159, loss=0.01975344493985176
I0307 20:14:35.939016 139517462681344 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.18715256452560425, loss=0.019735505804419518
I0307 20:14:57.100425 139517471074048 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.2088531255722046, loss=0.02115427888929844
I0307 20:15:18.752469 139517462681344 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.19565410912036896, loss=0.019502777606248856
I0307 20:15:24.787855 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:16:40.179775 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:16:42.122174 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:16:44.024072 139659327427776 submission_runner.py:469] Time since start: 13908.63s, 	Step: 48329, 	{'train/accuracy': 0.9950525760650635, 'train/loss': 0.015712063759565353, 'train/mean_average_precision': 0.7076125233639754, 'validation/accuracy': 0.9862523674964905, 'validation/loss': 0.05357007682323456, 'validation/mean_average_precision': 0.24951509579469222, 'validation/num_examples': 43793, 'test/accuracy': 0.9853423833847046, 'test/loss': 0.057396870106458664, 'test/mean_average_precision': 0.23706950968264806, 'test/num_examples': 43793, 'score': 10334.556473255157, 'total_duration': 13908.63451051712, 'accumulated_submission_time': 10334.556473255157, 'accumulated_eval_time': 3571.8606209754944, 'accumulated_logging_time': 0.9702322483062744}
I0307 20:16:44.069871 139517471074048 logging_writer.py:48] [48329] accumulated_eval_time=3571.86, accumulated_logging_time=0.970232, accumulated_submission_time=10334.6, global_step=48329, preemption_count=0, score=10334.6, test/accuracy=0.985342, test/loss=0.0573969, test/mean_average_precision=0.23707, test/num_examples=43793, total_duration=13908.6, train/accuracy=0.995053, train/loss=0.0157121, train/mean_average_precision=0.707613, validation/accuracy=0.986252, validation/loss=0.0535701, validation/mean_average_precision=0.249515, validation/num_examples=43793
I0307 20:16:59.583272 139517462681344 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.1794460415840149, loss=0.02016761526465416
I0307 20:17:21.248132 139517471074048 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.201268270611763, loss=0.020896008238196373
I0307 20:17:42.898617 139517462681344 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.19897067546844482, loss=0.018555041402578354
I0307 20:18:04.534616 139517471074048 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.1971181184053421, loss=0.020619617775082588
I0307 20:18:26.194020 139517462681344 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.19461293518543243, loss=0.019727101549506187
I0307 20:18:47.825540 139517471074048 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.1778687983751297, loss=0.018686868250370026
I0307 20:19:09.485793 139517462681344 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.21422123908996582, loss=0.02056044153869152
I0307 20:19:31.016018 139517471074048 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.19095228612422943, loss=0.020705193281173706
I0307 20:19:52.625392 139517462681344 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2123669683933258, loss=0.020216090604662895
I0307 20:20:13.941497 139517471074048 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.20570427179336548, loss=0.020594509318470955
I0307 20:20:35.298207 139517462681344 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2071576714515686, loss=0.02170364372432232
I0307 20:20:44.234722 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:21:57.748231 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:21:59.751046 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:22:01.821953 139659327427776 submission_runner.py:469] Time since start: 14226.43s, 	Step: 49443, 	{'train/accuracy': 0.9962520599365234, 'train/loss': 0.013007866218686104, 'train/mean_average_precision': 0.7611827810690932, 'validation/accuracy': 0.9862824082374573, 'validation/loss': 0.053753141313791275, 'validation/mean_average_precision': 0.25013255869141543, 'validation/num_examples': 43793, 'test/accuracy': 0.9853731393814087, 'test/loss': 0.057642221450805664, 'test/mean_average_precision': 0.23881116307435937, 'test/num_examples': 43793, 'score': 10574.680939912796, 'total_duration': 14226.432375192642, 'accumulated_submission_time': 10574.680939912796, 'accumulated_eval_time': 3649.4477050304413, 'accumulated_logging_time': 1.0275158882141113}
I0307 20:22:01.837348 139517471074048 logging_writer.py:48] [49443] accumulated_eval_time=3649.45, accumulated_logging_time=1.02752, accumulated_submission_time=10574.7, global_step=49443, preemption_count=0, score=10574.7, test/accuracy=0.985373, test/loss=0.0576422, test/mean_average_precision=0.238811, test/num_examples=43793, total_duration=14226.4, train/accuracy=0.996252, train/loss=0.0130079, train/mean_average_precision=0.761183, validation/accuracy=0.986282, validation/loss=0.0537531, validation/mean_average_precision=0.250133, validation/num_examples=43793
I0307 20:22:14.393668 139517462681344 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.1905028074979782, loss=0.01907205954194069
I0307 20:22:36.044407 139517471074048 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.19404630362987518, loss=0.018621716648340225
I0307 20:22:57.783368 139517462681344 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.20375153422355652, loss=0.020093554630875587
I0307 20:23:19.357180 139517471074048 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.2079564332962036, loss=0.020005831494927406
I0307 20:23:41.046622 139517462681344 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.19804759323596954, loss=0.020730553194880486
I0307 20:24:02.713258 139517471074048 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.24373607337474823, loss=0.021012842655181885
I0307 20:24:24.215086 139517462681344 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1976463347673416, loss=0.021735232323408127
I0307 20:24:45.867552 139517471074048 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.179618239402771, loss=0.019666379317641258
I0307 20:25:07.450149 139517462681344 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.20346586406230927, loss=0.020131109282374382
I0307 20:25:28.967918 139517471074048 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.19762372970581055, loss=0.020181739702820778
I0307 20:25:50.414009 139517462681344 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.19881948828697205, loss=0.020031340420246124
I0307 20:26:01.906662 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:27:15.703770 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:27:17.680490 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:27:19.609688 139659327427776 submission_runner.py:469] Time since start: 14544.22s, 	Step: 50554, 	{'train/accuracy': 0.9959763288497925, 'train/loss': 0.013618393801152706, 'train/mean_average_precision': 0.7463143858031025, 'validation/accuracy': 0.986258864402771, 'validation/loss': 0.05366867035627365, 'validation/mean_average_precision': 0.24943906443159541, 'validation/num_examples': 43793, 'test/accuracy': 0.9853415489196777, 'test/loss': 0.057474974542856216, 'test/mean_average_precision': 0.238388975929682, 'test/num_examples': 43793, 'score': 10814.71015739441, 'total_duration': 14544.220111846924, 'accumulated_submission_time': 10814.71015739441, 'accumulated_eval_time': 3727.150611639023, 'accumulated_logging_time': 1.0559494495391846}
I0307 20:27:19.624770 139517471074048 logging_writer.py:48] [50554] accumulated_eval_time=3727.15, accumulated_logging_time=1.05595, accumulated_submission_time=10814.7, global_step=50554, preemption_count=0, score=10814.7, test/accuracy=0.985342, test/loss=0.057475, test/mean_average_precision=0.238389, test/num_examples=43793, total_duration=14544.2, train/accuracy=0.995976, train/loss=0.0136184, train/mean_average_precision=0.746314, validation/accuracy=0.986259, validation/loss=0.0536687, validation/mean_average_precision=0.249439, validation/num_examples=43793
I0307 20:27:29.658305 139517462681344 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.21481548249721527, loss=0.01997370272874832
I0307 20:27:51.388526 139517471074048 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.19345657527446747, loss=0.01981649175286293
I0307 20:28:13.155901 139517462681344 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.18481343984603882, loss=0.020427223294973373
I0307 20:28:34.680578 139517471074048 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.17872679233551025, loss=0.020252365618944168
I0307 20:28:56.159645 139517462681344 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.18013182282447815, loss=0.02065150812268257
I0307 20:29:17.791772 139517471074048 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.23010170459747314, loss=0.02215433679521084
I0307 20:29:39.282464 139517462681344 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.22129769623279572, loss=0.021571027114987373
I0307 20:30:00.820576 139517471074048 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.19670528173446655, loss=0.020247207954525948
I0307 20:30:22.194888 139517462681344 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.18777604401111603, loss=0.01809529773890972
I0307 20:30:43.683339 139517471074048 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.20148730278015137, loss=0.01949632167816162
I0307 20:31:05.261090 139517462681344 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.20076122879981995, loss=0.01888473518192768
I0307 20:31:19.654425 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:32:33.843845 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:32:35.808968 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:32:37.709233 139659327427776 submission_runner.py:469] Time since start: 14862.32s, 	Step: 51668, 	{'train/accuracy': 0.996139645576477, 'train/loss': 0.013225937262177467, 'train/mean_average_precision': 0.7619706791230123, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053681742399930954, 'validation/mean_average_precision': 0.24968190509266971, 'validation/num_examples': 43793, 'test/accuracy': 0.9853874444961548, 'test/loss': 0.05751420557498932, 'test/mean_average_precision': 0.2392567066762917, 'test/num_examples': 43793, 'score': 11054.702872037888, 'total_duration': 14862.319735765457, 'accumulated_submission_time': 11054.702872037888, 'accumulated_eval_time': 3805.205352783203, 'accumulated_logging_time': 1.0803885459899902}
I0307 20:32:37.724462 139517471074048 logging_writer.py:48] [51668] accumulated_eval_time=3805.21, accumulated_logging_time=1.08039, accumulated_submission_time=11054.7, global_step=51668, preemption_count=0, score=11054.7, test/accuracy=0.985387, test/loss=0.0575142, test/mean_average_precision=0.239257, test/num_examples=43793, total_duration=14862.3, train/accuracy=0.99614, train/loss=0.0132259, train/mean_average_precision=0.761971, validation/accuracy=0.986314, validation/loss=0.0536817, validation/mean_average_precision=0.249682, validation/num_examples=43793
I0307 20:32:44.852596 139517462681344 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.22879481315612793, loss=0.021489225327968597
I0307 20:33:06.602807 139517471074048 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.18494635820388794, loss=0.018091285601258278
I0307 20:33:28.288296 139517462681344 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.18782305717468262, loss=0.01848604343831539
I0307 20:33:50.048799 139517471074048 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.21172989904880524, loss=0.02130974270403385
I0307 20:34:11.574131 139517462681344 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.21632032096385956, loss=0.022419221699237823
I0307 20:34:33.382538 139517471074048 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.21385321021080017, loss=0.02084529958665371
I0307 20:34:55.176990 139517462681344 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1934986561536789, loss=0.021110491827130318
I0307 20:35:16.742024 139517471074048 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1791302114725113, loss=0.01837695576250553
I0307 20:35:38.227170 139517462681344 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.20190706849098206, loss=0.021160289645195007
I0307 20:35:59.802469 139517471074048 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.17742721736431122, loss=0.019519245252013206
I0307 20:36:21.290285 139517462681344 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.18206050992012024, loss=0.01826970838010311
I0307 20:36:37.716195 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:37:51.980008 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:37:53.990070 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:37:55.927870 139659327427776 submission_runner.py:469] Time since start: 15180.54s, 	Step: 52777, 	{'train/accuracy': 0.9959949254989624, 'train/loss': 0.013461007736623287, 'train/mean_average_precision': 0.7558273580232853, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.05369365215301514, 'validation/mean_average_precision': 0.24974188079785217, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05752648413181305, 'test/mean_average_precision': 0.23936695339267547, 'test/num_examples': 43793, 'score': 11294.656462430954, 'total_duration': 15180.538317203522, 'accumulated_submission_time': 11294.656462430954, 'accumulated_eval_time': 3883.416901111603, 'accumulated_logging_time': 1.1056044101715088}
I0307 20:37:55.943613 139517471074048 logging_writer.py:48] [52777] accumulated_eval_time=3883.42, accumulated_logging_time=1.1056, accumulated_submission_time=11294.7, global_step=52777, preemption_count=0, score=11294.7, test/accuracy=0.985395, test/loss=0.0575265, test/mean_average_precision=0.239367, test/num_examples=43793, total_duration=15180.5, train/accuracy=0.995995, train/loss=0.013461, train/mean_average_precision=0.755827, validation/accuracy=0.98632, validation/loss=0.0536937, validation/mean_average_precision=0.249742, validation/num_examples=43793
I0307 20:38:01.169192 139517462681344 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.17856213450431824, loss=0.019394055008888245
I0307 20:38:23.010953 139517471074048 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.20783714950084686, loss=0.02026492729783058
I0307 20:38:44.747880 139517462681344 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.1723891645669937, loss=0.018792275339365005
I0307 20:39:06.369851 139517471074048 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.19181983172893524, loss=0.019563157111406326
I0307 20:39:28.045731 139517462681344 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.20613104104995728, loss=0.020292814821004868
I0307 20:39:49.739326 139517471074048 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.1970198005437851, loss=0.020028067752718925
I0307 20:40:11.288875 139517462681344 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.17742611467838287, loss=0.019194049760699272
I0307 20:40:32.889083 139517471074048 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.19781340658664703, loss=0.02079365774989128
I0307 20:40:54.505205 139517462681344 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.1887417435646057, loss=0.02061191201210022
I0307 20:41:16.040938 139517471074048 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.18053945899009705, loss=0.020040621981024742
I0307 20:41:37.810757 139517462681344 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1846015900373459, loss=0.019746599718928337
I0307 20:41:56.047577 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:43:11.687358 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:43:13.620876 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:43:15.553803 139659327427776 submission_runner.py:469] Time since start: 15500.16s, 	Step: 53885, 	{'train/accuracy': 0.9960029721260071, 'train/loss': 0.01352015696465969, 'train/mean_average_precision': 0.7529834796281675, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.05369365215301514, 'validation/mean_average_precision': 0.24986470278404002, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05752647668123245, 'test/mean_average_precision': 0.23950034746412585, 'test/num_examples': 43793, 'score': 11534.720926761627, 'total_duration': 15500.164242267609, 'accumulated_submission_time': 11534.720926761627, 'accumulated_eval_time': 3962.922998905182, 'accumulated_logging_time': 1.1303622722625732}
I0307 20:43:15.569443 139517471074048 logging_writer.py:48] [53885] accumulated_eval_time=3962.92, accumulated_logging_time=1.13036, accumulated_submission_time=11534.7, global_step=53885, preemption_count=0, score=11534.7, test/accuracy=0.985395, test/loss=0.0575265, test/mean_average_precision=0.2395, test/num_examples=43793, total_duration=15500.2, train/accuracy=0.996003, train/loss=0.0135202, train/mean_average_precision=0.752983, validation/accuracy=0.98632, validation/loss=0.0536937, validation/mean_average_precision=0.249865, validation/num_examples=43793
I0307 20:43:19.012747 139517462681344 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.22318512201309204, loss=0.02043556049466133
I0307 20:43:40.535925 139517471074048 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.21044567227363586, loss=0.021940141916275024
I0307 20:44:02.174176 139517462681344 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.23565274477005005, loss=0.02052423544228077
I0307 20:44:23.886916 139517471074048 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.19047747552394867, loss=0.019489284604787827
I0307 20:44:45.516832 139517462681344 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.20738284289836884, loss=0.021633557975292206
I0307 20:45:07.107699 139517471074048 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.1817474663257599, loss=0.02007363922894001
I0307 20:45:28.751998 139517462681344 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.19764438271522522, loss=0.018789537250995636
I0307 20:45:50.201904 139517471074048 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.21293498575687408, loss=0.021696580573916435
I0307 20:46:11.808375 139517462681344 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.19290541112422943, loss=0.019429493695497513
I0307 20:46:33.255524 139517471074048 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.22164879739284515, loss=0.021629417315125465
I0307 20:46:54.603281 139517462681344 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2101535201072693, loss=0.019821999594569206
I0307 20:47:15.610488 139659327427776 spec.py:321] Evaluating on the training split.
I0307 20:48:29.613092 139659327427776 spec.py:333] Evaluating on the validation split.
I0307 20:48:31.592605 139659327427776 spec.py:349] Evaluating on the test split.
I0307 20:48:33.529011 139659327427776 submission_runner.py:469] Time since start: 15818.14s, 	Step: 55000, 	{'train/accuracy': 0.9960130453109741, 'train/loss': 0.01342194527387619, 'train/mean_average_precision': 0.7463831152944623, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.05369365215301514, 'validation/mean_average_precision': 0.24973220394021475, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05752647668123245, 'test/mean_average_precision': 0.23935570564057673, 'test/num_examples': 43793, 'score': 11774.72370147705, 'total_duration': 15818.13951587677, 'accumulated_submission_time': 11774.72370147705, 'accumulated_eval_time': 4040.8414578437805, 'accumulated_logging_time': 1.1560711860656738}
I0307 20:48:33.544839 139517471074048 logging_writer.py:48] [55000] accumulated_eval_time=4040.84, accumulated_logging_time=1.15607, accumulated_submission_time=11774.7, global_step=55000, preemption_count=0, score=11774.7, test/accuracy=0.985395, test/loss=0.0575265, test/mean_average_precision=0.239356, test/num_examples=43793, total_duration=15818.1, train/accuracy=0.996013, train/loss=0.0134219, train/mean_average_precision=0.746383, validation/accuracy=0.98632, validation/loss=0.0536937, validation/mean_average_precision=0.249732, validation/num_examples=43793
I0307 20:48:33.775776 139517462681344 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.1880807727575302, loss=0.01983383484184742
I0307 20:48:55.345006 139517471074048 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.18898312747478485, loss=0.01872047781944275
I0307 20:49:16.849043 139517462681344 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.18802084028720856, loss=0.019675815477967262
I0307 20:49:38.466033 139517471074048 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.2162170708179474, loss=0.021457960829138756
I0307 20:49:59.771214 139517462681344 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.1986091583967209, loss=0.01881406456232071
I0307 20:50:21.368192 139517471074048 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2182340919971466, loss=0.020016299560666084
I0307 20:50:42.750268 139517462681344 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.20079444348812103, loss=0.02125582844018936
I0307 20:51:04.296304 139517471074048 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1720719039440155, loss=0.017804434522986412
I0307 20:51:25.928354 139517462681344 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.18833425641059875, loss=0.020297152921557426
I0307 20:51:47.440371 139517471074048 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2160770148038864, loss=0.02142391726374626
I0307 20:52:08.987488 139517462681344 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.24090175330638885, loss=0.021711116656661034
I0307 20:52:30.391210 139517471074048 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.22364813089370728, loss=0.020816633477807045
I0307 20:52:33.623009 139517462681344 logging_writer.py:48] [56116] global_step=56116, preemption_count=0, score=12014.8
I0307 20:52:33.782998 139659327427776 submission_runner.py:646] Tuning trial 1/5
I0307 20:52:33.783202 139659327427776 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 20:52:33.785690 139659327427776 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.48254281282424927, 'train/loss': 0.7525402903556824, 'train/mean_average_precision': 0.024656325580167525, 'validation/accuracy': 0.48500412702560425, 'validation/loss': 0.749235987663269, 'validation/mean_average_precision': 0.02684926652430975, 'validation/num_examples': 43793, 'test/accuracy': 0.4849407970905304, 'test/loss': 0.7482256889343262, 'test/mean_average_precision': 0.02805860381982226, 'test/num_examples': 43793, 'score': 11.804836511611938, 'total_duration': 222.5684370994568, 'accumulated_submission_time': 11.804836511611938, 'accumulated_eval_time': 210.76350450515747, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1117, {'train/accuracy': 0.9869800806045532, 'train/loss': 0.05193205550312996, 'train/mean_average_precision': 0.0589090869406095, 'validation/accuracy': 0.9843980073928833, 'validation/loss': 0.06138761714100838, 'validation/mean_average_precision': 0.05816697287384551, 'validation/num_examples': 43793, 'test/accuracy': 0.9834192395210266, 'test/loss': 0.06457491219043732, 'test/mean_average_precision': 0.05911169910342183, 'test/num_examples': 43793, 'score': 251.91265845298767, 'total_duration': 542.5642328262329, 'accumulated_submission_time': 251.91265845298767, 'accumulated_eval_time': 290.60338377952576, 'accumulated_logging_time': 0.017836570739746094, 'global_step': 1117, 'preemption_count': 0}), (2226, {'train/accuracy': 0.9877414703369141, 'train/loss': 0.044081032276153564, 'train/mean_average_precision': 0.14362420047860947, 'validation/accuracy': 0.9849979877471924, 'validation/loss': 0.05317061021924019, 'validation/mean_average_precision': 0.13290089444939004, 'validation/num_examples': 43793, 'test/accuracy': 0.9840046763420105, 'test/loss': 0.05624566972255707, 'test/mean_average_precision': 0.13035579983754264, 'test/num_examples': 43793, 'score': 491.98144578933716, 'total_duration': 861.2616300582886, 'accumulated_submission_time': 491.98144578933716, 'accumulated_eval_time': 369.1848440170288, 'accumulated_logging_time': 0.0359649658203125, 'global_step': 2226, 'preemption_count': 0}), (3335, {'train/accuracy': 0.9882873296737671, 'train/loss': 0.04083382710814476, 'train/mean_average_precision': 0.1849430265389228, 'validation/accuracy': 0.9853743314743042, 'validation/loss': 0.05053276941180229, 'validation/mean_average_precision': 0.16109067960310847, 'validation/num_examples': 43793, 'test/accuracy': 0.9844532608985901, 'test/loss': 0.05324564129114151, 'test/mean_average_precision': 0.1621799465838432, 'test/num_examples': 43793, 'score': 732.132963180542, 'total_duration': 1181.2754306793213, 'accumulated_submission_time': 732.132963180542, 'accumulated_eval_time': 448.99981331825256, 'accumulated_logging_time': 0.05407524108886719, 'global_step': 3335, 'preemption_count': 0}), (4454, {'train/accuracy': 0.9884161949157715, 'train/loss': 0.040067408233881, 'train/mean_average_precision': 0.21110601972482995, 'validation/accuracy': 0.9855407476425171, 'validation/loss': 0.04931637644767761, 'validation/mean_average_precision': 0.18409452381887398, 'validation/num_examples': 43793, 'test/accuracy': 0.9846756458282471, 'test/loss': 0.05195789784193039, 'test/mean_average_precision': 0.18511094464359082, 'test/num_examples': 43793, 'score': 972.2698929309845, 'total_duration': 1501.7163844108582, 'accumulated_submission_time': 972.2698929309845, 'accumulated_eval_time': 529.2544910907745, 'accumulated_logging_time': 0.07394766807556152, 'global_step': 4454, 'preemption_count': 0}), (5587, {'train/accuracy': 0.9885985255241394, 'train/loss': 0.03893957659602165, 'train/mean_average_precision': 0.23378070698116138, 'validation/accuracy': 0.9856057167053223, 'validation/loss': 0.04935465753078461, 'validation/mean_average_precision': 0.19008496069046132, 'validation/num_examples': 43793, 'test/accuracy': 0.9847164750099182, 'test/loss': 0.052222754806280136, 'test/mean_average_precision': 0.19325779178273966, 'test/num_examples': 43793, 'score': 1212.3269619941711, 'total_duration': 1820.1760606765747, 'accumulated_submission_time': 1212.3269619941711, 'accumulated_eval_time': 607.6043236255646, 'accumulated_logging_time': 0.09311032295227051, 'global_step': 5587, 'preemption_count': 0}), (6719, {'train/accuracy': 0.988862931728363, 'train/loss': 0.03792678192257881, 'train/mean_average_precision': 0.24610911905029084, 'validation/accuracy': 0.9859678149223328, 'validation/loss': 0.047702472656965256, 'validation/mean_average_precision': 0.20919302285404998, 'validation/num_examples': 43793, 'test/accuracy': 0.9850631356239319, 'test/loss': 0.05034596472978592, 'test/mean_average_precision': 0.21048278278916685, 'test/num_examples': 43793, 'score': 1452.4611961841583, 'total_duration': 2138.1511075496674, 'accumulated_submission_time': 1452.4611961841583, 'accumulated_eval_time': 685.4002315998077, 'accumulated_logging_time': 0.11127686500549316, 'global_step': 6719, 'preemption_count': 0}), (7862, {'train/accuracy': 0.9893061518669128, 'train/loss': 0.03617340326309204, 'train/mean_average_precision': 0.28092856857354503, 'validation/accuracy': 0.9862747192382812, 'validation/loss': 0.04634670913219452, 'validation/mean_average_precision': 0.2295277817267843, 'validation/num_examples': 43793, 'test/accuracy': 0.9853272438049316, 'test/loss': 0.04914823919534683, 'test/mean_average_precision': 0.22062230402788743, 'test/num_examples': 43793, 'score': 1692.6092796325684, 'total_duration': 2457.0944299697876, 'accumulated_submission_time': 1692.6092796325684, 'accumulated_eval_time': 764.1467208862305, 'accumulated_logging_time': 0.1309497356414795, 'global_step': 7862, 'preemption_count': 0}), (8997, {'train/accuracy': 0.9898136854171753, 'train/loss': 0.034795619547367096, 'train/mean_average_precision': 0.31071371611029064, 'validation/accuracy': 0.9863429069519043, 'validation/loss': 0.04595512896776199, 'validation/mean_average_precision': 0.23279257542281742, 'validation/num_examples': 43793, 'test/accuracy': 0.9855369925498962, 'test/loss': 0.0485963337123394, 'test/mean_average_precision': 0.22497764709640183, 'test/num_examples': 43793, 'score': 1932.6417162418365, 'total_duration': 2774.0133028030396, 'accumulated_submission_time': 1932.6417162418365, 'accumulated_eval_time': 840.9837901592255, 'accumulated_logging_time': 0.15025901794433594, 'global_step': 8997, 'preemption_count': 0}), (10134, {'train/accuracy': 0.989830493927002, 'train/loss': 0.03414575010538101, 'train/mean_average_precision': 0.3157925279331685, 'validation/accuracy': 0.9864922761917114, 'validation/loss': 0.04527546837925911, 'validation/mean_average_precision': 0.24112616647087454, 'validation/num_examples': 43793, 'test/accuracy': 0.9856991171836853, 'test/loss': 0.047831762582063675, 'test/mean_average_precision': 0.23618721068908818, 'test/num_examples': 43793, 'score': 2172.648035287857, 'total_duration': 3090.792464494705, 'accumulated_submission_time': 2172.648035287857, 'accumulated_eval_time': 917.7064299583435, 'accumulated_logging_time': 0.16934490203857422, 'global_step': 10134, 'preemption_count': 0}), (11272, {'train/accuracy': 0.9901042580604553, 'train/loss': 0.03306565806269646, 'train/mean_average_precision': 0.3439462078911937, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.045292507857084274, 'validation/mean_average_precision': 0.24053192754892036, 'validation/num_examples': 43793, 'test/accuracy': 0.9855281114578247, 'test/loss': 0.04801567643880844, 'test/mean_average_precision': 0.2323789082099413, 'test/num_examples': 43793, 'score': 2412.687742948532, 'total_duration': 3408.6963777542114, 'accumulated_submission_time': 2412.687742948532, 'accumulated_eval_time': 995.523065328598, 'accumulated_logging_time': 0.18798589706420898, 'global_step': 11272, 'preemption_count': 0}), (12411, {'train/accuracy': 0.9901197552680969, 'train/loss': 0.0328025259077549, 'train/mean_average_precision': 0.3511143515888282, 'validation/accuracy': 0.9867009520530701, 'validation/loss': 0.04495331272482872, 'validation/mean_average_precision': 0.2558747943500725, 'validation/num_examples': 43793, 'test/accuracy': 0.9858735203742981, 'test/loss': 0.04774082452058792, 'test/mean_average_precision': 0.24560777151153387, 'test/num_examples': 43793, 'score': 2652.730261325836, 'total_duration': 3725.874457836151, 'accumulated_submission_time': 2652.730261325836, 'accumulated_eval_time': 1072.6096420288086, 'accumulated_logging_time': 0.20878291130065918, 'global_step': 12411, 'preemption_count': 0}), (13548, {'train/accuracy': 0.9903571605682373, 'train/loss': 0.031637243926525116, 'train/mean_average_precision': 0.3866782287342651, 'validation/accuracy': 0.9867476224899292, 'validation/loss': 0.04467985779047012, 'validation/mean_average_precision': 0.2572974737341221, 'validation/num_examples': 43793, 'test/accuracy': 0.9857951998710632, 'test/loss': 0.04761962592601776, 'test/mean_average_precision': 0.24620493812266, 'test/num_examples': 43793, 'score': 2892.8241715431213, 'total_duration': 4044.611449956894, 'accumulated_submission_time': 2892.8241715431213, 'accumulated_eval_time': 1151.20370221138, 'accumulated_logging_time': 0.22852706909179688, 'global_step': 13548, 'preemption_count': 0}), (14683, {'train/accuracy': 0.990379273891449, 'train/loss': 0.03158879280090332, 'train/mean_average_precision': 0.37781407800689526, 'validation/accuracy': 0.9867516756057739, 'validation/loss': 0.04452819004654884, 'validation/mean_average_precision': 0.25664048218291374, 'validation/num_examples': 43793, 'test/accuracy': 0.9858528971672058, 'test/loss': 0.04751362279057503, 'test/mean_average_precision': 0.24661634877420657, 'test/num_examples': 43793, 'score': 3132.937895298004, 'total_duration': 4362.585327625275, 'accumulated_submission_time': 3132.937895298004, 'accumulated_eval_time': 1229.015308856964, 'accumulated_logging_time': 0.24912166595458984, 'global_step': 14683, 'preemption_count': 0}), (15820, {'train/accuracy': 0.9910165071487427, 'train/loss': 0.02982884831726551, 'train/mean_average_precision': 0.41986058026756756, 'validation/accuracy': 0.9867488145828247, 'validation/loss': 0.04419844597578049, 'validation/mean_average_precision': 0.26172257204319543, 'validation/num_examples': 43793, 'test/accuracy': 0.985920250415802, 'test/loss': 0.047168903052806854, 'test/mean_average_precision': 0.2473847450603414, 'test/num_examples': 43793, 'score': 3373.0578005313873, 'total_duration': 4678.907758235931, 'accumulated_submission_time': 3373.0578005313873, 'accumulated_eval_time': 1305.1622958183289, 'accumulated_logging_time': 0.27553749084472656, 'global_step': 15820, 'preemption_count': 0}), (16958, {'train/accuracy': 0.990696907043457, 'train/loss': 0.030731679871678352, 'train/mean_average_precision': 0.4010825200729872, 'validation/accuracy': 0.9867163896560669, 'validation/loss': 0.04472581669688225, 'validation/mean_average_precision': 0.2593270907578323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857121706008911, 'test/loss': 0.04772987961769104, 'test/mean_average_precision': 0.24526947222641993, 'test/num_examples': 43793, 'score': 3613.074487924576, 'total_duration': 4995.577400684357, 'accumulated_submission_time': 3613.074487924576, 'accumulated_eval_time': 1381.7629730701447, 'accumulated_logging_time': 0.29963135719299316, 'global_step': 16958, 'preemption_count': 0}), (18095, {'train/accuracy': 0.9913542866706848, 'train/loss': 0.028638113290071487, 'train/mean_average_precision': 0.45954159622254487, 'validation/accuracy': 0.9868413805961609, 'validation/loss': 0.04417752847075462, 'validation/mean_average_precision': 0.26452343290676844, 'validation/num_examples': 43793, 'test/accuracy': 0.986014187335968, 'test/loss': 0.04686443880200386, 'test/mean_average_precision': 0.2605700218770182, 'test/num_examples': 43793, 'score': 3853.121359348297, 'total_duration': 5312.948255300522, 'accumulated_submission_time': 3853.121359348297, 'accumulated_eval_time': 1459.0348358154297, 'accumulated_logging_time': 0.3217177391052246, 'global_step': 18095, 'preemption_count': 0}), (19227, {'train/accuracy': 0.9910323619842529, 'train/loss': 0.029413660988211632, 'train/mean_average_precision': 0.41896966487057685, 'validation/accuracy': 0.9868364930152893, 'validation/loss': 0.04398559406399727, 'validation/mean_average_precision': 0.2708583521190506, 'validation/num_examples': 43793, 'test/accuracy': 0.9859935641288757, 'test/loss': 0.04672582820057869, 'test/mean_average_precision': 0.26307612537913017, 'test/num_examples': 43793, 'score': 4093.1363949775696, 'total_duration': 5631.066077470779, 'accumulated_submission_time': 4093.1363949775696, 'accumulated_eval_time': 1537.0879225730896, 'accumulated_logging_time': 0.34250545501708984, 'global_step': 19227, 'preemption_count': 0}), (20350, {'train/accuracy': 0.9910579323768616, 'train/loss': 0.029218640178442, 'train/mean_average_precision': 0.440907992438448, 'validation/accuracy': 0.986901044845581, 'validation/loss': 0.044548410922288895, 'validation/mean_average_precision': 0.26575967266384554, 'validation/num_examples': 43793, 'test/accuracy': 0.9859480857849121, 'test/loss': 0.047507863491773605, 'test/mean_average_precision': 0.25504836256985625, 'test/num_examples': 43793, 'score': 4333.230062246323, 'total_duration': 5948.759950399399, 'accumulated_submission_time': 4333.230062246323, 'accumulated_eval_time': 1614.6390228271484, 'accumulated_logging_time': 0.3642137050628662, 'global_step': 20350, 'preemption_count': 0}), (21481, {'train/accuracy': 0.991349995136261, 'train/loss': 0.02845444716513157, 'train/mean_average_precision': 0.4583176141692592, 'validation/accuracy': 0.9868364930152893, 'validation/loss': 0.0442361980676651, 'validation/mean_average_precision': 0.2664014460740803, 'validation/num_examples': 43793, 'test/accuracy': 0.9859383702278137, 'test/loss': 0.04711903631687164, 'test/mean_average_precision': 0.2527427512009371, 'test/num_examples': 43793, 'score': 4573.3611352443695, 'total_duration': 6267.332014083862, 'accumulated_submission_time': 4573.3611352443695, 'accumulated_eval_time': 1693.0299170017242, 'accumulated_logging_time': 0.38426899909973145, 'global_step': 21481, 'preemption_count': 0}), (22612, {'train/accuracy': 0.9913470149040222, 'train/loss': 0.02843548357486725, 'train/mean_average_precision': 0.4482000966798827, 'validation/accuracy': 0.9869250059127808, 'validation/loss': 0.04414811730384827, 'validation/mean_average_precision': 0.2728610464994278, 'validation/num_examples': 43793, 'test/accuracy': 0.9859678745269775, 'test/loss': 0.04713272675871849, 'test/mean_average_precision': 0.25413614597329665, 'test/num_examples': 43793, 'score': 4813.378000736237, 'total_duration': 6583.438756942749, 'accumulated_submission_time': 4813.378000736237, 'accumulated_eval_time': 1769.070101261139, 'accumulated_logging_time': 0.4041714668273926, 'global_step': 22612, 'preemption_count': 0}), (23741, {'train/accuracy': 0.9915456175804138, 'train/loss': 0.027390239760279655, 'train/mean_average_precision': 0.47999502630236, 'validation/accuracy': 0.9867910742759705, 'validation/loss': 0.04482853040099144, 'validation/mean_average_precision': 0.2655767803293215, 'validation/num_examples': 43793, 'test/accuracy': 0.9857779145240784, 'test/loss': 0.0479108989238739, 'test/mean_average_precision': 0.24697440228104617, 'test/num_examples': 43793, 'score': 5053.436358690262, 'total_duration': 6901.231354236603, 'accumulated_submission_time': 5053.436358690262, 'accumulated_eval_time': 1846.7526998519897, 'accumulated_logging_time': 0.42779111862182617, 'global_step': 23741, 'preemption_count': 0}), (24867, {'train/accuracy': 0.9913817644119263, 'train/loss': 0.028227442875504494, 'train/mean_average_precision': 0.45525468313761785, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04429924488067627, 'validation/mean_average_precision': 0.27070838177985534, 'validation/num_examples': 43793, 'test/accuracy': 0.9860563278198242, 'test/loss': 0.047134868800640106, 'test/mean_average_precision': 0.2606326773671783, 'test/num_examples': 43793, 'score': 5293.4833998680115, 'total_duration': 7220.089718103409, 'accumulated_submission_time': 5293.4833998680115, 'accumulated_eval_time': 1925.5133485794067, 'accumulated_logging_time': 0.44914865493774414, 'global_step': 24867, 'preemption_count': 0}), (25991, {'train/accuracy': 0.9920634031295776, 'train/loss': 0.025894856080412865, 'train/mean_average_precision': 0.506709979263162, 'validation/accuracy': 0.9868572354316711, 'validation/loss': 0.04447468742728233, 'validation/mean_average_precision': 0.26789646633195857, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.047472622245550156, 'test/mean_average_precision': 0.2560846180210622, 'test/num_examples': 43793, 'score': 5533.565752744675, 'total_duration': 7538.145130634308, 'accumulated_submission_time': 5533.565752744675, 'accumulated_eval_time': 2003.4340472221375, 'accumulated_logging_time': 0.4715595245361328, 'global_step': 25991, 'preemption_count': 0}), (27111, {'train/accuracy': 0.9916271567344666, 'train/loss': 0.026868902146816254, 'train/mean_average_precision': 0.4790074681747368, 'validation/accuracy': 0.9869075417518616, 'validation/loss': 0.04508747532963753, 'validation/mean_average_precision': 0.2726442952403928, 'validation/num_examples': 43793, 'test/accuracy': 0.9860550761222839, 'test/loss': 0.048241425305604935, 'test/mean_average_precision': 0.25946120585306226, 'test/num_examples': 43793, 'score': 5773.625909566879, 'total_duration': 7856.656160593033, 'accumulated_submission_time': 5773.625909566879, 'accumulated_eval_time': 2081.835661172867, 'accumulated_logging_time': 0.49282169342041016, 'global_step': 27111, 'preemption_count': 0}), (28228, {'train/accuracy': 0.9924265742301941, 'train/loss': 0.024819646030664444, 'train/mean_average_precision': 0.53403962775716, 'validation/accuracy': 0.9869489669799805, 'validation/loss': 0.04451194405555725, 'validation/mean_average_precision': 0.27481056311260504, 'validation/num_examples': 43793, 'test/accuracy': 0.9859973192214966, 'test/loss': 0.047571323812007904, 'test/mean_average_precision': 0.2589014093001228, 'test/num_examples': 43793, 'score': 6013.698362588882, 'total_duration': 8176.153395414352, 'accumulated_submission_time': 6013.698362588882, 'accumulated_eval_time': 2161.2076761722565, 'accumulated_logging_time': 0.5143706798553467, 'global_step': 28228, 'preemption_count': 0}), (29347, {'train/accuracy': 0.9921352863311768, 'train/loss': 0.025597229599952698, 'train/mean_average_precision': 0.5200810427492805, 'validation/accuracy': 0.986847460269928, 'validation/loss': 0.04491814225912094, 'validation/mean_average_precision': 0.2664187236608896, 'validation/num_examples': 43793, 'test/accuracy': 0.985929548740387, 'test/loss': 0.0479218065738678, 'test/mean_average_precision': 0.2528803977516929, 'test/num_examples': 43793, 'score': 6253.750548362732, 'total_duration': 8494.56211233139, 'accumulated_submission_time': 6253.750548362732, 'accumulated_eval_time': 2239.5133967399597, 'accumulated_logging_time': 0.5379643440246582, 'global_step': 29347, 'preemption_count': 0}), (30468, {'train/accuracy': 0.9923573732376099, 'train/loss': 0.025053629651665688, 'train/mean_average_precision': 0.5168561382590843, 'validation/accuracy': 0.9869363903999329, 'validation/loss': 0.0449998676776886, 'validation/mean_average_precision': 0.27093491677922665, 'validation/num_examples': 43793, 'test/accuracy': 0.9860504269599915, 'test/loss': 0.048078544437885284, 'test/mean_average_precision': 0.2592491103125053, 'test/num_examples': 43793, 'score': 6493.779803276062, 'total_duration': 8809.95774102211, 'accumulated_submission_time': 6493.779803276062, 'accumulated_eval_time': 2314.8305563926697, 'accumulated_logging_time': 0.5593171119689941, 'global_step': 30468, 'preemption_count': 0}), (31587, {'train/accuracy': 0.9926300644874573, 'train/loss': 0.02374393120408058, 'train/mean_average_precision': 0.5493526286592454, 'validation/accuracy': 0.9870033860206604, 'validation/loss': 0.045271873474121094, 'validation/mean_average_precision': 0.2751791614452758, 'validation/num_examples': 43793, 'test/accuracy': 0.9860584139823914, 'test/loss': 0.04844682291150093, 'test/mean_average_precision': 0.2597082628577487, 'test/num_examples': 43793, 'score': 6733.836859226227, 'total_duration': 9129.265543699265, 'accumulated_submission_time': 6733.836859226227, 'accumulated_eval_time': 2394.029878616333, 'accumulated_logging_time': 0.5814833641052246, 'global_step': 31587, 'preemption_count': 0}), (32711, {'train/accuracy': 0.9925702214241028, 'train/loss': 0.023990873247385025, 'train/mean_average_precision': 0.545553500948556, 'validation/accuracy': 0.9869193434715271, 'validation/loss': 0.0455615371465683, 'validation/mean_average_precision': 0.27216154715818497, 'validation/num_examples': 43793, 'test/accuracy': 0.9860382080078125, 'test/loss': 0.048560917377471924, 'test/mean_average_precision': 0.2574208522597094, 'test/num_examples': 43793, 'score': 6973.859038114548, 'total_duration': 9443.520685434341, 'accumulated_submission_time': 6973.859038114548, 'accumulated_eval_time': 2468.2128133773804, 'accumulated_logging_time': 0.6035270690917969, 'global_step': 32711, 'preemption_count': 0}), (33789, {'train/accuracy': 0.9934369325637817, 'train/loss': 0.021386293694376945, 'train/mean_average_precision': 0.6039116568987395, 'validation/accuracy': 0.9869063496589661, 'validation/loss': 0.04584026709198952, 'validation/mean_average_precision': 0.27104542879251936, 'validation/num_examples': 43793, 'test/accuracy': 0.9859910011291504, 'test/loss': 0.048934489488601685, 'test/mean_average_precision': 0.2596579320773102, 'test/num_examples': 43793, 'score': 7213.986223697662, 'total_duration': 9763.120081424713, 'accumulated_submission_time': 7213.986223697662, 'accumulated_eval_time': 2547.632602930069, 'accumulated_logging_time': 0.6295428276062012, 'global_step': 33789, 'preemption_count': 0}), (34913, {'train/accuracy': 0.9927451610565186, 'train/loss': 0.023204749450087547, 'train/mean_average_precision': 0.5658095586527297, 'validation/accuracy': 0.9868612885475159, 'validation/loss': 0.04628916457295418, 'validation/mean_average_precision': 0.2696627607899645, 'validation/num_examples': 43793, 'test/accuracy': 0.985939621925354, 'test/loss': 0.049364764243364334, 'test/mean_average_precision': 0.2539269310041063, 'test/num_examples': 43793, 'score': 7454.029727220535, 'total_duration': 10079.66783618927, 'accumulated_submission_time': 7454.029727220535, 'accumulated_eval_time': 2624.0861914157867, 'accumulated_logging_time': 0.6527044773101807, 'global_step': 34913, 'preemption_count': 0}), (36036, {'train/accuracy': 0.993628203868866, 'train/loss': 0.020763562992215157, 'train/mean_average_precision': 0.6191475711928272, 'validation/accuracy': 0.9868194460868835, 'validation/loss': 0.046570561826229095, 'validation/mean_average_precision': 0.2700427519962571, 'validation/num_examples': 43793, 'test/accuracy': 0.9858313798904419, 'test/loss': 0.0498359315097332, 'test/mean_average_precision': 0.25920728306436336, 'test/num_examples': 43793, 'score': 7694.060019254684, 'total_duration': 10398.837435245514, 'accumulated_submission_time': 7694.060019254684, 'accumulated_eval_time': 2703.173148870468, 'accumulated_logging_time': 0.6759274005889893, 'global_step': 36036, 'preemption_count': 0}), (37154, {'train/accuracy': 0.993422269821167, 'train/loss': 0.02095273695886135, 'train/mean_average_precision': 0.598515751992627, 'validation/accuracy': 0.9868576526641846, 'validation/loss': 0.04717548191547394, 'validation/mean_average_precision': 0.27039233755447034, 'validation/num_examples': 43793, 'test/accuracy': 0.9859105944633484, 'test/loss': 0.0504155308008194, 'test/mean_average_precision': 0.2550039953521819, 'test/num_examples': 43793, 'score': 7934.0755879879, 'total_duration': 10716.692224264145, 'accumulated_submission_time': 7934.0755879879, 'accumulated_eval_time': 2780.9604473114014, 'accumulated_logging_time': 0.6981160640716553, 'global_step': 37154, 'preemption_count': 0}), (38273, {'train/accuracy': 0.993793249130249, 'train/loss': 0.019994808360934258, 'train/mean_average_precision': 0.6234364863375629, 'validation/accuracy': 0.9866920113563538, 'validation/loss': 0.04793078452348709, 'validation/mean_average_precision': 0.2625957496125843, 'validation/num_examples': 43793, 'test/accuracy': 0.9859004616737366, 'test/loss': 0.051143474876880646, 'test/mean_average_precision': 0.25116897686096334, 'test/num_examples': 43793, 'score': 8174.071074962616, 'total_duration': 11037.229346990585, 'accumulated_submission_time': 8174.071074962616, 'accumulated_eval_time': 2861.4471204280853, 'accumulated_logging_time': 0.7253048419952393, 'global_step': 38273, 'preemption_count': 0}), (39390, {'train/accuracy': 0.9942801594734192, 'train/loss': 0.01863309182226658, 'train/mean_average_precision': 0.6550697077654688, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.048125702887773514, 'validation/mean_average_precision': 0.2655068313219917, 'validation/num_examples': 43793, 'test/accuracy': 0.9857551455497742, 'test/loss': 0.051581624895334244, 'test/mean_average_precision': 0.2515382030447509, 'test/num_examples': 43793, 'score': 8414.090274810791, 'total_duration': 11357.180511713028, 'accumulated_submission_time': 8414.090274810791, 'accumulated_eval_time': 2941.3255622386932, 'accumulated_logging_time': 0.7496404647827148, 'global_step': 39390, 'preemption_count': 0}), (40509, {'train/accuracy': 0.9939073324203491, 'train/loss': 0.01933678798377514, 'train/mean_average_precision': 0.64784043836471, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.04882986098527908, 'validation/mean_average_precision': 0.26305309566465684, 'validation/num_examples': 43793, 'test/accuracy': 0.9857977032661438, 'test/loss': 0.05236607417464256, 'test/mean_average_precision': 0.24949630422558006, 'test/num_examples': 43793, 'score': 8654.076290130615, 'total_duration': 11680.204643249512, 'accumulated_submission_time': 8654.076290130615, 'accumulated_eval_time': 3024.312996149063, 'accumulated_logging_time': 0.773061990737915, 'global_step': 40509, 'preemption_count': 0}), (41630, {'train/accuracy': 0.994847297668457, 'train/loss': 0.016862019896507263, 'train/mean_average_precision': 0.694057481408302, 'validation/accuracy': 0.9865572452545166, 'validation/loss': 0.04959559813141823, 'validation/mean_average_precision': 0.25781379753699946, 'validation/num_examples': 43793, 'test/accuracy': 0.985562264919281, 'test/loss': 0.05327992141246796, 'test/mean_average_precision': 0.24537853381887362, 'test/num_examples': 43793, 'score': 8894.088335990906, 'total_duration': 11997.745305538177, 'accumulated_submission_time': 8894.088335990906, 'accumulated_eval_time': 3101.7880811691284, 'accumulated_logging_time': 0.7954974174499512, 'global_step': 41630, 'preemption_count': 0}), (42736, {'train/accuracy': 0.9941988587379456, 'train/loss': 0.018283061683177948, 'train/mean_average_precision': 0.6564854197468704, 'validation/accuracy': 0.9865552186965942, 'validation/loss': 0.05036221072077751, 'validation/mean_average_precision': 0.2599248903636448, 'validation/num_examples': 43793, 'test/accuracy': 0.9856275320053101, 'test/loss': 0.05392998084425926, 'test/mean_average_precision': 0.24534255280443062, 'test/num_examples': 43793, 'score': 9134.207527399063, 'total_duration': 12318.852712869644, 'accumulated_submission_time': 9134.207527399063, 'accumulated_eval_time': 3182.724787712097, 'accumulated_logging_time': 0.8188595771789551, 'global_step': 42736, 'preemption_count': 0}), (43855, {'train/accuracy': 0.9953716397285461, 'train/loss': 0.015282107517123222, 'train/mean_average_precision': 0.7246460761293376, 'validation/accuracy': 0.9865056872367859, 'validation/loss': 0.05144544690847397, 'validation/mean_average_precision': 0.2547158524794639, 'validation/num_examples': 43793, 'test/accuracy': 0.9855795502662659, 'test/loss': 0.05520850419998169, 'test/mean_average_precision': 0.24428296618916157, 'test/num_examples': 43793, 'score': 9374.253158330917, 'total_duration': 12635.567828655243, 'accumulated_submission_time': 9374.253158330917, 'accumulated_eval_time': 3259.3139259815216, 'accumulated_logging_time': 0.8692541122436523, 'global_step': 43855, 'preemption_count': 0}), (44977, {'train/accuracy': 0.9948879480361938, 'train/loss': 0.01627352088689804, 'train/mean_average_precision': 0.6932686653719271, 'validation/accuracy': 0.986436665058136, 'validation/loss': 0.052207138389348984, 'validation/mean_average_precision': 0.25352518680269764, 'validation/num_examples': 43793, 'test/accuracy': 0.9855159521102905, 'test/loss': 0.05578320100903511, 'test/mean_average_precision': 0.24101867055998927, 'test/num_examples': 43793, 'score': 9614.322991847992, 'total_duration': 12953.340996742249, 'accumulated_submission_time': 9614.322991847992, 'accumulated_eval_time': 3336.963909626007, 'accumulated_logging_time': 0.8951094150543213, 'global_step': 44977, 'preemption_count': 0}), (46095, {'train/accuracy': 0.9952924847602844, 'train/loss': 0.015211712568998337, 'train/mean_average_precision': 0.7254823150809586, 'validation/accuracy': 0.9863481521606445, 'validation/loss': 0.05264874920248985, 'validation/mean_average_precision': 0.2506719634003946, 'validation/num_examples': 43793, 'test/accuracy': 0.9854135513305664, 'test/loss': 0.05640161409974098, 'test/mean_average_precision': 0.23969290206991867, 'test/num_examples': 43793, 'score': 9854.41327881813, 'total_duration': 13272.259805679321, 'accumulated_submission_time': 9854.41327881813, 'accumulated_eval_time': 3415.7390475273132, 'accumulated_logging_time': 0.9191961288452148, 'global_step': 46095, 'preemption_count': 0}), (47213, {'train/accuracy': 0.9951796531677246, 'train/loss': 0.015382890589535236, 'train/mean_average_precision': 0.713931345592469, 'validation/accuracy': 0.9863579273223877, 'validation/loss': 0.05329151824116707, 'validation/mean_average_precision': 0.24951376141884868, 'validation/num_examples': 43793, 'test/accuracy': 0.9854022264480591, 'test/loss': 0.057217709720134735, 'test/mean_average_precision': 0.23853706032703315, 'test/num_examples': 43793, 'score': 10094.540465831757, 'total_duration': 13589.325511932373, 'accumulated_submission_time': 10094.540465831757, 'accumulated_eval_time': 3492.624537229538, 'accumulated_logging_time': 0.9445290565490723, 'global_step': 47213, 'preemption_count': 0}), (48329, {'train/accuracy': 0.9950525760650635, 'train/loss': 0.015712063759565353, 'train/mean_average_precision': 0.7076125233639754, 'validation/accuracy': 0.9862523674964905, 'validation/loss': 0.05357007682323456, 'validation/mean_average_precision': 0.24951509579469222, 'validation/num_examples': 43793, 'test/accuracy': 0.9853423833847046, 'test/loss': 0.057396870106458664, 'test/mean_average_precision': 0.23706950968264806, 'test/num_examples': 43793, 'score': 10334.556473255157, 'total_duration': 13908.63451051712, 'accumulated_submission_time': 10334.556473255157, 'accumulated_eval_time': 3571.8606209754944, 'accumulated_logging_time': 0.9702322483062744, 'global_step': 48329, 'preemption_count': 0}), (49443, {'train/accuracy': 0.9962520599365234, 'train/loss': 0.013007866218686104, 'train/mean_average_precision': 0.7611827810690932, 'validation/accuracy': 0.9862824082374573, 'validation/loss': 0.053753141313791275, 'validation/mean_average_precision': 0.25013255869141543, 'validation/num_examples': 43793, 'test/accuracy': 0.9853731393814087, 'test/loss': 0.057642221450805664, 'test/mean_average_precision': 0.23881116307435937, 'test/num_examples': 43793, 'score': 10574.680939912796, 'total_duration': 14226.432375192642, 'accumulated_submission_time': 10574.680939912796, 'accumulated_eval_time': 3649.4477050304413, 'accumulated_logging_time': 1.0275158882141113, 'global_step': 49443, 'preemption_count': 0}), (50554, {'train/accuracy': 0.9959763288497925, 'train/loss': 0.013618393801152706, 'train/mean_average_precision': 0.7463143858031025, 'validation/accuracy': 0.986258864402771, 'validation/loss': 0.05366867035627365, 'validation/mean_average_precision': 0.24943906443159541, 'validation/num_examples': 43793, 'test/accuracy': 0.9853415489196777, 'test/loss': 0.057474974542856216, 'test/mean_average_precision': 0.238388975929682, 'test/num_examples': 43793, 'score': 10814.71015739441, 'total_duration': 14544.220111846924, 'accumulated_submission_time': 10814.71015739441, 'accumulated_eval_time': 3727.150611639023, 'accumulated_logging_time': 1.0559494495391846, 'global_step': 50554, 'preemption_count': 0}), (51668, {'train/accuracy': 0.996139645576477, 'train/loss': 0.013225937262177467, 'train/mean_average_precision': 0.7619706791230123, 'validation/accuracy': 0.9863136410713196, 'validation/loss': 0.053681742399930954, 'validation/mean_average_precision': 0.24968190509266971, 'validation/num_examples': 43793, 'test/accuracy': 0.9853874444961548, 'test/loss': 0.05751420557498932, 'test/mean_average_precision': 0.2392567066762917, 'test/num_examples': 43793, 'score': 11054.702872037888, 'total_duration': 14862.319735765457, 'accumulated_submission_time': 11054.702872037888, 'accumulated_eval_time': 3805.205352783203, 'accumulated_logging_time': 1.0803885459899902, 'global_step': 51668, 'preemption_count': 0}), (52777, {'train/accuracy': 0.9959949254989624, 'train/loss': 0.013461007736623287, 'train/mean_average_precision': 0.7558273580232853, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.05369365215301514, 'validation/mean_average_precision': 0.24974188079785217, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05752648413181305, 'test/mean_average_precision': 0.23936695339267547, 'test/num_examples': 43793, 'score': 11294.656462430954, 'total_duration': 15180.538317203522, 'accumulated_submission_time': 11294.656462430954, 'accumulated_eval_time': 3883.416901111603, 'accumulated_logging_time': 1.1056044101715088, 'global_step': 52777, 'preemption_count': 0}), (53885, {'train/accuracy': 0.9960029721260071, 'train/loss': 0.01352015696465969, 'train/mean_average_precision': 0.7529834796281675, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.05369365215301514, 'validation/mean_average_precision': 0.24986470278404002, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05752647668123245, 'test/mean_average_precision': 0.23950034746412585, 'test/num_examples': 43793, 'score': 11534.720926761627, 'total_duration': 15500.164242267609, 'accumulated_submission_time': 11534.720926761627, 'accumulated_eval_time': 3962.922998905182, 'accumulated_logging_time': 1.1303622722625732, 'global_step': 53885, 'preemption_count': 0}), (55000, {'train/accuracy': 0.9960130453109741, 'train/loss': 0.01342194527387619, 'train/mean_average_precision': 0.7463831152944623, 'validation/accuracy': 0.9863201379776001, 'validation/loss': 0.05369365215301514, 'validation/mean_average_precision': 0.24973220394021475, 'validation/num_examples': 43793, 'test/accuracy': 0.9853950142860413, 'test/loss': 0.05752647668123245, 'test/mean_average_precision': 0.23935570564057673, 'test/num_examples': 43793, 'score': 11774.72370147705, 'total_duration': 15818.13951587677, 'accumulated_submission_time': 11774.72370147705, 'accumulated_eval_time': 4040.8414578437805, 'accumulated_logging_time': 1.1560711860656738, 'global_step': 55000, 'preemption_count': 0})], 'global_step': 56116}
I0307 20:52:33.785814 139659327427776 submission_runner.py:649] Timing: 12014.752438783646
I0307 20:52:33.785856 139659327427776 submission_runner.py:651] Total number of evals: 50
I0307 20:52:33.785889 139659327427776 submission_runner.py:652] ====================
I0307 20:52:33.786322 139659327427776 submission_runner.py:750] Final ogbg score: 0
