python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=429289231 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-02-36-27.log
2025-03-07 02:36:44.993646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741315005.576714       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741315005.717331       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 02:37:35.543675 140252175811776 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax.
I0307 02:37:38.283214 140252175811776 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 02:37:38.286489 140252175811776 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 02:37:38.315533 140252175811776 submission_runner.py:606] Using RNG seed 429289231
I0307 02:37:44.067896 140252175811776 submission_runner.py:615] --- Tuning run 3/5 ---
I0307 02:37:44.068096 140252175811776 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_3.
I0307 02:37:44.068295 140252175811776 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_3/hparams.json.
I0307 02:37:44.308913 140252175811776 submission_runner.py:218] Initializing dataset.
I0307 02:37:46.042687 140252175811776 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:37:46.438829 140252175811776 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:37:46.752945 140252175811776 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:37:48.428675 140252175811776 submission_runner.py:229] Initializing model.
I0307 02:38:12.327283 140252175811776 submission_runner.py:272] Initializing optimizer.
I0307 02:38:13.452609 140252175811776 submission_runner.py:279] Initializing metrics bundle.
I0307 02:38:13.452889 140252175811776 submission_runner.py:301] Initializing checkpoint and logger.
I0307 02:38:13.454013 140252175811776 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0307 02:38:13.454121 140252175811776 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_3/meta_data_0.json.
I0307 02:38:14.042818 140252175811776 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_3/flags_0.json.
I0307 02:38:14.369220 140252175811776 submission_runner.py:337] Starting training loop.
I0307 02:39:13.578919 140115747211008 logging_writer.py:48] [0] global_step=0, grad_norm=0.6888806223869324, loss=6.936972618103027
I0307 02:39:13.901072 140252175811776 spec.py:321] Evaluating on the training split.
I0307 02:39:14.391794 140252175811776 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:39:14.418213 140252175811776 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:39:14.463794 140252175811776 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:39:33.808855 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 02:39:34.365569 140252175811776 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:39:34.374011 140252175811776 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:39:34.412467 140252175811776 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:40:26.780493 140252175811776 spec.py:349] Evaluating on the test split.
I0307 02:40:27.299587 140252175811776 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:40:27.360012 140252175811776 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 02:40:27.407733 140252175811776 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:40:42.235342 140252175811776 submission_runner.py:469] Time since start: 147.87s, 	Step: 1, 	{'train/accuracy': 0.0015744578558951616, 'train/loss': 6.911673069000244, 'validation/accuracy': 0.0011399999493733048, 'validation/loss': 6.9118852615356445, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.912051677703857, 'test/num_examples': 10000, 'score': 59.53156042098999, 'total_duration': 147.86599707603455, 'accumulated_submission_time': 59.53156042098999, 'accumulated_eval_time': 88.33413577079773, 'accumulated_logging_time': 0}
I0307 02:40:42.318538 140096086976256 logging_writer.py:48] [1] accumulated_eval_time=88.3341, accumulated_logging_time=0, accumulated_submission_time=59.5316, global_step=1, preemption_count=0, score=59.5316, test/accuracy=0.0011, test/loss=6.91205, test/num_examples=10000, total_duration=147.866, train/accuracy=0.00157446, train/loss=6.91167, validation/accuracy=0.00114, validation/loss=6.91189, validation/num_examples=50000
I0307 02:41:19.886542 140096078583552 logging_writer.py:48] [100] global_step=100, grad_norm=0.6804521679878235, loss=6.9112420082092285
I0307 02:41:56.082665 140096086976256 logging_writer.py:48] [200] global_step=200, grad_norm=0.6753303408622742, loss=6.86728572845459
I0307 02:42:32.481741 140096078583552 logging_writer.py:48] [300] global_step=300, grad_norm=0.7164998054504395, loss=6.788196086883545
I0307 02:43:09.381455 140096086976256 logging_writer.py:48] [400] global_step=400, grad_norm=0.7307186722755432, loss=6.697402477264404
I0307 02:43:45.922068 140096078583552 logging_writer.py:48] [500] global_step=500, grad_norm=0.7884847521781921, loss=6.575037956237793
I0307 02:44:24.017811 140096086976256 logging_writer.py:48] [600] global_step=600, grad_norm=0.8194576501846313, loss=6.45000696182251
I0307 02:45:01.829916 140096078583552 logging_writer.py:48] [700] global_step=700, grad_norm=0.8477721810340881, loss=6.312185764312744
I0307 02:45:39.954532 140096086976256 logging_writer.py:48] [800] global_step=800, grad_norm=0.993175745010376, loss=6.213835716247559
I0307 02:46:17.943278 140096078583552 logging_writer.py:48] [900] global_step=900, grad_norm=1.0366442203521729, loss=6.149362564086914
I0307 02:46:56.056023 140096086976256 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3811835050582886, loss=6.038214206695557
I0307 02:47:33.593333 140096078583552 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.1074695587158203, loss=5.864569187164307
I0307 02:48:11.608340 140096086976256 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.56776762008667, loss=5.812225341796875
I0307 02:48:48.883054 140096078583552 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8797059059143066, loss=5.79641580581665
I0307 02:49:12.445307 140252175811776 spec.py:321] Evaluating on the training split.
I0307 02:49:25.579154 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 02:50:21.294665 140252175811776 spec.py:349] Evaluating on the test split.
I0307 02:50:23.304953 140252175811776 submission_runner.py:469] Time since start: 728.94s, 	Step: 1365, 	{'train/accuracy': 0.06050701439380646, 'train/loss': 5.488076210021973, 'validation/accuracy': 0.05341999977827072, 'validation/loss': 5.582878589630127, 'validation/num_examples': 50000, 'test/accuracy': 0.03830000013113022, 'test/loss': 5.781375885009766, 'test/num_examples': 10000, 'score': 569.453928232193, 'total_duration': 728.9356865882874, 'accumulated_submission_time': 569.453928232193, 'accumulated_eval_time': 159.19375038146973, 'accumulated_logging_time': 0.1060185432434082}
I0307 02:50:23.331506 140096095368960 logging_writer.py:48] [1365] accumulated_eval_time=159.194, accumulated_logging_time=0.106019, accumulated_submission_time=569.454, global_step=1365, preemption_count=0, score=569.454, test/accuracy=0.0383, test/loss=5.78138, test/num_examples=10000, total_duration=728.936, train/accuracy=0.060507, train/loss=5.48808, validation/accuracy=0.05342, validation/loss=5.58288, validation/num_examples=50000
I0307 02:50:36.795136 140096103761664 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.4177021980285645, loss=5.73396110534668
I0307 02:51:14.650665 140096095368960 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.467766523361206, loss=5.577317714691162
I0307 02:51:51.937454 140096103761664 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.830778121948242, loss=5.438936710357666
I0307 02:52:29.877790 140096095368960 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.8917531967163086, loss=5.461604118347168
I0307 02:53:07.831009 140096103761664 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.7948577404022217, loss=5.357250213623047
I0307 02:53:46.510279 140096095368960 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.307218551635742, loss=5.318821430206299
I0307 02:54:24.454313 140096103761664 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.281613349914551, loss=5.176366806030273
I0307 02:55:02.408059 140096095368960 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.931896209716797, loss=5.165935516357422
I0307 02:55:40.186317 140096103761664 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.979731559753418, loss=5.012087821960449
I0307 02:56:18.670039 140096095368960 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.649295330047607, loss=5.07350492477417
I0307 02:56:56.594218 140096103761664 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.532095193862915, loss=4.994195938110352
I0307 02:57:34.267259 140096095368960 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.885464668273926, loss=4.83561372756958
I0307 02:58:11.527941 140096103761664 logging_writer.py:48] [2600] global_step=2600, grad_norm=5.713024139404297, loss=4.981298923492432
I0307 02:58:50.061275 140096095368960 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.8129496574401855, loss=4.751030445098877
I0307 02:58:53.474442 140252175811776 spec.py:321] Evaluating on the training split.
I0307 02:59:04.929061 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 02:59:23.477099 140252175811776 spec.py:349] Evaluating on the test split.
I0307 02:59:25.342308 140252175811776 submission_runner.py:469] Time since start: 1270.97s, 	Step: 2710, 	{'train/accuracy': 0.15455596148967743, 'train/loss': 4.43271541595459, 'validation/accuracy': 0.13439999520778656, 'validation/loss': 4.582358360290527, 'validation/num_examples': 50000, 'test/accuracy': 0.09490000456571579, 'test/loss': 5.032994270324707, 'test/num_examples': 10000, 'score': 1079.4112739562988, 'total_duration': 1270.9730455875397, 'accumulated_submission_time': 1079.4112739562988, 'accumulated_eval_time': 191.06157279014587, 'accumulated_logging_time': 0.14062929153442383}
I0307 02:59:25.382447 140096103761664 logging_writer.py:48] [2710] accumulated_eval_time=191.062, accumulated_logging_time=0.140629, accumulated_submission_time=1079.41, global_step=2710, preemption_count=0, score=1079.41, test/accuracy=0.0949, test/loss=5.03299, test/num_examples=10000, total_duration=1270.97, train/accuracy=0.154556, train/loss=4.43272, validation/accuracy=0.1344, validation/loss=4.58236, validation/num_examples=50000
I0307 03:00:00.367531 140096095368960 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.533483982086182, loss=4.748824119567871
I0307 03:00:39.138900 140096103761664 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.9239373207092285, loss=4.752378940582275
I0307 03:01:17.965986 140096095368960 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.257030487060547, loss=4.601910591125488
I0307 03:01:55.808934 140096103761664 logging_writer.py:48] [3100] global_step=3100, grad_norm=7.278168201446533, loss=4.5979156494140625
I0307 03:02:34.224594 140096095368960 logging_writer.py:48] [3200] global_step=3200, grad_norm=5.462571144104004, loss=4.476624488830566
I0307 03:03:12.653931 140096103761664 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.86562442779541, loss=4.424355506896973
I0307 03:03:51.100232 140096095368960 logging_writer.py:48] [3400] global_step=3400, grad_norm=6.515964031219482, loss=4.358732223510742
I0307 03:04:29.264923 140096103761664 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.228334426879883, loss=4.42160701751709
I0307 03:05:07.554781 140096095368960 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.7192111015319824, loss=4.307160377502441
I0307 03:05:46.064568 140096103761664 logging_writer.py:48] [3700] global_step=3700, grad_norm=6.406853199005127, loss=4.218161106109619
I0307 03:06:24.303003 140096095368960 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.794326305389404, loss=4.197960376739502
I0307 03:07:02.422699 140096103761664 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.753154754638672, loss=4.089048862457275
I0307 03:07:40.939620 140096095368960 logging_writer.py:48] [4000] global_step=4000, grad_norm=6.4652276039123535, loss=4.203313827514648
I0307 03:07:55.535915 140252175811776 spec.py:321] Evaluating on the training split.
I0307 03:08:06.940694 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 03:08:28.076523 140252175811776 spec.py:349] Evaluating on the test split.
I0307 03:08:29.899038 140252175811776 submission_runner.py:469] Time since start: 1815.53s, 	Step: 4039, 	{'train/accuracy': 0.24754862487316132, 'train/loss': 3.6730093955993652, 'validation/accuracy': 0.21332000195980072, 'validation/loss': 3.9054133892059326, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 4.465139865875244, 'test/num_examples': 10000, 'score': 1589.411122560501, 'total_duration': 1815.5297737121582, 'accumulated_submission_time': 1589.411122560501, 'accumulated_eval_time': 225.42464971542358, 'accumulated_logging_time': 0.18996691703796387}
I0307 03:08:29.922322 140096103761664 logging_writer.py:48] [4039] accumulated_eval_time=225.425, accumulated_logging_time=0.189967, accumulated_submission_time=1589.41, global_step=4039, preemption_count=0, score=1589.41, test/accuracy=0.1522, test/loss=4.46514, test/num_examples=10000, total_duration=1815.53, train/accuracy=0.247549, train/loss=3.67301, validation/accuracy=0.21332, validation/loss=3.90541, validation/num_examples=50000
I0307 03:08:53.757426 140096095368960 logging_writer.py:48] [4100] global_step=4100, grad_norm=7.1539835929870605, loss=4.187923908233643
I0307 03:09:31.807961 140096103761664 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.582188129425049, loss=4.1658196449279785
I0307 03:10:10.435087 140096095368960 logging_writer.py:48] [4300] global_step=4300, grad_norm=5.662481307983398, loss=3.976954221725464
I0307 03:10:49.019261 140096103761664 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.949333190917969, loss=3.9538984298706055
I0307 03:11:27.572705 140096095368960 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.02121114730835, loss=3.954620599746704
I0307 03:12:05.352759 140096103761664 logging_writer.py:48] [4600] global_step=4600, grad_norm=8.894828796386719, loss=3.7456085681915283
I0307 03:12:42.400855 140096095368960 logging_writer.py:48] [4700] global_step=4700, grad_norm=5.57873010635376, loss=3.7860913276672363
I0307 03:13:21.380024 140096103761664 logging_writer.py:48] [4800] global_step=4800, grad_norm=6.290350437164307, loss=3.8098959922790527
I0307 03:14:00.758228 140096095368960 logging_writer.py:48] [4900] global_step=4900, grad_norm=7.352038383483887, loss=3.667658805847168
I0307 03:14:39.584336 140096103761664 logging_writer.py:48] [5000] global_step=5000, grad_norm=6.594536781311035, loss=3.719437837600708
I0307 03:15:19.062631 140096095368960 logging_writer.py:48] [5100] global_step=5100, grad_norm=9.066641807556152, loss=3.7409188747406006
I0307 03:15:58.195606 140096103761664 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.9186105728149414, loss=3.6604254245758057
I0307 03:16:36.353097 140096095368960 logging_writer.py:48] [5300] global_step=5300, grad_norm=8.878323554992676, loss=3.6590888500213623
I0307 03:17:00.045652 140252175811776 spec.py:321] Evaluating on the training split.
I0307 03:17:11.244819 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 03:17:30.282526 140252175811776 spec.py:349] Evaluating on the test split.
I0307 03:17:32.128753 140252175811776 submission_runner.py:469] Time since start: 2357.76s, 	Step: 5363, 	{'train/accuracy': 0.3169642686843872, 'train/loss': 3.187441110610962, 'validation/accuracy': 0.2798999845981598, 'validation/loss': 3.434617757797241, 'validation/num_examples': 50000, 'test/accuracy': 0.20250001549720764, 'test/loss': 4.08212947845459, 'test/num_examples': 10000, 'score': 2099.3799159526825, 'total_duration': 2357.759490966797, 'accumulated_submission_time': 2099.3799159526825, 'accumulated_eval_time': 257.50771045684814, 'accumulated_logging_time': 0.2220747470855713}
I0307 03:17:32.220420 140096103761664 logging_writer.py:48] [5363] accumulated_eval_time=257.508, accumulated_logging_time=0.222075, accumulated_submission_time=2099.38, global_step=5363, preemption_count=0, score=2099.38, test/accuracy=0.2025, test/loss=4.08213, test/num_examples=10000, total_duration=2357.76, train/accuracy=0.316964, train/loss=3.18744, validation/accuracy=0.2799, validation/loss=3.43462, validation/num_examples=50000
I0307 03:17:47.059400 140096095368960 logging_writer.py:48] [5400] global_step=5400, grad_norm=9.08055591583252, loss=3.6381261348724365
I0307 03:18:25.508012 140096103761664 logging_writer.py:48] [5500] global_step=5500, grad_norm=6.770047187805176, loss=3.639941930770874
I0307 03:19:03.988050 140096095368960 logging_writer.py:48] [5600] global_step=5600, grad_norm=6.085519313812256, loss=3.5138778686523438
I0307 03:19:42.709015 140096103761664 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.599032402038574, loss=3.6187822818756104
I0307 03:20:21.478862 140096095368960 logging_writer.py:48] [5800] global_step=5800, grad_norm=5.577571868896484, loss=3.435119867324829
I0307 03:20:59.537244 140096103761664 logging_writer.py:48] [5900] global_step=5900, grad_norm=8.85855770111084, loss=3.3652467727661133
I0307 03:21:38.049493 140096095368960 logging_writer.py:48] [6000] global_step=6000, grad_norm=5.866148948669434, loss=3.289747476577759
I0307 03:22:17.015523 140096103761664 logging_writer.py:48] [6100] global_step=6100, grad_norm=8.003256797790527, loss=3.3028628826141357
I0307 03:22:55.346322 140096095368960 logging_writer.py:48] [6200] global_step=6200, grad_norm=6.988997459411621, loss=3.272505760192871
I0307 03:23:34.145568 140096103761664 logging_writer.py:48] [6300] global_step=6300, grad_norm=6.655354976654053, loss=3.260122299194336
I0307 03:24:13.152252 140096095368960 logging_writer.py:48] [6400] global_step=6400, grad_norm=7.901430130004883, loss=3.268329620361328
I0307 03:24:51.830624 140096103761664 logging_writer.py:48] [6500] global_step=6500, grad_norm=6.654021263122559, loss=3.2934865951538086
I0307 03:25:30.307635 140096095368960 logging_writer.py:48] [6600] global_step=6600, grad_norm=6.02387809753418, loss=3.243201971054077
I0307 03:26:02.459348 140252175811776 spec.py:321] Evaluating on the training split.
I0307 03:26:13.989953 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 03:26:36.208104 140252175811776 spec.py:349] Evaluating on the test split.
I0307 03:26:38.055953 140252175811776 submission_runner.py:469] Time since start: 2903.69s, 	Step: 6685, 	{'train/accuracy': 0.3978993892669678, 'train/loss': 2.6877329349517822, 'validation/accuracy': 0.35613998770713806, 'validation/loss': 2.9459691047668457, 'validation/num_examples': 50000, 'test/accuracy': 0.27410000562667847, 'test/loss': 3.620755672454834, 'test/num_examples': 10000, 'score': 2609.481994152069, 'total_duration': 2903.6866738796234, 'accumulated_submission_time': 2609.481994152069, 'accumulated_eval_time': 293.1042561531067, 'accumulated_logging_time': 0.32263851165771484}
I0307 03:26:38.080585 140096103761664 logging_writer.py:48] [6685] accumulated_eval_time=293.104, accumulated_logging_time=0.322639, accumulated_submission_time=2609.48, global_step=6685, preemption_count=0, score=2609.48, test/accuracy=0.2741, test/loss=3.62076, test/num_examples=10000, total_duration=2903.69, train/accuracy=0.397899, train/loss=2.68773, validation/accuracy=0.35614, validation/loss=2.94597, validation/num_examples=50000
I0307 03:26:44.308025 140096095368960 logging_writer.py:48] [6700] global_step=6700, grad_norm=7.640244007110596, loss=3.180018901824951
I0307 03:27:22.750227 140096103761664 logging_writer.py:48] [6800] global_step=6800, grad_norm=8.866665840148926, loss=3.1042702198028564
I0307 03:28:01.548576 140096095368960 logging_writer.py:48] [6900] global_step=6900, grad_norm=8.048514366149902, loss=3.0972115993499756
I0307 03:28:40.241068 140096103761664 logging_writer.py:48] [7000] global_step=7000, grad_norm=7.3019843101501465, loss=3.2328391075134277
I0307 03:29:18.275424 140096095368960 logging_writer.py:48] [7100] global_step=7100, grad_norm=6.973880767822266, loss=3.143051862716675
I0307 03:29:56.668953 140096103761664 logging_writer.py:48] [7200] global_step=7200, grad_norm=7.6587066650390625, loss=3.027825355529785
I0307 03:30:35.272059 140096095368960 logging_writer.py:48] [7300] global_step=7300, grad_norm=7.052717685699463, loss=3.0367698669433594
I0307 03:31:13.717931 140096103761664 logging_writer.py:48] [7400] global_step=7400, grad_norm=8.395150184631348, loss=3.1755497455596924
I0307 03:31:51.781553 140096095368960 logging_writer.py:48] [7500] global_step=7500, grad_norm=6.516180515289307, loss=3.0321526527404785
I0307 03:32:30.274970 140096103761664 logging_writer.py:48] [7600] global_step=7600, grad_norm=11.542425155639648, loss=2.886552572250366
I0307 03:33:08.872721 140096095368960 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.176807403564453, loss=3.1119065284729004
I0307 03:33:47.744860 140096103761664 logging_writer.py:48] [7800] global_step=7800, grad_norm=6.159725189208984, loss=3.1417181491851807
I0307 03:34:26.231991 140096095368960 logging_writer.py:48] [7900] global_step=7900, grad_norm=5.197370529174805, loss=2.9214539527893066
I0307 03:35:04.524495 140096103761664 logging_writer.py:48] [8000] global_step=8000, grad_norm=5.962371349334717, loss=2.9829225540161133
I0307 03:35:08.271956 140252175811776 spec.py:321] Evaluating on the training split.
I0307 03:35:19.306174 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 03:35:40.185672 140252175811776 spec.py:349] Evaluating on the test split.
I0307 03:35:42.012807 140252175811776 submission_runner.py:469] Time since start: 3447.64s, 	Step: 8011, 	{'train/accuracy': 0.4621332883834839, 'train/loss': 2.350759744644165, 'validation/accuracy': 0.41543999314308167, 'validation/loss': 2.6091153621673584, 'validation/num_examples': 50000, 'test/accuracy': 0.3175000250339508, 'test/loss': 3.291898250579834, 'test/num_examples': 10000, 'score': 3119.4521787166595, 'total_duration': 3447.64355969429, 'accumulated_submission_time': 3119.4521787166595, 'accumulated_eval_time': 326.84507513046265, 'accumulated_logging_time': 0.44254541397094727}
I0307 03:35:42.071962 140096095368960 logging_writer.py:48] [8011] accumulated_eval_time=326.845, accumulated_logging_time=0.442545, accumulated_submission_time=3119.45, global_step=8011, preemption_count=0, score=3119.45, test/accuracy=0.3175, test/loss=3.2919, test/num_examples=10000, total_duration=3447.64, train/accuracy=0.462133, train/loss=2.35076, validation/accuracy=0.41544, validation/loss=2.60912, validation/num_examples=50000
I0307 03:36:16.898119 140096103761664 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.962063312530518, loss=2.8480677604675293
I0307 03:36:55.507557 140096095368960 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.338858127593994, loss=2.765155792236328
I0307 03:37:34.147123 140096103761664 logging_writer.py:48] [8300] global_step=8300, grad_norm=9.538840293884277, loss=2.7771565914154053
I0307 03:38:13.221034 140096095368960 logging_writer.py:48] [8400] global_step=8400, grad_norm=6.8694562911987305, loss=2.875417470932007
I0307 03:38:51.715015 140096103761664 logging_writer.py:48] [8500] global_step=8500, grad_norm=6.9846367835998535, loss=2.842714548110962
I0307 03:39:30.425110 140096095368960 logging_writer.py:48] [8600] global_step=8600, grad_norm=7.305400848388672, loss=2.804304361343384
I0307 03:40:09.107046 140096103761664 logging_writer.py:48] [8700] global_step=8700, grad_norm=8.292643547058105, loss=2.840146780014038
I0307 03:40:48.284836 140096095368960 logging_writer.py:48] [8800] global_step=8800, grad_norm=5.885936737060547, loss=2.839231252670288
I0307 03:41:27.253304 140096103761664 logging_writer.py:48] [8900] global_step=8900, grad_norm=5.451898574829102, loss=2.741492748260498
I0307 03:42:06.023155 140096095368960 logging_writer.py:48] [9000] global_step=9000, grad_norm=7.117415904998779, loss=2.7952523231506348
I0307 03:42:44.427580 140096103761664 logging_writer.py:48] [9100] global_step=9100, grad_norm=6.496162414550781, loss=2.6819491386413574
I0307 03:43:23.176009 140096095368960 logging_writer.py:48] [9200] global_step=9200, grad_norm=4.702418804168701, loss=2.6692490577697754
I0307 03:44:01.495851 140096103761664 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.3751044273376465, loss=2.5026140213012695
I0307 03:44:12.304234 140252175811776 spec.py:321] Evaluating on the training split.
I0307 03:44:23.817855 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 03:44:46.565732 140252175811776 spec.py:349] Evaluating on the test split.
I0307 03:44:48.330988 140252175811776 submission_runner.py:469] Time since start: 3993.96s, 	Step: 9329, 	{'train/accuracy': 0.5075733065605164, 'train/loss': 2.1023128032684326, 'validation/accuracy': 0.45879998803138733, 'validation/loss': 2.3864471912384033, 'validation/num_examples': 50000, 'test/accuracy': 0.35180002450942993, 'test/loss': 3.1156678199768066, 'test/num_examples': 10000, 'score': 3629.5358498096466, 'total_duration': 3993.9617400169373, 'accumulated_submission_time': 3629.5358498096466, 'accumulated_eval_time': 362.87179923057556, 'accumulated_logging_time': 0.5103325843811035}
I0307 03:44:48.367777 140096095368960 logging_writer.py:48] [9329] accumulated_eval_time=362.872, accumulated_logging_time=0.510333, accumulated_submission_time=3629.54, global_step=9329, preemption_count=0, score=3629.54, test/accuracy=0.3518, test/loss=3.11567, test/num_examples=10000, total_duration=3993.96, train/accuracy=0.507573, train/loss=2.10231, validation/accuracy=0.4588, validation/loss=2.38645, validation/num_examples=50000
I0307 03:45:16.281649 140096103761664 logging_writer.py:48] [9400] global_step=9400, grad_norm=6.405508518218994, loss=2.736663341522217
I0307 03:45:54.894397 140096095368960 logging_writer.py:48] [9500] global_step=9500, grad_norm=6.370846271514893, loss=2.6760244369506836
I0307 03:46:33.133283 140096103761664 logging_writer.py:48] [9600] global_step=9600, grad_norm=8.127588272094727, loss=2.698087215423584
I0307 03:47:11.400343 140096095368960 logging_writer.py:48] [9700] global_step=9700, grad_norm=7.1982293128967285, loss=2.6015195846557617
I0307 03:47:50.333176 140096103761664 logging_writer.py:48] [9800] global_step=9800, grad_norm=6.723674297332764, loss=2.752403497695923
I0307 03:48:29.366000 140096095368960 logging_writer.py:48] [9900] global_step=9900, grad_norm=9.410466194152832, loss=2.6025712490081787
I0307 03:49:08.329375 140096103761664 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.724015712738037, loss=2.5210204124450684
I0307 03:49:47.350282 140096095368960 logging_writer.py:48] [10100] global_step=10100, grad_norm=8.923849105834961, loss=2.4832701683044434
I0307 03:50:26.214196 140096103761664 logging_writer.py:48] [10200] global_step=10200, grad_norm=8.490373611450195, loss=2.581786632537842
I0307 03:51:04.857398 140096095368960 logging_writer.py:48] [10300] global_step=10300, grad_norm=7.757775783538818, loss=2.4577388763427734
I0307 03:51:43.630754 140096103761664 logging_writer.py:48] [10400] global_step=10400, grad_norm=7.441298007965088, loss=2.532089948654175
I0307 03:52:22.270181 140096095368960 logging_writer.py:48] [10500] global_step=10500, grad_norm=6.350615978240967, loss=2.6770308017730713
I0307 03:53:01.332546 140096103761664 logging_writer.py:48] [10600] global_step=10600, grad_norm=10.064602851867676, loss=2.5404767990112305
I0307 03:53:18.504069 140252175811776 spec.py:321] Evaluating on the training split.
I0307 03:53:30.978260 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 03:53:53.263960 140252175811776 spec.py:349] Evaluating on the test split.
I0307 03:53:55.002461 140252175811776 submission_runner.py:469] Time since start: 4540.63s, 	Step: 10646, 	{'train/accuracy': 0.542390763759613, 'train/loss': 1.9380462169647217, 'validation/accuracy': 0.4893999993801117, 'validation/loss': 2.205112934112549, 'validation/num_examples': 50000, 'test/accuracy': 0.3776000142097473, 'test/loss': 2.91927170753479, 'test/num_examples': 10000, 'score': 4139.530784130096, 'total_duration': 4540.633215904236, 'accumulated_submission_time': 4139.530784130096, 'accumulated_eval_time': 399.37016129493713, 'accumulated_logging_time': 0.5549983978271484}
I0307 03:53:55.020408 140096095368960 logging_writer.py:48] [10646] accumulated_eval_time=399.37, accumulated_logging_time=0.554998, accumulated_submission_time=4139.53, global_step=10646, preemption_count=0, score=4139.53, test/accuracy=0.3776, test/loss=2.91927, test/num_examples=10000, total_duration=4540.63, train/accuracy=0.542391, train/loss=1.93805, validation/accuracy=0.4894, validation/loss=2.20511, validation/num_examples=50000
I0307 03:54:16.270191 140096103761664 logging_writer.py:48] [10700] global_step=10700, grad_norm=6.433673858642578, loss=2.597996473312378
I0307 03:54:54.891935 140096095368960 logging_writer.py:48] [10800] global_step=10800, grad_norm=5.465110778808594, loss=2.4683501720428467
I0307 03:55:33.772285 140096103761664 logging_writer.py:48] [10900] global_step=10900, grad_norm=8.618714332580566, loss=2.3982865810394287
I0307 03:56:12.507427 140096095368960 logging_writer.py:48] [11000] global_step=11000, grad_norm=10.22852897644043, loss=2.4762704372406006
I0307 03:56:51.428126 140096103761664 logging_writer.py:48] [11100] global_step=11100, grad_norm=6.205380439758301, loss=2.36822772026062
I0307 03:57:30.120085 140096095368960 logging_writer.py:48] [11200] global_step=11200, grad_norm=8.603389739990234, loss=2.5073893070220947
I0307 03:58:09.194008 140096103761664 logging_writer.py:48] [11300] global_step=11300, grad_norm=7.691194534301758, loss=2.419919729232788
I0307 03:58:47.663717 140096095368960 logging_writer.py:48] [11400] global_step=11400, grad_norm=6.663145542144775, loss=2.3879055976867676
I0307 03:59:26.277981 140096103761664 logging_writer.py:48] [11500] global_step=11500, grad_norm=6.694370269775391, loss=2.4370508193969727
I0307 04:00:04.788525 140096095368960 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.911845684051514, loss=2.3638904094696045
I0307 04:00:43.656674 140096103761664 logging_writer.py:48] [11700] global_step=11700, grad_norm=8.619333267211914, loss=2.5704288482666016
I0307 04:01:22.448895 140096095368960 logging_writer.py:48] [11800] global_step=11800, grad_norm=7.3751397132873535, loss=2.4712119102478027
I0307 04:02:01.115308 140096103761664 logging_writer.py:48] [11900] global_step=11900, grad_norm=6.117668151855469, loss=2.3284006118774414
I0307 04:02:25.004263 140252175811776 spec.py:321] Evaluating on the training split.
I0307 04:02:41.969810 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 04:03:06.250529 140252175811776 spec.py:349] Evaluating on the test split.
I0307 04:03:08.038283 140252175811776 submission_runner.py:469] Time since start: 5093.67s, 	Step: 11963, 	{'train/accuracy': 0.5576570630073547, 'train/loss': 1.8505250215530396, 'validation/accuracy': 0.5041399598121643, 'validation/loss': 2.143873691558838, 'validation/num_examples': 50000, 'test/accuracy': 0.38770002126693726, 'test/loss': 2.876908540725708, 'test/num_examples': 10000, 'score': 4649.374207019806, 'total_duration': 5093.669030427933, 'accumulated_submission_time': 4649.374207019806, 'accumulated_eval_time': 442.40414237976074, 'accumulated_logging_time': 0.5805182456970215}
I0307 04:03:08.176951 140096095368960 logging_writer.py:48] [11963] accumulated_eval_time=442.404, accumulated_logging_time=0.580518, accumulated_submission_time=4649.37, global_step=11963, preemption_count=0, score=4649.37, test/accuracy=0.3877, test/loss=2.87691, test/num_examples=10000, total_duration=5093.67, train/accuracy=0.557657, train/loss=1.85053, validation/accuracy=0.50414, validation/loss=2.14387, validation/num_examples=50000
I0307 04:03:22.918587 140096103761664 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.291451930999756, loss=2.4085731506347656
I0307 04:04:01.703734 140096095368960 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.83935022354126, loss=2.3127641677856445
I0307 04:04:40.127528 140096103761664 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.239025592803955, loss=2.3151214122772217
I0307 04:05:18.945324 140096095368960 logging_writer.py:48] [12300] global_step=12300, grad_norm=7.464247703552246, loss=2.344102621078491
I0307 04:05:57.741948 140096103761664 logging_writer.py:48] [12400] global_step=12400, grad_norm=6.420445919036865, loss=2.465272903442383
I0307 04:06:36.114148 140096095368960 logging_writer.py:48] [12500] global_step=12500, grad_norm=8.489344596862793, loss=2.4224376678466797
I0307 04:07:14.893498 140096103761664 logging_writer.py:48] [12600] global_step=12600, grad_norm=6.9628777503967285, loss=2.2760627269744873
I0307 04:07:53.523415 140096095368960 logging_writer.py:48] [12700] global_step=12700, grad_norm=5.062230587005615, loss=2.2894861698150635
I0307 04:08:32.336289 140096103761664 logging_writer.py:48] [12800] global_step=12800, grad_norm=7.250995635986328, loss=2.4077045917510986
I0307 04:09:11.080321 140096095368960 logging_writer.py:48] [12900] global_step=12900, grad_norm=7.047276496887207, loss=2.353771924972534
I0307 04:09:50.038123 140096103761664 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.281796455383301, loss=2.382023811340332
I0307 04:10:28.944354 140096095368960 logging_writer.py:48] [13100] global_step=13100, grad_norm=7.0880866050720215, loss=2.4623498916625977
I0307 04:11:07.637575 140096103761664 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.734903335571289, loss=2.304966449737549
I0307 04:11:38.146412 140252175811776 spec.py:321] Evaluating on the training split.
I0307 04:11:54.714148 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 04:12:20.795405 140252175811776 spec.py:349] Evaluating on the test split.
I0307 04:12:22.580206 140252175811776 submission_runner.py:469] Time since start: 5648.21s, 	Step: 13280, 	{'train/accuracy': 0.5846221446990967, 'train/loss': 1.7235958576202393, 'validation/accuracy': 0.5286799669265747, 'validation/loss': 2.000023126602173, 'validation/num_examples': 50000, 'test/accuracy': 0.4109000265598297, 'test/loss': 2.7437474727630615, 'test/num_examples': 10000, 'score': 5159.199308156967, 'total_duration': 5648.210936307907, 'accumulated_submission_time': 5159.199308156967, 'accumulated_eval_time': 486.8378872871399, 'accumulated_logging_time': 0.7265846729278564}
I0307 04:12:22.652334 140096095368960 logging_writer.py:48] [13280] accumulated_eval_time=486.838, accumulated_logging_time=0.726585, accumulated_submission_time=5159.2, global_step=13280, preemption_count=0, score=5159.2, test/accuracy=0.4109, test/loss=2.74375, test/num_examples=10000, total_duration=5648.21, train/accuracy=0.584622, train/loss=1.7236, validation/accuracy=0.52868, validation/loss=2.00002, validation/num_examples=50000
I0307 04:12:30.818376 140096103761664 logging_writer.py:48] [13300] global_step=13300, grad_norm=7.040496826171875, loss=2.226771593093872
I0307 04:13:11.779235 140096095368960 logging_writer.py:48] [13400] global_step=13400, grad_norm=8.333060264587402, loss=2.257960319519043
I0307 04:13:50.295112 140096103761664 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.023158550262451, loss=2.381957769393921
I0307 04:14:29.415571 140096095368960 logging_writer.py:48] [13600] global_step=13600, grad_norm=4.889688491821289, loss=2.2972450256347656
I0307 04:15:08.222278 140096103761664 logging_writer.py:48] [13700] global_step=13700, grad_norm=5.60116720199585, loss=2.2575812339782715
I0307 04:15:47.669087 140096095368960 logging_writer.py:48] [13800] global_step=13800, grad_norm=8.20947265625, loss=2.3329765796661377
I0307 04:16:26.700109 140096103761664 logging_writer.py:48] [13900] global_step=13900, grad_norm=9.119921684265137, loss=2.359269618988037
I0307 04:17:05.188740 140096095368960 logging_writer.py:48] [14000] global_step=14000, grad_norm=6.747682094573975, loss=2.442963123321533
I0307 04:17:44.243315 140096103761664 logging_writer.py:48] [14100] global_step=14100, grad_norm=11.16021728515625, loss=2.2589900493621826
I0307 04:18:23.282303 140096095368960 logging_writer.py:48] [14200] global_step=14200, grad_norm=10.472257614135742, loss=2.2736260890960693
I0307 04:19:02.223818 140096103761664 logging_writer.py:48] [14300] global_step=14300, grad_norm=6.368400573730469, loss=2.385427951812744
I0307 04:19:41.596998 140096095368960 logging_writer.py:48] [14400] global_step=14400, grad_norm=7.2486724853515625, loss=2.3044235706329346
I0307 04:20:19.743124 140096103761664 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.5823588371276855, loss=2.240083932876587
I0307 04:20:52.914738 140252175811776 spec.py:321] Evaluating on the training split.
I0307 04:21:06.547373 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 04:21:30.955113 140252175811776 spec.py:349] Evaluating on the test split.
I0307 04:21:32.730212 140252175811776 submission_runner.py:469] Time since start: 6198.36s, 	Step: 14587, 	{'train/accuracy': 0.5922752022743225, 'train/loss': 1.687596321105957, 'validation/accuracy': 0.5329799652099609, 'validation/loss': 1.9866801500320435, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.706833839416504, 'test/num_examples': 10000, 'score': 5669.280317783356, 'total_duration': 6198.360827922821, 'accumulated_submission_time': 5669.280317783356, 'accumulated_eval_time': 526.6531901359558, 'accumulated_logging_time': 0.8367834091186523}
I0307 04:21:32.815378 140096095368960 logging_writer.py:48] [14587] accumulated_eval_time=526.653, accumulated_logging_time=0.836783, accumulated_submission_time=5669.28, global_step=14587, preemption_count=0, score=5669.28, test/accuracy=0.4167, test/loss=2.70683, test/num_examples=10000, total_duration=6198.36, train/accuracy=0.592275, train/loss=1.6876, validation/accuracy=0.53298, validation/loss=1.98668, validation/num_examples=50000
I0307 04:21:38.256750 140096103761664 logging_writer.py:48] [14600] global_step=14600, grad_norm=7.609963893890381, loss=2.314697742462158
I0307 04:22:17.105503 140096095368960 logging_writer.py:48] [14700] global_step=14700, grad_norm=9.909863471984863, loss=2.3748579025268555
I0307 04:22:55.848040 140096103761664 logging_writer.py:48] [14800] global_step=14800, grad_norm=9.485055923461914, loss=2.1509318351745605
I0307 04:23:34.044362 140096095368960 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.382728099822998, loss=2.3651108741760254
I0307 04:24:12.854832 140096103761664 logging_writer.py:48] [15000] global_step=15000, grad_norm=9.438385009765625, loss=2.24969744682312
I0307 04:24:51.873638 140096095368960 logging_writer.py:48] [15100] global_step=15100, grad_norm=7.478503227233887, loss=2.1804146766662598
I0307 04:25:30.565059 140096103761664 logging_writer.py:48] [15200] global_step=15200, grad_norm=5.018370628356934, loss=2.2303760051727295
I0307 04:26:09.479612 140096095368960 logging_writer.py:48] [15300] global_step=15300, grad_norm=5.077371597290039, loss=2.224879741668701
I0307 04:26:48.437883 140096103761664 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.498068809509277, loss=2.1941001415252686
I0307 04:27:27.645894 140096095368960 logging_writer.py:48] [15500] global_step=15500, grad_norm=5.5433149337768555, loss=2.256680965423584
I0307 04:28:06.278509 140096103761664 logging_writer.py:48] [15600] global_step=15600, grad_norm=7.3967604637146, loss=2.2233245372772217
I0307 04:28:44.748940 140096095368960 logging_writer.py:48] [15700] global_step=15700, grad_norm=4.48098087310791, loss=2.303924322128296
I0307 04:29:23.465785 140096103761664 logging_writer.py:48] [15800] global_step=15800, grad_norm=6.468645095825195, loss=2.139026641845703
I0307 04:30:01.804857 140096095368960 logging_writer.py:48] [15900] global_step=15900, grad_norm=6.700055122375488, loss=2.25837779045105
I0307 04:30:02.943766 140252175811776 spec.py:321] Evaluating on the training split.
I0307 04:30:21.048299 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 04:30:45.443367 140252175811776 spec.py:349] Evaluating on the test split.
I0307 04:30:47.215734 140252175811776 submission_runner.py:469] Time since start: 6752.85s, 	Step: 15904, 	{'train/accuracy': 0.615234375, 'train/loss': 1.5854220390319824, 'validation/accuracy': 0.5554800033569336, 'validation/loss': 1.882692575454712, 'validation/num_examples': 50000, 'test/accuracy': 0.4368000328540802, 'test/loss': 2.622231960296631, 'test/num_examples': 10000, 'score': 6179.263180017471, 'total_duration': 6752.846360683441, 'accumulated_submission_time': 6179.263180017471, 'accumulated_eval_time': 570.9250020980835, 'accumulated_logging_time': 0.9297308921813965}
I0307 04:30:47.281105 140096103761664 logging_writer.py:48] [15904] accumulated_eval_time=570.925, accumulated_logging_time=0.929731, accumulated_submission_time=6179.26, global_step=15904, preemption_count=0, score=6179.26, test/accuracy=0.4368, test/loss=2.62223, test/num_examples=10000, total_duration=6752.85, train/accuracy=0.615234, train/loss=1.58542, validation/accuracy=0.55548, validation/loss=1.88269, validation/num_examples=50000
I0307 04:31:25.001801 140096095368960 logging_writer.py:48] [16000] global_step=16000, grad_norm=5.987884044647217, loss=2.116615056991577
I0307 04:32:03.956850 140096103761664 logging_writer.py:48] [16100] global_step=16100, grad_norm=7.504514217376709, loss=2.30026912689209
I0307 04:32:42.496487 140096095368960 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.770164966583252, loss=2.1835522651672363
I0307 04:33:21.298139 140096103761664 logging_writer.py:48] [16300] global_step=16300, grad_norm=6.50676155090332, loss=2.1734588146209717
I0307 04:33:59.698671 140096095368960 logging_writer.py:48] [16400] global_step=16400, grad_norm=7.149582862854004, loss=2.229512929916382
I0307 04:34:38.516789 140096103761664 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.312554359436035, loss=2.2244791984558105
I0307 04:35:17.609295 140096095368960 logging_writer.py:48] [16600] global_step=16600, grad_norm=5.945812225341797, loss=2.1013729572296143
I0307 04:35:56.396994 140096103761664 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.867079734802246, loss=2.285964250564575
I0307 04:36:35.283348 140096095368960 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.802401542663574, loss=2.2521896362304688
I0307 04:37:14.369149 140096103761664 logging_writer.py:48] [16900] global_step=16900, grad_norm=6.725522041320801, loss=2.1399354934692383
I0307 04:37:53.480440 140096095368960 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.573668956756592, loss=2.2194266319274902
I0307 04:38:32.305327 140096103761664 logging_writer.py:48] [17100] global_step=17100, grad_norm=5.174842357635498, loss=2.088763475418091
I0307 04:39:10.885671 140096095368960 logging_writer.py:48] [17200] global_step=17200, grad_norm=7.705434799194336, loss=2.1863455772399902
I0307 04:39:17.532820 140252175811776 spec.py:321] Evaluating on the training split.
I0307 04:39:31.403437 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 04:39:54.288227 140252175811776 spec.py:349] Evaluating on the test split.
I0307 04:39:56.054328 140252175811776 submission_runner.py:469] Time since start: 7301.68s, 	Step: 17218, 	{'train/accuracy': 0.6101123690605164, 'train/loss': 1.593916893005371, 'validation/accuracy': 0.5503199696540833, 'validation/loss': 1.901633381843567, 'validation/num_examples': 50000, 'test/accuracy': 0.43640002608299255, 'test/loss': 2.6130807399749756, 'test/num_examples': 10000, 'score': 6689.372481584549, 'total_duration': 7301.684935092926, 'accumulated_submission_time': 6689.372481584549, 'accumulated_eval_time': 609.4463336467743, 'accumulated_logging_time': 1.003150224685669}
I0307 04:39:56.117998 140096103761664 logging_writer.py:48] [17218] accumulated_eval_time=609.446, accumulated_logging_time=1.00315, accumulated_submission_time=6689.37, global_step=17218, preemption_count=0, score=6689.37, test/accuracy=0.4364, test/loss=2.61308, test/num_examples=10000, total_duration=7301.68, train/accuracy=0.610112, train/loss=1.59392, validation/accuracy=0.55032, validation/loss=1.90163, validation/num_examples=50000
I0307 04:40:28.223305 140096095368960 logging_writer.py:48] [17300] global_step=17300, grad_norm=6.424908638000488, loss=2.136794090270996
I0307 04:41:06.915045 140096103761664 logging_writer.py:48] [17400] global_step=17400, grad_norm=7.106617450714111, loss=2.144146203994751
I0307 04:41:46.119200 140096095368960 logging_writer.py:48] [17500] global_step=17500, grad_norm=7.843863010406494, loss=2.0666821002960205
I0307 04:42:24.896790 140096103761664 logging_writer.py:48] [17600] global_step=17600, grad_norm=7.97176456451416, loss=2.2725272178649902
I0307 04:43:03.893833 140096095368960 logging_writer.py:48] [17700] global_step=17700, grad_norm=4.394371509552002, loss=2.2724661827087402
I0307 04:43:42.450875 140096103761664 logging_writer.py:48] [17800] global_step=17800, grad_norm=6.853585243225098, loss=2.1963274478912354
I0307 04:44:21.499276 140096095368960 logging_writer.py:48] [17900] global_step=17900, grad_norm=6.295531272888184, loss=2.2405734062194824
I0307 04:45:00.189880 140096103761664 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.328219890594482, loss=2.168703079223633
I0307 04:45:39.338869 140096095368960 logging_writer.py:48] [18100] global_step=18100, grad_norm=7.403343200683594, loss=2.33835506439209
I0307 04:46:18.060402 140096103761664 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.827437400817871, loss=2.205408811569214
I0307 04:46:57.171506 140096095368960 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.240080833435059, loss=2.198092460632324
I0307 04:47:36.381320 140096103761664 logging_writer.py:48] [18400] global_step=18400, grad_norm=5.9906840324401855, loss=2.180067539215088
I0307 04:48:15.256969 140096095368960 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.709835529327393, loss=2.169720411300659
I0307 04:48:26.415699 140252175811776 spec.py:321] Evaluating on the training split.
I0307 04:48:41.529382 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 04:49:03.566014 140252175811776 spec.py:349] Evaluating on the test split.
I0307 04:49:05.302990 140252175811776 submission_runner.py:469] Time since start: 7850.93s, 	Step: 18530, 	{'train/accuracy': 0.6128427982330322, 'train/loss': 1.5683091878890991, 'validation/accuracy': 0.5608199834823608, 'validation/loss': 1.8562084436416626, 'validation/num_examples': 50000, 'test/accuracy': 0.43650001287460327, 'test/loss': 2.5754940509796143, 'test/num_examples': 10000, 'score': 7199.530290365219, 'total_duration': 7850.933743238449, 'accumulated_submission_time': 7199.530290365219, 'accumulated_eval_time': 648.333596944809, 'accumulated_logging_time': 1.0754914283752441}
I0307 04:49:05.323966 140096103761664 logging_writer.py:48] [18530] accumulated_eval_time=648.334, accumulated_logging_time=1.07549, accumulated_submission_time=7199.53, global_step=18530, preemption_count=0, score=7199.53, test/accuracy=0.4365, test/loss=2.57549, test/num_examples=10000, total_duration=7850.93, train/accuracy=0.612843, train/loss=1.56831, validation/accuracy=0.56082, validation/loss=1.85621, validation/num_examples=50000
I0307 04:49:32.835332 140096095368960 logging_writer.py:48] [18600] global_step=18600, grad_norm=5.107542514801025, loss=2.0463755130767822
I0307 04:50:11.616383 140096103761664 logging_writer.py:48] [18700] global_step=18700, grad_norm=5.146587371826172, loss=2.168609619140625
I0307 04:50:50.529580 140096095368960 logging_writer.py:48] [18800] global_step=18800, grad_norm=6.056588172912598, loss=2.2509078979492188
I0307 04:51:29.161983 140096103761664 logging_writer.py:48] [18900] global_step=18900, grad_norm=5.0922369956970215, loss=2.231903553009033
I0307 04:52:08.386083 140096095368960 logging_writer.py:48] [19000] global_step=19000, grad_norm=5.446976661682129, loss=2.184905529022217
I0307 04:52:47.029884 140096103761664 logging_writer.py:48] [19100] global_step=19100, grad_norm=6.611019134521484, loss=2.1897802352905273
I0307 04:53:25.712609 140096095368960 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.9139773845672607, loss=2.0267422199249268
I0307 04:54:04.562275 140096103761664 logging_writer.py:48] [19300] global_step=19300, grad_norm=6.71173095703125, loss=2.060544013977051
I0307 04:54:43.356329 140096095368960 logging_writer.py:48] [19400] global_step=19400, grad_norm=7.505786418914795, loss=2.156529664993286
I0307 04:55:22.207877 140096103761664 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.768783092498779, loss=2.0688109397888184
I0307 04:56:01.094436 140096095368960 logging_writer.py:48] [19600] global_step=19600, grad_norm=7.435506343841553, loss=2.151585340499878
I0307 04:56:39.315845 140096103761664 logging_writer.py:48] [19700] global_step=19700, grad_norm=5.195991039276123, loss=2.206810474395752
I0307 04:57:17.721568 140096095368960 logging_writer.py:48] [19800] global_step=19800, grad_norm=7.932981491088867, loss=2.1749215126037598
I0307 04:57:35.390351 140252175811776 spec.py:321] Evaluating on the training split.
I0307 04:57:51.054908 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 04:58:14.350287 140252175811776 spec.py:349] Evaluating on the test split.
I0307 04:58:16.075440 140252175811776 submission_runner.py:469] Time since start: 8401.71s, 	Step: 19846, 	{'train/accuracy': 0.6099330186843872, 'train/loss': 1.5969574451446533, 'validation/accuracy': 0.557420015335083, 'validation/loss': 1.875210165977478, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.591240644454956, 'test/num_examples': 10000, 'score': 7709.453641653061, 'total_duration': 8401.706051588058, 'accumulated_submission_time': 7709.453641653061, 'accumulated_eval_time': 689.0185160636902, 'accumulated_logging_time': 1.1042068004608154}
I0307 04:58:16.101729 140096103761664 logging_writer.py:48] [19846] accumulated_eval_time=689.019, accumulated_logging_time=1.10421, accumulated_submission_time=7709.45, global_step=19846, preemption_count=0, score=7709.45, test/accuracy=0.4388, test/loss=2.59124, test/num_examples=10000, total_duration=8401.71, train/accuracy=0.609933, train/loss=1.59696, validation/accuracy=0.55742, validation/loss=1.87521, validation/num_examples=50000
I0307 04:58:37.483895 140096095368960 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.8275017738342285, loss=2.2599380016326904
I0307 04:59:15.961454 140096103761664 logging_writer.py:48] [20000] global_step=20000, grad_norm=4.9345502853393555, loss=2.09977650642395
I0307 04:59:54.860597 140096095368960 logging_writer.py:48] [20100] global_step=20100, grad_norm=5.729201316833496, loss=2.0086965560913086
I0307 05:00:33.870070 140096103761664 logging_writer.py:48] [20200] global_step=20200, grad_norm=5.7858781814575195, loss=2.2252445220947266
I0307 05:01:12.742957 140096095368960 logging_writer.py:48] [20300] global_step=20300, grad_norm=5.9274702072143555, loss=2.1018388271331787
I0307 05:01:51.948581 140096103761664 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.5629467964172363, loss=2.1747047901153564
I0307 05:02:30.901180 140096095368960 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.837347030639648, loss=2.1808972358703613
I0307 05:03:09.624276 140096103761664 logging_writer.py:48] [20600] global_step=20600, grad_norm=5.203620433807373, loss=2.0457687377929688
I0307 05:03:48.407570 140096095368960 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.230833053588867, loss=2.02406907081604
I0307 05:04:26.742665 140096103761664 logging_writer.py:48] [20800] global_step=20800, grad_norm=4.0905022621154785, loss=2.1411561965942383
I0307 05:05:05.612942 140096095368960 logging_writer.py:48] [20900] global_step=20900, grad_norm=4.450869083404541, loss=2.23785662651062
I0307 05:05:44.480826 140096103761664 logging_writer.py:48] [21000] global_step=21000, grad_norm=5.37646484375, loss=2.041928291320801
I0307 05:06:23.275625 140096095368960 logging_writer.py:48] [21100] global_step=21100, grad_norm=5.972201824188232, loss=2.2190401554107666
I0307 05:06:46.344483 140252175811776 spec.py:321] Evaluating on the training split.
I0307 05:07:00.545434 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 05:07:23.977215 140252175811776 spec.py:349] Evaluating on the test split.
I0307 05:07:25.700647 140252175811776 submission_runner.py:469] Time since start: 8951.33s, 	Step: 21160, 	{'train/accuracy': 0.6230069994926453, 'train/loss': 1.5220811367034912, 'validation/accuracy': 0.5679999589920044, 'validation/loss': 1.8059231042861938, 'validation/num_examples': 50000, 'test/accuracy': 0.444100022315979, 'test/loss': 2.556509017944336, 'test/num_examples': 10000, 'score': 8219.55773806572, 'total_duration': 8951.331392765045, 'accumulated_submission_time': 8219.55773806572, 'accumulated_eval_time': 728.3746435642242, 'accumulated_logging_time': 1.1385698318481445}
I0307 05:07:25.818758 140096103761664 logging_writer.py:48] [21160] accumulated_eval_time=728.375, accumulated_logging_time=1.13857, accumulated_submission_time=8219.56, global_step=21160, preemption_count=0, score=8219.56, test/accuracy=0.4441, test/loss=2.55651, test/num_examples=10000, total_duration=8951.33, train/accuracy=0.623007, train/loss=1.52208, validation/accuracy=0.568, validation/loss=1.80592, validation/num_examples=50000
I0307 05:07:41.655260 140096095368960 logging_writer.py:48] [21200] global_step=21200, grad_norm=4.766599178314209, loss=2.2268075942993164
I0307 05:08:20.888627 140096103761664 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.963430881500244, loss=2.024219512939453
I0307 05:08:59.345712 140096095368960 logging_writer.py:48] [21400] global_step=21400, grad_norm=4.8905839920043945, loss=2.053920030593872
I0307 05:09:38.117368 140096103761664 logging_writer.py:48] [21500] global_step=21500, grad_norm=5.9401350021362305, loss=2.1180002689361572
I0307 05:10:20.642441 140096095368960 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.566413402557373, loss=2.118844747543335
I0307 05:11:03.567945 140096103761664 logging_writer.py:48] [21700] global_step=21700, grad_norm=4.576026916503906, loss=2.0619430541992188
I0307 05:11:42.750594 140096095368960 logging_writer.py:48] [21800] global_step=21800, grad_norm=4.110365390777588, loss=2.1085023880004883
I0307 05:12:21.856848 140096103761664 logging_writer.py:48] [21900] global_step=21900, grad_norm=4.677509784698486, loss=2.147538661956787
I0307 05:13:00.658253 140096095368960 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.219460487365723, loss=2.165483236312866
I0307 05:13:39.321648 140096103761664 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.789907932281494, loss=2.073354721069336
I0307 05:14:18.192821 140096095368960 logging_writer.py:48] [22200] global_step=22200, grad_norm=5.9413371086120605, loss=2.0693509578704834
I0307 05:14:56.823341 140096103761664 logging_writer.py:48] [22300] global_step=22300, grad_norm=6.277814865112305, loss=2.1996212005615234
I0307 05:15:35.246989 140096095368960 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.9357728958129883, loss=2.1285674571990967
I0307 05:15:55.967855 140252175811776 spec.py:321] Evaluating on the training split.
I0307 05:16:12.067880 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 05:16:36.033879 140252175811776 spec.py:349] Evaluating on the test split.
I0307 05:16:37.756386 140252175811776 submission_runner.py:469] Time since start: 9503.39s, 	Step: 22455, 	{'train/accuracy': 0.6252391338348389, 'train/loss': 1.5378427505493164, 'validation/accuracy': 0.5707799792289734, 'validation/loss': 1.81388258934021, 'validation/num_examples': 50000, 'test/accuracy': 0.4540000259876251, 'test/loss': 2.5297319889068604, 'test/num_examples': 10000, 'score': 8729.5656042099, 'total_duration': 9503.386975765228, 'accumulated_submission_time': 8729.5656042099, 'accumulated_eval_time': 770.1629872322083, 'accumulated_logging_time': 1.2649180889129639}
I0307 05:16:37.781082 140096103761664 logging_writer.py:48] [22455] accumulated_eval_time=770.163, accumulated_logging_time=1.26492, accumulated_submission_time=8729.57, global_step=22455, preemption_count=0, score=8729.57, test/accuracy=0.454, test/loss=2.52973, test/num_examples=10000, total_duration=9503.39, train/accuracy=0.625239, train/loss=1.53784, validation/accuracy=0.57078, validation/loss=1.81388, validation/num_examples=50000
I0307 05:16:55.659071 140096095368960 logging_writer.py:48] [22500] global_step=22500, grad_norm=4.102700233459473, loss=1.9934558868408203
I0307 05:17:34.593413 140096103761664 logging_writer.py:48] [22600] global_step=22600, grad_norm=5.200852870941162, loss=2.116177558898926
I0307 05:18:13.570217 140096095368960 logging_writer.py:48] [22700] global_step=22700, grad_norm=4.620643615722656, loss=2.1208598613739014
I0307 05:18:52.491912 140096103761664 logging_writer.py:48] [22800] global_step=22800, grad_norm=4.136539936065674, loss=1.9552487134933472
I0307 05:19:31.250102 140096095368960 logging_writer.py:48] [22900] global_step=22900, grad_norm=4.180089473724365, loss=2.270432710647583
I0307 05:20:10.030564 140096103761664 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.8604562282562256, loss=2.079087734222412
I0307 05:20:48.550368 140096095368960 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.62296724319458, loss=2.1498093605041504
I0307 05:21:27.443120 140096103761664 logging_writer.py:48] [23200] global_step=23200, grad_norm=4.024377822875977, loss=2.132467269897461
I0307 05:22:06.450772 140096095368960 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.2563040256500244, loss=2.032180070877075
I0307 05:22:45.007601 140096103761664 logging_writer.py:48] [23400] global_step=23400, grad_norm=4.3866753578186035, loss=2.201368808746338
I0307 05:23:23.708394 140096095368960 logging_writer.py:48] [23500] global_step=23500, grad_norm=4.659390449523926, loss=2.111969470977783
I0307 05:24:02.230195 140096103761664 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.653477907180786, loss=2.0871195793151855
I0307 05:24:41.103051 140096095368960 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.6296403408050537, loss=2.004704236984253
I0307 05:25:08.032996 140252175811776 spec.py:321] Evaluating on the training split.
I0307 05:25:24.723671 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 05:25:51.626698 140252175811776 spec.py:349] Evaluating on the test split.
I0307 05:25:53.383004 140252175811776 submission_runner.py:469] Time since start: 10059.01s, 	Step: 23770, 	{'train/accuracy': 0.6300023794174194, 'train/loss': 1.4954612255096436, 'validation/accuracy': 0.5751799941062927, 'validation/loss': 1.779248833656311, 'validation/num_examples': 50000, 'test/accuracy': 0.458700031042099, 'test/loss': 2.473832845687866, 'test/num_examples': 10000, 'score': 9239.627389669418, 'total_duration': 10059.013589382172, 'accumulated_submission_time': 9239.627389669418, 'accumulated_eval_time': 815.5128004550934, 'accumulated_logging_time': 1.3518857955932617}
I0307 05:25:53.432628 140096103761664 logging_writer.py:48] [23770] accumulated_eval_time=815.513, accumulated_logging_time=1.35189, accumulated_submission_time=9239.63, global_step=23770, preemption_count=0, score=9239.63, test/accuracy=0.4587, test/loss=2.47383, test/num_examples=10000, total_duration=10059, train/accuracy=0.630002, train/loss=1.49546, validation/accuracy=0.57518, validation/loss=1.77925, validation/num_examples=50000
I0307 05:26:05.646856 140096095368960 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.3118796348571777, loss=2.022714376449585
I0307 05:26:44.362171 140096103761664 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.3215181827545166, loss=2.1371946334838867
I0307 05:27:23.422839 140096095368960 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.901441812515259, loss=2.2158069610595703
I0307 05:28:02.770529 140096103761664 logging_writer.py:48] [24100] global_step=24100, grad_norm=4.067989826202393, loss=2.137270927429199
I0307 05:28:41.526731 140096095368960 logging_writer.py:48] [24200] global_step=24200, grad_norm=5.884893894195557, loss=2.050628900527954
I0307 05:29:20.313428 140096103761664 logging_writer.py:48] [24300] global_step=24300, grad_norm=4.186321258544922, loss=2.0909762382507324
I0307 05:29:59.636692 140096095368960 logging_writer.py:48] [24400] global_step=24400, grad_norm=4.955484390258789, loss=2.0571930408477783
I0307 05:30:38.491261 140096103761664 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.2156174182891846, loss=1.9886343479156494
I0307 05:31:17.449359 140096095368960 logging_writer.py:48] [24600] global_step=24600, grad_norm=4.235830307006836, loss=1.943990707397461
I0307 05:31:56.234567 140096103761664 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.451324701309204, loss=2.008762836456299
I0307 05:32:35.214617 140096095368960 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.2141358852386475, loss=2.0164151191711426
I0307 05:33:14.267473 140096103761664 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.5949177742004395, loss=2.0141117572784424
I0307 05:33:52.948912 140096095368960 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.7495310306549072, loss=1.9545210599899292
I0307 05:34:23.640878 140252175811776 spec.py:321] Evaluating on the training split.
I0307 05:34:38.444185 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 05:35:01.258149 140252175811776 spec.py:349] Evaluating on the test split.
I0307 05:35:03.105435 140252175811776 submission_runner.py:469] Time since start: 10608.74s, 	Step: 25079, 	{'train/accuracy': 0.6388113498687744, 'train/loss': 1.459547996520996, 'validation/accuracy': 0.5857999920845032, 'validation/loss': 1.7483519315719604, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.450523853302002, 'test/num_examples': 10000, 'score': 9749.690160751343, 'total_duration': 10608.736176490784, 'accumulated_submission_time': 9749.690160751343, 'accumulated_eval_time': 854.977322101593, 'accumulated_logging_time': 1.409886121749878}
I0307 05:35:03.210520 140096103761664 logging_writer.py:48] [25079] accumulated_eval_time=854.977, accumulated_logging_time=1.40989, accumulated_submission_time=9749.69, global_step=25079, preemption_count=0, score=9749.69, test/accuracy=0.4674, test/loss=2.45052, test/num_examples=10000, total_duration=10608.7, train/accuracy=0.638811, train/loss=1.45955, validation/accuracy=0.5858, validation/loss=1.74835, validation/num_examples=50000
I0307 05:35:11.997167 140096095368960 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.2744953632354736, loss=2.110494613647461
I0307 05:35:50.680329 140096103761664 logging_writer.py:48] [25200] global_step=25200, grad_norm=6.189933776855469, loss=2.0332319736480713
I0307 05:36:29.063814 140096095368960 logging_writer.py:48] [25300] global_step=25300, grad_norm=4.216989040374756, loss=2.0066614151000977
I0307 05:37:07.750987 140096103761664 logging_writer.py:48] [25400] global_step=25400, grad_norm=4.367244720458984, loss=2.0156688690185547
I0307 05:37:46.442758 140096095368960 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.6258575916290283, loss=2.04776930809021
I0307 05:38:25.606912 140096103761664 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.6267802715301514, loss=2.0029072761535645
I0307 05:39:04.841627 140096095368960 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.504769802093506, loss=2.0374603271484375
I0307 05:39:43.559269 140096103761664 logging_writer.py:48] [25800] global_step=25800, grad_norm=4.634181976318359, loss=1.9186146259307861
I0307 05:40:22.583836 140096095368960 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.4186222553253174, loss=1.9514672756195068
I0307 05:41:01.418262 140096103761664 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.6497716903686523, loss=2.099118709564209
I0307 05:41:40.404238 140096095368960 logging_writer.py:48] [26100] global_step=26100, grad_norm=4.733092308044434, loss=2.0688211917877197
I0307 05:42:18.490391 140096103761664 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.8912930488586426, loss=1.9990049600601196
I0307 05:42:57.820326 140096095368960 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.459848165512085, loss=2.0519869327545166
I0307 05:43:33.376289 140252175811776 spec.py:321] Evaluating on the training split.
I0307 05:43:50.967216 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 05:44:15.836141 140252175811776 spec.py:349] Evaluating on the test split.
I0307 05:44:17.585762 140252175811776 submission_runner.py:469] Time since start: 11163.22s, 	Step: 26392, 	{'train/accuracy': 0.6297831535339355, 'train/loss': 1.4772435426712036, 'validation/accuracy': 0.5799599885940552, 'validation/loss': 1.7559103965759277, 'validation/num_examples': 50000, 'test/accuracy': 0.4563000202178955, 'test/loss': 2.4692189693450928, 'test/num_examples': 10000, 'score': 10259.714161396027, 'total_duration': 11163.216361999512, 'accumulated_submission_time': 10259.714161396027, 'accumulated_eval_time': 899.1866092681885, 'accumulated_logging_time': 1.522904872894287}
I0307 05:44:17.715752 140096103761664 logging_writer.py:48] [26392] accumulated_eval_time=899.187, accumulated_logging_time=1.5229, accumulated_submission_time=10259.7, global_step=26392, preemption_count=0, score=10259.7, test/accuracy=0.4563, test/loss=2.46922, test/num_examples=10000, total_duration=11163.2, train/accuracy=0.629783, train/loss=1.47724, validation/accuracy=0.57996, validation/loss=1.75591, validation/num_examples=50000
I0307 05:44:21.633128 140096095368960 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.409346103668213, loss=2.158794403076172
I0307 05:44:59.957527 140096103761664 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.504629135131836, loss=2.0966196060180664
I0307 05:45:38.848292 140096095368960 logging_writer.py:48] [26600] global_step=26600, grad_norm=4.724588394165039, loss=2.0987582206726074
I0307 05:46:17.483842 140096103761664 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.884553909301758, loss=1.9577980041503906
I0307 05:46:55.973139 140096095368960 logging_writer.py:48] [26800] global_step=26800, grad_norm=4.189675807952881, loss=2.0041282176971436
I0307 05:47:34.542718 140096103761664 logging_writer.py:48] [26900] global_step=26900, grad_norm=4.457539081573486, loss=2.0079355239868164
I0307 05:48:13.439087 140096095368960 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.594041585922241, loss=2.0641181468963623
I0307 05:48:52.272903 140096103761664 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.785370111465454, loss=1.917046070098877
I0307 05:49:30.843295 140096095368960 logging_writer.py:48] [27200] global_step=27200, grad_norm=4.054059982299805, loss=1.9467601776123047
I0307 05:50:09.506772 140096103761664 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.993757963180542, loss=1.974636197090149
I0307 05:50:48.272349 140096095368960 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.506082773208618, loss=2.0034942626953125
I0307 05:51:27.171080 140096103761664 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.3055057525634766, loss=1.9513344764709473
I0307 05:52:06.518250 140096095368960 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.135286808013916, loss=2.0760529041290283
I0307 05:52:45.462312 140096103761664 logging_writer.py:48] [27700] global_step=27700, grad_norm=4.394321918487549, loss=2.09393572807312
I0307 05:52:47.782196 140252175811776 spec.py:321] Evaluating on the training split.
I0307 05:53:05.359400 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 05:53:28.881412 140252175811776 spec.py:349] Evaluating on the test split.
I0307 05:53:30.675706 140252175811776 submission_runner.py:469] Time since start: 11716.31s, 	Step: 27707, 	{'train/accuracy': 0.6364795565605164, 'train/loss': 1.47107994556427, 'validation/accuracy': 0.5870000123977661, 'validation/loss': 1.7378978729248047, 'validation/num_examples': 50000, 'test/accuracy': 0.4585000276565552, 'test/loss': 2.4690916538238525, 'test/num_examples': 10000, 'score': 10769.301445007324, 'total_duration': 11716.3062915802, 'accumulated_submission_time': 10769.301445007324, 'accumulated_eval_time': 942.0799193382263, 'accumulated_logging_time': 1.9949395656585693}
I0307 05:53:30.793529 140096095368960 logging_writer.py:48] [27707] accumulated_eval_time=942.08, accumulated_logging_time=1.99494, accumulated_submission_time=10769.3, global_step=27707, preemption_count=0, score=10769.3, test/accuracy=0.4585, test/loss=2.46909, test/num_examples=10000, total_duration=11716.3, train/accuracy=0.63648, train/loss=1.47108, validation/accuracy=0.587, validation/loss=1.7379, validation/num_examples=50000
I0307 05:54:07.259186 140096103761664 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.3142781257629395, loss=1.9874767065048218
I0307 05:54:45.753418 140096095368960 logging_writer.py:48] [27900] global_step=27900, grad_norm=4.0353007316589355, loss=2.0024678707122803
I0307 05:55:24.444837 140096103761664 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.6783692836761475, loss=1.9675281047821045
I0307 05:56:03.156499 140096095368960 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.8263189792633057, loss=1.9802675247192383
I0307 05:56:42.008823 140096103761664 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.6476335525512695, loss=2.0891053676605225
I0307 05:57:20.989751 140096095368960 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.8265089988708496, loss=2.004437208175659
I0307 05:57:59.628736 140096103761664 logging_writer.py:48] [28400] global_step=28400, grad_norm=4.156492233276367, loss=2.073352098464966
I0307 05:58:38.422477 140096095368960 logging_writer.py:48] [28500] global_step=28500, grad_norm=4.269537448883057, loss=1.979151964187622
I0307 05:59:16.869562 140096103761664 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.342599630355835, loss=1.9822676181793213
I0307 05:59:56.255625 140096095368960 logging_writer.py:48] [28700] global_step=28700, grad_norm=4.783755302429199, loss=2.0119435787200928
I0307 06:00:35.018825 140096103761664 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.7838878631591797, loss=2.009605884552002
I0307 06:01:13.877219 140096095368960 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.047927141189575, loss=2.0881710052490234
I0307 06:01:52.869402 140096103761664 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.800365447998047, loss=1.9568171501159668
I0307 06:02:00.807080 140252175811776 spec.py:321] Evaluating on the training split.
I0307 06:02:16.521400 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 06:02:41.256343 140252175811776 spec.py:349] Evaluating on the test split.
I0307 06:02:43.013973 140252175811776 submission_runner.py:469] Time since start: 12268.64s, 	Step: 29022, 	{'train/accuracy': 0.6213727593421936, 'train/loss': 1.5282877683639526, 'validation/accuracy': 0.5713399648666382, 'validation/loss': 1.8057664632797241, 'validation/num_examples': 50000, 'test/accuracy': 0.45100003480911255, 'test/loss': 2.5242435932159424, 'test/num_examples': 10000, 'score': 11279.171776771545, 'total_duration': 12268.644584417343, 'accumulated_submission_time': 11279.171776771545, 'accumulated_eval_time': 984.2866439819336, 'accumulated_logging_time': 2.121225357055664}
I0307 06:02:43.093964 140096095368960 logging_writer.py:48] [29022] accumulated_eval_time=984.287, accumulated_logging_time=2.12123, accumulated_submission_time=11279.2, global_step=29022, preemption_count=0, score=11279.2, test/accuracy=0.451, test/loss=2.52424, test/num_examples=10000, total_duration=12268.6, train/accuracy=0.621373, train/loss=1.52829, validation/accuracy=0.57134, validation/loss=1.80577, validation/num_examples=50000
I0307 06:03:13.775996 140096103761664 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.8798534870147705, loss=2.076587200164795
I0307 06:03:52.263845 140096095368960 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.93730092048645, loss=2.068490743637085
I0307 06:04:31.481276 140096103761664 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.3896992206573486, loss=2.063978910446167
I0307 06:05:10.369002 140096095368960 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.5790772438049316, loss=2.0525360107421875
I0307 06:05:48.851640 140096103761664 logging_writer.py:48] [29500] global_step=29500, grad_norm=4.370936393737793, loss=1.991448163986206
I0307 06:06:27.700479 140096095368960 logging_writer.py:48] [29600] global_step=29600, grad_norm=4.118886470794678, loss=1.9353817701339722
I0307 06:07:06.502632 140096103761664 logging_writer.py:48] [29700] global_step=29700, grad_norm=4.452300548553467, loss=1.9671847820281982
I0307 06:07:45.163339 140096095368960 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.8131136894226074, loss=2.0443809032440186
I0307 06:08:23.477172 140096103761664 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.486980438232422, loss=1.8747682571411133
I0307 06:09:02.167541 140096095368960 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.9018197059631348, loss=1.951639175415039
I0307 06:09:41.276578 140096103761664 logging_writer.py:48] [30100] global_step=30100, grad_norm=4.1432318687438965, loss=1.9752856492996216
I0307 06:10:19.891815 140096095368960 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.1395325660705566, loss=2.094703197479248
I0307 06:10:58.443584 140096103761664 logging_writer.py:48] [30300] global_step=30300, grad_norm=4.048207759857178, loss=1.9945285320281982
I0307 06:11:13.403812 140252175811776 spec.py:321] Evaluating on the training split.
I0307 06:11:30.535397 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 06:11:54.076977 140252175811776 spec.py:349] Evaluating on the test split.
I0307 06:11:55.828351 140252175811776 submission_runner.py:469] Time since start: 12821.46s, 	Step: 30340, 	{'train/accuracy': 0.6427375674247742, 'train/loss': 1.4392234086990356, 'validation/accuracy': 0.5919399857521057, 'validation/loss': 1.7061538696289062, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.4611458778381348, 'test/num_examples': 10000, 'score': 11789.3055331707, 'total_duration': 12821.458944797516, 'accumulated_submission_time': 11789.3055331707, 'accumulated_eval_time': 1026.710994720459, 'accumulated_logging_time': 2.2361581325531006}
I0307 06:11:55.944134 140096095368960 logging_writer.py:48] [30340] accumulated_eval_time=1026.71, accumulated_logging_time=2.23616, accumulated_submission_time=11789.3, global_step=30340, preemption_count=0, score=11789.3, test/accuracy=0.4628, test/loss=2.46115, test/num_examples=10000, total_duration=12821.5, train/accuracy=0.642738, train/loss=1.43922, validation/accuracy=0.59194, validation/loss=1.70615, validation/num_examples=50000
I0307 06:12:19.573855 140096103761664 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.3120853900909424, loss=2.044922113418579
I0307 06:12:58.230294 140096095368960 logging_writer.py:48] [30500] global_step=30500, grad_norm=5.091617584228516, loss=2.038349151611328
I0307 06:13:37.059880 140096103761664 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.4854981899261475, loss=2.0374257564544678
I0307 06:14:15.873906 140096095368960 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.7865450382232666, loss=1.9328548908233643
I0307 06:14:54.783769 140096103761664 logging_writer.py:48] [30800] global_step=30800, grad_norm=4.7217020988464355, loss=1.9286751747131348
I0307 06:15:33.250178 140096095368960 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.9928061962127686, loss=2.0280771255493164
I0307 06:16:12.036763 140096103761664 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.4090054035186768, loss=1.9846796989440918
I0307 06:16:50.315407 140096095368960 logging_writer.py:48] [31100] global_step=31100, grad_norm=4.428305149078369, loss=1.9137637615203857
I0307 06:17:28.740877 140096103761664 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.641808032989502, loss=1.973590612411499
I0307 06:18:07.599365 140096095368960 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.7713441848754883, loss=1.8600329160690308
I0307 06:18:46.148861 140096103761664 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.3739700317382812, loss=2.1053991317749023
I0307 06:19:24.780482 140096095368960 logging_writer.py:48] [31500] global_step=31500, grad_norm=4.1081109046936035, loss=2.014051675796509
I0307 06:20:03.313644 140096103761664 logging_writer.py:48] [31600] global_step=31600, grad_norm=5.445228099822998, loss=2.1149449348449707
I0307 06:20:25.961983 140252175811776 spec.py:321] Evaluating on the training split.
I0307 06:20:41.244838 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 06:21:03.791182 140252175811776 spec.py:349] Evaluating on the test split.
I0307 06:21:05.543762 140252175811776 submission_runner.py:469] Time since start: 13371.17s, 	Step: 31660, 	{'train/accuracy': 0.6451091766357422, 'train/loss': 1.4213815927505493, 'validation/accuracy': 0.5963599681854248, 'validation/loss': 1.692355751991272, 'validation/num_examples': 50000, 'test/accuracy': 0.4635000228881836, 'test/loss': 2.4634056091308594, 'test/num_examples': 10000, 'score': 12299.138736248016, 'total_duration': 13371.174358844757, 'accumulated_submission_time': 12299.138736248016, 'accumulated_eval_time': 1066.2925863265991, 'accumulated_logging_time': 2.3966000080108643}
I0307 06:21:05.628846 140096095368960 logging_writer.py:48] [31660] accumulated_eval_time=1066.29, accumulated_logging_time=2.3966, accumulated_submission_time=12299.1, global_step=31660, preemption_count=0, score=12299.1, test/accuracy=0.4635, test/loss=2.46341, test/num_examples=10000, total_duration=13371.2, train/accuracy=0.645109, train/loss=1.42138, validation/accuracy=0.59636, validation/loss=1.69236, validation/num_examples=50000
I0307 06:21:21.652128 140096103761664 logging_writer.py:48] [31700] global_step=31700, grad_norm=4.110970497131348, loss=2.0365591049194336
I0307 06:22:00.153063 140096095368960 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.927591562271118, loss=2.01182222366333
I0307 06:22:38.968253 140096103761664 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.884596109390259, loss=1.936678171157837
I0307 06:23:17.864569 140096095368960 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.310509443283081, loss=1.962153673171997
I0307 06:23:56.657009 140096103761664 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.7880642414093018, loss=1.9926421642303467
I0307 06:24:35.902930 140096095368960 logging_writer.py:48] [32200] global_step=32200, grad_norm=4.273075103759766, loss=1.9486547708511353
I0307 06:25:14.770601 140096103761664 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.597747564315796, loss=1.9472644329071045
I0307 06:25:53.464573 140096095368960 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.292983055114746, loss=2.049156904220581
I0307 06:26:31.860962 140096103761664 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.4063284397125244, loss=1.9798038005828857
I0307 06:27:10.828454 140096095368960 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.8381435871124268, loss=2.010634183883667
I0307 06:27:50.026891 140096103761664 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.8035342693328857, loss=2.0604405403137207
I0307 06:28:28.924449 140096095368960 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.422670841217041, loss=2.0050973892211914
I0307 06:29:07.546895 140096103761664 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.9484946727752686, loss=1.9628024101257324
I0307 06:29:35.777674 140252175811776 spec.py:321] Evaluating on the training split.
I0307 06:29:48.787749 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 06:30:14.501174 140252175811776 spec.py:349] Evaluating on the test split.
I0307 06:30:16.263576 140252175811776 submission_runner.py:469] Time since start: 13921.89s, 	Step: 32974, 	{'train/accuracy': 0.6282286047935486, 'train/loss': 1.5148309469223022, 'validation/accuracy': 0.5799799561500549, 'validation/loss': 1.7580074071884155, 'validation/num_examples': 50000, 'test/accuracy': 0.45670002698898315, 'test/loss': 2.4863736629486084, 'test/num_examples': 10000, 'score': 12809.105134248734, 'total_duration': 13921.894159793854, 'accumulated_submission_time': 12809.105134248734, 'accumulated_eval_time': 1106.778297662735, 'accumulated_logging_time': 2.527034282684326}
I0307 06:30:16.365073 140096095368960 logging_writer.py:48] [32974] accumulated_eval_time=1106.78, accumulated_logging_time=2.52703, accumulated_submission_time=12809.1, global_step=32974, preemption_count=0, score=12809.1, test/accuracy=0.4567, test/loss=2.48637, test/num_examples=10000, total_duration=13921.9, train/accuracy=0.628229, train/loss=1.51483, validation/accuracy=0.57998, validation/loss=1.75801, validation/num_examples=50000
I0307 06:30:26.820840 140096103761664 logging_writer.py:48] [33000] global_step=33000, grad_norm=4.04637336730957, loss=2.030033588409424
I0307 06:31:05.549897 140096095368960 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.4426333904266357, loss=1.9459788799285889
I0307 06:31:43.916113 140096103761664 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.1248481273651123, loss=2.047137498855591
I0307 06:32:22.753294 140096095368960 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.0998406410217285, loss=1.8708579540252686
I0307 06:33:02.034966 140096103761664 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.410027027130127, loss=2.012666940689087
I0307 06:33:40.717333 140096095368960 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.7732198238372803, loss=1.9205328226089478
I0307 06:34:19.591324 140096103761664 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.5387699604034424, loss=1.9090251922607422
I0307 06:34:58.761064 140096095368960 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.947359800338745, loss=2.0380702018737793
I0307 06:35:37.657143 140096103761664 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.2558717727661133, loss=2.0657858848571777
I0307 06:36:16.864914 140096095368960 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.490602970123291, loss=1.9739638566970825
I0307 06:36:55.533831 140096103761664 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.4505228996276855, loss=2.082209348678589
I0307 06:37:33.950712 140096095368960 logging_writer.py:48] [34100] global_step=34100, grad_norm=4.147697925567627, loss=1.9790143966674805
I0307 06:38:12.744457 140096103761664 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.613450288772583, loss=2.0316004753112793
I0307 06:38:46.571982 140252175811776 spec.py:321] Evaluating on the training split.
I0307 06:38:59.264912 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 06:39:25.756428 140252175811776 spec.py:349] Evaluating on the test split.
I0307 06:39:27.506268 140252175811776 submission_runner.py:469] Time since start: 14473.14s, 	Step: 34288, 	{'train/accuracy': 0.6402662396430969, 'train/loss': 1.4535552263259888, 'validation/accuracy': 0.590179979801178, 'validation/loss': 1.7174993753433228, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.4660301208496094, 'test/num_examples': 10000, 'score': 13319.156252861023, 'total_duration': 14473.136858940125, 'accumulated_submission_time': 13319.156252861023, 'accumulated_eval_time': 1147.7123908996582, 'accumulated_logging_time': 2.6464807987213135}
I0307 06:39:27.594073 140096095368960 logging_writer.py:48] [34288] accumulated_eval_time=1147.71, accumulated_logging_time=2.64648, accumulated_submission_time=13319.2, global_step=34288, preemption_count=0, score=13319.2, test/accuracy=0.4689, test/loss=2.46603, test/num_examples=10000, total_duration=14473.1, train/accuracy=0.640266, train/loss=1.45356, validation/accuracy=0.59018, validation/loss=1.7175, validation/num_examples=50000
I0307 06:39:32.668438 140096103761664 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.37080454826355, loss=2.0520806312561035
I0307 06:40:11.173296 140096095368960 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.82479190826416, loss=1.8853156566619873
I0307 06:40:50.052458 140096103761664 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.940854549407959, loss=1.8868153095245361
I0307 06:41:28.730421 140096095368960 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.384657144546509, loss=1.865480899810791
I0307 06:42:07.613336 140096103761664 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.2264089584350586, loss=1.8911494016647339
I0307 06:42:46.261374 140096095368960 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.44966459274292, loss=1.9619534015655518
I0307 06:43:24.827538 140096103761664 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.7332088947296143, loss=1.9669886827468872
I0307 06:44:03.668712 140096095368960 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.525559663772583, loss=1.9587104320526123
I0307 06:44:42.850672 140096103761664 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.4560372829437256, loss=1.9802331924438477
I0307 06:45:21.529741 140096095368960 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.6510121822357178, loss=1.9797260761260986
I0307 06:46:00.416317 140096103761664 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.4460864067077637, loss=1.9603853225708008
I0307 06:46:39.296885 140096095368960 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.5355920791625977, loss=1.9181487560272217
I0307 06:47:18.205208 140096103761664 logging_writer.py:48] [35500] global_step=35500, grad_norm=4.189537525177002, loss=2.001500368118286
I0307 06:47:57.011330 140096095368960 logging_writer.py:48] [35600] global_step=35600, grad_norm=4.798125267028809, loss=1.9734537601470947
I0307 06:47:57.802038 140252175811776 spec.py:321] Evaluating on the training split.
I0307 06:48:10.655513 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 06:48:39.574155 140252175811776 spec.py:349] Evaluating on the test split.
I0307 06:48:41.331074 140252175811776 submission_runner.py:469] Time since start: 15026.96s, 	Step: 35603, 	{'train/accuracy': 0.6328921914100647, 'train/loss': 1.498120903968811, 'validation/accuracy': 0.5820800065994263, 'validation/loss': 1.7467761039733887, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.497425079345703, 'test/num_examples': 10000, 'score': 13829.203861951828, 'total_duration': 15026.96167421341, 'accumulated_submission_time': 13829.203861951828, 'accumulated_eval_time': 1191.2412407398224, 'accumulated_logging_time': 2.7574026584625244}
I0307 06:48:41.415308 140096103761664 logging_writer.py:48] [35603] accumulated_eval_time=1191.24, accumulated_logging_time=2.7574, accumulated_submission_time=13829.2, global_step=35603, preemption_count=0, score=13829.2, test/accuracy=0.4579, test/loss=2.49743, test/num_examples=10000, total_duration=15027, train/accuracy=0.632892, train/loss=1.49812, validation/accuracy=0.58208, validation/loss=1.74678, validation/num_examples=50000
I0307 06:49:19.572223 140096095368960 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.0455129146575928, loss=1.9306424856185913
I0307 06:49:58.842418 140096103761664 logging_writer.py:48] [35800] global_step=35800, grad_norm=4.091809272766113, loss=1.9923453330993652
I0307 06:50:37.502263 140096095368960 logging_writer.py:48] [35900] global_step=35900, grad_norm=4.4886088371276855, loss=1.8564541339874268
I0307 06:51:16.639164 140096103761664 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.652235746383667, loss=1.8592942953109741
I0307 06:51:55.244745 140096095368960 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.5914392471313477, loss=1.9174195528030396
I0307 06:52:33.828152 140096103761664 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.7763001918792725, loss=1.8945218324661255
I0307 06:53:12.975077 140096095368960 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.2679100036621094, loss=1.9045871496200562
I0307 06:53:51.461093 140096103761664 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.6714894771575928, loss=1.9806571006774902
I0307 06:54:30.590800 140096095368960 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.491999864578247, loss=2.0137367248535156
I0307 06:55:09.267047 140096103761664 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.7406370639801025, loss=2.0063517093658447
I0307 06:55:48.445400 140096095368960 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.067913293838501, loss=1.8818392753601074
I0307 06:56:27.157328 140096103761664 logging_writer.py:48] [36800] global_step=36800, grad_norm=5.531944751739502, loss=2.0170912742614746
I0307 06:57:06.277375 140096095368960 logging_writer.py:48] [36900] global_step=36900, grad_norm=4.757888317108154, loss=1.9915505647659302
I0307 06:57:11.649148 140252175811776 spec.py:321] Evaluating on the training split.
I0307 06:57:24.291348 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 06:57:46.816363 140252175811776 spec.py:349] Evaluating on the test split.
I0307 06:57:48.600004 140252175811776 submission_runner.py:469] Time since start: 15574.23s, 	Step: 36915, 	{'train/accuracy': 0.6465640664100647, 'train/loss': 1.417842149734497, 'validation/accuracy': 0.594219982624054, 'validation/loss': 1.6937551498413086, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.435868501663208, 'test/num_examples': 10000, 'score': 14339.268918037415, 'total_duration': 15574.23060297966, 'accumulated_submission_time': 14339.268918037415, 'accumulated_eval_time': 1228.191910982132, 'accumulated_logging_time': 2.87359619140625}
I0307 06:57:48.695539 140096103761664 logging_writer.py:48] [36915] accumulated_eval_time=1228.19, accumulated_logging_time=2.8736, accumulated_submission_time=14339.3, global_step=36915, preemption_count=0, score=14339.3, test/accuracy=0.468, test/loss=2.43587, test/num_examples=10000, total_duration=15574.2, train/accuracy=0.646564, train/loss=1.41784, validation/accuracy=0.59422, validation/loss=1.69376, validation/num_examples=50000
I0307 06:58:22.069654 140096095368960 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.6699116230010986, loss=2.0325613021850586
I0307 06:59:00.584757 140096103761664 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.654788017272949, loss=1.9467167854309082
I0307 06:59:39.476246 140096095368960 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.841226816177368, loss=1.933225154876709
I0307 07:00:18.486551 140096103761664 logging_writer.py:48] [37300] global_step=37300, grad_norm=4.385195732116699, loss=1.9314223527908325
I0307 07:00:57.004823 140096095368960 logging_writer.py:48] [37400] global_step=37400, grad_norm=4.001494407653809, loss=1.9026234149932861
I0307 07:01:35.317332 140096103761664 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.5679214000701904, loss=2.036707878112793
I0307 07:02:14.826975 140096095368960 logging_writer.py:48] [37600] global_step=37600, grad_norm=4.444268226623535, loss=1.874884843826294
I0307 07:02:53.600657 140096103761664 logging_writer.py:48] [37700] global_step=37700, grad_norm=4.23024845123291, loss=2.0359556674957275
I0307 07:03:32.662281 140096095368960 logging_writer.py:48] [37800] global_step=37800, grad_norm=4.405597686767578, loss=1.9548485279083252
I0307 07:04:11.155920 140096103761664 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.4443459510803223, loss=1.9518568515777588
I0307 07:04:49.690563 140096095368960 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.5174736976623535, loss=1.8405108451843262
I0307 07:05:28.503280 140096103761664 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.319884777069092, loss=1.8800475597381592
I0307 07:06:07.588155 140096095368960 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.527533531188965, loss=2.0101966857910156
I0307 07:06:18.683701 140252175811776 spec.py:321] Evaluating on the training split.
I0307 07:06:31.297571 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 07:07:00.300989 140252175811776 spec.py:349] Evaluating on the test split.
I0307 07:07:02.066347 140252175811776 submission_runner.py:469] Time since start: 16127.70s, 	Step: 38230, 	{'train/accuracy': 0.6446309089660645, 'train/loss': 1.4139775037765503, 'validation/accuracy': 0.5981400012969971, 'validation/loss': 1.6679935455322266, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.380847692489624, 'test/num_examples': 10000, 'score': 14849.096669197083, 'total_duration': 16127.696937084198, 'accumulated_submission_time': 14849.096669197083, 'accumulated_eval_time': 1271.5743641853333, 'accumulated_logging_time': 2.988579034805298}
I0307 07:07:02.134384 140096103761664 logging_writer.py:48] [38230] accumulated_eval_time=1271.57, accumulated_logging_time=2.98858, accumulated_submission_time=14849.1, global_step=38230, preemption_count=0, score=14849.1, test/accuracy=0.4792, test/loss=2.38085, test/num_examples=10000, total_duration=16127.7, train/accuracy=0.644631, train/loss=1.41398, validation/accuracy=0.59814, validation/loss=1.66799, validation/num_examples=50000
I0307 07:07:29.495410 140096095368960 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.372561454772949, loss=2.0007100105285645
I0307 07:08:08.484717 140096103761664 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.680859327316284, loss=1.8632419109344482
I0307 07:08:47.225892 140096095368960 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.7382051944732666, loss=1.8602944612503052
I0307 07:09:26.106889 140096103761664 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.839770555496216, loss=1.841406226158142
I0307 07:10:04.809144 140096095368960 logging_writer.py:48] [38700] global_step=38700, grad_norm=4.539191246032715, loss=2.0231921672821045
I0307 07:10:43.501800 140096103761664 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.980966091156006, loss=1.9705543518066406
I0307 07:11:22.304199 140096095368960 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.418766975402832, loss=1.829368233680725
I0307 07:12:00.987557 140096103761664 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.7551095485687256, loss=2.0315022468566895
I0307 07:12:39.738087 140096095368960 logging_writer.py:48] [39100] global_step=39100, grad_norm=4.573268890380859, loss=1.885926604270935
I0307 07:13:18.600279 140096103761664 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.8373117446899414, loss=1.921877384185791
I0307 07:13:57.407262 140096095368960 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.604816198348999, loss=1.96210515499115
I0307 07:14:36.203906 140096103761664 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.1406922340393066, loss=1.9072459936141968
I0307 07:15:15.107776 140096095368960 logging_writer.py:48] [39500] global_step=39500, grad_norm=4.025667667388916, loss=1.7821141481399536
I0307 07:15:32.248790 140252175811776 spec.py:321] Evaluating on the training split.
I0307 07:15:45.208449 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 07:16:10.620640 140252175811776 spec.py:349] Evaluating on the test split.
I0307 07:16:12.373849 140252175811776 submission_runner.py:469] Time since start: 16678.00s, 	Step: 39545, 	{'train/accuracy': 0.6444315910339355, 'train/loss': 1.4265717267990112, 'validation/accuracy': 0.5979399681091309, 'validation/loss': 1.679437279701233, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.3826851844787598, 'test/num_examples': 10000, 'score': 15359.023358106613, 'total_duration': 16678.004452466965, 'accumulated_submission_time': 15359.023358106613, 'accumulated_eval_time': 1311.6992518901825, 'accumulated_logging_time': 3.10727858543396}
I0307 07:16:12.521058 140096103761664 logging_writer.py:48] [39545] accumulated_eval_time=1311.7, accumulated_logging_time=3.10728, accumulated_submission_time=15359, global_step=39545, preemption_count=0, score=15359, test/accuracy=0.4689, test/loss=2.38269, test/num_examples=10000, total_duration=16678, train/accuracy=0.644432, train/loss=1.42657, validation/accuracy=0.59794, validation/loss=1.67944, validation/num_examples=50000
I0307 07:16:34.343652 140096095368960 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.7190511226654053, loss=1.9034491777420044
I0307 07:17:13.680888 140096103761664 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.311093330383301, loss=1.9137314558029175
I0307 07:17:52.739927 140096095368960 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.394740343093872, loss=1.932775855064392
I0307 07:18:31.469570 140096103761664 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.136063098907471, loss=1.8106433153152466
I0307 07:19:10.290247 140096095368960 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.31337833404541, loss=1.8240785598754883
I0307 07:19:49.566189 140096103761664 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.659174680709839, loss=1.9307758808135986
I0307 07:20:28.571415 140096095368960 logging_writer.py:48] [40200] global_step=40200, grad_norm=4.200370788574219, loss=2.105266809463501
I0307 07:21:07.140976 140096103761664 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.8022334575653076, loss=1.835573434829712
I0307 07:21:46.154129 140096095368960 logging_writer.py:48] [40400] global_step=40400, grad_norm=4.01449728012085, loss=1.9434928894042969
I0307 07:22:24.667661 140096103761664 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.262532949447632, loss=1.9381083250045776
I0307 07:23:03.249947 140096095368960 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.847407579421997, loss=1.9051111936569214
I0307 07:23:42.161648 140096103761664 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.383681297302246, loss=1.93886399269104
I0307 07:24:20.822421 140096095368960 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.735785484313965, loss=1.8319050073623657
I0307 07:24:42.404278 140252175811776 spec.py:321] Evaluating on the training split.
I0307 07:24:54.816210 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 07:25:18.166135 140252175811776 spec.py:349] Evaluating on the test split.
I0307 07:25:19.952048 140252175811776 submission_runner.py:469] Time since start: 17225.58s, 	Step: 40857, 	{'train/accuracy': 0.6428372263908386, 'train/loss': 1.4379843473434448, 'validation/accuracy': 0.5939800143241882, 'validation/loss': 1.694046974182129, 'validation/num_examples': 50000, 'test/accuracy': 0.46790000796318054, 'test/loss': 2.4174466133117676, 'test/num_examples': 10000, 'score': 15868.74512887001, 'total_duration': 17225.582670211792, 'accumulated_submission_time': 15868.74512887001, 'accumulated_eval_time': 1349.246863603592, 'accumulated_logging_time': 3.2763900756835938}
I0307 07:25:20.036363 140096103761664 logging_writer.py:48] [40857] accumulated_eval_time=1349.25, accumulated_logging_time=3.27639, accumulated_submission_time=15868.7, global_step=40857, preemption_count=0, score=15868.7, test/accuracy=0.4679, test/loss=2.41745, test/num_examples=10000, total_duration=17225.6, train/accuracy=0.642837, train/loss=1.43798, validation/accuracy=0.59398, validation/loss=1.69405, validation/num_examples=50000
I0307 07:25:37.304093 140096095368960 logging_writer.py:48] [40900] global_step=40900, grad_norm=4.152295112609863, loss=2.037140130996704
I0307 07:26:16.223089 140096103761664 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.6896517276763916, loss=1.9187899827957153
I0307 07:26:55.036905 140096095368960 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.886528491973877, loss=1.9337071180343628
I0307 07:27:33.536375 140096103761664 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.9595799446105957, loss=1.8709181547164917
I0307 07:28:13.032192 140096095368960 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.862316846847534, loss=1.958739995956421
I0307 07:28:51.824842 140096103761664 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.871652603149414, loss=1.9659723043441772
I0307 07:29:30.678147 140096095368960 logging_writer.py:48] [41500] global_step=41500, grad_norm=4.051478862762451, loss=1.9590123891830444
I0307 07:30:09.472132 140096103761664 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.056278705596924, loss=1.9527944326400757
I0307 07:30:48.227628 140096095368960 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.4860117435455322, loss=1.920215368270874
I0307 07:31:26.832148 140096103761664 logging_writer.py:48] [41800] global_step=41800, grad_norm=4.027544975280762, loss=1.9652371406555176
I0307 07:32:05.906627 140096095368960 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.5949361324310303, loss=1.9627958536148071
I0307 07:32:44.510669 140096103761664 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.2982892990112305, loss=1.926428198814392
I0307 07:33:23.523743 140096095368960 logging_writer.py:48] [42100] global_step=42100, grad_norm=4.019391059875488, loss=1.9088529348373413
I0307 07:33:49.982903 140252175811776 spec.py:321] Evaluating on the training split.
I0307 07:34:02.108034 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 07:34:26.056973 140252175811776 spec.py:349] Evaluating on the test split.
I0307 07:34:27.816856 140252175811776 submission_runner.py:469] Time since start: 17773.45s, 	Step: 42169, 	{'train/accuracy': 0.6518255472183228, 'train/loss': 1.3834835290908813, 'validation/accuracy': 0.5985400080680847, 'validation/loss': 1.6672126054763794, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.3768794536590576, 'test/num_examples': 10000, 'score': 16378.515176057816, 'total_duration': 17773.447425365448, 'accumulated_submission_time': 16378.515176057816, 'accumulated_eval_time': 1387.080607175827, 'accumulated_logging_time': 3.399587631225586}
I0307 07:34:27.872010 140096103761664 logging_writer.py:48] [42169] accumulated_eval_time=1387.08, accumulated_logging_time=3.39959, accumulated_submission_time=16378.5, global_step=42169, preemption_count=0, score=16378.5, test/accuracy=0.4816, test/loss=2.37688, test/num_examples=10000, total_duration=17773.4, train/accuracy=0.651826, train/loss=1.38348, validation/accuracy=0.59854, validation/loss=1.66721, validation/num_examples=50000
I0307 07:34:40.256507 140096095368960 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.9100327491760254, loss=2.019124746322632
I0307 07:35:19.093950 140096103761664 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.9787802696228027, loss=1.9349327087402344
I0307 07:35:58.102294 140096095368960 logging_writer.py:48] [42400] global_step=42400, grad_norm=4.247544765472412, loss=1.8529269695281982
I0307 07:36:38.381735 140096103761664 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.6285078525543213, loss=2.00110125541687
I0307 07:37:17.618710 140096095368960 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.4404635429382324, loss=1.8416675329208374
I0307 07:37:56.541304 140096103761664 logging_writer.py:48] [42700] global_step=42700, grad_norm=4.262524604797363, loss=1.915542483329773
I0307 07:38:35.171772 140096095368960 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.687901735305786, loss=1.9997130632400513
I0307 07:39:14.072525 140096103761664 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.3075270652771, loss=1.9611010551452637
I0307 07:39:53.063612 140096095368960 logging_writer.py:48] [43000] global_step=43000, grad_norm=4.0803446769714355, loss=1.992090106010437
I0307 07:40:31.608633 140096103761664 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.572378635406494, loss=2.0062856674194336
I0307 07:41:10.484216 140096095368960 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.079710006713867, loss=1.8435235023498535
I0307 07:41:48.979960 140096103761664 logging_writer.py:48] [43300] global_step=43300, grad_norm=4.062389373779297, loss=2.022902488708496
I0307 07:42:27.739667 140096095368960 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.398677110671997, loss=1.9413278102874756
I0307 07:42:58.013909 140252175811776 spec.py:321] Evaluating on the training split.
I0307 07:43:10.841403 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 07:43:37.876137 140252175811776 spec.py:349] Evaluating on the test split.
I0307 07:43:39.619676 140252175811776 submission_runner.py:469] Time since start: 18325.25s, 	Step: 43480, 	{'train/accuracy': 0.6549346446990967, 'train/loss': 1.377758502960205, 'validation/accuracy': 0.602899968624115, 'validation/loss': 1.6348503828048706, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.3226985931396484, 'test/num_examples': 10000, 'score': 16888.492291212082, 'total_duration': 18325.250215291977, 'accumulated_submission_time': 16888.492291212082, 'accumulated_eval_time': 1428.686133146286, 'accumulated_logging_time': 3.4777886867523193}
I0307 07:43:39.686484 140096103761664 logging_writer.py:48] [43480] accumulated_eval_time=1428.69, accumulated_logging_time=3.47779, accumulated_submission_time=16888.5, global_step=43480, preemption_count=0, score=16888.5, test/accuracy=0.4827, test/loss=2.3227, test/num_examples=10000, total_duration=18325.3, train/accuracy=0.654935, train/loss=1.37776, validation/accuracy=0.6029, validation/loss=1.63485, validation/num_examples=50000
I0307 07:43:47.892901 140096095368960 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.6671383380889893, loss=1.9798529148101807
I0307 07:44:26.977907 140096103761664 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.364119291305542, loss=2.0595014095306396
I0307 07:45:05.872700 140096095368960 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.251013994216919, loss=1.864223837852478
I0307 07:45:44.958788 140096103761664 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.8752920627593994, loss=1.841482162475586
I0307 07:46:23.274801 140096095368960 logging_writer.py:48] [43900] global_step=43900, grad_norm=4.164290428161621, loss=1.7557123899459839
I0307 07:47:01.607753 140096103761664 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.9330904483795166, loss=2.071820020675659
I0307 07:47:40.440289 140096095368960 logging_writer.py:48] [44100] global_step=44100, grad_norm=4.029764652252197, loss=2.0188374519348145
I0307 07:48:19.455557 140096103761664 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.641395330429077, loss=1.8649243116378784
I0307 07:48:58.239740 140096095368960 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.1729941368103027, loss=2.0207955837249756
I0307 07:49:36.885087 140096103761664 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.7012979984283447, loss=1.9538220167160034
I0307 07:50:15.603605 140096095368960 logging_writer.py:48] [44500] global_step=44500, grad_norm=4.242228984832764, loss=1.7771735191345215
I0307 07:50:54.593162 140096103761664 logging_writer.py:48] [44600] global_step=44600, grad_norm=4.204176902770996, loss=1.8540294170379639
I0307 07:51:33.553877 140096095368960 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.637576103210449, loss=1.8285984992980957
I0307 07:52:09.639310 140252175811776 spec.py:321] Evaluating on the training split.
I0307 07:52:22.264610 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 07:52:49.187013 140252175811776 spec.py:349] Evaluating on the test split.
I0307 07:52:50.933663 140252175811776 submission_runner.py:469] Time since start: 18876.56s, 	Step: 44794, 	{'train/accuracy': 0.6507692933082581, 'train/loss': 1.39634370803833, 'validation/accuracy': 0.6025999784469604, 'validation/loss': 1.6475313901901245, 'validation/num_examples': 50000, 'test/accuracy': 0.4798000156879425, 'test/loss': 2.3794052600860596, 'test/num_examples': 10000, 'score': 17398.270622491837, 'total_duration': 18876.564233779907, 'accumulated_submission_time': 17398.270622491837, 'accumulated_eval_time': 1469.9802823066711, 'accumulated_logging_time': 3.58227276802063}
I0307 07:52:51.007416 140096103761664 logging_writer.py:48] [44794] accumulated_eval_time=1469.98, accumulated_logging_time=3.58227, accumulated_submission_time=17398.3, global_step=44794, preemption_count=0, score=17398.3, test/accuracy=0.4798, test/loss=2.37941, test/num_examples=10000, total_duration=18876.6, train/accuracy=0.650769, train/loss=1.39634, validation/accuracy=0.6026, validation/loss=1.64753, validation/num_examples=50000
I0307 07:52:53.814552 140096095368960 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.424787759780884, loss=1.904350996017456
I0307 07:53:31.872236 140096103761664 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.5801823139190674, loss=1.8855630159378052
I0307 07:54:10.506215 140096095368960 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.464355707168579, loss=1.9540629386901855
I0307 07:54:49.960964 140096103761664 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.566885471343994, loss=1.8216160535812378
I0307 07:55:28.300721 140096095368960 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.6688148975372314, loss=1.9317814111709595
I0307 07:56:06.849303 140096103761664 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.222968578338623, loss=1.8309950828552246
I0307 07:56:45.626384 140096095368960 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.3903820514678955, loss=1.8918735980987549
I0307 07:57:23.810093 140096103761664 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.2083659172058105, loss=1.6957039833068848
I0307 07:58:02.519768 140096095368960 logging_writer.py:48] [45600] global_step=45600, grad_norm=4.566980838775635, loss=1.9149004220962524
I0307 07:58:41.449479 140096103761664 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.7079343795776367, loss=1.8042417764663696
I0307 07:59:20.112427 140096095368960 logging_writer.py:48] [45800] global_step=45800, grad_norm=4.280133247375488, loss=1.8256564140319824
I0307 07:59:59.092223 140096103761664 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.911716938018799, loss=1.8198715448379517
I0307 08:00:38.186792 140096095368960 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.69596266746521, loss=1.8806958198547363
I0307 08:01:16.892091 140096103761664 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.2930350303649902, loss=1.8923625946044922
I0307 08:01:21.127324 140252175811776 spec.py:321] Evaluating on the training split.
I0307 08:01:32.793383 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 08:01:59.183673 140252175811776 spec.py:349] Evaluating on the test split.
I0307 08:02:00.932414 140252175811776 submission_runner.py:469] Time since start: 19426.56s, 	Step: 46112, 	{'train/accuracy': 0.6620097160339355, 'train/loss': 1.3622193336486816, 'validation/accuracy': 0.6071000099182129, 'validation/loss': 1.631041407585144, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.352956771850586, 'test/num_examples': 10000, 'score': 17908.219122886658, 'total_duration': 19426.56300163269, 'accumulated_submission_time': 17908.219122886658, 'accumulated_eval_time': 1509.7851781845093, 'accumulated_logging_time': 3.686852216720581}
I0307 08:02:00.989860 140096095368960 logging_writer.py:48] [46112] accumulated_eval_time=1509.79, accumulated_logging_time=3.68685, accumulated_submission_time=17908.2, global_step=46112, preemption_count=0, score=17908.2, test/accuracy=0.4862, test/loss=2.35296, test/num_examples=10000, total_duration=19426.6, train/accuracy=0.66201, train/loss=1.36222, validation/accuracy=0.6071, validation/loss=1.63104, validation/num_examples=50000
I0307 08:02:35.623965 140096103761664 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.343571901321411, loss=2.0174882411956787
I0307 08:03:15.052693 140096095368960 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.9327774047851562, loss=1.936115026473999
I0307 08:03:54.001286 140096103761664 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.7635648250579834, loss=1.8092628717422485
I0307 08:04:31.952564 140096095368960 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.3537797927856445, loss=1.7642261981964111
I0307 08:05:10.614234 140096103761664 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.2043371200561523, loss=1.8917171955108643
I0307 08:05:49.531780 140096095368960 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.0577871799468994, loss=1.8364760875701904
I0307 08:06:27.907772 140096103761664 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.7905030250549316, loss=1.898531198501587
I0307 08:07:06.548456 140096095368960 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.678959608078003, loss=1.899614691734314
I0307 08:07:45.528187 140096103761664 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.253894567489624, loss=1.887817621231079
I0307 08:08:24.491887 140096095368960 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.4553675651550293, loss=1.9342811107635498
I0307 08:09:03.633548 140096103761664 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.4834299087524414, loss=2.0841140747070312
I0307 08:09:42.758257 140096095368960 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.9977452754974365, loss=1.9401748180389404
I0307 08:10:21.868325 140096103761664 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.2192609310150146, loss=1.8506581783294678
I0307 08:10:31.142168 140252175811776 spec.py:321] Evaluating on the training split.
I0307 08:10:43.479315 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 08:11:06.827269 140252175811776 spec.py:349] Evaluating on the test split.
I0307 08:11:08.628767 140252175811776 submission_runner.py:469] Time since start: 19974.26s, 	Step: 47425, 	{'train/accuracy': 0.6451291441917419, 'train/loss': 1.4150234460830688, 'validation/accuracy': 0.6019799709320068, 'validation/loss': 1.6587527990341187, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.3831734657287598, 'test/num_examples': 10000, 'score': 18418.20294713974, 'total_duration': 19974.259385347366, 'accumulated_submission_time': 18418.20294713974, 'accumulated_eval_time': 1547.2716102600098, 'accumulated_logging_time': 3.7745580673217773}
I0307 08:11:08.716286 140096095368960 logging_writer.py:48] [47425] accumulated_eval_time=1547.27, accumulated_logging_time=3.77456, accumulated_submission_time=18418.2, global_step=47425, preemption_count=0, score=18418.2, test/accuracy=0.4763, test/loss=2.38317, test/num_examples=10000, total_duration=19974.3, train/accuracy=0.645129, train/loss=1.41502, validation/accuracy=0.60198, validation/loss=1.65875, validation/num_examples=50000
I0307 08:11:38.512387 140096103761664 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.7946665287017822, loss=1.913051724433899
I0307 08:12:17.541298 140096095368960 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.5097873210906982, loss=1.902964472770691
I0307 08:12:56.307927 140096103761664 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.995255470275879, loss=1.9191923141479492
I0307 08:13:35.432395 140096095368960 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.6262619495391846, loss=1.8932334184646606
I0307 08:14:14.056772 140096103761664 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.7378039360046387, loss=1.8938413858413696
I0307 08:14:53.060911 140096095368960 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.152202606201172, loss=1.8660699129104614
I0307 08:15:32.012460 140096103761664 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.8584909439086914, loss=1.9358139038085938
I0307 08:16:10.991866 140096095368960 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.534618616104126, loss=1.7613329887390137
I0307 08:16:49.465825 140096103761664 logging_writer.py:48] [48300] global_step=48300, grad_norm=4.22642183303833, loss=1.9808099269866943
I0307 08:17:28.071086 140096095368960 logging_writer.py:48] [48400] global_step=48400, grad_norm=4.471064567565918, loss=1.9451181888580322
I0307 08:18:07.554513 140096103761664 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.922206163406372, loss=1.9222543239593506
I0307 08:18:46.947668 140096095368960 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.899254560470581, loss=1.8943670988082886
I0307 08:19:25.657748 140096103761664 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.9921836853027344, loss=1.8427780866622925
I0307 08:19:38.973760 140252175811776 spec.py:321] Evaluating on the training split.
I0307 08:19:51.687192 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 08:20:16.977941 140252175811776 spec.py:349] Evaluating on the test split.
I0307 08:20:18.771083 140252175811776 submission_runner.py:469] Time since start: 20524.40s, 	Step: 48735, 	{'train/accuracy': 0.6591398119926453, 'train/loss': 1.3654074668884277, 'validation/accuracy': 0.6089400053024292, 'validation/loss': 1.624113917350769, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.348018169403076, 'test/num_examples': 10000, 'score': 18928.298800230026, 'total_duration': 20524.401693820953, 'accumulated_submission_time': 18928.298800230026, 'accumulated_eval_time': 1587.0687594413757, 'accumulated_logging_time': 3.8860855102539062}
I0307 08:20:18.838456 140096095368960 logging_writer.py:48] [48735] accumulated_eval_time=1587.07, accumulated_logging_time=3.88609, accumulated_submission_time=18928.3, global_step=48735, preemption_count=0, score=18928.3, test/accuracy=0.4872, test/loss=2.34802, test/num_examples=10000, total_duration=20524.4, train/accuracy=0.65914, train/loss=1.36541, validation/accuracy=0.60894, validation/loss=1.62411, validation/num_examples=50000
I0307 08:20:44.833269 140096103761664 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.7929675579071045, loss=1.7996759414672852
I0307 08:21:24.995883 140096095368960 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.81766414642334, loss=1.9239625930786133
I0307 08:22:04.896731 140096103761664 logging_writer.py:48] [49000] global_step=49000, grad_norm=4.182183742523193, loss=1.9048540592193604
I0307 08:22:43.705306 140096095368960 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.8943610191345215, loss=1.8268977403640747
I0307 08:23:22.484405 140096103761664 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.6280646324157715, loss=1.9577256441116333
I0307 08:24:01.279252 140096095368960 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.5439441204071045, loss=1.8441522121429443
I0307 08:24:39.847582 140096103761664 logging_writer.py:48] [49400] global_step=49400, grad_norm=4.044929027557373, loss=1.9241386651992798
I0307 08:25:18.488230 140096095368960 logging_writer.py:48] [49500] global_step=49500, grad_norm=4.085222244262695, loss=1.8988553285598755
I0307 08:25:57.245641 140096103761664 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.4468226432800293, loss=1.8057230710983276
I0307 08:26:36.401093 140096095368960 logging_writer.py:48] [49700] global_step=49700, grad_norm=4.023153781890869, loss=1.9590442180633545
I0307 08:27:15.411483 140096103761664 logging_writer.py:48] [49800] global_step=49800, grad_norm=4.334049701690674, loss=1.9256141185760498
I0307 08:27:54.456359 140096095368960 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.5515756607055664, loss=1.9708002805709839
I0307 08:28:33.616778 140096103761664 logging_writer.py:48] [50000] global_step=50000, grad_norm=4.325895309448242, loss=1.8746545314788818
I0307 08:28:48.787380 140252175811776 spec.py:321] Evaluating on the training split.
I0307 08:29:01.492408 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 08:29:26.528413 140252175811776 spec.py:349] Evaluating on the test split.
I0307 08:29:28.273240 140252175811776 submission_runner.py:469] Time since start: 21073.90s, 	Step: 50039, 	{'train/accuracy': 0.6532804369926453, 'train/loss': 1.3911547660827637, 'validation/accuracy': 0.5999400019645691, 'validation/loss': 1.6686179637908936, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.369701862335205, 'test/num_examples': 10000, 'score': 19438.077170610428, 'total_duration': 21073.903835058212, 'accumulated_submission_time': 19438.077170610428, 'accumulated_eval_time': 1626.554432630539, 'accumulated_logging_time': 3.985607624053955}
I0307 08:29:28.401823 140096095368960 logging_writer.py:48] [50039] accumulated_eval_time=1626.55, accumulated_logging_time=3.98561, accumulated_submission_time=19438.1, global_step=50039, preemption_count=0, score=19438.1, test/accuracy=0.4771, test/loss=2.3697, test/num_examples=10000, total_duration=21073.9, train/accuracy=0.65328, train/loss=1.39115, validation/accuracy=0.59994, validation/loss=1.66862, validation/num_examples=50000
I0307 08:29:52.741579 140096103761664 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.6058480739593506, loss=1.9305248260498047
I0307 08:30:31.835617 140096095368960 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.191518783569336, loss=1.8254239559173584
I0307 08:31:10.848669 140096103761664 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.544586181640625, loss=1.934802770614624
I0307 08:31:49.898338 140096095368960 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.087653398513794, loss=1.8410083055496216
I0307 08:32:28.604056 140096103761664 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.9530491828918457, loss=1.8397316932678223
I0307 08:33:07.800310 140096095368960 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.378732442855835, loss=1.9271126985549927
I0307 08:33:47.296274 140096103761664 logging_writer.py:48] [50700] global_step=50700, grad_norm=4.205251216888428, loss=1.8711717128753662
I0307 08:34:26.112428 140096095368960 logging_writer.py:48] [50800] global_step=50800, grad_norm=4.156116962432861, loss=2.075779914855957
I0307 08:35:05.652371 140096103761664 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.6497089862823486, loss=1.84610915184021
I0307 08:35:45.726170 140096095368960 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.2900660037994385, loss=1.8480594158172607
I0307 08:36:25.428818 140096103761664 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.432953119277954, loss=1.8370976448059082
I0307 08:37:04.747378 140096095368960 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.238746166229248, loss=1.8773205280303955
I0307 08:37:44.637294 140096103761664 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.6874804496765137, loss=1.9305225610733032
I0307 08:37:58.658760 140252175811776 spec.py:321] Evaluating on the training split.
I0307 08:38:10.877951 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 08:38:37.162408 140252175811776 spec.py:349] Evaluating on the test split.
I0307 08:38:38.903760 140252175811776 submission_runner.py:469] Time since start: 21624.53s, 	Step: 51336, 	{'train/accuracy': 0.6596181392669678, 'train/loss': 1.3574062585830688, 'validation/accuracy': 0.6097800135612488, 'validation/loss': 1.6208540201187134, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.351058006286621, 'test/num_examples': 10000, 'score': 19948.14453125, 'total_duration': 21624.534368276596, 'accumulated_submission_time': 19948.14453125, 'accumulated_eval_time': 1666.7992641925812, 'accumulated_logging_time': 4.159074544906616}
I0307 08:38:39.002196 140096095368960 logging_writer.py:48] [51336] accumulated_eval_time=1666.8, accumulated_logging_time=4.15907, accumulated_submission_time=19948.1, global_step=51336, preemption_count=0, score=19948.1, test/accuracy=0.4804, test/loss=2.35106, test/num_examples=10000, total_duration=21624.5, train/accuracy=0.659618, train/loss=1.35741, validation/accuracy=0.60978, validation/loss=1.62085, validation/num_examples=50000
I0307 08:39:05.281645 140096103761664 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.746391534805298, loss=1.7696418762207031
I0307 08:39:46.171580 140096095368960 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.3749735355377197, loss=1.8257980346679688
I0307 08:40:26.538820 140096103761664 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.8281328678131104, loss=1.897817611694336
I0307 08:41:06.882560 140096095368960 logging_writer.py:48] [51700] global_step=51700, grad_norm=4.54482889175415, loss=1.9354192018508911
I0307 08:41:47.037579 140096103761664 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.4767959117889404, loss=1.9291833639144897
I0307 08:42:27.834859 140096095368960 logging_writer.py:48] [51900] global_step=51900, grad_norm=4.4338274002075195, loss=1.769546627998352
I0307 08:43:08.360036 140096103761664 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.1270253658294678, loss=1.8441686630249023
I0307 08:43:54.397756 140096095368960 logging_writer.py:48] [52100] global_step=52100, grad_norm=4.07384729385376, loss=1.9127408266067505
I0307 08:44:41.370537 140096103761664 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.3740508556365967, loss=1.8252900838851929
I0307 08:45:33.488388 140096095368960 logging_writer.py:48] [52300] global_step=52300, grad_norm=4.089608669281006, loss=1.8345520496368408
I0307 08:46:47.529125 140096103761664 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.325277805328369, loss=1.8174959421157837
I0307 08:47:08.978463 140252175811776 spec.py:321] Evaluating on the training split.
I0307 08:47:21.454130 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 08:47:45.092376 140252175811776 spec.py:349] Evaluating on the test split.
I0307 08:47:46.833045 140252175811776 submission_runner.py:469] Time since start: 22172.46s, 	Step: 52446, 	{'train/accuracy': 0.6522839665412903, 'train/loss': 1.4034955501556396, 'validation/accuracy': 0.601099967956543, 'validation/loss': 1.6656488180160522, 'validation/num_examples': 50000, 'test/accuracy': 0.4815000295639038, 'test/loss': 2.3792152404785156, 'test/num_examples': 10000, 'score': 20457.959265708923, 'total_duration': 22172.46365404129, 'accumulated_submission_time': 20457.959265708923, 'accumulated_eval_time': 1704.6536703109741, 'accumulated_logging_time': 4.288142681121826}
I0307 08:47:46.908921 140096095368960 logging_writer.py:48] [52446] accumulated_eval_time=1704.65, accumulated_logging_time=4.28814, accumulated_submission_time=20458, global_step=52446, preemption_count=0, score=20458, test/accuracy=0.4815, test/loss=2.37922, test/num_examples=10000, total_duration=22172.5, train/accuracy=0.652284, train/loss=1.4035, validation/accuracy=0.6011, validation/loss=1.66565, validation/num_examples=50000
I0307 08:48:10.804383 140096103761664 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.39422607421875, loss=1.7816542387008667
I0307 08:48:52.026408 140096095368960 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.8466458320617676, loss=1.7898948192596436
I0307 08:49:31.287189 140096103761664 logging_writer.py:48] [52700] global_step=52700, grad_norm=4.47117280960083, loss=1.8028643131256104
I0307 08:50:10.196931 140096095368960 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.785390615463257, loss=1.9820820093154907
I0307 08:50:48.331637 140096103761664 logging_writer.py:48] [52900] global_step=52900, grad_norm=4.139624118804932, loss=1.7869899272918701
I0307 08:51:29.811863 140096095368960 logging_writer.py:48] [53000] global_step=53000, grad_norm=4.028311729431152, loss=1.9036740064620972
I0307 08:52:13.701951 140096103761664 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.316929578781128, loss=1.8471366167068481
I0307 08:53:06.775104 140096095368960 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.700429677963257, loss=1.9279838800430298
I0307 08:53:58.027015 140096103761664 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.58174991607666, loss=1.9642466306686401
I0307 08:54:43.167760 140096095368960 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.7382028102874756, loss=1.8143359422683716
I0307 08:55:24.195566 140096103761664 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.528364658355713, loss=1.7496304512023926
I0307 08:56:03.881218 140096095368960 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.6295669078826904, loss=1.7747831344604492
I0307 08:56:17.061243 140252175811776 spec.py:321] Evaluating on the training split.
I0307 08:56:30.090612 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 08:56:57.245411 140252175811776 spec.py:349] Evaluating on the test split.
I0307 08:56:58.999598 140252175811776 submission_runner.py:469] Time since start: 22724.63s, 	Step: 53634, 	{'train/accuracy': 0.6658561825752258, 'train/loss': 1.3239845037460327, 'validation/accuracy': 0.6091200113296509, 'validation/loss': 1.6169421672821045, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.338318109512329, 'test/num_examples': 10000, 'score': 20967.94323539734, 'total_duration': 22724.63018989563, 'accumulated_submission_time': 20967.94323539734, 'accumulated_eval_time': 1746.5918550491333, 'accumulated_logging_time': 4.393389463424683}
I0307 08:56:59.089288 140096103761664 logging_writer.py:48] [53634] accumulated_eval_time=1746.59, accumulated_logging_time=4.39339, accumulated_submission_time=20967.9, global_step=53634, preemption_count=0, score=20967.9, test/accuracy=0.484, test/loss=2.33832, test/num_examples=10000, total_duration=22724.6, train/accuracy=0.665856, train/loss=1.32398, validation/accuracy=0.60912, validation/loss=1.61694, validation/num_examples=50000
I0307 08:57:25.750332 140096095368960 logging_writer.py:48] [53700] global_step=53700, grad_norm=4.172184467315674, loss=1.8703222274780273
I0307 08:58:17.742544 140096103761664 logging_writer.py:48] [53800] global_step=53800, grad_norm=4.242493629455566, loss=1.9135820865631104
I0307 08:59:06.767311 140096095368960 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.3736681938171387, loss=1.8696683645248413
I0307 08:59:51.663908 140096103761664 logging_writer.py:48] [54000] global_step=54000, grad_norm=4.397513389587402, loss=1.7931584119796753
I0307 09:00:35.673906 140096095368960 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.927255392074585, loss=1.839531421661377
I0307 09:01:21.334787 140096103761664 logging_writer.py:48] [54200] global_step=54200, grad_norm=4.036925315856934, loss=1.9162133932113647
I0307 09:02:07.978356 140096095368960 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.495793342590332, loss=1.9527190923690796
I0307 09:02:59.797729 140096103761664 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.3491671085357666, loss=1.8730089664459229
I0307 09:03:53.297313 140096095368960 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.551788806915283, loss=1.8188323974609375
I0307 09:04:33.793392 140096103761664 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.5398921966552734, loss=1.877076506614685
I0307 09:05:18.977292 140096095368960 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.500821590423584, loss=1.8302021026611328
I0307 09:05:29.031622 140252175811776 spec.py:321] Evaluating on the training split.
I0307 09:05:42.146598 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 09:06:10.187322 140252175811776 spec.py:349] Evaluating on the test split.
I0307 09:06:11.918415 140252175811776 submission_runner.py:469] Time since start: 23277.55s, 	Step: 54723, 	{'train/accuracy': 0.6975247263908386, 'train/loss': 1.187575101852417, 'validation/accuracy': 0.6101799607276917, 'validation/loss': 1.6016238927841187, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.310293197631836, 'test/num_examples': 10000, 'score': 21477.729937314987, 'total_duration': 23277.549021959305, 'accumulated_submission_time': 21477.729937314987, 'accumulated_eval_time': 1789.4784786701202, 'accumulated_logging_time': 4.512249708175659}
I0307 09:06:12.016387 140096103761664 logging_writer.py:48] [54723] accumulated_eval_time=1789.48, accumulated_logging_time=4.51225, accumulated_submission_time=21477.7, global_step=54723, preemption_count=0, score=21477.7, test/accuracy=0.4954, test/loss=2.31029, test/num_examples=10000, total_duration=23277.5, train/accuracy=0.697525, train/loss=1.18758, validation/accuracy=0.61018, validation/loss=1.60162, validation/num_examples=50000
I0307 09:06:49.468192 140096095368960 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.8054189682006836, loss=1.753415822982788
I0307 09:07:37.744355 140096103761664 logging_writer.py:48] [54900] global_step=54900, grad_norm=4.317237854003906, loss=1.9022955894470215
I0307 09:08:26.325203 140096095368960 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.6037752628326416, loss=1.8888345956802368
I0307 09:09:51.731954 140096103761664 logging_writer.py:48] [55100] global_step=55100, grad_norm=4.134809494018555, loss=1.8627477884292603
I0307 09:10:36.677381 140096095368960 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.811873435974121, loss=1.7728034257888794
I0307 09:11:22.810178 140096103761664 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.9076297283172607, loss=1.878311276435852
I0307 09:12:13.927864 140096095368960 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.573068141937256, loss=1.8557178974151611
I0307 09:13:03.309753 140096103761664 logging_writer.py:48] [55500] global_step=55500, grad_norm=4.043418884277344, loss=1.904287338256836
I0307 09:13:50.924269 140096095368960 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.6495518684387207, loss=1.8972828388214111
I0307 09:14:41.918378 140096103761664 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.45475697517395, loss=1.93397057056427
I0307 09:14:41.929295 140252175811776 spec.py:321] Evaluating on the training split.
I0307 09:14:54.997591 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 09:15:16.470391 140252175811776 spec.py:349] Evaluating on the test split.
I0307 09:15:18.197633 140252175811776 submission_runner.py:469] Time since start: 23823.83s, 	Step: 55701, 	{'train/accuracy': 0.6644411683082581, 'train/loss': 1.3461406230926514, 'validation/accuracy': 0.6157199740409851, 'validation/loss': 1.5817506313323975, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.2826731204986572, 'test/num_examples': 10000, 'score': 21987.48577094078, 'total_duration': 23823.82820034027, 'accumulated_submission_time': 21987.48577094078, 'accumulated_eval_time': 1825.74658203125, 'accumulated_logging_time': 4.65355110168457}
I0307 09:15:18.300739 140096095368960 logging_writer.py:48] [55701] accumulated_eval_time=1825.75, accumulated_logging_time=4.65355, accumulated_submission_time=21987.5, global_step=55701, preemption_count=0, score=21987.5, test/accuracy=0.488, test/loss=2.28267, test/num_examples=10000, total_duration=23823.8, train/accuracy=0.664441, train/loss=1.34614, validation/accuracy=0.61572, validation/loss=1.58175, validation/num_examples=50000
I0307 09:16:09.709224 140096103761664 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.613266944885254, loss=1.8380794525146484
I0307 09:17:05.710322 140096095368960 logging_writer.py:48] [55900] global_step=55900, grad_norm=4.005011081695557, loss=1.856919765472412
I0307 09:17:58.998225 140096103761664 logging_writer.py:48] [56000] global_step=56000, grad_norm=4.177248001098633, loss=1.961183786392212
I0307 09:19:01.987947 140096095368960 logging_writer.py:48] [56100] global_step=56100, grad_norm=4.204779624938965, loss=1.7950634956359863
I0307 09:20:13.476012 140096103761664 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.9872913360595703, loss=1.8720707893371582
I0307 09:21:09.626324 140096095368960 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.7149856090545654, loss=1.9072246551513672
I0307 09:21:52.272862 140096103761664 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.420908212661743, loss=1.8727794885635376
I0307 09:22:33.116461 140096095368960 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.688910722732544, loss=1.8379684686660767
I0307 09:23:31.786936 140096103761664 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.53497576713562, loss=1.792738676071167
I0307 09:23:48.223125 140252175811776 spec.py:321] Evaluating on the training split.
I0307 09:24:01.672991 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 09:24:29.750325 140252175811776 spec.py:349] Evaluating on the test split.
I0307 09:24:31.488347 140252175811776 submission_runner.py:469] Time since start: 24377.12s, 	Step: 56624, 	{'train/accuracy': 0.6606943607330322, 'train/loss': 1.3473140001296997, 'validation/accuracy': 0.6124599575996399, 'validation/loss': 1.6119436025619507, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.314908742904663, 'test/num_examples': 10000, 'score': 22497.274667978287, 'total_duration': 24377.118958473206, 'accumulated_submission_time': 22497.274667978287, 'accumulated_eval_time': 1869.0116336345673, 'accumulated_logging_time': 4.784945964813232}
I0307 09:24:31.621612 140096095368960 logging_writer.py:48] [56624] accumulated_eval_time=1869.01, accumulated_logging_time=4.78495, accumulated_submission_time=22497.3, global_step=56624, preemption_count=0, score=22497.3, test/accuracy=0.4867, test/loss=2.31491, test/num_examples=10000, total_duration=24377.1, train/accuracy=0.660694, train/loss=1.34731, validation/accuracy=0.61246, validation/loss=1.61194, validation/num_examples=50000
I0307 09:25:02.512156 140096103761664 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.92974853515625, loss=1.734584093093872
I0307 09:25:47.238331 140096095368960 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.600654363632202, loss=1.9566969871520996
I0307 09:26:40.874258 140096103761664 logging_writer.py:48] [56900] global_step=56900, grad_norm=4.176911354064941, loss=1.8727362155914307
I0307 09:27:27.233046 140096095368960 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.6072959899902344, loss=1.893465280532837
I0307 09:28:16.678618 140096103761664 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.6281418800354004, loss=1.8101730346679688
I0307 09:29:09.890813 140096095368960 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.381052255630493, loss=1.92893385887146
I0307 09:29:49.801472 140096103761664 logging_writer.py:48] [57300] global_step=57300, grad_norm=4.323523044586182, loss=1.9211505651474
I0307 09:30:46.438503 140096095368960 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.948773145675659, loss=1.892866611480713
I0307 09:31:40.893113 140096103761664 logging_writer.py:48] [57500] global_step=57500, grad_norm=4.066812992095947, loss=1.8719580173492432
I0307 09:32:31.052531 140096095368960 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.777721881866455, loss=1.8252568244934082
I0307 09:33:01.707319 140252175811776 spec.py:321] Evaluating on the training split.
I0307 09:33:14.329796 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 09:33:35.758498 140252175811776 spec.py:349] Evaluating on the test split.
I0307 09:33:37.537132 140252175811776 submission_runner.py:469] Time since start: 24923.17s, 	Step: 57668, 	{'train/accuracy': 0.6625877022743225, 'train/loss': 1.3463044166564941, 'validation/accuracy': 0.6072399616241455, 'validation/loss': 1.6348316669464111, 'validation/num_examples': 50000, 'test/accuracy': 0.4823000133037567, 'test/loss': 2.378462791442871, 'test/num_examples': 10000, 'score': 23007.19713640213, 'total_duration': 24923.167719125748, 'accumulated_submission_time': 23007.19713640213, 'accumulated_eval_time': 1904.841246843338, 'accumulated_logging_time': 4.962131977081299}
I0307 09:33:37.613886 140096103761664 logging_writer.py:48] [57668] accumulated_eval_time=1904.84, accumulated_logging_time=4.96213, accumulated_submission_time=23007.2, global_step=57668, preemption_count=0, score=23007.2, test/accuracy=0.4823, test/loss=2.37846, test/num_examples=10000, total_duration=24923.2, train/accuracy=0.662588, train/loss=1.3463, validation/accuracy=0.60724, validation/loss=1.63483, validation/num_examples=50000
I0307 09:33:50.881854 140096095368960 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.4139368534088135, loss=1.8420698642730713
I0307 09:35:00.786458 140096103761664 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.7318150997161865, loss=1.9277266263961792
I0307 09:36:31.272406 140096095368960 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.09149169921875, loss=1.800142765045166
I0307 09:37:32.738739 140096103761664 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.626030683517456, loss=1.776339054107666
I0307 09:38:36.904846 140096095368960 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.94757342338562, loss=1.7992008924484253
I0307 09:39:26.777838 140096103761664 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.653930187225342, loss=1.9223389625549316
I0307 09:40:18.421003 140096095368960 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.4920945167541504, loss=1.8320037126541138
I0307 09:41:16.027539 140096103761664 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.738877296447754, loss=1.800087332725525
I0307 09:42:07.925955 140252175811776 spec.py:321] Evaluating on the training split.
I0307 09:42:21.200961 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 09:42:44.642906 140252175811776 spec.py:349] Evaluating on the test split.
I0307 09:42:46.416136 140252175811776 submission_runner.py:469] Time since start: 25472.05s, 	Step: 58483, 	{'train/accuracy': 0.6560307741165161, 'train/loss': 1.3707544803619385, 'validation/accuracy': 0.6067399978637695, 'validation/loss': 1.6136571168899536, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.355103015899658, 'test/num_examples': 10000, 'score': 23517.374615430832, 'total_duration': 25472.04661512375, 'accumulated_submission_time': 23517.374615430832, 'accumulated_eval_time': 1943.3311386108398, 'accumulated_logging_time': 5.079399347305298}
I0307 09:42:46.544495 140096095368960 logging_writer.py:48] [58483] accumulated_eval_time=1943.33, accumulated_logging_time=5.0794, accumulated_submission_time=23517.4, global_step=58483, preemption_count=0, score=23517.4, test/accuracy=0.4837, test/loss=2.3551, test/num_examples=10000, total_duration=25472, train/accuracy=0.656031, train/loss=1.37075, validation/accuracy=0.60674, validation/loss=1.61366, validation/num_examples=50000
I0307 09:42:53.892933 140096103761664 logging_writer.py:48] [58500] global_step=58500, grad_norm=4.042136192321777, loss=1.8016330003738403
I0307 09:43:33.174380 140096095368960 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.597930431365967, loss=1.935534119606018
I0307 09:44:19.868468 140096103761664 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.6696245670318604, loss=1.8429030179977417
I0307 09:45:21.416841 140096095368960 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.356111764907837, loss=1.7840092182159424
I0307 09:46:11.775965 140096103761664 logging_writer.py:48] [58900] global_step=58900, grad_norm=4.49398136138916, loss=1.7357507944107056
I0307 09:47:00.175377 140096095368960 logging_writer.py:48] [59000] global_step=59000, grad_norm=4.0088582038879395, loss=1.8297454118728638
I0307 09:47:56.655200 140096103761664 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.9363834857940674, loss=1.859036922454834
I0307 09:48:55.825468 140096095368960 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.9579455852508545, loss=1.7978334426879883
I0307 09:50:08.428113 140096103761664 logging_writer.py:48] [59300] global_step=59300, grad_norm=5.0599284172058105, loss=1.8925474882125854
I0307 09:51:04.073852 140096095368960 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.5632646083831787, loss=1.880804419517517
I0307 09:51:16.744477 140252175811776 spec.py:321] Evaluating on the training split.
I0307 09:51:29.416357 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 09:51:51.604117 140252175811776 spec.py:349] Evaluating on the test split.
I0307 09:51:53.362947 140252175811776 submission_runner.py:469] Time since start: 26018.99s, 	Step: 59430, 	{'train/accuracy': 0.6508290767669678, 'train/loss': 1.3903214931488037, 'validation/accuracy': 0.5988999605178833, 'validation/loss': 1.667960524559021, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.379729747772217, 'test/num_examples': 10000, 'score': 24027.426962852478, 'total_duration': 26018.993552446365, 'accumulated_submission_time': 24027.426962852478, 'accumulated_eval_time': 1979.9494287967682, 'accumulated_logging_time': 5.245790958404541}
I0307 09:51:53.460763 140096103761664 logging_writer.py:48] [59430] accumulated_eval_time=1979.95, accumulated_logging_time=5.24579, accumulated_submission_time=24027.4, global_step=59430, preemption_count=0, score=24027.4, test/accuracy=0.4782, test/loss=2.37973, test/num_examples=10000, total_duration=26019, train/accuracy=0.650829, train/loss=1.39032, validation/accuracy=0.5989, validation/loss=1.66796, validation/num_examples=50000
I0307 09:52:28.140341 140096095368960 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.7528886795043945, loss=1.821527361869812
I0307 09:53:38.101046 140096103761664 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.6384057998657227, loss=1.9033393859863281
I0307 09:54:34.744134 140096095368960 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.359783887863159, loss=1.7183177471160889
I0307 09:55:23.026414 140096103761664 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.83443021774292, loss=1.8633344173431396
I0307 09:56:27.434226 140096095368960 logging_writer.py:48] [59900] global_step=59900, grad_norm=4.547143459320068, loss=1.8781161308288574
I0307 09:57:36.024901 140096103761664 logging_writer.py:48] [60000] global_step=60000, grad_norm=4.2708539962768555, loss=1.8323994874954224
2025-03-07 09:57:57.030395: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:58:24.565387 140096095368960 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.6919329166412354, loss=1.7882099151611328
I0307 09:59:05.292854 140096103761664 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.502459764480591, loss=1.8317241668701172
I0307 09:59:45.064867 140096095368960 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.9990835189819336, loss=1.9839667081832886
I0307 10:00:23.477653 140252175811776 spec.py:321] Evaluating on the training split.
I0307 10:00:36.477245 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 10:00:57.271553 140252175811776 spec.py:349] Evaluating on the test split.
I0307 10:00:58.981544 140252175811776 submission_runner.py:469] Time since start: 26564.61s, 	Step: 60354, 	{'train/accuracy': 0.671875, 'train/loss': 1.3070734739303589, 'validation/accuracy': 0.616320013999939, 'validation/loss': 1.6041799783706665, 'validation/num_examples': 50000, 'test/accuracy': 0.4847000241279602, 'test/loss': 2.330476999282837, 'test/num_examples': 10000, 'score': 24537.296875, 'total_duration': 26564.612133026123, 'accumulated_submission_time': 24537.296875, 'accumulated_eval_time': 2015.4531314373016, 'accumulated_logging_time': 5.384232759475708}
I0307 10:00:59.035715 140096103761664 logging_writer.py:48] [60354] accumulated_eval_time=2015.45, accumulated_logging_time=5.38423, accumulated_submission_time=24537.3, global_step=60354, preemption_count=0, score=24537.3, test/accuracy=0.4847, test/loss=2.33048, test/num_examples=10000, total_duration=26564.6, train/accuracy=0.671875, train/loss=1.30707, validation/accuracy=0.61632, validation/loss=1.60418, validation/num_examples=50000
I0307 10:01:40.411280 140096095368960 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.672501564025879, loss=1.8939241170883179
I0307 10:02:39.160638 140096103761664 logging_writer.py:48] [60500] global_step=60500, grad_norm=4.294215202331543, loss=1.8157697916030884
I0307 10:04:04.298313 140096095368960 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.8143835067749023, loss=1.854882836341858
I0307 10:05:18.343524 140096103761664 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.8966000080108643, loss=1.8917782306671143
I0307 10:06:43.120346 140096095368960 logging_writer.py:48] [60800] global_step=60800, grad_norm=4.241964340209961, loss=1.8350909948349
I0307 10:07:28.483982 140096103761664 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.305999517440796, loss=1.8526519536972046
I0307 10:08:19.291392 140096095368960 logging_writer.py:48] [61000] global_step=61000, grad_norm=5.072299480438232, loss=1.7770575284957886
I0307 10:09:08.561966 140096103761664 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.4586191177368164, loss=1.8359432220458984
I0307 10:09:29.248561 140252175811776 spec.py:321] Evaluating on the training split.
I0307 10:09:42.079365 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 10:10:03.323884 140252175811776 spec.py:349] Evaluating on the test split.
I0307 10:10:05.034162 140252175811776 submission_runner.py:469] Time since start: 27110.66s, 	Step: 61143, 	{'train/accuracy': 0.6684868931770325, 'train/loss': 1.3220707178115845, 'validation/accuracy': 0.619219958782196, 'validation/loss': 1.5733745098114014, 'validation/num_examples': 50000, 'test/accuracy': 0.4913000166416168, 'test/loss': 2.2740910053253174, 'test/num_examples': 10000, 'score': 25047.395356416702, 'total_duration': 27110.66473674774, 'accumulated_submission_time': 25047.395356416702, 'accumulated_eval_time': 2051.238523244858, 'accumulated_logging_time': 5.462580680847168}
I0307 10:10:05.097855 140096095368960 logging_writer.py:48] [61143] accumulated_eval_time=2051.24, accumulated_logging_time=5.46258, accumulated_submission_time=25047.4, global_step=61143, preemption_count=0, score=25047.4, test/accuracy=0.4913, test/loss=2.27409, test/num_examples=10000, total_duration=27110.7, train/accuracy=0.668487, train/loss=1.32207, validation/accuracy=0.61922, validation/loss=1.57337, validation/num_examples=50000
I0307 10:10:39.582815 140096103761664 logging_writer.py:48] [61200] global_step=61200, grad_norm=4.149960517883301, loss=1.8050893545150757
I0307 10:12:12.083302 140096095368960 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.2318484783172607, loss=1.799838900566101
I0307 10:12:58.655722 140096103761664 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.7289700508117676, loss=1.8178399801254272
I0307 10:13:53.558529 140096095368960 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.9934403896331787, loss=1.955559253692627
I0307 10:14:39.704327 140096103761664 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.720921277999878, loss=1.9378094673156738
I0307 10:16:00.846882 140096095368960 logging_writer.py:48] [61700] global_step=61700, grad_norm=4.047650337219238, loss=1.9550882577896118
I0307 10:17:23.320062 140096103761664 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.5710887908935547, loss=1.802177906036377
I0307 10:18:35.034800 140252175811776 spec.py:321] Evaluating on the training split.
I0307 10:18:46.629702 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 10:19:06.709478 140252175811776 spec.py:349] Evaluating on the test split.
I0307 10:19:08.422258 140252175811776 submission_runner.py:469] Time since start: 27654.05s, 	Step: 61885, 	{'train/accuracy': 0.6733099222183228, 'train/loss': 1.2973337173461914, 'validation/accuracy': 0.6195399761199951, 'validation/loss': 1.5772764682769775, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.285935401916504, 'test/num_examples': 10000, 'score': 25557.219066143036, 'total_duration': 27654.05300974846, 'accumulated_submission_time': 25557.219066143036, 'accumulated_eval_time': 2084.6259570121765, 'accumulated_logging_time': 5.5539679527282715}
I0307 10:19:08.510785 140096095368960 logging_writer.py:48] [61885] accumulated_eval_time=2084.63, accumulated_logging_time=5.55397, accumulated_submission_time=25557.2, global_step=61885, preemption_count=0, score=25557.2, test/accuracy=0.4957, test/loss=2.28594, test/num_examples=10000, total_duration=27654.1, train/accuracy=0.67331, train/loss=1.29733, validation/accuracy=0.61954, validation/loss=1.57728, validation/num_examples=50000
I0307 10:19:26.621718 140096103761664 logging_writer.py:48] [61900] global_step=61900, grad_norm=4.287245750427246, loss=1.776350498199463
I0307 10:20:49.952570 140096095368960 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.345543146133423, loss=1.8280658721923828
I0307 10:21:38.194591 140096103761664 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.5951688289642334, loss=1.7441520690917969
I0307 10:22:38.370625 140096095368960 logging_writer.py:48] [62200] global_step=62200, grad_norm=4.217250823974609, loss=1.8639912605285645
I0307 10:24:02.959326 140096103761664 logging_writer.py:48] [62300] global_step=62300, grad_norm=4.6650238037109375, loss=1.876147985458374
I0307 10:26:01.977080 140096095368960 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.7063164710998535, loss=1.8898913860321045
I0307 10:27:17.801409 140096103761664 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.9913461208343506, loss=1.7762064933776855
I0307 10:27:38.461750 140252175811776 spec.py:321] Evaluating on the training split.
I0307 10:27:50.045994 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 10:28:11.679762 140252175811776 spec.py:349] Evaluating on the test split.
I0307 10:28:13.414237 140252175811776 submission_runner.py:469] Time since start: 28199.04s, 	Step: 62524, 	{'train/accuracy': 0.6745057106018066, 'train/loss': 1.2949031591415405, 'validation/accuracy': 0.6291199922561646, 'validation/loss': 1.5317565202713013, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.226806402206421, 'test/num_examples': 10000, 'score': 26067.06433200836, 'total_duration': 28199.04498410225, 'accumulated_submission_time': 26067.06433200836, 'accumulated_eval_time': 2119.5784056186676, 'accumulated_logging_time': 5.673795700073242}
I0307 10:28:13.455661 140096095368960 logging_writer.py:48] [62524] accumulated_eval_time=2119.58, accumulated_logging_time=5.6738, accumulated_submission_time=26067.1, global_step=62524, preemption_count=0, score=26067.1, test/accuracy=0.5062, test/loss=2.22681, test/num_examples=10000, total_duration=28199, train/accuracy=0.674506, train/loss=1.2949, validation/accuracy=0.62912, validation/loss=1.53176, validation/num_examples=50000
I0307 10:29:56.468940 140096103761664 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.2732701301574707, loss=1.8385210037231445
I0307 10:30:40.173766 140096095368960 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.642052412033081, loss=1.8921899795532227
I0307 10:31:41.258123 140096103761664 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.8232686519622803, loss=1.691757082939148
I0307 10:32:56.329932 140096095368960 logging_writer.py:48] [62900] global_step=62900, grad_norm=4.056192874908447, loss=1.8029426336288452
I0307 10:33:53.527958 140096103761664 logging_writer.py:48] [63000] global_step=63000, grad_norm=4.309593677520752, loss=1.9059511423110962
I0307 10:35:17.719963 140096095368960 logging_writer.py:48] [63100] global_step=63100, grad_norm=4.486965179443359, loss=1.837024211883545
I0307 10:36:29.058013 140096103761664 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.5116126537323, loss=1.860994577407837
I0307 10:36:43.447178 140252175811776 spec.py:321] Evaluating on the training split.
I0307 10:36:55.207419 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 10:37:19.315712 140252175811776 spec.py:349] Evaluating on the test split.
I0307 10:37:21.031282 140252175811776 submission_runner.py:469] Time since start: 28746.66s, 	Step: 63226, 	{'train/accuracy': 0.673270046710968, 'train/loss': 1.2910690307617188, 'validation/accuracy': 0.6248999834060669, 'validation/loss': 1.5530205965042114, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.2459170818328857, 'test/num_examples': 10000, 'score': 26576.9688808918, 'total_duration': 28746.66203737259, 'accumulated_submission_time': 26576.9688808918, 'accumulated_eval_time': 2157.1624808311462, 'accumulated_logging_time': 5.7231504917144775}
I0307 10:37:21.117161 140096095368960 logging_writer.py:48] [63226] accumulated_eval_time=2157.16, accumulated_logging_time=5.72315, accumulated_submission_time=26577, global_step=63226, preemption_count=0, score=26577, test/accuracy=0.5047, test/loss=2.24592, test/num_examples=10000, total_duration=28746.7, train/accuracy=0.67327, train/loss=1.29107, validation/accuracy=0.6249, validation/loss=1.55302, validation/num_examples=50000
I0307 10:38:14.033591 140096103761664 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.7876322269439697, loss=1.8390655517578125
I0307 10:40:04.926006 140096095368960 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.711322546005249, loss=1.7353225946426392
I0307 10:41:18.132887 140096103761664 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.421271800994873, loss=1.7764499187469482
I0307 10:42:26.864057 140096095368960 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.722263813018799, loss=1.8859002590179443
I0307 10:43:20.772455 140096103761664 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.412836790084839, loss=1.8389813899993896
I0307 10:44:42.802464 140096095368960 logging_writer.py:48] [63800] global_step=63800, grad_norm=3.798792600631714, loss=1.9290868043899536
I0307 10:45:51.551817 140252175811776 spec.py:321] Evaluating on the training split.
I0307 10:46:03.532316 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 10:46:24.739145 140252175811776 spec.py:349] Evaluating on the test split.
I0307 10:46:26.454730 140252175811776 submission_runner.py:469] Time since start: 29292.09s, 	Step: 63855, 	{'train/accuracy': 0.6629862785339355, 'train/loss': 1.3470371961593628, 'validation/accuracy': 0.6160199642181396, 'validation/loss': 1.582107663154602, 'validation/num_examples': 50000, 'test/accuracy': 0.4894000291824341, 'test/loss': 2.2999372482299805, 'test/num_examples': 10000, 'score': 27087.323586702347, 'total_duration': 29292.085461616516, 'accumulated_submission_time': 27087.323586702347, 'accumulated_eval_time': 2192.065351486206, 'accumulated_logging_time': 5.816307067871094}
I0307 10:46:26.503249 140096103761664 logging_writer.py:48] [63855] accumulated_eval_time=2192.07, accumulated_logging_time=5.81631, accumulated_submission_time=27087.3, global_step=63855, preemption_count=0, score=27087.3, test/accuracy=0.4894, test/loss=2.29994, test/num_examples=10000, total_duration=29292.1, train/accuracy=0.662986, train/loss=1.34704, validation/accuracy=0.61602, validation/loss=1.58211, validation/num_examples=50000
I0307 10:46:46.384391 140096095368960 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.7814035415649414, loss=1.8415957689285278
I0307 10:48:19.604793 140096103761664 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.455632209777832, loss=1.7265689373016357
I0307 10:49:29.364211 140096095368960 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.863340139389038, loss=1.816865086555481
I0307 10:50:34.981645 140096103761664 logging_writer.py:48] [64200] global_step=64200, grad_norm=4.2696380615234375, loss=1.8757060766220093
I0307 10:51:59.543871 140096095368960 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.402979850769043, loss=1.891934871673584
I0307 10:53:35.851869 140096103761664 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.6292502880096436, loss=1.8313922882080078
I0307 10:54:56.515753 140252175811776 spec.py:321] Evaluating on the training split.
I0307 10:55:07.112927 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 10:55:27.911302 140252175811776 spec.py:349] Evaluating on the test split.
I0307 10:55:29.634537 140252175811776 submission_runner.py:469] Time since start: 29835.27s, 	Step: 64497, 	{'train/accuracy': 0.6769172549247742, 'train/loss': 1.2932140827178955, 'validation/accuracy': 0.6228199601173401, 'validation/loss': 1.55232834815979, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.275650978088379, 'test/num_examples': 10000, 'score': 27597.25462293625, 'total_duration': 29835.26527905464, 'accumulated_submission_time': 27597.25462293625, 'accumulated_eval_time': 2225.1841027736664, 'accumulated_logging_time': 5.873381614685059}
I0307 10:55:29.671435 140096095368960 logging_writer.py:48] [64497] accumulated_eval_time=2225.18, accumulated_logging_time=5.87338, accumulated_submission_time=27597.3, global_step=64497, preemption_count=0, score=27597.3, test/accuracy=0.4927, test/loss=2.27565, test/num_examples=10000, total_duration=29835.3, train/accuracy=0.676917, train/loss=1.29321, validation/accuracy=0.62282, validation/loss=1.55233, validation/num_examples=50000
I0307 10:55:31.226888 140096103761664 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.6541309356689453, loss=1.6929526329040527
I0307 10:56:46.750260 140096095368960 logging_writer.py:48] [64600] global_step=64600, grad_norm=4.081542015075684, loss=1.7109766006469727
I0307 10:58:12.399045 140096103761664 logging_writer.py:48] [64700] global_step=64700, grad_norm=4.16353702545166, loss=1.784683108329773
I0307 10:59:09.847615 140096095368960 logging_writer.py:48] [64800] global_step=64800, grad_norm=4.639448165893555, loss=1.8986941576004028
I0307 11:00:36.528802 140096103761664 logging_writer.py:48] [64900] global_step=64900, grad_norm=4.0174665451049805, loss=1.8819763660430908
I0307 11:03:04.241225 140096095368960 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.563948631286621, loss=1.8534656763076782
I0307 11:04:00.405584 140252175811776 spec.py:321] Evaluating on the training split.
I0307 11:04:11.488782 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 11:04:31.134206 140252175811776 spec.py:349] Evaluating on the test split.
I0307 11:04:32.896074 140252175811776 submission_runner.py:469] Time since start: 30378.53s, 	Step: 65020, 	{'train/accuracy': 0.6654974222183228, 'train/loss': 1.3372374773025513, 'validation/accuracy': 0.6230199933052063, 'validation/loss': 1.56284761428833, 'validation/num_examples': 50000, 'test/accuracy': 0.49310001730918884, 'test/loss': 2.2624058723449707, 'test/num_examples': 10000, 'score': 28107.88705444336, 'total_duration': 30378.526815652847, 'accumulated_submission_time': 28107.88705444336, 'accumulated_eval_time': 2257.674558162689, 'accumulated_logging_time': 5.951249361038208}
I0307 11:04:32.976427 140096103761664 logging_writer.py:48] [65020] accumulated_eval_time=2257.67, accumulated_logging_time=5.95125, accumulated_submission_time=28107.9, global_step=65020, preemption_count=0, score=28107.9, test/accuracy=0.4931, test/loss=2.26241, test/num_examples=10000, total_duration=30378.5, train/accuracy=0.665497, train/loss=1.33724, validation/accuracy=0.62302, validation/loss=1.56285, validation/num_examples=50000
I0307 11:05:11.612199 140096095368960 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.29460072517395, loss=1.7705870866775513
I0307 11:05:50.307160 140096103761664 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.8757035732269287, loss=1.8892173767089844
I0307 11:06:28.784578 140096095368960 logging_writer.py:48] [65300] global_step=65300, grad_norm=4.2130513191223145, loss=1.7966426610946655
I0307 11:07:07.600755 140096103761664 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.624868392944336, loss=1.8358491659164429
I0307 11:07:46.290248 140096095368960 logging_writer.py:48] [65500] global_step=65500, grad_norm=4.043339252471924, loss=1.8554651737213135
I0307 11:08:24.803161 140096103761664 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.2792725563049316, loss=1.790802001953125
I0307 11:09:03.507983 140096095368960 logging_writer.py:48] [65700] global_step=65700, grad_norm=4.75343656539917, loss=1.8942220211029053
I0307 11:09:42.201423 140096103761664 logging_writer.py:48] [65800] global_step=65800, grad_norm=4.080942630767822, loss=1.7748427391052246
I0307 11:10:20.944776 140096095368960 logging_writer.py:48] [65900] global_step=65900, grad_norm=4.41758394241333, loss=1.9026907682418823
I0307 11:11:00.263440 140096103761664 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.778337240219116, loss=1.8034298419952393
I0307 11:11:38.944655 140096095368960 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.74365496635437, loss=1.7967021465301514
I0307 11:12:18.013333 140096103761664 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.74088454246521, loss=1.7528644800186157
I0307 11:12:57.641590 140096095368960 logging_writer.py:48] [66300] global_step=66300, grad_norm=4.133584022521973, loss=1.743801474571228
I0307 11:13:03.271415 140252175811776 spec.py:321] Evaluating on the training split.
I0307 11:13:13.894893 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 11:13:43.142790 140252175811776 spec.py:349] Evaluating on the test split.
I0307 11:13:44.868210 140252175811776 submission_runner.py:469] Time since start: 30930.50s, 	Step: 66315, 	{'train/accuracy': 0.6778539419174194, 'train/loss': 1.2795910835266113, 'validation/accuracy': 0.6269599795341492, 'validation/loss': 1.5225732326507568, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.245404005050659, 'test/num_examples': 10000, 'score': 28618.030217170715, 'total_duration': 30930.498950004578, 'accumulated_submission_time': 28618.030217170715, 'accumulated_eval_time': 2299.2713062763214, 'accumulated_logging_time': 6.039647579193115}
I0307 11:13:44.905686 140096103761664 logging_writer.py:48] [66315] accumulated_eval_time=2299.27, accumulated_logging_time=6.03965, accumulated_submission_time=28618, global_step=66315, preemption_count=0, score=28618, test/accuracy=0.5032, test/loss=2.2454, test/num_examples=10000, total_duration=30930.5, train/accuracy=0.677854, train/loss=1.27959, validation/accuracy=0.62696, validation/loss=1.52257, validation/num_examples=50000
I0307 11:14:18.870621 140096095368960 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.7495367527008057, loss=1.7662858963012695
I0307 11:14:57.661433 140096103761664 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.3711633682250977, loss=1.818678617477417
I0307 11:15:37.062732 140096095368960 logging_writer.py:48] [66600] global_step=66600, grad_norm=4.186943531036377, loss=1.8330258131027222
I0307 11:16:16.152663 140096103761664 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.4986448287963867, loss=1.81574285030365
I0307 11:16:54.992141 140096095368960 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.528665065765381, loss=1.8603169918060303
I0307 11:17:34.119939 140096103761664 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.758862257003784, loss=1.8121905326843262
I0307 11:18:13.535309 140096095368960 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.6069207191467285, loss=1.7474168539047241
I0307 11:18:52.503107 140096103761664 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.9600040912628174, loss=1.8098900318145752
I0307 11:19:32.328875 140096095368960 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.7635514736175537, loss=1.9512258768081665
I0307 11:20:11.470942 140096103761664 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.860783576965332, loss=1.8308227062225342
I0307 11:20:50.817861 140096095368960 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.527531385421753, loss=1.8178293704986572
I0307 11:21:30.647315 140096103761664 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.892761707305908, loss=1.9205677509307861
I0307 11:22:10.336063 140096095368960 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.8029375076293945, loss=1.8414219617843628
I0307 11:22:14.904067 140252175811776 spec.py:321] Evaluating on the training split.
I0307 11:22:25.765814 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 11:22:50.538117 140252175811776 spec.py:349] Evaluating on the test split.
I0307 11:22:52.274322 140252175811776 submission_runner.py:469] Time since start: 31477.91s, 	Step: 67613, 	{'train/accuracy': 0.6728315949440002, 'train/loss': 1.3096870183944702, 'validation/accuracy': 0.6263799667358398, 'validation/loss': 1.5540915727615356, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.2706117630004883, 'test/num_examples': 10000, 'score': 29127.874561071396, 'total_duration': 31477.905072689056, 'accumulated_submission_time': 29127.874561071396, 'accumulated_eval_time': 2336.641523361206, 'accumulated_logging_time': 6.086534261703491}
I0307 11:22:52.364205 140096103761664 logging_writer.py:48] [67613] accumulated_eval_time=2336.64, accumulated_logging_time=6.08653, accumulated_submission_time=29127.9, global_step=67613, preemption_count=0, score=29127.9, test/accuracy=0.502, test/loss=2.27061, test/num_examples=10000, total_duration=31477.9, train/accuracy=0.672832, train/loss=1.30969, validation/accuracy=0.62638, validation/loss=1.55409, validation/num_examples=50000
I0307 11:23:27.239065 140096095368960 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.946664571762085, loss=1.7736456394195557
I0307 11:24:06.657295 140096103761664 logging_writer.py:48] [67800] global_step=67800, grad_norm=4.320501804351807, loss=1.72121262550354
I0307 11:24:46.586290 140096095368960 logging_writer.py:48] [67900] global_step=67900, grad_norm=4.246980667114258, loss=1.7246760129928589
I0307 11:25:26.186962 140096103761664 logging_writer.py:48] [68000] global_step=68000, grad_norm=4.119545936584473, loss=1.8102627992630005
I0307 11:26:06.041951 140096095368960 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.3735382556915283, loss=1.8214104175567627
I0307 11:26:45.401863 140096103761664 logging_writer.py:48] [68200] global_step=68200, grad_norm=4.1454057693481445, loss=1.801442265510559
I0307 11:27:25.208825 140096095368960 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.3507096767425537, loss=1.6775410175323486
I0307 11:28:04.399575 140096103761664 logging_writer.py:48] [68400] global_step=68400, grad_norm=4.059345245361328, loss=1.7829627990722656
I0307 11:28:44.569588 140096095368960 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.8357598781585693, loss=1.805397629737854
I0307 11:29:24.603978 140096103761664 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.9196786880493164, loss=1.7093322277069092
I0307 11:30:04.920357 140096095368960 logging_writer.py:48] [68700] global_step=68700, grad_norm=4.443275451660156, loss=1.9854105710983276
I0307 11:30:45.377601 140096103761664 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.4920477867126465, loss=1.8565229177474976
I0307 11:31:22.513258 140252175811776 spec.py:321] Evaluating on the training split.
I0307 11:31:33.512210 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 11:31:59.529169 140252175811776 spec.py:349] Evaluating on the test split.
I0307 11:32:01.253838 140252175811776 submission_runner.py:469] Time since start: 32026.88s, 	Step: 68894, 	{'train/accuracy': 0.6631656289100647, 'train/loss': 1.3368297815322876, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.582789659500122, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.2988550662994385, 'test/num_examples': 10000, 'score': 29637.865998983383, 'total_duration': 32026.884588956833, 'accumulated_submission_time': 29637.865998983383, 'accumulated_eval_time': 2375.3820683956146, 'accumulated_logging_time': 6.1842992305755615}
I0307 11:32:01.285569 140096095368960 logging_writer.py:48] [68894] accumulated_eval_time=2375.38, accumulated_logging_time=6.1843, accumulated_submission_time=29637.9, global_step=68894, preemption_count=0, score=29637.9, test/accuracy=0.4938, test/loss=2.29886, test/num_examples=10000, total_duration=32026.9, train/accuracy=0.663166, train/loss=1.33683, validation/accuracy=0.61482, validation/loss=1.58279, validation/num_examples=50000
I0307 11:32:03.986474 140096103761664 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.707186698913574, loss=1.860098958015442
I0307 11:32:44.268685 140096095368960 logging_writer.py:48] [69000] global_step=69000, grad_norm=4.8184919357299805, loss=1.7714024782180786
I0307 11:33:24.797103 140096103761664 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.7244255542755127, loss=1.775286316871643
I0307 11:34:05.687053 140096095368960 logging_writer.py:48] [69200] global_step=69200, grad_norm=4.237217426300049, loss=1.86346435546875
I0307 11:34:45.356825 140096103761664 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.5376040935516357, loss=1.9219591617584229
I0307 11:35:25.205211 140096095368960 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.7087018489837646, loss=1.751295566558838
I0307 11:36:05.479072 140096103761664 logging_writer.py:48] [69500] global_step=69500, grad_norm=4.598377227783203, loss=1.715247392654419
I0307 11:36:45.745444 140096095368960 logging_writer.py:48] [69600] global_step=69600, grad_norm=4.314455032348633, loss=1.836899995803833
I0307 11:37:25.970823 140096103761664 logging_writer.py:48] [69700] global_step=69700, grad_norm=4.863664627075195, loss=1.7694380283355713
I0307 11:38:06.081547 140096095368960 logging_writer.py:48] [69800] global_step=69800, grad_norm=4.460473537445068, loss=1.7351034879684448
I0307 11:38:46.157944 140096103761664 logging_writer.py:48] [69900] global_step=69900, grad_norm=4.576422214508057, loss=1.7663788795471191
I0307 11:39:26.923752 140096095368960 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.472747802734375, loss=1.77491295337677
I0307 11:40:07.662728 140096103761664 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.681532144546509, loss=1.7884176969528198
I0307 11:40:31.330094 140252175811776 spec.py:321] Evaluating on the training split.
I0307 11:40:42.112955 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 11:41:08.700815 140252175811776 spec.py:349] Evaluating on the test split.
I0307 11:41:10.445169 140252175811776 submission_runner.py:469] Time since start: 32576.08s, 	Step: 70160, 	{'train/accuracy': 0.6653778553009033, 'train/loss': 1.3295236825942993, 'validation/accuracy': 0.6199600100517273, 'validation/loss': 1.5702780485153198, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.2733988761901855, 'test/num_examples': 10000, 'score': 30147.74630355835, 'total_duration': 32576.075915575027, 'accumulated_submission_time': 30147.74630355835, 'accumulated_eval_time': 2414.497104883194, 'accumulated_logging_time': 6.2238428592681885}
I0307 11:41:10.472264 140096095368960 logging_writer.py:48] [70160] accumulated_eval_time=2414.5, accumulated_logging_time=6.22384, accumulated_submission_time=30147.7, global_step=70160, preemption_count=0, score=30147.7, test/accuracy=0.4974, test/loss=2.2734, test/num_examples=10000, total_duration=32576.1, train/accuracy=0.665378, train/loss=1.32952, validation/accuracy=0.61996, validation/loss=1.57028, validation/num_examples=50000
I0307 11:41:26.690103 140096103761664 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.4821741580963135, loss=1.7494263648986816
I0307 11:42:06.905968 140096095368960 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.4397761821746826, loss=1.777256727218628
I0307 11:42:47.060610 140096103761664 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.869208335876465, loss=1.7423677444458008
I0307 11:43:26.623062 140096095368960 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.728827476501465, loss=1.8245913982391357
I0307 11:44:06.885442 140096103761664 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.8895537853240967, loss=1.654931664466858
I0307 11:44:47.330319 140096095368960 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.7373874187469482, loss=1.80359947681427
I0307 11:45:27.178654 140096103761664 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.756096839904785, loss=1.7657374143600464
I0307 11:46:07.126169 140096095368960 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.9386792182922363, loss=1.6921181678771973
I0307 11:46:47.596397 140096103761664 logging_writer.py:48] [71000] global_step=71000, grad_norm=4.128737449645996, loss=1.915840744972229
I0307 11:47:27.699857 140096095368960 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.676413059234619, loss=1.7964462041854858
I0307 11:48:07.819142 140096103761664 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.655285120010376, loss=1.8587911128997803
I0307 11:48:47.786182 140096095368960 logging_writer.py:48] [71300] global_step=71300, grad_norm=4.181647777557373, loss=1.9359467029571533
I0307 11:49:27.328120 140096103761664 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.4984288215637207, loss=1.682386040687561
I0307 11:49:40.546960 140252175811776 spec.py:321] Evaluating on the training split.
I0307 11:49:51.136444 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 11:50:16.893103 140252175811776 spec.py:349] Evaluating on the test split.
I0307 11:50:18.671409 140252175811776 submission_runner.py:469] Time since start: 33124.30s, 	Step: 71434, 	{'train/accuracy': 0.6695232391357422, 'train/loss': 1.3318610191345215, 'validation/accuracy': 0.6173799633979797, 'validation/loss': 1.589481234550476, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.2894232273101807, 'test/num_examples': 10000, 'score': 30657.65567755699, 'total_duration': 33124.302161216736, 'accumulated_submission_time': 30657.65567755699, 'accumulated_eval_time': 2452.621523141861, 'accumulated_logging_time': 6.25874400138855}
I0307 11:50:18.698314 140096095368960 logging_writer.py:48] [71434] accumulated_eval_time=2452.62, accumulated_logging_time=6.25874, accumulated_submission_time=30657.7, global_step=71434, preemption_count=0, score=30657.7, test/accuracy=0.4951, test/loss=2.28942, test/num_examples=10000, total_duration=33124.3, train/accuracy=0.669523, train/loss=1.33186, validation/accuracy=0.61738, validation/loss=1.58948, validation/num_examples=50000
I0307 11:50:45.360046 140096103761664 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.4597785472869873, loss=1.6542105674743652
I0307 11:51:25.266503 140096095368960 logging_writer.py:48] [71600] global_step=71600, grad_norm=5.718694686889648, loss=1.8961260318756104
I0307 11:52:04.661460 140096103761664 logging_writer.py:48] [71700] global_step=71700, grad_norm=5.084299564361572, loss=1.8387218713760376
I0307 11:52:44.512208 140096095368960 logging_writer.py:48] [71800] global_step=71800, grad_norm=4.042604923248291, loss=1.7113577127456665
I0307 11:53:23.990429 140096103761664 logging_writer.py:48] [71900] global_step=71900, grad_norm=4.113410949707031, loss=1.7643426656723022
I0307 11:54:03.433660 140096095368960 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.4087002277374268, loss=1.6508615016937256
I0307 11:54:43.283056 140096103761664 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.900261402130127, loss=1.7315504550933838
I0307 11:55:23.440474 140096095368960 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.4668827056884766, loss=1.7504050731658936
I0307 11:56:02.755871 140096103761664 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.985365867614746, loss=1.6789186000823975
I0307 11:56:42.550634 140096095368960 logging_writer.py:48] [72400] global_step=72400, grad_norm=4.798190593719482, loss=1.883846402168274
I0307 11:57:23.101640 140096103761664 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.658450126647949, loss=1.8829237222671509
I0307 11:58:03.908487 140096095368960 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.6153321266174316, loss=1.7827799320220947
I0307 11:58:43.848126 140096103761664 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.4870858192443848, loss=1.6778583526611328
I0307 11:58:48.889122 140252175811776 spec.py:321] Evaluating on the training split.
I0307 11:59:00.208657 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 11:59:36.095015 140252175811776 spec.py:349] Evaluating on the test split.
I0307 11:59:37.845662 140252175811776 submission_runner.py:469] Time since start: 33683.48s, 	Step: 72712, 	{'train/accuracy': 0.6760203838348389, 'train/loss': 1.2870255708694458, 'validation/accuracy': 0.6233999729156494, 'validation/loss': 1.5428450107574463, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.2536087036132812, 'test/num_examples': 10000, 'score': 31167.679332971573, 'total_duration': 33683.47640490532, 'accumulated_submission_time': 31167.679332971573, 'accumulated_eval_time': 2501.578024148941, 'accumulated_logging_time': 6.294565439224243}
I0307 11:59:37.882873 140096095368960 logging_writer.py:48] [72712] accumulated_eval_time=2501.58, accumulated_logging_time=6.29457, accumulated_submission_time=31167.7, global_step=72712, preemption_count=0, score=31167.7, test/accuracy=0.5037, test/loss=2.25361, test/num_examples=10000, total_duration=33683.5, train/accuracy=0.67602, train/loss=1.28703, validation/accuracy=0.6234, validation/loss=1.54285, validation/num_examples=50000
I0307 12:00:12.810299 140096103761664 logging_writer.py:48] [72800] global_step=72800, grad_norm=4.07911491394043, loss=1.8235303163528442
I0307 12:00:52.025646 140096095368960 logging_writer.py:48] [72900] global_step=72900, grad_norm=4.008357048034668, loss=1.733059048652649
I0307 12:01:31.865529 140096103761664 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.5370612144470215, loss=1.7770884037017822
I0307 12:02:11.420438 140096095368960 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.5740795135498047, loss=1.7790266275405884
I0307 12:02:50.694882 140096103761664 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.9831254482269287, loss=1.891499400138855
I0307 12:03:30.776083 140096095368960 logging_writer.py:48] [73300] global_step=73300, grad_norm=4.122964859008789, loss=1.8136322498321533
I0307 12:04:10.511619 140096103761664 logging_writer.py:48] [73400] global_step=73400, grad_norm=4.126482009887695, loss=1.7928165197372437
I0307 12:04:50.298979 140096095368960 logging_writer.py:48] [73500] global_step=73500, grad_norm=4.117727279663086, loss=1.7769283056259155
I0307 12:05:29.466536 140096103761664 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.97578763961792, loss=1.7842671871185303
I0307 12:06:09.145793 140096095368960 logging_writer.py:48] [73700] global_step=73700, grad_norm=4.1409993171691895, loss=1.8466193675994873
2025-03-07 12:06:46.282425: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:06:48.531267 140096103761664 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.918424367904663, loss=1.7753148078918457
I0307 12:07:27.954411 140096095368960 logging_writer.py:48] [73900] global_step=73900, grad_norm=4.432669639587402, loss=1.83944571018219
I0307 12:08:07.740772 140096103761664 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.4332690238952637, loss=1.6783044338226318
I0307 12:08:08.164685 140252175811776 spec.py:321] Evaluating on the training split.
I0307 12:08:18.737975 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 12:08:42.464671 140252175811776 spec.py:349] Evaluating on the test split.
I0307 12:08:44.232434 140252175811776 submission_runner.py:469] Time since start: 34229.86s, 	Step: 74002, 	{'train/accuracy': 0.6828961968421936, 'train/loss': 1.2402745485305786, 'validation/accuracy': 0.6353999972343445, 'validation/loss': 1.4952324628829956, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2025585174560547, 'test/num_examples': 10000, 'score': 31677.792944431305, 'total_duration': 34229.863186597824, 'accumulated_submission_time': 31677.792944431305, 'accumulated_eval_time': 2537.645740032196, 'accumulated_logging_time': 6.3397369384765625}
I0307 12:08:44.311257 140096095368960 logging_writer.py:48] [74002] accumulated_eval_time=2537.65, accumulated_logging_time=6.33974, accumulated_submission_time=31677.8, global_step=74002, preemption_count=0, score=31677.8, test/accuracy=0.5101, test/loss=2.20256, test/num_examples=10000, total_duration=34229.9, train/accuracy=0.682896, train/loss=1.24027, validation/accuracy=0.6354, validation/loss=1.49523, validation/num_examples=50000
I0307 12:09:22.967954 140096103761664 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.622382879257202, loss=1.7415878772735596
I0307 12:10:02.093099 140096095368960 logging_writer.py:48] [74200] global_step=74200, grad_norm=4.394782543182373, loss=1.9010136127471924
I0307 12:10:41.323157 140096103761664 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.7541348934173584, loss=1.7886261940002441
I0307 12:11:20.553558 140096095368960 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.6948373317718506, loss=1.8512383699417114
I0307 12:11:59.385840 140096103761664 logging_writer.py:48] [74500] global_step=74500, grad_norm=4.0826096534729, loss=1.9468272924423218
I0307 12:12:38.590709 140096095368960 logging_writer.py:48] [74600] global_step=74600, grad_norm=4.090129375457764, loss=1.6823986768722534
I0307 12:13:18.543247 140096103761664 logging_writer.py:48] [74700] global_step=74700, grad_norm=4.43306827545166, loss=1.8502379655838013
I0307 12:13:58.281684 140096095368960 logging_writer.py:48] [74800] global_step=74800, grad_norm=4.116445064544678, loss=1.7611820697784424
I0307 12:14:38.262631 140096103761664 logging_writer.py:48] [74900] global_step=74900, grad_norm=4.158354759216309, loss=1.928856611251831
I0307 12:15:17.880395 140096095368960 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.978914737701416, loss=1.765716314315796
2025-03-07 12:15:34.149525: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:15:57.389050 140096103761664 logging_writer.py:48] [75100] global_step=75100, grad_norm=4.051770210266113, loss=1.85874605178833
I0307 12:16:36.311183 140096095368960 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.9342167377471924, loss=1.9267326593399048
I0307 12:17:14.336777 140252175811776 spec.py:321] Evaluating on the training split.
I0307 12:17:24.668199 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 12:17:48.190478 140252175811776 spec.py:349] Evaluating on the test split.
I0307 12:17:50.197990 140252175811776 submission_runner.py:469] Time since start: 34775.83s, 	Step: 75298, 	{'train/accuracy': 0.6762993931770325, 'train/loss': 1.2758315801620483, 'validation/accuracy': 0.6263599991798401, 'validation/loss': 1.5363458395004272, 'validation/num_examples': 50000, 'test/accuracy': 0.5016000270843506, 'test/loss': 2.235443592071533, 'test/num_examples': 10000, 'score': 32187.651284456253, 'total_duration': 34775.82873630524, 'accumulated_submission_time': 32187.651284456253, 'accumulated_eval_time': 2573.5069131851196, 'accumulated_logging_time': 6.426562786102295}
I0307 12:17:50.265201 140096103761664 logging_writer.py:48] [75298] accumulated_eval_time=2573.51, accumulated_logging_time=6.42656, accumulated_submission_time=32187.7, global_step=75298, preemption_count=0, score=32187.7, test/accuracy=0.5016, test/loss=2.23544, test/num_examples=10000, total_duration=34775.8, train/accuracy=0.676299, train/loss=1.27583, validation/accuracy=0.62636, validation/loss=1.53635, validation/num_examples=50000
I0307 12:17:51.374116 140096095368960 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.9692482948303223, loss=1.7431195974349976
I0307 12:18:30.559401 140096103761664 logging_writer.py:48] [75400] global_step=75400, grad_norm=4.153639316558838, loss=1.6776434183120728
I0307 12:19:09.276157 140096095368960 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.4056878089904785, loss=1.7487205266952515
I0307 12:19:48.200298 140096103761664 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.758265256881714, loss=1.6894655227661133
I0307 12:20:27.374614 140096095368960 logging_writer.py:48] [75700] global_step=75700, grad_norm=4.003751754760742, loss=1.7861088514328003
I0307 12:21:06.345831 140096103761664 logging_writer.py:48] [75800] global_step=75800, grad_norm=4.112090110778809, loss=1.8056811094284058
I0307 12:21:44.776086 140096095368960 logging_writer.py:48] [75900] global_step=75900, grad_norm=4.035854816436768, loss=1.6468135118484497
I0307 12:22:23.703985 140096103761664 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.900381565093994, loss=1.8204925060272217
I0307 12:23:03.277778 140096095368960 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.795393705368042, loss=1.7655709981918335
I0307 12:23:42.210160 140096103761664 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.8245456218719482, loss=1.778751254081726
2025-03-07 12:24:18.963331: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:24:21.845432 140096095368960 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.5802130699157715, loss=1.7736494541168213
I0307 12:25:01.224318 140096103761664 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.6495227813720703, loss=1.7123613357543945
I0307 12:25:40.165904 140096095368960 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.5903913974761963, loss=1.74900484085083
I0307 12:26:19.561347 140096103761664 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.6585912704467773, loss=1.800265908241272
I0307 12:26:20.323524 140252175811776 spec.py:321] Evaluating on the training split.
I0307 12:26:31.106537 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 12:26:59.420738 140252175811776 spec.py:349] Evaluating on the test split.
I0307 12:27:01.158856 140252175811776 submission_runner.py:469] Time since start: 35326.79s, 	Step: 76603, 	{'train/accuracy': 0.6835339665412903, 'train/loss': 1.251558542251587, 'validation/accuracy': 0.631339967250824, 'validation/loss': 1.5285115242004395, 'validation/num_examples': 50000, 'test/accuracy': 0.49960002303123474, 'test/loss': 2.238071918487549, 'test/num_examples': 10000, 'score': 32697.536794900894, 'total_duration': 35326.78960728645, 'accumulated_submission_time': 32697.536794900894, 'accumulated_eval_time': 2614.3422091007233, 'accumulated_logging_time': 6.507396697998047}
I0307 12:27:01.177216 140096095368960 logging_writer.py:48] [76603] accumulated_eval_time=2614.34, accumulated_logging_time=6.5074, accumulated_submission_time=32697.5, global_step=76603, preemption_count=0, score=32697.5, test/accuracy=0.4996, test/loss=2.23807, test/num_examples=10000, total_duration=35326.8, train/accuracy=0.683534, train/loss=1.25156, validation/accuracy=0.63134, validation/loss=1.52851, validation/num_examples=50000
I0307 12:27:39.403240 140096103761664 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.6728169918060303, loss=1.785409927368164
I0307 12:28:18.353496 140096095368960 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.842754364013672, loss=1.6984469890594482
I0307 12:28:57.688671 140096103761664 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.8905608654022217, loss=1.8098711967468262
I0307 12:29:36.833511 140096095368960 logging_writer.py:48] [77000] global_step=77000, grad_norm=4.175550937652588, loss=1.791818618774414
I0307 12:30:16.113443 140096103761664 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.6028668880462646, loss=1.7231266498565674
I0307 12:30:55.374693 140096095368960 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.7908308506011963, loss=1.7462263107299805
I0307 12:31:34.715005 140096103761664 logging_writer.py:48] [77300] global_step=77300, grad_norm=4.040565490722656, loss=1.8226608037948608
I0307 12:32:14.602943 140096095368960 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.514207124710083, loss=1.7576911449432373
I0307 12:32:54.143566 140096103761664 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.901627779006958, loss=1.7894457578659058
2025-03-07 12:33:11.455199: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:33:33.839819 140096095368960 logging_writer.py:48] [77600] global_step=77600, grad_norm=4.067896366119385, loss=1.7277085781097412
I0307 12:34:13.257310 140096103761664 logging_writer.py:48] [77700] global_step=77700, grad_norm=4.010326862335205, loss=1.7009364366531372
I0307 12:34:52.335617 140096095368960 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.576558828353882, loss=1.7283252477645874
I0307 12:35:31.205865 140096103761664 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.810609817504883, loss=1.7395085096359253
I0307 12:35:31.217007 140252175811776 spec.py:321] Evaluating on the training split.
I0307 12:35:42.094412 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 12:36:04.275192 140252175811776 spec.py:349] Evaluating on the test split.
I0307 12:36:06.069808 140252175811776 submission_runner.py:469] Time since start: 35871.70s, 	Step: 77901, 	{'train/accuracy': 0.6845304369926453, 'train/loss': 1.2409714460372925, 'validation/accuracy': 0.6314799785614014, 'validation/loss': 1.511762261390686, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.210052251815796, 'test/num_examples': 10000, 'score': 33207.410826444626, 'total_duration': 35871.70054578781, 'accumulated_submission_time': 33207.410826444626, 'accumulated_eval_time': 2649.194940328598, 'accumulated_logging_time': 6.533376693725586}
I0307 12:36:06.121239 140096095368960 logging_writer.py:48] [77901] accumulated_eval_time=2649.19, accumulated_logging_time=6.53338, accumulated_submission_time=33207.4, global_step=77901, preemption_count=0, score=33207.4, test/accuracy=0.5041, test/loss=2.21005, test/num_examples=10000, total_duration=35871.7, train/accuracy=0.68453, train/loss=1.24097, validation/accuracy=0.63148, validation/loss=1.51176, validation/num_examples=50000
I0307 12:36:45.333672 140096103761664 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.7691643238067627, loss=1.7272553443908691
I0307 12:37:24.285010 140096095368960 logging_writer.py:48] [78100] global_step=78100, grad_norm=4.060367107391357, loss=1.7338063716888428
I0307 12:38:03.316804 140096103761664 logging_writer.py:48] [78200] global_step=78200, grad_norm=4.528324604034424, loss=1.8791877031326294
I0307 12:38:42.018520 140096095368960 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.7558276653289795, loss=1.7984516620635986
I0307 12:39:20.669107 140096103761664 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.6397712230682373, loss=1.7624026536941528
I0307 12:40:00.008109 140096095368960 logging_writer.py:48] [78500] global_step=78500, grad_norm=4.0855793952941895, loss=1.8136935234069824
I0307 12:40:39.009840 140096103761664 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.7981832027435303, loss=1.6754202842712402
I0307 12:41:17.825487 140096095368960 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.7963802814483643, loss=1.8535879850387573
2025-03-07 12:41:54.694622: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:41:56.809201 140096103761664 logging_writer.py:48] [78800] global_step=78800, grad_norm=4.074248790740967, loss=1.7941755056381226
I0307 12:42:35.576830 140096095368960 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.969677209854126, loss=1.746269941329956
I0307 12:43:13.912806 140096103761664 logging_writer.py:48] [79000] global_step=79000, grad_norm=4.016808032989502, loss=1.806855320930481
I0307 12:43:52.160798 140096095368960 logging_writer.py:48] [79100] global_step=79100, grad_norm=4.117951393127441, loss=1.7651124000549316
I0307 12:44:30.737337 140096103761664 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.9210896492004395, loss=1.8301783800125122
I0307 12:44:36.408105 140252175811776 spec.py:321] Evaluating on the training split.
I0307 12:44:46.876073 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 12:45:08.453794 140252175811776 spec.py:349] Evaluating on the test split.
I0307 12:45:10.253612 140252175811776 submission_runner.py:469] Time since start: 36415.88s, 	Step: 79215, 	{'train/accuracy': 0.6728914380073547, 'train/loss': 1.2973780632019043, 'validation/accuracy': 0.62882000207901, 'validation/loss': 1.5235060453414917, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.242490291595459, 'test/num_examples': 10000, 'score': 33717.53262710571, 'total_duration': 36415.88436079025, 'accumulated_submission_time': 33717.53262710571, 'accumulated_eval_time': 2683.040412902832, 'accumulated_logging_time': 6.592703819274902}
I0307 12:45:10.278975 140096095368960 logging_writer.py:48] [79215] accumulated_eval_time=2683.04, accumulated_logging_time=6.5927, accumulated_submission_time=33717.5, global_step=79215, preemption_count=0, score=33717.5, test/accuracy=0.5041, test/loss=2.24249, test/num_examples=10000, total_duration=36415.9, train/accuracy=0.672891, train/loss=1.29738, validation/accuracy=0.62882, validation/loss=1.52351, validation/num_examples=50000
I0307 12:45:43.460120 140096103761664 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.8132574558258057, loss=1.7194483280181885
I0307 12:46:22.952625 140096095368960 logging_writer.py:48] [79400] global_step=79400, grad_norm=4.755044937133789, loss=1.8178818225860596
I0307 12:47:02.573559 140096103761664 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.7989349365234375, loss=1.714097499847412
I0307 12:47:41.878922 140096095368960 logging_writer.py:48] [79600] global_step=79600, grad_norm=4.387035846710205, loss=1.8251984119415283
I0307 12:48:21.430047 140096103761664 logging_writer.py:48] [79700] global_step=79700, grad_norm=5.013251781463623, loss=1.6650359630584717
I0307 12:49:00.376528 140096095368960 logging_writer.py:48] [79800] global_step=79800, grad_norm=4.084684371948242, loss=1.8022927045822144
I0307 12:49:39.269633 140096103761664 logging_writer.py:48] [79900] global_step=79900, grad_norm=4.101185321807861, loss=1.6679821014404297
I0307 12:50:18.197524 140096095368960 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.621589422225952, loss=1.7459449768066406
2025-03-07 12:50:36.082263: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:50:57.090381 140096103761664 logging_writer.py:48] [80100] global_step=80100, grad_norm=4.080331325531006, loss=1.7973036766052246
I0307 12:51:35.956248 140096095368960 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.910076856613159, loss=1.7976270914077759
I0307 12:52:14.986275 140096103761664 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.607032775878906, loss=1.8007856607437134
I0307 12:52:54.165663 140096095368960 logging_writer.py:48] [80400] global_step=80400, grad_norm=4.585700511932373, loss=1.799900770187378
I0307 12:53:32.714025 140096103761664 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.7674272060394287, loss=1.804031491279602
I0307 12:53:40.321089 140252175811776 spec.py:321] Evaluating on the training split.
I0307 12:53:51.224194 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 12:54:18.408335 140252175811776 spec.py:349] Evaluating on the test split.
I0307 12:54:20.152617 140252175811776 submission_runner.py:469] Time since start: 36965.78s, 	Step: 80521, 	{'train/accuracy': 0.6913862824440002, 'train/loss': 1.2131707668304443, 'validation/accuracy': 0.6385599970817566, 'validation/loss': 1.487841248512268, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.1738009452819824, 'test/num_examples': 10000, 'score': 34227.40818238258, 'total_duration': 36965.783370018005, 'accumulated_submission_time': 34227.40818238258, 'accumulated_eval_time': 2722.8719069957733, 'accumulated_logging_time': 6.62582802772522}
I0307 12:54:20.221334 140096095368960 logging_writer.py:48] [80521] accumulated_eval_time=2722.87, accumulated_logging_time=6.62583, accumulated_submission_time=34227.4, global_step=80521, preemption_count=0, score=34227.4, test/accuracy=0.5272, test/loss=2.1738, test/num_examples=10000, total_duration=36965.8, train/accuracy=0.691386, train/loss=1.21317, validation/accuracy=0.63856, validation/loss=1.48784, validation/num_examples=50000
I0307 12:54:51.913496 140096103761664 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.3005073070526123, loss=1.6276214122772217
I0307 12:55:32.030937 140096095368960 logging_writer.py:48] [80700] global_step=80700, grad_norm=4.573435306549072, loss=1.7330782413482666
I0307 12:56:11.678829 140096103761664 logging_writer.py:48] [80800] global_step=80800, grad_norm=4.546445846557617, loss=1.7761621475219727
I0307 12:56:50.312313 140096095368960 logging_writer.py:48] [80900] global_step=80900, grad_norm=4.020248889923096, loss=1.657148838043213
I0307 12:57:29.381124 140096103761664 logging_writer.py:48] [81000] global_step=81000, grad_norm=4.565606594085693, loss=1.8780248165130615
I0307 12:58:08.216376 140096095368960 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.479482650756836, loss=1.7515027523040771
I0307 12:58:47.324611 140096103761664 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.433180809020996, loss=1.8018851280212402
2025-03-07 12:59:25.319130: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:59:26.288935 140096095368960 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.8597686290740967, loss=1.6990444660186768
I0307 13:00:05.361850 140096103761664 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.8098561763763428, loss=1.7655577659606934
I0307 13:00:44.386023 140096095368960 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.303124904632568, loss=1.8673802614212036
I0307 13:01:23.044502 140096103761664 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.8127455711364746, loss=1.6695657968521118
I0307 13:02:01.663352 140096095368960 logging_writer.py:48] [81700] global_step=81700, grad_norm=4.90493106842041, loss=1.6241437196731567
I0307 13:02:40.160684 140096103761664 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.8775246143341064, loss=1.7554699182510376
I0307 13:02:50.165964 140252175811776 spec.py:321] Evaluating on the training split.
I0307 13:03:00.231131 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 13:03:24.180237 140252175811776 spec.py:349] Evaluating on the test split.
I0307 13:03:25.986123 140252175811776 submission_runner.py:469] Time since start: 37511.62s, 	Step: 81827, 	{'train/accuracy': 0.6926419138908386, 'train/loss': 1.2114121913909912, 'validation/accuracy': 0.6388799548149109, 'validation/loss': 1.4778668880462646, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.1856496334075928, 'test/num_examples': 10000, 'score': 34737.17697548866, 'total_duration': 37511.616874456406, 'accumulated_submission_time': 34737.17697548866, 'accumulated_eval_time': 2758.6920306682587, 'accumulated_logging_time': 6.711660623550415}
I0307 13:03:26.029705 140096095368960 logging_writer.py:48] [81827] accumulated_eval_time=2758.69, accumulated_logging_time=6.71166, accumulated_submission_time=34737.2, global_step=81827, preemption_count=0, score=34737.2, test/accuracy=0.5165, test/loss=2.18565, test/num_examples=10000, total_duration=37511.6, train/accuracy=0.692642, train/loss=1.21141, validation/accuracy=0.63888, validation/loss=1.47787, validation/num_examples=50000
I0307 13:03:55.266736 140096103761664 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.9694809913635254, loss=1.861297369003296
I0307 13:04:35.238112 140096095368960 logging_writer.py:48] [82000] global_step=82000, grad_norm=4.120326519012451, loss=1.760446310043335
I0307 13:05:14.314582 140096103761664 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.499884843826294, loss=1.6518276929855347
I0307 13:05:54.380238 140096095368960 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.9835102558135986, loss=1.6947659254074097
I0307 13:06:34.067861 140096103761664 logging_writer.py:48] [82300] global_step=82300, grad_norm=4.081031322479248, loss=1.691084623336792
I0307 13:07:13.609766 140096095368960 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.5628812313079834, loss=1.754075050354004
I0307 13:07:53.792257 140096103761664 logging_writer.py:48] [82500] global_step=82500, grad_norm=4.52027702331543, loss=1.7318637371063232
2025-03-07 13:08:14.182733: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:08:33.346486 140096095368960 logging_writer.py:48] [82600] global_step=82600, grad_norm=4.075281143188477, loss=1.808708906173706
I0307 13:09:12.987591 140096103761664 logging_writer.py:48] [82700] global_step=82700, grad_norm=4.918679237365723, loss=1.6839278936386108
I0307 13:09:52.618249 140096095368960 logging_writer.py:48] [82800] global_step=82800, grad_norm=4.005565643310547, loss=1.6710712909698486
I0307 13:10:32.617302 140096103761664 logging_writer.py:48] [82900] global_step=82900, grad_norm=4.24836540222168, loss=1.7420158386230469
I0307 13:11:12.250168 140096095368960 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.4784369468688965, loss=1.7230299711227417
I0307 13:11:51.882797 140096103761664 logging_writer.py:48] [83100] global_step=83100, grad_norm=4.111600399017334, loss=1.6530916690826416
I0307 13:11:56.259839 140252175811776 spec.py:321] Evaluating on the training split.
I0307 13:12:07.370438 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 13:12:30.979680 140252175811776 spec.py:349] Evaluating on the test split.
I0307 13:12:32.745441 140252175811776 submission_runner.py:469] Time since start: 38058.38s, 	Step: 83112, 	{'train/accuracy': 0.6907086968421936, 'train/loss': 1.2079216241836548, 'validation/accuracy': 0.6414200067520142, 'validation/loss': 1.4783744812011719, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.1825082302093506, 'test/num_examples': 10000, 'score': 35247.240793943405, 'total_duration': 38058.37618684769, 'accumulated_submission_time': 35247.240793943405, 'accumulated_eval_time': 2795.1775946617126, 'accumulated_logging_time': 6.763816833496094}
I0307 13:12:32.828197 140096095368960 logging_writer.py:48] [83112] accumulated_eval_time=2795.18, accumulated_logging_time=6.76382, accumulated_submission_time=35247.2, global_step=83112, preemption_count=0, score=35247.2, test/accuracy=0.5185, test/loss=2.18251, test/num_examples=10000, total_duration=38058.4, train/accuracy=0.690709, train/loss=1.20792, validation/accuracy=0.64142, validation/loss=1.47837, validation/num_examples=50000
I0307 13:13:07.783699 140096103761664 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.9220871925354004, loss=1.8242594003677368
I0307 13:13:47.728385 140096095368960 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.7610154151916504, loss=1.6453640460968018
I0307 13:14:27.220317 140096103761664 logging_writer.py:48] [83400] global_step=83400, grad_norm=4.2151947021484375, loss=1.710500955581665
I0307 13:15:07.076201 140096095368960 logging_writer.py:48] [83500] global_step=83500, grad_norm=5.030343532562256, loss=1.7170953750610352
I0307 13:15:45.451106 140096103761664 logging_writer.py:48] [83600] global_step=83600, grad_norm=4.044928073883057, loss=1.772234320640564
I0307 13:16:23.584273 140096095368960 logging_writer.py:48] [83700] global_step=83700, grad_norm=4.052133560180664, loss=1.6825330257415771
I0307 13:17:07.414247 140096103761664 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.805180072784424, loss=1.8651385307312012
2025-03-07 13:17:11.866358: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:17:47.163494 140096095368960 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.453048229217529, loss=1.8441722393035889
I0307 13:18:25.752284 140096103761664 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.6467976570129395, loss=1.8479297161102295
I0307 13:19:04.722067 140096095368960 logging_writer.py:48] [84100] global_step=84100, grad_norm=4.037833213806152, loss=1.7516976594924927
I0307 13:19:44.525309 140096103761664 logging_writer.py:48] [84200] global_step=84200, grad_norm=4.420492172241211, loss=1.6663864850997925
I0307 13:20:23.972595 140096095368960 logging_writer.py:48] [84300] global_step=84300, grad_norm=4.801235675811768, loss=1.715057373046875
I0307 13:21:02.851669 140252175811776 spec.py:321] Evaluating on the training split.
I0307 13:21:13.733248 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 13:21:33.439176 140252175811776 spec.py:349] Evaluating on the test split.
I0307 13:21:35.239921 140252175811776 submission_runner.py:469] Time since start: 38600.87s, 	Step: 84400, 	{'train/accuracy': 0.6739277839660645, 'train/loss': 1.2957476377487183, 'validation/accuracy': 0.6217600107192993, 'validation/loss': 1.5643218755722046, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.308706283569336, 'test/num_examples': 10000, 'score': 35757.09645867348, 'total_duration': 38600.87066411972, 'accumulated_submission_time': 35757.09645867348, 'accumulated_eval_time': 2827.5658082962036, 'accumulated_logging_time': 6.854340553283691}
I0307 13:21:35.307310 140096103761664 logging_writer.py:48] [84400] accumulated_eval_time=2827.57, accumulated_logging_time=6.85434, accumulated_submission_time=35757.1, global_step=84400, preemption_count=0, score=35757.1, test/accuracy=0.4951, test/loss=2.30871, test/num_examples=10000, total_duration=38600.9, train/accuracy=0.673928, train/loss=1.29575, validation/accuracy=0.62176, validation/loss=1.56432, validation/num_examples=50000
I0307 13:21:35.710600 140096095368960 logging_writer.py:48] [84400] global_step=84400, grad_norm=4.669229984283447, loss=1.7414968013763428
I0307 13:22:14.702046 140096103761664 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.9109537601470947, loss=1.8177770376205444
I0307 13:22:54.103376 140096095368960 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.583088397979736, loss=1.6684174537658691
I0307 13:23:33.899790 140096103761664 logging_writer.py:48] [84700] global_step=84700, grad_norm=4.662135124206543, loss=1.7633897066116333
I0307 13:24:13.799657 140096095368960 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.106324672698975, loss=1.813185691833496
I0307 13:24:53.698251 140096103761664 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.8586623668670654, loss=1.7579394578933716
I0307 13:25:33.338576 140096095368960 logging_writer.py:48] [85000] global_step=85000, grad_norm=4.4831461906433105, loss=1.7480134963989258
2025-03-07 13:25:55.049121: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:26:12.365539 140096103761664 logging_writer.py:48] [85100] global_step=85100, grad_norm=4.177931785583496, loss=1.7380856275558472
I0307 13:26:51.734778 140096095368960 logging_writer.py:48] [85200] global_step=85200, grad_norm=4.928372859954834, loss=1.795046329498291
I0307 13:27:31.031984 140096103761664 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.565434217453003, loss=1.6058433055877686
I0307 13:28:10.609958 140096095368960 logging_writer.py:48] [85400] global_step=85400, grad_norm=4.406334400177002, loss=1.6977273225784302
I0307 13:28:49.928226 140096103761664 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.047075271606445, loss=1.7438924312591553
I0307 13:29:29.302925 140096095368960 logging_writer.py:48] [85600] global_step=85600, grad_norm=4.4088568687438965, loss=1.7723944187164307
I0307 13:30:05.380756 140252175811776 spec.py:321] Evaluating on the training split.
I0307 13:30:16.382496 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 13:30:36.841751 140252175811776 spec.py:349] Evaluating on the test split.
I0307 13:30:38.635435 140252175811776 submission_runner.py:469] Time since start: 39144.27s, 	Step: 85692, 	{'train/accuracy': 0.6964684128761292, 'train/loss': 1.1936861276626587, 'validation/accuracy': 0.6434599757194519, 'validation/loss': 1.4651683568954468, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.168731927871704, 'test/num_examples': 10000, 'score': 36267.00308179855, 'total_duration': 39144.26618885994, 'accumulated_submission_time': 36267.00308179855, 'accumulated_eval_time': 2860.820461034775, 'accumulated_logging_time': 6.92926025390625}
I0307 13:30:38.674970 140096103761664 logging_writer.py:48] [85692] accumulated_eval_time=2860.82, accumulated_logging_time=6.92926, accumulated_submission_time=36267, global_step=85692, preemption_count=0, score=36267, test/accuracy=0.5177, test/loss=2.16873, test/num_examples=10000, total_duration=39144.3, train/accuracy=0.696468, train/loss=1.19369, validation/accuracy=0.64346, validation/loss=1.46517, validation/num_examples=50000
I0307 13:30:42.207560 140096095368960 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.8411359786987305, loss=1.7779781818389893
I0307 13:31:21.406313 140096103761664 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.8558130264282227, loss=1.613677978515625
I0307 13:32:00.882621 140096095368960 logging_writer.py:48] [85900] global_step=85900, grad_norm=4.28386926651001, loss=1.8108468055725098
I0307 13:32:40.584048 140096103761664 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.813908100128174, loss=1.6589927673339844
I0307 13:33:20.009796 140096095368960 logging_writer.py:48] [86100] global_step=86100, grad_norm=4.8465142250061035, loss=1.744856595993042
I0307 13:33:59.486773 140096103761664 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.797018051147461, loss=1.7082371711730957
I0307 13:34:38.301201 140096095368960 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.9171836376190186, loss=1.731908917427063
2025-03-07 13:34:42.376897: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:35:16.212730 140096103761664 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.8679585456848145, loss=1.6774842739105225
I0307 13:35:54.815625 140096095368960 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.173844814300537, loss=1.8685009479522705
I0307 13:36:32.626231 140096103761664 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.8550772666931152, loss=1.7617017030715942
I0307 13:37:10.242966 140096095368960 logging_writer.py:48] [86700] global_step=86700, grad_norm=4.550114154815674, loss=1.8636714220046997
I0307 13:37:48.337526 140096103761664 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.242151260375977, loss=1.6136966943740845
I0307 13:38:26.351371 140096095368960 logging_writer.py:48] [86900] global_step=86900, grad_norm=4.17996883392334, loss=1.6528425216674805
I0307 13:39:05.090140 140096103761664 logging_writer.py:48] [87000] global_step=87000, grad_norm=4.258289337158203, loss=1.7554235458374023
I0307 13:39:08.997553 140252175811776 spec.py:321] Evaluating on the training split.
I0307 13:39:20.610308 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 13:39:43.120155 140252175811776 spec.py:349] Evaluating on the test split.
I0307 13:39:44.902816 140252175811776 submission_runner.py:469] Time since start: 39690.53s, 	Step: 87011, 	{'train/accuracy': 0.6976243257522583, 'train/loss': 1.1826188564300537, 'validation/accuracy': 0.6449800133705139, 'validation/loss': 1.447867751121521, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.1379809379577637, 'test/num_examples': 10000, 'score': 36777.15213179588, 'total_duration': 39690.53355884552, 'accumulated_submission_time': 36777.15213179588, 'accumulated_eval_time': 2896.725681781769, 'accumulated_logging_time': 6.977392196655273}
I0307 13:39:44.940912 140096095368960 logging_writer.py:48] [87011] accumulated_eval_time=2896.73, accumulated_logging_time=6.97739, accumulated_submission_time=36777.2, global_step=87011, preemption_count=0, score=36777.2, test/accuracy=0.5233, test/loss=2.13798, test/num_examples=10000, total_duration=39690.5, train/accuracy=0.697624, train/loss=1.18262, validation/accuracy=0.64498, validation/loss=1.44787, validation/num_examples=50000
I0307 13:40:20.297133 140096103761664 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.8683366775512695, loss=1.6855562925338745
I0307 13:40:59.675408 140096095368960 logging_writer.py:48] [87200] global_step=87200, grad_norm=4.291659355163574, loss=1.6986228227615356
I0307 13:41:38.716122 140096103761664 logging_writer.py:48] [87300] global_step=87300, grad_norm=4.1619696617126465, loss=1.7462085485458374
I0307 13:42:17.595947 140096095368960 logging_writer.py:48] [87400] global_step=87400, grad_norm=4.2469072341918945, loss=1.7217351198196411
I0307 13:42:57.059678 140096103761664 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.21002721786499, loss=1.7649483680725098
2025-03-07 13:43:18.470094: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:43:36.523784 140096095368960 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.020070552825928, loss=1.7038171291351318
I0307 13:44:16.018329 140096103761664 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.8556885719299316, loss=1.6994515657424927
I0307 13:44:54.778066 140096095368960 logging_writer.py:48] [87800] global_step=87800, grad_norm=3.6107068061828613, loss=1.5928850173950195
I0307 13:45:33.356922 140096103761664 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.242729187011719, loss=1.7952206134796143
I0307 13:46:12.375724 140096095368960 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.046265602111816, loss=1.7221513986587524
I0307 13:46:51.359237 140096103761664 logging_writer.py:48] [88100] global_step=88100, grad_norm=4.086225509643555, loss=1.735550880432129
I0307 13:47:30.124657 140096095368960 logging_writer.py:48] [88200] global_step=88200, grad_norm=4.380927085876465, loss=1.7385112047195435
I0307 13:48:09.391391 140096103761664 logging_writer.py:48] [88300] global_step=88300, grad_norm=5.367260932922363, loss=1.734931468963623
I0307 13:48:15.227361 140252175811776 spec.py:321] Evaluating on the training split.
I0307 13:48:26.000825 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 13:48:50.051740 140252175811776 spec.py:349] Evaluating on the test split.
I0307 13:48:51.840017 140252175811776 submission_runner.py:469] Time since start: 40237.47s, 	Step: 88316, 	{'train/accuracy': 0.6968271732330322, 'train/loss': 1.1760650873184204, 'validation/accuracy': 0.6461399793624878, 'validation/loss': 1.4500885009765625, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.1409730911254883, 'test/num_examples': 10000, 'score': 37287.26937747002, 'total_duration': 40237.470774650574, 'accumulated_submission_time': 37287.26937747002, 'accumulated_eval_time': 2933.338308572769, 'accumulated_logging_time': 7.024348258972168}
I0307 13:48:51.880255 140096095368960 logging_writer.py:48] [88316] accumulated_eval_time=2933.34, accumulated_logging_time=7.02435, accumulated_submission_time=37287.3, global_step=88316, preemption_count=0, score=37287.3, test/accuracy=0.5304, test/loss=2.14097, test/num_examples=10000, total_duration=40237.5, train/accuracy=0.696827, train/loss=1.17607, validation/accuracy=0.64614, validation/loss=1.45009, validation/num_examples=50000
I0307 13:49:25.002506 140096103761664 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.84975004196167, loss=1.5763132572174072
I0307 13:50:04.668050 140096095368960 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.786193370819092, loss=1.7426631450653076
I0307 13:50:44.443351 140096103761664 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.652004241943359, loss=1.7324060201644897
I0307 13:51:23.807623 140096095368960 logging_writer.py:48] [88700] global_step=88700, grad_norm=4.2474164962768555, loss=1.6690667867660522
I0307 13:52:02.262211 140096103761664 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.6449689865112305, loss=1.6407910585403442
2025-03-07 13:52:08.306224: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:52:41.733969 140096095368960 logging_writer.py:48] [88900] global_step=88900, grad_norm=4.732034206390381, loss=1.681115746498108
I0307 13:53:19.312898 140096103761664 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.6244354248046875, loss=1.6942083835601807
I0307 13:53:57.771730 140096095368960 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.949113368988037, loss=1.7213960886001587
I0307 13:54:36.050193 140096103761664 logging_writer.py:48] [89200] global_step=89200, grad_norm=4.286473751068115, loss=1.7445834875106812
I0307 13:55:15.222878 140096095368960 logging_writer.py:48] [89300] global_step=89300, grad_norm=4.196256160736084, loss=1.6347851753234863
I0307 13:55:52.736225 140096103761664 logging_writer.py:48] [89400] global_step=89400, grad_norm=4.8376078605651855, loss=1.8447024822235107
I0307 13:56:31.036326 140096095368960 logging_writer.py:48] [89500] global_step=89500, grad_norm=4.000980854034424, loss=1.7279547452926636
I0307 13:57:09.047467 140096103761664 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.127979278564453, loss=1.762181282043457
I0307 13:57:21.956236 140252175811776 spec.py:321] Evaluating on the training split.
I0307 13:57:32.464409 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 13:57:56.776163 140252175811776 spec.py:349] Evaluating on the test split.
I0307 13:57:58.537014 140252175811776 submission_runner.py:469] Time since start: 40784.17s, 	Step: 89634, 	{'train/accuracy': 0.6983218789100647, 'train/loss': 1.1913554668426514, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.466795802116394, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1716911792755127, 'test/num_examples': 10000, 'score': 37797.17287135124, 'total_duration': 40784.16775941849, 'accumulated_submission_time': 37797.17287135124, 'accumulated_eval_time': 2969.9190514087677, 'accumulated_logging_time': 7.073339462280273}
I0307 13:57:58.567425 140096095368960 logging_writer.py:48] [89634] accumulated_eval_time=2969.92, accumulated_logging_time=7.07334, accumulated_submission_time=37797.2, global_step=89634, preemption_count=0, score=37797.2, test/accuracy=0.5225, test/loss=2.17169, test/num_examples=10000, total_duration=40784.2, train/accuracy=0.698322, train/loss=1.19136, validation/accuracy=0.64166, validation/loss=1.4668, validation/num_examples=50000
I0307 13:58:24.989851 140096103761664 logging_writer.py:48] [89700] global_step=89700, grad_norm=4.138995170593262, loss=1.667067289352417
I0307 13:59:04.909170 140096095368960 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.432056903839111, loss=1.7618626356124878
I0307 13:59:44.907693 140096103761664 logging_writer.py:48] [89900] global_step=89900, grad_norm=4.101263999938965, loss=1.6870958805084229
I0307 14:00:24.545593 140096095368960 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.7319726943969727, loss=1.6264690160751343
2025-03-07 14:00:47.482696: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:01:03.684639 140096103761664 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.372084617614746, loss=1.735926866531372
I0307 14:01:43.203749 140096095368960 logging_writer.py:48] [90200] global_step=90200, grad_norm=5.146813869476318, loss=1.6622445583343506
I0307 14:02:22.676385 140096103761664 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.8652918338775635, loss=1.6765267848968506
I0307 14:03:01.356066 140096095368960 logging_writer.py:48] [90400] global_step=90400, grad_norm=4.104153633117676, loss=1.7825920581817627
I0307 14:03:40.575334 140096103761664 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.405665397644043, loss=1.8435404300689697
I0307 14:04:19.933550 140096095368960 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.267421245574951, loss=1.7256057262420654
I0307 14:04:59.215554 140096103761664 logging_writer.py:48] [90700] global_step=90700, grad_norm=4.298868179321289, loss=1.6553858518600464
I0307 14:05:38.418161 140096095368960 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.5878586769104, loss=1.8025555610656738
I0307 14:06:17.611619 140096103761664 logging_writer.py:48] [90900] global_step=90900, grad_norm=4.635056972503662, loss=1.7091407775878906
I0307 14:06:28.802121 140252175811776 spec.py:321] Evaluating on the training split.
I0307 14:06:39.335180 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 14:07:04.671529 140252175811776 spec.py:349] Evaluating on the test split.
I0307 14:07:06.419147 140252175811776 submission_runner.py:469] Time since start: 41332.05s, 	Step: 90930, 	{'train/accuracy': 0.6973254084587097, 'train/loss': 1.1798337697982788, 'validation/accuracy': 0.6426199674606323, 'validation/loss': 1.4615617990493774, 'validation/num_examples': 50000, 'test/accuracy': 0.5235000252723694, 'test/loss': 2.1633799076080322, 'test/num_examples': 10000, 'score': 38307.24203848839, 'total_duration': 41332.04989647865, 'accumulated_submission_time': 38307.24203848839, 'accumulated_eval_time': 3007.5360400676727, 'accumulated_logging_time': 7.111950635910034}
I0307 14:07:06.476361 140096095368960 logging_writer.py:48] [90930] accumulated_eval_time=3007.54, accumulated_logging_time=7.11195, accumulated_submission_time=38307.2, global_step=90930, preemption_count=0, score=38307.2, test/accuracy=0.5235, test/loss=2.16338, test/num_examples=10000, total_duration=41332, train/accuracy=0.697325, train/loss=1.17983, validation/accuracy=0.64262, validation/loss=1.46156, validation/num_examples=50000
I0307 14:07:34.869831 140096103761664 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.253396034240723, loss=1.6598726511001587
I0307 14:08:14.465478 140096095368960 logging_writer.py:48] [91100] global_step=91100, grad_norm=4.636043071746826, loss=1.8578282594680786
I0307 14:08:53.821280 140096103761664 logging_writer.py:48] [91200] global_step=91200, grad_norm=4.47064208984375, loss=1.7784878015518188
I0307 14:09:32.984273 140096095368960 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.8335206508636475, loss=1.606190800666809
2025-03-07 14:09:35.675807: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:10:11.856201 140096103761664 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.639683723449707, loss=1.6696221828460693
I0307 14:10:51.332593 140096095368960 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.264395713806152, loss=1.676735520362854
I0307 14:11:30.174211 140096103761664 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.09633207321167, loss=1.6140919923782349
I0307 14:12:09.144785 140096095368960 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.099372863769531, loss=1.7046507596969604
I0307 14:12:47.962855 140096103761664 logging_writer.py:48] [91800] global_step=91800, grad_norm=4.575069427490234, loss=1.665230631828308
I0307 14:13:27.134248 140096095368960 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.906243801116943, loss=1.6344573497772217
I0307 14:14:06.878151 140096103761664 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.512994289398193, loss=1.8287506103515625
I0307 14:14:46.566463 140096095368960 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.284020900726318, loss=1.6373077630996704
I0307 14:15:26.097563 140096103761664 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.525963306427002, loss=1.7643754482269287
I0307 14:15:36.804854 140252175811776 spec.py:321] Evaluating on the training split.
I0307 14:15:47.985567 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 14:16:08.521564 140252175811776 spec.py:349] Evaluating on the test split.
I0307 14:16:10.313407 140252175811776 submission_runner.py:469] Time since start: 41875.94s, 	Step: 92228, 	{'train/accuracy': 0.7051976919174194, 'train/loss': 1.148040771484375, 'validation/accuracy': 0.649899959564209, 'validation/loss': 1.4317659139633179, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1536617279052734, 'test/num_examples': 10000, 'score': 38817.4027094841, 'total_duration': 41875.944149017334, 'accumulated_submission_time': 38817.4027094841, 'accumulated_eval_time': 3041.0445544719696, 'accumulated_logging_time': 7.177502632141113}
I0307 14:16:10.349371 140096095368960 logging_writer.py:48] [92228] accumulated_eval_time=3041.04, accumulated_logging_time=7.1775, accumulated_submission_time=38817.4, global_step=92228, preemption_count=0, score=38817.4, test/accuracy=0.5205, test/loss=2.15366, test/num_examples=10000, total_duration=41875.9, train/accuracy=0.705198, train/loss=1.14804, validation/accuracy=0.6499, validation/loss=1.43177, validation/num_examples=50000
I0307 14:16:39.028150 140096103761664 logging_writer.py:48] [92300] global_step=92300, grad_norm=5.0107903480529785, loss=1.7133265733718872
I0307 14:17:18.676190 140096095368960 logging_writer.py:48] [92400] global_step=92400, grad_norm=4.5454325675964355, loss=1.8161669969558716
I0307 14:17:58.704425 140096103761664 logging_writer.py:48] [92500] global_step=92500, grad_norm=4.635836601257324, loss=1.7037246227264404
2025-03-07 14:18:22.697530: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:18:38.350732 140096095368960 logging_writer.py:48] [92600] global_step=92600, grad_norm=4.228765487670898, loss=1.7170337438583374
I0307 14:19:17.206026 140096103761664 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.124732971191406, loss=1.5976974964141846
I0307 14:19:57.108659 140096095368960 logging_writer.py:48] [92800] global_step=92800, grad_norm=3.8641321659088135, loss=1.6279375553131104
I0307 14:20:36.875598 140096103761664 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.138540744781494, loss=1.691706657409668
I0307 14:21:15.990939 140096095368960 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.648707866668701, loss=1.6851089000701904
I0307 14:21:55.124383 140096103761664 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.0019354820251465, loss=1.6037731170654297
I0307 14:22:34.810379 140096095368960 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.9274139404296875, loss=1.6426464319229126
I0307 14:23:13.924338 140096103761664 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.3395538330078125, loss=1.6291999816894531
I0307 14:23:53.683570 140096095368960 logging_writer.py:48] [93400] global_step=93400, grad_norm=5.463348388671875, loss=1.8023009300231934
I0307 14:24:33.115506 140096103761664 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.6722612380981445, loss=1.665123701095581
I0307 14:24:40.365880 140252175811776 spec.py:321] Evaluating on the training split.
I0307 14:24:50.899995 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 14:25:12.029346 140252175811776 spec.py:349] Evaluating on the test split.
I0307 14:25:13.817068 140252175811776 submission_runner.py:469] Time since start: 42419.45s, 	Step: 93520, 	{'train/accuracy': 0.7074099183082581, 'train/loss': 1.1465513706207275, 'validation/accuracy': 0.6511399745941162, 'validation/loss': 1.4233825206756592, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.1446824073791504, 'test/num_examples': 10000, 'score': 39327.2528822422, 'total_duration': 42419.44781208038, 'accumulated_submission_time': 39327.2528822422, 'accumulated_eval_time': 3074.495703458786, 'accumulated_logging_time': 7.222236394882202}
I0307 14:25:13.857661 140096095368960 logging_writer.py:48] [93520] accumulated_eval_time=3074.5, accumulated_logging_time=7.22224, accumulated_submission_time=39327.3, global_step=93520, preemption_count=0, score=39327.3, test/accuracy=0.5166, test/loss=2.14468, test/num_examples=10000, total_duration=42419.4, train/accuracy=0.70741, train/loss=1.14655, validation/accuracy=0.65114, validation/loss=1.42338, validation/num_examples=50000
I0307 14:25:45.581228 140096103761664 logging_writer.py:48] [93600] global_step=93600, grad_norm=4.146148204803467, loss=1.6755659580230713
I0307 14:26:25.219424 140096095368960 logging_writer.py:48] [93700] global_step=93700, grad_norm=4.2034478187561035, loss=1.6521662473678589
I0307 14:27:05.318972 140096103761664 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.012847423553467, loss=1.7249321937561035
2025-03-07 14:27:08.585372: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:27:44.470494 140096095368960 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.690066337585449, loss=1.6614964008331299
I0307 14:28:23.743413 140096103761664 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.248104572296143, loss=1.6914012432098389
I0307 14:29:03.046007 140096095368960 logging_writer.py:48] [94100] global_step=94100, grad_norm=4.015623569488525, loss=1.6579701900482178
I0307 14:29:43.041977 140096103761664 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.633459091186523, loss=1.5903806686401367
I0307 14:30:22.127141 140096095368960 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.505362033843994, loss=1.6715749502182007
I0307 14:31:01.110169 140096103761664 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.134527206420898, loss=1.6288256645202637
I0307 14:31:40.168738 140096095368960 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.9575092792510986, loss=1.565280795097351
I0307 14:32:18.851680 140096103761664 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.453876972198486, loss=1.8185392618179321
I0307 14:32:57.943682 140096095368960 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.086920261383057, loss=1.507338285446167
I0307 14:33:37.130338 140096103761664 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.154322624206543, loss=1.5746703147888184
I0307 14:33:43.882559 140252175811776 spec.py:321] Evaluating on the training split.
I0307 14:33:55.201704 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 14:34:17.743258 140252175811776 spec.py:349] Evaluating on the test split.
I0307 14:34:19.486487 140252175811776 submission_runner.py:469] Time since start: 42965.12s, 	Step: 94818, 	{'train/accuracy': 0.7074099183082581, 'train/loss': 1.1425827741622925, 'validation/accuracy': 0.6491599678993225, 'validation/loss': 1.4305096864700317, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.1142382621765137, 'test/num_examples': 10000, 'score': 39837.09855508804, 'total_duration': 42965.11723256111, 'accumulated_submission_time': 39837.09855508804, 'accumulated_eval_time': 3110.099593400955, 'accumulated_logging_time': 7.280663728713989}
I0307 14:34:19.563824 140096095368960 logging_writer.py:48] [94818] accumulated_eval_time=3110.1, accumulated_logging_time=7.28066, accumulated_submission_time=39837.1, global_step=94818, preemption_count=0, score=39837.1, test/accuracy=0.5328, test/loss=2.11424, test/num_examples=10000, total_duration=42965.1, train/accuracy=0.70741, train/loss=1.14258, validation/accuracy=0.64916, validation/loss=1.43051, validation/num_examples=50000
I0307 14:34:52.119043 140096103761664 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.2960920333862305, loss=1.7305004596710205
I0307 14:35:31.536402 140096095368960 logging_writer.py:48] [95000] global_step=95000, grad_norm=4.530911445617676, loss=1.6526682376861572
2025-03-07 14:35:55.522232: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:36:11.616822 140096103761664 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.115039825439453, loss=1.6174553632736206
I0307 14:36:50.687102 140096095368960 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.217645645141602, loss=1.5222543478012085
I0307 14:37:30.232583 140096103761664 logging_writer.py:48] [95300] global_step=95300, grad_norm=4.006622791290283, loss=1.6896666288375854
I0307 14:38:09.334222 140096095368960 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.500789642333984, loss=1.7261438369750977
I0307 14:38:48.059856 140096103761664 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.241451263427734, loss=1.691035270690918
I0307 14:39:27.688750 140096095368960 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.269129276275635, loss=1.6304389238357544
I0307 14:40:06.747903 140096103761664 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.104794502258301, loss=1.5067884922027588
I0307 14:40:45.472689 140096095368960 logging_writer.py:48] [95800] global_step=95800, grad_norm=3.7512729167938232, loss=1.629928708076477
I0307 14:41:24.946815 140096103761664 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.974071502685547, loss=1.6825034618377686
I0307 14:42:04.409790 140096095368960 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.059978485107422, loss=1.621631383895874
I0307 14:42:43.247628 140096103761664 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.113652229309082, loss=1.6489732265472412
I0307 14:42:49.658241 140252175811776 spec.py:321] Evaluating on the training split.
I0307 14:43:00.542698 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 14:43:22.318598 140252175811776 spec.py:349] Evaluating on the test split.
I0307 14:43:24.115677 140252175811776 submission_runner.py:469] Time since start: 43509.75s, 	Step: 96117, 	{'train/accuracy': 0.7081871628761292, 'train/loss': 1.1413853168487549, 'validation/accuracy': 0.6512199640274048, 'validation/loss': 1.4193195104599, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.103017568588257, 'test/num_examples': 10000, 'score': 40346.99287080765, 'total_duration': 43509.74643325806, 'accumulated_submission_time': 40346.99287080765, 'accumulated_eval_time': 3144.55699968338, 'accumulated_logging_time': 7.397853136062622}
I0307 14:43:24.163452 140096095368960 logging_writer.py:48] [96117] accumulated_eval_time=3144.56, accumulated_logging_time=7.39785, accumulated_submission_time=40347, global_step=96117, preemption_count=0, score=40347, test/accuracy=0.5309, test/loss=2.10302, test/num_examples=10000, total_duration=43509.7, train/accuracy=0.708187, train/loss=1.14139, validation/accuracy=0.65122, validation/loss=1.41932, validation/num_examples=50000
I0307 14:43:56.580527 140096103761664 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.906332969665527, loss=1.7585501670837402
I0307 14:44:36.021691 140096095368960 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.642770290374756, loss=1.631832480430603
2025-03-07 14:44:40.821163: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:45:15.681053 140096103761664 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.856705665588379, loss=1.7102184295654297
I0307 14:45:55.228329 140096095368960 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.0420732498168945, loss=1.6649798154830933
I0307 14:46:34.270495 140096103761664 logging_writer.py:48] [96600] global_step=96600, grad_norm=4.420753002166748, loss=1.5505820512771606
I0307 14:47:13.485344 140096095368960 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.546901702880859, loss=1.6925840377807617
I0307 14:47:52.894363 140096103761664 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.4784722328186035, loss=1.7459980249404907
I0307 14:48:32.571907 140096095368960 logging_writer.py:48] [96900] global_step=96900, grad_norm=4.141355991363525, loss=1.5792135000228882
I0307 14:49:11.802467 140096103761664 logging_writer.py:48] [97000] global_step=97000, grad_norm=4.20659875869751, loss=1.6161658763885498
I0307 14:49:51.319836 140096095368960 logging_writer.py:48] [97100] global_step=97100, grad_norm=4.215803146362305, loss=1.659973382949829
I0307 14:50:30.813353 140096103761664 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.0071330070495605, loss=1.6490596532821655
I0307 14:51:10.122458 140096095368960 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.099183082580566, loss=1.5791391134262085
I0307 14:51:49.558023 140096103761664 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.209519863128662, loss=1.756095290184021
I0307 14:51:54.346665 140252175811776 spec.py:321] Evaluating on the training split.
I0307 14:52:05.231052 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 14:52:27.724261 140252175811776 spec.py:349] Evaluating on the test split.
I0307 14:52:29.505087 140252175811776 submission_runner.py:469] Time since start: 44055.14s, 	Step: 97413, 	{'train/accuracy': 0.7104990482330322, 'train/loss': 1.1292941570281982, 'validation/accuracy': 0.6515399813652039, 'validation/loss': 1.4190757274627686, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.1343791484832764, 'test/num_examples': 10000, 'score': 40857.00880885124, 'total_duration': 44055.13583922386, 'accumulated_submission_time': 40857.00880885124, 'accumulated_eval_time': 3179.7153894901276, 'accumulated_logging_time': 7.4534080028533936}
I0307 14:52:29.576960 140096095368960 logging_writer.py:48] [97413] accumulated_eval_time=3179.72, accumulated_logging_time=7.45341, accumulated_submission_time=40857, global_step=97413, preemption_count=0, score=40857, test/accuracy=0.5267, test/loss=2.13438, test/num_examples=10000, total_duration=44055.1, train/accuracy=0.710499, train/loss=1.12929, validation/accuracy=0.65154, validation/loss=1.41908, validation/num_examples=50000
I0307 14:53:03.993679 140096103761664 logging_writer.py:48] [97500] global_step=97500, grad_norm=4.274423122406006, loss=1.5483609437942505
2025-03-07 14:53:30.324752: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:53:42.603828 140096095368960 logging_writer.py:48] [97600] global_step=97600, grad_norm=5.014354705810547, loss=1.6359909772872925
I0307 14:54:20.369174 140096103761664 logging_writer.py:48] [97700] global_step=97700, grad_norm=5.1689324378967285, loss=1.6237308979034424
I0307 14:54:59.677303 140096095368960 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.894786834716797, loss=1.6895391941070557
I0307 14:55:39.214257 140096103761664 logging_writer.py:48] [97900] global_step=97900, grad_norm=3.9978418350219727, loss=1.6954594850540161
I0307 14:56:18.670151 140096095368960 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.167754650115967, loss=1.6395031213760376
I0307 14:56:58.155471 140096103761664 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.958169460296631, loss=1.6150119304656982
I0307 14:57:37.675688 140096095368960 logging_writer.py:48] [98200] global_step=98200, grad_norm=4.420430660247803, loss=1.6610679626464844
I0307 14:58:17.223757 140096103761664 logging_writer.py:48] [98300] global_step=98300, grad_norm=6.256305694580078, loss=1.602105975151062
I0307 14:58:56.148342 140096095368960 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.9496965408325195, loss=1.6539227962493896
I0307 14:59:35.639361 140096103761664 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.3543877601623535, loss=1.4958282709121704
I0307 15:00:14.915784 140096095368960 logging_writer.py:48] [98600] global_step=98600, grad_norm=5.268344402313232, loss=1.639236330986023
I0307 15:00:54.512220 140096103761664 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.790926456451416, loss=1.5852376222610474
I0307 15:00:59.639801 140252175811776 spec.py:321] Evaluating on the training split.
I0307 15:01:10.405899 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 15:01:35.180869 140252175811776 spec.py:349] Evaluating on the test split.
I0307 15:01:36.930147 140252175811776 submission_runner.py:469] Time since start: 44602.56s, 	Step: 98714, 	{'train/accuracy': 0.7137077450752258, 'train/loss': 1.1122026443481445, 'validation/accuracy': 0.6550799608230591, 'validation/loss': 1.4026848077774048, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.080806255340576, 'test/num_examples': 10000, 'score': 41366.90090370178, 'total_duration': 44602.56089806557, 'accumulated_submission_time': 41366.90090370178, 'accumulated_eval_time': 3217.0057003498077, 'accumulated_logging_time': 7.5330095291137695}
I0307 15:01:36.977732 140096095368960 logging_writer.py:48] [98714] accumulated_eval_time=3217.01, accumulated_logging_time=7.53301, accumulated_submission_time=41366.9, global_step=98714, preemption_count=0, score=41366.9, test/accuracy=0.5362, test/loss=2.08081, test/num_examples=10000, total_duration=44602.6, train/accuracy=0.713708, train/loss=1.1122, validation/accuracy=0.65508, validation/loss=1.40268, validation/num_examples=50000
I0307 15:02:11.165966 140096103761664 logging_writer.py:48] [98800] global_step=98800, grad_norm=5.048908710479736, loss=1.5757873058319092
2025-03-07 15:02:17.488466: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:02:50.879958 140096095368960 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.747556209564209, loss=1.6939409971237183
I0307 15:03:30.163139 140096103761664 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.9571404457092285, loss=1.7015565633773804
I0307 15:04:09.289588 140096095368960 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.333259105682373, loss=1.6764512062072754
I0307 15:04:47.958677 140096103761664 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.073323726654053, loss=1.63133704662323
I0307 15:05:27.209913 140096095368960 logging_writer.py:48] [99300] global_step=99300, grad_norm=4.194674491882324, loss=1.6075067520141602
I0307 15:06:06.699811 140096103761664 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.504920482635498, loss=1.6555016040802002
I0307 15:06:46.152914 140096095368960 logging_writer.py:48] [99500] global_step=99500, grad_norm=5.298096179962158, loss=1.5819921493530273
I0307 15:07:25.563191 140096103761664 logging_writer.py:48] [99600] global_step=99600, grad_norm=4.566737651824951, loss=1.6676127910614014
I0307 15:08:05.222226 140096095368960 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.353141784667969, loss=1.612130045890808
I0307 15:08:45.066435 140096103761664 logging_writer.py:48] [99800] global_step=99800, grad_norm=4.16649866104126, loss=1.614250659942627
I0307 15:09:24.306016 140096095368960 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.683199882507324, loss=1.6610188484191895
I0307 15:10:03.958937 140096103761664 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.673192501068115, loss=1.6307154893875122
I0307 15:10:07.122353 140252175811776 spec.py:321] Evaluating on the training split.
I0307 15:10:18.213724 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 15:10:44.593004 140252175811776 spec.py:349] Evaluating on the test split.
I0307 15:10:46.332997 140252175811776 submission_runner.py:469] Time since start: 45151.96s, 	Step: 100009, 	{'train/accuracy': 0.7061742544174194, 'train/loss': 1.1462246179580688, 'validation/accuracy': 0.6499199867248535, 'validation/loss': 1.42534601688385, 'validation/num_examples': 50000, 'test/accuracy': 0.5236999988555908, 'test/loss': 2.113070487976074, 'test/num_examples': 10000, 'score': 41876.876831531525, 'total_duration': 45151.96375083923, 'accumulated_submission_time': 41876.876831531525, 'accumulated_eval_time': 3256.2163178920746, 'accumulated_logging_time': 7.588646411895752}
I0307 15:10:46.422223 140096095368960 logging_writer.py:48] [100009] accumulated_eval_time=3256.22, accumulated_logging_time=7.58865, accumulated_submission_time=41876.9, global_step=100009, preemption_count=0, score=41876.9, test/accuracy=0.5237, test/loss=2.11307, test/num_examples=10000, total_duration=45152, train/accuracy=0.706174, train/loss=1.14622, validation/accuracy=0.64992, validation/loss=1.42535, validation/num_examples=50000
2025-03-07 15:11:08.449337: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:11:22.448031 140096103761664 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.345648765563965, loss=1.6915191411972046
I0307 15:12:01.437755 140096095368960 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.284139633178711, loss=1.6437585353851318
I0307 15:12:40.744500 140096103761664 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.321791648864746, loss=1.5113351345062256
I0307 15:13:20.079555 140096095368960 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.645036220550537, loss=1.6648621559143066
I0307 15:13:59.618898 140096103761664 logging_writer.py:48] [100500] global_step=100500, grad_norm=4.8694071769714355, loss=1.7164885997772217
I0307 15:14:38.984283 140096095368960 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.026840686798096, loss=1.7458049058914185
I0307 15:15:18.553878 140096103761664 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.511425495147705, loss=1.7249865531921387
I0307 15:15:57.725437 140096095368960 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.222443580627441, loss=1.653855800628662
I0307 15:16:36.792842 140096103761664 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.29600715637207, loss=1.6300766468048096
I0307 15:17:16.366120 140096095368960 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.46847677230835, loss=1.5364365577697754
I0307 15:17:56.172586 140096103761664 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.674137592315674, loss=1.5067076683044434
I0307 15:18:35.328323 140096095368960 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.613247871398926, loss=1.6243849992752075
I0307 15:19:15.082229 140096103761664 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.833749294281006, loss=1.6808761358261108
I0307 15:19:16.693917 140252175811776 spec.py:321] Evaluating on the training split.
I0307 15:19:27.783112 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 15:19:50.743613 140252175811776 spec.py:349] Evaluating on the test split.
I0307 15:19:52.520249 140252175811776 submission_runner.py:469] Time since start: 45698.15s, 	Step: 101305, 	{'train/accuracy': 0.7112563848495483, 'train/loss': 1.1239590644836426, 'validation/accuracy': 0.6556000113487244, 'validation/loss': 1.4065600633621216, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.123613119125366, 'test/num_examples': 10000, 'score': 42386.97715163231, 'total_duration': 45698.15100455284, 'accumulated_submission_time': 42386.97715163231, 'accumulated_eval_time': 3292.0426218509674, 'accumulated_logging_time': 7.685492753982544}
I0307 15:19:52.579543 140096095368960 logging_writer.py:48] [101305] accumulated_eval_time=3292.04, accumulated_logging_time=7.68549, accumulated_submission_time=42387, global_step=101305, preemption_count=0, score=42387, test/accuracy=0.5249, test/loss=2.12361, test/num_examples=10000, total_duration=45698.2, train/accuracy=0.711256, train/loss=1.12396, validation/accuracy=0.6556, validation/loss=1.40656, validation/num_examples=50000
2025-03-07 15:19:57.283029: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:20:30.108189 140096103761664 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.9239602088928223, loss=1.6499242782592773
I0307 15:21:09.330237 140096095368960 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.278574466705322, loss=1.5261037349700928
I0307 15:21:48.219573 140096103761664 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.740366458892822, loss=1.5559821128845215
I0307 15:22:27.128567 140096095368960 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.084807872772217, loss=1.4968855381011963
I0307 15:23:06.106755 140096103761664 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.446745872497559, loss=1.5781501531600952
I0307 15:23:44.764757 140096095368960 logging_writer.py:48] [101900] global_step=101900, grad_norm=4.374344348907471, loss=1.5891287326812744
I0307 15:24:23.880540 140096103761664 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.879476070404053, loss=1.6671266555786133
I0307 15:25:02.830996 140096095368960 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.185216426849365, loss=1.6357582807540894
I0307 15:25:41.737114 140096103761664 logging_writer.py:48] [102200] global_step=102200, grad_norm=5.061248779296875, loss=1.6378430128097534
I0307 15:26:20.847297 140096095368960 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.578307628631592, loss=1.6581525802612305
I0307 15:27:00.015555 140096103761664 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.373542308807373, loss=1.5550087690353394
I0307 15:27:39.558000 140096095368960 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.3913893699646, loss=1.5515344142913818
2025-03-07 15:28:05.912814: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:28:19.093086 140096103761664 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.562474727630615, loss=1.615226149559021
I0307 15:28:22.549000 140252175811776 spec.py:321] Evaluating on the training split.
I0307 15:28:33.162856 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 15:28:54.282130 140252175811776 spec.py:349] Evaluating on the test split.
I0307 15:28:56.058462 140252175811776 submission_runner.py:469] Time since start: 46241.69s, 	Step: 102610, 	{'train/accuracy': 0.7088049650192261, 'train/loss': 1.1257026195526123, 'validation/accuracy': 0.6520400047302246, 'validation/loss': 1.4184962511062622, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.137503147125244, 'test/num_examples': 10000, 'score': 42896.77038502693, 'total_duration': 46241.6892080307, 'accumulated_submission_time': 42896.77038502693, 'accumulated_eval_time': 3325.5520470142365, 'accumulated_logging_time': 7.760801792144775}
I0307 15:28:56.102642 140096095368960 logging_writer.py:48] [102610] accumulated_eval_time=3325.55, accumulated_logging_time=7.7608, accumulated_submission_time=42896.8, global_step=102610, preemption_count=0, score=42896.8, test/accuracy=0.5273, test/loss=2.1375, test/num_examples=10000, total_duration=46241.7, train/accuracy=0.708805, train/loss=1.1257, validation/accuracy=0.65204, validation/loss=1.4185, validation/num_examples=50000
I0307 15:29:31.437027 140096103761664 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.697059154510498, loss=1.552103877067566
I0307 15:30:10.422463 140096095368960 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.89942741394043, loss=1.6367794275283813
I0307 15:30:49.495837 140096103761664 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.404723167419434, loss=1.6413600444793701
I0307 15:31:28.542800 140096095368960 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.737090587615967, loss=1.552863597869873
I0307 15:32:07.325145 140096103761664 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.837397575378418, loss=1.591721773147583
I0307 15:32:46.267175 140096095368960 logging_writer.py:48] [103200] global_step=103200, grad_norm=5.322853088378906, loss=1.640978455543518
I0307 15:33:25.911054 140096103761664 logging_writer.py:48] [103300] global_step=103300, grad_norm=5.4903974533081055, loss=1.4833656549453735
I0307 15:34:05.142415 140096095368960 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.652952671051025, loss=1.6498554944992065
I0307 15:34:44.328631 140096103761664 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.238739967346191, loss=1.5976890325546265
I0307 15:35:23.429516 140096095368960 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.718231678009033, loss=1.6579704284667969
I0307 15:36:02.622068 140096103761664 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.174853324890137, loss=1.5566694736480713
I0307 15:36:42.355863 140096095368960 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.495806694030762, loss=1.6334065198898315
2025-03-07 15:36:49.035919: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:37:21.423693 140096103761664 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.517650127410889, loss=1.6838963031768799
I0307 15:37:26.122156 140252175811776 spec.py:321] Evaluating on the training split.
I0307 15:37:37.108912 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 15:37:59.046829 140252175811776 spec.py:349] Evaluating on the test split.
I0307 15:38:00.848899 140252175811776 submission_runner.py:469] Time since start: 46786.48s, 	Step: 103913, 	{'train/accuracy': 0.7117944955825806, 'train/loss': 1.1196249723434448, 'validation/accuracy': 0.655460000038147, 'validation/loss': 1.4164386987686157, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.1376898288726807, 'test/num_examples': 10000, 'score': 43406.62056684494, 'total_duration': 46786.47964453697, 'accumulated_submission_time': 43406.62056684494, 'accumulated_eval_time': 3360.2787458896637, 'accumulated_logging_time': 7.812975168228149}
I0307 15:38:00.911820 140096095368960 logging_writer.py:48] [103913] accumulated_eval_time=3360.28, accumulated_logging_time=7.81298, accumulated_submission_time=43406.6, global_step=103913, preemption_count=0, score=43406.6, test/accuracy=0.5242, test/loss=2.13769, test/num_examples=10000, total_duration=46786.5, train/accuracy=0.711794, train/loss=1.11962, validation/accuracy=0.65546, validation/loss=1.41644, validation/num_examples=50000
I0307 15:38:35.074374 140096103761664 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.614375114440918, loss=1.793585181236267
I0307 15:39:14.364082 140096095368960 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.355496406555176, loss=1.696694016456604
I0307 15:39:53.524039 140096103761664 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.5294060707092285, loss=1.6250572204589844
I0307 15:40:32.762973 140096095368960 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.4804511070251465, loss=1.57891047000885
I0307 15:41:11.818242 140096103761664 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.2859392166137695, loss=1.5155272483825684
I0307 15:41:51.490224 140096095368960 logging_writer.py:48] [104500] global_step=104500, grad_norm=4.50497579574585, loss=1.54209303855896
I0307 15:42:30.492625 140096103761664 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.247028827667236, loss=1.6060644388198853
I0307 15:43:09.729831 140096095368960 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.686517238616943, loss=1.6220753192901611
I0307 15:43:49.152819 140096103761664 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.4944071769714355, loss=1.558131456375122
I0307 15:44:28.442002 140096095368960 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.69929838180542, loss=1.5583267211914062
I0307 15:45:07.654290 140096103761664 logging_writer.py:48] [105000] global_step=105000, grad_norm=4.517457008361816, loss=1.6043323278427124
2025-03-07 15:45:34.825663: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:45:47.308508 140096095368960 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.469264984130859, loss=1.6635724306106567
I0307 15:46:26.045151 140096103761664 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.094136714935303, loss=1.5291613340377808
I0307 15:46:31.241831 140252175811776 spec.py:321] Evaluating on the training split.
I0307 15:46:42.588024 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 15:47:02.359968 140252175811776 spec.py:349] Evaluating on the test split.
I0307 15:47:04.118178 140252175811776 submission_runner.py:469] Time since start: 47329.75s, 	Step: 105214, 	{'train/accuracy': 0.7189492583274841, 'train/loss': 1.089512825012207, 'validation/accuracy': 0.6606400012969971, 'validation/loss': 1.3863310813903809, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.097107410430908, 'test/num_examples': 10000, 'score': 43916.78245329857, 'total_duration': 47329.74892568588, 'accumulated_submission_time': 43916.78245329857, 'accumulated_eval_time': 3393.1550545692444, 'accumulated_logging_time': 7.883579730987549}
I0307 15:47:04.174807 140096095368960 logging_writer.py:48] [105214] accumulated_eval_time=3393.16, accumulated_logging_time=7.88358, accumulated_submission_time=43916.8, global_step=105214, preemption_count=0, score=43916.8, test/accuracy=0.5322, test/loss=2.09711, test/num_examples=10000, total_duration=47329.7, train/accuracy=0.718949, train/loss=1.08951, validation/accuracy=0.66064, validation/loss=1.38633, validation/num_examples=50000
I0307 15:47:38.111365 140096103761664 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.568052291870117, loss=1.668452262878418
I0307 15:48:17.123236 140096095368960 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.357929706573486, loss=1.7371487617492676
I0307 15:48:56.317847 140096103761664 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.581614017486572, loss=1.6140714883804321
I0307 15:49:35.083367 140096095368960 logging_writer.py:48] [105600] global_step=105600, grad_norm=4.315222263336182, loss=1.5383126735687256
I0307 15:50:14.404627 140096103761664 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.585087776184082, loss=1.5880969762802124
I0307 15:50:52.907352 140096095368960 logging_writer.py:48] [105800] global_step=105800, grad_norm=5.106080532073975, loss=1.598439335823059
I0307 15:51:32.131419 140096103761664 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.104818820953369, loss=1.6231403350830078
I0307 15:52:11.602334 140096095368960 logging_writer.py:48] [106000] global_step=106000, grad_norm=5.160355567932129, loss=1.5822265148162842
I0307 15:52:50.895523 140096103761664 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.356348514556885, loss=1.4207406044006348
I0307 15:53:30.242003 140096095368960 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.5403547286987305, loss=1.5936635732650757
I0307 15:54:10.261745 140096103761664 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.439450740814209, loss=1.5880990028381348
2025-03-07 15:54:17.964676: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:54:49.453331 140096095368960 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.318206787109375, loss=1.568953514099121
I0307 15:55:28.458446 140096103761664 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.611231803894043, loss=1.5329142808914185
I0307 15:55:34.459904 140252175811776 spec.py:321] Evaluating on the training split.
I0307 15:55:45.062074 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 15:56:09.543421 140252175811776 spec.py:349] Evaluating on the test split.
I0307 15:56:11.339600 140252175811776 submission_runner.py:469] Time since start: 47876.97s, 	Step: 106516, 	{'train/accuracy': 0.7255460619926453, 'train/loss': 1.0444070100784302, 'validation/accuracy': 0.6671800017356873, 'validation/loss': 1.3501098155975342, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.0455236434936523, 'test/num_examples': 10000, 'score': 44426.897867918015, 'total_duration': 47876.97033929825, 'accumulated_submission_time': 44426.897867918015, 'accumulated_eval_time': 3430.034717321396, 'accumulated_logging_time': 7.948538541793823}
I0307 15:56:11.395447 140096095368960 logging_writer.py:48] [106516] accumulated_eval_time=3430.03, accumulated_logging_time=7.94854, accumulated_submission_time=44426.9, global_step=106516, preemption_count=0, score=44426.9, test/accuracy=0.5449, test/loss=2.04552, test/num_examples=10000, total_duration=47877, train/accuracy=0.725546, train/loss=1.04441, validation/accuracy=0.66718, validation/loss=1.35011, validation/num_examples=50000
I0307 15:56:43.830104 140096103761664 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.9995036125183105, loss=1.7142285108566284
I0307 15:57:22.679033 140096095368960 logging_writer.py:48] [106700] global_step=106700, grad_norm=5.073692321777344, loss=1.6370854377746582
I0307 15:58:01.992721 140096103761664 logging_writer.py:48] [106800] global_step=106800, grad_norm=5.169841289520264, loss=1.6354013681411743
I0307 15:58:40.887490 140096095368960 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.892237186431885, loss=1.5299633741378784
I0307 15:59:19.987907 140096103761664 logging_writer.py:48] [107000] global_step=107000, grad_norm=5.601747512817383, loss=1.5671833753585815
I0307 15:59:59.046215 140096095368960 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.761643409729004, loss=1.540178894996643
I0307 16:00:37.851110 140096103761664 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.978621482849121, loss=1.571845293045044
I0307 16:01:16.746752 140096095368960 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.157264232635498, loss=1.5108981132507324
I0307 16:01:55.748288 140096103761664 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.0698161125183105, loss=1.6259781122207642
I0307 16:02:35.128188 140096095368960 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.374572277069092, loss=1.5548787117004395
2025-03-07 16:03:03.466509: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:03:14.701423 140096103761664 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.230221748352051, loss=1.4477131366729736
I0307 16:03:53.657501 140096095368960 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.287275314331055, loss=1.5079920291900635
I0307 16:04:32.995121 140096103761664 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.634511470794678, loss=1.6355199813842773
I0307 16:04:41.534006 140252175811776 spec.py:321] Evaluating on the training split.
I0307 16:04:52.294969 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 16:05:12.936506 140252175811776 spec.py:349] Evaluating on the test split.
I0307 16:05:14.719254 140252175811776 submission_runner.py:469] Time since start: 48420.35s, 	Step: 107823, 	{'train/accuracy': 0.725027859210968, 'train/loss': 1.0664734840393066, 'validation/accuracy': 0.6644999980926514, 'validation/loss': 1.3645597696304321, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.0575408935546875, 'test/num_examples': 10000, 'score': 44936.8701236248, 'total_duration': 48420.34999704361, 'accumulated_submission_time': 44936.8701236248, 'accumulated_eval_time': 3463.219923019409, 'accumulated_logging_time': 8.012507438659668}
I0307 16:05:14.766226 140096095368960 logging_writer.py:48] [107823] accumulated_eval_time=3463.22, accumulated_logging_time=8.01251, accumulated_submission_time=44936.9, global_step=107823, preemption_count=0, score=44936.9, test/accuracy=0.5415, test/loss=2.05754, test/num_examples=10000, total_duration=48420.3, train/accuracy=0.725028, train/loss=1.06647, validation/accuracy=0.6645, validation/loss=1.36456, validation/num_examples=50000
I0307 16:05:45.245108 140096103761664 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.475396633148193, loss=1.6155297756195068
I0307 16:06:24.454006 140096095368960 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.563361644744873, loss=1.4479442834854126
I0307 16:07:03.450488 140096103761664 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.7809624671936035, loss=1.5888104438781738
I0307 16:07:42.244105 140096095368960 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.655734539031982, loss=1.4943532943725586
I0307 16:08:20.835426 140096103761664 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.539458274841309, loss=1.6400731801986694
I0307 16:08:59.629001 140096095368960 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.456601619720459, loss=1.6127464771270752
I0307 16:09:38.410108 140096103761664 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.273768424987793, loss=1.4970605373382568
I0307 16:10:17.133548 140096095368960 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.9165449142456055, loss=1.5778411626815796
I0307 16:10:55.573388 140096103761664 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.724460124969482, loss=1.6423101425170898
I0307 16:11:34.967409 140096095368960 logging_writer.py:48] [108800] global_step=108800, grad_norm=5.042853832244873, loss=1.5518170595169067
2025-03-07 16:11:43.988286: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:12:14.121488 140096103761664 logging_writer.py:48] [108900] global_step=108900, grad_norm=5.0058488845825195, loss=1.6352322101593018
I0307 16:12:52.931762 140096095368960 logging_writer.py:48] [109000] global_step=109000, grad_norm=5.070704460144043, loss=1.5453165769577026
I0307 16:13:31.289113 140096103761664 logging_writer.py:48] [109100] global_step=109100, grad_norm=5.131545066833496, loss=1.6532655954360962
I0307 16:13:44.952199 140252175811776 spec.py:321] Evaluating on the training split.
I0307 16:13:55.970314 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 16:14:18.986978 140252175811776 spec.py:349] Evaluating on the test split.
I0307 16:14:20.766213 140252175811776 submission_runner.py:469] Time since start: 48966.40s, 	Step: 109136, 	{'train/accuracy': 0.7351921200752258, 'train/loss': 1.0098414421081543, 'validation/accuracy': 0.6715599894523621, 'validation/loss': 1.3341665267944336, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.0304386615753174, 'test/num_examples': 10000, 'score': 45446.887754678726, 'total_duration': 48966.39695715904, 'accumulated_submission_time': 45446.887754678726, 'accumulated_eval_time': 3499.0338978767395, 'accumulated_logging_time': 8.067375421524048}
I0307 16:14:20.821163 140096095368960 logging_writer.py:48] [109136] accumulated_eval_time=3499.03, accumulated_logging_time=8.06738, accumulated_submission_time=45446.9, global_step=109136, preemption_count=0, score=45446.9, test/accuracy=0.5434, test/loss=2.03044, test/num_examples=10000, total_duration=48966.4, train/accuracy=0.735192, train/loss=1.00984, validation/accuracy=0.67156, validation/loss=1.33417, validation/num_examples=50000
I0307 16:14:46.201671 140096103761664 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.50840425491333, loss=1.5770387649536133
I0307 16:15:24.821416 140096095368960 logging_writer.py:48] [109300] global_step=109300, grad_norm=5.107407569885254, loss=1.5931757688522339
I0307 16:16:03.870379 140096103761664 logging_writer.py:48] [109400] global_step=109400, grad_norm=5.003508567810059, loss=1.5573335886001587
I0307 16:16:43.067369 140096095368960 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.558459758758545, loss=1.5812156200408936
I0307 16:17:21.900861 140096103761664 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.5921711921691895, loss=1.6136040687561035
I0307 16:18:01.313530 140096095368960 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.414396286010742, loss=1.457144021987915
I0307 16:18:41.026823 140096103761664 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.829169273376465, loss=1.5780041217803955
I0307 16:19:20.220725 140096095368960 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.3439836502075195, loss=1.5458672046661377
I0307 16:19:59.425542 140096103761664 logging_writer.py:48] [110000] global_step=110000, grad_norm=5.302513599395752, loss=1.5934042930603027
2025-03-07 16:20:28.807391: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:20:39.328301 140096095368960 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.495279788970947, loss=1.529260516166687
I0307 16:21:18.378268 140096103761664 logging_writer.py:48] [110200] global_step=110200, grad_norm=5.316667556762695, loss=1.5983216762542725
I0307 16:21:58.000849 140096095368960 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.264983654022217, loss=1.470412015914917
I0307 16:22:37.461256 140096103761664 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.542276382446289, loss=1.5442979335784912
I0307 16:22:51.125340 140252175811776 spec.py:321] Evaluating on the training split.
I0307 16:23:01.631632 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 16:23:25.270085 140252175811776 spec.py:349] Evaluating on the test split.
I0307 16:23:27.022994 140252175811776 submission_runner.py:469] Time since start: 49512.65s, 	Step: 110436, 	{'train/accuracy': 0.7273596525192261, 'train/loss': 1.0478661060333252, 'validation/accuracy': 0.6682800054550171, 'validation/loss': 1.3483519554138184, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0515429973602295, 'test/num_examples': 10000, 'score': 45957.02745056152, 'total_duration': 49512.65373420715, 'accumulated_submission_time': 45957.02745056152, 'accumulated_eval_time': 3534.931508541107, 'accumulated_logging_time': 8.130779027938843}
I0307 16:23:27.080397 140096095368960 logging_writer.py:48] [110436] accumulated_eval_time=3534.93, accumulated_logging_time=8.13078, accumulated_submission_time=45957, global_step=110436, preemption_count=0, score=45957, test/accuracy=0.5436, test/loss=2.05154, test/num_examples=10000, total_duration=49512.7, train/accuracy=0.72736, train/loss=1.04787, validation/accuracy=0.66828, validation/loss=1.34835, validation/num_examples=50000
I0307 16:23:52.423451 140096103761664 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.386054039001465, loss=1.6092365980148315
I0307 16:24:31.620445 140096095368960 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.366852760314941, loss=1.461604356765747
I0307 16:25:10.926141 140096103761664 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.887561798095703, loss=1.5717568397521973
I0307 16:25:50.503886 140096095368960 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.770267486572266, loss=1.5218814611434937
I0307 16:26:29.472304 140096103761664 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.774856090545654, loss=1.4316715002059937
I0307 16:27:08.081853 140096095368960 logging_writer.py:48] [111000] global_step=111000, grad_norm=5.487526893615723, loss=1.6661198139190674
I0307 16:27:46.979635 140096103761664 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.546839714050293, loss=1.5839020013809204
I0307 16:28:26.350597 140096095368960 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.293301582336426, loss=1.5078749656677246
I0307 16:29:05.137657 140096103761664 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.701082229614258, loss=1.6031112670898438
2025-03-07 16:29:14.390737: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:29:43.916799 140096095368960 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.799589157104492, loss=1.6138975620269775
I0307 16:30:23.131256 140096103761664 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.454075336456299, loss=1.6113208532333374
I0307 16:31:02.060247 140096095368960 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.3624725341796875, loss=1.5173910856246948
I0307 16:31:41.194594 140096103761664 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.945263862609863, loss=1.4926862716674805
I0307 16:31:57.062180 140252175811776 spec.py:321] Evaluating on the training split.
I0307 16:32:07.870444 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 16:32:28.588839 140252175811776 spec.py:349] Evaluating on the test split.
I0307 16:32:30.366940 140252175811776 submission_runner.py:469] Time since start: 50056.00s, 	Step: 111742, 	{'train/accuracy': 0.7318040132522583, 'train/loss': 1.0298599004745483, 'validation/accuracy': 0.668179988861084, 'validation/loss': 1.3314546346664429, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.024582862854004, 'test/num_examples': 10000, 'score': 46466.828177928925, 'total_duration': 50055.99768257141, 'accumulated_submission_time': 46466.828177928925, 'accumulated_eval_time': 3568.2362315654755, 'accumulated_logging_time': 8.21045994758606}
I0307 16:32:30.414521 140096095368960 logging_writer.py:48] [111742] accumulated_eval_time=3568.24, accumulated_logging_time=8.21046, accumulated_submission_time=46466.8, global_step=111742, preemption_count=0, score=46466.8, test/accuracy=0.544, test/loss=2.02458, test/num_examples=10000, total_duration=50056, train/accuracy=0.731804, train/loss=1.02986, validation/accuracy=0.66818, validation/loss=1.33145, validation/num_examples=50000
I0307 16:32:53.409488 140096103761664 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.932671546936035, loss=1.5291590690612793
I0307 16:33:32.431950 140096095368960 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.806572914123535, loss=1.4764342308044434
I0307 16:34:12.154891 140096103761664 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.584315776824951, loss=1.5419642925262451
I0307 16:34:51.547581 140096095368960 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.928195476531982, loss=1.5973131656646729
I0307 16:35:30.669375 140096103761664 logging_writer.py:48] [112200] global_step=112200, grad_norm=5.202773094177246, loss=1.5131350755691528
I0307 16:36:10.567166 140096095368960 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.8000969886779785, loss=1.5473697185516357
I0307 16:36:50.153359 140096103761664 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.474401473999023, loss=1.525386095046997
I0307 16:37:29.492099 140096095368960 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.82154655456543, loss=1.4366908073425293
2025-03-07 16:37:59.572521: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:38:09.272055 140096103761664 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.434819221496582, loss=1.573649287223816
I0307 16:38:48.813096 140096095368960 logging_writer.py:48] [112700] global_step=112700, grad_norm=5.02036190032959, loss=1.5387747287750244
I0307 16:39:28.898451 140096103761664 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.711732387542725, loss=1.4794228076934814
I0307 16:40:08.473755 140096095368960 logging_writer.py:48] [112900] global_step=112900, grad_norm=5.24406623840332, loss=1.532904028892517
I0307 16:40:48.106184 140096103761664 logging_writer.py:48] [113000] global_step=113000, grad_norm=5.348950386047363, loss=1.55433988571167
I0307 16:41:00.472816 140252175811776 spec.py:321] Evaluating on the training split.
I0307 16:41:11.188827 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 16:41:33.441125 140252175811776 spec.py:349] Evaluating on the test split.
I0307 16:41:35.170651 140252175811776 submission_runner.py:469] Time since start: 50600.80s, 	Step: 113032, 	{'train/accuracy': 0.7305285334587097, 'train/loss': 1.0380269289016724, 'validation/accuracy': 0.6700800061225891, 'validation/loss': 1.3370519876480103, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.0114362239837646, 'test/num_examples': 10000, 'score': 46976.71987295151, 'total_duration': 50600.80140542984, 'accumulated_submission_time': 46976.71987295151, 'accumulated_eval_time': 3602.934042453766, 'accumulated_logging_time': 8.26657247543335}
I0307 16:41:35.218062 140096095368960 logging_writer.py:48] [113032] accumulated_eval_time=3602.93, accumulated_logging_time=8.26657, accumulated_submission_time=46976.7, global_step=113032, preemption_count=0, score=46976.7, test/accuracy=0.5479, test/loss=2.01144, test/num_examples=10000, total_duration=50600.8, train/accuracy=0.730529, train/loss=1.03803, validation/accuracy=0.67008, validation/loss=1.33705, validation/num_examples=50000
I0307 16:42:02.241370 140096103761664 logging_writer.py:48] [113100] global_step=113100, grad_norm=5.128390789031982, loss=1.5434205532073975
I0307 16:42:41.559314 140096095368960 logging_writer.py:48] [113200] global_step=113200, grad_norm=5.3143086433410645, loss=1.6093480587005615
I0307 16:43:21.184034 140096103761664 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.892943859100342, loss=1.4457533359527588
I0307 16:44:00.961562 140096095368960 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.8175129890441895, loss=1.5719424486160278
I0307 16:44:40.938585 140096103761664 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.900764465332031, loss=1.5893199443817139
I0307 16:45:20.754071 140096095368960 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.983223915100098, loss=1.5515598058700562
I0307 16:46:01.031165 140096103761664 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.986566543579102, loss=1.5305688381195068
I0307 16:46:41.369482 140096095368960 logging_writer.py:48] [113800] global_step=113800, grad_norm=4.693066596984863, loss=1.5336072444915771
2025-03-07 16:46:51.832449: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:47:20.807525 140096103761664 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.6972575187683105, loss=1.443145513534546
I0307 16:48:00.397756 140096095368960 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.796654224395752, loss=1.5689613819122314
I0307 16:48:40.070073 140096103761664 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.802182197570801, loss=1.6069899797439575
I0307 16:49:19.383003 140096095368960 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.797994613647461, loss=1.4969176054000854
I0307 16:49:59.289536 140096103761664 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.858851432800293, loss=1.5613670349121094
I0307 16:50:05.516538 140252175811776 spec.py:321] Evaluating on the training split.
I0307 16:50:16.151498 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 16:50:36.473596 140252175811776 spec.py:349] Evaluating on the test split.
I0307 16:50:38.267171 140252175811776 submission_runner.py:469] Time since start: 51143.90s, 	Step: 114317, 	{'train/accuracy': 0.7356504797935486, 'train/loss': 1.0100760459899902, 'validation/accuracy': 0.6712999939918518, 'validation/loss': 1.3365951776504517, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.0522353649139404, 'test/num_examples': 10000, 'score': 47486.85285496712, 'total_duration': 51143.89784693718, 'accumulated_submission_time': 47486.85285496712, 'accumulated_eval_time': 3635.684566259384, 'accumulated_logging_time': 8.321794986724854}
I0307 16:50:38.381143 140096095368960 logging_writer.py:48] [114317] accumulated_eval_time=3635.68, accumulated_logging_time=8.32179, accumulated_submission_time=47486.9, global_step=114317, preemption_count=0, score=47486.9, test/accuracy=0.543, test/loss=2.05224, test/num_examples=10000, total_duration=51143.9, train/accuracy=0.73565, train/loss=1.01008, validation/accuracy=0.6713, validation/loss=1.3366, validation/num_examples=50000
I0307 16:51:11.523662 140096103761664 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.905038356781006, loss=1.6340903043746948
I0307 16:51:50.938189 140096095368960 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.419663429260254, loss=1.4790191650390625
I0307 16:52:30.632169 140096103761664 logging_writer.py:48] [114600] global_step=114600, grad_norm=5.0962347984313965, loss=1.5362776517868042
I0307 16:53:10.563926 140096095368960 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.9269022941589355, loss=1.5939745903015137
I0307 16:53:49.949390 140096103761664 logging_writer.py:48] [114800] global_step=114800, grad_norm=5.173639297485352, loss=1.4684959650039673
I0307 16:54:29.459415 140096095368960 logging_writer.py:48] [114900] global_step=114900, grad_norm=5.161787033081055, loss=1.6182854175567627
I0307 16:55:09.548734 140096103761664 logging_writer.py:48] [115000] global_step=115000, grad_norm=4.734508037567139, loss=1.4824535846710205
2025-03-07 16:55:41.249143: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:55:49.906185 140096095368960 logging_writer.py:48] [115100] global_step=115100, grad_norm=5.18599796295166, loss=1.5937968492507935
I0307 16:56:29.792003 140096103761664 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.820662021636963, loss=1.557387351989746
I0307 16:57:09.604086 140096095368960 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.729443073272705, loss=1.5255210399627686
I0307 16:57:49.077806 140096103761664 logging_writer.py:48] [115400] global_step=115400, grad_norm=5.35694694519043, loss=1.496404767036438
I0307 16:58:28.624900 140096095368960 logging_writer.py:48] [115500] global_step=115500, grad_norm=5.233304500579834, loss=1.580711841583252
I0307 16:59:07.965930 140096103761664 logging_writer.py:48] [115600] global_step=115600, grad_norm=5.076904773712158, loss=1.446725606918335
I0307 16:59:08.383742 140252175811776 spec.py:321] Evaluating on the training split.
I0307 16:59:19.083588 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 16:59:40.350495 140252175811776 spec.py:349] Evaluating on the test split.
I0307 16:59:42.117112 140252175811776 submission_runner.py:469] Time since start: 51687.75s, 	Step: 115602, 	{'train/accuracy': 0.7401546239852905, 'train/loss': 0.9764506220817566, 'validation/accuracy': 0.6774199604988098, 'validation/loss': 1.3063912391662598, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.0029489994049072, 'test/num_examples': 10000, 'score': 47996.67247462273, 'total_duration': 51687.74786758423, 'accumulated_submission_time': 47996.67247462273, 'accumulated_eval_time': 3669.4179055690765, 'accumulated_logging_time': 8.460081338882446}
I0307 16:59:42.192031 140096095368960 logging_writer.py:48] [115602] accumulated_eval_time=3669.42, accumulated_logging_time=8.46008, accumulated_submission_time=47996.7, global_step=115602, preemption_count=0, score=47996.7, test/accuracy=0.5466, test/loss=2.00295, test/num_examples=10000, total_duration=51687.7, train/accuracy=0.740155, train/loss=0.976451, validation/accuracy=0.67742, validation/loss=1.30639, validation/num_examples=50000
I0307 17:00:21.108415 140096103761664 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.964192867279053, loss=1.5118106603622437
I0307 17:01:00.616311 140096095368960 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.5687336921691895, loss=1.567279577255249
I0307 17:01:40.476823 140096103761664 logging_writer.py:48] [115900] global_step=115900, grad_norm=5.018777370452881, loss=1.4357945919036865
I0307 17:02:20.266965 140096095368960 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.937803745269775, loss=1.486176609992981
I0307 17:03:00.140177 140096103761664 logging_writer.py:48] [116100] global_step=116100, grad_norm=4.390323162078857, loss=1.4667088985443115
I0307 17:03:39.943055 140096095368960 logging_writer.py:48] [116200] global_step=116200, grad_norm=5.196305751800537, loss=1.4803768396377563
I0307 17:04:20.434073 140096103761664 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.618221282958984, loss=1.3814347982406616
2025-03-07 17:04:31.802776: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:05:00.420338 140096095368960 logging_writer.py:48] [116400] global_step=116400, grad_norm=5.23563289642334, loss=1.538351058959961
I0307 17:05:39.778842 140096103761664 logging_writer.py:48] [116500] global_step=116500, grad_norm=5.163708209991455, loss=1.427836537361145
I0307 17:06:19.074475 140096095368960 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.857298851013184, loss=1.5816903114318848
I0307 17:06:58.745675 140096103761664 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.950263977050781, loss=1.4918400049209595
I0307 17:07:38.628114 140096095368960 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.823009967803955, loss=1.4796366691589355
I0307 17:08:12.224764 140252175811776 spec.py:321] Evaluating on the training split.
I0307 17:08:22.558831 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 17:08:45.576421 140252175811776 spec.py:349] Evaluating on the test split.
I0307 17:08:47.342155 140252175811776 submission_runner.py:469] Time since start: 52232.97s, 	Step: 116886, 	{'train/accuracy': 0.744160532951355, 'train/loss': 0.9764852523803711, 'validation/accuracy': 0.679099977016449, 'validation/loss': 1.3015769720077515, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 1.9918854236602783, 'test/num_examples': 10000, 'score': 48506.53856897354, 'total_duration': 52232.97290062904, 'accumulated_submission_time': 48506.53856897354, 'accumulated_eval_time': 3704.535264492035, 'accumulated_logging_time': 8.543070554733276}
I0307 17:08:47.418352 140096103761664 logging_writer.py:48] [116886] accumulated_eval_time=3704.54, accumulated_logging_time=8.54307, accumulated_submission_time=48506.5, global_step=116886, preemption_count=0, score=48506.5, test/accuracy=0.5511, test/loss=1.99189, test/num_examples=10000, total_duration=52233, train/accuracy=0.744161, train/loss=0.976485, validation/accuracy=0.6791, validation/loss=1.30158, validation/num_examples=50000
I0307 17:08:53.279649 140096095368960 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.728237152099609, loss=1.3881350755691528
I0307 17:09:32.355954 140096103761664 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.759448528289795, loss=1.5854620933532715
I0307 17:10:11.967989 140096095368960 logging_writer.py:48] [117100] global_step=117100, grad_norm=4.779885292053223, loss=1.4610995054244995
I0307 17:10:51.481519 140096103761664 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.506063938140869, loss=1.4835748672485352
I0307 17:11:31.460022 140096095368960 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.892772197723389, loss=1.5128024816513062
I0307 17:12:11.079003 140096103761664 logging_writer.py:48] [117400] global_step=117400, grad_norm=5.110212802886963, loss=1.5940951108932495
I0307 17:12:50.886722 140096095368960 logging_writer.py:48] [117500] global_step=117500, grad_norm=5.275694847106934, loss=1.4747711420059204
2025-03-07 17:13:22.597673: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:13:30.645326 140096103761664 logging_writer.py:48] [117600] global_step=117600, grad_norm=5.200690746307373, loss=1.5449862480163574
I0307 17:14:10.141355 140096095368960 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.796173095703125, loss=1.5600788593292236
I0307 17:14:49.190636 140096103761664 logging_writer.py:48] [117800] global_step=117800, grad_norm=5.044116973876953, loss=1.524803876876831
I0307 17:15:28.603814 140096095368960 logging_writer.py:48] [117900] global_step=117900, grad_norm=5.026604175567627, loss=1.4369453191757202
I0307 17:16:08.436983 140096103761664 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.4740729331970215, loss=1.5110405683517456
I0307 17:16:48.145433 140096095368960 logging_writer.py:48] [118100] global_step=118100, grad_norm=5.316805839538574, loss=1.558652639389038
I0307 17:17:17.433843 140252175811776 spec.py:321] Evaluating on the training split.
I0307 17:17:27.731367 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 17:17:51.756647 140252175811776 spec.py:349] Evaluating on the test split.
I0307 17:17:53.487845 140252175811776 submission_runner.py:469] Time since start: 52779.12s, 	Step: 118174, 	{'train/accuracy': 0.7464524507522583, 'train/loss': 0.9514623880386353, 'validation/accuracy': 0.6811800003051758, 'validation/loss': 1.2951253652572632, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.0076942443847656, 'test/num_examples': 10000, 'score': 49016.38579249382, 'total_duration': 52779.118599653244, 'accumulated_submission_time': 49016.38579249382, 'accumulated_eval_time': 3740.5892341136932, 'accumulated_logging_time': 8.627598762512207}
I0307 17:17:53.515080 140096103761664 logging_writer.py:48] [118174] accumulated_eval_time=3740.59, accumulated_logging_time=8.6276, accumulated_submission_time=49016.4, global_step=118174, preemption_count=0, score=49016.4, test/accuracy=0.5523, test/loss=2.00769, test/num_examples=10000, total_duration=52779.1, train/accuracy=0.746452, train/loss=0.951462, validation/accuracy=0.68118, validation/loss=1.29513, validation/num_examples=50000
I0307 17:18:04.246916 140096095368960 logging_writer.py:48] [118200] global_step=118200, grad_norm=5.77531623840332, loss=1.45682954788208
I0307 17:18:43.606342 140096103761664 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.825253009796143, loss=1.4449000358581543
I0307 17:19:23.139100 140096095368960 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.809686660766602, loss=1.5358532667160034
I0307 17:20:03.112842 140096103761664 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.872539520263672, loss=1.5606305599212646
I0307 17:20:42.660510 140096095368960 logging_writer.py:48] [118600] global_step=118600, grad_norm=5.0065765380859375, loss=1.424437403678894
I0307 17:21:21.994212 140096103761664 logging_writer.py:48] [118700] global_step=118700, grad_norm=5.524894714355469, loss=1.427454948425293
I0307 17:22:02.381122 140096095368960 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.871002674102783, loss=1.411221981048584
2025-03-07 17:22:14.982987: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:22:42.190424 140096103761664 logging_writer.py:48] [118900] global_step=118900, grad_norm=5.825958728790283, loss=1.4491418600082397
I0307 17:23:21.165440 140096095368960 logging_writer.py:48] [119000] global_step=119000, grad_norm=4.794922828674316, loss=1.4586763381958008
I0307 17:24:00.773556 140096103761664 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.117161273956299, loss=1.6116747856140137
I0307 17:24:40.692045 140096095368960 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.656551361083984, loss=1.3504257202148438
I0307 17:25:20.046391 140096103761664 logging_writer.py:48] [119300] global_step=119300, grad_norm=4.80366325378418, loss=1.5290607213974
I0307 17:25:59.418039 140096095368960 logging_writer.py:48] [119400] global_step=119400, grad_norm=5.1910719871521, loss=1.5744463205337524
I0307 17:26:23.739172 140252175811776 spec.py:321] Evaluating on the training split.
I0307 17:26:34.451770 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 17:26:56.856102 140252175811776 spec.py:349] Evaluating on the test split.
I0307 17:26:58.590301 140252175811776 submission_runner.py:469] Time since start: 53324.22s, 	Step: 119462, 	{'train/accuracy': 0.7521523833274841, 'train/loss': 0.9495605230331421, 'validation/accuracy': 0.6805599927902222, 'validation/loss': 1.2911232709884644, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 1.9860697984695435, 'test/num_examples': 10000, 'score': 49526.444617033005, 'total_duration': 53324.22105407715, 'accumulated_submission_time': 49526.444617033005, 'accumulated_eval_time': 3775.4403302669525, 'accumulated_logging_time': 8.662558555603027}
I0307 17:26:58.632451 140096103761664 logging_writer.py:48] [119462] accumulated_eval_time=3775.44, accumulated_logging_time=8.66256, accumulated_submission_time=49526.4, global_step=119462, preemption_count=0, score=49526.4, test/accuracy=0.5582, test/loss=1.98607, test/num_examples=10000, total_duration=53324.2, train/accuracy=0.752152, train/loss=0.949561, validation/accuracy=0.68056, validation/loss=1.29112, validation/num_examples=50000
I0307 17:27:14.214426 140096095368960 logging_writer.py:48] [119500] global_step=119500, grad_norm=5.148467540740967, loss=1.5451467037200928
I0307 17:27:53.837558 140096103761664 logging_writer.py:48] [119600] global_step=119600, grad_norm=6.1287336349487305, loss=1.5652323961257935
I0307 17:28:33.194301 140096095368960 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.036033630371094, loss=1.4460570812225342
I0307 17:29:12.703445 140096103761664 logging_writer.py:48] [119800] global_step=119800, grad_norm=5.103982448577881, loss=1.4885681867599487
I0307 17:29:52.315325 140096095368960 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.9363203048706055, loss=1.4623570442199707
I0307 17:30:32.247541 140096103761664 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.3584818840026855, loss=1.5252277851104736
2025-03-07 17:31:04.480185: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:31:11.717921 140096095368960 logging_writer.py:48] [120100] global_step=120100, grad_norm=5.343191623687744, loss=1.558356761932373
I0307 17:31:51.552300 140096103761664 logging_writer.py:48] [120200] global_step=120200, grad_norm=5.3938212394714355, loss=1.5579133033752441
I0307 17:32:30.952199 140096095368960 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.202117919921875, loss=1.432887315750122
I0307 17:33:10.706331 140096103761664 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.941141605377197, loss=1.4697526693344116
I0307 17:33:50.303172 140096095368960 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.040355682373047, loss=1.558469295501709
I0307 17:34:29.637427 140096103761664 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.665157794952393, loss=1.3572403192520142
I0307 17:35:09.002691 140096095368960 logging_writer.py:48] [120700] global_step=120700, grad_norm=4.94099760055542, loss=1.4521372318267822
I0307 17:35:28.888545 140252175811776 spec.py:321] Evaluating on the training split.
I0307 17:35:39.531666 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 17:35:59.057093 140252175811776 spec.py:349] Evaluating on the test split.
I0307 17:36:00.853273 140252175811776 submission_runner.py:469] Time since start: 53866.48s, 	Step: 120752, 	{'train/accuracy': 0.7479073405265808, 'train/loss': 0.9605219960212708, 'validation/accuracy': 0.6820600032806396, 'validation/loss': 1.2853432893753052, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 1.975382924079895, 'test/num_examples': 10000, 'score': 50036.53489971161, 'total_duration': 53866.48399853706, 'accumulated_submission_time': 50036.53489971161, 'accumulated_eval_time': 3807.4050166606903, 'accumulated_logging_time': 8.712469816207886}
I0307 17:36:00.927853 140096103761664 logging_writer.py:48] [120752] accumulated_eval_time=3807.41, accumulated_logging_time=8.71247, accumulated_submission_time=50036.5, global_step=120752, preemption_count=0, score=50036.5, test/accuracy=0.559, test/loss=1.97538, test/num_examples=10000, total_duration=53866.5, train/accuracy=0.747907, train/loss=0.960522, validation/accuracy=0.68206, validation/loss=1.28534, validation/num_examples=50000
I0307 17:36:20.802302 140096095368960 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.6073713302612305, loss=1.4961657524108887
I0307 17:36:59.601457 140096103761664 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.9166717529296875, loss=1.5186196565628052
I0307 17:37:39.327203 140096095368960 logging_writer.py:48] [121000] global_step=121000, grad_norm=6.315117835998535, loss=1.6157870292663574
I0307 17:38:18.787809 140096103761664 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.0161237716674805, loss=1.5195118188858032
I0307 17:38:57.836396 140096095368960 logging_writer.py:48] [121200] global_step=121200, grad_norm=5.071240425109863, loss=1.4497995376586914
I0307 17:39:37.727555 140096103761664 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.493232727050781, loss=1.6405384540557861
2025-03-07 17:39:50.826001: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:40:17.266243 140096095368960 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.2325968742370605, loss=1.5373364686965942
I0307 17:40:56.422245 140096103761664 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.069339275360107, loss=1.5039814710617065
I0307 17:41:35.312508 140096095368960 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.878046989440918, loss=1.4088633060455322
I0307 17:42:14.690927 140096103761664 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.966115474700928, loss=1.4971143007278442
I0307 17:42:53.934771 140096095368960 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.21423864364624, loss=1.4246577024459839
I0307 17:43:33.447495 140096103761664 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.860894203186035, loss=1.4812607765197754
I0307 17:44:12.957038 140096095368960 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.8993425369262695, loss=1.4167934656143188
I0307 17:44:31.024699 140252175811776 spec.py:321] Evaluating on the training split.
I0307 17:44:41.555259 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 17:45:05.972629 140252175811776 spec.py:349] Evaluating on the test split.
I0307 17:45:07.762720 140252175811776 submission_runner.py:469] Time since start: 54413.39s, 	Step: 122047, 	{'train/accuracy': 0.7556002736091614, 'train/loss': 0.9197654724121094, 'validation/accuracy': 0.6878799796104431, 'validation/loss': 1.2549914121627808, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 1.9407501220703125, 'test/num_examples': 10000, 'score': 50545.90423154831, 'total_duration': 54413.393464803696, 'accumulated_submission_time': 50545.90423154831, 'accumulated_eval_time': 3844.143002271652, 'accumulated_logging_time': 9.354947805404663}
I0307 17:45:07.831853 140096103761664 logging_writer.py:48] [122047] accumulated_eval_time=3844.14, accumulated_logging_time=9.35495, accumulated_submission_time=50545.9, global_step=122047, preemption_count=0, score=50545.9, test/accuracy=0.5604, test/loss=1.94075, test/num_examples=10000, total_duration=54413.4, train/accuracy=0.7556, train/loss=0.919765, validation/accuracy=0.68788, validation/loss=1.25499, validation/num_examples=50000
I0307 17:45:29.122781 140096095368960 logging_writer.py:48] [122100] global_step=122100, grad_norm=5.017627239227295, loss=1.4700770378112793
I0307 17:46:08.607322 140096103761664 logging_writer.py:48] [122200] global_step=122200, grad_norm=4.804817199707031, loss=1.4140400886535645
I0307 17:46:48.002302 140096095368960 logging_writer.py:48] [122300] global_step=122300, grad_norm=4.802542209625244, loss=1.4434680938720703
I0307 17:47:27.454756 140096103761664 logging_writer.py:48] [122400] global_step=122400, grad_norm=5.298314571380615, loss=1.422888159751892
I0307 17:48:06.543040 140096095368960 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.0727925300598145, loss=1.4648953676223755
2025-03-07 17:48:39.625576: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:48:45.932613 140096103761664 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.848502159118652, loss=1.38947331905365
I0307 17:49:24.712909 140096095368960 logging_writer.py:48] [122700] global_step=122700, grad_norm=4.832784652709961, loss=1.3583102226257324
I0307 17:50:03.466978 140096103761664 logging_writer.py:48] [122800] global_step=122800, grad_norm=5.304742336273193, loss=1.4995957612991333
I0307 17:50:42.250678 140096095368960 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.972681045532227, loss=1.4207655191421509
I0307 17:51:21.004885 140096103761664 logging_writer.py:48] [123000] global_step=123000, grad_norm=5.245995044708252, loss=1.3934811353683472
I0307 17:51:59.856237 140096095368960 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.563730716705322, loss=1.347579836845398
I0307 17:52:38.548812 140096103761664 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.199558258056641, loss=1.5006736516952515
I0307 17:53:17.680825 140096095368960 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.342357635498047, loss=1.4009062051773071
I0307 17:53:38.051543 140252175811776 spec.py:321] Evaluating on the training split.
I0307 17:53:48.922163 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 17:54:09.535690 140252175811776 spec.py:349] Evaluating on the test split.
I0307 17:54:11.343988 140252175811776 submission_runner.py:469] Time since start: 54956.97s, 	Step: 123353, 	{'train/accuracy': 0.759207546710968, 'train/loss': 0.9096981287002563, 'validation/accuracy': 0.6882199645042419, 'validation/loss': 1.2617321014404297, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 1.9792543649673462, 'test/num_examples': 10000, 'score': 51055.950160980225, 'total_duration': 54956.97474074364, 'accumulated_submission_time': 51055.950160980225, 'accumulated_eval_time': 3877.4354162216187, 'accumulated_logging_time': 9.440228700637817}
I0307 17:54:11.391148 140096103761664 logging_writer.py:48] [123353] accumulated_eval_time=3877.44, accumulated_logging_time=9.44023, accumulated_submission_time=51056, global_step=123353, preemption_count=0, score=51056, test/accuracy=0.5629, test/loss=1.97925, test/num_examples=10000, total_duration=54957, train/accuracy=0.759208, train/loss=0.909698, validation/accuracy=0.68822, validation/loss=1.26173, validation/num_examples=50000
I0307 17:54:30.371375 140096095368960 logging_writer.py:48] [123400] global_step=123400, grad_norm=5.333573818206787, loss=1.4202015399932861
I0307 17:55:09.255824 140096103761664 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.996848106384277, loss=1.4788748025894165
I0307 17:55:48.575392 140096095368960 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.856711387634277, loss=1.5396449565887451
I0307 17:56:28.218116 140096103761664 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.935602188110352, loss=1.415304183959961
I0307 17:57:07.770574 140096095368960 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.220544815063477, loss=1.4184985160827637
2025-03-07 17:57:21.774459: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:57:47.106049 140096103761664 logging_writer.py:48] [123900] global_step=123900, grad_norm=5.171751499176025, loss=1.4172301292419434
I0307 17:58:26.102359 140096095368960 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.16964864730835, loss=1.4769600629806519
I0307 17:59:05.405569 140096103761664 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.5246686935424805, loss=1.467441439628601
I0307 17:59:44.258364 140096095368960 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.691512584686279, loss=1.4897799491882324
I0307 18:00:22.438749 140096103761664 logging_writer.py:48] [124300] global_step=124300, grad_norm=5.271539688110352, loss=1.4106500148773193
I0307 18:01:01.956159 140096095368960 logging_writer.py:48] [124400] global_step=124400, grad_norm=5.169850826263428, loss=1.394379734992981
I0307 18:01:41.619355 140096103761664 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.886068820953369, loss=1.4586769342422485
I0307 18:02:20.918223 140096095368960 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.4018964767456055, loss=1.4258264303207397
I0307 18:02:41.497485 140252175811776 spec.py:321] Evaluating on the training split.
I0307 18:02:51.664369 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 18:03:14.508865 140252175811776 spec.py:349] Evaluating on the test split.
I0307 18:03:16.288968 140252175811776 submission_runner.py:469] Time since start: 55501.92s, 	Step: 124654, 	{'train/accuracy': 0.7636519074440002, 'train/loss': 0.890670895576477, 'validation/accuracy': 0.6882799863815308, 'validation/loss': 1.2617956399917603, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9657535552978516, 'test/num_examples': 10000, 'score': 51565.887436151505, 'total_duration': 55501.91971373558, 'accumulated_submission_time': 51565.887436151505, 'accumulated_eval_time': 3912.2268607616425, 'accumulated_logging_time': 9.495509624481201}
I0307 18:03:16.332098 140096103761664 logging_writer.py:48] [124654] accumulated_eval_time=3912.23, accumulated_logging_time=9.49551, accumulated_submission_time=51565.9, global_step=124654, preemption_count=0, score=51565.9, test/accuracy=0.5621, test/loss=1.96575, test/num_examples=10000, total_duration=55501.9, train/accuracy=0.763652, train/loss=0.890671, validation/accuracy=0.68828, validation/loss=1.2618, validation/num_examples=50000
I0307 18:03:35.073978 140096095368960 logging_writer.py:48] [124700] global_step=124700, grad_norm=4.6138153076171875, loss=1.4097027778625488
I0307 18:04:14.762551 140096103761664 logging_writer.py:48] [124800] global_step=124800, grad_norm=6.948451519012451, loss=1.5105557441711426
I0307 18:04:54.143325 140096095368960 logging_writer.py:48] [124900] global_step=124900, grad_norm=5.178185939788818, loss=1.432572841644287
I0307 18:05:33.560918 140096103761664 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.387368679046631, loss=1.424560785293579
2025-03-07 18:06:07.971540: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:06:13.335678 140096095368960 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.8011064529418945, loss=1.4393737316131592
I0307 18:06:52.733826 140096103761664 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.925107479095459, loss=1.416948676109314
I0307 18:07:32.498516 140096095368960 logging_writer.py:48] [125300] global_step=125300, grad_norm=4.8291521072387695, loss=1.3851300477981567
I0307 18:08:12.123747 140096103761664 logging_writer.py:48] [125400] global_step=125400, grad_norm=5.670234203338623, loss=1.5459446907043457
I0307 18:08:51.719892 140096095368960 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.913912296295166, loss=1.4845236539840698
I0307 18:09:31.454489 140096103761664 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.520291328430176, loss=1.3308740854263306
I0307 18:10:11.256870 140096095368960 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.352593898773193, loss=1.3130097389221191
I0307 18:10:51.086715 140096103761664 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.161947727203369, loss=1.4823614358901978
I0307 18:11:30.692530 140096095368960 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.469873905181885, loss=1.4050328731536865
I0307 18:11:46.613685 140252175811776 spec.py:321] Evaluating on the training split.
I0307 18:11:57.481277 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 18:12:17.969472 140252175811776 spec.py:349] Evaluating on the test split.
I0307 18:12:19.796205 140252175811776 submission_runner.py:469] Time since start: 56045.43s, 	Step: 125940, 	{'train/accuracy': 0.7603037357330322, 'train/loss': 0.9058126211166382, 'validation/accuracy': 0.6906599998474121, 'validation/loss': 1.2521040439605713, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 1.9706153869628906, 'test/num_examples': 10000, 'score': 52076.00114798546, 'total_duration': 56045.42695713043, 'accumulated_submission_time': 52076.00114798546, 'accumulated_eval_time': 3945.4093585014343, 'accumulated_logging_time': 9.546129941940308}
I0307 18:12:19.842693 140096103761664 logging_writer.py:48] [125940] accumulated_eval_time=3945.41, accumulated_logging_time=9.54613, accumulated_submission_time=52076, global_step=125940, preemption_count=0, score=52076, test/accuracy=0.5607, test/loss=1.97062, test/num_examples=10000, total_duration=56045.4, train/accuracy=0.760304, train/loss=0.905813, validation/accuracy=0.69066, validation/loss=1.2521, validation/num_examples=50000
I0307 18:12:44.004950 140096095368960 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.313597202301025, loss=1.4183869361877441
I0307 18:13:23.435070 140096103761664 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.6625871658325195, loss=1.4036219120025635
I0307 18:14:03.285605 140096095368960 logging_writer.py:48] [126200] global_step=126200, grad_norm=4.9125752449035645, loss=1.3012579679489136
I0307 18:14:43.479341 140096103761664 logging_writer.py:48] [126300] global_step=126300, grad_norm=5.211963653564453, loss=1.380314826965332
2025-03-07 18:14:58.458124: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:15:23.008111 140096095368960 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.832920551300049, loss=1.3566577434539795
I0307 18:16:02.419053 140096103761664 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.229621410369873, loss=1.43650484085083
I0307 18:16:42.307967 140096095368960 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.039275646209717, loss=1.464040994644165
I0307 18:17:21.567206 140096103761664 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.741250038146973, loss=1.5328667163848877
I0307 18:18:01.383083 140096095368960 logging_writer.py:48] [126800] global_step=126800, grad_norm=5.283406734466553, loss=1.4327161312103271
I0307 18:18:41.098113 140096103761664 logging_writer.py:48] [126900] global_step=126900, grad_norm=4.992521286010742, loss=1.3857041597366333
I0307 18:19:20.411225 140096095368960 logging_writer.py:48] [127000] global_step=127000, grad_norm=6.1076273918151855, loss=1.4267466068267822
I0307 18:20:00.011206 140096103761664 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.3257904052734375, loss=1.4260039329528809
I0307 18:20:38.932331 140096095368960 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.475812911987305, loss=1.3457207679748535
I0307 18:20:49.875220 140252175811776 spec.py:321] Evaluating on the training split.
I0307 18:21:00.280088 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 18:21:22.342848 140252175811776 spec.py:349] Evaluating on the test split.
I0307 18:21:24.155462 140252175811776 submission_runner.py:469] Time since start: 56589.79s, 	Step: 127228, 	{'train/accuracy': 0.7605229616165161, 'train/loss': 0.897878110408783, 'validation/accuracy': 0.6896599531173706, 'validation/loss': 1.2560820579528809, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 1.9651907682418823, 'test/num_examples': 10000, 'score': 52585.853692770004, 'total_duration': 56589.78621697426, 'accumulated_submission_time': 52585.853692770004, 'accumulated_eval_time': 3979.689565896988, 'accumulated_logging_time': 9.615399837493896}
I0307 18:21:24.206202 140096103761664 logging_writer.py:48] [127228] accumulated_eval_time=3979.69, accumulated_logging_time=9.6154, accumulated_submission_time=52585.9, global_step=127228, preemption_count=0, score=52585.9, test/accuracy=0.5615, test/loss=1.96519, test/num_examples=10000, total_duration=56589.8, train/accuracy=0.760523, train/loss=0.897878, validation/accuracy=0.68966, validation/loss=1.25608, validation/num_examples=50000
I0307 18:21:52.890002 140096095368960 logging_writer.py:48] [127300] global_step=127300, grad_norm=4.711240291595459, loss=1.438633918762207
I0307 18:22:32.244308 140096103761664 logging_writer.py:48] [127400] global_step=127400, grad_norm=5.559841632843018, loss=1.417054533958435
I0307 18:23:12.039718 140096095368960 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.022661209106445, loss=1.3632172346115112
2025-03-07 18:23:48.361766: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:23:52.498908 140096103761664 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.324165344238281, loss=1.387826681137085
I0307 18:24:32.018053 140096095368960 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.125677585601807, loss=1.4252992868423462
I0307 18:25:11.519903 140096103761664 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.12166166305542, loss=1.3587720394134521
I0307 18:25:50.936513 140096095368960 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.51951265335083, loss=1.3430445194244385
I0307 18:26:30.291614 140096103761664 logging_writer.py:48] [128000] global_step=128000, grad_norm=4.782370090484619, loss=1.3349833488464355
I0307 18:27:09.017961 140096095368960 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.616124629974365, loss=1.4083788394927979
I0307 18:27:48.319050 140096103761664 logging_writer.py:48] [128200] global_step=128200, grad_norm=7.082925796508789, loss=1.363968014717102
I0307 18:28:27.723384 140096095368960 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.520229339599609, loss=1.3558002710342407
I0307 18:29:07.219009 140096103761664 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.019293308258057, loss=1.3249293565750122
I0307 18:29:47.012530 140096095368960 logging_writer.py:48] [128500] global_step=128500, grad_norm=4.81471586227417, loss=1.3474713563919067
I0307 18:29:54.225542 140252175811776 spec.py:321] Evaluating on the training split.
I0307 18:30:04.995733 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 18:30:28.782510 140252175811776 spec.py:349] Evaluating on the test split.
I0307 18:30:30.541859 140252175811776 submission_runner.py:469] Time since start: 57136.17s, 	Step: 128519, 	{'train/accuracy': 0.7751514315605164, 'train/loss': 0.8432853817939758, 'validation/accuracy': 0.698419988155365, 'validation/loss': 1.2087401151657104, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.894498586654663, 'test/num_examples': 10000, 'score': 53095.70389556885, 'total_duration': 57136.17261147499, 'accumulated_submission_time': 53095.70389556885, 'accumulated_eval_time': 4016.0058567523956, 'accumulated_logging_time': 9.67530870437622}
I0307 18:30:30.622213 140096103761664 logging_writer.py:48] [128519] accumulated_eval_time=4016.01, accumulated_logging_time=9.67531, accumulated_submission_time=53095.7, global_step=128519, preemption_count=0, score=53095.7, test/accuracy=0.5723, test/loss=1.8945, test/num_examples=10000, total_duration=57136.2, train/accuracy=0.775151, train/loss=0.843285, validation/accuracy=0.69842, validation/loss=1.20874, validation/num_examples=50000
I0307 18:31:03.258356 140096095368960 logging_writer.py:48] [128600] global_step=128600, grad_norm=5.804887294769287, loss=1.3717050552368164
I0307 18:31:42.842306 140096103761664 logging_writer.py:48] [128700] global_step=128700, grad_norm=5.16008186340332, loss=1.4645600318908691
I0307 18:32:23.509172 140096095368960 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.315982818603516, loss=1.359560251235962
2025-03-07 18:32:39.867241: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:33:03.254106 140096103761664 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.924566268920898, loss=1.3226842880249023
I0307 18:33:42.315865 140096095368960 logging_writer.py:48] [129000] global_step=129000, grad_norm=6.256457328796387, loss=1.4921858310699463
I0307 18:34:21.481685 140096103761664 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.8364129066467285, loss=1.3291198015213013
I0307 18:35:00.788637 140096095368960 logging_writer.py:48] [129200] global_step=129200, grad_norm=5.518797397613525, loss=1.4644558429718018
I0307 18:35:39.922952 140096103761664 logging_writer.py:48] [129300] global_step=129300, grad_norm=5.4277544021606445, loss=1.4192330837249756
I0307 18:36:19.292871 140096095368960 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.5315656661987305, loss=1.3017191886901855
I0307 18:36:58.212991 140096103761664 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.528651714324951, loss=1.4325873851776123
I0307 18:37:37.368024 140096095368960 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.432741165161133, loss=1.3339358568191528
I0307 18:38:16.413936 140096103761664 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.697500228881836, loss=1.4759957790374756
I0307 18:38:56.420137 140096095368960 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.380496978759766, loss=1.480204463005066
I0307 18:39:00.955396 140252175811776 spec.py:321] Evaluating on the training split.
I0307 18:39:11.581820 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 18:39:30.726883 140252175811776 spec.py:349] Evaluating on the test split.
I0307 18:39:32.541958 140252175811776 submission_runner.py:469] Time since start: 57678.17s, 	Step: 129812, 	{'train/accuracy': 0.7685546875, 'train/loss': 0.8609792590141296, 'validation/accuracy': 0.6956799626350403, 'validation/loss': 1.2237547636032104, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9257469177246094, 'test/num_examples': 10000, 'score': 53605.86897301674, 'total_duration': 57678.17271089554, 'accumulated_submission_time': 53605.86897301674, 'accumulated_eval_time': 4047.5923838615417, 'accumulated_logging_time': 9.763998985290527}
I0307 18:39:32.636805 140096103761664 logging_writer.py:48] [129812] accumulated_eval_time=4047.59, accumulated_logging_time=9.764, accumulated_submission_time=53605.9, global_step=129812, preemption_count=0, score=53605.9, test/accuracy=0.568, test/loss=1.92575, test/num_examples=10000, total_duration=57678.2, train/accuracy=0.768555, train/loss=0.860979, validation/accuracy=0.69568, validation/loss=1.22375, validation/num_examples=50000
I0307 18:40:07.458001 140096095368960 logging_writer.py:48] [129900] global_step=129900, grad_norm=5.619670391082764, loss=1.437971830368042
I0307 18:40:46.540189 140096103761664 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.612192153930664, loss=1.382643461227417
2025-03-07 18:41:22.861765: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:41:26.358569 140096095368960 logging_writer.py:48] [130100] global_step=130100, grad_norm=5.971824645996094, loss=1.2794475555419922
I0307 18:42:05.753419 140096103761664 logging_writer.py:48] [130200] global_step=130200, grad_norm=5.596632957458496, loss=1.338438630104065
I0307 18:42:44.795233 140096095368960 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.500089645385742, loss=1.5534170866012573
I0307 18:43:24.115886 140096103761664 logging_writer.py:48] [130400] global_step=130400, grad_norm=5.575767993927002, loss=1.3014436960220337
I0307 18:44:03.396127 140096095368960 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.9748311042785645, loss=1.3954358100891113
I0307 18:44:42.323087 140096103761664 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.528051376342773, loss=1.4029653072357178
I0307 18:45:21.831378 140096095368960 logging_writer.py:48] [130700] global_step=130700, grad_norm=5.369198322296143, loss=1.418557047843933
I0307 18:46:01.190224 140096103761664 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.866151809692383, loss=1.3114013671875
I0307 18:46:40.781743 140096095368960 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.423923015594482, loss=1.3609956502914429
I0307 18:47:20.278380 140096103761664 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.741966247558594, loss=1.3743767738342285
I0307 18:47:59.957651 140096095368960 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.1370320320129395, loss=1.2895500659942627
I0307 18:48:02.592194 140252175811776 spec.py:321] Evaluating on the training split.
I0307 18:48:13.405459 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 18:48:37.014766 140252175811776 spec.py:349] Evaluating on the test split.
I0307 18:48:38.780562 140252175811776 submission_runner.py:469] Time since start: 58224.41s, 	Step: 131108, 	{'train/accuracy': 0.7782405614852905, 'train/loss': 0.8348714113235474, 'validation/accuracy': 0.6997999548912048, 'validation/loss': 1.2101482152938843, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 1.904631495475769, 'test/num_examples': 10000, 'score': 54115.65716171265, 'total_duration': 58224.411308288574, 'accumulated_submission_time': 54115.65716171265, 'accumulated_eval_time': 4083.7807126045227, 'accumulated_logging_time': 9.86639666557312}
I0307 18:48:38.874734 140096103761664 logging_writer.py:48] [131108] accumulated_eval_time=4083.78, accumulated_logging_time=9.8664, accumulated_submission_time=54115.7, global_step=131108, preemption_count=0, score=54115.7, test/accuracy=0.5784, test/loss=1.90463, test/num_examples=10000, total_duration=58224.4, train/accuracy=0.778241, train/loss=0.834871, validation/accuracy=0.6998, validation/loss=1.21015, validation/num_examples=50000
I0307 18:49:15.371926 140096095368960 logging_writer.py:48] [131200] global_step=131200, grad_norm=5.628747940063477, loss=1.3242369890213013
I0307 18:49:55.521387 140096103761664 logging_writer.py:48] [131300] global_step=131300, grad_norm=4.882681369781494, loss=1.3366562128067017
2025-03-07 18:50:12.450335: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:50:35.377466 140096095368960 logging_writer.py:48] [131400] global_step=131400, grad_norm=5.59089994430542, loss=1.352816104888916
I0307 18:51:15.021766 140096103761664 logging_writer.py:48] [131500] global_step=131500, grad_norm=5.420024394989014, loss=1.3336427211761475
I0307 18:51:54.699723 140096095368960 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.452450752258301, loss=1.3815364837646484
I0307 18:52:34.317350 140096103761664 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.420090675354004, loss=1.339383840560913
I0307 18:53:13.876237 140096095368960 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.183095932006836, loss=1.3301292657852173
I0307 18:53:53.460806 140096103761664 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.201975345611572, loss=1.3738346099853516
I0307 18:54:32.303022 140096095368960 logging_writer.py:48] [132000] global_step=132000, grad_norm=6.56982946395874, loss=1.3826854228973389
I0307 18:55:11.737813 140096103761664 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.326505184173584, loss=1.339064121246338
I0307 18:55:51.276335 140096095368960 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.845366954803467, loss=1.3852503299713135
I0307 18:56:31.020044 140096103761664 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.689540386199951, loss=1.4807134866714478
I0307 18:57:08.971663 140252175811776 spec.py:321] Evaluating on the training split.
I0307 18:57:19.572650 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 18:57:41.753797 140252175811776 spec.py:349] Evaluating on the test split.
I0307 18:57:43.562015 140252175811776 submission_runner.py:469] Time since start: 58769.19s, 	Step: 132397, 	{'train/accuracy': 0.7766461968421936, 'train/loss': 0.830456018447876, 'validation/accuracy': 0.698639988899231, 'validation/loss': 1.204158902168274, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.885985255241394, 'test/num_examples': 10000, 'score': 54625.58908200264, 'total_duration': 58769.19275927544, 'accumulated_submission_time': 54625.58908200264, 'accumulated_eval_time': 4118.371031522751, 'accumulated_logging_time': 9.967960357666016}
I0307 18:57:43.609128 140096095368960 logging_writer.py:48] [132397] accumulated_eval_time=4118.37, accumulated_logging_time=9.96796, accumulated_submission_time=54625.6, global_step=132397, preemption_count=0, score=54625.6, test/accuracy=0.5768, test/loss=1.88599, test/num_examples=10000, total_duration=58769.2, train/accuracy=0.776646, train/loss=0.830456, validation/accuracy=0.69864, validation/loss=1.20416, validation/num_examples=50000
I0307 18:57:45.036282 140096103761664 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.45543098449707, loss=1.358485460281372
I0307 18:58:24.671327 140096095368960 logging_writer.py:48] [132500] global_step=132500, grad_norm=5.46609354019165, loss=1.3113473653793335
2025-03-07 18:59:01.992272: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:59:04.699685 140096103761664 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.582238674163818, loss=1.471710205078125
I0307 18:59:44.569046 140096095368960 logging_writer.py:48] [132700] global_step=132700, grad_norm=5.464179992675781, loss=1.3896111249923706
I0307 19:00:24.372735 140096103761664 logging_writer.py:48] [132800] global_step=132800, grad_norm=6.312941551208496, loss=1.4568942785263062
I0307 19:01:03.801180 140096095368960 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.939764976501465, loss=1.4094605445861816
I0307 19:01:43.187739 140096103761664 logging_writer.py:48] [133000] global_step=133000, grad_norm=6.470165729522705, loss=1.3684810400009155
I0307 19:02:22.526540 140096095368960 logging_writer.py:48] [133100] global_step=133100, grad_norm=5.815035820007324, loss=1.3440688848495483
I0307 19:03:01.665538 140096103761664 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.862765312194824, loss=1.3945460319519043
I0307 19:03:40.814324 140096095368960 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.45673942565918, loss=1.3737300634384155
I0307 19:04:20.072909 140096103761664 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.537273406982422, loss=1.3582699298858643
I0307 19:04:59.753993 140096095368960 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.796902656555176, loss=1.3273102045059204
I0307 19:05:39.520791 140096103761664 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.664942741394043, loss=1.3521702289581299
I0307 19:06:13.724036 140252175811776 spec.py:321] Evaluating on the training split.
I0307 19:06:24.489910 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 19:06:47.871603 140252175811776 spec.py:349] Evaluating on the test split.
I0307 19:06:49.649667 140252175811776 submission_runner.py:469] Time since start: 59315.28s, 	Step: 133688, 	{'train/accuracy': 0.7813296914100647, 'train/loss': 0.8155295252799988, 'validation/accuracy': 0.6997599601745605, 'validation/loss': 1.2029445171356201, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 1.901422381401062, 'test/num_examples': 10000, 'score': 55135.537725925446, 'total_duration': 59315.28038811684, 'accumulated_submission_time': 55135.537725925446, 'accumulated_eval_time': 4154.2966051101685, 'accumulated_logging_time': 10.023295640945435}
I0307 19:06:49.718924 140096095368960 logging_writer.py:48] [133688] accumulated_eval_time=4154.3, accumulated_logging_time=10.0233, accumulated_submission_time=55135.5, global_step=133688, preemption_count=0, score=55135.5, test/accuracy=0.5756, test/loss=1.90142, test/num_examples=10000, total_duration=59315.3, train/accuracy=0.78133, train/loss=0.81553, validation/accuracy=0.69976, validation/loss=1.20294, validation/num_examples=50000
I0307 19:06:54.902806 140096103761664 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.252223968505859, loss=1.2708284854888916
I0307 19:07:34.630686 140096095368960 logging_writer.py:48] [133800] global_step=133800, grad_norm=5.284689426422119, loss=1.3557829856872559
2025-03-07 19:07:52.643867: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:08:14.885349 140096103761664 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.841297149658203, loss=1.3913190364837646
I0307 19:08:54.425057 140096095368960 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.0678205490112305, loss=1.3151562213897705
I0307 19:09:33.814989 140096103761664 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.524693489074707, loss=1.319430947303772
I0307 19:10:13.229504 140096095368960 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.867329120635986, loss=1.420703649520874
I0307 19:10:52.497803 140096103761664 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.519951820373535, loss=1.4741132259368896
I0307 19:11:32.540985 140096095368960 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.859687328338623, loss=1.397523045539856
I0307 19:12:12.548604 140096103761664 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.992226600646973, loss=1.3581883907318115
I0307 19:12:52.200397 140096095368960 logging_writer.py:48] [134600] global_step=134600, grad_norm=6.532024383544922, loss=1.3692946434020996
I0307 19:13:31.952527 140096103761664 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.760917663574219, loss=1.263227939605713
I0307 19:14:12.199233 140096095368960 logging_writer.py:48] [134800] global_step=134800, grad_norm=5.4444475173950195, loss=1.3554767370224
I0307 19:14:52.172035 140096103761664 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.596315860748291, loss=1.3291404247283936
I0307 19:15:19.781860 140252175811776 spec.py:321] Evaluating on the training split.
I0307 19:15:30.158362 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 19:15:52.041373 140252175811776 spec.py:349] Evaluating on the test split.
I0307 19:15:53.841842 140252175811776 submission_runner.py:469] Time since start: 59859.47s, 	Step: 134970, 	{'train/accuracy': 0.7872887253761292, 'train/loss': 0.7722330689430237, 'validation/accuracy': 0.7044199705123901, 'validation/loss': 1.199290156364441, 'validation/num_examples': 50000, 'test/accuracy': 0.5774000287055969, 'test/loss': 1.885286569595337, 'test/num_examples': 10000, 'score': 55645.414152383804, 'total_duration': 59859.47259473801, 'accumulated_submission_time': 55645.414152383804, 'accumulated_eval_time': 4188.356557846069, 'accumulated_logging_time': 10.117146015167236}
I0307 19:15:53.885089 140096095368960 logging_writer.py:48] [134970] accumulated_eval_time=4188.36, accumulated_logging_time=10.1171, accumulated_submission_time=55645.4, global_step=134970, preemption_count=0, score=55645.4, test/accuracy=0.5774, test/loss=1.88529, test/num_examples=10000, total_duration=59859.5, train/accuracy=0.787289, train/loss=0.772233, validation/accuracy=0.70442, validation/loss=1.19929, validation/num_examples=50000
I0307 19:16:06.635193 140096103761664 logging_writer.py:48] [135000] global_step=135000, grad_norm=6.383316516876221, loss=1.3639843463897705
2025-03-07 19:16:45.370253: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:16:46.641316 140096095368960 logging_writer.py:48] [135100] global_step=135100, grad_norm=5.380895614624023, loss=1.3205058574676514
I0307 19:17:26.730597 140096103761664 logging_writer.py:48] [135200] global_step=135200, grad_norm=5.9852519035339355, loss=1.3597081899642944
I0307 19:18:06.758647 140096095368960 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.317372798919678, loss=1.2438021898269653
I0307 19:18:46.303367 140096103761664 logging_writer.py:48] [135400] global_step=135400, grad_norm=5.4504075050354, loss=1.4916037321090698
I0307 19:19:26.167839 140096095368960 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.709542274475098, loss=1.3788485527038574
I0307 19:20:05.979221 140096103761664 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.911667823791504, loss=1.1240571737289429
I0307 19:20:45.563637 140096095368960 logging_writer.py:48] [135700] global_step=135700, grad_norm=5.460019111633301, loss=1.1954752206802368
I0307 19:21:25.224809 140096103761664 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.891232967376709, loss=1.3770787715911865
I0307 19:22:04.748739 140096095368960 logging_writer.py:48] [135900] global_step=135900, grad_norm=5.723169803619385, loss=1.2954069375991821
I0307 19:22:44.691448 140096103761664 logging_writer.py:48] [136000] global_step=136000, grad_norm=5.889636516571045, loss=1.310680866241455
I0307 19:23:24.977005 140096095368960 logging_writer.py:48] [136100] global_step=136100, grad_norm=6.0090651512146, loss=1.3826826810836792
I0307 19:24:05.327533 140096103761664 logging_writer.py:48] [136200] global_step=136200, grad_norm=6.213757038116455, loss=1.2338345050811768
I0307 19:24:23.904813 140252175811776 spec.py:321] Evaluating on the training split.
I0307 19:24:34.749659 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 19:24:54.569139 140252175811776 spec.py:349] Evaluating on the test split.
I0307 19:24:56.363980 140252175811776 submission_runner.py:469] Time since start: 60401.99s, 	Step: 136247, 	{'train/accuracy': 0.7858338356018066, 'train/loss': 0.7971621155738831, 'validation/accuracy': 0.7060199975967407, 'validation/loss': 1.1900910139083862, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.882903814315796, 'test/num_examples': 10000, 'score': 56155.26686882973, 'total_duration': 60401.994733810425, 'accumulated_submission_time': 56155.26686882973, 'accumulated_eval_time': 4220.815694093704, 'accumulated_logging_time': 10.169111013412476}
I0307 19:24:56.411619 140096095368960 logging_writer.py:48] [136247] accumulated_eval_time=4220.82, accumulated_logging_time=10.1691, accumulated_submission_time=56155.3, global_step=136247, preemption_count=0, score=56155.3, test/accuracy=0.5789, test/loss=1.8829, test/num_examples=10000, total_duration=60402, train/accuracy=0.785834, train/loss=0.797162, validation/accuracy=0.70602, validation/loss=1.19009, validation/num_examples=50000
I0307 19:25:17.544643 140096103761664 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.677257537841797, loss=1.2891857624053955
2025-03-07 19:25:37.026696: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:25:57.812409 140096095368960 logging_writer.py:48] [136400] global_step=136400, grad_norm=5.772266387939453, loss=1.3336952924728394
I0307 19:26:37.665800 140096103761664 logging_writer.py:48] [136500] global_step=136500, grad_norm=5.689425468444824, loss=1.4251489639282227
I0307 19:27:17.192887 140096095368960 logging_writer.py:48] [136600] global_step=136600, grad_norm=5.426600933074951, loss=1.2899762392044067
I0307 19:27:56.884757 140096103761664 logging_writer.py:48] [136700] global_step=136700, grad_norm=6.016948223114014, loss=1.3154712915420532
I0307 19:28:36.132340 140096095368960 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.886910915374756, loss=1.3817709684371948
I0307 19:29:15.657553 140096103761664 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.379408836364746, loss=1.23188316822052
I0307 19:29:56.265915 140096095368960 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.589123249053955, loss=1.2940617799758911
I0307 19:30:36.281392 140096103761664 logging_writer.py:48] [137100] global_step=137100, grad_norm=5.901910305023193, loss=1.293949007987976
I0307 19:31:15.568260 140096095368960 logging_writer.py:48] [137200] global_step=137200, grad_norm=6.089219570159912, loss=1.3503865003585815
I0307 19:31:55.909479 140096103761664 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.888294219970703, loss=1.2747913599014282
I0307 19:32:35.932229 140096095368960 logging_writer.py:48] [137400] global_step=137400, grad_norm=6.770315170288086, loss=1.337668538093567
I0307 19:33:16.077981 140096103761664 logging_writer.py:48] [137500] global_step=137500, grad_norm=5.973854064941406, loss=1.3468044996261597
I0307 19:33:26.520437 140252175811776 spec.py:321] Evaluating on the training split.
I0307 19:33:37.477205 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 19:34:05.497362 140252175811776 spec.py:349] Evaluating on the test split.
I0307 19:34:07.302660 140252175811776 submission_runner.py:469] Time since start: 60952.93s, 	Step: 137527, 	{'train/accuracy': 0.7935865521430969, 'train/loss': 0.7578516602516174, 'validation/accuracy': 0.7087999582290649, 'validation/loss': 1.1751463413238525, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.8559602499008179, 'test/num_examples': 10000, 'score': 56665.20880031586, 'total_duration': 60952.933406829834, 'accumulated_submission_time': 56665.20880031586, 'accumulated_eval_time': 4261.597877979279, 'accumulated_logging_time': 10.225021123886108}
I0307 19:34:07.377532 140096095368960 logging_writer.py:48] [137527] accumulated_eval_time=4261.6, accumulated_logging_time=10.225, accumulated_submission_time=56665.2, global_step=137527, preemption_count=0, score=56665.2, test/accuracy=0.585, test/loss=1.85596, test/num_examples=10000, total_duration=60952.9, train/accuracy=0.793587, train/loss=0.757852, validation/accuracy=0.7088, validation/loss=1.17515, validation/num_examples=50000
2025-03-07 19:34:36.674235: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:34:37.183688 140096103761664 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.780161380767822, loss=1.289190411567688
I0307 19:35:17.675755 140096095368960 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.809216022491455, loss=1.308136224746704
I0307 19:35:57.505246 140096103761664 logging_writer.py:48] [137800] global_step=137800, grad_norm=6.7989654541015625, loss=1.3952655792236328
I0307 19:36:37.004640 140096095368960 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.493920803070068, loss=1.2275878190994263
I0307 19:37:16.651697 140096103761664 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.630952835083008, loss=1.392362356185913
I0307 19:37:56.825743 140096095368960 logging_writer.py:48] [138100] global_step=138100, grad_norm=5.777406692504883, loss=1.242864966392517
I0307 19:38:37.004984 140096103761664 logging_writer.py:48] [138200] global_step=138200, grad_norm=6.0347208976745605, loss=1.1734706163406372
I0307 19:39:16.517810 140096095368960 logging_writer.py:48] [138300] global_step=138300, grad_norm=5.811674118041992, loss=1.209364652633667
I0307 19:39:56.312382 140096103761664 logging_writer.py:48] [138400] global_step=138400, grad_norm=5.349689960479736, loss=1.2408490180969238
I0307 19:40:36.552501 140096095368960 logging_writer.py:48] [138500] global_step=138500, grad_norm=6.263954162597656, loss=1.3558077812194824
I0307 19:41:16.673386 140096103761664 logging_writer.py:48] [138600] global_step=138600, grad_norm=5.7199506759643555, loss=1.3172580003738403
I0307 19:41:56.322919 140096095368960 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.10944938659668, loss=1.26313054561615
I0307 19:42:36.102658 140096103761664 logging_writer.py:48] [138800] global_step=138800, grad_norm=6.061767101287842, loss=1.2714788913726807
I0307 19:42:37.423349 140252175811776 spec.py:321] Evaluating on the training split.
I0307 19:42:47.999906 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 19:43:09.963621 140252175811776 spec.py:349] Evaluating on the test split.
I0307 19:43:11.765213 140252175811776 submission_runner.py:469] Time since start: 61497.40s, 	Step: 138804, 	{'train/accuracy': 0.7951211333274841, 'train/loss': 0.7586897611618042, 'validation/accuracy': 0.7091799974441528, 'validation/loss': 1.1667213439941406, 'validation/num_examples': 50000, 'test/accuracy': 0.5847000479698181, 'test/loss': 1.8426032066345215, 'test/num_examples': 10000, 'score': 57175.09089779854, 'total_duration': 61497.39592576027, 'accumulated_submission_time': 57175.09089779854, 'accumulated_eval_time': 4295.9396686553955, 'accumulated_logging_time': 10.307328939437866}
I0307 19:43:11.844318 140096095368960 logging_writer.py:48] [138804] accumulated_eval_time=4295.94, accumulated_logging_time=10.3073, accumulated_submission_time=57175.1, global_step=138804, preemption_count=0, score=57175.1, test/accuracy=0.5847, test/loss=1.8426, test/num_examples=10000, total_duration=61497.4, train/accuracy=0.795121, train/loss=0.75869, validation/accuracy=0.70918, validation/loss=1.16672, validation/num_examples=50000
2025-03-07 19:43:30.373787: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:43:50.234622 140096103761664 logging_writer.py:48] [138900] global_step=138900, grad_norm=5.908957481384277, loss=1.236086368560791
I0307 19:44:29.929158 140096095368960 logging_writer.py:48] [139000] global_step=139000, grad_norm=6.0616254806518555, loss=1.3456177711486816
I0307 19:45:09.639245 140096103761664 logging_writer.py:48] [139100] global_step=139100, grad_norm=6.245652198791504, loss=1.382741093635559
I0307 19:45:49.840297 140096095368960 logging_writer.py:48] [139200] global_step=139200, grad_norm=5.822037220001221, loss=1.2402995824813843
I0307 19:46:29.265424 140096103761664 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.285556316375732, loss=1.2954938411712646
I0307 19:47:08.746535 140096095368960 logging_writer.py:48] [139400] global_step=139400, grad_norm=6.215867519378662, loss=1.216689109802246
I0307 19:47:48.581609 140096103761664 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.214940071105957, loss=1.275018572807312
I0307 19:48:28.170464 140096095368960 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.659155368804932, loss=1.283700942993164
I0307 19:49:07.757310 140096103761664 logging_writer.py:48] [139700] global_step=139700, grad_norm=7.184601306915283, loss=1.2236928939819336
I0307 19:49:47.992227 140096095368960 logging_writer.py:48] [139800] global_step=139800, grad_norm=6.1722493171691895, loss=1.3080239295959473
I0307 19:50:27.891041 140096103761664 logging_writer.py:48] [139900] global_step=139900, grad_norm=6.1974358558654785, loss=1.2729719877243042
I0307 19:51:07.446842 140096095368960 logging_writer.py:48] [140000] global_step=140000, grad_norm=6.340806484222412, loss=1.2544245719909668
I0307 19:51:42.081809 140252175811776 spec.py:321] Evaluating on the training split.
I0307 19:51:52.603509 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 19:52:15.451949 140252175811776 spec.py:349] Evaluating on the test split.
I0307 19:52:17.195278 140252175811776 submission_runner.py:469] Time since start: 62042.83s, 	Step: 140087, 	{'train/accuracy': 0.7984095811843872, 'train/loss': 0.7405584454536438, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.1850301027297974, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.8664261102676392, 'test/num_examples': 10000, 'score': 57685.15996623039, 'total_duration': 62042.82602930069, 'accumulated_submission_time': 57685.15996623039, 'accumulated_eval_time': 4331.053107976913, 'accumulated_logging_time': 10.394988298416138}
I0307 19:52:17.277914 140096103761664 logging_writer.py:48] [140087] accumulated_eval_time=4331.05, accumulated_logging_time=10.395, accumulated_submission_time=57685.2, global_step=140087, preemption_count=0, score=57685.2, test/accuracy=0.5845, test/loss=1.86643, test/num_examples=10000, total_duration=62042.8, train/accuracy=0.79841, train/loss=0.740558, validation/accuracy=0.70642, validation/loss=1.18503, validation/num_examples=50000
I0307 19:52:22.818542 140096095368960 logging_writer.py:48] [140100] global_step=140100, grad_norm=5.950726509094238, loss=1.2680888175964355
2025-03-07 19:52:23.076882: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:53:02.310990 140096103761664 logging_writer.py:48] [140200] global_step=140200, grad_norm=5.814467906951904, loss=1.2864940166473389
I0307 19:53:42.391074 140096095368960 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.45625114440918, loss=1.3917242288589478
I0307 19:54:22.355408 140096103761664 logging_writer.py:48] [140400] global_step=140400, grad_norm=6.300893306732178, loss=1.3119252920150757
I0307 19:55:01.714899 140096095368960 logging_writer.py:48] [140500] global_step=140500, grad_norm=5.828045845031738, loss=1.3807799816131592
I0307 19:55:40.925050 140096103761664 logging_writer.py:48] [140600] global_step=140600, grad_norm=5.5228352546691895, loss=1.1597387790679932
I0307 19:56:20.840681 140096095368960 logging_writer.py:48] [140700] global_step=140700, grad_norm=6.164744853973389, loss=1.1871886253356934
I0307 19:57:00.667565 140096103761664 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.38860559463501, loss=1.3935495615005493
I0307 19:57:40.196874 140096095368960 logging_writer.py:48] [140900] global_step=140900, grad_norm=6.43583345413208, loss=1.2593685388565063
I0307 19:58:20.037243 140096103761664 logging_writer.py:48] [141000] global_step=141000, grad_norm=5.910924434661865, loss=1.314386010169983
I0307 19:59:00.273567 140096095368960 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.6917805671691895, loss=1.2811381816864014
I0307 19:59:40.170394 140096103761664 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.136441707611084, loss=1.0796103477478027
I0307 20:00:19.998177 140096095368960 logging_writer.py:48] [141300] global_step=141300, grad_norm=5.896420478820801, loss=1.171027421951294
2025-03-07 20:00:40.673678: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:00:47.378862 140252175811776 spec.py:321] Evaluating on the training split.
I0307 20:00:58.372091 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 20:01:19.304927 140252175811776 spec.py:349] Evaluating on the test split.
I0307 20:01:21.120338 140252175811776 submission_runner.py:469] Time since start: 62586.75s, 	Step: 141370, 	{'train/accuracy': 0.8007014989852905, 'train/loss': 0.7408760786056519, 'validation/accuracy': 0.7092999815940857, 'validation/loss': 1.165321946144104, 'validation/num_examples': 50000, 'test/accuracy': 0.5843999981880188, 'test/loss': 1.8240811824798584, 'test/num_examples': 10000, 'score': 58195.09639596939, 'total_duration': 62586.75109219551, 'accumulated_submission_time': 58195.09639596939, 'accumulated_eval_time': 4364.7945556640625, 'accumulated_logging_time': 10.48622989654541}
I0307 20:01:21.173889 140096103761664 logging_writer.py:48] [141370] accumulated_eval_time=4364.79, accumulated_logging_time=10.4862, accumulated_submission_time=58195.1, global_step=141370, preemption_count=0, score=58195.1, test/accuracy=0.5844, test/loss=1.82408, test/num_examples=10000, total_duration=62586.8, train/accuracy=0.800701, train/loss=0.740876, validation/accuracy=0.7093, validation/loss=1.16532, validation/num_examples=50000
I0307 20:01:33.519457 140096095368960 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.716125011444092, loss=1.410419225692749
I0307 20:02:13.643676 140096103761664 logging_writer.py:48] [141500] global_step=141500, grad_norm=5.8437180519104, loss=1.2547476291656494
I0307 20:02:53.620054 140096095368960 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.25308084487915, loss=1.2180591821670532
I0307 20:03:33.584655 140096103761664 logging_writer.py:48] [141700] global_step=141700, grad_norm=5.884951114654541, loss=1.2100344896316528
I0307 20:04:13.419476 140096095368960 logging_writer.py:48] [141800] global_step=141800, grad_norm=6.660938262939453, loss=1.3143306970596313
I0307 20:04:53.224687 140096103761664 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.953457832336426, loss=1.275653600692749
I0307 20:05:33.060280 140096095368960 logging_writer.py:48] [142000] global_step=142000, grad_norm=6.370579242706299, loss=1.30221426486969
I0307 20:06:13.184339 140096103761664 logging_writer.py:48] [142100] global_step=142100, grad_norm=5.568517684936523, loss=1.2227095365524292
I0307 20:06:52.823994 140096095368960 logging_writer.py:48] [142200] global_step=142200, grad_norm=6.068839073181152, loss=1.322333812713623
I0307 20:07:32.774975 140096103761664 logging_writer.py:48] [142300] global_step=142300, grad_norm=5.8806867599487305, loss=1.236218810081482
I0307 20:08:13.013674 140096095368960 logging_writer.py:48] [142400] global_step=142400, grad_norm=5.986374855041504, loss=1.2453560829162598
I0307 20:08:52.827275 140096103761664 logging_writer.py:48] [142500] global_step=142500, grad_norm=5.659425735473633, loss=1.1568537950515747
I0307 20:09:33.648245 140096095368960 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.015413284301758, loss=1.1692169904708862
2025-03-07 20:09:34.852202: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:09:51.265453 140252175811776 spec.py:321] Evaluating on the training split.
I0307 20:10:01.907186 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 20:10:20.609482 140252175811776 spec.py:349] Evaluating on the test split.
I0307 20:10:22.407090 140252175811776 submission_runner.py:469] Time since start: 63128.04s, 	Step: 142644, 	{'train/accuracy': 0.8115234375, 'train/loss': 0.6917855143547058, 'validation/accuracy': 0.7137799859046936, 'validation/loss': 1.137590765953064, 'validation/num_examples': 50000, 'test/accuracy': 0.5895000100135803, 'test/loss': 1.818124532699585, 'test/num_examples': 10000, 'score': 58705.022627830505, 'total_duration': 63128.03783941269, 'accumulated_submission_time': 58705.022627830505, 'accumulated_eval_time': 4395.936160326004, 'accumulated_logging_time': 10.548065185546875}
I0307 20:10:22.473540 140096103761664 logging_writer.py:48] [142644] accumulated_eval_time=4395.94, accumulated_logging_time=10.5481, accumulated_submission_time=58705, global_step=142644, preemption_count=0, score=58705, test/accuracy=0.5895, test/loss=1.81812, test/num_examples=10000, total_duration=63128, train/accuracy=0.811523, train/loss=0.691786, validation/accuracy=0.71378, validation/loss=1.13759, validation/num_examples=50000
I0307 20:10:44.574532 140096095368960 logging_writer.py:48] [142700] global_step=142700, grad_norm=6.375009536743164, loss=1.2834913730621338
I0307 20:11:24.334903 140096103761664 logging_writer.py:48] [142800] global_step=142800, grad_norm=6.685600280761719, loss=1.3327574729919434
I0307 20:12:04.052057 140096095368960 logging_writer.py:48] [142900] global_step=142900, grad_norm=6.508586406707764, loss=1.3305649757385254
I0307 20:12:44.219112 140096103761664 logging_writer.py:48] [143000] global_step=143000, grad_norm=5.974985122680664, loss=1.1702922582626343
I0307 20:13:23.511446 140096095368960 logging_writer.py:48] [143100] global_step=143100, grad_norm=6.257815361022949, loss=1.152752161026001
I0307 20:14:02.738412 140096103761664 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.776387691497803, loss=1.23485267162323
I0307 20:14:41.844777 140096095368960 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.888779640197754, loss=1.3125237226486206
I0307 20:15:21.262597 140096103761664 logging_writer.py:48] [143400] global_step=143400, grad_norm=6.289686679840088, loss=1.3137176036834717
I0307 20:16:00.151180 140096095368960 logging_writer.py:48] [143500] global_step=143500, grad_norm=6.564062595367432, loss=1.1668857336044312
I0307 20:16:39.748897 140096103761664 logging_writer.py:48] [143600] global_step=143600, grad_norm=5.932017803192139, loss=1.2668702602386475
I0307 20:17:19.386840 140096095368960 logging_writer.py:48] [143700] global_step=143700, grad_norm=6.459693431854248, loss=1.3891180753707886
I0307 20:17:59.235889 140096103761664 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.247024059295654, loss=1.2540037631988525
2025-03-07 20:18:20.869157: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:18:39.318271 140096095368960 logging_writer.py:48] [143900] global_step=143900, grad_norm=6.59984827041626, loss=1.3048996925354004
I0307 20:18:52.873135 140252175811776 spec.py:321] Evaluating on the training split.
I0307 20:19:03.601138 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 20:19:24.871208 140252175811776 spec.py:349] Evaluating on the test split.
I0307 20:19:26.671253 140252175811776 submission_runner.py:469] Time since start: 63672.30s, 	Step: 143935, 	{'train/accuracy': 0.8088328838348389, 'train/loss': 0.706734299659729, 'validation/accuracy': 0.7102599740028381, 'validation/loss': 1.1563358306884766, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 1.8385635614395142, 'test/num_examples': 10000, 'score': 59215.257622241974, 'total_duration': 63672.30200910568, 'accumulated_submission_time': 59215.257622241974, 'accumulated_eval_time': 4429.734256267548, 'accumulated_logging_time': 10.622171640396118}
I0307 20:19:26.719518 140096103761664 logging_writer.py:48] [143935] accumulated_eval_time=4429.73, accumulated_logging_time=10.6222, accumulated_submission_time=59215.3, global_step=143935, preemption_count=0, score=59215.3, test/accuracy=0.5875, test/loss=1.83856, test/num_examples=10000, total_duration=63672.3, train/accuracy=0.808833, train/loss=0.706734, validation/accuracy=0.71026, validation/loss=1.15634, validation/num_examples=50000
I0307 20:19:53.402452 140096095368960 logging_writer.py:48] [144000] global_step=144000, grad_norm=6.296706676483154, loss=1.281814694404602
I0307 20:20:32.977081 140096103761664 logging_writer.py:48] [144100] global_step=144100, grad_norm=5.936382293701172, loss=1.1892685890197754
I0307 20:21:12.777070 140096095368960 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.506030559539795, loss=1.3299022912979126
I0307 20:21:52.805106 140096103761664 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.7768449783325195, loss=1.2552433013916016
I0307 20:22:32.922564 140096095368960 logging_writer.py:48] [144400] global_step=144400, grad_norm=5.9758100509643555, loss=1.183237075805664
I0307 20:23:12.751984 140096103761664 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.749992847442627, loss=1.2402552366256714
I0307 20:23:52.888728 140096095368960 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.6159796714782715, loss=1.2861989736557007
I0307 20:24:32.847106 140096103761664 logging_writer.py:48] [144700] global_step=144700, grad_norm=6.542365550994873, loss=1.199034571647644
I0307 20:25:12.712312 140096095368960 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.112825393676758, loss=1.2641754150390625
I0307 20:25:52.560552 140096103761664 logging_writer.py:48] [144900] global_step=144900, grad_norm=6.296199798583984, loss=1.2149051427841187
I0307 20:26:32.819922 140096095368960 logging_writer.py:48] [145000] global_step=145000, grad_norm=6.380904197692871, loss=1.1199637651443481
I0307 20:27:12.688379 140096103761664 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.870727062225342, loss=1.2551655769348145
2025-03-07 20:27:14.811025: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:27:52.668929 140096095368960 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.632221698760986, loss=1.1432430744171143
I0307 20:27:56.687195 140252175811776 spec.py:321] Evaluating on the training split.
I0307 20:28:07.310877 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 20:28:29.305364 140252175811776 spec.py:349] Evaluating on the test split.
I0307 20:28:31.105506 140252175811776 submission_runner.py:469] Time since start: 64216.74s, 	Step: 145211, 	{'train/accuracy': 0.8124003410339355, 'train/loss': 0.6877221465110779, 'validation/accuracy': 0.7174400091171265, 'validation/loss': 1.131879210472107, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 1.813792109489441, 'test/num_examples': 10000, 'score': 59725.05596446991, 'total_duration': 64216.73625707626, 'accumulated_submission_time': 59725.05596446991, 'accumulated_eval_time': 4464.152533054352, 'accumulated_logging_time': 10.679142236709595}
I0307 20:28:31.173160 140096103761664 logging_writer.py:48] [145211] accumulated_eval_time=4464.15, accumulated_logging_time=10.6791, accumulated_submission_time=59725.1, global_step=145211, preemption_count=0, score=59725.1, test/accuracy=0.5892, test/loss=1.81379, test/num_examples=10000, total_duration=64216.7, train/accuracy=0.8124, train/loss=0.687722, validation/accuracy=0.71744, validation/loss=1.13188, validation/num_examples=50000
I0307 20:29:06.957049 140096095368960 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.1328277587890625, loss=1.1731793880462646
I0307 20:29:46.427250 140096103761664 logging_writer.py:48] [145400] global_step=145400, grad_norm=6.175375938415527, loss=1.154676079750061
I0307 20:30:25.939285 140096095368960 logging_writer.py:48] [145500] global_step=145500, grad_norm=5.816603660583496, loss=1.1601628065109253
I0307 20:31:05.990220 140096103761664 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.3011155128479, loss=1.2357699871063232
I0307 20:31:45.452986 140096095368960 logging_writer.py:48] [145700] global_step=145700, grad_norm=6.762906074523926, loss=1.2350958585739136
I0307 20:32:24.760753 140096103761664 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.989788055419922, loss=1.3232874870300293
I0307 20:33:04.389677 140096095368960 logging_writer.py:48] [145900] global_step=145900, grad_norm=6.532415390014648, loss=1.3422425985336304
I0307 20:33:44.175607 140096103761664 logging_writer.py:48] [146000] global_step=146000, grad_norm=7.421060562133789, loss=1.2481712102890015
I0307 20:34:24.448579 140096095368960 logging_writer.py:48] [146100] global_step=146100, grad_norm=6.1181864738464355, loss=1.206984519958496
I0307 20:35:04.217004 140096103761664 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.875101089477539, loss=1.2054712772369385
I0307 20:35:44.556292 140096095368960 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.565951824188232, loss=1.1976704597473145
2025-03-07 20:36:07.362191: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:36:24.788070 140096103761664 logging_writer.py:48] [146400] global_step=146400, grad_norm=6.199632167816162, loss=1.137944221496582
I0307 20:37:01.251682 140252175811776 spec.py:321] Evaluating on the training split.
I0307 20:37:11.513069 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 20:37:31.270255 140252175811776 spec.py:349] Evaluating on the test split.
I0307 20:37:33.080911 140252175811776 submission_runner.py:469] Time since start: 64758.71s, 	Step: 146492, 	{'train/accuracy': 0.8207908272743225, 'train/loss': 0.6453962326049805, 'validation/accuracy': 0.7214599847793579, 'validation/loss': 1.1162569522857666, 'validation/num_examples': 50000, 'test/accuracy': 0.5996000170707703, 'test/loss': 1.7889827489852905, 'test/num_examples': 10000, 'score': 60234.96898651123, 'total_duration': 64758.7116651535, 'accumulated_submission_time': 60234.96898651123, 'accumulated_eval_time': 4495.981732130051, 'accumulated_logging_time': 10.755004405975342}
I0307 20:37:33.122664 140096095368960 logging_writer.py:48] [146492] accumulated_eval_time=4495.98, accumulated_logging_time=10.755, accumulated_submission_time=60235, global_step=146492, preemption_count=0, score=60235, test/accuracy=0.5996, test/loss=1.78898, test/num_examples=10000, total_duration=64758.7, train/accuracy=0.820791, train/loss=0.645396, validation/accuracy=0.72146, validation/loss=1.11626, validation/num_examples=50000
I0307 20:37:36.506022 140096103761664 logging_writer.py:48] [146500] global_step=146500, grad_norm=6.232035160064697, loss=1.1882002353668213
I0307 20:38:15.659236 140096095368960 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.721615791320801, loss=1.1568655967712402
I0307 20:38:54.316320 140096103761664 logging_writer.py:48] [146700] global_step=146700, grad_norm=7.410012245178223, loss=1.2481443881988525
I0307 20:39:33.837501 140096095368960 logging_writer.py:48] [146800] global_step=146800, grad_norm=7.578028678894043, loss=1.1781872510910034
I0307 20:40:12.946835 140096103761664 logging_writer.py:48] [146900] global_step=146900, grad_norm=6.493127822875977, loss=1.205946922302246
I0307 20:40:51.855502 140096095368960 logging_writer.py:48] [147000] global_step=147000, grad_norm=6.047287464141846, loss=1.2258341312408447
I0307 20:41:31.673497 140096103761664 logging_writer.py:48] [147100] global_step=147100, grad_norm=6.076215744018555, loss=1.142837643623352
I0307 20:42:11.035243 140096095368960 logging_writer.py:48] [147200] global_step=147200, grad_norm=6.129812717437744, loss=1.1886045932769775
I0307 20:42:51.018499 140096103761664 logging_writer.py:48] [147300] global_step=147300, grad_norm=6.850762367248535, loss=1.2112661600112915
I0307 20:43:31.046833 140096095368960 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.009031772613525, loss=1.1937707662582397
I0307 20:44:11.120686 140096103761664 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.059985160827637, loss=1.1647642850875854
I0307 20:44:51.039443 140096095368960 logging_writer.py:48] [147600] global_step=147600, grad_norm=6.0601420402526855, loss=1.2072930335998535
2025-03-07 20:44:55.259993: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:45:31.194399 140096103761664 logging_writer.py:48] [147700] global_step=147700, grad_norm=6.954895496368408, loss=1.1438196897506714
I0307 20:46:03.149698 140252175811776 spec.py:321] Evaluating on the training split.
I0307 20:46:13.641130 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 20:46:34.270137 140252175811776 spec.py:349] Evaluating on the test split.
I0307 20:46:36.064631 140252175811776 submission_runner.py:469] Time since start: 65301.70s, 	Step: 147781, 	{'train/accuracy': 0.8203921914100647, 'train/loss': 0.6459478139877319, 'validation/accuracy': 0.7193399667739868, 'validation/loss': 1.1231507062911987, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.82082200050354, 'test/num_examples': 10000, 'score': 60744.82824254036, 'total_duration': 65301.6953830719, 'accumulated_submission_time': 60744.82824254036, 'accumulated_eval_time': 4528.896654129028, 'accumulated_logging_time': 10.804770469665527}
I0307 20:46:36.122554 140096095368960 logging_writer.py:48] [147781] accumulated_eval_time=4528.9, accumulated_logging_time=10.8048, accumulated_submission_time=60744.8, global_step=147781, preemption_count=0, score=60744.8, test/accuracy=0.5906, test/loss=1.82082, test/num_examples=10000, total_duration=65301.7, train/accuracy=0.820392, train/loss=0.645948, validation/accuracy=0.71934, validation/loss=1.12315, validation/num_examples=50000
I0307 20:46:43.921206 140096103761664 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.290090084075928, loss=1.1445550918579102
I0307 20:47:23.207418 140096095368960 logging_writer.py:48] [147900] global_step=147900, grad_norm=6.905077934265137, loss=1.0792160034179688
I0307 20:48:03.240887 140096103761664 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.797272682189941, loss=1.2524794340133667
I0307 20:48:43.517507 140096095368960 logging_writer.py:48] [148100] global_step=148100, grad_norm=7.421590328216553, loss=1.2397890090942383
I0307 20:49:23.117085 140096103761664 logging_writer.py:48] [148200] global_step=148200, grad_norm=7.149590969085693, loss=1.2475571632385254
I0307 20:50:03.512456 140096095368960 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.346445560455322, loss=1.1933281421661377
I0307 20:50:43.470367 140096103761664 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.395003795623779, loss=1.1805039644241333
I0307 20:51:23.835115 140096095368960 logging_writer.py:48] [148500] global_step=148500, grad_norm=7.210153102874756, loss=1.2241593599319458
I0307 20:52:03.921804 140096103761664 logging_writer.py:48] [148600] global_step=148600, grad_norm=7.186294078826904, loss=1.2523956298828125
I0307 20:52:44.048018 140096095368960 logging_writer.py:48] [148700] global_step=148700, grad_norm=6.385400295257568, loss=1.103046178817749
I0307 20:53:24.023325 140096103761664 logging_writer.py:48] [148800] global_step=148800, grad_norm=6.698746204376221, loss=1.1251754760742188
2025-03-07 20:53:48.449271: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:54:04.564835 140096095368960 logging_writer.py:48] [148900] global_step=148900, grad_norm=6.583384037017822, loss=1.2533659934997559
I0307 20:54:44.127776 140096103761664 logging_writer.py:48] [149000] global_step=149000, grad_norm=6.586701393127441, loss=1.163493275642395
I0307 20:55:06.386867 140252175811776 spec.py:321] Evaluating on the training split.
I0307 20:55:17.004512 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 20:55:37.475095 140252175811776 spec.py:349] Evaluating on the test split.
I0307 20:55:39.285768 140252175811776 submission_runner.py:469] Time since start: 65844.92s, 	Step: 149057, 	{'train/accuracy': 0.8243383169174194, 'train/loss': 0.6349853873252869, 'validation/accuracy': 0.721780002117157, 'validation/loss': 1.1156972646713257, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.818621039390564, 'test/num_examples': 10000, 'score': 61254.92183613777, 'total_duration': 65844.91651058197, 'accumulated_submission_time': 61254.92183613777, 'accumulated_eval_time': 4561.7955157756805, 'accumulated_logging_time': 10.871318340301514}
I0307 20:55:39.331698 140096095368960 logging_writer.py:48] [149057] accumulated_eval_time=4561.8, accumulated_logging_time=10.8713, accumulated_submission_time=61254.9, global_step=149057, preemption_count=0, score=61254.9, test/accuracy=0.5961, test/loss=1.81862, test/num_examples=10000, total_duration=65844.9, train/accuracy=0.824338, train/loss=0.634985, validation/accuracy=0.72178, validation/loss=1.1157, validation/num_examples=50000
I0307 20:55:57.153152 140096103761664 logging_writer.py:48] [149100] global_step=149100, grad_norm=6.919623374938965, loss=1.2148457765579224
I0307 20:56:36.610256 140096095368960 logging_writer.py:48] [149200] global_step=149200, grad_norm=7.074939727783203, loss=1.2682955265045166
I0307 20:57:16.608040 140096103761664 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.956993579864502, loss=1.216449499130249
I0307 20:57:56.765376 140096095368960 logging_writer.py:48] [149400] global_step=149400, grad_norm=6.978730201721191, loss=1.1048965454101562
I0307 20:58:36.339905 140096103761664 logging_writer.py:48] [149500] global_step=149500, grad_norm=6.720402240753174, loss=1.1705399751663208
I0307 20:59:16.193631 140096095368960 logging_writer.py:48] [149600] global_step=149600, grad_norm=7.751317501068115, loss=1.2735371589660645
I0307 20:59:56.247636 140096103761664 logging_writer.py:48] [149700] global_step=149700, grad_norm=6.425719261169434, loss=1.1737922430038452
I0307 21:00:36.573623 140096095368960 logging_writer.py:48] [149800] global_step=149800, grad_norm=6.6893510818481445, loss=1.2285614013671875
I0307 21:01:16.394454 140096103761664 logging_writer.py:48] [149900] global_step=149900, grad_norm=7.078141689300537, loss=1.2100876569747925
I0307 21:01:56.387338 140096095368960 logging_writer.py:48] [150000] global_step=150000, grad_norm=6.821590423583984, loss=1.1640762090682983
I0307 21:02:36.674665 140096103761664 logging_writer.py:48] [150100] global_step=150100, grad_norm=7.057033061981201, loss=1.2134194374084473
2025-03-07 21:02:40.552530: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:03:16.497274 140096095368960 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.21157693862915, loss=1.0928311347961426
I0307 21:03:56.299863 140096103761664 logging_writer.py:48] [150300] global_step=150300, grad_norm=6.916224479675293, loss=1.250559687614441
I0307 21:04:09.554244 140252175811776 spec.py:321] Evaluating on the training split.
I0307 21:04:20.410763 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 21:04:43.233208 140252175811776 spec.py:349] Evaluating on the test split.
I0307 21:04:45.027719 140252175811776 submission_runner.py:469] Time since start: 66390.66s, 	Step: 150334, 	{'train/accuracy': 0.8295400142669678, 'train/loss': 0.6065824031829834, 'validation/accuracy': 0.7263399958610535, 'validation/loss': 1.0979609489440918, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.796953797340393, 'test/num_examples': 10000, 'score': 61764.951920986176, 'total_duration': 66390.65845131874, 'accumulated_submission_time': 61764.951920986176, 'accumulated_eval_time': 4597.268939495087, 'accumulated_logging_time': 10.951146841049194}
I0307 21:04:45.069236 140096095368960 logging_writer.py:48] [150334] accumulated_eval_time=4597.27, accumulated_logging_time=10.9511, accumulated_submission_time=61765, global_step=150334, preemption_count=0, score=61765, test/accuracy=0.6003, test/loss=1.79695, test/num_examples=10000, total_duration=66390.7, train/accuracy=0.82954, train/loss=0.606582, validation/accuracy=0.72634, validation/loss=1.09796, validation/num_examples=50000
I0307 21:05:11.500116 140096103761664 logging_writer.py:48] [150400] global_step=150400, grad_norm=7.386407375335693, loss=1.1695256233215332
I0307 21:05:51.344799 140096095368960 logging_writer.py:48] [150500] global_step=150500, grad_norm=7.007136821746826, loss=1.0720151662826538
I0307 21:06:31.716692 140096103761664 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.595651149749756, loss=1.1788663864135742
I0307 21:07:11.770760 140096095368960 logging_writer.py:48] [150700] global_step=150700, grad_norm=7.163359642028809, loss=1.1629215478897095
I0307 21:07:51.444243 140096103761664 logging_writer.py:48] [150800] global_step=150800, grad_norm=7.078555107116699, loss=1.1438637971878052
I0307 21:08:31.547038 140096095368960 logging_writer.py:48] [150900] global_step=150900, grad_norm=6.134672164916992, loss=1.0598268508911133
I0307 21:09:11.849269 140096103761664 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.402035713195801, loss=1.1396799087524414
I0307 21:09:52.138008 140096095368960 logging_writer.py:48] [151100] global_step=151100, grad_norm=7.447059631347656, loss=1.1321359872817993
I0307 21:10:32.031701 140096103761664 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.461653709411621, loss=1.2110176086425781
I0307 21:11:12.180336 140096095368960 logging_writer.py:48] [151300] global_step=151300, grad_norm=6.747177600860596, loss=1.1937599182128906
2025-03-07 21:11:36.896197: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:11:52.640688 140096103761664 logging_writer.py:48] [151400] global_step=151400, grad_norm=7.423933506011963, loss=1.2425177097320557
I0307 21:12:32.855403 140096095368960 logging_writer.py:48] [151500] global_step=151500, grad_norm=6.604236602783203, loss=1.0860706567764282
I0307 21:13:12.226508 140096103761664 logging_writer.py:48] [151600] global_step=151600, grad_norm=6.978316307067871, loss=1.1520429849624634
I0307 21:13:15.321385 140252175811776 spec.py:321] Evaluating on the training split.
I0307 21:13:26.118642 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 21:13:46.706492 140252175811776 spec.py:349] Evaluating on the test split.
I0307 21:13:48.507949 140252175811776 submission_runner.py:469] Time since start: 66934.14s, 	Step: 151609, 	{'train/accuracy': 0.8327686190605164, 'train/loss': 0.602982223033905, 'validation/accuracy': 0.7255799770355225, 'validation/loss': 1.0966386795043945, 'validation/num_examples': 50000, 'test/accuracy': 0.5989000201225281, 'test/loss': 1.7889082431793213, 'test/num_examples': 10000, 'score': 62275.03699326515, 'total_duration': 66934.13870358467, 'accumulated_submission_time': 62275.03699326515, 'accumulated_eval_time': 4630.4554743766785, 'accumulated_logging_time': 11.000649690628052}
I0307 21:13:48.579353 140096095368960 logging_writer.py:48] [151609] accumulated_eval_time=4630.46, accumulated_logging_time=11.0006, accumulated_submission_time=62275, global_step=151609, preemption_count=0, score=62275, test/accuracy=0.5989, test/loss=1.78891, test/num_examples=10000, total_duration=66934.1, train/accuracy=0.832769, train/loss=0.602982, validation/accuracy=0.72558, validation/loss=1.09664, validation/num_examples=50000
I0307 21:14:24.752547 140096103761664 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.667932510375977, loss=1.1672215461730957
I0307 21:15:04.681257 140096095368960 logging_writer.py:48] [151800] global_step=151800, grad_norm=6.797449111938477, loss=1.1034793853759766
I0307 21:15:44.933882 140096103761664 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.032894611358643, loss=1.056553602218628
I0307 21:16:24.572522 140096095368960 logging_writer.py:48] [152000] global_step=152000, grad_norm=7.151890277862549, loss=1.0951318740844727
I0307 21:17:05.023241 140096103761664 logging_writer.py:48] [152100] global_step=152100, grad_norm=7.26884651184082, loss=1.0896954536437988
I0307 21:17:44.891945 140096095368960 logging_writer.py:48] [152200] global_step=152200, grad_norm=6.970585346221924, loss=1.2714987993240356
I0307 21:18:25.212674 140096103761664 logging_writer.py:48] [152300] global_step=152300, grad_norm=7.313406944274902, loss=1.1878185272216797
I0307 21:19:05.192472 140096095368960 logging_writer.py:48] [152400] global_step=152400, grad_norm=7.092011451721191, loss=1.2302758693695068
I0307 21:19:45.123633 140096103761664 logging_writer.py:48] [152500] global_step=152500, grad_norm=7.731786727905273, loss=1.2459051609039307
I0307 21:20:25.643985 140096095368960 logging_writer.py:48] [152600] global_step=152600, grad_norm=6.811521053314209, loss=1.1014264822006226
2025-03-07 21:20:30.944470: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:21:06.028252 140096103761664 logging_writer.py:48] [152700] global_step=152700, grad_norm=7.123685359954834, loss=1.1513557434082031
I0307 21:21:45.908079 140096095368960 logging_writer.py:48] [152800] global_step=152800, grad_norm=6.828480243682861, loss=1.1632329225540161
I0307 21:22:18.693494 140252175811776 spec.py:321] Evaluating on the training split.
I0307 21:22:29.556339 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 21:22:50.254247 140252175811776 spec.py:349] Evaluating on the test split.
I0307 21:22:52.064050 140252175811776 submission_runner.py:469] Time since start: 67477.69s, 	Step: 152883, 	{'train/accuracy': 0.8391461968421936, 'train/loss': 0.5757182836532593, 'validation/accuracy': 0.7305200099945068, 'validation/loss': 1.0808967351913452, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.7599254846572876, 'test/num_examples': 10000, 'score': 62784.98686337471, 'total_duration': 67477.69480133057, 'accumulated_submission_time': 62784.98686337471, 'accumulated_eval_time': 4663.8260016441345, 'accumulated_logging_time': 11.07994270324707}
I0307 21:22:52.128536 140096103761664 logging_writer.py:48] [152883] accumulated_eval_time=4663.83, accumulated_logging_time=11.0799, accumulated_submission_time=62785, global_step=152883, preemption_count=0, score=62785, test/accuracy=0.605, test/loss=1.75993, test/num_examples=10000, total_duration=67477.7, train/accuracy=0.839146, train/loss=0.575718, validation/accuracy=0.73052, validation/loss=1.0809, validation/num_examples=50000
I0307 21:22:59.354724 140096095368960 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.109808921813965, loss=1.133743166923523
I0307 21:23:39.140661 140096103761664 logging_writer.py:48] [153000] global_step=153000, grad_norm=6.775600433349609, loss=1.2204737663269043
I0307 21:24:19.275431 140096095368960 logging_writer.py:48] [153100] global_step=153100, grad_norm=7.727003574371338, loss=1.19552481174469
I0307 21:24:59.391201 140096103761664 logging_writer.py:48] [153200] global_step=153200, grad_norm=6.949532985687256, loss=1.112672209739685
I0307 21:25:39.297446 140096095368960 logging_writer.py:48] [153300] global_step=153300, grad_norm=6.775486469268799, loss=1.1049349308013916
I0307 21:26:19.021439 140096103761664 logging_writer.py:48] [153400] global_step=153400, grad_norm=6.878713130950928, loss=1.148461937904358
I0307 21:26:59.346748 140096095368960 logging_writer.py:48] [153500] global_step=153500, grad_norm=6.835013389587402, loss=1.1314373016357422
I0307 21:27:39.566299 140096103761664 logging_writer.py:48] [153600] global_step=153600, grad_norm=6.920805931091309, loss=1.0183830261230469
I0307 21:28:19.402538 140096095368960 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.257011413574219, loss=1.014234185218811
I0307 21:28:59.723384 140096103761664 logging_writer.py:48] [153800] global_step=153800, grad_norm=7.868703365325928, loss=1.0362496376037598
2025-03-07 21:29:25.560185: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:29:40.251805 140096095368960 logging_writer.py:48] [153900] global_step=153900, grad_norm=7.088260650634766, loss=1.136506199836731
I0307 21:30:20.271263 140096103761664 logging_writer.py:48] [154000] global_step=154000, grad_norm=6.940977096557617, loss=1.1762880086898804
I0307 21:31:00.081268 140096095368960 logging_writer.py:48] [154100] global_step=154100, grad_norm=7.532747745513916, loss=1.146394968032837
I0307 21:31:22.299694 140252175811776 spec.py:321] Evaluating on the training split.
I0307 21:31:32.678775 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 21:31:54.353852 140252175811776 spec.py:349] Evaluating on the test split.
I0307 21:31:56.162635 140252175811776 submission_runner.py:469] Time since start: 68021.79s, 	Step: 154157, 	{'train/accuracy': 0.8438097834587097, 'train/loss': 0.5621725916862488, 'validation/accuracy': 0.7273600101470947, 'validation/loss': 1.0979546308517456, 'validation/num_examples': 50000, 'test/accuracy': 0.6049000024795532, 'test/loss': 1.778617262840271, 'test/num_examples': 10000, 'score': 63294.97476530075, 'total_duration': 68021.79338860512, 'accumulated_submission_time': 63294.97476530075, 'accumulated_eval_time': 4697.68891620636, 'accumulated_logging_time': 11.17039442062378}
I0307 21:31:56.262943 140096103761664 logging_writer.py:48] [154157] accumulated_eval_time=4697.69, accumulated_logging_time=11.1704, accumulated_submission_time=63295, global_step=154157, preemption_count=0, score=63295, test/accuracy=0.6049, test/loss=1.77862, test/num_examples=10000, total_duration=68021.8, train/accuracy=0.84381, train/loss=0.562173, validation/accuracy=0.72736, validation/loss=1.09795, validation/num_examples=50000
I0307 21:32:13.838370 140096095368960 logging_writer.py:48] [154200] global_step=154200, grad_norm=7.389936923980713, loss=1.13764488697052
I0307 21:32:53.368475 140096103761664 logging_writer.py:48] [154300] global_step=154300, grad_norm=6.433022975921631, loss=1.0132099390029907
I0307 21:33:32.629029 140096095368960 logging_writer.py:48] [154400] global_step=154400, grad_norm=6.729544639587402, loss=1.0763652324676514
I0307 21:34:12.312689 140096103761664 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.351744651794434, loss=1.180261254310608
I0307 21:34:51.931716 140096095368960 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.523525238037109, loss=1.1016837358474731
I0307 21:35:31.257050 140096103761664 logging_writer.py:48] [154700] global_step=154700, grad_norm=7.718876361846924, loss=1.1603630781173706
I0307 21:36:11.129417 140096095368960 logging_writer.py:48] [154800] global_step=154800, grad_norm=6.887553691864014, loss=1.0578231811523438
I0307 21:36:50.778240 140096103761664 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.829516887664795, loss=1.1985867023468018
I0307 21:37:30.287034 140096095368960 logging_writer.py:48] [155000] global_step=155000, grad_norm=6.724155902862549, loss=1.1040823459625244
I0307 21:38:10.222079 140096103761664 logging_writer.py:48] [155100] global_step=155100, grad_norm=6.916718006134033, loss=1.039607286453247
2025-03-07 21:38:16.773660: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:38:50.532727 140096095368960 logging_writer.py:48] [155200] global_step=155200, grad_norm=6.9800028800964355, loss=1.0942034721374512
I0307 21:39:30.000127 140096103761664 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.390594482421875, loss=1.09403657913208
I0307 21:40:09.140791 140096095368960 logging_writer.py:48] [155400] global_step=155400, grad_norm=7.138908863067627, loss=1.160709261894226
I0307 21:40:26.556053 140252175811776 spec.py:321] Evaluating on the training split.
I0307 21:40:36.969293 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 21:40:55.224108 140252175811776 spec.py:349] Evaluating on the test split.
I0307 21:40:57.045348 140252175811776 submission_runner.py:469] Time since start: 68562.68s, 	Step: 155445, 	{'train/accuracy': 0.8530173897743225, 'train/loss': 0.5314945578575134, 'validation/accuracy': 0.7315199971199036, 'validation/loss': 1.0715699195861816, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.7311569452285767, 'test/num_examples': 10000, 'score': 63805.09834551811, 'total_duration': 68562.676060915, 'accumulated_submission_time': 63805.09834551811, 'accumulated_eval_time': 4728.1781396865845, 'accumulated_logging_time': 11.278674840927124}
I0307 21:40:57.099768 140096103761664 logging_writer.py:48] [155445] accumulated_eval_time=4728.18, accumulated_logging_time=11.2787, accumulated_submission_time=63805.1, global_step=155445, preemption_count=0, score=63805.1, test/accuracy=0.6086, test/loss=1.73116, test/num_examples=10000, total_duration=68562.7, train/accuracy=0.853017, train/loss=0.531495, validation/accuracy=0.73152, validation/loss=1.07157, validation/num_examples=50000
I0307 21:41:19.045111 140096095368960 logging_writer.py:48] [155500] global_step=155500, grad_norm=7.196404457092285, loss=1.0951358079910278
I0307 21:41:58.312516 140096103761664 logging_writer.py:48] [155600] global_step=155600, grad_norm=7.095040321350098, loss=1.011228322982788
I0307 21:42:37.967715 140096095368960 logging_writer.py:48] [155700] global_step=155700, grad_norm=7.259964942932129, loss=1.111765742301941
I0307 21:43:17.826203 140096103761664 logging_writer.py:48] [155800] global_step=155800, grad_norm=8.03128719329834, loss=1.1287063360214233
I0307 21:43:57.820239 140096095368960 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.010062217712402, loss=1.1131281852722168
I0307 21:44:37.856872 140096103761664 logging_writer.py:48] [156000] global_step=156000, grad_norm=7.372342586517334, loss=1.1391996145248413
I0307 21:45:18.189422 140096095368960 logging_writer.py:48] [156100] global_step=156100, grad_norm=7.846407890319824, loss=1.1182289123535156
I0307 21:45:58.675553 140096103761664 logging_writer.py:48] [156200] global_step=156200, grad_norm=6.814881801605225, loss=1.0993620157241821
I0307 21:46:38.533412 140096095368960 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.564633369445801, loss=1.1510320901870728
2025-03-07 21:47:04.293695: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:47:17.861680 140096103761664 logging_writer.py:48] [156400] global_step=156400, grad_norm=7.321352005004883, loss=1.0338280200958252
I0307 21:47:56.924023 140096095368960 logging_writer.py:48] [156500] global_step=156500, grad_norm=7.531186580657959, loss=1.0680643320083618
I0307 21:48:36.069490 140096103761664 logging_writer.py:48] [156600] global_step=156600, grad_norm=7.445979595184326, loss=1.1097970008850098
I0307 21:49:14.840806 140096095368960 logging_writer.py:48] [156700] global_step=156700, grad_norm=6.967002868652344, loss=1.0467159748077393
I0307 21:49:27.341333 140252175811776 spec.py:321] Evaluating on the training split.
I0307 21:49:38.318507 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 21:49:58.445953 140252175811776 spec.py:349] Evaluating on the test split.
I0307 21:50:00.257555 140252175811776 submission_runner.py:469] Time since start: 69105.89s, 	Step: 156733, 	{'train/accuracy': 0.8538544178009033, 'train/loss': 0.523781955242157, 'validation/accuracy': 0.7325199842453003, 'validation/loss': 1.0809742212295532, 'validation/num_examples': 50000, 'test/accuracy': 0.6030000448226929, 'test/loss': 1.7881934642791748, 'test/num_examples': 10000, 'score': 64315.171521663666, 'total_duration': 69105.88829541206, 'accumulated_submission_time': 64315.171521663666, 'accumulated_eval_time': 4761.094323158264, 'accumulated_logging_time': 11.342117309570312}
I0307 21:50:00.323509 140096103761664 logging_writer.py:48] [156733] accumulated_eval_time=4761.09, accumulated_logging_time=11.3421, accumulated_submission_time=64315.2, global_step=156733, preemption_count=0, score=64315.2, test/accuracy=0.603, test/loss=1.78819, test/num_examples=10000, total_duration=69105.9, train/accuracy=0.853854, train/loss=0.523782, validation/accuracy=0.73252, validation/loss=1.08097, validation/num_examples=50000
I0307 21:50:27.328910 140096095368960 logging_writer.py:48] [156800] global_step=156800, grad_norm=8.110788345336914, loss=1.1426887512207031
I0307 21:51:06.731423 140096103761664 logging_writer.py:48] [156900] global_step=156900, grad_norm=6.53076696395874, loss=0.9738280773162842
I0307 21:51:46.734399 140096095368960 logging_writer.py:48] [157000] global_step=157000, grad_norm=6.894886493682861, loss=1.1788172721862793
I0307 21:52:26.458865 140096103761664 logging_writer.py:48] [157100] global_step=157100, grad_norm=6.792060375213623, loss=1.0562174320220947
I0307 21:53:05.619633 140096095368960 logging_writer.py:48] [157200] global_step=157200, grad_norm=7.996626853942871, loss=1.0234739780426025
I0307 21:53:46.131319 140096103761664 logging_writer.py:48] [157300] global_step=157300, grad_norm=6.792940139770508, loss=1.0542311668395996
I0307 21:54:26.275484 140096095368960 logging_writer.py:48] [157400] global_step=157400, grad_norm=7.120187759399414, loss=0.99373859167099
I0307 21:55:05.709332 140096103761664 logging_writer.py:48] [157500] global_step=157500, grad_norm=7.9826226234436035, loss=0.972793698310852
I0307 21:55:45.162757 140096095368960 logging_writer.py:48] [157600] global_step=157600, grad_norm=6.6411638259887695, loss=0.8834348320960999
2025-03-07 21:55:51.700025: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:56:25.221921 140096103761664 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.729686260223389, loss=1.0732345581054688
I0307 21:57:04.970392 140096095368960 logging_writer.py:48] [157800] global_step=157800, grad_norm=7.549116611480713, loss=1.1258410215377808
I0307 21:57:44.777455 140096103761664 logging_writer.py:48] [157900] global_step=157900, grad_norm=7.370418548583984, loss=1.0968539714813232
I0307 21:58:24.409831 140096095368960 logging_writer.py:48] [158000] global_step=158000, grad_norm=6.796987056732178, loss=1.0300830602645874
I0307 21:58:30.293115 140252175811776 spec.py:321] Evaluating on the training split.
I0307 21:58:40.933626 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 21:59:02.984939 140252175811776 spec.py:349] Evaluating on the test split.
I0307 21:59:04.800887 140252175811776 submission_runner.py:469] Time since start: 69650.43s, 	Step: 158016, 	{'train/accuracy': 0.8593351244926453, 'train/loss': 0.5009995698928833, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.0563217401504517, 'validation/num_examples': 50000, 'test/accuracy': 0.6119000315666199, 'test/loss': 1.7387914657592773, 'test/num_examples': 10000, 'score': 64824.97560667992, 'total_duration': 69650.43163871765, 'accumulated_submission_time': 64824.97560667992, 'accumulated_eval_time': 4795.602059841156, 'accumulated_logging_time': 11.415817260742188}
I0307 21:59:04.853044 140096103761664 logging_writer.py:48] [158016] accumulated_eval_time=4795.6, accumulated_logging_time=11.4158, accumulated_submission_time=64825, global_step=158016, preemption_count=0, score=64825, test/accuracy=0.6119, test/loss=1.73879, test/num_examples=10000, total_duration=69650.4, train/accuracy=0.859335, train/loss=0.501, validation/accuracy=0.73584, validation/loss=1.05632, validation/num_examples=50000
I0307 21:59:38.290279 140096095368960 logging_writer.py:48] [158100] global_step=158100, grad_norm=8.295470237731934, loss=1.0666230916976929
I0307 22:00:17.977183 140096103761664 logging_writer.py:48] [158200] global_step=158200, grad_norm=7.326632022857666, loss=1.023618221282959
I0307 22:00:57.730306 140096095368960 logging_writer.py:48] [158300] global_step=158300, grad_norm=7.444705486297607, loss=1.101078987121582
I0307 22:01:37.308554 140096103761664 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.939679145812988, loss=1.0820156335830688
I0307 22:02:17.141393 140096095368960 logging_writer.py:48] [158500] global_step=158500, grad_norm=7.596231937408447, loss=1.0745737552642822
I0307 22:02:57.038453 140096103761664 logging_writer.py:48] [158600] global_step=158600, grad_norm=7.097474098205566, loss=1.10133957862854
I0307 22:03:36.753878 140096095368960 logging_writer.py:48] [158700] global_step=158700, grad_norm=6.860538482666016, loss=1.0221977233886719
I0307 22:04:16.367351 140096103761664 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.32766056060791, loss=1.0722062587738037
2025-03-07 22:04:43.087603: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:04:55.934771 140096095368960 logging_writer.py:48] [158900] global_step=158900, grad_norm=7.462549209594727, loss=1.073476791381836
I0307 22:05:35.005630 140096103761664 logging_writer.py:48] [159000] global_step=159000, grad_norm=6.533332824707031, loss=0.9499525427818298
I0307 22:06:14.432908 140096095368960 logging_writer.py:48] [159100] global_step=159100, grad_norm=8.233845710754395, loss=1.0081324577331543
I0307 22:06:54.099279 140096103761664 logging_writer.py:48] [159200] global_step=159200, grad_norm=7.826414108276367, loss=1.0986777544021606
I0307 22:07:33.718917 140096095368960 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.656920433044434, loss=1.0566222667694092
I0307 22:07:34.938129 140252175811776 spec.py:321] Evaluating on the training split.
I0307 22:07:45.795726 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 22:08:05.375226 140252175811776 spec.py:349] Evaluating on the test split.
I0307 22:08:07.190923 140252175811776 submission_runner.py:469] Time since start: 70192.82s, 	Step: 159304, 	{'train/accuracy': 0.8622847199440002, 'train/loss': 0.49015215039253235, 'validation/accuracy': 0.7390799522399902, 'validation/loss': 1.048416018486023, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.763635277748108, 'test/num_examples': 10000, 'score': 65334.89833903313, 'total_duration': 70192.82166719437, 'accumulated_submission_time': 65334.89833903313, 'accumulated_eval_time': 4827.854813575745, 'accumulated_logging_time': 11.475934743881226}
I0307 22:08:07.224075 140096103761664 logging_writer.py:48] [159304] accumulated_eval_time=4827.85, accumulated_logging_time=11.4759, accumulated_submission_time=65334.9, global_step=159304, preemption_count=0, score=65334.9, test/accuracy=0.6076, test/loss=1.76364, test/num_examples=10000, total_duration=70192.8, train/accuracy=0.862285, train/loss=0.490152, validation/accuracy=0.73908, validation/loss=1.04842, validation/num_examples=50000
I0307 22:08:45.546816 140096095368960 logging_writer.py:48] [159400] global_step=159400, grad_norm=7.729894638061523, loss=1.0373573303222656
I0307 22:09:25.162597 140096103761664 logging_writer.py:48] [159500] global_step=159500, grad_norm=7.6921000480651855, loss=1.1432409286499023
I0307 22:10:05.161375 140096095368960 logging_writer.py:48] [159600] global_step=159600, grad_norm=7.009265899658203, loss=1.008075475692749
I0307 22:10:45.053396 140096103761664 logging_writer.py:48] [159700] global_step=159700, grad_norm=8.023903846740723, loss=1.0497198104858398
I0307 22:11:24.758174 140096095368960 logging_writer.py:48] [159800] global_step=159800, grad_norm=8.543551445007324, loss=1.0357842445373535
I0307 22:12:05.250598 140096103761664 logging_writer.py:48] [159900] global_step=159900, grad_norm=8.167893409729004, loss=1.1266602277755737
I0307 22:12:44.535791 140096095368960 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.2336931228637695, loss=1.0013540983200073
I0307 22:13:24.547986 140096103761664 logging_writer.py:48] [160100] global_step=160100, grad_norm=7.172266006469727, loss=1.1377722024917603
2025-03-07 22:13:32.099908: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:14:04.383645 140096095368960 logging_writer.py:48] [160200] global_step=160200, grad_norm=8.330826759338379, loss=1.1221575736999512
I0307 22:14:43.809489 140096103761664 logging_writer.py:48] [160300] global_step=160300, grad_norm=7.662930488586426, loss=1.0329196453094482
I0307 22:15:22.725528 140096095368960 logging_writer.py:48] [160400] global_step=160400, grad_norm=7.938295364379883, loss=1.0992300510406494
I0307 22:16:02.206826 140096103761664 logging_writer.py:48] [160500] global_step=160500, grad_norm=8.152099609375, loss=1.133379340171814
I0307 22:16:37.244642 140252175811776 spec.py:321] Evaluating on the training split.
I0307 22:16:47.763428 140252175811776 spec.py:333] Evaluating on the validation split.
I0307 22:17:08.357146 140252175811776 spec.py:349] Evaluating on the test split.
I0307 22:17:10.183084 140252175811776 submission_runner.py:469] Time since start: 70735.81s, 	Step: 160591, 	{'train/accuracy': 0.8680644035339355, 'train/loss': 0.4690193831920624, 'validation/accuracy': 0.7403599619865417, 'validation/loss': 1.0462883710861206, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.7406188249588013, 'test/num_examples': 10000, 'score': 65844.75536394119, 'total_duration': 70735.81380200386, 'accumulated_submission_time': 65844.75536394119, 'accumulated_eval_time': 4860.79319190979, 'accumulated_logging_time': 11.51626706123352}
I0307 22:17:10.257563 140096095368960 logging_writer.py:48] [160591] accumulated_eval_time=4860.79, accumulated_logging_time=11.5163, accumulated_submission_time=65844.8, global_step=160591, preemption_count=0, score=65844.8, test/accuracy=0.6144, test/loss=1.74062, test/num_examples=10000, total_duration=70735.8, train/accuracy=0.868064, train/loss=0.469019, validation/accuracy=0.74036, validation/loss=1.04629, validation/num_examples=50000
I0307 22:17:14.332286 140096103761664 logging_writer.py:48] [160600] global_step=160600, grad_norm=8.112820625305176, loss=1.0712976455688477
I0307 22:17:53.979372 140096095368960 logging_writer.py:48] [160700] global_step=160700, grad_norm=7.5096540451049805, loss=1.023951530456543
I0307 22:18:33.317465 140096103761664 logging_writer.py:48] [160800] global_step=160800, grad_norm=7.824965953826904, loss=1.0370805263519287
I0307 22:19:13.068865 140096095368960 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.500196933746338, loss=1.0180577039718628
I0307 22:19:52.689833 140096103761664 logging_writer.py:48] [161000] global_step=161000, grad_norm=8.494941711425781, loss=1.0531294345855713
I0307 22:20:32.502179 140096095368960 logging_writer.py:48] [161100] global_step=161100, grad_norm=7.948390483856201, loss=0.9845932722091675
I0307 22:21:12.641966 140096103761664 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.015419006347656, loss=0.9642181396484375
I0307 22:21:52.664896 140096095368960 logging_writer.py:48] [161300] global_step=161300, grad_norm=7.635242462158203, loss=1.0213029384613037
2025-03-07 22:22:21.107344: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:22:33.014894 140096103761664 logging_writer.py:48] [161400] global_step=161400, grad_norm=7.301325798034668, loss=1.0100635290145874
I0307 22:23:12.662202 140096095368960 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.545986175537109, loss=0.9377399682998657
I0307 22:23:52.230484 140096103761664 logging_writer.py:48] [161600] global_step=161600, grad_norm=7.474512100219727, loss=1.0001552104949951
I0307 22:24:32.265084 140096095368960 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.60467529296875, loss=1.0262070894241333
I0307 22:25:11.944761 140096103761664 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.113785743713379, loss=1.047695517539978
I0307 22:25:40.353360 140096095368960 logging_writer.py:48] [161872] global_step=161872, preemption_count=0, score=66354.6
I0307 22:25:42.320653 140252175811776 submission_runner.py:646] Tuning trial 3/5
I0307 22:25:42.320859 140252175811776 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 22:25:42.325599 140252175811776 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0015744578558951616, 'train/loss': 6.911673069000244, 'validation/accuracy': 0.0011399999493733048, 'validation/loss': 6.9118852615356445, 'validation/num_examples': 50000, 'test/accuracy': 0.0010999999940395355, 'test/loss': 6.912051677703857, 'test/num_examples': 10000, 'score': 59.53156042098999, 'total_duration': 147.86599707603455, 'accumulated_submission_time': 59.53156042098999, 'accumulated_eval_time': 88.33413577079773, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1365, {'train/accuracy': 0.06050701439380646, 'train/loss': 5.488076210021973, 'validation/accuracy': 0.05341999977827072, 'validation/loss': 5.582878589630127, 'validation/num_examples': 50000, 'test/accuracy': 0.03830000013113022, 'test/loss': 5.781375885009766, 'test/num_examples': 10000, 'score': 569.453928232193, 'total_duration': 728.9356865882874, 'accumulated_submission_time': 569.453928232193, 'accumulated_eval_time': 159.19375038146973, 'accumulated_logging_time': 0.1060185432434082, 'global_step': 1365, 'preemption_count': 0}), (2710, {'train/accuracy': 0.15455596148967743, 'train/loss': 4.43271541595459, 'validation/accuracy': 0.13439999520778656, 'validation/loss': 4.582358360290527, 'validation/num_examples': 50000, 'test/accuracy': 0.09490000456571579, 'test/loss': 5.032994270324707, 'test/num_examples': 10000, 'score': 1079.4112739562988, 'total_duration': 1270.9730455875397, 'accumulated_submission_time': 1079.4112739562988, 'accumulated_eval_time': 191.06157279014587, 'accumulated_logging_time': 0.14062929153442383, 'global_step': 2710, 'preemption_count': 0}), (4039, {'train/accuracy': 0.24754862487316132, 'train/loss': 3.6730093955993652, 'validation/accuracy': 0.21332000195980072, 'validation/loss': 3.9054133892059326, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 4.465139865875244, 'test/num_examples': 10000, 'score': 1589.411122560501, 'total_duration': 1815.5297737121582, 'accumulated_submission_time': 1589.411122560501, 'accumulated_eval_time': 225.42464971542358, 'accumulated_logging_time': 0.18996691703796387, 'global_step': 4039, 'preemption_count': 0}), (5363, {'train/accuracy': 0.3169642686843872, 'train/loss': 3.187441110610962, 'validation/accuracy': 0.2798999845981598, 'validation/loss': 3.434617757797241, 'validation/num_examples': 50000, 'test/accuracy': 0.20250001549720764, 'test/loss': 4.08212947845459, 'test/num_examples': 10000, 'score': 2099.3799159526825, 'total_duration': 2357.759490966797, 'accumulated_submission_time': 2099.3799159526825, 'accumulated_eval_time': 257.50771045684814, 'accumulated_logging_time': 0.2220747470855713, 'global_step': 5363, 'preemption_count': 0}), (6685, {'train/accuracy': 0.3978993892669678, 'train/loss': 2.6877329349517822, 'validation/accuracy': 0.35613998770713806, 'validation/loss': 2.9459691047668457, 'validation/num_examples': 50000, 'test/accuracy': 0.27410000562667847, 'test/loss': 3.620755672454834, 'test/num_examples': 10000, 'score': 2609.481994152069, 'total_duration': 2903.6866738796234, 'accumulated_submission_time': 2609.481994152069, 'accumulated_eval_time': 293.1042561531067, 'accumulated_logging_time': 0.32263851165771484, 'global_step': 6685, 'preemption_count': 0}), (8011, {'train/accuracy': 0.4621332883834839, 'train/loss': 2.350759744644165, 'validation/accuracy': 0.41543999314308167, 'validation/loss': 2.6091153621673584, 'validation/num_examples': 50000, 'test/accuracy': 0.3175000250339508, 'test/loss': 3.291898250579834, 'test/num_examples': 10000, 'score': 3119.4521787166595, 'total_duration': 3447.64355969429, 'accumulated_submission_time': 3119.4521787166595, 'accumulated_eval_time': 326.84507513046265, 'accumulated_logging_time': 0.44254541397094727, 'global_step': 8011, 'preemption_count': 0}), (9329, {'train/accuracy': 0.5075733065605164, 'train/loss': 2.1023128032684326, 'validation/accuracy': 0.45879998803138733, 'validation/loss': 2.3864471912384033, 'validation/num_examples': 50000, 'test/accuracy': 0.35180002450942993, 'test/loss': 3.1156678199768066, 'test/num_examples': 10000, 'score': 3629.5358498096466, 'total_duration': 3993.9617400169373, 'accumulated_submission_time': 3629.5358498096466, 'accumulated_eval_time': 362.87179923057556, 'accumulated_logging_time': 0.5103325843811035, 'global_step': 9329, 'preemption_count': 0}), (10646, {'train/accuracy': 0.542390763759613, 'train/loss': 1.9380462169647217, 'validation/accuracy': 0.4893999993801117, 'validation/loss': 2.205112934112549, 'validation/num_examples': 50000, 'test/accuracy': 0.3776000142097473, 'test/loss': 2.91927170753479, 'test/num_examples': 10000, 'score': 4139.530784130096, 'total_duration': 4540.633215904236, 'accumulated_submission_time': 4139.530784130096, 'accumulated_eval_time': 399.37016129493713, 'accumulated_logging_time': 0.5549983978271484, 'global_step': 10646, 'preemption_count': 0}), (11963, {'train/accuracy': 0.5576570630073547, 'train/loss': 1.8505250215530396, 'validation/accuracy': 0.5041399598121643, 'validation/loss': 2.143873691558838, 'validation/num_examples': 50000, 'test/accuracy': 0.38770002126693726, 'test/loss': 2.876908540725708, 'test/num_examples': 10000, 'score': 4649.374207019806, 'total_duration': 5093.669030427933, 'accumulated_submission_time': 4649.374207019806, 'accumulated_eval_time': 442.40414237976074, 'accumulated_logging_time': 0.5805182456970215, 'global_step': 11963, 'preemption_count': 0}), (13280, {'train/accuracy': 0.5846221446990967, 'train/loss': 1.7235958576202393, 'validation/accuracy': 0.5286799669265747, 'validation/loss': 2.000023126602173, 'validation/num_examples': 50000, 'test/accuracy': 0.4109000265598297, 'test/loss': 2.7437474727630615, 'test/num_examples': 10000, 'score': 5159.199308156967, 'total_duration': 5648.210936307907, 'accumulated_submission_time': 5159.199308156967, 'accumulated_eval_time': 486.8378872871399, 'accumulated_logging_time': 0.7265846729278564, 'global_step': 13280, 'preemption_count': 0}), (14587, {'train/accuracy': 0.5922752022743225, 'train/loss': 1.687596321105957, 'validation/accuracy': 0.5329799652099609, 'validation/loss': 1.9866801500320435, 'validation/num_examples': 50000, 'test/accuracy': 0.41670000553131104, 'test/loss': 2.706833839416504, 'test/num_examples': 10000, 'score': 5669.280317783356, 'total_duration': 6198.360827922821, 'accumulated_submission_time': 5669.280317783356, 'accumulated_eval_time': 526.6531901359558, 'accumulated_logging_time': 0.8367834091186523, 'global_step': 14587, 'preemption_count': 0}), (15904, {'train/accuracy': 0.615234375, 'train/loss': 1.5854220390319824, 'validation/accuracy': 0.5554800033569336, 'validation/loss': 1.882692575454712, 'validation/num_examples': 50000, 'test/accuracy': 0.4368000328540802, 'test/loss': 2.622231960296631, 'test/num_examples': 10000, 'score': 6179.263180017471, 'total_duration': 6752.846360683441, 'accumulated_submission_time': 6179.263180017471, 'accumulated_eval_time': 570.9250020980835, 'accumulated_logging_time': 0.9297308921813965, 'global_step': 15904, 'preemption_count': 0}), (17218, {'train/accuracy': 0.6101123690605164, 'train/loss': 1.593916893005371, 'validation/accuracy': 0.5503199696540833, 'validation/loss': 1.901633381843567, 'validation/num_examples': 50000, 'test/accuracy': 0.43640002608299255, 'test/loss': 2.6130807399749756, 'test/num_examples': 10000, 'score': 6689.372481584549, 'total_duration': 7301.684935092926, 'accumulated_submission_time': 6689.372481584549, 'accumulated_eval_time': 609.4463336467743, 'accumulated_logging_time': 1.003150224685669, 'global_step': 17218, 'preemption_count': 0}), (18530, {'train/accuracy': 0.6128427982330322, 'train/loss': 1.5683091878890991, 'validation/accuracy': 0.5608199834823608, 'validation/loss': 1.8562084436416626, 'validation/num_examples': 50000, 'test/accuracy': 0.43650001287460327, 'test/loss': 2.5754940509796143, 'test/num_examples': 10000, 'score': 7199.530290365219, 'total_duration': 7850.933743238449, 'accumulated_submission_time': 7199.530290365219, 'accumulated_eval_time': 648.333596944809, 'accumulated_logging_time': 1.0754914283752441, 'global_step': 18530, 'preemption_count': 0}), (19846, {'train/accuracy': 0.6099330186843872, 'train/loss': 1.5969574451446533, 'validation/accuracy': 0.557420015335083, 'validation/loss': 1.875210165977478, 'validation/num_examples': 50000, 'test/accuracy': 0.43880000710487366, 'test/loss': 2.591240644454956, 'test/num_examples': 10000, 'score': 7709.453641653061, 'total_duration': 8401.706051588058, 'accumulated_submission_time': 7709.453641653061, 'accumulated_eval_time': 689.0185160636902, 'accumulated_logging_time': 1.1042068004608154, 'global_step': 19846, 'preemption_count': 0}), (21160, {'train/accuracy': 0.6230069994926453, 'train/loss': 1.5220811367034912, 'validation/accuracy': 0.5679999589920044, 'validation/loss': 1.8059231042861938, 'validation/num_examples': 50000, 'test/accuracy': 0.444100022315979, 'test/loss': 2.556509017944336, 'test/num_examples': 10000, 'score': 8219.55773806572, 'total_duration': 8951.331392765045, 'accumulated_submission_time': 8219.55773806572, 'accumulated_eval_time': 728.3746435642242, 'accumulated_logging_time': 1.1385698318481445, 'global_step': 21160, 'preemption_count': 0}), (22455, {'train/accuracy': 0.6252391338348389, 'train/loss': 1.5378427505493164, 'validation/accuracy': 0.5707799792289734, 'validation/loss': 1.81388258934021, 'validation/num_examples': 50000, 'test/accuracy': 0.4540000259876251, 'test/loss': 2.5297319889068604, 'test/num_examples': 10000, 'score': 8729.5656042099, 'total_duration': 9503.386975765228, 'accumulated_submission_time': 8729.5656042099, 'accumulated_eval_time': 770.1629872322083, 'accumulated_logging_time': 1.2649180889129639, 'global_step': 22455, 'preemption_count': 0}), (23770, {'train/accuracy': 0.6300023794174194, 'train/loss': 1.4954612255096436, 'validation/accuracy': 0.5751799941062927, 'validation/loss': 1.779248833656311, 'validation/num_examples': 50000, 'test/accuracy': 0.458700031042099, 'test/loss': 2.473832845687866, 'test/num_examples': 10000, 'score': 9239.627389669418, 'total_duration': 10059.013589382172, 'accumulated_submission_time': 9239.627389669418, 'accumulated_eval_time': 815.5128004550934, 'accumulated_logging_time': 1.3518857955932617, 'global_step': 23770, 'preemption_count': 0}), (25079, {'train/accuracy': 0.6388113498687744, 'train/loss': 1.459547996520996, 'validation/accuracy': 0.5857999920845032, 'validation/loss': 1.7483519315719604, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.450523853302002, 'test/num_examples': 10000, 'score': 9749.690160751343, 'total_duration': 10608.736176490784, 'accumulated_submission_time': 9749.690160751343, 'accumulated_eval_time': 854.977322101593, 'accumulated_logging_time': 1.409886121749878, 'global_step': 25079, 'preemption_count': 0}), (26392, {'train/accuracy': 0.6297831535339355, 'train/loss': 1.4772435426712036, 'validation/accuracy': 0.5799599885940552, 'validation/loss': 1.7559103965759277, 'validation/num_examples': 50000, 'test/accuracy': 0.4563000202178955, 'test/loss': 2.4692189693450928, 'test/num_examples': 10000, 'score': 10259.714161396027, 'total_duration': 11163.216361999512, 'accumulated_submission_time': 10259.714161396027, 'accumulated_eval_time': 899.1866092681885, 'accumulated_logging_time': 1.522904872894287, 'global_step': 26392, 'preemption_count': 0}), (27707, {'train/accuracy': 0.6364795565605164, 'train/loss': 1.47107994556427, 'validation/accuracy': 0.5870000123977661, 'validation/loss': 1.7378978729248047, 'validation/num_examples': 50000, 'test/accuracy': 0.4585000276565552, 'test/loss': 2.4690916538238525, 'test/num_examples': 10000, 'score': 10769.301445007324, 'total_duration': 11716.3062915802, 'accumulated_submission_time': 10769.301445007324, 'accumulated_eval_time': 942.0799193382263, 'accumulated_logging_time': 1.9949395656585693, 'global_step': 27707, 'preemption_count': 0}), (29022, {'train/accuracy': 0.6213727593421936, 'train/loss': 1.5282877683639526, 'validation/accuracy': 0.5713399648666382, 'validation/loss': 1.8057664632797241, 'validation/num_examples': 50000, 'test/accuracy': 0.45100003480911255, 'test/loss': 2.5242435932159424, 'test/num_examples': 10000, 'score': 11279.171776771545, 'total_duration': 12268.644584417343, 'accumulated_submission_time': 11279.171776771545, 'accumulated_eval_time': 984.2866439819336, 'accumulated_logging_time': 2.121225357055664, 'global_step': 29022, 'preemption_count': 0}), (30340, {'train/accuracy': 0.6427375674247742, 'train/loss': 1.4392234086990356, 'validation/accuracy': 0.5919399857521057, 'validation/loss': 1.7061538696289062, 'validation/num_examples': 50000, 'test/accuracy': 0.4628000259399414, 'test/loss': 2.4611458778381348, 'test/num_examples': 10000, 'score': 11789.3055331707, 'total_duration': 12821.458944797516, 'accumulated_submission_time': 11789.3055331707, 'accumulated_eval_time': 1026.710994720459, 'accumulated_logging_time': 2.2361581325531006, 'global_step': 30340, 'preemption_count': 0}), (31660, {'train/accuracy': 0.6451091766357422, 'train/loss': 1.4213815927505493, 'validation/accuracy': 0.5963599681854248, 'validation/loss': 1.692355751991272, 'validation/num_examples': 50000, 'test/accuracy': 0.4635000228881836, 'test/loss': 2.4634056091308594, 'test/num_examples': 10000, 'score': 12299.138736248016, 'total_duration': 13371.174358844757, 'accumulated_submission_time': 12299.138736248016, 'accumulated_eval_time': 1066.2925863265991, 'accumulated_logging_time': 2.3966000080108643, 'global_step': 31660, 'preemption_count': 0}), (32974, {'train/accuracy': 0.6282286047935486, 'train/loss': 1.5148309469223022, 'validation/accuracy': 0.5799799561500549, 'validation/loss': 1.7580074071884155, 'validation/num_examples': 50000, 'test/accuracy': 0.45670002698898315, 'test/loss': 2.4863736629486084, 'test/num_examples': 10000, 'score': 12809.105134248734, 'total_duration': 13921.894159793854, 'accumulated_submission_time': 12809.105134248734, 'accumulated_eval_time': 1106.778297662735, 'accumulated_logging_time': 2.527034282684326, 'global_step': 32974, 'preemption_count': 0}), (34288, {'train/accuracy': 0.6402662396430969, 'train/loss': 1.4535552263259888, 'validation/accuracy': 0.590179979801178, 'validation/loss': 1.7174993753433228, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.4660301208496094, 'test/num_examples': 10000, 'score': 13319.156252861023, 'total_duration': 14473.136858940125, 'accumulated_submission_time': 13319.156252861023, 'accumulated_eval_time': 1147.7123908996582, 'accumulated_logging_time': 2.6464807987213135, 'global_step': 34288, 'preemption_count': 0}), (35603, {'train/accuracy': 0.6328921914100647, 'train/loss': 1.498120903968811, 'validation/accuracy': 0.5820800065994263, 'validation/loss': 1.7467761039733887, 'validation/num_examples': 50000, 'test/accuracy': 0.4579000174999237, 'test/loss': 2.497425079345703, 'test/num_examples': 10000, 'score': 13829.203861951828, 'total_duration': 15026.96167421341, 'accumulated_submission_time': 13829.203861951828, 'accumulated_eval_time': 1191.2412407398224, 'accumulated_logging_time': 2.7574026584625244, 'global_step': 35603, 'preemption_count': 0}), (36915, {'train/accuracy': 0.6465640664100647, 'train/loss': 1.417842149734497, 'validation/accuracy': 0.594219982624054, 'validation/loss': 1.6937551498413086, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.435868501663208, 'test/num_examples': 10000, 'score': 14339.268918037415, 'total_duration': 15574.23060297966, 'accumulated_submission_time': 14339.268918037415, 'accumulated_eval_time': 1228.191910982132, 'accumulated_logging_time': 2.87359619140625, 'global_step': 36915, 'preemption_count': 0}), (38230, {'train/accuracy': 0.6446309089660645, 'train/loss': 1.4139775037765503, 'validation/accuracy': 0.5981400012969971, 'validation/loss': 1.6679935455322266, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.380847692489624, 'test/num_examples': 10000, 'score': 14849.096669197083, 'total_duration': 16127.696937084198, 'accumulated_submission_time': 14849.096669197083, 'accumulated_eval_time': 1271.5743641853333, 'accumulated_logging_time': 2.988579034805298, 'global_step': 38230, 'preemption_count': 0}), (39545, {'train/accuracy': 0.6444315910339355, 'train/loss': 1.4265717267990112, 'validation/accuracy': 0.5979399681091309, 'validation/loss': 1.679437279701233, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.3826851844787598, 'test/num_examples': 10000, 'score': 15359.023358106613, 'total_duration': 16678.004452466965, 'accumulated_submission_time': 15359.023358106613, 'accumulated_eval_time': 1311.6992518901825, 'accumulated_logging_time': 3.10727858543396, 'global_step': 39545, 'preemption_count': 0}), (40857, {'train/accuracy': 0.6428372263908386, 'train/loss': 1.4379843473434448, 'validation/accuracy': 0.5939800143241882, 'validation/loss': 1.694046974182129, 'validation/num_examples': 50000, 'test/accuracy': 0.46790000796318054, 'test/loss': 2.4174466133117676, 'test/num_examples': 10000, 'score': 15868.74512887001, 'total_duration': 17225.582670211792, 'accumulated_submission_time': 15868.74512887001, 'accumulated_eval_time': 1349.246863603592, 'accumulated_logging_time': 3.2763900756835938, 'global_step': 40857, 'preemption_count': 0}), (42169, {'train/accuracy': 0.6518255472183228, 'train/loss': 1.3834835290908813, 'validation/accuracy': 0.5985400080680847, 'validation/loss': 1.6672126054763794, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.3768794536590576, 'test/num_examples': 10000, 'score': 16378.515176057816, 'total_duration': 17773.447425365448, 'accumulated_submission_time': 16378.515176057816, 'accumulated_eval_time': 1387.080607175827, 'accumulated_logging_time': 3.399587631225586, 'global_step': 42169, 'preemption_count': 0}), (43480, {'train/accuracy': 0.6549346446990967, 'train/loss': 1.377758502960205, 'validation/accuracy': 0.602899968624115, 'validation/loss': 1.6348503828048706, 'validation/num_examples': 50000, 'test/accuracy': 0.48270002007484436, 'test/loss': 2.3226985931396484, 'test/num_examples': 10000, 'score': 16888.492291212082, 'total_duration': 18325.250215291977, 'accumulated_submission_time': 16888.492291212082, 'accumulated_eval_time': 1428.686133146286, 'accumulated_logging_time': 3.4777886867523193, 'global_step': 43480, 'preemption_count': 0}), (44794, {'train/accuracy': 0.6507692933082581, 'train/loss': 1.39634370803833, 'validation/accuracy': 0.6025999784469604, 'validation/loss': 1.6475313901901245, 'validation/num_examples': 50000, 'test/accuracy': 0.4798000156879425, 'test/loss': 2.3794052600860596, 'test/num_examples': 10000, 'score': 17398.270622491837, 'total_duration': 18876.564233779907, 'accumulated_submission_time': 17398.270622491837, 'accumulated_eval_time': 1469.9802823066711, 'accumulated_logging_time': 3.58227276802063, 'global_step': 44794, 'preemption_count': 0}), (46112, {'train/accuracy': 0.6620097160339355, 'train/loss': 1.3622193336486816, 'validation/accuracy': 0.6071000099182129, 'validation/loss': 1.631041407585144, 'validation/num_examples': 50000, 'test/accuracy': 0.4862000346183777, 'test/loss': 2.352956771850586, 'test/num_examples': 10000, 'score': 17908.219122886658, 'total_duration': 19426.56300163269, 'accumulated_submission_time': 17908.219122886658, 'accumulated_eval_time': 1509.7851781845093, 'accumulated_logging_time': 3.686852216720581, 'global_step': 46112, 'preemption_count': 0}), (47425, {'train/accuracy': 0.6451291441917419, 'train/loss': 1.4150234460830688, 'validation/accuracy': 0.6019799709320068, 'validation/loss': 1.6587527990341187, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.3831734657287598, 'test/num_examples': 10000, 'score': 18418.20294713974, 'total_duration': 19974.259385347366, 'accumulated_submission_time': 18418.20294713974, 'accumulated_eval_time': 1547.2716102600098, 'accumulated_logging_time': 3.7745580673217773, 'global_step': 47425, 'preemption_count': 0}), (48735, {'train/accuracy': 0.6591398119926453, 'train/loss': 1.3654074668884277, 'validation/accuracy': 0.6089400053024292, 'validation/loss': 1.624113917350769, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.348018169403076, 'test/num_examples': 10000, 'score': 18928.298800230026, 'total_duration': 20524.401693820953, 'accumulated_submission_time': 18928.298800230026, 'accumulated_eval_time': 1587.0687594413757, 'accumulated_logging_time': 3.8860855102539062, 'global_step': 48735, 'preemption_count': 0}), (50039, {'train/accuracy': 0.6532804369926453, 'train/loss': 1.3911547660827637, 'validation/accuracy': 0.5999400019645691, 'validation/loss': 1.6686179637908936, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.369701862335205, 'test/num_examples': 10000, 'score': 19438.077170610428, 'total_duration': 21073.903835058212, 'accumulated_submission_time': 19438.077170610428, 'accumulated_eval_time': 1626.554432630539, 'accumulated_logging_time': 3.985607624053955, 'global_step': 50039, 'preemption_count': 0}), (51336, {'train/accuracy': 0.6596181392669678, 'train/loss': 1.3574062585830688, 'validation/accuracy': 0.6097800135612488, 'validation/loss': 1.6208540201187134, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.351058006286621, 'test/num_examples': 10000, 'score': 19948.14453125, 'total_duration': 21624.534368276596, 'accumulated_submission_time': 19948.14453125, 'accumulated_eval_time': 1666.7992641925812, 'accumulated_logging_time': 4.159074544906616, 'global_step': 51336, 'preemption_count': 0}), (52446, {'train/accuracy': 0.6522839665412903, 'train/loss': 1.4034955501556396, 'validation/accuracy': 0.601099967956543, 'validation/loss': 1.6656488180160522, 'validation/num_examples': 50000, 'test/accuracy': 0.4815000295639038, 'test/loss': 2.3792152404785156, 'test/num_examples': 10000, 'score': 20457.959265708923, 'total_duration': 22172.46365404129, 'accumulated_submission_time': 20457.959265708923, 'accumulated_eval_time': 1704.6536703109741, 'accumulated_logging_time': 4.288142681121826, 'global_step': 52446, 'preemption_count': 0}), (53634, {'train/accuracy': 0.6658561825752258, 'train/loss': 1.3239845037460327, 'validation/accuracy': 0.6091200113296509, 'validation/loss': 1.6169421672821045, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.338318109512329, 'test/num_examples': 10000, 'score': 20967.94323539734, 'total_duration': 22724.63018989563, 'accumulated_submission_time': 20967.94323539734, 'accumulated_eval_time': 1746.5918550491333, 'accumulated_logging_time': 4.393389463424683, 'global_step': 53634, 'preemption_count': 0}), (54723, {'train/accuracy': 0.6975247263908386, 'train/loss': 1.187575101852417, 'validation/accuracy': 0.6101799607276917, 'validation/loss': 1.6016238927841187, 'validation/num_examples': 50000, 'test/accuracy': 0.49540001153945923, 'test/loss': 2.310293197631836, 'test/num_examples': 10000, 'score': 21477.729937314987, 'total_duration': 23277.549021959305, 'accumulated_submission_time': 21477.729937314987, 'accumulated_eval_time': 1789.4784786701202, 'accumulated_logging_time': 4.512249708175659, 'global_step': 54723, 'preemption_count': 0}), (55701, {'train/accuracy': 0.6644411683082581, 'train/loss': 1.3461406230926514, 'validation/accuracy': 0.6157199740409851, 'validation/loss': 1.5817506313323975, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.2826731204986572, 'test/num_examples': 10000, 'score': 21987.48577094078, 'total_duration': 23823.82820034027, 'accumulated_submission_time': 21987.48577094078, 'accumulated_eval_time': 1825.74658203125, 'accumulated_logging_time': 4.65355110168457, 'global_step': 55701, 'preemption_count': 0}), (56624, {'train/accuracy': 0.6606943607330322, 'train/loss': 1.3473140001296997, 'validation/accuracy': 0.6124599575996399, 'validation/loss': 1.6119436025619507, 'validation/num_examples': 50000, 'test/accuracy': 0.48670002818107605, 'test/loss': 2.314908742904663, 'test/num_examples': 10000, 'score': 22497.274667978287, 'total_duration': 24377.118958473206, 'accumulated_submission_time': 22497.274667978287, 'accumulated_eval_time': 1869.0116336345673, 'accumulated_logging_time': 4.784945964813232, 'global_step': 56624, 'preemption_count': 0}), (57668, {'train/accuracy': 0.6625877022743225, 'train/loss': 1.3463044166564941, 'validation/accuracy': 0.6072399616241455, 'validation/loss': 1.6348316669464111, 'validation/num_examples': 50000, 'test/accuracy': 0.4823000133037567, 'test/loss': 2.378462791442871, 'test/num_examples': 10000, 'score': 23007.19713640213, 'total_duration': 24923.167719125748, 'accumulated_submission_time': 23007.19713640213, 'accumulated_eval_time': 1904.841246843338, 'accumulated_logging_time': 4.962131977081299, 'global_step': 57668, 'preemption_count': 0}), (58483, {'train/accuracy': 0.6560307741165161, 'train/loss': 1.3707544803619385, 'validation/accuracy': 0.6067399978637695, 'validation/loss': 1.6136571168899536, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.355103015899658, 'test/num_examples': 10000, 'score': 23517.374615430832, 'total_duration': 25472.04661512375, 'accumulated_submission_time': 23517.374615430832, 'accumulated_eval_time': 1943.3311386108398, 'accumulated_logging_time': 5.079399347305298, 'global_step': 58483, 'preemption_count': 0}), (59430, {'train/accuracy': 0.6508290767669678, 'train/loss': 1.3903214931488037, 'validation/accuracy': 0.5988999605178833, 'validation/loss': 1.667960524559021, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.379729747772217, 'test/num_examples': 10000, 'score': 24027.426962852478, 'total_duration': 26018.993552446365, 'accumulated_submission_time': 24027.426962852478, 'accumulated_eval_time': 1979.9494287967682, 'accumulated_logging_time': 5.245790958404541, 'global_step': 59430, 'preemption_count': 0}), (60354, {'train/accuracy': 0.671875, 'train/loss': 1.3070734739303589, 'validation/accuracy': 0.616320013999939, 'validation/loss': 1.6041799783706665, 'validation/num_examples': 50000, 'test/accuracy': 0.4847000241279602, 'test/loss': 2.330476999282837, 'test/num_examples': 10000, 'score': 24537.296875, 'total_duration': 26564.612133026123, 'accumulated_submission_time': 24537.296875, 'accumulated_eval_time': 2015.4531314373016, 'accumulated_logging_time': 5.384232759475708, 'global_step': 60354, 'preemption_count': 0}), (61143, {'train/accuracy': 0.6684868931770325, 'train/loss': 1.3220707178115845, 'validation/accuracy': 0.619219958782196, 'validation/loss': 1.5733745098114014, 'validation/num_examples': 50000, 'test/accuracy': 0.4913000166416168, 'test/loss': 2.2740910053253174, 'test/num_examples': 10000, 'score': 25047.395356416702, 'total_duration': 27110.66473674774, 'accumulated_submission_time': 25047.395356416702, 'accumulated_eval_time': 2051.238523244858, 'accumulated_logging_time': 5.462580680847168, 'global_step': 61143, 'preemption_count': 0}), (61885, {'train/accuracy': 0.6733099222183228, 'train/loss': 1.2973337173461914, 'validation/accuracy': 0.6195399761199951, 'validation/loss': 1.5772764682769775, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.285935401916504, 'test/num_examples': 10000, 'score': 25557.219066143036, 'total_duration': 27654.05300974846, 'accumulated_submission_time': 25557.219066143036, 'accumulated_eval_time': 2084.6259570121765, 'accumulated_logging_time': 5.5539679527282715, 'global_step': 61885, 'preemption_count': 0}), (62524, {'train/accuracy': 0.6745057106018066, 'train/loss': 1.2949031591415405, 'validation/accuracy': 0.6291199922561646, 'validation/loss': 1.5317565202713013, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.226806402206421, 'test/num_examples': 10000, 'score': 26067.06433200836, 'total_duration': 28199.04498410225, 'accumulated_submission_time': 26067.06433200836, 'accumulated_eval_time': 2119.5784056186676, 'accumulated_logging_time': 5.673795700073242, 'global_step': 62524, 'preemption_count': 0}), (63226, {'train/accuracy': 0.673270046710968, 'train/loss': 1.2910690307617188, 'validation/accuracy': 0.6248999834060669, 'validation/loss': 1.5530205965042114, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.2459170818328857, 'test/num_examples': 10000, 'score': 26576.9688808918, 'total_duration': 28746.66203737259, 'accumulated_submission_time': 26576.9688808918, 'accumulated_eval_time': 2157.1624808311462, 'accumulated_logging_time': 5.7231504917144775, 'global_step': 63226, 'preemption_count': 0}), (63855, {'train/accuracy': 0.6629862785339355, 'train/loss': 1.3470371961593628, 'validation/accuracy': 0.6160199642181396, 'validation/loss': 1.582107663154602, 'validation/num_examples': 50000, 'test/accuracy': 0.4894000291824341, 'test/loss': 2.2999372482299805, 'test/num_examples': 10000, 'score': 27087.323586702347, 'total_duration': 29292.085461616516, 'accumulated_submission_time': 27087.323586702347, 'accumulated_eval_time': 2192.065351486206, 'accumulated_logging_time': 5.816307067871094, 'global_step': 63855, 'preemption_count': 0}), (64497, {'train/accuracy': 0.6769172549247742, 'train/loss': 1.2932140827178955, 'validation/accuracy': 0.6228199601173401, 'validation/loss': 1.55232834815979, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.275650978088379, 'test/num_examples': 10000, 'score': 27597.25462293625, 'total_duration': 29835.26527905464, 'accumulated_submission_time': 27597.25462293625, 'accumulated_eval_time': 2225.1841027736664, 'accumulated_logging_time': 5.873381614685059, 'global_step': 64497, 'preemption_count': 0}), (65020, {'train/accuracy': 0.6654974222183228, 'train/loss': 1.3372374773025513, 'validation/accuracy': 0.6230199933052063, 'validation/loss': 1.56284761428833, 'validation/num_examples': 50000, 'test/accuracy': 0.49310001730918884, 'test/loss': 2.2624058723449707, 'test/num_examples': 10000, 'score': 28107.88705444336, 'total_duration': 30378.526815652847, 'accumulated_submission_time': 28107.88705444336, 'accumulated_eval_time': 2257.674558162689, 'accumulated_logging_time': 5.951249361038208, 'global_step': 65020, 'preemption_count': 0}), (66315, {'train/accuracy': 0.6778539419174194, 'train/loss': 1.2795910835266113, 'validation/accuracy': 0.6269599795341492, 'validation/loss': 1.5225732326507568, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.245404005050659, 'test/num_examples': 10000, 'score': 28618.030217170715, 'total_duration': 30930.498950004578, 'accumulated_submission_time': 28618.030217170715, 'accumulated_eval_time': 2299.2713062763214, 'accumulated_logging_time': 6.039647579193115, 'global_step': 66315, 'preemption_count': 0}), (67613, {'train/accuracy': 0.6728315949440002, 'train/loss': 1.3096870183944702, 'validation/accuracy': 0.6263799667358398, 'validation/loss': 1.5540915727615356, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.2706117630004883, 'test/num_examples': 10000, 'score': 29127.874561071396, 'total_duration': 31477.905072689056, 'accumulated_submission_time': 29127.874561071396, 'accumulated_eval_time': 2336.641523361206, 'accumulated_logging_time': 6.086534261703491, 'global_step': 67613, 'preemption_count': 0}), (68894, {'train/accuracy': 0.6631656289100647, 'train/loss': 1.3368297815322876, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.582789659500122, 'validation/num_examples': 50000, 'test/accuracy': 0.49380001425743103, 'test/loss': 2.2988550662994385, 'test/num_examples': 10000, 'score': 29637.865998983383, 'total_duration': 32026.884588956833, 'accumulated_submission_time': 29637.865998983383, 'accumulated_eval_time': 2375.3820683956146, 'accumulated_logging_time': 6.1842992305755615, 'global_step': 68894, 'preemption_count': 0}), (70160, {'train/accuracy': 0.6653778553009033, 'train/loss': 1.3295236825942993, 'validation/accuracy': 0.6199600100517273, 'validation/loss': 1.5702780485153198, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.2733988761901855, 'test/num_examples': 10000, 'score': 30147.74630355835, 'total_duration': 32576.075915575027, 'accumulated_submission_time': 30147.74630355835, 'accumulated_eval_time': 2414.497104883194, 'accumulated_logging_time': 6.2238428592681885, 'global_step': 70160, 'preemption_count': 0}), (71434, {'train/accuracy': 0.6695232391357422, 'train/loss': 1.3318610191345215, 'validation/accuracy': 0.6173799633979797, 'validation/loss': 1.589481234550476, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.2894232273101807, 'test/num_examples': 10000, 'score': 30657.65567755699, 'total_duration': 33124.302161216736, 'accumulated_submission_time': 30657.65567755699, 'accumulated_eval_time': 2452.621523141861, 'accumulated_logging_time': 6.25874400138855, 'global_step': 71434, 'preemption_count': 0}), (72712, {'train/accuracy': 0.6760203838348389, 'train/loss': 1.2870255708694458, 'validation/accuracy': 0.6233999729156494, 'validation/loss': 1.5428450107574463, 'validation/num_examples': 50000, 'test/accuracy': 0.5037000179290771, 'test/loss': 2.2536087036132812, 'test/num_examples': 10000, 'score': 31167.679332971573, 'total_duration': 33683.47640490532, 'accumulated_submission_time': 31167.679332971573, 'accumulated_eval_time': 2501.578024148941, 'accumulated_logging_time': 6.294565439224243, 'global_step': 72712, 'preemption_count': 0}), (74002, {'train/accuracy': 0.6828961968421936, 'train/loss': 1.2402745485305786, 'validation/accuracy': 0.6353999972343445, 'validation/loss': 1.4952324628829956, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2025585174560547, 'test/num_examples': 10000, 'score': 31677.792944431305, 'total_duration': 34229.863186597824, 'accumulated_submission_time': 31677.792944431305, 'accumulated_eval_time': 2537.645740032196, 'accumulated_logging_time': 6.3397369384765625, 'global_step': 74002, 'preemption_count': 0}), (75298, {'train/accuracy': 0.6762993931770325, 'train/loss': 1.2758315801620483, 'validation/accuracy': 0.6263599991798401, 'validation/loss': 1.5363458395004272, 'validation/num_examples': 50000, 'test/accuracy': 0.5016000270843506, 'test/loss': 2.235443592071533, 'test/num_examples': 10000, 'score': 32187.651284456253, 'total_duration': 34775.82873630524, 'accumulated_submission_time': 32187.651284456253, 'accumulated_eval_time': 2573.5069131851196, 'accumulated_logging_time': 6.426562786102295, 'global_step': 75298, 'preemption_count': 0}), (76603, {'train/accuracy': 0.6835339665412903, 'train/loss': 1.251558542251587, 'validation/accuracy': 0.631339967250824, 'validation/loss': 1.5285115242004395, 'validation/num_examples': 50000, 'test/accuracy': 0.49960002303123474, 'test/loss': 2.238071918487549, 'test/num_examples': 10000, 'score': 32697.536794900894, 'total_duration': 35326.78960728645, 'accumulated_submission_time': 32697.536794900894, 'accumulated_eval_time': 2614.3422091007233, 'accumulated_logging_time': 6.507396697998047, 'global_step': 76603, 'preemption_count': 0}), (77901, {'train/accuracy': 0.6845304369926453, 'train/loss': 1.2409714460372925, 'validation/accuracy': 0.6314799785614014, 'validation/loss': 1.511762261390686, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.210052251815796, 'test/num_examples': 10000, 'score': 33207.410826444626, 'total_duration': 35871.70054578781, 'accumulated_submission_time': 33207.410826444626, 'accumulated_eval_time': 2649.194940328598, 'accumulated_logging_time': 6.533376693725586, 'global_step': 77901, 'preemption_count': 0}), (79215, {'train/accuracy': 0.6728914380073547, 'train/loss': 1.2973780632019043, 'validation/accuracy': 0.62882000207901, 'validation/loss': 1.5235060453414917, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.242490291595459, 'test/num_examples': 10000, 'score': 33717.53262710571, 'total_duration': 36415.88436079025, 'accumulated_submission_time': 33717.53262710571, 'accumulated_eval_time': 2683.040412902832, 'accumulated_logging_time': 6.592703819274902, 'global_step': 79215, 'preemption_count': 0}), (80521, {'train/accuracy': 0.6913862824440002, 'train/loss': 1.2131707668304443, 'validation/accuracy': 0.6385599970817566, 'validation/loss': 1.487841248512268, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.1738009452819824, 'test/num_examples': 10000, 'score': 34227.40818238258, 'total_duration': 36965.783370018005, 'accumulated_submission_time': 34227.40818238258, 'accumulated_eval_time': 2722.8719069957733, 'accumulated_logging_time': 6.62582802772522, 'global_step': 80521, 'preemption_count': 0}), (81827, {'train/accuracy': 0.6926419138908386, 'train/loss': 1.2114121913909912, 'validation/accuracy': 0.6388799548149109, 'validation/loss': 1.4778668880462646, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.1856496334075928, 'test/num_examples': 10000, 'score': 34737.17697548866, 'total_duration': 37511.616874456406, 'accumulated_submission_time': 34737.17697548866, 'accumulated_eval_time': 2758.6920306682587, 'accumulated_logging_time': 6.711660623550415, 'global_step': 81827, 'preemption_count': 0}), (83112, {'train/accuracy': 0.6907086968421936, 'train/loss': 1.2079216241836548, 'validation/accuracy': 0.6414200067520142, 'validation/loss': 1.4783744812011719, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.1825082302093506, 'test/num_examples': 10000, 'score': 35247.240793943405, 'total_duration': 38058.37618684769, 'accumulated_submission_time': 35247.240793943405, 'accumulated_eval_time': 2795.1775946617126, 'accumulated_logging_time': 6.763816833496094, 'global_step': 83112, 'preemption_count': 0}), (84400, {'train/accuracy': 0.6739277839660645, 'train/loss': 1.2957476377487183, 'validation/accuracy': 0.6217600107192993, 'validation/loss': 1.5643218755722046, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.308706283569336, 'test/num_examples': 10000, 'score': 35757.09645867348, 'total_duration': 38600.87066411972, 'accumulated_submission_time': 35757.09645867348, 'accumulated_eval_time': 2827.5658082962036, 'accumulated_logging_time': 6.854340553283691, 'global_step': 84400, 'preemption_count': 0}), (85692, {'train/accuracy': 0.6964684128761292, 'train/loss': 1.1936861276626587, 'validation/accuracy': 0.6434599757194519, 'validation/loss': 1.4651683568954468, 'validation/num_examples': 50000, 'test/accuracy': 0.5177000164985657, 'test/loss': 2.168731927871704, 'test/num_examples': 10000, 'score': 36267.00308179855, 'total_duration': 39144.26618885994, 'accumulated_submission_time': 36267.00308179855, 'accumulated_eval_time': 2860.820461034775, 'accumulated_logging_time': 6.92926025390625, 'global_step': 85692, 'preemption_count': 0}), (87011, {'train/accuracy': 0.6976243257522583, 'train/loss': 1.1826188564300537, 'validation/accuracy': 0.6449800133705139, 'validation/loss': 1.447867751121521, 'validation/num_examples': 50000, 'test/accuracy': 0.523300051689148, 'test/loss': 2.1379809379577637, 'test/num_examples': 10000, 'score': 36777.15213179588, 'total_duration': 39690.53355884552, 'accumulated_submission_time': 36777.15213179588, 'accumulated_eval_time': 2896.725681781769, 'accumulated_logging_time': 6.977392196655273, 'global_step': 87011, 'preemption_count': 0}), (88316, {'train/accuracy': 0.6968271732330322, 'train/loss': 1.1760650873184204, 'validation/accuracy': 0.6461399793624878, 'validation/loss': 1.4500885009765625, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.1409730911254883, 'test/num_examples': 10000, 'score': 37287.26937747002, 'total_duration': 40237.470774650574, 'accumulated_submission_time': 37287.26937747002, 'accumulated_eval_time': 2933.338308572769, 'accumulated_logging_time': 7.024348258972168, 'global_step': 88316, 'preemption_count': 0}), (89634, {'train/accuracy': 0.6983218789100647, 'train/loss': 1.1913554668426514, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.466795802116394, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.1716911792755127, 'test/num_examples': 10000, 'score': 37797.17287135124, 'total_duration': 40784.16775941849, 'accumulated_submission_time': 37797.17287135124, 'accumulated_eval_time': 2969.9190514087677, 'accumulated_logging_time': 7.073339462280273, 'global_step': 89634, 'preemption_count': 0}), (90930, {'train/accuracy': 0.6973254084587097, 'train/loss': 1.1798337697982788, 'validation/accuracy': 0.6426199674606323, 'validation/loss': 1.4615617990493774, 'validation/num_examples': 50000, 'test/accuracy': 0.5235000252723694, 'test/loss': 2.1633799076080322, 'test/num_examples': 10000, 'score': 38307.24203848839, 'total_duration': 41332.04989647865, 'accumulated_submission_time': 38307.24203848839, 'accumulated_eval_time': 3007.5360400676727, 'accumulated_logging_time': 7.111950635910034, 'global_step': 90930, 'preemption_count': 0}), (92228, {'train/accuracy': 0.7051976919174194, 'train/loss': 1.148040771484375, 'validation/accuracy': 0.649899959564209, 'validation/loss': 1.4317659139633179, 'validation/num_examples': 50000, 'test/accuracy': 0.5205000042915344, 'test/loss': 2.1536617279052734, 'test/num_examples': 10000, 'score': 38817.4027094841, 'total_duration': 41875.944149017334, 'accumulated_submission_time': 38817.4027094841, 'accumulated_eval_time': 3041.0445544719696, 'accumulated_logging_time': 7.177502632141113, 'global_step': 92228, 'preemption_count': 0}), (93520, {'train/accuracy': 0.7074099183082581, 'train/loss': 1.1465513706207275, 'validation/accuracy': 0.6511399745941162, 'validation/loss': 1.4233825206756592, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.1446824073791504, 'test/num_examples': 10000, 'score': 39327.2528822422, 'total_duration': 42419.44781208038, 'accumulated_submission_time': 39327.2528822422, 'accumulated_eval_time': 3074.495703458786, 'accumulated_logging_time': 7.222236394882202, 'global_step': 93520, 'preemption_count': 0}), (94818, {'train/accuracy': 0.7074099183082581, 'train/loss': 1.1425827741622925, 'validation/accuracy': 0.6491599678993225, 'validation/loss': 1.4305096864700317, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.1142382621765137, 'test/num_examples': 10000, 'score': 39837.09855508804, 'total_duration': 42965.11723256111, 'accumulated_submission_time': 39837.09855508804, 'accumulated_eval_time': 3110.099593400955, 'accumulated_logging_time': 7.280663728713989, 'global_step': 94818, 'preemption_count': 0}), (96117, {'train/accuracy': 0.7081871628761292, 'train/loss': 1.1413853168487549, 'validation/accuracy': 0.6512199640274048, 'validation/loss': 1.4193195104599, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.103017568588257, 'test/num_examples': 10000, 'score': 40346.99287080765, 'total_duration': 43509.74643325806, 'accumulated_submission_time': 40346.99287080765, 'accumulated_eval_time': 3144.55699968338, 'accumulated_logging_time': 7.397853136062622, 'global_step': 96117, 'preemption_count': 0}), (97413, {'train/accuracy': 0.7104990482330322, 'train/loss': 1.1292941570281982, 'validation/accuracy': 0.6515399813652039, 'validation/loss': 1.4190757274627686, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.1343791484832764, 'test/num_examples': 10000, 'score': 40857.00880885124, 'total_duration': 44055.13583922386, 'accumulated_submission_time': 40857.00880885124, 'accumulated_eval_time': 3179.7153894901276, 'accumulated_logging_time': 7.4534080028533936, 'global_step': 97413, 'preemption_count': 0}), (98714, {'train/accuracy': 0.7137077450752258, 'train/loss': 1.1122026443481445, 'validation/accuracy': 0.6550799608230591, 'validation/loss': 1.4026848077774048, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.080806255340576, 'test/num_examples': 10000, 'score': 41366.90090370178, 'total_duration': 44602.56089806557, 'accumulated_submission_time': 41366.90090370178, 'accumulated_eval_time': 3217.0057003498077, 'accumulated_logging_time': 7.5330095291137695, 'global_step': 98714, 'preemption_count': 0}), (100009, {'train/accuracy': 0.7061742544174194, 'train/loss': 1.1462246179580688, 'validation/accuracy': 0.6499199867248535, 'validation/loss': 1.42534601688385, 'validation/num_examples': 50000, 'test/accuracy': 0.5236999988555908, 'test/loss': 2.113070487976074, 'test/num_examples': 10000, 'score': 41876.876831531525, 'total_duration': 45151.96375083923, 'accumulated_submission_time': 41876.876831531525, 'accumulated_eval_time': 3256.2163178920746, 'accumulated_logging_time': 7.588646411895752, 'global_step': 100009, 'preemption_count': 0}), (101305, {'train/accuracy': 0.7112563848495483, 'train/loss': 1.1239590644836426, 'validation/accuracy': 0.6556000113487244, 'validation/loss': 1.4065600633621216, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.123613119125366, 'test/num_examples': 10000, 'score': 42386.97715163231, 'total_duration': 45698.15100455284, 'accumulated_submission_time': 42386.97715163231, 'accumulated_eval_time': 3292.0426218509674, 'accumulated_logging_time': 7.685492753982544, 'global_step': 101305, 'preemption_count': 0}), (102610, {'train/accuracy': 0.7088049650192261, 'train/loss': 1.1257026195526123, 'validation/accuracy': 0.6520400047302246, 'validation/loss': 1.4184962511062622, 'validation/num_examples': 50000, 'test/accuracy': 0.5273000001907349, 'test/loss': 2.137503147125244, 'test/num_examples': 10000, 'score': 42896.77038502693, 'total_duration': 46241.6892080307, 'accumulated_submission_time': 42896.77038502693, 'accumulated_eval_time': 3325.5520470142365, 'accumulated_logging_time': 7.760801792144775, 'global_step': 102610, 'preemption_count': 0}), (103913, {'train/accuracy': 0.7117944955825806, 'train/loss': 1.1196249723434448, 'validation/accuracy': 0.655460000038147, 'validation/loss': 1.4164386987686157, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.1376898288726807, 'test/num_examples': 10000, 'score': 43406.62056684494, 'total_duration': 46786.47964453697, 'accumulated_submission_time': 43406.62056684494, 'accumulated_eval_time': 3360.2787458896637, 'accumulated_logging_time': 7.812975168228149, 'global_step': 103913, 'preemption_count': 0}), (105214, {'train/accuracy': 0.7189492583274841, 'train/loss': 1.089512825012207, 'validation/accuracy': 0.6606400012969971, 'validation/loss': 1.3863310813903809, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.097107410430908, 'test/num_examples': 10000, 'score': 43916.78245329857, 'total_duration': 47329.74892568588, 'accumulated_submission_time': 43916.78245329857, 'accumulated_eval_time': 3393.1550545692444, 'accumulated_logging_time': 7.883579730987549, 'global_step': 105214, 'preemption_count': 0}), (106516, {'train/accuracy': 0.7255460619926453, 'train/loss': 1.0444070100784302, 'validation/accuracy': 0.6671800017356873, 'validation/loss': 1.3501098155975342, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.0455236434936523, 'test/num_examples': 10000, 'score': 44426.897867918015, 'total_duration': 47876.97033929825, 'accumulated_submission_time': 44426.897867918015, 'accumulated_eval_time': 3430.034717321396, 'accumulated_logging_time': 7.948538541793823, 'global_step': 106516, 'preemption_count': 0}), (107823, {'train/accuracy': 0.725027859210968, 'train/loss': 1.0664734840393066, 'validation/accuracy': 0.6644999980926514, 'validation/loss': 1.3645597696304321, 'validation/num_examples': 50000, 'test/accuracy': 0.5415000319480896, 'test/loss': 2.0575408935546875, 'test/num_examples': 10000, 'score': 44936.8701236248, 'total_duration': 48420.34999704361, 'accumulated_submission_time': 44936.8701236248, 'accumulated_eval_time': 3463.219923019409, 'accumulated_logging_time': 8.012507438659668, 'global_step': 107823, 'preemption_count': 0}), (109136, {'train/accuracy': 0.7351921200752258, 'train/loss': 1.0098414421081543, 'validation/accuracy': 0.6715599894523621, 'validation/loss': 1.3341665267944336, 'validation/num_examples': 50000, 'test/accuracy': 0.5434000492095947, 'test/loss': 2.0304386615753174, 'test/num_examples': 10000, 'score': 45446.887754678726, 'total_duration': 48966.39695715904, 'accumulated_submission_time': 45446.887754678726, 'accumulated_eval_time': 3499.0338978767395, 'accumulated_logging_time': 8.067375421524048, 'global_step': 109136, 'preemption_count': 0}), (110436, {'train/accuracy': 0.7273596525192261, 'train/loss': 1.0478661060333252, 'validation/accuracy': 0.6682800054550171, 'validation/loss': 1.3483519554138184, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0515429973602295, 'test/num_examples': 10000, 'score': 45957.02745056152, 'total_duration': 49512.65373420715, 'accumulated_submission_time': 45957.02745056152, 'accumulated_eval_time': 3534.931508541107, 'accumulated_logging_time': 8.130779027938843, 'global_step': 110436, 'preemption_count': 0}), (111742, {'train/accuracy': 0.7318040132522583, 'train/loss': 1.0298599004745483, 'validation/accuracy': 0.668179988861084, 'validation/loss': 1.3314546346664429, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.024582862854004, 'test/num_examples': 10000, 'score': 46466.828177928925, 'total_duration': 50055.99768257141, 'accumulated_submission_time': 46466.828177928925, 'accumulated_eval_time': 3568.2362315654755, 'accumulated_logging_time': 8.21045994758606, 'global_step': 111742, 'preemption_count': 0}), (113032, {'train/accuracy': 0.7305285334587097, 'train/loss': 1.0380269289016724, 'validation/accuracy': 0.6700800061225891, 'validation/loss': 1.3370519876480103, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.0114362239837646, 'test/num_examples': 10000, 'score': 46976.71987295151, 'total_duration': 50600.80140542984, 'accumulated_submission_time': 46976.71987295151, 'accumulated_eval_time': 3602.934042453766, 'accumulated_logging_time': 8.26657247543335, 'global_step': 113032, 'preemption_count': 0}), (114317, {'train/accuracy': 0.7356504797935486, 'train/loss': 1.0100760459899902, 'validation/accuracy': 0.6712999939918518, 'validation/loss': 1.3365951776504517, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.0522353649139404, 'test/num_examples': 10000, 'score': 47486.85285496712, 'total_duration': 51143.89784693718, 'accumulated_submission_time': 47486.85285496712, 'accumulated_eval_time': 3635.684566259384, 'accumulated_logging_time': 8.321794986724854, 'global_step': 114317, 'preemption_count': 0}), (115602, {'train/accuracy': 0.7401546239852905, 'train/loss': 0.9764506220817566, 'validation/accuracy': 0.6774199604988098, 'validation/loss': 1.3063912391662598, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.0029489994049072, 'test/num_examples': 10000, 'score': 47996.67247462273, 'total_duration': 51687.74786758423, 'accumulated_submission_time': 47996.67247462273, 'accumulated_eval_time': 3669.4179055690765, 'accumulated_logging_time': 8.460081338882446, 'global_step': 115602, 'preemption_count': 0}), (116886, {'train/accuracy': 0.744160532951355, 'train/loss': 0.9764852523803711, 'validation/accuracy': 0.679099977016449, 'validation/loss': 1.3015769720077515, 'validation/num_examples': 50000, 'test/accuracy': 0.5511000156402588, 'test/loss': 1.9918854236602783, 'test/num_examples': 10000, 'score': 48506.53856897354, 'total_duration': 52232.97290062904, 'accumulated_submission_time': 48506.53856897354, 'accumulated_eval_time': 3704.535264492035, 'accumulated_logging_time': 8.543070554733276, 'global_step': 116886, 'preemption_count': 0}), (118174, {'train/accuracy': 0.7464524507522583, 'train/loss': 0.9514623880386353, 'validation/accuracy': 0.6811800003051758, 'validation/loss': 1.2951253652572632, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 2.0076942443847656, 'test/num_examples': 10000, 'score': 49016.38579249382, 'total_duration': 52779.118599653244, 'accumulated_submission_time': 49016.38579249382, 'accumulated_eval_time': 3740.5892341136932, 'accumulated_logging_time': 8.627598762512207, 'global_step': 118174, 'preemption_count': 0}), (119462, {'train/accuracy': 0.7521523833274841, 'train/loss': 0.9495605230331421, 'validation/accuracy': 0.6805599927902222, 'validation/loss': 1.2911232709884644, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 1.9860697984695435, 'test/num_examples': 10000, 'score': 49526.444617033005, 'total_duration': 53324.22105407715, 'accumulated_submission_time': 49526.444617033005, 'accumulated_eval_time': 3775.4403302669525, 'accumulated_logging_time': 8.662558555603027, 'global_step': 119462, 'preemption_count': 0}), (120752, {'train/accuracy': 0.7479073405265808, 'train/loss': 0.9605219960212708, 'validation/accuracy': 0.6820600032806396, 'validation/loss': 1.2853432893753052, 'validation/num_examples': 50000, 'test/accuracy': 0.5590000152587891, 'test/loss': 1.975382924079895, 'test/num_examples': 10000, 'score': 50036.53489971161, 'total_duration': 53866.48399853706, 'accumulated_submission_time': 50036.53489971161, 'accumulated_eval_time': 3807.4050166606903, 'accumulated_logging_time': 8.712469816207886, 'global_step': 120752, 'preemption_count': 0}), (122047, {'train/accuracy': 0.7556002736091614, 'train/loss': 0.9197654724121094, 'validation/accuracy': 0.6878799796104431, 'validation/loss': 1.2549914121627808, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 1.9407501220703125, 'test/num_examples': 10000, 'score': 50545.90423154831, 'total_duration': 54413.393464803696, 'accumulated_submission_time': 50545.90423154831, 'accumulated_eval_time': 3844.143002271652, 'accumulated_logging_time': 9.354947805404663, 'global_step': 122047, 'preemption_count': 0}), (123353, {'train/accuracy': 0.759207546710968, 'train/loss': 0.9096981287002563, 'validation/accuracy': 0.6882199645042419, 'validation/loss': 1.2617321014404297, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 1.9792543649673462, 'test/num_examples': 10000, 'score': 51055.950160980225, 'total_duration': 54956.97474074364, 'accumulated_submission_time': 51055.950160980225, 'accumulated_eval_time': 3877.4354162216187, 'accumulated_logging_time': 9.440228700637817, 'global_step': 123353, 'preemption_count': 0}), (124654, {'train/accuracy': 0.7636519074440002, 'train/loss': 0.890670895576477, 'validation/accuracy': 0.6882799863815308, 'validation/loss': 1.2617956399917603, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9657535552978516, 'test/num_examples': 10000, 'score': 51565.887436151505, 'total_duration': 55501.91971373558, 'accumulated_submission_time': 51565.887436151505, 'accumulated_eval_time': 3912.2268607616425, 'accumulated_logging_time': 9.495509624481201, 'global_step': 124654, 'preemption_count': 0}), (125940, {'train/accuracy': 0.7603037357330322, 'train/loss': 0.9058126211166382, 'validation/accuracy': 0.6906599998474121, 'validation/loss': 1.2521040439605713, 'validation/num_examples': 50000, 'test/accuracy': 0.560699999332428, 'test/loss': 1.9706153869628906, 'test/num_examples': 10000, 'score': 52076.00114798546, 'total_duration': 56045.42695713043, 'accumulated_submission_time': 52076.00114798546, 'accumulated_eval_time': 3945.4093585014343, 'accumulated_logging_time': 9.546129941940308, 'global_step': 125940, 'preemption_count': 0}), (127228, {'train/accuracy': 0.7605229616165161, 'train/loss': 0.897878110408783, 'validation/accuracy': 0.6896599531173706, 'validation/loss': 1.2560820579528809, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 1.9651907682418823, 'test/num_examples': 10000, 'score': 52585.853692770004, 'total_duration': 56589.78621697426, 'accumulated_submission_time': 52585.853692770004, 'accumulated_eval_time': 3979.689565896988, 'accumulated_logging_time': 9.615399837493896, 'global_step': 127228, 'preemption_count': 0}), (128519, {'train/accuracy': 0.7751514315605164, 'train/loss': 0.8432853817939758, 'validation/accuracy': 0.698419988155365, 'validation/loss': 1.2087401151657104, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.894498586654663, 'test/num_examples': 10000, 'score': 53095.70389556885, 'total_duration': 57136.17261147499, 'accumulated_submission_time': 53095.70389556885, 'accumulated_eval_time': 4016.0058567523956, 'accumulated_logging_time': 9.67530870437622, 'global_step': 128519, 'preemption_count': 0}), (129812, {'train/accuracy': 0.7685546875, 'train/loss': 0.8609792590141296, 'validation/accuracy': 0.6956799626350403, 'validation/loss': 1.2237547636032104, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9257469177246094, 'test/num_examples': 10000, 'score': 53605.86897301674, 'total_duration': 57678.17271089554, 'accumulated_submission_time': 53605.86897301674, 'accumulated_eval_time': 4047.5923838615417, 'accumulated_logging_time': 9.763998985290527, 'global_step': 129812, 'preemption_count': 0}), (131108, {'train/accuracy': 0.7782405614852905, 'train/loss': 0.8348714113235474, 'validation/accuracy': 0.6997999548912048, 'validation/loss': 1.2101482152938843, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 1.904631495475769, 'test/num_examples': 10000, 'score': 54115.65716171265, 'total_duration': 58224.411308288574, 'accumulated_submission_time': 54115.65716171265, 'accumulated_eval_time': 4083.7807126045227, 'accumulated_logging_time': 9.86639666557312, 'global_step': 131108, 'preemption_count': 0}), (132397, {'train/accuracy': 0.7766461968421936, 'train/loss': 0.830456018447876, 'validation/accuracy': 0.698639988899231, 'validation/loss': 1.204158902168274, 'validation/num_examples': 50000, 'test/accuracy': 0.5768000483512878, 'test/loss': 1.885985255241394, 'test/num_examples': 10000, 'score': 54625.58908200264, 'total_duration': 58769.19275927544, 'accumulated_submission_time': 54625.58908200264, 'accumulated_eval_time': 4118.371031522751, 'accumulated_logging_time': 9.967960357666016, 'global_step': 132397, 'preemption_count': 0}), (133688, {'train/accuracy': 0.7813296914100647, 'train/loss': 0.8155295252799988, 'validation/accuracy': 0.6997599601745605, 'validation/loss': 1.2029445171356201, 'validation/num_examples': 50000, 'test/accuracy': 0.5756000280380249, 'test/loss': 1.901422381401062, 'test/num_examples': 10000, 'score': 55135.537725925446, 'total_duration': 59315.28038811684, 'accumulated_submission_time': 55135.537725925446, 'accumulated_eval_time': 4154.2966051101685, 'accumulated_logging_time': 10.023295640945435, 'global_step': 133688, 'preemption_count': 0}), (134970, {'train/accuracy': 0.7872887253761292, 'train/loss': 0.7722330689430237, 'validation/accuracy': 0.7044199705123901, 'validation/loss': 1.199290156364441, 'validation/num_examples': 50000, 'test/accuracy': 0.5774000287055969, 'test/loss': 1.885286569595337, 'test/num_examples': 10000, 'score': 55645.414152383804, 'total_duration': 59859.47259473801, 'accumulated_submission_time': 55645.414152383804, 'accumulated_eval_time': 4188.356557846069, 'accumulated_logging_time': 10.117146015167236, 'global_step': 134970, 'preemption_count': 0}), (136247, {'train/accuracy': 0.7858338356018066, 'train/loss': 0.7971621155738831, 'validation/accuracy': 0.7060199975967407, 'validation/loss': 1.1900910139083862, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.882903814315796, 'test/num_examples': 10000, 'score': 56155.26686882973, 'total_duration': 60401.994733810425, 'accumulated_submission_time': 56155.26686882973, 'accumulated_eval_time': 4220.815694093704, 'accumulated_logging_time': 10.169111013412476, 'global_step': 136247, 'preemption_count': 0}), (137527, {'train/accuracy': 0.7935865521430969, 'train/loss': 0.7578516602516174, 'validation/accuracy': 0.7087999582290649, 'validation/loss': 1.1751463413238525, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.8559602499008179, 'test/num_examples': 10000, 'score': 56665.20880031586, 'total_duration': 60952.933406829834, 'accumulated_submission_time': 56665.20880031586, 'accumulated_eval_time': 4261.597877979279, 'accumulated_logging_time': 10.225021123886108, 'global_step': 137527, 'preemption_count': 0}), (138804, {'train/accuracy': 0.7951211333274841, 'train/loss': 0.7586897611618042, 'validation/accuracy': 0.7091799974441528, 'validation/loss': 1.1667213439941406, 'validation/num_examples': 50000, 'test/accuracy': 0.5847000479698181, 'test/loss': 1.8426032066345215, 'test/num_examples': 10000, 'score': 57175.09089779854, 'total_duration': 61497.39592576027, 'accumulated_submission_time': 57175.09089779854, 'accumulated_eval_time': 4295.9396686553955, 'accumulated_logging_time': 10.307328939437866, 'global_step': 138804, 'preemption_count': 0}), (140087, {'train/accuracy': 0.7984095811843872, 'train/loss': 0.7405584454536438, 'validation/accuracy': 0.7064200043678284, 'validation/loss': 1.1850301027297974, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.8664261102676392, 'test/num_examples': 10000, 'score': 57685.15996623039, 'total_duration': 62042.82602930069, 'accumulated_submission_time': 57685.15996623039, 'accumulated_eval_time': 4331.053107976913, 'accumulated_logging_time': 10.394988298416138, 'global_step': 140087, 'preemption_count': 0}), (141370, {'train/accuracy': 0.8007014989852905, 'train/loss': 0.7408760786056519, 'validation/accuracy': 0.7092999815940857, 'validation/loss': 1.165321946144104, 'validation/num_examples': 50000, 'test/accuracy': 0.5843999981880188, 'test/loss': 1.8240811824798584, 'test/num_examples': 10000, 'score': 58195.09639596939, 'total_duration': 62586.75109219551, 'accumulated_submission_time': 58195.09639596939, 'accumulated_eval_time': 4364.7945556640625, 'accumulated_logging_time': 10.48622989654541, 'global_step': 141370, 'preemption_count': 0}), (142644, {'train/accuracy': 0.8115234375, 'train/loss': 0.6917855143547058, 'validation/accuracy': 0.7137799859046936, 'validation/loss': 1.137590765953064, 'validation/num_examples': 50000, 'test/accuracy': 0.5895000100135803, 'test/loss': 1.818124532699585, 'test/num_examples': 10000, 'score': 58705.022627830505, 'total_duration': 63128.03783941269, 'accumulated_submission_time': 58705.022627830505, 'accumulated_eval_time': 4395.936160326004, 'accumulated_logging_time': 10.548065185546875, 'global_step': 142644, 'preemption_count': 0}), (143935, {'train/accuracy': 0.8088328838348389, 'train/loss': 0.706734299659729, 'validation/accuracy': 0.7102599740028381, 'validation/loss': 1.1563358306884766, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 1.8385635614395142, 'test/num_examples': 10000, 'score': 59215.257622241974, 'total_duration': 63672.30200910568, 'accumulated_submission_time': 59215.257622241974, 'accumulated_eval_time': 4429.734256267548, 'accumulated_logging_time': 10.622171640396118, 'global_step': 143935, 'preemption_count': 0}), (145211, {'train/accuracy': 0.8124003410339355, 'train/loss': 0.6877221465110779, 'validation/accuracy': 0.7174400091171265, 'validation/loss': 1.131879210472107, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 1.813792109489441, 'test/num_examples': 10000, 'score': 59725.05596446991, 'total_duration': 64216.73625707626, 'accumulated_submission_time': 59725.05596446991, 'accumulated_eval_time': 4464.152533054352, 'accumulated_logging_time': 10.679142236709595, 'global_step': 145211, 'preemption_count': 0}), (146492, {'train/accuracy': 0.8207908272743225, 'train/loss': 0.6453962326049805, 'validation/accuracy': 0.7214599847793579, 'validation/loss': 1.1162569522857666, 'validation/num_examples': 50000, 'test/accuracy': 0.5996000170707703, 'test/loss': 1.7889827489852905, 'test/num_examples': 10000, 'score': 60234.96898651123, 'total_duration': 64758.7116651535, 'accumulated_submission_time': 60234.96898651123, 'accumulated_eval_time': 4495.981732130051, 'accumulated_logging_time': 10.755004405975342, 'global_step': 146492, 'preemption_count': 0}), (147781, {'train/accuracy': 0.8203921914100647, 'train/loss': 0.6459478139877319, 'validation/accuracy': 0.7193399667739868, 'validation/loss': 1.1231507062911987, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.82082200050354, 'test/num_examples': 10000, 'score': 60744.82824254036, 'total_duration': 65301.6953830719, 'accumulated_submission_time': 60744.82824254036, 'accumulated_eval_time': 4528.896654129028, 'accumulated_logging_time': 10.804770469665527, 'global_step': 147781, 'preemption_count': 0}), (149057, {'train/accuracy': 0.8243383169174194, 'train/loss': 0.6349853873252869, 'validation/accuracy': 0.721780002117157, 'validation/loss': 1.1156972646713257, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.818621039390564, 'test/num_examples': 10000, 'score': 61254.92183613777, 'total_duration': 65844.91651058197, 'accumulated_submission_time': 61254.92183613777, 'accumulated_eval_time': 4561.7955157756805, 'accumulated_logging_time': 10.871318340301514, 'global_step': 149057, 'preemption_count': 0}), (150334, {'train/accuracy': 0.8295400142669678, 'train/loss': 0.6065824031829834, 'validation/accuracy': 0.7263399958610535, 'validation/loss': 1.0979609489440918, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.796953797340393, 'test/num_examples': 10000, 'score': 61764.951920986176, 'total_duration': 66390.65845131874, 'accumulated_submission_time': 61764.951920986176, 'accumulated_eval_time': 4597.268939495087, 'accumulated_logging_time': 10.951146841049194, 'global_step': 150334, 'preemption_count': 0}), (151609, {'train/accuracy': 0.8327686190605164, 'train/loss': 0.602982223033905, 'validation/accuracy': 0.7255799770355225, 'validation/loss': 1.0966386795043945, 'validation/num_examples': 50000, 'test/accuracy': 0.5989000201225281, 'test/loss': 1.7889082431793213, 'test/num_examples': 10000, 'score': 62275.03699326515, 'total_duration': 66934.13870358467, 'accumulated_submission_time': 62275.03699326515, 'accumulated_eval_time': 4630.4554743766785, 'accumulated_logging_time': 11.000649690628052, 'global_step': 151609, 'preemption_count': 0}), (152883, {'train/accuracy': 0.8391461968421936, 'train/loss': 0.5757182836532593, 'validation/accuracy': 0.7305200099945068, 'validation/loss': 1.0808967351913452, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.7599254846572876, 'test/num_examples': 10000, 'score': 62784.98686337471, 'total_duration': 67477.69480133057, 'accumulated_submission_time': 62784.98686337471, 'accumulated_eval_time': 4663.8260016441345, 'accumulated_logging_time': 11.07994270324707, 'global_step': 152883, 'preemption_count': 0}), (154157, {'train/accuracy': 0.8438097834587097, 'train/loss': 0.5621725916862488, 'validation/accuracy': 0.7273600101470947, 'validation/loss': 1.0979546308517456, 'validation/num_examples': 50000, 'test/accuracy': 0.6049000024795532, 'test/loss': 1.778617262840271, 'test/num_examples': 10000, 'score': 63294.97476530075, 'total_duration': 68021.79338860512, 'accumulated_submission_time': 63294.97476530075, 'accumulated_eval_time': 4697.68891620636, 'accumulated_logging_time': 11.17039442062378, 'global_step': 154157, 'preemption_count': 0}), (155445, {'train/accuracy': 0.8530173897743225, 'train/loss': 0.5314945578575134, 'validation/accuracy': 0.7315199971199036, 'validation/loss': 1.0715699195861816, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.7311569452285767, 'test/num_examples': 10000, 'score': 63805.09834551811, 'total_duration': 68562.676060915, 'accumulated_submission_time': 63805.09834551811, 'accumulated_eval_time': 4728.1781396865845, 'accumulated_logging_time': 11.278674840927124, 'global_step': 155445, 'preemption_count': 0}), (156733, {'train/accuracy': 0.8538544178009033, 'train/loss': 0.523781955242157, 'validation/accuracy': 0.7325199842453003, 'validation/loss': 1.0809742212295532, 'validation/num_examples': 50000, 'test/accuracy': 0.6030000448226929, 'test/loss': 1.7881934642791748, 'test/num_examples': 10000, 'score': 64315.171521663666, 'total_duration': 69105.88829541206, 'accumulated_submission_time': 64315.171521663666, 'accumulated_eval_time': 4761.094323158264, 'accumulated_logging_time': 11.342117309570312, 'global_step': 156733, 'preemption_count': 0}), (158016, {'train/accuracy': 0.8593351244926453, 'train/loss': 0.5009995698928833, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.0563217401504517, 'validation/num_examples': 50000, 'test/accuracy': 0.6119000315666199, 'test/loss': 1.7387914657592773, 'test/num_examples': 10000, 'score': 64824.97560667992, 'total_duration': 69650.43163871765, 'accumulated_submission_time': 64824.97560667992, 'accumulated_eval_time': 4795.602059841156, 'accumulated_logging_time': 11.415817260742188, 'global_step': 158016, 'preemption_count': 0}), (159304, {'train/accuracy': 0.8622847199440002, 'train/loss': 0.49015215039253235, 'validation/accuracy': 0.7390799522399902, 'validation/loss': 1.048416018486023, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.763635277748108, 'test/num_examples': 10000, 'score': 65334.89833903313, 'total_duration': 70192.82166719437, 'accumulated_submission_time': 65334.89833903313, 'accumulated_eval_time': 4827.854813575745, 'accumulated_logging_time': 11.475934743881226, 'global_step': 159304, 'preemption_count': 0}), (160591, {'train/accuracy': 0.8680644035339355, 'train/loss': 0.4690193831920624, 'validation/accuracy': 0.7403599619865417, 'validation/loss': 1.0462883710861206, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.7406188249588013, 'test/num_examples': 10000, 'score': 65844.75536394119, 'total_duration': 70735.81380200386, 'accumulated_submission_time': 65844.75536394119, 'accumulated_eval_time': 4860.79319190979, 'accumulated_logging_time': 11.51626706123352, 'global_step': 160591, 'preemption_count': 0})], 'global_step': 161872}
I0307 22:25:42.325809 140252175811776 submission_runner.py:649] Timing: 66354.59686398506
I0307 22:25:42.325855 140252175811776 submission_runner.py:651] Total number of evals: 130
I0307 22:25:42.325884 140252175811776 submission_runner.py:652] ====================
I0307 22:25:42.326112 140252175811776 submission_runner.py:750] Final imagenet_resnet score: 2
